{
  "date": "2024-04-13",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-13 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 36 篇论文，主要聚焦于 AI 模型优化（如 LLM 在多模态任务中的创新）、计算机视觉应用（如视频识别和遥感变化检测）、医疗诊断技术，以及环境风险评估；重点包括 CuriousLLM 的知识图谱增强问答方法和 Intellecta 数据集的提出，这些文章展示了 LLM 在实际应用中的潜力，并有部分论文被 NAACL 2025 和 CVPR 2024 等会议接受。\n\n### 重点论文推荐\n我们先聊聊那些重要、话题度高或有潜在影响的论文，尤其是涉及 LLM 和 AI 创新的。\n\n- **CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced Knowledge Graph Reasoning**（中文：CuriousLLM：利用 LLM 增强知识图谱推理提升多文档问答）  \n  这篇论文提出了一种基于好奇心驱动的 LLM 代理模型，通过生成后续问题优化信息检索，避免了传统方法的幻觉问题和计算开销。主要贡献是新数据集 Follow-upQA 的开发，以及在多文档问答任务中显著提升性能，被 NAACL 2025 接受。\n\n- **Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning**（中文：Intellecta Cognitiva：一个用于提升学术知识和机器推理的综合数据集）  \n  作者 Ajmal PS 等构建了 115 亿 tokens 的合成数据集，结合教科书数据，用于提升 LLM 的推理和教育叙事生成。该数据集强调了合成数据的潜力，提供了一个大规模、道德合规的资源，支持 LLM 在复杂任务中的训练。\n\n- **Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning**（中文：基于直觉的 Rank-1 专家混合模型，用于高效参数微调）  \n  这篇论文引入了 Intuition-MoR1E 框架，利用人类认知灵感优化 LLM 多任务微调，通过语义聚类和 Rank-1 专家减少参数需求。主要发现是其在 14 个数据集上比基线提升 2.15% 的准确率，展示了高效多任务学习的潜力。\n\n- **Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal Pre-Training Approach**（中文：面向高效简历理解的多粒度多模态预训练方法）  \n  作者提出 ERU 模型，通过布局感知的多模态融合和自监督任务提取简历结构信息。主要贡献是提高了简历信息提取的效率和准确性，被 ICME 2024 接受，适用于在线招聘场景。\n\n- **NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT**（中文：NeurIT：提升神经惯性跟踪在室内机器人 IoT 中的极限）  \n  这篇论文开发了 Time-Frequency Block-recurrent Transformer 框架，用于高效处理事件数据，实现室内机器人定位。主要发现是其在各种环境下比基线提升 48.21% 的准确率，并开源了代码和数据集，推动了机器人 IoT 的实际应用。\n\n### 其他相关论文简述\n接下来，我们快速聊聊其他论文，按主题归类，优先选取有实际影响力的，跳过较理论化或冗长内容。\n\n- **AI 和计算机视觉领域**  \n  - **Exploring Explainability in Video Action Recognition**（中文：探索视频动作识别的可解释性）  \n    论文扩展了 Video-TCAV 方法，量化视频动作模型中概念的重要性，并通过机器辅助生成时空概念。主要发现是动态概念比静态概念更有效，被 CVPR 2024 接受，提升了视频 AI 的透明度。\n  - **Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective**（中文：从扩散桥模型视角重新思考迭代立体匹配）  \n    作者设计了 T-GRU 和注意力网络，融入扩散模型优化立体匹配，实现了 Scene Flow 数据集上的 7% 改进，强调了高精度深度图生成。\n  - **Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery**（中文：超越已知聚类：探测新原型用于高效泛化类发现）  \n    提出自监督原型学习框架，解决聚类不足问题，在多个数据集上提升性能，并开源代码。\n  - **ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model**（中文：ChangeAnywhere：利用语义潜在扩散模型生成遥感变化检测样本）  \n    该方法使用单时相图像生成双时相数据集，提高了遥感变化检测的泛化能力，显著提升了模型在基准数据集上的表现。\n\n- **医疗和生物应用**  \n  - **Advanced Neural Network Architecture for Enhanced Multi-Lead ECG Arrhythmia Detection**（中文：增强多导联 ECG 心律失常检测的高级神经网络架构）  \n    论文设计了六层 CNN 模型，实现了对五种心跳类型的精确分类，提高了心律失常诊断的准确性。\n  - **Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance**（中文：智能辅助：用于家庭机器人主动适配的策略对手建模）  \n    引入对手建模模块，使机器人根据人类能力提供自适应帮助，实验证明了其在模拟环境中的优越性。\n  - **Adapting Mental Health Prediction for Cross-lingual Learning**（中文：适应跨语言学习的心理健康预测）  \n    通过元学习和 LLM 的 in-context 学习，改进了低资源语言（如斯瓦希里语）的心理健康预测，F1 分数比基线提升 18%。\n\n- **其他领域快速掠过**  \n  其余论文如气候风险评估（Assessing Climate Transition Risks in the Colombian Processed Food Sector）、合成数据生成（A Lightweight Spatiotemporal Network）和理论综述（Theoretical research on generative diffusion models）等，虽然有贡献，但影响力较小，我们简要提及：前者使用模糊逻辑评估食品行业风险，强调碳税影响；后者讨论了扩散模型的训练和采样方法；这些领域的论文未见特别突破性进展，故不展开。\n\n今天的 arXiv 快报到此结束，AI 领域的创新尤其值得关注。如果您对某个主题感兴趣，建议查看原文深入阅读！",
  "papers": [
    {
      "arxiv_id": "2404.09101v1",
      "title": "Mixture of Experts Soften the Curse of Dimensionality in Operator Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Anastasis Kratsios",
        "Takashi Furuya",
        "Jose Antonio Lara Benitez",
        "Matti Lassas",
        "Maarten de Hoop"
      ],
      "abstract": "In this paper, we construct a mixture of neural operators (MoNOs) between\nfunction spaces whose complexity is distributed over a network of expert neural\noperators (NOs), with each NO satisfying parameter scaling restrictions. Our\nmain result is a \\textit{distributed} universal approximation theorem\nguaranteeing that any Lipschitz non-linear operator between $L^2([0,1]^d)$\nspaces can be approximated uniformly over the Sobolev unit ball therein, to any\ngiven $\\varepsilon>0$ accuracy, by an MoNO while satisfying the constraint\nthat: each expert NO has a depth, width, and rank of\n$\\mathcal{O}(\\varepsilon^{-1})$. Naturally, our result implies that the\nrequired number of experts must be large, however, each NO is guaranteed to be\nsmall enough to be loadable into the active memory of most computers for\nreasonable accuracies $\\varepsilon$. During our analysis, we also obtain new\nquantitative expression rates for classical NOs approximating uniformly\ncontinuous non-linear operators uniformly on compact subsets of $L^2([0,1]^d)$.",
      "tldr_zh": "本论文提出了Mixture of Experts（MoE）框架，通过构建Mixture of Neural Operators（MoNOs），将复杂度分布在多个专家神经算子（expert NOs）网络中，每个NO的参数规模（如深度、宽度和秩）限制在O(ε^{-1})，从而缓解Operator Learning中的维数诅咒。研究的主要贡献是一个分布式通用逼近定理（distributed universal approximation theorem），证明任何Lipschitz非线性算子在L^2([0,1]^d)空间之间，可以在Sobolev单位球上均匀逼近到任意ε>0的精度，同时确保每个NO足够小以适应大多数计算机的活跃内存。实验分析还提供了经典NOs逼近均匀连续非线性算子的全新定量表达式率，提升了高维函数空间学习的效率和可行性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NA",
        "math.NA",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09101v1",
      "published_date": "2024-04-13 23:20:16 UTC",
      "updated_date": "2024-04-13 23:20:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:05:05.848832"
    },
    {
      "arxiv_id": "2404.09091v2",
      "title": "Semantic In-Domain Product Identification for Search Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Sanat Sharma",
        "Jayant Kumar",
        "Twisha Naik",
        "Zhaoyu Lu",
        "Arvind Srikantan",
        "Tracy Holloway King"
      ],
      "abstract": "Accurate explicit and implicit product identification in search queries is\ncritical for enhancing user experiences, especially at a company like Adobe\nwhich has over 50 products and covers queries across hundreds of tools. In this\nwork, we present a novel approach to training a product classifier from user\nbehavioral data. Our semantic model led to >25% relative improvement in CTR\n(click through rate) across the deployed surfaces; a >50% decrease in null\nrate; a 2x increase in the app cards surfaced, which helps drive product\nvisibility.",
      "tldr_zh": "这篇论文提出了一种基于用户行为数据的语义产品分类器，用于在搜索查询中准确识别显式和隐式产品，尤其适用于如Adobe这样的多产品环境。方法通过训练模型从用户行为中提取信息，显著提升了查询处理效率。实验结果显示，该模型使点击率(CTR)相对提高了超过25%，null rate降低了50%以上，并使app cards surfaced增加一倍，从而增强了产品可见性和用户体验。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09091v2",
      "published_date": "2024-04-13 22:18:14 UTC",
      "updated_date": "2024-05-29 16:01:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:05:17.073631"
    },
    {
      "arxiv_id": "2404.16055v1",
      "title": "Assessing Climate Transition Risks in the Colombian Processed Food Sector: A Fuzzy Logic and Multicriteria Decision-Making Approach",
      "title_zh": "评估哥伦比亚加工食品部门的气候转型风险：一种模糊逻辑和多准则决策方法",
      "authors": [
        "Juan F. Pérez-Pérez",
        "Pablo Isaza Gómez",
        "Isis Bonet",
        "María Solange Sánchez-Pinzón",
        "Fabio Caraffini",
        "Christian Lochmuller"
      ],
      "abstract": "Climate risk assessment is becoming increasingly important. For\norganisations, identifying and assessing climate-related risks is challenging,\nas they can come from multiple sources. This study identifies and assesses the\nmain climate transition risks in the colombian processed food sector. As\ntransition risks are vague, our approach uses Fuzzy Logic and compares it to\nvarious multi-criteria decision-making methods to classify the different\nclimate transition risks an organisation may be exposed to. This approach\nallows us to use linguistic expressions for risk analysis and to better\ndescribe risks and their consequences. The results show that the risks ranked\nas the most critical for this organisation in their order were price volatility\nand raw materials availability, the change to less carbon-intensive production\nor consumption patterns, the increase in carbon taxes and technological change,\nand the associated development or implementation costs. These risks show a\ncritical risk level, which implies that they are the most significant risks for\nthe organisation in the case study. These results highlight the importance of\ninvestments needed to meet regulatory requirements, which are the main drivers\nfor organisations at the financial level.",
      "tldr_zh": "这篇论文评估了哥伦比亚加工食品行业的气候转型风险，使用 Fuzzy Logic 和 Multicriteria Decision-Making 方法来识别和分类这些模糊风险。研究通过语言表达分析风险及其后果，发现最关键的风险依次包括价格波动和原材料可用性、转向低碳生产模式、碳税增加以及技术变革的成本。这些风险被评为高危级别，强调了组织需进行重大投资以应对监管要求，从而提升金融和运营韧性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.16055v1",
      "published_date": "2024-04-13 21:49:49 UTC",
      "updated_date": "2024-04-13 21:49:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:05:28.899670"
    },
    {
      "arxiv_id": "2404.09077v3",
      "title": "CuriousLLM: Elevating Multi-Document Question Answering with LLM-Enhanced Knowledge Graph Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zukang Yang",
        "Zixuan Zhu",
        "Xuan Zhu"
      ],
      "abstract": "Large Language Models (LLMs) have achieved significant success in open-domain\nquestion answering. However, they continue to face challenges such as\nhallucinations and knowledge cutoffs. These issues can be mitigated through\nin-context learning by providing LLMs with relevant context before generating\nanswers. Recent literature proposes Knowledge Graph Prompting (KGP) which\nintegrates knowledge graphs with an LLM-based traversal agent to substantially\nenhance document retrieval quality. However, KGP requires costly fine-tuning\nwith large datasets and remains prone to hallucination. In this paper, we\npropose CuriousLLM, an enhancement that integrates a curiosity-driven reasoning\nmechanism into an LLM agent. This mechanism enables the agent to generate\nrelevant follow-up questions, thereby guiding the information retrieval process\nmore efficiently. Central to our approach is the development of the new\nFollow-upQA dataset, which includes questions and supporting evidence as input,\nwith follow-up questions serving as ground truths. These follow-up questions\neither inquire about what is still missing to fully answer the user's query or\nuse special tokens to signify that the retrieved evidence is sufficient. Our\nexperiments show that CuriousLLM significantly boosts LLM performance in\nmulti-document question answering (MD-QA), circumventing the substantial\ncomputational costs and latency from the original KGP framework.",
      "tldr_zh": "该论文提出 CuriousLLM，一种通过好奇心驱动推理机制提升多文档问答（MD-QA）的框架，以解决 Large Language Models (LLMs) 的幻觉（hallucinations）和知识截止（knowledge cutoffs）问题。CuriousLLM 让 LLM agent 生成相关后续问题（follow-up questions），从而更高效地指导信息检索过程，避免了 Knowledge Graph Prompting (KGP) 的高微调成本和延迟。研究者开发了新的 Follow-upQA 数据集，提供问题、证据和后续问题作为 ground truths，用于评估系统。实验结果显示，CuriousLLM 显著提高了 MD-QA 的性能，为更可靠的问答系统奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication in NAACL 2025. The official version will be\n  available in the ACL Anthology",
      "pdf_url": "http://arxiv.org/pdf/2404.09077v3",
      "published_date": "2024-04-13 20:43:46 UTC",
      "updated_date": "2025-02-18 06:52:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:05:42.188648"
    },
    {
      "arxiv_id": "2404.15347v1",
      "title": "Advanced Neural Network Architecture for Enhanced Multi-Lead ECG Arrhythmia Detection through Optimized Feature Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Bhavith Chandra Challagundla"
      ],
      "abstract": "Cardiovascular diseases are a pervasive global health concern, contributing\nsignificantly to morbidity and mortality rates worldwide. Among these\nconditions, arrhythmia, characterized by irregular heart rhythms, presents\nformidable diagnostic challenges. This study introduces an innovative approach\nutilizing deep learning techniques, specifically Convolutional Neural Networks\n(CNNs), to address the complexities of arrhythmia classification. Leveraging\nmulti-lead Electrocardiogram (ECG) data, our CNN model, comprising six layers\nwith a residual block, demonstrates promising outcomes in identifying five\ndistinct heartbeat types: Left Bundle Branch Block (LBBB), Right Bundle Branch\nBlock (RBBB), Atrial Premature Contraction (APC), Premature Ventricular\nContraction (PVC), and Normal Beat. Through rigorous experimentation, we\nhighlight the transformative potential of our methodology in enhancing\ndiagnostic accuracy for cardiovascular arrhythmias. Arrhythmia diagnosis\nremains a critical challenge in cardiovascular care, often relying on manual\ninterpretation of ECG signals, which can be time-consuming and prone to\nsubjectivity. To address these limitations, we propose a novel approach that\nleverages deep learning algorithms to automate arrhythmia classification. By\nemploying advanced CNN architectures and multi-lead ECG data, our methodology\noffers a robust solution for precise and efficient arrhythmia detection.\nThrough comprehensive evaluation, we demonstrate the effectiveness of our\napproach in facilitating more accurate clinical decision-making, thereby\nimproving patient outcomes in managing cardiovascular arrhythmias.",
      "tldr_zh": "本研究针对心律失常诊断的挑战，提出了一种先进的神经网络架构，利用多导联 ECG 数据进行优化特征提取，以提升分类准确性。该 CNN 模型包括六层和一个残差块，能够有效识别五种心跳类型：LBBB、RBBB、APC、PVC 和正常心跳。通过实验验证，该方法显著提高了诊断效率，减少了手动解释的主观性和耗时问题，从而改善临床决策和患者预后。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.15347v1",
      "published_date": "2024-04-13 19:56:15 UTC",
      "updated_date": "2024-04-13 19:56:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:05:53.785915"
    },
    {
      "arxiv_id": "2404.09067v1",
      "title": "Exploring Explainability in Video Action Recognition",
      "title_zh": "探索视频动作识别中的可解释性",
      "authors": [
        "Avinab Saha",
        "Shashank Gupta",
        "Sravan Kumar Ankireddy",
        "Karl Chahine",
        "Joydeep Ghosh"
      ],
      "abstract": "Image Classification and Video Action Recognition are perhaps the two most\nfoundational tasks in computer vision. Consequently, explaining the inner\nworkings of trained deep neural networks is of prime importance. While numerous\nefforts focus on explaining the decisions of trained deep neural networks in\nimage classification, exploration in the domain of its temporal version, video\naction recognition, has been scant. In this work, we take a deeper look at this\nproblem. We begin by revisiting Grad-CAM, one of the popular feature\nattribution methods for Image Classification, and its extension to Video Action\nRecognition tasks and examine the method's limitations. To address these, we\nintroduce Video-TCAV, by building on TCAV for Image Classification tasks, which\naims to quantify the importance of specific concepts in the decision-making\nprocess of Video Action Recognition models. As the scalable generation of\nconcepts is still an open problem, we propose a machine-assisted approach to\ngenerate spatial and spatiotemporal concepts relevant to Video Action\nRecognition for testing Video-TCAV. We then establish the importance of\ntemporally-varying concepts by demonstrating the superiority of dynamic\nspatiotemporal concepts over trivial spatial concepts. In conclusion, we\nintroduce a framework for investigating hypotheses in action recognition and\nquantitatively testing them, thus advancing research in the explainability of\ndeep neural networks used in video action recognition.",
      "tldr_zh": "本论文探讨了视频动作识别中深度神经网络决策的可解释性问题，指出虽然图像分类的解释方法已广泛研究，但视频领域的探索相对不足。作者首先重新审视Grad-CAM及其在视频动作识别中的扩展，并揭示其局限性；随后引入Video-TCAV方法，基于TCAV框架量化特定概念（如空间和时空概念）对模型决策的重要性。论文提出一种机器辅助方法生成相关概念，并通过实验证明动态时空概念比静态空间概念更有效。最后，该框架为测试动作识别假设提供定量工具，推动视频动作识别模型的可解释性研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 10 figures, Accepted to the 3rd Explainable AI for Computer\n  Vision (XAI4CV) Workshop at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.09067v1",
      "published_date": "2024-04-13 19:34:14 UTC",
      "updated_date": "2024-04-13 19:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:06:05.091840"
    },
    {
      "arxiv_id": "2404.09051v1",
      "title": "Rethinking Iterative Stereo Matching from Diffusion Bridge Model Perspective",
      "title_zh": "从扩散桥接模型视角重新思考迭代立体匹配",
      "authors": [
        "Yuguang Shi"
      ],
      "abstract": "Recently, iteration-based stereo matching has shown great potential. However,\nthese models optimize the disparity map using RNN variants. The discrete\noptimization process poses a challenge of information loss, which restricts the\nlevel of detail that can be expressed in the generated disparity map. In order\nto address these issues, we propose a novel training approach that incorporates\ndiffusion models into the iterative optimization process. We designed a\nTime-based Gated Recurrent Unit (T-GRU) to correlate temporal and disparity\noutputs. Unlike standard recurrent units, we employ Agent Attention to generate\nmore expressive features. We also designed an attention-based context network\nto capture a large amount of contextual information. Experiments on several\npublic benchmarks show that we have achieved competitive stereo matching\nperformance. Our model ranks first in the Scene Flow dataset, achieving over a\n7% improvement compared to competing methods, and requires only 8 iterations to\nachieve state-of-the-art results.",
      "tldr_zh": "本文从扩散桥模型视角重新审视迭代立体匹配问题，指出现有基于 RNN 变体的优化过程易导致信息丢失和视差图细节不足。论文提出一种新训练方法，将 diffusion models 融入迭代优化中，并设计 Time-based Gated Recurrent Unit (T-GRU) 和 Agent Attention 生成更具表现力的特征，同时引入 attention-based context network 捕获丰富上下文信息。在公共基准测试中，该模型在 Scene Flow 数据集上排名第一，比竞争方法提升超过7%，且仅需8次迭代即可达到最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "tip. arXiv admin note: text overlap with arXiv:2303.06615 by other\n  authors",
      "pdf_url": "http://arxiv.org/pdf/2404.09051v1",
      "published_date": "2024-04-13 17:31:11 UTC",
      "updated_date": "2024-04-13 17:31:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:06:17.034100"
    },
    {
      "arxiv_id": "2404.09045v1",
      "title": "Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zita Lifelo",
        "Huansheng Ning",
        "Sahraoui Dhelim"
      ],
      "abstract": "Timely identification is essential for the efficient handling of mental\nhealth illnesses such as depression. However, the current research fails to\nadequately address the prediction of mental health conditions from social media\ndata in low-resource African languages like Swahili. This study introduces two\ndistinct approaches utilising model-agnostic meta-learning and leveraging large\nlanguage models (LLMs) to address this gap. Experiments are conducted on three\ndatasets translated to low-resource language and applied to four mental health\ntasks, which include stress, depression, depression severity and suicidal\nideation prediction. we first apply a meta-learning model with\nself-supervision, which results in improved model initialisation for rapid\nadaptation and cross-lingual transfer. The results show that our meta-trained\nmodel performs significantly better than standard fine-tuning methods,\noutperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\%\nover XLM-R and mBERT. In parallel, we use LLMs' in-context learning\ncapabilities to assess their performance accuracy across the Swahili mental\nhealth prediction tasks by analysing different cross-lingual prompting\napproaches. Our analysis showed that Swahili prompts performed better than\ncross-lingual prompts but less than English prompts. Our findings show that\nin-context learning can be achieved through cross-lingual transfer through\ncarefully crafted prompt templates with examples and instructions.",
      "tldr_zh": "这篇论文针对低资源语言（如斯瓦希里语）社交媒体数据的心理健康预测问题，提出了两种方法：利用meta-training结合自监督学习的元学习模型，以及Large Language Model的in-context learning进行跨语言适应。实验在翻译后的数据集上评估了四种任务，包括压力、抑郁、抑郁严重度和自杀意念预测，结果显示meta-trained模型在macro F1 score上比基线XLM-R和mBERT分别提高了18%和0.8%。此外，分析表明斯瓦希里语提示优于跨语言提示但不如英语提示，通过精心设计的提示模板，可以实现有效的跨语言转移。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09045v1",
      "published_date": "2024-04-13 17:11:35 UTC",
      "updated_date": "2024-04-13 17:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:06:30.356982"
    },
    {
      "arxiv_id": "2404.09042v1",
      "title": "Improving Personalisation in Valence and Arousal Prediction using Data Augmentation",
      "title_zh": "使用数据增强改进效价和唤醒预测中的个性化",
      "authors": [
        "Munachiso Nwadike",
        "Jialin Li",
        "Hanan Salam"
      ],
      "abstract": "In the field of emotion recognition and Human-Machine Interaction (HMI),\npersonalised approaches have exhibited their efficacy in capturing\nindividual-specific characteristics and enhancing affective prediction\naccuracy. However, personalisation techniques often face the challenge of\nlimited data for target individuals. This paper presents our work on an\nenhanced personalisation strategy, that leverages data augmentation to develop\ntailored models for continuous valence and arousal prediction. Our proposed\napproach, Distance Weighting Augmentation (DWA), employs a weighting-based\naugmentation method that expands a target individual's dataset, leveraging\ndistance metrics to identify similar samples at the segment-level. Experimental\nresults on the MuSe-Personalisation 2023 Challenge dataset demonstrate that our\nmethod significantly improves the performance of features sets which have low\nbaseline performance, on the test set. This improvement in poor-performing\nfeatures comes without sacrificing performance on high-performing features. In\nparticular, our method achieves a maximum combined testing CCC of 0.78,\ncompared to the reported baseline score of 0.76 (reproduced at 0.72). It also\nachieved a peak arousal and valence scores of 0.81 and 0.76, compared to\nreproduced baseline scores of 0.76 and 0.67 respectively. Through this work, we\nmake significant contributions to the advancement of personalised affective\ncomputing models, enhancing the practicality and adaptability of data-level\npersonalisation in real world contexts.",
      "tldr_zh": "本研究针对情感识别和人机交互(HMI)领域中个性化方法的数据不足问题，提出了一种增强策略，使用数据增强技术来改善Valence和Arousal预测的准确性。方法名为Distance Weighting Augmentation (DWA)，通过基于距离度量的权重机制在段级扩展目标个体的数据集，从而生成个性化的情感预测模型。在MuSe-Personalisation 2023 Challenge数据集上的实验显示，DWA显著提升了表现较差特征的性能，实现组合测试CCC达0.78（基线0.72），唤醒度峰值0.81和情感值峰值0.76，同时不影响高性能特征。该工作推动了个性化情感计算模型的进步，提升了其在现实场景中的实用性和适应性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09042v1",
      "published_date": "2024-04-13 16:57:37 UTC",
      "updated_date": "2024-04-13 16:57:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:06:41.290734"
    },
    {
      "arxiv_id": "2404.09022v1",
      "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Benjue Weng"
      ],
      "abstract": "With the surge of ChatGPT,the use of large models has significantly\nincreased,rapidly rising to prominence across the industry and sweeping across\nthe internet. This article is a comprehensive review of fine-tuning methods for\nlarge models. This paper investigates the latest technological advancements and\nthe application of advanced methods in aspects such as task-adaptive\nfine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge\ndistillation,multi-task learning,parameter-efficient fine-tuning,and dynamic\nfine-tuning.",
      "tldr_zh": "这篇论文对大型语言模型(Large Language Models)的范式和微调策略进行了全面回顾和分析，背景是ChatGPT的兴起推动了这些模型在行业和互联网中的广泛应用。文章调查了最新的技术进展，包括任务适应微调(task-adaptive fine-tuning)、领域适应微调(domain-adaptive fine-tuning)、少样本学习(few-shot learning)、知识蒸馏(knowledge distillation)、多任务学习(multi-task learning)、参数高效微调(parameter-efficient fine-tuning)和动态微调(dynamic fine-tuning)等方法。最终，该综述为优化大型语言模型的应用提供了宝贵的指导和见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09022v1",
      "published_date": "2024-04-13 15:03:03 UTC",
      "updated_date": "2024-04-13 15:03:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:06:54.958365"
    },
    {
      "arxiv_id": "2404.13067v1",
      "title": "Towards Efficient Resume Understanding: A Multi-Granularity Multi-Modal Pre-Training Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Feihu Jiang",
        "Chuan Qin",
        "Jingshuai Zhang",
        "Kaichun Yao",
        "Xi Chen",
        "Dazhong Shen",
        "Chen Zhu",
        "Hengshu Zhu",
        "Hui Xiong"
      ],
      "abstract": "In the contemporary era of widespread online recruitment, resume\nunderstanding has been widely acknowledged as a fundamental and crucial task,\nwhich aims to extract structured information from resume documents\nautomatically. Compared to the traditional rule-based approaches, the\nutilization of recently proposed pre-trained document understanding models can\ngreatly enhance the effectiveness of resume understanding. The present\napproaches have, however, disregarded the hierarchical relations within the\nstructured information presented in resumes, and have difficulty parsing\nresumes in an efficient manner. To this end, in this paper, we propose a novel\nmodel, namely ERU, to achieve efficient resume understanding. Specifically, we\nfirst introduce a layout-aware multi-modal fusion transformer for encoding the\nsegments in the resume with integrated textual, visual, and layout information.\nThen, we design three self-supervised tasks to pre-train this module via a\nlarge number of unlabeled resumes. Next, we fine-tune the model with a\nmulti-granularity sequence labeling task to extract structured information from\nresumes. Finally, extensive experiments on a real-world dataset clearly\ndemonstrate the effectiveness of ERU.",
      "tldr_zh": "本文提出ERU模型，用于高效的简历理解，旨在自动提取简历中的结构化信息，同时解决现有方法忽略层次关系和效率问题的不足。ERU采用布局感知的多模态融合Transformer，整合文本、视觉和布局信息，并通过三个自监督任务在大量无标签简历上进行预训练。最终，通过多粒度序列标注任务微调模型，实验在真实数据集上证明了ERU的有效性，提升了简历理解的准确性和效率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICME 2024 Accepted",
      "pdf_url": "http://arxiv.org/pdf/2404.13067v1",
      "published_date": "2024-04-13 14:31:24 UTC",
      "updated_date": "2024-04-13 14:31:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:07:07.869504"
    },
    {
      "arxiv_id": "2404.09016v1",
      "title": "Theoretical research on generative diffusion models: an overview",
      "title_zh": "翻译失败",
      "authors": [
        "Melike Nur Yeğin",
        "Mehmet Fatih Amasyalı"
      ],
      "abstract": "Generative diffusion models showed high success in many fields with a\npowerful theoretical background. They convert the data distribution to noise\nand remove the noise back to obtain a similar distribution. Many existing\nreviews focused on the specific application areas without concentrating on the\nresearch about the algorithm. Unlike them we investigated the theoretical\ndevelopments of the generative diffusion models. These approaches mainly divide\ninto two: training-based and sampling-based. Awakening to this allowed us a\nclear and understandable categorization for the researchers who will make new\ndevelopments in the future.",
      "tldr_zh": "这篇论文概述了生成扩散模型（generative diffusion models）的理论研究，强调这些模型通过将数据分布转换为噪声并逐步去除噪声来生成类似分布。不同于现有评论主要关注特定应用领域，本文专注于算法的理论发展，并将其分为训练-based 和 sampling-based 两种主要方法。这种分类提供了一个清晰、易懂的框架，有助于未来研究者进行新创新。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09016v1",
      "published_date": "2024-04-13 14:08:56 UTC",
      "updated_date": "2024-04-13 14:08:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:07:18.921925"
    },
    {
      "arxiv_id": "2404.09005v7",
      "title": "Proof-of-Learning with Incentive Security",
      "title_zh": "翻译失败",
      "authors": [
        "Zishuo Zhao",
        "Zhixuan Fang",
        "Xuechao Wang",
        "Xi Chen",
        "Hongxu Su",
        "Haibo Xiao",
        "Yuan Zhou"
      ],
      "abstract": "Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or\nProof-of-Stake (PoS) mechanisms for decentralized consensus and security\nassurance. However, the substantial energy expenditure stemming from\ncomputationally intensive yet meaningless tasks has raised considerable\nconcerns surrounding traditional PoW approaches, The PoS mechanism, while free\nof energy consumption, is subject to security and economic issues. Addressing\nthese issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ\nchallenges of practical significance as PoW, thereby imbuing energy consumption\nwith tangible value. While previous efforts in Proof of Learning (PoL) explored\nthe utilization of deep learning model training SGD tasks as PoUW challenges,\nrecent research has revealed its vulnerabilities to adversarial attacks and the\ntheoretical hardness in crafting a byzantine-secure PoL mechanism. In this\npaper, we introduce the concept of incentive-security that incentivizes\nrational provers to behave honestly for their best interest, bypassing the\nexisting hardness to design a PoL mechanism with computational efficiency, a\nprovable incentive-security guarantee and controllable difficulty.\nParticularly, our work is secure against two attacks, and also improves the\ncomputational overhead from $\\Theta(1)$ to $O(\\frac{\\log E}{E})$. Furthermore,\nwhile most recent research assumes trusted problem providers and verifiers, our\ndesign also guarantees frontend incentive-security even when problem providers\nare untrusted, and verifier incentive-security that bypasses the Verifier's\nDilemma. By incorporating ML training into blockchain consensus mechanisms with\nprovable guarantees, our research not only proposes an eco-friendly solution to\nblockchain systems, but also provides a proposal for a completely decentralized\ncomputing power market in the new AI age.",
      "tldr_zh": "本论文针对区块链共识机制的缺陷（如PoW的高能耗和PoS的安全经济问题）提出了一种基于Incentive-Security的Proof-of-Learning (PoL)机制。该机制通过激励理性证明者诚实行为，设计出计算高效的PoL系统，将计算开销从Θ(1)改进为O(log E / E)，并提供对特定攻击的证明安全保证。同时，该设计在问题提供者不信任的情况下，仍能确保前端Incentive-Security和Verifier's Dilemma的规避。通过将机器学习训练融入区块链共识，该研究为环保区块链解决方案和去中心化计算力市场提供了可证明的框架。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.09005v7",
      "published_date": "2024-04-13 13:18:40 UTC",
      "updated_date": "2025-01-08 02:10:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:07:31.883974"
    },
    {
      "arxiv_id": "2404.09001v1",
      "title": "Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Cao",
        "Zidong Wang",
        "Siwen Xie",
        "Anji Liu",
        "Lifeng Fan"
      ],
      "abstract": "Despite the significant demand for assistive technology among vulnerable\ngroups (e.g., the elderly, children, and the disabled) in daily tasks, research\ninto advanced AI-driven assistive solutions that genuinely accommodate their\ndiverse needs remains sparse. Traditional human-machine interaction tasks often\nrequire machines to simply help without nuanced consideration of human\nabilities and feelings, such as their opportunity for practice and learning,\nsense of self-improvement, and self-esteem. Addressing this gap, we define a\npivotal and novel challenge Smart Help, which aims to provide proactive yet\nadaptive support to human agents with diverse disabilities and dynamic goals in\nvarious tasks and environments. To establish this challenge, we leverage\nAI2-THOR to build a new interactive 3D realistic household environment for the\nSmart Help task. We introduce an innovative opponent modeling module that\nprovides a nuanced understanding of the main agent's capabilities and goals, in\norder to optimize the assisting agent's helping policy. Rigorous experiments\nvalidate the efficacy of our model components and show the superiority of our\nholistic approach against established baselines. Our findings illustrate the\npotential of AI-imbued assistive robots in improving the well-being of\nvulnerable groups.",
      "tldr_zh": "该研究针对弱势群体（如老人、儿童和残疾人）在日常任务中的辅助需求，提出了一个新挑战Smart Help，旨在提供主动且适应的机器人支持，同时考虑人类的实践机会、自我提升感和自尊心。为此，研究利用AI2-THOR构建了一个交互式3D家庭环境，并引入创新的opponent modeling module来理解主代理的能力和目标，从而优化辅助代理的帮助策略。实验结果显示，该方法的组件有效，且整体方法优于现有基线，能够显著提升AI驱动机器人对弱势群体的福祉。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.09001v1",
      "published_date": "2024-04-13 13:03:59 UTC",
      "updated_date": "2024-04-13 13:03:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:07:44.033002"
    },
    {
      "arxiv_id": "2404.08995v4",
      "title": "Beyond Known Clusters: Probe New Prototypes for Efficient Generalized Class Discovery",
      "title_zh": "超越已知聚类：探索新原型用于高效的广义类发现",
      "authors": [
        "Ye Wang",
        "Yaxiong Wang",
        "Yujiao Wu",
        "Bingchen Zhao",
        "Xueming Qian"
      ],
      "abstract": "Generalized Class Discovery (GCD) aims to dynamically assign labels to\nunlabelled data partially based on knowledge learned from labelled data, where\nthe unlabelled data may come from known or novel classes. The prevailing\napproach generally involves clustering across all data and learning conceptions\nby prototypical contrastive learning. However, existing methods largely hinge\non the performance of clustering algorithms and are thus subject to their\ninherent limitations. Firstly, the estimated cluster number is often smaller\nthan the ground truth, making the existing methods suffer from the lack of\nprototypes for comprehensive conception learning. To address this issue, we\npropose an adaptive probing mechanism that introduces learnable potential\nprototypes to expand cluster prototypes (centers). As there is no ground truth\nfor the potential prototype, we develop a self-supervised prototype learning\nframework to optimize the potential prototype in an end-to-end fashion.\nSecondly, clustering is computationally intensive, and the conventional\nstrategy of clustering both labelled and unlabelled instances exacerbates this\nissue. To counteract this inefficiency, we opt to cluster only the unlabelled\ninstances and subsequently expand the cluster prototypes with our introduced\npotential prototypes to fast explore novel classes. Despite the simplicity of\nour proposed method, extensive empirical analysis on a wide range of datasets\nconfirms that our method consistently delivers state-of-the-art results.\nSpecifically, our method surpasses the nearest competitor by a significant\nmargin of 9.7% within the Stanford Cars dataset and 12x clustering efficiency\nwithin the Herbarium 19 dataset. We will make the code and checkpoints publicly\navailable at https://github.com/xjtuYW/PNP.git.",
      "tldr_zh": "该研究针对 Generalized Class Discovery (GCD) 的挑战，提出一种高效方法来处理未标注数据中已知和新类别的动态标签分配问题。现有方法依赖聚类算法，但常因估计的聚类数不足而限制原型学习，因此作者引入自适应探测机制，使用可学习的 potential prototypes 来扩展聚类原型，并通过端到端的自监督原型学习框架优化这些潜在原型。不同于传统方法，该方法仅对未标注实例进行聚类，然后扩展原型，从而显著提高计算效率。实验结果显示，该方法在多种数据集上取得最先进性能，例如在 Stanford Cars 数据集上比竞争对手高出9.7%，并在 Herbarium 19 数据集上实现12倍的聚类效率提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08995v4",
      "published_date": "2024-04-13 12:41:40 UTC",
      "updated_date": "2024-04-30 07:13:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:07:55.645707"
    },
    {
      "arxiv_id": "2404.08991v1",
      "title": "Business models for the simulation hypothesis",
      "title_zh": "模拟假设的商业模式",
      "authors": [
        "Evangelos Katsamakas"
      ],
      "abstract": "The simulation hypothesis suggests that we live in a computer simulation.\nThat notion has attracted significant scholarly and popular interest. This\narticle explores the simulation hypothesis from a business perspective. Due to\nthe lack of a name for a universe consistent with the simulation hypothesis, we\npropose the term simuverse. We argue that if we live in a simulation, there\nmust be a business justification. Therefore, we ask: If we live in a simuverse,\nwhat is its business model? We identify and explore business model scenarios,\nsuch as simuverse as a project, service, or platform. We also explore business\nmodel pathways and risk management issues. The article contributes to the\nsimulation hypothesis literature and is the first to provide a business model\nperspective on the simulation hypothesis. The article discusses theoretical and\npractical implications and identifies opportunities for future research related\nto sustainability, digital transformation, and Artificial Intelligence (AI).",
      "tldr_zh": "这篇论文从商业角度探讨了 simulation hypothesis（模拟假设），即我们可能生活在计算机模拟中，并首次提出“simuverse”一词来描述这种宇宙。作者论证了如果我们生活在 simuverse 中，则需有 business model（商业模式）来支撑，并分析了可能的场景，如将 simuverse 视为项目、服务或平台，同时探讨了商业模式路径和风险管理问题。论文为 simulation hypothesis 文献提供了新视角，讨论了理论与实际含义，并指出了未来研究机会，包括可持续性、数字转型和 AI 的应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08991v1",
      "published_date": "2024-04-13 12:36:20 UTC",
      "updated_date": "2024-04-13 12:36:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:08:07.410282"
    },
    {
      "arxiv_id": "2404.08990v1",
      "title": "A Fourier-enhanced multi-modal 3D small object optical mark recognition and positioning method for percutaneous abdominal puncture surgical navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Zezhao Guo",
        "Yanzhong Guo",
        "Zhanfang Zhao"
      ],
      "abstract": "Navigation for thoracoabdominal puncture surgery is used to locate the needle\nentry point on the patient's body surface. The traditional reflective ball\nnavigation method is difficult to position the needle entry point on the soft,\nirregular, smooth chest and abdomen. Due to the lack of clear characteristic\npoints on the body surface using structured light technology, it is difficult\nto identify and locate arbitrary needle insertion points. Based on the high\nstability and high accuracy requirements of surgical navigation, this paper\nproposed a novel method, a muti-modal 3D small object medical marker detection\nmethod, which identifies the center of a small single ring as the needle\ninsertion point. Moreover, this novel method leverages Fourier transform\nenhancement technology to augment the dataset, enrich image details, and\nenhance the network's capability. The method extracts the Region of Interest\n(ROI) of the feature image from both enhanced and original images, followed by\ngenerating a mask map. Subsequently, the point cloud of the ROI from the depth\nmap is obtained through the registration of ROI point cloud contour fitting. In\naddition, this method employs Tukey loss for optimal precision. The\nexperimental results show this novel method proposed in this paper not only\nachieves high-precision and high-stability positioning, but also enables the\npositioning of any needle insertion point.",
      "tldr_zh": "本研究提出了一种基于 Fourier-enhanced 的多模态 3D 小物体光学标记识别和定位方法，用于经皮腹部穿刺手术导航，以解决传统反射球和结构光技术在软、光滑胸腹部定位针入点的难题。该方法利用 Fourier transform 增强技术扩充数据集、提取 Region of Interest (ROI) 并生成掩码图，随后通过 ROI 点云轮廓拟合和 Tukey loss 优化，实现高精度和高稳定性的定位。实验结果表明，该方法不仅能准确识别小单环中心作为针插入点，还支持任意位置的插入点定位。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 6 figures,",
      "pdf_url": "http://arxiv.org/pdf/2404.08990v1",
      "published_date": "2024-04-13 12:28:40 UTC",
      "updated_date": "2024-04-13 12:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:08:21.466882"
    },
    {
      "arxiv_id": "2404.08986v2",
      "title": "Airship Formations for Animal Motion Capture and Behavior Analysis",
      "title_zh": "飞艇编队用于动物运动捕捉和行为分析",
      "authors": [
        "Eric Price",
        "Aamir Ahmad"
      ],
      "abstract": "Using UAVs for wildlife observation and motion capture offers manifold\nadvantages for studying animals in the wild, especially grazing herds in open\nterrain. The aerial perspective allows observation at a scale and depth that is\nnot possible on the ground, offering new insights into group behavior. However,\nthe very nature of wildlife field-studies puts traditional fixed wing and\nmulti-copter systems to their limits: limited flight time, noise and safety\naspects affect their efficacy, where lighter than air systems can remain on\nstation for many hours. Nevertheless, airships are challenging from a ground\nhandling perspective as well as from a control point of view, being voluminous\nand highly affected by wind. In this work, we showcase a system designed to use\nairship formations to track, follow, and visually record wild horses from\nmultiple angles, including airship design, simulation, control, on board\ncomputer vision, autonomous operation and practical aspects of field\nexperiments.",
      "tldr_zh": "这篇论文探讨了使用空气船编队（airship formations）进行野生动物运动捕捉（motion capture）和行为分析的优势，特别是针对野外放牧群体的空中观察。相比传统固定翼或多旋翼UAVs，空气船提供更长的飞行时间、更低噪音和更安全的选项，但面临地面处理和风力控制的挑战。作者设计了一个完整系统，包括空气船设计、模拟、控制、机载计算机视觉和自主操作，通过野外实验成功跟踪并从多个角度视觉记录野马的行为，展示了该方法的实际可行性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for presentation at the 2nd International Conference on\n  Design and Engineering of Lighter-Than-Air systems (DELTAS2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.08986v2",
      "published_date": "2024-04-13 12:18:19 UTC",
      "updated_date": "2024-05-24 10:59:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:08:32.980436"
    },
    {
      "arxiv_id": "2404.08985v1",
      "title": "Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning",
      "title_zh": "翻译失败",
      "authors": [
        "Yijiang Liu",
        "Rongyu Zhang",
        "Huanrui Yang",
        "Kurt Keutzer",
        "Yuan Du",
        "Li Du",
        "Shanghang Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nperforming multiple tasks in multimedia applications, ranging from content\ngeneration to interactive entertainment, and artistic creation. However, the\ndiversity of downstream tasks in multitask scenarios presents substantial\nadaptation challenges for LLMs. While traditional methods often succumb to\nknowledge confusion on their monolithic dense models, Mixture-of-Experts (MoE)\nhas been emerged as a promising solution with its sparse architecture for\neffective task decoupling. Inspired by the principles of human cognitive\nneuroscience, we design a novel framework \\texttt{Intuition-MoR1E} that\nleverages the inherent semantic clustering of instances to mimic the human\nbrain to deal with multitask, offering implicit guidance to router for\noptimized feature allocation. Moreover, we introduce cutting-edge Rank-1\nExperts formulation designed to manage a spectrum of intuitions, demonstrating\nenhanced parameter efficiency and effectiveness in multitask LLM finetuning.\nExtensive experiments demonstrate that Intuition-MoR1E achieves superior\nefficiency and 2.15\\% overall accuracy improvement across 14 public datasets\nagainst other state-of-the-art baselines.",
      "tldr_zh": "本文提出了一种名为 Intuition-MoR1E 的新框架，用于参数高效的 LLMs 微调，旨在解决多任务场景下的大型语言模型 (LLMs) 面临的适应挑战和知识混乱问题。该框架借鉴人类认知神经科学原理，通过实例的语义聚类为 Mixture-of-Experts (MoE) 路由器提供隐式指导，并引入 Rank-1 Experts 设计来优化特征分配和参数效率。实验在 14 个公共数据集上表明，Intuition-MoR1E 比其他最先进基线提高了 2.15% 的整体准确率，展示了其在多任务 LLM 微调中的优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08985v1",
      "published_date": "2024-04-13 12:14:58 UTC",
      "updated_date": "2024-04-13 12:14:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:08:45.077958"
    },
    {
      "arxiv_id": "2404.08978v2",
      "title": "Incremental Residual Concept Bottleneck Models",
      "title_zh": "增量残差概念瓶颈模型",
      "authors": [
        "Chenming Shang",
        "Shiji Zhou",
        "Hengyuan Zhang",
        "Xinzhe Ni",
        "Yujiu Yang",
        "Yuwang Wang"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.",
      "tldr_zh": "本研究针对Concept Bottleneck Models (CBMs)中概念库不完整的问题，提出了Incremental Residual Concept Bottleneck Model (Res-CBM)，它通过一组可优化的向量补充缺失概念，并利用增量概念发现模块将这些向量转换为候选概念库中的潜在概念，从而提升模型的性能和概念完整性。Res-CBM 作为一种后处理方法，可应用于任何用户定义的概念库，同时引入了Concept Utilization Efficiency (CUE)指标来评估CBMs的描述效率。实验结果显示，Res-CBM 在准确性和效率上优于现有最先进方法，并在多个数据集上与黑箱模型表现出相当的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08978v2",
      "published_date": "2024-04-13 12:02:19 UTC",
      "updated_date": "2024-04-17 10:59:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:08:56.204000"
    },
    {
      "arxiv_id": "2404.08964v1",
      "title": "Understanding Multimodal Deep Neural Networks: A Concept Selection View",
      "title_zh": "理解多模态深度神经网络：一种概念选择视角",
      "authors": [
        "Chenming Shang",
        "Hengyuan Zhang",
        "Hao Wen",
        "Yujiu Yang"
      ],
      "abstract": "The multimodal deep neural networks, represented by CLIP, have generated rich\ndownstream applications owing to their excellent performance, thus making\nunderstanding the decision-making process of CLIP an essential research topic.\nDue to the complex structure and the massive pre-training data, it is often\nregarded as a black-box model that is too difficult to understand and\ninterpret. Concept-based models map the black-box visual representations\nextracted by deep neural networks onto a set of human-understandable concepts\nand use the concepts to make predictions, enhancing the transparency of the\ndecision-making process. However, these methods involve the datasets labeled\nwith fine-grained attributes by expert knowledge, which incur high costs and\nintroduce excessive human prior knowledge and bias. In this paper, we observe\nthe long-tail distribution of concepts, based on which we propose a two-stage\nConcept Selection Model (CSM) to mine core concepts without introducing any\nhuman priors. The concept greedy rough selection algorithm is applied to\nextract head concepts, and then the concept mask fine selection method performs\nthe extraction of core concepts. Experiments show that our approach achieves\ncomparable performance to end-to-end black-box models, and human evaluation\ndemonstrates that the concepts discovered by our method are interpretable and\ncomprehensible for humans.",
      "tldr_zh": "本研究针对多模态深度神经网络（如CLIP）的黑盒决策问题，提出了一种基于概念选择的视角，以提升模型的可解释性。作者观察到概念的长尾分布，开发了Concept Selection Model (CSM)，一个两阶段方法：首先使用概念贪婪粗选算法提取头部概念，其次采用概念掩码细选方法挖掘核心概念，而无需引入人类先验知识。实验结果显示，该方法在性能上可与端到端黑盒模型媲美，且人性化评估证实其发现的概念对人类易于理解和解释。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08964v1",
      "published_date": "2024-04-13 11:06:49 UTC",
      "updated_date": "2024-04-13 11:06:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:09:08.751872"
    },
    {
      "arxiv_id": "2404.10014v1",
      "title": "A biologically inspired computational trust model for open multi-agent systems which is resilient to trustor population changes",
      "title_zh": "翻译失败",
      "authors": [
        "Zoi Lygizou",
        "Dimitris Kalles"
      ],
      "abstract": "Current trust and reputation models continue to have significant limitations,\nsuch as the inability to deal with agents constantly entering or exiting open\nmulti-agent systems (open MAS), as well as continuously changing behaviors. Our\nstudy is based on CA, a previously proposed decentralized computational trust\nmodel from the trustee's point of view, inspired by synaptic plasticity and the\nformation of assemblies in the human brain. It is designed to meet the\nrequirements of highly dynamic and open MAS, and its main difference with most\nconventional trust and reputation models is that the trustor does not select a\ntrustee to delegate a task; instead, the trustee determines whether it is\nqualified to successfully execute it. We ran a series of simulations to compare\nCA model to FIRE, a well-established, decentralized trust and reputation model\nfor open MAS under conditions of continuous trustee and trustor population\nreplacement, as well as continuous change of trustees' abilities to perform\ntasks. The main finding is that FIRE is superior to changes in the trustee\npopulation, whereas CA is resilient to the trustor population changes. When the\ntrustees switch performance profiles FIRE clearly outperforms despite the fact\nthat both models' performances are significantly impacted by this environmental\nchange. Findings lead us to conclude that learning to use the appropriate trust\nmodel, according to the dynamic conditions in effect could maximize the\ntrustor's benefits.",
      "tldr_zh": "本研究提出了一种受生物启发（inspired by synaptic plasticity and the formation of assemblies）的计算信任模型 CA，用于开放多智能体系统（open multi-agent systems），该模型对 trustor 人口变化具有韧性，能够处理代理进出和行为变化的动态环境。不同于传统模型，CA 从 trustee 的视角出发，由 trustee 自行决定是否合格执行任务，并通过模拟实验与 FIRE 模型比较。结果显示，CA 在 trustor 人口变化时表现优于 FIRE，但在 trustee 性能变化时不如后者；因此，建议根据具体动态条件选择合适的 trust model，以最大化 trustor 的利益。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.10014v1",
      "published_date": "2024-04-13 10:56:32 UTC",
      "updated_date": "2024-04-13 10:56:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:09:22.306961"
    },
    {
      "arxiv_id": "2404.08939v1",
      "title": "NeurIT: Pushing the Limit of Neural Inertial Tracking for Indoor Robotic IoT",
      "title_zh": "翻译失败",
      "authors": [
        "Xinzhe Zheng",
        "Sijie Ji",
        "Yipeng Pan",
        "Kaiwen Zhang",
        "Chenshu Wu"
      ],
      "abstract": "Inertial tracking is vital for robotic IoT and has gained popularity thanks\nto the ubiquity of low-cost Inertial Measurement Units (IMUs) and deep\nlearning-powered tracking algorithms. Existing works, however, have not fully\nutilized IMU measurements, particularly magnetometers, nor maximized the\npotential of deep learning to achieve the desired accuracy. To enhance the\ntracking accuracy for indoor robotic applications, we introduce NeurIT, a\nsequence-to-sequence framework that elevates tracking accuracy to a new level.\nNeurIT employs a Time-Frequency Block-recurrent Transformer (TF-BRT) at its\ncore, combining the power of recurrent neural network (RNN) and Transformer to\nlearn representative features in both time and frequency domains. To fully\nutilize IMU information, we strategically employ body-frame differentiation of\nthe magnetometer, which considerably reduces the tracking error. NeurIT is\nimplemented on a customized robotic platform and evaluated in various indoor\nenvironments. Experimental results demonstrate that NeurIT achieves a mere\n1-meter tracking error over a 300-meter distance. Notably, it significantly\noutperforms state-of-the-art baselines by 48.21% on unseen data. NeurIT also\nperforms comparably to the visual-inertial approach (Tango Phone) in\nvision-favored conditions and surpasses it in plain environments. We believe\nNeurIT takes an important step forward toward practical neural inertial\ntracking for ubiquitous and scalable tracking of robotic things. NeurIT,\nincluding the source code and the dataset, is open-sourced here:\nhttps://github.com/NeurIT-Project/NeurIT.",
      "tldr_zh": "本文提出 NeurIT，一种序列到序列框架，旨在提升室内机器人 IoT 的神经惯性跟踪精度，通过 Time-Frequency Block-recurrent Transformer (TF-BRT) 结合 RNN 和 Transformer 在时间和频率域学习特征，并优化 IMU 磁力计的 body-frame differentiation 以减少跟踪错误。NeurIT 在自定义机器人平台上实验中，实现了 300 米距离内仅 1 米的跟踪错误，并在未见数据上比现有基线提升 48.21%。此外，它在视觉有利条件下与视觉-惯性方法 (Tango Phone) 相当，而在普通环境中表现更优，为实用神经惯性跟踪提供了重要进展，并开源了代码和数据集。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08939v1",
      "published_date": "2024-04-13 09:24:50 UTC",
      "updated_date": "2024-04-13 09:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:09:34.023819"
    },
    {
      "arxiv_id": "2404.08937v1",
      "title": "ChimpVLM: Ethogram-Enhanced Chimpanzee Behaviour Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Otto Brookes",
        "Majid Mirmehdi",
        "Hjalmar Kuhl",
        "Tilo Burghardt"
      ],
      "abstract": "We show that chimpanzee behaviour understanding from camera traps can be\nenhanced by providing visual architectures with access to an embedding of text\ndescriptions that detail species behaviours. In particular, we present a\nvision-language model which employs multi-modal decoding of visual features\nextracted directly from camera trap videos to process query tokens representing\nbehaviours and output class predictions. Query tokens are initialised using a\nstandardised ethogram of chimpanzee behaviour, rather than using random or\nname-based initialisations. In addition, the effect of initialising query\ntokens using a masked language model fine-tuned on a text corpus of known\nbehavioural patterns is explored. We evaluate our system on the PanAf500 and\nPanAf20K datasets and demonstrate the performance benefits of our multi-modal\ndecoding approach and query initialisation strategy on multi-class and\nmulti-label recognition tasks, respectively. Results and ablations corroborate\nperformance improvements. We achieve state-of-the-art performance over vision\nand vision-language models in top-1 accuracy (+6.34%) on PanAf500 and overall\n(+1.1%) and tail-class (+2.26%) mean average precision on PanAf20K. We share\ncomplete source code and network weights for full reproducibility of results\nand easy utilisation.",
      "tldr_zh": "本文提出ChimpVLM，一种视觉语言模型，通过整合文本描述嵌入来增强相机陷阱视频中黑猩猩行为的识别，利用多模态解码处理视觉特征，并以标准化ethogram初始化查询标记，同时探索基于微调掩码语言模型的初始化策略。相比传统方法，该模型在PanAf500数据集上实现了state-of-the-art的top-1准确率提升6.34%，而在PanAf20K数据集上，整体mean average precision (mAP)提升1.1%，尾类mAP提升2.26%。实验结果和消融分析证实了多模态解码和查询初始化策略的有效性。作者共享了完整的源代码和网络权重，以支持结果的复现和应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08937v1",
      "published_date": "2024-04-13 09:17:51 UTC",
      "updated_date": "2024-04-13 09:17:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:09:48.209641"
    },
    {
      "arxiv_id": "2404.08931v1",
      "title": "Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling",
      "title_zh": "基于掩码图像建模的无标签异常检测在航空农业图像中",
      "authors": [
        "Sambal Shikhar",
        "Anupam Sobti"
      ],
      "abstract": "Detecting various types of stresses (nutritional, water, nitrogen, etc.) in\nagricultural fields is critical for farmers to ensure maximum productivity.\nHowever, stresses show up in different shapes and sizes across different crop\ntypes and varieties. Hence, this is posed as an anomaly detection task in\nagricultural images. Accurate anomaly detection in agricultural UAV images is\nvital for early identification of field irregularities. Traditional supervised\nlearning faces challenges in adapting to diverse anomalies, necessitating\nextensive annotated data. In this work, we overcome this limitation with\nself-supervised learning using a masked image modeling approach. Masked\nAutoencoders (MAE) extract meaningful normal features from unlabeled image\nsamples which produces high reconstruction error for the abnormal pixels during\nreconstruction. To remove the need of using only ``normal\" data while training,\nwe use an anomaly suppression loss mechanism that effectively minimizes the\nreconstruction of anomalous pixels and allows the model to learn anomalous\nareas without explicitly separating ``normal\" images for training. Evaluation\non the Agriculture-Vision data challenge shows a mIOU score improvement in\ncomparison to prior state of the art in unsupervised and self-supervised\nmethods. A single model generalizes across all the anomaly categories in the\nAgri-Vision Challenge Dataset",
      "tldr_zh": "该研究针对农业无人机图像中的异常检测（如营养、水分压力），提出了一种无标签（label-free）的自监督学习方法，使用Masked Image Modeling来克服传统监督学习对标注数据依赖的问题。方法基于Masked Autoencoders (MAE)从无标签图像中提取正常特征，并引入anomaly suppression loss机制，以最小化异常像素的重建，从而无需分离“正常”图像进行训练。在Agriculture-Vision数据集上，该模型实现了mIOU分数的显著提升，并证明了一个单一模型能泛化到所有异常类别，提高了农业异常检测的效率和准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "The paper has been accepted to CVPR 2024 5th Workshop on Vision for\n  Agriculture as an Oral Paper",
      "pdf_url": "http://arxiv.org/pdf/2404.08931v1",
      "published_date": "2024-04-13 08:49:17 UTC",
      "updated_date": "2024-04-13 08:49:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:09:57.347072"
    },
    {
      "arxiv_id": "2404.13066v2",
      "title": "Leveraging Large Language Model as Simulated Patients for Clinical Education",
      "title_zh": "利用大型语言模型作为模拟患者用于临床教育",
      "authors": [
        "Yanzeng Li",
        "Cheng Zeng",
        "Jialun Zhong",
        "Ruoyu Zhang",
        "Minhao Zhang",
        "Lei Zou"
      ],
      "abstract": "Simulated Patients (SPs) play a crucial role in clinical medical education by\nproviding realistic scenarios for student practice. However, the high cost of\ntraining and hiring qualified SPs, along with the heavy workload and potential\nrisks they face in consistently portraying actual patients, limit students'\naccess to this type of clinical training. Consequently, the integration of\ncomputer program-based simulated patients has emerged as a valuable educational\ntool in recent years. With the rapid development of Large Language Models\n(LLMs), their exceptional capabilities in conversational artificial\nintelligence and role-playing have been demonstrated, making them a feasible\noption for implementing Virtual Simulated Patient (VSP). In this paper, we\npresent an integrated model-agnostic framework called CureFun that harnesses\nthe potential of LLMs in clinical medical education. This framework facilitates\nnatural conversations between students and simulated patients, evaluates their\ndialogue, and provides suggestions to enhance students' clinical inquiry\nskills. Through comprehensive evaluations, our approach demonstrates more\nauthentic and professional SP-scenario dialogue flows compared to other\nLLM-based chatbots, thus proving its proficiency in simulating patients.\nAdditionally, leveraging CureFun's evaluation ability, we assess several\nmedical LLMs and discuss the possibilities and limitations of using LLMs as\nvirtual doctors from the perspective of their diagnostic abilities.",
      "tldr_zh": "本论文探讨了利用Large Language Models (LLMs) 作为Simulated Patients (SPs) 来提升临床医疗教育，以解决传统SPs 培训成本高、工作量大和风险高的局限性。研究提出一个模型无关框架CureFun，利用LLMs 实现学生与虚拟模拟患者（Virtual Simulated Patient, VSP）的自然对话、对话评估以及临床询问技能建议。实验结果表明，CureFun 在模拟患者场景中比其他LLM-based chatbots 更真实和专业；此外，通过评估多个医疗LLMs，该框架还讨论了LLMs 作为虚拟医生的诊断能力及其潜在可能性和限制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13066v2",
      "published_date": "2024-04-13 06:36:32 UTC",
      "updated_date": "2024-04-25 02:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:10:10.380845"
    },
    {
      "arxiv_id": "2404.13065v1",
      "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Ajmal PS",
        "Ditto PS",
        "Jithin VG"
      ],
      "abstract": "Intellecta dataset emerges as an innovative synthetic dataset, engineered to\nenhance the cognitive processing capabilities of contemporary language models.\nWith a composition of 11.53 billion tokens, integrating 8.01 billion tokens of\nsynthetic data with 3.52 billion tokens of rich textbook data, Intellecta is\ncrafted to foster advanced reasoning and comprehensive educational narrative\ngeneration. Leveraging the Mixtral-8x7B-Instruct-v0.1 model, the dataset\nfacilitates the generation of complex thought processes and detailed,\ntextbook-style explanations, thus enabling language models to engage in both\ncritical thinking and profound educational discourse. This hybrid dataset\nstands as a testament to the potential of synthetic data in pushing the\nboundaries of AI, offering a repository that is not only vast and varied but\nalso refined to align with ethical standards and intellectual rigor.",
      "tldr_zh": "该研究引入了Intellecta Cognitiva数据集，这是一个创新的合成数据集，旨在提升当代语言模型的认知处理能力，总计11.53 billion tokens，包括8.01 billion tokens的合成数据和3.52 billion tokens的教科书数据。利用Mixtral-8x7B-Instruct-v0.1模型，该数据集生成复杂思考过程和详细的教科书式解释，支持高级推理和教育性叙述。Intellecta展示了合成数据在推动AI边界方面的潜力，同时确保符合伦理标准和智力严谨性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13065v1",
      "published_date": "2024-04-13 06:11:25 UTC",
      "updated_date": "2024-04-13 06:11:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:10:20.414201"
    },
    {
      "arxiv_id": "2404.08892v1",
      "title": "ChangeAnywhere: Sample Generation for Remote Sensing Change Detection via Semantic Latent Diffusion Model",
      "title_zh": "ChangeAnywhere：基于语义潜在扩散模型的遥感变化检测样本生成",
      "authors": [
        "Kai Tang",
        "Jin Chen"
      ],
      "abstract": "Remote sensing change detection (CD) is a pivotal technique that pinpoints\nchanges on a global scale based on multi-temporal images. With the recent\nexpansion of deep learning, supervised deep learning-based CD models have shown\nsatisfactory performance. However, CD sample labeling is very time-consuming as\nit is densely labeled and requires expert knowledge. To alleviate this problem,\nwe introduce ChangeAnywhere, a novel CD sample generation method using the\nsemantic latent diffusion model and single-temporal images. Specifically,\nChangeAnywhere leverages the relative ease of acquiring large single-temporal\nsemantic datasets to generate large-scale, diverse, and semantically annotated\nbi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD\nsamples, i.e., change implies semantically different, and non-change implies\nreasonable change under the same semantic constraints. We generated\nChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD\nsamples based on the proposed method. The ChangeAnywhere-100K significantly\nimproved both zero-shot and few-shot performance on two CD benchmark datasets\nfor various deep learning-based CD models, as demonstrated by transfer\nexperiments. This paper delineates the enormous potential of ChangeAnywhere for\nCD sample generation and demonstrates the subsequent enhancement of model\nperformance. Therefore, ChangeAnywhere offers a potent tool for remote sensing\nCD. All codes and pre-trained models will be available at\nhttps://github.com/tangkai-RS/ChangeAnywhere.",
      "tldr_zh": "该论文提出 ChangeAnywhere，一种利用 Semantic Latent Diffusion Model 从单时相图像生成遥感变化检测（Remote Sensing Change Detection）样本的方法，以解决样本标注耗时的问题。该方法捕捉变化（即语义不同）和非变化（即相同语义约束下的合理差异）的核心本质，生成大规模多样化数据集 ChangeAnywhere-100K，包含10万对双时相样本。实验结果显示，该数据集显著提升了各种深度学习 CD 模型在零样本和少样本场景下的性能，展示了其在遥感 CD 领域的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Concise manuscript version of ChangeAnywhere",
      "pdf_url": "http://arxiv.org/pdf/2404.08892v1",
      "published_date": "2024-04-13 03:46:35 UTC",
      "updated_date": "2024-04-13 03:46:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:10:35.447297"
    },
    {
      "arxiv_id": "2405.01392v1",
      "title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration",
      "title_zh": "翻译失败",
      "authors": [
        "David Maranto"
      ],
      "abstract": "As spacecraft journey further from Earth with more complex missions, systems\nof greater autonomy and onboard intelligence are called for. Reducing reliance\non human-based mission control becomes increasingly critical if we are to\nincrease our rate of solar-system-wide exploration. Recent work has explored\nAI-based goal-oriented systems to increase the level of autonomy in mission\nexecution. These systems make use of symbolic reasoning managers to make\ninferences from the state of a spacecraft and a handcrafted knowledge base,\nenabling autonomous generation of tasks and re-planning. Such systems have\nproven to be successful in controlled cases, but they are difficult to\nimplement as they require human-crafted ontological models to allow the\nspacecraft to understand the world. Reinforcement learning has been applied to\ntrain robotic agents to pursue a goal. A new architecture for autonomy is\ncalled for. This work explores the application of Large Language Models (LLMs)\nas the high-level control system of a spacecraft. Using a systems engineering\napproach, this work presents the design and development of an agentic\nspacecraft controller by leveraging an LLM as a reasoning engine, to evaluate\nthe utility of such an architecture in achieving higher levels of spacecraft\nautonomy. A series of deep space mission scenarios simulated within the popular\ngame engine Kerbal Space Program (KSP) are used as case studies to evaluate the\nimplementation against the requirements. It is shown the reasoning and planning\nabilities of present-day LLMs do not scale well as the complexity of a mission\nincreases, but this can be alleviated with adequate prompting frameworks and\nstrategic selection of the agent's level of authority over the host spacecraft.\nThis research evaluates the potential of LLMs in augmenting autonomous\ndecision-making systems for future robotic space applications.",
      "tldr_zh": "该研究提出 LLMSat，一种基于 Large Language Models (LLMs) 的目标导向代理，用于提升航天器的自主性，减少对地面控制的依赖。论文采用系统工程方法设计了 LLMSat 架构，将 LLMs 作为高层次推理引擎，并在 Kerbal Space Program (KSP) 模拟环境中测试深空任务场景。结果显示，LLMs 的推理和规划能力在复杂任务中表现不佳，但通过优化提示框架和代理权限选择，可以显著缓解这些问题，为未来机器人太空应用的决策系统提供潜在增强。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "physics.space-ph"
      ],
      "primary_category": "cs.RO",
      "comment": "B.A.Sc thesis",
      "pdf_url": "http://arxiv.org/pdf/2405.01392v1",
      "published_date": "2024-04-13 03:33:17 UTC",
      "updated_date": "2024-04-13 03:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:10:45.910008"
    },
    {
      "arxiv_id": "2404.08886v1",
      "title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Henry Peng Zou",
        "Gavin Heqing Yu",
        "Ziwei Fan",
        "Dan Bu",
        "Han Liu",
        "Peng Dai",
        "Dongmei Jia",
        "Cornelia Caragea"
      ],
      "abstract": "In e-commerce, accurately extracting product attribute values from multimodal\ndata is crucial for improving user experience and operational efficiency of\nretailers. However, previous approaches to multimodal attribute value\nextraction often struggle with implicit attribute values embedded in images or\ntext, rely heavily on extensive labeled data, and can easily confuse similar\nattribute values. To address these issues, we introduce EIVEN, a data- and\nparameter-efficient generative framework that pioneers the use of multimodal\nLLM for implicit attribute value extraction. EIVEN leverages the rich inherent\nknowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled\ndata. We also introduce a novel Learning-by-Comparison technique to reduce\nmodel confusion by enforcing attribute value comparison and difference\nidentification. Additionally, we construct initial open-source datasets for\nmultimodal implicit attribute value extraction. Our extensive experiments\nreveal that EIVEN significantly outperforms existing methods in extracting\nimplicit attribute values while requiring less labeled data.",
      "tldr_zh": "本研究提出 EIVEN，一种高效的生成框架，利用 Multimodal LLM 从多模态数据中提取隐含属性值，以解决电商领域中现有方法对标注数据依赖过大和混淆相似值的问题。EIVEN 借助预训练 LLM 和视觉编码器的内在知识，减少了对标注数据的需求，同时引入 Learning-by-Comparison 技术，通过强制比较属性值和识别差异来提升模型准确性。研究者还构建了初始开源数据集，实验结果显示 EIVEN 在提取隐含属性值方面显著优于现有方法，同时需要更少的标注数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NAACL 2024 Industry Track",
      "pdf_url": "http://arxiv.org/pdf/2404.08886v1",
      "published_date": "2024-04-13 03:15:56 UTC",
      "updated_date": "2024-04-13 03:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:10:57.230735"
    },
    {
      "arxiv_id": "2404.08872v1",
      "title": "Enhanced Hydrogen Evolution Activity of MOS$_2$-rGO Composite Synthesized via Hydrothermal Technique",
      "title_zh": "翻译失败",
      "authors": [
        "Abhishek Sebastian",
        "Pragna R"
      ],
      "abstract": "Hydrogen evolution reaction (HER) has emerged as a promising technique for\nthe production of clean and sustainable energy. In recent years, researchers\nhave been exploring various materials for efficient HER activity. In this\nstudy, we report the synthesis of two different materials, namely MOS$_2$ and\nMoS$_2$-rGO, through a hydrothermal technique. X-ray diffraction (XRD),\nFourier-transform infrared (FTIR) spectroscopy, and Raman spectroscopy were\nused to characterize the materials. XRD analysis revealed the formation of\nhexagonal MOS$_2$ with a high degree of crystallinity. FTIR analysis confirmed\nthe presence of Mo-S bonds, while Raman spectroscopy provided evidence for the\nformation of MOS$_2$.To evaluate the HER activity of the materials, linear\nsweep voltammetry (LSV) was performed. The results showed that MOS$_2$ and\nMOS$_2$-rGO had good HER activity with low onset potentials and high current\ndensities. The MOS$_2$-rGO material showed improved HER activity compared to\nMOS$_2$, indicating the potential of graphene oxide as a co-catalyst to enhance\nthe performance of MOS$_2$.",
      "tldr_zh": "本文研究了通过水热技术合成 MOS$_2$ 和 MoS$_2$-rGO 复合材料，以提升氢演化反应 (HER) 的活性。材料表征使用 X-ray diffraction (XRD)、Fourier-transform infrared (FTIR) 光谱和 Raman 光谱，证实了六角形 MOS$_2$ 的高结晶度和 Mo-S 键的存在。线性扫描伏安法 (LSV) 测试结果显示，MoS$_2$-rGO 复合材料比纯 MOS$_2$ 具有更低的起始电位和更高的电流密度，证明 rGO 作为助催化剂能显著增强 HER 性能。该研究为开发高效清洁能源材料提供了新途径。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "This research is an excerpt from the report of TARP (Technical\n  Answers for Real-World Problems)",
      "pdf_url": "http://arxiv.org/pdf/2404.08872v1",
      "published_date": "2024-04-13 02:06:32 UTC",
      "updated_date": "2024-04-13 02:06:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:11:12.321956"
    },
    {
      "arxiv_id": "2404.10790v1",
      "title": "Multimodal Attack Detection for Action Recognition Models",
      "title_zh": "针对动作识别",
      "authors": [
        "Furkan Mumcu",
        "Yasin Yilmaz"
      ],
      "abstract": "Adversarial machine learning attacks on video action recognition models is a\ngrowing research area and many effective attacks were introduced in recent\nyears. These attacks show that action recognition models can be breached in\nmany ways. Hence using these models in practice raises significant security\nconcerns. However, there are very few works which focus on defending against or\ndetecting attacks. In this work, we propose a novel universal detection method\nwhich is compatible with any action recognition model. In our extensive\nexperiments, we show that our method consistently detects various attacks\nagainst different target models with high true positive rates while satisfying\nvery low false positive rates. Tested against four state-of-the-art attacks\ntargeting four action recognition models, the proposed detector achieves an\naverage AUC of 0.911 over 16 test cases while the best performance achieved by\nthe existing detectors is 0.645 average AUC. This 41.2% improvement is enabled\nby the robustness of the proposed detector to varying attack methods and target\nmodels. The lowest AUC achieved by our detector across the 16 test cases is\n0.837 while the competing detector's performance drops as low as 0.211. We also\nshow that the proposed detector is robust to varying attack strengths. In\naddition, we analyze our method's real-time performance with different hardware\nsetups to demonstrate its potential as a practical defense mechanism.",
      "tldr_zh": "这篇论文针对视频动作识别模型(adversarial machine learning attacks)面临的各种攻击风险，提出了一种新型的通用检测方法，该方法兼容任何动作识别模型。研究通过广泛实验证明，该检测器能以高真阳性率检测多种攻击，同时保持低假阳性率。相比现有检测器，该方法在针对四种最先进攻击和四种目标模型的16个测试案例中，平均AUC达到0.911，比竞争方法提高了41.2%，并显示出对不同攻击强度和方法的鲁棒性。此外，论文还分析了该检测器的实时性能，证明其作为实际防御机制的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.10790v1",
      "published_date": "2024-04-13 01:31:25 UTC",
      "updated_date": "2024-04-13 01:31:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:11:24.038584"
    },
    {
      "arxiv_id": "2404.08866v1",
      "title": "An evaluation framework for synthetic data generation models",
      "title_zh": "合成数据生成模型的评估框架",
      "authors": [
        "Ioannis E. Livieris",
        "Nikos Alimpertis",
        "George Domalis",
        "Dimitris Tsakalidis"
      ],
      "abstract": "Nowadays, the use of synthetic data has gained popularity as a cost-efficient\nstrategy for enhancing data augmentation for improving machine learning models\nperformance as well as addressing concerns related to sensitive data privacy.\nTherefore, the necessity of ensuring quality of generated synthetic data, in\nterms of accurate representation of real data, consists of primary importance.\nIn this work, we present a new framework for evaluating synthetic data\ngeneration models' ability for developing high-quality synthetic data. The\nproposed approach is able to provide strong statistical and theoretical\ninformation about the evaluation framework and the compared models' ranking.\nTwo use case scenarios demonstrate the applicability of the proposed framework\nfor evaluating the ability of synthetic data generation models to generated\nhigh quality data. The implementation code can be found in\nhttps://github.com/novelcore/synthetic_data_evaluation_framework.",
      "tldr_zh": "本研究提出一个新的评估框架，用于评估合成数据生成模型（synthetic data generation models）的能力，以确保生成的合成数据能准确代表真实数据，从而提升机器学习模型性能并保护敏感数据隐私。该框架通过提供强有力的统计和理论信息，对不同模型进行排名和比较，强调了合成数据质量的重要性。两个实际用例场景证明了框架的有效性，可用于生成高质量合成数据。实现代码可在 https://github.com/novelcore/synthetic_data_evaluation_framework 获取。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted for presentation at IFIP International\n  Conference on Artificial Intelligence Applications and Innovations",
      "pdf_url": "http://arxiv.org/pdf/2404.08866v1",
      "published_date": "2024-04-13 01:16:45 UTC",
      "updated_date": "2024-04-13 01:16:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:11:32.823280"
    },
    {
      "arxiv_id": "2404.08858v1",
      "title": "A Lightweight Spatiotemporal Network for Online Eye Tracking with Event Camera",
      "title_zh": "翻译失败",
      "authors": [
        "Yan Ru Pei",
        "Sasskia Brüers",
        "Sébastien Crouzet",
        "Douglas McLelland",
        "Olivier Coenen"
      ],
      "abstract": "Event-based data are commonly encountered in edge computing environments\nwhere efficiency and low latency are critical. To interface with such data and\nleverage their rich temporal features, we propose a causal spatiotemporal\nconvolutional network. This solution targets efficient implementation on\nedge-appropriate hardware with limited resources in three ways: 1) deliberately\ntargets a simple architecture and set of operations (convolutions, ReLU\nactivations) 2) can be configured to perform online inference efficiently via\nbuffering of layer outputs 3) can achieve more than 90% activation sparsity\nthrough regularization during training, enabling very significant efficiency\ngains on event-based processors. In addition, we propose a general affine\naugmentation strategy acting directly on the events, which alleviates the\nproblem of dataset scarcity for event-based systems. We apply our model on the\nAIS 2024 event-based eye tracking challenge, reaching a score of 0.9916 p10\naccuracy on the Kaggle private testset.",
      "tldr_zh": "这篇论文提出了一种轻量级时空网络（Spatiotemporal Network），用于事件相机（Event Camera）下的在线眼动追踪，旨在提升边缘计算环境的效率和低延迟。网络采用因果时空卷积设计，包括简单架构（如卷积和ReLU activations）、通过层输出缓冲实现在线推理，以及训练中的正则化以达到超过90%的激活稀疏性，从而显著提高处理器效率。论文还引入了一种直接作用于事件的仿射增强策略（Affine Augmentation），以缓解事件数据数据集稀缺的问题。在AIS 2024事件-based眼动追踪挑战中，该模型在Kaggle私人测试集上取得了0.9916的p10准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08858v1",
      "published_date": "2024-04-13 00:13:20 UTC",
      "updated_date": "2024-04-13 00:13:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:11:48.564968"
    },
    {
      "arxiv_id": "2404.08857v2",
      "title": "Voice Attribute Editing with Text Prompt",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyan Sheng",
        "Yang Ai",
        "Li-Juan Liu",
        "Jia Pan",
        "Zhen-Hua Ling"
      ],
      "abstract": "Despite recent advancements in speech generation with text prompt providing\ncontrol over speech style, voice attributes in synthesized speech remain\nelusive and challenging to control. This paper introduces a novel task: voice\nattribute editing with text prompt, with the goal of making relative\nmodifications to voice attributes according to the actions described in the\ntext prompt. To solve this task, VoxEditor, an end-to-end generative model, is\nproposed. In VoxEditor, addressing the insufficiency of text prompt, a Residual\nMemory (ResMem) block is designed, that efficiently maps voice attributes and\nthese descriptors into the shared feature space. Additionally, the ResMem block\nis enhanced with a voice attribute degree prediction (VADP) block to align\nvoice attributes with corresponding descriptors, addressing the imprecision of\ntext prompt caused by non-quantitative descriptions of voice attributes. We\nalso establish the open-source VCTK-RVA dataset, which leads the way in manual\nannotations detailing voice characteristic differences among different\nspeakers. Extensive experiments demonstrate the effectiveness and\ngeneralizability of our proposed method in terms of both objective and\nsubjective metrics. The dataset and audio samples are available on the website.",
      "tldr_zh": "本文提出了一种新任务：使用文本提示（Text Prompt）对语音属性进行相对编辑，以解决现有语音合成中属性控制的挑战。作者开发了端到端生成模型VoxEditor，该模型引入Residual Memory (ResMem)块来映射语音属性和描述到共享特征空间，并通过Voice Attribute Degree Prediction (VADP)块处理文本提示的不精确性。研究还建立了开源数据集VCTK-RVA，包含手动标注的语音特征差异。实验结果显示，VoxEditor在客观和主观指标上表现出色，并具有良好的泛化性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08857v2",
      "published_date": "2024-04-13 00:07:40 UTC",
      "updated_date": "2024-12-01 03:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:11:58.494777"
    },
    {
      "arxiv_id": "2404.08856v1",
      "title": "On Speculative Decoding for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mukul Gagrani",
        "Raghavv Goel",
        "Wonseok Jeon",
        "Junyoung Park",
        "Mingu Lee",
        "Christopher Lott"
      ],
      "abstract": "Inference with Multimodal Large Language Models (MLLMs) is slow due to their\nlarge-language-model backbone which suffers from memory bandwidth bottleneck\nand generates tokens auto-regressively. In this paper, we explore the\napplication of speculative decoding to enhance the inference efficiency of\nMLLMs, specifically the LLaVA 7B model. We show that a language-only model can\nserve as a good draft model for speculative decoding with LLaVA 7B, bypassing\nthe need for image tokens and their associated processing components from the\ndraft model. Our experiments across three different tasks show that speculative\ndecoding can achieve a memory-bound speedup of up to 2.37$\\times$ using a 115M\nparameter language model that we trained from scratch. Additionally, we\nintroduce a compact LLaVA draft model incorporating an image adapter, which\nshows marginal performance gains in image captioning while maintaining\ncomparable results in other tasks.",
      "tldr_zh": "这篇论文探讨了 speculative decoding 在 Multimodal Large Language Models (MLLMs) 中的应用，以解决其推理过程因内存带宽瓶颈和自回归 token 生成而导致的低效问题。研究以 LLaVA 7B 模型为例，使用一个仅语言模型作为 draft model，避免了图像 token 的处理，从而实现了高达 2.37× 的内存绑定加速。实验在三个任务上验证了这一方法的效果，并引入了一个包含图像适配器的紧凑 LLaVA draft model，在图像 captioning 任务中获得微小性能提升，同时在其他任务中保持可比结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted as a spotlight paper to ELVM workshop at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.08856v1",
      "published_date": "2024-04-13 00:02:36 UTC",
      "updated_date": "2024-04-13 00:02:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T00:12:10.785837"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 36,
  "processed_papers_count": 36,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T00:12:32.728205"
}
[
  {
    "arxiv_id": "2404.09384v2",
    "title": "Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches",
    "authors": [
      "VÃ­ctor A. Braberman",
      "Flavia Bonomo-Braberman",
      "Yiannis Charalambous",
      "Juan G. Colonna",
      "Lucas C. Cordeiro",
      "Rosiane de Freitas"
    ],
    "abstract": "Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners\nhave been \"playing\" with prompts (e.g., In-Context Learning) to see how to make\nthe most of pre-trained Language Models. By homogeneously dissecting more than\na hundred articles, we investigate how software testing and verification\nresearch communities have leveraged LLMs capabilities. First, we validate that\ndownstream tasks are adequate to convey a nontrivial modular blueprint of\nprompt-based proposals in scope. Moreover, we name and classify the concrete\ndownstream tasks we recover in both validation research papers and solution\nproposals. In order to perform classification, mapping, and analysis, we also\ndevelop a novel downstream-task taxonomy. The main taxonomy requirement is to\nhighlight commonalities while exhibiting variation points of task types that\nenable pinpointing emerging patterns in a varied spectrum of Software\nEngineering problems that encompasses testing, fuzzing, fault localization,\nvulnerability detection, static analysis, and program verification approaches.\nAvenues for future research are also discussed based on conceptual clusters\ninduced by the taxonomy.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "F.3.1; D.2.4; D.2.5; I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09384v2",
    "published_date": "2024-04-14 23:45:23 UTC",
    "updated_date": "2024-09-08 13:59:39 UTC"
  },
  {
    "arxiv_id": "2404.13070v2",
    "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
    "authors": [
      "Taylor Webb",
      "Keith J. Holyoak",
      "Hongjing Lu"
    ],
    "abstract": "We recently reported evidence that large language models are capable of\nsolving a wide range of text-based analogy problems in a zero-shot manner,\nindicating the presence of an emergent capacity for analogical reasoning. Two\nrecent commentaries have challenged these results, citing evidence from\nso-called `counterfactual' tasks in which the standard sequence of the alphabet\nis arbitrarily permuted so as to decrease similarity with materials that may\nhave been present in the language model's training data. Here, we reply to\nthese critiques, clarifying some misunderstandings about the test materials\nused in our original work, and presenting evidence that language models are\nalso capable of generalizing to these new counterfactual task variants.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13070v2",
    "published_date": "2024-04-14 21:51:02 UTC",
    "updated_date": "2024-04-29 19:48:56 UTC"
  },
  {
    "arxiv_id": "2404.09366v1",
    "title": "Understanding the Role of Temperature in Diverse Question Generation by GPT-4",
    "authors": [
      "Arav Agarwal",
      "Karthik Mittal",
      "Aidan Doyle",
      "Pragnya Sridhar",
      "Zipiao Wan",
      "Jacob Arthur Doughty",
      "Jaromir Savelka",
      "Majd Sakr"
    ],
    "abstract": "We conduct a preliminary study of the effect of GPT's temperature parameter\non the diversity of GPT4-generated questions. We find that using higher\ntemperature values leads to significantly higher diversity, with different\ntemperatures exposing different types of similarity between generated sets of\nquestions. We also demonstrate that diverse question generation is especially\ndifficult for questions targeting lower levels of Bloom's Taxonomy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09366v1",
    "published_date": "2024-04-14 21:38:50 UTC",
    "updated_date": "2024-04-14 21:38:50 UTC"
  },
  {
    "arxiv_id": "2404.09359v5",
    "title": "Evaluation Framework for Feedback Generation Methods in Skeletal Movement Assessment",
    "authors": [
      "Tal Hakim"
    ],
    "abstract": "The application of machine-learning solutions to movement assessment from\nskeleton videos has attracted significant research attention in recent years.\nThis advancement has made rehabilitation at home more accessible, utilizing\nmovement assessment algorithms that can operate on affordable equipment for\nhuman pose detection and analysis from 2D or 3D videos. While the primary\nobjective of automatic assessment tasks is to score movements, the automatic\ngeneration of feedback highlighting key movement issues has the potential to\nsignificantly enhance and accelerate the rehabilitation process. While numerous\nresearch works exist in the field of automatic movement assessment, only a\nhandful address feedback generation. In this study, we propose terminology and\ncriteria for the classification, evaluation, and comparison of feedback\ngeneration solutions. We discuss the challenges associated with each feedback\ngeneration approach and use our proposed criteria to classify existing\nsolutions. To our knowledge, this is the first work that formulates feedback\ngeneration in skeletal movement assessment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to xAI4Biometrics at ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.09359v5",
    "published_date": "2024-04-14 21:14:47 UTC",
    "updated_date": "2024-09-13 20:35:20 UTC"
  },
  {
    "arxiv_id": "2404.10019v1",
    "title": "Can AI Understand Our Universe? Test of Fine-Tuning GPT by Astrophysical Data",
    "authors": [
      "Yu Wang",
      "Shu-Rui Zhang",
      "Aidin Momtaz",
      "Rahim Moradi",
      "Fatemeh Rastegarnia",
      "Narek Sahakyan",
      "Soroush Shakeri",
      "Liang Li"
    ],
    "abstract": "ChatGPT has been the most talked-about concept in recent months, captivating\nboth professionals and the general public alike, and has sparked discussions\nabout the changes that artificial intelligence (AI) will bring to the world. As\nphysicists and astrophysicists, we are curious about if scientific data can be\ncorrectly analyzed by large language models (LLMs) and yield accurate physics.\nIn this article, we fine-tune the generative pre-trained transformer (GPT)\nmodel by the astronomical data from the observations of galaxies, quasars,\nstars, gamma-ray bursts (GRBs), and the simulations of black holes (BHs), the\nfine-tuned model demonstrates its capability to classify astrophysical\nphenomena, distinguish between two types of GRBs, deduce the redshift of\nquasars, and estimate BH parameters. We regard this as a successful test,\nmarking the LLM's proven efficacy in scientific research. With the ever-growing\nvolume of multidisciplinary data and the advancement of AI technology, we look\nforward to the emergence of a more fundamental and comprehensive understanding\nof our universe. This article also shares some interesting thoughts on data\ncollection and AI design. Using the approach of understanding the universe -\nlooking outward at data and inward for fundamental building blocks - as a\nguideline, we propose a method of series expansion for AI, suggesting ways to\ntrain and control AI that is smarter than humans.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA",
      "astro-ph.HE",
      "cs.AI",
      "cs.LG",
      "physics.data-an"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "27 pages, 7 figures. Comments welcome",
    "pdf_url": "http://arxiv.org/pdf/2404.10019v1",
    "published_date": "2024-04-14 20:52:19 UTC",
    "updated_date": "2024-04-14 20:52:19 UTC"
  },
  {
    "arxiv_id": "2404.09356v1",
    "title": "LLeMpower: Understanding Disparities in the Control and Access of Large Language Models",
    "authors": [
      "Vishwas Sathish",
      "Hannah Lin",
      "Aditya K Kamath",
      "Anish Nyayachavadi"
    ],
    "abstract": "Large Language Models (LLMs) are a powerful technology that augment human\nskill to create new opportunities, akin to the development of steam engines and\nthe internet. However, LLMs come with a high cost. They require significant\ncomputing resources and energy to train and serve. Inequity in their control\nand access has led to concentration of ownership and power to a small\ncollection of corporations. In our study, we collect training and inference\nrequirements for various LLMs. We then analyze the economic strengths of\nnations and organizations in the context of developing and serving these\nmodels. Additionally, we also look at whether individuals around the world can\naccess and use this emerging technology. We compare and contrast these groups\nto show that these technologies are monopolized by a surprisingly few entities.\nWe conclude with a qualitative study on the ethical implications of our\nfindings and discuss future directions towards equity in LLM access.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "K.4.0; K.7.4"
    ],
    "primary_category": "cs.CY",
    "comment": "11 total pages, 7 page text, 4 page references, 3 figures (with\n  subfigures), 1 table",
    "pdf_url": "http://arxiv.org/pdf/2404.09356v1",
    "published_date": "2024-04-14 20:49:53 UTC",
    "updated_date": "2024-04-14 20:49:53 UTC"
  },
  {
    "arxiv_id": "2406.15371v1",
    "title": "Affirmative safety: An approach to risk management for high-risk AI",
    "authors": [
      "Akash R. Wasil",
      "Joshua Clymer",
      "David Krueger",
      "Emily Dardaman",
      "Simeon Campos",
      "Evan R. Murphy"
    ],
    "abstract": "Prominent AI experts have suggested that companies developing high-risk AI\nsystems should be required to show that such systems are safe before they can\nbe developed or deployed. The goal of this paper is to expand on this idea and\nexplore its implications for risk management. We argue that entities developing\nor deploying high-risk AI systems should be required to present evidence of\naffirmative safety: a proactive case that their activities keep risks below\nacceptable thresholds. We begin the paper by highlighting global security risks\nfrom AI that have been acknowledged by AI experts and world governments. Next,\nwe briefly describe principles of risk management from other high-risk fields\n(e.g., nuclear safety). Then, we propose a risk management approach for\nadvanced AI in which model developers must provide evidence that their\nactivities keep certain risks below regulator-set thresholds. As a first step\ntoward understanding what affirmative safety cases should include, we\nillustrate how certain kinds of technical evidence and operational evidence can\nsupport an affirmative safety case. In the technical section, we discuss\nbehavioral evidence (evidence about model outputs), cognitive evidence\n(evidence about model internals), and developmental evidence (evidence about\nthe training process). In the operational section, we offer examples of\norganizational practices that could contribute to affirmative safety cases:\ninformation security practices, safety culture, and emergency response\ncapacity. Finally, we briefly compare our approach to the NIST AI Risk\nManagement Framework. Overall, we hope our work contributes to ongoing\ndiscussions about national and global security risks posed by AI and regulatory\napproaches to address these risks.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15371v1",
    "published_date": "2024-04-14 20:48:55 UTC",
    "updated_date": "2024-04-14 20:48:55 UTC"
  },
  {
    "arxiv_id": "2404.09352v1",
    "title": "Counteracting Concept Drift by Learning with Future Malware Predictions",
    "authors": [
      "Branislav Bosansky",
      "Lada Hospodkova",
      "Michal Najman",
      "Maria Rigaki",
      "Elnaz Babayeva",
      "Viliam Lisy"
    ],
    "abstract": "The accuracy of deployed malware-detection classifiers degrades over time due\nto changes in data distributions and increasing discrepancies between training\nand testing data. This phenomenon is known as the concept drift. While the\nconcept drift can be caused by various reasons in general, new malicious files\nare created by malware authors with a clear intention of avoiding detection.\nThe existence of the intention opens a possibility for predicting such future\nsamples. Including predicted samples in training data should consequently\nincrease the accuracy of the classifiers on new testing data.\n  We compare two methods for predicting future samples: (1) adversarial\ntraining and (2) generative adversarial networks (GANs). The first method\nexplicitly seeks for adversarial examples against the classifier that are then\nused as a part of training data. Similarly, GANs also generate synthetic\ntraining data. We use GANs to learn changes in data distributions within\ndifferent time periods of training data and then apply these changes to\ngenerate samples that could be in testing data. We compare these prediction\nmethods on two different datasets: (1) Ember public dataset and (2) the\ninternal dataset of files incoming to Avast. We show that while adversarial\ntraining yields more robust classifiers, this method is not a good predictor of\nfuture malware in general. This is in contrast with previously reported\npositive results in different domains (including natural language processing\nand spam detection). On the other hand, we show that GANs can be successfully\nused as predictors of future malware. We specifically examine malware families\nthat exhibit significant changes in their data distributions over time and the\nexperimental results confirm that GAN-based predictions can significantly\nimprove the accuracy of the classifier on new, previously unseen data.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09352v1",
    "published_date": "2024-04-14 20:28:07 UTC",
    "updated_date": "2024-04-14 20:28:07 UTC"
  },
  {
    "arxiv_id": "2404.09339v1",
    "title": "Towards Practical Tool Usage for Continually Learning LLMs",
    "authors": [
      "Jerry Huang",
      "Prasanna Parthasarathi",
      "Mehdi Rezagholizadeh",
      "Sarath Chandar"
    ],
    "abstract": "Large language models (LLMs) show an innate skill for solving language based\ntasks. But insights have suggested an inability to adjust for information or\ntask-solving skills becoming outdated, as their knowledge, stored directly\nwithin their parameters, remains static in time. Tool use helps by offloading\nwork to systems that the LLM can access through an interface, but LLMs that use\nthem still must adapt to nonstationary environments for prolonged use, as new\ntools can emerge and existing tools can change. Nevertheless, tools require\nless specialized knowledge, therefore we hypothesize they are better suited for\ncontinual learning (CL) as they rely less on parametric memory for solving\ntasks and instead focus on learning when to apply pre-defined tools. To verify\nthis, we develop a synthetic benchmark and follow this by aggregating existing\nNLP tasks to form a more realistic testing scenario. While we demonstrate\nscaling model size is not a solution, regardless of tool usage, continual\nlearning techniques can enable tool LLMs to both adapt faster while forgetting\nless, highlighting their potential as continual learners.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 11 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.09339v1",
    "published_date": "2024-04-14 19:45:47 UTC",
    "updated_date": "2024-04-14 19:45:47 UTC"
  },
  {
    "arxiv_id": "2404.09336v1",
    "title": "Self-Selected Attention Span for Accelerating Large Language Model Inference",
    "authors": [
      "Tian Jin",
      "Wanzin Yazar",
      "Zifei Xu",
      "Sayeh Sharify",
      "Xin Wang"
    ],
    "abstract": "Large language models (LLMs) can solve challenging tasks. However, their\ninference computation on modern GPUs is highly inefficient due to the\nincreasing number of tokens they must attend to as they generate new ones. To\naddress this inefficiency, we capitalize on LLMs' problem-solving capabilities\nto optimize their own inference-time efficiency. We demonstrate with two\nspecific tasks: (a) evaluating complex arithmetic expressions and (b)\nsummarizing news articles. For both tasks, we create custom datasets to\nfine-tune an LLM. The goal of fine-tuning is twofold: first, to make the LLM\nlearn to solve the evaluation or summarization task, and second, to train it to\nidentify the minimal attention spans required for each step of the task. As a\nresult, the fine-tuned model is able to convert these self-identified minimal\nattention spans into sparse attention masks on-the-fly during inference. We\ndevelop a custom CUDA kernel to take advantage of the reduced context to attend\nto. We demonstrate that using this custom CUDA kernel improves the throughput\nof LLM inference by 28%. Our work presents an end-to-end demonstration showing\nthat training LLMs to self-select their attention spans speeds up\nautoregressive inference in solving real-world tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09336v1",
    "published_date": "2024-04-14 19:36:04 UTC",
    "updated_date": "2024-04-14 19:36:04 UTC"
  },
  {
    "arxiv_id": "2404.09331v2",
    "title": "SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "abstract": "Recent trends have shown that autonomous agents, such as Autonomous Ground\nVehicles (AGVs), Unmanned Aerial Vehicles (UAVs), and mobile robots,\neffectively improve human productivity in solving diverse tasks. However, since\nthese agents are typically powered by portable batteries, they require\nextremely low power/energy consumption to operate in a long lifespan. To solve\nthis challenge, neuromorphic computing has emerged as a promising solution,\nwhere bio-inspired Spiking Neural Networks (SNNs) use spikes from event-based\ncameras or data conversion pre-processing to perform sparse computations\nefficiently. However, the studies of SNN deployments for autonomous agents are\nstill at an early stage. Hence, the optimization stages for enabling efficient\nembodied SNN deployments for autonomous agents have not been defined\nsystematically. Toward this, we propose a novel framework called SNN4Agents\nthat consists of a set of optimization techniques for designing\nenergy-efficient embodied SNNs targeting autonomous agent applications. Our\nSNN4Agents employs weight quantization, timestep reduction, and attention\nwindow reduction to jointly improve the energy efficiency, reduce the memory\nfootprint, optimize the processing latency, while maintaining high accuracy. In\nthe evaluation, we investigate use cases of event-based car recognition, and\nexplore the trade-offs among accuracy, latency, memory, and energy consumption.\nThe experimental results show that our proposed framework can maintain high\naccuracy (i.e., 84.12% accuracy) with 68.75% memory saving, 3.58x speed-up, and\n4.03x energy efficiency improvement as compared to the state-of-the-art work\nfor NCARS dataset. In this manner, our SNN4Agents framework paves the way\ntoward enabling energy-efficient embodied SNN deployments for autonomous\nagents.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication at Frontiers in Robotics and AI (FROBT) -\n  Section Robot Vision and Artificial Perception",
    "pdf_url": "http://arxiv.org/pdf/2404.09331v2",
    "published_date": "2024-04-14 19:06:00 UTC",
    "updated_date": "2024-06-18 08:36:11 UTC"
  },
  {
    "arxiv_id": "2404.09326v3",
    "title": "Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers",
    "authors": [
      "Diana-Nicoleta Grigore",
      "Mariana-Iuliana Georgescu",
      "Jon Alvarez Justo",
      "Tor Johansen",
      "Andreea Iuliana Ionescu",
      "Radu Tudor Ionescu"
    ],
    "abstract": "Few-shot knowledge distillation recently emerged as a viable approach to\nharness the knowledge of large-scale pre-trained models, using limited data and\ncomputational resources. In this paper, we propose a novel few-shot feature\ndistillation approach for vision transformers. Our approach is based on two key\nsteps. Leveraging the fact that vision transformers have a consistent\ndepth-wise structure, we first copy the weights from intermittent layers of\nexisting pre-trained vision transformers (teachers) into shallower\narchitectures (students), where the intermittence factor controls the\ncomplexity of the student transformer with respect to its teacher. Next, we\nemploy an enhanced version of Low-Rank Adaptation (LoRA) to distill knowledge\ninto the student in a few-shot scenario, aiming to recover the information\nprocessing carried out by the skipped teacher layers. We present comprehensive\nexperiments with supervised and self-supervised transformers as teachers, on\nsix data sets from various domains (natural, medical and satellite images) and\ntasks (classification and segmentation). The empirical results confirm the\nsuperiority of our approach over state-of-the-art competitors. Moreover, the\nablation results demonstrate the usefulness of each component of the proposed\npipeline. We release our code at https://github.com/dianagrigore/WeCoLoRA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025",
    "pdf_url": "http://arxiv.org/pdf/2404.09326v3",
    "published_date": "2024-04-14 18:57:38 UTC",
    "updated_date": "2024-10-30 16:27:20 UTC"
  },
  {
    "arxiv_id": "2404.09322v1",
    "title": "The intelligent prediction and assessment of financial information risk in the cloud computing model",
    "authors": [
      "Yufu Wang",
      "Mingwei Zhu",
      "Jiaqiang Yuan",
      "Guanghui Wang",
      "Hong Zhou"
    ],
    "abstract": "Cloud computing (cloud computing) is a kind of distributed computing,\nreferring to the network \"cloud\" will be a huge data calculation and processing\nprogram into countless small programs, and then, through the system composed of\nmultiple servers to process and analyze these small programs to get the results\nand return to the user. This report explores the intersection of cloud\ncomputing and financial information processing, identifying risks and\nchallenges faced by financial institutions in adopting cloud technology. It\ndiscusses the need for intelligent solutions to enhance data processing\nefficiency and accuracy while addressing security and privacy concerns. Drawing\non regulatory frameworks, the report proposes policy recommendations to\nmitigate concentration risks associated with cloud computing in the financial\nindustry. By combining intelligent forecasting and evaluation technologies with\ncloud computing models, the study aims to provide effective solutions for\nfinancial data processing and management, facilitating the industry's\ntransition towards digital transformation.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09322v1",
    "published_date": "2024-04-14 18:42:20 UTC",
    "updated_date": "2024-04-14 18:42:20 UTC"
  },
  {
    "arxiv_id": "2404.09317v1",
    "title": "Characterizing Soft-Error Resiliency in Arm's Ethos-U55 Embedded Machine Learning Accelerator",
    "authors": [
      "Abhishek Tyagi",
      "Reiley Jeyapaul",
      "Chuteng Zhu",
      "Paul Whatmough",
      "Yuhao Zhu"
    ],
    "abstract": "As Neural Processing Units (NPU) or accelerators are increasingly deployed in\na variety of applications including safety critical applications such as\nautonomous vehicle, and medical imaging, it is critical to understand the\nfault-tolerance nature of the NPUs. We present a reliability study of Arm's\nEthos-U55, an important industrial-scale NPU being utilised in embedded and IoT\napplications. We perform large scale RTL-level fault injections to characterize\nEthos-U55 against the Automotive Safety Integrity Level D (ASIL-D) resiliency\nstandard commonly used for safety-critical applications such as autonomous\nvehicles. We show that, under soft errors, all four configurations of the NPU\nfall short of the required level of resiliency for a variety of neural networks\nrunning on the NPU. We show that it is possible to meet the ASIL-D level\nresiliency without resorting to conventional strategies like Dual Core Lock\nStep (DCLS) that has an area overhead of 100%. We achieve so through selective\nprotection, where hardware structures are selectively protected (e.g.,\nduplicated, hardened) based on their sensitivity to soft errors and their\nsilicon areas. To identify the optimal configuration that minimizes the area\noverhead while meeting the ASIL-D standard, the main challenge is the large\nsearch space associated with the time-consuming RTL simulation. To address this\nchallenge, we present a statistical analysis tool that is validated against Arm\nsilicon and that allows us to quickly navigate hundreds of billions of fault\nsites without exhaustive RTL fault injections. We show that by carefully\nduplicating a small fraction of the functional blocks and hardening the Flops\nin other blocks meets the ASIL-D safety standard while introducing an area\noverhead of only 38%.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09317v1",
    "published_date": "2024-04-14 18:16:16 UTC",
    "updated_date": "2024-04-14 18:16:16 UTC"
  },
  {
    "arxiv_id": "2404.10795v1",
    "title": "Intelligent Message Behavioral Identification System",
    "authors": [
      "Yuvaraju Chinnam",
      "Bosubabu Sambana"
    ],
    "abstract": "On social media platforms, the act of predicting reposting is seen as a\nchallenging issue related to Short Message Services (SMS). This study examines\nthe issue of predicting picture reposting in SMS and forecasts users' behavior\nin sharing photographs on Twitter. Several research vary. The paper introduces\na network called Image Retweet Modeling (IRM) that models heterogeneous image\nretransmission. It considers the user's previous reposting of the image tweet,\nthe next contact in the SMS, and the preferences of the reposted person. Three\naspects connected to content. A text-guided multimodal neural network is\ndeveloped to create a novel multi-faceted attention ranking network\nmethodology. This allows for learning the joint image Twitter representation\nand user preference representation in the prediction job. Multiple experiments\nconducted on extensive data sets demonstrate that our approach outperforms\ncurrent methods on Social Network platforms.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.10795v1",
    "published_date": "2024-04-14 18:09:08 UTC",
    "updated_date": "2024-04-14 18:09:08 UTC"
  },
  {
    "arxiv_id": "2404.09313v3",
    "title": "Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment",
    "authors": [
      "Zhiqing Hong",
      "Rongjie Huang",
      "Xize Cheng",
      "Yongqi Wang",
      "Ruiqi Li",
      "Fuming You",
      "Zhou Zhao",
      "Zhimeng Zhang"
    ],
    "abstract": "A song is a combination of singing voice and accompaniment. However, existing\nworks focus on singing voice synthesis and music generation independently.\nLittle attention was paid to explore song synthesis. In this work, we propose a\nnovel task called text-to-song synthesis which incorporating both vocals and\naccompaniments generation. We develop Melodist, a two-stage text-to-song method\nthat consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A)\nsynthesis. Melodist leverages tri-tower contrastive pretraining to learn more\neffective text representation for controllable V2A synthesis. A Chinese song\ndataset mined from a music website is built up to alleviate data scarcity for\nour research. The evaluation results on our dataset demonstrate that Melodist\ncan synthesize songs with comparable quality and style consistency. Audio\nsamples can be found in https://text2songMelodist.github.io/Sample/.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "ACL 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2404.09313v3",
    "published_date": "2024-04-14 18:00:05 UTC",
    "updated_date": "2024-05-20 05:50:36 UTC"
  },
  {
    "arxiv_id": "2407.00824v1",
    "title": "A data-driven approach to modeling brain activity using differential equations",
    "authors": [
      "Kuratov Andrey"
    ],
    "abstract": "This research focuses on an innovative task of extracting equations from\nincomplete data, moving away from traditional methods used for complete\nsolutions. The study addresses the challenge of extracting equations from data,\nparticularly in the study of brain activity using electrophysiological data,\nwhich is often limited by insufficient information. The study provides a brief\nreview of existing open-source equation derivation approaches in the context of\nmodeling brain activity. The section below introduces a novel algorithm that\nemploys incomplete data and prior domain knowledge to recover differential\nequations. The algorithm's practicality in real-world scenarios is demonstrated\nthrough its application on both synthetic and real datasets.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.00824v1",
    "published_date": "2024-04-14 17:33:09 UTC",
    "updated_date": "2024-04-14 17:33:09 UTC"
  },
  {
    "arxiv_id": "2404.09305v2",
    "title": "OWLOOP: Interfaces for Mapping OWL Axioms into OOP Hierarchies",
    "authors": [
      "Luca Buoncompagni",
      "Fulvio Mastrogiovanni"
    ],
    "abstract": "The paper tackles the issue of mapping logic axioms formalised in the\nOntology Web Language (OWL) within the Object-Oriented Programming (OOP)\nparadigm. The issues of mapping OWL axioms hierarchies and OOP objects\nhierarchies are due to OWL-based reasoning algorithms, which might change an\nOWL hierarchy at runtime; instead, OOP hierarchies are usually defined as\nstatic structures. Although programming paradigms based on reflection allow\nchanging the OOP hierarchies at runtime and mapping OWL axioms dynamically,\nthere are no currently available mechanisms that do not limit the reasoning\nalgorithms. Thus, the factory-based paradigm is typically used since it\ndecouples the OWL and OOP hierarchies. However, the factory inhibits OOP\npolymorphism and introduces a paradigm shift with respect to widely accepted\nOOP paradigms. We present the OWLOOP API, which exploits the factory to not\nlimit reasoning algorithms, and it provides novel OOP interfaces concerning the\naxioms in an ontology. OWLOOP is designed to limit the paradigm shift required\nfor using ontologies while improving, through OOP-like polymorphism, the\nmodularity of software architectures that exploit logic reasoning. The paper\ndetails our OWL to OOP mapping mechanism, and it shows the benefits and\nlimitations of OWLOOP through examples concerning a robot in a smart\nenvironment.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.RO",
      "cs.SE",
      "68T27 (Primary) 68T30, 68N19, 68T40 (Secondary)",
      "D.2.11; D.1.5; D.1.6; E.2; I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "This manuscript details the implementation of the OWLOOP API. A\n  simplified (and \"citable\") presentation of our API has been published in the\n  SoftwareX Elsevier journal with the title \"OWLOOP: A modular API to describe\n  OWL axioms in OOP objects hierarchies\" (\n  https://doi.org/10.1016/j.softx.2021.100952). The OWLOOP API repository is\n  available at https://github.com/buoncubi/owloop",
    "pdf_url": "http://arxiv.org/pdf/2404.09305v2",
    "published_date": "2024-04-14 17:07:59 UTC",
    "updated_date": "2024-04-19 17:43:26 UTC"
  },
  {
    "arxiv_id": "2404.12399v1",
    "title": "Model Failure or Data Corruption? Exploring Inconsistencies in Building Energy Ratings with Self-Supervised Contrastive Learning",
    "authors": [
      "Qian Xiao",
      "Dan Liu",
      "Kevin Credit"
    ],
    "abstract": "Building Energy Rating (BER) stands as a pivotal metric, enabling building\nowners, policymakers, and urban planners to understand the energy-saving\npotential through improving building energy efficiency. As such, enhancing\nbuildings' BER levels is expected to directly contribute to the reduction of\ncarbon emissions and promote climate improvement. Nonetheless, the BER\nassessment process is vulnerable to missing and inaccurate measurements. In\nthis study, we introduce \\texttt{CLEAR}, a data-driven approach designed to\nscrutinize the inconsistencies in BER assessments through self-supervised\ncontrastive learning. We validated the effectiveness of \\texttt{CLEAR} using a\ndataset representing Irish building stocks. Our experiments uncovered evidence\nof inconsistent BER assessments, highlighting measurement data corruption\nwithin this real-world dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.12399v1",
    "published_date": "2024-04-14 17:07:11 UTC",
    "updated_date": "2024-04-14 17:07:11 UTC"
  },
  {
    "arxiv_id": "2404.09304v1",
    "title": "Monte Carlo Search Algorithms Discovering Monte Carlo Tree Search Exploration Terms",
    "authors": [
      "Tristan Cazenave"
    ],
    "abstract": "Monte Carlo Tree Search and Monte Carlo Search have good results for many\ncombinatorial problems. In this paper we propose to use Monte Carlo Search to\ndesign mathematical expressions that are used as exploration terms for Monte\nCarlo Tree Search algorithms. The optimized Monte Carlo Tree Search algorithms\nare PUCT and SHUSS. We automatically design the PUCT and the SHUSS root\nexploration terms. For small search budgets of 32 evaluations the discovered\nroot exploration terms make both algorithms competitive with usual PUCT.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09304v1",
    "published_date": "2024-04-14 17:06:20 UTC",
    "updated_date": "2024-04-14 17:06:20 UTC"
  },
  {
    "arxiv_id": "2404.09302v2",
    "title": "High Significant Fault Detection in Azure Core Workload Insights",
    "authors": [
      "Pranay Lohia",
      "Laurent Boue",
      "Sharath Rangappa",
      "Vijay Agneeswaran"
    ],
    "abstract": "Azure Core workload insights have time-series data with different metric\nunits. Faults or Anomalies are observed in these time-series data owing to\nfaults observed with respect to metric name, resources region, dimensions, and\nits dimension value associated with the data. For Azure Core, an important task\nis to highlight faults or anomalies to the user on a dashboard that they can\nperceive easily. The number of anomalies reported should be highly significant\nand in a limited number, e.g., 5-20 anomalies reported per hour. The reported\nanomalies will have significant user perception and high reconstruction error\nin any time-series forecasting model. Hence, our task is to automatically\nidentify 'high significant anomalies' and their associated information for user\nperception.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in IAAI 2024, which is the Industrial track of AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.09302v2",
    "published_date": "2024-04-14 16:57:41 UTC",
    "updated_date": "2024-07-25 06:05:54 UTC"
  },
  {
    "arxiv_id": "2404.16057v1",
    "title": "LuminLab: An AI-Powered Building Retrofit and Energy Modelling Platform",
    "authors": [
      "Kevin Credit",
      "Qian Xiao",
      "Jack Lehane",
      "Juan Vazquez",
      "Dan Liu",
      "Leo De Figueiredo"
    ],
    "abstract": "This paper describes the technical and conceptual development of the LuminLab\nplatform, an online tool that integrates a purpose-fit human-centric AI chatbot\nand predictive energy model into a streamlined front-end that can rapidly\nproduce and discuss building retrofit plans in natural language. The platform\nprovides users with the ability to engage with a range of possible retrofit\npathways tailored to their individual budget and building needs on-demand.\nGiven the complicated and costly nature of building retrofit projects, which\nrely on a variety of stakeholder groups with differing goals and incentives, we\nfeel that AI-powered tools such as this have the potential to pragmatically\nde-silo knowledge, improve communication, and empower individual homeowners to\nundertake incremental retrofit projects that might not happen otherwise.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16057v1",
    "published_date": "2024-04-14 16:47:00 UTC",
    "updated_date": "2024-04-14 16:47:00 UTC"
  },
  {
    "arxiv_id": "2404.09292v2",
    "title": "Bridging Data Islands: Geographic Heterogeneity-Aware Federated Learning for Collaborative Remote Sensing Semantic Segmentation",
    "authors": [
      "Jieyi Tan",
      "Yansheng Li",
      "Sergey A. Bartalev",
      "Shinkarenko Stanislav",
      "Bo Dang",
      "Yongjun Zhang",
      "Liangqi Yuan",
      "Wei Chen"
    ],
    "abstract": "Remote sensing semantic segmentation (RSS) is an essential technology in\nearth observation missions. Due to concerns over geographic information\nsecurity, data privacy, storage bottleneck and industry competition,\nhigh-quality annotated remote sensing images are often isolated and distributed\nacross institutions. The issue of remote sensing data islands poses challenges\nfor fully utilizing isolated datasets to train a global model. Federated\nlearning (FL), a privacy-preserving distributed collaborative learning\ntechnology, offers a potential solution to leverage isolated remote sensing\ndata. Typically, remote sensing images from different institutions exhibit\nsignificant geographic heterogeneity, characterized by coupled\nclass-distribution heterogeneity and object-appearance heterogeneity. However,\nexisting FL methods lack consideration of them, leading to a decline in the\nperformance of the global model when FL is directly applied to RSS. We propose\na novel Geographic heterogeneity-aware Federated learning (GeoFed) framework to\nbridge data islands in RSS. Our framework consists of three modules, including\nthe Global Insight Enhancement (GIE) module, the Essential Feature Mining (EFM)\nmodule and the Local-Global Balance (LoGo) module. Through the GIE module,\nclass distribution heterogeneity is alleviated by introducing a prior global\nclass distribution vector. We design an EFM module to alleviate object\nappearance heterogeneity by constructing essential features. Furthermore, the\nLoGo module enables the model to possess both global generalization capability\nand local adaptation. Extensive experiments on three public datasets (i.e.,\nFedFBP, FedCASID, FedInria) demonstrate that our GeoFed framework consistently\noutperforms the current state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages,12 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.09292v2",
    "published_date": "2024-04-14 15:58:35 UTC",
    "updated_date": "2024-12-24 14:07:46 UTC"
  },
  {
    "arxiv_id": "2404.10539v1",
    "title": "VideoSAGE: Video Summarization with Graph Representation Learning",
    "authors": [
      "Jose M. Rojas Chaves",
      "Subarna Tripathi"
    ],
    "abstract": "We propose a graph-based representation learning framework for video\nsummarization. First, we convert an input video to a graph where nodes\ncorrespond to each of the video frames. Then, we impose sparsity on the graph\nby connecting only those pairs of nodes that are within a specified temporal\ndistance. We then formulate the video summarization task as a binary node\nclassification problem, precisely classifying video frames whether they should\nbelong to the output summary video. A graph constructed this way aims to\ncapture long-range interactions among video frames, and the sparsity ensures\nthe model trains without hitting the memory and compute bottleneck. Experiments\non two datasets(SumMe and TVSum) demonstrate the effectiveness of the proposed\nnimble model compared to existing state-of-the-art summarization approaches\nwhile being one order of magnitude more efficient in compute time and memory",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2207.07783",
    "pdf_url": "http://arxiv.org/pdf/2404.10539v1",
    "published_date": "2024-04-14 15:49:02 UTC",
    "updated_date": "2024-04-14 15:49:02 UTC"
  },
  {
    "arxiv_id": "2404.09286v1",
    "title": "Artificial Intelligence enhanced Security Problems in Real-Time Scenario using Blowfish Algorithm",
    "authors": [
      "Yuvaraju Chinnam",
      "Bosubabu Sambana"
    ],
    "abstract": "In a nutshell, \"the cloud\" refers to a collection of interconnected computing\nresources made possible by an extensive, real-time communication network like\nthe internet. Because of its potential to reduce processing costs, the emerging\nparadigm of cloud computing has recently attracted a large number of academics.\nThe exponential expansion of cloud computing has made the rapid expansion of\ncloud services very remarkable. Ensuring the security of personal information\nin today's interconnected world is no easy task. These days, security is really\ncrucial. Models of security that are relevant to cloud computing include\nconfidentiality, authenticity, accessibility, data integrity, and recovery.\nUsing the Hybrid Encryption this study, we cover all the security issues and\nleaks in cloud infrastructure.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09286v1",
    "published_date": "2024-04-14 15:38:34 UTC",
    "updated_date": "2024-04-14 15:38:34 UTC"
  },
  {
    "arxiv_id": "2404.10017v1",
    "title": "Model-based Offline Quantum Reinforcement Learning",
    "authors": [
      "Simon Eisenmann",
      "Daniel Hein",
      "Steffen Udluft",
      "Thomas A. Runkler"
    ],
    "abstract": "This paper presents the first algorithm for model-based offline quantum\nreinforcement learning and demonstrates its functionality on the cart-pole\nbenchmark. The model and the policy to be optimized are each implemented as\nvariational quantum circuits. The model is trained by gradient descent to fit a\npre-recorded data set. The policy is optimized with a gradient-free\noptimization scheme using the return estimate given by the model as the fitness\nfunction. This model-based approach allows, in principle, full realization on a\nquantum computer during the optimization phase and gives hope that a quantum\nadvantage can be achieved as soon as sufficiently powerful quantum computers\nare available.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.10017v1",
    "published_date": "2024-04-14 15:11:27 UTC",
    "updated_date": "2024-04-14 15:11:27 UTC"
  },
  {
    "arxiv_id": "2404.09275v1",
    "title": "TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning",
    "authors": [
      "Quang Minh Dinh",
      "Minh Khoi Ho",
      "Anh Quan Dang",
      "Hung Phong Tran"
    ],
    "abstract": "Traffic video description and analysis have received much attention recently\ndue to the growing demand for efficient and reliable urban surveillance\nsystems. Most existing methods only focus on locating traffic event segments,\nwhich severely lack descriptive details related to the behaviour and context of\nall the subjects of interest in the events. In this paper, we present\nTrafficVLM, a novel multi-modal dense video captioning model for vehicle ego\ncamera view. TrafficVLM models traffic video events at different levels of\nanalysis, both spatially and temporally, and generates long fine-grained\ndescriptions for the vehicle and pedestrian at different phases of the event.\nWe also propose a conditional component for TrafficVLM to control the\ngeneration outputs and a multi-task fine-tuning paradigm to enhance\nTrafficVLM's learning capability. Experiments show that TrafficVLM performs\nwell on both vehicle and overhead camera views. Our solution achieved\noutstanding results in Track 2 of the AI City Challenge 2024, ranking us third\nin the challenge standings. Our code is publicly available at\nhttps://github.com/quangminhdinh/TrafficVLM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09275v1",
    "published_date": "2024-04-14 14:51:44 UTC",
    "updated_date": "2024-04-14 14:51:44 UTC"
  },
  {
    "arxiv_id": "2404.09265v1",
    "title": "Make Split, not Hijack: Preventing Feature-Space Hijacking Attacks in Split Learning",
    "authors": [
      "Tanveer Khan",
      "Mindaugas Budzys",
      "Antonis Michalas"
    ],
    "abstract": "The popularity of Machine Learning (ML) makes the privacy of sensitive data\nmore imperative than ever. Collaborative learning techniques like Split\nLearning (SL) aim to protect client data while enhancing ML processes. Though\npromising, SL has been proved to be vulnerable to a plethora of attacks, thus\nraising concerns about its effectiveness on data privacy. In this work, we\nintroduce a hybrid approach combining SL and Function Secret Sharing (FSS) to\nensure client data privacy. The client adds a random mask to the activation map\nbefore sending it to the servers. The servers cannot access the original\nfunction but instead work with shares generated using FSS. Consequently, during\nboth forward and backward propagation, the servers cannot reconstruct the\nclient's raw data from the activation map. Furthermore, through visual\ninvertibility, we demonstrate that the server is incapable of reconstructing\nthe raw image data from the activation map when using FSS. It enhances privacy\nby reducing privacy leakage compared to other SL-based approaches where the\nserver can access client input information. Our approach also ensures security\nagainst feature space hijacking attack, protecting sensitive information from\npotential manipulation. Our protocols yield promising results, reducing\ncommunication overhead by over 2x and training time by over 7x compared to the\nsame model with FSS, without any SL. Also, we show that our approach achieves\n>96% accuracy and remains equivalent to the plaintext models.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted In Proceedings of the 29th ACM Symposium on Access Control\n  Models and Technologies (SACMAT '24)",
    "pdf_url": "http://arxiv.org/pdf/2404.09265v1",
    "published_date": "2024-04-14 14:14:31 UTC",
    "updated_date": "2024-04-14 14:14:31 UTC"
  },
  {
    "arxiv_id": "2404.09263v1",
    "title": "Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection",
    "authors": [
      "Jin Yang",
      "Ping Wei",
      "Huan Li",
      "Ziyang Ren"
    ],
    "abstract": "Video moment retrieval and highlight detection are two highly valuable tasks\nin video understanding, but until recently they have been jointly studied.\nAlthough existing studies have made impressive advancement recently, they\npredominantly follow the data-driven bottom-up paradigm. Such paradigm\noverlooks task-specific and inter-task effects, resulting in poor model\nperformance. In this paper, we propose a novel task-driven top-down framework\nTaskWeave for joint moment retrieval and highlight detection. The framework\nintroduces a task-decoupled unit to capture task-specific and common\nrepresentations. To investigate the interplay between the two tasks, we propose\nan inter-task feedback mechanism, which transforms the results of one task as\nguiding masks to assist the other task. Different from existing methods, we\npresent a task-dependent joint loss function to optimize the model.\nComprehensive experiments and in-depth ablation studies on QVHighlights, TVSum,\nand Charades-STA datasets corroborate the effectiveness and flexibility of the\nproposed framework. Codes are available at\nhttps://github.com/EdenGabriel/TaskWeave.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09263v1",
    "published_date": "2024-04-14 14:06:42 UTC",
    "updated_date": "2024-04-14 14:06:42 UTC"
  },
  {
    "arxiv_id": "2404.09259v2",
    "title": "FedCCL: Federated Dual-Clustered Feature Contrast Under Domain Heterogeneity",
    "authors": [
      "Yu Qiao",
      "Huy Q. Le",
      "Mengchun Zhang",
      "Apurba Adhikary",
      "Chaoning Zhang",
      "Choong Seon Hong"
    ],
    "abstract": "Federated learning (FL) facilitates a privacy-preserving neural network\ntraining paradigm through collaboration between edge clients and a central\nserver. One significant challenge is that the distributed data is not\nindependently and identically distributed (non-IID), typically including both\nintra-domain and inter-domain heterogeneity. However, recent research is\nlimited to simply using averaged signals as a form of regularization and only\nfocusing on one aspect of these non-IID challenges. Given these limitations,\nthis paper clarifies these two non-IID challenges and attempts to introduce\ncluster representation to address them from both local and global perspectives.\nSpecifically, we propose a dual-clustered feature contrast-based FL framework\nwith dual focuses. First, we employ clustering on the local representations of\neach client, aiming to capture intra-class information based on these local\nclusters at a high level of granularity. Then, we facilitate cross-client\nknowledge sharing by pulling the local representation closer to clusters shared\nby clients with similar semantics while pushing them away from clusters with\ndissimilar semantics. Second, since the sizes of local clusters belonging to\nthe same class may differ for each client, we further utilize clustering on the\nglobal side and conduct averaging to create a consistent global signal for\nguiding each local training in a contrastive manner. Experimental results on\nmultiple datasets demonstrate that our proposal achieves comparable or superior\nperformance gain under intra-domain and inter-domain heterogeneity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work was accepted by Information Fusion Journal",
    "pdf_url": "http://arxiv.org/pdf/2404.09259v2",
    "published_date": "2024-04-14 13:56:30 UTC",
    "updated_date": "2024-09-11 08:07:01 UTC"
  },
  {
    "arxiv_id": "2404.09248v1",
    "title": "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts",
    "authors": [
      "Jing-Cheng Pang",
      "Si-Hang Yang",
      "Kaiyuan Li",
      "Jiaji Zhang",
      "Xiong-Hui Chen",
      "Nan Tang",
      "Yang Yu"
    ],
    "abstract": "Reinforcement learning (RL) trains agents to accomplish complex tasks through\nenvironmental interaction data, but its capacity is also limited by the scope\nof the available data. To obtain a knowledgeable agent, a promising approach is\nto leverage the knowledge from large language models (LLMs). Despite previous\nstudies combining LLMs with RL, seamless integration of the two components\nremains challenging due to their semantic gap. This paper introduces a novel\nmethod, Knowledgeable Agents from Language Model Rollouts (KALM), which\nextracts knowledge from LLMs in the form of imaginary rollouts that can be\neasily learned by the agent through offline reinforcement learning methods. The\nprimary challenge of KALM lies in LLM grounding, as LLMs are inherently limited\nto textual data, whereas environmental data often comprise numerical vectors\nunseen to LLMs. To address this, KALM fine-tunes the LLM to perform various\ntasks based on environmental data, including bidirectional translation between\nnatural language descriptions of skills and their corresponding rollout data.\nThis grounding process enhances the LLM's comprehension of environmental\ndynamics, enabling it to generate diverse and meaningful imaginary rollouts\nthat reflect novel skills. Initial empirical evaluations on the CLEVR-Robot\nenvironment demonstrate that KALM enables agents to complete complex\nrephrasings of task goals and extend their capabilities to novel tasks\nrequiring unprecedented optimal behaviors. KALM achieves a success rate of 46%\nin executing tasks with unseen goals, substantially surpassing the 26% success\nrate achieved by baseline methods. Furthermore, KALM effectively enables the\nLLM to comprehend environmental dynamics, resulting in the generation of\nmeaningful imaginary rollouts that reflect novel skills and demonstrate the\nseamless integration of large language models and reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09248v1",
    "published_date": "2024-04-14 13:19:40 UTC",
    "updated_date": "2024-04-14 13:19:40 UTC"
  },
  {
    "arxiv_id": "2404.09221v2",
    "title": "Exploring and Improving Drafts in Blockwise Parallel Decoding",
    "authors": [
      "Taehyeon Kim",
      "Ananda Theertha Suresh",
      "Kishore Papineni",
      "Michael Riley",
      "Sanjiv Kumar",
      "Adrian Benton"
    ],
    "abstract": "Despite the remarkable strides made by autoregressive language models, their\npotential is often hampered by the slow inference speeds inherent in sequential\ntoken generation. Blockwise parallel decoding (BPD) was proposed by Stern et\nal. as a method to improve inference speed of language models by simultaneously\npredicting multiple future tokens, termed block drafts, which are subsequently\nverified and conditionally accepted by the autoregressive model. This paper\ncontributes to the understanding and improvement of block drafts in two ways.\nFirst, we analyze the token distributions produced by multiple prediction\nheads. Secondly, we leverage this analysis to develop algorithms to improve BPD\ninference speed by refining the block drafts using n-gram and neural language\nmodels. Experiments demonstrate that refined block drafts yield a +5-21%\nincrease in block efficiency (i.e., the number of accepted tokens from the\nblock draft) across diverse datasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09221v2",
    "published_date": "2024-04-14 11:49:38 UTC",
    "updated_date": "2024-06-05 05:00:35 UTC"
  },
  {
    "arxiv_id": "2404.09210v1",
    "title": "FedDistill: Global Model Distillation for Local Model De-Biasing in Non-IID Federated Learning",
    "authors": [
      "Changlin Song",
      "Divya Saxena",
      "Jiannong Cao",
      "Yuqing Zhao"
    ],
    "abstract": "Federated Learning (FL) is a novel approach that allows for collaborative\nmachine learning while preserving data privacy by leveraging models trained on\ndecentralized devices. However, FL faces challenges due to non-uniformly\ndistributed (non-iid) data across clients, which impacts model performance and\nits generalization capabilities. To tackle the non-iid issue, recent efforts\nhave utilized the global model as a teaching mechanism for local models.\nHowever, our pilot study shows that their effectiveness is constrained by\nimbalanced data distribution, which induces biases in local models and leads to\na 'local forgetting' phenomenon, where the ability of models to generalize\ndegrades over time, particularly for underrepresented classes. This paper\nintroduces FedDistill, a framework enhancing the knowledge transfer from the\nglobal model to local models, focusing on the issue of imbalanced class\ndistribution. Specifically, FedDistill employs group distillation, segmenting\nclasses based on their frequency in local datasets to facilitate a focused\ndistillation process to classes with fewer samples. Additionally, FedDistill\ndissects the global model into a feature extractor and a classifier. This\nseparation empowers local models with more generalized data representation\ncapabilities and ensures more accurate classification across all classes.\nFedDistill mitigates the adverse effects of data imbalance, ensuring that local\nmodels do not forget underrepresented classes but instead become more adept at\nrecognizing and classifying them accurately. Our comprehensive experiments\ndemonstrate FedDistill's effectiveness, surpassing existing baselines in\naccuracy and convergence speed across several benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 9 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.09210v1",
    "published_date": "2024-04-14 10:23:30 UTC",
    "updated_date": "2024-04-14 10:23:30 UTC"
  },
  {
    "arxiv_id": "2404.09204v1",
    "title": "TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models",
    "authors": [
      "Ya-Qi Yu",
      "Minghui Liao",
      "Jihao Wu",
      "Yongxin Liao",
      "Xiaoyu Zheng",
      "Wei Zeng"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive results on\nvarious multimodal tasks. However, most existing MLLMs are not well suited for\ndocument-oriented tasks, which require fine-grained image perception and\ninformation compression. In this paper, we present TextHawk, a MLLM that is\nspecifically designed for document-oriented tasks, while preserving the general\ncapabilities of MLLMs. TextHawk is aimed to explore efficient fine-grained\nperception by designing four dedicated components. Firstly, a ReSampling and\nReArrangement (ReSA) module is proposed to reduce the redundancy in the\ndocument texts and lower the computational cost of the MLLM. We explore\nencoding the positions of each local feature by presenting Scalable Positional\nEmbeddings (SPEs), which can preserve the scalability of various image sizes. A\nQuery Proposal Network (QPN) is then adopted to initialize the queries\ndynamically among different sub-images. To further enhance the fine-grained\nvisual perceptual ability of the MLLM, we design a Multi-Level Cross-Attention\n(MLCA) mechanism that captures the hierarchical structure and semantic\nrelations of document images. Furthermore, we create a new instruction-tuning\ndataset for document-oriented tasks by enriching the multimodal document data\nwith Gemini Pro. We conduct extensive experiments on both general and\ndocument-oriented MLLM benchmarks, and show that TextHawk outperforms the\nstate-of-the-art methods, demonstrating its effectiveness and superiority in\nfine-grained document perception and general abilities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09204v1",
    "published_date": "2024-04-14 09:48:37 UTC",
    "updated_date": "2024-04-14 09:48:37 UTC"
  },
  {
    "arxiv_id": "2404.09192v1",
    "title": "Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling",
    "authors": [
      "Quanxiu Wang",
      "Hui Huang",
      "Mingjie Wang",
      "Yong Dai",
      "Jinzuomu Zhong",
      "Benlai Tang"
    ],
    "abstract": "Over the past decade, a series of unflagging efforts have been dedicated to\ndeveloping highly expressive and controllable text-to-speech (TTS) systems. In\ngeneral, the holistic TTS comprises two interconnected components: the frontend\nmodule and the backend module. The frontend excels in capturing linguistic\nrepresentations from the raw text input, while the backend module converts\nlinguistic cues to speech. The research community has shown growing interest in\nthe study of the frontend component, recognizing its pivotal role in\ntext-to-speech systems, including Text Normalization (TN), Prosody Boundary\nPrediction (PBP), and Polyphone Disambiguation (PD). Nonetheless, the\nlimitations posed by insufficient annotated textual data and the reliance on\nhomogeneous text signals significantly undermine the effectiveness of its\nsupervised learning. To evade this obstacle, a novel two-stage TTS frontend\nprediction pipeline, named TAP-FM, is proposed in this paper. Specifically,\nduring the first learning phase, we present a Multi-scale Contrastive\nText-audio Pre-training protocol (MC-TAP), which hammers at acquiring richer\ninsights via multi-granularity contrastive pre-training in an unsupervised\nmanner. Instead of mining homogeneous features in prior pre-training\napproaches, our framework demonstrates the ability to delve deep into both\nglobal and local text-audio semantic and acoustic representations. Furthermore,\na parallelized TTS frontend model is delicately devised to execute TN, PD, and\nPBP prediction tasks, respectively in the second stage. Finally, extensive\nexperiments illustrate the superiority of our proposed method, achieving\nstate-of-the-art performance.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09192v1",
    "published_date": "2024-04-14 08:56:19 UTC",
    "updated_date": "2024-04-14 08:56:19 UTC"
  },
  {
    "arxiv_id": "2405.06655v1",
    "title": "RNA Secondary Structure Prediction Using Transformer-Based Deep Learning Models",
    "authors": [
      "Yanlin Zhou",
      "Tong Zhan",
      "Yichao Wu",
      "Bo Song",
      "Chenxi Shi"
    ],
    "abstract": "The Human Genome Project has led to an exponential increase in data related\nto the sequence, structure, and function of biomolecules. Bioinformatics is an\ninterdisciplinary research field that primarily uses computational methods to\nanalyze large amounts of biological macromolecule data. Its goal is to discover\nhidden biological patterns and related information. Furthermore, analysing\nadditional relevant information can enhance the study of biological operating\nmechanisms. This paper discusses the fundamental concepts of RNA, RNA secondary\nstructure, and its prediction.Subsequently, the application of machine learning\ntechnologies in predicting the structure of biological macromolecules is\nexplored. This chapter describes the relevant knowledge of algorithms and\ncomputational complexity and presents a RNA tertiary structure prediction\nalgorithm based on ResNet. To address the issue of the current scoring\nfunction's unsuitability for long RNA, a scoring model based on ResNet is\nproposed, and a structure prediction algorithm is designed. The chapter\nconcludes by presenting some open and interesting challenges in the field of\nRNA tertiary structure prediction.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06655v1",
    "published_date": "2024-04-14 08:36:14 UTC",
    "updated_date": "2024-04-14 08:36:14 UTC"
  },
  {
    "arxiv_id": "2404.09173v3",
    "title": "TransformerFAM: Feedback attention is working memory",
    "authors": [
      "Dongseong Hwang",
      "Weiran Wang",
      "Zhuoyuan Huo",
      "Khe Chai Sim",
      "Pedro Moreno Mengibar"
    ],
    "abstract": "While Transformers have revolutionized deep learning, their quadratic\nattention complexity hinders their ability to process infinitely long inputs.\nWe propose Feedback Attention Memory (FAM), a novel Transformer architecture\nthat leverages a feedback loop to enable the network to attend to its own\nlatent representations. This design fosters the emergence of working memory\nwithin the Transformer, allowing it to process indefinitely long sequences.\nTransformerFAM requires no additional weights, enabling seamless integration\nwith pre-trained models. Our experiments show that TransformerFAM significantly\nimproves Transformer performance on long-context tasks across various model\nsizes (1B, 8B, and 24B). These results showcase the potential to empower Large\nLanguage Models (LLMs) to process sequences of unlimited length.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 12 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.09173v3",
    "published_date": "2024-04-14 07:43:45 UTC",
    "updated_date": "2024-05-07 13:23:46 UTC"
  },
  {
    "arxiv_id": "2404.09172v2",
    "title": "LoopAnimate: Loopable Salient Object Animation",
    "authors": [
      "Fanyi Wang",
      "Peng Liu",
      "Haotian Hu",
      "Dan Meng",
      "Jingwen Su",
      "Jinjin Xu",
      "Yanhao Zhang",
      "Xiaoming Ren",
      "Zhiwang Zhang"
    ],
    "abstract": "Research on diffusion model-based video generation has advanced rapidly.\nHowever, limitations in object fidelity and generation length hinder its\npractical applications. Additionally, specific domains like animated wallpapers\nrequire seamless looping, where the first and last frames of the video match\nseamlessly. To address these challenges, this paper proposes LoopAnimate, a\nnovel method for generating videos with consistent start and end frames. To\nenhance object fidelity, we introduce a framework that decouples multi-level\nimage appearance and textual semantic information. Building upon an\nimage-to-image diffusion model, our approach incorporates both pixel-level and\nfeature-level information from the input image, injecting image appearance and\ntextual semantic embeddings at different positions of the diffusion model.\nExisting UNet-based video generation models require to input the entire videos\nduring training to encode temporal and positional information at once. However,\ndue to limitations in GPU memory, the number of frames is typically restricted\nto 16. To address this, this paper proposes a three-stage training strategy\nwith progressively increasing frame numbers and reducing fine-tuning modules.\nAdditionally, we introduce the Temporal E nhanced Motion Module(TEMM) to extend\nthe capacity for encoding temporal and positional information up to 36 frames.\nThe proposed LoopAnimate, which for the first time extends the single-pass\ngeneration length of UNet-based video generation models to 35 frames while\nmaintaining high-quality video generation. Experiments demonstrate that\nLoopAnimate achieves state-of-the-art performance in both objective metrics,\nsuch as fidelity and temporal consistency, and subjective evaluation results.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09172v2",
    "published_date": "2024-04-14 07:36:18 UTC",
    "updated_date": "2024-04-16 14:56:32 UTC"
  },
  {
    "arxiv_id": "2404.09167v1",
    "title": "Survey on Embedding Models for Knowledge Graph and its Applications",
    "authors": [
      "Manita Pote"
    ],
    "abstract": "Knowledge Graph (KG) is a graph based data structure to represent facts of\nthe world where nodes represent real world entities or abstract concept and\nedges represent relation between the entities. Graph as representation for\nknowledge has several drawbacks like data sparsity, computational complexity\nand manual feature engineering. Knowledge Graph embedding tackles the drawback\nby representing entities and relation in low dimensional vector space by\ncapturing the semantic relation between them. There are different KG embedding\nmodels. Here, we discuss translation based and neural network based embedding\nmodels which differ based on semantic property, scoring function and\narchitecture they use. Further, we discuss application of KG in some domains\nthat use deep learning models and leverage social media data.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09167v1",
    "published_date": "2024-04-14 07:15:59 UTC",
    "updated_date": "2024-04-14 07:15:59 UTC"
  },
  {
    "arxiv_id": "2404.09163v1",
    "title": "GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning",
    "authors": [
      "Amani Namboori",
      "Shivam Mangale",
      "Andy Rosenbaum",
      "Saleh Soltan"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) with capabilities like\nIn-Context Learning (ICL) has ushered in new possibilities for data generation\nacross various domains while minimizing the need for extensive data collection\nand modeling techniques. Researchers have explored ways to use this generated\nsynthetic data to optimize smaller student models for reduced deployment costs\nand lower latency in downstream tasks. However, ICL-generated data often\nsuffers from low quality as the task specificity is limited with few examples\nused in ICL. In this paper, we propose GeMQuAD - a semi-supervised learning\napproach, extending the WeakDAP framework, applied to a dataset generated\nthrough ICL with just one example in the target language using AlexaTM 20B\nSeq2Seq LLM. Through our approach, we iteratively identify high-quality data to\nenhance model performance, especially for low-resource multilingual setting in\nthe context of Extractive Question Answering task. Our framework outperforms\nthe machine translation-augmented model by 0.22/1.68 F1/EM (Exact Match) points\nfor Hindi and 0.82/1.37 F1/EM points for Spanish on the MLQA dataset, and it\nsurpasses the performance of model trained on an English-only dataset by\n5.05/6.50 F1/EM points for Hindi and 3.81/3.69 points F1/EM for Spanish on the\nsame dataset. Notably, our approach uses a pre-trained LLM for generation with\nno fine-tuning (FT), utilizing just a single annotated example in ICL to\ngenerate data, providing a cost-effective development process.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to The 37th International Conference on Neural Information\n  Processing Systems (NeurIPS 2023)December 10-16, 2023 - SyntheticData4ML\n  workshop, New Orleans, United States https://neurips.cc/Conferences/2023",
    "pdf_url": "http://arxiv.org/pdf/2404.09163v1",
    "published_date": "2024-04-14 06:55:42 UTC",
    "updated_date": "2024-04-14 06:55:42 UTC"
  },
  {
    "arxiv_id": "2404.09158v2",
    "title": "StreakNet-Arch: An Anti-scattering Network-based Architecture for Underwater Carrier LiDAR-Radar Imaging",
    "authors": [
      "Xuelong Li",
      "Hongjun An",
      "Guangying Li",
      "Xing Wang",
      "Guanghua Cheng",
      "Zhe Sun"
    ],
    "abstract": "In this paper, we introduce StreakNet-Arch, a novel signal processing\narchitecture designed for Underwater Carrier LiDAR-Radar (UCLR) imaging\nsystems, to address the limitations in scatter suppression and real-time\nimaging. StreakNet-Arch formulates the signal processing as a real-time,\nend-to-end binary classification task, enabling real-time image acquisition. To\nachieve this, we leverage Self-Attention networks and propose a novel Double\nBranch Cross Attention (DBC-Attention) mechanism that surpasses the performance\nof traditional methods. Furthermore, we present a method for embedding\nstreak-tube camera images into attention networks, effectively acting as a\nlearned bandpass filter. To facilitate further research, we contribute a\npublicly available streak-tube camera image dataset. The dataset contains\n2,695,168 real-world underwater 3D point cloud data. These advancements\nsignificantly improve UCLR capabilities, enhancing its performance and\napplicability in underwater imaging tasks. The source code and dataset can be\nfound at https://github.com/BestAnHongjun/StreakNet .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Reduce the number of pages to 13",
    "pdf_url": "http://arxiv.org/pdf/2404.09158v2",
    "published_date": "2024-04-14 06:19:46 UTC",
    "updated_date": "2024-04-23 11:45:29 UTC"
  },
  {
    "arxiv_id": "2404.09155v2",
    "title": "Mitigating Heterogeneity among Factor Tensors via Lie Group Manifolds for Tensor Decomposition Based Temporal Knowledge Graph Embedding",
    "authors": [
      "Jiang Li",
      "Xiangdong Su",
      "Guanglai Gao"
    ],
    "abstract": "Recent studies have highlighted the effectiveness of tensor decomposition\nmethods in the Temporal Knowledge Graphs Embedding (TKGE) task. However, we\nfound that inherent heterogeneity among factor tensors in tensor decomposition\nsignificantly hinders the tensor fusion process and further limits the\nperformance of link prediction. To overcome this limitation, we introduce a\nnovel method that maps factor tensors onto a unified smooth Lie group manifold\nto make the distribution of factor tensors approximating homogeneous in tensor\ndecomposition. We provide the theoretical proof of our motivation that\nhomogeneous tensors are more effective than heterogeneous tensors in tensor\nfusion and approximating the target for tensor decomposition based TKGE\nmethods. The proposed method can be directly integrated into existing tensor\ndecomposition based TKGE methods without introducing extra parameters.\nExtensive experiments demonstrate the effectiveness of our method in mitigating\nthe heterogeneity and in enhancing the tensor decomposition based TKGE models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09155v2",
    "published_date": "2024-04-14 06:10:46 UTC",
    "updated_date": "2025-02-19 03:11:50 UTC"
  },
  {
    "arxiv_id": "2404.09146v1",
    "title": "Fusion-Mamba for Cross-modality Object Detection",
    "authors": [
      "Wenhao Dong",
      "Haodong Zhu",
      "Shaohui Lin",
      "Xiaoyan Luo",
      "Yunhang Shen",
      "Xuhui Liu",
      "Juan Zhang",
      "Guodong Guo",
      "Baochang Zhang"
    ],
    "abstract": "Cross-modality fusing complementary information from different modalities\neffectively improves object detection performance, making it more useful and\nrobust for a wider range of applications. Existing fusion strategies combine\ndifferent types of images or merge different backbone features through\nelaborated neural network modules. However, these methods neglect that modality\ndisparities affect cross-modality fusion performance, as different modalities\nwith different camera focal lengths, placements, and angles are hardly fused.\nIn this paper, we investigate cross-modality fusion by associating cross-modal\nfeatures in a hidden state space based on an improved Mamba with a gating\nmechanism. We design a Fusion-Mamba block (FMB) to map cross-modal features\ninto a hidden state space for interaction, thereby reducing disparities between\ncross-modal features and enhancing the representation consistency of fused\nfeatures. FMB contains two modules: the State Space Channel Swapping (SSCS)\nmodule facilitates shallow feature fusion, and the Dual State Space Fusion\n(DSSF) enables deep fusion in a hidden state space. Through extensive\nexperiments on public datasets, our proposed approach outperforms the\nstate-of-the-art methods on $m$AP with 5.9% on $M^3FD$ and 4.9% on FLIR-Aligned\ndatasets, demonstrating superior object detection performance. To the best of\nour knowledge, this is the first work to explore the potential of Mamba for\ncross-modal fusion and establish a new baseline for cross-modality object\ndetection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09146v1",
    "published_date": "2024-04-14 05:28:46 UTC",
    "updated_date": "2024-04-14 05:28:46 UTC"
  },
  {
    "arxiv_id": "2404.09145v2",
    "title": "ToNER: Type-oriented Named Entity Recognition with Generative Language Model",
    "authors": [
      "Guochao Jiang",
      "Ziqin Luo",
      "Yuchen Shi",
      "Dixuan Wang",
      "Jiaqing Liang",
      "Deqing Yang"
    ],
    "abstract": "In recent years, the fine-tuned generative models have been proven more\npowerful than the previous tagging-based or span-based models on named entity\nrecognition (NER) task. It has also been found that the information related to\nentities, such as entity types, can prompt a model to achieve NER better.\nHowever, it is not easy to determine the entity types indeed existing in the\ngiven sentence in advance, and inputting too many potential entity types would\ndistract the model inevitably. To exploit entity types' merit on promoting NER\ntask, in this paper we propose a novel NER framework, namely ToNER based on a\ngenerative model. In ToNER, a type matching model is proposed at first to\nidentify the entity types most likely to appear in the sentence. Then, we\nappend a multiple binary classification task to fine-tune the generative\nmodel's encoder, so as to generate the refined representation of the input\nsentence. Moreover, we add an auxiliary task for the model to discover the\nentity types which further fine-tunes the model to output more accurate\nresults. Our extensive experiments on some NER benchmarks verify the\neffectiveness of our proposed strategies in ToNER that are oriented towards\nentity types' exploitation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.09145v2",
    "published_date": "2024-04-14 05:13:37 UTC",
    "updated_date": "2024-06-11 14:05:03 UTC"
  },
  {
    "arxiv_id": "2404.09138v1",
    "title": "From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation",
    "authors": [
      "Artur Kiulian",
      "Anton Polishko",
      "Mykola Khandoga",
      "Oryna Chubych",
      "Jack Connor",
      "Raghav Ravishankar",
      "Adarsh Shirawalmath"
    ],
    "abstract": "In the rapidly advancing field of AI and NLP, generative large language\nmodels (LLMs) stand at the forefront of innovation, showcasing unparalleled\nabilities in text understanding and generation. However, the limited\nrepresentation of low-resource languages like Ukrainian poses a notable\nchallenge, restricting the reach and relevance of this technology. Our paper\naddresses this by fine-tuning the open-source Gemma and Mistral LLMs with\nUkrainian datasets, aiming to improve their linguistic proficiency and\nbenchmarking them against other existing models capable of processing Ukrainian\nlanguage. This endeavor not only aims to mitigate language bias in technology\nbut also promotes inclusivity in the digital realm. Our transparent and\nreproducible approach encourages further NLP research and development.\nAdditionally, we present the Ukrainian Knowledge and Instruction Dataset (UKID)\nto aid future efforts in language model fine-tuning. Our research not only\nadvances the field of NLP but also highlights the importance of linguistic\ndiversity in AI, which is crucial for cultural preservation, education, and\nexpanding AI's global utility. Ultimately, we advocate for a future where\ntechnology is inclusive, enabling AI to communicate effectively across all\nlanguages, especially those currently underrepresented.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09138v1",
    "published_date": "2024-04-14 04:25:41 UTC",
    "updated_date": "2024-04-14 04:25:41 UTC"
  },
  {
    "arxiv_id": "2404.09136v1",
    "title": "TLDR at SemEval-2024 Task 2: T5-generated clinical-Language summaries for DeBERTa Report Analysis",
    "authors": [
      "Spandan Das",
      "Vinay Samuel",
      "Shahriar Noroozizadeh"
    ],
    "abstract": "This paper introduces novel methodologies for the Natural Language Inference\nfor Clinical Trials (NLI4CT) task. We present TLDR (T5-generated\nclinical-Language summaries for DeBERTa Report Analysis) which incorporates\nT5-model generated premise summaries for improved entailment and contradiction\nanalysis in clinical NLI tasks. This approach overcomes the challenges posed by\nsmall context windows and lengthy premises, leading to a substantial\nimprovement in Macro F1 scores: a 0.184 increase over truncated premises. Our\ncomprehensive experimental evaluation, including detailed error analysis and\nablations, confirms the superiority of TLDR in achieving consistency and\nfaithfulness in predictions against semantically altered inputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09136v1",
    "published_date": "2024-04-14 04:14:30 UTC",
    "updated_date": "2024-04-14 04:14:30 UTC"
  },
  {
    "arxiv_id": "2404.09123v1",
    "title": "Provable Interactive Learning with Hindsight Instruction Feedback",
    "authors": [
      "Dipendra Misra",
      "Aldo Pacchiano",
      "Robert E. Schapire"
    ],
    "abstract": "We study interactive learning in a setting where the agent has to generate a\nresponse (e.g., an action or trajectory) given a context and an instruction. In\ncontrast, to typical approaches that train the system using reward or expert\nsupervision on response, we study learning with hindsight instruction where a\nteacher provides an instruction that is most suitable for the agent's generated\nresponse. This hindsight labeling of instruction is often easier to provide\nthan providing expert supervision of the optimal response which may require\nexpert knowledge or can be impractical to elicit. We initiate the theoretical\nanalysis of interactive learning with hindsight labeling. We first provide a\nlower bound showing that in general, the regret of any algorithm must scale\nwith the size of the agent's response space. We then study a specialized\nsetting where the underlying instruction-response distribution can be\ndecomposed as a low-rank matrix. We introduce an algorithm called LORIL for\nthis setting and show that its regret scales as $\\sqrt{T}$ where $T$ is the\nnumber of rounds and depends on the intrinsic rank but does not depend on the\nsize of the agent's response space. We provide experiments in two domains\nshowing that LORIL outperforms baselines even when the low-rank assumption is\nviolated.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09123v1",
    "published_date": "2024-04-14 02:18:07 UTC",
    "updated_date": "2024-04-14 02:18:07 UTC"
  },
  {
    "arxiv_id": "2404.09114v1",
    "title": "Intelligent Chemical Purification Technique Based on Machine Learning",
    "authors": [
      "Wenchao Wu",
      "Hao Xu",
      "Dongxiao Zhang",
      "Fanyang Mo"
    ],
    "abstract": "We present an innovative of artificial intelligence with column\nchromatography, aiming to resolve inefficiencies and standardize data\ncollection in chemical separation and purification domain. By developing an\nautomated platform for precise data acquisition and employing advanced machine\nlearning algorithms, we constructed predictive models to forecast key\nseparation parameters, thereby enhancing the efficiency and quality of\nchromatographic processes. The application of transfer learning allows the\nmodel to adapt across various column specifications, broadening its utility. A\nnovel metric, separation probability ($S_p$), quantifies the likelihood of\neffective compound separation, validated through experimental verification.\nThis study signifies a significant step forward int the application of AI in\nchemical research, offering a scalable solution to traditional chromatography\nchallenges and providing a foundation for future technological advancements in\nchemical analysis and purification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 5 Figures, Submitted to Nature Machine Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2404.09114v1",
    "published_date": "2024-04-14 01:44:58 UTC",
    "updated_date": "2024-04-14 01:44:58 UTC"
  },
  {
    "arxiv_id": "2406.11868v1",
    "title": "Ethical Framework for Responsible Foundational Models in Medical Imaging",
    "authors": [
      "Abhijit Das",
      "Debesh Jha",
      "Jasmer Sanjotra",
      "Onkar Susladkar",
      "Suramyaa Sarkar",
      "Ashish Rauniyar",
      "Nikhil Tomar",
      "Vanshali Sharma",
      "Ulas Bagci"
    ],
    "abstract": "Foundational models (FMs) have tremendous potential to revolutionize medical\nimaging. However, their deployment in real-world clinical settings demands\nextensive ethical considerations. This paper aims to highlight the ethical\nconcerns related to FMs and propose a framework to guide their responsible\ndevelopment and implementation within medicine. We meticulously examine ethical\nissues such as privacy of patient data, bias mitigation, algorithmic\ntransparency, explainability and accountability. The proposed framework is\ndesigned to prioritize patient welfare, mitigate potential risks, and foster\ntrust in AI-assisted healthcare.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11868v1",
    "published_date": "2024-04-14 01:18:03 UTC",
    "updated_date": "2024-04-14 01:18:03 UTC"
  },
  {
    "arxiv_id": "2404.09110v1",
    "title": "ProSAS: An O-RAN Approach to Spectrum Sharing between NR and LTE",
    "authors": [
      "Sneihil Gopal",
      "David Griffith",
      "Richard A. Rouil",
      "Chunmei Liu"
    ],
    "abstract": "The Open Radio Access Network (O-RAN), an industry-driven initiative,\nutilizes intelligent Radio Access Network (RAN) controllers and open interfaces\nto facilitate efficient spectrum sharing between LTE and NR RANs. In this\npaper, we introduce the Proactive Spectrum Adaptation Scheme (ProSAS), a\ndata-driven, O-RAN-compatible spectrum sharing solution. ProSAS is an\nintelligent radio resource demand prediction and management scheme for\nintent-driven spectrum management that minimizes surplus or deficit experienced\nby both RANs. We illustrate the effectiveness of this solution using real-world\nLTE resource usage data and synthetically generated NR data. Lastly, we discuss\na high-level O-RAN-compatible architecture of the proposed solution.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted for publication at IEEE International Conference on\n  Communications (ICC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.09110v1",
    "published_date": "2024-04-14 01:02:19 UTC",
    "updated_date": "2024-04-14 01:02:19 UTC"
  },
  {
    "arxiv_id": "2404.09105v2",
    "title": "EGGS: Edge Guided Gaussian Splatting for Radiance Fields",
    "authors": [
      "Yuanhao Gong"
    ],
    "abstract": "The Gaussian splatting methods are getting popular. However, their loss\nfunction only contains the $\\ell_1$ norm and the structural similarity between\nthe rendered and input images, without considering the edges in these images.\nIt is well-known that the edges in an image provide important information.\nTherefore, in this paper, we propose an Edge Guided Gaussian Splatting (EGGS)\nmethod that leverages the edges in the input images. More specifically, we give\nthe edge region a higher weight than the flat region. With such edge guidance,\nthe resulting Gaussian particles focus more on the edges instead of the flat\nregions. Moreover, such edge guidance does not crease the computation cost\nduring the training and rendering stage. The experiments confirm that such\nsimple edge-weighted loss function indeed improves about $1\\sim2$ dB on several\ndifference data sets. With simply plugging in the edge guidance, the proposed\nmethod can improve all Gaussian splatting methods in different scenarios, such\nas human head modeling, building 3D reconstruction, etc.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.09105v2",
    "published_date": "2024-04-14 00:08:56 UTC",
    "updated_date": "2024-04-22 08:40:43 UTC"
  }
]
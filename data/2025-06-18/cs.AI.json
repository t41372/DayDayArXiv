{
  "date": "2025-06-18",
  "category": "cs.AI",
  "summary": "ä½ å¥½ï¼æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-06-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å……æ»¡äº†å…³äº **Test-time Computeï¼ˆæ¨ç†æ—¶è®¡ç®—ï¼‰** å’Œ **Reasoningï¼ˆæ¨ç†ï¼‰** çš„æ·±åº¦æ¢è®¨ï¼Œä»æ§åˆ¶æ¨ç†æ·±åº¦ã€è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›åˆ°æ¨ç†è¿‡ç¨‹ä¸­çš„éšç§æ³„éœ²ï¼Œå­¦ç•Œæ­£åœ¨ç–¯ç‹‚æŒ–æ˜ LLM \"æ€è€ƒ\" çš„æœ¬è´¨ï¼›åŒæ—¶ï¼ŒTencent å‘å¸ƒçš„ Hunyuan3D 2.1 å’Œ Zisserman å›¢é˜Ÿçš„è§†é¢‘è®¡æ•°å·¥ä½œä¹Ÿä¸ºå¤šæ¨¡æ€é¢†åŸŸå¸¦æ¥äº†å¼ºåŠ›æ›´æ–°ã€‚\n\n---\n\n### ğŸš€ æ·±åº¦æ¨ç†ä¸æ¨¡å‹æœºç† (Reasoning & Interpretability)\n\n**6. åˆ†æ•°æ¨ç†ï¼šé€šè¿‡æ½œåœ¨å¼•å¯¼å‘é‡æ§åˆ¶æ¨ç†æ—¶çš„è®¡ç®—é‡**\n**Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute**\n> *Sheng Liu, James Zou et al.*\nè¿™æ˜¯ä¸€ç¯‡å…³äº Test-time compute çš„é‡ç£…æ–‡ç« ã€‚ç°æœ‰çš„ CoT æˆ– Majority Voting å¾€å¾€å¯¹æ‰€æœ‰é—®é¢˜ä¸€è§†åŒä»ã€‚ä½œè€…æå‡ºäº† **Fractional Reasoning**ï¼Œä¸€ç§æ— éœ€è®­ç»ƒã€ä¸æ¨¡å‹æ— å…³çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡æå–ä¸æ·±åº¦æ¨ç†ç›¸å…³çš„ **Latent Steering Vectorï¼ˆæ½œåœ¨å¼•å¯¼å‘é‡ï¼‰**ï¼Œå¹¶åœ¨æ¨ç†æ—¶é€šè¿‡å¯è°ƒçš„ç¼©æ”¾å› å­é‡æ–°åº”ç”¨å®ƒï¼Œä»è€Œè¿ç»­æ§åˆ¶æ¨¡å‹çš„æ¨ç†å¼ºåº¦ã€‚è¿™è®©æ¨¡å‹èƒ½æ ¹æ®é—®é¢˜éš¾æ˜“ç¨‹åº¦â€œå®šåˆ¶â€æ€è€ƒæ·±åº¦ï¼Œåœ¨ GSM8K å’Œ MATH500 ä¸Šæ•ˆæœæ˜¾è‘—ã€‚\n\n**5. è¯­è¨€æ¨¡å‹å¯ä»¥å¯¹å—æ‰°åŠ¨çš„æ¨ç†è¿›è¡Œå•æ¬¡è‡ªæˆ‘ä¿®æ­£**\n**Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning**\n> *Sam Silver, Sara Hooker, et al.*\nå…³äº LLM æ˜¯å¦èƒ½ Self-Correction ä¸€ç›´æœ‰äº‰è®®ã€‚è¿™ç¯‡è®ºæ–‡å‘ç°ï¼Œå³ä½¿æ²¡æœ‰é’ˆå¯¹é•¿ CoT è¿›è¡Œå¾®è°ƒï¼ŒLLM ä¹Ÿè¡¨ç°å‡ºäº†å¼ºå¤§çš„ **Intrinsic Self-Correctionï¼ˆå†…åœ¨è‡ªæˆ‘ä¿®æ­£ï¼‰** èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡åˆæˆæ‰°åŠ¨ CoT å®éªŒå‘ç°ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å•æ¬¡è¯è¯­ä¸­è¿›è¡Œä»éšå¼ä¿®æ­£åˆ°æ˜¾å¼æ‰¿è®¤é”™è¯¯çš„å„ç§ä¿®æ­£ã€‚è¿™æš—ç¤ºç°ä»£æ¨¡å‹çš„â€œæ¨ç†â€èƒ½åŠ›å¯èƒ½ä¸»è¦æ¥è‡ªå¯¹å·²å­˜åœ¨ç‰¹å¾çš„æ”¾å¤§ã€‚\n\n**25. å¯†é›† SAE æ½œåœ¨å˜é‡æ˜¯ç‰¹æ€§ï¼Œè€Œé Bug**\n**Dense SAE Latents Are Features, Not Bugs**\n> *Xiaoqing Sun, Max Tegmark (æ³°æ ¼é©¬å…‹å¤§ä½¬çš„æ–‡ç« )*\n**Sparse Autoencoders (SAEs)** æ˜¯ç›®å‰è§£é‡Š LLM çš„ä¸»æµæ–¹æ³•ã€‚é€šå¸¸è®¤ä¸º SAE çš„ Latent åº”è¯¥æ˜¯ç¨€ç–çš„ï¼Œå¯†é›†çš„ï¼ˆDenseï¼‰è¢«è§†ä¸ºå¤±è´¥ã€‚ä½† Tegmark å›¢é˜Ÿå‘ç°ï¼Œè¿™äº› Dense Latents å®é™…ä¸Šç¼–ç äº†é‡è¦çš„åŠŸèƒ½ï¼Œå¦‚ä½ç½®è·Ÿè¸ªã€ä¸Šä¸‹æ–‡ç»‘å®šå’Œè¯æ€§ç­‰ã€‚å®ƒä»¬åœ¨æ®‹å·®æµä¸­å¾€å¾€æˆå¯¹å‡ºç°ä»¥é‡æ„ç‰¹å®šæ–¹å‘ã€‚ç»“è®ºï¼šä¸è¦è½»æ˜“ä¸¢å¼ƒå¯†é›†ç‰¹å¾ï¼Œå®ƒä»¬æ˜¯æ¨¡å‹è®¡ç®—çš„åŸºçŸ³ã€‚\n\n**110. æˆªæ–­ PPOï¼šæå‡æ¨ç†å‹ LLM çš„è®­ç»ƒæ•ˆç‡**\n**Truncated Proximal Policy Optimization**\n> *Tiantian Fan et al.*\né’ˆå¯¹ Test-time scaling æ¨¡å‹ï¼ˆäº§ç”Ÿé•¿ CoT çš„æ¨¡å‹ï¼‰ï¼Œä¼ ç»Ÿçš„ PPO æ•ˆç‡å¤ªä½ã€‚ä½œè€…æå‡ºäº† **Truncated PPO (T-PPO)**ï¼Œé€šè¿‡æµçº¿å‹çš„ç­–ç•¥æ›´æ–°å’Œé•¿åº¦å—é™çš„å“åº”ç”Ÿæˆæ¥æå‡æ•ˆç‡ã€‚ç‰¹åˆ«æ˜¯ **Extended GAE** å…è®¸ä»ä¸å®Œæ•´çš„å“åº”ä¸­ä¼°è®¡ä¼˜åŠ¿ï¼Œè§£å†³äº†é•¿åºåˆ—ç”Ÿæˆæ—¶çš„ç¡¬ä»¶é—²ç½®é—®é¢˜ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€éšç§ä¸ Agent (Safety, Privacy & Agents)\n\n**28. æ³„æ¼çš„æ€ç»´ï¼šå¤§å‹æ¨ç†æ¨¡å‹ä¸æ˜¯ç§å¯†çš„æ€è€ƒè€…**\n**Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers**\n> *Tommaso Green, Seong Joon Oh et al.*\n**è­¦é’Ÿé•¿é¸£ï¼** æˆ‘ä»¬é€šå¸¸è®¤ä¸ºæ¨¡å‹çš„ CoTï¼ˆæ€ç»´é“¾ï¼‰æ˜¯å†…éƒ¨çš„ã€å®‰å…¨çš„ã€‚ä½†è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œæ¨ç†è½¨è¿¹ï¼ˆReasoning Tracesï¼‰ç»å¸¸åŒ…å«æ•æ„Ÿçš„ç”¨æˆ·æ•°æ®ã€‚æ›´å¯æ€•çš„æ˜¯ï¼Œ**Test-time compute** çš„å¢åŠ ï¼ˆæ€è€ƒè¶Šå¤šï¼‰åè€ŒåŠ å‰§äº†éšç§æ³„éœ²ï¼Œå› ä¸ºæ¨¡å‹å˜å¾—æ›´åŠ â€œå•°å—¦â€ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒçŸ›ç›¾ï¼šæ¨ç†æå‡äº†æ•ˆç”¨ï¼Œå´æ‰©å¤§äº†éšç§æ”»å‡»é¢ã€‚\n\n**57. ä¸Šä¸‹æ–‡æ“çºµæ”»å‡»ï¼šWeb Agent å®¹æ˜“å—åˆ°å†…å­˜æ±¡æŸ“çš„å½±å“**\n**Context manipulation attacks : Web agents are susceptible to corrupted memory**\n> *Atharv Singh Patlan, Prateek Mittal et al.*\nWeb Agents ä¾èµ–å¤–éƒ¨è®°å¿†æ¥ç»´æŠ¤ä¸Šä¸‹æ–‡ã€‚ä½œè€…æå‡ºäº† **Plan Injectionï¼ˆè®¡åˆ’æ³¨å…¥ï¼‰** æ”»å‡»ï¼Œé’ˆå¯¹ Agent çš„ä¸Šä¸‹æ–‡ç®¡ç†è¿›è¡Œæ”»å‡»ã€‚æ¯”èµ·ä¼ ç»Ÿçš„ Prompt Injectionï¼Œè¿™ç§æ”»å‡»åœ¨ Browser-use å’Œ Agent-E ç­‰æµè¡Œ Agent ä¸Šçš„æˆåŠŸç‡é«˜å‡º 3å€ï¼Œç”šè‡³èƒ½é€šè¿‡é€»è¾‘æ¡¥æ¥å®ç°éšç§çªƒå–ã€‚\n\n**15. MEM1ï¼šå­¦ä¹ ååŒè®°å¿†ä¸æ¨ç†çš„é«˜æ•ˆé•¿è§†ç¨‹ Agent**\n**MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents**\n> *Zijian Zhou, Paul Pu Liang et al.*\né’ˆå¯¹é•¿è§†ç¨‹ä»»åŠ¡ï¼ˆLong-horizon tasksï¼‰ï¼Œç®€å•çš„å…¨ä¸Šä¸‹æ–‡ Prompting ä¼šå¯¼è‡´æ˜¾å­˜çˆ†ç‚¸ã€‚**MEM1** æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„ RL æ¡†æ¶ï¼Œå®ƒç»´æŠ¤ä¸€ä¸ªç´§å‡‘çš„å…±äº«å†…éƒ¨çŠ¶æ€ï¼Œæ¯ä¸€è½®åŠ¨æ€æ›´æ–°ï¼Œæ—¢åšè®°å¿†æ•´åˆåˆåšæ¨ç†ã€‚åœ¨å¤šè·³ QA ä»»åŠ¡ä¸Šï¼Œæ¯” Qwen2.5-14B æå‡äº† 3.5å€æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº† 3.7å€å†…å­˜ã€‚\n\n---\n\n### ğŸ¨ å¤šæ¨¡æ€ã€3D ä¸è§†è§‰ (Multimodal, 3D & Vision)\n\n**65. Hunyuan3D 2.1ï¼šä»å›¾åƒåˆ°ç”Ÿäº§çº§ PBR æè´¨çš„é«˜ä¿çœŸ 3D èµ„äº§**\n**Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material**\n> *Team Hunyuan3D (è…¾è®¯æ··å…ƒå›¢é˜Ÿ)*\nè…¾è®¯å‘å¸ƒäº† Hunyuan3D 2.1ã€‚è¿™æ˜¯ä¸€ä¸ªåŒ…å« **Shape Generation (Hunyuan3D-DiT)** å’Œ **Texture Synthesis (Hunyuan3D-Paint)** çš„åŒç»„ä»¶ç³»ç»Ÿã€‚æœ€å¤§çš„å–ç‚¹æ˜¯â€œç”Ÿäº§çº§ PBR æè´¨â€å’Œå…¨æµç¨‹å¼€æºï¼ˆä»£ç å’Œæ¨¡å‹ï¼‰ï¼Œè¿™å¯¹äºæ¸¸æˆå’Œ VR å¼€å‘è€…æ¥è¯´æ˜¯ä¸ªå¤§æ–°é—»ã€‚\n\n**74. è§†é¢‘ä¸­çš„å¼€æ”¾ä¸–ç•Œç‰©ä½“è®¡æ•°**\n**Open-World Object Counting in Videos**\n> *Niki Amini-Naieni, Andrew Zisserman (è®¡ç®—æœºè§†è§‰æ³°æ–—)*\nZisserman å›¢é˜Ÿçš„æ–°ä½œã€‚æå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼šç»™å®šæ–‡æœ¬æè¿°æˆ–å›¾åƒç¤ºä¾‹ï¼Œç»Ÿè®¡è§†é¢‘ä¸­æ‰€æœ‰å”¯ä¸€ç›®æ ‡å®ä¾‹çš„æ•°é‡ã€‚è¿™æ¯”å•å¸§è®¡æ•°éš¾å¾—å¤šï¼Œå› ä¸ºæ¶‰åŠé®æŒ¡å’Œé‡è¯†åˆ«ã€‚ä»–ä»¬æå‡ºäº† **CountVid** æ¨¡å‹å’Œä¸€ä¸ªæ–°æ•°æ®é›† **VideoCount**ï¼ˆåŒ…å«ä¼é¹…å’Œé‡‘å±ç»“æ™¶è§†é¢‘ï¼‰ã€‚\n\n**27. Sekaiï¼šé¢å‘ä¸–ç•Œæ¢ç´¢çš„è§†é¢‘æ•°æ®é›†**\n**Sekai: A Video Dataset towards World Exploration**\n> *Zhen Li, Yu Qiao et al.*\n\"Sekai\" æ—¥è¯­æ„ä¸ºâ€œä¸–ç•Œâ€ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„ç¬¬ä¸€äººç§°/æ— äººæœºè§†è§’è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å« 5000+ å°æ—¶ï¼Œè¦†ç›– 100+ å›½å®¶ã€‚è¿™ä¸ªæ•°æ®é›†æ˜ç¡®æ˜¯ä¸ºäº†è®­ç»ƒ **World Exploration** ç±»çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ ‡æ³¨äº†å¤©æ°”ã€äººç¾¤å¯†åº¦å’Œç›¸æœºè½¨è¿¹ï¼Œæ˜¯è¿ˆå‘ World Simulators çš„é‡è¦ä¸€æ­¥ã€‚\n\n---\n\n### ğŸ§  å…¶ä»–å€¼å¾—å…³æ³¨çš„æ¢ç´¢ (Exploratory & Scientific AI)\n\n**102. ç”¨äº ARC-AGI çš„ç¥ç»å…ƒèƒè‡ªåŠ¨æœº**\n**Neural Cellular Automata for ARC-AGI**\n> *Kevin Xu, Risto Miikkulainen*\n**ARC-AGI** æ˜¯ç›®å‰æœ€éš¾çš„æŠ½è±¡æ¨ç†åŸºå‡†ä¹‹ä¸€ã€‚è¿™ç¯‡è®ºæ–‡æ²¡æœ‰ç”¨ LLMï¼Œè€Œæ˜¯å›å½’äº† **Neural Cellular Automata (NCA)**ã€‚ä½œè€…å‘ç°é€šè¿‡æ¢¯åº¦è®­ç»ƒçš„ NCA åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å˜æ¢å’Œå°‘æ ·æœ¬æ³›åŒ–çš„ç½‘æ ¼ä»»åŠ¡ä¸Šéå¸¸æœ‰æ½œåŠ›ï¼Œä¸ºè§£å†³ ARC æä¾›äº†ä¸€ç§é LLM çš„æ€è·¯ã€‚\n\n**1. è”é‚¦å­¦ä¹ ä¸­ç”¨äºå¤šæ ·åŒ–å®¢æˆ·ç«¯é€‰æ‹©çš„å¹‚èŒƒæ•°ä½™å¼¦ç›¸ä¼¼åº¦**\n**PNCS: Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning**\n> *Liangyan Li, Jun Chen et al.*\nåœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œå¦‚ä½•é€‰æ‹©å®¢æˆ·ç«¯ä¸€ç›´æ˜¯ä¸ªéš¾é¢˜ã€‚æœ¬æ–‡æå‡ºçš„ **PNCS** åˆ©ç”¨é«˜é˜¶æ¢¯åº¦çŸ©æ¥æ•æ‰è¿œç¨‹å®¢æˆ·ç«¯ä¹‹é—´å¤æ‚çš„æ¢¯åº¦ç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆNon-IIDï¼‰çš„åœºæ™¯ä¸‹ï¼Œèƒ½æ˜¾è‘—æå‡æ”¶æ•›é€Ÿåº¦ã€‚\n\n**9. è·¨æ¨¡æ€å­¦ä¹ ï¼šä» H&E æŸ“è‰²å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹ IHC ç”Ÿç‰©æ ‡å¿—ç‰©**\n**Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images**\n> *Amit Das, Saeed Hassanpour et al.*\nç—…ç†å­¦é‡ç£…åº”ç”¨ã€‚IHC æŸ“è‰²åˆè´µåˆæ…¢ã€‚**HistoStainAlign** æ¡†æ¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œç›´æ¥ä»å»‰ä»·çš„ H&E å›¾åƒé¢„æµ‹ P53ã€PD-L1 ç­‰ IHC æŸ“è‰²æ¨¡å¼ï¼Œæ— éœ€ç»„ç»‡é…å‡†ã€‚è¿™å¯èƒ½æˆä¸ºç™Œç—‡è¯Šæ–­çš„é‡è¦é¢„ç­›é€‰å·¥å…·ã€‚\n\n**14. æˆ‘çŸ¥é“å»å¹´å¤å¤©å“ªä¸ª LLM å†™äº†ä½ çš„ä»£ç ï¼šç”¨äºä½œè€…å½’å±çš„ LLM ç”Ÿæˆä»£ç é£æ ¼è®¡é‡**\n**I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution**\n> *Tamas Bisztray et al.*\næŒºæœ‰æ„æ€çš„å®‰å…¨/å–è¯ç ”ç©¶ã€‚ä½œè€…å‘å¸ƒäº† **CodeT5-Authorship**ï¼Œåªéœ€è¦çœ‹ encoder çš„è¾“å‡ºï¼Œå°±èƒ½ä»¥ 97.56% çš„å‡†ç¡®ç‡åˆ†è¾¨å‡ºä¸€æ®µ C ä»£ç æ˜¯ GPT-4o å†™çš„è¿˜æ˜¯ Claude 3.5 å†™çš„ã€‚\n\n---\nğŸ‰ **ç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæˆ‘ä»¬æ˜å¤©è§ï¼**",
  "papers": [
    {
      "arxiv_id": "2506.15923v1",
      "title": "PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning",
      "title_zh": "PNCSï¼šé¢å‘è”é‚¦å­¦ä¹ å¤šæ ·åŒ–å®¢æˆ·ç«¯é€‰æ‹©çš„å¹‚èŒƒæ•°ä½™å¼¦ç›¸ä¼¼åº¦",
      "authors": [
        "Liangyan Li",
        "Yangyi Liu",
        "Yimo Ning",
        "Stefano Rini",
        "Jun Chen"
      ],
      "abstract": "Federated Learning (FL) has emerged as a powerful paradigm for leveraging diverse datasets from multiple sources while preserving data privacy by avoiding centralized storage. However, many existing approaches fail to account for the intricate gradient correlations between remote clients, a limitation that becomes especially problematic in data heterogeneity scenarios. In this work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity (PNCS) to improve client selection for model aggregation. By capturing higher-order gradient moments, PNCS addresses non-IID data challenges, enhancing convergence speed and accuracy. Additionally, we introduce a simple algorithm ensuring diverse client selection through a selection history queue. Experiments with a VGG16 model across varied data partitions demonstrate consistent improvements over state-of-the-art methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è”é‚¦å­¦ä¹  (Federated Learning) åœ¨æ•°æ®å¼‚æ„æ€§ (data heterogeneity) åœºæ™¯ä¸‹éš¾ä»¥æœ‰æ•ˆæ•è·å®¢æˆ·ç«¯æ¢¯åº¦ç›¸å…³æ€§çš„å±€é™ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¹‚èŒƒæ•°ä½™å¼¦ç›¸ä¼¼åº¦ (Power-Norm Cosine Similarity, PNCS) çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•æ‰é«˜é˜¶æ¢¯åº¦çŸ© (higher-order gradient moments) æ¥åº”å¯¹éç‹¬ç«‹åŒåˆ†å¸ƒ (non-IID) æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§ç»“åˆé€‰æ‹©å†å²é˜Ÿåˆ— (selection history queue) çš„ç®€æ˜“ç®—æ³•ï¼Œæ—¨åœ¨ç¡®ä¿å‚ä¸èšåˆçš„å®¢æˆ·ç«¯å…·æœ‰å……åˆ†çš„å¤šæ ·æ€§ã€‚åœ¨ VGG16 æ¨¡å‹åŠå¤šç§æ•°æ®åˆ†åŒºç¯å¢ƒä¸‹çš„å®éªŒè¡¨æ˜ï¼ŒPNCS ç›¸æ¯”äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå…·æœ‰æŒç»­çš„ç«äº‰ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15923v1",
      "published_date": "2025-06-18 23:49:48 UTC",
      "updated_date": "2025-06-18 23:49:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:23.690762+00:00"
    },
    {
      "arxiv_id": "2507.00037v1",
      "title": "Model Fusion via Neuron Interpolation",
      "title_zh": "åŸºäºç¥ç»å…ƒæ’å€¼çš„æ¨¡å‹èåˆ",
      "authors": [
        "Phoomraphee Luenam",
        "Andreas Spanopoulos",
        "Amit Sant",
        "Thomas Hofmann",
        "Sotiris Anagnostidis",
        "Sidak Pal Singh"
      ],
      "abstract": "Model fusion aims to combine the knowledge of multiple models by creating one representative model that captures the strengths of all of its parents. However, this process is non-trivial due to differences in internal representations, which can stem from permutation invariance, random initialization, or differently distributed training data. We present a novel, neuron-centric family of model fusion algorithms designed to integrate multiple trained neural networks into a single network effectively regardless of training data distribution. Our algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Unlike prior approaches, our approach incorporates neuron attribution scores into the fusion process. Furthermore, our algorithms can generalize to arbitrary layer types. Experimental results on various benchmark datasets demonstrate that our algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios. The code is available at https://github.com/AndrewSpano/neuron-interpolation-model-fusion.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡å‹èåˆï¼ˆModel fusionï¼‰ä¸­å› å†…éƒ¨è¡¨ç¤ºå·®å¼‚ã€æ’åˆ—ä¸å˜æ€§ï¼ˆpermutation invarianceï¼‰ä»¥åŠè®­ç»ƒæ•°æ®åˆ†å¸ƒä¸å‡å¯¼è‡´çš„é›†æˆéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„ä»¥ç¥ç»å…ƒä¸ºä¸­å¿ƒçš„èåˆç®—æ³•å®¶æ—ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†çˆ¶æ¨¡å‹çš„ä¸­é—´å±‚ç¥ç»å…ƒè¿›è¡Œåˆ†ç»„æ¥æ„å»ºç›®æ ‡è¡¨ç¤ºï¼Œå¹¶ä½¿èåˆæ¨¡å‹é€šè¿‡å…¶å¯¹åº”çš„å­ç½‘ç»œè¿›è¡Œæœ‰æ•ˆé€¼è¿‘ã€‚ä¸åŒäºä»¥å¾€çš„æ–¹æ¡ˆï¼Œè¯¥ç®—æ³•åœ¨èåˆè¿‡ç¨‹ä¸­å¼•å…¥äº†ç¥ç»å…ƒå½’å› åˆ†æ•°ï¼ˆneuron attribution scoresï¼‰ï¼Œä¸”èƒ½å¤Ÿçµæ´»æ¨å¹¿è‡³ä»»æ„ç¥ç»å…ƒå±‚ç±»å‹ã€‚åœ¨å¤šç§åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰å’Œéç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰èåˆåœºæ™¯ä¸‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„èåˆæŠ€æœ¯ã€‚è¯¥é¡¹å·¥ä½œä¸ºæ•´åˆå¤šä¸ªè®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæä¾›äº†é«˜æ•ˆä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 figures, 15 tables, 23 pages",
      "pdf_url": "https://arxiv.org/pdf/2507.00037v1",
      "published_date": "2025-06-18 23:31:05 UTC",
      "updated_date": "2025-06-18 23:31:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:32.990097+00:00"
    },
    {
      "arxiv_id": "2506.17325v1",
      "title": "RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences",
      "title_zh": "RadarSeqï¼šåŸºäºé›·è¾¾å›¾åºåˆ—çš„ç”¨æˆ·æµå¤±é¢„æµ‹æ—¶åºè§†è§‰æ¡†æ¶",
      "authors": [
        "Sina Najafi",
        "M. Hadi Sepanj",
        "Fahimeh Jafari"
      ],
      "abstract": "Predicting user churn in non-subscription gig platforms, where disengagement is implicit, poses unique challenges due to the absence of explicit labels and the dynamic nature of user behavior. Existing methods often rely on aggregated snapshots or static visual representations, which obscure temporal cues critical for early detection. In this work, we propose a temporally-aware computer vision framework that models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features. By integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior. Extensive experiments on a large real-world dataset demonstrate that our method outperforms classical models and ViT-based radar chart baselines, yielding gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with improved interpretability. The framework's modular design, explainability tools, and efficient deployment characteristics make it suitable for large-scale churn modeling in dynamic gig-economy platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RadarSeqï¼Œä¸€ç§é’ˆå¯¹éè®¢é˜…å¼é›¶å·¥å¹³å°ç”¨æˆ·æµå¤±é¢„æµ‹çš„æ—¶é—´è§†è§‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³éšå¼æµå¤±èƒŒæ™¯ä¸‹ç¼ºä¹æ˜¾å¼æ ‡ç­¾å’Œè¡Œä¸ºåŠ¨æ€æ€§çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å»ºæ¨¡ä¸ºä¸€ç³»åˆ— Radar Chart å›¾åƒåºåˆ—ï¼Œæ¯ä¸ªå›¾åƒç¼–ç äº†å¤©çº§åˆ«çš„è¡Œä¸ºç‰¹å¾ï¼Œä»¥æ•æ‰å…³é”®çš„æ—¶é—´çº¿ç´¢ã€‚é€šè¿‡æ•´åˆé¢„è®­ç»ƒçš„ CNN ç¼–ç å™¨ä¸ Bidirectional LSTM (BiLSTM) æ¶æ„ï¼ŒRadarSeq èƒ½å¤Ÿæœ‰æ•ˆæå–æµå¤±è¡Œä¸ºèƒŒåçš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚åœ¨å¤§å‹çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ F1 åˆ†æ•°ã€Precision å’Œ AUC æ–¹é¢åˆ†åˆ«æå‡äº† 17.7ã€29.4 å’Œ 16.1ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹å’ŒåŸºäº ViT çš„ Radar Chart åŸºå‡†ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å…·å¤‡æ¨¡å—åŒ–è®¾è®¡ã€å¯è§£é‡Šæ€§å·¥å…·å’Œé«˜æ•ˆéƒ¨ç½²ç‰¹æ€§ï¼Œä¸ºåŠ¨æ€é›¶å·¥ç»æµå¹³å°çš„å¤§è§„æ¨¡æµå¤±å»ºæ¨¡æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17325v1",
      "published_date": "2025-06-18 22:20:49 UTC",
      "updated_date": "2025-06-18 22:20:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:39.238241+00:00"
    },
    {
      "arxiv_id": "2506.15896v1",
      "title": "KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction",
      "title_zh": "KG-FGNNï¼šé¢å‘æ–½è‚¥åœŸå£¤æ¸©å®¤æ°”ä½“é€šé‡é¢„æµ‹çš„çŸ¥è¯†å¼•å¯¼å‹ GNN åŸºç¡€æ¨¡å‹",
      "authors": [
        "Yu Zhang",
        "Gaoshan Bi",
        "Simon Jeffery",
        "Max Davis",
        "Yang Li",
        "Qing Xue",
        "Po Yang"
      ],
      "abstract": "Precision soil greenhouse gas (GHG) flux prediction is essential in agricultural systems for assessing environmental impacts, developing emission mitigation strategies and promoting sustainable agriculture. Due to the lack of advanced sensor and network technologies on majority of farms, there are challenges in obtaining comprehensive and diverse agricultural data. As a result, the scarcity of agricultural data seriously obstructs the application of machine learning approaches in precision soil GHG flux prediction. This research proposes a knowledge-guided graph neural network framework that addresses the above challenges by integrating knowledge embedded in an agricultural process-based model and graph neural network techniques. Specifically, we utilise the agricultural process-based model to simulate and generate multi-dimensional agricultural datasets for 47 countries that cover a wide range of agricultural variables. To extract key agricultural features and integrate correlations among agricultural features in the prediction process, we propose a machine learning framework that integrates the autoencoder and multi-target multi-graph based graph neural networks, which utilises the autoencoder to selectively extract significant agricultural features from the agricultural process-based model simulation data and the graph neural network to integrate correlations among agricultural features for accurately predict fertilisation-oriented soil GHG fluxes. Comprehensive experiments were conducted with both the agricultural simulation dataset and real-world agricultural dataset to evaluate the proposed approach in comparison with well-known baseline and state-of-the-art regression methods. The results demonstrate that our proposed approach provides superior accuracy and stability in fertilisation-oriented soil GHG prediction.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å†œä¸šé¢†åŸŸä¸­ç”±äºä¼ æ„Ÿå™¨å’Œç½‘ç»œæŠ€æœ¯åŒ®ä¹å¯¼è‡´çš„åœŸå£¤æ¸©å®¤æ°”ä½“(GHG)é€šé‡æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæå‡ºäº†KG-FGNNï¼Œä¸€ç§çŸ¥è¯†å¼•å¯¼çš„å›¾ç¥ç»ç½‘ç»œåŸºç¡€æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆåŸºäºè¿‡ç¨‹çš„å†œä¸šæ¨¡å‹(Agricultural process-based model)çš„ä¸“ä¸šçŸ¥è¯†ä¸å›¾ç¥ç»ç½‘ç»œ(GNN)æŠ€æœ¯ï¼Œæ—¨åœ¨å®ç°ç²¾ç¡®çš„é¢å‘æ–½è‚¥çš„åœŸå£¤æ¸©å®¤æ°”ä½“é€šé‡é¢„æµ‹ã€‚å…·ä½“è€Œè¨€ï¼Œç ”ç©¶åˆ©ç”¨è¿‡ç¨‹æ¨¡å‹æ¨¡æ‹Ÿç”Ÿæˆäº†è¦†ç›–47ä¸ªå›½å®¶çš„å¤šç»´å†œä¸šæ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨è‡ªåŠ¨ç¼–ç å™¨(Autoencoder)å’Œå¤šç›®æ ‡å¤šå›¾ç¥ç»ç½‘ç»œ(Multi-target multi-graph based GNN)æ¥æå–å…³é”®ç‰¹å¾å¹¶æ•è·å†œä¸šå˜é‡é—´çš„å¤æ‚ç›¸å…³æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒKG-FGNNåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„åŸºå‡†æ¨¡å‹å’Œæœ€å…ˆè¿›(SOTA)çš„å›å½’æ–¹æ³•ï¼Œå±•ç°äº†å“è¶Šçš„é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚è¯¥ç ”ç©¶ä¸ºå¯æŒç»­å†œä¸šä¸­è¯„ä¼°ç¯å¢ƒå½±å“å’Œåˆ¶å®šå‡æ’ç­–ç•¥æä¾›äº†æœ‰æ•ˆçš„æ•°æ®é©±åŠ¨æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15896v1",
      "published_date": "2025-06-18 21:40:24 UTC",
      "updated_date": "2025-06-18 21:40:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:35.394018+00:00"
    },
    {
      "arxiv_id": "2506.15894v1",
      "title": "Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning",
      "title_zh": "è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°å—æ‰°æ¨ç†çš„å•æ¬¡è¯è¯­è‡ªæˆ‘ä¿®æ­£",
      "authors": [
        "Sam Silver",
        "Jimin Sun",
        "Ivan Zhang",
        "Sara Hooker",
        "Eddie Kim"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive mathematical reasoning capabilities, yet their performance remains brittle to minor variations in problem description and prompting strategy. Furthermore, reasoning is vulnerable to sampling-induced errors which autoregressive models must primarily address using self-correction via additionally-generated tokens. To better understand self-correction capabilities of recent models, we conduct experiments measuring models' ability to self-correct synthetic perturbations introduced into their Chain of Thought (CoT) reasoning. We observe robust single-utterance intrinsic self-correction behavior across a range of open-weight models and datasets, ranging from subtle, implicit corrections to explicit acknowledgments and corrections of errors. Our findings suggest that LLMs, including those not finetuned for long CoT, may possess stronger intrinsic self-correction capabilities than commonly shown in the literature. The presence of this ability suggests that recent \"reasoning\" model work involves amplification of traits already meaningfully present in models.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é“¾å¼æ€ç»´(Chain of Thought)æ¨ç†è¿‡ç¨‹ä¸­å¤„ç†å¹²æ‰°æ—¶çš„å•è½®è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼Œæ—¨åœ¨è§£å†³æ¨¡å‹å¯¹é—®é¢˜æè¿°å˜ä½“å’Œé‡‡æ ·é”™è¯¯æ•æ„Ÿçš„é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨æ¨ç†è·¯å¾„ä¸­å¼•å…¥åˆæˆå¹²æ‰°(synthetic perturbations)ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†å¤šç§å¼€æºæƒé‡æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ç­‰ä»»åŠ¡ä¸­çš„çº é”™è¡¨ç°ã€‚å®éªŒè§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹æ™®éå±•ç°å‡ºå¼ºå¥çš„å†…åœ¨è‡ªæˆ‘ä¿®æ­£(intrinsic self-correction)è¡Œä¸ºï¼Œèƒ½å¤Ÿé€šè¿‡å•è½®è¾“å‡ºå®ç°ä»éšå¼é€»è¾‘è½¬å‘åˆ°æ˜¾å¼é”™è¯¯æ‰¿è®¤ä¸çº æ­£çš„è·¨è¶Šã€‚ç ”ç©¶å‘ç°ï¼Œå³ä¾¿åœ¨æœªç»é•¿CoTä¸“é—¨å¾®è°ƒçš„æ¨¡å‹ä¸­ï¼Œä¹Ÿå­˜åœ¨ç€æ¯”ä»¥å¾€æ–‡çŒ®æŠ¥é“æ›´å¼ºçš„å†…åœ¨çº é”™æ½œèƒ½ã€‚è¿™ä¸€å‘ç°æš—ç¤ºäº†è¿‘æœŸä¸»æµâ€œæ¨ç†æ¨¡å‹â€çš„æŠ€æœ¯çªç ´ï¼Œæœ¬è´¨ä¸Šæ˜¯å¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹ä¸­å·²ç„¶å­˜åœ¨çš„å…³é”®ç‰¹è´¨è¿›è¡Œäº†æœ‰æ•ˆæ”¾å¤§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15894v1",
      "published_date": "2025-06-18 21:35:44 UTC",
      "updated_date": "2025-06-18 21:35:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:40.243897+00:00"
    },
    {
      "arxiv_id": "2506.15882v2",
      "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute",
      "title_zh": "åŸºäºæ½œè½¬å‘å‘é‡çš„åˆ†æ•°æ¨ç†ï¼šæå‡æ¨ç†æ—¶è®¡ç®—",
      "authors": [
        "Sheng Liu",
        "Tianlang Chen",
        "Pan Lu",
        "Haotian Ye",
        "Yizheng Chen",
        "Lei Xing",
        "James Zou"
      ],
      "abstract": "Test-time compute has emerged as a powerful paradigm for improving the performance of large language models (LLMs), where generating multiple outputs or refining individual chains can significantly boost answer accuracy. However, existing methods like Best-of-N, majority voting, and self-reflection typically apply reasoning in a uniform way across inputs, overlooking the fact that different problems may require different levels of reasoning depth. In this work, we propose Fractional Reasoning, a training-free and model-agnostic framework that enables continuous control over reasoning intensity at inference time, going beyond the limitations of fixed instructional prompts. Our method operates by extracting the latent steering vector associated with deeper reasoning and reapplying it with a tunable scaling factor, allowing the model to tailor its reasoning process to the complexity of each input. This supports two key modes of test-time scaling: (1) improving output quality in breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing the correctness of individual reasoning chains in depth-based strategies (e.g., self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that Fractional Reasoning consistently improves performance across diverse reasoning tasks and models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Fractional Reasoningï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒä¸”æ¨¡å‹æ— å…³çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨ç†æ—¶çš„è¿ç»­æ§åˆ¶æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†å¼ºåº¦ã€‚é’ˆå¯¹ç°æœ‰æµ‹è¯•æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰æ–¹æ³•ï¼ˆå¦‚ Best-of-Nã€majority voting å’Œ self-reflectionï¼‰é€šå¸¸ä»¥ç»Ÿä¸€æ–¹å¼åº”ç”¨æ¨ç†è€Œå¿½ç•¥é—®é¢˜éš¾åº¦å·®å¼‚çš„å±€é™æ€§ï¼Œè¯¥æ–¹æ³•é€šè¿‡æå–ä¸æ·±å±‚æ¨ç†ç›¸å…³çš„ latent steering vectorï¼Œå¹¶ç»“åˆå¯è°ƒç¼©æ”¾å› å­å°†å…¶é‡æ–°åº”ç”¨äºæ¨ç†è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹æ ¹æ®è¾“å…¥å¤æ‚åº¦é‡èº«å®šåˆ¶æ¨ç†æ·±åº¦ï¼Œä»è€ŒåŒæ—¶æå‡å®½åº¦ä¼˜å…ˆç­–ç•¥çš„è¾“å‡ºè´¨é‡å’Œæ·±åº¦ä¼˜å…ˆç­–ç•¥çš„å•é“¾æ­£ç¡®æ€§ã€‚åœ¨ GSM8Kã€MATH500 å’Œ GPQA ç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFractional Reasoning åœ¨å¤šç§æ¨ç†ä»»åŠ¡å’Œæ¨¡å‹ä¸­å‡èƒ½æŒç»­æå‡æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 5 figures, Project website: https://shengliu66.github.io/fractreason/",
      "pdf_url": "https://arxiv.org/pdf/2506.15882v2",
      "published_date": "2025-06-18 21:15:59 UTC",
      "updated_date": "2025-09-25 09:27:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:45.502286+00:00"
    },
    {
      "arxiv_id": "2506.15880v1",
      "title": "Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search",
      "title_zh": "åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ è±¡æ£‹åšå¼ˆç¨‹åº",
      "authors": [
        "Berk Yilmaz",
        "Junyu Hu",
        "Jinsong Liu"
      ],
      "abstract": "This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸­å›½è±¡æ£‹çš„ Deep Reinforcement Learning (DRL) ç³»ç»Ÿï¼Œé€šè¿‡å°†ç¥ç»ç½‘ç»œä¸ Monte Carlo Tree Search (MCTS) ç›¸ç»“åˆï¼Œå®ç°äº†æˆ˜ç•¥æ€§çš„è‡ªæˆ‘åšå¼ˆä¸è‡ªæˆ‘è¿›åŒ–ã€‚è¯¥ç³»ç»Ÿé’ˆå¯¹ä¸­å›½è±¡æ£‹ç‰¹æœ‰çš„æ£‹ç›˜å¸ƒå±€ã€æ£‹å­ç§»åŠ¨é™åˆ¶å’Œèƒœåˆ©æ¡ä»¶ï¼Œåˆ©ç”¨ Policy-Value Networks é…åˆ MCTS æ¥æ¨¡æ‹Ÿèµ°æ³•åæœå¹¶ç²¾ç‚¼å†³ç­–ã€‚é€šè¿‡æœ‰æ•ˆå…‹æœé«˜åˆ†æ”¯å› å­å’Œéå¯¹ç§°æ£‹å­åŠ¨æ€ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•å±•ç¤ºäº† DRL-MCTS æ¡†æ¶åœ¨ç‰¹å®šé¢†åŸŸè§„åˆ™ç³»ç»Ÿä¸‹çš„å¼ºå¤§é€‚åº”èƒ½åŠ›ã€‚æ­¤é¡¹å·¥ä½œä¸ä»…æå‡äº†äººå·¥æ™ºèƒ½åœ¨å…·æœ‰æ–‡åŒ–æ„ä¹‰çš„ç­–ç•¥æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œä¹Ÿä¸ºå¤æ‚åšå¼ˆç³»ç»Ÿçš„ AI å»ºæ¨¡æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "All authors contributed equally to this work.24 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15880v1",
      "published_date": "2025-06-18 21:11:17 UTC",
      "updated_date": "2025-06-18 21:11:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:54.422140+00:00"
    },
    {
      "arxiv_id": "2506.15862v1",
      "title": "MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers",
      "title_zh": "MoRï¼šèåˆç¨€ç–ã€ç¨ å¯†ä¸äººå·¥æ£€ç´¢å™¨ä»¥æ›´ä¼˜åœ°å¤„ç†å¤šæ ·åŒ–æŸ¥è¯¢",
      "authors": [
        "Jushaan Singh Kalra",
        "Xinran Zhao",
        "To Eun Kim",
        "Fengyu Cai",
        "Fernando Diaz",
        "Tongshuang Wu"
      ],
      "abstract": "Retrieval-augmented Generation (RAG) is powerful, but its effectiveness hinges on which retrievers we use and how. Different retrievers offer distinct, often complementary signals: BM25 captures lexical matches; dense retrievers, semantic similarity. Yet in practice, we typically fix a single retriever based on heuristics, which fails to generalize across diverse information needs. Can we dynamically select and integrate multiple retrievers for each individual query, without the need for manual selection? In our work, we validate this intuition with quantitative analysis and introduce mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers. Extensive experiments show that such mixtures are effective and efficient: Despite totaling just 0.8B parameters, this mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Further analysis also shows that this mixture framework can help incorporate specialized non-oracle human information sources as retrievers to achieve good collaboration, with a 58.9% relative performance improvement over simulated humans alone.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç³»ç»Ÿä¸­å•ä¸€æ£€ç´¢å™¨éš¾ä»¥å…¼é¡¾è¯æ±‡ä¸è¯­ä¹‰åŒ¹é…çš„é—®é¢˜ï¼Œæå‡ºäº† MoR (Mixture of Retrievers) æ¡†æ¶ã€‚MoR æ˜¯ä¸€ç§é›¶æ ·æœ¬ (Zero-shot) çš„å¼‚æ„æ£€ç´¢å™¨åŠ æƒç»„åˆæ–¹æ³•ï¼Œèƒ½å¤Ÿä¸ºæ¯ä¸ªç‰¹å®šæŸ¥è¯¢åŠ¨æ€æ•´åˆ BM25 ç­‰ç¨€ç–æ£€ç´¢å™¨ä¸ Dense Retrievers çš„äº’è¡¥ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…æ‹¥æœ‰ 0.8B å‚æ•°çš„ MoR æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæ‰€æœ‰ç‹¬ç«‹æ£€ç´¢å™¨ï¼Œç”šè‡³æ¯” 7B è§„æ¨¡çš„å¤§æ¨¡å‹å¹³å‡å‡†ç¡®ç‡é«˜å‡º 3.9%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½æœ‰æ•ˆååŒéä¸“å®¶çš„å¤–éƒ¨äººå·¥ä¿¡æ¯æºä½œä¸ºæ£€ç´¢å™¨ï¼Œç›¸æ¯”å•çº¯çš„æ¨¡æ‹Ÿäººå·¥æ£€ç´¢å®ç°äº† 58.9% çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŠ¨æ€é›†æˆå¼‚æ„æ£€ç´¢å™¨åœ¨æå‡ RAG ç³»ç»Ÿæ³›åŒ–èƒ½åŠ›ã€å‡†ç¡®æ€§ä¸æ•ˆç‡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "19 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15862v1",
      "published_date": "2025-06-18 20:15:23 UTC",
      "updated_date": "2025-06-18 20:15:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:38:51.394705+00:00"
    },
    {
      "arxiv_id": "2506.15853v1",
      "title": "Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images",
      "title_zh": "åŸºäº H&E æŸ“è‰²å…¨åˆ‡ç‰‡å›¾åƒé¢„æµ‹ IHC ç”Ÿç‰©æ ‡å¿—ç‰©çš„è·¨æ¨¡æ€å­¦ä¹ ",
      "authors": [
        "Amit Das",
        "Naofumi Tomita",
        "Kyle J. Syme",
        "Weijie Ma",
        "Paige O'Connor",
        "Kristin N. Corbett",
        "Bing Ren",
        "Xiaoying Liu",
        "Saeed Hassanpour"
      ],
      "abstract": "Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological analysis, offering reliable visualization of cellular morphology and tissue architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry (IHC) staining provides molecular insights by detecting specific proteins within tissues, enhancing diagnostic accuracy, and improving treatment planning. However, IHC staining is costly, time-consuming, and resource-intensive, requiring specialized expertise. To address these limitations, this study proposes HistoStainAlign, a novel deep learning framework that predicts IHC staining patterns directly from H&E whole-slide images (WSIs) by learning joint representations of morphological and molecular features. The framework integrates paired H&E and IHC embeddings through a contrastive training strategy, capturing complementary features across staining modalities without patch-level annotations or tissue registration. The model was evaluated on gastrointestinal and lung tissue WSIs with three commonly used IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI: 0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC stains. Embedding analyses demonstrated the robustness of the contrastive alignment in capturing meaningful cross-stain relationships. Comparisons with a baseline model further highlight the advantage of incorporating contrastive learning for improved stain pattern prediction. This study demonstrates the potential of computational approaches to serve as a pre-screening tool, helping prioritize cases for IHC staining and improving workflow efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HistoStainAlignï¼Œä¸€ç§æ—¨åœ¨ç›´æ¥ä»è‹æœ¨ç²¾-ä¼Šçº¢ (H&E) æŸ“è‰²å…¨åˆ‡ç‰‡å›¾åƒ (WSIs) ä¸­é¢„æµ‹å…ç–«ç»„åŒ– (IHC) ç”Ÿç‰©æ ‡å¿—ç‰©çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ç­–ç•¥ (contrastive training strategy) æ•´åˆé…å¯¹çš„ H&E å’Œ IHC åµŒå…¥å‘é‡ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•è·å½¢æ€å­¦ä¸åˆ†å­ç‰¹å¾çš„è”åˆè¡¨ç¤ºã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæ— éœ€å—çº§æ ‡æ³¨ (patch-level annotations) æˆ–å¤æ‚çš„ç»„ç»‡é…å‡† (tissue registration)ï¼Œå³å¯æå–è·¨æ¨¡æ€æŸ“è‰²çš„äº’è¡¥ç‰¹å¾ã€‚ç ”ç©¶åœ¨èƒƒè‚ é“å’Œè‚ºéƒ¨ç»„ç»‡åˆ‡ç‰‡ä¸Šé’ˆå¯¹ P53ã€PD-L1 å’Œ Ki-67 ä¸‰ç§å…³é”® IHC æ ‡å¿—ç‰©è¿›è¡Œäº†éªŒè¯ï¼Œåˆ†åˆ«å–å¾—äº† 0.735ã€0.830 å’Œ 0.723 çš„åŠ æƒ F1 åˆ†æ•°ã€‚åµŒå…¥åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†å¯¹æ¯”å¯¹é½ç­–ç•¥åœ¨æ•æ‰è·¨æŸ“è‰²å…³ç³»æ–¹é¢çš„é²æ£’æ€§ï¼Œä¸”æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†è®¡ç®—ç—…ç†å­¦ä½œä¸ºé¢„ç­›æŸ¥å·¥å…·çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿä¼˜å…ˆå¤„ç†ç‰¹å®šç—…ä¾‹ï¼Œä»è€Œæ˜¾è‘—æå‡ç—…ç†è¯Šæ–­çš„å·¥ä½œæµæ•ˆç‡å¹¶é™ä½åŒ»ç–—æˆæœ¬ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15853v1",
      "published_date": "2025-06-18 20:01:14 UTC",
      "updated_date": "2025-06-18 20:01:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:02.408782+00:00"
    },
    {
      "arxiv_id": "2506.15850v2",
      "title": "Uncertainty Estimation by Human Perception versus Neural Models",
      "title_zh": "äººç±»æ„ŸçŸ¥ä¸ç¥ç»æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡å¯¹æ¯”",
      "authors": [
        "Pedro Mendes",
        "Paolo Romano",
        "David Garlan"
      ],
      "abstract": "Modern neural networks (NNs) often achieve high predictive accuracy but are poorly calibrated, producing overconfident predictions even when wrong. This miscalibration poses serious challenges in applications where reliable uncertainty estimates are critical. In this work, we investigate how human perceptual uncertainty compares to uncertainty estimated by NNs. Using three vision benchmarks annotated with both human disagreement and crowdsourced confidence, we assess the correlation between model-predicted uncertainty and human-perceived uncertainty. Our results show that current methods only weakly align with human intuition, with correlations varying significantly across tasks and uncertainty metrics. Notably, we find that incorporating human-derived soft labels into the training process can improve calibration without compromising accuracy. These findings reveal a persistent gap between model and human uncertainty and highlight the potential of leveraging human insights to guide the development of more trustworthy AI systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å¯¹æ¯”äº†äººç±»æ„ŸçŸ¥çš„ä¸ç¡®å®šæ€§ä¸ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰ä¼°è®¡çš„ä¸ç¡®å®šæ€§ï¼Œæ—¨åœ¨è§£å†³ç°ä»£æ¨¡å‹åœ¨é¢„æµ‹ä¸­è¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡å’Œæ ¡å‡†ï¼ˆCalibrationï¼‰ä¸ä½³çš„é—®é¢˜ã€‚ä½œè€…åˆ©ç”¨ä¸‰ä¸ªå¸¦æœ‰æ ‡æ³¨çš„äººç±»æ„è§åˆ†æ­§å’Œä¼—åŒ…ç½®ä¿¡åº¦çš„è§†è§‰åŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°äº†æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§æŒ‡æ ‡ï¼ˆUncertainty Metricsï¼‰ä¸äººç±»æ„ŸçŸ¥ä¸ç¡®å®šæ€§ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹çš„æ–¹æ³•ä¸äººç±»ç›´è§‰çš„å¥‘åˆåº¦è¾ƒä½ï¼Œä¸”ç›¸å…³æ€§åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æºè‡ªäººç±»çš„è½¯æ ‡ç­¾ï¼ˆSoft Labelsï¼‰å¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹çš„æ ¡å‡†æ•ˆæœï¼Œä¸”ä¸ä¼šç‰ºç‰²é¢„æµ‹å‡†ç¡®æ€§ã€‚è¯¥å‘ç°æ­ç¤ºäº†æ¨¡å‹ä¸äººç±»åœ¨ä¸ç¡®å®šæ€§è®¤çŸ¥ä¸Šå­˜åœ¨çš„æŒç»­å·®è·ï¼Œå¹¶å¼ºè°ƒäº†åˆ©ç”¨äººç±»è§è§£æ¥å¼€å‘æ›´å€¼å¾—ä¿¡èµ–çš„ AI ç³»ç»Ÿçš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15850v2",
      "published_date": "2025-06-18 20:00:20 UTC",
      "updated_date": "2025-09-10 18:24:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:16.893387+00:00"
    },
    {
      "arxiv_id": "2506.21597v1",
      "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering",
      "title_zh": "ClinIQLink 2025 åŒ»å­¦é—®ç­”å…±äº«ä»»åŠ¡ç»¼è¿°",
      "authors": [
        "Brandon Colelough",
        "Davis Bartels",
        "Dina Demner-Fushman"
      ],
      "abstract": "In this paper, we present an overview of ClinIQLink, a shared task, collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test large language models (LLMs) on medically-oriented question answering aimed at the level of a General Practitioner. The challenge supplies 4,978 expert-verified, medical source-grounded question-answer pairs that cover seven formats: true/false, multiple choice, unordered list, short answer, short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled in Docker or Apptainer images, are executed on the CodaBench platform or the University of Maryland's Zaratan cluster. An automated harness (Task 1) scores closed-ended items by exact match and open-ended items with a three-tier embedding metric. A subsequent physician panel (Task 2) audits the top model responses.",
      "tldr_zh": "è¯¥è®ºæ–‡æ¦‚è¿°äº†ä¸ ACL 2025 ç¬¬ 24 å±Š BioNLP ç ”è®¨ä¼šå…±åŒä¸¾åŠçš„ ClinIQLink 2025 è¯„æµ‹ä»»åŠ¡ï¼Œæ—¨åœ¨å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å…¨ç§‘åŒ»ç”Ÿæ°´å¹³ä¸Šçš„åŒ»å­¦é—®ç­” (medical question-answering) èƒ½åŠ›è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚è¯¥æŒ‘æˆ˜æä¾›äº† 4,978 ä¸ªç»è¿‡ä¸“å®¶éªŒè¯ä¸”å…·æœ‰åŒ»å­¦æ¥æºä¾æ®çš„é—®é¢˜-å›ç­”å¯¹ï¼Œæ¶µç›–äº† true/falseã€multiple choiceã€multi-hop ä»¥åŠ short answer ç­‰ä¸ƒç§æ•°æ®æ ¼å¼ã€‚å‚ä¸ç³»ç»Ÿé€šè¿‡ Docker æˆ– Apptainer é•œåƒéƒ¨ç½²ï¼Œå¹¶åœ¨ CodaBench å¹³å°æˆ– Zaratan é›†ç¾¤ä¸Šæ‰§è¡Œã€‚è¯„ä¼°æµç¨‹åŒ…æ‹¬ Task 1 çš„è‡ªåŠ¨åŒ–è¯„åˆ†ï¼Œé‡‡ç”¨ç²¾ç¡®åŒ¹é…å’Œä¸‰å±‚åµŒå…¥æŒ‡æ ‡ (embedding metric) è¡¡é‡é—­å£åŠå¼€å£é—®é¢˜ï¼›éšååœ¨ Task 2 ä¸­ï¼Œç”±åŒ»å¸ˆä¸“å®¶ç»„ (physician panel) å¯¹è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹å“åº”è¿›è¡Œäººå·¥å®¡è®¡ã€‚è¯¥ä»»åŠ¡é€šè¿‡ç»“åˆå¤šç»´åº¦è‡ªåŠ¨åŒ–åº¦é‡ä¸ä¸´åºŠä¸“å®¶è¯„å®¡ï¼Œä¸ºåŒ»ç–—é¢†åŸŸ AI ç³»ç»Ÿçš„å‡†ç¡®æ€§ä¸å¯é æ€§æä¾›äº†ä¸¥æ ¼çš„è¯„ä¼°åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.21597v1",
      "published_date": "2025-06-18 19:56:32 UTC",
      "updated_date": "2025-06-18 19:56:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:39.994220+00:00"
    },
    {
      "arxiv_id": "2506.15847v1",
      "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation",
      "title_zh": "SafeMimicï¼šé¢å‘ç§»åŠ¨æ“ä½œçš„å®‰å…¨è‡ªä¸»äººåˆ°æœºå™¨äººæ¨¡ä»¿",
      "authors": [
        "Arpit Bahety",
        "Arnav Balaji",
        "Ben Abbatematteo",
        "Roberto MartÃ­n-MartÃ­n"
      ],
      "abstract": "For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot's own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety Q-functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific actions that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SafeMimic æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨æ“ä½œï¼ˆMobile Manipulationï¼‰æœºå™¨äººä»…é€šè¿‡è§‚çœ‹å•æ®µäººç±»æ¼”ç¤ºè§†é¢‘å³å¯å®‰å…¨ä¸”è‡ªä¸»åœ°å­¦ä¹ æ–°ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆå°†è§†é¢‘è§£æä¸ºä¸åŒç‰‡æ®µï¼Œæ¨æ–­å…¶è¯­ä¹‰å˜åŒ–ä¸åŠ¨ä½œé€»è¾‘ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºæœºå™¨äººçš„ç¬¬ä¸€äººç§°è§†è§’ï¼ˆEgocentric Perspectiveï¼‰ã€‚ä¸ºäº†é€‚åº”æœºå™¨äººçš„ç‰©ç†å½¢æ€ï¼ˆMorphologyï¼‰ï¼ŒSafeMimic åœ¨äººç±»åŠ¨ä½œåŸºç¡€ä¸Šé‡‡æ ·å€™é€‰åŠ¨ä½œï¼Œå¹¶åˆ©ç”¨åœ¨æ¨¡æ‹Ÿä¸­è®­ç»ƒçš„å®‰å…¨æ€§ Q å‡½æ•°ï¼ˆSafety Q-functionsï¼‰é›†åˆè¿›è¡Œæ»šåŠ¨æ—¶åŸŸï¼ˆReceding Horizonï¼‰å®‰å…¨éªŒè¯ã€‚å½“æ— æ³•å®‰å…¨æ¨è¿›ä»»åŠ¡æ—¶ï¼Œæ¡†æ¶èƒ½å¤Ÿå›æº¯ï¼ˆBacktrackï¼‰å¹¶å°è¯•ä¸åŒçš„åŠ¨ä½œåºåˆ—æˆ–æŠ“å–æ¨¡å¼ï¼ˆGrasping Modesï¼‰ä»¥å®Œæˆç›®æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSafeMimic åœ¨ 7 é¡¹è·¨ç¯å¢ƒã€è·¨ç”¨æˆ·çš„ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æœºå™¨äººè‡ªä¸»å­¦ä¹ å¤šæ­¥ç§»åŠ¨æ“ä½œè¡Œä¸ºçš„å®‰å…¨æ€§ä¸æ•ˆç‡ï¼Œç›¸è¾ƒäºç°æœ‰å…ˆè¿›åŸºå‡†æ¨¡å‹å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15847v1",
      "published_date": "2025-06-18 19:55:10 UTC",
      "updated_date": "2025-06-18 19:55:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:48.193565+00:00"
    },
    {
      "arxiv_id": "2506.15846v1",
      "title": "Finance Language Model Evaluation (FLaME)",
      "title_zh": "FLaMEï¼šé‡‘èè¯­è¨€æ¨¡å‹è¯„ä¼°",
      "authors": [
        "Glenn Matlin",
        "Mika Okamoto",
        "Huzaifa Pardawala",
        "Yang Yang",
        "Sudheer Chava"
      ],
      "abstract": "Language Models (LMs) have demonstrated impressive capabilities with core Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly specialized knowledge-intensive tasks in finance remains difficult to assess due to major gaps in the methodologies of existing evaluation frameworks, which have caused an erroneous belief in a far lower bound of LMs' performance on common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for these FinNLP tasks, we present the first holistic benchmarking suite for Financial Language Model Evaluation (FLaME). We are the first research paper to comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source our framework software along with all data and results.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Finance Language Model Evaluation (FLaME)ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹é‡‘èè¯­è¨€æ¨¡å‹è¯„ä¼°çš„å…¨é¢åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚è¯¥é¡¹ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ¡†æ¶åœ¨æ–¹æ³•è®ºä¸Šçš„ç¼ºé™·ï¼Œè¿™äº›ç¼ºé™·æ›¾å¯¼è‡´ç ”ç©¶è€…å¯¹ Language Models (LMs) åœ¨å¸¸è§é‡‘èè‡ªç„¶è¯­è¨€å¤„ç† (FinNLP) ä»»åŠ¡ä¸Šçš„è¡¨ç°äº§ç”Ÿé”™è¯¯ä¸”åä½çš„è¯„ä¼°ã€‚é€šè¿‡å¯¹23ä¸ªåŸºç¡€ LMs åœ¨20ä¸ªæ ¸å¿ƒé‡‘èä»»åŠ¡ä¸Šçš„å®è¯åˆ†æï¼Œè¯¥å·¥ä½œé¦–æ¬¡æ·±å…¥ç ”ç©¶äº†åŸºç¡€æ¨¡å‹ä¸æ¨ç†å¢å¼ºå‹ (reasoning-reinforced) LMs ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶ç»“æœè¯æ˜äº† LMs åœ¨å¤„ç†é«˜åº¦ä¸“ä¸šåŒ–ä¸”çŸ¥è¯†å¯†é›†å‹çš„é‡‘èä»»åŠ¡æ—¶å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼Œä½œè€…å¼€æºäº† FLaME æ¡†æ¶è½¯ä»¶ä»¥åŠæ‰€æœ‰ç›¸å…³çš„å®éªŒæ•°æ®ä¸ç»“æœï¼Œä¸ºé‡‘èé¢†åŸŸè¯­è¨€æ¨¡å‹çš„æ ‡å‡†åŒ–è¯„ä¼°æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15846v1",
      "published_date": "2025-06-18 19:54:33 UTC",
      "updated_date": "2025-06-18 19:54:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:42.488408+00:00"
    },
    {
      "arxiv_id": "2506.17323v1",
      "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution",
      "title_zh": "æˆ‘çŸ¥é“å»å¹´å¤å¤©æ˜¯å“ªä¸ª LLM å†™äº†ä½ çš„ä»£ç ï¼šé¢å‘ä½œè€…å½’å› çš„ LLM ç”Ÿæˆä»£ç é£æ ¼åˆ†æ",
      "authors": [
        "Tamas Bisztray",
        "Bilel Cherif",
        "Richard A. Dubniczky",
        "Nils Gruschka",
        "Bertalan Borsos",
        "Mohamed Amine Ferrag",
        "Attila Kovacs",
        "Vasileios Mavroeidis",
        "Norbert Tihanyi"
      ],
      "abstract": "Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. We released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯†åˆ« Large Language Models (LLMs) ç”Ÿæˆä»£ç å½’å±çš„é—®é¢˜ï¼Œå¼€å±•äº†é¦–ä¸ªé’ˆå¯¹ C programs çš„ä½œè€…å½’å±ç³»ç»Ÿæ€§ç ”ç©¶ã€‚ä½œè€…æå‡ºäº† CodeT5-Authorship æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»…åˆ©ç”¨ CodeT5 çš„ encoder å±‚å’ŒåŒå±‚ classification headï¼Œæ—¨åœ¨é€šè¿‡ä»£ç é£æ ¼åˆ†æè¯†åˆ«ç‰¹å®šçš„ç”Ÿæˆæ¨¡å‹ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œç ”ç©¶è€…æ„å»ºäº†åŒ…å« 32,000 ä¸ªå¯ç¼–è¯‘ç¨‹åºçš„ LLM-AuthorBench åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›– 8 ç§æœ€å…ˆè¿› LLMs åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­ç”Ÿæˆçš„ä»£ç ã€‚å®éªŒæ•°æ®è¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŒºåˆ† GPT-4.1 å’Œ GPT-4o æ—¶çš„å‡†ç¡®ç‡è¾¾åˆ° 97.56%ï¼Œåœ¨ 5 ç±»ä¸»æµæ¨¡å‹çš„å¤šåˆ†ç±»åœºæ™¯ä¸‹ä¹Ÿå®ç°äº† 95.40% çš„å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€æº CodeT5-Authorship æ¶æ„å’ŒåŸºå‡†æµ‹è¯•é›†ï¼Œä¸º AI ç”Ÿæˆå†…å®¹çš„å®‰å…¨ç›‘ç®¡å’Œä½œè€…è¯†åˆ«æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.17323v1",
      "published_date": "2025-06-18 19:49:41 UTC",
      "updated_date": "2025-06-18 19:49:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:51.184970+00:00"
    },
    {
      "arxiv_id": "2506.15841v2",
      "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
      "title_zh": "MEM1ï¼šé€šè¿‡ååŒè®°å¿†ä¸æ¨ç†å®ç°é«˜æ•ˆé•¿ç¨‹æ™ºèƒ½ä½“",
      "authors": [
        "Zijian Zhou",
        "Ao Qu",
        "Zhaoxuan Wu",
        "Sunghwan Kim",
        "Alok Prakash",
        "Daniela Rus",
        "Jinhua Zhao",
        "Bryan Kian Hsiang Low",
        "Paul Pu Liang"
      ],
      "abstract": "Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿æ—¶ç¨‹ï¼ˆLong-horizonï¼‰å¤šè½®äº¤äº’ä¸­å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“é¢ä¸´çš„ä¸Šä¸‹æ–‡æ— é™å¢é•¿åŠæ¨ç†æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œæå‡ºäº† MEM1 æ¡†æ¶ã€‚MEM1 æ˜¯ä¸€ç§åŸºäºç«¯åˆ°ç«¯å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨æ¯ä¸€è½®æ›´æ–°ä¸€ä¸ªç´§å‡‘çš„å…±äº«å†…éƒ¨çŠ¶æ€ï¼Œå®ç°äº†åœ¨é•¿å¤šè½®ä»»åŠ¡ä¸­ä»¥æ’å®šå†…å­˜ï¼ˆConstant memoryï¼‰è¿è¡Œã€‚è¯¥çŠ¶æ€å·§å¦™åœ°å°†å…ˆå‰çš„è®°å¿†ä¸ç¯å¢ƒçš„æ–°è§‚å¯Ÿç»“æœç›¸ç»“åˆï¼ŒåŒæ—¶ç­–ç•¥æ€§åœ°èˆå¼ƒæ— å…³æˆ–å†—ä½™çš„ä¿¡æ¯ï¼Œä»¥ååŒæ”¯æŒè®°å¿†å·©å›ºå’Œæ¨ç†è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡ç»„åˆç°æœ‰æ•°æ®é›†æ„å»ºå¤æ‚ä»»åŠ¡åºåˆ—ä»¥æ¨¡æ‹ŸçœŸå®çš„å¤šè½®ç¯å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨åŒ…æ‹¬æ£€ç´¢é—®ç­”å’Œå¤šè½®è´­ç‰©åœ¨å†…çš„ä»»åŠ¡ä¸­ï¼ŒMEM1-7B çš„æ€§èƒ½è¾¾åˆ° Qwen2.5-14B-Instruct çš„ 3.5 å€ï¼ŒåŒæ—¶å†…å­˜å ç”¨å‡å°‘äº† 3.7 å€ã€‚è¯¥æˆæœè¯æ˜äº†æ¨ç†é©±åŠ¨çš„è®°å¿†å·©å›ºï¼ˆReasoning-driven memory consolidationï¼‰æ˜¯è®­ç»ƒé•¿æ—¶ç¨‹äº¤äº’æ™ºèƒ½ä½“æ—¶ä¼˜åŒ–æ•ˆç‡ä¸æ€§èƒ½çš„å¯æ‰©å±•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15841v2",
      "published_date": "2025-06-18 19:44:46 UTC",
      "updated_date": "2025-07-17 08:53:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:03.931672+00:00"
    },
    {
      "arxiv_id": "2506.21596v2",
      "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering",
      "title_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²æ•™æé—®ç­”ä¸­çš„è¯„ä¼°",
      "authors": [
        "Hessa A. Alawwad",
        "Anas Zafar",
        "Areej Alhothali",
        "Usman Naseem",
        "Ali Alkhathlan",
        "Amani Jamal"
      ],
      "abstract": "Multimodal large language models (MLLMs) have shown success in vision-language tasks, but their ability to reason over complex educational materials remains largely untested. This work presents the first evaluation of state-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the textbook question answering (TQA) task using the CK12-QA dataset. We introduce a multimodal retrieval-augmented generation (RAG) pipeline to simulate real-world learning by providing relevant lesson paragraphs and diagrams as context. Our zero-shot experiments reveal a critical trade-off: while retrieved context improves LLaVA's performance on text-based questions, it significantly degrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based tasks, dropping its validation accuracy from 74.07% to 25.93%. We term this statistically significant phenomenon \"catastrophic context interference.\" Furthermore, fine-tuning highlights architectural differences: LLaMA 3.2-Vision's performance improves to 71.16% on the test set, demonstrating its capacity to learn multimodal integration, whereas LLaVA's performance declines, indicating challenges with generalization. Our results underscore the challenges MLLMs face in modality prioritization and context integration, providing a benchmark and pointing to key directions for developing more robust AI-driven educational tools.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨æ•™ç§‘ä¹¦é—®ç­” (TQA) ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œä¸»è¦é’ˆå¯¹ LLaVA-1.5 å’Œ LLaMA 3.2-Vision ç­‰å‰æ²¿æ¨¡å‹åœ¨ CK12-QA æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æµæ°´çº¿æ¥æ¨¡æ‹ŸçœŸå®å­¦ä¹ åœºæ™¯ï¼Œä½†é›¶æ ·æœ¬ (zero-shot) å®éªŒæ­ç¤ºäº†â€œç¾éš¾æ€§ä¸Šä¸‹æ–‡å¹²æ‰° (catastrophic context interference)â€ç°è±¡ï¼Œå³æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡è™½èƒ½æå‡æ–‡æœ¬é—®ç­”æ€§èƒ½ï¼Œå´å¯¼è‡´ LLaMA 3.2-Vision åœ¨å›¾è¡¨ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡ä» 74.07% éª¤é™è‡³ 25.93%ã€‚å¾®è°ƒ (fine-tuning) å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼ŒLLaMA 3.2-Vision è¡¨ç°å‡ºè¾ƒå¼ºçš„å¤šæ¨¡æ€æ•´åˆå­¦ä¹ èƒ½åŠ›ï¼Œè€Œ LLaVA åˆ™åœ¨æ³›åŒ–æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº† MLLMs åœ¨æ¨¡æ€ä¼˜å…ˆçº§æ’åºå’Œå¤æ‚ä¸Šä¸‹æ–‡æ•´åˆæ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæ„å»ºæ›´ç¨³å¥çš„ AI é©±åŠ¨æ•™è‚²å·¥å…·æä¾›äº†é‡è¦çš„åŸºå‡†å’Œç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "8 Pages",
      "pdf_url": "https://arxiv.org/pdf/2506.21596v2",
      "published_date": "2025-06-18 19:31:35 UTC",
      "updated_date": "2025-07-15 09:14:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:39:59.195389+00:00"
    },
    {
      "arxiv_id": "2506.15828v2",
      "title": "Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning",
      "title_zh": "Context Mattersï¼åŸºäºå¤§è¯­è¨€æ¨¡å‹ç›®æ ‡æ¾å¼›çš„å¯è¡Œ 3D åœºæ™¯è§„åˆ’",
      "authors": [
        "Emanuele Musumeci",
        "Michele Brienza",
        "Francesco Argenziano",
        "Abdel Hakim Drid",
        "Vincenzo Suriani",
        "Daniele Nardi",
        "Domenico D. Bloisi"
      ],
      "abstract": "Embodied agents need to plan and act reliably in real and complex 3D environments. Classical planning (e.g., PDDL) offers structure and guarantees, but in practice it fails under noisy perception and incorrect predicate grounding. On the other hand, Large Language Models (LLMs)-based planners leverage commonsense reasoning, yet frequently propose actions that are unfeasible or unsafe. Following recent works that combine the two approaches, we introduce ContextMatters, a framework that fuses LLMs and classical planning to perform hierarchical goal relaxation: the LLM helps ground symbols to the scene and, when the target is unreachable, it proposes functionally equivalent goals that progressively relax constraints, adapting the goal to the context of the agent's environment. Operating on 3D Scene Graphs, this mechanism turns many nominally unfeasible tasks into tractable plans and enables context-aware partial achievement when full completion is not achievable. Our experimental results show a +52.45% Success Rate improvement over state-of-the-art LLMs+PDDL baseline, demonstrating the effectiveness of our approach. Moreover, we validate the execution of ContextMatter in a real world scenario by deploying it on a TIAGo robot. Code, dataset, and supplementary materials are available to the community at https://lab-rococo-sapienza.github.io/context-matters/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ContextMatters æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å…·èº«æ™ºèƒ½ä½“ (Embodied agents) åœ¨å¤æ‚ 3D ç¯å¢ƒä¸­è¿›è¡Œä»»åŠ¡è§„åˆ’æ—¶ï¼Œç»å…¸è§„åˆ’ (Classical planning) å¯¹å™ªå£°æ„ŸçŸ¥æ•æ„Ÿä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs) è§„åˆ’ç»“æœä¸å¯è¡Œçš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡èåˆ LLMs ä¸ç»å…¸è§„åˆ’ï¼Œå¼•å…¥äº†å±‚æ¬¡åŒ–ç›®æ ‡æ¾å¼› (hierarchical goal relaxation) æœºåˆ¶ï¼Œå¹¶åŸºäº 3D Scene Graphs è¿›è¡Œæ“ä½œã€‚å½“åŸå§‹ç›®æ ‡ä¸å¯è¾¾æ—¶ï¼ŒContextMatters åˆ©ç”¨ LLM å°†ç¬¦å·é”šå®šåˆ°å®é™…åœºæ™¯ï¼Œå¹¶æå‡ºåŠŸèƒ½ç­‰ä»·çš„ç›®æ ‡æ¥é€æ­¥æ”¾å®½çº¦æŸï¼Œä»è€Œå°†ä¸å¯è¡Œä»»åŠ¡è½¬åŒ–ä¸ºå¯æ‰§è¡Œè®¡åˆ’ã€‚è¿™ä¸€æ–¹æ³•æœ‰æ•ˆå®ç°äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç›®æ ‡éƒ¨åˆ†è¾¾æˆï¼Œåœ¨å®éªŒä¸­æ¯”ç°æœ‰çš„ LLMs+PDDL åŸºçº¿æ¨¡å‹çš„æˆåŠŸç‡ (Success Rate) æå‡äº† 52.45%ã€‚æœ€åï¼Œç ”ç©¶å›¢é˜Ÿåœ¨ TIAGo æœºå™¨äººä¸Šå®Œæˆäº†å®æœºéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ¡ˆåœ¨çœŸå®ç‰©ç†ç¯å¢ƒä¸‹çš„å¯é æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15828v2",
      "published_date": "2025-06-18 19:14:56 UTC",
      "updated_date": "2025-10-08 11:45:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:11.448578+00:00"
    },
    {
      "arxiv_id": "2506.15803v2",
      "title": "Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma",
      "title_zh": "ç”¨äºé¼»å’½ç™Œé«˜æ•ˆè´¨å­å¼§å½¢æ²»ç–—è®¡åˆ’ä¼˜åŒ–çš„å¿«é€Ÿèƒ½é‡å±‚é¢„é€‰æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Bohan Yang",
        "Gang Liu",
        "Yang Zhong",
        "Rirao Dao",
        "Yujia Qian",
        "Ke Shi",
        "Anke Tang",
        "Yong Luo",
        "Qi Kong",
        "Jingnan Liu"
      ],
      "abstract": "Proton arc therapy (PAT) is an emerging and promising modality in radiotherapy, offering improved dose distribution and treatment robustness over intensity-modulated proton therapy. Yet, identifying the optimal energy layer (EL) sequence remains challenging due to the intensive computational demand and prolonged treatment delivery time. This study proposes an unsupervised deep learning model for fast EL pre-selection that minimizes EL switch (ELS) time while maintaining high plan quality. We introduce a novel data representation method, spot-count representation, which encodes the number of proton spots intersecting the target and organs at risk (OAR) in a matrix structured by sorted gantry angles and energy layers. This representation serves as the input of an U-Net style architecture, SPArc_dl, which is trained using a tri-objective function: maximizing spot-counts on target, minimizing spot-counts on OAR, and reducing ELS time. The model is evaluated on 35 nasopharyngeal cancer cases, and its performance is compared to SPArc_particle_swarm (SPArc_ps). SPArc_dl produces EL pre-selection that significantly improves both plan quality and delivery efficiency. Compared to SPArc_ps, it enhances the conformity index by 0.1 (p<0.01), reduces the homogeneity index by 0.71 (p<0.01), lowers the brainstem mean dose by 0.25 (p<0.01), and shortens the ELS time by 37.2% (p < 0.01). The results unintentionally reveal employing unchanged ELS is more time-wise efficient than descended ELS. SPArc_dl's inference time is within 1 second. However, SPArc_dl plan demonstrates limitation in robustness. The proposed spot-count representation lays a foundation for incorporating unsupervised deep learning approaches into EL pre-selection task. SPArc_dl is a fast tool for generating high-quality PAT plans by strategically pre-selecting EL to reduce delivery time while maintaining excellent dosimetric performance.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºé¼»å’½ç™Œè´¨å­å¼§å½¢æ²»ç–—(Proton arc therapy, PAT)è®¡åˆ’ä¼˜åŒ–çš„æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³èƒ½é‡å±‚(energy layer, EL)é¢„é€‰è¿‡ç¨‹ä¸­è®¡ç®—éœ€æ±‚å¤§å’Œæ²»ç–—äº¤ä»˜æ—¶é—´é•¿çš„é—®é¢˜ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†åä¸º spot-count representation çš„æ–°å‹æ•°æ®è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶å¼€å‘äº†åŸºäº U-Net æ¶æ„çš„ SPArc_dl æ¨¡å‹ï¼Œé€šè¿‡åŒæ—¶ä¼˜åŒ–ç›®æ ‡è¦†ç›–ã€å±åŠå™¨å®˜(OAR)ä¿æŠ¤åŠèƒ½é‡å±‚åˆ‡æ¢(ELS)æ—¶é—´æ¥ç”Ÿæˆæ²»ç–—è®¡åˆ’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPArc_dl åœ¨ 35 ä¾‹é¼»å’½ç™Œç—…ä¾‹ä¸­å®ç°äº†ä¸åˆ° 1 ç§’çš„æ¨ç†é€Ÿåº¦ï¼Œä¸”åœ¨æå‡é€‚å½¢æŒ‡æ•°çš„åŒæ—¶å°† ELS æ—¶é—´ç¼©çŸ­äº† 37.2%ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ„å¤–å‘ç°ä¿æŒä¸å˜çš„ ELS åˆ‡æ¢æ¯”ä¸‹é™å¼ ELS æ›´å…·æ—¶é—´æ•ˆç‡ã€‚è™½ç„¶è¯¥æ¨¡å‹åœ¨é²æ£’æ€§æ–¹é¢ä»æœ‰å±€é™ï¼Œä½†å®ƒä¸ºåˆ©ç”¨æ— ç›‘ç£æ·±åº¦å­¦ä¹ æå‡ PAT æ²»ç–—æ•ˆç‡å’Œè®¡åˆ’è´¨é‡å¥ å®šäº†åŸºç¡€ï¼Œæ˜¯å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡æ”¾å°„æ²»ç–—æ–¹æ¡ˆçš„æœ‰æ•ˆå·¥å…·ã€‚",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15803v2",
      "published_date": "2025-06-18 18:40:10 UTC",
      "updated_date": "2025-08-07 08:09:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:10.128040+00:00"
    },
    {
      "arxiv_id": "2506.15794v1",
      "title": "Veracity: An Open-Source AI Fact-Checking System",
      "title_zh": "Veracityï¼šå¼€æºäººå·¥æ™ºèƒ½äº‹å®æ ¸æŸ¥ç³»ç»Ÿ",
      "authors": [
        "Taylor Lynn Curtis",
        "Maximilian Puelma Touzel",
        "William Garneau",
        "Manon Gruaz",
        "Mike Pinder",
        "Li Wei Wang",
        "Sukanya Krishna",
        "Luda Cohen",
        "Jean-FranÃ§ois Godbout",
        "Reihaneh Rabbany",
        "Kellin Pelrine"
      ],
      "abstract": "The proliferation of misinformation poses a significant threat to society, exacerbated by the capabilities of generative AI. This demo paper introduces Veracity, an open-source AI system designed to empower individuals to combat misinformation through transparent and accessible fact-checking. Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments with intuitive explanations. Key features include multilingual support, numerical scoring of claim veracity, and an interactive interface inspired by familiar messaging applications. This paper will showcase Veracity's ability to not only detect misinformation but also explain its reasoning, fostering media literacy and promoting a more informed society.",
      "tldr_zh": "è¯¥è®ºæ–‡ä»‹ç»äº† Veracityï¼Œä¸€ä¸ªæ—¨åœ¨åº”å¯¹è™šå‡ä¿¡æ¯ä¼ æ’­çš„å¼€æº AI äº‹å®æ ¸æŸ¥ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ Large Language Models (LLMs) ä¸ web retrieval agents ä¹‹é—´çš„ååŒä½œç”¨ï¼Œå¯¹ç”¨æˆ·æäº¤çš„å£°æ˜è¿›è¡Œåˆ†æå¹¶æä¾›å¯é çš„ veracity è¯„ä¼°åŠç›´è§‚è§£é‡Šã€‚Veracity çš„æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬æ”¯æŒ multilingualã€æä¾›æ•°å€¼åŒ–çš„è¯„åˆ†ï¼Œä»¥åŠé‡‡ç”¨å—å³æ—¶é€šè®¯åº”ç”¨å¯å‘çš„äº¤äº’å¼ç•Œé¢ã€‚é€šè¿‡å±•ç¤ºå…¶æ¨ç†è¿‡ç¨‹ï¼Œè¯¥ç³»ç»Ÿä¸ä»…èƒ½æœ‰æ•ˆæ£€æµ‹è™šå‡ä¿¡æ¯ï¼Œè¿˜èƒ½é€šè¿‡æå‡åª’ä½“ç´ å…»æ¥ä¿ƒè¿›æ„å»ºä¸€ä¸ªä¿¡æ¯æ›´åŠ é€æ˜çš„ç¤¾ä¼šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15794v1",
      "published_date": "2025-06-18 18:24:59 UTC",
      "updated_date": "2025-06-18 18:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:13.450011+00:00"
    },
    {
      "arxiv_id": "2506.15793v1",
      "title": "Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products",
      "title_zh": "åŸºäº Kroneker æ—‹è½¬ç§¯çš„å‘é‡ç¬¦å·é”®å€¼å­˜å‚¨çº¿æ€§å¯¹æ•°çº§æ¸…ç†",
      "authors": [
        "Ruipeng Liu",
        "Qinru Qiu",
        "Simon Khan",
        "Garrett E. Katz"
      ],
      "abstract": "A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is the ``clean-up'' step, which decodes the noisy vectors retrieved from the architecture. Clean-up typically compares noisy vectors against a ``codebook'' of prototype vectors, incurring computational complexity that is quadratic or similar. We present a new codebook representation that supports efficient clean-up, based on Kroneker products of rotation-like matrices. The resulting clean-up time complexity is linearithmic, i.e. $\\mathcal{O}(N\\,\\text{log}\\,N)$, where $N$ is the vector dimension and also the number of vectors in the codebook. Clean-up space complexity is $\\mathcal{O}(N)$. Furthermore, the codebook is not stored explicitly in computer memory: It can be represented in $\\mathcal{O}(\\text{log}\\,N)$ space, and individual vectors in the codebook can be materialized in $\\mathcal{O}(N)$ time and space. At the same time, asymptotic memory capacity remains comparable to standard approaches. Computer experiments confirm these results, demonstrating several orders of magnitude more scalability than baseline VSA techniques.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‘é‡ç¬¦å·æ¶æ„ (Vector-Symbolic Architectures, VSAs) ä¸­ä»æ¶æ„æ£€ç´¢å™ªå£°å‘é‡æ—¶é¢ä¸´çš„â€œæ¸…ç†â€ (clean-up) è®¡ç®—ç“¶é¢ˆé—®é¢˜ï¼Œå³ä¼ ç»Ÿè§£ç è¿‡ç¨‹é€šå¸¸å…·æœ‰å¹³æ–¹çº§æˆ–ç±»ä¼¼çš„è®¡ç®—å¤æ‚åº¦ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ—‹è½¬ç±»çŸ©é˜µçš„ Kronecker ç§¯ (Kroneker products of rotation-like matrices) çš„æ–°å‹ç æœ¬ (codebook) è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥æ”¯æŒé«˜æ•ˆçš„æ¸…ç†æ“ä½œã€‚è¯¥æ–¹æ³•å°†æ¸…ç†çš„æ—¶é—´å¤æ‚åº¦æ˜¾è‘—é™ä½è‡³æ‹Ÿçº¿æ€§çº§åˆ« $\\mathcal{O}(N \\log N)$ï¼Œå…¶ä¸­ $N$ æ—¢æ˜¯å‘é‡ç»´åº¦ä¹Ÿæ˜¯ç æœ¬ä¸­çš„å‘é‡æ•°é‡ï¼ŒåŒæ—¶å°†ç©ºé—´å¤æ‚åº¦ç»´æŒåœ¨ $\\mathcal{O}(N)$ã€‚æ­¤å¤–ï¼Œè¯¥ç æœ¬æ— éœ€åœ¨è®¡ç®—æœºå†…å­˜ä¸­æ˜¾å¼å­˜å‚¨ï¼Œä»…éœ€ $\\mathcal{O}(\\log N)$ çš„ç©ºé—´å³å¯è¡¨ç¤ºï¼Œä¸”å•ä¸ªç æœ¬å‘é‡å¯åœ¨ $\\mathcal{O}(N)$ çš„æ—¶é—´ä¸ç©ºé—´å†…å®ä¾‹åŒ–ã€‚åœ¨ä¿æŒæ¸è¿‘å†…å­˜å®¹é‡ (asymptotic memory capacity) ä¸æ ‡å‡†æ–¹æ³•ç›¸å½“çš„å‰æä¸‹ï¼Œè®¡ç®—æœºå®éªŒè¯æ˜è¯¥æŠ€æœ¯åœ¨å¯æ‰©å±•æ€§ä¸Šæ¯”åŸºçº¿ VSA æŠ€æœ¯æé«˜äº†æ•°ä¸ªæ•°é‡çº§ã€‚è¿™ä¸€ç ”ç©¶ä¸ºå®ç°è¶…å¤§è§„æ¨¡å‘é‡ç¬¦å·é”®å€¼å­˜å‚¨æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "10 pages, 10 figures, conference paper",
      "pdf_url": "https://arxiv.org/pdf/2506.15793v1",
      "published_date": "2025-06-18 18:23:28 UTC",
      "updated_date": "2025-06-18 18:23:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:14.011532+00:00"
    },
    {
      "arxiv_id": "2506.15791v1",
      "title": "TRUST: Transparent, Robust and Ultra-Sparse Trees",
      "title_zh": "TRUSTï¼šé€æ˜ã€é²æ£’ä¸”è¶…ç¨€ç–æ ‘",
      "authors": [
        "Albert Dorador"
      ],
      "abstract": "Piecewise-constant regression trees remain popular for their interpretability, yet often lag behind black-box models like Random Forest in predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and Ultra-Sparse Trees), a novel regression tree model that combines the accuracy of Random Forests with the interpretability of shallow decision trees and sparse linear models. TRUST further enhances transparency by leveraging Large Language Models to generate tailored, user-friendly explanations. Extensive validation on synthetic and real-world benchmark datasets demonstrates that TRUST consistently outperforms other interpretable models -- including CART, Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy of Random Forest and offering substantial gains in both accuracy and interpretability over M5', a well-established model that is conceptually related.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†TRUST (Transparent, Robust and Ultra-Sparse Trees)ï¼Œä¸€ç§ç»“åˆäº†é«˜é¢„æµ‹å‡†ç¡®ç‡ä¸å¼ºå¯è§£é‡Šæ€§çš„æ–°å‹å›å½’æ ‘æ¨¡å‹ã€‚TRUSTé€šè¿‡æ•´åˆRandom Forestçš„å‡†ç¡®æ€§ã€æµ…å±‚å†³ç­–æ ‘ä»¥åŠç¨€ç–çº¿æ€§æ¨¡å‹ (sparse linear models) çš„å¯è§£é‡Šæ€§ï¼Œæœ‰æ•ˆè§£å†³äº†ä¼ ç»Ÿåˆ†æ®µå¸¸æ•°å›å½’æ ‘åœ¨é¢„æµ‹æ€§èƒ½ä¸Šçš„å±€é™ã€‚è¯¥æ¨¡å‹è¿›ä¸€æ­¥åˆ©ç”¨Large Language Modelsç”Ÿæˆå®šåˆ¶åŒ–ä¸”æ˜“äºç†è§£çš„è§£é‡Šï¼Œæ˜¾è‘—æå‡äº†å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ã€‚åœ¨åˆæˆåŠçœŸå®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒTRUSTåœ¨å‡†ç¡®ç‡ä¸ŠæŒç»­ä¼˜äºCARTã€Lassoå’ŒNode Harvestç­‰ä¸»æµå¯è§£é‡Šæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå…¶é¢„æµ‹ç²¾åº¦å¯ä¸Random Forestç›¸åª²ç¾ï¼Œå¹¶åœ¨å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„å¹³è¡¡ä¸Šæ˜¾è‘—ä¼˜äºç»å…¸çš„M5'æ¨¡å‹ã€‚",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15791v1",
      "published_date": "2025-06-18 18:21:19 UTC",
      "updated_date": "2025-06-18 18:21:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:36.607505+00:00"
    },
    {
      "arxiv_id": "2506.15787v5",
      "title": "SLR: Automated Synthesis for Scalable Logical Reasoning",
      "title_zh": "SLRï¼šé¢å‘å¯æ‰©å±•é€»è¾‘æ¨ç†çš„è‡ªåŠ¨åŒ–åˆæˆ",
      "authors": [
        "Lukas Helff",
        "Ahmad Omar",
        "Felix Friedrich",
        "Antonia WÃ¼st",
        "Hikaru Shindo",
        "Rupert Mitchell",
        "Tim Woydt",
        "Patrick Schramowski",
        "Wolfgang Stammer",
        "Kristian Kersting"
      ],
      "abstract": "We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SLRï¼Œä¸€ä¸ªç”¨äºå¤§è§„æ¨¡é€»è¾‘æ¨ç†(Scalable Logical Reasoning)ç³»ç»ŸåŒ–è¯„ä¼°å’Œè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚SLRèƒ½è‡ªåŠ¨ç”Ÿæˆå½’çº³æ¨ç†ä»»åŠ¡æç¤ºã€æä¾›å¯éªŒè¯å¥–åŠ±çš„éªŒè¯ç¨‹åº(validation program)ä»¥åŠæ½œåœ¨çš„çœŸç†è§„åˆ™(ground-truth rule)ï¼Œå®ç°äº†æ— éœ€äººå·¥æ ‡æ³¨ä¸”å¯ç²¾ç¡®æ§åˆ¶éš¾åº¦çš„è‡ªåŠ¨åŒ–åˆæˆè¿‡ç¨‹ã€‚åŸºäºè¯¥æ¡†æ¶æ„å»ºçš„SLR-BenchåŸºå‡†æµ‹è¯•åŒ…å«1.9ä¸‡ä¸ªæç¤ºï¼Œæ­ç¤ºäº†å½“ä»£æ¨¡å‹è™½èƒ½ç”Ÿæˆåˆè§„è¯­æ³•ä½†å¸¸åœ¨é€»è¾‘æ¨ç†(logical inference)ä¸Šå¤±è´¥ï¼Œä¸”æ¨ç†æ¨¡å‹æ™®éé¢ä¸´æé«˜çš„æµ‹è¯•è®¡ç®—æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡SLRè¿›è¡Œçš„è¯¾ç¨‹å­¦ä¹ (curriculum learning)ä½¿Llama-3-8Bçš„å‡†ç¡®ç‡ç¿»å€ï¼Œä»¥æä½çš„æˆæœ¬è¾¾åˆ°äº†ä¸Gemini-Flash-Thinkingç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚æ­¤å¤–ï¼Œè¿™ç§é€šè¿‡SLRè·å¾—çš„æ¨ç†èƒ½åŠ›è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§ï¼Œåœ¨å¤šç§æ—¢æœ‰åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15787v5",
      "published_date": "2025-06-18 18:10:30 UTC",
      "updated_date": "2026-01-06 11:21:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:39.188195+00:00"
    },
    {
      "arxiv_id": "2506.15786v1",
      "title": "Graphics4Science: Computer Graphics for Scientific Impacts",
      "title_zh": "Graphics4Scienceï¼šè®¡ç®—æœºå›¾å½¢å­¦èµ‹èƒ½ç§‘å­¦å½±å“åŠ›",
      "authors": [
        "Peter Yichen Chen",
        "Minghao Guo",
        "Hanspeter Pfister",
        "Ming Lin",
        "William Freeman",
        "Qixing Huang",
        "Han-Wei Shen",
        "Wojciech Matusik"
      ],
      "abstract": "Computer graphics, often associated with films, games, and visual effects, has long been a powerful tool for addressing scientific challenges--from its origins in 3D visualization for medical imaging to its role in modern computational modeling and simulation. This course explores the deep and evolving relationship between computer graphics and science, highlighting past achievements, ongoing contributions, and open questions that remain. We show how core methods, such as geometric reasoning and physical modeling, provide inductive biases that help address challenges in both fields, especially in data-scarce settings. To that end, we aim to reframe graphics as a modeling language for science by bridging vocabulary gaps between the two communities. Designed for both newcomers and experts, Graphics4Science invites the graphics community to engage with science, tackle high-impact problems where graphics expertise can make a difference, and contribute to the future of scientific discovery. Additional details are available on the course website: https://graphics4science.github.io",
      "tldr_zh": "Graphics4Scienceæ¢è®¨äº†Computer Graphicsåœ¨è§£å†³åŒ»å­¦å½±åƒã€è®¡ç®—å»ºæ¨¡å’Œä»¿çœŸç­‰ç§‘å­¦æŒ‘æˆ˜ä¸­çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒäº†å…¶ä½œä¸ºç§‘å­¦ç ”ç©¶å¼ºå¤§å·¥å…·çš„åœ°ä½ã€‚è¯¥ç ”ç©¶é€šè¿‡Geometric reasoningå’ŒPhysical modelingç­‰æ ¸å¿ƒæ–¹æ³•æä¾›å½’çº³åç½®(Inductive biases)ï¼Œæ—¨åœ¨åº”å¯¹æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„ç§‘å­¦é—®é¢˜ã€‚æ–‡ç« æå‡ºå°†Computer Graphicsé‡æ„ä¸ºä¸€ç§ç§‘å­¦å»ºæ¨¡è¯­è¨€ï¼Œä»¥æ¶ˆé™¤è®¡ç®—æœºå›¾å½¢å­¦ä¸ç§‘å­¦ç•Œä¹‹é—´çš„æ²Ÿé€šéš”é˜‚ã€‚é€šè¿‡å›é¡¾è¿‡å¾€æˆå°±å¹¶æå‡ºå¼€æ”¾æ€§é—®é¢˜ï¼Œè¯¥å·¥ä½œæ—¨åœ¨é‚€è¯·å›¾å½¢å­¦ä¸“å®¶å‚ä¸é«˜å½±å“åŠ›çš„ç§‘å­¦ä»»åŠ¡ï¼Œä¸ºæœªæ¥çš„ç§‘å­¦å‘ç°è´¡çŒ®å›¾å½¢å­¦ä¸“ä¸šçŸ¥è¯†ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG",
        "physics.comp-ph",
        "physics.optics"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15786v1",
      "published_date": "2025-06-18 18:06:58 UTC",
      "updated_date": "2025-06-18 18:06:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:55.356763+00:00"
    },
    {
      "arxiv_id": "2506.15774v1",
      "title": "Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints",
      "title_zh": "é€šè¿‡è€—æ•£è¿‡æ»¡è¶³çº¦æŸæå‡éšæœº 3-SAT æ±‚è§£å™¨",
      "authors": [
        "J. Schwardt",
        "J. C. Budich"
      ],
      "abstract": "We introduce and benchmark a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT that drastically outperforms existing solvers in the notoriously difficult realm of critically hard instances. Our construction is based on the crucial observation that well established previous approaches such as WalkSAT are prone to get stuck in local minima that are distinguished from true solutions by a larger number of oversatisfied combinatorial constraints. To address this issue, the proposed algorithm, coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their unfavorable abundance so as to render them critical. We analyze and benchmark our algorithm on a randomly generated sample of hard but satisfiable 3-SAT instances with varying problem sizes up to N=15000. Quite remarkably, we find that DOCSAT outperforms both WalkSAT and other well known algorithms including the complete solver Kissat, even when comparing its ability to solve the hardest quintile of the sample to the average performance of its competitors. The essence of DOCSAT may be seen as a way of harnessing statistical structure beyond the primary cost function of a combinatorial problem to avoid or escape local minima traps in stochastic local search, which opens avenues for generalization to other optimization problems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ NP-complete çš„æ»¡è¶³é—®é¢˜ 3-SAT æå‡ºäº†ä¸€ç§éšæœºå±€éƒ¨æœç´¢(stochastic local search)å¯å‘å¼ç®—æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—æå‡åœ¨æéš¾å®ä¾‹ä¸Šçš„æ±‚è§£æ•ˆç‡ã€‚ä½œè€…å‘ç° WalkSAT ç­‰ç°æœ‰æ±‚è§£å™¨å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼(local minima)ï¼Œè€Œè¿™äº›é™·é˜±å¾€å¾€ä¼´éšç€è¿‡é‡çš„è¿‡åº¦æ»¡è¶³ç»„åˆçº¦æŸ(oversatisfied combinatorial constraints)ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†åä¸º DOCSAT çš„ç®—æ³•ï¼Œé€šè¿‡æ¶ˆè§£è¿‡åº¦æ»¡è¶³çº¦æŸ(dissipating oversatisfied constraints)æ¥å‡å°‘å…¶å†—ä½™ï¼Œä»è€Œå¼•å¯¼æœç´¢è·³å‡ºé™·é˜±ã€‚åœ¨è§„æ¨¡é«˜è¾¾ N=15000 çš„éšæœº 3-SAT å®ä¾‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDOCSAT çš„è¡¨ç°å…¨é¢è¶…è¶Šäº† WalkSAT ä»¥åŠå®Œå¤‡æ±‚è§£å™¨ Kissatã€‚å®éªŒè¯æ˜ï¼ŒDOCSAT åœ¨å¤„ç†æ ·æœ¬ä¸­æœ€éš¾çš„äº”åˆ†ä¹‹ä¸€å®ä¾‹æ—¶çš„è¡¨ç°ç”šè‡³ä¼˜äºç«äº‰å¯¹æ‰‹çš„å¹³å‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨ä¸»è¦æˆæœ¬å‡½æ•°ä¹‹å¤–çš„ç»Ÿè®¡ç»“æ„æ¥è§„é¿å±€éƒ¨æœ€å°å€¼ï¼Œä¸ºè§£å†³å…¶ä»–å¤æ‚çš„ç»„åˆä¼˜åŒ–é—®é¢˜å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.stat-mech",
        "cs.DS",
        "math.CO"
      ],
      "primary_category": "cs.AI",
      "comment": "5+1 pages, 6+2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15774v1",
      "published_date": "2025-06-18 18:00:01 UTC",
      "updated_date": "2025-06-18 18:00:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:39.457173+00:00"
    },
    {
      "arxiv_id": "2506.15679v2",
      "title": "Dense SAE Latents Are Features, Not Bugs",
      "title_zh": "ç¨ å¯† SAE æ½œå˜é‡æ˜¯ç‰¹å¾ï¼Œè€Œéç¼ºé™·",
      "authors": [
        "Xiaoqing Sun",
        "Alessandro Stolfo",
        "Joshua Engels",
        "Ben Wu",
        "Senthooran Rajamanoharan",
        "Mrinmaya Sachan",
        "Max Tegmark"
      ],
      "abstract": "Sparse autoencoders (SAEs) are designed to extract interpretable features from language models by enforcing a sparsity constraint. Ideally, training an SAE would yield latents that are both sparse and semantically meaningful. However, many SAE latents activate frequently (i.e., are \\emph{dense}), raising concerns that they may be undesirable artifacts of the training procedure. In this work, we systematically investigate the geometry, function, and origin of dense latents and show that they are not only persistent but often reflect meaningful model representations. We first demonstrate that dense latents tend to form antipodal pairs that reconstruct specific directions in the residual stream, and that ablating their subspace suppresses the emergence of new dense features in retrained SAEs -- suggesting that high density features are an intrinsic property of the residual space. We then introduce a taxonomy of dense latents, identifying classes tied to position tracking, context binding, entropy regulation, letter-specific output signals, part-of-speech, and principal component reconstruction. Finally, we analyze how these features evolve across layers, revealing a shift from structural features in early layers, to semantic features in mid layers, and finally to output-oriented signals in the last layers of the model. Our findings indicate that dense latents serve functional roles in language model computation and should not be dismissed as training noise.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†ç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders, SAEs)ä¸­é¢‘ç¹æ¿€æ´»çš„å¯†é›†æ½œå˜é‡(dense latents)ï¼Œæ—¨åœ¨è¯æ˜è¿™äº›å˜é‡å¹¶éè®­ç»ƒè¿‡ç¨‹äº§ç”Ÿçš„å™ªå£°ï¼Œè€Œæ˜¯åæ˜ äº†è¯­è¨€æ¨¡å‹å†…éƒ¨çš„æœ‰æ„ä¹‰è¡¨å¾ã€‚é€šè¿‡åˆ†æå‡ ä½•ç»“æ„ä¸åŠŸèƒ½ï¼Œç ”ç©¶å‘ç°å¯†é›†æ½œå˜é‡å€¾å‘äºå½¢æˆå¯¹è·–å¯¹(antipodal pairs)ä»¥é‡æ„æ®‹å·®æµ(residual stream)ä¸­çš„ç‰¹å®šæ–¹å‘ï¼Œä¸”æ¶ˆèå®éªŒè¯æ˜äº†è¿™äº›é«˜å¯†åº¦ç‰¹å¾æ˜¯æ¨¡å‹ç©ºé—´çš„å†…åœ¨å±æ€§ã€‚ç ”ç©¶è€…ä¸ºæ­¤æå‡ºäº†ä¸€å¥—åˆ†ç±»å­¦ï¼Œæ¶µç›–äº†ä½ç½®è¿½è¸ª(position tracking)ã€ä¸Šä¸‹æ–‡ç»‘å®š(context binding)ã€ç†µè°ƒèŠ‚(entropy regulation)ä»¥åŠè¯æ€§(part-of-speech)ç­‰å…·ä½“åŠŸèƒ½ç±»åˆ«ã€‚è·¨å±‚åˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†å¯†é›†æ½œå˜é‡éšæ·±åº¦çš„æ¼”å˜è¿‡ç¨‹ï¼Œå³ä»æ—©æœŸå±‚çš„ç»“æ„æ€§ç‰¹å¾è½¬å‘ä¸­æœŸå±‚çš„è¯­ä¹‰ç‰¹å¾ï¼Œæœ€ç»ˆåœ¨æœ«å±‚æ¼”å˜ä¸ºé¢å‘è¾“å‡ºçš„ä¿¡å·ã€‚è¿™äº›å‘ç°è¡¨æ˜å¯†é›†æ½œå˜é‡åœ¨è¯­è¨€æ¨¡å‹è®¡ç®—ä¸­å…·æœ‰æ˜ç¡®çš„åŠŸèƒ½æ€§è§’è‰²ï¼Œä¸åº”è¢«ç®€å•è§†ä¸ºè®­ç»ƒä¼ªå½±ï¼Œä¸ºç†è§£æ¨¡å‹å†…éƒ¨å·¥ä½œæœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2025 poster",
      "pdf_url": "https://arxiv.org/pdf/2506.15679v2",
      "published_date": "2025-06-18 17:59:35 UTC",
      "updated_date": "2025-11-05 16:12:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:40.737444+00:00"
    },
    {
      "arxiv_id": "2506.15677v3",
      "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence",
      "title_zh": "å…·èº« Web æ™ºèƒ½ä½“ï¼šè¿æ¥ç‰©ç†ä¸æ•°å­—é¢†åŸŸï¼Œå®ç°èåˆçš„æ™ºèƒ½ä½“æ™ºèƒ½",
      "authors": [
        "Yining Hong",
        "Rui Sun",
        "Bingxuan Li",
        "Xingcheng Yao",
        "Maxine Wu",
        "Alexander Chien",
        "Da Yin",
        "Ying Nian Wu",
        "Zhecan James Wang",
        "Kai-Wei Chang"
      ],
      "abstract": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† Embodied Web Agents è¿™ä¸€æ–°èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ AI æ™ºèƒ½ä½“åœ¨æ•°å­—ä¿¡æ¯å¤„ç†ä¸ç‰©ç†ä¸–ç•Œäº¤äº’ (Embodiment) ä¹‹é—´é•¿æœŸå­˜åœ¨çš„éš”ç¦»é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„ä»¿çœŸå¹³å°ï¼Œå°†é€¼çœŸçš„ 3D å®¤å†…å¤–ç¯å¢ƒä¸åŠŸèƒ½æ€§ Web æ¥å£ç´§å¯†é›†æˆï¼Œå®ç°äº†å…·èº«æ„ŸçŸ¥ä¸ Web è§„æ¨¡æ¨ç† (Web-scale reasoning) çš„æœ‰æœºç»“åˆã€‚åŸºäºæ­¤å¹³å°ï¼Œç ”ç©¶è€…æ¨å‡ºäº†åŒ…å«çƒ¹é¥ªã€å¯¼èˆªã€è´­ç‰©åŠåœ°ç†å®šä½ç­‰ä»»åŠ¡çš„ Embodied Web Agents Benchmarkï¼Œç”¨ä»¥ç³»ç»Ÿè¯„ä¼°æ™ºèƒ½ä½“åœ¨è·¨ç‰©ç†ä¸æ•°å­—é¢†åŸŸåè°ƒæ¨ç†çš„èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„ AI ç³»ç»Ÿä¸äººç±»è¡¨ç°ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæ­ç¤ºäº†å…·èº«è®¤çŸ¥ (Embodied cognition) ä¸å¤§è§„æ¨¡ Web çŸ¥è¯†è·å–äº¤å‰é¢†åŸŸçš„æŒ‘æˆ˜ä¸æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘å…·å¤‡ç»¼åˆæ™ºèƒ½çš„æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„åŸºç¡€æ¡†æ¶å’Œè¯„ä¼°å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15677v3",
      "published_date": "2025-06-18 17:58:17 UTC",
      "updated_date": "2025-07-29 22:40:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:58.978022+00:00"
    },
    {
      "arxiv_id": "2506.15675v3",
      "title": "Sekai: A Video Dataset towards World Exploration",
      "title_zh": "Sekaiï¼šé¢å‘ä¸–ç•Œæ¢ç´¢çš„è§†é¢‘æ•°æ®é›†",
      "authors": [
        "Zhen Li",
        "Chuanhao Li",
        "Xiaofeng Mao",
        "Shaoheng Lin",
        "Ming Li",
        "Shitian Zhao",
        "Zhaopan Xu",
        "Xinyue Li",
        "Yukang Feng",
        "Jianwen Sun",
        "Zizhen Li",
        "Fanrui Zhang",
        "Jiaxin Ai",
        "Zhixiang Wang",
        "Yuwei Wu",
        "Tong He",
        "Jiangmiao Pang",
        "Yu Qiao",
        "Yunde Jia",
        "Kaipeng Zhang"
      ],
      "abstract": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning \"world\" in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Comprehensive analyses and experiments demonstrate the dataset's scale, diversity, annotation quality, and effectiveness for training video generation models. We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications. The project page is https://lixsp11.github.io/sekai-project/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sekaiï¼Œä¸€ä¸ªæ—¨åœ¨æ¨åŠ¨ä¸–ç•Œæ¢ç´¢(World Exploration)çš„é«˜è´¨é‡ç¬¬ä¸€äººç§°è§†è§’(First-Person View)è§†é¢‘æ•°æ®é›†ã€‚é’ˆå¯¹ç°æœ‰è§†é¢‘ç”Ÿæˆæ•°æ®é›†åœ¨åœ°ç†ä½ç½®æœ‰é™ã€æ—¶é•¿çŸ­ã€åœºæ™¯é™æ€ä»¥åŠç¼ºä¹æ¢ç´¢æ ‡æ³¨ç­‰æ–¹é¢çš„å±€é™ï¼ŒSekai æä¾›äº†è¶…è¿‡ 5,000 å°æ—¶çš„æ­¥è¡Œæˆ–æ— äººæœºè§†è§’è§†é¢‘ï¼Œæ¶µç›–äº†å…¨çƒ 100 å¤šä¸ªå›½å®¶å’Œåœ°åŒºçš„ 750 ä¸ªåŸå¸‚ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€å¥—é«˜æ•ˆçš„å·¥å…·ç®±ï¼Œå¯¹è§†é¢‘è¿›è¡Œäº†åŒ…æ‹¬åœ°ç†ä½ç½®ã€åœºæ™¯ã€å¤©æ°”ã€äººç¾¤å¯†åº¦ã€æ–‡æœ¬æè¿°(Captions)ä»¥åŠç›¸æœºè½¨è¿¹(Camera Trajectories)åœ¨å†…çš„å¤šç»´åº¦ä¸°å¯Œæ ‡æ³¨ã€‚ç»¼åˆåˆ†æä¸å®éªŒç»“æœè¯æ˜äº†è¯¥æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œæ ‡æ³¨è´¨é‡ä¸Šçš„ä¼˜åŠ¿ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹(Video Generation Models)æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ•°æ®é›†çš„æ¨å‡ºå°†æ˜¾è‘—æƒ åŠè§†é¢‘ç”Ÿæˆå’Œè‡ªä¸»ä¸–ç•Œæ¢ç´¢é¢†åŸŸï¼Œå¹¶ä¸ºç›¸å…³åº”ç”¨ç ”ç©¶æä¾›é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15675v3",
      "published_date": "2025-06-18 17:57:06 UTC",
      "updated_date": "2025-11-09 16:58:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:55.979977+00:00"
    },
    {
      "arxiv_id": "2506.15674v2",
      "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
      "title_zh": "æ€ç»´æ³„éœ²ï¼šå¤§å‹æ¨ç†æ¨¡å‹å¹¶éç§å¯†æ€è€ƒè€…",
      "authors": [
        "Tommaso Green",
        "Martin Gubri",
        "Haritz Puerto",
        "Sangdoo Yun",
        "Seong Joon Oh"
      ],
      "abstract": "We study privacy leakage in the reasoning traces of large reasoning models used as personal agents. Unlike final outputs, reasoning traces are often assumed to be internal and safe. We challenge this assumption by showing that reasoning traces frequently contain sensitive user data, which can be extracted via prompt injections or accidentally leak into outputs. Through probing and agentic evaluations, we demonstrate that test-time compute approaches, particularly increased reasoning steps, amplify such leakage. While increasing the budget of those test-time compute approaches makes models more cautious in their final answers, it also leads them to reason more verbosely and leak more in their own thinking. This reveals a core tension: reasoning improves utility but enlarges the privacy attack surface. We argue that safety efforts must extend to the model's internal thinking, not just its outputs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models)ä½œä¸ºä¸ªäººæ™ºèƒ½ä½“æ—¶æ¨ç†è½¨è¿¹ä¸­çš„éšç§æ³„éœ²é—®é¢˜ï¼ŒæŒ‘æˆ˜äº†æ¨ç†è½¨è¿¹æ˜¯å†…éƒ¨ä¸”å®‰å…¨çš„ä¸€è´¯å‡è®¾ã€‚é€šè¿‡æ¢æµ‹å’Œä»£ç†è¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨ç†è½¨è¿¹ä¸­ç»å¸¸åŒ…å«æ•æ„Ÿçš„ç”¨æˆ·æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯èƒ½é€šè¿‡æç¤ºæ³¨å…¥(Prompt Injections)æˆ–æ„å¤–æ³„æ¼åˆ°æœ€ç»ˆè¾“å‡ºä¸­ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¡¨æ˜ï¼Œæµ‹è¯•æ—¶è®¡ç®—(Test-time compute)æ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯å¢åŠ æ¨ç†æ­¥éª¤ï¼‰ä¼šæ”¾å¤§è¿™ç§æ³„éœ²é£é™©ã€‚å°½ç®¡å¢åŠ è®¡ç®—é¢„ç®—ä½¿æ¨¡å‹åœ¨æœ€ç»ˆå›ç­”æ—¶æ›´è°¨æ…ï¼Œä½†ä¹Ÿä¼šå¯¼è‡´å…¶å†…éƒ¨æ€ç»´è¿‡ç¨‹æ›´åŠ å†—é•¿ï¼Œä»è€Œæš´éœ²æ›´å¤šéšç§ä¿¡æ¯ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†æå‡æ¨ç†èƒ½åŠ›ä¸æ‰©å¤§éšç§æ”»å‡»é¢ä¹‹é—´çš„æ ¸å¿ƒçŸ›ç›¾ï¼Œå¼ºè°ƒå®‰å…¨é˜²æŠ¤å¿…é¡»å»¶ä¼¸è‡³æ¨¡å‹çš„å†…éƒ¨æ€ç»´ï¼Œè€Œéä»…ä»…å±€é™äºæœ€ç»ˆè¾“å‡ºã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2506.15674v2",
      "published_date": "2025-06-18 17:57:01 UTC",
      "updated_date": "2025-10-01 08:33:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:40:54.963597+00:00"
    },
    {
      "arxiv_id": "2506.15672v1",
      "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence",
      "title_zh": "SwarmAgenticï¼šé€šè¿‡ç¾¤ä½“æ™ºèƒ½è¿ˆå‘å…¨è‡ªåŠ¨æ™ºèƒ½ä½“ç³»ç»Ÿç”Ÿæˆ",
      "authors": [
        "Yao Zhang",
        "Chenyang Lin",
        "Shijie Tang",
        "Haokun Chen",
        "Shijie Zhou",
        "Yunpu Ma",
        "Volker Tresp"
      ],
      "abstract": "The rapid progress of Large Language Models has advanced agentic systems in decision-making, coordination, and task execution. Yet, existing agentic system generation frameworks lack full autonomy, missing from-scratch agent generation, self-optimizing agent functionality, and collaboration, limiting adaptability and scalability. We propose SwarmAgentic, a framework for fully automated agentic system generation that constructs agentic systems from scratch and jointly optimizes agent functionality and collaboration as interdependent components through language-driven exploration. To enable efficient search over system-level structures, SwarmAgentic maintains a population of candidate systems and evolves them via feedback-guided updates, drawing inspiration from Particle Swarm Optimization (PSO). We evaluate our method on six real-world, open-ended, and exploratory tasks involving high-level planning, system-level coordination, and creative reasoning. Given only a task description and an objective function, SwarmAgentic outperforms all baselines, achieving a +261.8% relative improvement over ADAS on the TravelPlanner benchmark, highlighting the effectiveness of full automation in structurally unconstrained tasks. This framework marks a significant step toward scalable and autonomous agentic system design, bridging swarm intelligence with fully automated system multi-agent generation. Our code is publicly released at https://yaoz720.github.io/SwarmAgentic/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SwarmAgentic æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆAgentic Systemï¼‰çš„å…¨è‡ªåŠ¨ç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰æ¡†æ¶åœ¨ä»é›¶æ„å»ºã€åŠŸèƒ½è‡ªä¼˜åŒ–åŠåä½œæœºåˆ¶æ–¹é¢çš„è‡ªåŠ¨åŒ–ä¸è¶³ã€‚è¯¥æ¡†æ¶å°†æ™ºèƒ½ä½“åŠŸèƒ½ä¸åä½œè§†ä¸ºç›¸äº’ä¾èµ–çš„ç»„ä»¶ï¼Œé€šè¿‡è¯­è¨€é©±åŠ¨çš„æ¢ç´¢è¿›è¡Œè”åˆä¼˜åŒ–ã€‚SwarmAgentic å€Ÿé‰´äº†ç²’å­ç¾¤ä¼˜åŒ–ï¼ˆParticle Swarm Optimization, PSOï¼‰çš„æ€æƒ³ï¼Œé€šè¿‡ç»´æŠ¤å€™é€‰ç³»ç»Ÿç§ç¾¤å¹¶åˆ©ç”¨åé¦ˆå¼•å¯¼æ›´æ–°ï¼Œå®ç°äº†é«˜æ•ˆçš„ç³»ç»Ÿçº§ç»“æ„æœç´¢ã€‚åœ¨æ¶‰åŠé«˜çº§è§„åˆ’ã€åè°ƒå’Œæ¨ç†çš„å…­é¡¹ç°å®ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒSwarmAgentic ä»…å‡­ä»»åŠ¡æè¿°å’Œç›®æ ‡å‡½æ•°ä¾¿è¶…è¶Šäº†æ‰€æœ‰åŸºå‡†æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯åœ¨ TravelPlanner åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½è¾ƒ ADAS æå‡äº† 261.8%ï¼Œå……åˆ†å±•ç¤ºäº†å…¨è‡ªåŠ¨åŒ–åœ¨å¤„ç†ç»“æ„ä¸å—é™ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æˆåŠŸå°†ç¾¤ä½“æ™ºèƒ½ï¼ˆSwarm Intelligenceï¼‰åº”ç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªä¸»è®¾è®¡ï¼Œä¸ºæ„å»ºå¯æ‰©å±•ä¸”å…¨è‡ªåŠ¨çš„ AI ç³»ç»Ÿè¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "41 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.15672v1",
      "published_date": "2025-06-18 17:54:55 UTC",
      "updated_date": "2025-06-18 17:54:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:08.672735+00:00"
    },
    {
      "arxiv_id": "2506.15655v2",
      "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree",
      "title_zh": "cASTï¼šåŸºäºæŠ½è±¡è¯­æ³•æ ‘çš„ç»“æ„åŒ–åˆ†å—å¢å¼ºä»£ç æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Yilin Zhang",
        "Xinran Zhao",
        "Zora Zhiruo Wang",
        "Chenyang Yang",
        "Jiayi Wei",
        "Tongshuang Wu"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become essential for large-scale code generation, grounding predictions in external code corpora to improve actuality. However, a critical yet underexplored aspect of RAG pipelines is chunking -- the process of dividing documents into retrievable units. Existing line-based chunking heuristics often break semantic structures, splitting functions or merging unrelated code, which can degrade generation quality. We propose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method that recursively breaks large AST nodes into smaller chunks and merges sibling nodes while respecting size limits. This approach generates self-contained, semantically coherent units across programming languages and tasks, improving performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3 points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation. Our work highlights the importance of structure-aware chunking for scaling retrieval-enhanced code intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†cASTï¼Œä¸€ç§åˆ©ç”¨æŠ½è±¡è¯­æ³•æ ‘(Abstract Syntax Tree)è¿›è¡Œç»“æ„åŒ–åˆ†å—çš„å¢å¼ºå‹ä»£ç æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»ŸåŸºäºè¡Œ(line-based)çš„å¯å‘å¼åˆ†å—å®¹æ˜“ç ´åè¯­ä¹‰ç»“æ„ã€å¯¼è‡´å‡½æ•°æ–­è£‚æˆ–æ— å…³ä»£ç åˆå¹¶çš„é—®é¢˜ï¼ŒcASTé€šè¿‡é€’å½’åˆ†è§£å¤§å‹ASTèŠ‚ç‚¹å¹¶æ ¹æ®å¤§å°é™åˆ¶åˆå¹¶å…„å¼ŸèŠ‚ç‚¹ï¼Œç”Ÿæˆäº†è¯­ä¹‰è¿è´¯ä¸”è‡ªåŒ…å«çš„æ£€ç´¢å•å…ƒã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§ç¼–ç¨‹è¯­è¨€å’Œä»»åŠ¡ï¼Œåœ¨RepoEvalæ£€ç´¢ä»»åŠ¡ä¸­å°†Recall@5æå‡äº†4.3ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶åœ¨SWE-benchç”Ÿæˆä»»åŠ¡ä¸­å°†Pass@1æå‡äº†2.67ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ä¼˜åŒ–æ£€ç´¢å•å…ƒçš„è´¨é‡ï¼ŒcASTæ˜¾è‘—æ”¹å–„äº†å¤§è§„æ¨¡ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸è¡¨ç°ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº†ç»“æ„æ„ŸçŸ¥åˆ†å—åœ¨æå‡æ£€ç´¢å¢å¼ºä»£ç æ™ºèƒ½(retrieval-enhanced code intelligence)æ°´å¹³ä¸­çš„å…³é”®ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15655v2",
      "published_date": "2025-06-18 17:31:51 UTC",
      "updated_date": "2025-10-02 19:20:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:09.635005+00:00"
    },
    {
      "arxiv_id": "2506.15651v1",
      "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning",
      "title_zh": "AutoRuleï¼šåŸºäºæ€ç»´é“¾æ¨ç†æå–çš„è§„åˆ™å¥–åŠ±æå‡åå¥½å­¦ä¹ ",
      "authors": [
        "Tevin Wang",
        "Chenyan Xiong"
      ],
      "abstract": "Rule-based rewards offer a promising strategy for improving reinforcement learning from human feedback (RLHF), but current approaches often rely on manual rule engineering. We present AutoRule, a fully automated method for extracting rules from preference feedback and formulating them into rule-based rewards. AutoRule extraction operates in three stages: it leverages a reasoning model to interpret user preferences, identifies candidate rules from the reasoning chain of these interpretations, and synthesizes them into a unified rule set. Leveraging the finalized rule set, we employ language-model verifiers to compute the fraction of rules satisfied by each output, using this metric as an auxiliary reward alongside the learned reward model during policy optimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\% relative improvement in length-controlled win rate on AlpacaEval2.0, and a 6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset, compared to a GRPO baseline trained with the same learned reward model but without the rule-based auxiliary reward. Our analysis confirms that the extracted rules exhibit good agreement with dataset preference. We find that AutoRule demonstrates reduced reward hacking compared to a learned reward model when run over two episodes. Finally, our case study suggests that the extracted rules capture unique qualities valued in different datasets. The extracted rules are provided in the appendix, and the code is open-sourced at https://github.com/cxcscmu/AutoRule.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AutoRuleï¼Œè¿™æ˜¯ä¸€ç§ä»åå¥½åé¦ˆä¸­è‡ªåŠ¨æå–è§„åˆ™å¹¶å°†å…¶è½¬åŒ–ä¸ºè§„åˆ™å¥–åŠ± (rule-based rewards) çš„æ–¹æ³•ï¼Œæ—¨åœ¨å…‹æœå¼ºåŒ–å­¦ä¹  (RLHF) ä¸­äººå·¥è§„åˆ™å·¥ç¨‹çš„å±€é™æ€§ã€‚AutoRule çš„æå–è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåˆ©ç”¨æ¨ç†æ¨¡å‹è§£è¯»ç”¨æˆ·åå¥½ã€ä»æ¨ç†é“¾ (reasoning chain) ä¸­è¯†åˆ«å€™é€‰è§„åˆ™ä»¥åŠåˆæˆç»Ÿä¸€è§„åˆ™é›†ã€‚åœ¨ç­–ç•¥ä¼˜åŒ–é˜¶æ®µï¼Œè¯¥æ–¹æ³•é€šè¿‡è¯­è¨€æ¨¡å‹éªŒè¯å™¨è®¡ç®—è§„åˆ™æ»¡è¶³ç‡ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾…åŠ©å¥–åŠ±ä¸å­¦ä¹ åˆ°çš„å¥–åŠ±æ¨¡å‹å…±åŒä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ AutoRule è®­ç»ƒçš„ Llama-3-8B æ¨¡å‹åœ¨ AlpacaEval2.0 ä¸Šçš„é•¿åº¦æ§åˆ¶èƒœç‡ç›¸å¯¹æå‡äº† 28.6%ï¼Œä¸”åœ¨ MT-Bench ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚åˆ†æè¯å®ï¼ŒAutoRule æå–çš„è§„åˆ™ä¸æ•°æ®é›†åå¥½é«˜åº¦ä¸€è‡´ï¼Œå¹¶èƒ½æ¯”ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹æ›´æœ‰æ•ˆåœ°å‡å°‘å¥–åŠ±ä½œå¼Š (reward hacking) ç°è±¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15651v1",
      "published_date": "2025-06-18 17:29:19 UTC",
      "updated_date": "2025-06-18 17:29:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:31.592727+00:00"
    },
    {
      "arxiv_id": "2507.07036v1",
      "title": "Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic",
      "title_zh": "è·¨å°ºåº¦ç©ºé—´å¼‚è´¨æ€§å»ºæ¨¡ï¼šæ­ç¤º Antarctic æµ·å†°é€€ç¼©ä¸å†°æ¶æ¶ˆèä¹‹é—´çš„å…³è”",
      "authors": [
        "Maloy Kumar Devnath",
        "Sudip Chakraborty",
        "Vandana P. Janeja"
      ],
      "abstract": "Spatial phenomena often exhibit heterogeneity across spatial extents and in proximity, making them complex to model-especially in dynamic regions like ice shelves and sea ice. In this study, we address this challenge by exploring the linkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although atmospheric forcing and basal melting have been widely studied, the direct impact of sea ice retreat on AIS mass loss remains underexplored. Traditional models treat sea ice and AIS as separate systems. It limits their ability to capture localized linkages and cascading feedback. To overcome this, we propose Spatial-Link, a novel graph-based framework that quantifies spatial heterogeneity to capture linkages between sea ice retreat and AIS melt. Our method constructs a spatial graph using Delaunay triangulation of satellite-derived ice change matrices, where nodes represent regions of significant change and edges encode proximity and directional consistency. We extract and statistically validate linkage paths using breadth-first search and Monte Carlo simulations. Results reveal non-local, spatially heterogeneous coupling patterns, suggesting sea ice loss can initiate or amplify downstream AIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid and progresses toward ice shelves-establishing a direct linkage. To our knowledge, this is the first proposed methodology linking sea ice retreat to AIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level rise projections and inform climate adaptation strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å—ææµ·å†°é€€ç¼©(Sea Ice Retreat)ä¸å†°æ¶èåŒ–(Antarctic ice shelf melt)ä¹‹é—´çš„å…³è”ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹å°†äºŒè€…è§†ä¸ºç‹¬ç«‹ç³»ç»Ÿè€Œéš¾ä»¥æ•æ‰å±€éƒ¨è”åŠ¨å’Œçº§è”åé¦ˆçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Spatial-Linkï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºå›¾(graph-based)çš„æ¡†æ¶ï¼Œé€šè¿‡é‡åŒ–ç©ºé—´å¼‚è´¨æ€§æ¥æ•æ‰æµ·å†°ä¸å†°æ¶ä¹‹é—´çš„åŠ¨æ€è”ç³»ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å«æ˜Ÿè¡ç”Ÿçš„å†°å±‚å˜åŒ–çŸ©é˜µè¿›è¡ŒDelaunay triangulationæ„å»ºç©ºé—´å›¾ï¼Œå¹¶ç»“åˆå¹¿åº¦ä¼˜å…ˆæœç´¢(BFS)å’Œè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ(Monte Carlo simulations)å¯¹è”åŠ¨è·¯å¾„è¿›è¡Œç»Ÿè®¡éªŒè¯ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†éå±€éƒ¨ä¸”å…·æœ‰ç©ºé—´å¼‚è´¨æ€§çš„è€¦åˆæ¨¡å¼ï¼Œè¡¨æ˜æµ·å†°æµå¤±ä¼šå¼•å‘æˆ–æ”¾å¤§ä¸‹æ¸¸çš„å†°æ¶èåŒ–è¿‡ç¨‹ã€‚åˆ†æè¿›ä¸€æ­¥å±•ç¤ºäº†æµ·å†°é€€ç¼©å¦‚ä½•è·¨è¶Šæµ·æ´‹ç½‘æ ¼å¹¶å‘å†°æ¶æ¼”è¿›ï¼Œå»ºç«‹äº†äºŒè€…ä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚ä½œä¸ºé¦–ä¸ªè¿æ¥æµ·å†°é€€ç¼©ä¸å†°æ¶èåŒ–çš„æ–¹æ³•è®ºï¼ŒSpatial-Linkä¸ºæé«˜æµ·å¹³é¢ä¸Šå‡é¢„æµ‹çš„å‡†ç¡®æ€§ä»¥åŠåˆ¶å®šæ°”å€™é€‚åº”ç­–ç•¥æä¾›äº†å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨å·¥å…·ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.07036v1",
      "published_date": "2025-06-18 17:19:07 UTC",
      "updated_date": "2025-06-18 17:19:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:46.893441+00:00"
    },
    {
      "arxiv_id": "2506.15647v1",
      "title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement",
      "title_zh": "æ¢ç´¢ä¸åˆ©ç”¨å¤§æ¨ç†æ¨¡å‹çš„å†…åœ¨æ•ˆç‡ï¼šå®ç°è‡ªå¼•å¯¼å¼æ•ˆç‡å¢å¼º",
      "authors": [
        "Weixiang Zhao",
        "Jiahe Guo",
        "Yang Deng",
        "Xingyu Sui",
        "Yulin Hu",
        "Yanyan Zhao",
        "Wanxiang Che",
        "Bing Qin",
        "Tat-Seng Chua",
        "Ting Liu"
      ],
      "abstract": "Recent advancements in large reasoning models (LRMs) have significantly enhanced language models' capabilities in complex problem-solving by emulating human-like deliberative thinking. However, these models often exhibit overthinking (i.e., the generation of unnecessarily verbose and redundant content), which hinders efficiency and inflates inference cost. In this work, we explore the representational and behavioral origins of this inefficiency, revealing that LRMs inherently possess the capacity for more concise reasoning. Empirical analyses show that correct reasoning paths vary significantly in length, and the shortest correct responses often suffice, indicating untapped efficiency potential. Exploiting these findings, we propose two lightweight methods to enhance LRM efficiency. First, we introduce Efficiency Steering, a training-free activation steering technique that modulates reasoning behavior via a single direction in the model's representation space. Second, we develop Self-Rewarded Efficiency RL, a reinforcement learning framework that dynamically balances task accuracy and brevity by rewarding concise correct solutions. Extensive experiments on seven LRM backbones across multiple mathematical reasoning benchmarks demonstrate that our methods significantly reduce reasoning length while preserving or improving task performance. Our results highlight that reasoning efficiency can be improved by leveraging and guiding the intrinsic capabilities of existing models in a self-guided manner.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹æ¨ç†æ¨¡å‹(Large Reasoning Models, LRMs)ä¸­æ™®éå­˜åœ¨çš„â€œè¿‡æ€â€(overthinking)ç°è±¡å±•å¼€æ¢ç´¢ï¼Œå³æ¨¡å‹ç”Ÿæˆå†—ä½™å†…å®¹å¯¼è‡´æ¨ç†æˆæœ¬å¢åŠ çš„é—®é¢˜ã€‚é€šè¿‡å¯¹è¡¨å¾å’Œè¡Œä¸ºæºå¤´çš„åˆ†æï¼Œç ”ç©¶å‘ç°LRMså†…åœ¨å…·å¤‡æ›´ç®€æ´çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”æœ€çŸ­çš„æ­£ç¡®è·¯å¾„é€šå¸¸è¶³ä»¥è§£å†³å¤æ‚é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§è½»é‡çº§ä¼˜åŒ–æ–¹æ³•ï¼šä¸€æ˜¯æ•ˆç‡å¼•å¯¼(Efficiency Steering)ï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„æ¿€æ´»å¼•å¯¼æŠ€æœ¯ï¼Œé€šè¿‡è°ƒèŠ‚æ¨¡å‹è¡¨å¾ç©ºé—´ä¸­çš„å•ä¸€æ–¹å‘æ¥æ”¹å˜æ¨ç†è¡Œä¸ºï¼›äºŒæ˜¯è‡ªæˆ‘å¥–åŠ±æ•ˆç‡å¼ºåŒ–å­¦ä¹ (Self-Rewarded Efficiency RL)ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¡†æ¶åŠ¨æ€å¹³è¡¡ä»»åŠ¡å‡†ç¡®æ€§ä¸å›å¤çš„ç®€æ´åº¦ã€‚åœ¨ä¸ƒä¸ªLRMéª¨å¹²æ¨¡å‹å’Œå¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨æ˜¾è‘—ç¼©çŸ­æ¨ç†é•¿åº¦çš„åŒæ—¶ï¼Œæœ‰æ•ˆä¿æŒç”šè‡³æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¯ä»¥é€šè¿‡è‡ªæˆ‘å¼•å¯¼çš„æ–¹å¼ï¼Œåˆ©ç”¨å¹¶å¼•å¯¼ç°æœ‰æ¨¡å‹çš„å†…åœ¨æ½œèƒ½æ¥æå‡æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15647v1",
      "published_date": "2025-06-18 17:18:12 UTC",
      "updated_date": "2025-06-18 17:18:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:38.188849+00:00"
    },
    {
      "arxiv_id": "2506.15645v1",
      "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models",
      "title_zh": "æ­ç§˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰è´¨é‡æ‚–è®º",
      "authors": [
        "Shuo Xing",
        "Lanqing Guo",
        "Hongyuan Hua",
        "Seoyoung Lee",
        "Peiran Li",
        "Yufei Wang",
        "Zhangyang Wang",
        "Zhengzhong Tu"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¾“å…¥è§†è§‰è´¨é‡å¦‚ä½•å½±å“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models, MLLMs) çš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†ä¸€ä¸ªâ€œè§†è§‰è´¨é‡æ‚–è®ºâ€ (visual-quality paradox)ï¼Œå³äººç±»æ„ŸçŸ¥çš„å›¾åƒè´¨é‡æå‡å¹¶ä¸ç­‰åŒäºæ¨¡å‹ç†è§£èƒ½åŠ›çš„å¢å¼ºã€‚é€šè¿‡å¯¹å¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œä½œè€…å‘ç°æ¨¡å‹åœ¨å›¾åƒåç¦»äººç±»å®¡ç¾ä¿çœŸåº¦æ—¶åè€Œå¯èƒ½è¡¨ç°æ›´ä½³ï¼Œä¸”ç°æœ‰çš„ restoration pipelines éš¾ä»¥è°ƒå’Œè¿™ç§ç‰¹å®šçš„æ¨¡å‹åå¥½ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†è§†è§‰è´¨é‡æµ‹è¯•æ—¶è°ƒä¼˜ (Visual-Quality Test-Time Tuning, VQ-TTT)ï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§é€‚é…æ¨¡å—ï¼Œé€šè¿‡åœ¨å†»ç»“çš„ vision encoder å‰æ’å…¥å¯å­¦ä¹ çš„ low-rank kernel å¹¶ç»“åˆ LoRA å¾®è°ƒæµ…å±‚ç½‘ç»œæ¥è°ƒèŠ‚å›¾åƒç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­åŠ¨æ€è°ƒæ•´å›¾åƒï¼Œä½¿å…¶ä¸æ¨¡å‹åå¥½å¯¹é½ï¼Œåœ¨æ— éœ€å¤–éƒ¨æ¨¡å‹æˆ–é¢å¤–è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡äº†å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚è¿™ä¸€å‘ç°é‡æ–°å®šä¹‰äº† MLLMs è¯­å¢ƒä¸‹çš„ä¼˜è´¨è§†è§‰è¾“å…¥ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ AI ä½œä¸ºä¸»è¦æ•°æ®æ¶ˆè´¹è€…çš„æ—¶ä»£ï¼Œè‡ªé€‚åº”å›¾åƒå¤„ç†ç›¸è¾ƒäºé€šç”¨å›¾åƒä¿®å¤çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.15645v1",
      "published_date": "2025-06-18 17:14:07 UTC",
      "updated_date": "2025-06-18 17:14:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:33.585419+00:00"
    },
    {
      "arxiv_id": "2506.15639v1",
      "title": "The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy",
      "title_zh": "AI æ”¿ç­–æ¨¡å—ï¼šåŸ¹å…»è®¡ç®—æœºä¸“ä¸šå­¦ç”Ÿçš„äººå·¥æ™ºèƒ½ä¼¦ç†ä¸æ”¿ç­–èƒœä»»åŠ›",
      "authors": [
        "James Weichert",
        "Daniel Dunlap",
        "Mohammed Farghally",
        "Hoda Eldardiry"
      ],
      "abstract": "As artificial intelligence (AI) further embeds itself into many settings across personal and professional contexts, increasing attention must be paid not only to AI ethics, but also to the governance and regulation of AI technologies through AI policy. However, the prevailing post-secondary computing curriculum is currently ill-equipped to prepare future AI practitioners to confront increasing demands to implement abstract ethical principles and normative policy preferences into the design and development of AI systems. We believe that familiarity with the 'AI policy landscape' and the ability to translate ethical principles to practices will in the future constitute an important responsibility for even the most technically-focused AI engineers.\n  Toward preparing current computer science (CS) students for these new expectations, we developed an AI Policy Module to introduce discussions of AI policy into the CS curriculum. Building on a successful pilot in fall 2024, in this innovative practice full paper we present an updated and expanded version of the module, including a technical assignment on \"AI regulation\". We present the findings from our pilot of the AI Policy Module 2.0, evaluating student attitudes towards AI ethics and policy through pre- and post-module surveys. Following the module, students reported increased concern about the ethical impacts of AI technologies while also expressing greater confidence in their abilities to engage in discussions about AI regulation. Finally, we highlight the AI Regulation Assignment as an effective and engaging tool for exploring the limits of AI alignment and emphasizing the role of 'policy' in addressing ethical challenges.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹åœ¨ AI ethics ä¸æ²»ç†ç›‘ç®¡æ•™è‚²æ–¹é¢çš„ç¼ºå¤±ï¼Œå¼€å‘å¹¶å®æ–½äº† AI Policy Moduleï¼Œæ—¨åœ¨åŸ¹å…»å­¦ç”Ÿå°†æŠ½è±¡ä¼¦ç†åŸåˆ™è½¬åŒ–ä¸ºæŠ€æœ¯å®è·µçš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥åŒ…å« AI regulation æŠ€æœ¯ä½œä¸šçš„ 2.0 ç‰ˆæœ¬ï¼Œè¯¥æ¨¡å—ä¸ºå­¦ç”Ÿæä¾›äº†æ·±å…¥ç†è§£ AI policy æ™¯è§‚çš„æ¸ é“ã€‚è¯•ç‚¹è°ƒæŸ¥ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è¯¥è¯¾ç¨‹åï¼Œå­¦ç”Ÿå¯¹ AI æŠ€æœ¯ä¼¦ç†å½±å“çš„å…³æ³¨åº¦æ˜¾è‘—æå‡ï¼Œå¹¶åœ¨è®¨è®º AI regulation è®®é¢˜æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„ä¿¡å¿ƒã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº† AI Regulation Assignment åœ¨æ¢ç´¢ AI alignment å±€é™æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒäº†æ”¿ç­–æ•™è‚²åœ¨åº”å¯¹æŠ€æœ¯ä¼¦ç†æŒ‘æˆ˜ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä¸ºåŸ¹å…»å…·å¤‡è·¨å­¦ç§‘ç´ å…»çš„æœªæ¥ AI å·¥ç¨‹å¸ˆå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15639v1",
      "published_date": "2025-06-18 17:09:58 UTC",
      "updated_date": "2025-06-18 17:09:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:38.288347+00:00"
    },
    {
      "arxiv_id": "2506.15629v1",
      "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability",
      "title_zh": "ç»“åˆæŒ‡ä»¤éµå¾ªèƒ½åŠ›é‡æ–°å®¡è§†å¤§è¯­è¨€æ¨¡å‹çš„ç»„åˆæ³›åŒ–èƒ½åŠ›",
      "authors": [
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "In generative commonsense reasoning tasks such as CommonGen, generative large language models (LLMs) compose sentences that include all given concepts. However, when focusing on instruction-following capabilities, if a prompt specifies a concept order, LLMs must generate sentences that adhere to the specified order. To address this, we propose Ordered CommonGen, a benchmark designed to evaluate the compositional generalization and instruction-following abilities of LLMs. This benchmark measures ordered coverage to assess whether concepts are generated in the specified order, enabling a simultaneous evaluation of both abilities. We conducted a comprehensive analysis using 36 LLMs and found that, while LLMs generally understand the intent of instructions, biases toward specific concept order patterns often lead to low-diversity outputs or identical results even when the concept order is altered. Moreover, even the most instruction-compliant LLM achieved only about 75% ordered coverage, highlighting the need for improvements in both instruction-following and compositional generalization capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è€ƒè™‘æŒ‡ä»¤éµå¾ª(Instruction Following)èƒ½åŠ›æ—¶çš„ç»„åˆæ³›åŒ–(Compositional Generalization)è¡¨ç°ã€‚ä½œè€…æå‡ºäº†Ordered CommonGenåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡è¦æ±‚æ¨¡å‹æŒ‰ç…§æŒ‡å®šçš„æ¦‚å¿µé¡ºåºç”Ÿæˆå¥å­ï¼Œæ¥åŒæ­¥è¯„ä¼°å…¶ç»„åˆæ³›åŒ–ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚è¯¥åŸºå‡†å¼•å…¥äº†æœ‰åºè¦†ç›–ç‡(Ordered Coverage)æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¨¡å‹å¯¹ç‰¹å®šé¡ºåºæŒ‡ä»¤çš„æ‰§è¡Œç¨‹åº¦ã€‚é€šè¿‡å¯¹36ä¸ªå¤§è¯­è¨€æ¨¡å‹çš„å…¨é¢åˆ†æï¼Œç ”ç©¶å‘ç°LLMsè™½ç„¶èƒ½ç†è§£æŒ‡ä»¤æ„å›¾ï¼Œä½†å¾€å¾€å—é™äºç‰¹å®šçš„æ¦‚å¿µæ’åºåå¥½ï¼Œå¯¼è‡´å³ä¾¿åœ¨æ›´æ”¹é¡ºåºæŒ‡ä»¤æ—¶ä¹Ÿå€¾å‘äºç”Ÿæˆä½å¤šæ ·æ€§æˆ–ç›¸åŒçš„å†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¡¨ç°æœ€ä¼˜çš„æ¨¡å‹ä¹Ÿä»…èƒ½è¾¾åˆ°çº¦75%çš„æœ‰åºè¦†ç›–ç‡ï¼Œè¿™åæ˜ å‡ºå½“å‰æ¨¡å‹åœ¨åŒæ—¶å¤„ç†å¤æ‚çº¦æŸæŒ‡ä»¤ä¸å®ç°é«˜è´¨é‡ç»„åˆæ³›åŒ–æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2506.15629v1",
      "published_date": "2025-06-18 17:00:54 UTC",
      "updated_date": "2025-06-18 17:00:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:56.389597+00:00"
    },
    {
      "arxiv_id": "2506.15626v2",
      "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction",
      "title_zh": "åŸºäº MRI çš„ BrainAGE è”é‚¦å­¦ä¹ ï¼šä¸€é¡¹å…³äºå’ä¸­ååŠŸèƒ½é¢„åé¢„æµ‹çš„å¤šä¸­å¿ƒç ”ç©¶",
      "authors": [
        "Vincent Roca",
        "Marc Tommasi",
        "Paul Andrey",
        "AurÃ©lien Bellet",
        "Markus D. Schirmer",
        "Hilde Henon",
        "Laurent Puy",
        "Julien Ramon",
        "GrÃ©gory Kuchcinski",
        "Martin Bretzner",
        "Renaud Lopes"
      ],
      "abstract": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a neuroimaging biomarker reflecting brain health. However, training robust BrainAGE models requires large datasets, often restricted by privacy concerns. This study evaluates the performance of federated learning (FL) for BrainAGE estimation in ischemic stroke patients treated with mechanical thrombectomy, and investigates its association with clinical phenotypes and functional outcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients across 16 hospital centers. We implemented standard machine learning and deep learning models for BrainAGE estimates under three data management strategies: centralized learning (pooled data), FL (local training at each site), and single-site learning. We reported prediction errors and examined associations between BrainAGE and vascular risk factors (e.g., diabetes mellitus, hypertension, smoking), as well as functional outcomes at three months post-stroke. Logistic regression evaluated BrainAGE's predictive value for these outcomes, adjusting for age, sex, vascular risk factors, stroke severity, time between MRI and arterial puncture, prior intravenous thrombolysis, and recanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate predictions, FL consistently outperformed single-site models. BrainAGE was significantly higher in patients with diabetes mellitus across all models. Comparisons between patients with good and poor functional outcomes, and multivariate predictions of these outcomes showed the significance of the association between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data centralization. The strong association between BrainAGE, vascular risk factors, and post-stroke recovery highlights its potential for prognostic modeling in stroke care.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨æ¥å—æœºæ¢°å–æ “æ²»ç–—çš„ç¼ºè¡€æ€§è„‘å’ä¸­æ‚£è€…ä¸­ï¼Œåº”ç”¨è”é‚¦å­¦ä¹ (Federated Learning)æŠ€æœ¯è¿›è¡Œè„‘é¢„æµ‹å¹´é¾„å·®(BrainAGE)ä¼°è®¡çš„å¯è¡Œæ€§åŠå…¶ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¥è‡ª16ä¸ªåŒ»ç–—ä¸­å¿ƒçš„1674åå’ä¸­æ‚£è€…çš„FLAIRç£å…±æŒ¯æˆåƒæ•°æ®ï¼Œå¯¹æ¯”äº†é›†ä¸­å¼å­¦ä¹ ã€è”é‚¦å­¦ä¹ (FL)åŠå•ä¸­å¿ƒå­¦ä¹ åœ¨BrainAGEæ¨¡å‹è®­ç»ƒä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶é›†ä¸­å¼å­¦ä¹ çš„é¢„æµ‹ç²¾åº¦æœ€é«˜ï¼Œä½†è”é‚¦å­¦ä¹ åœ¨ä¸æ±‡èšåŸå§‹æ•°æ®çš„å‰æä¸‹è¡¨ç°æŒç»­ä¼˜äºå•ä¸­å¿ƒæ¨¡å‹ï¼Œæœ‰æ•ˆè§£å†³äº†éšç§ä¿æŠ¤å¸¦æ¥çš„æ•°æ®å­¤å²›é—®é¢˜ã€‚ç ”ç©¶è¿˜å‘ç°BrainAGEä¸ç³–å°¿ç—…ç­‰è¡€ç®¡é£é™©å› ç´ æ˜¾è‘—ç›¸å…³ï¼Œä¸”é€»è¾‘å›å½’åˆ†æè¯å®äº†å…¶å¯¹å’ä¸­åä¸‰ä¸ªæœˆåŠŸèƒ½é¢„å(functional outcome)å…·æœ‰é‡è¦çš„é¢„æµ‹æ„ä¹‰ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è”é‚¦å­¦ä¹ åœ¨å¤šä¸­å¿ƒç¥ç»å½±åƒç ”ç©¶ä¸­çš„ç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†BrainAGEä½œä¸ºè¯„ä¼°å’ä¸­åº·å¤æ½œåŠ›ç”Ÿç‰©æ ‡å¿—ç‰©çš„ä¸´åºŠä»·å€¼ï¼Œä¸ºæå‡å’ä¸­æŠ¤ç†çš„é¢„åå»ºæ¨¡æ°´å¹³æä¾›äº†æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15626v2",
      "published_date": "2025-06-18 16:56:44 UTC",
      "updated_date": "2025-06-19 10:16:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:19.646843+00:00"
    },
    {
      "arxiv_id": "2506.15624v1",
      "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games",
      "title_zh": "åŠ¨æ€è·¯ç”±åšå¼ˆä¸­çŠ¶æ€è¡¨ç¤ºå¯¹ LLM æ™ºèƒ½ä½“è¡Œä¸ºçš„å½±å“",
      "authors": [
        "Lyle Goodyear",
        "Rachel Guo",
        "Ramesh Johari"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic settings, but their stateless nature necessitates creating a natural language representation of history. We present a unifying framework for systematically constructing natural language \"state\" representations for prompting LLM agents in repeated multi-agent games. Previous work on games with LLM agents has taken an ad hoc approach to encoding game history, which not only obscures the impact of state representation on agents' behavior, but also limits comparability between studies. Our framework addresses these gaps by characterizing methods of state representation along three axes: action informativeness (i.e., the extent to which the state representation captures actions played); reward informativeness (i.e., the extent to which the state representation describes rewards obtained); and prompting style (or natural language compression, i.e., the extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it admits a simple equilibrium both in theory and in human subject experiments \\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find that there are key dependencies of LLM agent behavior on the natural language state representation. In particular, we observe that representations which provide agents with (1) summarized, rather than complete, natural language representations of past history; (2) information about regrets, rather than raw payoffs; and (3) limited information about others' actions lead to behavior that more closely matches game theoretic equilibrium predictions, and with more stable game play by the agents. By contrast, other representations can exhibit either large deviations from equilibrium, higher variation in dynamic game play over time, or both.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨åŠ¨æ€è·¯ç”±åšå¼ˆ (Dynamic Routing Games) ä¸­ï¼ŒçŠ¶æ€è¡¨ç¤º (State Representation) å¯¹å¤§è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“è¡Œä¸ºçš„å½±å“ã€‚ç”±äº LLM å…·æœ‰æ— çŠ¶æ€ç‰¹æ€§ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä»åŠ¨ä½œä¿¡æ¯ä¸°å¯Œåº¦ã€å¥–åŠ±ä¿¡æ¯ä¸°å¯Œåº¦ä»¥åŠæç¤ºé£æ ¼ (Prompting Style) ä¸‰ä¸ªç»´åº¦ç³»ç»ŸåŒ–åœ°æ„å»ºè‡ªç„¶è¯­è¨€çŠ¶æ€ã€‚é€šè¿‡åœ¨åŠ¨æ€è‡ªç§è·¯ç”±åšå¼ˆ (Dynamic Selfish Routing Game) ä¸­çš„å®éªŒå‘ç°ï¼Œæä¾›æ€»ç»“æ€§ (Summarized) å†å²è®°å½•ã€åæ‚”å€¼ (Regrets) è€ŒéåŸå§‹æ”¶ç›Š (Raw Payoffs)ï¼Œä»¥åŠé™åˆ¶ä»–äººåŠ¨ä½œä¿¡æ¯çš„è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½ä½¿æ™ºèƒ½ä½“è¡Œä¸ºæ›´æ¥è¿‘åšå¼ˆè®ºçš„å‡è¡¡ (Equilibrium) é¢„æµ‹ï¼Œå¹¶è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å®šæ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–è¡¨ç¤ºæ–¹å¼ä¼šå¯¼è‡´è¡Œä¸ºæ˜¾è‘—åç¦»å‡è¡¡æˆ–åœ¨åŠ¨æ€åšå¼ˆä¸­äº§ç”Ÿå‰§çƒˆæ³¢åŠ¨ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è‡ªç„¶è¯­è¨€çŠ¶æ€çš„è®¾è®¡å¯¹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ LLM å†³ç­–è¡Œä¸ºçš„å…³é”®è°ƒæ§ä½œç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages, 20 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15624v1",
      "published_date": "2025-06-18 16:53:38 UTC",
      "updated_date": "2025-06-18 16:53:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:41:55.803493+00:00"
    },
    {
      "arxiv_id": "2506.15620v1",
      "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair Classification",
      "title_zh": "GFLCï¼šé¢å‘å…¬å¹³åˆ†ç±»çš„åŸºäºå›¾çš„å…¬å¹³æ„ŸçŸ¥æ ‡ç­¾çº æ­£",
      "authors": [
        "Modar Sulaiman",
        "Kallol Roy"
      ],
      "abstract": "Fairness in machine learning (ML) has a critical importance for building trustworthy machine learning system as artificial intelligence (AI) systems increasingly impact various aspects of society, including healthcare decisions and legal judgments. Moreover, numerous studies demonstrate evidence of unfair outcomes in ML and the need for more robust fairness-aware methods. However, the data we use to train and develop debiasing techniques often contains biased and noisy labels. As a result, the label bias in the training data affects model performance and misrepresents the fairness of classifiers during testing. To tackle this problem, our paper presents Graph-based Fairness-aware Label Correction (GFLC), an efficient method for correcting label noise while preserving demographic parity in datasets. In particular, our approach combines three key components: prediction confidence measure, graph-based regularization through Ricci-flow-optimized graph Laplacians, and explicit demographic parity incentives. Our experimental findings show the effectiveness of our proposed approach and show significant improvements in the trade-off between performance and fairness metrics compared to the baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ ä¸­è®­ç»ƒæ•°æ®å­˜åœ¨çš„æœ‰åå’Œå™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œæå‡ºäº† GFLCï¼ˆGraph-based Fairness-aware Label Correctionï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨çº æ­£æ ‡ç­¾å™ªå£°çš„åŒæ—¶ç»´æŠ¤æ•°æ®é›†çš„äººå£ç»Ÿè®¡å­¦å¹³ä»·ï¼ˆdemographic parityï¼‰ã€‚è¯¥æ¡†æ¶ç»“åˆäº†é¢„æµ‹ç½®ä¿¡åº¦åº¦é‡ï¼ˆprediction confidence measureï¼‰ã€åŸºäº Ricci-flow ä¼˜åŒ–çš„å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼ˆgraph Laplaciansï¼‰çš„å›¾æ­£åˆ™åŒ–ï¼Œä»¥åŠæ˜¾å¼çš„äººå£ç»Ÿè®¡å­¦å¹³ä»·æ¿€åŠ±æœºåˆ¶ã€‚é€šè¿‡è¿™ä¸‰ä¸ªç»„ä»¶çš„ååŒä½œç”¨ï¼ŒGFLC èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶ä¿®å¤å—æŸæ ‡ç­¾ï¼Œä»è€Œå‡å°‘æ ‡ç­¾åè§å¯¹æ¨¡å‹è®­ç»ƒå’Œå…¬å¹³æ€§æµ‹è¯•çš„è´Ÿé¢å½±å“ã€‚å®éªŒç»“æœè¯æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒGFLC åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸å…¬å¹³æ€§æŒ‡æ ‡çš„æƒè¡¡ï¼ˆtrade-offï¼‰æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œä¸ºæ„å»ºæ›´å…¬å¹³ã€æ›´ç¨³å¥çš„å¯ä¿¡äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15620v1",
      "published_date": "2025-06-18 16:51:26 UTC",
      "updated_date": "2025-06-18 16:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:17.305975+00:00"
    },
    {
      "arxiv_id": "2506.15617v1",
      "title": "The Compositional Architecture of Regret in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åæ‚”æœºåˆ¶çš„æ„æˆå¼æ¶æ„",
      "authors": [
        "Xiangxiang Cui",
        "Shu Yang",
        "Tianjin Huang",
        "Wanyu Lin",
        "Lijie Hu",
        "Di Wang"
      ],
      "abstract": "Regret in Large Language Models refers to their explicit regret expression when presented with evidence contradicting their previously generated misinformation. Studying the regret mechanism is crucial for enhancing model reliability and helps in revealing how cognition is coded in neural networks. To understand this mechanism, we need to first identify regret expressions in model outputs, then analyze their internal representation. This analysis requires examining the model's hidden states, where information processing occurs at the neuron level. However, this faces three key challenges: (1) the absence of specialized datasets capturing regret expressions, (2) the lack of metrics to find the optimal regret representation layer, and (3) the lack of metrics for identifying and analyzing regret neurons. Addressing these limitations, we propose: (1) a workflow for constructing a comprehensive regret dataset through strategically designed prompting scenarios, (2) the Supervised Compression-Decoupling Index (S-CDI) metric to identify optimal regret representation layers, and (3) the Regret Dominance Score (RDS) metric to identify regret neurons and the Group Impact Coefficient (GIC) to analyze activation patterns. Our experimental results successfully identified the optimal regret representation layer using the S-CDI metric, which significantly enhanced performance in probe classification experiments. Additionally, we discovered an M-shaped decoupling pattern across model layers, revealing how information processing alternates between coupling and decoupling phases. Through the RDS metric, we categorized neurons into three distinct functional groups: regret neurons, non-regret neurons, and dual neurons.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„æ‚”æ¨(Regret)æœºåˆ¶ï¼Œå³æ¨¡å‹åœ¨é¢å¯¹åé©³å…¶å…ˆå‰é”™è¯¯ä¿¡æ¯çš„è¯æ®æ—¶æ˜¾å¼è¡¨è¾¾æ‚”æ¨çš„ç°è±¡ã€‚ä¸ºäº†å…‹æœç›¸å…³æ•°æ®é›†åŒ®ä¹å’Œç¼ºä¹è¯„ä¼°æŒ‡æ ‡çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€å¥—æ„å»ºå…¨é¢æ‚”æ¨æ•°æ®é›†çš„å·¥ä½œæµï¼Œå¹¶å¼•å…¥äº†ç›‘ç£å‹ç¼©-è§£è€¦æŒ‡æ•°(S-CDI)ä»¥è¯†åˆ«æœ€ä½³çš„æ‚”æ¨è¡¨ç¤ºå±‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†æ‚”æ¨ä¸»å¯¼å¾—åˆ†(RDS)æŒ‡æ ‡æ¥å®šä½ç‰¹å®šç¥ç»å…ƒï¼Œä»¥åŠç¾¤ä½“å½±å“ç³»æ•°(GIC)æ¥åˆ†æå…¶æ¿€æ´»æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨S-CDIç¡®å®šçš„æœ€ä½³è¡¨ç¤ºå±‚æ˜¾è‘—æå‡äº†æ¢é’ˆåˆ†ç±»å®éªŒçš„æ€§èƒ½ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹å±‚é—´å­˜åœ¨ä¸€ç§Må‹è§£è€¦(M-shaped decoupling)æ¨¡å¼ã€‚é€šè¿‡RDSæŒ‡æ ‡ï¼Œç ”ç©¶æˆåŠŸå°†ç¥ç»å…ƒåˆ’åˆ†ä¸ºæ‚”æ¨ç¥ç»å…ƒã€éæ‚”æ¨ç¥ç»å…ƒå’ŒåŒé‡ç¥ç»å…ƒä¸‰ç±»åŠŸèƒ½ç»„ã€‚è¯¥ç ”ç©¶æ·±å…¥æ­ç¤ºäº†ç¥ç»å…ƒå±‚é¢å¦‚ä½•å¤„ç†ä¿¡æ¯ï¼Œä¸ºå¢å¼ºæ¨¡å‹å¯é æ€§åŠç†è§£ç¥ç»ç½‘ç»œçš„è®¤çŸ¥ç¼–ç å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "https://arxiv.org/pdf/2506.15617v1",
      "published_date": "2025-06-18 16:50:34 UTC",
      "updated_date": "2025-06-18 16:50:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:08.097983+00:00"
    },
    {
      "arxiv_id": "2506.15606v3",
      "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
      "title_zh": "LoXï¼šé€šè¿‡ä½ç§©å¤–æ¨å¢å¼ºå¤§è¯­è¨€æ¨¡å‹å¯¹æŠ—å¾®è°ƒçš„å®‰å…¨æ€§é²æ£’æ€§",
      "authors": [
        "Gabriel J. Perin",
        "Runjin Chen",
        "Xuxi Chen",
        "Nina S. T. Hirata",
        "Zhangyang Wang",
        "Junyuan Hong"
      ],
      "abstract": "Large Language Models (LLMs) have become indispensable in real-world applications. However, their widespread adoption raises significant safety concerns, particularly in responding to socially harmful questions. Despite substantial efforts to improve model safety through alignment, aligned models can still have their safety protections undermined by subsequent fine-tuning - even when the additional training data appears benign. In this paper, we empirically demonstrate that this vulnerability stems from the sensitivity of safety-critical low-rank subspaces in LLM parameters to fine-tuning. Building on this insight, we propose a novel training-free method, termed Low-Rank Extrapolation (LoX), to enhance safety robustness by extrapolating the safety subspace of an aligned LLM. Our experimental results confirm the effectiveness of LoX, demonstrating significant improvements in robustness against both benign and malicious fine-tuning attacks while preserving the model's adaptability to new tasks. For instance, LoX leads to 11% to 54% absolute reductions in attack success rates (ASR) facing benign or malicious fine-tuning attacks. By investigating the ASR landscape of parameters, we attribute the success of LoX to that the extrapolation moves LLM parameters to a flatter zone, thereby less sensitive to perturbations. The code is available at github.com/VITA-Group/LoX.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç»è¿‡å¾®è°ƒï¼ˆå³ä½¿æ˜¯è‰¯æ€§æ•°æ®ï¼‰åå®‰å…¨å¯¹é½æ˜“è¢«ç ´åçš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ç ”ç©¶é€šè¿‡å®è¯åˆ†æå‘ç°ï¼Œè¿™ä¸€å®‰å…¨æ€§æ¼æ´æºäºLLMå‚æ•°ä¸­å®‰å…¨å…³é”®çš„ä½ç§©å­ç©ºé—´(low-rank subspaces)å¯¹å¾®è°ƒè¿‡ç¨‹å…·æœ‰æé«˜çš„æ•æ„Ÿæ€§ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•LoX (Low-Rank Extrapolation)ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¤–æ¨å·²å¯¹é½æ¨¡å‹çš„å®‰å…¨å­ç©ºé—´æ¥æ˜¾è‘—å¢å¼ºå…¶å®‰å…¨é²æ£’æ€§ã€‚å®éªŒè¯æ˜ï¼ŒLoXåœ¨é¢å¯¹è‰¯æ€§æˆ–æ¶æ„å¾®è°ƒæ”»å‡»æ—¶ï¼Œèƒ½å°†æ”»å‡»æˆåŠŸç‡(ASR)ç»å¯¹é™ä½11%è‡³54%ï¼ŒåŒæ—¶æœ‰æ•ˆä¿ç•™æ¨¡å‹å¯¹æ–°ä»»åŠ¡çš„é€‚åº”èƒ½åŠ›ã€‚é€šè¿‡å¯¹å‚æ•°æ™¯è§‚çš„è°ƒæŸ¥ï¼Œç ”ç©¶æ­ç¤ºLoXä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºå¤–æ¨æ“ä½œå°†æ¨¡å‹å‚æ•°å¼•å¯¼è‡³å¯¹æ‰°åŠ¨æ›´ä¸æ•æ„Ÿçš„å¹³å¦åŒºåŸŸ(flatter zone)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15606v3",
      "published_date": "2025-06-18 16:30:02 UTC",
      "updated_date": "2025-07-25 18:57:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:34.434272+00:00"
    },
    {
      "arxiv_id": "2506.15598v1",
      "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns",
      "title_zh": "ä»æ¨¡å‹åˆ°è¯¾å ‚ï¼šè€ƒè™‘å™äº‹ä¸éš¾åº¦å› ç´ çš„è‘¡è„ç‰™è¯­ç”Ÿæˆå¼å¤šé€‰é¢˜è¯„ä¼°",
      "authors": [
        "Bernardo Leite",
        "Henrique Lopes Cardoso",
        "Pedro Pinto",
        "Abel Ferreira",
        "LuÃ­s Abreu",
        "Isabel Rangel",
        "Sandra Monteiro"
      ],
      "abstract": "While MCQs are valuable for learning and evaluation, manually creating them with varying difficulty levels and targeted reading skills remains a time-consuming and costly task. Recent advances in generative AI provide an opportunity to automate MCQ generation efficiently. However, assessing the actual quality and reliability of generated MCQs has received limited attention -- particularly regarding cases where generation fails. This aspect becomes particularly important when the generated MCQs are meant to be applied in real-world settings. Additionally, most MCQ generation studies focus on English, leaving other languages underexplored. This paper investigates the capabilities of current generative models in producing MCQs for reading comprehension in Portuguese, a morphologically rich language. Our study focuses on generating MCQs that align with curriculum-relevant narrative elements and span different difficulty levels. We evaluate these MCQs through expert review and by analyzing the psychometric properties extracted from student responses to assess their suitability for elementary school students. Our results show that current models can generate MCQs of comparable quality to human-authored ones. However, we identify issues related to semantic clarity and answerability. Also, challenges remain in generating distractors that engage students and meet established criteria for high-quality MCQ option design.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)ä¸ºè‘¡è„ç‰™è¯­é˜…è¯»ç†è§£è‡ªåŠ¨ç”Ÿæˆå¤šé€‰é¢˜(MCQs)çš„èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨å™äº‹å…ƒç´ å’Œä¸åŒéš¾åº¦å±‚çº§çš„å¯¹é½ã€‚é’ˆå¯¹æ‰‹åŠ¨åˆ›å»ºé«˜è´¨é‡é¢˜ç›®è€—æ—¶è´¹åŠ›çš„é—®é¢˜ï¼Œç ”ç©¶è€…é€šè¿‡ä¸“å®¶è¯„å®¡ä»¥åŠåˆ†æå°å­¦ç”Ÿå®é™…ç­”é¢˜çš„å¿ƒç†æµ‹é‡å­¦(Psychometric properties)ç‰¹å¾ï¼Œè¯„ä¼°äº†ç”Ÿæˆé¢˜ç›®åœ¨çœŸå®æ•™å­¦ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹ç”Ÿæˆçš„é¢˜ç›®åœ¨æ•´ä½“è´¨é‡ä¸Šå·²èƒ½ä¸äººå·¥ç¼–å†™çš„é¢˜ç›®ç›¸åª²ç¾ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹ŸæŒ‡å‡ºç”Ÿæˆé¢˜ç›®åœ¨è¯­ä¹‰æ¸…æ™°åº¦å’Œå¯å›ç­”æ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œä¸”åœ¨è®¾è®¡èƒ½æœ‰æ•ˆå¸å¼•å­¦ç”Ÿå¹¶ç¬¦åˆé«˜æ ‡å‡†å‡†åˆ™çš„å¹²æ‰°é¡¹(Distractors)æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶ä¸ºè‘¡è„ç‰™è¯­è¿™ä¸€å½¢æ€ä¸°å¯Œè¯­è¨€çš„è‡ªåŠ¨åŒ–è¯„ä¼°æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶æ­ç¤ºäº†æ¨¡å‹åœ¨å®é™…æ•™è‚²åœºæ™¯åº”ç”¨ä¸­ä»éœ€æ”¹è¿›çš„å…³é”®æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This is a preprint version of the manuscript currently under review at an international journal",
      "pdf_url": "https://arxiv.org/pdf/2506.15598v1",
      "published_date": "2025-06-18 16:19:46 UTC",
      "updated_date": "2025-06-18 16:19:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:36.728151+00:00"
    },
    {
      "arxiv_id": "2506.15594v1",
      "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts",
      "title_zh": "WikiMixQAï¼šé¢å‘è¡¨æ ¼ä¸å›¾è¡¨é—®ç­”çš„å¤šæ¨¡æ€åŸºå‡†",
      "authors": [
        "Negar Foroutan",
        "Angelika Romanou",
        "Matin Ansaripour",
        "Julian Martin Eisenschlos",
        "Karl Aberer",
        "RÃ©mi Lebret"
      ],
      "abstract": "Documents are fundamental to preserving and disseminating information, often incorporating complex layouts, tables, and charts that pose significant challenges for automatic document understanding (DU). While vision-language large models (VLLMs) have demonstrated improvements across various tasks, their effectiveness in processing long-context vision inputs remains unclear. This paper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice questions (MCQs) designed to evaluate cross-modal reasoning over tables and charts extracted from 4,000 Wikipedia pages spanning seven distinct topics. Unlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring models to synthesize information from multiple modalities. We evaluate 12 state-of-the-art vision-language models, revealing that while proprietary models achieve ~70% accuracy when provided with direct context, their performance deteriorates significantly when retrieval from long documents is required. Among these, GPT-4-o is the only model exceeding 50% accuracy in this setting, whereas open-source models perform considerably worse, with a maximum accuracy of 27%. These findings underscore the challenges of long-context, multi-modal reasoning and establish WikiMixQA as a crucial benchmark for advancing document understanding research.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†WikiMixQAï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«1,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜(MCQs)çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€å¤§æ¨¡å‹(VLLMs)åœ¨å¤„ç†è¡¨æ ¼(Tables)å’Œå›¾è¡¨(Charts)æ—¶çš„è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†ä»4,000ä¸ªç»´åŸºç™¾ç§‘é¡µé¢ä¸­æå–ç´ æï¼Œå¼ºè°ƒæ¨¡å‹å¿…é¡»ç»¼åˆå¤„ç†å¤šç§æ¨¡æ€çš„ä¿¡æ¯ä»¥è§£å†³å¤æ‚çš„æ–‡æ¡£ç†è§£(Document Understanding)ä»»åŠ¡ã€‚å®éªŒè¯„ä¼°äº†12ç§æœ€å…ˆè¿›çš„VLLMsï¼Œå‘ç°è™½ç„¶é—­æºæ¨¡å‹åœ¨ç›´æ¥ä¸Šä¸‹æ–‡ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨é•¿æ–‡æ¡£æ£€ç´¢åœºæ™¯ä¸­æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚GPT-4oæ˜¯é•¿ä¸Šä¸‹æ–‡è®¾ç½®ä¸‹å”¯ä¸€å‡†ç¡®ç‡è¶…è¿‡50%çš„æ¨¡å‹ï¼Œè€Œå¼€æºæ¨¡å‹çš„æœ€é«˜å‡†ç¡®ç‡ä»…ä¸º27%ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å°†WikiMixQAç¡®ç«‹ä¸ºæ¨è¿›è‡ªåŠ¨æ–‡æ¡£ç†è§£ç ”ç©¶çš„é‡è¦åŸºå‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2506.15594v1",
      "published_date": "2025-06-18 16:09:18 UTC",
      "updated_date": "2025-06-18 16:09:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:47.634342+00:00"
    },
    {
      "arxiv_id": "2506.15591v3",
      "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution",
      "title_zh": "é¢å‘ç»†èŠ‚ä¸°å¯Œä¸”æ—¶åºä¸€è‡´è§†é¢‘è¶…åˆ†è¾¨ç‡çš„ä¸€æ­¥æ‰©æ•£",
      "authors": [
        "Yujing Sun",
        "Lingchen Sun",
        "Shuaizheng Liu",
        "Rongyuan Wu",
        "Zhengqiang Zhang",
        "Lei Zhang"
      ],
      "abstract": "It is a challenging problem to reproduce rich spatial details while maintaining temporal consistency in real-world video super-resolution (Real-VSR), especially when we leverage pre-trained generative models such as stable diffusion (SD) for realistic details synthesis. Existing SD-based Real-VSR methods often compromise spatial details for temporal coherence, resulting in suboptimal visual quality. We argue that the key lies in how to effectively extract the degradation-robust temporal consistency priors from the low-quality (LQ) input video and enhance the video details while maintaining the extracted consistency priors. To achieve this, we propose a Dual LoRA Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion model, achieving realistic frame details and temporal consistency simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module to aggregate complementary information across frames, and train a Consistency-LoRA (C-LoRA) to learn robust temporal representations from degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with the temporal space defined by C-LoRA to keep temporal coherence. The two phases alternate iteratively for optimization, collaboratively delivering consistent and detail-rich outputs. During inference, the two LoRA branches are merged into the SD model, allowing efficient and high-quality video restoration in a single diffusion step. Experiments show that DLoRAL achieves strong performance in both accuracy and speed. Code and models are available at https://github.com/yjsunnn/DLoRAL.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº† DLoRAL èŒƒå¼ï¼Œä¸€ç§åŸºäº Stable Diffusion (SD) çš„å•æ­¥æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³çœŸå®åœºæ™¯è§†é¢‘è¶…åˆ†è¾¨ç‡ (Real-VSR) ä¸­ç©ºé—´ç»†èŠ‚ä¸æ—¶é—´ä¸€è‡´æ€§ (temporal consistency) éš¾ä»¥å…¼é¡¾çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†è·¨å¸§æ£€ç´¢ (Cross-Frame Retrieval, CFR) æ¨¡å—æ¥èšåˆå¸§é—´çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨ Consistency-LoRA (C-LoRA) ä»é€€åŒ–è§†é¢‘ä¸­å­¦ä¹ é²æ£’çš„æ—¶é—´è¡¨å¾ã€‚åœ¨ä¿æŒæ—¶é—´ä¸€è‡´æ€§çš„åŸºç¡€ä¸Šï¼ŒDetail-LoRA (D-LoRA) è¢«ç”¨äºå¢å¼ºç©ºé—´ç»†èŠ‚ï¼Œå¹¶ç¡®ä¿å…¶ä¸ C-LoRA å®šä¹‰çš„æ—¶é—´ç©ºé—´å¯¹é½ä»¥ç»´æŒè§†é¢‘è¿è´¯æ€§ã€‚è¿™ä¸¤ä¸ªæ¨¡å—é€šè¿‡äº¤æ›¿è¿­ä»£è¿›è¡Œä¼˜åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†é˜¶æ®µå°† LoRA åˆ†æ”¯åˆå¹¶ï¼Œå®ç°ä»…éœ€å•æ­¥æ‰©æ•£çš„é«˜è´¨é‡è§†é¢‘ä¿®å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDLoRAL åœ¨è§†è§‰è´¨é‡ã€å‡†ç¡®æ€§ä»¥åŠå¤„ç†é€Ÿåº¦æ–¹é¢å‡å–å¾—äº†æ˜¾è‘—æå‡ï¼Œä¸ºç”Ÿæˆå¼è§†é¢‘å¢å¼ºæä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by Neurips2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15591v3",
      "published_date": "2025-06-18 16:06:30 UTC",
      "updated_date": "2025-10-22 11:35:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:43.015580+00:00"
    },
    {
      "arxiv_id": "2506.15567v3",
      "title": "Intelligent Assistants for the Semiconductor Failure Analysis with LLM-Based Planning Agents",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹è§„åˆ’æ™ºèƒ½ä½“çš„åŠå¯¼ä½“å¤±æ•ˆåˆ†ææ™ºèƒ½åŠ©æ‰‹",
      "authors": [
        "Aline Dobrovsky",
        "Konstantin Schekotihin",
        "Christian Burmer"
      ],
      "abstract": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process. The integration of AI components within the computational infrastructure of FA labs has the potential to automate a variety of tasks, including the detection of non-conformities in images, the retrieval of analogous cases from diverse data sources, and the generation of reports from annotated images. However, as the number of deployed AI models increases, the challenge lies in orchestrating these components into cohesive and efficient workflows that seamlessly integrate with the FA process.\n  This paper investigates the design and implementation of an agentic AI system for semiconductor FA using a Large Language Model (LLM)-based Planning Agent (LPA). The LPA integrates LLMs with advanced planning capabilities and external tool utilization, allowing autonomous processing of complex queries, retrieval of relevant data from external systems, and generation of human-readable responses. The evaluation results demonstrate the agent's operational effectiveness and reliability in supporting FA tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠå¯¼ä½“æ•…éšœåˆ†æ(Failure Analysis, FA)è¿‡ç¨‹ä¸­å› AIç»„ä»¶å¢å¤šè€Œå¯¼è‡´çš„æµç¨‹ç¼–æ’æŒ‘æˆ˜ï¼Œæå‡ºå¹¶å®ç°äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(Large Language Model, LLM)è§„åˆ’æ™ºèƒ½ä½“(Planning Agent, LPA)çš„æ™ºèƒ½ç³»ç»Ÿã€‚è¯¥LPAé€šè¿‡å°†LLMä¸é«˜çº§è§„åˆ’èƒ½åŠ›åŠå¤–éƒ¨å·¥å…·åˆ©ç”¨ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹å¤æ‚æŸ¥è¯¢çš„è‡ªä¸»å¤„ç†ã€å¤šæºæ•°æ®çš„ç²¾ç¡®æ£€ç´¢ä»¥åŠäººç±»å¯è¯»å“åº”çš„è‡ªåŠ¨ç”Ÿæˆã€‚ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆé›†æˆå›¾åƒå¼‚å¸¸æ£€æµ‹ã€ç±»ä¼¼æ¡ˆä¾‹æ£€ç´¢åŠæŠ¥å‘Šç”Ÿæˆç­‰å¤šæ ·åŒ–ä»»åŠ¡ï¼Œè§£å†³äº†å°†å¤šä¸ªAIæ¨¡å‹æ•´åˆè¿›æ•…éšœåˆ†æå·¥ä½œæµçš„éš¾é¢˜ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ”¯æŒåŠå¯¼ä½“æ•…éšœåˆ†æä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—çš„æ“ä½œæœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚è¿™ç§åŸºäºæ™ºèƒ½ä½“çš„æ–¹æ³•ä¸ºåŠå¯¼ä½“åˆ¶é€ ä¸šçš„é«˜æ•ˆè‡ªåŠ¨åŒ–å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡å¤„ç†æä¾›äº†å…¨æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "This technical report provides evaluation details of the experiments presented in the paper accepted to ISTFA 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15567v3",
      "published_date": "2025-06-18 15:43:10 UTC",
      "updated_date": "2025-09-02 15:08:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:50.837959+00:00"
    },
    {
      "arxiv_id": "2506.15559v1",
      "title": "Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates",
      "title_zh": "è¿ˆå‘å¯è§£é‡Šçš„å®¤å†…å®šä½ï¼šåŸºäºé€»è¾‘é—¨çš„ Wi-Fi æŒ‡çº¹ç¥ç»ç½‘ç»œå­¦ä¹ è§£æ",
      "authors": [
        "Danish Gufran",
        "Sudeep Pasricha"
      ],
      "abstract": "Indoor localization using deep learning (DL) has demonstrated strong accuracy in mapping Wi-Fi RSS fingerprints to physical locations; however, most existing DL frameworks function as black-box models, offering limited insight into how predictions are made or how models respond to real-world noise over time. This lack of interpretability hampers our ability to understand the impact of temporal variations - caused by environmental dynamics - and to adapt models for long-term reliability. To address this, we introduce LogNet, a novel logic gate-based framework designed to interpret and enhance DL-based indoor localization. LogNet enables transparent reasoning by identifying which access points (APs) are most influential for each reference point (RP) and reveals how environmental noise disrupts DL-driven localization decisions. This interpretability allows us to trace and diagnose model failures and adapt DL systems for more stable long-term deployments. Evaluations across multiple real-world building floorplans and over two years of temporal variation show that LogNet not only interprets the internal behavior of DL models but also improves performance-achieving up to 1.1x to 2.8x lower localization error, 3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to prior DL-based models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨ Wi-Fi RSS æŒ‡çº¹å®¤å†…å®šä½ä¸­å­˜åœ¨çš„â€œé»‘ç›’â€æ¨¡å‹å±€é™æ€§å’Œç¯å¢ƒå™ªå£°é€‚åº”æ€§å·®çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºé€»è¾‘é—¨ (logic gate) çš„æ–°å‹è§£é‡Šæ€§æ¡†æ¶ LogNetã€‚è¯¥æ¡†æ¶é€šè¿‡è¯†åˆ«å¯¹æ¯ä¸ªå‚è€ƒç‚¹ (RP) æœ€å…·å½±å“åŠ›çš„è®¿é—®ç‚¹ (AP)ï¼Œå®ç°äº†é€æ˜åŒ–æ¨ç†ï¼Œå¹¶æ­ç¤ºäº†ç¯å¢ƒå™ªå£°å¯¹å®šä½å†³ç­–çš„å…·ä½“å½±å“ã€‚è¿™ç§å¯è§£é‡Šæ€§ä¸ä»…èƒ½å¸®åŠ©è¯Šæ–­æ¨¡å‹æ•…éšœï¼Œè¿˜å¢å¼ºäº†æ·±åº¦å­¦ä¹ ç³»ç»Ÿåœ¨é•¿æœŸéƒ¨ç½²ä¸­çš„ç¨³å®šæ€§ã€‚åœ¨è·¨è¶Šä¸¤å¹´çš„å¤šåœºæ™¯çœŸå®è¯„ä¼°ä¸­ï¼ŒLogNet å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶å®šä½è¯¯å·®æ¯”ä»¥å¾€ DL æ¨¡å‹é™ä½äº† 1.1 è‡³ 2.8 å€ã€‚æ­¤å¤–ï¼ŒLogNet å®ç°äº†æ˜¾è‘—çš„è½»é‡åŒ–å’Œé«˜æ•ˆæ€§ï¼Œæ¨¡å‹å°ºå¯¸ç¼©å°äº† 3.4 è‡³ 43.3 å€ï¼Œå»¶è¿Ÿé™ä½äº† 1.5 è‡³ 3.6 å€ï¼Œä¸ºå¯è§£é‡Šä¸”é«˜æ•ˆçš„å®¤å†…å®šä½æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15559v1",
      "published_date": "2025-06-18 15:34:41 UTC",
      "updated_date": "2025-06-18 15:34:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:50.247230+00:00"
    },
    {
      "arxiv_id": "2506.15554v1",
      "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones",
      "title_zh": "DAILOCï¼šé¢å‘æ™ºèƒ½æ‰‹æœºå®¤å†…å®šä½çš„é¢†åŸŸå¢é‡å­¦ä¹ ",
      "authors": [
        "Akhil Singampalli",
        "Danish Gufran",
        "Sudeep Pasricha"
      ],
      "abstract": "Wi-Fi fingerprinting-based indoor localization faces significant challenges in real-world deployments due to domain shifts arising from device heterogeneity and temporal variations within indoor environments. Existing approaches often address these issues independently, resulting in poor generalization and susceptibility to catastrophic forgetting over time. In this work, we propose DAILOC, a novel domain-incremental learning framework that jointly addresses both temporal and device-induced domain shifts. DAILOC introduces a novel disentanglement strategy that separates domain shifts from location-relevant features using a multi-level variational autoencoder. Additionally, we introduce a novel memory-guided class latent alignment mechanism to address the effects of catastrophic forgetting over time. Experiments across multiple smartphones, buildings, and time instances demonstrate that DAILOC significantly outperforms state-of-the-art methods, achieving up to 2.74x lower average error and 4.6x lower worst-case error.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DAILOCï¼Œä¸€ç§æ–°å‹çš„é’ˆå¯¹æ™ºèƒ½æ‰‹æœºå®¤å†…å®šä½çš„Domain-Incremental Learningæ¡†æ¶ï¼Œæ—¨åœ¨åŒæ­¥è§£å†³Wi-FiæŒ‡çº¹å®šä½åœ¨å®é™…éƒ¨ç½²ä¸­å› è®¾å¤‡å¼‚æ„æ€§å’Œç¯å¢ƒæ—¶é—´å˜åŒ–å¯¼è‡´çš„Domain Shiftsé—®é¢˜ã€‚DAILOCé€šè¿‡å¼•å…¥ä¸€ç§åŸºäºMulti-level Variational Autoencoderçš„è§£è€¦ç­–ç•¥ï¼Œå®ç°äº†Domain Shiftsä¸ä½ç½®ç›¸å…³ç‰¹å¾çš„æœ‰æ•ˆåˆ†ç¦»ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§Memory-guided Class Latent Alignmentæœºåˆ¶ï¼Œä»¥å…‹æœæ¨¡å‹åœ¨é•¿æ—¶é—´è¿è¡Œä¸­äº§ç”Ÿçš„Catastrophic Forgettingæ•ˆåº”ã€‚åœ¨å¤šç§æ‰‹æœºè®¾å¤‡ã€å»ºç­‘ç¯å¢ƒå’Œæ—¶é—´è·¨åº¦ä¸‹çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDAILOCæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå…¶å¹³å‡è¯¯å·®å’Œæœ€åæƒ…å†µè¯¯å·®åˆ†åˆ«é™ä½äº†2.74å€å’Œ4.6å€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15554v1",
      "published_date": "2025-06-18 15:27:40 UTC",
      "updated_date": "2025-06-18 15:27:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:42:57.594282+00:00"
    },
    {
      "arxiv_id": "2506.15549v2",
      "title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation",
      "title_zh": "CLAIMï¼šé¢å‘çœŸå®ä¸”å¤šæ ·åŒ–å¿ƒè‚Œç˜¢ç—•åˆæˆä¸åˆ†å‰²çš„ä¸´åºŠå¼•å¯¼ LGE å¢å¼º",
      "authors": [
        "Farheen Ramzan",
        "Yusuf Kiberu",
        "Nikesh Jathanna",
        "Shahnaz Jamil-Copley",
        "Richard H. Clayton",
        "Chen Chen"
      ],
      "abstract": "Deep learning-based myocardial scar segmentation from late gadolinium enhancement (LGE) cardiac MRI has shown great potential for accurate and timely diagnosis and treatment planning for structural cardiac diseases. However, the limited availability and variability of LGE images with high-quality scar labels restrict the development of robust segmentation models. To address this, we introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE \\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial Scar Synthesis and Segmentation framework, a framework for anatomically grounded scar generation and segmentation. At its core is the SMILE module (Scar Mask generation guided by cLinical knowledgE), which conditions a diffusion-based generator on the clinically adopted AHA 17-segment model to synthesize images with anatomically consistent and spatially diverse scar patterns. In addition, CLAIM employs a joint training strategy in which the scar segmentation network is optimized alongside the generator, aiming to enhance both the realism of synthesized scars and the accuracy of the scar segmentation performance. Experimental results show that CLAIM produces anatomically coherent scar patterns and achieves higher Dice similarity with real scar distributions compared to baseline models. Our approach enables controllable and realistic myocardial scar synthesis and has demonstrated utility for downstream medical imaging task. Code is available at https://github.com/farheenjabeen/CLAIM-Scar-Synthesis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CLAIM æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ™šæœŸé’†å¢å¼º (Late Gadolinium Enhancement, LGE) å¿ƒè„ç£å…±æŒ¯æˆåƒä¸­ç”±äºé«˜è´¨é‡æ ‡æ³¨æ•°æ®ä¸è¶³è€Œé™åˆ¶å¿ƒè‚Œç˜¢ç—• (Myocardial Scar) åˆ†å‰²æ¨¡å‹æ€§èƒ½çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ SMILE æ¨¡å—ï¼Œå®ƒåˆ©ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆå™¨ (Diffusion-based generator) å¹¶ç»“åˆä¸´åºŠå¸¸ç”¨çš„ AHA 17-segment modelï¼Œåˆæˆäº†å…·æœ‰è§£å‰–ä¸€è‡´æ€§å’Œç©ºé—´å¤šæ ·æ€§çš„ç˜¢ç—•æ¨¡å¼ã€‚CLAIM è¿˜å¼•å…¥äº†è”åˆè®­ç»ƒç­–ç•¥ (Joint training strategy)ï¼Œé€šè¿‡åŒæ—¶ä¼˜åŒ–åˆ†å‰²ç½‘ç»œä¸ç”Ÿæˆå™¨æ¥æå‡åˆæˆç˜¢ç—•çš„çœŸå®æ„Ÿä¸åˆ†å‰²å‡†ç¡®åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„ç˜¢ç—•æ¨¡å¼åœ¨è§£å‰–é€»è¾‘ä¸Šæ›´åŠ è¿è´¯ï¼Œä¸”åœ¨ Dice ç›¸ä¼¼åº¦ç­‰æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…å®ç°äº†å¯æ§ä¸”çœŸå®çš„å½±åƒåˆæˆï¼Œä¹Ÿä¸ºæå‡ä¸‹æ¸¸åŒ»å­¦å½±åƒä»»åŠ¡çš„æ€§èƒ½å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 Pages",
      "pdf_url": "https://arxiv.org/pdf/2506.15549v2",
      "published_date": "2025-06-18 15:21:34 UTC",
      "updated_date": "2025-06-25 14:37:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:04.086070+00:00"
    },
    {
      "arxiv_id": "2506.15543v2",
      "title": "Learning Algorithms in the Limit",
      "title_zh": "æé™æ„ä¹‰ä¸‹çš„ç®—æ³•å­¦ä¹ ",
      "authors": [
        "Hristo Papazov",
        "Nicolas Flammarion"
      ],
      "abstract": "This paper studies the problem of learning computable functions in the limit by extending Gold's inductive inference framework to incorporate \\textit{computational observations} and \\textit{restricted input sources}. Complimentary to the traditional Input-Output Observations, we introduce Time-Bound Observations, and Policy-Trajectory Observations to study the learnability of general recursive functions under more realistic constraints. While input-output observations do not suffice for learning the class of general recursive functions in the limit, we overcome this learning barrier by imposing computational complexity constraints or supplementing with approximate time-bound observations. Further, we build a formal framework around observations of \\textit{computational agents} and show that learning computable functions from policy trajectories reduces to learning rational functions from input and output, thereby revealing interesting connections to finite-state transducer inference. On the negative side, we show that computable or polynomial-mass characteristic sets cannot exist for the class of linear-time computable functions even for policy-trajectory observations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨æé™æ¡ä»¶ä¸‹å­¦ä¹ å¯è®¡ç®—å‡½æ•°çš„é—®é¢˜ï¼Œé€šè¿‡æ‰©å±• Gold çš„å½’çº³æ¨ç†æ¡†æ¶ (inductive inference framework)ï¼Œå¼•å…¥äº†è®¡ç®—è§‚æµ‹ (computational observations) å’Œå—é™è¾“å…¥æºã€‚é™¤äº†ä¼ ç»Ÿçš„è¾“å…¥-è¾“å‡ºè§‚æµ‹ (Input-Output Observations) å¤–ï¼Œè®ºæ–‡æå‡ºäº†æ—¶é—´é™åˆ¶è§‚æµ‹ (Time-Bound Observations) å’Œç­–ç•¥è½¨è¿¹è§‚æµ‹ (Policy-Trajectory Observations)ï¼Œæ—¨åœ¨æ›´çœŸå®çš„çº¦æŸä¸‹ç ”ç©¶ä¸€èˆ¬é€’å½’å‡½æ•°çš„å¯å­¦ä¹ æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å•çº¯çš„è¾“å…¥-è¾“å‡ºè§‚æµ‹ä¸è¶³ä»¥åœ¨æé™ä¸‹å­¦ä¹ æ‰€æœ‰ä¸€èˆ¬é€’å½’å‡½æ•°ï¼Œä½†é€šè¿‡å¼•å…¥è®¡ç®—å¤æ‚åº¦çº¦æŸæˆ–è¡¥å……è¿‘ä¼¼çš„æ—¶é—´é™åˆ¶è§‚æµ‹å¯ä»¥å…‹æœè¿™ä¸€å­¦ä¹ éšœç¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ­ç¤ºäº†ä»ç­–ç•¥è½¨è¿¹å­¦ä¹ å¯è®¡ç®—å‡½æ•°å¯ä»¥ç®€åŒ–ä¸ºä»è¾“å…¥è¾“å‡ºå­¦ä¹ æœ‰ç†å‡½æ•°ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸æœ‰é™çŠ¶æ€è½¬æ¢å™¨ (finite-state transducer) æ¨ç†ä¹‹é—´çš„è”ç³»ã€‚æœ€åï¼Œç ”ç©¶åœ¨è´Ÿé¢ç»“æœä¸­è¯æ˜ï¼Œå³ä½¿å¯¹äºç­–ç•¥è½¨è¿¹è§‚æµ‹ï¼Œçº¿æ€§æ—¶é—´å¯è®¡ç®—å‡½æ•°ç±»ä¹Ÿä¸å­˜åœ¨å¯è®¡ç®—æˆ–å¤šé¡¹å¼è´¨é‡çš„ç‰¹å¾é›† (characteristic sets)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS",
        "cs.FL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at COLT 2025. This version matches the proceedings version apart from a small notational change in section 3",
      "pdf_url": "https://arxiv.org/pdf/2506.15543v2",
      "published_date": "2025-06-18 15:17:03 UTC",
      "updated_date": "2025-07-10 14:01:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:02.188613+00:00"
    },
    {
      "arxiv_id": "2506.15541v1",
      "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity",
      "title_zh": "å†…åœ¨ä¸å¤–åœ¨ç»„ç»‡åŒ–æ³¨æ„åŠ›ï¼šSoftmax ä¸å˜æ€§ä¸ç½‘ç»œç¨€ç–æ€§",
      "authors": [
        "Oluwadamilola Fasina",
        "Ruben V. C. Pohle",
        "Pei-Chun Su",
        "Ronald R. Coifman"
      ],
      "abstract": "We examine the intrinsic (within the attention head) and extrinsic (amongst the attention heads) structure of the self-attention mechanism in transformers. Theoretical evidence for invariance of the self-attention mechanism to softmax activation is obtained by appealing to paradifferential calculus, (and is supported by computational examples), which relies on the intrinsic organization of the attention heads. Furthermore, we use an existing methodology for hierarchical organization of tensors to examine network structure by constructing hierarchal partition trees with respect to the query, key, and head axes of network 3-tensors. Such an organization is consequential since it allows one to profitably execute common signal processing tasks on a geometry where the organized network 3-tensors exhibit regularity. We exemplify this qualitatively, by visualizing the hierarchical organization of the tree comprised of attention heads and the diffusion map embeddings, and quantitatively by investigating network sparsity with the expansion coefficients of individual attention heads and the entire network with respect to the bi and tri-haar bases (respectively) on the space of queries, keys, and heads of the network. To showcase the utility of our theoretical and methodological findings, we provide computational examples using vision and language transformers. The ramifications of these findings are two-fold: (1) a subsequent step in interpretability analysis is theoretically admitted, and can be exploited empirically for downstream interpretability tasks (2) one can use the network 3-tensor organization for empirical network applications such as model pruning (by virtue of network sparsity) and network architecture comparison.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº† Transformer æ¨¡å‹ä¸­è‡ªæ³¨æ„åŠ›æœºåˆ¶ (self-attention mechanism) çš„å†…åœ¨ä¸å¤–åœ¨ç»“æ„ï¼Œå³æ³¨æ„å¤´å†…éƒ¨åŠå¤´ä¹‹é—´çš„ç»„ç»‡é€»è¾‘ã€‚åˆ©ç”¨åå¾®åˆ†ç®—å­ (paradifferential calculus) ç†è®ºå¹¶ç»“åˆè®¡ç®—å®ä¾‹ï¼Œä½œè€…è¯æ˜äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ Softmax æ¿€æ´»ä¸‹çš„ä¸å˜æ€§ (softmax invariance)ï¼Œæ­ç¤ºäº†æ³¨æ„å¤´çš„å†…åœ¨ç»„ç»‡ç‰¹æ€§ã€‚é€šè¿‡æ„å»ºé’ˆå¯¹ Queryã€Key å’Œ Head è½´çš„åˆ†å±‚åˆ’åˆ†æ ‘ (hierarchical partition trees)ï¼Œè¯¥ç ”ç©¶å®ç°äº†å¯¹ç½‘ç»œ 3-tensors çš„ç»“æ„åŒ–ç»„ç»‡ï¼Œå¹¶åˆ©ç”¨ Bi-Haar å’Œ Tri-Haar åŸºå‡½æ•°å®šé‡ç ”ç©¶äº†ç½‘ç»œçš„ç¨€ç–æ€§ (network sparsity)ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡æ‰©æ•£æ˜ å°„åµŒå…¥ (diffusion map embeddings) å¯è§†åŒ–äº†æ³¨æ„å¤´çš„åˆ†å±‚ç»„ç»‡ï¼Œå¹¶åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹ä¸­éªŒè¯äº†ç†è®ºçš„æœ‰æ•ˆæ€§ã€‚è¿™äº›å‘ç°ä¸ä»…ä¸ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ (interpretability) åˆ†æå¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œè¿˜ä¸ºæ¨¡å‹å‰ªæ (model pruning) å’Œç½‘ç»œæ¶æ„æ¯”è¾ƒç­‰ä¸‹æ¸¸ä»»åŠ¡æä¾›äº†å…¨æ–°çš„å®è¯å·¥å…·ã€‚",
      "categories": [
        "math.NA",
        "cs.AI"
      ],
      "primary_category": "math.NA",
      "comment": "16 pages, 6 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.15541v1",
      "published_date": "2025-06-18 15:14:56 UTC",
      "updated_date": "2025-06-18 15:14:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:07.992279+00:00"
    },
    {
      "arxiv_id": "2506.15538v4",
      "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework",
      "title_zh": "åˆ©ç”¨ PRISM æ•æ‰å¤šä¹‰æ€§ï¼šä¸€ç§å¤šæ¦‚å¿µç‰¹å¾æè¿°æ¡†æ¶",
      "authors": [
        "Laura Kopf",
        "Nils Feldhus",
        "Kirill Bykov",
        "Philine Lou Bommer",
        "Anna HedstrÃ¶m",
        "Marina M. -C. HÃ¶hne",
        "Oliver Eberle"
      ],
      "abstract": "Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è‡ªåŠ¨åŒ–å¯è§£é‡Šæ€§ç ”ç©¶ä¸­å­˜åœ¨çš„å±€é™ï¼Œæå‡ºäº†PRISMï¼ˆPolysemantic FeatuRe Identification and Scoring Methodï¼‰è¿™ä¸€åˆ›æ–°çš„å¤šæ¦‚å¿µç‰¹å¾æè¿°æ¡†æ¶ã€‚ä¼ ç»Ÿçš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)è‡ªåŠ¨åŒ–ç¥ç»å…ƒæè¿°æ–¹æ³•é€šå¸¸å‡è®¾æ¯ä¸ªç¥ç»å…ƒä»…ç¼–ç å•ä¸€æ¦‚å¿µ(monosemanticity)ï¼Œè¿™å¿½ç•¥äº†ç¥ç»å…ƒå¹¿æ³›å­˜åœ¨çš„å¤šä¹‰æ€§(polysemanticity)ç°è±¡ï¼Œä»è€Œé™åˆ¶äº†æè¿°çš„å‡†ç¡®æ€§ã€‚PRISMæ¡†æ¶ä¸“é—¨ä¸ºæ•æ‰LLMsä¸­ç‰¹å¾çš„å¤æ‚æ€§è€Œè®¾è®¡ï¼Œèƒ½å¤Ÿç”Ÿæˆæ¶µç›–å•è¯­ä¹‰å’Œå¤šè¯­ä¹‰è¡Œä¸ºçš„ç»†è‡´æè¿°ã€‚é€šè¿‡ä¸ç°æœ‰æ–¹æ³•çš„å¹¿æ³›åŸºå‡†æµ‹è¯•å¯¹æ¯”ï¼ŒPRISMåœ¨ç‰¹å¾æè¿°å¾—åˆ†(description score)å’Œå¤šä¹‰æ€§å¾—åˆ†(polysemanticity score)ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´å¿ å®äºæ¨¡å‹å®é™…è¡Œä¸ºçš„ç‰¹å¾æè¿°ï¼Œä¸ºæ·±å…¥ç†è§£å¤æ‚æ¨¡å‹çš„å†…éƒ¨æœºç†æä¾›äº†æ›´æœ‰æ•ˆçš„æ‰‹æ®µã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15538v4",
      "published_date": "2025-06-18 15:13:07 UTC",
      "updated_date": "2025-11-12 16:22:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:31.585003+00:00"
    },
    {
      "arxiv_id": "2506.15513v1",
      "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation",
      "title_zh": "RePCSï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„æ•°æ®è®°å¿†è¯Šæ–­",
      "authors": [
        "Le Vu Anh",
        "Nguyen Viet Anh",
        "Mehmet Dik",
        "Luong Van Nghia"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has become a common strategy for updating large language model (LLM) responses with current, external information. However, models may still rely on memorized training data, bypass the retrieved evidence, and produce contaminated outputs. We introduce Retrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects such behavior without requiring model access or retraining. RePCS compares two inference paths: (i) a parametric path using only the query, and (ii) a retrieval-augmented path using both the query and retrieved context by computing the Kullback-Leibler (KL) divergence between their output distributions. A low divergence suggests that the retrieved context had minimal impact, indicating potential memorization. This procedure is model-agnostic, requires no gradient or internal state access, and adds only a single additional forward pass. We further derive PAC-style guarantees that link the KL threshold to user-defined false positive and false negative rates. On the Prompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result outperforms the strongest prior method by 6.5 percentage points while keeping latency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight, black-box safeguard to verify whether a RAG system meaningfully leverages retrieval, making it especially valuable in safety-critical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation, RAG) ç³»ç»Ÿå¯èƒ½å¿½ç•¥æ£€ç´¢è¯æ®è€Œä¾èµ–è®°å¿†æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº† RePCS (Retrieval-Path Contamination Scoring) è¯Šæ–­æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½œä¸ºä¸€ç§é»‘ç›’æ‰‹æ®µï¼Œæ— éœ€æ¨¡å‹è®¿é—®æƒé™æˆ–é‡è®­ç»ƒï¼Œé€šè¿‡è®¡ç®—å‚æ•°è·¯å¾„ (parametric path) ä¸æ£€ç´¢å¢å¼ºè·¯å¾„è¾“å‡ºåˆ†å¸ƒä¹‹é—´çš„ Kullback-Leibler (KL) æ•£åº¦æ¥æ£€æµ‹æ¨¡å‹è¡Œä¸ºã€‚è¾ƒä½çš„æ•£åº¦è¡¨æ˜æ£€ç´¢ä¸Šä¸‹æ–‡å¯¹è¾“å‡ºå½±å“æå°ï¼Œä»è€Œè¯†åˆ«å‡ºæ½œåœ¨çš„æ•°æ®è®°å¿†ã€‚RePCS å…·æœ‰æ¨¡å‹æ— å…³æ€§ (model-agnostic)ï¼Œä¸éœ€è¦è®¿é—®æ¢¯åº¦æˆ–å†…éƒ¨çŠ¶æ€ï¼Œä¸”ä»…å¢åŠ æä½çš„å‰å‘ä¼ æ’­å¼€é”€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº† PAC-style ç†è®ºä¿è¯ï¼Œä»¥ä¾¿ç”¨æˆ·æ ¹æ®è¯¯æŠ¥å’Œæ¼æŠ¥ç‡è®¾å®šé˜ˆå€¼ã€‚åœ¨ Prompt-WNQA åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRePCS å®ç°äº† 0.918 çš„ ROC-AUCï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æœ€å¼ºæ–¹æ³•ä¸”å»¶è¿Ÿå¢é‡ä½äº 4.7%ã€‚è¯¥æˆæœä¸ºç¡®ä¿ RAG ç³»ç»Ÿæœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨ä¿¡æ¯æä¾›äº†ä¸€ç§è½»é‡çº§çš„éªŒè¯æ–¹æ¡ˆï¼Œåœ¨å®‰å…¨å…³é”®é¢†åŸŸå…·æœ‰æ˜¾è‘—åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 7 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.15513v1",
      "published_date": "2025-06-18 14:48:19 UTC",
      "updated_date": "2025-06-18 14:48:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:28.691983+00:00"
    },
    {
      "arxiv_id": "2506.15512v1",
      "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach",
      "title_zh": "åˆ©ç”¨ LangChain ä¸­çš„ GPT é›†æˆä¼˜åŒ–åŸºäº Web çš„äººå·¥æ™ºèƒ½æŸ¥è¯¢æ£€ç´¢ï¼šä¸€ç§ CoT å¢å¼ºçš„æç¤ºå·¥ç¨‹æ–¹æ³•",
      "authors": [
        "Wenqi Guan",
        "Yang Fang"
      ],
      "abstract": "Large Language Models have brought a radical change in the process of remote learning students, among other aspects of educative activities. Current retrieval of remote learning resources lacks depth in contextual meaning that provides comprehensive information on complex student queries. This work proposes a novel approach to enhancing remote learning retrieval by integrating GPT-based models within the LangChain framework. We achieve this system in a more intuitive and productive manner using CoT reasoning and prompt engineering. The framework we propose puts much emphasis on increasing the precision and relevance of the retrieval results to return comprehensive and contextually enriched explanations and resources that best suit each student's needs. We also assess the effectiveness of our approach against paradigmatic LLMs and report improvements in user satisfaction and learning outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨ LangChain æ¡†æ¶ä¸‹é›†æˆ GPT æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ– Web ç«¯ AI æŸ¥è¯¢æ£€ç´¢å¹¶è§£å†³è¿œç¨‹å­¦ä¹ èµ„æºæ£€ç´¢ä¸­æ·±åº¦èƒŒæ™¯ä¿¡æ¯åŒ®ä¹çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆ CoT (Chain-of-Thought) æ¨ç†ä¸ Prompt Engineering æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»¥æ›´ç›´è§‚ã€é«˜æ•ˆçš„æ–¹å¼æå‡æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚ç ”ç©¶é‡ç‚¹åœ¨äºæé«˜æ£€ç´¢ç»“æœçš„ Precision (ç²¾ç¡®åº¦) å’Œ Relevance (ç›¸å…³æ€§)ï¼Œä»è€Œä¸ºå­¦ç”Ÿæä¾›é’ˆå¯¹æ€§å¼ºä¸”èƒŒæ™¯ä¿¡æ¯ä¸°å¯Œçš„å­¦ä¹ èµ„æºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”¨æˆ·æ»¡æ„åº¦å’Œå­¦ä¹ æˆæœæ–¹é¢å‡ä¼˜äºä¼ ç»Ÿ LLMsï¼Œä¸ºå¤æ‚æ•™è‚²æŸ¥è¯¢æä¾›äº†æ›´å…·æ·±åº¦çš„ä¸Šä¸‹æ–‡ç†è§£å’Œæ£€ç´¢æ”¯æŒã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15512v1",
      "published_date": "2025-06-18 14:47:59 UTC",
      "updated_date": "2025-06-18 14:47:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:30.985145+00:00"
    },
    {
      "arxiv_id": "2506.15507v2",
      "title": "Over-squashing in Spatiotemporal Graph Neural Networks",
      "title_zh": "æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œä¸­çš„è¿‡åº¦æŒ¤å‹",
      "authors": [
        "Ivan Marisca",
        "Jacob Bamberger",
        "Cesare Alippi",
        "Michael M. Bronstein"
      ],
      "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that, counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ—¶ç©ºå›¾ç¥ç»ç½‘ç»œ (STGNNs) ä¸­çš„è¿‡åº¦å‹ç¼© (over-squashing) é—®é¢˜ï¼Œè¿™æ˜¯å›¾ç¥ç»ç½‘ç»œåœ¨å¤„ç†èŠ‚ç‚¹åºåˆ—ä¿¡æ¯ä¼ æ’­æ—¶é¢ä¸´çš„åŸºæœ¬å±€é™ã€‚ä½œè€…é¦–æ¬¡æ­£å¼å®šä¹‰äº†æ—¶ç©ºè¿‡åº¦å‹ç¼©é—®é¢˜ï¼Œå¹¶æ­ç¤ºäº†å…¶ä¸é™æ€å›¾åœºæ™¯æˆªç„¶ä¸åŒçš„ç‰¹å¾ã€‚ç ”ç©¶é€šè¿‡ç†è®ºåˆ†æå‘ç°ï¼Œä¸ç›´è§‰ç›¸åï¼Œå·ç§¯ STGNNs æ›´æœ‰åˆ©äºæ¥è‡ªæ—¶é—´ä¸Šè¾ƒè¿œè€Œéè¾ƒè¿‘ç‚¹çš„ä¿¡æ¯ä¼ æ’­ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¯æ˜äº†æ— è®ºé‡‡ç”¨â€œæ—¶é—´ä¸ç©ºé—´ (time-and-space)â€è¿˜æ˜¯â€œå…ˆæ—¶é—´åç©ºé—´ (time-then-space)â€çš„å¤„ç†èŒƒå¼ï¼Œå…¶æ¶æ„å‡ä¼šå—åˆ°è¿™ä¸€ç°è±¡çš„åŒç­‰å½±å“ã€‚è¯¥ç ”ç©¶åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†ä¸Šè¿°ç»“è®ºï¼Œä¸ºç†è§£ STGNNs çš„è¿è¡Œæœºåˆ¶åŠè®¾è®¡æ›´æœ‰æ•ˆçš„æ¶æ„æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15507v2",
      "published_date": "2025-06-18 14:45:06 UTC",
      "updated_date": "2025-11-02 17:09:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:39.695865+00:00"
    },
    {
      "arxiv_id": "2506.15499v1",
      "title": "Pixel-level Certified Explanations via Randomized Smoothing",
      "title_zh": "åŸºäºéšæœºå¹³æ»‘çš„åƒç´ çº§è®¤è¯è§£é‡Š",
      "authors": [
        "Alaa Anani",
        "Tobias Lorenz",
        "Mario Fritz",
        "Bernt Schiele"
      ],
      "abstract": "Post-hoc attribution methods aim to explain deep learning predictions by highlighting influential input pixels. However, these explanations are highly non-robust: small, imperceptible input perturbations can drastically alter the attribution map while maintaining the same prediction. This vulnerability undermines their trustworthiness and calls for rigorous robustness guarantees of pixel-level attribution scores. We introduce the first certification framework that guarantees pixel-level robustness for any black-box attribution method using randomized smoothing. By sparsifying and smoothing attribution maps, we reformulate the task as a segmentation problem and certify each pixel's importance against $\\ell_2$-bounded perturbations. We further propose three evaluation metrics to assess certified robustness, localization, and faithfulness. An extensive evaluation of 12 attribution methods across 5 ImageNet models shows that our certified attributions are robust, interpretable, and faithful, enabling reliable use in downstream tasks. Our code is at https://github.com/AlaaAnani/certified-attributions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦å­¦ä¹ ä¸­äº‹åå½’å› æ–¹æ³• (Post-hoc attribution methods) ææ˜“å—åˆ°å¾®å°è¾“å…¥æ‰°åŠ¨å½±å“è€Œå¤±æ•ˆçš„é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºéšæœºå¹³æ»‘ (Randomized Smoothing) çš„åƒç´ çº§é²æ£’æ€§è®¤è¯æ¡†æ¶ã€‚é€šè¿‡å°†å½’å› å›¾ç¨€ç–åŒ–å¹¶å°†å…¶é‡æ–°å®šä¹‰ä¸ºåˆ†å‰²é—®é¢˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä¸ºä»»æ„é»‘ç›’å½’å› æ–¹æ³•åœ¨ $\\ell_2$ èŒƒæ•°å—é™çš„æ‰°åŠ¨ä¸‹æä¾›åƒç´ çº§çš„é‡è¦æ€§ä¿è¯ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜è®¾è®¡äº†ä¸‰ä¸ªè¯„ä¼°æŒ‡æ ‡æ¥å…¨é¢è¡¡é‡è§£é‡Šçš„å·²è®¤è¯é²æ£’æ€§ (certified robustness)ã€å®šä½ç²¾åº¦ (localization) å’Œå¿ å®åº¦ (faithfulness)ã€‚åœ¨ 5 ä¸ª ImageNet æ¨¡å‹å’Œ 12 ç§å½’å› æ–¹æ³•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„å½’å› ç»“æœä¸ä»…é²æ£’ã€å¯è§£é‡Šä¸”å…·æœ‰é«˜åº¦å¿ å®åº¦ï¼Œèƒ½å¤Ÿä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›å¯é çš„è§£é‡Šæ”¯æŒã€‚è¯¥å·¥ä½œé€šè¿‡å½¢å¼åŒ–çš„é²æ£’æ€§ä¿éšœï¼Œè§£å†³äº†æ˜¾è‘—æ€§å›¾åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„ä¸ç¨³å®šæ€§ï¼Œä¸ºæ„å»ºå¯ä¿¡çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15499v1",
      "published_date": "2025-06-18 14:41:24 UTC",
      "updated_date": "2025-06-18 14:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:34.488379+00:00"
    },
    {
      "arxiv_id": "2506.15498v2",
      "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling",
      "title_zh": "SPAREï¼šåŸºäºå‚è€ƒå¼•å¯¼è¯„ä¼°çš„è‡ªåŠ¨åŒ–è¿‡ç¨‹ç›‘ç£ä¸å¥–åŠ±å»ºæ¨¡å•æ¬¡æ ‡æ³¨æ¡†æ¶",
      "authors": [
        "Md Imbesat Hassan Rizvi",
        "Xiaodan Zhu",
        "Iryna Gurevych"
      ],
      "abstract": "Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables efficient per-step annotation by jointly aligning solution steps to reference solutions and determine its accuracy with explicit reasoning in single generation. We demonstrate SPARE's effectiveness across four diverse datasets spanning mathematical reasoning (GSM8K, MATH), multi-hop question answering (MuSiQue-Ans), and spatial reasoning (SpaRP), showing consistent improvements in two applications: (1) training Process Reward Models (PRMs) for ranking and aggregating multiple generations, and (2) fine-tuning models via offline reinforcement learning for greedy decoding. On ProcessBench, SPARE demonstrates data-efficient out-of-distribution generalization, using only $\\sim$16% of training samples compared to human-labeled and other synthetically trained baselines. Additionally, it achieves competitive performance with MCTS-based methods while offering 2.3$\\times$ speedup in terms of total token count. Manual analysis reveals complementary precision-recall characteristics with MCTS approaches, suggesting potential for ensemble methods. These results establish SPARE as a practical and scalable solution for automatic process supervision in LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SPAREï¼ˆSingle-Pass Annotation with Reference-Guided Evaluationï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤æ‚å¤šæ­¥æ¨ç†ä¸­è‡ªåŠ¨è¿‡ç¨‹æ ‡æ³¨æŒ‘æˆ˜çš„æ–°å‹ç»“æ„åŒ–æ¡†æ¶ã€‚SPAREé€šè¿‡åœ¨å•æ¬¡ç”Ÿæˆä¸­å°†è§£é¢˜æ­¥éª¤ä¸å‚è€ƒè§£æ³•å¯¹é½ï¼Œå¹¶åˆ©ç”¨æ˜¾å¼æ¨ç†ç¡®å®šæ¯ä¸€æ­¥çš„å‡†ç¡®æ€§ï¼Œå®ç°äº†é«˜æ•ˆçš„é€æ­¥æ ‡æ³¨ã€‚å®éªŒåœ¨æ•°å­¦æ¨ç†ï¼ˆGSM8K, MATHï¼‰ã€å¤šè·³é—®ç­”å’Œç©ºé—´æ¨ç†ç­‰å¤šä¸ªé¢†åŸŸè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶åœ¨è®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å’Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆoffline RLï¼‰å¾®è°ƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚åœ¨ProcessBenchæµ‹è¯•ä¸­ï¼ŒSPAREä»…ä½¿ç”¨çº¦16%çš„è®­ç»ƒæ ·æœ¬ä¾¿å®ç°äº†ä¼˜å¼‚çš„åˆ†å¸ƒå¤–æ³›åŒ–æ€§èƒ½ï¼Œå±•ç°äº†æé«˜çš„æ•°æ®æ•ˆç‡ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒSPAREåœ¨æ€»Tokenæ¶ˆè€—ä¸Šå®ç°äº†2.3å€çš„åŠ é€Ÿï¼Œä¸”åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ç‰¹å¾ä¸Šä¸å…¶å½¢æˆäº’è¡¥ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSPAREä¸ºLLMæ¨ç†çš„è‡ªåŠ¨è¿‡ç¨‹ç›‘ç£æä¾›äº†ä¸€ç§å®ç”¨ä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages main content, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2506.15498v2",
      "published_date": "2025-06-18 14:37:59 UTC",
      "updated_date": "2025-08-22 00:04:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:44.386587+00:00"
    },
    {
      "arxiv_id": "2506.17318v1",
      "title": "Context manipulation attacks : Web agents are susceptible to corrupted memory",
      "title_zh": "ä¸Šä¸‹æ–‡æ“çºµæ”»å‡»ï¼šWeb æ™ºèƒ½ä½“æ˜“å—å—æŸå†…å­˜çš„å½±å“",
      "authors": [
        "Atharv Singh Patlan",
        "Ashwin Hebbar",
        "Pramod Viswanath",
        "Prateek Mittal"
      ],
      "abstract": "Autonomous web navigation agents, which translate natural language instructions into sequences of browser actions, are increasingly deployed for complex tasks across e-commerce, information retrieval, and content discovery. Due to the stateless nature of large language models (LLMs), these agents rely heavily on external memory systems to maintain context across interactions. Unlike centralized systems where context is securely stored server-side, agent memory is often managed client-side or by third-party applications, creating significant security vulnerabilities. This was recently exploited to attack production systems.\n  We introduce and formalize \"plan injection,\" a novel context manipulation attack that corrupts these agents' internal task representations by targeting this vulnerable context. Through systematic evaluation of two popular web agents, Browser-use and Agent-E, we show that plan injections bypass robust prompt injection defenses, achieving up to 3x higher attack success rates than comparable prompt-based attacks. Furthermore, \"context-chained injections,\" which craft logical bridges between legitimate user goals and attacker objectives, lead to a 17.7% increase in success rate for privacy exfiltration tasks. Our findings highlight that secure memory handling must be a first-class concern in agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªä¸» Web å¯¼èˆªæ™ºèƒ½ä½“ (Web agents) åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶å› ä¾èµ–å¤–éƒ¨å†…å­˜ç³»ç»Ÿè€Œé¢ä¸´çš„å®‰å…¨è„†å¼±æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…å­˜ç”±å®¢æˆ·ç«¯æˆ–ç¬¬ä¸‰æ–¹åº”ç”¨ç®¡ç†çš„æƒ…å†µä¸‹ã€‚ä½œè€…æå‡ºå¹¶å½¢å¼åŒ–äº†â€œè®¡åˆ’æ³¨å…¥â€ (plan injection)ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸Šä¸‹æ–‡æ“çºµæ”»å‡» (context manipulation attack)ï¼Œæ—¨åœ¨é€šè¿‡ç ´åæ™ºèƒ½ä½“çš„å†…éƒ¨ä»»åŠ¡è¡¨ç¤ºæ¥ç¯¡æ”¹å…¶è¡Œä¸ºã€‚é€šè¿‡å¯¹ Browser-use å’Œ Agent-E ä¸¤ç§æµè¡Œçš„ Web æ™ºèƒ½ä½“è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œç ”ç©¶å‘ç°è®¡åˆ’æ³¨å…¥èƒ½å¤Ÿç»•è¿‡ç°æœ‰çš„æç¤ºæ³¨å…¥ (prompt injection) é˜²å¾¡æœºåˆ¶ï¼Œæ”»å‡»æˆåŠŸç‡æ¯”åŒç±»åŸºäºæç¤ºçš„æ”»å‡»é«˜å‡º 3 å€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†â€œä¸Šä¸‹æ–‡é“¾å¼æ³¨å…¥â€ (context-chained injections)ï¼Œé€šè¿‡åœ¨ç”¨æˆ·åˆæ³•ç›®æ ‡ä¸æ”»å‡»è€…ç›®æ ‡ä¹‹é—´å»ºç«‹é€»è¾‘æ¡¥æ¢ï¼Œä½¿éšç§å¤–æ³„ä»»åŠ¡çš„æˆåŠŸç‡æå‡äº† 17.7%ã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†åœ¨ä»£ç†ç³»ç»Ÿ (agentic systems) ä¸­ï¼Œå®‰å…¨çš„å†…å­˜å¤„ç†æœºåˆ¶å¿…é¡»ä½œä¸ºæ ¸å¿ƒçš„è®¾è®¡è€ƒé‡ï¼Œä»¥åº”å¯¹æ—¥ç›Šå¤æ‚çš„å®‰å…¨å¨èƒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.17318v1",
      "published_date": "2025-06-18 14:29:02 UTC",
      "updated_date": "2025-06-18 14:29:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:49.790039+00:00"
    },
    {
      "arxiv_id": "2506.15483v1",
      "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects",
      "title_zh": "GenHOIï¼šé¢å‘æœªçŸ¥ç‰©ä½“çš„æ–‡æœ¬é©±åŠ¨ 4D äºº-ç‰©äº¤äº’æ³›åŒ–åˆæˆ",
      "authors": [
        "Shujia Li",
        "Haiyu Zhang",
        "Xinyuan Chen",
        "Yaohui Wang",
        "Yutong Ban"
      ],
      "abstract": "While diffusion models and large-scale motion datasets have advanced text-driven human motion synthesis, extending these advances to 4D human-object interaction (HOI) remains challenging, mainly due to the limited availability of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel two-stage framework aimed at achieving two key objectives: 1) generalization to unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the initial stage of our framework, we employ an Object-AnchorNet to reconstruct sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI datasets, thereby mitigating the dependence on large-scale 4D HOI datasets. Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the second stage to seamlessly interpolate sparse 3D HOI keyframes into densely temporally coherent 4D HOI sequences. To enhance the quality of generated 4D HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to extract human-object contact patterns and a novel Contact-Aware HOI Attention to effectively integrate the contact signals into diffusion models. Experimental results show that we achieve state-of-the-art results on the publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong generalization abilities to unseen objects, while enabling high-fidelity 4D HOI generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡4Dæ•°æ®åŒ®ä¹å¯¼è‡´çš„äººæœºäº¤äº’åˆæˆæŒ‘æˆ˜ï¼Œæå‡ºäº†GenHOIæ¡†æ¶ä»¥å®ç°å¯¹æœªè§ç‰©ä½“çš„æ–‡æœ¬é©±åŠ¨4D Human-Object Interaction (HOI)åˆæˆã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œé¦–å…ˆé€šè¿‡Object-AnchorNetä¸ºæœªçŸ¥ç‰©ä½“é‡å»ºç¨€ç–çš„3D HOIå…³é”®å¸§ï¼Œæœ‰æ•ˆç¼“è§£äº†å¯¹å¤§è§„æ¨¡4D HOIæ•°æ®é›†çš„ä¾èµ–ã€‚éšåï¼Œç ”ç©¶å¼•å…¥äº†Contact-Aware Diffusion Model (ContactDM)ï¼Œåˆ©ç”¨Contact-Aware Encoderæå–æ¥è§¦æ¨¡å¼å¹¶é€šè¿‡Contact-Aware HOI Attentionå°†ä¿¡å·æ•´åˆï¼Œä»è€Œç”Ÿæˆé«˜ä¿çœŸçš„4D HOIåºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenHOIåœ¨OMOMOå’Œ3D-FUTUREæ•°æ®é›†ä¸Šå–å¾—äº†SOTAç»“æœï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤„ç†æœªè§ç‰©ä½“æ—¶çš„æ³›åŒ–èƒ½åŠ›å’Œç”Ÿæˆåºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15483v1",
      "published_date": "2025-06-18 14:17:53 UTC",
      "updated_date": "2025-06-18 14:17:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:54.797535+00:00"
    },
    {
      "arxiv_id": "2506.15480v2",
      "title": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact",
      "title_zh": "æœ‰æ— ä¸Šä¸‹æ–‡æ”¯æŒçš„æŒ‡ä»¤å¾®è°ƒï¼šè¡Œä¸ºè½¬å˜ä¸ä¸‹æ¸¸å½±å“",
      "authors": [
        "Hyunji Lee",
        "Seunghyun Yoon",
        "Yunjae Won",
        "Hanseok Oh",
        "Geewook Kim",
        "Trung Bui",
        "Franck Dernoncourt",
        "Elias Stengel-Eskin",
        "Mohit Bansal",
        "Minjoon Seo"
      ],
      "abstract": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æŒ‡ä»¤å¾®è°ƒ(Instruction Tuning)è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å¸¦æœ‰ä¸Šä¸‹æ–‡(context-augmented)ä¸ä¸å¸¦ä¸Šä¸‹æ–‡(context-free)çš„æ•°æ®å¯¹æ¨¡å‹è¡Œä¸ºåŠå…¶ä¸‹æ¸¸æ€§èƒ½çš„ä¸åŒå½±å“ã€‚ç ”ç©¶å‘ç°åœ¨æ–‡æœ¬é¢†åŸŸï¼Œç»ä¸Šä¸‹æ–‡å¢å¼ºè®­ç»ƒçš„æ¨¡å‹èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å®ç°æ›´å¥½çš„æ‰æ ¹(grounding)ï¼Œå¹¶ä¿ƒä½¿æ¨¡å‹ä»ä¾èµ–å‚æ•°åŒ–çŸ¥è¯†(parametric knowledge)è½¬å‘ä¾èµ–ä¸Šä¸‹æ–‡ã€‚åœ¨å¤šæ¨¡æ€æ‰©å±•ä¸­ï¼Œä»¥è¯¥ç±»æ¨¡å‹ä¸ºéª¨å¹²çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æ˜¾è‘—å‡å°‘äº†å¹»è§‰(hallucination)å¹¶æå‡äº†è§†è§‰æ‰æ ¹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å»ºè®®åœ¨å®é™…éƒ¨ç½²ä¸­é€šè¿‡è·¯ç”±(routing)æœºåˆ¶ç»´æŒç‹¬ç«‹çš„ä¸Šä¸‹æ–‡å¢å¼ºå’Œæ— ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œè¿™ç§æ–¹æ¡ˆæ¯”å•ä¸€æ··åˆæ¨¡å‹èƒ½æ›´ç¨³å¥åœ°å‘æŒ¥äº’è¡¥ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15480v2",
      "published_date": "2025-06-18 14:13:56 UTC",
      "updated_date": "2026-01-08 16:32:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:52.895986+00:00"
    },
    {
      "arxiv_id": "2506.15468v1",
      "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI",
      "title_zh": "åŸºäºäººç±»ä¸äººå·¥æ™ºèƒ½é—´ Metropolis-Hastings äº¤äº’çš„å…±åˆ›å¼å­¦ä¹ ",
      "authors": [
        "Ryota Okumura",
        "Tadahiro Taniguchi",
        "Akira Taniguchi",
        "Yoshinobu Hagiwara"
      ],
      "abstract": "We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shared external representations, a process we interpret as symbol emergence. Unlike traditional AI teaching based on unilateral knowledge transfer, this addresses the challenge of integrating information from inherently different modalities. We empirically test this framework using a human-AI interaction model based on the Metropolis-Hastings naming game (MHNG), a decentralized Bayesian inference mechanism. In an online experiment, 69 participants played a joint attention naming game (JA-NG) with one of three computer agent types (MH-based, always-accept, or always-reject) under partial observability. Results show that human-AI pairs with an MH-based agent significantly improved categorization accuracy through interaction and achieved stronger convergence toward a shared sign system. Furthermore, human acceptance behavior aligned closely with the MH-derived acceptance probability. These findings provide the first empirical evidence for co-creative learning emerging in human-AI dyads via MHNG-based interaction. This suggests a promising path toward symbiotic AI systems that learn with humans, rather than from them, by dynamically aligning perceptual experiences, opening a new venue for symbiotic AI alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å…±åˆ›å­¦ä¹ ï¼ˆco-creative learningï¼‰è¿™ä¸€æ–°èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿç‰©ä¸äººå·¥æ™ºèƒ½ä½“ä¹‹é—´ç›¸äº’æ•´åˆå±€éƒ¨æ„ŸçŸ¥ä¿¡æ¯ä¸çŸ¥è¯†ï¼Œå…±åŒæ„å»ºå…±äº«çš„å¤–éƒ¨è¡¨å¾ï¼Œä»è€Œå®ç°ç¬¦å·æ¶Œç°ï¼ˆsymbol emergenceï¼‰ã€‚ä¸ä¼ ç»Ÿçš„å•å‘çŸ¥è¯†è½¬ç§»æ•™å­¦ä¸åŒï¼Œè¯¥èŒƒå¼é€šè¿‡åŸºäºMetropolis-Hastingså‘½åæ¸¸æˆï¼ˆMHNGï¼‰çš„äººæœºäº¤äº’æ¨¡å‹ï¼Œåˆ©ç”¨å»ä¸­å¿ƒåŒ–çš„è´å¶æ–¯æ¨ç†ï¼ˆBayesian inferenceï¼‰æœºåˆ¶æ¥è§£å†³ä¸åŒæ¨¡æ€ä¿¡æ¯çš„æ•´åˆæŒ‘æˆ˜ã€‚åœ¨æ¶‰åŠ69åå‚ä¸è€…çš„åœ¨çº¿å®éªŒä¸­ï¼Œç ”ç©¶è¯æ˜äº†ä¸MH-basedæ™ºèƒ½ä½“é…å¯¹çš„äººç±»ç»„èƒ½æ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®æ€§ï¼Œå¹¶å®ç°æ›´å¼ºçš„å…±äº«ç¬¦å·ç³»ç»Ÿæ”¶æ•›ã€‚å®éªŒæ•°æ®è¿›ä¸€æ­¥è¡¨æ˜ï¼Œäººç±»çš„æ¥å—è¡Œä¸ºä¸MHç®—æ³•æ¨å¯¼å‡ºçš„æ¥å—æ¦‚ç‡é«˜åº¦å¥‘åˆã€‚è¯¥ç ”ç©¶ä¸ºé€šè¿‡MHNGäº¤äº’åœ¨äººæœºäºŒå…ƒç»„ä¸­äº§ç”Ÿå…±åˆ›å­¦ä¹ æä¾›äº†é¦–ä¸ªå®è¯è¯æ®ï¼Œä¸ºå¼€å‘èƒ½å¤Ÿä¸äººç±»åŠ¨æ€å¯¹é½æ„ŸçŸ¥ç»éªŒå¹¶å…±åŒå­¦ä¹ çš„å…±ç”Ÿäººå·¥æ™ºèƒ½ï¼ˆsymbiotic AIï¼‰ç³»ç»Ÿå¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15468v1",
      "published_date": "2025-06-18 13:58:45 UTC",
      "updated_date": "2025-06-18 13:58:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:43:57.984485+00:00"
    },
    {
      "arxiv_id": "2506.15455v1",
      "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
      "title_zh": "RE-IMAGINEï¼šé¢å‘æ¨ç†è¯„ä¼°çš„ç¬¦å·åŒ–åŸºå‡†åˆæˆ",
      "authors": [
        "Xinnuo Xu",
        "Rachel Lawrence",
        "Kshitij Dubey",
        "Atharva Pandey",
        "Risa Ueno",
        "Fabian Falck",
        "Aditya V. Nori",
        "Rahul Sharma",
        "Amit Sharma",
        "Javier Gonzalez"
      ],
      "abstract": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning benchmarks. However, it is still unclear whether the observed results arise from true reasoning or from statistical recall of the training set. Inspired by the ladder of causation (Pearl, 2009) and its three levels (associations, interventions and counterfactuals), this paper introduces RE-IMAGINE, a framework to characterize a hierarchy of reasoning ability in LLMs, alongside an automated pipeline to generate problem variations at different levels of the hierarchy. By altering problems in an intermediate symbolic representation, RE-IMAGINE generates arbitrarily many problems that are not solvable using memorization alone. Moreover, the framework is general and can work across reasoning domains, including math, code, and logic. We demonstrate our framework on four widely-used benchmarks to evaluate several families of LLMs, and observe reductions in performance when the models are queried with problem variations. These assessments indicate a degree of reliance on statistical recall for past performance, and open the door to further research targeting skills across the reasoning hierarchy.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RE-IMAGINEï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„å±‚æ¬¡åŒ–æ¡†æ¶åŠå…¶è‡ªåŠ¨åŒ–ç”Ÿæˆç®¡çº¿ã€‚å— Pearl çš„å› æœé˜¶æ¢¯ï¼ˆladder of causationï¼‰å¯å‘ï¼Œè¯¥æ¡†æ¶å°†æ¨ç†èƒ½åŠ›åˆ’åˆ†ä¸ºå…³è”ï¼ˆassociationsï¼‰ã€å¹²é¢„ï¼ˆinterventionsï¼‰å’Œåäº‹å®ï¼ˆcounterfactualsï¼‰ä¸‰ä¸ªå±‚çº§ã€‚é€šè¿‡åœ¨ä¸­é—´ç¬¦å·è¡¨ç¤ºï¼ˆsymbolic representationï¼‰å±‚é¢ä¿®æ”¹é—®é¢˜ï¼ŒRE-IMAGINE èƒ½å¤Ÿç”Ÿæˆå¤§é‡æ— æ³•ä»…é€šè¿‡è®°å¿†è§£å†³çš„å˜ä½“é¢˜ç›®ï¼Œå¹¶å¹¿æ³›é€‚ç”¨äºæ•°å­¦ã€ä»£ç å’Œé€»è¾‘ç­‰æ¨ç†é¢†åŸŸã€‚åœ¨å››ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLLMs åœ¨é¢å¯¹è¿™äº›å˜ä½“é—®é¢˜æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ç°æœ‰æ¨¡å‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–ç»Ÿè®¡å¬å›ï¼ˆstatistical recallï¼‰è€ŒéçœŸæ­£çš„æ¨ç†ï¼Œä¸ºæœªæ¥é’ˆå¯¹æ¨ç†å±‚æ¬¡ç»“æ„çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15455v1",
      "published_date": "2025-06-18 13:35:47 UTC",
      "updated_date": "2025-06-18 13:35:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:23.259882+00:00"
    },
    {
      "arxiv_id": "2506.15453v1",
      "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation",
      "title_zh": "é€šè¿‡å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ä»£ç ç‰‡æ®µæè¿°ç”Ÿæˆæ­ç¤ºä»£ç æ„å›¾",
      "authors": [
        "Yusuf Sulistyo Nugroho",
        "Farah Danisha Salam",
        "Brittany Reid",
        "Raula Gaikovina Kula",
        "Kazumasa Shimari",
        "Kenichi Matsumoto"
      ],
      "abstract": "Documenting code snippets is essential to pinpoint key areas where both developers and users should pay attention. Examples include usage examples and other Application Programming Interfaces (APIs), which are especially important for third-party libraries. With the rise of Large Language Models (LLMs), the key goal is to investigate the kinds of description developers commonly use and evaluate how well an LLM, in this case Llama, can support description generation. We use NPM Code Snippets, consisting of 185,412 packages with 1,024,579 code snippets. From there, we use 400 code snippets (and their descriptions) as samples. First, our manual classification found that the majority of original descriptions (55.5%) highlight example-based usage. This finding emphasizes the importance of clear documentation, as some descriptions lacked sufficient detail to convey intent. Second, the LLM correctly identified the majority of original descriptions as \"Example\" (79.75%), which is identical to our manual finding, showing a propensity for generalization. Third, compared to the originals, the produced description had an average similarity score of 0.7173, suggesting relevance but room for improvement. Scores below 0.9 indicate some irrelevance. Our results show that depending on the task of the code snippet, the intention of the document may differ from being instructions for usage, installations, or descriptive learning examples for any user of a library.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆä»£ç ç‰‡æ®µæè¿°æ¥æ­ç¤ºå…¶æ„å›¾çš„é‡è¦æ€§ï¼Œé‡ç‚¹åˆ†æäº†å¼€å‘è€…å¸¸ç”¨çš„æè¿°ç±»å‹ä»¥åŠ Llama æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸçš„æ”¯æŒèƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨åŒ…å«è¶…è¿‡100ä¸‡ä¸ªä»£ç ç‰‡æ®µçš„ NPM Code Snippets æ•°æ®é›†ï¼Œé€‰å–400ä¸ªæ ·æœ¬è¿›è¡Œäº†äººå·¥åˆ†ç±»ä¸æ¨¡å‹è¯„ä¼°å¯¹æ¯”ã€‚æ‰‹å·¥åˆ†ç±»ç»“æœæ˜¾ç¤ºï¼Œ55.5%çš„åŸå§‹æè¿°ä¾§é‡äºåŸºäºç¤ºä¾‹çš„ç”¨æ³•(example-based usage)ï¼Œåæ˜ å‡ºéƒ¨åˆ†åŸå§‹æ–‡æ¡£åœ¨ä¼ è¾¾æ„å›¾æ–¹é¢å­˜åœ¨ç»†èŠ‚ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ Llama æ¨¡å‹åœ¨è¯†åˆ«â€œç¤ºä¾‹â€ç±»å‹æè¿°æ—¶è¾¾åˆ°äº†79.75%çš„å‡†ç¡®ç‡ï¼Œä¸äººå·¥åˆ†ç±»å‘ç°é«˜åº¦å»åˆï¼Œä½“ç°äº†å…¶æ³›åŒ–åˆ†æçš„æ½œåŠ›ã€‚ç”Ÿæˆæè¿°ä¸åŸå§‹æè¿°çš„å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†ä¸º0.7173ï¼Œæ˜¾ç¤ºå‡ºç›¸å…³æ€§çš„åŒæ—¶ä¹Ÿæ­ç¤ºäº†æ¨¡å‹åœ¨ç²¾å‡†åº¦ä¸Šä»æœ‰å¾…æ”¹è¿›ã€‚ç ”ç©¶ç»“è®ºå¼ºè°ƒï¼Œä»£ç ç‰‡æ®µæ–‡æ¡£çš„æ„å›¾ä¼šéšä»»åŠ¡ä¸åŒè€Œå‘ˆç°ä¸ºä½¿ç”¨æŒ‡ä»¤ã€å®‰è£…è¯´æ˜æˆ–æè¿°æ€§å­¦ä¹ ç¤ºä¾‹ç­‰å¤šç§å½¢å¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "6 pages, 3 figures, 4 tables, conference paper",
      "pdf_url": "https://arxiv.org/pdf/2506.15453v1",
      "published_date": "2025-06-18 13:33:34 UTC",
      "updated_date": "2025-06-18 13:33:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:14.225936+00:00"
    },
    {
      "arxiv_id": "2506.15452v1",
      "title": "Warping and Matching Subsequences Between Time Series",
      "title_zh": "æ—¶é—´åºåˆ—å­åºåˆ—è§„æ•´ä¸åŒ¹é…",
      "authors": [
        "Simiao Lin",
        "Wannes Meert",
        "Pieter Robberechts",
        "Hendrik Blockeel"
      ],
      "abstract": "Comparing time series is essential in various tasks such as clustering and classification. While elastic distance measures that allow warping provide a robust quantitative comparison, a qualitative comparison on top of them is missing. Traditional visualizations focus on point-to-point alignment and do not convey the broader structural relationships at the level of subsequences. This limitation makes it difficult to understand how and where one time series shifts, speeds up or slows down with respect to another. To address this, we propose a novel technique that simplifies the warping path to highlight, quantify and visualize key transformations (shift, compression, difference in amplitude). By offering a clearer representation of how subsequences match between time series, our method enhances interpretability in time series comparison.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—(Time Series)æ¯”è¾ƒä¸­ç¼ºä¹å®šæ€§åˆ†æçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€åŒ–å˜å½¢è·¯å¾„(Warping Path)çš„æ–°æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„å¼¹æ€§è·ç¦»åº¦é‡è™½ç„¶æä¾›äº†é²æ£’çš„å®šé‡æ¯”è¾ƒï¼Œä½†å…¶ç‚¹å¯¹ç‚¹å¯¹é½çš„å¯è§†åŒ–æ–¹å¼éš¾ä»¥æ­ç¤ºå­åºåˆ—(Subsequences)å±‚é¢çš„å®è§‚ç»“æ„å…³ç³»ï¼Œå¯¼è‡´éš¾ä»¥ç†è§£æ—¶é—´åºåˆ—é—´çš„åç§»æˆ–é€Ÿåº¦å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç®€åŒ–å˜å½¢è·¯å¾„æ¥çªå‡ºã€é‡åŒ–å¹¶å¯è§†åŒ–è¯¸å¦‚åç§»(Shift)ã€å‹ç¼©(Compression)ä»¥åŠæŒ¯å¹…å·®å¼‚(Difference in Amplitude)ç­‰å…³é”®å˜æ¢ã€‚è¯¥æŠ€æœ¯é€šè¿‡æä¾›å­åºåˆ—åŒ¹é…çš„æ›´æ¸…æ™°è¡¨ç¤ºï¼Œæ˜¾è‘—å¢å¼ºäº†æ—¶é—´åºåˆ—æ¯”è¾ƒçš„å¯è§£é‡Šæ€§(Interpretability)ã€‚è¿™ç§å®šæ€§åˆ†ææ‰‹æ®µä¸ºç†è§£å¤æ‚æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„ç»“æ„åŠ¨æ€æä¾›äº†é‡è¦è¡¥å……ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15452v1",
      "published_date": "2025-06-18 13:25:48 UTC",
      "updated_date": "2025-06-18 13:25:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:15.354291+00:00"
    },
    {
      "arxiv_id": "2506.15446v1",
      "title": "Zero-Shot Reinforcement Learning Under Partial Observability",
      "title_zh": "éƒ¨åˆ†å¯è§‚æµ‹æ€§ä¸‹çš„é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Scott Jeen",
        "Tom Bewley",
        "Jonathan M. Cullen"
      ],
      "abstract": "Recent work has shown that, under certain assumptions, zero-shot reinforcement learning (RL) methods can generalise to any unseen task in an environment after reward-free pre-training. Access to Markov states is one such assumption, yet, in many real-world applications, the Markov state is only partially observable. Here, we explore how the performance of standard zero-shot RL methods degrades when subjected to partially observability, and show that, as in single-task RL, memory-based architectures are an effective remedy. We evaluate our memory-based zero-shot RL methods in domains where the states, rewards and a change in dynamics are partially observed, and show improved performance over memory-free baselines. Our code is open-sourced via: https://enjeeneer.io/projects/bfms-with-memory/.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨éƒ¨åˆ†å¯è§‚æµ‹æ€§(Partial Observability)ä¸‹çš„é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ (Zero-Shot Reinforcement Learning)æ€§èƒ½è¡¨ç°ã€‚è™½ç„¶ç°æœ‰ç ”ç©¶è¡¨æ˜é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ åœ¨æ»¡è¶³é©¬å°”å¯å¤«çŠ¶æ€(Markov states)å‡è®¾æ—¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†åœ¨ç°å®åº”ç”¨ä¸­çŠ¶æ€å¾€å¾€éš¾ä»¥è¢«å®Œå…¨è§‚æµ‹ã€‚ä½œè€…æ·±å…¥åˆ†æäº†æ ‡å‡†é›¶æ ·æœ¬å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨é¢å¯¹éƒ¨åˆ†å¯è§‚æµ‹æ€§æ—¶å‡ºç°çš„æ€§èƒ½è¡°å‡é—®é¢˜ï¼Œå¹¶æå‡ºå¼•å…¥åŸºäºè®°å¿†çš„æ¶æ„(Memory-based architectures)ä½œä¸ºæœ‰æ•ˆçš„è¡¥æ•‘æ–¹æ¡ˆã€‚é€šè¿‡åœ¨çŠ¶æ€ã€å¥–åŠ±ä»¥åŠåŠ¨åŠ›å­¦å˜åŒ–å‡ä¸ºéƒ¨åˆ†è§‚æµ‹çš„å¤æ‚é¢†åŸŸè¿›è¡Œè¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜äº†åŸºäºè®°å¿†çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæ— è®°å¿†çš„åŸºå‡†æ¨¡å‹(Memory-free baselines)ã€‚è¯¥ç ”ç©¶ä¸ºæå‡å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¸ç¡®å®šç°å®ç¯å¢ƒä¸­çš„é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Reinforcement Learning Conference 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15446v1",
      "published_date": "2025-06-18 13:18:36 UTC",
      "updated_date": "2025-06-18 13:18:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:26.020658+00:00"
    },
    {
      "arxiv_id": "2506.15442v1",
      "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material",
      "title_zh": "Hunyuan3D 2.1ï¼šä»å›¾åƒåˆ°å…·æœ‰å·¥ä¸šçº§ PBR æè´¨çš„é«˜ä¿çœŸ 3D èµ„äº§",
      "authors": [
        "Team Hunyuan3D",
        "Shuhui Yang",
        "Mingxin Yang",
        "Yifei Feng",
        "Xin Huang",
        "Sheng Zhang",
        "Zebin He",
        "Di Luo",
        "Haolin Liu",
        "Yunfei Zhao",
        "Qingxiang Lin",
        "Zeqiang Lai",
        "Xianghui Yang",
        "Huiwen Shi",
        "Zibo Zhao",
        "Bowen Zhang",
        "Hongyu Yan",
        "Lifu Wang",
        "Sicong Liu",
        "Jihong Zhang",
        "Meng Chen",
        "Liang Dong",
        "Yiwen Jia",
        "Yulin Cai",
        "Jiaao Yu",
        "Yixuan Tang",
        "Dongyuan Guo",
        "Junlin Yu",
        "Hao Zhang",
        "Zheng Ye",
        "Peng He",
        "Runzhou Wu",
        "Shida Wei",
        "Chao Zhang",
        "Yonghao Tan",
        "Yifu Sun",
        "Lin Niu",
        "Shirui Huang",
        "Bojian Zheng",
        "Shu Liu",
        "Shilin Chen",
        "Xiang Yuan",
        "Xiaofeng Yang",
        "Kai Liu",
        "Jianchen Zhu",
        "Peng Chen",
        "Tian Liu",
        "Di Wang",
        "Yuhong Liu",
        "Linus",
        "Jie Jiang",
        "Jingwei Huang",
        "Chunchao Guo"
      ],
      "abstract": "3D AI-generated content (AIGC) is a passionate field that has significantly accelerated the creation of 3D models in gaming, film, and design. Despite the development of several groundbreaking models that have revolutionized 3D generation, the field remains largely accessible only to researchers, developers, and designers due to the complexities involved in collecting, processing, and training 3D models. To address these challenges, we introduce Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a comprehensive, step-by-step guide on processing 3D data, training a 3D generative model, and evaluating its performance using Hunyuan3D 2.1, an advanced system for producing high-resolution, textured 3D assets. The system comprises two core components: the Hunyuan3D-DiT for shape generation and the Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow, including data preparation, model architecture, training strategies, evaluation metrics, and deployment. By the conclusion of this tutorial, you will have the knowledge to finetune or develop a robust 3D generative model suitable for applications in gaming, virtual reality, and industrial design.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† Hunyuan3D 2.1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é™ä½ 3D å†…å®¹ç”Ÿæˆ (3D AIGC) é—¨æ§›å¹¶ç”Ÿäº§é«˜ä¿çœŸ 3D èµ„äº§çš„å…ˆè¿›ç³»ç»Ÿã€‚è¯¥ç³»ç»ŸåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šç”¨äºå½¢çŠ¶ç”Ÿæˆçš„ Hunyuan3D-DiT ä»¥åŠç”¨äºçº¹ç†åˆæˆçš„ Hunyuan3D-Paintï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰å·¥ä¸šçº§ PBR (Physically Based Rendering) æè´¨çš„é«˜åˆ†è¾¨ç‡ 3D æ¨¡å‹ã€‚é€šè¿‡æ·±å…¥æ¢è®¨æ•°æ®å¤„ç†ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œè¯¥ç ”ç©¶è¯¦ç»†å±•ç¤ºäº†ä»å›¾åƒåˆ°é«˜è´¨é‡ 3D èµ„äº§çš„å®Œæ•´å·¥ä½œæµã€‚Hunyuan3D 2.1 æ˜¾è‘—æå‡äº† 3D æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œä¸ºæ¸¸æˆã€è™šæ‹Ÿç°å® (VR) å’Œå·¥ä¸šè®¾è®¡ç­‰åº”ç”¨åœºæ™¯æä¾›äº†ç”Ÿäº§å°±ç»ªçš„è§£å†³æ–¹æ¡ˆã€‚è¯¥å·¥ä½œçš„æ•™ç¨‹åŒ–å±æ€§ä¸ºç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¾®è°ƒæˆ–æ„å»ºç¨³å¥çš„ 3D ç”Ÿæˆæ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1",
      "pdf_url": "https://arxiv.org/pdf/2506.15442v1",
      "published_date": "2025-06-18 13:14:46 UTC",
      "updated_date": "2025-06-18 13:14:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:23.103939+00:00"
    },
    {
      "arxiv_id": "2506.15758v1",
      "title": "Linear-Time Primitives for Algorithm Development in Graphical Causal Inference",
      "title_zh": "å›¾å› æœæ¨æ–­ç®—æ³•å¼€å‘çš„çº¿æ€§æ—¶é—´åŸè¯­",
      "authors": [
        "Marcel WienÃ¶bst",
        "Sebastian Weichwald",
        "Leonard Henckel"
      ],
      "abstract": "We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† CIflyï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå›¾å½¢å› æœæ¨æ–­ (Graphical Causal Inference) çš„é«˜æ•ˆç®—æ³•åŸè¯­æ¡†æ¶ï¼Œå°†å¯è¾¾æ€§ (Reachability) éš”ç¦»ä¸ºå¯é‡ç”¨çš„æ ¸å¿ƒæ“ä½œã€‚å…¶æ ¸å¿ƒæ´å¯Ÿåœ¨äºï¼Œè®¸å¤šå› æœæ¨ç†ä»»åŠ¡å¯ä»¥ç®€åŒ–ä¸ºåœ¨éå†è¿‡ç¨‹ä¸­åŠ¨æ€æ„å»ºçš„ç‰¹å®šçŠ¶æ€ç©ºé—´å›¾ (State-space graphs) ä¸­çš„å¯è¾¾æ€§é—®é¢˜ã€‚ç ”ç©¶è€…é€šè¿‡å½¢å¼åŒ–è§„åˆ™è¡¨æ¶æ„æŒ‡å®šç®—æ³•ï¼Œå¹¶è¯æ˜äº†å…¶å…·æœ‰çº¿æ€§æ—¶é—´ (Linear time) å¤æ‚åº¦ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„é“å¾·åŒ– (Moralization) å’Œæ½œåœ¨æŠ•å½± (Latent projection) ç­‰è®¡ç®—ä¸Šç­‰æ•ˆäºå¸ƒå°”çŸ©é˜µä¹˜æ³• (Boolean matrix multiplication) çš„æ–¹æ³•ï¼ŒCIfly æä¾›äº†æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¼€æº Rust å®ç°ï¼Œæ”¯æŒä» Python å’Œ R è°ƒç”¨çš„é«˜æ€§èƒ½æ‰§è¡Œï¼Œå¹¶é€šè¿‡é‡æ–°å®ç°å¤šç§å› æœæ¨æ–­ä»»åŠ¡åŠå¼€å‘æ–°çš„å·¥å…·å˜é‡ (Instrumental variables) ç®—æ³•å±•ç¤ºäº†å…¶æ•ˆç”¨ã€‚ä½œä¸ºå›¾å½¢å› æœæ¨æ–­çš„çµæ´»éª¨å¹²ï¼ŒCIfly æœ‰æ•ˆå¼•å¯¼äº†ç®—æ³•å¼€å‘å¹¶å®ç°äº†ç®€æ˜“ä¸”é«˜æ•ˆçš„éƒ¨ç½²ã€‚",
      "categories": [
        "cs.AI",
        "cs.DS",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15758v1",
      "published_date": "2025-06-18 12:52:25 UTC",
      "updated_date": "2025-06-18 12:52:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:29.583965+00:00"
    },
    {
      "arxiv_id": "2506.15421v1",
      "title": "Reward Models in Deep Reinforcement Learning: A Survey",
      "title_zh": "æ·±åº¦å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¨¡å‹ç»¼è¿°",
      "authors": [
        "Rui Yu",
        "Shenghua Wan",
        "Yucen Wang",
        "Chen-Xiao Gao",
        "Le Gan",
        "Zongzhang Zhang",
        "De-Chuan Zhan"
      ],
      "abstract": "In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature.",
      "tldr_zh": "è¯¥ç»¼è¿°è®ºæ–‡å¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)ä¸­çš„å¥–åŠ±æ¨¡å‹(Reward Models)è¿›è¡Œäº†å…¨é¢çš„ç³»ç»Ÿæ€§å›é¡¾ã€‚è®ºæ–‡è¯¦ç»†æ¢è®¨äº†å¥–åŠ±æ¨¡å‹å¦‚ä½•ä½œä¸ºç›®æ ‡ä»£ç†å¼•å¯¼æ™ºèƒ½ä½“è¿›è¡Œç­–ç•¥ä¼˜åŒ–(Policy Optimization)ï¼Œä»¥ç¡®ä¿å…¶è¡Œä¸ºä¸è®¾è®¡è€…çš„çœŸå®æ„å›¾ä¿æŒä¸€è‡´ã€‚ç ”ç©¶é€šè¿‡æ•°æ®æ¥æºã€å®ç°æœºåˆ¶å’Œå­¦ä¹ èŒƒå¼(Learning Paradigm)ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼Œå¯¹è¿‘å¹´æ¥çš„å¥–åŠ±æ¨¡å‹æ„å»ºæ–¹æ³•è¿›è¡Œäº†ç³»ç»ŸåŒ–çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æ·±å…¥è®¨è®ºäº†è¿™äº›æŠ€æœ¯åœ¨ä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨ç°çŠ¶ï¼Œå¹¶æ€»ç»“äº†è¯„ä¼°å¥–åŠ±æ¨¡å‹æœ‰æ•ˆæ€§çš„ä¸»æµæ–¹æ³•ã€‚æœ€åï¼Œè¯¥ç»¼è¿°å¡«è¡¥äº†å½“å‰æ–‡çŒ®ä¸­ç¼ºä¹å¥–åŠ±æ¨¡å‹ç³»ç»Ÿæ€§è¯„ä»·çš„ç©ºç™½ï¼Œå¹¶æŒ‡å‡ºäº†è¯¥é¢†åŸŸæœªæ¥å…·æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IJCAI 2025 Survey Track (To Appear)",
      "pdf_url": "https://arxiv.org/pdf/2506.15421v1",
      "published_date": "2025-06-18 12:46:39 UTC",
      "updated_date": "2025-06-18 12:46:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:39.973642+00:00"
    },
    {
      "arxiv_id": "2506.15408v1",
      "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI",
      "title_zh": "ç»Ÿä¸€ VXAIï¼šå¯è§£é‡Šäººå·¥æ™ºèƒ½è¯„ä¼°çš„ç³»ç»Ÿç»¼è¿°ä¸æ¡†æ¶",
      "authors": [
        "David Dembinsky",
        "Adriano Lucieri",
        "Stanislav Frolov",
        "Hiba Najjar",
        "Ko Watanabe",
        "Andreas Dengel"
      ],
      "abstract": "Modern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI). We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£ AI ç³»ç»Ÿä¸­é»‘ç›’æ¨¡å‹ç”±äºç¼ºä¹é€æ˜åº¦è€Œå¼•å‘çš„å¯ä¿¡åº¦æŒ‘æˆ˜ï¼Œæ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (XAI) åœ¨æä¾›äººç±»å¯ç†è§£è§£é‡Šæ–¹é¢çš„é‡è¦æ€§ã€‚ç”±äº XAI é¢†åŸŸç›®å‰ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’ŒæŒ‡æ ‡å…±è¯†ï¼Œä½œè€…éµå¾ª PRISMA æŒ‡å—å¼€å±•äº†ç³»ç»Ÿæ–‡çŒ®ç»¼è¿°ï¼Œå¹¶æå‡ºäº†ç»Ÿä¸€çš„ XAI è¯„ä¼°æ¡†æ¶ (VXAI)ã€‚ç ”ç©¶é€šè¿‡è¯†åˆ« 362 ç¯‡ç›¸å…³å‡ºç‰ˆç‰©ï¼Œå°†ç°æœ‰çš„è¯„ä¼°æ‰‹æ®µå½’çº³ä¸º 41 ä¸ªåŠŸèƒ½ç›¸ä¼¼çš„æŒ‡æ ‡ç»„ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªæ¶µç›–è§£é‡Šç±»å‹ (explanation type)ã€è¯„ä¼°æƒ…å¢ƒæ€§ (evaluation contextuality) å’Œè§£é‡Šè´¨é‡è¦ç´  (explanation quality desiderata) çš„ä¸‰ç»´åˆ†ç±»ä½“ç³»ã€‚VXAI æ¡†æ¶ä¸ºè¯„ä¼°å·¥ä½œæä¾›äº†ç›®å‰æœ€å…¨é¢ä¸”ç»“æ„åŒ–çš„æ¦‚è¿°ï¼Œæœ‰æ•ˆæ”¯æŒäº†ç³»ç»ŸåŒ–çš„æŒ‡æ ‡é€‰æ‹©å¹¶ä¿ƒè¿›äº†ä¸åŒæ–¹æ³•é—´çš„å¯æ¯”æ€§ã€‚è¿™ä¸€æˆæœä¸ä»…æ•´åˆäº†ç°æœ‰çš„è¯„ä¼°èµ„æºï¼Œè¿˜ä¸º XAI è¯„ä¼°é¢†åŸŸçš„æœªæ¥æ‰©å±•å¥ å®šäº†çµæ´»çš„åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to TMLR, under review",
      "pdf_url": "https://arxiv.org/pdf/2506.15408v1",
      "published_date": "2025-06-18 12:25:37 UTC",
      "updated_date": "2025-06-18 12:25:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:51.271374+00:00"
    },
    {
      "arxiv_id": "2506.15402v1",
      "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System",
      "title_zh": "MCOO-SLAMï¼šå¤šç›¸æœºå…¨å‘ç‰©ä½“çº§ SLAM ç³»ç»Ÿ",
      "authors": [
        "Miaoxin Pan",
        "Jinnan Li",
        "Yaowen Zhang",
        "Yi Yang",
        "Yufeng Yue"
      ],
      "abstract": "Object-level SLAM offers structured and semantically meaningful environment representations, making it more interpretable and suitable for high-level robotic tasks. However, most existing approaches rely on RGB-D sensors or monocular views, which suffer from narrow fields of view, occlusion sensitivity, and limited depth perception-especially in large-scale or outdoor environments. These limitations often restrict the system to observing only partial views of objects from limited perspectives, leading to inaccurate object modeling and unreliable data association. In this work, we propose MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully leverages surround-view camera configurations to achieve robust, consistent, and semantically enriched mapping in complex outdoor scenarios. Our approach integrates point features and object-level landmarks enhanced with open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is introduced for robust object association across multiple views, leading to improved consistency and accurate object modeling, and an omnidirectional loop closure module is designed to enable viewpoint-invariant place recognition using scene-level descriptors. Furthermore, the constructed map is abstracted into a hierarchical 3D scene graph to support downstream reasoning tasks. Extensive experiments in real-world demonstrate that MCOO-SLAM achieves accurate localization and scalable object-level mapping with improved robustness to occlusion, pose variation, and environmental complexity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¼ ç»Ÿç‰©ä½“çº§SLAM (Object-level SLAM) åœ¨å¤„ç†çª„è§†åœºã€é®æŒ¡æ•æ„Ÿä»¥åŠæ·±åº¦æ„ŸçŸ¥å—é™ç­‰é—®é¢˜ï¼Œæå‡ºäº†MCOO-SLAMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ‘„åƒå¤´å…¨å‘ç‰©ä½“SLAMç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå……åˆ†åˆ©ç”¨ç¯è§†æ‘„åƒå¤´é…ç½®ï¼Œåœ¨å¤æ‚çš„æˆ·å¤–åœºæ™¯ä¸­å®ç°é²æ£’ä¸”ä¸€è‡´çš„è¯­ä¹‰å¢å¼ºå»ºå›¾ã€‚MCOO-SLAM é›†æˆäº†ç‚¹ç‰¹å¾å’Œå¢å¼ºäº†å¼€æ”¾è¯æ±‡è¯­ä¹‰ (open-vocabulary semantics) çš„ç‰©ä½“çº§åœ°æ ‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§è¯­ä¹‰-å‡ ä½•-æ—¶åºèåˆç­–ç•¥ (semantic-geometric-temporal fusion strategy)ï¼Œä»¥å®ç°è·¨å¤šè§†å›¾çš„ç¨³å¥ç‰©ä½“å…³è”ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè®¾è®¡äº†ä¸€ä¸ªå…¨å‘é—­ç¯æ£€æµ‹æ¨¡å—ï¼Œåˆ©ç”¨åœºæ™¯çº§æè¿°ç¬¦å®ç°è§†ç‚¹æ— å…³çš„åœ°ç‚¹è¯†åˆ«ï¼Œå¹¶å°†æ„å»ºçš„åœ°å›¾æŠ½è±¡ä¸ºå±‚æ¬¡åŒ–3Dåœºæ™¯å›¾ (hierarchical 3D scene graph) ä»¥æ”¯æŒä¸‹æ¸¸æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCOO-SLAM åœ¨çœŸå®åœºæ™¯ä¸­å®ç°äº†ç²¾ç¡®çš„å®šä½å’Œå¯æ‰©å±•çš„ç‰©ä½“çº§å»ºå›¾ï¼Œåœ¨åº”å¯¹é®æŒ¡ã€å§¿æ€å˜åŒ–å’Œç¯å¢ƒå¤æ‚æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15402v1",
      "published_date": "2025-06-18 12:20:34 UTC",
      "updated_date": "2025-06-18 12:20:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:48.115786+00:00"
    },
    {
      "arxiv_id": "2506.15395v1",
      "title": "A Real-time Endoscopic Image Denoising System",
      "title_zh": "å®æ—¶å†…çª¥é•œå›¾åƒå»å™ªç³»ç»Ÿ",
      "authors": [
        "Yu Xing",
        "Shishi Huang",
        "Meng Lv",
        "Guo Chen",
        "Huailiang Wang",
        "Lingzhi Sui"
      ],
      "abstract": "Endoscopes featuring a miniaturized design have significantly enhanced operational flexibility, portability, and diagnostic capability while substantially reducing the invasiveness of medical procedures. Recently, single-use endoscopes equipped with an ultra-compact analogue image sensor measuring less than 1mm x 1mm bring revolutionary advancements to medical diagnosis. They reduce the structural redundancy and large capital expenditures associated with reusable devices, eliminate the risk of patient infections caused by inadequate disinfection, and alleviate patient suffering. However, the limited photosensitive area results in reduced photon capture per pixel, requiring higher photon sensitivity settings to maintain adequate brightness. In high-contrast medical imaging scenarios, the small-sized sensor exhibits a constrained dynamic range, making it difficult to simultaneously capture details in both highlights and shadows, and additional localized digital gain is required to compensate. Moreover, the simplified circuit design and analog signal transmission introduce additional noise sources. These factors collectively contribute to significant noise issues in processed endoscopic images. In this work, we developed a comprehensive noise model for analog image sensors in medical endoscopes, addressing three primary noise types: fixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise. Building on this analysis, we propose a hybrid denoising system that synergistically combines traditional image processing algorithms with advanced learning-based techniques for captured raw frames from sensors. Experiments demonstrate that our approach effectively reduces image noise without fine detail loss or color distortion, while achieving real-time performance on FPGA platforms and an average PSNR improvement from 21.16 to 33.05 on our test dataset.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸€æ¬¡æ€§å†…çª¥é•œä¸­è¶…å°å‹æ¨¡æ‹Ÿå›¾åƒä¼ æ„Ÿå™¨å¸¦æ¥çš„ä¸¥é‡å™ªå£°é—®é¢˜ï¼Œå¼€å‘äº†ä¸€å¥—å…¨é¢çš„å™ªå£°æ¨¡å‹ï¼Œæ¶µç›–äº†å›ºå®šæ¨¡å¼å™ªå£°(fixed-pattern noise)ã€å‘¨æœŸæ€§æ¡å¸¦å™ªå£°(periodic banding noise)ä»¥åŠæ··åˆæ³Šæ¾-é«˜æ–¯å™ªå£°(mixed Poisson-Gaussian noise)ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆä¼ ç»Ÿå›¾åƒå¤„ç†ç®—æ³•ä¸å…ˆè¿›å­¦ä¹ å‹æŠ€æœ¯(learning-based techniques)çš„æ··åˆå»å™ªç³»ç»Ÿï¼Œç”¨äºå¤„ç†ä¼ æ„Ÿå™¨æ•è·çš„åŸå§‹å¸§(raw frames)ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ‰æ•ˆå‡å°‘å›¾åƒå™ªå£°çš„åŒæ—¶ï¼Œé¿å…äº†ç»†èŠ‚ä¸¢å¤±å’Œè‰²å½©å¤±çœŸï¼Œå¹¶åœ¨FPGAå¹³å°ä¸Šå®ç°äº†å®æ—¶(real-time)æ€§èƒ½ã€‚åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œè¯¥ç³»ç»Ÿå°†å¹³å‡å³°å€¼ä¿¡å™ªæ¯”(PSNR)ä»21.16æ˜¾è‘—æå‡è‡³33.05ï¼Œä¸ºé«˜æ€§èƒ½å¾®å‹å†…çª¥é•œæˆåƒæä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15395v1",
      "published_date": "2025-06-18 12:12:10 UTC",
      "updated_date": "2025-06-18 12:12:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:44:40.663007+00:00"
    },
    {
      "arxiv_id": "2506.15388v1",
      "title": "Evaluation Pipeline for systematically searching for Anomaly Detection Systems",
      "title_zh": "ç³»ç»ŸåŒ–æœç´¢å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿçš„è¯„ä¼°æµç¨‹",
      "authors": [
        "Florian Rokohl",
        "Alexander Lehnert",
        "Marc Reichenbach"
      ],
      "abstract": "Digitalization in the medical world provides major benefits while making it a target for attackers and thus hard to secure. To deal with network intruders we propose an anomaly detection system on hardware to detect malicious clients in real-time. We meet real-time and power restrictions using FPGAs. Overall system performance is achieved via the presented holistic system evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—æ•°å­—åŒ–è½¬å‹å¸¦æ¥çš„ç½‘ç»œå®‰å…¨å¨èƒï¼Œæå‡ºäº†ä¸€ç§éƒ¨ç½²åœ¨ç¡¬ä»¶ä¸Šçš„å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ (Anomaly Detection System)ï¼Œæ—¨åœ¨å®æ—¶è¯†åˆ«æ¶æ„å®¢æˆ·ç«¯ã€‚ä¸ºäº†åœ¨æ»¡è¶³é«˜å®æ—¶æ€§è¦æ±‚çš„åŒæ—¶å…¼é¡¾åŠŸè€—é™åˆ¶ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº† FPGA ç¡¬ä»¶åŠ é€ŸæŠ€æœ¯è¿›è¡Œå®ç°ã€‚ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€å¥—ç”¨äºç³»ç»Ÿæ€§æœç´¢å’Œä¼˜åŒ–å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿçš„æ•´ä½“ç³»ç»Ÿè¯„ä¼° (holistic system evaluation) ç®¡çº¿ã€‚é€šè¿‡è¯¥è¯„ä¼°æµç¨‹ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿç³»ç»Ÿåœ°éªŒè¯ç¡¬ä»¶ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½ï¼Œä»è€Œç¡®ä¿åœ¨èµ„æºå—é™çš„åŒ»ç–—è®¾å¤‡ç½‘ç»œä¸­å®ç°é«˜æ•ˆçš„å®‰å…¨é˜²æŠ¤ã€‚è¯¥å·¥ä½œä¸ºå¹³è¡¡ç¡¬ä»¶æ€§èƒ½ä¸ç½‘ç»œå®‰å…¨éœ€æ±‚æä¾›äº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°èŒƒå¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Submitted to 18th HiPEAC Workshop on Reconfigurable Computing (WRC'2024)",
      "pdf_url": "https://arxiv.org/pdf/2506.15388v1",
      "published_date": "2025-06-18 12:03:49 UTC",
      "updated_date": "2025-06-18 12:03:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:14.687012+00:00"
    },
    {
      "arxiv_id": "2506.18924v1",
      "title": "Connecting Vision and Emissions: A Behavioural AI Approach to Carbon Estimation in Road Design",
      "title_zh": "è¡”æ¥è§†è§‰ä¸æ’æ”¾ï¼šé“è·¯è®¾è®¡ä¸­ç¢³æ’æ”¾ä¼°ç®—çš„è¡Œä¸ºäººå·¥æ™ºèƒ½æ–¹æ³•",
      "authors": [
        "Ammar K Al Mhdawi",
        "Nonso Nnamoko",
        "Safanah Mudheher Raafat",
        "M. K. S. Al-Mhdawi",
        "Amjad J Humaidi"
      ],
      "abstract": "We present an enhanced YOLOv8 real time vehicle detection and classification framework, for estimating carbon emissions in urban environments. The system enhances YOLOv8 architecture to detect, segment, and track vehicles from live traffic video streams. Once a vehicle is localized, a dedicated deep learning-based identification module is employed to recognize license plates and classify vehicle types. Since YOLOv8 lacks the built-in capacity for fine grained recognition tasks such as reading license plates or determining vehicle attributes beyond class labels, our framework incorporates a hybrid pipeline where each detected vehicle is tracked and its bounding box is cropped and passed to a deep Optical Character Recognition (OCR) module. This OCR system, composed of multiple convolutional neural network (CNN) layers, is trained specifically for character-level detection and license plate decoding under varied conditions such as motion blur, occlusion, and diverse font styles. Additionally, the recognized plate information is validated using a real time API that cross references with an external vehicle registration database to ensure accurate classification and emission estimation. This multi-stage approach enables precise, automated calculation of per vehicle carbon emissions. Extensive evaluation was conducted using a diverse vehicle dataset enriched with segmentation masks and annotated license plates. The YOLOv8 detector achieved a mean Average Precision (mAP@0.5) of approximately 71% for bounding boxes and 70% for segmentation masks. Character level OCR accuracy reached up to 99% with the best performing CNN model. These results affirm the feasibility of combining real time object detection with deep OCR for practical deployment in smart transportation systems, offering a scalable solution for automated, vehicle specific carbon emission monitoring.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºå‹ YOLOv8 å®æ—¶è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»æ¡†æ¶ï¼Œæ—¨åœ¨å‡†ç¡®ä¼°ç®—åŸå¸‚é“è·¯è®¾è®¡ä¸­çš„ç¢³æ’æ”¾ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ”¹è¿› YOLOv8 æ¶æ„å®ç°äº†å¯¹äº¤é€šè§†é¢‘æµä¸­è½¦è¾†çš„æ£€æµ‹ã€åˆ†å‰²ä¸è¿½è¸ªï¼Œå¹¶é’ˆå¯¹ç»†ç²’åº¦è¯†åˆ«éœ€æ±‚å¼•å…¥äº†é›†æˆæ·±åº¦ OCR æ¨¡å—çš„æ··åˆæµæ°´çº¿ã€‚è¯¥ OCR æ¨¡å—ç”±å¤šå±‚ CNN æ„æˆï¼Œä¸“é—¨ç”¨äºåœ¨è¿åŠ¨æ¨¡ç³Šå’Œé®æŒ¡ç­‰å¤æ‚ç¯å¢ƒä¸‹è¿›è¡Œè½¦ç‰Œè§£ç ï¼Œå¹¶é€šè¿‡ API æ¥å£ä¸å¤–éƒ¨è½¦è¾†ç™»è®°æ•°æ®åº“è¿›è¡Œå®æ—¶äº¤å‰æ ¡éªŒï¼Œä»¥ç¡®ä¿è½¦è¾†å±æ€§è¯†åˆ«å’Œæ’æ”¾ä¼°ç®—çš„ç²¾ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒYOLOv8 æ£€æµ‹å™¨åœ¨æ£€æµ‹æ¡†å’Œåˆ†å‰²æ©ç ä¸Šçš„ mAP@0.5 åˆ†åˆ«è¾¾åˆ°çº¦ 71% å’Œ 70%ï¼Œè€Œå­—ç¬¦çº§ OCR å‡†ç¡®ç‡é«˜è¾¾ 99%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ç»“åˆå®æ—¶ç›®æ ‡æ£€æµ‹ä¸æ·±åº¦ OCR æŠ€æœ¯åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„å¯è¡Œæ€§ï¼Œä¸ºè‡ªåŠ¨åŒ–çš„è½¦è¾†ç‰¹å®šç¢³æ’æ”¾ç›‘æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.18924v1",
      "published_date": "2025-06-18 11:50:24 UTC",
      "updated_date": "2025-06-18 11:50:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:27.786967+00:00"
    },
    {
      "arxiv_id": "2506.15377v1",
      "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation",
      "title_zh": "é¢å‘è§†è§‰å¯¼èˆªçš„é«˜æ•ˆä¸”å¯æ³›åŒ–çš„ç¯å¢ƒç†è§£",
      "authors": [
        "Ruoyu Wang",
        "Xinshu Li",
        "Chen Wang",
        "Lina Yao"
      ],
      "abstract": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·èº«æ™ºèƒ½(Embodied AI)ä¸­çš„è§†è§‰å¯¼èˆª(Visual Navigation)ä»»åŠ¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å†å²åºåˆ—æ•°æ®æ—¶å¿½è§†å†…éƒ¨å…³è”ç»“æ„çš„é—®é¢˜ã€‚ä½œè€…é€šè¿‡å› æœå…³ç³»(Causality)çš„è§†è§’åˆ†æäº†å¯¼èˆªä»»åŠ¡çš„ç‹¬ç‰¹æ€§ï¼Œå¹¶æå‡ºäº†å› æœæ„ŸçŸ¥å¯¼èˆª(Causality-Aware Navigation, CAN)æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨çš„å› æœç†è§£æ¨¡å—(Causal Understanding Module)ï¼Œé€šè¿‡æ•æ‰æ•°æ®é—´çš„å†…éƒ¨å…³è”æ¥æ˜¾è‘—å¢å¼ºæ™ºèƒ½ä½“çš„ç¯å¢ƒç†è§£èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCANåœ¨å¤šç§æ¨¡æ‹Ÿç¯å¢ƒå’Œä»»åŠ¡ä¸­æŒç»­ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¨¡å—åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å’Œç›‘ç£å­¦ä¹ (Supervised Learning)è®¾ç½®ä¸‹å‡å…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨æå‡æ€§èƒ½çš„åŒæ—¶ä¸ä¼šäº§ç”Ÿé¢å¤–çš„è®¡ç®—å¼€é”€(Computational Overhead)ï¼Œä¸ºå®ç°é«˜æ•ˆä¸”é€šç”¨çš„ç¯å¢ƒç†è§£æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15377v1",
      "published_date": "2025-06-18 11:47:02 UTC",
      "updated_date": "2025-06-18 11:47:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:17.626516+00:00"
    },
    {
      "arxiv_id": "2506.15368v2",
      "title": "Open-World Object Counting in Videos",
      "title_zh": "è§†é¢‘ä¸­çš„å¼€æ”¾ä¸–ç•Œç›®æ ‡è®¡æ•°",
      "authors": [
        "Niki Amini-Naieni",
        "Andrew Zisserman"
      ],
      "abstract": "We introduce a new task of open-world object counting in videos: given a text description, or an image example, that specifies the target object, the objective is to enumerate all the unique instances of the target objects in the video. This task is especially challenging in crowded scenes with occlusions and objects of similar appearance, where avoiding double counting and identifying reappearances is crucial. To this end, we make the following contributions: we introduce a model, CountVid, for this task. It leverages an image-based counting model, and a promptable video segmentation and tracking model, to enable automated open-world object counting across video frames. To evaluate its performance, we introduce VideoCount, a new dataset for this novel task built from the TAO and MOT20 tracking datasets, as well as from videos of penguins and metal alloy crystallization captured by x-rays. Using this dataset, we demonstrate that CountVid provides accurate object counts, and significantly outperforms strong baselines. The VideoCount dataset, the CountVid model, and all the code are available at https://www.robots.ox.ac.uk/~vgg/research/countvid/.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†è§†é¢‘ä¸­å¼€æ”¾ä¸–ç•Œç‰©ä½“è®¡æ•°(Open-World Object Counting)çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°æˆ–å›¾åƒç¤ºä¾‹ç»Ÿè®¡è§†é¢‘ä¸­ç›®æ ‡ç‰©ä½“çš„å”¯ä¸€å®ä¾‹æ•°é‡ã€‚é’ˆå¯¹æ‹¥æŒ¤åœºæ™¯ã€é®æŒ¡ä»¥åŠç›¸ä¼¼å¤–è§‚å¯¼è‡´çš„é‡å¤è®¡æ•°å’Œé‡ç°è¯†åˆ«éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†CountVidæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ•´åˆåŸºäºå›¾åƒçš„è®¡æ•°æ¨¡å‹ä¸å¯æç¤ºçš„è§†é¢‘åˆ†å‰²ä¸è·Ÿè¸ªæ¨¡å‹(Promptable Video Segmentation and Tracking)ï¼Œå®ç°äº†è·¨è§†é¢‘å¸§çš„è‡ªåŠ¨åŒ–ç‰©ä½“è®¡æ•°ã€‚ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è€…è¿˜åŸºäºTAOã€MOT20ä»¥åŠä¼é¹…å’Œé‡‘å±åˆé‡‘ç»“æ™¶è§†é¢‘æ„å»ºäº†VideoCountæ•°æ®é›†ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCountVidèƒ½å¤Ÿæä¾›ç²¾ç¡®çš„ç‰©ä½“è®¡æ•°ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2506.15368v2",
      "published_date": "2025-06-18 11:35:30 UTC",
      "updated_date": "2025-12-12 15:11:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:26.588078+00:00"
    },
    {
      "arxiv_id": "2506.15756v1",
      "title": "RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains",
      "title_zh": "RecBayesï¼šé¢å‘å¤§å‹éƒ¨åˆ†å¯è§‚æµ‹é¢†åŸŸçš„å¾ªç¯è´å¶æ–¯ä¸´æ—¶å›¢é˜Ÿåä½œ",
      "authors": [
        "JoÃ£o G. Ribeiro",
        "Yaniv Oren",
        "Alberto Sardinha",
        "Matthijs Spaan",
        "Francisco S. Melo"
      ],
      "abstract": "This paper proposes RecBayes, a novel approach for ad hoc teamwork under partial observability, a setting where agents are deployed on-the-fly to environments where pre-existing teams operate, that never requires, at any stage, access to the states of the environment or the actions of its teammates. We show that by relying on a recurrent Bayesian classifier trained using past experiences, an ad hoc agent is effectively able to identify known teams and tasks being performed from observations alone. Unlike recent approaches such as PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some stage fully observable states of the environment, actions of teammates, or both, or approaches such as ATPO (Ribeiro et al., 2023) that require the environments to be small enough to be tabularly modelled (Ribeiro et al., 2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes is both able to handle arbitrarily large spaces while never relying on either states and teammates' actions. Our results in benchmark domains from the multi-agent systems literature, adapted for partial observability and scaled up to 1M states and 2^125 observations, show that RecBayes is effective at identifying known teams and tasks being performed from partial observations alone, and as a result, is able to assist the teams in solving the tasks effectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RecBayesï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤§è§„æ¨¡éƒ¨åˆ†å¯è§‚æµ‹åŸŸ(Partially Observable Domains)ä¸‹å³æ—¶å›¢é˜Ÿåä½œ(Ad Hoc Teamwork)çš„æ–°å‹æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶åœ¨ä»»ä½•é˜¶æ®µéƒ½ä¸éœ€è¦è·å–ç¯å¢ƒçŠ¶æ€(States)æˆ–é˜Ÿå‹çš„åŠ¨ä½œ(Actions)ï¼Œä»…å‡­å±€éƒ¨è§‚å¯Ÿ(Observations)å³å¯å®ç°é«˜æ•ˆåä½œã€‚RecBayesåˆ©ç”¨åŸºäºè¿‡å»ç»éªŒè®­ç»ƒçš„å¾ªç¯è´å¶æ–¯åˆ†ç±»å™¨(Recurrent Bayesian Classifier)ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿä»è§‚å¯Ÿæµä¸­å‡†ç¡®è¯†åˆ«å·²çŸ¥çš„å›¢é˜ŸåŠå…¶æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ã€‚ç›¸æ¯”äºPO-GPLã€FEATæˆ–ATPOç­‰ç°æœ‰æ–¹æ³•ï¼ŒRecBayesä¸ä»…æ‘†è„±äº†å¯¹å…¨è§‚æµ‹ä¿¡æ¯çš„ä¾èµ–ï¼Œä¸”å…·å¤‡å¤„ç†ä»»æ„å¤§è§„æ¨¡æœç´¢ç©ºé—´çš„èƒ½åŠ›ã€‚å®éªŒåœ¨æ‰©å±•è‡³100ä¸‡ä¸ªçŠ¶æ€å’Œ2^125ä¸ªè§‚å¯Ÿç‚¹çš„åŸºå‡†åŸŸä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¯æ˜RecBayesåœ¨ä»…ä¾èµ–éƒ¨åˆ†è§‚æµ‹çš„æƒ…å†µä¸‹ï¼Œä¾ç„¶èƒ½æœ‰æ•ˆååŠ©å›¢é˜Ÿå®Œæˆä»»åŠ¡ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å¤§å‹ã€å¤æ‚ä¸”ä¿¡æ¯å—é™çš„ç¯å¢ƒä¸­å®ç°è‡ªä¸»å¤šæ™ºèƒ½ä½“åä½œæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15756v1",
      "published_date": "2025-06-18 11:30:52 UTC",
      "updated_date": "2025-06-18 11:30:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:22.792919+00:00"
    },
    {
      "arxiv_id": "2506.17312v1",
      "title": "Heterogeneous Temporal Hypergraph Neural Network",
      "title_zh": "å¼‚è´¨æ—¶åºè¶…å›¾ç¥ç»ç½‘ç»œ",
      "authors": [
        "Huan Liu",
        "Pengfei Jiao",
        "Mengzhou Gao",
        "Chaochao Chen",
        "Di Jin"
      ],
      "abstract": "Graph representation learning (GRL) has emerged as an effective technique for modeling graph-structured data. When modeling heterogeneity and dynamics in real-world complex networks, GRL methods designed for complex heterogeneous temporal graphs (HTGs) have been proposed and have achieved successful applications in various fields. However, most existing GRL methods mainly focus on preserving the low-order topology information while ignoring higher-order group interaction relationships, which are more consistent with real-world networks. In addition, most existing hypergraph methods can only model static homogeneous graphs, limiting their ability to model high-order interactions in HTGs. Therefore, to simultaneously enable the GRL model to capture high-order interaction relationships in HTGs, we first propose a formal definition of heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge construction algorithm that does not rely on additional information. Then, a novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical attention mechanism module that simultaneously performs temporal message-passing between heterogeneous nodes and hyperedges to capture rich semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN performs contrastive learning by maximizing the consistency between low-order correlated heterogeneous node pairs on HTG to avoid the low-order structural ambiguity issue. Detailed experimental results on three real-world HTG datasets verify the effectiveness of the proposed HTHGN for modeling high-order interactions in HTGs and demonstrate significant performance improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å›¾è¡¨ç¤ºå­¦ä¹  (Graph Representation Learning) åœ¨å¤„ç†å¼‚æ„æ—¶åºå›¾ (Heterogeneous Temporal Graphs, HTGs) æ—¶ï¼Œå¾€å¾€åªå…³æ³¨ä½é˜¶æ‹“æ‰‘è€Œå¿½ç•¥é«˜é˜¶ç¾¤ç»„äº¤äº’çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¼‚æ„æ—¶åºè¶…å›¾ç¥ç»ç½‘ç»œ (HTHGN)ã€‚ä½œè€…é¦–å…ˆç»™å‡ºäº†å¼‚æ„æ—¶åºè¶…å›¾çš„æ­£å¼å®šä¹‰ï¼Œå¹¶è®¾è®¡äº† $P$-uniform å¼‚æ„è¶…è¾¹æ„å»ºç®—æ³•ï¼Œä»¥åœ¨ä¸ä¾èµ–é¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹æ•æ‰é«˜é˜¶å…³ç³»ã€‚HTHGN æ ¸å¿ƒåŒ…å«ä¸€ä¸ªå±‚æ¬¡åŒ–æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—ï¼Œé€šè¿‡åœ¨å¼‚æ„èŠ‚ç‚¹ä¸è¶…è¾¹ä¹‹é—´è¿›è¡Œæ—¶åºæ¶ˆæ¯ä¼ é€’ï¼Œä»è€Œåœ¨æ›´å¹¿çš„æ„Ÿå—é‡ä¸­æå–ä¸°å¯Œè¯­ä¹‰ã€‚ä¸ºäº†è§£å†³ä½é˜¶ç»“æ„æ­§ä¹‰ï¼Œè¯¥æ¡†æ¶è¿˜å¼•å…¥äº†å¯¹æ¯”å­¦ä¹  (Contrastive Learning) æ¥å¢å¼ºä½é˜¶ç›¸å…³å¼‚æ„èŠ‚ç‚¹å¯¹çš„ä¸€è‡´æ€§ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œçš„ HTG æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒHTHGN åœ¨å»ºæ¨¡é«˜é˜¶äº¤äº’æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.17312v1",
      "published_date": "2025-06-18 10:36:11 UTC",
      "updated_date": "2025-06-18 10:36:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:29.489922+00:00"
    },
    {
      "arxiv_id": "2506.15329v1",
      "title": "When and How Unlabeled Data Provably Improve In-Context Learning",
      "title_zh": "æ— æ ‡ç­¾æ•°æ®ä½•æ—¶ä»¥åŠå¦‚ä½•å¯è¯æ˜åœ°æå‡ä¸Šä¸‹æ–‡å­¦ä¹ ",
      "authors": [
        "Yingcong Li",
        "Xiangyu Chang",
        "Muti Kara",
        "Xiaofeng Liu",
        "Amit Roy-Chowdhury",
        "Samet Oymak"
      ],
      "abstract": "Recent research shows that in-context learning (ICL) can be effective even when demonstrations have missing or incorrect labels. To shed light on this capability, we examine a canonical setting where the demonstrations are drawn according to a binary Gaussian mixture model (GMM) and a certain fraction of the demonstrations have missing labels. We provide a comprehensive theoretical study to show that: (1) The loss landscape of one-layer linear attention models recover the optimal fully-supervised estimator but completely fail to exploit unlabeled data; (2) In contrast, multilayer or looped transformers can effectively leverage unlabeled data by implicitly constructing estimators of the form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting features and partially-observed labels (with missing entries set to zero). We characterize the class of polynomials that can be expressed as a function of depth and draw connections to Expectation Maximization, an iterative pseudo-labeling algorithm commonly used in semi-supervised learning. Importantly, the leading polynomial power is exponential in depth, so mild amount of depth/looping suffices. As an application of theory, we propose looping off-the-shelf tabular foundation models to enhance their semi-supervision capabilities. Extensive evaluations on real-world datasets show that our method significantly improves the semisupervised tabular learning performance over the standard single pass inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning, ICL) åœ¨æ¼”ç¤ºç¤ºä¾‹åŒ…å«æ— æ ‡ç­¾æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åŸºäºäºŒå…ƒé«˜æ–¯æ··åˆæ¨¡å‹ (Gaussian mixture model, GMM) è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå•å±‚çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ (one-layer linear attention models) æ— æ³•åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ï¼Œè€Œå¤šå±‚æˆ–å¾ªç¯å˜æ¢å™¨ (multilayer or looped transformers) åˆ™èƒ½é€šè¿‡éšå¼æ„å»ºå¤šé¡¹å¼ä¼°è®¡å™¨æ¥æœ‰æ•ˆæå–ä¿¡æ¯ã€‚è¿™äº›å¤šé¡¹å¼ä¼°è®¡å™¨åœ¨é€»è¾‘ä¸Šä¸åŠç›‘ç£å­¦ä¹ ä¸­çš„æœŸæœ›æœ€å¤§åŒ– (Expectation Maximization, EM) ç®—æ³•ç›¸é€šï¼Œä¸”å…¶å¤šé¡¹å¼é˜¶æ•°éšæ¨¡å‹æ·±åº¦æŒ‡æ•°çº§å¢é•¿ã€‚åŸºäºæ­¤ç†è®ºï¼Œç ”ç©¶è€…æå‡ºé€šè¿‡å¾ªç¯æ‰§è¡Œç°æˆçš„è¡¨æ ¼åŸºç¡€æ¨¡å‹ (tabular foundation models) æ¥å¢å¼ºå…¶åŠç›‘ç£å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸æ¯”æ ‡å‡†çš„å•æ¬¡æ¨ç† (single pass inference) æ˜¾è‘—æå‡äº†åŠç›‘ç£è¡¨æ ¼å­¦ä¹ çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15329v1",
      "published_date": "2025-06-18 10:01:17 UTC",
      "updated_date": "2025-06-18 10:01:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:39.285211+00:00"
    },
    {
      "arxiv_id": "2506.15316v1",
      "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor",
      "title_zh": "J3DAIï¼šé¢å‘ 3D å †å  CMOS å›¾åƒä¼ æ„Ÿå™¨çš„å¾®å‹ DNN è¾¹ç¼˜ AI åŠ é€Ÿå™¨",
      "authors": [
        "Benoit Tain",
        "Raphael Millet",
        "Romain Lemaire",
        "Michal Szczepanski",
        "Laurent Alacoque",
        "Emmanuel Pluchart",
        "Sylvain Choisnet",
        "Rohit Prasad",
        "Jerome Chossat",
        "Pascal Pierunek",
        "Pascal Vivet",
        "Sebastien Thuries"
      ],
      "abstract": "This paper presents J3DAI, a tiny deep neural network-based hardware accelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial intelligence (AI) chip integrating a Deep Neural Network (DNN)-based accelerator. The DNN accelerator is designed to efficiently perform neural network tasks such as image classification and segmentation. This paper focuses on the digital system of J3DAI, highlighting its Performance-Power-Area (PPA) characteristics and showcasing advanced edge AI capabilities on a CMOS image sensor. To support hardware, we utilized the Aidge comprehensive software framework, which enables the programming of both the host processor and the DNN accelerator. Aidge supports post-training quantization, significantly reducing memory footprint and computational complexity, making it crucial for deploying models on resource-constrained hardware like J3DAI. Our experimental results demonstrate the versatility and efficiency of this innovative design in the field of edge AI, showcasing its potential to handle both simple and computationally intensive tasks. Future work will focus on further optimizing the architecture and exploring new applications to fully leverage the capabilities of J3DAI. As edge AI continues to grow in importance, innovations like J3DAI will play a crucial role in enabling real-time, low-latency, and energy-efficient AI processing at the edge.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†J3DAIï¼Œä¸€ç§ä¸“ä¸ºä¸‰å±‚3D-stacked CMOS image sensorè®¾è®¡çš„å¾®å‹DNN-basedç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œæ—¨åœ¨é€šè¿‡é›†æˆAIèŠ¯ç‰‡é«˜æ•ˆæ‰§è¡Œå›¾åƒåˆ†ç±»ä¸åˆ†å‰²ä»»åŠ¡ã€‚ç ”ç©¶é‡ç‚¹é˜è¿°äº†J3DAIçš„æ•°å­—ç³»ç»ŸåŠå…¶åœ¨Performance-Power-Area (PPA) æ–¹é¢çš„ç‰¹æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨CMOSå›¾åƒä¼ æ„Ÿå™¨ä¸Šçš„å…ˆè¿›edge AIèƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒç¡¬ä»¶è¿è¡Œï¼Œç ”ç©¶é‡‡ç”¨äº†Aidgeè½¯ä»¶æ¡†æ¶ï¼Œåˆ©ç”¨post-training quantizationæŠ€æœ¯æ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨å’Œè®¡ç®—å¤æ‚åº¦ï¼Œè§£å†³äº†èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²éš¾é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥è®¾è®¡åœ¨å¤„ç†ç®€å•åŠè®¡ç®—å¯†é›†å‹ä»»åŠ¡æ—¶å‡å…·æœ‰æé«˜çš„çµæ´»æ€§ä¸æ•ˆç‡ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶ã€ä½å»¶è¿Ÿä¸”èƒ½æºé«˜æ•ˆçš„è¾¹ç¼˜ç«¯AIå¤„ç†ã€‚J3DAIçš„åˆ›æ–°ä¸ºæœªæ¥åœ¨è¾¹ç¼˜è®¡ç®—é¢†åŸŸå®ç°é«˜æ€§èƒ½AIé›†æˆæä¾›äº†é‡è¦æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00 \\c{opyright}2025 IEEE",
      "pdf_url": "https://arxiv.org/pdf/2506.15316v1",
      "published_date": "2025-06-18 09:46:02 UTC",
      "updated_date": "2025-06-18 09:46:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:35.790776+00:00"
    },
    {
      "arxiv_id": "2506.15313v1",
      "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning",
      "title_zh": "MapFMï¼šåŸºäºåŸºç¡€æ¨¡å‹é©±åŠ¨ä¸å¤šä»»åŠ¡ä¸Šä¸‹æ–‡å­¦ä¹ çš„é«˜ç²¾åœ°å›¾æ„å»º",
      "authors": [
        "Leonid Ivanov",
        "Vasily Yuryev",
        "Dmitry Yudin"
      ],
      "abstract": "In autonomous driving, high-definition (HD) maps and semantic maps in bird's-eye view (BEV) are essential for accurate localization, planning, and decision-making. This paper introduces an enhanced End-to-End model named MapFM for online vectorized HD map generation. We show significantly boost feature representation quality by incorporating powerful foundation model for encoding camera images. To further enrich the model's understanding of the environment and improve prediction quality, we integrate auxiliary prediction heads for semantic segmentation in the BEV representation. This multi-task learning approach provides richer contextual supervision, leading to a more comprehensive scene representation and ultimately resulting in higher accuracy and improved quality of the predicted vectorized HD maps. The source code is available at https://github.com/LIvanoff/MapFM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MapFMï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºçš„ç«¯åˆ°ç«¯(End-to-End)æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºåœ¨çº¿çŸ¢é‡åŒ–é«˜ç²¾åœ°å›¾(HD map)çš„ç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¼ºå¤§çš„åŸºç¡€æ¨¡å‹(foundation model)æ¥ç¼–ç æ‘„åƒæœºå›¾åƒï¼Œæ˜¾è‘—æå‡äº†ç‰¹å¾è¡¨ç¤ºçš„è´¨é‡ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯¹ç¯å¢ƒçš„ç†è§£å¹¶ä¼˜åŒ–é¢„æµ‹æ•ˆæœï¼Œç ”ç©¶å›¢é˜Ÿé›†æˆäº†ç”¨äºé¸Ÿç°å›¾(BEV)è¯­ä¹‰åˆ†å‰²çš„è¾…åŠ©é¢„æµ‹å¤´ã€‚è¿™ç§å¤šä»»åŠ¡å­¦ä¹ (multi-task learning)æ–¹æ³•èƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç›‘ç£ï¼Œä»è€Œæ„å»ºå‡ºæ›´å…¨é¢çš„åœºæ™¯è¡¨å¾ã€‚å®éªŒè¯æ˜ï¼ŒMapFM æœ€ç»ˆæ˜¾è‘—æé«˜äº†é¢„æµ‹çŸ¢é‡åŒ–é«˜ç²¾åœ°å›¾çš„å‡†ç¡®æ€§ä¸è´¨é‡ã€‚ç›®å‰ï¼Œè¯¥ç ”ç©¶çš„æºä»£ç å·²åœ¨ç›¸å…³å¹³å°ä¸Šå…¬å¼€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. Submitted. 12 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15313v1",
      "published_date": "2025-06-18 09:42:30 UTC",
      "updated_date": "2025-06-18 09:42:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:44.994240+00:00"
    },
    {
      "arxiv_id": "2506.15309v1",
      "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation",
      "title_zh": "ä¸»åŠ¨å­¦ä¹ å¼•å¯¼çš„ Seq2Seq å˜åˆ†è‡ªç¼–ç å™¨ç”¨äºå¤šé¶ç‚¹æŠ‘åˆ¶å‰‚ç”Ÿæˆ",
      "authors": [
        "JÃºlia Vilalta-Mor",
        "Alexis Molina",
        "Laura Ortega Varga",
        "Isaac Filella-Merce",
        "Victor Guallar"
      ],
      "abstract": "Simultaneously optimizing molecules against multiple therapeutic targets remains a profound challenge in drug discovery, particularly due to sparse rewards and conflicting design constraints. We propose a structured active learning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational autoencoder (VAE) into iterative loops designed to balance chemical diversity, molecular quality, and multi-target affinity. Our method alternates between expanding chemically feasible regions of latent space and progressively constraining molecules based on increasingly stringent multi-target docking thresholds. In a proof-of-concept study targeting three related coronavirus main proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently generated a structurally diverse set of pan-inhibitor candidates. We demonstrate that careful timing and strategic placement of chemical filters within this active learning pipeline markedly enhance exploration of beneficial chemical space, transforming the sparse-reward, multi-objective drug design problem into an accessible computational task. Our framework thus provides a generalizable roadmap for efficiently navigating complex polypharmacological landscapes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰ä¸åºåˆ—åˆ°åºåˆ—å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆSeq2Seq Variational Autoencoder, VAEï¼‰çš„ç»“æ„åŒ–èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤šé¶ç‚¹è¯ç‰©å‘ç°ä¸­å¥–åŠ±ç¨€ç–å’Œè®¾è®¡çº¦æŸå†²çªçš„æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£å¾ªç¯å¹³è¡¡åŒ–å­¦å¤šæ ·æ€§ã€åˆ†å­è´¨é‡å’Œå¤šé¶ç‚¹äº²å’ŒåŠ›ï¼Œåœ¨æ‰©å±•æ½œç©ºé—´çš„åŒ–å­¦å¯è¡ŒåŒºåŸŸä¸åº”ç”¨ä¸¥æ ¼çš„å¤šé¶ç‚¹å¯¹æ¥ï¼ˆdockingï¼‰é˜ˆå€¼çº¦æŸä¹‹é—´è¿›è¡Œäº¤æ›¿ã€‚åœ¨é’ˆå¯¹ SARS-CoV-2ã€SARS-CoV å’Œ MERS-CoV ä¸‰ç§å† çŠ¶ç—…æ¯’ä¸»è›‹ç™½é…¶çš„éªŒè¯ç ”ç©¶ä¸­ï¼Œè¯¥æ¡†æ¶æˆåŠŸé«˜æ•ˆåœ°ç”Ÿæˆäº†ä¸€ç»„å…·æœ‰ç»“æ„å¤šæ ·æ€§çš„å…¨æŠ‘åˆ¶å‰‚ï¼ˆpan-inhibitorï¼‰å€™é€‰è¯ç‰©ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ä¸»åŠ¨å­¦ä¹ æµç¨‹ä¸­æˆ˜ç•¥æ€§åœ°è®¾ç½®åŒ–å­¦è¿‡æ»¤å™¨èƒ½æ˜¾è‘—å¢å¼ºå¯¹æœ‰ç›ŠåŒ–å­¦ç©ºé—´çš„æ¢ç´¢æ•ˆç‡ï¼Œå°†å¤æ‚çš„å¤šç›®æ ‡è®¾è®¡é—®é¢˜è½¬åŒ–ä¸ºå¯è¡Œçš„è®¡ç®—ä»»åŠ¡ã€‚è¯¥æ¡†æ¶ä¸ºé«˜æ•ˆæ¢ç´¢å¤æ‚çš„è¯ç†å­¦ï¼ˆpolypharmacologicalï¼‰æ™¯è§‚æä¾›äº†ä¸€æ¡å…·æœ‰é€šç”¨æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15309v1",
      "published_date": "2025-06-18 09:39:51 UTC",
      "updated_date": "2025-06-18 09:39:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:45:47.186515+00:00"
    },
    {
      "arxiv_id": "2506.15304v1",
      "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification",
      "title_zh": "ConLIDï¼šé¢å‘ä½èµ„æºè¯­è¨€è¯†åˆ«çš„æœ‰ç›‘ç£å¯¹æ¯”å­¦ä¹ ",
      "authors": [
        "Negar Foroutan",
        "Jakhongir Saydaliev",
        "Ye Eun Kim",
        "Antoine Bosselut"
      ],
      "abstract": "Language identification (LID) is a critical step in curating multilingual LLM pretraining corpora from web crawls. While many studies on LID model training focus on collecting diverse training data to improve performance, low-resource languages -- often limited to single-domain data, such as the Bible -- continue to perform poorly. To resolve these class imbalance and bias issues, we propose a novel supervised contrastive learning (SCL) approach to learn domain-invariant representations for low-resource languages. Through an extensive analysis, we show that our approach improves LID performance on out-of-domain data for low-resource languages by 3.2%, demonstrating its effectiveness in enhancing LID models.",
      "tldr_zh": "è¯­è¨€è¯†åˆ«(Language identification, LID)æ˜¯æ„å»ºå¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹(LLM)é¢„è®­ç»ƒè¯­æ–™åº“çš„å…³é”®ç¯èŠ‚ï¼Œä½†ä½èµ„æºè¯­è¨€å› å¸¸å—é™äºå•é¢†åŸŸæ•°æ®ï¼ˆå¦‚åœ£ç»ï¼‰è€Œå¯¼è‡´æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³ä½èµ„æºè¯­è¨€å­˜åœ¨çš„ç±»åˆ«ä¸å¹³è¡¡å’Œé¢†åŸŸåå·®é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ConLIDæ¡†æ¶ï¼Œé‡‡ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ (Supervised Contrastive Learning, SCL)æ¥å­¦ä¹ é¢†åŸŸæ— å…³è¡¨å¾(domain-invariant representations)ã€‚å¹¿æ³›çš„åˆ†æç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿ä½èµ„æºè¯­è¨€åœ¨å¼‚åŸŸæ•°æ®(out-of-domain data)ä¸Šçš„è¯†åˆ«æ€§èƒ½æå‡äº†3.2%ã€‚å®éªŒç»“æœå……åˆ†è¯æ˜äº†ConLIDåœ¨å¢å¼ºè¯­è¨€è¯†åˆ«æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¼˜åŒ–ä½èµ„æºè¯­è¨€çš„è¯­æ–™ç­›é€‰æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to EMNLP",
      "pdf_url": "https://arxiv.org/pdf/2506.15304v1",
      "published_date": "2025-06-18 09:35:33 UTC",
      "updated_date": "2025-06-18 09:35:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:08.797745+00:00"
    },
    {
      "arxiv_id": "2506.15301v3",
      "title": "A Survey on LLM-Assisted Clinical Trial Recruitment",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ä¸´åºŠè¯•éªŒæ‹›å‹Ÿç»¼è¿°",
      "authors": [
        "Shrestha Ghosh",
        "Moritz Schneider",
        "Carina Reinicke",
        "Carsten Eickhoff"
      ],
      "abstract": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸´åºŠè¯•éªŒæ‹›å‹Ÿï¼ˆClinical Trial Recruitmentï¼‰ä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºå°½ç®¡ LLMs åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸè¡¨ç°å“è¶Šï¼Œä½†åœ¨ä¸´åºŠè¯•éªŒè¿™ä¸€å…³é”®é¢†åŸŸçš„æ•´åˆä»é¢ä¸´æŒ‘æˆ˜ã€‚ç”±äºä¸´åºŠè¯•éªŒæ–¹æ¡ˆå’Œæ‚£è€…æ•°æ®å‡æ¶‰åŠå¤æ‚çš„è‡ªç„¶è¯­è¨€å’Œéç»“æ„åŒ–æ–‡æœ¬ï¼ŒLLMs çš„çŸ¥è¯†èšåˆä¸æ¨ç†èƒ½åŠ›ä¸ºä¼˜åŒ–è¯•éªŒä¸æ‚£è€…åŒ¹é…ï¼ˆTrial-patient matchingï¼‰æä¾›äº†å·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹ä¸´åºŠæ‹›å‹Ÿä¸­åŸºäº LLM çš„æ–°å…´æ–¹æ³•è¿›è¡Œäº†èƒŒæ™¯åŒ–åˆ†æï¼Œæ—¨åœ¨æ¢ç´¢æ¯”ä¼ ç»Ÿé’ˆå¯¹ç‰¹å®šè¯•éªŒçš„æ–¹æ³•æ›´å…·é€šç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚ä½œè€…æ‰¹åˆ¤æ€§åœ°å®¡æŸ¥äº†ç°æœ‰çš„è¯„ä¼°åŸºå‡†ï¼ˆBenchmarksï¼‰ã€ç ”ç©¶æ–¹æ³•åŠè¯„ä»·æ¡†æ¶ï¼Œå¹¶æŒ‡å‡ºäº†ç›®å‰è¿‡åº¦ä¾èµ–ä¸“æœ‰æ¨¡å‹å’Œè¯„ä¼°åŸºå‡†å¼ºåº¦ä¸è¶³ç­‰ç°çŠ¶ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æ€»ç»“äº†åœ¨ä¸´åºŠç ”ç©¶ä¸­éƒ¨ç½² LLM æŠ€æœ¯æ‰€é¢ä¸´çš„å®é™…éšœç¢ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•æŒ‡æ˜äº†æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to IJCNLP-AACl 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15301v3",
      "published_date": "2025-06-18 09:32:16 UTC",
      "updated_date": "2025-12-30 10:28:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:05.893150+00:00"
    },
    {
      "arxiv_id": "2507.01029v1",
      "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning",
      "title_zh": "PathCoTï¼šé¢å‘é›¶æ ·æœ¬ç—…ç†è§†è§‰æ¨ç†çš„é“¾å¼æ€ç»´æç¤º",
      "authors": [
        "Junjie Zhou",
        "Yingli Zuo",
        "Shichang Feng",
        "Peng Wan",
        "Qi Zhu",
        "Daoqiang Zhang",
        "Wei Shao"
      ],
      "abstract": "With the development of generative artificial intelligence and instruction tuning techniques, multimodal large language models (MLLMs) have made impressive progress on general reasoning tasks. Benefiting from the chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning problem step-by-step. However, existing MLLMs still face significant challenges when applied to pathology visual reasoning tasks: (1) LLMs often underperforms because they lack domain-specific information, which can lead to model hallucinations. (2) The additional reasoning steps in CoT may introduce errors, leading to the divergence of answers. To address these limitations, we propose PathCoT, a novel zero-shot CoT prompting method which integrates the pathology expert-knowledge into the reasoning process of MLLMs and incorporates self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides the MLLM with prior knowledge to perform as pathology experts, and provides comprehensive analysis of the image with their domain-specific knowledge. By incorporating the experts' knowledge, PathCoT can obtain the answers with CoT reasoning. Furthermore, PathCoT incorporates a self-evaluation step that assesses both the results generated directly by MLLMs and those derived through CoT, finally determining the reliable answer. The experimental results on the PathMMU dataset demonstrate the effectiveness of our method on pathology visual understanding and reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PathCoTï¼Œä¸€ç§ç”¨äºé›¶æ ·æœ¬(Zero-shot)ç—…ç†è§†è§‰æ¨ç†çš„é“¾å¼æ€ç»´(Chain-of-Thought)æç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨ç—…ç†é¢†åŸŸå› ç¼ºä¹ä¸“ä¸šçŸ¥è¯†è€Œå¯¼è‡´çš„å¹»è§‰å’Œæ¨ç†æ­¥éª¤é”™è¯¯é—®é¢˜ã€‚è¯¥æ–¹æ³•å°†ç—…ç†ä¸“å®¶çŸ¥è¯†é›†æˆåˆ° MLLMs çš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¼•å¯¼æ¨¡å‹æ¨¡æ‹Ÿä¸“å®¶è§†è§’å¹¶åˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†å¯¹å›¾åƒè¿›è¡Œæ·±å…¥åˆ†æã€‚PathCoT é€šè¿‡å¼•å…¥å…ˆéªŒçŸ¥è¯†æ¥è¾…åŠ©ç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œå¹¶å¢åŠ äº†ä¸€ä¸ªå…³é”®çš„è‡ªæˆ‘è¯„ä¼°ç¯èŠ‚ï¼Œé€šè¿‡å¯¹æ¯”æ¨¡å‹ç›´æ¥ç”Ÿæˆçš„ç­”æ¡ˆä¸é€šè¿‡ CoT æ¨ç†å¾—å‡ºçš„ç»“æœï¼Œä»è€Œç­›é€‰å‡ºæœ€å¯é çš„ç»“è®ºä»¥å‡å°‘ç­”æ¡ˆåç¦»ã€‚åœ¨ PathMMU æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœå……åˆ†è¯æ˜äº† PathCoT åœ¨å¢å¼ºç—…ç†å›¾åƒç†è§£å’Œè§†è§‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01029v1",
      "published_date": "2025-06-18 09:20:23 UTC",
      "updated_date": "2025-06-18 09:20:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:10.493633+00:00"
    },
    {
      "arxiv_id": "2506.15290v2",
      "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models",
      "title_zh": "åŸºäºæœè£…æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹çš„å®½æ¾ç¨€ç–æƒ¯æ€§ä¼ æ„Ÿå™¨äººä½“åŠ¨ä½œæ•æ‰",
      "authors": [
        "Andela Ilic",
        "Jiaxi Jiang",
        "Paul Streli",
        "Xintong Liu",
        "Christian Holz"
      ],
      "abstract": "Motion capture using sparse inertial sensors has shown great promise due to its portability and lack of occlusion issues compared to camera-based tracking. Existing approaches typically assume that IMU sensors are tightly attached to the human body. However, this assumption often does not hold in real-world scenarios. In this paper, we present Garment Inertial Poser (GaIP), a method for estimating full-body poses from sparse and loosely attached IMU sensors. We first simulate IMU recordings using an existing garment-aware human motion dataset. Our transformer-based diffusion models synthesize loose IMU data and estimate human poses from this challenging loose IMU data. We also demonstrate that incorporating garment-related parameters during training on loose IMU data effectively maintains expressiveness and enhances the ability to capture variations introduced by looser or tighter garments. Our experiments show that our diffusion methods trained on simulated and synthetic data outperform state-of-the-art inertial full-body pose estimators, both quantitatively and qualitatively, opening up a promising direction for future research on motion capture from such realistic sensor placements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Garment Inertial Poser (GaIP)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä»æ¾æ•£ä¸”ç¨€ç–çš„æƒ¯æ€§ä¼ æ„Ÿå™¨ (Sparse Inertial Sensors) ä¸­ä¼°è®¡å…¨èº«å§¿æ€çš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ä¼ æ„Ÿå™¨å¿…é¡»ç´§è´´èº«ä½“çš„å±€é™æ€§ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æœè£…æ„ŸçŸ¥çš„äººä½“è¿åŠ¨æ•°æ®é›†æ¨¡æ‹Ÿ IMU æ•°æ®ï¼Œå¹¶é‡‡ç”¨åŸºäº Transformer çš„æ‰©æ•£æ¨¡å‹ (Diffusion Models) å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¾æ•£ä¼ æ„Ÿå™¨ä¿¡å·ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥æœè£…ç›¸å…³å‚æ•°ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒå®½æ¾ç¨‹åº¦è¡£ç‰©æ‰€å¸¦æ¥çš„è¿åŠ¨å˜åŒ–ï¼Œæ˜¾è‘—å¢å¼ºäº†ç³»ç»Ÿçš„è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGaIP åœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šå‡ä¼˜äºç°æœ‰çš„ SOTA æƒ¯æ€§å…¨èº«å§¿æ€ä¼°è®¡å™¨ã€‚è¯¥é¡¹å·¥ä½œä¸ºåœ¨çœŸå®ç”Ÿæ´»åœºæ™¯ä¸‹å®ç°æ›´å…·é²æ£’æ€§çš„è¿åŠ¨æ•æ‰æŠ€æœ¯å¥ å®šäº†åŸºç¡€ï¼Œå…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted by IJCAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15290v2",
      "published_date": "2025-06-18 09:16:36 UTC",
      "updated_date": "2025-08-13 08:45:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:09.596307+00:00"
    },
    {
      "arxiv_id": "2506.15271v1",
      "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data",
      "title_zh": "åˆ©ç”¨åˆæˆæ•°æ®å®ç°äº‹åæ•°æ®é›†æ¨ç†",
      "authors": [
        "Bihe Zhao",
        "Pratyush Maini",
        "Franziska Boenisch",
        "Adam Dziedzic"
      ],
      "abstract": "The remarkable capabilities of Large Language Models (LLMs) can be mainly attributed to their massive training datasets, which are often scraped from the internet without respecting data owners' intellectual property rights. Dataset Inference (DI) offers a potential remedy by identifying whether a suspect dataset was used in training, thereby enabling data owners to verify unauthorized use. However, existing DI methods require a private set-known to be absent from training-that closely matches the compromised dataset's distribution. Such in-distribution, held-out data is rarely available in practice, severely limiting the applicability of DI. In this work, we address this challenge by synthetically generating the required held-out set. Our approach tackles two key obstacles: (1) creating high-quality, diverse synthetic data that accurately reflects the original distribution, which we achieve via a data generator trained on a carefully designed suffix-based completion task, and (2) bridging likelihood gaps between real and synthetic data, which is realized through post-hoc calibration. Extensive experiments on diverse text datasets show that using our generated data as a held-out set enables DI to detect the original training sets with high confidence, while maintaining a low false positive rate. This result empowers copyright owners to make legitimate claims on data usage and demonstrates our method's reliability for real-world litigations. Our code is available at https://github.com/sprintml/PostHocDatasetInference.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è®­ç»ƒä¸­æ•°æ®é›†æ¨ç†(Dataset Inference, DI)é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ï¼Œå³éš¾ä»¥è·å–ä¸è®­ç»ƒé›†åŒåˆ†å¸ƒä½†æœªå‚ä¸è®­ç»ƒçš„ç§æœ‰ç•™å­˜é›†(held-out set)çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨åˆæˆæ•°æ®è§£é”äº‹åæ•°æ®é›†æ¨ç†çš„æ–°æ–¹æ¡ˆã€‚ç ”ç©¶è€…é€šè¿‡è®¾è®¡åŸºäºåç¼€è¡¥å…¨ä»»åŠ¡(suffix-based completion task)çš„æ•°æ®ç”Ÿæˆå™¨ï¼Œç”Ÿæˆäº†èƒ½å¤Ÿå‡†ç¡®åæ˜ åŸå§‹åˆ†å¸ƒçš„é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„åˆæˆæ•°æ®ã€‚ä¸ºäº†ç¼©å°çœŸå®æ•°æ®ä¸åˆæˆæ•°æ®ä¹‹é—´çš„ä¼¼ç„¶å·®è·ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†äº‹åæ ¡å‡†(post-hoc calibration)æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å¤šç§æ–‡æœ¬æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç”Ÿæˆçš„åˆæˆæ•°æ®ä½œä¸ºç•™å­˜é›†èƒ½å¤Ÿä½¿DIä»¥é«˜ç½®ä¿¡åº¦å’Œä½è¯¯æŠ¥ç‡æˆåŠŸè¯†åˆ«åŸå§‹è®­ç»ƒé›†ã€‚è¯¥ç ”ç©¶æˆæœä¸ºç‰ˆæƒæ‰€æœ‰è€…æä¾›äº†åˆæ³•ä¸»å¼ æ•°æ®ä½¿ç”¨æƒçš„å¼ºåŠ›å·¥å…·ï¼Œå¹¶ä¸ºè§£å†³ç°å®åœºæ™¯ä¸­æ•°æ®ä¾µæƒçš„æ³•å¾‹è¯‰è®¼æä¾›äº†å¯é çš„æŠ€æœ¯éªŒè¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICML 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15271v1",
      "published_date": "2025-06-18 08:46:59 UTC",
      "updated_date": "2025-06-18 08:46:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:23.193760+00:00"
    },
    {
      "arxiv_id": "2506.15260v1",
      "title": "Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing",
      "title_zh": "åŠå¯¼ä½“åˆ¶é€ ä¸­ç¼ºé™·å›¾åƒåˆ†ç±»çš„é¢†åŸŸè‡ªé€‚åº”",
      "authors": [
        "Adrian Poniatowski",
        "Natalie Gentner",
        "Manuel Barusco",
        "Davide Dalle Pezze",
        "Samuele Salti",
        "Gian Antonio Susto"
      ],
      "abstract": "In the semiconductor sector, due to high demand but also strong and increasing competition, time to market and quality are key factors in securing significant market share in various application areas. Thanks to the success of deep learning methods in recent years in the computer vision domain, Industry 4.0 and 5.0 applications, such as defect classification, have achieved remarkable success. In particular, Domain Adaptation (DA) has proven highly effective since it focuses on using the knowledge learned on a (source) domain to adapt and perform effectively on a different but related (target) domain. By improving robustness and scalability, DA minimizes the need for extensive manual re-labeling or re-training of models. This not only reduces computational and resource costs but also allows human experts to focus on high-value tasks. Therefore, we tested the efficacy of DA techniques in semi-supervised and unsupervised settings within the context of the semiconductor field. Moreover, we propose the DBACS approach, a CycleGAN-inspired model enhanced with additional loss terms to improve performance. All the approaches are studied and validated on real-world Electron Microscope images considering the unsupervised and semi-supervised settings, proving the usefulness of our method in advancing DA techniques for the semiconductor field.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰æŠ€æœ¯åœ¨åŠå¯¼ä½“åˆ¶é€ ç¼ºé™·å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡è¿ç§»æºåŸŸçŸ¥è¯†æ¥è§£å†³ç›®æ ‡åŸŸæ ‡æ³¨æ•°æ®åŒ®ä¹çš„é—®é¢˜ï¼Œä»è€Œé™ä½æ‰‹åŠ¨æ ‡æ³¨å’Œæ¨¡å‹é‡è®­çš„æˆæœ¬ã€‚ç ”ç©¶è€…åœ¨åŠç›‘ç£å’Œæ— ç›‘ç£è®¾ç½®ä¸‹å¯¹å¤šç§é¢†åŸŸè‡ªé€‚åº”ï¼ˆDomain Adaptationï¼‰æŠ€æœ¯è¿›è¡Œäº†æ·±å…¥æµ‹è¯•ï¼Œä»¥æå‡å·¥ä¸šåœºæ™¯ä¸‹çš„æ¨¡å‹é²æ£’æ€§ä¸å¯æ‰©å±•æ€§ã€‚è®ºæ–‡é‡ç‚¹æå‡ºäº†åä¸º DBACS çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§å— CycleGAN å¯å‘å¹¶ç»“åˆäº†é¢å¤–æŸå¤±é¡¹ï¼ˆloss termsï¼‰çš„æ”¹è¿›æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ–ç¼ºé™·åˆ†ç±»æ€§èƒ½ã€‚å®éªŒé€šè¿‡çœŸå®çš„ç”µå­æ˜¾å¾®é•œï¼ˆElectron Microscopeï¼‰å›¾åƒè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¯æ˜äº† DBACS åœ¨å¤„ç†åŠå¯¼ä½“å®é™…ç”Ÿäº§ç¯å¢ƒæ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºåŠå¯¼ä½“è¡Œä¸šçš„å·¥ä¸š 4.0 ä¸ 5.0 åº”ç”¨æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒï¼ŒåŠ©åŠ›ä¼ä¸šåœ¨æ¿€çƒˆçš„å¸‚åœºç«äº‰ä¸­ç¼©çŸ­äº§å“ä¸Šå¸‚æ—¶é—´å¹¶ç¡®ä¿åˆ¶é€ è´¨é‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15260v1",
      "published_date": "2025-06-18 08:37:55 UTC",
      "updated_date": "2025-06-18 08:37:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:29.238251+00:00"
    },
    {
      "arxiv_id": "2506.15253v1",
      "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments",
      "title_zh": "RAS-Evalï¼šçœŸå®ä¸–ç•Œç¯å¢ƒä¸‹ LLM æ™ºèƒ½ä½“å®‰å…¨æ€§è¯„ä¼°çš„ç»¼åˆåŸºå‡†",
      "authors": [
        "Yuchuan Fu",
        "Xiaohan Yuan",
        "Dongxia Wang"
      ],
      "abstract": "The rapid deployment of Large language model (LLM) agents in critical domains like healthcare and finance necessitates robust security frameworks. To address the absence of standardized evaluation benchmarks for these agents in dynamic environments, we introduce RAS-Eval, a comprehensive security benchmark supporting both simulated and real-world tool execution. RAS-Eval comprises 80 test cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration (CWE) categories, with tools implemented in JSON, LangGraph, and Model Context Protocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse scenarios, revealing significant vulnerabilities: attacks reduced agent task completion rates (TCR) by 36.78% on average and achieved an 85.65% success rate in academic settings. Notably, scaling laws held for security capabilities, with larger models outperforming smaller counterparts. Our findings expose critical risks in real-world agent deployments and provide a foundational framework for future security research. Code and data are available at https://github.com/lanzer-tree/RAS-Eval.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å’Œé‡‘èç­‰å…³é”®é¢†åŸŸä¸­ Large language model (LLM) æ™ºèƒ½ä½“å¿«é€Ÿéƒ¨ç½²å¸¦æ¥çš„å®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº† RAS-Evalï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¯æŒæ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒå·¥å…·æ‰§è¡Œçš„ç»¼åˆæ€§å®‰å…¨è¯„ä¼°åŸºå‡†ã€‚RAS-Eval åŒ…å« 80 ä¸ªæµ‹è¯•æ¡ˆä¾‹å’Œæ˜ å°„åˆ° 11 ä¸ª Common Weakness Enumeration (CWE) ç±»åˆ«çš„ 3,802 ä¸ªæ”»å‡»ä»»åŠ¡ï¼Œå¹¶æ”¯æŒ JSONã€LangGraph å’Œ Model Context Protocol (MCP) ç­‰å¤šç§å·¥å…·æ ¼å¼ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å¤šç§åœºæ™¯ä¸‹å¯¹ 6 ç§æœ€å…ˆè¿›çš„ LLMs è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨ç°å®éƒ¨ç½²ä¸­å­˜åœ¨çš„é‡å¤§å®‰å…¨æ¼æ´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”»å‡»å¹³å‡å¯¼è‡´æ™ºèƒ½ä½“çš„ä»»åŠ¡å®Œæˆç‡ (Task Completion Rate, TCR) ä¸‹é™äº† 36.78%ï¼Œè€Œåœ¨å­¦æœ¯ç¯å¢ƒä¸‹çš„æ”»å‡»æˆåŠŸç‡é«˜è¾¾ 85.65%ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å®‰å…¨èƒ½åŠ›éµå¾ªç¼©æ”¾å®šå¾‹ (Scaling Laws)ï¼Œå³æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œå…¶å®‰å…¨è¡¨ç°é€šå¸¸ä¼˜äºè¾ƒå°è§„æ¨¡çš„åŒç±»æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…æš´éœ²äº†ç°å®ä¸–ç•Œæ™ºèƒ½ä½“éƒ¨ç½²çš„å…³é”®é£é™©ï¼Œè¿˜ä¸ºæœªæ¥çš„å®‰å…¨ç ”ç©¶æä¾›äº†ä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ä»£ç å’Œæ•°æ®é›†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15253v1",
      "published_date": "2025-06-18 08:30:36 UTC",
      "updated_date": "2025-06-18 08:30:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:27.288961+00:00"
    },
    {
      "arxiv_id": "2506.15251v1",
      "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language Model",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ Kronecker é€‚é…ä¸­çš„å¥‡å¼‚å€¼åˆ†è§£",
      "authors": [
        "Yee Hin Chong",
        "Peng Qu"
      ],
      "abstract": "Large pre-trained Transformer models achieve state-of-the-art results across diverse language and reasoning tasks, but full fine-tuning incurs substantial storage, memory, and computational overhead. Parameter-efficient fine-tuning (PEFT) methods mitigate these costs by learning only a small subset of task-specific parameters, yet existing approaches either introduce inference-time latency (adapter modules), suffer from suboptimal convergence (randomly initialized low-rank updates), or rely on fixed rank choices that may not match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that combines Kronecker-product tensor factorization with SVD-driven initialization and spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD) procedure extracts principal components of the full weight update into compact Kronecker factors, while an adaptive rank selection algorithm uses energy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal mathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires only 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or exceeding baseline performance. Moreover, SoKA exhibits faster convergence and more stable gradients, highlighting its robustness and efficiency for large-scale model adaptation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SoKA (SVD on Kronecker Adaptation)ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(Large Language Models)å…¨é‡å¾®è°ƒçš„é«˜å¼€é”€ä»¥åŠç°æœ‰å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•å­˜åœ¨çš„æ¨ç†å»¶è¿Ÿã€æ”¶æ•›ç¼“æ…¢æˆ–å›ºå®šç§©é™åˆ¶ç­‰é—®é¢˜ã€‚è¯¥ç­–ç•¥å°†Kronecker-productå¼ é‡åˆ†è§£ä¸SVDé©±åŠ¨çš„åˆå§‹åŒ–ç›¸ç»“åˆï¼Œé€šè¿‡KPSVDç¨‹åºå°†å…¨æƒé‡æ›´æ–°çš„ä¸»è¦æˆåˆ†æå–ä¸ºç´§å‡‘çš„Kroneckerå› å­ã€‚SoKAè¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”ç§©é€‰æ‹©ç®—æ³•ï¼Œåˆ©ç”¨èƒ½é‡é˜ˆå€¼å’Œæ‹ç‚¹å‡†åˆ™åŠ¨æ€ä¿®å‰ªä¸é‡è¦çš„æˆåˆ†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡å¤æ‚åº¦çµæ´»è°ƒæ•´ã€‚åœ¨LLaMA2-7Bæ¨¡å‹ä¸Šé’ˆå¯¹ç®—æœ¯æ¨ç†(GSM8K)ã€å½¢å¼æ•°å­¦(MATH)å’Œä»£ç ç”Ÿæˆ(MBPP)çš„å®éªŒè¡¨æ˜ï¼ŒSoKAä»…éœ€0.99Mä¸ªå¯è®­ç»ƒå‚æ•°ã€‚è¿™ä¸€å‚æ•°é‡æ¯”LoRAå’ŒPiSSAå‡å°‘äº†25%ï¼ŒåŒæ—¶åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡èƒ½è¾¾åˆ°æˆ–è¶…è¶ŠåŸºçº¿æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒSoKAå±•ç°å‡ºæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹çš„é«˜æ•ˆé€‚é…æä¾›äº†ä¸€ç§ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15251v1",
      "published_date": "2025-06-18 08:28:53 UTC",
      "updated_date": "2025-06-18 08:28:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:30.687637+00:00"
    },
    {
      "arxiv_id": "2506.15225v1",
      "title": "Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels",
      "title_zh": "é¢å‘ä¸ç¡®å®šæ€§æµ·æ´‹MECçš„æ— äººæœºä¸èˆ¹èˆ¶ååŒè”åˆè®¡ç®—å¸è½½ä¸èµ„æºåˆ†é…",
      "authors": [
        "Jiahao You",
        "Ziye Jia",
        "Chao Dong",
        "Qihui Wu",
        "Zhu Han"
      ],
      "abstract": "The computation demands from the maritime Internet of Things (MIoT) increase rapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels based multi-access edge computing (MEC) can fulfill these MIoT requirements. However, the uncertain maritime tasks present significant challenges of inefficient computation offloading and resource allocation. In this paper, we focus on the maritime computation offloading and resource allocation through the cooperation of UAVs and vessels, with consideration of uncertain tasks. Specifically, we propose a cooperative MEC framework for computation offloading and resource allocation, including MIoT devices, UAVs and vessels. Then, we formulate the optimization problem to minimize the total execution time. As for the uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the unpredictable task arrivals and varying computational resource availability. \nBy converting the long-term constraints into short-term constraints, we obtain a set of small-scale optimization problems. Further, considering the heterogeneity of actions and resources of UAVs and vessels, we reformulate the small-scale optimization problem into a Markov game (MG). Moreover, a heterogeneous-agent soft actor-critic is proposed to sequentially update various neural networks and effectively solve the MG problem. \nFinally, simulations are conducted to verify the effectiveness in addressing computational offloading and resource allocation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ·æ´‹ç‰©è”ç½‘(MIoT)ä¸­è®¡ç®—éœ€æ±‚å¿«é€Ÿå¢é•¿åŠä¸ç¡®å®šæ€§ä»»åŠ¡å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ— äººæœº(UAVs)ä¸èˆ¹åªåä½œçš„è”åˆè®¡ç®—å¸è½½ä¸èµ„æºåˆ†é…æ–¹æ¡ˆã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«MIoTè®¾å¤‡ã€UAVså’Œèˆ¹åªçš„åä½œå¼å¤šæ¥å…¥è¾¹ç¼˜è®¡ç®—(MEC)æ¡†æ¶ï¼Œæ—¨åœ¨æœ€å°åŒ–ç³»ç»Ÿçš„æ€»æ‰§è¡Œæ—¶é—´ã€‚ä¸ºäº†åº”å¯¹ä¸å¯é¢„æµ‹çš„ä»»åŠ¡åˆ°è¾¾å’Œå˜åŒ–çš„èµ„æºå¯ç”¨æ€§ï¼Œä½œè€…åˆ©ç”¨Lyapunovä¼˜åŒ–æŠ€æœ¯å°†é•¿æœŸçº¦æŸè½¬åŒ–ä¸ºä¸€ç³»åˆ—çŸ­æœŸçš„å°è§„æ¨¡ä¼˜åŒ–é—®é¢˜ã€‚è€ƒè™‘åˆ°UAVså’Œèˆ¹åªåœ¨åŠ¨ä½œå’Œèµ„æºä¸Šçš„å¼‚æ„æ€§ï¼Œè¯¥æ–¹æ¡ˆå°†å°è§„æ¨¡ä¼˜åŒ–é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºé©¬å°”å¯å¤«åšå¼ˆ(Markov game)ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å¼‚æ„æ™ºèƒ½ä½“è½¯æ¼”å‘˜-è¯„è®ºå®¶(heterogeneous-agent soft actor-critic)ç®—æ³•ï¼Œé€šè¿‡é¡ºåºæ›´æ–°ç¥ç»ç½‘ç»œæ¥æœ‰æ•ˆæ±‚è§£è¯¥åšå¼ˆé—®é¢˜ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½æœ‰æ•ˆè§£å†³æµ·æ´‹ç¯å¢ƒä¸‹çš„è®¡ç®—å¸è½½ä¸èµ„æºåˆ†é…éš¾é¢˜ã€‚",
      "categories": [
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15225v1",
      "published_date": "2025-06-18 08:10:50 UTC",
      "updated_date": "2025-06-18 08:10:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:31.092681+00:00"
    },
    {
      "arxiv_id": "2507.01028v2",
      "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning",
      "title_zh": "éå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ çš„åŒé‡è§†è§’",
      "authors": [
        "Jean Ponce",
        "Basile Terver",
        "Martial Hebert",
        "Michael Arbel"
      ],
      "abstract": "The {\\em stop gradient} and {\\em exponential moving average} iterative procedures are commonly used in non-contrastive approaches to self-supervised learning to avoid representation collapse, with excellent performance in downstream applications in practice. This presentation investigates these procedures from the dual viewpoints of optimization and dynamical systems. We show that, in general, although they {\\em do not} optimize the original objective, or {\\em any} other smooth function, they {\\em do} avoid collapse Following~\\citet{Tian21}, but without any of the extra assumptions used in their proofs, we then show using a dynamical system perspective that, in the linear case, minimizing the original objective function without the use of a stop gradient or exponential moving average {\\em always} leads to collapse. Conversely, we characterize explicitly the equilibria of the dynamical systems associated with these two procedures in this linear setting as algebraic varieties in their parameter space, and show that they are, in general, {\\em asymptotically stable}. Our theoretical findings are illustrated by empirical experiments with real and synthetic data.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»ä¼˜åŒ–å’ŒåŠ¨åŠ›ç³»ç»Ÿï¼ˆDynamical Systemsï¼‰çš„åŒé‡è§†è§’æ¢è®¨äº†éå¯¹æ¯”è‡ªç›‘ç£å­¦ä¹ ï¼ˆNon-Contrastive SSLï¼‰ä¸­é˜²æ­¢è¡¨å¾å´©æºƒï¼ˆRepresentation Collapseï¼‰çš„å…³é”®æŠ€æœ¯ï¼Œå³åœæ­¢æ¢¯åº¦ï¼ˆStop Gradientï¼‰å’ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆExponential Moving Average, EMAï¼‰ã€‚ä½œè€…æŒ‡å‡ºï¼Œå°½ç®¡è¿™äº›è¿­ä»£è¿‡ç¨‹åœ¨é€šç”¨æƒ…å†µä¸‹å¹¶æœªä¼˜åŒ–åŸå§‹ç›®æ ‡å‡½æ•°æˆ–ä»»ä½•å¹³æ»‘å‡½æ•°ï¼Œä½†å®ƒä»¬ç¡®å®èƒ½å¤Ÿæœ‰æ•ˆé¿å…å´©æºƒã€‚è¯¥æ–‡åœ¨æ— éœ€é¢å¤–å‡è®¾çš„æƒ…å†µä¸‹é€šè¿‡åŠ¨åŠ›ç³»ç»Ÿè§†è§’è¯æ˜ï¼Œåœ¨çº¿æ€§æƒ…å½¢ä¸‹è‹¥ä¸ä½¿ç”¨åœæ­¢æ¢¯åº¦æˆ– EMAï¼Œæœ€å°åŒ–åŸå§‹ç›®æ ‡å‡½æ•°å°†ä¸å¯é¿å…åœ°å¯¼è‡´å´©æºƒã€‚ç ”ç©¶è¿›ä¸€æ­¥å°†çº¿æ€§è®¾ç½®ä¸‹ä¸è¿™ä¸¤ç§ç¨‹åºç›¸å…³çš„åŠ¨åŠ›ç³»ç»Ÿå¹³è¡¡ç‚¹æ˜ç¡®åˆ»ç”»ä¸ºå‚æ•°ç©ºé—´ä¸­çš„ä»£æ•°ç°‡ï¼ˆAlgebraic Varietiesï¼‰ï¼Œå¹¶è¯æ˜äº†è¿™äº›å¹³è¡¡ç‚¹åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯æ¸è¿‘ç¨³å®šçš„ï¼ˆAsymptotically Stableï¼‰ã€‚æœ€åï¼Œé€šè¿‡çœŸå®å’Œåˆæˆæ•°æ®çš„å®éªŒéªŒè¯äº†ä¸Šè¿°ç†è®ºå‘ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.01028v2",
      "published_date": "2025-06-18 07:46:51 UTC",
      "updated_date": "2025-10-14 12:45:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:46:42.988747+00:00"
    },
    {
      "arxiv_id": "2506.15208v1",
      "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals",
      "title_zh": "ç”¨äºè¯†åˆ«å¯æŒç»­å‘å±•ç›®æ ‡çš„å¤§è¯­è¨€æ¨¡å‹ä»»åŠ¡é€‚é…æŠ€æœ¯æ¯”è¾ƒç ”ç©¶",
      "authors": [
        "Andrea Cadeddu",
        "Alessandro Chessa",
        "Vincenzo De Leo",
        "Gianni Fenu",
        "Enrico Motta",
        "Francesco Osborne",
        "Diego Reforgiato Recupero",
        "Angelo Salatino",
        "Luca Secchi"
      ],
      "abstract": "In 2012, the United Nations introduced 17 Sustainable Development Goals (SDGs) aimed at creating a more sustainable and improved future by 2030. However, tracking progress toward these goals is difficult because of the extensive scale and complexity of the data involved. Text classification models have become vital tools in this area, automating the analysis of vast amounts of text from a variety of sources. Additionally, large language models (LLMs) have recently proven indispensable for many natural language processing tasks, including text classification, thanks to their ability to recognize complex linguistic patterns and semantics. This study analyzes various proprietary and open-source LLMs for a single-label, multi-class text classification task focused on the SDGs. Then, it also evaluates the effectiveness of task adaptation techniques (i.e., in-context learning approaches), namely Zero-Shot and Few-Shot Learning, as well as Fine-Tuning within this domain. The results reveal that smaller models, when optimized through prompt engineering, can perform on par with larger models like OpenAI's GPT (Generative Pre-trained Transformer).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯†åˆ«è”åˆå›½æå‡ºçš„17é¡¹å¯æŒç»­å‘å±•ç›®æ ‡(Sustainable Development Goals, SDGs)è¿™ä¸€ä»»åŠ¡ï¼Œç³»ç»Ÿåœ°å¯¹æ¯”åˆ†æäº†å¤šç§ç§æœ‰åŠå¼€æºå¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)åœ¨å•æ ‡ç­¾ã€å¤šåˆ†ç±»(single-label, multi-class)æ–‡æœ¬åˆ†ç±»ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶é‡ç‚¹è¯„ä¼°äº†é›¶æ ·æœ¬å­¦ä¹ (Zero-Shot Learning)ã€å°‘æ ·æœ¬å­¦ä¹ (Few-Shot Learning)ä»¥åŠå¾®è°ƒ(Fine-Tuning)ç­‰ä»»åŠ¡è‡ªé€‚åº”æŠ€æœ¯åœ¨SDGsé¢†åŸŸä¸­çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡æç¤ºå·¥ç¨‹(prompt engineering)ä¼˜åŒ–çš„è¾ƒå°æ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä»¥ä¸OpenAIçš„GPT(Generative Pre-trained Transformer)ç­‰å¤§å‹æ¨¡å‹ç›¸åª²ç¾ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†LLMsè¯†åˆ«å¤æ‚è¯­è¨€æ¨¡å¼å’Œè¯­ä¹‰çš„èƒ½åŠ›ï¼Œä¸ºå¤§è§„æ¨¡ã€å¤æ‚çš„å¯æŒç»­å‘å±•æ–‡æœ¬æ•°æ®çš„è‡ªåŠ¨åŒ–åˆ†ææä¾›äº†æœ‰åŠ›å·¥å…·ã€‚ç ”ç©¶æœ€ç»ˆå¼ºè°ƒäº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–è½»é‡åŒ–æ¨¡å‹çš„æ½œåŠ›ï¼Œä¸ºé«˜æ•ˆç›‘æµ‹å…¨çƒå‘å±•ç›®æ ‡çš„è¿›å±•æä¾›äº†åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Submitted to IEEE Access",
      "pdf_url": "https://arxiv.org/pdf/2506.15208v1",
      "published_date": "2025-06-18 07:42:32 UTC",
      "updated_date": "2025-06-18 07:42:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:25.090430+00:00"
    },
    {
      "arxiv_id": "2506.15207v2",
      "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study",
      "title_zh": "é¢å‘è‡ªä¸»å¤šå«æ˜Ÿå¯¹åœ°è§‚æµ‹çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼šç°å®æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Mohamad A. Hady",
        "Siyi Hu",
        "Mahardhika Pratama",
        "Jimmy Cao",
        "Ryszard Kowalczyk"
      ],
      "abstract": "The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised Earth Observation (EO) missions, addressing challenges in climate monitoring, disaster management, and more. However, autonomous coordination in multi-satellite systems remains a fundamental challenge. Traditional optimisation approaches struggle to handle the real-time decision-making demands of dynamic EO missions, necessitating the use of Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we investigate RL-based autonomous EO mission planning by modelling single-satellite operations and extending to multi-satellite constellations using MARL frameworks. We address key challenges, including energy and data storage limitations, uncertainties in satellite observations, and the complexities of decentralised coordination under partial observability. By leveraging a near-realistic satellite simulation environment, we evaluate the training stability and performance of state-of-the-art MARL algorithms, including PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can effectively balance imaging and resource management while addressing non-stationarity and reward interdependency in multi-satellite coordination. The insights gained from this study provide a foundation for autonomous satellite operations, offering practical guidelines for improving policy learning in decentralised EO missions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½åœ°çƒè½¨é“(LEO)å«æ˜Ÿä»»åŠ¡éœ€æ±‚æ¿€å¢çš„èƒŒæ™¯ä¸‹ï¼Œåˆ©ç”¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ (MARL)è§£å†³è‡ªä¸»å¤šå«æ˜Ÿåœ°çƒè§‚æµ‹(EO)åè°ƒæŒ‘æˆ˜çš„æ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡å¯¹å•å«æ˜Ÿæ“ä½œå»ºæ¨¡å¹¶å°†å…¶æ‰©å±•è‡³å¤šå«æ˜Ÿæ˜Ÿåº§ï¼Œå¯¹æ¯”è¯„ä¼°äº†PPOã€IPPOã€MAPPOå’ŒHAPPOç­‰å…ˆè¿›ç®—æ³•åœ¨èµ„æºå—é™å’Œéƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚åœ¨æ¥è¿‘çœŸå®çš„å«æ˜Ÿæ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œè¯¥æ¡†æ¶æˆåŠŸåº”å¯¹äº†èƒ½é‡ä¸å­˜å‚¨é™åˆ¶ã€è§‚æµ‹ä¸ç¡®å®šæ€§ä»¥åŠå»ä¸­å¿ƒåŒ–åè°ƒçš„å¤æ‚æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMARLèƒ½å¤Ÿæœ‰æ•ˆå¹³è¡¡æˆåƒä»»åŠ¡ä¸èµ„æºç®¡ç†ï¼Œå¹¶å¦¥å–„å¤„ç†å¤šå«æ˜Ÿç³»ç»Ÿä¸­çš„éå¹³ç¨³æ€§å’Œå¥–åŠ±ç›¸äº’ä¾èµ–é—®é¢˜ã€‚è¯¥å·¥ä½œä¸ºè‡ªä¸»å«æ˜Ÿè¿è¡Œå¥ å®šäº†é‡è¦åŸºç¡€ï¼Œå¹¶ä¸ºæ”¹è¿›å»ä¸­å¿ƒåŒ–è§‚æµ‹ä»»åŠ¡ä¸­çš„ç­–ç•¥å­¦ä¹ æä¾›äº†å®ç”¨æŒ‡å—ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15207v2",
      "published_date": "2025-06-18 07:42:11 UTC",
      "updated_date": "2025-11-05 03:26:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:00.787058+00:00"
    },
    {
      "arxiv_id": "2506.15196v2",
      "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges",
      "title_zh": "HeurAgenixï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚ç»„åˆä¼˜åŒ–æŒ‘æˆ˜",
      "authors": [
        "Xianliang Yang",
        "Ling Zhang",
        "Haolong Qian",
        "Lei Song",
        "Jiang Bian"
      ],
      "abstract": "Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HeurAgenixï¼Œä¸€ç§æ—¨åœ¨è§£å†³å¤æ‚ Combinatorial Optimization (CO) æŒ‘æˆ˜çš„ä¸¤é˜¶æ®µ Hyper-heuristic æ¡†æ¶ã€‚åœ¨å¯å‘å¼è¿›åŒ–é˜¶æ®µï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ Large Language Models (LLMs) å¯¹æ¯”ç§å­å¯å‘å¼è§£ä¸é«˜è´¨é‡è§£ï¼Œå¹¶ä»ä¸­æå–å¯é‡ç”¨çš„è¿›åŒ–ç­–ç•¥ã€‚åœ¨é—®é¢˜æ±‚è§£è¿‡ç¨‹ä¸­ï¼ŒHeurAgenix ä¾æ® LLMs çš„æ„ŸçŸ¥èƒ½åŠ›ä¸ºæ¯ä¸ªé—®é¢˜çŠ¶æ€åŠ¨æ€é€‰æ‹©æœ€æœ‰æ½œåŠ›çš„å¯å‘å¼ç®—æ³•ï¼Œä¸”æ”¯æŒä½¿ç”¨æ¨ç†æˆæœ¬æ›´ä½çš„å¾®è°ƒè½»é‡åŒ–æ¨¡å‹ä½œä¸ºé€‰æ‹©å™¨ã€‚ä¸ºè§£å†³ CO é¢†åŸŸå¯é ç›‘ç£ä¿¡å·ç¨€ç¼ºçš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†åŒé‡å¥–åŠ±æœºåˆ¶æ¥å¾®è°ƒé€‰æ‹©å™¨ï¼Œé€šè¿‡ç»“åˆé€‰æ‹©åå¥½ä¸çŠ¶æ€æ„ŸçŸ¥ä¿¡å·ç¡®ä¿å†³ç­–çš„é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHeurAgenix åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸ä»…ä¼˜äºç°æœ‰çš„åŸºäº LLMs çš„è¶…å¯å‘å¼æ–¹æ³•ï¼Œç”šè‡³è¾¾åˆ°æˆ–è¶…è¶Šäº†ä¸“ç”¨ Solvers çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "27 pages,9 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15196v2",
      "published_date": "2025-06-18 07:20:01 UTC",
      "updated_date": "2025-06-24 14:48:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:13.844256+00:00"
    },
    {
      "arxiv_id": "2506.15189v1",
      "title": "Accessible Gesture-Driven Augmented Reality Interaction System",
      "title_zh": "æ— éšœç¢æ‰‹åŠ¿é©±åŠ¨å¢å¼ºç°å®äº¤äº’ç³»ç»Ÿ",
      "authors": [
        "Yikan Wang"
      ],
      "abstract": "Augmented reality (AR) offers immersive interaction but remains inaccessible for users with motor impairments or limited dexterity due to reliance on precise input methods. This study proposes a gesture-based interaction system for AR environments, leveraging deep learning to recognize hand and body gestures from wearable sensors and cameras, adapting interfaces to user capabilities. The system employs vision transformers (ViTs), temporal convolutional networks (TCNs), and graph attention networks (GATs) for gesture processing, with federated learning ensuring privacy-preserving model training across diverse users. Reinforcement learning optimizes interface elements like menu layouts and interaction modes. Experiments demonstrate a 20% improvement in task completion efficiency and a 25% increase in user satisfaction for motor-impaired users compared to baseline AR systems. This approach enhances AR accessibility and scalability. Keywords: Deep learning, Federated learning, Gesture recognition, Augmented reality, Accessibility, Human-computer interaction",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é¢å‘å¯è®¿é—®æ€§çš„æ‰‹åŠ¿é©±åŠ¨å¢å¼ºç°å®(AR)äº¤äº’ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³è¿åŠ¨éšœç¢ç”¨æˆ·åœ¨ä¾èµ–ç²¾å‡†è¾“å…¥æ³•çš„ä¼ ç»ŸARç¯å¢ƒä¸­é¢ä¸´çš„äº¤äº’å›°éš¾ã€‚ç³»ç»Ÿåˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œé€šè¿‡ç©¿æˆ´å¼ä¼ æ„Ÿå™¨å’Œæ‘„åƒå¤´è¯†åˆ«æ‰‹éƒ¨åŠèº«ä½“æ‰‹åŠ¿ï¼Œå¹¶æ ¹æ®ç”¨æˆ·èƒ½åŠ›è‡ªé€‚åº”è°ƒæ•´ç•Œé¢ã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œè¯¥æ–¹æ¡ˆæ•´åˆäº†è§†è§‰å˜æ¢å™¨(Vision Transformers, ViTs)ã€æ—¶é—´å·ç§¯ç½‘ç»œ(Temporal Convolutional Networks, TCNs)å’Œå›¾æ³¨æ„åŠ›ç½‘ç»œ(Graph Attention Networks, GATs)è¿›è¡Œæ‰‹åŠ¿å¤„ç†ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨è”é‚¦å­¦ä¹ (Federated Learning)ç¡®ä¿è·¨ç”¨æˆ·çš„éšç§ä¿æŠ¤ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ä¼˜åŒ–èœå•å¸ƒå±€ç­‰äº¤äº’å…ƒç´ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿä½¿è¿åŠ¨éšœç¢ç”¨æˆ·çš„ä»»åŠ¡å®Œæˆæ•ˆç‡æå‡äº†20%ï¼Œç”¨æˆ·æ»¡æ„åº¦å¢åŠ äº†25%ï¼Œæ˜¾è‘—å¢å¼ºäº†ARç³»ç»Ÿçš„å¯è®¿é—®æ€§ä¸æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15189v1",
      "published_date": "2025-06-18 07:10:48 UTC",
      "updated_date": "2025-06-18 07:10:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:06.390274+00:00"
    },
    {
      "arxiv_id": "2506.15182v1",
      "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šå‚æ•°ä½“éƒ¨ MRI åºåˆ—åˆ†ç±»",
      "authors": [
        "Boah Kim",
        "Tejas Sudharshan Mathai",
        "Kimberly Helm",
        "Peter A. Pinto",
        "Ronald M. Summers"
      ],
      "abstract": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series types acquired with different imaging protocols. The DICOM headers of these series often have incorrect information due to the sheer diversity of protocols and occasional technologist errors. To address this, we present a deep learning-based classification model to classify 8 different body mpMRI series types so that radiologists read the exams efficiently. Using mpMRI data from various institutions, multiple deep learning-based classifiers of ResNet, EfficientNet, and DenseNet are trained to classify 8 different MRI series, and their performance is compared. Then, the best-performing classifier is identified, and its classification capability under the setting of different training data quantities is studied. Also, the model is evaluated on the out-of-training-distribution datasets. Moreover, the model is trained using mpMRI exams obtained from different scanners in two training strategies, and its performance is tested. Experimental results show that the DenseNet-121 model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the other classification models with p-value$<$0.05. The model shows greater than 0.95 accuracy when trained with over 729 studies of the training data, whose performance improves as the training data quantities grew larger. On the external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and 0.810 accuracy for each. These results indicate that in both the internal and external datasets, the DenseNet-121 model attains high accuracy for the task of classifying 8 body MRI series types.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå‚æ•°ç£å…±æŒ¯æˆåƒ(mpMRI)åºåˆ—ä¸­å› æˆåƒåè®®å¤šæ ·åŠäººä¸ºæ“ä½œå¤±è¯¯å¯¼è‡´çš„DICOMå¤´ä¿¡æ¯ä¸å‡†ç¡®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†ç±»æ¨¡å‹ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«8ç§èº«ä½“mpMRIåºåˆ—ç±»å‹ä»¥æå‡æ”¾å°„ç§‘åŒ»å¸ˆçš„è¯Šæ–­æ•ˆç‡ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹æ¯”äº†ResNetã€EfficientNetå’ŒDenseNetç­‰æ¶æ„çš„æ€§èƒ½ï¼Œå¹¶ç³»ç»Ÿåˆ†æäº†è®­ç»ƒæ•°æ®é‡ä»¥åŠè·¨æ‰«æä»ªã€è·¨æœºæ„åœºæ™¯å¯¹åˆ†ç±»æ•ˆæœçš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDenseNet-121æ¨¡å‹è¡¨ç°æœ€ä¸ºå‡ºè‰²ï¼Œåœ¨å†…éƒ¨æµ‹è¯•ä¸­å®ç°äº†0.966çš„F1-scoreå’Œ0.972çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†DLDSå’ŒCPTAC-UCECä¸Šåˆ†åˆ«è·å¾—äº†0.872å’Œ0.810çš„å‡†ç¡®ç‡ï¼Œè¯æ˜å…¶å…·å¤‡è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨å¤„ç†å¤æ‚ä¸´åºŠå½±åƒæ•°æ®åˆ†ç±»ä»»åŠ¡ä¸­çš„é«˜åº¦å¯é æ€§ä¸å®é™…åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15182v1",
      "published_date": "2025-06-18 06:55:38 UTC",
      "updated_date": "2025-06-18 06:55:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:10.084029+00:00"
    },
    {
      "arxiv_id": "2506.15167v2",
      "title": "LLM Agent for Hyper-Parameter Optimization",
      "title_zh": "ç”¨äºè¶…å‚æ•°ä¼˜åŒ–çš„ LLM æ™ºèƒ½ä½“",
      "authors": [
        "Wanzhe Wang",
        "Jianqiu Peng",
        "Menghao Hu",
        "Weihuang Zhong",
        "Tong Zhang",
        "Shuai Wang",
        "Yixin Zhang",
        "Mingjie Shao",
        "Wanli Ni"
      ],
      "abstract": "Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹æ— äººæœº(UAV)è½¨è¿¹ä¸é€šä¿¡é¢†åŸŸä¸­WS-PSO-CMç®—æ³•è¶…å‚æ•°ä¼˜åŒ–(Hyper-Parameter Optimization)è‡ªåŠ¨åŒ–ç¨‹åº¦ä½ä¸”è¿‡åº¦ä¾èµ–å¯å‘å¼æ–¹æ³•çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è‡ªåŠ¨åŒ–è°ƒä¼˜æ™ºèƒ½ä½“ã€‚è¯¥æ™ºèƒ½ä½“é‡‡ç”¨è¿­ä»£æ¡†æ¶å¹¶ç»“åˆModel Context Protocol (MCP)æŠ€æœ¯ï¼Œé€šè¿‡Profileé…ç½®è¶…å‚æ•°è¾¹ç•Œã€ä¼˜åŒ–ç­–ç•¥åŠä»»åŠ¡ç›®æ ‡ï¼Œå®ç°å¯¹WS-PSO-CMç®—æ³•çš„è‡ªä¸»è¿­ä»£ä¸æ€§èƒ½æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMæ™ºèƒ½ä½“ç”Ÿæˆçš„è¶…å‚æ•°åœ¨æœ€å°æ€»é€Ÿç‡(minimal sum-rate)æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„äººç±»å¯å‘å¼å’Œéšæœºç”Ÿæˆæ–¹æ³•ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å…·å¤‡PSOå’ŒWS-PSO-CMç›¸å…³çŸ¥è¯†çš„LLMæ™ºèƒ½ä½“åœ¨å¯»æ‰¾é«˜æ€§èƒ½è¶…å‚æ•°æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¤æ‚é€šä¿¡ç®—æ³•çš„è‡ªåŠ¨ä¼˜åŒ–æä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI"
      ],
      "primary_category": "cs.IT",
      "comment": "6 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15167v2",
      "published_date": "2025-06-18 06:28:22 UTC",
      "updated_date": "2025-07-09 13:20:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:14.999239+00:00"
    },
    {
      "arxiv_id": "2506.15154v1",
      "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
      "title_zh": "SonicVerseï¼šèåˆéŸ³ä¹ç‰¹å¾ä¿¡æ¯çš„å¤šä»»åŠ¡å­¦ä¹ éŸ³ä¹æè¿°ç”Ÿæˆ",
      "authors": [
        "Anuradha Chopra",
        "Abhinaba Roy",
        "Dorien Herremans"
      ],
      "abstract": "Detailed captions that accurately reflect the characteristics of a music piece can enrich music databases and drive forward research in music AI. This paper introduces a multi-task music captioning model, SonicVerse, that integrates caption generation with auxiliary music feature detection tasks such as key detection, vocals detection, and more, so as to directly capture both low-level acoustic details as well as high-level musical attributes. The key contribution is a projection-based architecture that transforms audio input into language tokens, while simultaneously detecting music features through dedicated auxiliary heads. The outputs of these heads are also projected into language tokens, to enhance the captioning input. This framework not only produces rich, descriptive captions for short music fragments but also directly enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model. To train the model, we extended the MusicBench dataset by annotating it with music features using MIRFLEX, a modular music feature extractor, resulting in paired audio, captions and music feature data. Experimental results show that incorporating features in this way improves the quality and detail of the generated captions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SonicVerseï¼Œä¸€ç§å¤šä»»åŠ¡éŸ³ä¹å­—å¹•(Music Captioning)ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡å°†å­—å¹•ç”Ÿæˆä¸è°ƒæ€§æ£€æµ‹(key detection)å’Œäººå£°æ£€æµ‹(vocals detection)ç­‰è¾…åŠ©ä»»åŠ¡ç›¸ç»“åˆï¼Œæ•æ‰éŸ³é¢‘çš„åº•å±‚å£°å­¦ç»†èŠ‚ä¸é«˜å±‚éŸ³ä¹å±æ€§ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºé‡‡ç”¨ä¸€ç§åŸºäºæŠ•å½±(projection-based)çš„æ¶æ„ï¼Œåœ¨å°†éŸ³é¢‘è½¬åŒ–ä¸ºè¯­è¨€æ ‡è®°çš„åŒæ—¶ï¼Œåˆ©ç”¨ä¸“ç”¨è¾…åŠ©å¤´æ£€æµ‹ç‰¹å¾å¹¶å°†å…¶åé¦ˆè‡³å­—å¹•ç”Ÿæˆè¾“å…¥ä¸­ã€‚ä¸ºæ”¯æŒè®­ç»ƒï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æ¨¡å—åŒ–ç‰¹å¾æå–å™¨MIRFLEXæ‰©å±•äº†MusicBenchæ•°æ®é›†ï¼Œæ ‡æ³¨äº†ä¸°å¯Œçš„éŸ³ä¹ç‰¹å¾æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§å¤šä»»åŠ¡å­¦ä¹ æ–¹å¼æ˜¾è‘—å¢å¼ºäº†ç”Ÿæˆå­—å¹•çš„è´¨é‡å’Œæè¿°æ·±åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç»“åˆå¤§è¯­è¨€æ¨¡å‹(LLM)å®ç°äº†å¯¹é•¿éŸ³ä¹ç‰‡æ®µçš„æ—¶åºåŒ–è¯¦ç»†æè¿°ï¼Œä¸ºéŸ³ä¹AIé¢†åŸŸçš„ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "14 pages, 2 figures, Accepted to AIMC 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15154v1",
      "published_date": "2025-06-18 05:51:36 UTC",
      "updated_date": "2025-06-18 05:51:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:26.491233+00:00"
    },
    {
      "arxiv_id": "2506.15751v1",
      "title": "Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts",
      "title_zh": "Sysformerï¼šåˆ©ç”¨è‡ªé€‚åº”ç³»ç»Ÿæç¤ºä¿éšœå†»ç»“å¤§è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§",
      "authors": [
        "Kartik Sharma",
        "Yiqiao Jin",
        "Vineeth Rakesh",
        "Yingtong Dou",
        "Menghai Pan",
        "Mahashweta Das",
        "Srijan Kumar"
      ],
      "abstract": "As large language models (LLMs) are deployed in safety-critical settings, it is essential to ensure that their responses comply with safety standards. Prior research has revealed that LLMs often fail to grasp the notion of safe behaviors, resulting in either unjustified refusals to harmless prompts or the generation of harmful content. While substantial efforts have been made to improve their robustness, existing defenses often rely on costly fine-tuning of model parameters or employ suboptimal heuristic techniques. In this work, we take a novel approach to safeguard LLMs by learning to adapt the system prompts in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a fixed system prompt, we investigate the impact of tailoring the system prompt to each specific user input on the safety of the responses. To this end, we propose $\\textbf{Sysformer}$, a trans$\\textbf{former}$ model that updates an initial $\\textbf{sys}$tem prompt to a more robust system prompt in the LLM input embedding space while attending to the user prompt. While keeping the LLM parameters frozen, the Sysformer is trained to refuse to respond to a set of harmful prompts while responding ideally to a set of safe ones. Through extensive experiments on $5$ LLMs from different families and $2$ recent benchmarks, we demonstrate that Sysformer can significantly enhance the robustness of LLMs, leading to upto $80\\%$ gain in the refusal rate on harmful prompts while enhancing the compliance with the safe prompts by upto $90\\%$. Results also generalize well to sophisticated jailbreaking attacks, making LLMs upto $100\\%$ more robust against different attack strategies. We hope our findings lead to cheaper safeguarding of LLMs and motivate future investigations into designing variable system prompts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Sysformerï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å­¦ä¹ è‡ªé€‚åº”ç³»ç»Ÿæç¤º(system prompts)æ¥ä¿éšœå†»ç»“çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å®‰å…¨æ€§çš„æ–°é¢–æ–¹æ³•ã€‚ä¸ºäº†å…‹æœç°æœ‰é˜²å¾¡æ‰‹æ®µä¾èµ–é«˜æ˜‚å¾®è°ƒæˆæœ¬æˆ–å¯å‘å¼æŠ€æœ¯çš„å±€é™æ€§ï¼ŒSysformeråˆ©ç”¨ä¸€ä¸ªtransformeræ¨¡å‹åœ¨LLMçš„è¾“å…¥åµŒå…¥ç©ºé—´å†…ï¼Œç»“åˆç”¨æˆ·æç¤º(user prompt)åŠ¨æ€æ›´æ–°åˆå§‹ç³»ç»Ÿæç¤ºã€‚åœ¨ä¿æŒLLMå‚æ•°ä¸å˜çš„å‰æä¸‹ï¼Œè¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒèƒ½å¤Ÿç²¾å‡†æ‹’ç»æœ‰å®³æŒ‡ä»¤ï¼Œå¹¶å¯¹å®‰å…¨æŒ‡ä»¤åšå‡ºç†æƒ³å“åº”ã€‚åœ¨5ç§ä¸åŒç³»åˆ—çš„LLMså’Œ2ä¸ªæœ€æ–°åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSysformeræ˜¾è‘—æå‡äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œä½¿æœ‰å®³æç¤ºçš„æ‹’ç»ç‡æå‡äº†é«˜è¾¾80%ï¼Œå®‰å…¨æç¤ºçš„åˆè§„æ€§æé«˜äº†90%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹å¤æ‚çš„è¶Šç‹±æ”»å‡»(jailbreaking attacks)å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°†é˜²å¾¡æ€§èƒ½æå‡äº†é«˜è¾¾100%ï¼Œä¸ºå¤§æ¨¡å‹çš„å®‰å…¨é˜²æŠ¤æä¾›äº†ä¸€ç§ä½æˆæœ¬ä¸”é«˜æ•ˆçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15751v1",
      "published_date": "2025-06-18 05:48:05 UTC",
      "updated_date": "2025-06-18 05:48:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:34.588815+00:00"
    },
    {
      "arxiv_id": "2506.15138v1",
      "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models",
      "title_zh": "Thunder-Tokï¼šæœ€å°åŒ–ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹éŸ©æ–‡åˆ†è¯ä¸­çš„å•å­—æ ‡è®°æ•°",
      "authors": [
        "Gyeongje Cho",
        "Yeonkyoun So",
        "Chanwoo Park",
        "Sangmin Lee",
        "Sungmok Jung",
        "Jaejin Lee"
      ],
      "abstract": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce token fertility without compromising model performance. Our approach uses a rule-based pre-tokenization method that aligns with the linguistic structure of the Korean language. We also create a seed vocabulary containing tokens that resemble linguistic units and employ a branching entropy-based selection algorithm. These techniques increase the average token length, thus lowering fertility while preserving linguistic information. Experimental results indicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces the number of tokens by 10%, improving the inference speed by 10%) compared to BPE without compromising performance across various downstream tasks. These findings demonstrate that our linguistically informed approach is effective and practical for designing efficient tokenizers for language models.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¨å‡ºäº† Thunder-Tokï¼Œä¸€ç§ä¸“ä¸ºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹è®¾è®¡çš„æ–°å‹éŸ©è¯­åˆ†è¯å™¨ (tokenizer)ï¼Œæ—¨åœ¨ä¸æŸå®³æ¨¡å‹æ€§èƒ½çš„å‰æä¸‹é™ä½åˆ†è¯å€ç‡ (token fertility)ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡‡ç”¨ä¸éŸ©è¯­è¯­è¨€ç»“æ„ç›¸å¥‘åˆçš„åŸºäºè§„åˆ™çš„é¢„åˆ†è¯ (pre-tokenization) æ‰‹æ®µï¼Œå¹¶ç»“åˆåŸºäºåˆ†æ”¯ç†µ (branching entropy) çš„é€‰æ‹©ç®—æ³•åŠåŒ…å«ç±»è¯­è¨€å•å…ƒæ ‡è®°çš„ç§å­è¯è¡¨ (seed vocabulary)ï¼Œæ˜¾è‘—å¢åŠ äº†å¹³å‡æ ‡è®°é•¿åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ BPE ç›¸æ¯”ï¼ŒThunder-Tok åœ¨å„ç±»ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒæ€§èƒ½ç¨³å®šçš„åŒæ—¶ï¼Œå°†åˆ†è¯å€ç‡é™ä½äº†çº¦ 10%ï¼Œç›´æ¥å¸¦æ¥äº† 10% çš„æ¨ç†é€Ÿåº¦æå‡ã€‚è¿™ä¸€ç ”ç©¶æˆæœå……åˆ†è¯æ˜äº†ç»“åˆè¯­è¨€å­¦ç‰¹å¾çš„ä¼˜åŒ–ç­–ç•¥åœ¨æ„å»ºé«˜æ•ˆã€å®ç”¨çš„è¯­è¨€æ¨¡å‹åˆ†è¯å™¨æ–¹é¢çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15138v1",
      "published_date": "2025-06-18 04:40:44 UTC",
      "updated_date": "2025-06-18 04:40:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:49.494319+00:00"
    },
    {
      "arxiv_id": "2508.00835v2",
      "title": "PCS Workflow for Veridical Data Science in the Age of AI",
      "title_zh": "äººå·¥æ™ºèƒ½æ—¶ä»£ä¸‹çœŸå®æ€§æ•°æ®ç§‘å­¦çš„ PCS å·¥ä½œæµ",
      "authors": [
        "Zachary T. Rewolinski",
        "Bin Yu"
      ],
      "abstract": "Data science is a pillar of artificial intelligence (AI), which is transforming nearly every domain of human activity, from the social and physical sciences to engineering and medicine. While data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty. The Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science offers a principled approach to addressing this challenge throughout the DSLC. This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI. We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)æ—¶ä»£èƒŒæ™¯ä¸‹æ•°æ®ç§‘å­¦(Data Science)é¢ä¸´çš„å‘ç°éš¾ä»¥å¤ç°çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæ•°æ®ç§‘å­¦ç”Ÿå‘½å‘¨æœŸ(DSLC)ä¸­äººä¸ºé€‰æ‹©å¸¦æ¥çš„ä¸ç¡®å®šæ€§æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ›´æ–°ä¸”ç®€åŒ–çš„Predictability-Computability-Stability (PCS)å·¥ä½œæµï¼Œæ—¨åœ¨ä¸ºå®ç°çœŸå®å¯é (veridical)çš„æ•°æ®ç§‘å­¦æä¾›åŸåˆ™æ€§æ–¹æ³•ã€‚è¯¥å·¥ä½œæµä¸“ä¸ºä»ä¸šè€…è®¾è®¡ï¼Œå¹¶èå…¥äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„è¾…åŠ©æŒ‡å¯¼ï¼Œä»¥å¢å¼ºåˆ†æçš„ä¸¥è°¨æ€§ã€‚é€šè¿‡å…·ä½“çš„è¿è¡Œç¤ºä¾‹å’Œæ¡ˆä¾‹ç ”ç©¶ï¼Œä½œè€…å±•ç¤ºäº†PCSæ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆèƒ½ï¼Œç‰¹åˆ«æ˜¯æ­ç¤ºäº†æ•°æ®æ¸…æ´—(Data Cleaning)é˜¶æ®µçš„ä¸»è§‚åˆ¤æ–­å¯¹ä¸‹æ¸¸é¢„æµ‹ç»“æœäº§ç”Ÿçš„ä¸ç¡®å®šæ€§å½±å“ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2508.00835v2",
      "published_date": "2025-06-18 04:25:28 UTC",
      "updated_date": "2025-12-03 18:51:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:47:41.694646+00:00"
    },
    {
      "arxiv_id": "2506.15131v2",
      "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å¯¹å¼€æ”¾åŸŸå¯¹è¯ä¸­çš„ä¸€å¯¹å¤šç‰¹æ€§è¿›è¡Œå»ºæ¨¡",
      "authors": [
        "Jing Yang Lee",
        "Kong-Aik Lee",
        "Woon-Seng Gan"
      ],
      "abstract": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby multiple appropriate responses exist for a single dialogue context. Despite prior research showing that modeling this property boosts response diversity, most modern LLM-based dialogue agents do not explicitly do so. In this work, we model the o2m property of OD in LLMs by decomposing OD generation into two key tasks: Multi-Response Generation (MRG) and Preference-based Selection (PS), which entail generating a set of n semantically and lexically diverse high-quality responses for a given dialogue context, followed by selecting a single response based on human preference, respectively. To facilitate MRG and PS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the o2m property by featuring multiple plausible responses for each context. Leveraging o2mDial, we propose new in-context learning and instruction-tuning strategies, as well as novel evaluation metrics for MRG, alongside a model-based approach for PS. Empirical results demonstrate that applying the proposed two-stage framework to smaller LLMs for OD generation enhances overall response diversity while maintaining contextual coherence, improving response quality by up to 90%, bringing them closer to the performance of larger models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æ”¾åŸŸå¯¹è¯(Open-domain Dialogue)ä¸­ä¸€ä¸ªè¯­å¢ƒå¯¹åº”å¤šä¸ªåˆç†å›å¤çš„ä¸€å¯¹å¤š(one-to-many)å±æ€§ï¼ŒæŒ‡å‡ºç›®å‰çš„è¯­è¨€å¤§æ¨¡å‹(LLMs)å¤§å¤šç¼ºä¹å¯¹æ­¤å±æ€§çš„æ˜¾å¼å»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…å°†ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºå¤šå›å¤ç”Ÿæˆ(Multi-Response Generation, MRG)å’ŒåŸºäºåå¥½çš„é€‰æ‹©(Preference-based Selection, PS)ä¸¤ä¸ªé˜¶æ®µã€‚æœ¬æ–‡å¼•å…¥äº†ä¸“é—¨è®¾è®¡çš„è¯­æ–™åº“o2mDialä»¥æ”¯æŒè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è¯­å¢ƒå­¦ä¹ (in-context learning)å’ŒæŒ‡ä»¤å¾®è°ƒ(instruction-tuning)ç­–ç•¥ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼€å‘äº†é’ˆå¯¹MRGçš„æ–°å‹è¯„ä¼°æŒ‡æ ‡ä»¥åŠé’ˆå¯¹PSçš„æ¨¡å‹åŒ–æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ä¸¤é˜¶æ®µæ¡†æ¶èƒ½æ˜¾è‘—æå‡è¾ƒå°è§„æ¨¡LLMsçš„å›å¤å¤šæ ·æ€§ä¸ä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚æœ€ç»ˆï¼Œè¯¥æ–¹æ³•å°†å›å¤è´¨é‡æé«˜äº†å¤šè¾¾90%ï¼Œä½¿å…¶åœ¨å¯¹è¯è¡¨ç°ä¸Šèƒ½å¤Ÿä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç›¸åª²ç¾ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15131v2",
      "published_date": "2025-06-18 04:19:33 UTC",
      "updated_date": "2026-01-02 17:03:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:07.201683+00:00"
    },
    {
      "arxiv_id": "2506.15746v3",
      "title": "Neural Cellular Automata for ARC-AGI",
      "title_zh": "é¢å‘ ARC-AGI çš„ç¥ç»å…ƒèƒè‡ªåŠ¨æœº",
      "authors": [
        "Kevin Xu",
        "Risto Miikkulainen"
      ],
      "abstract": "Cellular automata and their differentiable counterparts, Neural Cellular Automata (NCA), are highly expressive and capable of surprisingly complex behaviors. This paper explores how NCAs perform when applied to tasks requiring precise transformations and few-shot generalization, using the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) as a domain that challenges their capabilities in ways not previously explored. Specifically, this paper uses gradient-based training to learn iterative update rules that transform input grids into their outputs from the training examples and apply them to the test inputs. Results suggest that gradient-trained NCA models are a promising and efficient approach to a range of abstract grid-based tasks from ARC. Along with discussing the impacts of various design modifications and training constraints, this work examines the behavior and properties of NCAs applied to ARC to give insights for broader applications of self-organizing systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢ç´¢äº†ç¥ç»å…ƒç»†èƒè‡ªåŠ¨æœº(Neural Cellular Automata, NCA)åœ¨å¤„ç†éœ€è¦ç²¾ç¡®å˜æ¢å’Œå°‘æ ·æœ¬æ³›åŒ–(few-shot generalization)ä»»åŠ¡æ—¶çš„è¡¨ç°ï¼Œå¹¶ä»¥é€šç”¨äººå·¥æ™ºèƒ½æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“(ARC-AGI)ä½œä¸ºæŒ‘æˆ˜å…¶èƒ½åŠ›çš„æµ‹è¯•åŸºå‡†ã€‚ç ”ç©¶é‡‡ç”¨äº†åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ–¹æ³•(gradient-based training)ï¼Œé€šè¿‡ä»è®­ç»ƒç¤ºä¾‹ä¸­å­¦ä¹ è¿­ä»£æ›´æ–°è§„åˆ™ï¼Œå®ç°å°†è¾“å…¥ç½‘æ ¼è½¬æ¢ä¸ºç›®æ ‡è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡æ¢¯åº¦è®­ç»ƒçš„NCAæ¨¡å‹åœ¨å¤„ç†ARCä¸­å„ç§åŸºäºç½‘æ ¼çš„æŠ½è±¡ä»»åŠ¡æ—¶ï¼Œæ˜¯ä¸€ç§å…·æœ‰æ½œåŠ›ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥å·¥ä½œè¿˜æ·±å…¥æ¢è®¨äº†ä¸åŒè®¾è®¡ä¿®æ”¹å’Œè®­ç»ƒçº¦æŸå¯¹æ€§èƒ½çš„å½±å“ï¼Œåˆ†æäº†NCAåœ¨å¤„ç†ARCä»»åŠ¡æ—¶çš„è¡Œä¸ºç‰¹æ€§ã€‚è¿™ä¸€æ¢ç´¢æ€§å·¥ä½œä¸ºè‡ªç»„ç»‡ç³»ç»Ÿ(self-organizing systems)åœ¨æ›´å¹¿æ³›é¢†åŸŸä¸­çš„åº”ç”¨æä¾›äº†é‡è¦çš„è§è§£ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "8 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15746v3",
      "published_date": "2025-06-18 03:47:31 UTC",
      "updated_date": "2025-12-02 01:36:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:09.087655+00:00"
    },
    {
      "arxiv_id": "2506.15120v1",
      "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study with a RÃ©nyi Divergence-Based Solution",
      "title_zh": "æ¨èç³»ç»ŸæŸå¤±å‡½æ•°çš„è¿›é˜¶ï¼šåŸºäº RÃ©nyi æ•£åº¦çš„è§£å†³æ–¹æ¡ˆåŠå…¶å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "Shengjia Zhang",
        "Jiawei Chen",
        "Changdong Li",
        "Sheng Zhou",
        "Qihao Shi",
        "Yan Feng",
        "Chun Chen",
        "Can Wang"
      ],
      "abstract": "Loss functions play a pivotal role in optimizing recommendation models. Among various loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are particularly effective. Their theoretical connections and differences warrant in-depth exploration. This work conducts comprehensive analyses of these losses, yielding significant insights: 1) Common strengths -- both can be viewed as augmentations of traditional losses with Distributional Robust Optimization (DRO), enhancing robustness to distributional shifts; 2) Respective limitations -- stemming from their use of different distribution distance metrics in DRO optimization, SL exhibits high sensitivity to false negative instances, whereas CCL suffers from low data utilization. To address these limitations, this work proposes a new loss function, DrRL, which generalizes SL and CCL by leveraging RÃ©nyi-divergence in DRO optimization. DrRL incorporates the advantageous structures of both SL and CCL, and can be demonstrated to effectively mitigate their limitations. Extensive experiments have been conducted to validate the superiority of DrRL on both recommendation accuracy and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨èç³»ç»Ÿä¸­çš„æŸå¤±å‡½æ•°è¿›è¡Œäº†æ·±å…¥æ¯”è¾ƒï¼Œé‡ç‚¹åˆ†æäº†Softmax Loss (SL)å’ŒCosine Contrastive Loss (CCL)çš„ç†è®ºè”ç³»ä¸å·®å¼‚ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ä¸¤ç§æŸå¤±å‡½æ•°éƒ½å¯ä»¥è¢«è§†ä¸ºé€šè¿‡åˆ†å¸ƒé²æ£’ä¼˜åŒ–(Distributional Robust Optimization, DRO)å¯¹ä¼ ç»ŸæŸå¤±çš„å¢å¼ºï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹åˆ†å¸ƒåç§»çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œç”±äºä¸¤è€…åœ¨DROä¼˜åŒ–ä¸­é‡‡ç”¨äº†ä¸åŒçš„åˆ†å¸ƒè·ç¦»åº¦é‡ï¼ŒSLå¯¹å‡è´Ÿä¾‹(false negative)è¡¨ç°å‡ºé«˜åº¦æ•æ„Ÿæ€§ï¼Œè€ŒCCLåˆ™é¢ä¸´æ•°æ®åˆ©ç”¨ç‡è¾ƒä½çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹æŸå¤±å‡½æ•°DrRLï¼Œé€šè¿‡åœ¨DROä¼˜åŒ–ä¸­å¼•å…¥RÃ©nyi-divergenceï¼Œå®ç°äº†å¯¹SLå’ŒCCLçš„æ³›åŒ–é›†æˆã€‚DrRLç»“åˆäº†SLå’ŒCCLçš„ä¼˜åŠ¿ç»“æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç¼“è§£ä¸¤è€…åœ¨è´Ÿé‡‡æ ·æ•æ„Ÿåº¦å’Œæ•°æ®æ•ˆç‡æ–¹é¢çš„ä¸è¶³ã€‚å®éªŒç»“æœéªŒè¯äº†DrRLåœ¨æ¨èå‡†ç¡®æ€§å’Œé²æ£’æ€§æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "AAAI 2025",
      "pdf_url": "https://arxiv.org/pdf/2506.15120v1",
      "published_date": "2025-06-18 03:39:13 UTC",
      "updated_date": "2025-06-18 03:39:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:11.184597+00:00"
    },
    {
      "arxiv_id": "2506.15113v2",
      "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning",
      "title_zh": "Transit for Allï¼šåŸºäºåŒºåŸŸè¡¨ç¤ºå­¦ä¹ çš„å…¬å¹³â€œè‡ªè¡Œè½¦-åœ°é“â€è¡”æ¥ç©ºé—´åˆ¶å›¾",
      "authors": [
        "Min Namgung",
        "JangHyeon Lee",
        "Fangyi Ding",
        "Yao-Yi Chiang"
      ],
      "abstract": "Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned (\"cold-start\") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Transit for All (TFA)ç©ºé—´è®¡ç®—æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–å…±äº«å•è½¦ç³»ç»Ÿ (Bike-sharing systems, BSS) çš„å¸ƒå±€æ¥æå‡åŸå¸‚äº¤é€šçš„å…¬å¹³æ€§ï¼Œè§£å†³ä½æ”¶å…¥å’Œå°‘æ•°ç¾¤ä½“ç¤¾åŒºé¢ä¸´çš„ç¬¬ä¸€å…¬é‡Œå’Œæœ€åä¸€å…¬é‡Œæ¥é©³éš¾é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŒºåŸŸè¡¨ç¤ºå­¦ä¹  (region representation learning) æ•´åˆå¤šæ¨¡æ€åœ°ç†ç©ºé—´æ•°æ®ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹å¤„äºâ€œå†·å¯åŠ¨â€ (cold-start) çŠ¶æ€çš„æ–°è®¾ç«™ç‚¹éœ€æ±‚ã€‚ç ”ç©¶å¼•å…¥äº†åŠ æƒå…¬å…±äº¤é€šé€šè¾¾åº¦æ°´å¹³ (weighted Public Transport Accessibility Level, wPTAL) æŒ‡æ ‡ï¼Œé€šè¿‡å°†é¢„æµ‹éœ€æ±‚ä¸ä¼ ç»Ÿäº¤é€šæŒ‡æ ‡ç›¸ç»“åˆï¼Œå®ç°äº†å¯¹é€šè¾¾åº¦çš„å…¨é¢è¯„ä¼°ã€‚é€šè¿‡å¯¹çº½çº¦å¸‚ (NYC) çš„æ¡ˆä¾‹ç ”ç©¶ï¼ŒTFAæˆåŠŸè¯†åˆ«å‡ºå†å²ä¸Šè¢«å¿½è§†åœ°åŒºçš„äº¤é€šå¯è¾¾æ€§ç¼ºå£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºwPTALå¼•å¯¼çš„ç«™ç‚¹æˆ˜ç•¥å¸ƒå±€èƒ½æ˜¾è‘—å‡å°‘å› ç»æµå’Œäººå£å› ç´ å¯¼è‡´çš„äº¤é€šæœåŠ¡å·®å¼‚ã€‚è¯¥æ¡†æ¶ä¸ºåŸå¸‚è§„åˆ’è€…æä¾›äº†å®è·µæŒ‡å¯¼ï¼Œæœ‰åŠ©äºé€šè¿‡æŠ€æœ¯æ‰‹æ®µå¢å¼ºç¤¾ä¼šå…¬å¹³å¹¶æ”¹å–„æ¬ å‘è¾¾ç¤¾åŒºçš„ç”Ÿæ´»è´¨é‡ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "SIGSPATIAL 25",
      "pdf_url": "https://arxiv.org/pdf/2506.15113v2",
      "published_date": "2025-06-18 03:31:07 UTC",
      "updated_date": "2025-09-06 20:55:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:14.089463+00:00"
    },
    {
      "arxiv_id": "2507.02877v1",
      "title": "AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations",
      "title_zh": "AuraGenomeï¼šå¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å³æ—¶ã€å¯å¤ç”¨ä¸”å¯æ‰©å±•çš„ç¯å½¢åŸºå› ç»„å¯è§†åŒ–æ¡†æ¶",
      "authors": [
        "Chi Zhang",
        "Yu Dong",
        "Yang Wang",
        "Yuetong Han",
        "Guihua Shan",
        "Bixia Tang"
      ],
      "abstract": "Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¨å‡ºäº† AuraGenomeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å¿«é€Ÿã€å¯å¤ç”¨ä¸”å¯æ‰©å±•çš„å¤šå±‚ç¯å½¢åŸºå› ç»„å¯è§†åŒ– (circular genome visualizations)ï¼Œè§£å†³äº†ç°æœ‰å·¥å…·è„šæœ¬å¤æ‚ä¸”æ‰‹åŠ¨é…ç½®ç¹ççš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†è¯­ä¹‰é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµ (multi-agent workflow) ä¸äº¤äº’å¼å¯è§†åŒ–åˆ†æç³»ç»Ÿç›¸ç»“åˆï¼Œåˆ©ç”¨ä¸ƒä¸ªä¸“é—¨çš„ LLM é©±åŠ¨æ™ºèƒ½ä½“ååŒå®Œæˆæ„å›¾è¯†åˆ« (intent recognition)ã€å¸ƒå±€è§„åˆ’ (layout planning) å’Œä»£ç ç”Ÿæˆ (code generation) ç­‰æ ¸å¿ƒä»»åŠ¡ã€‚ç³»ç»Ÿæ”¯æŒç¯å½¢ (ring)ã€å¾„å‘ (radial) å’Œå¼¦å›¾ (chord-based) ç­‰å¤šç§åè°ƒè§†å›¾å¸ƒå±€ï¼Œèƒ½å¤Ÿç²¾ç»†å±•ç¤ºå¤æ‚çš„ç»“æ„å˜å¼‚ä¸åŸºå› è°ƒæ§ã€‚æ­¤å¤–ï¼ŒAuraGenome è¿˜æä¾›å®æ—¶ç»†åŒ–ã€é…ç½®å¤ç”¨åŠé«˜è´¨é‡æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½ï¼Œæ˜¾è‘—é™ä½äº†ç”¨æˆ·çš„å­¦ä¹ é—¨æ§›å¹¶æé«˜äº†åˆ†ææ•ˆç‡ã€‚é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶å’Œä¸€é¡¹å…¨é¢çš„ç”¨æˆ·ç ”ç©¶ï¼Œè¯¥æ¡†æ¶åœ¨å¤„ç†å¤šå±‚åŸºå› ç»„æ•°æ®å¯è§†åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†å……åˆ†éªŒè¯ï¼Œä¸ºç”Ÿç‰©ä¿¡æ¯å­¦ç ”ç©¶æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.GR",
        "cs.HC"
      ],
      "primary_category": "q-bio.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.02877v1",
      "published_date": "2025-06-18 03:29:30 UTC",
      "updated_date": "2025-06-18 03:29:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:25.400928+00:00"
    },
    {
      "arxiv_id": "2506.22462v1",
      "title": "Privacy-aware IoT Fall Detection Services For Aging in Place",
      "title_zh": "é¢å‘å±…å®¶å…»è€çš„éšç§æ„ŸçŸ¥ç‰©è”ç½‘è·Œå€’æ£€æµ‹æœåŠ¡",
      "authors": [
        "Abdallah Lakhdari",
        "Jiajie Li",
        "Amani Abusafia",
        "Athman Bouguettaya"
      ],
      "abstract": "Fall detection is critical to support the growing elderly population, projected to reach 2.1 billion by 2050. However, existing methods often face data scarcity challenges or compromise privacy. We propose a novel IoT-based Fall Detection as a Service (FDaaS) framework to assist the elderly in living independently and safely by accurately detecting falls. We design a service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors as an IoT health-sensing service, ensuring privacy and minimal intrusion. We address the challenges of data scarcity by utilizing a Fall Detection Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques. We developed a protocol to collect a comprehensive dataset of the elderly daily activities and fall events. This resulted in a real dataset that carefully mimics the elderly's routine. We rigorously evaluate and compare various models using this dataset. Experimental results show our approach achieves 90.72% accuracy and 89.33% precision in distinguishing between fall events and regular activities of daily living.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¥ç›Šå¢é•¿çš„è€é¾„åŒ–äººå£éœ€æ±‚ï¼Œæå‡ºäº†ä¸€ç§åä¸ºIoT-based Fall Detection as a Service (FDaaS)çš„æ–°å‹ç‰©è”ç½‘è·Œå€’æ£€æµ‹æœåŠ¡æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒè€å¹´äººå®‰å…¨ã€ç‹¬ç«‹åœ°å±…å®¶å…»è€ã€‚ä¸ºè§£å†³ç°æœ‰ç›‘æµ‹æ–¹æ³•åœ¨éšç§ä¿æŠ¤æ–¹é¢çš„ä¸è¶³ï¼Œè¯¥æ¶æ„é‡‡ç”¨äº†Ultra-wideband (UWB)é›·è¾¾ä¼ æ„Ÿå™¨ä½œä¸ºå¥åº·æ„ŸçŸ¥æ‰‹æ®µï¼Œç¡®ä¿äº†ç›‘æµ‹è¿‡ç¨‹çš„éšç§æ€§å’Œéä¾µå…¥æ€§ã€‚é’ˆå¯¹é¢†åŸŸå†…æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼Œç ”ç©¶å¼•å…¥äº†Fall Detection Generative Pre-trained Transformer (FD-GPT)å¹¶ç»“åˆæ•°æ®å¢å¼ºæŠ€æœ¯ï¼ŒåŒæ—¶æ„å»ºäº†ä¸€å¥—åŒ…å«è€å¹´äººæ—¥å¸¸ç”Ÿæ´»ä¸è·Œå€’äº‹ä»¶çš„ç»¼åˆæ•°æ®é›†ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åŒºåˆ†è·Œå€’è¡Œä¸ºä¸æ—¥å¸¸æ´»åŠ¨æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†90.72%ï¼Œç²¾ç¡®ç‡è¾¾åˆ°89.33%ã€‚è¯¥ç ”ç©¶é€šè¿‡æœåŠ¡å¯¼å‘çš„ä½“ç³»ç»“æ„ï¼Œä¸ºå®ç°éšç§æ„ŸçŸ¥ä¸”é«˜ç²¾åº¦çš„è·Œå€’æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "eess.SP",
      "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE International Conference on Web Services (ICWS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2506.22462v1",
      "published_date": "2025-06-18 03:28:07 UTC",
      "updated_date": "2025-06-18 03:28:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:29.886500+00:00"
    },
    {
      "arxiv_id": "2507.00033v1",
      "title": "Moment Sampling in Video LLMs for Long-Form Video QA",
      "title_zh": "è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘é—®ç­”ä¸­çš„æ—¶åˆ»é‡‡æ ·",
      "authors": [
        "Mustafa Chasmai",
        "Gauri Jagatap",
        "Gouthaman KV",
        "Grant Van Horn",
        "Subhransu Maji",
        "Andrea Fanelli"
      ],
      "abstract": "Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose \"moment sampling\", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘å¤§è¯­è¨€æ¨¡å‹(Video LLMs)åœ¨å¤„ç†é•¿è§†é¢‘é—®ç­”(Long-form VideoQA)æ—¶ä¼ ç»Ÿç­‰é—´éš”å¸§é‡‡æ ·(frame sub-sampling)å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§åä¸ºMoment Samplingçš„åˆ›æ–°æ¨¡å‹æ— å…³(model-agnostic)æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥è½»é‡çº§çš„æ–‡æœ¬åˆ°è§†é¢‘æ—¶åˆ»æ£€ç´¢(text-to-video moment retrieval)æ¨¡å‹ï¼Œé€šè¿‡é—®é¢˜çš„ä¸Šä¸‹æ–‡å¼•å¯¼å¸§é‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œä¼˜å…ˆé€‰å–ä¸é—®é¢˜æœ€ç›¸å…³çš„å…³é”®å¸§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å››ä¸ªä¸»æµé•¿è§†é¢‘é—®ç­”æ•°æ®é›†ä¸Šï¼Œç»“åˆMoment Samplingåçš„å››ç§æœ€å…ˆè¿›Video LLMså‡å±•ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é€šè¿‡ç²¾å‡†è¯†åˆ«å…³é”®è§†é¢‘ç‰‡æ®µå¹¶å‰”é™¤å†—ä½™ä¿¡æ¯ï¼Œè¯¥ç ”ç©¶æœ‰æ•ˆå¢å¼ºäº†æ¨¡å‹åœ¨é•¿è§†é¢‘ä»»åŠ¡ä¸­çš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ä¸è®¡ç®—æ•ˆç‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025",
      "pdf_url": "https://arxiv.org/pdf/2507.00033v1",
      "published_date": "2025-06-18 03:23:56 UTC",
      "updated_date": "2025-06-18 03:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:26.585251+00:00"
    },
    {
      "arxiv_id": "2506.15081v1",
      "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification",
      "title_zh": "é€šè¿‡ç¯‡ç« æ„ŸçŸ¥çš„è¯è¯­æ¾„æ¸…æå‡å¯¹è¯ç¯‡ç« è§£æ",
      "authors": [
        "Yaxin Fan",
        "Peifeng Li",
        "Qiaoming Zhu"
      ],
      "abstract": "Dialogue discourse parsing aims to identify and analyze discourse relations between the utterances within dialogues. However, linguistic features in dialogues, such as omission and idiom, frequently introduce ambiguities that obscure the intended discourse relations, posing significant challenges for parsers. To address this issue, we propose a Discourse-aware Clarification Module (DCM) to enhance the performance of the dialogue discourse parser. DCM employs two distinct reasoning processes: clarification type reasoning and discourse goal reasoning. The former analyzes linguistic features, while the latter distinguishes the intended relation from the ambiguous one. Furthermore, we introduce Contribution-aware Preference Optimization (CPO) to mitigate the risk of erroneous clarifications, thereby reducing cascading errors. CPO enables the parser to assess the contributions of the clarifications from DCM and provide feedback to optimize the DCM, enhancing its adaptability and alignment with the parser's requirements. Extensive experiments on the STAC and Molweni datasets demonstrate that our approach effectively resolves ambiguities and significantly outperforms the state-of-the-art (SOTA) baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¯¹è¯è¯è¯­è§£æ(Dialogue discourse parsing)ä¸­å› çœç•¥ã€ä¹ è¯­ç­‰è¯­è¨€ç‰¹å¾å¯¼è‡´çš„æ­§ä¹‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¾€å¾€ä¼šæ¨¡ç³Šé¢„æœŸçš„è¯­ç”¨å…³ç³»å¹¶ç»™è§£æå™¨å¸¦æ¥å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è¯è¯­æ„ŸçŸ¥æ¾„æ¸…æ¨¡å—(Discourse-aware Clarification Module, DCM)ï¼Œè¯¥æ¨¡å—é€šè¿‡æ¾„æ¸…ç±»å‹æ¨ç†åˆ†æè¯­è¨€ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¯è¯­ç›®æ ‡æ¨ç†ä»æ­§ä¹‰ä¸­è¾¨åˆ«å‡ºé¢„æœŸçš„å…³ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥é™ä½çº§è”é”™è¯¯çš„é£é™©ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†è´¡çŒ®æ„ŸçŸ¥åå¥½ä¼˜åŒ–(Contribution-aware Preference Optimization, CPO)æœºåˆ¶ï¼Œä½¿è§£æå™¨èƒ½å¤Ÿè¯„ä¼°æ¾„æ¸…å†…å®¹çš„è´¡çŒ®å¹¶åé¦ˆç»™DCMä»¥å®ç°ååŒä¼˜åŒ–ã€‚åœ¨STACå’ŒMolweniæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤å¯¹è¯ä¸­çš„æ­§ä¹‰ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç›®å‰çš„å…ˆè¿›åŸºçº¿(SOTA)æ¨¡å‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by ACL2025(main conference)",
      "pdf_url": "https://arxiv.org/pdf/2506.15081v1",
      "published_date": "2025-06-18 02:47:14 UTC",
      "updated_date": "2025-06-18 02:47:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:30.491605+00:00"
    },
    {
      "arxiv_id": "2506.15051v1",
      "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization",
      "title_zh": "é¢å‘è‡ªé€‚åº”è¶…å‚æ•°ä¼˜åŒ–çš„åºåˆ—ç­–ç•¥æ¢¯åº¦",
      "authors": [
        "Zheng Li",
        "Jerry Cheng",
        "Huanying Helen Gu"
      ],
      "abstract": "Reinforcement learning is essential for neural architecture search and hyperparameter optimization, but the conventional approaches impede widespread use due to prohibitive time and computational costs. Inspired by DeepSeek-V3 multi-token prediction architecture, we propose Sequential Policy Gradient modeling (SPG), a novel trajectory generation paradigm for lightweight online hyperparameter optimization. In contrast to conventional policy gradient methods, SPG extends the base model with temporary modules, enabling it to generate state-action (padded) trajectories in a single forward pass. Our experiments demonstrate that models gain performance when retrained with SPG on their original datasets and also outperform standard transfer fine-tuning. We evaluate on five datasets spanning computer vision (ImageNet, COCO), natural language processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial applicability of SPG. The proposed method demonstrates consistent improvements across widely adopted models, achieving performance gains of $+0.2\\sim7\\%$, with significantly low computational costs. Fully reproducible code and pre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Sequential Policy Gradient (SPG)ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè½»é‡åŒ–åœ¨çº¿è¶…å‚æ•°ä¼˜åŒ–(Hyperparameter Optimization)çš„æ–°å‹è½¨è¿¹ç”ŸæˆèŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨æ¨¡å‹ä¼˜åŒ–ä¸­é¢ä¸´çš„é«˜æ˜‚æ—¶é—´å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚å—DeepSeek-V3çš„å¤šæ ‡è®°é¢„æµ‹(multi-token prediction)æ¶æ„å¯å‘ï¼ŒSPGé€šè¿‡åœ¨åŸºç¡€æ¨¡å‹ä¸­å¼•å…¥ä¸´æ—¶æ¨¡å—ï¼Œå®ç°äº†åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­ç”Ÿæˆå®Œæ•´çš„çŠ¶æ€-åŠ¨ä½œè½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨åŸå§‹æ•°æ®é›†ä¸Šä½¿ç”¨SPGé‡è®­çš„æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¼˜äºæ ‡å‡†çš„è¿ç§»å¾®è°ƒ(transfer fine-tuning)æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆåœ¨æ¶µç›–è®¡ç®—æœºè§†è§‰(ImageNet, COCO)ã€è‡ªç„¶è¯­è¨€å¤„ç†(GLUE, SQuAD)å’ŒéŸ³é¢‘(SUPERB)é¢†åŸŸçš„äº”å¤§ä¸»æµæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒéªŒè¯äº†å…¶å¼ºå¤§çš„å·¥ä¸šé€‚ç”¨æ€§ã€‚SPGåœ¨å¤šç§æ¨¡å‹ä¸Šç¨³å®šå®ç°äº†+0.2~7%çš„æ€§èƒ½å¢ç›Šï¼Œä¸”ä¿æŒäº†æä½çš„è®¡ç®—å¼€é”€ï¼Œç›®å‰å·²å¼€æºé¢„è®­ç»ƒæ¨¡å‹å’Œä»£ç ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2506.15051v1",
      "published_date": "2025-06-18 01:21:39 UTC",
      "updated_date": "2025-06-18 01:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:49:40.188519+00:00"
    },
    {
      "arxiv_id": "2506.15050v1",
      "title": "Truncated Proximal Policy Optimization",
      "title_zh": "æˆªæ–­å¼è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Tiantian Fan",
        "Lingjun Liu",
        "Yu Yue",
        "Jiaze Chen",
        "Chengyi Wang",
        "Qiying Yu",
        "Chi Zhang",
        "Zhiqi Lin",
        "Ruofei Zhu",
        "Yufeng Yuan",
        "Xiaochen Zuo",
        "Bole Ma",
        "Mofan Zhang",
        "Gaohong Liu",
        "Ru Zhang",
        "Haotian Zhou",
        "Cong Xie",
        "Ruidong Zhu",
        "Zhi Zhang",
        "Xin Liu",
        "Mingxuan Wang",
        "Lin Yan",
        "Yonghui Wu"
      ],
      "abstract": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (RL)è¿›è¡Œé•¿é“¾å¼æ€ç»´(CoT)æ¨ç†è®­ç»ƒæ—¶ï¼Œå› Proximal Policy Optimization (PPO) å›ºæœ‰çš„åŒæ­¥ç”Ÿæˆè¿‡ç¨‹å¯¼è‡´çš„ç¡¬ä»¶åˆ©ç”¨ç‡ä½å’Œè®­ç»ƒè€—æ—¶é•¿ç­‰é—®é¢˜ï¼Œæå‡ºäº†Truncated Proximal Policy Optimization (T-PPO) æ¡†æ¶ã€‚T-PPO é€šè¿‡å¼•å…¥Extended Generalized Advantage Estimation (EGAE) æŠ€æœ¯ï¼Œå®ç°äº†ä»ä¸å®Œæ•´çš„å“åº”ä¸­è¿›è¡Œä¼˜åŠ¿ä¼°è®¡ï¼ŒåŒæ—¶ä¿è¯äº†ç­–ç•¥å­¦ä¹ çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¾è®¡äº†ä¸€ç§è®¡ç®—ä¼˜åŒ–æœºåˆ¶ï¼Œé€šè¿‡ç‹¬ç«‹ä¼˜åŒ–ç­–ç•¥ä¸ä»·å€¼æ¨¡å‹ï¼Œå¹¶é€‰æ‹©æ€§è¿‡æ»¤æç¤ºè¯å’Œæˆªæ–­Tokenä»¥å‡å°‘å†—ä½™è®¡ç®—ï¼Œä»è€Œåœ¨ä¸æŸå¤±æ”¶æ•›æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚åœ¨AIME 2024åŸºå‡†æµ‹è¯•åŠ32BåŸºåº§æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒT-PPO å°†æ¨ç†å¤§æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡æå‡äº†é«˜è¾¾2.5å€ï¼Œä¸”æ€§èƒ½ä¼˜äºç°æœ‰çš„ç«äº‰æ–¹æ¡ˆã€‚è¯¥æˆæœä¸ºæå‡å¤æ‚æ¨ç†ä»»åŠ¡çš„è®­ç»ƒæ•ˆç‡æä¾›äº†æ›´é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15050v1",
      "published_date": "2025-06-18 01:21:38 UTC",
      "updated_date": "2025-06-18 01:21:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:48:58.691020+00:00"
    },
    {
      "arxiv_id": "2506.15047v1",
      "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers",
      "title_zh": "å°†ç…§é¡¾è€…éœ€æ±‚æ˜ å°„è‡³ AI èŠå¤©æœºå™¨äººè®¾è®¡ï¼šé˜¿å°”èŒ¨æµ·é»˜ç—…åŠå¤±æ™ºç—‡ç…§é¡¾è€…å¿ƒç†å¥åº·æ”¯æŒçš„ä¼˜åŠ¿ä¸å·®è·",
      "authors": [
        "Jiayue Melissa Shi",
        "Dong Whi Yoo",
        "Keran Wang",
        "Violeta J. Rodriguez",
        "Ravi Karkar",
        "Koustuv Saha"
      ],
      "abstract": "Family caregivers of individuals with Alzheimer's Disease and Related Dementia (AD/ADRD) face significant emotional and logistical challenges that place them at heightened risk for stress, anxiety, and depression. Although recent advances in generative AI -- particularly large language models (LLMs) -- offer new opportunities to support mental health, little is known about how caregivers perceive and engage with such technologies. To address this gap, we developed Carey, a GPT-4o-based chatbot designed to provide informational and emotional support to AD/ADRD caregivers. Using Carey as a technology probe, we conducted semi-structured interviews with 16 family caregivers following scenario-driven interactions grounded in common caregiving stressors. Through inductive coding and reflexive thematic analysis, we surface a systemic understanding of caregiver needs and expectations across six themes -- on-demand information access, emotional support, safe space for disclosure, crisis management, personalization, and data privacy. For each of these themes, we also identified the nuanced tensions in the caregivers' desires and concerns. We present a mapping of caregiver needs, AI chatbot's strengths, gaps, and design recommendations. Our findings offer theoretical and practical insights to inform the design of proactive, trustworthy, and caregiver-centered AI systems that better support the evolving mental health needs of AD/ADRD caregivers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…åŠç›¸å…³ç—´å‘†ç—‡ï¼ˆAD/ADRDï¼‰å®¶åº­ç…§é¡¾è€…é¢ä¸´çš„æ²‰é‡æƒ…ç»ªä¸ç‰©æµæŒ‘æˆ˜ï¼Œå¼€å‘äº†åŸºäº GPT-4o çš„èŠå¤©æœºå™¨äºº Carey ä»¥æä¾›ä¿¡æ¯å’Œæƒ…æ„Ÿæ”¯æŒã€‚ç ”ç©¶è€…å°† Carey ä½œä¸ºæŠ€æœ¯æ¢é’ˆï¼ˆtechnology probeï¼‰ï¼Œé€šè¿‡åœºæ™¯é©±åŠ¨çš„äº¤äº’å¯¹ 16 åå®¶åº­ç…§é¡¾è€…è¿›è¡Œäº†åŠç»“æ„åŒ–è®¿è°ˆï¼Œæ—¨åœ¨æ¢ç´¢ç…§é¡¾è€…å¯¹ç”Ÿæˆå¼ AI æŠ€æœ¯çš„æ„ŸçŸ¥ä¸å‚ä¸æ–¹å¼ã€‚é€šè¿‡åæ€æ€§ä¸»é¢˜åˆ†æï¼Œç ”ç©¶è¯†åˆ«äº†ç…§é¡¾è€…åœ¨æŒ‰éœ€ä¿¡æ¯è·å–ï¼ˆon-demand information accessï¼‰ã€æƒ…æ„Ÿæ”¯æŒï¼ˆemotional supportï¼‰ã€å®‰å…¨æŠ«éœ²ç©ºé—´ã€å±æœºç®¡ç†ï¼ˆcrisis managementï¼‰ã€ä¸ªæ€§åŒ–ï¼ˆpersonalizationï¼‰åŠæ•°æ®éšç§ï¼ˆdata privacyï¼‰å…­å¤§ç»´åº¦çš„éœ€æ±‚ä¸æœŸå¾…ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†ç…§é¡¾è€…åœ¨éœ€æ±‚ä¸æ‹…å¿§ä¹‹é—´çš„å¾®å¦™å†²çªï¼Œå¹¶ç³»ç»Ÿåœ°æ˜ å°„äº†ç…§é¡¾è€…éœ€æ±‚ä¸ AI èŠå¤©æœºå™¨äººçš„ä¼˜åŠ¿ä¸å·®è·ï¼ˆstrengths and gapsï¼‰ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡æ›´å…·å‰ç»æ€§ã€å¯ä¿¡èµ–ä¸”ä»¥ç…§é¡¾è€…ä¸ºä¸­å¿ƒï¼ˆcaregiver-centeredï¼‰çš„ AI ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºè§è§£ä¸å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.15047v1",
      "published_date": "2025-06-18 01:16:09 UTC",
      "updated_date": "2025-06-18 01:16:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:49:55.383488+00:00"
    },
    {
      "arxiv_id": "2506.22461v1",
      "title": "Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation",
      "title_zh": "æœºå™¨å­¦ä¹ åŠ©åŠ›ä¸»åŠ¨å¼åœ°ä¸‹æ°´ç®¡ç†ï¼šé¢„è­¦ä¸èµ„æºé…ç½®",
      "authors": [
        "Chuan Li",
        "Ruoxuan Yang"
      ],
      "abstract": "Groundwater supports ecosystems, agriculture, and drinking water supplies worldwide, yet effective monitoring remains challenging due to sparse data, computational constraints, and delayed outputs from traditional approaches. We develop a machine learning pipeline that predicts groundwater level categories using climate data, hydro-meteorological records, and physiographic attributes processed through AutoGluon's automated ensemble framework. Our approach integrates geospatial preprocessing, domain-driven feature engineering, and automated model selection to overcome conventional monitoring limitations. Applied to a large-scale French dataset (n $>$ 3,440,000 observations from 1,500+ wells), the model achieves weighted F\\_1 scores of 0.927 on validation data and 0.67 on temporally distinct test data. Scenario-based evaluations demonstrate practical utility for early warning systems and water allocation decisions under changing climate conditions. The open-source implementation provides a scalable framework for integrating machine learning into national groundwater monitoring networks, enabling more responsive and data-driven water management strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§æœºå™¨å­¦ä¹ (Machine Learning)æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡ä¸»åŠ¨å¼åœ°ä¸‹æ°´ç®¡ç†è§£å†³ä¼ ç»Ÿç›‘æµ‹æ–¹æ³•ä¸­æ•°æ®ç¨€ç–å’Œè®¡ç®—å—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ–¹æ¡ˆåˆ©ç”¨AutoGluonçš„è‡ªåŠ¨åŒ–é›†æˆæ¡†æ¶(Automated Ensemble Framework)ï¼Œæ•´åˆäº†æ°”å€™æ•°æ®ã€æ°´æ–‡æ°”è±¡è®°å½•åŠåœ°è²Œå±æ€§ï¼Œå¹¶ç»“åˆåœ°ç†ç©ºé—´é¢„å¤„ç†å’Œé¢†åŸŸé©±åŠ¨çš„ç‰¹å¾å·¥ç¨‹(Feature Engineering)è¿›è¡Œåœ°ä¸‹æ°´ä½ç±»åˆ«çš„åˆ†ç±»é¢„æµ‹ã€‚é€šè¿‡å¯¹åŒ…å«è¶…è¿‡344ä¸‡æ¡è§‚æµ‹è®°å½•çš„å¤§è§„æ¨¡æ³•å›½æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œè¯¥æ¨¡å‹åœ¨éªŒè¯é›†å’Œæ—¶é—´ç‹¬ç«‹æµ‹è¯•é›†ä¸Šåˆ†åˆ«å–å¾—äº†0.927å’Œ0.67çš„åŠ æƒF1åˆ†æ•°(Weighted F1 Scores)ã€‚åŸºäºæƒ…æ™¯çš„è¯„ä¼°è¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨æ°”å€™å˜åŒ–èƒŒæ™¯ä¸‹å¯¹äºé¢„è­¦ç³»ç»Ÿ(Early Warning Systems)å’Œæ°´èµ„æºåˆ†é…å†³ç­–çš„å®ç”¨ä»·å€¼ã€‚è¯¥å¼€æºå®ç°ä¸ºå›½å®¶çº§åœ°ä¸‹æ°´ç›‘æµ‹ç½‘ç»œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæ”¯æŒæ›´å…·å“åº”æ€§å’Œæ•°æ®é©±åŠ¨çš„æ°´èµ„æºç®¡ç†ç­–ç•¥ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2506.22461v1",
      "published_date": "2025-06-18 00:41:04 UTC",
      "updated_date": "2025-06-18 00:41:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:49:57.635340+00:00"
    },
    {
      "arxiv_id": "2507.00032v2",
      "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Exercise Recommendation",
      "title_zh": "Ken åˆ©ç”¨å±‚ï¼šåŸºäºå­¦ç”Ÿè®¤çŸ¥è§†é‡å†…èµ«å¸ƒå›æ”¾çš„è‡ªé€‚åº”ä¹ é¢˜æ¨è",
      "authors": [
        "Grey Kuling",
        "Marinka Zitnik"
      ],
      "abstract": "Adaptive exercise recommendation (ER) aims to choose the next activity that matches a learner's evolving Zone of Proximal Development (ZPD). We present KUL-Rec, a biologically inspired ER system that couples a fast Hebbian memory with slow replay-based consolidation to enable continual, few-shot personalization from sparse interactions. The model operates in an embedding space, allowing a single architecture to handle both tabular knowledge-tracing logs and open-ended short-answer text. We align evaluation with tutoring needs using bidirectional ranking and rank-sensitive metrics (nDCG, Recall@K). Across ten public datasets, KUL-Rec improves macro nDCG (0.316 vs. 0.265 for the strongest baseline) and Recall@10 (0.305 vs. 0.211), while achieving low inference latency and an $\\approx99$\\% reduction in peak GPU memory relative to a competitive graph-based model. In a 13-week graduate course, KUL-Rec personalized weekly short-answer quizzes generated by a retrieval-augmented pipeline and the personalized quizzes were associated with lower perceived difficulty and higher helpfulness (p < .05). An embedding robustness audit highlights that encoder choice affects semantic alignment, motivating routine audits when deploying open-response assessment. Together, these results indicate that Hebbian replay with bounded consolidation offers a practical path to real-time, interpretable ER that scales across data modalities and classroom settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KUL-Recï¼Œè¿™æ˜¯ä¸€ç§å—ç”Ÿç‰©å¯å‘çš„è‡ªé€‚åº”ç»ƒä¹ æ¨è(Exercise Recommendation)ç³»ç»Ÿï¼Œæ—¨åœ¨ç²¾å‡†åŒ¹é…å­¦ä¹ è€…çš„åŠ¨æ€è¿‘ä¾§å‘å±•åŒºé—´(Zone of Proximal Development)ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ç»“åˆäº†å¿«é€Ÿçš„èµ«å¸ƒè®°å¿†(Hebbian memory)ä¸åŸºäºæ…¢é€Ÿé‡æ”¾(replay-based)çš„å·©å›ºæœºåˆ¶ï¼Œå®ç°äº†åœ¨ç¨€ç–äº¤äº’æ•°æ®ä¸‹çš„æŒç»­ã€å°‘æ ·æœ¬ä¸ªæ€§åŒ–æ¨èã€‚KUL-Rec åœ¨åµŒå…¥ç©ºé—´(embedding space)ä¸­è¿è¡Œï¼Œå…¶ç»Ÿä¸€æ¶æ„èƒ½å¤ŸåŒæ—¶å¤„ç†ä¼ ç»Ÿçš„ç»“æ„åŒ–çŸ¥è¯†è¿½è¸ªæ—¥å¿—å’Œå¼€æ”¾å¼ç®€ç­”æ–‡æœ¬ã€‚åœ¨ 10 ä¸ªå…¬å…±æ•°æ®é›†çš„æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨ nDCG å’Œ Recall@10 æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶ç›¸æ¯”ç«äº‰æ€§å›¾æ¨¡å‹å‡å°‘äº†çº¦ 99% çš„å³°å€¼ GPU æ˜¾å­˜å ç”¨ã€‚åœ¨ä¸ºæœŸ 13 å‘¨çš„ç ”ç©¶ç”Ÿè¯¾ç¨‹å®è·µä¸­ï¼Œè¯¥ç³»ç»Ÿç”Ÿæˆçš„ä¸ªæ€§åŒ–ç®€ç­”æµ‹éªŒæ˜¾è‘—é™ä½äº†å­¦ç”Ÿçš„æ„ŸçŸ¥éš¾åº¦å¹¶æé«˜äº†å­¦ä¹ å¸®åŠ©åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åµŒå…¥é²æ£’æ€§å®¡è®¡æ¢è®¨äº†ç¼–ç å™¨é€‰æ‹©å¯¹è¯­ä¹‰å¯¹é½çš„å½±å“ã€‚è¯¥æˆæœè¯æ˜äº†å¸¦è¾¹ç•Œå·©å›ºçš„èµ«å¸ƒé‡æ”¾æœºåˆ¶æ˜¯å®ç°è·¨æ¨¡æ€ã€å®æ—¶ä¸”å¯è§£é‡Šçš„è‡ªé€‚åº”æ•™å­¦æ¨èçš„æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2507.00032v2",
      "published_date": "2025-06-18 00:06:28 UTC",
      "updated_date": "2025-11-17 15:10:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T22:50:06.381311+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 113,
  "processed_papers_count": 113,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T22:50:47.589257+00:00"
}
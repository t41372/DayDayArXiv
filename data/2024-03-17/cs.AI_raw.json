[
  {
    "arxiv_id": "2403.11368v1",
    "title": "Driving Style Alignment for LLM-powered Driver Agent",
    "authors": [
      "Ruoxuan Yang",
      "Xinyue Zhang",
      "Anais Fernandez-Laaksonen",
      "Xin Ding",
      "Jiangtao Gong"
    ],
    "abstract": "Recently, LLM-powered driver agents have demonstrated considerable potential\nin the field of autonomous driving, showcasing human-like reasoning and\ndecision-making abilities.However, current research on aligning driver agent\nbehaviors with human driving styles remains limited, partly due to the scarcity\nof high-quality natural language data from human driving behaviors.To address\nthis research gap, we propose a multi-alignment framework designed to align\ndriver agents with human driving styles through demonstrations and feedback.\nNotably, we construct a natural language dataset of human driver behaviors\nthrough naturalistic driving experiments and post-driving interviews, offering\nhigh-quality human demonstrations for LLM alignment. The framework's\neffectiveness is validated through simulation experiments in the CARLA urban\ntraffic simulator and further corroborated by human evaluations. Our research\noffers valuable insights into designing driving agents with diverse driving\nstyles.The implementation of the framework and details of the dataset can be\nfound at the link.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T42"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11368v1",
    "published_date": "2024-03-17 23:07:13 UTC",
    "updated_date": "2024-03-17 23:07:13 UTC"
  },
  {
    "arxiv_id": "2403.11363v1",
    "title": "IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight",
    "authors": [
      "Theodor Stoecker",
      "Nico Hambauer",
      "Patrick Zschech",
      "Mathias Kraus"
    ],
    "abstract": "Feature selection is a critical component in predictive analytics that\nsignificantly affects the prediction accuracy and interpretability of models.\nIntrinsic methods for feature selection are built directly into model learning,\nproviding a fast and attractive option for large amounts of data. Machine\nlearning algorithms, such as penalized regression models (e.g., lasso) are the\nmost common choice when it comes to in-built feature selection. However, they\nfail to capture non-linear relationships, which ultimately affects their\nability to predict outcomes in intricate datasets. In this paper, we propose\nIGANN Sparse, a novel machine learning model from the family of generalized\nadditive models, which promotes sparsity through a non-linear feature selection\nprocess during training. This ensures interpretability through improved model\nsparsity without sacrificing predictive performance. Moreover, IGANN Sparse\nserves as an exploratory tool for information systems researchers to unveil\nimportant non-linear relationships in domains that are characterized by complex\npatterns. Our ongoing research is directed at a thorough evaluation of the\nIGANN Sparse model, including user studies that allow to assess how well users\nof the model can benefit from the reduced number of features. This will allow\nfor a deeper understanding of the interactions between linear vs. non-linear\nmodeling, number of selected features, and predictive performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint conditionally accepted for archival and presentation at the\n  32nd European Conference on Information Systems (ECIS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2403.11363v1",
    "published_date": "2024-03-17 22:44:36 UTC",
    "updated_date": "2024-03-17 22:44:36 UTC"
  },
  {
    "arxiv_id": "2403.15444v1",
    "title": "A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition",
    "authors": [
      "Abhi Kamboj",
      "Minh Do"
    ],
    "abstract": "Despite living in a multi-sensory world, most AI models are limited to\ntextual and visual understanding of human motion and behavior. In fact, full\nsituational awareness of human motion could best be understood through a\ncombination of sensors. In this survey we investigate how knowledge can be\ntransferred and utilized amongst modalities for Human Activity/Action\nRecognition (HAR), i.e. cross-modality transfer learning. We motivate the\nimportance and potential of IMU data and its applicability in cross-modality\nlearning as well as the importance of studying the HAR problem. We categorize\nHAR related tasks by time and abstractness and then compare various types of\nmultimodal HAR datasets. We also distinguish and expound on many related but\ninconsistently used terms in the literature, such as transfer learning, domain\nadaptation, representation learning, sensor fusion, and multimodal learning,\nand describe how cross-modal learning fits with all these concepts. We then\nreview the literature in IMU-based cross-modal transfer for HAR. The two main\napproaches for cross-modal transfer are instance-based transfer, where\ninstances of one modality are mapped to another (e.g. knowledge is transferred\nin the input space), or feature-based transfer, where the model relates the\nmodalities in an intermediate latent space (e.g. knowledge is transferred in\nthe feature space). Finally, we discuss future research directions and\napplications in cross-modal HAR.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15444v1",
    "published_date": "2024-03-17 22:31:14 UTC",
    "updated_date": "2024-03-17 22:31:14 UTC"
  },
  {
    "arxiv_id": "2403.12107v1",
    "title": "Scenarios for the Transition to AGI",
    "authors": [
      "Anton Korinek",
      "Donghyun Suh"
    ],
    "abstract": "We analyze how output and wages behave under different scenarios for\ntechnological progress that may culminate in Artificial General Intelligence\n(AGI), defined as the ability of AI systems to perform all tasks that humans\ncan perform. We assume that human work can be decomposed into atomistic tasks\nthat differ in their complexity. Advances in technology make ever more complex\ntasks amenable to automation. The effects on wages depend on a race between\nautomation and capital accumulation. If the distribution of task complexity\nexhibits a sufficiently thick infinite tail, then there is always enough work\nfor humans, and wages may rise forever. By contrast, if the complexity of tasks\nthat humans can perform is bounded and full automation is reached, then wages\ncollapse. But declines may occur even before if large-scale automation outpaces\ncapital accumulation and makes labor too abundant. Automating productivity\ngrowth may lead to broad-based gains in the returns to all factors. By\ncontrast, bottlenecks to growth from irreproducible scarce factors may\nexacerbate the decline in wages.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12107v1",
    "published_date": "2024-03-17 22:22:28 UTC",
    "updated_date": "2024-03-17 22:22:28 UTC"
  },
  {
    "arxiv_id": "2403.11353v4",
    "title": "TransPeakNet: Solvent-Aware 2D NMR Prediction via Multi-Task Pre-Training and Unsupervised Learning",
    "authors": [
      "Yunrui Li",
      "Hao Xu",
      "Ambrish Kumar",
      "Duosheng Wang",
      "Christian Heiss",
      "Parastoo Azadi",
      "Pengyu Hong"
    ],
    "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is essential for revealing\nmolecular structure, electronic environment, and dynamics. Accurate NMR shift\nprediction allows researchers to validate structures by comparing predicted and\nobserved shifts. While Machine Learning (ML) has improved one-dimensional (1D)\nNMR shift prediction, predicting 2D NMR remains challenging due to limited\nannotated data. To address this, we introduce an unsupervised training\nframework for predicting cross-peaks in 2D NMR, specifically Heteronuclear\nSingle Quantum Coherence (HSQC).Our approach pretrains an ML model on an\nannotated 1D dataset of 1H and 13C shifts, then finetunes it in an unsupervised\nmanner using unlabeled HSQC data, which simultaneously generates cross-peak\nannotations. Our model also adjusts for solvent effects. Evaluation on 479\nexpert-annotated HSQC spectra demonstrates our model's superiority over\ntraditional methods (ChemDraw and Mestrenova), achieving Mean Absolute Errors\n(MAEs) of 2.05 ppm and 0.165 ppm for 13C shifts and 1H shifts respectively. Our\nalgorithmic annotations show a 95.21% concordance with experts' assignments,\nunderscoring the approach's potential for structural elucidation in fields like\norganic chemistry, pharmaceuticals, and natural products.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11353v4",
    "published_date": "2024-03-17 21:52:51 UTC",
    "updated_date": "2024-12-16 00:31:21 UTC"
  },
  {
    "arxiv_id": "2403.11348v1",
    "title": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits",
    "authors": [
      "Mintong Kang",
      "Nezihe Merve Gürel",
      "Linyi Li",
      "Bo Li"
    ],
    "abstract": "Conformal prediction has shown spurring performance in constructing\nstatistically rigorous prediction sets for arbitrary black-box machine learning\nmodels, assuming the data is exchangeable. However, even small adversarial\nperturbations during the inference can violate the exchangeability assumption,\nchallenge the coverage guarantees, and result in a subsequent decline in\nempirical coverage. In this work, we propose a certifiably robust\nlearning-reasoning conformal prediction framework (COLEP) via probabilistic\ncircuits, which comprise a data-driven learning component that trains\nstatistical models to learn different semantic concepts, and a reasoning\ncomponent that encodes knowledge and characterizes the relationships among the\ntrained models for logic reasoning. To achieve exact and efficient reasoning,\nwe employ probabilistic circuits (PCs) within the reasoning component.\nTheoretically, we provide end-to-end certification of prediction coverage for\nCOLEP in the presence of bounded adversarial perturbations. We also provide\ncertified coverage considering the finite size of the calibration set.\nFurthermore, we prove that COLEP achieves higher prediction coverage and\naccuracy over a single model as long as the utilities of knowledge models are\nnon-trivial. Empirically, we show the validity and tightness of our certified\ncoverage, demonstrating the robust conformal prediction of COLEP on various\ndatasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to\n12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on\nAwA2.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11348v1",
    "published_date": "2024-03-17 21:23:45 UTC",
    "updated_date": "2024-03-17 21:23:45 UTC"
  },
  {
    "arxiv_id": "2403.11346v3",
    "title": "CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using Synthetic Back-Translation Data",
    "authors": [
      "Kung Yin Hong",
      "Lifeng Han",
      "Riza Batista-Navarro",
      "Goran Nenadic"
    ],
    "abstract": "Neural Machine Translation (NMT) for low-resource languages is still a\nchallenging task in front of NLP researchers. In this work, we deploy a\nstandard data augmentation methodology by back-translation to a new language\ntranslation direction Cantonese-to-English. We present the models we fine-tuned\nusing the limited amount of real data and the synthetic data we generated using\nback-translation including OpusMT, NLLB, and mBART. We carried out automatic\nevaluation using a range of different metrics including lexical-based and\nembedding-based. Furthermore. we create a user-friendly interface for the\nmodels we included in this\\textsc{ CantonMT} research project and make it\navailable to facilitate Cantonese-to-English MT research. Researchers can add\nmore models into this platform via our open-source\\textsc{ CantonMT} toolkit\n\\url{https://github.com/kenrickkung/CantoneseTranslation}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by: The 25th Annual Conference of The European Association\n  for Machine Translation, 24 - 27 June 2024, Sheffield, UK (forthcoming)",
    "pdf_url": "http://arxiv.org/pdf/2403.11346v3",
    "published_date": "2024-03-17 21:16:17 UTC",
    "updated_date": "2024-06-09 22:10:04 UTC"
  },
  {
    "arxiv_id": "2403.11345v2",
    "title": "Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective",
    "authors": [
      "Muhammad Aneeq uz Zaman",
      "Alec Koppel",
      "Mathieu Laurière",
      "Tamer Başar"
    ],
    "abstract": "We address in this paper Reinforcement Learning (RL) among agents that are\ngrouped into teams such that there is cooperation within each team but\ngeneral-sum (non-zero sum) competition across different teams. To develop an RL\nmethod that provably achieves a Nash equilibrium, we focus on a\nlinear-quadratic structure. Moreover, to tackle the non-stationarity induced by\nmulti-agent interactions in the finite population setting, we consider the case\nwhere the number of agents within each team is infinite, i.e., the mean-field\nsetting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTG). We\ncharacterize the Nash equilibrium (NE) of the GS-MFTG, under a standard\ninvertibility condition. This MFTG NE is then shown to be $O(1/M)$-NE for the\nfinite population game where $M$ is a lower bound on the number of agents in\neach team. These structural results motivate an algorithm called Multi-player\nReceding-horizon Natural Policy Gradient (MRNPG), where each team minimizes its\ncumulative cost \\emph{independently} in a receding-horizon manner. Despite the\nnon-convexity of the problem, we establish that the resulting algorithm\nconverges to a global NE through a novel problem decomposition into\nsub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs\n(HJI) equations, in which \\emph{independent natural policy gradient} is shown\nto exhibit linear convergence under time-independent diagonal dominance.\nNumerical studies included corroborate the theoretical results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11345v2",
    "published_date": "2024-03-17 21:11:55 UTC",
    "updated_date": "2025-02-08 17:59:43 UTC"
  },
  {
    "arxiv_id": "2403.11337v1",
    "title": "Enhancing Bandwidth Efficiency for Video Motion Transfer Applications using Deep Learning Based Keypoint Prediction",
    "authors": [
      "Xue Bai",
      "Tasmiah Haque",
      "Sumit Mohan",
      "Yuliang Cai",
      "Byungheon Jeong",
      "Adam Halasz",
      "Srinjoy Das"
    ],
    "abstract": "We propose a deep learning based novel prediction framework for enhanced\nbandwidth reduction in motion transfer enabled video applications such as video\nconferencing, virtual reality gaming and privacy preservation for patient\nhealth monitoring. To model complex motion, we use the First Order Motion Model\n(FOMM) that represents dynamic objects using learned keypoints along with their\nlocal affine transformations. Keypoints are extracted by a self-supervised\nkeypoint detector and organized in a time series corresponding to the video\nframes. Prediction of keypoints, to enable transmission using lower frames per\nsecond on the source device, is performed using a Variational Recurrent Neural\nNetwork (VRNN). The predicted keypoints are then synthesized to video frames\nusing an optical flow estimator and a generator network. This efficacy of\nleveraging keypoint based representations in conjunction with VRNN based\nprediction for both video animation and reconstruction is demonstrated on three\ndiverse datasets. For real-time applications, our results show the\neffectiveness of our proposed architecture by enabling up to 2x additional\nbandwidth reduction over existing keypoint based video motion transfer\nframeworks without significantly compromising video quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11337v1",
    "published_date": "2024-03-17 20:36:43 UTC",
    "updated_date": "2024-03-17 20:36:43 UTC"
  },
  {
    "arxiv_id": "2403.11330v2",
    "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
    "authors": [
      "Dong Won Lee",
      "Hae Won Park",
      "Yoon Kim",
      "Cynthia Breazeal",
      "Louis-Philippe Morency"
    ],
    "abstract": "We describe an approach for aligning an LLM-based dialogue agent based on\nglobal (i.e., dialogue-level) rewards, while also taking into account\nnaturally-occurring multimodal signals. At a high level, our approach (dubbed\nGELI) learns a local, turn-level reward model by decomposing the human-provided\nGlobal Explicit (GE) session-level reward, using Local Implicit (LI) multimodal\nreward signals to crossmodally shape the reward decomposition step. This\ndecomposed reward model is then used as part of the standard RHLF pipeline\nimprove an LLM-based dialog agent. We run quantitative and qualitative human\nstudies to evaluate the performance of our GELI approach, and find that it\nshows consistent improvements across various conversational metrics compared to\nbaseline methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.11330v2",
    "published_date": "2024-03-17 20:21:26 UTC",
    "updated_date": "2024-04-23 03:17:07 UTC"
  },
  {
    "arxiv_id": "2403.11328v1",
    "title": "Domain-Guided Masked Autoencoders for Unique Player Identification",
    "authors": [
      "Bavesh Balaji",
      "Jerrin Bright",
      "Sirisha Rambhatla",
      "Yuhao Chen",
      "Alexander Wong",
      "John Zelek",
      "David A Clausi"
    ],
    "abstract": "Unique player identification is a fundamental module in vision-driven sports\nanalytics. Identifying players from broadcast videos can aid with various\ndownstream tasks such as player assessment, in-game analysis, and broadcast\nproduction. However, automatic detection of jersey numbers using deep features\nis challenging primarily due to: a) motion blur, b) low resolution video feed,\nand c) occlusions. With their recent success in various vision tasks, masked\nautoencoders (MAEs) have emerged as a superior alternative to conventional\nfeature extractors. However, most MAEs simply zero-out image patches either\nrandomly or focus on where to mask rather than how to mask. Motivated by human\nvision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to\nfacilitate robust feature extraction in the presence of motion blur for player\nidentification. We further introduce a new spatio-temporal network leveraging\nour novel d-MAE for unique player identification. We conduct experiments on\nthree large-scale sports datasets, including a curated baseball dataset, the\nSoccerNet dataset, and an in-house ice hockey dataset. We preprocess the\ndatasets using an upgraded keyframe identification (KfID) module by focusing on\nframes containing jersey numbers. Additionally, we propose a keyframe-fusion\ntechnique to augment keyframes, preserving spatial and temporal context. Our\nspatio-temporal network showcases significant improvements, surpassing the\ncurrent state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies,\nrespectively. Rigorous ablations highlight the effectiveness of our\ndomain-guided masking approach and the refined KfID module, resulting in\nperformance enhancements of 1.48% and 1.84% respectively, compared to original\narchitectures.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Submitted to 21st International Conference on Robots and Vision\n  (CRV'24), Guelph, Ontario, Canada",
    "pdf_url": "http://arxiv.org/pdf/2403.11328v1",
    "published_date": "2024-03-17 20:14:57 UTC",
    "updated_date": "2024-03-17 20:14:57 UTC"
  },
  {
    "arxiv_id": "2403.11322v5",
    "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
    "authors": [
      "Yiran Wu",
      "Tianwei Yue",
      "Shaokun Zhang",
      "Chi Wang",
      "Qingyun Wu"
    ],
    "abstract": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11322v5",
    "published_date": "2024-03-17 19:54:16 UTC",
    "updated_date": "2024-09-14 21:55:08 UTC"
  },
  {
    "arxiv_id": "2403.11304v1",
    "title": "Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving",
    "authors": [
      "Steffen Hagedorn",
      "Marcel Milich",
      "Alexandru P. Condurache"
    ],
    "abstract": "Planning the trajectory of the controlled ego vehicle is a key challenge in\nautomated driving. As for human drivers, predicting the motions of surrounding\nvehicles is important to plan the own actions. Recent motion prediction methods\nutilize equivariant neural networks to exploit geometric symmetries in the\nscene. However, no existing method combines motion prediction and trajectory\nplanning in a joint step while guaranteeing equivariance under\nroto-translations of the input space. We address this gap by proposing a\nlightweight equivariant planning model that generates multi-modal joint\npredictions for all vehicles and selects one mode as the ego plan. The\nequivariant network design improves sample efficiency, guarantees output\nstability, and reduces model parameters. We further propose equivariant route\nattraction to guide the ego vehicle along a high-level route provided by an\noff-the-shelf GPS navigation system. This module creates a momentum from\nembedded vehicle positions toward the route in latent space while keeping the\nequivariance property. Route attraction enables goal-oriented behavior without\nforcing the vehicle to stick to the exact route. We conduct experiments on the\nchallenging nuScenes dataset to investigate the capability of our planner. The\nresults show that the planned trajectory is stable under roto-translations of\nthe input scene which demonstrates the equivariance of our model. Despite using\nonly a small split of the dataset for training, our method improves L2 distance\nat 3 s by 20.6 % and surpasses the state of the art.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11304v1",
    "published_date": "2024-03-17 18:53:46 UTC",
    "updated_date": "2024-03-17 18:53:46 UTC"
  },
  {
    "arxiv_id": "2403.11299v2",
    "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
    "authors": [
      "Guohao Sun",
      "Can Qin",
      "Jiamian Wang",
      "Zeyuan Chen",
      "Ran Xu",
      "Zhiqiang Tao"
    ],
    "abstract": "Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11299v2",
    "published_date": "2024-03-17 18:42:38 UTC",
    "updated_date": "2024-07-15 17:37:11 UTC"
  },
  {
    "arxiv_id": "2405.15774v1",
    "title": "Comprehensive Autonomous Vehicle Optimal Routing With Dynamic Heuristics",
    "authors": [
      "Ragav V",
      "Jesher Joshua M",
      "Syed Ibrahim S P"
    ],
    "abstract": "Auto manufacturers and research groups are working on autonomous driving for\nlong period and achieved significant progress. Autonomous vehicles (AV) are\nexpected to transform road traffic reduction from current conditions, avoiding\naccidents and congestion. As the implementation of an autonomous vehicle\necosystem includes complex automotive technology, ethics, passenger behaviour,\ntraffic management policies and liability etc., the maturity of AV solutions\nare still evolving. The proposed model to improve AV user experience, uses a\nhybrid AV Network of multiple connected autonomous vehicles which communicate\nwith each other in an environment shared by human driven vehicles. The proposed\nOptimal AV Network (OAVN) solution provides better coordination and\noptimization of autonomous vehicles, improved Transportation efficiency,\nimproved passenger comfort and safety, real-time dynamic adaption of traffic &\nroad conditions along with improved in-cabin assistance with inputs from\nvarious sensors. The true optimal solution for this problem, is to devise an\nautomated guidance system for vehicles in an AV network, to reach destinations\nin best possible routes along with passenger comfort and safety. A custom\ninformed search model is proposed along with other heuristic goals for better\nuser experience. The results are analysed and compared to evaluate the\neffectiveness of the solution and identify gaps and future enhancements.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.15774v1",
    "published_date": "2024-03-17 18:21:56 UTC",
    "updated_date": "2024-03-17 18:21:56 UTC"
  },
  {
    "arxiv_id": "2403.11292v1",
    "title": "Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction",
    "authors": [
      "Asma Sattar",
      "Georgios Deligiorgis",
      "Marco Trincavelli",
      "Davide Bacciu"
    ],
    "abstract": "Dynamic multi-relational graphs are an expressive relational representation\nfor data enclosing entities and relations of different types, and where\nrelationships are allowed to vary in time. Addressing predictive tasks over\nsuch data requires the ability to find structure embeddings that capture the\ndiversity of the relationships involved, as well as their dynamic evolution. In\nthis work, we establish a novel class of challenging tasks for dynamic\nmulti-relational graphs involving out-of-domain link prediction, where the\nrelationship being predicted is not available in the input graph. We then\nintroduce a novel Graph Neural Network model, named GOOD, designed specifically\nto tackle the out-of-domain generalization problem. GOOD introduces a novel\ndesign concept for multi-relation embedding aggregation, based on the idea that\ngood representations are such when it is possible to disentangle the mixing\nproportions of the different relational embeddings that have produced it. We\nalso propose five benchmarks based on two retail domains, where we show that\nGOOD can effectively generalize predictions out of known relationship types and\nachieve state-of-the-art results. Most importantly, we provide insights into\nproblems where out-of-domain prediction might be preferred to an in-domain\nformulation, that is, where the relationship to be predicted has very few\npositive examples.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 3 figures, 3 Tables, conference [accepted in IEEE WCCI 2024]",
    "pdf_url": "http://arxiv.org/pdf/2403.11292v1",
    "published_date": "2024-03-17 18:08:22 UTC",
    "updated_date": "2024-03-17 18:08:22 UTC"
  },
  {
    "arxiv_id": "2403.13843v2",
    "title": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A Review",
    "authors": [
      "Yassine Habchi",
      "Hamza Kheddar",
      "Yassine Himeur",
      "Mohamed Chahine Ghanem"
    ],
    "abstract": "The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13843v2",
    "published_date": "2024-03-17 17:45:04 UTC",
    "updated_date": "2025-04-14 15:10:31 UTC"
  },
  {
    "arxiv_id": "2404.04264v5",
    "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs",
    "authors": [
      "Lihui Liu",
      "Zihao Wang",
      "Ruizhong Qiu",
      "Yikun Ban",
      "Eunice Chan",
      "Yangqiu Song",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "abstract": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.04264v5",
    "published_date": "2024-03-17 17:01:45 UTC",
    "updated_date": "2024-12-12 23:17:01 UTC"
  },
  {
    "arxiv_id": "2403.11265v2",
    "title": "Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation",
    "authors": [
      "Silvia Corbara",
      "Alejandro Moreo"
    ],
    "abstract": "Authorship Verification (AV) is a text classification task concerned with\ninferring whether a candidate text has been written by one specific author or\nby someone else. It has been shown that many AV systems are vulnerable to\nadversarial attacks, where a malicious author actively tries to fool the\nclassifier by either concealing their writing style, or by imitating the style\nof another author. In this paper, we investigate the potential benefits of\naugmenting the classifier training set with (negative) synthetic examples.\nThese synthetic examples are generated to imitate the style of the author of\ninterest. We analyze the improvements in classifier prediction that this\naugmentation brings to bear in the task of AV in an adversarial setting. In\nparticular, we experiment with three different generator architectures (one\nbased on Recurrent Neural Networks, another based on small-scale transformers,\nand another based on the popular GPT model) and with two training strategies\n(one inspired by standard Language Models, and another inspired by Wasserstein\nGenerative Adversarial Networks). We evaluate our hypothesis on five datasets\n(three of which have been specifically collected to represent an adversarial\nsetting) and using two learning algorithms for the AV classifier (Support\nVector Machines and Convolutional Neural Networks). This experimentation has\nyielded negative results, revealing that, although our methodology proves\neffective in many adversarial settings, its benefits are too sporadic for a\npragmatical application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11265v2",
    "published_date": "2024-03-17 16:36:26 UTC",
    "updated_date": "2024-10-29 13:01:18 UTC"
  },
  {
    "arxiv_id": "2403.11262v1",
    "title": "Understanding Diffusion Models by Feynman's Path Integral",
    "authors": [
      "Yuji Hirono",
      "Akinori Tanaka",
      "Kenji Fukushima"
    ],
    "abstract": "Score-based diffusion models have proven effective in image generation and\nhave gained widespread usage; however, the underlying factors contributing to\nthe performance disparity between stochastic and deterministic (i.e., the\nprobability flow ODEs) sampling schemes remain unclear. We introduce a novel\nformulation of diffusion models using Feynman's path integral, which is a\nformulation originally developed for quantum physics. We find this formulation\nproviding comprehensive descriptions of score-based generative models, and\ndemonstrate the derivation of backward stochastic differential equations and\nloss functions.The formulation accommodates an interpolating parameter\nconnecting stochastic and deterministic sampling schemes, and we identify this\nparameter as a counterpart of Planck's constant in quantum physics. This\nanalogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a\nwell-established technique in quantum physics, for evaluating the negative\nlog-likelihood to assess the performance disparity between stochastic and\ndeterministic sampling schemes.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "hep-th"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.11262v1",
    "published_date": "2024-03-17 16:24:29 UTC",
    "updated_date": "2024-03-17 16:24:29 UTC"
  },
  {
    "arxiv_id": "2403.11261v1",
    "title": "A Lie Group Approach to Riemannian Batch Normalization",
    "authors": [
      "Ziheng Chen",
      "Yue Song",
      "Yunmei Liu",
      "Nicu Sebe"
    ],
    "abstract": "Manifold-valued measurements exist in numerous applications within computer\nvision and machine learning. Recent studies have extended Deep Neural Networks\n(DNNs) to manifolds, and concomitantly, normalization techniques have also been\nadapted to several manifolds, referred to as Riemannian normalization.\nNonetheless, most of the existing Riemannian normalization methods have been\nderived in an ad hoc manner and only apply to specific manifolds. This paper\nestablishes a unified framework for Riemannian Batch Normalization (RBN)\ntechniques on Lie groups. Our framework offers the theoretical guarantee of\ncontrolling both the Riemannian mean and variance. Empirically, we focus on\nSymmetric Positive Definite (SPD) manifolds, which possess three distinct types\nof Lie group structures. Using the deformation concept, we generalize the\nexisting Lie groups on SPD manifolds into three families of parameterized Lie\ngroups. Specific normalization layers induced by these Lie groups are then\nproposed for SPD neural networks. We demonstrate the effectiveness of our\napproach through three sets of experiments: radar recognition, human action\nrecognition, and electroencephalography (EEG) classification. The code is\navailable at https://github.com/GitZH-Chen/LieBN.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MS"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11261v1",
    "published_date": "2024-03-17 16:24:07 UTC",
    "updated_date": "2024-03-17 16:24:07 UTC"
  },
  {
    "arxiv_id": "2403.11259v2",
    "title": "A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty",
    "authors": [
      "Taha-Hossein Hejazi",
      "Zahra Ghadimkhani",
      "Arezoo Borji"
    ],
    "abstract": "Placing applications in mobile edge computing servers presents a complex\nchallenge involving many servers, users, and their requests. Existing\nalgorithms take a long time to solve high-dimensional problems with significant\nuncertainty scenarios. Therefore, an efficient approach is required to maximize\nthe quality of service while considering all technical constraints. One of\nthese approaches is machine learning, which emulates optimal solutions for\napplication placement in edge servers. Machine learning models are expected to\nlearn how to allocate user requests to servers based on the spatial positions\nof users and servers. In this study, the problem is formulated as a two-stage\nstochastic programming. A sufficient amount of training records is generated by\nvarying parameters such as user locations, their request rates, and solving the\noptimization model. Then, based on the distance features of each user from the\navailable servers and their request rates, machine learning models generate\ndecision variables for the first stage of the stochastic optimization model,\nwhich is the user-to-server request allocation, and are employed as independent\ndecision agents that reliably mimic the optimization model. Support Vector\nMachines (SVM) and Multi-layer Perceptron (MLP) are used in this research to\nachieve practical decisions from the stochastic optimization models. The\nperformance of each model has shown an execution effectiveness of over 80%.\nThis research aims to provide a more efficient approach for tackling\nhigh-dimensional problems and scenarios with uncertainties in mobile edge\ncomputing by leveraging machine learning models for optimal decision-making in\nrequest allocation to edge servers. These results suggest that machine-learning\nmodels can significantly improve solution times compared to conventional\napproaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11259v2",
    "published_date": "2024-03-17 16:23:00 UTC",
    "updated_date": "2024-03-23 08:27:02 UTC"
  },
  {
    "arxiv_id": "2403.15443v2",
    "title": "Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images",
    "authors": [
      "Arezoo Borji",
      "Taha-Hossein Hejazi",
      "Abbas Seifi"
    ],
    "abstract": "Alzheimer's disease is a progressive neurodegenerative disorder that\nprimarily affects cognitive functions such as memory, thinking, and behavior.\nIn this disease, there is a critical phase, mild cognitive impairment, that is\nreally important to be diagnosed early since some patients with progressive MCI\nwill develop the disease. This study delves into the challenging task of\nclassifying Alzheimer's disease into four distinct groups: control normal (CN),\nprogressive mild cognitive impairment (pMCI), stable mild cognitive impairment\n(sMCI), and Alzheimer's disease (AD). This classification is based on a\nthorough examination of PET scan images obtained from the ADNI dataset, which\nprovides a thorough understanding of the disease's progression. Several\ndeep-learning and traditional machine-learning models have been used to detect\nAlzheimer's disease. In this paper, three deep-learning models, namely VGG16\nand AlexNet, and a custom Convolutional neural network (CNN) with 8-fold\ncross-validation have been used for classification. Finally, an ensemble\ntechnique is used to improve the overall result of these models. The results\nshow that using deep-learning models to tell the difference between MCI\npatients gives an overall average accuracy of 93.13% and an AUC of 94.4%.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15443v2",
    "published_date": "2024-03-17 16:12:50 UTC",
    "updated_date": "2024-04-01 16:37:08 UTC"
  },
  {
    "arxiv_id": "2403.12106v1",
    "title": "Circular Belief Propagation for Approximate Probabilistic Inference",
    "authors": [
      "Vincent Bouttier",
      "Renaud Jardri",
      "Sophie Deneve"
    ],
    "abstract": "Belief Propagation (BP) is a simple probabilistic inference algorithm,\nconsisting of passing messages between nodes of a graph representing a\nprobability distribution. Its analogy with a neural network suggests that it\ncould have far-ranging applications for neuroscience and artificial\nintelligence. Unfortunately, it is only exact when applied to cycle-free\ngraphs, which restricts the potential of the algorithm. In this paper, we\npropose Circular Belief Propagation (CBP), an extension of BP which limits the\ndetrimental effects of message reverberation caused by cycles by learning to\ndetect and cancel spurious correlations and belief amplifications. We show in\nnumerical experiments involving binary probabilistic graphs that CBP far\noutperforms BP and reaches good performance compared to that of previously\nproposed algorithms.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12106v1",
    "published_date": "2024-03-17 15:59:39 UTC",
    "updated_date": "2024-03-17 15:59:39 UTC"
  },
  {
    "arxiv_id": "2403.11220v3",
    "title": "CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations",
    "authors": [
      "Yuwei Zhang",
      "Yan Wu",
      "Yanming Liu",
      "Xinyue Peng"
    ],
    "abstract": "Object detection methods under known single degradations have been\nextensively investigated. However, existing approaches require prior knowledge\nof the degradation type and train a separate model for each, limiting their\npractical applications in unpredictable environments. To address this\nchallenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,\nCPA-Enhancer, for object detection under unknown degradations. Specifically,\nCPA-Enhancer progressively adapts its enhancement strategy under the\nstep-by-step guidance of CoT prompts, that encode degradation-related\ninformation. To the best of our knowledge, it's the first work that exploits\nCoT prompting for object detection tasks. Overall, CPA-Enhancer is a\nplug-and-play enhancement model that can be integrated into any generic\ndetectors to achieve substantial gains on degraded images, without knowing the\ndegradation type priorly. Experimental results demonstrate that CPA-Enhancer\nnot only sets the new state of the art for object detection but also boosts the\nperformance of other downstream vision tasks under unknown degradations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11220v3",
    "published_date": "2024-03-17 13:43:10 UTC",
    "updated_date": "2024-03-22 11:42:40 UTC"
  },
  {
    "arxiv_id": "2403.11219v1",
    "title": "Causality from Bottom to Top: A Survey",
    "authors": [
      "Abraham Itzhak Weinberg",
      "Cristiano Premebida",
      "Diego Resende Faria"
    ],
    "abstract": "Causality has become a fundamental approach for explaining the relationships\nbetween events, phenomena, and outcomes in various fields of study. It has\ninvaded various fields and applications, such as medicine, healthcare,\neconomics, finance, fraud detection, cybersecurity, education, public policy,\nrecommender systems, anomaly detection, robotics, control, sociology,\nmarketing, and advertising. In this paper, we survey its development over the\npast five decades, shedding light on the differences between causality and\nother approaches, as well as the preconditions for using it. Furthermore, the\npaper illustrates how causality interacts with new approaches such as\nArtificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning,\nReinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality\non various fields, its contribution, and its interaction with state-of-the-art\napproaches. Additionally, the paper exemplifies the trustworthiness and\nexplainability of causality models. We offer several ways to evaluate causality\nmodels and discuss future directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11219v1",
    "published_date": "2024-03-17 13:39:43 UTC",
    "updated_date": "2024-03-17 13:39:43 UTC"
  },
  {
    "arxiv_id": "2403.11217v1",
    "title": "Research on Personal Credit Risk Assessment Methods Based on Causal Inference",
    "authors": [
      "Jiaxin Wang",
      "YiLong Ma"
    ],
    "abstract": "The discussion on causality in human history dates back to ancient Greece,\nyet to this day, there is still no consensus. Fundamentally, this stems from\nthe nature of human cognition, as understanding causality requires abstract\ntools to transcend the limitations of human cognition. In recent decades, the\nrapid development of mathematical and computational tools has provided new\ntheoretical and technical means for exploring causality, creating more avenues\nfor investigation.\n  Based on this, this paper introduces a new definition of causality using\ncategory theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to\navoid the self-referential contradictions in set theory, notably the Russell\nparadox. Within this framework, the feasibility of indicator synthesis in\ncausal inference is demonstrated. Due to the limitations in the development of\ncategory theory-related technical tools, this paper adopts the widely-used\nprobabilistic causal graph tool proposed by Judea Pearl in 1995 to study the\napplication of causal inference in personal credit risk management. The\nspecific work includes: research on the construction method of causal inference\nindex system, definition of causality and feasibility proof of indicator\nsynthesis causal inference within this framework, application methods of causal\ngraph model and intervention alternative criteria in personal credit risk\nmanagement, and so on.",
    "categories": [
      "cs.AI",
      "math.CT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11217v1",
    "published_date": "2024-03-17 13:34:45 UTC",
    "updated_date": "2024-03-17 13:34:45 UTC"
  },
  {
    "arxiv_id": "2403.11207v2",
    "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
    "authors": [
      "Paul S. Scotti",
      "Mihir Tripathy",
      "Cesar Kadir Torrico Villanueva",
      "Reese Kneeland",
      "Tong Chen",
      "Ashutosh Narang",
      "Charan Santhirasegaran",
      "Jonathan Xu",
      "Thomas Naselaris",
      "Kenneth A. Norman",
      "Tanishq Mathew Abraham"
    ],
    "abstract": "Reconstructions of visual perception from brain activity have improved\ntremendously, but the practical utility of such methods has been limited. This\nis because such models are trained independently per subject where each subject\nrequires dozens of hours of expensive fMRI training data to attain high-quality\nresults. The present work showcases high-quality reconstructions using only 1\nhour of fMRI training data. We pretrain our model across 7 subjects and then\nfine-tune on minimal data from a new subject. Our novel functional alignment\nprocedure linearly maps all brain data to a shared-subject latent space,\nfollowed by a shared non-linear mapping to CLIP image space. We then map from\nCLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP\nlatents as inputs instead of text. This approach improves out-of-subject\ngeneralization with limited training data and also attains state-of-the-art\nimage retrieval and reconstruction metrics compared to single-subject\napproaches. MindEye2 demonstrates how accurate reconstructions of perception\nare possible from a single visit to the MRI facility. All code is available on\nGitHub.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "In Forty-first International Conference on Machine Learning, 2024.\n  Code at https://github.com/MedARC-AI/MindEyeV2. Published as a conference\n  paper at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11207v2",
    "published_date": "2024-03-17 13:15:22 UTC",
    "updated_date": "2024-06-15 23:07:17 UTC"
  },
  {
    "arxiv_id": "2403.14706v1",
    "title": "Safeguarding Marketing Research: The Generation, Identification, and Mitigation of AI-Fabricated Disinformation",
    "authors": [
      "Anirban Mukherjee"
    ],
    "abstract": "Generative AI has ushered in the ability to generate content that closely\nmimics human contributions, introducing an unprecedented threat: Deployed en\nmasse, these models can be used to manipulate public opinion and distort\nperceptions, resulting in a decline in trust towards digital platforms. This\nstudy contributes to marketing literature and practice in three ways. First, it\ndemonstrates the proficiency of AI in fabricating disinformative user-generated\ncontent (UGC) that mimics the form of authentic content. Second, it quantifies\nthe disruptive impact of such UGC on marketing research, highlighting the\nsusceptibility of analytics frameworks to even minimal levels of\ndisinformation. Third, it proposes and evaluates advanced detection frameworks,\nrevealing that standard techniques are insufficient for filtering out\nAI-generated disinformation. We advocate for a comprehensive approach to\nsafeguarding marketing research that integrates advanced algorithmic solutions,\nenhanced human oversight, and a reevaluation of regulatory and ethical\nframeworks. Our study seeks to serve as a catalyst, providing a foundation for\nfuture research and policy-making aimed at navigating the intricate challenges\nat the nexus of technology, ethics, and marketing.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14706v1",
    "published_date": "2024-03-17 13:08:28 UTC",
    "updated_date": "2024-03-17 13:08:28 UTC"
  },
  {
    "arxiv_id": "2404.00017v1",
    "title": "Psittacines of Innovation? Assessing the True Novelty of AI Creations",
    "authors": [
      "Anirban Mukherjee"
    ],
    "abstract": "We examine whether Artificial Intelligence (AI) systems generate truly novel\nideas rather than merely regurgitating patterns learned during training.\nUtilizing a novel experimental design, we task an AI with generating project\ntitles for hypothetical crowdfunding campaigns. We compare within AI-generated\nproject titles, measuring repetition and complexity. We compare between the\nAI-generated titles and actual observed field data using an extension of\nmaximum mean discrepancy--a metric derived from the application of kernel mean\nembeddings of statistical distributions to high-dimensional machine learning\n(large language) embedding vectors--yielding a structured analysis of AI output\nnovelty. Results suggest that (1) the AI generates unique content even under\nincreasing task complexity, and at the limits of its computational\ncapabilities, (2) the generated content has face validity, being consistent\nwith both inputs to other generative AI and in qualitative comparison to field\ndata, and (3) exhibits divergence from field data, mitigating concerns relating\nto intellectual property rights. We discuss implications for copyright and\ntrademark law.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00017v1",
    "published_date": "2024-03-17 13:08:11 UTC",
    "updated_date": "2024-03-17 13:08:11 UTC"
  },
  {
    "arxiv_id": "2403.11204v2",
    "title": "Partitioned Neural Network Training via Synthetic Intermediate Labels",
    "authors": [
      "Cevat Volkan Karadağ",
      "Nezih Topaloğlu"
    ],
    "abstract": "The proliferation of extensive neural network architectures, particularly\ndeep learning models, presents a challenge in terms of resource-intensive\ntraining. GPU memory constraints have become a notable bottleneck in training\nsuch sizable models. Existing strategies, including data parallelism, model\nparallelism, pipeline parallelism, and fully sharded data parallelism, offer\npartial solutions. Model parallelism, in particular, enables the distribution\nof the entire model across multiple GPUs, yet the ensuing data communication\nbetween these partitions slows down training. Additionally, the substantial\nmemory overhead required to store auxiliary parameters on each GPU compounds\ncomputational demands. Instead of using the entire model for training, this\nstudy advocates partitioning the model across GPUs and generating synthetic\nintermediate labels to train individual segments. These labels, produced\nthrough a random process, mitigate memory overhead and computational load. This\napproach results in a more efficient training process that minimizes data\ncommunication while maintaining model accuracy. To validate this method, a\n6-layer fully connected neural network is partitioned into two parts and its\nperformance is assessed on the extended MNIST dataset. Experimental results\nindicate that the proposed approach achieves similar testing accuracies to\nconventional training methods, while significantly reducing memory and\ncomputational requirements. This work contributes to mitigating the\nresource-intensive nature of training large neural networks, paving the way for\nmore efficient deep learning model development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "68T01 (Primary) 68T07, 68T05 (Secondary)",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.11204v2",
    "published_date": "2024-03-17 13:06:29 UTC",
    "updated_date": "2025-02-05 19:27:50 UTC"
  },
  {
    "arxiv_id": "2403.11202v2",
    "title": "Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework",
    "authors": [
      "Kaiyan Chang",
      "Kun Wang",
      "Nan Yang",
      "Ying Wang",
      "Dantong Jin",
      "Wenlong Zhu",
      "Zhirong Chen",
      "Cangyuan Li",
      "Hao Yan",
      "Yunhao Zhou",
      "Zhuoliang Zhao",
      "Yuan Cheng",
      "Yudong Pan",
      "Yiqi Liu",
      "Mengdi Wang",
      "Shengwen Liang",
      "Yinhe Han",
      "Huawei Li",
      "Xiaowei Li"
    ],
    "abstract": "Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.AR",
    "comment": "DAC 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11202v2",
    "published_date": "2024-03-17 13:01:03 UTC",
    "updated_date": "2024-07-10 09:06:40 UTC"
  },
  {
    "arxiv_id": "2403.11199v1",
    "title": "Graph Unitary Message Passing",
    "authors": [
      "Haiquan Qiu",
      "Yatao Bian",
      "Quanming Yao"
    ],
    "abstract": "Message passing mechanism contributes to the success of GNNs in various\napplications, but also brings the oversquashing problem. Recent works combat\noversquashing by improving the graph spectrums with rewiring techniques,\ndisrupting the structural bias in graphs, and having limited improvement on\noversquashing in terms of oversquashing measure. Motivated by unitary RNN, we\npropose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs\nby applying unitary adjacency matrix for message passing. To design GUMP, a\ntransformation is first proposed to make general graphs have unitary adjacency\nmatrix and keep its structural bias. Then, unitary adjacency matrix is obtained\nwith a unitary projection algorithm, which is implemented by utilizing the\nintrinsic structure of unitary adjacency matrix and allows GUMP to be\npermutation-equivariant. Experimental results show the effectiveness of GUMP in\nimproving the performance on various graph learning tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.11199v1",
    "published_date": "2024-03-17 12:55:23 UTC",
    "updated_date": "2024-03-17 12:55:23 UTC"
  },
  {
    "arxiv_id": "2403.14705v1",
    "title": "Concept-Best-Matching: Evaluating Compositionality in Emergent Communication",
    "authors": [
      "Boaz Carmeli",
      "Yonatan Belinkov",
      "Ron Meir"
    ],
    "abstract": "Artificial agents that learn to communicate in order to accomplish a given\ntask acquire communication protocols that are typically opaque to a human. A\nlarge body of work has attempted to evaluate the emergent communication via\nvarious evaluation measures, with \\emph{compositionality} featuring as a\nprominent desired trait. However, current evaluation procedures do not directly\nexpose the compositionality of the emergent communication. We propose a\nprocedure to assess the compositionality of emergent communication by finding\nthe best-match between emerged words and natural language concepts. The\nbest-match algorithm provides both a global score and a translation-map from\nemergent words to natural language concepts. To the best of our knowledge, it\nis the first time that such direct and interpretable mapping between emergent\nwords and human concepts is provided.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14705v1",
    "published_date": "2024-03-17 12:47:02 UTC",
    "updated_date": "2024-03-17 12:47:02 UTC"
  },
  {
    "arxiv_id": "2403.15442v3",
    "title": "Artificial Intelligence for Cochlear Implants: Review of Strategies, Challenges, and Perspectives",
    "authors": [
      "Billel Essaid",
      "Hamza Kheddar",
      "Noureddine Batel",
      "Muhammad E. H. Chowdhury",
      "Abderrahmane Lakas"
    ],
    "abstract": "Automatic speech recognition (ASR) plays a pivotal role in our daily lives,\noffering utility not only for interacting with machines but also for\nfacilitating communication for individuals with partial or profound hearing\nimpairments. The process involves receiving the speech signal in analog form,\nfollowed by various signal processing algorithms to make it compatible with\ndevices of limited capacities, such as cochlear implants (CIs). Unfortunately,\nthese implants, equipped with a finite number of electrodes, often result in\nspeech distortion during synthesis. Despite efforts by researchers to enhance\nreceived speech quality using various state-of-the-art (SOTA) signal processing\ntechniques, challenges persist, especially in scenarios involving multiple\nsources of speech, environmental noise, and other adverse conditions. The\nadvent of new artificial intelligence (AI) methods has ushered in cutting-edge\nstrategies to address the limitations and difficulties associated with\ntraditional signal processing techniques dedicated to CIs. This review aims to\ncomprehensively cover advancements in CI-based ASR and speech enhancement,\namong other related aspects. The primary objective is to provide a thorough\noverview of metrics and datasets, exploring the capabilities of AI algorithms\nin this biomedical field, and summarizing and commenting on the best results\nobtained. Additionally, the review will delve into potential applications and\nsuggest future directions to bridge existing research gaps in this domain.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15442v3",
    "published_date": "2024-03-17 11:28:23 UTC",
    "updated_date": "2025-01-12 22:16:50 UTC"
  },
  {
    "arxiv_id": "2403.11175v1",
    "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
    "authors": [
      "Yingru Li",
      "Zhi-Quan Luo"
    ],
    "abstract": "This work advances randomized exploration in reinforcement learning (RL) with\nfunction approximation modeled by linear mixture MDPs. We establish the first\nprior-dependent Bayesian regret bound for RL with function approximation; and\nrefine the Bayesian regret analysis for posterior sampling reinforcement\nlearning (PSRL), presenting an upper bound of ${\\mathcal{O}}(d\\sqrt{H^3 T \\log\nT})$, where $d$ represents the dimensionality of the transition kernel, $H$ the\nplanning horizon, and $T$ the total number of interactions. This signifies a\nmethodological enhancement by optimizing the $\\mathcal{O}(\\sqrt{\\log T})$\nfactor over the previous benchmark (Osband and Van Roy, 2014) specified to\nlinear mixture MDPs. Our approach, leveraging a value-targeted model learning\nperspective, introduces a decoupling argument and a variance reduction\ntechnique, moving beyond traditional analyses reliant on confidence sets and\nconcentration inequalities to formalize Bayesian regret bounds more\neffectively.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "Published in the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS)",
    "pdf_url": "http://arxiv.org/pdf/2403.11175v1",
    "published_date": "2024-03-17 11:23:51 UTC",
    "updated_date": "2024-03-17 11:23:51 UTC"
  },
  {
    "arxiv_id": "2403.11169v4",
    "title": "Correcting misinformation on social media with a large language model",
    "authors": [
      "Xinyi Zhou",
      "Ashish Sharma",
      "Amy X. Zhang",
      "Tim Althoff"
    ],
    "abstract": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "50 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.11169v4",
    "published_date": "2024-03-17 10:59:09 UTC",
    "updated_date": "2024-09-03 05:51:40 UTC"
  },
  {
    "arxiv_id": "2403.11162v1",
    "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
    "authors": [
      "Xiaoyu Wu",
      "Yang Hua",
      "Chumeng Liang",
      "Jiaru Zhang",
      "Hao Wang",
      "Tao Song",
      "Haibing Guan"
    ],
    "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools,\nespecially for few-shot generation where a pretrained model is fine-tuned on a\nsmall set of images to capture a specific style or object. Despite their\nsuccess, concerns exist about potential copyright violations stemming from the\nuse of unauthorized data in this process. In response, we present Contrasting\nGradient Inversion for Diffusion Models (CGI-DM), a novel method featuring\nvivid visual representations for digital copyright authentication. Our approach\ninvolves removing partial information of an image and recovering missing\ndetails by exploiting conceptual differences between the pretrained and\nfine-tuned models. We formulate the differences as KL divergence between latent\nvariables of the two models when given the same input image, which can be\nmaximized through Monte Carlo sampling and Projected Gradient Descent (PGD).\nThe similarity between original and recovered images serves as a strong\nindicator of potential infringements. Extensive experiments on the WikiArt and\nDreambooth datasets demonstrate the high accuracy of CGI-DM in digital\ncopyright authentication, surpassing alternative validation techniques. Code\nimplementation is available at https://github.com/Nicholas0228/Revelio.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11162v1",
    "published_date": "2024-03-17 10:06:38 UTC",
    "updated_date": "2024-03-17 10:06:38 UTC"
  },
  {
    "arxiv_id": "2403.14704v2",
    "title": "A minimal coalition logic",
    "authors": [
      "Yinfeng Li",
      "Fengkui Ju"
    ],
    "abstract": "Coalition Logic is a central logic in logical studies of strategic reasoning,\nwhose models are concurrent game models. In this paper, first, we\nsystematically discuss three assumptions of concurrent game models and argue\nthat they are too strong. The first is seriality; that is, every coalition\nalways has an available joint action. The second is the independence of agents;\nthat is, the merge of two available joint actions of two disjoint coalitions is\nalways an available joint action of the union of the two coalitions. The third\nis determinism; that is, all available joint actions of the grand coalition\nalways have a unique outcome. Second, we present a coalition logic based on\ngeneral concurrent game models which do not have the three assumptions and show\nits completeness. This logic seems minimal for reasoning about coalitional\npowers.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14704v2",
    "published_date": "2024-03-17 09:33:37 UTC",
    "updated_date": "2025-01-13 04:47:41 UTC"
  },
  {
    "arxiv_id": "2403.11152v1",
    "title": "Evaluation Ethics of LLMs in Legal Domain",
    "authors": [
      "Ruizhe Zhang",
      "Haitao Li",
      "Yueyue Wu",
      "Qingyao Ai",
      "Yiqun Liu",
      "Min Zhang",
      "Shaoping Ma"
    ],
    "abstract": "In recent years, the utilization of large language models for natural\nlanguage dialogue has gained momentum, leading to their widespread adoption\nacross various domains. However, their universal competence in addressing\nchallenges specific to specialized fields such as law remains a subject of\nscrutiny. The incorporation of legal ethics into the model has been overlooked\nby researchers. We asserts that rigorous ethic evaluation is essential to\nensure the effective integration of large language models in legal domains,\nemphasizing the need to assess domain-specific proficiency and domain-specific\nethic. To address this, we propose a novelty evaluation methodology, utilizing\nauthentic legal cases to evaluate the fundamental language abilities,\nspecialized legal knowledge and legal robustness of large language models\n(LLMs). The findings from our comprehensive evaluation contribute significantly\nto the academic discourse surrounding the suitability and performance of large\nlanguage models in legal domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, in processing of ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11152v1",
    "published_date": "2024-03-17 09:05:13 UTC",
    "updated_date": "2024-03-17 09:05:13 UTC"
  },
  {
    "arxiv_id": "2403.12100v1",
    "title": "Learning Time Slot Preferences via Mobility Tree for Next POI Recommendation",
    "authors": [
      "Tianhao Huang",
      "Xuan Pan",
      "Xiangrui Cai",
      "Ying Zhang",
      "Xiaojie Yuan"
    ],
    "abstract": "Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic\nranking of POIs based on users' current check-in trajectories. The\nrecommendation performance of this task is contingent upon a comprehensive\nunderstanding of users' personalized behavioral patterns through Location-based\nSocial Networks (LBSNs) data. While prior studies have adeptly captured\nsequential patterns and transitional relationships within users' check-in\ntrajectories, a noticeable gap persists in devising a mechanism for discerning\nspecialized behavioral patterns during distinct time slots, such as noon,\nafternoon, or evening. In this paper, we introduce an innovative data structure\ntermed the ``Mobility Tree'', tailored for hierarchically describing users'\ncheck-in records. The Mobility Tree encompasses multi-granularity time slot\nnodes to learn user preferences across varying temporal periods. Meanwhile, we\npropose the Mobility Tree Network (MTNet), a multitask framework for\npersonalized preference learning based on Mobility Trees. We develop a\nfour-step node interaction operation to propagate feature information from the\nleaf nodes to the root node. Additionally, we adopt a multitask training\nstrategy to push the model towards learning a robust representation. The\ncomprehensive experimental results demonstrate the superiority of MTNet over\nten state-of-the-art next POI recommendation models across three real-world\nLBSN datasets, substantiating the efficacy of time slot preference learning\nfacilitated by Mobility Tree.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.12100v1",
    "published_date": "2024-03-17 08:43:12 UTC",
    "updated_date": "2024-03-17 08:43:12 UTC"
  },
  {
    "arxiv_id": "2403.15441v1",
    "title": "Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks",
    "authors": [
      "Yuxuan Song",
      "Jingjing Gong",
      "Yanru Qu",
      "Hao Zhou",
      "Mingyue Zheng",
      "Jingjing Liu",
      "Wei-Ying Ma"
    ],
    "abstract": "Advanced generative model (e.g., diffusion model) derived from simplified\ncontinuity assumptions of data distribution, though showing promising progress,\nhas been difficult to apply directly to geometry generation applications due to\nthe multi-modality and noise-sensitive nature of molecule geometry. This work\nintroduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits\nmolecule geometry by modeling diverse modalities in the differentiable\nparameter space of distributions. GeoBFN maintains the SE-(3) invariant density\nmodeling property by incorporating equivariant inter-dependency modeling on\nparameters of distributions and unifying the probabilistic modeling of\ndifferent modalities. Through optimized training and sampling techniques, we\ndemonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D\nmolecule generation benchmarks in terms of generation quality (90.87% molecule\nstability in QM9 and 85.6% atom stability in GEOM-DRUG. GeoBFN can also conduct\nsampling with any number of steps to reach an optimal trade-off between\nefficiency and quality (e.g., 20-times speedup without sacrificing\nperformance).",
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "physics.chem-ph",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15441v1",
    "published_date": "2024-03-17 08:40:06 UTC",
    "updated_date": "2024-03-17 08:40:06 UTC"
  },
  {
    "arxiv_id": "2403.11124v2",
    "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
    "authors": [
      "Feifan Song",
      "Bowen Yu",
      "Hao Lang",
      "Haiyang Yu",
      "Fei Huang",
      "Houfeng Wang",
      "Yongbin Li"
    ],
    "abstract": "Alignment with human preference prevents large language models (LLMs) from\ngenerating misleading or toxic content while requiring high-cost human\nfeedback. Assuming resources of human annotation are limited, there are two\ndifferent ways of allocating considered: more diverse PROMPTS or more diverse\nRESPONSES to be labeled. Nonetheless, a straightforward comparison between\ntheir impact is absent. In this work, we first control the diversity of both\nsides according to the number of samples for fine-tuning, which can directly\nreflect their influence. We find that instead of numerous prompts, more\nresponses but fewer prompts better trigger LLMs for human alignment.\nAdditionally, the concept of diversity for prompts can be more complex than\nresponses that are typically quantified by single digits. Consequently, a new\nformulation of prompt diversity is proposed, further implying a linear\ncorrelation with the final performance of LLMs after fine-tuning. We also\nleverage it on data augmentation and conduct experiments to show its effect on\ndifferent algorithms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11124v2",
    "published_date": "2024-03-17 07:08:55 UTC",
    "updated_date": "2024-03-30 16:48:16 UTC"
  },
  {
    "arxiv_id": "2403.11116v4",
    "title": "PhD: A ChatGPT-Prompted Visual hallucination Evaluation Dataset",
    "authors": [
      "Jiazhen Liu",
      "Yuhan Fu",
      "Ruobing Xie",
      "Runquan Xie",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Xirong Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) hallucinate, resulting in an\nemerging topic of visual hallucination evaluation (VHE). This paper contributes\na ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective\nVHE at a large scale. The essence of VHE is to ask an MLLM questions about\nspecific images to assess its susceptibility to hallucination. Depending on\nwhat to ask (objects, attributes, sentiment, etc.) and how the questions are\nasked, we structure PhD along two dimensions, i.e. task and mode. Five visual\nrecognition tasks, ranging from low-level (object / attribute recognition) to\nmiddle-level (sentiment / position recognition and counting), are considered.\nBesides a normal visual QA mode, which we term PhD-base, PhD also asks\nquestions with specious context (PhD-sec) or with incorrect context ({PhD-icc),\nor with AI-generated counter common sense images (PhD-ccs). We construct PhD by\na ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules:\ntask-specific hallucinatory item (hitem) selection, hitem-embedded question\ngeneration, specious / incorrect context generation, and counter-common-sense\n(CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA\ntriplets in total, PhD reveals considerable variability in MLLMs' performance\nacross various modes and tasks, offering valuable insights into the nature of\nhallucination. As such, PhD stands as a potent tool not only for VHE but may\nalso play a significant role in the refinement of MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2025, Highlight",
    "pdf_url": "http://arxiv.org/pdf/2403.11116v4",
    "published_date": "2024-03-17 06:53:44 UTC",
    "updated_date": "2025-04-14 12:11:51 UTC"
  },
  {
    "arxiv_id": "2403.11114v1",
    "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning",
    "authors": [
      "Jingcheng Jiang",
      "Haiyin Piao",
      "Yu Fu",
      "Yihang Hao",
      "Chuanlu Jiang",
      "Ziqi Wei",
      "Xin Yang"
    ],
    "abstract": "Reviewing the previous work of diversity Rein-forcement Learning,diversity is\noften obtained via an augmented loss function,which requires a balance between\nreward and diversity.Generally,diversity optimization algorithms use\nMulti-armed Bandits algorithms to select the coefficient in the pre-defined\nspace. However, the dynamic distribution of reward signals for MABs or the\nconflict between quality and diversity limits the performance of these methods.\nWe introduce the Phasic Diversity Optimization (PDO) algorithm, a\nPopulation-Based Training framework that separates reward and diversity\ntraining into distinct phases instead of optimizing a multi-objective function.\nIn the auxiliary phase, agents with poor performance diversified via\ndeterminants will not replace the better agents in the archive. The decoupling\nof reward and diversity allows us to use an aggressive diversity optimization\nin the auxiliary phase without performance degradation. Furthermore, we\nconstruct a dogfight scenario for aerial agents to demonstrate the practicality\nof the PDO algorithm. We introduce two implementations of PDO archive and\nconduct tests in the newly proposed adversarial dogfight and MuJoCo\nsimulations. The results show that our proposed algorithm achieves better\nperformance than baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "14J60 (Primary)",
      "I.2.9"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.11114v1",
    "published_date": "2024-03-17 06:41:09 UTC",
    "updated_date": "2024-03-17 06:41:09 UTC"
  },
  {
    "arxiv_id": "2403.11106v1",
    "title": "Self-Supervised Quantization-Aware Knowledge Distillation",
    "authors": [
      "Kaiqi Zhao",
      "Ming Zhao"
    ],
    "abstract": "Quantization-aware training (QAT) and Knowledge Distillation (KD) are\ncombined to achieve competitive performance in creating low-bit deep learning\nmodels. However, existing works applying KD to QAT require tedious\nhyper-parameter tuning to balance the weights of different loss terms, assume\nthe availability of labeled training data, and require complex, computationally\nintensive training procedures for good performance. To address these\nlimitations, this paper proposes a novel Self-Supervised Quantization-Aware\nKnowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and\nbackward dynamics of various quantization functions, making it flexible for\nincorporating various QAT works. Then it formulates QAT as a co-optimization\nproblem that simultaneously minimizes the KL-Loss between the full-precision\nand low-bit models for KD and the discretization error for quantization,\nwithout supervision from labels. A comprehensive evaluation shows that SQAKD\nsubstantially outperforms the state-of-the-art QAT and KD works for a variety\nof model architectures. Our code is at: https://github.com/kaiqi123/SQAKD.git.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.11106v1",
    "published_date": "2024-03-17 06:20:28 UTC",
    "updated_date": "2024-03-17 06:20:28 UTC"
  },
  {
    "arxiv_id": "2403.11092v1",
    "title": "Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts",
    "authors": [
      "Michael Saxon",
      "Yiran Luo",
      "Sharon Levy",
      "Chitta Baral",
      "Yezhou Yang",
      "William Yang Wang"
    ],
    "abstract": "Benchmarks of the multilingual capabilities of text-to-image (T2I) models\ncompare generated images prompted in a test language to an expected image\ndistribution over a concept set. One such benchmark, \"Conceptual Coverage\nAcross Languages\" (CoCo-CroLa), assesses the tangible noun inventory of T2I\nmodels by prompting them to generate pictures from a concept list translated to\nseven languages and comparing the output image populations. Unfortunately, we\nfind that this benchmark contains translation errors of varying severity in\nSpanish, Japanese, and Chinese. We provide corrections for these errors and\nanalyze how impactful they are on the utility and validity of CoCo-CroLa as a\nbenchmark. We reassess multiple baseline T2I models with the revisions, compare\nthe outputs elicited under the new translations to those conditioned on the\nold, and show that a correction's impactfulness on the image-domain benchmark\nresults can be predicted in the text domain with similarity scores. Our\nfindings will guide the future development of T2I multilinguality metrics by\nproviding analytical tools for practical translation decisions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "eess.IV"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2403.11092v1",
    "published_date": "2024-03-17 05:05:11 UTC",
    "updated_date": "2024-03-17 05:05:11 UTC"
  },
  {
    "arxiv_id": "2403.11082v1",
    "title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning",
    "authors": [
      "Javad Rafiei Asl",
      "Prajwal Panzade",
      "Eduardo Blanco",
      "Daniel Takabi",
      "Zhipeng Cai"
    ],
    "abstract": "Pre-trained language models (PLMs) have consistently demonstrated outstanding\nperformance across a diverse spectrum of natural language processing tasks.\nNevertheless, despite their success with unseen data, current PLM-based\nrepresentations often exhibit poor robustness in adversarial settings. In this\npaper, we introduce RobustSentEmbed, a self-supervised sentence embedding\nframework designed to improve both generalization and robustness in diverse\ntext representation tasks and against a diverse set of adversarial attacks.\nThrough the generation of high-risk adversarial perturbations and their\nutilization in a novel objective function, RobustSentEmbed adeptly learns\nhigh-quality and robust sentence embeddings. Our experiments confirm the\nsuperiority of RobustSentEmbed over state-of-the-art representations.\nSpecifically, Our framework achieves a significant reduction in the success\nrate of various adversarial attacks, notably reducing the BERTAttack success\nrate by almost half (from 75.51\\% to 38.81\\%). The framework also yields\nimprovements of 1.59\\% and 0.23\\% in semantic textual similarity tasks and\nvarious transfer tasks, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the Annual Conference of the North American Chapter of\n  the Association for Computational Linguistics (NAACL Findings) 2024.\n  [https://openreview.net/forum?id=9dEAg4lJEA]",
    "pdf_url": "http://arxiv.org/pdf/2403.11082v1",
    "published_date": "2024-03-17 04:29:45 UTC",
    "updated_date": "2024-03-17 04:29:45 UTC"
  },
  {
    "arxiv_id": "2403.11075v2",
    "title": "GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment",
    "authors": [
      "Lance Ying",
      "Kunal Jha",
      "Shivam Aarya",
      "Joshua B. Tenenbaum",
      "Antonio Torralba",
      "Tianmin Shu"
    ],
    "abstract": "Verbal communication plays a crucial role in human cooperation, particularly\nwhen the partners only have incomplete information about the task, environment,\nand each other's mental state. In this paper, we propose a novel cooperative\ncommunication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates\nverbal communication as a planning problem that minimizes the misalignment\nbetween the parts of agents' mental states that are relevant to the goals. This\napproach enables an embodied assistant to reason about when and how to\nproactively initialize communication with humans verbally using natural\nlanguage to help achieve better cooperation. We evaluate our approach against\nstrong baselines in two challenging environments, Overcooked (a multiplayer\ngame) and VirtualHome (a household simulator). Our experimental results\ndemonstrate that large language models struggle with generating meaningful\ncommunication that is grounded in the social and physical context. In contrast,\nour approach can successfully generate concise verbal communication for the\nembodied assistant to effectively boost the performance of the cooperation as\nwell as human users' perception of the assistant.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.11075v2",
    "published_date": "2024-03-17 03:52:52 UTC",
    "updated_date": "2025-01-14 06:02:50 UTC"
  },
  {
    "arxiv_id": "2403.11074v1",
    "title": "Audio-Visual Segmentation via Unlabeled Frame Exploitation",
    "authors": [
      "Jinxiang Liu",
      "Yikun Liu",
      "Fei Zhang",
      "Chen Ju",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "abstract": "Audio-visual segmentation (AVS) aims to segment the sounding objects in video\nframes. Although great progress has been witnessed, we experimentally reveal\nthat current methods reach marginal performance gain within the use of the\nunlabeled frames, leading to the underutilization issue. To fully explore the\npotential of the unlabeled frames for AVS, we explicitly divide them into two\ncategories based on their temporal characteristics, i.e., neighboring frame\n(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,\noften contain rich motion information that assists in the accurate localization\nof sounding objects. Contrary to NFs, DFs have long temporal distances from the\nlabeled frame, which share semantic-similar objects with appearance variations.\nConsidering their unique characteristics, we propose a versatile framework that\neffectively leverages them to tackle AVS. Specifically, for NFs, we exploit the\nmotion cues as the dynamic guidance to improve the objectness localization.\nBesides, we exploit the semantic cues in DFs by treating them as valid\naugmentations to the labeled frames, which are then used to enrich data\ndiversity in a self-training manner. Extensive experimental results demonstrate\nthe versatility and superiority of our method, unleashing the power of the\nabundant unlabeled frames.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.11074v1",
    "published_date": "2024-03-17 03:45:14 UTC",
    "updated_date": "2024-03-17 03:45:14 UTC"
  },
  {
    "arxiv_id": "2403.11073v1",
    "title": "Tokensome: Towards a Genetic Vision-Language GPT for Explainable and Cognitive Karyotyping",
    "authors": [
      "Haoxi Zhang",
      "Xinxu Zhang",
      "Yuanxin Lin",
      "Maiqi Wang",
      "Yi Lai",
      "Yu Wang",
      "Linfeng Yu",
      "Yufeng Xu",
      "Ran Cheng",
      "Edward Szczerbicki"
    ],
    "abstract": "Automatic karyotype analysis is often defined as a visual perception task\nfocused solely on chromosomal object-level modeling. This definition has led\nmost existing methods to overlook componential and holistic information,\nsignificantly constraining model performance. Moreover, the lack of\ninterpretability in current technologies hinders clinical adoption. In this\npaper, we introduce Tokensome, a novel vision-language model based on\nchromosome tokenization for explainable and cognitive karyotyping. Tokensome\nelevates the method from the conventional visual perception layer to the\ncognitive decision-making layer. This elevation enables the integration of\ndomain knowledge and cognitive reasoning via knowledge graphs and LLMs,\nmarkedly enhancing model's explainability and facilitating abnormality\ndetection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2403.11073v1",
    "published_date": "2024-03-17 03:38:50 UTC",
    "updated_date": "2024-03-17 03:38:50 UTC"
  },
  {
    "arxiv_id": "2404.13050v1",
    "title": "FlowMind: Automatic Workflow Generation with LLMs",
    "authors": [
      "Zhen Zeng",
      "William Watson",
      "Nicole Cho",
      "Saba Rahimi",
      "Shayleen Reynolds",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "abstract": "The rapidly evolving field of Robotic Process Automation (RPA) has made\nsignificant strides in automating repetitive processes, yet its effectiveness\ndiminishes in scenarios requiring spontaneous or unpredictable tasks demanded\nby users. This paper introduces a novel approach, FlowMind, leveraging the\ncapabilities of Large Language Models (LLMs) such as Generative Pretrained\nTransformer (GPT), to address this limitation and create an automatic workflow\ngeneration system. In FlowMind, we propose a generic prompt recipe for a\nlecture that helps ground LLM reasoning with reliable Application Programming\nInterfaces (APIs). With this, FlowMind not only mitigates the common issue of\nhallucinations in LLMs, but also eliminates direct interaction between LLMs and\nproprietary data or code, thus ensuring the integrity and confidentiality of\ninformation - a cornerstone in financial services. FlowMind further simplifies\nuser interaction by presenting high-level descriptions of auto-generated\nworkflows, enabling users to inspect and provide feedback effectively. We also\nintroduce NCEN-QA, a new dataset in finance for benchmarking question-answering\ntasks from N-CEN reports on funds. We used NCEN-QA to evaluate the performance\nof workflows generated by FlowMind against baseline and ablation variants of\nFlowMind. We demonstrate the success of FlowMind, the importance of each\ncomponent in the proposed lecture recipe, and the effectiveness of user\ninteraction and feedback in FlowMind.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in ACM ICAIF 2023",
    "pdf_url": "http://arxiv.org/pdf/2404.13050v1",
    "published_date": "2024-03-17 00:36:37 UTC",
    "updated_date": "2024-03-17 00:36:37 UTC"
  },
  {
    "arxiv_id": "2403.11047v1",
    "title": "From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting",
    "authors": [
      "Zhen Zeng",
      "Rachneet Kaur",
      "Suchetha Siddagangappa",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "abstract": "Time series forecasting plays a crucial role in decision-making across\nvarious domains, but it presents significant challenges. Recent studies have\nexplored image-driven approaches using computer vision models to address these\nchallenges, often employing lineplots as the visual representation of time\nseries data. In this paper, we propose a novel approach that uses\ntime-frequency spectrograms as the visual representation of time series data.\nWe introduce the use of a vision transformer for multimodal learning,\nshowcasing the advantages of our approach across diverse datasets from\ndifferent domains. To evaluate its effectiveness, we compare our method against\nstatistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based\napproach (DeepAR), other visual representations of time series data (lineplot\nimages), and an ablation study on using only the time series as input. Our\nexperiments demonstrate the benefits of utilizing spectrograms as a visual\nrepresentation for time series data, along with the advantages of employing a\nvision transformer for simultaneous learning in both the time and frequency\ndomains.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ACM ICAIF 2023",
    "pdf_url": "http://arxiv.org/pdf/2403.11047v1",
    "published_date": "2024-03-17 00:14:29 UTC",
    "updated_date": "2024-03-17 00:14:29 UTC"
  },
  {
    "arxiv_id": "2403.11046v2",
    "title": "Regulating Chatbot Output via Inter-Informational Competition",
    "authors": [
      "Jiawei Zhang"
    ],
    "abstract": "The advent of ChatGPT has sparked over a year of regulatory frenzy. However,\nfew existing studies have rigorously questioned the assumption that, if left\nunregulated, AI chatbot's output would inflict tangible, severe real harm on\nhuman affairs. Most researchers have overlooked the critical possibility that\nthe information market itself can effectively mitigate these risks and, as a\nresult, they tend to use regulatory tools to address the issue directly. This\nArticle develops a yardstick for reevaluating both AI-related content risks and\ncorresponding regulatory proposals by focusing on inter-informational\ncompetition among various outlets. The decades-long history of regulating\ninformation and communications technologies indicates that regulators tend to\nerr too much on the side of caution and to put forward excessive regulatory\nmeasures when encountering the uncertainties brought about by new technologies.\nIn fact, a trove of empirical evidence has demonstrated that market competition\namong information outlets can effectively mitigate most risks and that\noverreliance on regulation is not only unnecessary but detrimental, as well.\nThis Article argues that sufficient competition among chatbots and other\ninformation outlets in the information marketplace can sufficiently mitigate\nand even resolve most content risks posed by generative AI technologies. This\nrenders certain loudly advocated regulatory strategies, like mandatory\nprohibitions, licensure, curation of datasets, and notice-and-response regimes,\ntruly unnecessary and even toxic to desirable competition and innovation\nthroughout the AI industry. Ultimately, the ideas that I advance in this\nArticle should pour some much-needed cold water on the regulatory frenzy over\ngenerative AI and steer the issue back to a rational track.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.CY",
    "comment": "50-page legal Article, forthcoming in Northwestern Journal of\n  Technology and Intellectual Property",
    "pdf_url": "http://arxiv.org/pdf/2403.11046v2",
    "published_date": "2024-03-17 00:11:15 UTC",
    "updated_date": "2024-11-19 18:18:04 UTC"
  }
]
{
  "date": "2025-10-04",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-10-04 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**ï¼š\nä»Šå¤©çš„ arXiv å……æ»¡äº†å…³äº **Test-Time Scaling (æµ‹è¯•æ—¶æ‰©å±•)** å’Œ **Reasoning (æ¨ç†)** æœºåˆ¶çš„æ·±åº¦åæ€ã€‚æˆ‘ä»¬çœ‹åˆ°äº† Google ç­‰æœºæ„çš„ç ”ç©¶è€…æ¢è®¨è®­ç»ƒæ•°æ®å¦‚ä½•å†³å®šæ¨ç†æ—¶çš„è®¡ç®—æ•ˆç‡ï¼Œä»¥åŠ RL åœ¨â€œé›¶å¥–åŠ±â€å›°å¢ƒä¸‹çš„å¤±æ•ˆåˆ†æã€‚æ­¤å¤–ï¼Œ**å›¾åƒç”Ÿæˆçš„è‡ªæˆ‘è®­ç»ƒ (Self-Training)** è¿æ¥äº†æ‰“ç ´â€œæ¨¡å‹å´©æºƒâ€çš„æ–°æ–¹æ³•ï¼Œ**ä»£ç ç”Ÿæˆ**é¢†åŸŸä¹Ÿåœ¨ C è½¬ Rust å’Œ CUDA ä¼˜åŒ–ä¸Šå–å¾—äº†çªç ´ã€‚æœ€åï¼Œè¿˜æœ‰ä¸€ä¸ªå…³äº arXiv æºç æ³„æ¼éšç§çš„æœ‰è¶£ï¼ˆä¸”æƒŠæ‚šï¼‰çš„å®‰å…¨å®¡è®¡ã€‚\n\n---\n\n### ğŸš€ æ¨ç†æ¨¡å‹ä¸ Test-Time Scaling (æ¨ç†æ—¶æ‰©å±•)\n\n**1. ç†è§£è®­ç»ƒæ•°æ®åœ¨ Test-Time Scaling ä¸­çš„è§’è‰²**\n**# title: Understanding the Role of Training Data in Test-Time Scaling**\n> **Authors:** Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni (Google Research)\n\n*   **æ ¸å¿ƒå‘ç°**ï¼šTest-time scalingï¼ˆå¦‚ OpenAI o1ï¼‰é€šè¿‡å¢åŠ æ¨ç†è®¡ç®—é‡æ¥æå‡æ€§èƒ½ï¼Œä½†å®ƒä½•æ—¶æœ‰æ•ˆï¼Ÿæœ¬æ–‡é€šè¿‡çº¿æ€§å›å½’ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æƒé‡é¢„æµ‹è¿›è¡Œç†è®ºåˆ†æã€‚\n*   **å…³é”®ç»“è®º**ï¼š\n    1.  **ä»¥ç®—åŠ›æ¢ Context**ï¼šåœ¨å›ºå®šçš„æµ‹è¯•è¯¯å·®ä¸‹ï¼Œå¢åŠ æµ‹è¯•æ—¶è®¡ç®—é‡å¯ä»¥å‡å°‘å¯¹è®­ç»ƒ Prompt ä¸­ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆin-context examplesï¼‰çš„éœ€æ±‚ã€‚\n    2.  **æ•°æ®å†³å®šä¸Šé™**ï¼šå¦‚æœä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æŠ€èƒ½æœªåœ¨è®­ç»ƒæ•°æ®ä¸­å……åˆ†ä½“ç°ï¼Œç›²ç›®å¢åŠ æ¨ç†è®¡ç®—é‡åè€Œä¼šæŸå®³æ€§èƒ½ã€‚\n    3.  **ä»»åŠ¡éš¾åº¦**ï¼šåœ¨å¤šæ ·åŒ–ä¸”å›°éš¾çš„ä»»åŠ¡é›†ä¸Šè®­ç»ƒï¼Œæœ€æœ‰åˆ©äº Test-time scaling çš„å‘æŒ¥ã€‚\n\n**2. å½“ RL æœŸé—´æ²¡æœ‰ä»»ä½•å¥–åŠ±æ—¶ä½ èƒ½åšä»€ä¹ˆï¼Ÿ**\n**# title: What Can You Do When You Have Zero Rewards During RL?**\n> **Authors:** Jatin Prakash, Anirudh Buvanesh\n\n*   **ç—›ç‚¹**ï¼šç”¨ RL æå‡ LLM æ¨ç†èƒ½åŠ›ï¼ˆå¦‚ o1-like æ¨¡å‹ï¼‰å¾ˆç«ï¼Œä½†å¦‚æœåŸºåº§æ¨¡å‹å¤ªå¼±ï¼Œé‡‡æ ·ä¸å‡ºä»»ä½•æ­£ç¡®ç­”æ¡ˆï¼ˆZero Rewardï¼‰ï¼Œæ¢¯åº¦å°±ä¼šæ¶ˆå¤±ï¼Œè®­ç»ƒåœæ»ã€‚\n*   **å‘ç°**ï¼šä½œè€…æµ‹è¯•äº†å„ç§å¤æ‚çš„ RL æŠ€å·§ï¼ˆå¯†é›†å¥–åŠ±ã€å¤šæ ·æ€§æ¿€åŠ±ç­‰ï¼‰ï¼Œå‘ç°ç»Ÿç»Ÿæ²¡ç”¨ã€‚\n*   **è§£å†³æ–¹æ¡ˆ**ï¼š**Data-Centric Intervention**ã€‚ç®€å•åœ°åœ¨è®­ç»ƒé›†ä¸­åŠ å…¥æ›´å®¹æ˜“çš„æ ·æœ¬ï¼Œè®©æ¨¡å‹å…ˆâ€œå°åˆ°ç”œå¤´â€ï¼Œå°±èƒ½æœ€ç»ˆè§£å†³åŸå§‹éš¾é¢˜ã€‚è¿™æ¯”ä¿®æ”¹ RL ç®—æ³•æœ‰æ•ˆå¾—å¤šã€‚\n\n**3. æ­¥éª¤å‰ªæï¼šè¶…è¶Š Token é•¿åº¦çš„é«˜æ•ˆæ¨ç†**\n**# title: Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models**\n**# title: Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models**\n*   **Step Pruner (Paper 37)**: é’ˆå¯¹æ¨ç†æ¨¡å‹â€œè¿‡åº¦æ€è€ƒ (Overthinking)â€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ RL æ¡†æ¶ï¼Œå¥–åŠ±ç®€æ´çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶åœ¨è¾“å‡ºä¸å†ç¼©çŸ­æ—¶åŠ¨æ€åœæ­¢è®­ç»ƒã€‚åœ¨ AIME24 ä¸Šå‡å°‘äº† 69.7% çš„ Token ç”¨é‡ã€‚\n*   **Less Diverse, Less Safe (Paper 7)**: è­¦å‘Šï¼Test-Time Scaling ä¾èµ–å€™é€‰æ± çš„å¤šæ ·æ€§ã€‚å¦‚æœå€™é€‰å¤šæ ·æ€§å—é™ï¼ˆå³ä½¿æ˜¯å¾ˆå°çš„é™åˆ¶ï¼‰ï¼Œæ¨¡å‹ç”Ÿæˆä¸å®‰å…¨å†…å®¹çš„æ¦‚ç‡ä¼šæ˜¾è‘—å¢åŠ ï¼Œç°æœ‰çš„é˜²æŠ¤æ ï¼ˆGuardrailsï¼‰å¾ˆéš¾æ£€æµ‹åˆ°è¿™ç§ç”±å¤šæ ·æ€§ç¼ºå¤±å¼•å‘çš„é£é™©ã€‚\n\n---\n\n### ğŸ› ï¸ æ¨¡å‹è®­ç»ƒã€ä¼˜åŒ–ä¸æ¶æ„\n\n**4. Neonï¼šåˆ©ç”¨è‡ªæˆ‘è®­ç»ƒä¸­çš„è´Ÿå‘å¤–æ¨æ”¹è¿›å›¾åƒç”Ÿæˆ**\n**# title: Neon: Negative Extrapolation From Self-Training Improves Image Generation**\n> **Authors:** Sina Alemohammad, et al. (Rice University, UT Austin)\n\n*   **æ ¸å¿ƒåˆ›æ–°**ï¼šé€šå¸¸ç”¨ç”Ÿæˆæ¨¡å‹è‡ªå·±çš„æ•°æ®å¾®è°ƒè‡ªå·±ä¼šå¯¼è‡´â€œæ¨¡å‹å´©æºƒ (Model Collapse/MAD)â€ã€‚æœ¬æ–‡åå…¶é“è€Œè¡Œä¹‹ã€‚\n*   **æ–¹æ³•**ï¼šå…ˆç”¨åˆæˆæ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œæ¨¡å‹è‚¯å®šä¼šé€€åŒ–ï¼ˆåå‘é«˜æ¦‚ç‡åŒºåŸŸï¼‰ã€‚ç„¶åï¼Œå°†æƒé‡æ›´æ–°çš„æ–¹å‘**åè¿‡æ¥ (Negative Extrapolation)**ï¼Œå³â€œä»¥æ­¤ä¸ºæˆ’ï¼Œå‘åæ–¹å‘èµ°â€ã€‚\n*   **æ•ˆæœ**ï¼šè¿™å®é™…ä¸Šçº æ­£äº†åˆæˆæ•°æ®ä¸çœŸå®åˆ†å¸ƒçš„åå·®ã€‚åœ¨ ImageNet ä¸Šï¼ŒNeon ä»…ç”¨ 1% çš„é¢å¤–è®¡ç®—é‡å°±å°† FID åˆ·åˆ°äº† SOTA (1.02)ã€‚\n\n**5. æœ€ä¼˜æ‰©å±•éœ€è¦æœ€ä¼˜èŒƒæ•°**\n**# title: Optimal Scaling Needs Optimal Norm**\n> **Authors:** Oleg Filatov, et al.\n\n*   **å‘ç°**ï¼šåœ¨ä½¿ç”¨ Scion ä¼˜åŒ–å™¨æ—¶ï¼Œè·¨è¶Šä¸åŒæ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°çš„â€œæœ€ä¼˜æ‰©å±•è§„åˆ™â€å—ä¸€ä¸ªå•ä¸€å˜é‡æ§åˆ¶ï¼š**è¾“å‡ºå±‚çš„ç®—å­èŒƒæ•° (Operator Norm)**ã€‚\n*   **è´¡çŒ®**ï¼šè¿™è¢«ç§°ä¸ºâ€œèŒƒæ•°è¿ç§» (Norm Transfer)â€ã€‚å¦‚æœä½ ä¿æŒè¿™ä¸ªèŒƒæ•°ä¸å˜ï¼Œå°±èƒ½æ‰¾åˆ°æœ€ä¼˜çš„å­¦ä¹ ç‡/Batch Size ç»„åˆã€‚è¿™ä¸ºå¤§æ¨¡å‹è®­ç»ƒè°ƒå‚æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚\n\n**6. REGï¼šç”¨äºé²æ£’è®­ç»ƒåŠ¨åŠ›å­¦çš„æ­£åˆ™åŒ–ä¼˜åŒ–å™¨**\n**# title: REG: A Regularization Optimizer for Robust Training Dynamics**\n*   **èƒŒæ™¯**ï¼šMuon ä¼˜åŒ–å™¨æœ€è¿‘å¾ˆç«ï¼Œä½†å®ƒä¾èµ–çŸ©é˜µç¬¦å·å‡½æ•°ï¼Œè®­ç»ƒä¸ç¨³å®šä¸”éš¾ä»¥å¾®è°ƒã€‚\n*   **æ–¹æ³•**ï¼šæå‡º REG ä¼˜åŒ–å™¨ï¼Œç”¨è¡Œåˆ—ç¼©æ”¾ (RACS) ç®—å­æ›¿ä»£ Muon çš„æ¿€è¿›æ“ä½œã€‚æ—¢ä¿ç•™äº†æ€§èƒ½ï¼Œåˆåƒ AdamW ä¸€æ ·ç¨³å®šã€‚\n\n---\n\n### ğŸ’» ä»£ç æ™ºèƒ½ä¸ Agent (æ™ºèƒ½ä½“)\n\n**7. é’ˆå¯¹ Agent ç³»ç»Ÿçš„å°å‹è¯­è¨€æ¨¡å‹ï¼šæ¶æ„ä¸æƒè¡¡ç»¼è¿°**\n**# title: Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs**\n> **Authors:** Raghav Sharma, Manan Mehta\n\n*   **è§‚ç‚¹**ï¼šå¯¹äº Agent å·¥ä½œè´Ÿè½½ï¼ˆå—é™äº Schema å’Œ APIï¼‰ï¼Œ1-12B å‚æ•°çš„ SLMï¼ˆå°æ¨¡å‹ï¼‰é€šå¸¸ä¼˜äºå¤§æ¨¡å‹ã€‚\n*   **æŒ‡å—**ï¼šæœ¬æ–‡æ€»ç»“äº† Phi-4, Qwen-2.5, Llama-3.2 ç­‰å°æ¨¡å‹åœ¨ Agent ä¸­çš„åº”ç”¨ï¼Œå¹¶ç»™å‡ºäº†å·¥ç¨‹è®¾è®¡æ¨¡å¼ï¼šSchema ä¼˜å…ˆçš„ Promptingï¼Œç±»å‹å®‰å…¨çš„å‡½æ•°æ³¨å†Œè¡¨ï¼Œä»¥åŠéªŒè¯å™¨ä¼˜å…ˆçš„å·¥å…·æ‰§è¡Œã€‚\n\n**8. ACToRï¼šå¯¹æŠ—æ€§ Agent åä½œå®ç° C åˆ° Rust çš„ç¿»è¯‘**\n**# title: Adversarial Agent Collaboration for C to Rust Translation**\n*   **ä»»åŠ¡**ï¼šå°†é—ç•™çš„ C ä»£ç è½¬ä¸ºå†…å­˜å®‰å…¨çš„ Rustã€‚\n*   **æ–¹æ³•**ï¼šå— GAN å¯å‘ï¼Œè®¾è®¡äº†ä¸¤ä¸ª Agentã€‚ä¸€ä¸ªâ€œç”Ÿæˆå™¨ Agentâ€å†™ä»£ç è¿‡æµ‹è¯•ï¼Œä¸€ä¸ªâ€œåˆ¤åˆ«å™¨ Agentâ€ä¸“é—¨æ‰¾æ–°çš„å¤±è´¥æµ‹è¯•ç”¨ä¾‹ã€‚\n*   **æ•ˆæœ**ï¼šåœ¨ 63 ä¸ªçœŸå®å‘½ä»¤è¡Œå·¥å…·ï¼ˆå¹³å‡ 473 è¡Œä»£ç ï¼‰ä¸Šå®ç°äº† 90% çš„æµ‹è¯•é€šè¿‡ç‡ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚\n\n**9. EvoEngineerï¼šç”¨ LLM æŒæ¡è‡ªåŠ¨åŒ– CUDA Kernel ä»£ç è¿›åŒ–**\n**# title: EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models**\n*   **æŒ‘æˆ˜**ï¼šCUDA ä¼˜åŒ–æéš¾ï¼Œä¸”å¯¹æ­£ç¡®æ€§è¦æ±‚æé«˜ã€‚\n*   **æ–¹æ³•**ï¼šå»ºç«‹äº†ä¸€ä¸ªåä¸º EvoEngineer çš„ä»£ç è¿›åŒ–æ¡†æ¶ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ­£ç¡®æ€§ã€‚\n*   **ç»“æœ**ï¼šåœ¨ 91 ä¸ªçœŸå® CUDA kernel ä¸Šï¼Œè·å¾—äº†å¹³å‡ 2.72 å€çš„åŠ é€Ÿï¼Œä¸”ä»£ç æœ‰æ•ˆç‡æ¥è¿‘ 70%ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸éšç§\n\n**10. ä½ è¢« LaTeXpOsEd äº†ï¼šarXiv é¢„å°æœ¬æºç ä¿¡æ¯æ³„éœ²çš„ç³»ç»Ÿæ€§åˆ†æ**\n**# title: You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models**\n> **Authors:** Richard A. Dubniczky, et al.\n\n*   **è­¦é’Ÿé•¿é¸£**ï¼šarXiv æä¾›æºç ï¼ˆLaTeXï¼‰ä¸‹è½½ã€‚ä½œè€…å®¡è®¡äº† 10 ä¸‡ä»½æäº¤ï¼Œå‘ç°äº†å¤§é‡éšç§æ³„éœ²ã€‚\n*   **æ³„éœ²å†…å®¹**ï¼šPIIï¼ˆä¸ªäººä¿¡æ¯ï¼‰ã€å¸¦ GPS çš„åŸå§‹å›¾ç‰‡ã€å…¬å¼€çš„ Google Drive/Dropbox é“¾æ¥ã€**å¯ç¼–è¾‘**çš„ SharePoint é“¾æ¥ã€GitHub/Google å‡­è¯ã€API å¯†é’¥ï¼Œç”šè‡³æ˜¯ä½œè€…é—´çš„ç§å¯†åæ§½å’Œä¼šè®®æŠ•ç¨¿è´¦å·ã€‚\n*   **å»ºè®®**ï¼šä¸Šä¼  arXiv å‰ï¼Œè¯·åŠ¡å¿…æ¸…ç†ä½ çš„ LaTeX æºç å’Œæ³¨é‡Šï¼\n\n**11. å½©è™¹å¡«å……ï¼šç¼“è§£æŒ‡ä»¤å¾®è°ƒæ‰©æ•£ LLM ä¸­çš„æå‰ç»ˆæ­¢é—®é¢˜**\n**# title: Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs**\n*   **é—®é¢˜**ï¼šæ‰©æ•£å¤§æ¨¡å‹ (Diffusion LLMs) åœ¨å¤„ç†é•¿åºåˆ—æ—¶ï¼Œå€¾å‘äºç”Ÿæˆå¤§é‡çš„ `<eos>`ï¼ˆç»“æŸç¬¦ï¼‰å¯¼è‡´è¾“å‡ºè¿‡çŸ­æˆ–å´©æºƒã€‚\n*   **åŸå› **ï¼š`<eos>` æ—¢ä½œä¸ºç»“æŸç¬¦åˆä½œä¸ºå¡«å……ç¬¦ (Padding)ï¼Œå¯¼è‡´æ¦‚ç‡é›†ä¸­ã€‚\n*   **æ–¹æ³•**ï¼šRainbow Paddingã€‚ä½¿ç”¨å¾ªç¯çš„ä¸åŒ Token æ¥ä»£æ›¿å•ä¸€çš„ Paddingï¼Œæ‰“ç ´æ¦‚ç‡é›†ä¸­ã€‚ç®€å•æœ‰æ•ˆã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€\n\n**12. ç¨€æœ‰çš„æ–‡æœ¬è¯­ä¹‰ä¸€ç›´è—åœ¨ä½ çš„æ‰©æ•£ Transformer é‡Œ**\n**# title: Rare Text Semantics Were Always There in Your Diffusion Transformer**\n*   **å‘ç°**ï¼šç°åœ¨çš„æ–‡ç”Ÿå›¾æ¨¡å‹ï¼ˆå¦‚ MM-DiTï¼‰å¾ˆéš¾ç”Ÿæˆç½•è§çš„ Prompt æ¦‚å¿µã€‚\n*   **Trick**ï¼šä½œè€…å‘ç°ä¸éœ€è¦å¾®è°ƒï¼Œåªéœ€è¦åœ¨ Joint-Attention æ¨¡å—ä¹‹å‰ï¼Œæ•°å­¦ä¸Šæ‰©å¤§æ–‡æœ¬ Token Embedding çš„æ–¹å·®ï¼ˆVariance Scale-upï¼‰ï¼Œé‚£äº›éšè—çš„ç½•è§è¯­ä¹‰å°±ä¼šåœ¨å›¾åƒä¸­æ˜¾ç°å‡ºæ¥ã€‚\n\n**13. EmbodiSwapï¼šç”¨äºé›¶æ ·æœ¬æœºå™¨äººæ¨¡ä»¿å­¦ä¹ **\n**# title: EmbodiSwap for Zero-Shot Robot Imitation Learning**\n*   **æ–¹æ³•**ï¼šå°†äººç±»è§†é¢‘ä¸­çš„äººâ€œæ¢çš®â€æˆæœºå™¨äººï¼ˆç”Ÿæˆé€¼çœŸçš„åˆæˆè¦†ç›–å±‚ï¼‰ï¼Œç„¶åç”¨è¿™äº›åˆæˆè§†é¢‘è®­ç»ƒæœºå™¨äººç­–ç•¥ã€‚\n*   **æŠ€æœ¯æ ˆ**ï¼šåˆ©ç”¨äº† V-JEPA ä½œä¸ºè§†è§‰éª¨å¹²ï¼ˆé€šå¸¸ç”¨äºè§†é¢‘ç†è§£ï¼Œè¿™é‡Œç”¨äºæ¨¡ä»¿å­¦ä¹ ï¼‰ï¼Œæ•ˆæœä¼˜äºä¼ ç»Ÿçš„æœºå™¨äººè§†è§‰æ¨¡å‹ã€‚\n\n---\n\n### âš—ï¸ ç§‘å­¦ä¸å…¶ä»–æœ‰è¶£ç ”ç©¶\n\n**14. ç¥ç»è´å¶æ–¯æ»¤æ³¢**\n**# title: Neural Bayesian Filtering**\n> **Authors:** Christopher Solinas, ..., Martin Schmid, Michael Bowling (DeepMind / Alberta)\n\n*   **æ–¹æ³•**ï¼šNBF (Neural Bayesian Filtering)ã€‚å°†è´å¶æ–¯æ»¤æ³¢ï¼ˆå¤„ç†éƒ¨åˆ†å¯è§‚æµ‹ç³»ç»Ÿçš„çŠ¶æ€ä¼°è®¡ï¼‰ä¸æ·±åº¦ç”Ÿæˆæ¨¡å‹ç»“åˆã€‚\n*   **æœºåˆ¶**ï¼šå°†â€œä¿¡å¿µ (Beliefs)â€æ˜ å°„åˆ°å›ºå®šé•¿åº¦çš„ Embedding å‘é‡ï¼Œåœ¨æ­¤ç©ºé—´å†…è¿›è¡Œæ›´æ–°ã€‚ç»“åˆäº†ç»å…¸æ»¤æ³¢çš„é«˜æ•ˆæ€§å’Œæ·±åº¦å­¦ä¹ çš„è¡¨è¾¾åŠ›ã€‚\n\n**15. Rezwanï¼šåˆ©ç”¨ LLM å¤„ç†æµ·é‡åœ£è®­æ–‡æœ¬**\n**# title: Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development**\n*   **åº”ç”¨**ï¼šæ•°å­—äººæ–‡ã€‚åˆ©ç”¨ LLM å¤„ç†äº† 120 ä¸‡æ¡ä¼Šæ–¯å…°åœ£è®­ï¼ˆHadithï¼‰ï¼Œå®Œæˆäº†åˆ†æ®µã€ä¼ æ’­é“¾åˆ†ç¦»ã€ç¿»è¯‘ï¼ˆ12ç§è¯­è¨€ï¼‰ã€æ‘˜è¦å’Œæ‰“æ ‡ç­¾ã€‚\n*   **æ„ä¹‰**ï¼šå±•ç¤ºäº† AI å¦‚ä½•ä»¥æä½çš„æˆæœ¬ï¼ˆç›¸æ¯”äººå·¥ 22.9ä¸‡å°æ—¶ï¼‰å®Œæˆå¤§è§„æ¨¡å®—æ•™/å†å²æ–‡æœ¬çš„ç»“æ„åŒ–å¤„ç†ã€‚",
  "papers": [
    {
      "arxiv_id": "2510.03971v1",
      "title": "What Can You Do When You Have Zero Rewards During RL?",
      "title_zh": "å¼ºåŒ–å­¦ä¹ ä¸­é­é‡é›¶å¥–åŠ±æ—¶è¯¥å¦‚ä½•åº”å¯¹ï¼Ÿ",
      "authors": [
        "Jatin Prakash",
        "Anirudh Buvanesh"
      ],
      "abstract": "Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task introduced in Bachmann et al. (2024) and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research at: https://github.com/rl4reasoning/rl-baselines",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)åœ¨å¤§è¯­è¨€æ¨¡å‹(LLMs)æ¨ç†ä»»åŠ¡ä¸­é¢ä¸´çš„â€œé›¶å¥–åŠ±éšœç¢â€(zero-reward barrier)ï¼Œå³å½“åŸºç¡€æ¨¡å‹æ— æ³•é‡‡æ ·åˆ°æ­£ç¡®è§£æ—¶è®­ç»ƒå› é›¶æ¢¯åº¦è€Œåœæ»çš„é—®é¢˜ã€‚ç ”ç©¶è€…åˆ©ç”¨å›¾æœç´¢ä»»åŠ¡(graph search task)è¯„ä¼°äº†å¯†é›†å¥–åŠ±(dense rewards)ã€å¤šæ ·æ€§æ¿€åŠ±(diversity incentives)åŠä¿¡ç”¨åˆ†é…(credit assignment)ç­‰å¤šç§æ–¹æ³•ï¼Œå‘ç°è‹¥åˆå§‹é‡‡æ ·æˆåŠŸç‡ä¸ºé›¶ï¼Œè¿™äº›ç®—æ³•å±‚é¢çš„æ”¹è¿›å‡æ— æ³•çªç ´è¯¥éšœç¢ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•çš„ä»¥æ•°æ®ä¸ºä¸­å¿ƒ(data-centric)çš„å¹²é¢„æ‰‹æ®µï¼Œé€šè¿‡åœ¨è®­ç»ƒé›†ä¸­å¼•å…¥è¾ƒæ˜“æ ·æœ¬ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹RLç®—æ³•çš„å‰æä¸‹æœ€ç»ˆæ”»å…‹åŸå§‹éš¾é¢˜ã€‚å®éªŒè¯¦ç»†åˆ†æäº†å„åŸºçº¿æ–¹æ³•çš„å¤±æ•ˆæ¨¡å¼ï¼Œå¹¶å¼€æºäº†ç›¸å…³ä»£ç å®ç°ä»¥æ”¯æŒåç»­ç ”ç©¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03971v1",
      "published_date": "2025-10-04 23:10:38 UTC",
      "updated_date": "2025-10-04 23:10:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:03.548202+00:00"
    },
    {
      "arxiv_id": "2510.03970v1",
      "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning",
      "title_zh": "è¿ˆå‘ç¢³æ„ŸçŸ¥å®¹å™¨ç¼–æ’ï¼šåŸºäºè”é‚¦å­¦ä¹ çš„å·¥ä½œè´Ÿè½½èƒ½è€—é¢„æµ‹",
      "authors": [
        "Zainab Saad",
        "Jialin Yang",
        "Henry Leung",
        "Steve Drew"
      ],
      "abstract": "The growing reliance on large-scale data centers to run resource-intensive workloads has significantly increased the global carbon footprint, underscoring the need for sustainable computing solutions. While container orchestration platforms like Kubernetes help optimize workload scheduling to reduce carbon emissions, existing methods often depend on centralized machine learning models that raise privacy concerns and struggle to generalize across diverse environments. In this paper, we propose a federated learning approach for energy consumption prediction that preserves data privacy by keeping sensitive operational data within individual enterprises. By extending the Kubernetes Efficient Power Level Exporter (Kepler), our framework trains XGBoost models collaboratively across distributed clients using Flower's FedXgbBagging aggregation using a bagging strategy, eliminating the need for centralized data sharing. Experimental results on the SPECPower benchmark dataset show that our FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a centralized baseline. This work addresses the unresolved trade-off between data privacy and energy prediction efficiency in prior systems such as Kepler and CASPER and offers enterprises a viable pathway toward sustainable cloud computing without compromising operational privacy.",
      "tldr_zh": "éšç€å¤§è§„æ¨¡æ•°æ®ä¸­å¿ƒèµ„æºå¯†é›†å‹å·¥ä½œè´Ÿè½½å¼•å‘çš„ç¢³è¶³è¿¹é—®é¢˜æ—¥ç›Šä¸¥é‡ï¼Œç°æœ‰çš„å®¹å™¨ç¼–æ’ä¼˜åŒ–æ–¹æ³•å¸¸å› ä¾èµ–ä¸­å¿ƒåŒ–æœºå™¨å­¦ä¹ è€Œé¢ä¸´éšç§é£é™©å’Œæ³›åŒ–éš¾é¢˜ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Federated Learning (FL) çš„å·¥ä½œè´Ÿè½½èƒ½è€—é¢„æµ‹æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†æ•æ„Ÿè¿è¥æ•°æ®ä¿ç•™åœ¨å„ä¼ä¸šå†…éƒ¨æ¥ç¡®ä¿æ•°æ®éšç§ã€‚è¯¥æ¡†æ¶æ‰©å±•äº† Kubernetes Efficient Power Level Exporter (Kepler)ï¼Œåˆ©ç”¨ Flower çš„ FedXgbBagging èšåˆç­–ç•¥åœ¨åˆ†å¸ƒå¼å®¢æˆ·ç«¯ååŒè®­ç»ƒ XGBoost æ¨¡å‹ï¼Œä»è€Œæ¶ˆé™¤äº†ä¸­å¿ƒåŒ–æ•°æ®å…±äº«çš„éœ€æ±‚ã€‚åœ¨ SPECPower åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è”é‚¦å­¦ä¹ æ–¹æ³•æ¯”ä¸­å¿ƒåŒ–åŸºçº¿æ¨¡å‹çš„ Mean Absolute Error (MAE) é™ä½äº† 11.7%ã€‚è¿™é¡¹å·¥ä½œæœ‰æ•ˆè§£å†³äº†å…ˆå‰ç³»ç»Ÿä¸­æ•°æ®éšç§ä¸èƒ½è€—é¢„æµ‹æ•ˆç‡ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œä¸ºä¼ä¸šåœ¨ä¸ç‰ºç‰²è¿è¥éšç§çš„å‰æä¸‹å®ç°å¯æŒç»­äº‘è®¡ç®—æä¾›äº†å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to 2025 IEEE Smart World Congress (SWC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03970v1",
      "published_date": "2025-10-04 23:01:59 UTC",
      "updated_date": "2025-10-04 23:01:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:01.856778+00:00"
    },
    {
      "arxiv_id": "2510.03969v1",
      "title": "Quantifying Risks in Multi-turn Conversation with Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¤šè½®å¯¹è¯ä¸­çš„é£é™©é‡åŒ–",
      "authors": [
        "Chengxiao Wang",
        "Isha Chaudhary",
        "Qian Hu",
        "Weitong Ruan",
        "Rahul Gupta",
        "Gagandeep Singh"
      ],
      "abstract": "Large Language Models (LLMs) can produce catastrophic responses in conversational settings that pose serious risks to public safety and security. Existing evaluations often fail to fully reveal these vulnerabilities because they rely on fixed attack prompt sequences, lack statistical guarantees, and do not scale to the vast space of multi-turn conversations. In this work, we propose QRLLM, a novel, principled Certification framework for Catastrophic risks in multi-turn Conversation for LLMs that bounds the probability of an LLM generating catastrophic responses under multi-turn conversation distributions with statistical guarantees. We model multi-turn conversations as probability distributions over query sequences, represented by a Markov process on a query graph whose edges encode semantic similarity to capture realistic conversational flow, and quantify catastrophic risks using confidence intervals. We define several inexpensive and practical distributions: random node, graph path, adaptive with rejection. Our results demonstrate that these distributions can reveal substantial catastrophic risks in frontier models, with certified lower bounds as high as 70\\% for the worst model, highlighting the urgent need for improved safety training strategies in frontier LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†QRLLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¤šè½®å¯¹è¯ä¸­ç¾éš¾æ€§é£é™©çš„æ–°å‹è®¤è¯æ¡†æ¶(Certification framework)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•åœ¨ç»Ÿè®¡ä¿è¯å’Œæ‰©å±•æ€§æ–¹é¢çš„ä¸è¶³ã€‚ç ”ç©¶è€…å°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæŸ¥è¯¢å›¾ä¸Šçš„é©¬å°”å¯å¤«è¿‡ç¨‹(Markov process)ï¼Œé€šè¿‡ç¼–ç è¯­ä¹‰ç›¸ä¼¼åº¦çš„è¾¹æ¥æ•æ‰çœŸå®çš„å¯¹è¯æµï¼Œå¹¶åˆ©ç”¨ç½®ä¿¡åŒºé—´(confidence intervals)é‡åŒ–æ¨¡å‹ç”Ÿæˆç¾éš¾æ€§å“åº”çš„æ¦‚ç‡ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†éšæœºèŠ‚ç‚¹ã€å›¾è·¯å¾„åŠå¸¦æ‹’ç»çš„è‡ªé€‚åº”(adaptive with rejection)ç­‰å¤šç§å®ç”¨åˆ†å¸ƒï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºå‰æ²¿æ¨¡å‹æ½œåœ¨çš„å®‰å…¨æ¼æ´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‰æ²¿æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä¸­å­˜åœ¨æ˜¾è‘—çš„ç¾éš¾æ€§é£é™©ï¼Œéƒ¨åˆ†æ¨¡å‹çš„é£é™©è®¤è¯ä¸‹ç•Œé«˜è¾¾70%ã€‚è¿™ä¸€å‘ç°å‡¸æ˜¾äº†åœ¨å‰æ²¿LLMsä¸­æ”¹è¿›å®‰å…¨è®­ç»ƒç­–ç•¥çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03969v1",
      "published_date": "2025-10-04 23:00:40 UTC",
      "updated_date": "2025-10-04 23:00:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:06.745794+00:00"
    },
    {
      "arxiv_id": "2510.03962v1",
      "title": "SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data",
      "title_zh": "SPEARï¼šåŸºäºè½¯æç¤ºå¢å¼ºçš„æ—¶é—´åºåˆ—æ•°æ®å¼‚å¸¸è¯†åˆ«",
      "authors": [
        "Hanzhe Wei",
        "Jiajun Wu",
        "Jialin Yang",
        "Henry Leung",
        "Steve Drew"
      ],
      "abstract": "Time series anomaly detection plays a crucial role in a wide range of fields, such as healthcare and internet traffic monitoring. The emergence of large language models (LLMs) offers new opportunities for detecting anomalies in the ubiquitous time series data. Traditional approaches struggle with variable-length time series sequences and context-based anomalies. We propose Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage LLMs for anomaly detection with soft prompts and quantization. Our methodology involves quantizing and transforming the time series data into input embeddings and combining them with learnable soft prompt embeddings. These combined embeddings are then fed into a frozen LLM. The soft prompts are updated iteratively based on a cross-entropy loss, allowing the model to adapt to time series anomaly detection. The use of soft prompts helps adapt LLMs effectively to time series tasks, while quantization ensures optimal handling of sequences, as LLMs are designed to handle discrete sequences. Our experimental results demonstrate that soft prompts effectively increase LLMs' performance in downstream tasks regarding time series anomaly detection.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SPEAR (Soft Prompt Enhanced Anomaly Recognition)ï¼Œä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) è¿›è¡Œæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹çš„æ–°æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å˜é•¿åºåˆ—å’ŒåŸºäºä¸Šä¸‹æ–‡çš„å¼‚å¸¸æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒSPEAR ç»“åˆäº† Soft Prompt æŠ€æœ¯ä¸ Quantization ç­–ç•¥ã€‚è¯¥æ–¹æ³•å°†æ—¶é—´åºåˆ—æ•°æ®é‡åŒ–å¹¶è½¬åŒ–ä¸º Embeddingsï¼Œä¸å¯å­¦ä¹ çš„ Soft Prompt ç»“åˆåè¾“å…¥åˆ° Frozen LLM ä¸­ã€‚é€šè¿‡ Cross-Entropy Loss è¿­ä»£æ›´æ–° Soft Promptï¼Œä½¿é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿç²¾å‡†é€‚åº”å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚Quantization ç¡®ä¿äº† LLMs èƒ½å¤Ÿä»¥å…¶æ“…é•¿çš„ç¦»æ•£åºåˆ—æ–¹å¼å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSoft Prompt æ˜¾è‘—å¢å¼ºäº† LLMs åœ¨æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸ºè¯¥é¢†åŸŸçš„è·¨æ¨¡æ€åº”ç”¨æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to 2025 IEEE International Conference on Autonomous and Trusted Computing (ATC 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03962v1",
      "published_date": "2025-10-04 22:20:48 UTC",
      "updated_date": "2025-10-04 22:20:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:07.551567+00:00"
    },
    {
      "arxiv_id": "2510.03952v1",
      "title": "Strategy Logic, Imperfect Information, and Hyperproperties",
      "title_zh": "ç­–ç•¥é€»è¾‘ã€ä¸å®Œå…¨ä¿¡æ¯ä¸è¶…å±æ€§",
      "authors": [
        "Raven Beutner",
        "Bernd Finkbeiner"
      ],
      "abstract": "Strategy logic (SL) is a powerful temporal logic that enables first-class reasoning over strategic behavior in multi-agent systems (MAS). In many MASs, the agents (and their strategies) cannot observe the global state of the system, leading to many extensions of SL centered around imperfect information, such as strategy logic with imperfect information (SL$_\\mathit{ii}$). Along orthogonal lines, researchers have studied the combination of strategic behavior and hyperproperties. Hyperproperties are system properties that relate multiple executions in a system and commonly arise when specifying security policies. Hyper Strategy Logic (HyperSL) is a temporal logic that combines quantification over strategies with the ability to express hyperproperties on the executions of different strategy profiles. In this paper, we study the relation between SL$_\\mathit{ii}$ and HyperSL. Our main result is that both logics (restricted to formulas where no state formulas are nested within path formulas) are equivalent in the sense that we can encode SL$_\\mathit{ii}$ instances into HyperSL instances and vice versa. For the former direction, we build on the well-known observation that imperfect information is a hyperproperty. For the latter direction, we construct a self-composition of MASs and show how we can simulate hyperproperties using imperfect information.",
      "tldr_zh": "æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†ç­–ç•¥é€»è¾‘ï¼ˆStrategy Logic, SLï¼‰åœ¨å¤„ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­ä¸å®Œå…¨ä¿¡æ¯ï¼ˆImperfect Informationï¼‰ä¸è¶…å±æ€§ï¼ˆHyperpropertiesï¼‰ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºè¯æ˜äº†å—é™å½¢å¼ä¸‹çš„ä¸å®Œå…¨ä¿¡æ¯ç­–ç•¥é€»è¾‘ï¼ˆSL_iiï¼‰ä¸è¶…ç­–ç•¥é€»è¾‘ï¼ˆHyperSLï¼‰åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šæ˜¯ç­‰ä»·çš„ï¼Œå¹¶è¯¦ç»†ç»™å‡ºäº†åŒå‘çš„ç¼–ç è½¬æ¢æ–¹æ³•ã€‚ç ”ç©¶è€…é¦–å…ˆåŸºäºâ€œä¸å®Œå…¨ä¿¡æ¯æœ¬è´¨ä¸Šæ˜¯ä¸€ç§è¶…å±æ€§â€çš„å­¦æœ¯è§‚å¯Ÿï¼ŒæˆåŠŸå°† SL_ii å®ä¾‹ç¼–ç ä¸º HyperSL å®ä¾‹ã€‚éšåï¼Œé€šè¿‡æ„å»ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è‡ªç»„åˆï¼ˆself-compositionï¼‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨ä¸å®Œå…¨ä¿¡æ¯æ¥æ¨¡æ‹Ÿè¶…å±æ€§ï¼Œä»è€Œå®ç°äº†ä» HyperSL åˆ° SL_ii çš„åå‘ç¼–ç ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æˆ˜ç•¥æ¨ç†ä¸­ä¿¡æ¯å±€é™æ€§ä¸ç³»ç»Ÿæ‰§è¡Œå…³è”æ€§ä¹‹é—´çš„æ·±å±‚ç­‰ä»·å…³ç³»ï¼Œä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„é€»è¾‘å½¢å¼åŒ–åˆ†ææä¾›äº†ç»Ÿä¸€è§†è§’ã€‚",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LO",
      "comment": "KR 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03952v1",
      "published_date": "2025-10-04 21:37:14 UTC",
      "updated_date": "2025-10-04 21:37:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:37.156476+00:00"
    },
    {
      "arxiv_id": "2510.03930v1",
      "title": "LLM Chemistry Estimation for Multi-LLM Recommendation",
      "title_zh": "é¢å‘å¤š LLM æ¨èçš„ LLM åŒ–å­¦æ•ˆåº”è¯„ä¼°",
      "authors": [
        "Huascar Sanchez",
        "Briland Hitaj"
      ],
      "abstract": "Multi-LLM collaboration promises accurate, robust, and context-aware solutions, yet existing approaches rely on implicit selection and output assessment without analyzing whether collaborating models truly complement or conflict. We introduce LLM Chemistry -- a framework that measures when LLM combinations exhibit synergistic or antagonistic behaviors that shape collective performance beyond individual capabilities. We formalize the notion of chemistry among LLMs, propose algorithms that quantify it by analyzing interaction dependencies, and recommend optimal model ensembles accordingly. Our theoretical analysis shows that chemistry among collaborating LLMs is most evident under heterogeneous model profiles, with its outcome impact shaped by task type, group size, and complexity. Evaluation on classification, summarization, and program repair tasks provides initial evidence for these task-dependent effects, thereby reinforcing our theoretical results. This establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and a foundation for ensemble recommendation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LLM Chemistryæ¡†æ¶ï¼Œæ—¨åœ¨é‡åŒ–å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åä½œæ—¶è¡¨ç°å‡ºçš„ååŒï¼ˆsynergisticï¼‰æˆ–æ‹®æŠ—ï¼ˆantagonisticï¼‰è¡Œä¸ºï¼Œä»è€Œä¼˜åŒ–æ¨¡å‹é›†æˆæ•ˆæœã€‚é’ˆå¯¹ç°æœ‰åä½œæ–¹æ³•ä»…ä¾èµ–éšå¼é€‰æ‹©è€Œç¼ºä¹å¯¹æ¨¡å‹é—´äº’è¡¥æˆ–å†²çªæ·±åº¦åˆ†æçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡åˆ†æäº¤äº’ä¾èµ–æ€§æ¥è¡¡é‡LLMsä¹‹é—´çš„â€œåŒ–å­¦ååº”ï¼ˆchemistryï¼‰â€ï¼Œå¹¶æ®æ­¤æ¨èæœ€ä¼˜çš„æ¨¡å‹ç»„åˆã€‚ç†è®ºåˆ†ææŒ‡å‡ºï¼Œè¿™ç§åŒ–å­¦ååº”åœ¨å¼‚æ„æ¨¡å‹ï¼ˆheterogeneous model profilesï¼‰é…ç½®ä¸‹æœ€ä¸ºæ˜¾è‘—ï¼Œä¸”å…¶å½±å“å—ä»»åŠ¡ç±»å‹ã€ç¾¤ä½“è§„æ¨¡å’Œå¤æ‚åº¦çš„å…±åŒåˆ¶çº¦ã€‚é€šè¿‡åœ¨åˆ†ç±»ã€æ‘˜è¦å’Œç¨‹åºä¿®å¤ä»»åŠ¡ä¸Šçš„å®éªŒè¯„ä¼°ï¼Œç ”ç©¶éªŒè¯äº†LLM Chemistryä½œä¸ºå¤šLLMç³»ç»Ÿè¯Šæ–­å› ç´ çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºè‡ªåŠ¨åŒ–æ¨¡å‹æ¨èå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 5 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03930v1",
      "published_date": "2025-10-04 20:21:39 UTC",
      "updated_date": "2025-10-04 20:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:15.655487+00:00"
    },
    {
      "arxiv_id": "2510.08592v1",
      "title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models",
      "title_zh": "å¤šæ ·æ€§æ„ˆä½ï¼Œå®‰å…¨æ€§æ„ˆå·®ï¼šå¤§è¯­è¨€æ¨¡å‹æµ‹è¯•æ—¶ç¼©æ”¾ä¸­é—´æ¥è€Œæ™®éçš„é£é™©",
      "authors": [
        "Shahriar Kabir Nahin",
        "Hadi Askari",
        "Muhao Chen",
        "Anshuman Chhabra"
      ],
      "abstract": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple candidate responses and then operating over this set to find the best output. A tacit premise behind TTS is that sufficiently diverse candidate pools enhance reliability. In this work, we show that this assumption in TTS introduces a previously unrecognized failure mode. When candidate diversity is curtailed, even by a modest amount, TTS becomes much more likely to produce unsafe outputs. We present a reference-guided diversity reduction protocol (RefDiv) that serves as a diagnostic attack to stress test TTS pipelines. Through extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1, Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and Best-of-N), constraining diversity consistently signifies the rate at which TTS produces unsafe results. The effect is often stronger than that produced by prompts directly with high adversarial intent scores. This observed phenomenon also transfers across TTS strategies and to closed-source models (e.g. OpenAI o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant property of TTS rather than a model-specific artifact. Additionally, we find that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and OpenAI Moderation API), are unable to flag the adversarial input prompts generated by RefDiv, demonstrating that existing defenses offer limited protection against this diversity-driven failure mode. Through this work, we hope to motivate future research on designing robust TTS strategies that are both effective and secure against diversity-targeted stress tests as illustrated by RefDiv.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­æ¨ç†é˜¶æ®µæ‰©å±•(Test-Time Scaling, TTS)æ½œåœ¨çš„å®‰å…¨é£é™©ï¼ŒæŒ‡å‡ºå€™é€‰å“åº”æ± å¤šæ ·æ€§çš„é™ä½ä¼šå¯¼è‡´æ¨¡å‹æ›´å®¹æ˜“äº§ç”Ÿä¸å®‰å…¨è¾“å‡ºã€‚ä¸ºäº†é‡åŒ–è¿™ä¸€é£é™©ï¼Œä½œè€…æå‡ºäº† RefDiv (reference-guided diversity reduction protocol) è¯Šæ–­æ€§æ”»å‡»åè®®ï¼Œå¯¹å¤šç§ TTS æµæ°´çº¿è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚å®éªŒè¦†ç›–äº†åŒ…æ‹¬ Qwen3ã€Llama3.1 ä»¥åŠ OpenAI o3 å’Œ Gemini-2.5-Pro åœ¨å†…çš„å¤šä¸ªå¼€æºä¸é—­æºæ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå¤šæ ·æ€§å—é™å¯¹å®‰å…¨æ€§çš„å¨èƒåœ¨ä¸åŒæ¨¡å‹å’Œ TTS ç­–ç•¥ï¼ˆå¦‚ Monte Carlo Tree Search å’Œ Best-of-Nï¼‰ä¸­å…·æœ‰æ™®éæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§å¤šæ ·æ€§é©±åŠ¨çš„å¤±æ•ˆæ¨¡å¼å…¶ç ´ååŠ›å¾€å¾€å¼ºäºç›´æ¥çš„å¯¹æŠ—æ€§æç¤ºè¯ï¼Œä¸”ç°æœ‰çš„å®‰å…¨é˜²å¾¡å·¥å…·å¦‚ Llama-Guard å’Œ OpenAI Moderation API å‡éš¾ä»¥æœ‰æ•ˆè¯†åˆ«ç›¸å…³é£é™©ã€‚è¯¥å·¥ä½œæ­ç¤ºäº† TTS æœºåˆ¶ä¸­ä¸€ä¸ªæ­¤å‰æœªè¢«å¯Ÿè§‰çš„éšè”½å¤±æ•ˆæ¨¡å¼ï¼Œå¼ºè°ƒäº†åœ¨è¿½æ±‚æ¨ç†æ€§èƒ½æ‰©å±•çš„åŒæ—¶ï¼Œå¿…é¡»é‡æ–°è¯„ä¼°å¹¶å¢å¼ºå…¶é’ˆå¯¹å¤šæ ·æ€§æ”»å‡»çš„å®‰å…¨æ€§è®¾è®¡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.08592v1",
      "published_date": "2025-10-04 20:01:21 UTC",
      "updated_date": "2025-10-04 20:01:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:20.657899+00:00"
    },
    {
      "arxiv_id": "2510.03923v1",
      "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks",
      "title_zh": "è®ºè¿ç»­æ·±åº¦å›¾ç¥ç»ç½‘ç»œçš„æ”¶æ•›æ€§ä¸è§„æ¨¡å¯è¿ç§»æ€§",
      "authors": [
        "Mingsong Yan",
        "Charles Kulick",
        "Sui Tang"
      ],
      "abstract": "Continuous-depth graph neural networks, also known as Graph Neural Differential Equations (GNDEs), combine the structural inductive bias of Graph Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs, offering a scalable and principled framework for modeling dynamics on graphs. In this paper, we present a rigorous convergence analysis of GNDEs with time-varying parameters in the infinite-node limit, providing theoretical insights into their size transferability. To this end, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of GNDEs and establish their well-posedness. Leveraging tools from graphon theory and dynamical systems, we prove the trajectory-wise convergence of GNDE solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence rates under two deterministic graph sampling regimes: (1) weighted graphs sampled from smooth graphons, and (2) unweighted graphs sampled from $\\{0,1\\}$-valued (discontinuous) graphons. We further establish size transferability bounds, providing theoretical justification for the practical strategy of transferring GNDE models trained on moderate-sized graphs to larger, structurally similar graphs without retraining. Numerical experiments using synthetic and real data support our theoretical findings.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹è¿ç»­æ·±åº¦å›¾ç¥ç»ç½‘ç»œ (Continuous-depth Graph Neural Networks)ï¼Œå³å›¾ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ (Graph Neural Differential Equations, GNDEs) çš„æ”¶æ•›æ€§ä¸å°ºå¯¸å¯è¿ç§»æ€§ (Size Transferability) è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æã€‚ä½œè€…å¼•å…¥äº† Graphon Neural Differential Equations (Graphon-NDEs) ä½œä¸º GNDEs åœ¨æ— é™èŠ‚ç‚¹æé™ä¸‹çš„æ•°å­¦æ¨¡å‹ï¼Œå¹¶ç¡®ç«‹äº†å…¶è‰¯ç½®æ€§ (Well-posedness)ã€‚åˆ©ç”¨ Graphon ç†è®ºå’ŒåŠ¨åŠ›ç³»ç»Ÿåˆ†æå·¥å…·ï¼Œè¯¥å·¥ä½œè¯æ˜äº† GNDE çš„è§£åœ¨è½¨è¿¹ä¸Šæ”¶æ•›äº Graphon-NDE çš„è§£ï¼Œå¹¶æ¨å¯¼å‡ºäº†åœ¨å¹³æ»‘ Graphon é‡‡æ ·å’Œä¸è¿ç»­ Graphon é‡‡æ ·ä¸¤ç§æœºåˆ¶ä¸‹çš„æ˜¾å¼æ”¶æ•›é€Ÿç‡ã€‚ç ”ç©¶è¿›ä¸€æ­¥å»ºç«‹äº†å°ºå¯¸å¯è¿ç§»æ€§ç•Œé™ï¼Œä¸ºåœ¨ä¸è¿›è¡Œé‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å°†ä¸­ç­‰è§„æ¨¡å›¾ä¸Šè®­ç»ƒçš„æ¨¡å‹æ¨å¹¿è‡³ç»“æ„ç›¸ä¼¼çš„å¤§è§„æ¨¡å›¾æä¾›äº†åšå®çš„ç†è®ºä¾æ®ã€‚åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®çš„æ•°å€¼å®éªŒä¸€è‡´éªŒè¯äº†è¯¥ç†è®ºæ¨å¯¼çš„æ­£ç¡®æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03923v1",
      "published_date": "2025-10-04 19:59:21 UTC",
      "updated_date": "2025-10-04 19:59:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:20.257168+00:00"
    },
    {
      "arxiv_id": "2510.05165v1",
      "title": "Domain-Adapted Granger Causality for Real-Time Cross-Slice Attack Attribution in 6G Networks",
      "title_zh": "é¢å‘ 6G ç½‘ç»œå®æ—¶è·¨åˆ‡ç‰‡æ”»å‡»æº¯æºçš„é¢†åŸŸè‡ªé€‚åº”æ ¼å…°æ°å› æœåˆ†æ",
      "authors": [
        "Minh K. Quan",
        "Pubudu N. Pathirana"
      ],
      "abstract": "Cross-slice attack attribution in 6G networks faces the fundamental challenge of distinguishing genuine causal relationships from spurious correlations in shared infrastructure environments. We propose a theoretically-grounded domain-adapted Granger causality framework that integrates statistical causal inference with network-specific resource modeling for real-time attack attribution. Our approach addresses key limitations of existing methods by incorporating resource contention dynamics and providing formal statistical guarantees. Comprehensive evaluation on a production-grade 6G testbed with 1,100 empirically-validated attack scenarios demonstrates 89.2% attribution accuracy with sub-100ms response time, representing a statistically significant 10.1 percentage point improvement over state-of-the-art baselines. The framework provides interpretable causal explanations suitable for autonomous 6G security orchestration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Domain-Adapted Granger Causality æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ 6G ç½‘ç»œä¸­ Cross-Slice Attack Attribution é¢ä¸´çš„åŒºåˆ†çœŸå®å› æœå…³ç³»ä¸è™šå‡å…³è”çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†ç»Ÿè®¡å› æœæ¨ç†ä¸ç‰¹å®šç½‘ç»œçš„èµ„æºå»ºæ¨¡ç›¸ç»“åˆï¼Œé€šè¿‡è€ƒè™‘èµ„æºç«äº‰åŠ¨æ€å¹¶æä¾›æ­£å¼çš„ç»Ÿè®¡ä¿è¯ï¼Œæœ‰æ•ˆå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚åœ¨åŒ…å« 1,100 ä¸ªç»éªŒéªŒè¯æ”»å‡»åœºæ™¯çš„ç”Ÿäº§çº§ 6G æµ‹è¯•åºŠä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº† 89.2% çš„å½’å±å‡†ç¡®ç‡å’Œä½äº 100ms çš„å“åº”æ—¶é—´ï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›åŸºå‡†æå‡äº† 10.1 ä¸ªç™¾åˆ†ç‚¹ã€‚è¯¥æ¡†æ¶ä¸ä»…æä¾›äº†å¯è§£é‡Šçš„å› æœè¯´æ˜ï¼Œè¿˜ä¸ºå®ç°è‡ªä¸»çš„ 6G Security Orchestration å¥ å®šäº†ç†è®ºä¸æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted at NeurIPS 2025 Workshop on CauScien: Uncovering Causality in Science",
      "pdf_url": "https://arxiv.org/pdf/2510.05165v1",
      "published_date": "2025-10-04 19:56:55 UTC",
      "updated_date": "2025-10-04 19:56:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:28.160153+00:00"
    },
    {
      "arxiv_id": "2510.05164v1",
      "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading",
      "title_zh": "SATERï¼šä¸€ç§è‡ªæ„ŸçŸ¥ä¸” Token é«˜æ•ˆçš„è·¯ç”±ä¸çº§è”æ–¹æ³•",
      "authors": [
        "Yuanzhe Shen",
        "Yide Liu",
        "Zisu Huang",
        "Ruicheng Yin",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "abstract": "Large language models (LLMs) demonstrate remarkable performance across diverse tasks, yet their effectiveness frequently depends on costly commercial APIs or cloud services. Model selection thus entails a critical trade-off between performance and cost: high-performing LLMs typically incur substantial expenses, whereas budget-friendly small language models (SLMs) are constrained by limited capabilities. Current research primarily proposes two routing strategies: pre-generation routing and cascade routing. Both approaches have distinct characteristics, with cascade routing typically offering superior cost-effectiveness and accuracy despite its higher latency. To further address the limitations of both approaches, we introduce SATER, a dual-mode compatible approach that fine-tunes models through shortest-response preference optimization and a confidence-aware rejection mechanism. SATER significantly reduces redundant outputs and response times, while improving both the performance of pre-generation routing and the efficiency of cascade routing. Experiments across three SLMs and six datasets, varying in type and complexity, demonstrate that SATER achieves comparable performance while consistently reducing computational costs by over 50\\% and cascade latency by over 80\\%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SATERï¼Œä¸€ç§å…·å¤‡è‡ªæˆ‘æ„ŸçŸ¥å’ŒTokené«˜æ•ˆç‰¹æ€§çš„è·¯ç”±ä¸çº§è”æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„é«˜æˆæœ¬ä¸å°è¯­è¨€æ¨¡å‹(SLMs)æœ‰é™æ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚SATERæ”¯æŒåŒæ¨¡å¼å…¼å®¹ï¼Œé€šè¿‡ç»“åˆæœ€çŸ­å“åº”åå¥½ä¼˜åŒ–(shortest-response preference optimization)å’Œç½®ä¿¡åº¦æ„ŸçŸ¥æ‹’ç»æœºåˆ¶(confidence-aware rejection mechanism)å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†å†—ä½™è¾“å‡ºå¹¶ç¼©çŸ­äº†å“åº”æ—¶é—´ï¼ŒåŒæ—¶æå‡äº†é¢„ç”Ÿæˆè·¯ç”±(pre-generation routing)çš„å‡†ç¡®æ€§ä¸çº§è”è·¯ç”±(cascade routing)çš„è¿è¡Œæ•ˆç‡ã€‚åœ¨å¤šç§SLMså’Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSATERåœ¨ä¿æŒç›¸å½“æ€§èƒ½çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†è®¡ç®—æˆæœ¬é™ä½äº†50%ä»¥ä¸Šï¼Œå¹¶å°†çº§è”å»¶è¿Ÿé™ä½äº†80%ä»¥ä¸Šã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to EMNLP 2025 Main",
      "pdf_url": "https://arxiv.org/pdf/2510.05164v1",
      "published_date": "2025-10-04 19:55:36 UTC",
      "updated_date": "2025-10-04 19:55:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:36.754747+00:00"
    },
    {
      "arxiv_id": "2510.03921v1",
      "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition",
      "title_zh": "Talking Tennisï¼šåŸºäºä¸‰ç»´ç”Ÿç‰©åŠ›å­¦åŠ¨ä½œè¯†åˆ«çš„è¯­è¨€åé¦ˆ",
      "authors": [
        "Arushi Dashore",
        "Aryan Anumala",
        "Emily Hui",
        "Olivia Yang"
      ],
      "abstract": "Automated tennis stroke analysis has advanced significantly with the integration of biomechanical motion cues alongside deep learning techniques, enhancing stroke classification accuracy and player performance evaluation. Despite these advancements, existing systems often fail to connect biomechanical insights with actionable language feedback that is both accessible and meaningful to players and coaches. This research project addresses this gap by developing a novel framework that extracts key biomechanical features (such as joint angles, limb velocities, and kinetic chain patterns) from motion data using Convolutional Neural Network Long Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for relationships influencing stroke effectiveness and injury risk, forming the basis for feedback generation using large language models (LLMs). Leveraging the THETIS dataset and feature extraction techniques, our approach aims to produce feedback that is technically accurate, biomechanically grounded, and actionable for end-users. The experimental setup evaluates this framework on classification performance and interpretability, bridging the gap between explainable AI and sports biomechanics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç½‘çƒå‡»çƒåˆ†æç³»ç»Ÿéš¾ä»¥å°†ç”Ÿç‰©åŠ›å­¦æ•°æ®è½¬åŒ–ä¸ºå¯æ“ä½œåé¦ˆçš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ä¸ªç»“åˆæ·±åº¦å­¦ä¹ ä¸è¯­è¨€ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŸºäº CNN-LSTM çš„æ¨¡å‹ä»è¿åŠ¨æ•°æ®ä¸­æå–è¯¸å¦‚å…³èŠ‚è§’åº¦ã€è‚¢ä½“é€Ÿåº¦å’ŒåŠ¨åŠ›é“¾æ¨¡å¼ç­‰å…³é”® biomechanical featuresï¼Œå¹¶æ·±å…¥åˆ†æè¿™äº›ç‰¹å¾å¯¹å‡»çƒæ•ˆæœåŠå—ä¼¤é£é™©çš„å½±å“ã€‚é€šè¿‡æ•´åˆ Large Language Models (LLMs) ä¸ THETIS æ•°æ®é›†ï¼Œç³»ç»Ÿèƒ½å¤Ÿç”Ÿæˆæ—¢å…·å¤‡ç”Ÿç‰©åŠ›å­¦ä¾æ®åˆå¯¹è¿åŠ¨å‘˜å’Œæ•™ç»ƒå…·æœ‰å®é™…æŒ‡å¯¼æ„ä¹‰çš„è¯­è¨€åé¦ˆã€‚å®éªŒç»“æœè¯„ä¼°äº†è¯¥æ¡†æ¶åœ¨åˆ†ç±»æ€§èƒ½ä¸å¯è§£é‡Šæ€§æ–¹é¢çš„è¡¨ç°ï¼ŒæˆåŠŸå¼¥åˆäº† explainable AI ä¸ä½“è‚²ç”Ÿç‰©åŠ›å­¦ä¹‹é—´çš„åº”ç”¨é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 4 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03921v1",
      "published_date": "2025-10-04 19:55:30 UTC",
      "updated_date": "2025-10-04 19:55:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:52.146597+00:00"
    },
    {
      "arxiv_id": "2510.03914v1",
      "title": "Refactoring with LLMs: Bridging Human Expertise and Machine Understanding",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é‡æ„ï¼šè¡”æ¥äººç±»ä¸“ä¸šçŸ¥è¯†ä¸æœºå™¨ç†è§£",
      "authors": [
        "Yonnel Chen Kuang Piao",
        "Jean Carlors Paul",
        "Leuson Da Silva",
        "Arghavan Moradi Dakhel",
        "Mohammad Hamdaqa",
        "Foutse Khomh"
      ],
      "abstract": "Code refactoring is a fundamental software engineering practice aimed at improving code quality and maintainability. Despite its importance, developers often neglect refactoring due to the significant time, effort, and resources it requires, as well as the lack of immediate functional rewards. Although several automated refactoring tools have been proposed, they remain limited in supporting a broad spectrum of refactoring types. In this study, we explore whether instruction strategies inspired by human best-practice guidelines can enhance the ability of Large Language Models (LLMs) to perform diverse refactoring tasks automatically. Leveraging the instruction-following and code comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design multiple instruction strategies that encode motivations, procedural steps, and transformation objectives for 61 well-known refactoring types. We evaluate these strategies on benchmark examples and real-world code snippets from GitHub projects. Our results show that instruction designs grounded in Fowler's guidelines enable LLMs to successfully perform all benchmark refactoring types and preserve program semantics in real-world settings, an essential criterion for effective refactoring. Moreover, while descriptive instructions are more interpretable to humans, our results show that rule-based instructions often lead to better performance in specific scenarios. Interestingly, allowing models to focus on the overall goal of refactoring, rather than prescribing a fixed transformation type, can yield even greater improvements in code quality.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–ä»£ç é‡æ„ï¼ˆCode refactoringï¼‰çš„æ½œåŠ›ï¼Œæ—¨åœ¨è§£å†³å¼€å‘äººå‘˜å› é‡æ„æˆæœ¬é«˜æ˜‚è€Œå¿½è§†ä»£ç è´¨é‡æ”¹è¿›çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿå€Ÿé‰´ Martin Fowler çš„é‡æ„æŒ‡å—ï¼Œä¸º 61 ç§é‡æ„ç±»å‹è®¾è®¡äº†åŒ…å«åŠ¨æœºã€æ­¥éª¤å’Œè½¬æ¢ç›®æ ‡çš„å¤šç§æŒ‡ä»¤ç­–ç•¥ï¼Œå¹¶åœ¨ GPT-mini å’Œ DeepSeek-V3 ç­‰å…ˆè¿›æ¨¡å‹ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æŒ‡ä»¤ç­–ç•¥ä½¿ LLMs èƒ½å¤ŸæˆåŠŸæ‰§è¡Œæ‰€æœ‰åŸºå‡†é‡æ„ä»»åŠ¡ï¼Œå¹¶åœ¨çœŸå®ä»£ç åœºæ™¯ä¸­æœ‰æ•ˆä¿æŒäº†ç¨‹åºè¯­ä¹‰ï¼ˆProgram semanticsï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶æè¿°æ€§æŒ‡ä»¤æ›´æ˜“äºäººç±»ç†è§£ï¼Œä½†åœ¨ç‰¹å®šåœºæ™¯ä¸‹åŸºäºè§„åˆ™çš„æŒ‡ä»¤å¾€å¾€èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè®©æ¨¡å‹ä¸“æ³¨äºé‡æ„çš„æ•´ä½“ç›®æ ‡è€Œéé¢„è®¾çš„ç‰¹å®šè½¬æ¢ç±»å‹ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æ˜¾è‘—æå‡ä»£ç è´¨é‡ã€‚è¯¥å·¥ä½œæˆåŠŸå¼¥åˆäº†äººç±»ä¸“å®¶ç»éªŒä¸æœºå™¨ç†è§£ä¹‹é—´çš„å·®è·ï¼Œä¸ºå®ç°é«˜æ•ˆã€å¯è§£é‡Šçš„è‡ªåŠ¨åŒ–é‡æ„æä¾›äº†æ–°çš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "43 pages, 2 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03914v1",
      "published_date": "2025-10-04 19:40:42 UTC",
      "updated_date": "2025-10-04 19:40:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:53.045621+00:00"
    },
    {
      "arxiv_id": "2511.00004v1",
      "title": "Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment",
      "title_zh": "åŸºäºå¢å¼ºæŠ€æœ¯çš„è‡ªç„¶ç¾å®³è¯„ä¼°å¤šæ¨¡æ€å­¦ä¹ ",
      "authors": [
        "Adrian-Dinu Urse",
        "Dumitru-Clementin Cercel",
        "Florin Pop"
      ],
      "abstract": "Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªç„¶ç¾å®³è¯„ä¼°ä¸­ç¤¾äº¤åª’ä½“æ•°æ®é›†ï¼ˆå¦‚ CrisisMMDï¼‰å­˜åœ¨çš„ç±»åˆ«ä¸å¹³è¡¡å’Œæ ·æœ¬æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œæ¢ç´¢äº†å¤šç§å¢å¼ºæŠ€æœ¯ï¼ˆAugmentation Techniquesï¼‰çš„åº”ç”¨æ•ˆæœã€‚åœ¨è§†è§‰æ•°æ®å¤„ç†ä¸Šï¼Œç ”ç©¶åº”ç”¨äº†åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³• Real Guidance å’Œ DiffuseMixï¼›åœ¨æ–‡æœ¬å¢å¼ºæ–¹é¢ï¼Œåˆ™æ·±å…¥æ¢è®¨äº†å›è¯‘ï¼ˆBack-translationï¼‰ã€åŸºäº Transformer çš„æ”¹å†™ï¼ˆParaphrasingï¼‰ä»¥åŠåŸºäºå›¾åƒæè¿°çš„å¢å¼ºï¼ˆImage caption-based augmentationï¼‰ã€‚é€šè¿‡åœ¨å•æ¨¡æ€ï¼ˆUnimodalï¼‰ã€å¤šæ¨¡æ€ï¼ˆMultimodalï¼‰å’Œå¤šè§†å›¾å­¦ä¹ ï¼ˆMulti-view learningï¼‰æ¡†æ¶ä¸‹çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒç»“æœè¯æ˜æ‰€é€‰å¢å¼ºç­–ç•¥æ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ ·æœ¬è¾ƒå°‘çš„å°‘æ•°ç±»ï¼ˆUnderrepresented classesï¼‰æ—¶è¡¨ç°å“è¶Šã€‚å°½ç®¡å¤šè§†å›¾å­¦ä¹ ä»å±•ç°å‡ºè¿›ä¸€æ­¥ä¼˜åŒ–çš„æ½œåŠ›ï¼Œä½†è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„ç¾å®³è¯„ä¼°ç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„å¢å¼ºæ–¹æ¡ˆä¸å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted at 2025 IEEE 21st International Conference on Intelligent Computer Communication and Processing (ICCP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.00004v1",
      "published_date": "2025-10-04 18:51:54 UTC",
      "updated_date": "2025-10-04 18:51:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:55.844482+00:00"
    },
    {
      "arxiv_id": "2510.05163v1",
      "title": "Deep Learning-Based Multi-Factor Authentication: A Survey of Biometric and Smart Card Integration Approaches",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šå› ç´ èº«ä»½è®¤è¯ï¼šç”Ÿç‰©è¯†åˆ«ä¸æ™ºèƒ½å¡é›†æˆæ–¹æ³•ç»¼è¿°",
      "authors": [
        "Abdelilah Ganmati",
        "Karim Afdel",
        "Lahcen Koutti"
      ],
      "abstract": "In the era of pervasive cyber threats and exponential growth in digital services, the inadequacy of single-factor authentication has become increasingly evident. Multi-Factor Authentication (MFA), which combines knowledge-based factors (passwords, PINs), possession-based factors (smart cards, tokens), and inherence-based factors (biometric traits), has emerged as a robust defense mechanism. Recent breakthroughs in deep learning have transformed the capabilities of biometric systems, enabling higher accuracy, resilience to spoofing, and seamless integration with hardware-based solutions. At the same time, smart card technologies have evolved to include on-chip biometric verification, cryptographic processing, and secure storage, thereby enabling compact and secure multi-factor devices. This survey presents a comprehensive synthesis of recent work (2019-2025) at the intersection of deep learning, biometrics, and smart card technologies for MFA. We analyze biometric modalities (face, fingerprint, iris, voice), review hardware-based approaches (smart cards, NFC, TPMs, secure enclaves), and highlight integration strategies for real-world applications such as digital banking, healthcare IoT, and critical infrastructure. Furthermore, we discuss the major challenges that remain open, including usability-security tradeoffs, adversarial attacks on deep learning models, privacy concerns surrounding biometric data, and the need for standardization in MFA deployment. By consolidating current advancements, limitations, and research opportunities, this survey provides a roadmap for designing secure, scalable, and user-friendly authentication frameworks.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåˆ†æäº†2019è‡³2025å¹´é—´Deep Learningã€ç”Ÿç‰©è¯†åˆ«(Biometrics)ä¸æ™ºèƒ½å¡(Smart Card)æŠ€æœ¯åœ¨å¤šå› ç´ è®¤è¯(MFA)é¢†åŸŸçš„èåˆä¸æœ€æ–°è¿›å±•ã€‚ç ”ç©¶æŒ‡å‡ºDeep Learningçš„çªç ´æ˜¾è‘—æå‡äº†ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿçš„è¯†åˆ«å‡†ç¡®ç‡ã€æŠ—æ¬ºéª—èƒ½åŠ›ä»¥åŠä¸ç¡¬ä»¶è§£å†³æ–¹æ¡ˆçš„é›†æˆæ•ˆç‡ã€‚æ–‡ç« è¯¦ç»†æ¢è®¨äº†æ™ºèƒ½å¡æŠ€æœ¯çš„æ¼”è¿›ï¼ŒåŒ…æ‹¬ç‰‡ä¸Šç”Ÿç‰©è¯†åˆ«éªŒè¯ã€åŠ å¯†å¤„ç†å’Œå®‰å…¨å­˜å‚¨ï¼Œä½¿ç´§å‡‘ä¸”å®‰å…¨çš„MFAè®¾å¤‡æˆä¸ºå¯èƒ½ã€‚é€šè¿‡å¯¹äººè„¸ã€æŒ‡çº¹ã€è™¹è†œç­‰æ¨¡æ€åŠNFCã€TPMsã€å®‰å…¨é£åœ°(Secure Enclaves)ç­‰ç¡¬ä»¶ç­–ç•¥çš„ç»¼åˆå®¡æŸ¥ï¼Œè¯¥ç»¼è¿°å±•ç¤ºäº†å…¶åœ¨æ•°å­—é“¶è¡Œå’ŒåŒ»ç–—ç‰©è”ç½‘(Healthcare IoT)ç­‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ˜ç¡®äº†å¯¹æŠ—æ€§æ”»å‡»(Adversarial Attacks)ã€éšç§ä¿æŠ¤åŠæ ‡å‡†åŒ–ç­‰å°šå¾…è§£å†³çš„æŒ‘æˆ˜ï¼Œä¸ºæ„å»ºå®‰å…¨ã€å¯æ‰©å±•ä¸”ç”¨æˆ·å‹å¥½çš„èº«ä»½éªŒè¯æ¡†æ¶æä¾›äº†è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 3 figures, 6 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.05163v1",
      "published_date": "2025-10-04 18:34:16 UTC",
      "updated_date": "2025-10-04 18:34:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:44:57.954085+00:00"
    },
    {
      "arxiv_id": "2510.03892v1",
      "title": "Kantian-Utilitarian XAI: Meta-Explained",
      "title_zh": "åº·å¾·-åŠŸåˆ©ä¸»ä¹‰å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼šå…ƒè§£é‡Š",
      "authors": [
        "Zahra Atf",
        "Peter R. Lewis"
      ],
      "abstract": "We present a gamified explainable AI (XAI) system for ethically aware consumer decision-making in the coffee domain. Each session comprises six rounds with three options per round. Two symbolic engines provide real-time reasons: a Kantian module flags rule violations (e.g., child labor, deforestation risk without shade certification, opaque supply chains, unsafe decaf), and a utilitarian module scores options via multi-criteria aggregation over normalized attributes (price, carbon, water, transparency, farmer income share, taste/freshness, packaging, convenience). A meta-explainer with a regret bound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a deontically clean, near-parity option when welfare loss is small. We release a structured configuration (attribute schema, certification map, weights, rule set), a policy trace for auditability, and an interactive UI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸“ä¸ºå’–å•¡é¢†åŸŸä¼¦ç†æ¶ˆè´¹å†³ç­–è®¾è®¡çš„æ¸¸æˆåŒ–å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç”±ä¸¤ä¸ªç¬¦å·å¼•æ“é©±åŠ¨ï¼šKantianæ¨¡å—è´Ÿè´£è¯†åˆ«ç«¥å·¥ã€æ¯æ—é£é™©åŠä¾›åº”é“¾ä¸é€æ˜ç­‰è§„åˆ™è¿è§„è¡Œä¸ºï¼Œè€ŒUtilitarianæ¨¡å—é€šè¿‡å¤šå‡†åˆ™èšåˆå¯¹ä»·æ ¼ã€ç¢³è¶³è¿¹ã€æ°´è¶³è¿¹å’Œå†œæ°‘æ”¶å…¥ä»½é¢ç­‰å±æ€§è¿›è¡Œç»¼åˆè¯„åˆ†ã€‚ç ”ç©¶æ ¸å¿ƒåŒ…å«ä¸€ä¸ªè®¾å®šäº†æ‚”æ¨è¾¹ç•Œ(regret bound)ä¸º0.2çš„å…ƒè§£é‡Šå™¨(meta-explainer)ï¼Œç”¨äºåˆ†æåº·å¾·ä¸»ä¹‰ä¸åŠŸåˆ©ä¸»ä¹‰ä¹‹é—´çš„å¯¹é½æƒ…å†µã€‚å½“ç¦åˆ©æŸå¤±è¾ƒå°æ—¶ï¼Œè¯¥è§£é‡Šå™¨ä¼šè‡ªåŠ¨åˆ‡æ¢è‡³ç¬¦åˆä¹‰åŠ¡å‡†åˆ™ä¸”æ•ˆç”¨ç›¸è¿‘çš„é€‰é¡¹ï¼Œä»¥å¹³è¡¡ä¼¦ç†çº¦æŸä¸æ•´ä½“æ•ˆç›Šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†åŒ…å«å±æ€§æ¨¡å¼ã€è§„åˆ™é›†å’Œäº¤äº’å¼UIåœ¨å†…çš„ç»“æ„åŒ–é…ç½®åŠå¯å®¡è®¡ç­–ç•¥è¿½è¸ªï¼Œä¸ºä¼¦ç†æ„ŸçŸ¥å†³ç­–æä¾›äº†å®Œæ•´çš„æŠ€æœ¯æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for presentation as a poster at the 35th IEEE International Conference on Collaborative Advances in Software and Computing, 2025. Conference website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained",
      "pdf_url": "https://arxiv.org/pdf/2510.03892v1",
      "published_date": "2025-10-04 18:16:12 UTC",
      "updated_date": "2025-10-04 18:16:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:04.350435+00:00"
    },
    {
      "arxiv_id": "2510.03886v1",
      "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer",
      "title_zh": "æ‰©æ•£Transformerä¸­å§‹ç»ˆæ½œè—ç€çš„ç¨€æœ‰æ–‡æœ¬è¯­ä¹‰",
      "authors": [
        "Seil Kang",
        "Woojung Han",
        "Dayun Ju",
        "Seong Jae Hwang"
      ],
      "abstract": "Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT's outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨(MM-DiTs)åœ¨å¤„ç†ç½•è§æˆ–æå…·æƒ³è±¡åŠ›çš„æç¤ºè¯æ—¶è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒã€æ•°æ®æˆ–å¤–éƒ¨æ¨¡å—çš„å¹²é¢„æ–¹æ³•ï¼Œæ—¨åœ¨æŒ–æ˜æ¨¡å‹å†…éƒ¨éšè—çš„ç¨€ç–è¯­ä¹‰ã€‚ç ”ç©¶å›¢é˜Ÿå‘ç°ï¼Œé€šè¿‡åœ¨MM-DiTçš„è”åˆæ³¨æ„åŠ›æœºåˆ¶(joint-attention blocks)ä¹‹å‰ï¼Œåˆ©ç”¨æ–¹å·®æ”¾å¤§(variance scale-up)åœ¨æ•°å­¦ä¸Šæ‰©å±•æ–‡æœ¬æ ‡è®°åµŒå…¥(text token embeddings)å‘¨å›´çš„è¡¨ç¤ºç›†åœ°(representational basins)ï¼Œå¯ä»¥ä½¿ç½•è§è¯­ä¹‰åœ¨è¾“å‡ºä¸­æ¸…æ™°æµ®ç°ã€‚è¯¥æ–¹æ³•å·§å¦™åˆ©ç”¨äº†MM-DiTåºåˆ—åŒ–æ›´æ–°æ–‡æœ¬ä¸å›¾åƒåµŒå…¥çš„ç‰¹æ€§ï¼Œæœ‰æ•ˆè§£å†³äº†é¢„è®­ç»ƒé˜¶æ®µç¨€ç¼ºæ¦‚å¿µéš¾ä»¥ç•™ä¸‹æ·±åˆ»å°è®°çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å¹²é¢„æ‰‹æ®µåœ¨æ–‡æœ¬ç”Ÿæˆå›¾åƒã€æ–‡æœ¬ç”Ÿæˆè§†é¢‘ä»¥åŠæ–‡æœ¬é©±åŠ¨çš„å›¾åƒç¼–è¾‘ç­‰å¤šç§è§†è§‰ç”Ÿæˆä»»åŠ¡ä¸­å‡å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œæ­ç¤ºäº†å³ä¾¿æ²¡æœ‰é¢å¤–ä¼˜åŒ–ï¼Œç½•è§æ–‡æœ¬è¯­ä¹‰ä¹Ÿä¸€ç›´å­˜åœ¨äºæ‰©æ•£å˜æ¢å™¨ä¸­ï¼Œå¹¶å¯é€šè¿‡ç®€å•çš„æ•°å­¦å¹²é¢„è¢«æˆåŠŸæ¿€æ´»ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03886v1",
      "published_date": "2025-10-04 17:41:24 UTC",
      "updated_date": "2025-10-04 17:41:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:03.435222+00:00"
    },
    {
      "arxiv_id": "2510.03879v2",
      "title": "Adversarial Agent Collaboration for C to Rust Translation",
      "title_zh": "é¢å‘ C åˆ° Rust ç¿»è¯‘çš„å¯¹æŠ—æ€§æ™ºèƒ½ä½“åä½œ",
      "authors": [
        "Tianyu Li",
        "Ruishi Li",
        "Bo Wang",
        "Brandon Paulsen",
        "Umang Mathur",
        "Prateek Saxena"
      ],
      "abstract": "Translating C to memory-safe languages, like Rust, prevents critical memory safety vulnerabilities that are prevalent in legacy C software. Existing approaches for C to safe Rust translation, including LLM-assisted ones, do not generalize on larger (> 500 LoC) C codebases because they depend on complex program analyses that frequently break. In this work, we present ACToR (Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired by GANs, ACToR pits a generator agent against a discriminator agent, which collaborate to iteratively generate a Rust translation. On each iteration, the translator agent synthesizes and refines a Rust translation to pass an existing suite of tests, and then the discriminator agent finds new failing tests. We demonstrate that ACToR translates all of the 63 real-world command-line utilities considered in our benchmarks, which have an average size of 473 lines of code, and it achieves over 90% test pass rate with zero human intervention during translation. To our knowledge, it is the first work to show evidence that an agent-centric approach can reliably and automatically convert standalone command-line C programs at this scale. Furthermore, ACToR improves translation correctness by up to 25.1% compared to baseline, non-adversarial approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ACToR (Adversarial C To Rust translator)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“çš„ C è¯­è¨€åˆ° Rust è¯­è¨€ç¿»è¯‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¾ƒå¤§è§„æ¨¡ä»£ç åº“æ—¶å› è¿‡åº¦ä¾èµ–å¤æ‚ç¨‹åºåˆ†æè€Œéš¾ä»¥æ³›åŒ–çš„é—®é¢˜ã€‚å—åˆ° GANs çš„å¯å‘ï¼ŒACToR è®©ç”Ÿæˆå™¨æ™ºèƒ½ä½“ (generator agent) ä¸åˆ¤åˆ«å™¨æ™ºèƒ½ä½“ (discriminator agent) å±•å¼€å¯¹æŠ—æ€§åä½œï¼Œé€šè¿‡è¿­ä»£æ–¹å¼ç”Ÿæˆå¹¶ç²¾ç‚¼ Rust ç¿»è¯‘ç»“æœã€‚åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ï¼Œç¿»è¯‘æ™ºèƒ½ä½“è´Ÿè´£åˆæˆä»£ç ä»¥é€šè¿‡ç°æœ‰æµ‹è¯•é›†ï¼Œè€Œåˆ¤åˆ«å™¨æ™ºèƒ½ä½“åˆ™ä¸æ–­å¯»æ‰¾æ–°çš„å¤±è´¥æµ‹è¯•ç”¨ä¾‹æ¥é©±åŠ¨ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒACToR åœ¨ 63 ä¸ªçœŸå®ä¸–ç•Œå‘½ä»¤è¡Œå·¥å…·ï¼ˆå¹³å‡è§„æ¨¡ 473 LoCï¼‰çš„ç¿»è¯‘ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡ 90% çš„æµ‹è¯•é€šè¿‡ç‡ï¼Œä¸”å®Œå…¨æ— éœ€äººå·¥å¹²é¢„ã€‚ä½œä¸ºé¦–ä¸ªè¯æ˜æ™ºèƒ½ä½“æ¶æ„èƒ½å¯é è‡ªåŠ¨è½¬æ¢æ­¤ç±»è§„æ¨¡ C ç¨‹åºçš„å·¥ä½œï¼ŒACToR ç›¸æ¯”éå¯¹æŠ—æ€§åŸºçº¿æ–¹æ³•å°†ç¿»è¯‘æ­£ç¡®æ€§æå‡äº†é«˜è¾¾ 25.1%ã€‚è¯¥æ–¹æ³•ä¸ºæ¶ˆé™¤é—ç•™ C è½¯ä»¶ä¸­çš„å†…å­˜å®‰å…¨æ¼æ´æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å…·å¤‡æ‰©å±•æ€§çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03879v2",
      "published_date": "2025-10-04 17:08:36 UTC",
      "updated_date": "2025-12-08 09:32:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:08.850505+00:00"
    },
    {
      "arxiv_id": "2510.03878v1",
      "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks",
      "title_zh": "åŸºäºåŠ æƒé›†æˆå·ç§¯ç¥ç»ç½‘ç»œçš„å¤šæ¨¡æ€å£è…”ç™Œæ£€æµ‹",
      "authors": [
        "Ajo Babu George",
        "Sreehari J R Ajo Babu George",
        "Sreehari J R Ajo Babu George",
        "Sreehari J R"
      ],
      "abstract": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes significantly to its high global mortality rate, with over 50\\% of cases detected at advanced stages and a 5-year survival rate below 50\\% according to WHO statistics. This study aims to improve early detection of OSCC by developing a multimodal deep learning framework that integrates clinical, radiological, and histopathological images using a weighted ensemble of DenseNet-121 convolutional neural networks (CNNs). Material and Methods A retrospective study was conducted using publicly available datasets representing three distinct medical imaging modalities. Each modality-specific dataset was used to train a DenseNet-121 CNN via transfer learning. Augmentation and modality-specific preprocessing were applied to increase robustness. Predictions were fused using a validation-weighted ensemble strategy. Evaluation was performed using accuracy, precision, recall, F1-score. Results High validation accuracy was achieved for radiological (100\\%) and histopathological (95.12\\%) modalities, with clinical images performing lower (63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved diagnostic robustness with an overall accuracy of 84.58\\% on a multimodal validation dataset of 55 samples. Conclusion The multimodal ensemble framework bridges gaps in the current diagnostic workflow by offering a non-invasive, AI-assisted triage tool that enhances early identification of high-risk lesions. It supports clinicians in decision-making, aligning with global oncology guidelines to reduce diagnostic delays and improve patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶æé«˜å£è…”é³çŠ¶ç»†èƒç™Œ(Oral Squamous Cell Carcinoma, OSCC)çš„æ—©æœŸæ£€æµ‹æ•ˆç‡ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ•´åˆä¸´åºŠã€æ”¾å°„å­¦å’Œç»„ç»‡ç—…ç†å­¦å½±åƒçš„æ¶æ„ï¼Œé‡‡ç”¨åŠ æƒé›†æˆ(Weighted Ensemble)çš„ DenseNet-121 å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)è¿›è¡Œåˆ†ç±»ã€‚é€šè¿‡å¯¹å„ç‰¹å®šæ¨¡æ€è¿›è¡Œè¿ç§»å­¦ä¹ (Transfer Learning)è®­ç»ƒåŠé¢„å¤„ç†ï¼Œå¹¶åˆ©ç”¨éªŒè¯åŠ æƒç­–ç•¥èåˆé¢„æµ‹ç»“æœï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„è¯Šæ–­é²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ”¾å°„å­¦å’Œç»„ç»‡ç—…ç†å­¦æ¨¡æ€è¡¨ç°ä¼˜å¼‚ï¼Œè€Œå¤šæ¨¡æ€é›†æˆæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°äº†84.58%çš„æ•´ä½“å‡†ç¡®ç‡ã€‚è¯¥æ¡†æ¶ä½œä¸ºä¸€ç§æ— åˆ›çš„AIè¾…åŠ©åˆ†è¯Šå·¥å…·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¾…åŠ©ä¸´åºŠå†³ç­–å¹¶æ—©æœŸè¯†åˆ«é«˜å±ç—…å˜ï¼Œæœ‰åŠ©äºç¼©çŸ­è¯Šæ–­å»¶è¿Ÿå¹¶æ”¹å–„æ‚£è€…çš„ç”Ÿå­˜é¢„åã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03878v1",
      "published_date": "2025-10-04 17:06:46 UTC",
      "updated_date": "2025-10-04 17:06:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:11.552240+00:00"
    },
    {
      "arxiv_id": "2510.03873v1",
      "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis",
      "title_zh": "PoseGaze-AHPï¼šç”¨äºAIé©±åŠ¨çœ¼ç§‘ä¸å§¿åŠ¿è¯Šæ–­çš„åŸºäºçŸ¥è¯†çš„3Dæ•°æ®é›†",
      "authors": [
        "Saja Al-Dabet",
        "Sherzod Turaev",
        "Nazar Zaki",
        "Arif O. Khan",
        "Luai Eldweik"
      ],
      "abstract": "Diagnosing ocular-induced abnormal head posture (AHP) requires a comprehensive analysis of both head pose and ocular movements. However, existing datasets focus on these aspects separately, limiting the development of integrated diagnostic approaches and restricting AI-driven advancements in AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D dataset that synchronously captures head pose and gaze movement information for ocular-induced AHP assessment. Structured clinical data were extracted from medical literature using large language models (LLMs) through an iterative process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and complex prompting strategies. The extracted records were systematically imputed and transformed into 3D representations using the Neural Head Avatar (NHA) framework. The dataset includes 7,920 images generated from two head textures, covering a broad spectrum of ocular conditions. The extraction method achieved an overall accuracy of 91.92%, demonstrating its reliability for clinical dataset construction. PoseGaze-AHP is the first publicly available resource tailored for AI-driven ocular-induced AHP diagnosis, supporting the development of accurate and privacy-compliant diagnostic tools.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† PoseGaze-AHPï¼Œè¿™æ˜¯é¦–ä¸ªåŒæ­¥æ•è·å¤´å§¿ä¸çœ¼åŠ¨ä¿¡æ¯çš„ 3D æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³çœ¼æºæ€§ Abnormal Head Posture (AHP) è¯Šæ–­ä¸­æ•°æ®é›†æˆä¸è¶³çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Large Language Models (LLMs) ä¸­çš„ Claude 3.5 Sonnet æ¨¡å‹ï¼Œé€šè¿‡åˆ†æ­¥å¼å’Œå±‚æ¬¡åŒ–æç¤ºç­–ç•¥ä»åŒ»å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¸´åºŠæ•°æ®ï¼Œå¹¶åº”ç”¨ Neural Head Avatar (NHA) æ¡†æ¶å°†å…¶è½¬åŒ–ä¸º 3D è¡¨å¾ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¶µç›–å¹¿æ³›çœ¼ç§‘çŠ¶å†µçš„ 7,920 å¼ å›¾åƒï¼Œæ•°æ®æå–å‡†ç¡®ç‡é«˜è¾¾ 91.92%ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸´åºŠæ•°æ®é›†æ„å»ºä¸­çš„å¯é æ€§ã€‚ä½œä¸ºé¦–ä¸ªä¸“ä¸º AI é©±åŠ¨çš„ AHP è¯Šæ–­å®šåˆ¶çš„å…¬å¼€èµ„æºï¼ŒPoseGaze-AHP ä¸ºå¼€å‘ç²¾ç¡®ä¸”ä¿éšœéšç§çš„è¯Šæ–­å·¥å…·å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is a preprint version of a manuscript under review. All rights reserved by the authors",
      "pdf_url": "https://arxiv.org/pdf/2510.03873v1",
      "published_date": "2025-10-04 16:50:30 UTC",
      "updated_date": "2025-10-04 16:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:16.347468+00:00"
    },
    {
      "arxiv_id": "2510.03871v1",
      "title": "Optimal Scaling Needs Optimal Norm",
      "title_zh": "æœ€ä¼˜ç¼©æ”¾éœ€è¦æœ€ä¼˜èŒƒæ•°",
      "authors": [
        "Oleg Filatov",
        "Jiangtao Wang",
        "Jan Ebert",
        "Stefan Kesselheim"
      ],
      "abstract": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(Î·^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(Î·, B)$ reach the optimal norm, only a unique $(Î·^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(Î·^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
      "tldr_zh": "è¯¥ç ”ç©¶ä½¿ç”¨ Scion ä¼˜åŒ–å™¨å‘ç°ï¼Œè·¨æ¨¡å‹å’Œæ•°æ®é›†è§„æ¨¡çš„è”åˆæœ€ä¼˜ç¼©æ”¾ï¼ˆoptimal scalingï¼‰å—å•ä¸€ä¸å˜é‡é©±åŠ¨ï¼Œå³è¾“å‡ºå±‚çš„ç®—å­èŒƒæ•°ï¼ˆoperator normï¼‰ã€‚é€šè¿‡å¯¹é«˜è¾¾ 13 äº¿å‚æ•°å’Œ 1380 äº¿ token çš„æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç ”ç©¶è¯å®äº†æœ€ä¼˜å­¦ä¹ ç‡ä¸æ‰¹æ¬¡å¤§å°ç»„åˆ $(Î·^*, B^*)$ å§‹ç»ˆå¯¹åº”ç›¸åŒçš„ç®—å­èŒƒæ•°å€¼ï¼Œå¹¶å°†æ­¤ç°è±¡ç§°ä¸ºèŒƒæ•°è¿ç§»ï¼ˆnorm transferï¼‰ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¾¾åˆ°æ’å®šçš„ç®—å­èŒƒæ•°æ˜¯å®ç°æœ€ä¼˜ç¼©æ”¾çš„å¿…è¦éå……åˆ†æ¡ä»¶ï¼Œä¸” Scion çš„ç¼©æ”¾è§„åˆ™ä¸ Adam ä¼˜åŒ–å™¨ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œå®éªŒè¡¨æ˜æŒ‰å±‚ç»„ï¼ˆper-layer-groupï¼‰å¾®è°ƒå­¦ä¹ ç‡å¯æå‡æ€§èƒ½ï¼Œå…¶ä¸­è¾“å‡ºå±‚å¯¹å­¦ä¹ ç‡æœ€ä¸ºæ•æ„Ÿã€‚ä½œè€…æœ€åå‘å¸ƒäº†åˆ†å¸ƒå¼å®ç° Distributed Scion (Disco) åŠå…¶è®­ç»ƒæ—¥å¿—ï¼Œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒåŠ¨æ€ç ”ç©¶æä¾›äº†å®ç”¨çš„èŒƒæ•°å¼•å¯¼ç­–ç•¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03871v1",
      "published_date": "2025-10-04 16:48:36 UTC",
      "updated_date": "2025-10-04 16:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:18.047200+00:00"
    },
    {
      "arxiv_id": "2510.03868v1",
      "title": "AI Adoption Across Mission-Driven Organizations",
      "title_zh": "ä½¿å‘½é©±åŠ¨å‹ç»„ç»‡çš„äººå·¥æ™ºèƒ½é‡‡ç”¨",
      "authors": [
        "Dalia Ali",
        "Muneeb Ahmed",
        "Hailan Wang",
        "Arfa Khan",
        "Naira Paola Arnez Jordan",
        "Sunnie S. Y. Kim",
        "Meet Dilip Muchhala",
        "Anne Kathrin Merkle",
        "Orestis Papakyriakopoulos"
      ],
      "abstract": "Despite AI's promise for addressing global challenges, empirical understanding of AI adoption in mission-driven organizations (MDOs) remains limited. While research emphasizes individual applications or ethical principles, little is known about how resource-constrained, values-driven organizations navigate AI integration across operations. We conducted thematic analysis of semi-structured interviews with 15 practitioners from environmental, humanitarian, and development organizations across the Global North and South contexts. Our analysis examines how MDOs currently deploy AI, what barriers constrain adoption, and how practitioners envision future integration. MDOs adopt AI selectively, with sophisticated deployment in content creation and data analysis while maintaining human oversight for mission-critical applications. When AI's efficiency benefits conflict with organizational values, decision-making stalls rather than negotiating trade-offs. This study contributes empirical evidence that AI adoption in MDOs should be understood as conditional rather than inevitable, proceeding only where it strengthens organizational sovereignty and mission integrity while preserving human-centered approaches essential to their missions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä½¿å‘½é©±åŠ¨å‹ç»„ç»‡(Mission-Driven Organizations, MDOs)åœ¨é‡‡ç”¨äººå·¥æ™ºèƒ½(AI)è¿‡ç¨‹ä¸­çš„ç°çŠ¶ã€éšœç¢åŠæœªæ¥æ„¿æ™¯ã€‚é€šè¿‡å¯¹æ¥è‡ªå…¨çƒå—åŒ—èƒŒæ™¯çš„ç¯ä¿ã€äººé“ä¸»ä¹‰å’Œå‘å±•ç»„ç»‡çš„15ä½ä»ä¸šè€…è¿›è¡ŒåŠç»“æ„åŒ–è®¿è°ˆ(semi-structured interviews)å’Œä¸»é¢˜åˆ†æ(thematic analysis)ï¼Œç ”ç©¶æ­ç¤ºäº†è¿™äº›èµ„æºå—é™ä¸”ä»·å€¼è§‚é©±åŠ¨çš„ç»„ç»‡å¦‚ä½•æ•´åˆAIã€‚ç»“æœæ˜¾ç¤ºï¼ŒMDOsé€‰æ‹©æ€§åœ°åœ¨å†…å®¹åˆ›ä½œå’Œæ•°æ®åˆ†æä¸­éƒ¨ç½²AIï¼Œä½†åœ¨æ ¸å¿ƒä½¿å‘½åº”ç”¨ä¸­ä»ä¿æŒäººç±»ç›‘ç£(human oversight)ã€‚å½“AIçš„æ•ˆç‡ä¼˜åŠ¿ä¸ç»„ç»‡ä»·å€¼è§‚å‘ç”Ÿå†²çªæ—¶ï¼Œå†³ç­–å¾€å¾€å€¾å‘äºåœæ»è€Œéåœ¨æƒè¡¡ä¸­å¦¥åã€‚è¯¥ç ”ç©¶è´¡çŒ®äº†å…³é”®çš„ç»éªŒè¯æ®ï¼Œè¡¨æ˜MDOså¯¹AIçš„é‡‡ç”¨æ˜¯åŸºäºæ¡ä»¶çš„è€Œéå¿…ç„¶çš„ï¼Œä»…åœ¨èƒ½å¤Ÿå¢å¼ºç»„ç»‡ä¸»æƒ(organizational sovereignty)å’Œä½¿å‘½å®Œæ•´æ€§(mission integrity)çš„æƒ…å†µä¸‹æ‰ä¼šæ¨è¿›ã€‚è¿™ç§æ¨¡å¼ç¡®ä¿äº†ç»„ç»‡åœ¨åˆ©ç”¨æŠ€æœ¯çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿ç•™å¯¹å…¶ä½¿å‘½è‡³å…³é‡è¦çš„ä»¥äººä¸ºæœ¬çš„æ–¹æ³•ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, Submitted for CHI 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.03868v1",
      "published_date": "2025-10-04 16:28:54 UTC",
      "updated_date": "2025-10-04 16:28:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:32.157213+00:00"
    },
    {
      "arxiv_id": "2510.03865v2",
      "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration",
      "title_zh": "é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¢ç´¢é‡Šæ”¾å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Wenhao Deng",
        "Long Wei",
        "Chenglei Yu",
        "Tailin Wu"
      ],
      "abstract": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced the reasoning capabilities of large language models (LLMs), particularly for mathematical problem solving. However, a fundamental limitation remains: as the sampling budget increases, the advantage of RLVR-trained models over their pretrained bases often diminishes or even vanishes, revealing a strong dependence on the base model's restricted search space. We attribute this phenomenon to the widespread use of the reverse Kullback-Leibler (KL) divergence regularizer, whose mode-seeking behavior keeps the policy trapped inside the base model's support region and hampers wider exploration. To address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an algorithm to promote broader yet focused exploration. Our method (i) utilizes the forward KL penalty to replace the reverse KL penalty for out-of-distribution exploration, and (ii) reweights the reference policy to facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B models with RAPO on the 8K SimpleRL-Zero dataset, without supervised fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO consistently improves problem-solving performance. Notably, RAPO enables models to surpass the base model's performance ceiling and solves previously intractable problems, advancing the frontier of RLVR for challenging reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é€šè¿‡å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (Reinforcement learning with verifiable rewards, RLVR) æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹å½“å‰ RLVR æ–¹æ³•å› é€†å‘ Kullback-Leibler (KL) æ•£åº¦æ­£åˆ™åŒ–å¯¼è‡´çš„æ¢ç´¢å—é™åŠæ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œä½œè€…æå‡ºäº† RAPO (Rewards-Aware Policy Optimization) ç®—æ³•ä»¥ä¿ƒè¿›æ›´å¹¿æ³›ä¸”æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ã€‚RAPO é€šè¿‡é‡‡ç”¨æ­£å‘ KL æƒ©ç½šå®ç°åˆ†å¸ƒå¤–æ¢ç´¢ï¼Œå¹¶åˆ©ç”¨é‡åŠ æƒå‚è€ƒç­–ç•¥ä¼˜åŒ–åˆ†å¸ƒå†…æ¢ç´¢ã€‚åœ¨æ— éœ€ç›‘ç£å¾®è°ƒ (SFT) çš„æƒ…å†µä¸‹ï¼Œç ”ç©¶äººå‘˜åœ¨ SimpleRL-Zero æ•°æ®é›†ä¸Šè®­ç»ƒäº† Qwen2.5-3B å’Œ 7B æ¨¡å‹ã€‚åœ¨ AIME2024 å’Œ AIME2025 ç«èµ›é¢˜ç›®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRAPO ä¸€è‡´æ€§åœ°æé«˜äº†æ¨¡å‹çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿçªç ´åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ï¼Œè§£å†³äº†ä»¥å¾€éš¾ä»¥å¤„ç†çš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œæ˜¾è‘—æ¨è¿›äº† RLVR æŠ€æœ¯åœ¨é€»è¾‘æ¨ç†é¢†åŸŸçš„å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03865v2",
      "published_date": "2025-10-04 16:22:19 UTC",
      "updated_date": "2025-10-31 06:08:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:33.046669+00:00"
    },
    {
      "arxiv_id": "2510.03863v1",
      "title": "Spatial CAPTCHA: Generatively Benchmarking Spatial Reasoning for Human-Machine Differentiation",
      "title_zh": "Spatial CAPTCHAï¼šç”¨äºäººæœºåŒºåˆ†çš„ç©ºé—´æ¨ç†ç”Ÿæˆå¼åŸºå‡†æµ‹è¯•",
      "authors": [
        "Arina Kharlamova",
        "Bowei He",
        "Chen Ma",
        "Xue Liu"
      ],
      "abstract": "Online services rely on CAPTCHAs as a first line of defense against automated abuse, yet recent advances in multi-modal large language models (MLLMs) have eroded the effectiveness of conventional designs that focus on text recognition or 2D image understanding. To address this challenge, we present Spatial CAPTCHA, a novel human-verification framework that leverages fundamental differences in spatial reasoning between humans and MLLMs. Unlike existing CAPTCHAs which rely on low-level perception tasks that are vulnerable to modern AI, Spatial CAPTCHA generates dynamic questions requiring geometric reasoning, perspective-taking, occlusion handling, and mental rotation. These skills are intuitive for humans but difficult for state-of-the-art (SOTA) AI systems. The system employs a procedural generation pipeline with constraint-based difficulty control, automated correctness verification, and human-in-the-loop validation to ensure scalability, robustness, and adaptability. Evaluation on a corresponding benchmark, Spatial-CAPTCHA-Bench, demonstrates that humans vastly outperform 10 state-of-the-art MLLMs, with the best model achieving only 31.0% Pass@1 accuracy. Furthermore, we compare Spatial CAPTCHA with Google reCAPTCHA, which confirms its effectiveness as both a security mechanism and a diagnostic tool for spatial reasoning in AI.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Spatial CAPTCHAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„äººæœºéªŒè¯æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)å¯¹ä¼ ç»ŸéªŒè¯ç è®¾è®¡çš„æŒ‘æˆ˜ã€‚ä¸ä¾èµ–ä½çº§æ„ŸçŸ¥ä»»åŠ¡çš„ç°æœ‰è®¾è®¡ä¸åŒï¼ŒSpatial CAPTCHAåˆ©ç”¨äººç±»ä¸MLLMsåœ¨ç©ºé—´æ¨ç†èƒ½åŠ›ä¸Šçš„æ ¹æœ¬å·®å¼‚ï¼Œç”Ÿæˆéœ€è¦å‡ ä½•æ¨ç†(geometric reasoning)ã€é€è§†è§†è§’åˆ‡æ¢(perspective-taking)ã€é®æŒ¡å¤„ç†(occlusion handling)å’Œå¿ƒç†æ—‹è½¬(mental rotation)çš„åŠ¨æ€é—®é¢˜ã€‚ç³»ç»Ÿé‡‡ç”¨ç¨‹åºåŒ–ç”Ÿæˆæµæ°´çº¿ï¼Œç»“åˆåŸºäºçº¦æŸçš„éš¾åº¦æ§åˆ¶å’Œè‡ªåŠ¨æ­£ç¡®æ€§éªŒè¯ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ä¸é²æ£’æ€§ã€‚åœ¨é…å¥—åŸºå‡†æµ‹è¯•Spatial-CAPTCHA-Benchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œäººç±»çš„è¡¨ç°è¿œä¼˜äº10ç§æœ€å…ˆè¿›çš„MLLMsï¼Œå…¶ä¸­è¡¨ç°æœ€å¥½çš„æ¨¡å‹ä»…è¾¾åˆ°31.0%çš„Pass@1å‡†ç¡®ç‡ã€‚è¯¥å·¥ä½œè¯æ˜äº†Spatial CAPTCHAæ—¢å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„å®‰å…¨æœºåˆ¶ï¼Œä¹Ÿèƒ½ä½œä¸ºè¡¡é‡äººå·¥æ™ºèƒ½ç©ºé—´æ¨ç†èƒ½åŠ›çš„è¯Šæ–­å·¥å…·ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.03863v1",
      "published_date": "2025-10-04 16:19:21 UTC",
      "updated_date": "2025-10-04 16:19:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:36.230974+00:00"
    },
    {
      "arxiv_id": "2510.03862v1",
      "title": "Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework",
      "title_zh": "åŸºäº LLM çš„ä»£ç ç”Ÿæˆå®è¯ç ”ç©¶è®¾è®¡ï¼šè¿ˆå‘å‚è€ƒæ¡†æ¶",
      "authors": [
        "Nathalia Nascimento",
        "Everton Guimaraes",
        "Paulo Alencar"
      ],
      "abstract": "The rise of large language models (LLMs) has introduced transformative potential in automated code generation, addressing a wide range of software engineering challenges. However, empirical evaluation of LLM-based code generation lacks standardization, with studies varying widely in goals, tasks, and metrics, which limits comparability and reproducibility. In this paper, we propose a theoretical framework for designing and reporting empirical studies on LLM-based code generation. The framework is grounded in both our prior experience conducting such experiments and a comparative analysis of key similarities and differences among recent studies. It organizes evaluation around core components such as problem sources, quality attributes, and metrics, supporting structured and systematic experimentation. We demonstrate its applicability through representative case mappings and identify opportunities for refinement. Looking forward, we plan to evolve the framework into a more robust and mature tool for standardizing LLM evaluation across software engineering contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Large Language Models (LLMs) åœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆå®è¯è¯„ä¼°ä¸­ç¼ºä¹æ ‡å‡†åŒ–ã€å¯¼è‡´ç ”ç©¶å¯æ¯”æ€§å’Œå¯é‡å¤æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç”¨äºè®¾è®¡å’ŒæŠ¥å‘Šæ­¤ç±»ç ”ç©¶çš„ç†è®ºå‚è€ƒæ¡†æ¶ (Reference Framework)ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç ”ç©¶å›¢é˜Ÿçš„å®éªŒç»éªŒåŠå¯¹è¿‘æœŸç›¸å…³æ–‡çŒ®çš„å¯¹æ¯”åˆ†æï¼Œå°†è¯„ä¼°ä½“ç³»å›´ç»•é—®é¢˜æ¥æº (Problem Sources)ã€è´¨é‡å±æ€§ (Quality Attributes) å’Œåº¦é‡æŒ‡æ ‡ (Metrics) ç­‰æ ¸å¿ƒç»„ä»¶è¿›è¡Œç³»ç»ŸåŒ–æ„å»ºã€‚é€šè¿‡ä»£è¡¨æ€§çš„æ¡ˆä¾‹æ˜ å°„ï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¡†æ¶åœ¨æ”¯æŒç»“æ„åŒ–å’Œç³»ç»ŸåŒ–å®éªŒæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯†åˆ«äº†æ½œåœ¨çš„æ”¹è¿›ç©ºé—´ã€‚è¿™ä¸€æˆæœä¸ºè½¯ä»¶å·¥ç¨‹é¢†åŸŸä¸­ LLMs çš„è§„èŒƒåŒ–è¯„ä¼°æä¾›äº†å·¥å…·æ”¯æ’‘ï¼Œæ—¨åœ¨ä¿ƒè¿›è¯¥é¢†åŸŸå®è¯ç ”ç©¶çš„ä¸¥è°¨æ€§å’Œç³»ç»Ÿæ€§ã€‚æœªæ¥ï¼Œè¯¥æ¡†æ¶å°†æŒç»­æ¼”è¿›ä»¥é€‚åº”æ›´å¤šæ ·åŒ–çš„è½¯ä»¶å·¥ç¨‹ä¸Šä¸‹æ–‡è¯„ä¼°éœ€æ±‚ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "5 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03862v1",
      "published_date": "2025-10-04 16:15:54 UTC",
      "updated_date": "2025-10-04 16:15:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:38.352446+00:00"
    },
    {
      "arxiv_id": "2510.03859v1",
      "title": "Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT Infrastructure using LLM-Enhanced Contextual Reasoning",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¢å¼ºä¸Šä¸‹æ–‡æ¨ç†çš„å…³é”®ç‰©è”ç½‘åŸºç¡€è®¾æ–½å¼‚å¸¸æ£€æµ‹è‡ªé€‚åº”å¯è§£é‡Š AI æ™ºèƒ½ä½“",
      "authors": [
        "Raghav Sharma",
        "Manan Mehta"
      ],
      "abstract": "Ensuring that critical IoT systems function safely and smoothly depends a lot on finding anomalies quickly. As more complex systems, like smart healthcare, energy grids and industrial automation, appear, it is easier to see the shortcomings of older methods of detection. Monitoring failures usually happen in dynamic, high dimensional situations, especially when data is incomplete, messy or always evolving. Such limits point out the requirement for adaptive, intelligent systems that always improve and think. LLMs are now capable of significantly changing how context is understood and semantic inference is done across all types of data. This proposal suggests using an LLM supported contextual reasoning method along with XAI agents to improve how anomalies are found in significant IoT environments. To discover hidden patterns and notice inconsistencies in data streams, it uses attention methods, avoids dealing with details from every time step and uses memory buffers with meaning. Because no code AI stresses transparency and interpretability, people can check and accept the AI's decisions, helping ensure AI follows company policies. The two architectures are put together in a test that compares the results of the traditional model with those of the suggested LLM enhanced model. Important measures to check are the accuracy of detection, how much inaccurate information is included in the results, how clearly the findings can be read and how fast the system responds under different test situations. The metaheuristic is tested in simulations of real world smart grid and healthcare contexts to check its adaptability and reliability. From the study, we see that the new approach performs much better than most existing models in both accuracy and interpretation, so it could be a good fit for future anomaly detection tasks in IoT",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ™ºèƒ½åŒ»ç–—ã€èƒ½æºç”µç½‘ç­‰å…³é”® IoT åŸºç¡€è®¾æ–½åœ¨åŠ¨æ€é«˜ç»´ç¯å¢ƒä¸‹å¼‚å¸¸æ£€æµ‹æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ LLM å¢å¼ºå‹ä¸Šä¸‹æ–‡æ¨ç†ä¸ XAI æ™ºèƒ½ä½“çš„è‡ªé€‚åº”æ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ attention æœºåˆ¶å’Œè¯­ä¹‰ memory buffers æ¥æ•æ‰æ•°æ®æµä¸­çš„éšè—æ¨¡å¼ä¸ä¸ä¸€è‡´æ€§ï¼Œé¿å…äº†å¤„ç†å†—ä½™çš„æ—¶é—´æ­¥ç»†èŠ‚ã€‚é€šè¿‡å¼•å…¥ no-code AI æŠ€æœ¯ï¼Œç³»ç»Ÿæ˜¾è‘—æå‡äº†å†³ç­–çš„é€æ˜åº¦ä¸ interpretabilityï¼Œç¡®ä¿ AI è¿è¡Œç¬¦åˆè¡Œä¸šåˆè§„æ€§è¦æ±‚ã€‚å®éªŒåœ¨æ¨¡æ‹Ÿçš„çœŸå®ä¸–ç•Œæ™ºèƒ½ç”µç½‘å’ŒåŒ»ç–—åœºæ™¯ä¸­è¿›è¡Œï¼Œé€šè¿‡å¯¹æ¯”ä¼ ç»Ÿæ¨¡å‹ä¸ LLM å¢å¼ºæ¨¡å‹ï¼Œè¯„ä¼°äº†æ£€æµ‹å‡†ç¡®ç‡ã€è¯¯æŠ¥ç‡åŠå“åº”é€Ÿåº¦ç­‰å…³é”®æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨å‡†ç¡®æ€§å’Œè§£é‡Šæ€§æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ£€æµ‹æ¨¡å‹ã€‚è¿™ç§è‡ªé€‚åº”ç³»ç»Ÿä¸ºæœªæ¥æ„å»ºå¯è§£é‡Šã€å¯é çš„å…³é”® IoT ç›‘æ§åŸºç¡€è®¾æ–½æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "22 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03859v1",
      "published_date": "2025-10-04 16:12:45 UTC",
      "updated_date": "2025-10-04 16:12:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:42.852705+00:00"
    },
    {
      "arxiv_id": "2510.03856v1",
      "title": "AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images",
      "title_zh": "åŸºäºå¢å¼º CT å½±åƒçš„äººå·¥æ™ºèƒ½è¾…åŠ©èƒ¸è…”ç§¯æ¶²é‡ä¼°ç®—",
      "authors": [
        "Sanhita Basu",
        "Tomas FrÃ¶ding",
        "Ali Teymur Kahraman",
        "Dimitris Toumpanakis",
        "Tobias SjÃ¶blom"
      ],
      "abstract": "Background: Pleural Effusions (PE) is a common finding in many different clinical conditions, but accurately measuring their volume from CT scans is challenging. Purpose: To improve PE segmentation and quantification for enhanced clinical management, we have developed and trained a semi-supervised deep learning framework on contrast-enhanced CT volumes. Materials and Methods: This retrospective study collected CT Pulmonary Angiogram (CTPA) data from internal and external datasets. A subset of 100 cases was manually annotated for model training, while the remaining cases were used for testing and validation. A novel semi-supervised deep learning framework, Teacher-Teaching Assistant-Student (TTAS), was developed and used to enable efficient training in non-segmented examinations. Segmentation performance was compared to that of state-of-the-art models. Results: 100 patients (mean age, 72 years, 28 [standard deviation]; 55 men) were included in the study. The TTAS model demonstrated superior segmentation performance compared to state-of-the-art models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73 for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80 - 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The developed TTAS framework offered superior PE segmentation, aiding accurate volume determination from CT scans.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»å¯¹æ¯”å¢å¼º CT (Contrast-enhanced CT) å›¾åƒä¸­å‡†ç¡®æµ‹é‡èƒ¸è…”ç§¯æ¶² (Pleural Effusions, PE) ä½“ç§¯çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Teacher-Teaching Assistant-Student (TTAS) çš„åŠç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ CT è‚ºåŠ¨è„‰é€ å½± (CTPA) æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æé«˜ PE çš„åˆ†å‰²ç²¾åº¦ä¸é‡åŒ–æ•ˆç‡ï¼Œä»è€Œä¼˜åŒ–ä¸´åºŠç®¡ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTAS æ¨¡å‹è¡¨ç°å‡ºä¼˜äº SOTA æ¨¡å‹çš„åˆ†å‰²æ€§èƒ½ï¼Œå…¶å¹³å‡ Dice åˆ†æ•°è¾¾åˆ° 0.82ï¼Œæ˜¾è‘—é«˜äº nnU-Net çš„ 0.73ã€‚åœ¨ä½“ç§¯ä¼°ç®—æ–¹é¢ï¼ŒTTAS çš„å¹³å‡ç»å¯¹ä½“ç§¯å·®å¼‚ (Absolute Volume Difference, AbVD) ä»…ä¸º 6.49 mLï¼Œæ¯” nnU-Net çš„è¯¯å·®é™ä½äº†è¿‘å››å€ã€‚è¯¥ç ”ç©¶è¯æ˜ TTAS æ¡†æ¶èƒ½å¤Ÿå®ç°å“è¶Šçš„ PE åˆ†å‰²ï¼Œä¸ºä» CT æ‰«æä¸­ç²¾ç¡®æµ‹å®šç§¯æ¶²ä½“ç§¯æä¾›äº†é«˜æ•ˆçš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03856v1",
      "published_date": "2025-10-04 16:06:10 UTC",
      "updated_date": "2025-10-04 16:06:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:43.847351+00:00"
    },
    {
      "arxiv_id": "2510.03851v1",
      "title": "Algorithm Generation via Creative Ideation",
      "title_zh": "åŸºäºåˆ›é€ æ€§æ„æ€çš„ç®—æ³•ç”Ÿæˆ",
      "authors": [
        "Ruiying Ma",
        "Chieh-Jan Mike Liang",
        "Yanjie Gao",
        "Francis Y. Yan"
      ],
      "abstract": "Designing system algorithms remains challenging, where the discontinuous nature of the solution space often forces system engineers to rely on generic heuristics at the expense of performance. We study whether LLMs can practically drive algorithm generation, and find that they are biased towards well-known generic designs, rather than making the creative leaps needed to navigate the discontinuous solution space. To address this limitation, we introduce MetaMuse, a framework for creative ideation built on three self-reflection principles: (1) quantifying solution diversity and usefulness in measurable performance space, rather than abstract idea space, (2) steering ideation through external stimuli, rather than internal randomness, and (3) constructing executable solutions using waypoint reasoning, rather than free-form chain-of-thought. Extensive evaluation shows that MetaMuse can generate high-performing solutions for two critical problems at a global cloud provider: cache replacement (reducing cache misses by up to 35.76%) and online bin packing (reducing bin usage by up to 30.93%).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)é©±åŠ¨ç®—æ³•ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ä¸è¿ç»­è§£ç©ºé—´æ—¶å€¾å‘äºé€šç”¨è®¾è®¡è€Œç¼ºä¹åˆ›é€ åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MetaMuseæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŸºäºä¸‰ä¸ªè‡ªæˆ‘åæ€åŸåˆ™ï¼šåœ¨å¯è¡¡é‡çš„æ€§èƒ½ç©ºé—´è€ŒéæŠ½è±¡ç©ºé—´ä¸­é‡åŒ–è§£çš„å¤šæ ·æ€§ä¸æ•ˆç”¨ï¼Œé€šè¿‡å¤–éƒ¨åˆºæ¿€(external stimuli)å¼•å¯¼æ„æ€ï¼Œä»¥åŠåˆ©ç”¨èˆªç‚¹æ¨ç†(waypoint reasoning)æ„å»ºå¯æ‰§è¡Œæ–¹æ¡ˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMetaMuseåœ¨è§£å†³å…¨çƒäº‘æœåŠ¡æä¾›å•†çš„å…³é”®é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåˆ†åˆ«å°†ç¼“å­˜æ›¿æ¢(cache replacement)çš„æœªå‘½ä¸­ç‡é™ä½äº†35.76%ï¼Œå¹¶å°†åœ¨çº¿è£…ç®±(online bin packing)çš„èµ„æºæ¶ˆè€—å‡å°‘äº†30.93%ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†é€šè¿‡å¼•å…¥åˆ›é€ æ€§æ„æ€æœºåˆ¶ï¼ŒLLMsèƒ½å¤Ÿè¶…è¶Šä¼ ç»Ÿå¯å‘å¼æ–¹æ³•ï¼Œç”Ÿæˆé«˜æ€§èƒ½ä¸”å…·æœ‰åˆ›æ–°æ€§çš„ç³»ç»Ÿç®—æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03851v1",
      "published_date": "2025-10-04 15:52:31 UTC",
      "updated_date": "2025-10-04 15:52:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:47.743720+00:00"
    },
    {
      "arxiv_id": "2510.03847v1",
      "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs",
      "title_zh": "é¢å‘æ™ºèƒ½ä½“ç³»ç»Ÿçš„å°è¯­è¨€æ¨¡å‹ï¼šæ¶æ„ã€èƒ½åŠ›ä¸éƒ¨ç½²æƒè¡¡ç»¼è¿°",
      "authors": [
        "Raghav Sharma",
        "Manan Mehta"
      ],
      "abstract": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference",
      "tldr_zh": "è¯¥ç»¼è¿°æ¢è®¨äº†å°è¯­è¨€æ¨¡å‹(Small Language Models, SLMs)åœ¨æ™ºèƒ½ä½“(agentic)ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼ŒæŒ‡å‡ºåœ¨æ¨¡å¼(schema)å’ŒAPIçº¦æŸçš„å‡†ç¡®æ€§ä»»åŠ¡ä¸­ï¼Œ1-12Bå‚æ•°è§„æ¨¡çš„SLMså¾€å¾€ä¼˜äºå¤§è§„æ¨¡æ¨¡å‹ã€‚ç ”ç©¶é€šè¿‡åˆ†æPhi-4-Miniã€Qwen-2.5-7Bå’ŒDeepSeek-R1-Distillç­‰æ¨¡å‹ï¼Œç»“åˆBFCLè¯„ä¼°å’ŒvLLMç­‰æ¨ç†æ¡†æ¶ï¼Œè®ºè¯äº†SLMsé…åˆå¼•å¯¼å¼è§£ç (guided decoding)åœ¨å·¥å…·ä½¿ç”¨ã€å‡½æ•°è°ƒç”¨å’ŒRAGä»»åŠ¡ä¸Šçš„å·¨å¤§æ½œåŠ›ã€‚è®ºæ–‡æå‡ºäº†ä»¥SLMä¸ºé»˜è®¤ã€LLMä¸ºå›é€€(fallback)çš„ç³»ç»Ÿæ¶æ„ï¼Œå¹¶å¼•å…¥äº†å•æ¬¡æˆåŠŸä»»åŠ¡æˆæœ¬(CPS)ã€æ¨¡å¼æœ‰æ•ˆç‡å’Œæ¯è¯·æ±‚èƒ½è€—ç­‰ç”Ÿäº§çº§è¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å¼•å¯¼å¼è§£ç å’Œä¸¥æ ¼çš„JSON Schemaè¾“å‡ºï¼ŒSLMèƒ½ä»¥é™ä½10åˆ°100å€çš„Tokenæˆæœ¬å’Œæ›´ä½çš„å»¶è¿Ÿï¼Œè¾¾åˆ°æˆ–è¶…è¿‡LLMçš„æ€§èƒ½ã€‚æœ€åï¼Œç ”ç©¶æä¾›äº†åŒ…æ‹¬æ¨¡å¼ä¼˜å…ˆæç¤º(schema-first prompting)å’ŒLoRAè½»é‡åŒ–å¾®è°ƒåœ¨å†…çš„è®¾è®¡æ¨¡å¼ï¼Œä¸ºæ„å»ºå¿«é€Ÿã€ç»æµä¸”å¯é çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†å®è·µè“å›¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "9 Pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03847v1",
      "published_date": "2025-10-04 15:48:04 UTC",
      "updated_date": "2025-10-04 15:48:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:51.046952+00:00"
    },
    {
      "arxiv_id": "2510.03845v1",
      "title": "The Hidden Game Problem",
      "title_zh": "éšè—åšå¼ˆé—®é¢˜",
      "authors": [
        "Gon Buzaglo",
        "Noah Golowich",
        "Elad Hazan"
      ],
      "abstract": "This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown subset of strategies consistently yields higher rewards compared to the rest. The central question is whether efficient regret minimization algorithms can be designed to discover and exploit such hidden structures, leading to equilibrium in these subgames while maintaining rationality in general. We answer this question affirmatively by developing a composition of regret minimization techniques that achieve optimal external and swap regret bounds. Our approach ensures rapid convergence to correlated equilibria in hidden subgames, leveraging the hidden game structure for improved computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å…·æœ‰å¤§è§„æ¨¡ç­–ç•¥ç©ºé—´(strategy spaces)çš„ä¸€ç±»åšå¼ˆï¼Œå…¶åŠ¨æœºæºäºAIå¯¹é½(AI alignment)å’Œè¯­è¨€åšå¼ˆä¸­çš„æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†éšè—åšå¼ˆé—®é¢˜(hidden game problem)ï¼Œå³æ¯ä¸ªç©å®¶éƒ½å­˜åœ¨ä¸€ä¸ªæœªçŸ¥çš„ç­–ç•¥å­é›†ï¼Œèƒ½äº§ç”Ÿæ¯”å…¶ä»–ç­–ç•¥æ›´é«˜çš„ä¸€è‡´æ€§å¥–åŠ±ã€‚ç ”ç©¶çš„æ ¸å¿ƒåœ¨äºè®¾è®¡é«˜æ•ˆçš„æ‚”æ¨æœ€å°åŒ–ç®—æ³•(regret minimization algorithms)æ¥å‘ç°å¹¶åˆ©ç”¨æ­¤ç±»éšè—ç»“æ„ã€‚é€šè¿‡å¼€å‘ä¸€ç§æ‚”æ¨æœ€å°åŒ–æŠ€æœ¯çš„ç»„åˆæ–¹æ¡ˆï¼Œè¯¥ç ”ç©¶æˆåŠŸå®ç°äº†æœ€ä¼˜çš„å¤–éƒ¨æ‚”æ¨(external regret)å’Œäº¤æ¢æ‚”æ¨(swap regret)ç•Œé™ã€‚è¯¥æ–¹æ³•ç¡®ä¿äº†åœ¨éšè—å­åšå¼ˆä¸­èƒ½å¤Ÿå¿«é€Ÿæ”¶æ•›åˆ°ç›¸å…³å‡è¡¡(correlated equilibria)ï¼Œå¹¶æ˜¾è‘—æå‡äº†è®¡ç®—æ•ˆç‡ï¼Œä¸ºå¤æ‚åšå¼ˆç¯å¢ƒä¸‹çš„ç†æ€§å†³ç­–æä¾›äº†ç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03845v1",
      "published_date": "2025-10-04 15:46:04 UTC",
      "updated_date": "2025-10-04 15:46:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:45:55.054359+00:00"
    },
    {
      "arxiv_id": "2510.05162v2",
      "title": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam",
      "title_zh": "å¾®ç§¯åˆ†è€ƒè¯•æ‰‹å†™éƒ¨åˆ†çš„äººå·¥æ™ºèƒ½è¾…åŠ©è¯„åˆ†",
      "authors": [
        "Gerd Kortemeyer",
        "Alexander Caspar",
        "Daria Horica"
      ],
      "abstract": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å½“ä»£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal LLMsï¼‰åœ¨å¤§è§„æ¨¡å¼€æ”¾å¼å¾®ç§¯åˆ†æ‰‹å†™è€ƒè¯•è¯„åˆ†ä¸­çš„è¾…åŠ©æ½œåŠ›ï¼Œæ—¨åœ¨å¹³è¡¡è¯„ä¼°æ•ˆåº¦ä¸å·¥ä½œæ•ˆç‡ã€‚ç ”ç©¶é‡‡ç”¨ GPT-5 å‚ç…§åŠ©æ•™ï¼ˆTAsï¼‰çš„è¯„åˆ†å‡†åˆ™ï¼ˆRubricï¼‰è¿›è¡Œè¯„åˆ†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç»“åˆéƒ¨åˆ†å­¦åˆ†é˜ˆå€¼ä¸é¡¹ç›®ååº”ç†è®ºï¼ˆItem Response Theory, 2PLï¼‰é£é™©æµ‹åº¦çš„äººæœºåä½œè¿‡æ»¤æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶æœªç»ç­›é€‰çš„ AI è¯„åˆ†ä¸€è‡´æ€§ä»…å¤„äºä¸­ç­‰æ°´å¹³ï¼Œä½†åœ¨ç½®ä¿¡åº¦è¿‡æ»¤æœºåˆ¶ä¸‹å¯å®ç°äººç±»æ°´å¹³çš„å‡†ç¡®ç‡ï¼Œå°½ç®¡åœ¨æ­¤è®¾ç½®ä¸‹çº¦ 70% çš„é¢˜ç›®ä»éœ€äººå·¥å¹²é¢„ã€‚ç ”ç©¶è¯å®é€šè¿‡æ ¡å‡†ç½®ä¿¡åº¦å’Œä¿å®ˆè·¯ç”±ç­–ç•¥ï¼ŒAI èƒ½å¤Ÿå¯é åœ°åˆ†æ‹…å¤§é‡å¸¸è§„è¯„åˆ†ä»»åŠ¡ï¼Œä½¿ä¸“å®¶è¯„é˜…äººèƒ½é›†ä¸­ç²¾åŠ›å¤„ç†æ›´å…·æ­§ä¹‰æˆ–æ•™è‚²ä»·å€¼çš„å“åº”ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºé€šè¿‡æ”¹è¿›ç©ºé—´é”šå®šï¼ˆSpatial anchoringï¼‰å’Œç»†åŒ–è¯„åˆ†ç»†åˆ™ï¼Œæœ‰æœ›è¿›ä¸€æ­¥çªç ´ç°æœ‰è‡ªåŠ¨è¯„åˆ†çš„æ€§èƒ½å¤©èŠ±æ¿ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05162v2",
      "published_date": "2025-10-04 15:07:06 UTC",
      "updated_date": "2025-11-13 10:55:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:01.850569+00:00"
    },
    {
      "arxiv_id": "2510.03829v1",
      "title": "A4FN: an Agentic AI Architecture for Autonomous Flying Networks",
      "title_zh": "A4FNï¼šé¢å‘è‡ªä¸»é£è¡Œç½‘ç»œçš„æ™ºèƒ½ä½“ AI æ¶æ„",
      "authors": [
        "AndrÃ© Coelho",
        "Pedro Ribeiro",
        "Helder Fontes",
        "Rui Campos"
      ],
      "abstract": "This position paper presents A4FN, an Agentic Artificial Intelligence (AI) architecture for intent-driven automation in Flying Networks (FNs) using Unmanned Aerial Vehicles (UAVs) as access nodes. A4FN leverages Generative AI and Large Language Models (LLMs) to enable real-time, context-aware network control via a distributed agentic system. It comprises two components: the Perception Agent (PA), which semantically interprets multimodal input -- including imagery, audio, and telemetry data -- from UAV-mounted sensors to derive Service Level Specifications (SLSs); and the Decision-and-Action Agent (DAA), which reconfigures the network based on inferred intents. A4FN embodies key properties of Agentic AI, including autonomy, goal-driven reasoning, and continuous perception-action cycles. Designed for mission-critical, infrastructure-limited scenarios such as disaster response, it supports adaptive reconfiguration, dynamic resource management, and interoperability with emerging wireless technologies. The paper details the A4FN architecture, its core innovations, and open research challenges in multi-agent coordination and Agentic AI integration in next-generation FNs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†A4FNï¼Œä¸€ç§ä¸“ä¸ºä»¥æ— äººæœº(UAV)ä¸ºæ¥å…¥èŠ‚ç‚¹çš„é£è¡Œç½‘ç»œ(Flying Networks)è®¾è®¡çš„ä»£ç†å¼äººå·¥æ™ºèƒ½(Agentic AI)æ¶æ„ï¼Œæ—¨åœ¨å®ç°æ„å›¾é©±åŠ¨çš„ç½‘ç»œè‡ªåŠ¨åŒ–ã€‚è¯¥æ¶æ„ç»“åˆäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ï¼Œé€šè¿‡åˆ†å¸ƒå¼æ™ºèƒ½ä½“ç³»ç»Ÿå®ç°å®æ—¶ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ§åˆ¶ã€‚ç³»ç»Ÿä¸»è¦ç”±æ„ŸçŸ¥æ™ºèƒ½ä½“(Perception Agent)å’Œå†³ç­–ä¸è¡ŒåŠ¨æ™ºèƒ½ä½“(Decision-and-Action Agent)ç»„æˆï¼Œå‰è€…è´Ÿè´£è§£æå¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®ä»¥æ¨å¯¼æœåŠ¡æ°´å¹³è§„èŒƒ(SLSs)ï¼Œåè€…åˆ™æ ¹æ®æ¨æ–­å‡ºçš„æ„å›¾æ‰§è¡Œç½‘ç»œé‡æ„ã€‚A4FNå…·å¤‡è‡ªä¸»æ€§ã€ç›®æ ‡é©±åŠ¨æ¨ç†å’ŒæŒç»­æ„ŸçŸ¥-åŠ¨ä½œå¾ªç¯ç­‰æ ¸å¿ƒç‰¹æ€§ï¼Œç‰¹åˆ«é€‚ç”¨äºç¾éš¾å“åº”ç­‰åŸºç¡€è®¾æ–½å—é™çš„å…³é”®ä»»åŠ¡åœºæ™¯ã€‚é€šè¿‡æ”¯æŒè‡ªé€‚åº”é‡æ„å’ŒåŠ¨æ€èµ„æºç®¡ç†ï¼Œè¯¥ç ”ç©¶ä¸ºä¸‹ä¸€ä»£é£è¡Œç½‘ç»œä¸­å¤šæ™ºèƒ½ä½“åä½œå’Œä»£ç†å¼AIçš„é›†æˆæä¾›äº†æ¶æ„åˆ›æ–°æ–¹æ¡ˆåŠæœªæ¥ç ”ç©¶æŒ‘æˆ˜åˆ†æã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "This paper has been accepted for presentation in the Auto ML for Zero-Touch Network Management Workshop (WS04-01) at the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03829v1",
      "published_date": "2025-10-04 15:02:03 UTC",
      "updated_date": "2025-10-04 15:02:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:13.546229+00:00"
    },
    {
      "arxiv_id": "2510.03824v1",
      "title": "Proximal Diffusion Neural Sampler",
      "title_zh": "è¿‘ç«¯æ‰©æ•£ç¥ç»é‡‡æ ·å™¨",
      "authors": [
        "Wei Guo",
        "Jaemoo Choi",
        "Yuchen Zhu",
        "Molei Tao",
        "Yongxin Chen"
      ],
      "abstract": "The task of learning a diffusion-based neural sampler for drawing samples from an unnormalized target distribution can be viewed as a stochastic optimal control problem on path measures. However, the training of neural samplers can be challenging when the target distribution is multimodal with significant barriers separating the modes, potentially leading to mode collapse. We propose a framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that addresses these challenges by tackling the stochastic optimal control problem via proximal point method on the space of path measures. PDNS decomposes the learning process into a series of simpler subproblems that create a path gradually approaching the desired distribution. This staged procedure traces a progressively refined path to the desired distribution and promotes thorough exploration across modes. For a practical and efficient realization, we instantiate each proximal step with a proximal weighted denoising cross-entropy (WDCE) objective. We demonstrate the effectiveness and robustness of PDNS through extensive experiments on both continuous and discrete sampling tasks, including challenging scenarios in molecular dynamics and statistical physics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Proximal Diffusion Neural Sampler (PDNS) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä»éå½’ä¸€åŒ–ç›®æ ‡åˆ†å¸ƒé‡‡æ ·æ—¶ï¼Œå› å¤šæ¨¡æ€ (multimodal) ç‰¹æ€§å¯¼è‡´çš„æ¨¡å¼å´©å (mode collapse) éš¾é¢˜ã€‚è¯¥æ¡†æ¶å°†æ‰©æ•£ç¥ç»é‡‡æ ·çš„å­¦ä¹ å»ºæ¨¡ä¸ºè·¯å¾„æµ‹åº¦ä¸Šçš„éšæœºæœ€ä¼˜æ§åˆ¶ (stochastic optimal control) é—®é¢˜ï¼Œå¹¶é€šè¿‡è¿‘ç«¯ç‚¹ç®—æ³• (proximal point method) å°†å…¶åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¸è¿›çš„å­é—®é¢˜ã€‚è¿™ç§é˜¶æ®µå¼çš„å¤„ç†æ–¹å¼æœ‰æ•ˆåœ°è¿½è¸ªäº†é€šå¾€ç›®æ ‡åˆ†å¸ƒçš„è·¯å¾„ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒæ¨¡å¼é—´çš„æ¢ç´¢èƒ½åŠ›ã€‚ä¸ºäº†å®ç°é«˜æ•ˆè®­ç»ƒï¼Œç ”ç©¶è€…åœ¨æ¯ä¸ªè¿‘ç«¯æ­¥éª¤ä¸­é‡‡ç”¨äº†è¿‘ç«¯åŠ æƒå»å™ªäº¤å‰ç†µ (weighted denoising cross-entropy, WDCE) ç›®æ ‡å‡½æ•°ã€‚é€šè¿‡åœ¨åˆ†å­åŠ¨åŠ›å­¦ (molecular dynamics) å’Œç»Ÿè®¡ç‰©ç† (statistical physics) ç­‰æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼Œè¯æ˜äº† PDNS åœ¨å¤„ç†å¤æ‚åˆ†å¸ƒæ—¶å…·æœ‰å“è¶Šçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03824v1",
      "published_date": "2025-10-04 14:44:47 UTC",
      "updated_date": "2025-10-04 14:44:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:23.661850+00:00"
    },
    {
      "arxiv_id": "2510.03814v3",
      "title": "Detecting Invariant Manifolds in ReLU-Based RNNs",
      "title_zh": "æ¢æµ‹åŸºäº ReLU çš„å¾ªç¯ç¥ç»ç½‘ç»œä¸­çš„ä¸å˜æµå½¢",
      "authors": [
        "Lukas Eisenmann",
        "Alena BrÃ¤ndle",
        "Zahra Monfared",
        "Daniel Durstewitz"
      ],
      "abstract": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åœ¨æœºå™¨å­¦ä¹ å’ŒåŠ¨åŠ›ç³»ç»Ÿé‡æ„ä¸­å¹¿æ³›åº”ç”¨çš„å¾ªç¯ç¥ç»ç½‘ç»œ(RNNs)ï¼Œæ¢è®¨äº†å…¶åŠ¨åŠ›å­¦è¡Œä¸ºèƒŒåçš„æ‹“æ‰‘ä¸å‡ ä½•ç‰¹æ€§ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºæ£€æµ‹åŸºäºçº¿æ€§æ•´æµå•å…ƒ(ReLUs)çš„åˆ†æ®µçº¿æ€§å¾ªç¯ç¥ç»ç½‘ç»œ(PLRNNs)ä¸­ä¸å˜æµå½¢(Invariant Manifolds)çš„æ–°ç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿç²¾ç¡®è¿½è¸ªä¸åŒå¸å¼•ç›†(Basins of Attraction)ä¹‹é—´çš„è¾¹ç•Œï¼Œä»è€Œæœ‰æ•ˆè¡¨å¾å¤šç¨³æ€(Multistability)è¿™ä¸€é‡è¦çš„è®¡ç®—ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¯¥ç®—æ³•å¯»æ‰¾ç¨³å®šæµå½¢ä¸ä¸ç¨³å®šæµå½¢çš„äº¤ç‚¹ï¼Œå³åŒå®¿ç‚¹(Homoclinic Points)ï¼Œè¿›è€Œè¯å®äº†PLRNNsä¸­å­˜åœ¨æ··æ²Œ(Chaos)åŠ¨åŠ›å­¦ã€‚æœ€åï¼Œé€šè¿‡çš®å±‚ç¥ç»å…ƒç”µç”Ÿç†è®°å½•çš„å®è¯æ¡ˆä¾‹ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ­ç¤ºåº•å±‚åŠ¨åŠ›å­¦æœºåˆ¶æ–¹é¢çš„å®ç”¨ä»·å€¼ã€‚è¿™ä¸€æˆæœä¸ºæ·±å…¥ç†è§£å—è®­RNNsçš„å†…éƒ¨è¿ä½œä»¥åŠæå‡å¯è§£é‡Šäººå·¥æ™ºèƒ½(Explainable AI)æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03814v3",
      "published_date": "2025-10-04 13:55:19 UTC",
      "updated_date": "2025-12-03 07:28:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:18.552817+00:00"
    },
    {
      "arxiv_id": "2510.03813v2",
      "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization",
      "title_zh": "åŸºäºå¯¹æ¯”å™ªå£°ä¼˜åŒ–çš„å¤šæ ·åŒ–æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ",
      "authors": [
        "Byungjun Kim",
        "Soobin Um",
        "Jong Chul Ye"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have demonstrated impressive performance in generating high-fidelity images, largely enabled by text-guided inference. However, this advantage often comes with a critical drawback: limited diversity, as outputs tend to collapse into similar modes under strong text guidance. Existing approaches typically optimize intermediate latents or text conditions during inference, but these methods deliver only modest gains or remain sensitive to hyperparameter tuning. In this work, we introduce Contrastive Noise Optimization, a simple yet effective method that addresses the diversity issue from a distinct perspective. Unlike prior techniques that adapt intermediate latents, our approach shapes the initial noise to promote diverse outputs. Specifically, we develop a contrastive loss defined in the Tweedie data space and optimize a batch of noise latents. Our contrastive optimization repels instances within the batch to maximize diversity while keeping them anchored to a reference sample to preserve fidelity. We further provide theoretical insights into the mechanism of this preprocessing to substantiate its effectiveness. Extensive experiments across multiple T2I backbones demonstrate that our approach achieves a superior quality-diversity Pareto frontier while remaining robust to hyperparameter choices.",
      "tldr_zh": "æ–‡æœ¬ç”Ÿæˆå›¾åƒ (T2I) æ‰©æ•£æ¨¡å‹åœ¨å¼ºæ–‡æœ¬å¼•å¯¼ä¸‹å¾€å¾€é¢ä¸´ç”Ÿæˆå¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œè¾“å‡ºç»“æœå®¹æ˜“é™·å…¥ç›¸ä¼¼çš„æ¨¡å¼ã€‚è¯¥ç ”ç©¶æå‡ºäº† Contrastive Noise Optimizationï¼Œä¸€ç§é€šè¿‡ä¼˜åŒ–åˆå§‹å™ªå£°è€Œéä¸­é—´æ½œå˜é‡æ¥æå‡ç”Ÿæˆå¤šæ ·æ€§çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ Tweedie æ•°æ®ç©ºé—´å†…å®šä¹‰ Contrastive Lossï¼Œé€šè¿‡ä¼˜åŒ–ä¸€æ‰¹å™ªå£°æ½œå˜é‡ä½¿æ‰¹æ¬¡å†…çš„å®ä¾‹ç›¸äº’æ’æ–¥ä»¥æœ€å¤§åŒ–å¤šæ ·æ€§ï¼ŒåŒæ—¶å°†å…¶é”šå®šåœ¨å‚è€ƒæ ·æœ¬ä¸Šä»¥ä¿æŒå›¾åƒä¿çœŸåº¦ (Fidelity)ã€‚ç†è®ºåˆ†æä¸å¤šé¡¹ T2I ä¸»å¹²ç½‘ç»œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä¸å¤šæ ·æ€§ä¹‹é—´è¾¾åˆ°äº†ä¼˜å¼‚çš„ Pareto frontierã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆå¯¹è¶…å‚æ•°é€‰æ‹©å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼Œä¸ºè§£å†³æ‰©æ•£ç”Ÿæˆä¸­çš„æ¨¡å¼åç¼©é—®é¢˜æä¾›äº†ä¸€ä¸ªé«˜æ•ˆä¸”ç®€æ´çš„è§†è§’ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03813v2",
      "published_date": "2025-10-04 13:51:32 UTC",
      "updated_date": "2025-10-11 04:37:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:21.948545+00:00"
    },
    {
      "arxiv_id": "2510.03812v1",
      "title": "ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs",
      "title_zh": "ReTiDeï¼šåŸºäº FPGA çš„é«˜èƒ½æ•ˆåŠ¨æ€å½±åƒå¤„ç†å®æ—¶å»å™ª",
      "authors": [
        "Changhong Li",
        "ClÃ©ment Bled",
        "Rosa Fernandez",
        "Shreejith Shanker"
      ],
      "abstract": "Denoising is a core operation in modern video pipelines. In codecs, in-loop filters suppress sensor noise and quantisation artefacts to improve rate-distortion performance; in cinema post-production, denoisers are used for restoration, grain management, and plate clean-up. However, state-of-the-art deep denoisers are computationally intensive and, at scale, are typically deployed on GPUs, incurring high power and cost for real-time, high-resolution streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated denoising system that serves inference on data-centre Field Programmable Gate Arrays (FPGAs). A compact convolutional model is quantised (post-training quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration offloads computation from the host CPU/GPU to a networked FPGA service, while remaining callable from existing workflows, e.g., NUKE, without disrupting artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\\times$ Giga Operations Per Second (GOPS) throughput and 5.29$\\times$ higher energy efficiency than prior FPGA denoising accelerators, with negligible degradation in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These results indicate that specialised accelerators can provide practical, scalable denoising for both encoding pipelines and post-production, reducing energy per frame without sacrificing quality or workflow compatibility. Code is available at https://github.com/RCSL-TCD/ReTiDe.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReTiDeï¼Œä¸€ç§æ—¨åœ¨å®ç°èƒ½æ•ˆä¸å®æ—¶æ€§å¹³è¡¡çš„ FPGA ç¡¬ä»¶åŠ é€Ÿé™å™ªç³»ç»Ÿï¼Œä»¥è§£å†³æ·±åº¦å­¦ä¹ é™å™ªæ¨¡å‹åœ¨ GPU ä¸Šéƒ¨ç½²æ—¶åŠŸè€—ä¸æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å¯¹ç´§å‡‘å‹å·ç§¯æ¨¡å‹è¿›è¡Œè®­ç»ƒåé‡åŒ– (Post-training Quantization) å’Œé‡åŒ–æ„ŸçŸ¥å¾®è°ƒ (Quantization-aware Fine-tuning) è‡³ INT8 æ ¼å¼ï¼Œå¹¶éƒ¨ç½²äºåŸºäº AMD Deep Learning Processor Unit (DPU) çš„ FPGA ç¡¬ä»¶ã€‚ReTiDe é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼Œå¯å°†è®¡ç®—è´Ÿè½½ä» CPU/GPU å¸è½½è‡³ç½‘ç»œåŒ– FPGA æœåŠ¡ï¼Œä¸”èƒ½å¤Ÿæ— ç¼å…¼å®¹ NUKE ç­‰ä¸“ä¸šè§†é¢‘åæœŸå·¥ä½œæµã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ Peak Signal-to-Noise Ratio (PSNR) å’Œ Structural Similarity Index (SSIM) æŸå¤±æå°çš„å‰æä¸‹ï¼Œå…¶ååé‡è¾¾åˆ° 37.71$\\times$ Giga Operations Per Second (GOPS)ï¼Œèƒ½æ•ˆæ¯”æ­¤å‰çš„ FPGA åŠ é€Ÿå™¨æå‡äº† 5.29$\\times$ã€‚è¿™ä¸€æˆæœè¯æ˜äº†ä¸“ç”¨åŠ é€Ÿå™¨åœ¨è§†é¢‘ç¼–ç æµæ°´çº¿å’Œç”µå½±åæœŸåˆ¶ä½œä¸­æä¾›é«˜æ€§èƒ½ã€å¯æ‰©å±•ä¸”ä½èƒ½è€—é™å™ªå¤„ç†çš„å®é™…å¯è¡Œæ€§ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "This paper has been accepted by the 22nd ACM SIGGRAPH European Conference on Visual Media Production (CVMP 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03812v1",
      "published_date": "2025-10-04 13:43:43 UTC",
      "updated_date": "2025-10-04 13:43:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:25.349714+00:00"
    },
    {
      "arxiv_id": "2510.03807v2",
      "title": "6G-Enabled Digital Twin Framework for Real-Time Cyber-Physical Systems: An Experimental Validation with Industrial Bearing Fault Detection",
      "title_zh": "é¢å‘å®æ—¶ä¿¡æ¯ç‰©ç†ç³»ç»Ÿçš„ 6G æ•°å­—å­ªç”Ÿæ¡†æ¶ï¼šå·¥ä¸šè½´æ‰¿æ•…éšœæ£€æµ‹å®éªŒéªŒè¯",
      "authors": [
        "Vaskar Chakma",
        "Wooyeol Choi"
      ],
      "abstract": "Current Cyber-Physical Systems (CPS) integrated with Digital Twin (DT) technology face critical limitations in achieving real-time performance for mission-critical industrial applications. Existing 5G-enabled systems suffer from latencies exceeding 10ms, which are inadequate for applications requiring sub-millisecond response times, such as autonomous industrial control and predictive maintenance. This research aims to develop and validate a 6G-enabled Digital Twin framework that achieves ultra-low latency communication and real-time synchronization between physical industrial assets and their digital counterparts, specifically targeting bearing fault detection as a critical industrial use case. The proposed framework integrates terahertz communications (0.1-1 THz), intelligent reflecting surfaces, and edge artificial intelligence within a five-layer architecture. Experimental validation was conducted using the Case Western Reserve University (CWRU) bearing dataset, implementing comprehensive feature extraction (15 time and frequency domain features) and Random Forest classification algorithms. The system performance was evaluated against traditional WiFi-6 and 5G networks across multiple metrics, including classification accuracy, end-to-end latency, and scalability. It achieved 97.7% fault classification accuracy with 0.8ms end-to-end latency, representing a 15.6x improvement over WiFi-6 (12.5ms) and 5.25x improvement over 5G (4.2ms) networks. The system demonstrated superior scalability with sub-linear processing time growth and maintained consistent performance across four bearing fault categories (normal, inner race, outer race, and ball faults) with macro-averaged F1-scores exceeding 97%.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¯æŒ6Gçš„æ•°å­—å­ªç”Ÿ(6G-enabled Digital Twin)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰èµ›åšç‰©ç†ç³»ç»Ÿ(Cyber-Physical Systems)åœ¨é›†æˆæ•°å­—å­ªç”ŸæŠ€æœ¯æ—¶é¢ä¸´çš„å®æ—¶æ€§ç“¶é¢ˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº”å±‚æ¶æ„ï¼Œé›†æˆäº†å¤ªèµ«å…¹é€šä¿¡(terahertz communications)ã€æ™ºèƒ½åå°„è¡¨é¢(intelligent reflecting surfaces)ä»¥åŠè¾¹ç¼˜äººå·¥æ™ºèƒ½(edge artificial intelligence)ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚ç ”ç©¶ä»¥å·¥ä¸šè½´æ‰¿æ•…éšœæ£€æµ‹(bearing fault detection)ä¸ºå…·ä½“åº”ç”¨åœºæ™¯ï¼Œé€šè¿‡CWRUè½´æ‰¿æ•°æ®é›†å¹¶ç»“åˆç‰¹å¾æå–ä¸éšæœºæ£®æ—(Random Forest)ç®—æ³•è¿›è¡Œäº†å®éªŒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå®ç°äº†97.7%çš„æ•…éšœåˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸”ç«¯åˆ°ç«¯å»¶è¿Ÿ(end-to-end latency)æ˜¾è‘—é™ä½è‡³0.8msï¼Œç›¸æ¯”5Gç½‘ç»œæ€§èƒ½æå‡äº†5.25å€ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿåœ¨å¤„ç†å››ç§è½´æ‰¿æ•…éšœç±»åˆ«æ—¶å‡è¡¨ç°å‡ºæé«˜çš„ç¨³å®šæ€§ï¼Œå®å¹³å‡F1åˆ†æ•°è¶…è¿‡97%ã€‚è¯¥æ¡†æ¶è¿˜å±•ç°å‡ºä¼˜å¼‚çš„å¯æ‰©å±•æ€§ï¼Œå…¶å¤„ç†æ—¶é—´å¢é•¿å‘ˆäºšçº¿æ€§ï¼Œä¸ºå®ç°äºšæ¯«ç§’çº§å“åº”çš„è‡ªä¸»å·¥ä¸šæ§åˆ¶å’Œé¢„æµ‹æ€§ç»´æŠ¤å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "We no longer stand by the results as presented and prefer to retract the work publicly rather than allow continued citation of an invalid claim. We apologize for any inconvenience to the community. A corrected or substantially revised version may be submitted later under a new identifier",
      "pdf_url": "https://arxiv.org/pdf/2510.03807v2",
      "published_date": "2025-10-04 13:29:56 UTC",
      "updated_date": "2025-10-10 05:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:28.143147+00:00"
    },
    {
      "arxiv_id": "2510.03805v3",
      "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models",
      "title_zh": "è¶…è¶Š Token é•¿åº¦ï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆå‡†ç¡®æ¨ç†çš„ Step Pruner",
      "authors": [
        "Canhui Wu",
        "Qiong Cao",
        "Chang Li",
        "Zhenfang Wang",
        "Chao Xue",
        "Yuwei Fan",
        "Wei Xi",
        "Xiaodong He"
      ],
      "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks but often suffer from excessive verbosity, known as \"overthinking.\" Existing solutions via reinforcement learning (RL) typically penalize generated tokens to promote conciseness. However, these methods encounter two challenges: responses with fewer tokens do not always correspond to fewer reasoning steps, and models may develop hacking behavior in later stages of training by discarding reasoning steps to minimize token usage. In this work, we introduce \\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more efficient reasoning by favoring compact reasoning steps. Our step-aware reward function prioritizes correctness while imposing penalties for redundant steps, and withholds rewards for incorrect responses to prevent the reinforcement of erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when the model's output no longer shortens, training is halted to prevent hacking behavior caused by the merging of steps. Extensive experiments across four reasoning benchmarks demonstrate that SP achieves state-of-the-art accuracy while significantly reducing response length. For instance, on AIME24, SP reduces token usage by \\textbf{69.7\\%}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§æ¨ç†æ¨¡å‹ï¼ˆLarge Reasoning Models, LRMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­æ™®éå­˜åœ¨çš„â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆoverthinkingï¼‰åŠå›ç­”è¿‡äºå†—é•¿çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºStep Pruner (SP)çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯å¼•å¯¼æ¨¡å‹å‘æ›´é«˜æ•ˆçš„æ¨ç†æ­¥éª¤é æ‹¢ï¼Œè€Œéç®€å•åœ°æƒ©ç½šTokené•¿åº¦ã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ç§æ­¥éª¤æ„ŸçŸ¥å¥–åŠ±å‡½æ•°ï¼Œåœ¨ä¼˜å…ˆä¿è¯æ­£ç¡®æ€§çš„åŒæ—¶æƒ©ç½šå†—ä½™æ­¥éª¤ï¼Œå¹¶é€šè¿‡åŠ¨æ€åœæ­¢æœºåˆ¶æœ‰æ•ˆé˜²æ­¢äº†æ¨¡å‹å› è¿‡åº¦ç²¾ç®€è€Œäº§ç”Ÿçš„æ¨ç†æ­¥éª¤åˆå¹¶ç­‰â€œä½œå¼Šâ€è¡Œä¸ºã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼ŒSPåœ¨å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ï¼Œå¹¶å¤§å¹…ç¼©å‡äº†å“åº”é•¿åº¦ã€‚ç‰¹åˆ«æ˜¯åœ¨AIME24åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSPæˆåŠŸå‡å°‘äº†69.7%çš„Tokenæ¶ˆè€—ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†çš„æ•ˆç‡ä¸ç²¾ç¡®åº¦ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03805v3",
      "published_date": "2025-10-04 13:24:26 UTC",
      "updated_date": "2025-11-29 03:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:32.354158+00:00"
    },
    {
      "arxiv_id": "2510.03799v1",
      "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models",
      "title_zh": "è¯­è¨€æ¨¡å‹ä¸­ç¤¾ä¼šæ”¿æ²»æ¡†æ¶çš„æœºæ¢°å¯è§£é‡Šæ€§",
      "authors": [
        "Hadi Asghari",
        "Sami Nenno"
      ],
      "abstract": "This paper explores the ability of large language models to generate and recognize deep cognitive frames, particularly in socio-political contexts. We demonstrate that LLMs are highly fluent in generating texts that evoke specific frames and can recognize these frames in zero-shot settings. Inspired by mechanistic interpretability research, we investigate the location of the `strict father' and `nurturing parent' frames within the model's hidden representation, identifying singular dimensions that correlate strongly with their presence. Our findings contribute to understanding how LLMs capture and express meaningful human concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ç¤¾ä¼šæ”¿æ²»èƒŒæ™¯ä¸‹ç”Ÿæˆå’Œè¯†åˆ«æ·±å±‚è®¤çŸ¥æ¡†æ¶çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜LLMsèƒ½å¤Ÿæµç•…åœ°ç”Ÿæˆå”¤èµ·ç‰¹å®šæ¡†æ¶çš„æ–‡æœ¬ï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬(zero-shot)è®¾ç½®ä¸‹å‡†ç¡®è¯†åˆ«è¿™äº›æ¡†æ¶ã€‚å—æœºæ¢°è§£é‡Šæ€§(mechanistic interpretability)ç ”ç©¶å¯å‘ï¼Œä½œè€…æ·±å…¥è°ƒæŸ¥äº†æ¨¡å‹éšè—è¡¨ç¤ºä¸­â€œstrict fatherâ€å’Œâ€œnurturing parentâ€æ¡†æ¶çš„å®šä½ã€‚é€šè¿‡åˆ†æï¼Œç ”ç©¶æˆåŠŸè¯†åˆ«å‡ºäº†ä¸è¿™äº›æ¡†æ¶é«˜åº¦ç›¸å…³çš„å•ä¸€ç»´åº¦ã€‚è¿™äº›å‘ç°ä¸ºç†è§£LLMså¦‚ä½•æ•æ‰å’Œè¡¨è¾¾å…·æœ‰æ„ä¹‰çš„äººç±»æ¦‚å¿µæä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "Peer-reviewed and presented at Advances in Interpretable Machine Learning and Artificial Intelligence (AIMLAI) Workshop at ECML/PKDD 2024",
      "pdf_url": "https://arxiv.org/pdf/2510.03799v1",
      "published_date": "2025-10-04 12:39:39 UTC",
      "updated_date": "2025-10-04 12:39:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:34.847286+00:00"
    },
    {
      "arxiv_id": "2510.03788v1",
      "title": "Lightweight and Data-Efficient MultivariateTime Series Forecasting using Residual-Stacked Gaussian (RS-GLinear) Architecture",
      "title_zh": "åŸºäºæ®‹å·®å †å é«˜æ–¯ (RS-GLinear) æ¶æ„çš„è½»é‡çº§ã€æ•°æ®é«˜æ•ˆå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Abukar Ali"
      ],
      "abstract": "Following the success of Transformer architectures in language modeling, particularly their ability to capture long-range dependencies, researchers have explored how these architectures can be adapted for time-series forecasting. Transformer-based models have been proposed to handle both short- and long-term dependencies when predicting future values from historical data. However, studies such as those by Zeng et al. (2022) and Rizvi et al. (2025) have reported mixed results in long-term forecasting tasks. In this work, we evaluate the Gaussian-based Linear architecture introduced by Rizvi et al. (2025) and present an enhanced version called the Residual Stacked Gaussian Linear (RSGL) model. We also investigate the broader applicability of the RSGL model in additional domains, including financial time series and epidemiological data. Experimental results show that the RSGL model achieves improved prediction accuracy and robustness compared to both the baseline Gaussian Linear and Transformer-based models.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹ Transformer æ¶æ„åœ¨é•¿æœŸæ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä¸€çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¢å¼ºå‹æ¶æ„ Residual Stacked Gaussian Linear (RSGL)ã€‚è¯¥æ¨¡å‹æ˜¯åœ¨ Gaussian-based Linear æ¶æ„åŸºç¡€ä¸Šçš„æ”¹è¿›ï¼Œä¸“æ³¨äºå®ç°è½»é‡çº§ä¸”æ•°æ®é«˜æ•ˆçš„å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ã€‚ç ”ç©¶ä¸ä»…éªŒè¯äº† RSGL çš„åŸºæœ¬æ€§èƒ½ï¼Œè¿˜å°†å…¶åº”ç”¨æ‰©å±•è‡³é‡‘èæ—¶é—´åºåˆ—å’Œæµè¡Œç—…å­¦æ•°æ®ç­‰å¤šä¸ªé¢†åŸŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRSGL åœ¨é¢„æµ‹å‡†ç¡®æ€§å’Œé²æ£’æ€§ä¸Šå‡ä¼˜äºåŸºå‡†çš„ Gaussian Linear ä»¥åŠä¼ ç»Ÿçš„ Transformer æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºé•¿çŸ­æœŸä¾èµ–å…³ç³»çš„å»ºæ¨¡æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆä¸”å…·é²æ£’æ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03788v1",
      "published_date": "2025-10-04 11:44:29 UTC",
      "updated_date": "2025-10-04 11:44:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:38.952303+00:00"
    },
    {
      "arxiv_id": "2510.03781v1",
      "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development",
      "title_zh": "Rezwanï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œåœ£è®­æ–‡æœ¬ç»¼åˆå¤„ç†åŠ120ä¸‡è§„æ¨¡è¯­æ–™åº“çš„æ„å»º",
      "authors": [
        "Majid Asgari-Bidhendi",
        "Muhammad Amin Ghaseminia",
        "Alireza Shahbazi",
        "Sayyed Ali Hossayni",
        "Najmeh Torabian",
        "Behrouz Minaei-Bidgoli"
      ],
      "abstract": "This paper presents the development of Rezwan, a large-scale AI-assisted Hadith corpus comprising over 1.2M narrations, extracted and structured through a fully automated pipeline. Building on digital repositories such as Maktabat Ahl al-Bayt, the pipeline employs Large Language Models (LLMs) for segmentation, chain--text separation, validation, and multi-layer enrichment. Each narration is enhanced with machine translation into twelve languages, intelligent diacritization, abstractive summarization, thematic tagging, and cross-text semantic analysis. This multi-step process transforms raw text into a richly annotated research-ready infrastructure for digital humanities and Islamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled narrations, assessed by six domain experts. Results show near-human accuracy in structured tasks such as chain--text separation (9.33/10) and summarization (9.33/10), while highlighting ongoing challenges in diacritization and semantic similarity detection. Comparative analysis against the manually curated Noor Corpus demonstrates the superiority of Najm in both scale and quality, with a mean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis confirms the economic feasibility of the AI approach: tasks requiring over 229,000 hours of expert labor were completed within months at a fraction of the cost. The work introduces a new paradigm in religious text processing by showing how AI can augment human expertise, enabling large-scale, multilingual, and semantically enriched access to Islamic heritage.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº† Rezwanï¼Œä¸€ä¸ªåŒ…å«è¶…è¿‡ 120 ä¸‡æ¡å™äº‹çš„å¤§è§„æ¨¡ AI è¾…åŠ©åœ£è®­ï¼ˆHadithï¼‰è¯­æ–™åº“ï¼Œé€šè¿‡åˆ©ç”¨ Large Language Models (LLMs) æ„å»ºçš„å…¨è‡ªåŠ¨åŒ–æµæ°´çº¿å®ç°äº†æ–‡æœ¬çš„æå–ä¸ç»“æ„åŒ–ã€‚è¯¥æµæ°´çº¿æ¶µç›–äº†æ–‡æœ¬åˆ†å‰²ã€ä¼ è¿°é“¾ä¸æ­£æ–‡åˆ†ç¦»ï¼ˆchain-text separationï¼‰ã€12 ç§è¯­è¨€çš„æœºå™¨ç¿»è¯‘ã€æ™ºèƒ½åŠ ç¬¦ï¼ˆdiacritizationï¼‰ä»¥åŠä¸»é¢˜æ ‡æ³¨ç­‰æ ¸å¿ƒå¤„ç†ç¯èŠ‚ã€‚ç»è¿‡é¢†åŸŸä¸“å®¶çš„è¯„ä¼°ï¼ŒRezwan åœ¨ç»“æ„åŒ–å¤„ç†å’Œæ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘äººç±»çš„å‡†ç¡®ç‡ï¼ˆ9.33/10ï¼‰ï¼Œåœ¨è´¨é‡å’Œè§„æ¨¡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„äººå·¥ç­–åˆ’åº“ Noor Corpusã€‚æ­¤å¤–ï¼Œæˆæœ¬åˆ†æè¯æ˜äº† AI æ–¹æ³•çš„ç»æµå¯è¡Œæ€§ï¼Œå°†åŸæœ¬éœ€è¦ 22.9 ä¸‡å°æ—¶ä¸“å®¶åŠ³åŠ¨çš„ä»»åŠ¡ç¼©çŸ­è‡³æ•°æœˆå†…å®Œæˆã€‚è¯¥å·¥ä½œä¸ºå®—æ•™æ–‡æœ¬å¤„ç†å’Œæ•°å­—äººæ–‡ç ”ç©¶å¼•å…¥äº†æ–°èŒƒå¼ï¼Œæå¤§åœ°å¢å¼ºäº†å¯¹ä¼Šæ–¯å…°æ–‡åŒ–é—äº§è¿›è¡Œå¤§è§„æ¨¡ã€è¯­ä¹‰ä¸°å¯ŒåŒ–è®¿é—®çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03781v1",
      "published_date": "2025-10-04 11:09:10 UTC",
      "updated_date": "2025-10-04 11:09:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:40.553302+00:00"
    },
    {
      "arxiv_id": "2510.03777v1",
      "title": "GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time",
      "title_zh": "GuidedSamplingï¼šæ¨ç†é˜¶æ®µå¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–å€™é€‰è§£",
      "authors": [
        "Divij Handa",
        "Mihir Parmar",
        "Aswin RRV",
        "Md Nayem Uddin",
        "Hamid Palangi",
        "Chitta Baral"
      ],
      "abstract": "Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -> 3.03), yielding a diverse set of candidates than traditional RS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Repeated Sampling (RS) åœ¨æ¨ç†ç«¯é¢ä¸´çš„å€™é€‰è§£ç¼ºä¹å¤šæ ·æ€§ä¸”è§£æ³•å†—ä½™çš„é—®é¢˜ï¼Œæå‡ºäº† GuidedSampling æ¨ç†ç®—æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¨ç†è¿‡ç¨‹è§£è€¦ä¸ºæ¢ç´¢ (exploration) å’Œç”Ÿæˆ (generation) ä¸¤ä¸ªé˜¶æ®µï¼Œæ˜¾è‘—å¢åŠ äº†ç”Ÿæˆæ–¹æ¡ˆçš„å¹¿åº¦ã€‚åœ¨æ¢ç´¢é˜¶æ®µï¼Œç®—æ³•è´Ÿè´£è¯†åˆ«è§£å†³é—®é¢˜çš„å¤šç§ä¸åŒæ¦‚å¿µ (concepts)ï¼Œè€Œç”Ÿæˆé˜¶æ®µåˆ™åŸºäºç‰¹å®šæ¦‚å¿µè¾“å‡ºæœ€ç»ˆçš„å€™é€‰è§£ã€‚ç†è®ºåˆ†æç¡®å®šäº†è¯¥ç®—æ³•çš„è¾¹ç•Œï¼Œå®è¯ç»“æœè¡¨æ˜ï¼ŒGuidedSampling åœ¨ pass@50 æŒ‡æ ‡ä¸Šæ¯”ä¼ ç»Ÿçš„ RS å¹³å‡æ€§èƒ½æå‡äº†çº¦ 21.6%ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨ GuidedSampling è½¨è¿¹å¾®è°ƒçš„æ¨¡å‹åœ¨ pass@5 ä¸Šè¡¨ç°æ›´ä½³ï¼Œä¸”æ¯ä¸ªå®ä¾‹ç”Ÿæˆçš„æ¦‚å¿µæ•°é‡ä» 1.67 æå‡è‡³ 3.03ï¼Œè¯æ˜äº†å…¶åœ¨å¼•å¯¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¢ç´¢å¤šå…ƒåŒ–è§£æ³•æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03777v1",
      "published_date": "2025-10-04 11:02:39 UTC",
      "updated_date": "2025-10-04 11:02:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:56.550778+00:00"
    },
    {
      "arxiv_id": "2510.08591v1",
      "title": "The Enduring Dominance of Deep Neural Networks: A Critical Analysis of the Fundamental Limitations of Quantum Machine Learning and Spiking Neural Networks",
      "title_zh": "æ·±åº¦ç¥ç»ç½‘ç»œçš„æŒä¹…ä¸»å¯¼åœ°ä½ï¼šé‡å­æœºå™¨å­¦ä¹ ä¸è„‰å†²ç¥ç»ç½‘ç»œæ ¹æœ¬å±€é™æ€§çš„æ‰¹åˆ¤æ€§åˆ†æ",
      "authors": [
        "Takehiro Ishikawa"
      ],
      "abstract": "Recent advancements in QML and SNNs have generated considerable excitement, promising exponential speedups and brain-like energy efficiency to revolutionize AI. However, this paper argues that they are unlikely to displace DNNs in the near term. QML struggles with adapting backpropagation due to unitary constraints, measurement-induced state collapse, barren plateaus, and high measurement overheads, exacerbated by the limitations of current noisy intermediate-scale quantum hardware, overfitting risks due to underdeveloped regularization techniques, and a fundamental misalignment with machine learning's generalization. SNNs face restricted representational bandwidth, struggling with long-range dependencies and semantic encoding in language tasks due to their discrete, spike-based processing. Furthermore, the goal of faithfully emulating the brain might impose inherent inefficiencies like cognitive biases, limited working memory, and slow learning speeds. Even their touted energy-efficient advantages are overstated; optimized DNNs with quantization can outperform SNNs in energy costs under realistic conditions. Finally, SNN training incurs high computational overhead from temporal unfolding. In contrast, DNNs leverage efficient backpropagation, robust regularization, and innovations in LRMs that shift scaling to inference-time compute, enabling self-improvement via RL and search algorithms like MCTS while mitigating data scarcity. This superiority is evidenced by recent models such as xAI's Grok-4 Heavy, which advances SOTA performance, and gpt-oss-120b, which surpasses or approaches the performance of leading industry models despite its modest 120-billion-parameter size deployable on a single 80GB GPU. Furthermore, specialized ASICs amplify these efficiency gains. Ultimately, QML and SNNs may serve niche hybrid roles, but DNNs remain the dominant, practical paradigm for AI advancement.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰¹åˆ¤æ€§åœ°åˆ†æäº†é‡å­æœºå™¨å­¦ä¹ (Quantum Machine Learning, QML)å’Œè„‰å†²ç¥ç»ç½‘ç»œ(Spiking Neural Networks, SNNs)çš„å±€é™æ€§ï¼Œè®ºè¯äº†æ·±åº¦ç¥ç»ç½‘ç»œ(Deep Neural Networks, DNNs)åœ¨å½“å‰åŠæœªæ¥ä¸€æ®µæ—¶é—´å†…çš„ç»Ÿæ²»åœ°ä½ã€‚QMLå—é™äºé…‰çº¦æŸ(unitary constraints)ã€æµ‹é‡å¯¼è‡´çš„æ€åç¼©(state collapse)ä»¥åŠè´«ç˜ é«˜åŸ(barren plateaus)ç­‰æŠ€æœ¯ç“¶é¢ˆï¼Œéš¾ä»¥æœ‰æ•ˆåº”ç”¨åå‘ä¼ æ’­å¹¶é¢ä¸´ä¸¥é‡çš„ç¡¬ä»¶å™ªå£°å¹²æ‰°ã€‚SNNsåˆ™åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–å’Œè¯­ä¹‰ç¼–ç æ—¶é¢ä¸´è¡¨è¾¾å¸¦å®½å—é™çš„é—®é¢˜ï¼Œä¸”å…¶æ‰€è°“çš„èƒ½æ•ˆä¼˜åŠ¿åœ¨ç»è¿‡é‡åŒ–ä¼˜åŒ–çš„DNNsé¢å‰å¹¶ä¸å…·å¤‡ç»å¯¹ç«äº‰åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDNNsé€šè¿‡æ¨ç†æ—¶è®¡ç®—(inference-time compute)ã€å¼ºåŒ–å­¦ä¹ (RL)å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢(MCTS)ç­‰æŠ€æœ¯åˆ›æ–°æŒç»­çªç ´æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶åˆ©ç”¨ä¸“ç”¨é›†æˆç”µè·¯(ASICs)è¿›ä¸€æ­¥æ”¾å¤§æ•ˆç‡å¢ç›Šã€‚ä»¥Grok-4 Heavyå’Œgpt-oss-120bä¸ºä»£è¡¨çš„æœ€æ–°æ¨¡å‹å®è¯äº†DNNsåœ¨SOTAæ€§èƒ½å’Œå•GPUéƒ¨ç½²æ•ˆç‡ä¸Šçš„æ˜¾è‘—ä¼˜åŠ¿ã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œè™½ç„¶QMLå’ŒSNNså¯èƒ½åœ¨ç‰¹å®šé¢†åŸŸå‘æŒ¥åˆ©åŸºä½œç”¨ï¼Œä½†DNNsä¾ç„¶æ˜¯äººå·¥æ™ºèƒ½å‘å±•ä¸­æ— å¯æ›¿ä»£çš„ä¸»å¯¼èŒƒå¼ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.08591v1",
      "published_date": "2025-10-04 11:00:46 UTC",
      "updated_date": "2025-10-04 11:00:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:57.542560+00:00"
    },
    {
      "arxiv_id": "2510.03771v1",
      "title": "OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation",
      "title_zh": "OptAgentï¼šåŸºäºå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿçš„ç”µå­å•†åŠ¡æŸ¥è¯¢æ”¹å†™ä¼˜åŒ–",
      "authors": [
        "Divij Handa",
        "David Blincoe",
        "Orson Adams",
        "Yinlin Fu"
      ],
      "abstract": "Deploying capable and user-aligned LLM-based systems necessitates reliable evaluation. While LLMs excel in verifiable tasks like coding and mathematics, where gold-standard solutions are available, adoption remains challenging for subjective tasks that lack a single correct answer. E-commerce Query Rewriting (QR) is one such problem where determining whether a rewritten query properly captures the user intent is extremely difficult to figure out algorithmically. In this work, we introduce OptAgent, a novel framework that combines multi-agent simulations with genetic algorithms to verify and optimize queries for QR. Instead of relying on a static reward model or a single LLM judge, our approach uses multiple LLM-based agents, each acting as a simulated shopping customer, as a dynamic reward signal. The average of these agent-derived scores serves as an effective fitness function for an evolutionary algorithm that iteratively refines the user's initial query. We evaluate OptAgent on a dataset of 1000 real-world e-commerce queries in five different categories, and we observe an average improvement of 21.98% over the original user query and 3.36% over a Best-of-N LLM rewriting baseline.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† OptAgentï¼Œä¸€ç§ç»“åˆå¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿä¸é—ä¼ ç®—æ³• (genetic algorithms) çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–ç”µå­å•†åŠ¡ä¸­çš„æŸ¥è¯¢é‡å†™ (Query Rewriting, QR) ä»»åŠ¡ã€‚é’ˆå¯¹ç”µå­å•†åŠ¡æŸ¥è¯¢é‡å†™ç”±äºç¼ºä¹å”¯ä¸€æ ‡å‡†ç­”æ¡ˆè€Œéš¾ä»¥è¿›è¡Œç®—æ³•è¯„ä¼°çš„æŒ‘æˆ˜ï¼ŒOptAgent é€šè¿‡ä½¿ç”¨å¤šä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLM) çš„æ™ºèƒ½ä½“æ¨¡æ‹ŸçœŸå®è´­ç‰©è€…ï¼Œæ„å»ºäº†åŠ¨æ€å¥–åŠ±ä¿¡å·ã€‚è¿™äº›æ™ºèƒ½ä½“ç”Ÿæˆçš„å¹³å‡è¯„åˆ†ä½œä¸ºè¿›åŒ–ç®—æ³•çš„é€‚åº”åº¦å‡½æ•° (fitness function)ï¼Œä»è€Œå¯¹ç”¨æˆ·çš„åˆå§‹æŸ¥è¯¢è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒåœ¨åŒ…å« 1000 æ¡çœŸå®ç”µå•†æŸ¥è¯¢çš„æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º OptAgent ç›¸æ¯”åŸå§‹æŸ¥è¯¢å®ç°äº† 21.98% çš„å¹³å‡æ€§èƒ½æå‡ï¼Œå¹¶ä¼˜äº Best-of-N LLM é‡å†™åŸºçº¿æ¨¡å‹ 3.36%ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³ç¼ºä¹å®¢è§‚æ ‡å‡†çš„è¯„ä¼°éš¾é¢˜æä¾›äº†æ–°æ€è·¯ï¼Œå±•ç¤ºäº†å¤šæ™ºèƒ½ä½“æ¨¡æ‹Ÿåœ¨ä¼˜åŒ–å¤æ‚ä¸»è§‚ä»»åŠ¡ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03771v1",
      "published_date": "2025-10-04 10:41:09 UTC",
      "updated_date": "2025-10-04 10:41:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:46:58.303311+00:00"
    },
    {
      "arxiv_id": "2510.03763v1",
      "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization",
      "title_zh": "é€šè¿‡è‡ªé€‚åº”é‡‡æ ·-å¤ç”¨-æ··åˆåˆ†è§£æ¢¯åº¦åŠ é€Ÿé”åº¦æ„ŸçŸ¥æœ€å°åŒ–",
      "authors": [
        "Jiaxin Deng",
        "Junbiao Pang"
      ],
      "abstract": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles the computational cost of Stochastic Gradient Descent (SGD) by requiring twice the gradient calculations per optimization step. To mitigate this, we propose Adaptively sampling-Reusing-mixing decomposed gradients to significantly accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can be decomposed into the SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). Furthermore, we observe that the SGD gradient and PSF dynamically evolve during training, emphasizing the growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed to the reused PSF and the timely updated PSF still maintain the model's generalization ability. Extensive experiments show that ARSAM achieves state-of-the-art accuracies comparable to SAM across diverse network architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a speedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various challenge tasks (\\textit{e.g.}, human pose estimation, and model quantization) without sacrificing performance, demonstrating its broad practicality.% The code is publicly accessible at: https://github.com/ajiaaa/ARSAM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Sharpness-Aware Minimization (SAM)ç”±äºéœ€è¦åŒå€æ¢¯åº¦è®¡ç®—è€Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ARSAM (Adaptively sampling-Reusing-mixing decomposed gradients)åŠ é€Ÿç®—æ³•ã€‚ä½œè€…é¦–å…ˆæ­ç¤ºäº†SAMçš„æ¢¯åº¦å¯åˆ†è§£ä¸ºSGDæ¢¯åº¦ä¸äºŒé˜¶æ¢¯åº¦åœ¨ä¸€é˜¶æ¢¯åº¦ä¸Šçš„æŠ•å½±(Projection of the Second-order gradient onto the First-order gradient, PSF)ä¸¤éƒ¨åˆ†ã€‚é€šè¿‡è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦çš„åŠ¨æ€æ¼”åŒ–ï¼Œå‘ç°PSFåœ¨å¯»æ‰¾å¹³å¦æå°å€¼(flat minima)ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œæ®æ­¤ARSAMé‡‡ç”¨è‡ªé€‚åº”é‡ç”¨å’ŒåŠæ—¶æ›´æ–°PSFçš„ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒARSAMåœ¨CIFAR-10/100æ•°æ®é›†ä¸Šå®ç°äº†çº¦40%çš„åŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡ä¸åŸå§‹SAMç›¸å½“ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨äººä½“å§¿æ€ä¼°è®¡å’Œæ¨¡å‹é‡åŒ–ç­‰å¤šç§å¤æ‚ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„å®ç”¨æ€§å’Œæ€§èƒ½ä¿æŒèƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03763v1",
      "published_date": "2025-10-04 10:08:14 UTC",
      "updated_date": "2025-10-04 10:08:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:01.048657+00:00"
    },
    {
      "arxiv_id": "2510.03761v1",
      "title": "You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models",
      "title_zh": "LaTeXpOsEdï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é¢„å°æœ¬å¹³å°ä¿¡æ¯æ³„éœ²ç³»ç»Ÿåˆ†æ",
      "authors": [
        "Richard A. Dubniczky",
        "Bertalan Borsos",
        "Tihanyi Norbert"
      ],
      "abstract": "The widespread use of preprint repositories such as arXiv has accelerated the communication of scientific results but also introduced overlooked security risks. Beyond PDFs, these platforms provide unrestricted access to original source materials, including LaTeX sources, auxiliary code, figures, and embedded comments. In the absence of sanitization, submissions may disclose sensitive information that adversaries can harvest using open-source intelligence. In this work, we present the first large-scale security audit of preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to uncover hidden disclosures within non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25 state-of-the-art models. Our analysis uncovered thousands of PII leaks, GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders, editable private SharePoint links, exposed GitHub and Google credentials, and cloud API keys. We also uncovered confidential author communications, internal disagreements, and conference submission credentials, exposing information that poses serious reputational risks to both researchers and institutions. We urge the research community and repository operators to take immediate action to close these hidden security gaps. To support open science, we release all scripts and methods from this study but withhold sensitive findings that could be misused, in line with ethical principles. The source code and related material are available at the project website https://github.com/LaTeXpOsEd",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶å¯¹ arXiv ç­‰é¢„å°æœ¬åº“ä¸­è¢«å¿½è§†çš„ä¿¡æ¯æ³„éœ²é£é™©è¿›è¡Œäº†é¦–æ¬¡å¤§è§„æ¨¡ç³»ç»Ÿæ€§å®‰å…¨å®¡è®¡ã€‚ç ”ç©¶è€…æå‡ºäº†åä¸º LaTeXpOsEd çš„å››é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ¨¡å¼åŒ¹é…ã€é€»è¾‘è¿‡æ»¤ã€ä¼ ç»Ÿé‡‡é›†æŠ€æœ¯ä»¥åŠå¤§è¯­è¨€æ¨¡å‹ (LLMs)ï¼Œä¸“é—¨ç”¨äºæŒ–æ˜éå¼•ç”¨æ–‡ä»¶å’Œ LaTeX æ³¨é‡Šä¸­çš„éšè—æ•æ„Ÿä¿¡æ¯ã€‚é€šè¿‡å¯¹ 100,000 ç¯‡ arXiv æŠ•ç¨¿çš„ 1.2 TB æºç æ•°æ®è¿›è¡Œåˆ†æï¼Œå¹¶åˆ©ç”¨æ–°å¼€å‘çš„ LLMSec-DB åŸºå‡†æµ‹è¯•è¯„ä¼°äº† 25 ä¸ªå…ˆè¿›æ¨¡å‹çš„æ£€æµ‹èƒ½åŠ›ï¼Œè¯¥ç ”ç©¶å‘ç°äº†æ•°åƒä¸ªä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ (PII) æ³„éœ²ã€å¸¦æœ‰ GPS æ ‡è®°çš„ EXIF æ–‡ä»¶ã€ç§æœ‰äº‘å­˜å‚¨é“¾æ¥ä»¥åŠæš´éœ²çš„ API å¯†é’¥ã€‚æ­¤å¤–ï¼Œå®¡è®¡è¿˜æ­éœ²äº†æ¶‰åŠä½œè€…æœºå¯†é€šä¿¡å’Œå†…éƒ¨äº‰è®®ç­‰å¯èƒ½æŸå®³å­¦æœ¯å£°èª‰çš„ä¿¡æ¯æ³„éœ²ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†é¢„å°æœ¬å¹³å°äºŸéœ€åŠ å¼ºå®‰å…¨é˜²æŠ¤ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ç ”ç©¶è„šæœ¬å’Œæ–¹æ³•ä»¥æå‡å­¦æœ¯ç•Œçš„ç½‘ç»œå®‰å…¨æ„è¯†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03761v1",
      "published_date": "2025-10-04 10:03:17 UTC",
      "updated_date": "2025-10-04 10:03:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:05.256549+00:00"
    },
    {
      "arxiv_id": "2510.03760v1",
      "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models",
      "title_zh": "EvoEngineerï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç²¾é€šè‡ªåŠ¨åŒ– CUDA å†…æ ¸ä»£ç æ¼”åŒ–",
      "authors": [
        "Ping Guo",
        "Chenyu Zhu",
        "Siyuan Chen",
        "Fei Liu",
        "Xi Lin",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "abstract": "CUDA kernel optimization has become a critical bottleneck for AI performance, as deep learning training and inference efficiency directly depends on highly optimized GPU kernels.\n  Despite the promise of Large Language Models (LLMs) for automating kernel optimization, this field suffers from a fragmented ecosystem of isolated and incomparable approaches with unclear problem formulations.\n  Furthermore, general-purpose LLM code evolution methods cannot meet strict correctness requirements of CUDA kernel optimization.\n  We address these fundamental challenges by first formalizing CUDA kernel optimization as a code optimization task with a clear objective, constraints, and evaluation metrics.\n  We then establish the first systematic LLM-based code evolution framework, EvoEngineer, that provides guidance for designing and adapting optimization strategies to achieve a balance between performance and correctness.\n  Finally, we implement a kernel optimization system based on this framework and conduct extensive experiments on 91 real-world CUDA kernels.\n  Our results demonstrate that EvoEngineer achieves a principled balance between performance and correctness, with the highest averaged median speedup of \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of \\textbf{69.8}\\%, outperforming existing methods on both dimensions.\n  Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all operations over PyTorch kernels and delivers the highest speedup on \\textbf{28} (\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$} acceleration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹CUDA kernelä¼˜åŒ–åœ¨äººå·¥æ™ºèƒ½æ€§èƒ½ä¸­çš„ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ç³»ç»ŸåŒ–ä»£ç è¿›åŒ–æ¡†æ¶EvoEngineerã€‚ç ”ç©¶é¦–å…ˆå°†CUDA kernelä¼˜åŒ–æ­£å¼å®šä¹‰ä¸ºä¸€ç§å…·å¤‡æ˜ç¡®ç›®æ ‡ã€çº¦æŸå’Œè¯„ä¼°æŒ‡æ ‡çš„ä»£ç ä¼˜åŒ–ä»»åŠ¡ï¼Œè§£å†³äº†é¢†åŸŸå†…é—®é¢˜å®šä¹‰ä¸æ¸…æ™°åŠæ–¹æ³•ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚EvoEngineeræ¡†æ¶é€šè¿‡å¼•å¯¼è®¾è®¡å’Œè°ƒæ•´ä¼˜åŒ–ç­–ç•¥ï¼Œåœ¨æ»¡è¶³CUDAä»£ç ä¸¥æ ¼æ­£ç¡®æ€§è¦æ±‚çš„åŒæ—¶ï¼Œå®ç°äº†æ€§èƒ½ä¸æ­£ç¡®æ€§ä¹‹é—´çš„é«˜åº¦å¹³è¡¡ã€‚åœ¨å¯¹91ä¸ªçœŸå®ä¸–ç•ŒCUDA kernelçš„å¹¿æ³›å®éªŒä¸­ï¼ŒEvoEngineerå®ç°äº†2.72å€çš„å¹³å‡ä¸­ä½åŠ é€Ÿæ¯”ï¼Œä¸”ä»£ç æœ‰æ•ˆç‡è¾¾åˆ°69.8%ï¼Œåœ¨æ€§èƒ½å’Œæ­£ç¡®æ€§ä¸¤ä¸ªç»´åº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ç‰¹å®šæ“ä½œä¸Šå®ç°äº†æœ€é«˜36.75å€çš„åŠ é€Ÿï¼Œå¹¶ä¸ºå¯æ‰©å±•ã€è‡ªåŠ¨åŒ–çš„GPUå†…æ ¸ä»£ç æ¼”è¿›æä¾›äº†ç³»ç»ŸåŒ–çš„ç†è®ºæŒ‡å¯¼ä¸å®è·µæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review of ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2510.03760v1",
      "published_date": "2025-10-04 10:00:25 UTC",
      "updated_date": "2025-10-04 10:00:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:10.747461+00:00"
    },
    {
      "arxiv_id": "2510.03755v1",
      "title": "Code4MeV2: a Research-oriented Code-completion Platform",
      "title_zh": "Code4MeV2ï¼šé¢å‘ç ”ç©¶çš„ä»£ç è¡¥å…¨å¹³å°",
      "authors": [
        "Roham Koohestani",
        "Parham Bateni",
        "Aydin Ebrahimi",
        "Behdad Etezadi",
        "Kiarash Karimi",
        "Maliheh Izadi"
      ],
      "abstract": "The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates a barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on human--AI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4MeV2, a research-oriented, open-source code completion plugin for JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a client--server architecture and features inline code completion and a context-aware chat assistant. Its core contribution is a modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4MeV2 achieves industry-comparable performance in terms of code completion, with an average latency of 200~ms. We assess our tool through a combination of an expert evaluation and a user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found at https://app.code4me.me.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI-powered code completion å·¥å…·äº¤äº’æ•°æ®å¤šè¢«å¤§å…¬å¸å„æ–­è€Œå¯¼è‡´å­¦æœ¯ç•Œéš¾ä»¥è¿›è¡Œ human-AI interaction ç ”ç©¶çš„ç°çŠ¶ï¼Œæ¨å‡ºäº† Code4MeV2 å¼€æºå¹³å°ã€‚Code4MeV2 æ˜¯ä¸€ä¸ªä¸“é—¨é¢å‘ç ”ç©¶çš„ JetBrains IDEs æ’ä»¶ï¼Œé‡‡ç”¨ client-server æ¶æ„å¹¶é›†æˆäº† inline code completion ä¸ context-aware chat assistant åŠŸèƒ½ã€‚è¯¥å·¥å…·çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå…¶æ¨¡å—åŒ–ä¸”é€æ˜çš„ data collection æ¡†æ¶ï¼Œèµ‹äºˆç ”ç©¶äººå‘˜å¯¹ telemetry å’Œ context gathering çš„ç»†ç²’åº¦æ§åˆ¶æƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCode4MeV2 çš„ä»£ç è¡¥å…¨æ€§èƒ½å¯ä¸å·¥ä¸šç•Œäº§å“åª²ç¾ï¼Œå¹³å‡å»¶è¿Ÿä»…ä¸º 200msã€‚é€šè¿‡ä¸“å®¶è¯„å®¡ä¸ user study çš„åé¦ˆï¼ŒéªŒè¯äº†è¯¥å·¥å…·åœ¨ç ”ç©¶åœºæ™¯ä¸‹çš„ä¿¡æ¯ä¸°å¯Œåº¦ä¸å®ç”¨æ€§ï¼Œä¸ºå­¦æœ¯ç•Œå¼€å±•å¯é‡å¤çš„å¤§è§„æ¨¡æ•°æ®åˆ†ææä¾›äº†é‡è¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Under review for submission at a conference",
      "pdf_url": "https://arxiv.org/pdf/2510.03755v1",
      "published_date": "2025-10-04 09:40:43 UTC",
      "updated_date": "2025-10-04 09:40:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:11.347222+00:00"
    },
    {
      "arxiv_id": "2510.03748v1",
      "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation",
      "title_zh": "TreePromptï¼šé€šè¿‡å±‚çº§åŒ–å°‘æ ·æœ¬ç¤ºä¾‹é€‰æ‹©æå‡è‹±æ³¢ä¸è‹±å¾·ç¿»è¯‘æ€§èƒ½",
      "authors": [
        "Ramtin Kakavand",
        "Ebrahim Ansari"
      ],
      "abstract": "Large Language Models (LLMs) have consistently demonstrated strong performance in machine translation, especially when guided by high-quality prompts. Few-shot prompting is an effective technique to improve translation quality; however, most existing example selection methods focus solely on query-to-example similarity and do not account for the quality of the examples. In this work, we propose TreePrompt, a novel example selection approach that learns LLM preferences to identify high-quality, contextually relevant examples within a tree-structured framework. To further explore the balance between similarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN) and Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs - English-Persian (MIZAN) and English-German (WMT19) - show that integrating TreePrompt with AFSP or Random selection leads to improved translation performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æœºå™¨ç¿»è¯‘ä¸­ç°æœ‰å°‘æ ·æœ¬æç¤º(Few-shot prompting)ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ä»…å…³æ³¨ç›¸ä¼¼åº¦è€Œå¿½ç•¥è´¨é‡çš„é—®é¢˜ï¼Œæå‡ºäº†TreePromptã€‚ä½œä¸ºä¸€ç§æ–°å‹çš„é€‰æ‹©æ–¹æ³•ï¼ŒTreePromptåˆ©ç”¨æ ‘çŠ¶ç»“æ„æ¡†æ¶å­¦ä¹ LLMçš„åå¥½ï¼Œä»¥æ­¤è¯†åˆ«é«˜è´¨é‡ä¸”åœ¨ä¸Šä¸‹æ–‡ä¸­å…·æœ‰ç›¸å…³æ€§çš„ç¤ºä¾‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¹³è¡¡ç›¸ä¼¼æ€§ä¸è´¨é‡ï¼Œç ”ç©¶è€…å°†TreePromptä¸K-Nearest Neighbors (K-NN)ä»¥åŠAdaptive Few-Shot Prompting (AFSP)è¿›è¡Œäº†ç»“åˆã€‚é€šè¿‡åœ¨è‹±è¯­-æ³¢æ–¯è¯­(MIZAN)å’Œè‹±è¯­-å¾·è¯­(WMT19)ä¸¤ç»„è¯­è¨€å¯¹ä¸Šçš„è¯„ä¼°ï¼Œå®éªŒè¯æ˜å°†TreePromptä¸AFSPæˆ–éšæœºé€‰æ‹©(Random selection)é›†æˆèƒ½å¤Ÿæœ‰æ•ˆæå‡ç¿»è¯‘æ€§èƒ½ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†åœ¨åˆ†å±‚æ¡†æ¶ä¸­åˆ©ç”¨LLMåå¥½è¿›è¡Œç¤ºä¾‹é€‰æ‹©å¯¹äºæ”¹è¿›æœºå™¨ç¿»è¯‘è´¨é‡çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03748v1",
      "published_date": "2025-10-04 09:26:30 UTC",
      "updated_date": "2025-10-04 09:26:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:15.046776+00:00"
    },
    {
      "arxiv_id": "2510.03744v1",
      "title": "HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting",
      "title_zh": "HydroFusion-LMFï¼šç»“åˆå¤§æ¨¡å‹é€‚é…ä¸åŠç›‘ç£å¤šç½‘ç»œèåˆçš„é•¿æœŸé€æ—¥å¾„æµé¢„æµ‹",
      "authors": [
        "Qianfei Fan",
        "Jiayu Wei",
        "Peijun Zhu",
        "Wensheng Ye",
        "Meie Fang"
      ],
      "abstract": "Accurate decade-scale daily runoff forecasting in small watersheds is difficult because signals blend drifting trends, multi-scale seasonal cycles, regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet, PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single facets and under-utilize unlabeled spans, limiting regime adaptivity. We propose HydroFusion-LMF, a unified framework that (i) performs a learnable trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes residuals through a compact heterogeneous expert set (linear refinement, frequency kernel, patch Transformer, recurrent memory, dynamically normalized attention), (iii) fuses expert outputs via a hydrologic context-aware gate conditioned on day-of-year phase, antecedent precipitation, local variance, flood indicators, and static basin attributes, and (iv) augments supervision with a semi-supervised multi-task objective (composite MSE/MAE + extreme emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment, augmentation consistency, variance-filtered pseudo-labeling). Optional adapter / LoRA layers inject a frozen foundation time-series encoder efficiently. On a ~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818, improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions relative to baselines. The framework balances interpretability (explicit components, sparse gating) with performance, advancing label-efficient hydrologic forecasting under non-stationarity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HydroFusion-LMF æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°æµåŸŸåå¹´å°ºåº¦æ—¥å¾„æµé¢„æµ‹ä¸­ç”±äºè¶‹åŠ¿æ¼‚ç§»ã€å¤šå°ºåº¦å­£èŠ‚æ€§å‘¨æœŸå’Œæç«¯å€¼ç¨€ç–å¯¼è‡´çš„éå¹³ç¨³æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å¯å­¦ä¹ çš„è¶‹åŠ¿-å­£èŠ‚-æ®‹å·®åˆ†è§£(trend-seasonal-residual decomposition)é™ä½æ•°æ®å¤æ‚åº¦ï¼Œéšååˆ©ç”¨ä¸€ç»„åŒ…å«çº¿æ€§ç»†åŒ–ã€é¢‘ç‡æ ¸åŠ patch Transformer çš„å¼‚æ„ä¸“å®¶æ¨¡å‹(heterogeneous expert set)å¤„ç†æ®‹å·®ã€‚ä¸ºäº†å®ç°é«˜æ•ˆèåˆï¼Œç ”ç©¶è®¾è®¡äº†æ°´æ–‡èƒŒæ™¯æ„ŸçŸ¥é—¨æ§(hydrologic context-aware gate)ï¼Œç»“åˆæ°”è±¡æŒ‡æ ‡ä¸æµåŸŸå±æ€§åŠ¨æ€è°ƒèŠ‚ä¸“å®¶è¾“å‡ºã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¼•å…¥äº†åŠç›‘ç£å¤šä»»åŠ¡ç›®æ ‡(semi-supervised multi-task objective)å¹¶æ”¯æŒåŸºç¡€æ—¶é—´åºåˆ—ç¼–ç å™¨çš„å¤§è§„æ¨¡æ¨¡å‹é€‚é…(Large-Model Adaptation)ï¼Œæœ‰æ•ˆæå‡äº†æ ‡ç­¾åˆ©ç”¨ç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHydroFusion-LMF åœ¨åå¹´æ—¥å¾„æµæ•°æ®é›†ä¸Šçš„ MSE å’Œ MAE åˆ†åˆ«æ¯”æœ€å¼ºåŸºå‡†æ¨¡å‹ DLinear æå‡äº† 10.2% å’Œ 10.3%ã€‚è¯¥æ¡†æ¶åœ¨ä¿è¯é¢„æµ‹ç²¾åº¦çš„åŒæ—¶å…¼é¡¾äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œä¸ºéå¹³ç¨³ç¯å¢ƒä¸‹çš„æ ‡ç­¾é«˜æ•ˆ(label-efficient)æ°´æ–‡é¢„æµ‹æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.NE",
        "physics.geo-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "V1",
      "pdf_url": "https://arxiv.org/pdf/2510.03744v1",
      "published_date": "2025-10-04 09:09:06 UTC",
      "updated_date": "2025-10-04 09:09:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:19.256145+00:00"
    },
    {
      "arxiv_id": "2511.17509v1",
      "title": "Beyond Awareness: Investigating How AI and Psychological Factors Shape Human Self-Confidence Calibration",
      "title_zh": "è¶…è¶Šæ„è¯†ï¼šæ¢ç©¶äººå·¥æ™ºèƒ½ä¸å¿ƒç†å› ç´ å¦‚ä½•å¡‘é€ äººç±»è‡ªä¿¡æ ¡å‡†",
      "authors": [
        "Federico Maria Cau",
        "Lucio Davide Spano"
      ],
      "abstract": "Human-AI collaboration outcomes depend strongly on human self-confidence calibration, which drives reliance or resistance toward AI's suggestions. This work presents two studies examining whether calibration of self-confidence before decision tasks, low versus high levels of Need for Cognition (NFC), and Actively Open-Minded Thinking (AOT), leads to differences in decision accuracy, self-confidence appropriateness during the tasks, and metacognitive perceptions (global and affective). The first study presents strategies to identify well-calibrated users, also comparing decision accuracy and the appropriateness of self-confidence across NFC and AOT levels. The second study investigates the effects of calibrated self-confidence in AI-assisted decision-making (no AI, two-stage AI, and personalized AI), also considering different NFC and AOT levels. Our results show the importance of human self-confidence calibration and psychological traits when designing AI-assisted decision systems. We further propose design recommendations to address the challenge of calibrating self-confidence and supporting tailored, user-centric AI that accounts for individual traits.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨äººæœºåä½œ(Human-AI collaboration)ä¸­ï¼Œäººç±»çš„è‡ªä¿¡åº¦æ ¡å‡†(self-confidence calibration)å¦‚ä½•å½±å“å¯¹AIå»ºè®®çš„ä¾èµ–æˆ–æŠµåˆ¶ã€‚é€šè¿‡ä¸¤é¡¹å®éªŒç ”ç©¶ï¼Œä½œè€…åˆ†æäº†å†³ç­–ä»»åŠ¡å‰çš„è‡ªä¿¡åº¦æ ¡å‡†ã€è®¤çŸ¥éœ€æ±‚(Need for Cognition, NFC)ä»¥åŠä¸»åŠ¨å¼€æ”¾æ€ç»´(Actively Open-Minded Thinking, AOT)å¯¹å†³ç­–å‡†ç¡®æ€§ã€è‡ªä¿¡åº¦é€‚åˆ‡æ€§åŠå…ƒè®¤çŸ¥æ„ŸçŸ¥çš„å½±å“ã€‚ç¬¬ä¸€é¡¹ç ”ç©¶é‡ç‚¹åœ¨äºè¯†åˆ«è‡ªä¿¡åº¦æ ¡å‡†è‰¯å¥½çš„ç”¨æˆ·å¹¶æ¯”è¾ƒä¸åŒå¿ƒç†ç‰¹è´¨æ°´å¹³ä¸‹çš„è¡¨ç°ï¼Œç¬¬äºŒé¡¹ç ”ç©¶åˆ™è°ƒæŸ¥äº†åœ¨ä¸åŒAIè¾…åŠ©æ¨¡å¼ä¸‹è‡ªä¿¡åº¦æ ¡å‡†çš„æ•ˆæœã€‚å®éªŒç»“æœå¼ºè°ƒäº†åœ¨è®¾è®¡AIè¾…åŠ©å†³ç­–ç³»ç»Ÿæ—¶ï¼Œäººç±»è‡ªä¿¡åº¦æ ¡å‡†å’Œå¿ƒç†ç‰¹è´¨(psychological traits)çš„å…³é”®ä½œç”¨ã€‚ç ”ç©¶æœ€åæå‡ºäº†é’ˆå¯¹æ€§çš„è®¾è®¡å»ºè®®ï¼Œæ—¨åœ¨è§£å†³è‡ªä¿¡åº¦æ ¡å‡†éš¾é¢˜ï¼Œå¹¶æ”¯æŒè€ƒè™‘ä¸ªä½“å·®å¼‚çš„ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ(user-centric)çš„ä¸ªæ€§åŒ–AIç³»ç»Ÿã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17509v1",
      "published_date": "2025-10-04 08:42:57 UTC",
      "updated_date": "2025-10-04 08:42:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:19.051613+00:00"
    },
    {
      "arxiv_id": "2510.03734v1",
      "title": "Cost Efficient Fairness Audit Under Partial Feedback",
      "title_zh": "éƒ¨åˆ†åé¦ˆä¸‹çš„ç»æµé«˜æ•ˆå…¬å¹³æ€§å®¡è®¡",
      "authors": [
        "Nirjhar Das",
        "Mohit Sharma",
        "Praharsh Nanavati",
        "Kirankumar Shiragur",
        "Amit Deshpande"
      ],
      "abstract": "We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨éƒ¨åˆ†åé¦ˆ(Partial Feedback)æ¡ä»¶ä¸‹çš„åˆ†ç±»å™¨å…¬å¹³æ€§å®¡è®¡(Fairness Audit)é—®é¢˜ï¼Œå³ä»…èƒ½è§‚æµ‹åˆ°è¢«é¢„æµ‹ä¸ºæ­£å‘ç±»åˆ«çš„ä¸ªä½“çš„çœŸå®æ ‡ç­¾ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„æˆæœ¬æ¨¡å‹æ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æ ‡ç­¾è·å–æˆæœ¬ï¼Œæ—¨åœ¨å¯»æ‰¾æ¯”éšæœºæ¢ç´¢æ›´å…·æˆæœ¬æ•ˆç›Šçš„å®¡è®¡ç®—æ³•ã€‚é’ˆå¯¹ä¸åŒ…å«åˆ†å¸ƒå‡è®¾çš„é»‘ç›’æ¨¡å‹(Black-box Model)åœºæ™¯ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåœ¨æ¸©å’Œå‡è®¾ä¸‹çš„è¿‘ä¼˜å®¡è®¡ç®—æ³•ï¼›è€Œåœ¨æ··åˆæ¨¡å‹(Mixture Model)åœºæ™¯ä¸‹ï¼Œç ”ç©¶é€šè¿‡ç»“åˆæˆªæ–­æ ·æœ¬å­¦ä¹ (Learning from Truncated Samples)å’Œæœ€å¤§åéªŒæ¦‚ç‡ç®—å­(MAP Oracles)ï¼Œè®¾è®¡å‡ºæ¯”é»‘ç›’åœºæ™¯æˆæœ¬æ˜¾è‘—æ›´ä½çš„ç®—æ³•ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºäººå£ç»Ÿè®¡å­¦å¹³æƒ(Demographic Parity)ã€æœºä¼šå‡ç­‰(Equal Opportunity)å’Œç­‰åŒèµ”ç‡(Equalized Odds)ç­‰ä¸»æµå…¬å¹³æ€§æŒ‡æ ‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨Adult Incomeå’ŒLaw Schoolç­‰çœŸå®æ•°æ®é›†ä¸Šï¼Œæ‰€æç®—æ³•çš„å®¡è®¡æˆæœ¬æ¯”è‡ªç„¶åŸºçº¿é™ä½äº†çº¦50%ï¼Œè¯æ˜äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2025 RegML Workshop; Reliable ML Workshop",
      "pdf_url": "https://arxiv.org/pdf/2510.03734v1",
      "published_date": "2025-10-04 08:38:03 UTC",
      "updated_date": "2025-10-04 08:38:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:30.558651+00:00"
    },
    {
      "arxiv_id": "2510.03727v1",
      "title": "Bridging the Gap Between Multimodal Foundation Models and World Models",
      "title_zh": "å¼¥åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸ä¸–ç•Œæ¨¡å‹ä¹‹é—´çš„å·®è·",
      "authors": [
        "Xuehai He"
      ],
      "abstract": "Humans understand the world through the integration of multiple sensory modalities, enabling them to perceive, reason about, and imagine dynamic physical processes. Inspired by this capability, multimodal foundation models (MFMs) have emerged as powerful tools for multimodal understanding and generation. However, today's MFMs fall short of serving as effective world models. They lack the essential ability such as perform counterfactual reasoning, simulate dynamics, understand the spatiotemporal information, control generated visual outcomes, and perform multifaceted reasoning. We investigates what it takes to bridge the gap between multimodal foundation models and world models. We begin by improving the reasoning capabilities of MFMs through discriminative tasks and equipping MFMs with structured reasoning skills, such as causal inference, counterfactual thinking, and spatiotemporal reasoning, enabling them to go beyond surface correlations and understand deeper relationships within visual and textual data. Next, we explore generative capabilities of multimodal foundation models across both image and video modalities, introducing new frameworks for structured and controllable generation. Our approaches incorporate scene graphs, multimodal conditioning, and multimodal alignment strategies to guide the generation process, ensuring consistency with high-level semantics and fine-grained user intent. We further extend these techniques to controllable 4D generation, enabling interactive, editable, and morphable object synthesis over time and space.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å¼¥åˆå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹(Multimodal Foundation Models, MFMs)ä¸ä¸–ç•Œæ¨¡å‹(World Models)ä¹‹é—´çš„å·®è·ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨åäº‹å®æ¨ç†ã€åŠ¨æ€æ¨¡æ‹ŸåŠç©ºæ—¶ä¿¡æ¯å¤„ç†æ–¹é¢çš„å±€é™æ€§ã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡åˆ¤åˆ«ä»»åŠ¡å¼ºåŒ–äº†æ¨¡å‹çš„ç»“æ„åŒ–æ¨ç†æŠ€èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å› æœæ¨ç†(Causal Inference)ä¸åäº‹å®æ€ç»´(Counterfactual Thinking)ç­‰æ·±å±‚é€»è¾‘å…³ç³»ã€‚åœ¨ç”Ÿæˆèƒ½åŠ›æ–¹é¢ï¼Œç ”ç©¶å¼•å…¥äº†æ•´åˆåœºæ™¯å›¾(Scene Graphs)ä¸å¤šæ¨¡æ€å¯¹é½ç­–ç•¥çš„æ¡†æ¶ï¼Œç¡®ä¿å›¾åƒå’Œè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ç¬¦åˆé«˜å±‚è¯­ä¹‰ä¸ç²¾ç»†çš„ç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œç›¸å…³æŠ€æœ¯è¢«è¿›ä¸€æ­¥æ‰©å±•è‡³å¯æ§4Dç”Ÿæˆé¢†åŸŸï¼Œå®ç°äº†åœ¨æ—¶ç©ºç»´åº¦ä¸Šå¯äº¤äº’ã€å¯ç¼–è¾‘ä¸”å¯å˜å½¢çš„ç‰©ä½“åˆæˆã€‚è¿™ä¸€ç³»åˆ—æ–¹æ³•é€šè¿‡å¢å¼ºæ¨¡å‹çš„åˆ¤åˆ«å¼æ¨ç†ä¸ç”Ÿæˆå¼å»ºæ¨¡èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹å¯¹ç°å®ç‰©ç†ä¸–ç•Œçš„è®¤çŸ¥æ·±åº¦ä¸æ¨¡æ‹Ÿæ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "PhD thesis",
      "pdf_url": "https://arxiv.org/pdf/2510.03727v1",
      "published_date": "2025-10-04 08:14:20 UTC",
      "updated_date": "2025-10-04 08:14:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:37.148930+00:00"
    },
    {
      "arxiv_id": "2510.03717v1",
      "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning",
      "title_zh": "åŸºäºæ·±åº¦å­¦ä¹ çš„çœ¼åº•å›¾åƒåŠ¨é™è„‰åˆ†å‰²",
      "authors": [
        "Sharan SK",
        "Subin Sahayam",
        "Umarani Jayaraman",
        "Lakshmi Priya A"
      ],
      "abstract": "Segmenting of clinically important retinal blood vessels into arteries and veins is a prerequisite for retinal vessel analysis. Such analysis can provide potential insights and bio-markers for identifying and diagnosing various retinal eye diseases. Alteration in the regularity and width of the retinal blood vessels can act as an indicator of the health of the vasculature system all over the body. It can help identify patients at high risk of developing vasculature diseases like stroke and myocardial infarction. Over the years, various Deep Learning architectures have been proposed to perform retinal vessel segmentation. Recently, attention mechanisms have been increasingly used in image segmentation tasks. The work proposes a new Deep Learning approach for artery-vein segmentation. The new approach is based on the Attention mechanism that is incorporated into the WNet Deep Learning model, and we call the model as Attention-WNet. The proposed approach has been tested on publicly available datasets such as HRF and DRIVE datasets. The proposed approach has outperformed other state-of-art models available in the literature.",
      "tldr_zh": "çœ¼åº•å›¾åƒä¸­çš„è§†ç½‘è†œåŠ¨é™è„‰åˆ†å‰²æ˜¯è·å–çœ¼éƒ¨åŠå…¨èº«è¡€ç®¡ç–¾ç—…ï¼ˆå¦‚ä¸­é£å’Œå¿ƒè‚Œæ¢—æ­»ï¼‰ç”Ÿç‰©æ ‡å¿—ç‰©çš„é‡è¦å‰æã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Attention æœºåˆ¶çš„æ–°å‹æ·±åº¦å­¦ä¹ æ¶æ„ Attention-WNetï¼Œæ—¨åœ¨è¿›ä¸€æ­¥æå‡è§†ç½‘è†œè¡€ç®¡åˆ†å‰²çš„å‡†ç¡®åº¦ã€‚è¯¥æ¨¡å‹å°†æ³¨æ„åŠ›æœºåˆ¶æœ‰æœºæ•´åˆè¿› WNet æ¶æ„ä¸­ï¼Œå¹¶åœ¨å…¬å¼€çš„ HRF å’Œ DRIVE æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼ŒAttention-WNet çš„è¡¨ç°è¶…è¶Šäº†ç›®å‰æ–‡çŒ®ä¸­è®°è½½çš„ State-of-the-art æ¨¡å‹ã€‚è¯¥æ–¹æ³•çš„æˆåŠŸåº”ç”¨èƒ½æœ‰æ•ˆååŠ©è¯†åˆ«è¡€ç®¡ç³»ç»Ÿå¼‚å¸¸ï¼Œä¸ºä¸´åºŠä¸Šçš„ç–¾ç—…é¢„æµ‹å’Œè¯Šæ–­æä¾›å¯é çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 6 figures, preprint under review",
      "pdf_url": "https://arxiv.org/pdf/2510.03717v1",
      "published_date": "2025-10-04 07:42:30 UTC",
      "updated_date": "2025-10-04 07:42:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:58.253622+00:00"
    },
    {
      "arxiv_id": "2510.03706v1",
      "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning",
      "title_zh": "EmbodiSwapï¼šé¢å‘é›¶æ ·æœ¬æœºå™¨äººæ¨¡ä»¿å­¦ä¹ ",
      "authors": [
        "Eadom Dessalene",
        "Pavan Mantripragada",
        "Michael Maynord",
        "Yiannis Aloimonos"
      ],
      "abstract": "We introduce EmbodiSwap - a method for producing photorealistic synthetic robot overlays over human video. We employ EmbodiSwap for zero-shot imitation learning, bridging the embodiment gap between in-the-wild ego-centric human video and a target robot embodiment. We train a closed-loop robot manipulation policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a visual backbone, repurposing V-JEPA from the domain of video understanding to imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms alternative vision backbones more conventionally used within robotics. In real-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success rate, outperforming a few-shot trained $Ï€_0$ network as well as $Ï€_0$ trained over data produced by EmbodiSwap. We release (i) code for generating the synthetic robot overlays which takes as input human videos and an arbitrary robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference code, to facilitate reproducible research and broader adoption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† EmbodiSwapï¼Œè¿™æ˜¯ä¸€ç§åœ¨äººç±»è§†é¢‘ä¸Šç”Ÿæˆå†™å®åˆæˆæœºå™¨äººå›¾å±‚çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡é›¶æ ·æœ¬æ¨¡ä»¿å­¦ä¹  (zero-shot imitation learning) å¼¥è¡¥äººç±»è§†é¢‘ä¸ç›®æ ‡æœºå™¨äººä¹‹é—´çš„å…·èº«é¸¿æ²Ÿ (embodiment gap)ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ EmbodiSwap ç”Ÿæˆçš„æ•°æ®è®­ç»ƒé—­ç¯æœºå™¨äººæ“ä½œç­–ç•¥ï¼Œå¹¶åˆ›æ–°æ€§åœ°å°† V-JEPA è§†è§‰ä¸»å¹²ç½‘ç»œä»è§†é¢‘ç†è§£é¢†åŸŸå¼•å…¥åˆ°åˆæˆæœºå™¨äººè§†é¢‘çš„æ¨¡ä»¿å­¦ä¹ ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ V-JEPA çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„æœºå™¨äººè§†è§‰æ–¹æ¡ˆï¼Œå…¶é›¶æ ·æœ¬æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¸­å–å¾—äº† 82% çš„æˆåŠŸç‡ã€‚è¿™ä¸€è¡¨ç°ä¸ä»…è¶…è¿‡äº†å°‘æ ·æœ¬è®­ç»ƒçš„ $\\pi_0$ ç½‘ç»œï¼Œä¹Ÿä¼˜äºåœ¨ EmbodiSwap æ•°æ®ä¸Šè®­ç»ƒçš„ $\\pi_0$ ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¼€æºäº†ç›¸å…³ä»£ç ã€åŸºäºå¤§å‹è§†é¢‘æ•°æ®é›†åˆæˆçš„æœºå™¨äººæ•°æ®é›†ä»¥åŠæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›å­¦æœ¯ç•Œçš„å¤ç°ä¸è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Video link: https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing",
      "pdf_url": "https://arxiv.org/pdf/2510.03706v1",
      "published_date": "2025-10-04 07:11:20 UTC",
      "updated_date": "2025-10-04 07:11:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:49.352015+00:00"
    },
    {
      "arxiv_id": "2510.03701v1",
      "title": "Referring Expression Comprehension for Small Objects",
      "title_zh": "é¢å‘å°ç›®æ ‡çš„æŒ‡ç§°è¡¨è¾¾ç†è§£",
      "authors": [
        "Kanoko Goto",
        "Takumi Hirose",
        "Mahiro Ukai",
        "Shuhei Kurita",
        "Nakamasa Inoue"
      ],
      "abstract": "Referring expression comprehension (REC) aims to localize the target object described by a natural language expression. Recent advances in vision-language learning have led to significant performance improvements in REC tasks. However, localizing extremely small objects remains a considerable challenge despite its importance in real-world applications such as autonomous driving. To address this issue, we introduce a novel dataset and method for REC targeting small objects. First, we present the small object REC (SOREC) dataset, which consists of 100,000 pairs of referring expressions and corresponding bounding boxes for small objects in driving scenarios. Second, we propose the progressive-iterative zooming adapter (PIZA), an adapter module for parameter-efficient fine-tuning that enables models to progressively zoom in and localize small objects. In a series of experiments, we apply PIZA to GroundingDINO and demonstrate a significant improvement in accuracy on the SOREC dataset. Our dataset, codes and pre-trained models are publicly available on the project page.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç­‰å®é™…åº”ç”¨ä¸­æå°ç›®æ ‡çš„æŒ‡ä»£è¡¨è¾¾ç†è§£ (Referring Expression Comprehension, REC) éš¾é¢˜å±•å¼€äº†æ¢è®¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…é¦–å…ˆæå‡ºäº†åä¸º SOREC çš„å°ç›®æ ‡ REC æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«é©¾é©¶åœºæ™¯ä¸­ 100,000 å¯¹æŒ‡ä»£è¡¨è¾¾åŠå…¶å¯¹åº”çš„ç‰©ä½“è¾¹ç•Œæ¡†ã€‚éšåï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºæ¸è¿›è¿­ä»£ç¼©æ”¾é€‚é…å™¨ (Progressive-Iterative Zooming Adapter, PIZA) çš„æ–°å‹é€‚é…å™¨æ¨¡å—ï¼Œç”¨äºå®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ (Parameter-Efficient Fine-Tuning)ã€‚PIZA å…è®¸æ¨¡å‹é€šè¿‡æ¸è¿›å¼ç¼©æ”¾æœºåˆ¶æ¥ç²¾å‡†å®šä½å°ç›®æ ‡ï¼Œæœ‰æ•ˆå…‹æœäº†ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¾®å°å°ºå¯¸ç‰©ä½“æ—¶çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°† PIZA åº”ç”¨äº GroundingDINO æ¨¡å‹å¯æ˜¾è‘—æé«˜åœ¨ SOREC æ•°æ®é›†ä¸Šçš„å®šä½å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³æå°ç›®æ ‡çš„å®šä½æŒ‘æˆ˜æä¾›äº†æ–°çš„æ•°æ®é›†æ”¯æ’‘å’Œæœ‰æ•ˆçš„æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03701v1",
      "published_date": "2025-10-04 06:50:02 UTC",
      "updated_date": "2025-10-04 06:50:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:52.258115+00:00"
    },
    {
      "arxiv_id": "2510.03700v1",
      "title": "H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis",
      "title_zh": "H-DDxï¼šé‰´åˆ«è¯Šæ–­çš„å±‚çº§åŒ–è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Seungseop Lim",
        "Gibaeg Kim",
        "Hyunkyung Lee",
        "Wooseok Han",
        "Jean Seo",
        "Jaehyo Yoo",
        "Eunho Yang"
      ],
      "abstract": "An accurate differential diagnosis (DDx) is essential for patient care, shaping therapeutic decisions and influencing outcomes. Recently, Large Language Models (LLMs) have emerged as promising tools to support this process by generating a DDx list from patient narratives. However, existing evaluations of LLMs in this domain primarily rely on flat metrics, such as Top-k accuracy, which fail to distinguish between clinically relevant near-misses and diagnostically distant errors. To mitigate this limitation, we introduce H-DDx, a hierarchical evaluation framework that better reflects clinical relevance. H-DDx leverages a retrieval and reranking pipeline to map free-text diagnoses to ICD-10 codes and applies a hierarchical metric that credits predictions closely related to the ground-truth diagnosis. In benchmarking 22 leading models, we show that conventional flat metrics underestimate performance by overlooking clinically meaningful outputs, with our results highlighting the strengths of domain-specialized open-source models. Furthermore, our framework enhances interpretability by revealing hierarchical error patterns, demonstrating that LLMs often correctly identify the broader clinical context even when the precise diagnosis is missed.",
      "tldr_zh": "é‰´äºç°æœ‰çš„è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)é‰´åˆ«è¯Šæ–­(Differential Diagnosis, DDx)èƒ½åŠ›çš„æŒ‡æ ‡ï¼ˆå¦‚Top-kå‡†ç¡®ç‡ï¼‰æ— æ³•åŒºåˆ†ä¸´åºŠç›¸å…³çš„ç›¸è¿‘é”™è¯¯ä¸å®Œå…¨æ— å…³çš„è¯Šæ–­åå·®ï¼Œæœ¬ç ”ç©¶æå‡ºäº†H-DDxè¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ£€ç´¢ä¸é‡æ’åº(retrieval and reranking)æµæ°´çº¿å°†è‡ªç”±æ–‡æœ¬è¯Šæ–­æ˜ å°„è‡³ICD-10ä»£ç ï¼Œå¹¶åº”ç”¨åˆ†å±‚æŒ‡æ ‡å¯¹ä¸æ ‡å‡†è¯Šæ–­ä¸´åºŠç›¸å…³çš„é¢„æµ‹ç»™äºˆè¯„åˆ†ï¼Œä»è€Œæ›´çœŸå®åœ°åæ˜ ä¸´åºŠç›¸å…³æ€§ã€‚é€šè¿‡å¯¹22ä¸ªé¢†å…ˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œä¼ ç»ŸæŒ‡æ ‡ç”±äºå¿½ç•¥äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„è¾“å‡ºè€Œä½ä¼°äº†æ¨¡å‹æ€§èƒ½ï¼Œä¸”ç ”ç©¶ç»“æœç‰¹åˆ«å¼ºè°ƒäº†é¢†åŸŸä¸“ç”¨å¼€æºæ¨¡å‹çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒH-DDxé€šè¿‡æ­ç¤ºåˆ†å±‚é”™è¯¯æ¨¡å¼å¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œè¯æ˜LLMså³ä½¿åœ¨é—æ¼ç²¾ç¡®è¯Šæ–­æ—¶ï¼Œé€šå¸¸ä¹Ÿèƒ½æ­£ç¡®è¯†åˆ«æ›´å¹¿æ³›çš„ä¸´åºŠèƒŒæ™¯ã€‚è¯¥ç ”ç©¶ä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„å®é™…æ•ˆç”¨æä¾›äº†æ›´è´´è¿‘ä¸´åºŠç°å®çš„è¯„ä»·ç»´åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "GenAI4Health @NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2510.03700v1",
      "published_date": "2025-10-04 06:42:22 UTC",
      "updated_date": "2025-10-04 06:42:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:45.655860+00:00"
    },
    {
      "arxiv_id": "2510.03699v1",
      "title": "Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents",
      "title_zh": "åˆ©ç”¨ç»æ·±åº¦å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ RNN æ™ºèƒ½ä½“å‰–ææ–‘é©¬é±¼å¹¼é±¼æ•é£Ÿè¡Œä¸º",
      "authors": [
        "Raaghav Malik",
        "Satpreet H. Singh",
        "Sonja Johnson-Yu",
        "Nathan Wu",
        "Roy Harpaz",
        "Florian Engert",
        "Kanaka Rajan"
      ],
      "abstract": "Larval zebrafish hunting provides a tractable setting to study how ecological and energetic constraints shape adaptive behavior in both biological brains and artificial agents. Here we develop a minimal agent-based model, training recurrent policies with deep reinforcement learning in a bout-based zebrafish simulator. Despite its simplicity, the model reproduces hallmark hunting behaviors -- including eye vergence-linked pursuit, speed modulation, and stereotyped approach trajectories -- that closely match real larval zebrafish. Quantitative trajectory analyses show that pursuit bouts systematically reduce prey angle by roughly half before strike, consistent with measurements. Virtual experiments and parameter sweeps vary ecological and energetic constraints, bout kinematics (coupled vs. uncoupled turns and forward motion), and environmental factors such as food density, food speed, and vergence limits. These manipulations reveal how constraints and environments shape pursuit dynamics, strike success, and abort rates, yielding falsifiable predictions for neuroscience experiments. These sweeps identify a compact set of constraints -- binocular sensing, the coupling of forward speed and turning in bout kinematics, and modest energetic costs on locomotion and vergence -- that are sufficient for zebrafish-like hunting to emerge. Strikingly, these behaviors arise in minimal agents without detailed biomechanics, fluid dynamics, circuit realism, or imitation learning from real zebrafish data. Taken together, this work provides a normative account of zebrafish hunting as the optimal balance between energetic cost and sensory benefit, highlighting the trade-offs that structure vergence and trajectory dynamics. We establish a virtual lab that narrows the experimental search space and generates falsifiable predictions about behavior and neural coding.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŸºäºæ™ºèƒ½ä½“(agent-based)çš„æç®€æ¨¡å‹ï¼Œé€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning)è®­ç»ƒå¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ç­–ç•¥ï¼Œåœ¨åŸºäºåŠ¨ä½œ(bout-based)çš„æ–‘é©¬é±¼æ¨¡æ‹Ÿå™¨ä¸­é‡ç°äº†å¹¼å¹´æ–‘é©¬é±¼çš„æ•é£Ÿè¡Œä¸ºã€‚å°½ç®¡æ¨¡å‹ç»“æ„ç®€å•ï¼Œä½†å®ƒæˆåŠŸæ¨¡æ‹Ÿäº†åŒ…æ‹¬åŒçœ¼ä¼šèš(eye vergence)ç›¸å…³çš„è¿½é€ã€é€Ÿåº¦è°ƒèŠ‚ä»¥åŠå®šå‹è¶‹è¿‘è½¨è¿¹ç­‰æ ¸å¿ƒè¡Œä¸ºç‰¹å¾ï¼Œå…¶é‡åŒ–è½¨è¿¹åˆ†æä¸çœŸå®æ–‘é©¬é±¼æ•°æ®é«˜åº¦ä¸€è‡´ã€‚é€šè¿‡è™šæ‹Ÿå®éªŒå’Œå‚æ•°æ‰«æï¼Œç ”ç©¶è€…æ¢è®¨äº†ç”Ÿæ€ä¸èƒ½é‡çº¦æŸã€è¿åŠ¨å­¦ç‰¹å¾ä»¥åŠç¯å¢ƒå› ç´ å¯¹è¿½é€åŠ¨æ€ã€æ•é£ŸæˆåŠŸç‡åŠæ”¾å¼ƒç‡çš„å½±å“ã€‚å®éªŒè¯æ˜ï¼ŒåŒçœ¼æ„Ÿåº”(binocular sensing)ã€å‰è¿›é€Ÿåº¦ä¸è½¬å‘çš„è¿åŠ¨å­¦è€¦åˆä»¥åŠé€‚åº¦çš„èƒ½é‡æˆæœ¬ï¼Œæ˜¯ä½¿ç±»ä¼¼æ–‘é©¬é±¼æ•é£Ÿè¡Œä¸ºå‡ºç°çš„å……åˆ†æ¡ä»¶ã€‚è¯¥å·¥ä½œä»è§„èŒƒæ€§è§’åº¦è§£é‡Šäº†æ–‘é©¬é±¼æ•é£Ÿæ˜¯èƒ½é‡æˆæœ¬ä¸æ„Ÿå®˜æ”¶ç›Šä¹‹é—´çš„æœ€ä¼˜å¹³è¡¡ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªè™šæ‹Ÿå®éªŒå®¤ï¼Œä¸ºåç»­ç¥ç»ç§‘å­¦å®éªŒæä¾›äº†å¯è¯ä¼ªçš„é¢„æµ‹å’Œæ›´ç²¾ç¡®çš„ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "eess.SY"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03699v1",
      "published_date": "2025-10-04 06:40:32 UTC",
      "updated_date": "2025-10-04 06:40:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:52.957153+00:00"
    },
    {
      "arxiv_id": "2510.03696v1",
      "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models",
      "title_zh": "èšç„¦ç›®æ ‡ï¼šåŸºäºæ•™å¸ˆæ¨¡å‹çš„å¯¹è¯æ™ºèƒ½ä½“ä¸èŠå¤©æœºå™¨äººæ•°æ®é«˜æ•ˆç›®æ ‡å¯¼å‘è¯„ä¼°",
      "authors": [
        "Deepak Babu Piskala",
        "Sharlene Chen",
        "Udita Patel",
        "Parul Kalra",
        "Rafael Castrillo"
      ],
      "abstract": "Evaluating the quality of multi-turn chatbot interactions remains challenging, as most existing methods assess interactions at the turn level without addressing whether a user's overarching goal was fulfilled. A ``goal'' here refers to an information need or task, such as asking for policy information or applying for leave. We propose a comprehensive framework for goal-oriented evaluation of multi-agent systems (MAS), introducing the \\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals, and a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for failure in multi-agent chatbots. Our method segments conversations by user goals and evaluates success using all relevant turns. We present a model-based evaluation system combining teacher LLMs, where domain experts define goals, set quality standards serving as a guidance for the LLMs. The LLMs use ``thinking tokens'' to produce interpretable rationales, enabling \\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise setting, we apply our framework to evaluate AIDA, a zero-to-one employee conversational agent system built as a ground-up multi-agent conversational agent, and observe GSR improvement from 63\\% to 79\\% over six months since its inception. Our framework is generic and offers actionable insights through a detailed defect taxonomy based on analysis of failure points in multi-agent chatbots, diagnosing overall success, identifying key failure modes, and informing system improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè½®å¯¹è¯æœºå™¨äººè¯„ä¼°ä¸­è¿‡åº¦å…³æ³¨å•è½®äº¤äº’è€Œå¿½è§†ç”¨æˆ·æ•´ä½“ç›®æ ‡(Goal)è¾¾æˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢å‘ç›®æ ‡è¯„ä¼°çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)é€šç”¨æ¡†æ¶ã€‚ç ”ç©¶å¼•å…¥äº†ç›®æ ‡æˆåŠŸç‡(Goal Success Rate, GSR)æ¥è¡¡é‡ç›®æ ‡å®Œæˆç™¾åˆ†æ¯”ï¼Œå¹¶æ„å»ºäº†æ•…éšœæ ¹å› (Root Cause of Failure, RCOF)åˆ†ç±»ä½“ç³»ä»¥è¯†åˆ«å¤šæ™ºèƒ½ä½“å¯¹è¯æœºå™¨äººçš„å¤±æ•ˆåŸå› ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”¨æˆ·ç›®æ ‡å¯¹å¯¹è¯è¿›è¡Œåˆ†å‰²ï¼Œç»“åˆé¢†åŸŸä¸“å®¶å®šä¹‰çš„è´¨é‡æ ‡å‡†ï¼Œåˆ©ç”¨æ•™å¸ˆå¤§è¯­è¨€æ¨¡å‹(Teacher LLMs)åŠâ€œæ€è€ƒæ ‡è®°â€(thinking tokens)ç”Ÿæˆå…·æœ‰å¯è§£é‡Šæ€§çš„è¯„ä¼°ç†ç”±ã€‚åœ¨ä¼ä¸šçº§åº”ç”¨åœºæ™¯ä¸­ï¼Œè¯¥æ¡†æ¶è¢«ç”¨äºè¯„ä¼°å‘˜å·¥å¯¹è¯æ™ºèƒ½ä½“ç³»ç»ŸAIDAï¼Œå¹¶åœ¨å…­ä¸ªæœˆå†…å°†å…¶GSRä»63%æå‡è‡³79%ã€‚å®éªŒè¯æ˜è¯¥æ¡†æ¶å…·æœ‰è‰¯å¥½çš„æ•°æ®æ•ˆç‡(Data-Efficient)å’Œé€šç”¨æ€§ï¼Œèƒ½å¤Ÿé€šè¿‡è¯¦å°½çš„ç¼ºé™·åˆ†ç±»ä½“ç³»è¯Šæ–­ç³»ç»Ÿå¤±æ•ˆæ¨¡å¼ï¼Œä¸ºå¤šæ™ºèƒ½ä½“å¯¹è¯ç³»ç»Ÿçš„æŒç»­æ”¹è¿›æä¾›å¯è½åœ°çš„åˆ†ææ´å¯Ÿã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03696v1",
      "published_date": "2025-10-04 06:22:47 UTC",
      "updated_date": "2025-10-04 06:22:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:47:58.650209+00:00"
    },
    {
      "arxiv_id": "2510.09644v1",
      "title": "Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining",
      "title_zh": "åˆ©ç”¨ CCTV ç›‘æ§è§†é¢‘ä¸å¤šæºæ•°æ®ç°çŠ¶é¢„æµ‹åŠé¢‘ç¹æƒ…èŠ‚æŒ–æ˜å¢å¼ºåŸå¸‚äº¤é€šç®¡ç†",
      "authors": [
        "Shaharyar Alam Ansari",
        "Mohammad Luqman",
        "Aasim Zafar",
        "Savir Ali"
      ],
      "abstract": "Rapid urbanization has intensified traffic congestion, environmental strain, and inefficiencies in transportation systems, creating an urgent need for intelligent and adaptive traffic management solutions. Conventional systems relying on static signals and manual monitoring are inadequate for the dynamic nature of modern traffic. This research aims to develop a unified framework that integrates CCTV surveillance videos with multi-source data descriptors to enhance real-time urban traffic prediction. The proposed methodology incorporates spatio-temporal feature fusion, Frequent Episode Mining for sequential traffic pattern discovery, and a hybrid LSTM-Transformer model for robust traffic state forecasting. The framework was evaluated on the CityFlowV2 dataset comprising 313,931 annotated bounding boxes across 46 cameras. It achieved a high prediction accuracy of 98.46 percent, with a macro precision of 0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis revealed significant sequential patterns such as moderate-congested transitions with confidence levels exceeding 55 percent. The 46 sustained congestion alerts are system-generated, which shows practical value for proactive congestion management. This emphasizes the need for the incorporation of video stream analytics with data from multiple sources for the design of real-time, responsive, adaptable multi-level intelligent transportation systems, which makes urban mobility smarter and safer.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚äº¤é€šæ‹¥å µé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•´åˆCCTVç›‘æ§è§†é¢‘ä¸å¤šæºæ•°æ®çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å®æ—¶åŸå¸‚äº¤é€šé¢„æµ‹èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ—¶ç©ºç‰¹å¾èåˆ(Spatio-temporal feature fusion)ã€ç”¨äºå‘ç°åºåˆ—äº¤é€šæ¨¡å¼çš„é¢‘ç¹æƒ…èŠ‚æŒ–æ˜(Frequent Episode Mining, FEM)ä»¥åŠç”¨äºäº¤é€šçŠ¶æ€é¢„æµ‹çš„æ··åˆLSTM-Transformeræ¨¡å‹ã€‚ç ”ç©¶åœ¨CityFlowV2æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹è¾¾åˆ°äº†98.46%çš„é¢„æµ‹å‡†ç¡®ç‡ï¼Œä¸”å…¶å®ç²¾ç¡®ç‡ã€å®å¬å›ç‡å’Œå®F1åˆ†æ•°å‡è¡¨ç°ä¼˜å¼‚ã€‚é€šè¿‡FEMåˆ†æï¼Œç ”ç©¶æˆåŠŸæ­ç¤ºäº†ç½®ä¿¡åº¦è¶…è¿‡55%çš„äº¤é€šè½¬æ¢æ¨¡å¼ï¼Œå¹¶ç”Ÿæˆäº†46æ¡æŒç»­æ‹¥å µé¢„è­¦ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨ä¸»åŠ¨äº¤é€šç®¡ç†ä¸­çš„å®ç”¨ä»·å€¼ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»“åˆè§†é¢‘æµåˆ†æä¸å¤šæºæ•°æ®å¯¹äºæ„å»ºå®æ—¶ã€å“åº”è¿…é€Ÿä¸”é€‚åº”æ€§å¼ºçš„å¤šå±‚çº§æ™ºèƒ½äº¤é€šç³»ç»Ÿçš„é‡è¦æ€§ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†åŸå¸‚å‡ºè¡Œå‘æ›´æ™ºèƒ½ã€æ›´å®‰å…¨çš„æ–¹å‘å‘å±•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.09644v1",
      "published_date": "2025-10-04 06:11:34 UTC",
      "updated_date": "2025-10-04 06:11:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:00.862872+00:00"
    },
    {
      "arxiv_id": "2510.03691v1",
      "title": "REG: A Regularization Optimizer for Robust Training Dynamics",
      "title_zh": "REGï¼šé¢å‘ç¨³å¥è®­ç»ƒåŠ¨åŠ›å­¦çš„æ­£åˆ™åŒ–ä¼˜åŒ–å™¨",
      "authors": [
        "Zehua Liu",
        "Han Wu",
        "Xiaojin Fu",
        "Shuqi Liu",
        "Xiongwei Han",
        "Tao Zhong",
        "Mingxuan Yuan"
      ],
      "abstract": "Optimizers are crucial for the efficient training of Large Language Models (LLMs). While AdamW is the de facto standard, recent structure-aware optimizers like Muon have emerged, which regularize gradient updates by operating on entire weight matrices. The Muon optimizer balances the gradient updates along all the directions. However, Muon's reliance on the matrix sign function can lead to training instability, exhibits incompatibility when fine-tuning models pre-trained with AdamW. To address these limitations, we propose \\textbf{REG}, a novel optimizer that replaces Muon's aggressive matrix sign operator with the Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a matrix, the RACS operator regularizes the update steps in a less drastic manner, making it simpler to implement and more compatible with established training dynamics. Through extensive empirical experiments on LLM training, we demonstrate that our REG optimizer not only achieves superior performance and stability over AdamW, but also maintains consistency with the AdamW training paradigm. This consistency is particularly evident during the fine-tuning stage, where REG optimizer avoids the performance degradation observed with Muon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† REG ä¼˜åŒ–å™¨ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLM) è®­ç»ƒä¸­ Muon ç­‰ç»“æ„æ„ŸçŸ¥ä¼˜åŒ–å™¨å› ä¾èµ–çŸ©é˜µç¬¦å·å‡½æ•° (matrix sign function) è€Œå¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®šåŠä¸ AdamW é¢„è®­ç»ƒæ¨¡å‹ä¸å…¼å®¹çš„é—®é¢˜ã€‚REG é‡‡ç”¨è¡Œä¸åˆ—ç¼©æ”¾ (Row-and-Column-Scaling, RACS) ç®—å­å–ä»£äº† Muon çš„æ¿€è¿›è¿ç®—ï¼Œè¯¥ç®—å­åŸºäºçŸ©é˜µå¹³è¡¡ç†è®ºï¼Œä»¥æ›´æ¸©å’Œçš„æ–¹å¼æ­£åˆ™åŒ–æ›´æ–°æ­¥é•¿ã€‚å®éªŒè¯æ˜ï¼ŒREG ä¼˜åŒ–å™¨åœ¨ LLM è®­ç»ƒä¸­çš„æ€§èƒ½å’Œç¨³å®šæ€§å‡ä¼˜äº AdamWï¼Œä¸”å…¶ç®€å•çš„å®ç°æ–¹å¼ä½¿å…¶èƒ½ä¸ç°æœ‰çš„è®­ç»ƒåŠ¨æ€é«˜åº¦å…¼å®¹ã€‚ç‰¹åˆ«æ˜¯åœ¨å¾®è°ƒé˜¶æ®µï¼ŒREG ä¿æŒäº†ä¸ AdamW è®­ç»ƒèŒƒå¼çš„ä¸€è‡´æ€§ï¼ŒæˆåŠŸé¿å…äº† Muon å¸¸è§çš„æ€§èƒ½é€€åŒ–ç°è±¡ï¼Œä¸ºé²æ£’çš„å¤§æ¨¡å‹è®­ç»ƒæä¾›äº†é«˜æ•ˆçš„ä¼˜åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03691v1",
      "published_date": "2025-10-04 06:05:57 UTC",
      "updated_date": "2025-10-04 06:05:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:03.557314+00:00"
    },
    {
      "arxiv_id": "2510.03687v2",
      "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction",
      "title_zh": "MedReflectï¼šé€šè¿‡åæ€æ€§ä¿®æ­£æ•™å¯¼åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹å®ç°è‡ªæˆ‘æå‡",
      "authors": [
        "Yue Huang",
        "Yanyuan Chen",
        "Dexuan Xu",
        "Chenzhuo Zhao",
        "Weihua Yue",
        "Yu Huang"
      ],
      "abstract": "Medical problem-solving demands expert knowledge and intricate reasoning. Recent studies of large language models (LLMs) attempt to ease this complexity by introducing external knowledge verification through retrieval-augmented generation or by training on reasoning datasets. However, these approaches suffer from drawbacks such as retrieval overhead and high annotation costs, and they heavily rely on substituted external assistants to reach limited performance in medical field. In this paper, we introduce MedReflect, a generalizable framework designed to inspire LLMs with a physician-like reflective thinking mode. MedReflect generates a single-pass reflection chain that includes initial hypothesis generation, self-questioning, self-answering and decision refinement. This self-verified and self-reflective nature releases large language model's latent capability in medical problem-solving without external retrieval or heavy annotation. We demonstrate that MedReflect enables cost-efficient medical dataset construction. With only a minimal subset of randomly sampled training examples and lightweight fine-tuning, this approach achieves notable absolute accuracy improvements across a series of medical benchmarks while significantly cutting annotation requirements. Our results provide evidence that LLMs can learn to solve specialized medical problems via self-reflection and self-improvement, reducing reliance on external supervision and extensive task-specific fine-tuning data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedReflectï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¿€å‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡ç±»ä¼¼åŒ»ç”Ÿåæ€æ€§æ€ç»´æ¨¡å¼çš„é€šç”¨æ¡†æ¶ã€‚MedReflect é€šè¿‡ç”ŸæˆåŒ…å«åˆå§‹å‡è®¾ç”Ÿæˆã€è‡ªæˆ‘è´¨è¯¢ã€è‡ªæˆ‘å›ç­”å’Œå†³ç­–ä¼˜åŒ–åœ¨å†…çš„å•æ¬¡åæ€é“¾ï¼ˆreflection chainï¼‰ï¼Œå®ç°äº†åœ¨æ— éœ€å¤–éƒ¨æ£€ç´¢æˆ–é‡åº¦æ ‡æ³¨çš„æƒ…å†µä¸‹æŒ–æ˜æ¨¡å‹åœ¨åŒ»å­¦é—®é¢˜è§£å†³ä¸­çš„æ½œèƒ½ã€‚è¿™ç§è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘åæ€çš„ç‰¹æ€§ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œé«˜æˆæœ¬æ•ˆç›Šçš„åŒ»å­¦æ•°æ®é›†æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡æå°æ¯”ä¾‹çš„éšæœºé‡‡æ ·è®­ç»ƒæ ·æœ¬å’Œè½»é‡çº§å¾®è°ƒï¼ŒMedReflect å°±åœ¨ä¸€ç³»åˆ—åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶å¤§å¹…å‡å°‘äº†æ ‡æ³¨éœ€æ±‚ã€‚è¯¥å·¥ä½œè¯æ˜äº† LLMs å¯ä»¥é€šè¿‡è‡ªæˆ‘åæ€å’Œè‡ªæˆ‘æ”¹è¿›æ¥è§£å†³ä¸“ä¸šçš„åŒ»å­¦é—®é¢˜ï¼Œæœ‰æ•ˆé™ä½äº†å¯¹å¤–éƒ¨ç›‘ç£å’Œå¤§è§„æ¨¡ä»»åŠ¡ç‰¹å®šå¾®è°ƒæ•°æ®çš„ä¾èµ–ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03687v2",
      "published_date": "2025-10-04 06:00:48 UTC",
      "updated_date": "2026-01-16 10:35:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:16.151037+00:00"
    },
    {
      "arxiv_id": "2510.03680v1",
      "title": "Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs",
      "title_zh": "Rainbow Paddingï¼šç¼“è§£æŒ‡ä»¤å¾®è°ƒæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è¿‡æ—©ç»ˆæ­¢",
      "authors": [
        "Bumjun Kim",
        "Dongjae Jeon",
        "Dueun Kim",
        "Wonje Jeung",
        "Albert No"
      ],
      "abstract": "Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \\texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \\texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \\texttt{<eos>} as both termination and padding, which concentrates probability mass on \\texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \\texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \\texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹(dLLMs)åœ¨æŒ‡ä»¤å¾®è°ƒ(instruction-tuned)è¿‡ç¨‹ä¸­å‡ºç°çš„`<eos>` overflowé—®é¢˜è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œè¿™ä¸€ç¼ºé™·å¯¼è‡´æ¨¡å‹åœ¨åˆ†é…åºåˆ—é•¿åº¦å¢åŠ æ—¶åè€Œå‡ºç°è¿‡æ—©ç»ˆæ­¢æˆ–ç”Ÿæˆæ— æ•ˆçš„`<eos>`æ ‡è®°æµã€‚ç ”ç©¶å‘ç°å…¶æ ¹æœ¬åŸå› åœ¨äº`<eos>`åŒæ—¶æ‰¿æ‹…äº†ç»ˆæ­¢å’Œå¡«å……çš„åŒé‡è§’è‰²ï¼Œå¯¼è‡´æ¦‚ç‡è´¨é‡åœ¨åæœŸä½ç½®è¿‡åº¦é›†ä¸­å¹¶å‘å‰ä¼ æ’­è§¦å‘æå‰ç»ˆæ­¢ã€‚ä¸ºè§£å†³è¿™ä¸€ç¼ºé™·ï¼Œä½œè€…æå‡ºäº†Rainbow Paddingæ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨å¾ªç¯çš„ç¦»æ•£å¡«å……æ ‡è®°æ›¿ä»£é‡å¤çš„`<eos>`å ä½ç¬¦ï¼Œæœ‰æ•ˆåˆ†æ•£äº†æ¦‚ç‡è´¨é‡å¹¶æ‰“ç ´äº†`<eos>`çš„æ”¯é…åœ°ä½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRainbow Paddingæ˜¾è‘—æå‡äº†æ¨¡å‹çš„é•¿åº¦é²æ£’æ€§å’Œè¾“å‡ºè´¨é‡ï¼Œä»…éœ€ä½¿ç”¨ä¸ƒä¸ªä¸åŒçš„å¡«å……æ ‡è®°å³å¯æœ‰æ•ˆé˜²æ­¢è¿‡æ—©ç»ˆæ­¢ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å…·æœ‰æé«˜çš„å®ç”¨æ€§ï¼Œé€šè¿‡LoRAå¯¹æå°‘é‡æ•°æ®è¿›è¡Œå•è½®å¾®è°ƒå³å¯ä½¿ç°æœ‰æ¨¡å‹è·å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages. Project page available at~\\url{https://ai-isl.github.io/rainbow-padding}",
      "pdf_url": "https://arxiv.org/pdf/2510.03680v1",
      "published_date": "2025-10-04 05:24:27 UTC",
      "updated_date": "2025-10-04 05:24:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:28.743316+00:00"
    },
    {
      "arxiv_id": "2510.03666v1",
      "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations",
      "title_zh": "MonitorVLMï¼šä¸€ç§é¢å‘çŸ¿å±±ä½œä¸šå®‰å…¨è¿è§„æ£€æµ‹çš„è§†è§‰è¯­è¨€æ¡†æ¶",
      "authors": [
        "Jiang Wu",
        "Sichao Wu",
        "Yinsong Ma",
        "Guangyuan Yu",
        "Haoyuan Xu",
        "Lifang Zheng",
        "Jingliang Duan"
      ],
      "abstract": "Industrial accidents, particularly in high-risk domains such as surface and underground mining, are frequently caused by unsafe worker behaviors. Traditional manual inspection remains labor-intensive, error-prone, and insufficient for large-scale, dynamic environments, highlighting the urgent need for intelligent and automated safety monitoring. In this paper, we present MonitorVLM, a novel vision--language framework designed to detect safety violations directly from surveillance video streams. MonitorVLM introduces three key innovations: (1) a domain-specific violation dataset comprising 9,000 vision--question--answer (VQA) samples across 40 high-frequency mining regulations, enriched with augmentation and auxiliary detection cues; (2) a clause filter (CF) module that dynamically selects the Top-$K$ most relevant clauses, reducing inference latency by 13.56\\% while maintaining accuracy; and (3) a behavior magnifier (BM) module that enhances worker regions to improve fine-grained action recognition, yielding additional gains of 3.45% in precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM significantly outperforms baseline vision--language models, achieving improvements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score over the 72B unfine-tuned baseline. A lightweight web-based interface further integrates MonitorVLM into practical workflows, enabling automatic violation reporting with video timestamping. This study highlights the potential of multimodal large models to enhance occupational safety monitoring in mining and beyond.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MonitorVLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºçŸ¿ä¸šæ“ä½œä¸­çš„å®‰å…¨è¿è§„æ£€æµ‹è®¾è®¡çš„ Vision-Language Frameworkï¼Œæ—¨åœ¨é€šè¿‡ç›‘æ§è§†é¢‘æµè‡ªåŠ¨è¯†åˆ«ä¸å®‰å…¨è¡Œä¸ºã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ä¸€ä¸ªåŒ…å« 9,000 ä¸ª VQA æ ·æœ¬çš„é¢†åŸŸç‰¹å®šè¿è§„æ•°æ®é›†ï¼Œæ¶µç›–äº† 40 é¡¹é«˜é¢‘çŸ¿ä¸šæ³•è§„ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œç ”ç©¶å¼•å…¥äº† Clause Filter (CF) æ¨¡å—ä»¥åŠ¨æ€ç­›é€‰ç›¸å…³æ¡æ¬¾ï¼Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶å°†æ¨ç†å»¶è¿Ÿé™ä½äº† 13.56%ï¼›åŒæ—¶é€šè¿‡ Behavior Magnifier (BM) æ¨¡å—å¢å¼ºå·¥äººåŒºåŸŸå›¾åƒï¼Œä»è€Œä¼˜åŒ–ç»†ç²’åº¦åŠ¨ä½œè¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMonitorVLM åœ¨ F1 score ä¸Šæ¯”æœªå¾®è°ƒçš„ 72B åŸºå‡†æ¨¡å‹æå‡äº† 28.37%ï¼Œåœ¨ Precision å’Œ Recall ä¸Šä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼€å‘äº†è½»é‡çº§ Web ç•Œé¢ä»¥æ”¯æŒè‡ªåŠ¨è¿è§„æŠ¥å‘Šå’Œè§†é¢‘æ—¶é—´æˆ³åŠŸèƒ½ï¼Œå……åˆ†å±•ç¤ºäº†å¤šæ¨¡æ€å¤§æ¨¡å‹åœ¨å¢å¼ºçŸ¿ä¸šèŒä¸šå®‰å…¨ç›‘æ§æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03666v1",
      "published_date": "2025-10-04 04:46:21 UTC",
      "updated_date": "2025-10-04 04:46:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:25.257751+00:00"
    },
    {
      "arxiv_id": "2510.03662v1",
      "title": "Operationalizing Data Minimization for Privacy-Preserving LLM Prompting",
      "title_zh": "å®ç°éšç§ä¿æŠ¤ LLM æç¤ºçš„æ•°æ®æœ€å°åŒ–",
      "authors": [
        "Jijie Zhou",
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "abstract": "The rapid deployment of large language models (LLMs) in consumer applications has led to frequent exchanges of personal information. To obtain useful responses, users often share more than necessary, increasing privacy risks via memorization, context-based personalization, or security breaches. We present a framework to formally define and operationalize data minimization: for a given user prompt and response model, quantifying the least privacy-revealing disclosure that maintains utility, and we propose a priority-queue tree search to locate this optimal point within a privacy-ordered transformation space. We evaluated the framework on four datasets spanning open-ended conversations (ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth answers (CaseHold, MedQA), quantifying achievable data minimization with nine LLMs as the response model. Our results demonstrate that larger frontier LLMs can tolerate stronger data minimization while maintaining task quality than smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that LLMs struggle to predict optimal data minimization directly, showing a bias toward abstraction that leads to oversharing. This suggests not just a privacy gap, but a capability gap: models may lack awareness of what information they actually need to solve a task.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åº”ç”¨ä¸­è¿‡åº¦æ”¶é›†ä¸ªäººä¿¡æ¯çš„éšç§é£é™©ï¼Œæå‡ºäº†ä¸€ç§æ­£å¼å®šä¹‰å’Œå®æ–½æ•°æ®æœ€å°åŒ–ï¼ˆData Minimizationï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ—æ ‘æœç´¢ï¼ˆpriority-queue tree searchï¼‰åœ¨éšç§æ’åºçš„è½¬æ¢ç©ºé—´å†…ï¼Œé‡åŒ–å¹¶å¯»æ‰¾èƒ½å¤Ÿç»´æŒæ•ˆç”¨çš„æœ€å°ä¿¡æ¯æŠ«éœ²ç‚¹ã€‚é€šè¿‡åœ¨ ShareGPT å’Œ MedQA ç­‰å››ä¸ªæ•°æ®é›†ä»¥åŠä¹ç§ LLMs ä¸Šçš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å¤§å‹å‰æ²¿æ¨¡å‹ï¼ˆå¦‚ GPT-5ï¼‰æ¯”å°å‹æ¨¡å‹æ›´èƒ½å®¹å¿é«˜æ¯”ä¾‹çš„æ•°æ®è„±æ•ï¼ˆredactionï¼‰ï¼Œåœ¨ä¿æŒä»»åŠ¡è´¨é‡çš„åŒæ—¶å®ç°æ›´å¼ºçš„éšç§ä¿æŠ¤ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥æ­ç¤ºï¼ŒLLMs åœ¨ç›´æ¥é¢„æµ‹æœ€ä¼˜æœ€å°åŒ–æ–¹æ¡ˆæ—¶è¡¨ç°ä¸ä½³ï¼Œæ™®éå­˜åœ¨è¿‡åº¦åˆ†äº«çš„åè§ã€‚è¿™è¡¨æ˜æ¨¡å‹åœ¨è¯†åˆ«å®Œæˆä»»åŠ¡æ‰€éœ€æ ¸å¿ƒä¿¡æ¯æ–¹é¢å­˜åœ¨èƒ½åŠ›å·®è·ï¼ˆcapability gapï¼‰ï¼Œä¸ºæœªæ¥æ„å»ºå¯æ“ä½œçš„éšç§ä¿æŠ¤æç¤ºå·¥ç¨‹å¥ å®šäº†ç†è®ºä¸æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03662v1",
      "published_date": "2025-10-04 04:20:18 UTC",
      "updated_date": "2025-10-04 04:20:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:24.560374+00:00"
    },
    {
      "arxiv_id": "2510.03659v1",
      "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
      "title_zh": "é«˜å¯è§£é‡Šæ€§æ˜¯å¦æ„å‘³ç€æ›´ä½³çš„æ•ˆç”¨ï¼Ÿé’ˆå¯¹ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨çš„æˆå¯¹åˆ†æ",
      "authors": [
        "Xu Wang",
        "Yan Hu",
        "Benyou Wang",
        "Difan Zou"
      ],
      "abstract": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoders, SAEs)çš„å¯è§£é‡Šæ€§ä¸æ¨¡å‹æ“æ§æ•ˆç”¨(steering utility)ä¹‹é—´çš„å…³ç³»ï¼ŒæŒ‘æˆ˜äº†â€œé«˜å¯è§£é‡Šæ€§å¿…ç„¶å¸¦æ¥æ›´å¥½æ“æ§æ•ˆæœâ€çš„ä¼ ç»Ÿå‡è®¾ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨Gemma-2å’ŒQwen-2.5ç­‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè®­ç»ƒäº†90ä¸ªå…·æœ‰ä¸åŒæ¶æ„å’Œç¨€ç–åº¦çš„SAEsï¼Œå¹¶åˆ©ç”¨SAEBenchå’ŒAxBenchè¿›è¡Œäº†æ’åºä¸€è‡´æ€§åˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯è§£é‡Šæ€§ä¸æ“æ§æ•ˆç”¨ä¹‹é—´ä»…å­˜åœ¨è¾ƒå¼±çš„æ­£ç›¸å…³æ€§(Kendall's tau bçº¦ä¸º0.298)ï¼Œè¯´æ˜å¯è§£é‡Šæ€§å¹¶éè¡¡é‡æ“æ§æ€§èƒ½çš„æœ‰æ•ˆä»£ç†æŒ‡æ ‡ã€‚ä¸ºäº†å¯»æ‰¾çœŸæ­£èƒ½å¤Ÿé©±åŠ¨æ¨¡å‹è¡Œä¸ºçš„ç‰¹å¾ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸ºDelta Token Confidenceçš„æ–°å‹é€‰æ‹©å‡†åˆ™ï¼Œæ—¨åœ¨é€šè¿‡è¡¡é‡ç‰¹å¾æ”¾å¤§å¯¹ä¸‹æ–‡Tokenåˆ†å¸ƒçš„å½±å“æ¥ç­›é€‰ç‰¹å¾ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å°†ä¸‰ä¸ªLLMsçš„æ“æ§æ€§èƒ½æå‡äº†52.52%ã€‚æœ€å¼•äººæ³¨ç›®çš„æ˜¯ï¼Œåœ¨ç­›é€‰å‡ºé«˜æ•ˆæ“æ§ç‰¹å¾åï¼Œå¯è§£é‡Šæ€§ä¸æ•ˆç”¨ä¹‹é—´çš„ç›¸å…³æ€§å‡ ä¹æ¶ˆå¤±ç”šè‡³å˜ä¸ºè´Ÿå€¼ï¼Œè¿›ä¸€æ­¥æ­ç¤ºäº†ä¸¤è€…åœ¨é«˜æ•ˆæ“æ§ç‰¹å¾ä¸Šçš„æ˜¾è‘—èƒŒç¦»ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03659v1",
      "published_date": "2025-10-04 04:14:50 UTC",
      "updated_date": "2025-10-04 04:14:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:30.248742+00:00"
    },
    {
      "arxiv_id": "2510.03650v1",
      "title": "LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design",
      "title_zh": "LLMå¼•å¯¼çš„æ‹Ÿè’™ç‰¹å¡ç½—è®¾è®¡è¿›åŒ–ç¨‹åºåˆæˆ",
      "authors": [
        "Amir Sadikov"
      ],
      "abstract": "Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo (QMC) methods for high-dimensional integration. We cast two long-standing QMC design problems as program synthesis and solve them with an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness: (i) constructing finite 2D/3D point sets with low star discrepancy, and (ii) choosing Sobol' direction numbers that minimize randomized QMC error on downstream integrands. Our two-phase procedure combines constructive code proposals with iterative numerical refinement. On finite sets, we rediscover known optima in small 2D cases and set new best-known 2D benchmarks for N >= 40, while matching most known 3D optima up to the proven frontier (N <= 8) and reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol' parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC) mean-squared error for several 32-dimensional option-pricing tasks relative to widely used Joe--Kuo parameters, while preserving extensibility to any sample size and compatibility with standard randomizations. Taken together, the results demonstrate that LLM-driven evolutionary program synthesis can automate the discovery of high-quality QMC constructions, recovering classical designs where they are optimal and improving them where finite-N structure matters. Data and code are available at https://github.com/hockeyguy123/openevolve-star-discrepancy.git.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†å‡†è’™ç‰¹å¡ç½— (Quasi-Monte Carlo, QMC) è®¾è®¡ä¸­çš„ä½åå·®ç‚¹é›†æ„å»ºå’Œæ•°å­—åºåˆ—ä¼˜åŒ–é—®é¢˜è½¬åŒ–ä¸ºç¨‹åºåˆæˆä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç”±å¤§è¯­è¨€æ¨¡å‹å¼•å¯¼çš„è¿›åŒ–ç¨‹åºåˆæˆ (LLM-guided evolutionary program synthesis) æ–¹æ³•ã€‚è¯¥æ¡†æ¶é€šè¿‡ LLM åœ¨ç‰¹å®šä»»åŠ¡é€‚åº”åº¦ä¸‹å¯¹ä»£ç è¿›è¡Œå˜å¼‚å’Œé€‰æ‹©ï¼Œç»“åˆäº†æ„é€ æ€§ä»£ç ææ¡ˆä¸è¿­ä»£æ•°å€¼ç»†åŒ–ã€‚åœ¨æœ‰é™ç‚¹é›†æ„å»ºå®éªŒä¸­ï¼Œè¯¥æ–¹æ³•åˆ·æ–°äº† N >= 40 æ—¶çš„äºŒç»´æ˜Ÿå½¢åå·® (star discrepancy) åŸºå‡†ï¼Œå¹¶åœ¨ä¸‰ç»´åœºæ™¯ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€ä¼˜ç»“æœã€‚é’ˆå¯¹æ•°å­—åºåˆ—ä¼˜åŒ–ï¼Œæ¼”åŒ–å‡ºçš„ Sobol' æ–¹å‘æ•°åœ¨å¤šé¡¹ 32 ç»´æœŸæƒå®šä»·ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶å‡æ–¹è¯¯å·® (MSE) æ˜¾è‘—ä½äºå¹¿æ³›ä½¿ç”¨çš„ Joe-Kuo å‚æ•°ã€‚è¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡è®¡ç®—ç²¾åº¦çš„åŒæ—¶ï¼Œä¿ç•™äº†å¯¹ä»»æ„æ ·æœ¬é‡çš„å¯æ‰©å±•æ€§ä»¥åŠä¸æ ‡å‡†éšæœºåŒ–æŠ€æœ¯çš„å…¼å®¹æ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº† LLM é©±åŠ¨çš„è‡ªåŠ¨åŒ–è®¾è®¡èƒ½å¤Ÿæœ‰æ•ˆå‘ç°é«˜è´¨é‡çš„ QMC æ„é€ ï¼Œå°¤å…¶åœ¨å¤„ç†æœ‰é™æ ·æœ¬ç»“æ„æ—¶ç›¸è¾ƒäºä¼ ç»Ÿç»å…¸è®¾è®¡å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.NE",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03650v1",
      "published_date": "2025-10-04 03:32:41 UTC",
      "updated_date": "2025-10-04 03:32:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:34.548080+00:00"
    },
    {
      "arxiv_id": "2510.03639v1",
      "title": "Towards Unsupervised Speech Recognition at the Syllable-Level",
      "title_zh": "è¿ˆå‘éŸ³èŠ‚çº§æ— ç›‘ç£è¯­éŸ³è¯†åˆ«",
      "authors": [
        "Liming Wang",
        "Junrui Ni",
        "Kai-Wei Chang",
        "Saurabhchand Bhati",
        "David Harwath",
        "Mark Hasegawa-Johnson",
        "James R. Glass"
      ],
      "abstract": "Training speech recognizers with unpaired speech and text -- known as unsupervised speech recognition (UASR) -- is a crucial step toward extending ASR to low-resource languages in the long-tail distribution and enabling multimodal learning from non-parallel data. However, existing approaches based on phones often rely on costly resources such as grapheme-to-phoneme converters (G2Ps) and struggle to generalize to languages with ambiguous phoneme boundaries due to training instability. In this paper, we address both challenges by introducing a syllable-level UASR framework based on masked language modeling, which avoids the need for G2P and the instability of GAN-based methods. Our approach achieves up to a 40\\% relative reduction in character error rate (CER) on LibriSpeech and generalizes effectively to Mandarin, a language that has remained particularly difficult for prior methods. Code will be released upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§éŸ³èŠ‚çº§åˆ«(Syllable-level)çš„æ— ç›‘ç£è¯­éŸ³è¯†åˆ«(Unsupervised Speech Recognition, UASR)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºäºéŸ³ç´ çš„æ–¹æ³•å¯¹éŸ³ç´ è½¬æ¢å™¨(G2P)çš„ä¾èµ–ä»¥åŠè®­ç»ƒä¸ç¨³å®šçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŸºäºæ©ç è¯­è¨€æ¨¡å‹(Masked Language Modeling, MLM)æ„å»ºï¼Œæœ‰æ•ˆé¿å…äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)æ–¹æ³•åœ¨è®­ç»ƒä¸­çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨LibriSpeechæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾40%çš„å­—ç¬¦é”™è¯¯ç‡(Character Error Rate, CER)ç›¸å¯¹é™ä½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸­æ–‡(Mandarin)ç­‰æ­¤å‰éš¾ä»¥å¤„ç†çš„è¯­è¨€ä¸Šå±•ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºä½èµ„æºè¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’Œå¤šæ¨¡æ€å­¦ä¹ æä¾›äº†æ›´ç¨³å¥çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03639v1",
      "published_date": "2025-10-04 02:56:33 UTC",
      "updated_date": "2025-10-04 02:56:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:35.751831+00:00"
    },
    {
      "arxiv_id": "2510.03638v2",
      "title": "Implicit Models: Expressive Power Scales with Test-Time Compute",
      "title_zh": "éšå¼æ¨¡å‹ï¼šè¡¨è¾¾èƒ½åŠ›éšæµ‹è¯•æ—¶è®¡ç®—æ‰©å±•",
      "authors": [
        "Jialin Liu",
        "Lisang Ding",
        "Stanley Osher",
        "Wotao Yin"
      ],
      "abstract": "Implicit models, an emerging model class, compute outputs by iterating a single parameter block to a fixed point. This architecture realizes an infinite-depth, weight-tied network that trains with constant memory, significantly reducing memory needs for the same level of performance compared to explicit models. While it is empirically known that these compact models can often match or even exceed the accuracy of larger explicit networks by allocating more test-time compute, the underlying mechanism remains poorly understood.\n  We study this gap through a nonparametric analysis of expressive power. We provide a strict mathematical characterization, showing that a simple and regular implicit operator can, through iteration, progressively express more complex mappings. We prove that for a broad class of implicit models, this process lets the model's expressive power scale with test-time compute, ultimately matching a much richer function class. The theory is validated across four domains: image reconstruction, scientific computing, operations research, and LLM reasoning, demonstrating that as test-time iterations increase, the complexity of the learned mapping rises, while the solution quality simultaneously improves and stabilizes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†éšå¼æ¨¡å‹ï¼ˆImplicit Modelsï¼‰çš„è¡¨è¾¾èƒ½åŠ›ï¼Œè¿™ç±»æ¨¡å‹é€šè¿‡å°†å•ä¸ªå‚æ•°å—è¿­ä»£è‡³ä¸åŠ¨ç‚¹æ¥è®¡ç®—è¾“å‡ºï¼Œå…·æœ‰æ— é™æ·±åº¦ã€æƒé‡å…±äº«ä¸”å†…å­˜å ç”¨æ’å®šçš„ç‰¹ç‚¹ã€‚å°½ç®¡å®éªŒè¡¨æ˜è¿™ç±»ç´§å‡‘æ¨¡å‹é€šè¿‡å¢åŠ æ¨ç†æ—¶è®¡ç®—é‡ï¼ˆTest-Time Computeï¼‰å¯ä»¥åŒ¹é…ç”šè‡³è¶…è¶Šå¤§å‹æ˜¾å¼ç½‘ç»œï¼Œä½†å…¶èƒŒåçš„ç†è®ºæœºåˆ¶ä¸€ç›´å°šä¸æ˜ç¡®ã€‚æœ¬æ–‡é€šè¿‡éå‚æ•°åˆ†ææä¾›äº†ä¸¥æ ¼çš„æ•°å­¦è¡¨å¾ï¼Œè¯æ˜äº†ç®€å•çš„æ­£åˆ™éšå¼ç®—å­é€šè¿‡è¿­ä»£å¯ä»¥é€æ­¥è¡¨è¾¾æ›´å¤æ‚çš„æ˜ å°„ã€‚ç ”ç©¶è¯æ˜å¯¹äºå¹¿æ³›çš„éšå¼æ¨¡å‹ï¼Œå…¶è¡¨è¾¾èƒ½åŠ›éšæ¨ç†æ—¶è®¡ç®—é‡çš„å¢åŠ è€Œæ‰©å±•ï¼Œæœ€ç»ˆèƒ½å¤ŸåŒ¹é…æ›´ä¸°å¯Œçš„å‡½æ•°ç±»ã€‚è¯¥ç†è®ºåœ¨å›¾åƒé‡å»ºï¼ˆImage Reconstructionï¼‰ã€ç§‘å­¦è®¡ç®—ï¼ˆScientific Computingï¼‰ã€è¿ç­¹å­¦ï¼ˆOperations Researchï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ï¼ˆLLM Reasoningï¼‰å››ä¸ªé¢†åŸŸå¾—åˆ°äº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšç€æ¨ç†æ—¶è¿­ä»£æ¬¡æ•°å¢åŠ ï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„æ˜ å°„å¤æ‚åº¦æ˜¾è‘—æå‡ï¼ŒåŒæ—¶è§£çš„è´¨é‡å¾—åˆ°æ”¹å–„å¹¶è¶‹äºç¨³å®šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.RT",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03638v2",
      "published_date": "2025-10-04 02:49:22 UTC",
      "updated_date": "2025-11-28 21:40:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:40.847211+00:00"
    },
    {
      "arxiv_id": "2510.03633v1",
      "title": "Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å¢å¼ºæ¨æ–‡æƒ…æ„Ÿåˆ†æçš„è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿é¢„æµ‹",
      "authors": [
        "An Vuong",
        "Susan Gauch"
      ],
      "abstract": "Accurately predicting short-term stock price movement remains a challenging task due to the market's inherent volatility and sensitivity to investor sentiment. This paper discusses a deep learning framework that integrates emotion features extracted from tweet data with historical stock price information to forecast significant price changes on the following day. We utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby enhancing the quality of emotion features derived from three emotion analysis approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face library and two lexicon-based methods using National Research Council Canada (NRC) resources. These features are combined with previous-day stock price data to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA, AAPL, and AMZN stocks show that all three emotion analysis methods improve the average accuracy for predicting significant price movements, compared to the baseline model using only historical stock prices, which yields an accuracy of 13.5%. The DistilRoBERTa-based stock prediction model achieves the best performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced emotion analysis. These results demonstrate that using large language models to preprocess tweet content enhances the effectiveness of emotion analysis which in turn improves the accuracy of predicting significant stock price movements.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é›†æˆæ¨æ–‡æƒ…æ„Ÿç‰¹å¾ä¸å†å²è‚¡ä»·ä¿¡æ¯çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹æ¬¡æ—¥çš„è‚¡ç¥¨ä»·æ ¼æ˜¾è‘—å˜åŠ¨ã€‚ç ”ç©¶åˆ©ç”¨ Llama 3.1-8B-Instruct æ¨¡å‹å¯¹æ¨æ–‡è¿›è¡Œé¢„å¤„ç†ï¼Œä»è€Œå¢å¼ºäº†åŸºäº DistilRoBERTa çš„åˆ†ç±»å™¨å’ŒåŸºäºè¯å…¸çš„ NRC æ–¹æ³•æå–çš„æƒ…æ„Ÿç‰¹å¾è´¨é‡ã€‚è¿™äº›ç‰¹å¾éšåä¸å†å²è‚¡ä»·æ•°æ®ç»“åˆï¼Œè¾“å…¥åˆ° Long Short-Term Memory (LSTM) æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ TSLAã€AAPL å’Œ AMZN è‚¡ç¥¨ä¸Šï¼Œå¼•å…¥æƒ…æ„Ÿåˆ†æå‡æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨è‚¡ä»·çš„ 13.5% åŸºå‡†å‡†ç¡®ç‡ã€‚å…¶ä¸­ï¼ŒåŸºäº DistilRoBERTa çš„æ¨¡å‹åœ¨ç»è¿‡ LLaMA å¢å¼ºåï¼Œé¢„æµ‹å‡†ç¡®ç‡ä» 23.6% æå‡è‡³ 38.5%ã€‚è¯¥ç ”ç©¶è¯å®äº†åˆ©ç”¨ Large Language Models (LLMs) é¢„å¤„ç†ç¤¾äº¤åª’ä½“å†…å®¹èƒ½æœ‰æ•ˆæå‡æƒ…æ„Ÿåˆ†ææ•ˆèƒ½ï¼Œè¿›è€Œæé«˜çŸ­æœŸè‚¡å¸‚é¢„æµ‹çš„ç²¾ç¡®åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17th International Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (KDIR 2025), Marbella, Spain, Oct. 22-24, 2025 (to appear) Best Student Paper Finalist",
      "pdf_url": "https://arxiv.org/pdf/2510.03633v1",
      "published_date": "2025-10-04 02:31:44 UTC",
      "updated_date": "2025-10-04 02:31:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:45.337364+00:00"
    },
    {
      "arxiv_id": "2510.03632v1",
      "title": "MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information",
      "title_zh": "MITSï¼šåŸºäºé€ç‚¹äº’ä¿¡æ¯çš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå‹æ ‘æœç´¢æ¨ç†",
      "authors": [
        "Jiaxi Li",
        "Yucheng Shi",
        "Jin Lu",
        "Ninghao Liu"
      ],
      "abstract": "Tree search has become as a representative framework for test-time reasoning with large language models (LLMs), exemplified by methods such as Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning paths. However, it remains difficult to provide instant and reliable quantitative assessments of intermediate reasoning step quality, and extensive path exploration is computationally costly. To address this, we propose Mutual Information Tree Search (MITS), a novel framework that guides reasoning with information-theoretic principles. MITS introduces an effective scoring function based on pointwise mutual information (PMI), which enables step-wise evaluation of reasoning paths and search tree expansion via beam search without expensive look-ahead simulations, achieving superior reasoning performances while maintaining computational efficiency. The framework is complemented by an entropy-based dynamic sampling strategy that adaptively allocates computational resources to uncertain reasoning steps where exploration is most beneficial. For final prediction, MITS employs a weighted voting scheme that combines PMI scores with prediction consensus. Through comprehensive experiments on diverse reasoning benchmarks, MITS consistently surpasses baseline methods, establishing a principled and efficient framework for LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MITSï¼ˆMutual Information Tree Searchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨ä¿¡æ¯è®ºåŸç†æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†çš„æ–°æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ ‘æœç´¢æ–¹æ³•ä¸­ä¸­é—´æ¨ç†æ­¥éª¤è¯„ä¼°å›°éš¾ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼ŒMITS å¼•å…¥äº†åŸºäºç‚¹äº’ä¿¡æ¯ï¼ˆPointwise Mutual Information, PMIï¼‰çš„è¯„åˆ†å‡½æ•°ï¼Œå®ç°äº†å¯¹æ¨ç†è·¯å¾„çš„é€æ­¥è¯„ä¼°å’ŒåŸºäº Beam Search çš„æœç´¢æ ‘æ‰©å±•ï¼Œä¸”æ— éœ€æ˜‚è´µçš„å‰ç»æ¨¡æ‹Ÿï¼ˆlook-ahead simulationsï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŸºäºç†µçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œèƒ½å¤Ÿæ ¹æ®æ¨ç†æ­¥éª¤çš„ä¸ç¡®å®šæ€§è‡ªé€‚åº”åœ°åˆ†é…è®¡ç®—èµ„æºï¼Œå¹¶é‡‡ç”¨ç»“åˆ PMI åˆ†æ•°ä¸é¢„æµ‹å…±è¯†çš„åŠ æƒæŠ•ç¥¨æ–¹æ¡ˆè¿›è¡Œæœ€ç»ˆé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMITS åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡æŒç»­è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œåœ¨æ˜¾è‘—æå‡æ¨ç†æ€§èƒ½çš„åŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä¸º LLMs æ¨ç†æä¾›äº†ä¸€ä¸ªå…·æœ‰åŸåˆ™æ€§ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.03632v1",
      "published_date": "2025-10-04 02:30:40 UTC",
      "updated_date": "2025-10-04 02:30:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:44.144705+00:00"
    },
    {
      "arxiv_id": "2510.03623v1",
      "title": "Explainable but Vulnerable: Adversarial Attacks on XAI Explanation in Cybersecurity Applications",
      "title_zh": "å¯è§£é‡Šäº¦è„†å¼±ï¼šé’ˆå¯¹ç½‘ç»œå®‰å…¨åº”ç”¨ä¸­ XAI è§£é‡Šçš„å¯¹æŠ—æ€§æ”»å‡»",
      "authors": [
        "Maraz Mia",
        "Mir Mehedi A. Pritom"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) has aided machine learning (ML) researchers with the power of scrutinizing the decisions of the black-box models. XAI methods enable looking deep inside the models' behavior, eventually generating explanations along with a perceived trust and transparency. However, depending on any specific XAI method, the level of trust can vary. It is evident that XAI methods can themselves be a victim of post-adversarial attacks that manipulate the expected outcome from the explanation module. Among such attack tactics, fairwashing explanation (FE), manipulation explanation (ME), and backdoor-enabled manipulation attacks (BD) are the notable ones. In this paper, we try to understand these adversarial attack techniques, tactics, and procedures (TTPs) on explanation alteration and thus the effect on the model's decisions. We have explored a total of six different individual attack procedures on post-hoc explanation methods such as SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanation), and IG (Integrated Gradients), and investigated those adversarial attacks in cybersecurity applications scenarios such as phishing, malware, intrusion, and fraudulent website detection. Our experimental study reveals the actual effectiveness of these attacks, thus providing an urgency for immediate attention to enhance the resiliency of XAI methods and their applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable Artificial Intelligence, XAI) åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸé¢ä¸´çš„è„†å¼±æ€§ï¼Œæ·±å…¥åˆ†æäº†é’ˆå¯¹è§£é‡Šæ¨¡å—çš„å¯¹æŠ—æ€§æ”»å‡»ã€‚æ–‡ç« è¯¦ç»†ç ”ç©¶äº†å…¬å¹³æ´—ç™½è§£é‡Š (fairwashing explanation, FE)ã€æ“æ§è§£é‡Š (manipulation explanation, ME) ä»¥åŠåŸºäºåé—¨çš„æ“æ§æ”»å‡» (backdoor-enabled manipulation attacks, BD) ç­‰æ”»å‡»ç­–ç•¥ã€‚ä½œè€…é’ˆå¯¹ SHAPã€LIME å’Œ Integrated Gradients (IG) ç­‰ä¸»æµäº‹åè§£é‡Šæ–¹æ³• (post-hoc explanation methods) å®æ–½äº†å…­ç§ä¸åŒçš„æ”»å‡»ç¨‹åºã€‚å®éªŒåœºæ™¯æ¶µç›–äº†ç½‘ç»œé’“é±¼ã€æ¶æ„è½¯ä»¶ã€å…¥ä¾µæ£€æµ‹å’Œæ¬ºè¯ˆç½‘ç«™è¯†åˆ«ç­‰å…³é”®ç½‘ç»œå®‰å…¨ä»»åŠ¡ã€‚ç ”ç©¶ç»“æœè¯å®äº†è¿™äº›å¯¹æŠ—æ€§æ”»å‡»èƒ½å¤Ÿæœ‰æ•ˆç¯¡æ”¹è§£é‡Šç»“æœï¼Œè¿›è€Œç ´åæ¨¡å‹å†³ç­–çš„é€æ˜åº¦ä¸å¯ä¿¡åº¦ã€‚è¯¥å‘ç°æ­ç¤ºäº†ç°æœ‰ XAI æ–¹æ³•åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„å®‰å…¨éšæ‚£ï¼Œå¹¶å¼ºè°ƒäº†å¢å¼º XAI ç³»ç»ŸåŠå…¶åº”ç”¨æŠµå¾¡æ”»å‡»èƒ½åŠ›ã€æå‡é²æ£’æ€§çš„ç´§è¿«éœ€æ±‚ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 9 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.03623v1",
      "published_date": "2025-10-04 02:07:58 UTC",
      "updated_date": "2025-10-04 02:07:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:55.657174+00:00"
    },
    {
      "arxiv_id": "2510.03614v1",
      "title": "Neural Bayesian Filtering",
      "title_zh": "ç¥ç»è´å¶æ–¯æ»¤æ³¢",
      "authors": [
        "Christopher Solinas",
        "Radovan Haluska",
        "David Sychrovsky",
        "Finbarr Timbers",
        "Nolan Bard",
        "Michael Buro",
        "Martin Schmid",
        "Nathan R. Sturtevant",
        "Michael Bowling"
      ],
      "abstract": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining distributions over hidden states, called beliefs, in partially observable systems. NBF is trained to find a good latent representation of the beliefs induced by a task. It maps beliefs to fixed-length embedding vectors, which condition generative models for sampling. During filtering, particle-style updates compute posteriors in this embedding space using incoming observations and the environment's dynamics. NBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of particle impoverishment. We validate NBF in state estimation tasks in three partially observable environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Neural Bayesian Filtering (NBF)ï¼Œä¸€ç§ç”¨äºåœ¨éƒ¨åˆ†å¯è§‚æµ‹ç³»ç»Ÿ(partially observable systems)ä¸­ç»´æŠ¤éšè—çŠ¶æ€åˆ†å¸ƒï¼ˆå³ä¿¡å¿µï¼Œbeliefsï¼‰çš„ç®—æ³•ã€‚NBFæ—¨åœ¨å¯»æ‰¾ä»»åŠ¡è¯±å¯¼ä¿¡å¿µçš„è‰¯å¥½æ½œåœ¨è¡¨ç¤º(latent representation)ï¼Œå¹¶å°†ä¿¡å¿µæ˜ å°„ä¸ºå›ºå®šé•¿åº¦çš„åµŒå…¥å‘é‡(embedding vectors)ï¼Œä»¥è°ƒèŠ‚ç”Ÿæˆæ¨¡å‹(generative models)è¿›è¡Œé‡‡æ ·ã€‚åœ¨æ»¤æ³¢è¿‡ç¨‹ä¸­ï¼Œè¯¥ç®—æ³•åˆ©ç”¨å®æ—¶è§‚æµ‹æ•°æ®å’Œç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œé€šè¿‡ç²’å­å¼æ›´æ–°(particle-style updates)åœ¨åµŒå…¥ç©ºé—´ä¸­è®¡ç®—åéªŒæ¦‚ç‡ã€‚NBFç»“åˆäº†ç»å…¸æ»¤æ³¢å™¨çš„è®¡ç®—æ•ˆç‡ä¸æ·±åº¦ç”Ÿæˆæ¨¡å‹(deep generative models)çš„è¡¨è¾¾èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿½è¸ªå¿«é€Ÿå˜åŒ–ä¸”å¤šå³°(multimodal)çš„ä¿¡å¿µï¼ŒåŒæ—¶æœ‰æ•ˆç¼“è§£äº†ç²’å­åŒ®ä¹(particle impoverishment)çš„é£é™©ã€‚å®éªŒåœ¨ä¸‰ä¸ªéƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒçš„çŠ¶æ€ä¼°è®¡ä»»åŠ¡ä¸­éªŒè¯äº†NBFçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03614v1",
      "published_date": "2025-10-04 01:58:55 UTC",
      "updated_date": "2025-10-04 01:58:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:48:58.246092+00:00"
    },
    {
      "arxiv_id": "2510.03612v1",
      "title": "Cross-Modal Content Optimization for Steering Web Agent Preferences",
      "title_zh": "å¼•å¯¼ç½‘ç»œæ™ºèƒ½ä½“åå¥½çš„è·¨æ¨¡æ€å†…å®¹ä¼˜åŒ–",
      "authors": [
        "Tanqiu Jiang",
        "Min Bai",
        "Nikolaos Pappas",
        "Yanjun Qi",
        "Sandesh Swamy"
      ],
      "abstract": "Vision-language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item's visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing's images and textual metadata, with no insight into the agent's model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„ç½‘é¡µæ™ºèƒ½ä½“åœ¨å†…å®¹æ¨èå’Œäº§å“æ’åç­‰å…³é”®ä»»åŠ¡ä¸­é¢ä¸´çš„åå¥½æ“çºµé£é™©ï¼Œå¹¶æŒ‡å‡ºå½“å‰ç ”ç©¶å¤šå±€é™äºä¸åˆ‡å®é™…çš„ç™½ç›’è®¿é—®æˆ–å•æ¨¡æ€æ‰°åŠ¨ã€‚ä½œè€…æå‡ºäº†è·¨æ¨¡æ€åå¥½å¼•å¯¼(Cross-Modal Preference Steering, CPS)ï¼Œé¦–æ¬¡è¯æ˜äº†è”åˆåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬é€šé“å¯ä»¥åœ¨ç°å®çš„é»‘ç›’å¨èƒæ¨¡å‹ä¸‹å®ç°æ›´å¼ºå¤§çš„åå¥½æ“çºµã€‚CPSé€šè¿‡ååŒä¼˜åŒ–CLIPå¯è¿ç§»çš„å›¾åƒæ‰°åŠ¨å’ŒRLHFè¯±å¯¼çš„è¯­è¨€åå·®ï¼Œåœ¨ä»…ä¿®æ”¹è‡ªèº«æ¡ç›®å…ƒæ•°æ®çš„æƒ…å†µä¸‹å³å¯å¼•å¯¼æ™ºèƒ½ä½“å†³ç­–ã€‚åœ¨é’ˆå¯¹GPT-4.1ã€Qwen-2.5VLå’ŒPixtral-Largeç­‰ä¸»æµæ¨¡å‹çš„ç”µå½±ä¸ç”µå•†ä»»åŠ¡æµ‹è¯•ä¸­ï¼ŒCPSçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶å°†æ£€æµ‹ç‡é™ä½äº†70%ï¼Œå±•ç°äº†æé«˜çš„éšè”½æ€§ã€‚è¿™é¡¹å‘ç°çªæ˜¾äº†åœ¨æ™ºèƒ½ä½“ç³»ç»Ÿæ—¥ç›Šæ™®åŠçš„èƒŒæ™¯ä¸‹ï¼Œæ„å»ºé²æ£’é˜²å¾¡æœºåˆ¶ä»¥åº”å¯¹è·¨æ¨¡æ€å†…å®¹ä¼˜åŒ–çš„ç´§è¿«æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03612v1",
      "published_date": "2025-10-04 01:57:20 UTC",
      "updated_date": "2025-10-04 01:57:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:05.745453+00:00"
    },
    {
      "arxiv_id": "2510.03611v1",
      "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length",
      "title_zh": "LLM èƒ½å¦æ¨æ–­å›¾ç»“æ„ï¼Ÿè®°å¿†æ¼‚ç§»ä¸ä¸Šä¸‹æ–‡é•¿åº¦æ¢ç©¶",
      "authors": [
        "Raquib Bin Yousuf",
        "Aadyant Khatri",
        "Shengzhe Xu",
        "Mandar Sharma",
        "Naren Ramakrishnan"
      ],
      "abstract": "Recently proposed evaluation benchmarks aim to characterize the effective context length and the forgetting tendencies of large language models (LLMs). However, these benchmarks often rely on simplistic 'needle in a haystack' retrieval or continuation tasks that may not accurately reflect the performance of these models in information-dense scenarios. Thus, rather than simple next token prediction, we argue for evaluating these models on more complex reasoning tasks that requires them to induce structured relational knowledge from the text - such as graphs from potentially noisy natural language content. While the input text can be viewed as generated in terms of a graph, its structure is not made explicit and connections must be induced from distributed textual cues, separated by long contexts and interspersed with irrelevant information. Our findings reveal that LLMs begin to exhibit memory drift and contextual forgetting at much shorter effective lengths when tasked with this form of relational reasoning, compared to what existing benchmarks suggest. With these findings, we offer recommendations for the optimal use of popular LLMs for complex reasoning tasks. We further show that even models specialized for reasoning, such as OpenAI o1, remain vulnerable to early memory drift in these settings. These results point to significant limitations in the models' ability to abstract structured knowledge from unstructured input and highlight the need for architectural adaptations to improve long-range reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä»æ–‡æœ¬ä¸­è¯±å¯¼å›¾(graph)ç­‰ç»“æ„åŒ–å…³ç³»çŸ¥è¯†æ—¶çš„æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦(context length)ä¸é—å¿˜å€¾å‘ã€‚ä½œè€…è®¤ä¸ºç°æœ‰çš„â€œå¤§æµ·æé’ˆâ€(needle in a haystack)ç­‰åŸºå‡†æµ‹è¯•è¿‡äºç®€å•ï¼Œæ— æ³•åæ˜ æ¨¡å‹åœ¨ä¿¡æ¯å¯†é›†åœºæ™¯ä¸‹çš„çœŸå®æ€§èƒ½ï¼Œå› æ­¤æå‡ºé€šè¿‡æ›´å¤æ‚çš„å›¾è¯±å¯¼ä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ä»»åŠ¡æ¶‰åŠä»æ•£å¸ƒåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„å™ªå£°ä¿¡æ¯é‡Œæå–å…³ç³»æ—¶ï¼ŒLLMså±•ç°å‡ºè®°å¿†åç§»(memory drift)å’Œä¸Šä¸‹æ–‡é—å¿˜çš„è½¬æŠ˜ç‚¹è¿œæ—©äºç°æœ‰åŸºå‡†çš„é¢„æµ‹ã€‚å³ä½¿æ˜¯ä¸“é—¨ä¸ºæ¨ç†è®¾è®¡çš„OpenAI o1æ¨¡å‹ï¼Œåœ¨è¿™äº›å¤æ‚è®¾ç½®ä¸‹ä¾ç„¶è¡¨ç°å‡ºæ—©æœŸçš„è®°å¿†è„†å¼±æ€§ã€‚è¿™äº›ç»“æœæ­ç¤ºäº†æ¨¡å‹åœ¨æŠ½è±¡ç»“æ„åŒ–çŸ¥è¯†æ–¹é¢çš„æ˜¾è‘—å±€é™ï¼Œå¹¶å¼ºè°ƒäº†é’ˆå¯¹é•¿ç¨‹æ¨ç†(long-range reasoning)è¿›è¡Œæ¶æ„æ”¹è¿›çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "2025 IEEE International Conference on Knowledge Graph (ICKG)",
      "pdf_url": "https://arxiv.org/pdf/2510.03611v1",
      "published_date": "2025-10-04 01:56:07 UTC",
      "updated_date": "2025-10-04 01:56:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:07.249811+00:00"
    },
    {
      "arxiv_id": "2510.03610v1",
      "title": "PentestMCP: A Toolkit for Agentic Penetration Testing",
      "title_zh": "PentestMCPï¼šæ™ºèƒ½ä½“æ¸—é€æµ‹è¯•å·¥å…·é›†",
      "authors": [
        "Zachary Ezetta",
        "Wu-chang Feng"
      ],
      "abstract": "Agentic AI is transforming security by automating many tasks being performed manually. While initial agentic approaches employed a monolithic architecture, the Model-Context-Protocol has now enabled a remote-procedure call (RPC) paradigm to agentic applications, allowing for the flexible construction and composition of multi-function agents. This paper describes PentestMCP, a library of MCP server implementations that support agentic penetration testing. By supporting common penetration testing tasks such as network scanning, resource enumeration, service fingerprinting, vulnerability scanning, exploitation, and post-exploitation, PentestMCP allows a developer to customize multi-agent workflows for performing penetration tests.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»‹ç»äº† PentestMCPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨æ”¯æŒæ™ºèƒ½ä½“æ¸—é€æµ‹è¯• (Agentic Penetration Testing) çš„ MCP (Model-Context-Protocol) æœåŠ¡å®ç°åº“ã€‚éšç€ Agentic AI é€æ¸å°†åŸæœ¬ç”±äººå·¥æ‰§è¡Œçš„å®‰å…¨ä»»åŠ¡è‡ªåŠ¨åŒ–ï¼ŒPentestMCP åˆ©ç”¨ MCP å¯ç”¨çš„è¿œç¨‹è¿‡ç¨‹è°ƒç”¨ (RPC) èŒƒå¼ï¼Œæ”¹å˜äº†ä¼ ç»Ÿçš„å•ä½“æ¶æ„ï¼Œå®ç°äº†å¤šåŠŸèƒ½æ™ºèƒ½ä½“çš„çµæ´»æ„å»ºä¸ç»„åˆã€‚è¯¥å·¥å…·åŒ…æ”¯æŒç½‘ç»œæ‰«æ (Network Scanning)ã€èµ„æºæšä¸¾ (Resource Enumeration)ã€æœåŠ¡æŒ‡çº¹è¯†åˆ« (Service Fingerprinting)ã€æ¼æ´æ‰«æ (Vulnerability Scanning)ã€åˆ©ç”¨ (Exploitation) ä»¥åŠåæ¸—é€ (Post-exploitation) ç­‰å…³é”®æ¸—é€æµ‹è¯•ä»»åŠ¡ã€‚é€šè¿‡é›†æˆè¿™äº›åŠŸèƒ½ï¼ŒPentestMCP å…è®¸å¼€å‘è€…è‡ªå®šä¹‰å¤šæ™ºèƒ½ä½“å·¥ä½œæµ (Multi-agent Workflows) ä»¥æ‰§è¡Œè‡ªåŠ¨åŒ–çš„æ¸—é€æµ‹è¯•ã€‚è¿™ä¸€å·¥å…·åº“çš„æå‡ºï¼Œä¸ºæ„å»ºæ›´å…·æ‰©å±•æ€§å’Œçµæ´»æ€§çš„è‡ªåŠ¨åŒ–å®‰å…¨æ£€æµ‹ç³»ç»Ÿæä¾›äº†æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03610v1",
      "published_date": "2025-10-04 01:55:05 UTC",
      "updated_date": "2025-10-04 01:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:09.744606+00:00"
    },
    {
      "arxiv_id": "2510.03605v1",
      "title": "Understanding the Role of Training Data in Test-Time Scaling",
      "title_zh": "ç†è§£è®­ç»ƒæ•°æ®åœ¨æ¨ç†æ—¶æ‰©å±•ä¸­çš„ä½œç”¨",
      "authors": [
        "Adel Javanmard",
        "Baharan Mirzasoleiman",
        "Vahab Mirrokni"
      ],
      "abstract": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åœ¨ Test-Time Scaling ä¸­è®­ç»ƒæ•°æ®æ‰€èµ·çš„ä½œç”¨ï¼Œæ—¨åœ¨æ­ç¤ºé•¿ Chain-of-Thought (CoT) åœ¨ä½•ç§è®­ç»ƒæ¡ä»¶ä¸‹äº§ç”Ÿä»¥åŠä½•æ—¶èƒ½æå‡æ€§èƒ½ã€‚ç ”ç©¶è€…é€šè¿‡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†çº¿æ€§å›å½’çš„ In-Context Weight Prediction ä»»åŠ¡ä¸Šè¿›è¡Œç†è®ºåˆ†æï¼Œæ­ç¤ºäº†è®¡ç®—é‡ä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å›ºå®šæµ‹è¯•è¯¯å·®çš„å‰æä¸‹ï¼Œå¢åŠ æµ‹è¯•æ—¶è®¡ç®—é‡ (Test-Time Compute) å¯ä»¥æœ‰æ•ˆå‡å°‘è®­ç»ƒæ‰€éœ€çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡ (Context Length)ã€‚ç„¶è€Œï¼Œå¦‚æœä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„æŠ€èƒ½åœ¨è®­ç»ƒæ•°æ®ä¸­å¹¶ä¸å……è¶³ï¼Œå¢åŠ æµ‹è¯•æ—¶è®¡ç®—é‡åè€Œå¯èƒ½å¯¹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶é€šè¿‡ç‰¹å¾åæ–¹å·®çŸ©é˜µçš„æœ€å°ç‰¹å¾å€¼å®šä¹‰äº†ä»»åŠ¡éš¾åº¦ï¼Œè¯æ˜åœ¨å¤šæ ·ã€ç›¸å…³ä¸”å›°éš¾çš„ä»»åŠ¡é›†ä¸Šè®­ç»ƒæœ€æœ‰åˆ©äº Test-Time Scaling çš„å‘æŒ¥ã€‚è¯¥ç»“è®ºåœ¨å¤§å‹éçº¿æ€§ Transformer æ¶æ„å®éªŒä¸­å¾—åˆ°äº†éªŒè¯ï¼Œä¸ºç†è§£å’Œä¼˜åŒ–æ¨ç†æ¨¡å‹æä¾›äº†é‡è¦çš„ç†è®ºæ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03605v1",
      "published_date": "2025-10-04 01:38:48 UTC",
      "updated_date": "2025-10-04 01:38:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:12.967042+00:00"
    },
    {
      "arxiv_id": "2510.03604v1",
      "title": "Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends",
      "title_zh": "é¢å‘æ¶¡æ‰‡å‘åŠ¨æœºå‰©ä½™ä½¿ç”¨å¯¿å‘½é¢„æµ‹çš„æ·±åº¦é¢†åŸŸè‡ªé€‚åº”ï¼šæ–¹æ³•è®ºã€è¯„ä¼°ä¸æœªæ¥è¶‹åŠ¿",
      "authors": [
        "Yucheng Wang",
        "Mohamed Ragab",
        "Yubo Hou",
        "Zhenghua Chen",
        "Min Wu",
        "Xiaoli Li"
      ],
      "abstract": "Remaining Useful Life (RUL) prediction for turbofan engines plays a vital role in predictive maintenance, ensuring operational safety and efficiency in aviation. Although data-driven approaches using machine learning and deep learning have shown potential, they face challenges such as limited data and distribution shifts caused by varying operating conditions. Domain Adaptation (DA) has emerged as a promising solution, enabling knowledge transfer from source domains with abundant data to target domains with scarce data while mitigating distributional shifts. Given the unique properties of turbofan engines, such as complex operating conditions, high-dimensional sensor data, and slower-changing signals, it is essential to conduct a focused review of DA techniques specifically tailored to turbofan engines. To address this need, this paper provides a comprehensive review of DA solutions for turbofan engine RUL prediction, analyzing key methodologies, challenges, and recent advancements. A novel taxonomy tailored to turbofan engines is introduced, organizing approaches into methodology-based (how DA is applied), alignment-based (where distributional shifts occur due to operational variations), and problem-based (why certain adaptations are needed to address specific challenges). This taxonomy offers a multidimensional view that goes beyond traditional classifications by accounting for the distinctive characteristics of turbofan engine data and the standard process of applying DA techniques to this area. Additionally, we evaluate selected DA techniques on turbofan engine datasets, providing practical insights for practitioners and identifying key challenges. Future research directions are identified to guide the development of more effective DA techniques, advancing the state of RUL prediction for turbofan engines.",
      "tldr_zh": "è¯¥è®ºæ–‡ç³»ç»Ÿå›é¡¾äº†æ·±åº¦é¢†åŸŸé€‚åº”(Domain Adaptation, DA)åœ¨æ¶¡è½®é£æ‰‡å‘åŠ¨æœºå‰©ä½™ä½¿ç”¨å¯¿å‘½(Remaining Useful Life, RUL)é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨è§£å†³èˆªç©ºç»´æŠ¤ä¸­ç”±äºå·¥å†µå˜åŒ–å¯¼è‡´çš„æ•°æ®åˆ†å¸ƒåç§»å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚é’ˆå¯¹æ¶¡è½®é£æ‰‡å‘åŠ¨æœºé«˜ç»´ä¼ æ„Ÿå™¨æ•°æ®å’Œå¤æ‚è¿è¡Œæ¡ä»¶çš„ç‰¹æ€§ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„åˆ†ç±»æ³•ï¼Œä»æ–¹æ³•è®º(Methodology-based)ã€å¯¹é½ç­–ç•¥(Alignment-based)å’Œå…·ä½“é—®é¢˜(Problem-based)ä¸‰ä¸ªç»´åº¦å¯¹DAæŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿç»„ç»‡ã€‚è¿™ç§å¤šç»´åº¦è§†è§’è¶…è¶Šäº†ä¼ ç»Ÿçš„åˆ†ç±»æ–¹å¼ï¼Œæ·±å…¥æ¢è®¨äº†DAæŠ€æœ¯å¦‚ä½•é’ˆå¯¹å‘åŠ¨æœºæ•°æ®çš„ç‹¬ç‰¹å±æ€§è¿›è¡ŒçŸ¥è¯†è¿ç§»ã€‚è®ºæ–‡è¿˜åœ¨ç‰¹å®šçš„æ¶¡è½®é£æ‰‡å‘åŠ¨æœºæ•°æ®é›†ä¸Šè¯„ä¼°äº†é€‰å®šçš„DAæŠ€æœ¯ï¼Œä¸ºä»ä¸šè€…æä¾›äº†å®é™…çš„åº”ç”¨è§è§£å¹¶æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ã€‚æœ€åï¼Œç ”ç©¶æŒ‡æ˜äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œä¸ºå¼€å‘æ›´é«˜æ•ˆçš„RULé¢„æµ‹DAæŠ€æœ¯ä»¥åŠæå‡èˆªç©ºç³»ç»Ÿè¿è¡Œçš„å®‰å…¨æ€§å’Œæ•ˆç‡æä¾›äº†è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03604v1",
      "published_date": "2025-10-04 01:37:40 UTC",
      "updated_date": "2025-10-04 01:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:16.648319+00:00"
    },
    {
      "arxiv_id": "2510.09643v1",
      "title": "Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations",
      "title_zh": "Direct Routing Gradient (DRGrad)ï¼šé¢å‘å¤šä»»åŠ¡å­¦ä¹  (MTL) æ¨èçš„ä¸ªæ€§åŒ–ä¿¡æ¯æ‰‹æœ¯",
      "authors": [
        "Yuguang Liu",
        "Yiyun Miao",
        "Luyao Xia"
      ],
      "abstract": "Multi-task learning (MTL) has emerged as a successful strategy in industrial-scale recommender systems, offering significant advantages such as capturing diverse users' interests and accurately detecting different behaviors like ``click\" or ``dwell time\". However, negative transfer and the seesaw phenomenon pose challenges to MTL models due to the complex and often contradictory task correlations in real-world recommendations. To address the problem while making better use of personalized information, we propose a personalized Direct Routing Gradient framework (DRGrad), which consists of three key components: router, updater and personalized gate network. DRGrad judges the stakes between tasks in the training process, which can leverage all valid gradients for the respective task to reduce conflicts. We evaluate the efficiency of DRGrad on complex MTL using a real-world recommendation dataset with 15 billion samples. The results show that DRGrad's superior performance over competing state-of-the-art MTL models, especially in terms of AUC (Area Under the Curve) metrics, indicating that it effectively manages task conflicts in multi-task learning environments without increasing model complexity, while also addressing the deficiencies in noise processing. Moreover, experiments on the public Census-income dataset and Synthetic dataset, have demonstrated the capability of DRGrad in judging and routing the stakes between tasks with varying degrees of correlation and personalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Direct Routing Gradient (DRGrad)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å·¥ä¸šçº§æ¨èç³»ç»Ÿä¸­å¤šä»»åŠ¡å­¦ä¹  (MTL) é¢ä¸´çš„ negative transfer å’Œ seesaw phenomenon ç­‰æŒ‘æˆ˜ã€‚DRGrad ç”± routerã€updater å’Œ personalized gate network ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ç»„æˆï¼Œé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ¤æ–­ä»»åŠ¡é—´çš„åˆ©å®³å…³ç³»ï¼Œæœ‰æ•ˆåˆ©ç”¨æ‰€æœ‰æœ‰æ•ˆæ¢¯åº¦ä»¥å‡å°‘ä»»åŠ¡å†²çªï¼Œå¹¶å……åˆ†åˆ©ç”¨ä¸ªæ€§åŒ–ä¿¡æ¯ã€‚è¯¥æ¡†æ¶åœ¨åŒ…å«150äº¿æ ·æœ¬çš„çœŸå®æ¨èæ•°æ®é›†ä»¥åŠ Census-income å’Œåˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRGrad åœ¨ AUC æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ SOTA æ¨¡å‹ï¼Œèƒ½åœ¨ä¸å¢åŠ æ¨¡å‹å¤æ‚åº¦çš„å‰æä¸‹ï¼Œæœ‰æ•ˆç®¡ç†å¤šä»»åŠ¡ç¯å¢ƒä¸‹çš„å†²çªå¹¶æ”¹è¿›å™ªå£°å¤„ç†ã€‚ç ”ç©¶è¯æ˜äº† DRGrad åœ¨å¤„ç†å…·æœ‰ä¸åŒç›¸å…³æ€§å’Œä¸ªæ€§åŒ–ç¨‹åº¦çš„ä»»åŠ¡æ—¶ï¼Œå…·æœ‰å“è¶Šçš„è·¯ç”±åˆ¤æ–­ä¸æ¢¯åº¦åè°ƒèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.09643v1",
      "published_date": "2025-10-04 01:28:32 UTC",
      "updated_date": "2025-10-04 01:28:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:17.554954+00:00"
    },
    {
      "arxiv_id": "2510.03597v3",
      "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation",
      "title_zh": "Neonï¼šé€šè¿‡è‡ªè®­ç»ƒè´Ÿå¤–æ¨æå‡å›¾åƒç”Ÿæˆ",
      "authors": [
        "Sina Alemohammad",
        "Zhangyang Wang",
        "Richard G. Baraniuk"
      ],
      "abstract": "Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights. We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1% additional training compute. We demonstrate Neon's universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional training compute. Code is available at https://github.com/VITA-Group/Neon",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼AIå› é«˜è´¨é‡è®­ç»ƒæ•°æ®çŸ­ç¼ºè€Œä¾èµ–åˆæˆæ•°æ®å¯¼è‡´çš„æ¨¡å‹å´©æºƒ(Model Autophagy Disorder, MAD)é—®é¢˜ï¼Œæå‡ºäº†åä¸º Neon (Negative Extrapolation frOm self-traiNing) çš„æ–°å‹å­¦ä¹ æ–¹æ³•ã€‚Neon é‡‡å–äº†ä¸€ç§åç›´è§‰çš„ç­–ç•¥ï¼Œå…ˆåœ¨æ¨¡å‹è‡ªåˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œéšåé€šè¿‡åè½¬æ¢¯åº¦æ›´æ–°æ–¹å‘å®ç° Negative Extrapolationï¼Œä»è€Œä½¿æ¨¡å‹æƒé‡è¿œç¦»ç”±äº Self-Training å¯¼è‡´çš„æ€§èƒ½é€€åŒ–åŒºåŸŸã€‚ç ”ç©¶è¯æ˜ï¼Œè¿™ç§è´Ÿå‘å¤–æ¨èƒ½å¤Ÿä¿®æ­£æ¨ç†é‡‡æ ·å™¨äº§ç”Ÿçš„é«˜æ¦‚ç‡åŒºåŸŸåå·®ï¼Œä½¿æ¨¡å‹æ¢¯åº¦ä¸çœŸå®æ•°æ®åˆ†å¸ƒé‡æ–°å¯¹é½ã€‚è¯¥æ–¹æ³•å®ç°æå…¶ç®€ä¾¿ï¼Œä»…éœ€æå°‘çš„åˆæˆæ ·æœ¬ä¸”è®¡ç®—å¼€é”€é€šå¸¸ä½äº 1%ï¼Œä¸”åœ¨ Diffusionã€Flow Matching å’Œ Autoregressive ç­‰å¤šç§æ¶æ„ä¸Šå…·æœ‰å¹¿æ³›çš„æ™®é€‚æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒNeon åœ¨ ImageNet 256x256 ä»»åŠ¡ä¸­åŠ©åŠ› xAR-L æ¨¡å‹è¾¾åˆ°äº† 1.02 çš„ FID æ–°çºªå½•ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆçš„è´¨é‡ä¸å¤šæ ·æ€§ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03597v3",
      "published_date": "2025-10-04 01:20:30 UTC",
      "updated_date": "2025-10-13 22:13:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:26.055712+00:00"
    },
    {
      "arxiv_id": "2510.06253v1",
      "title": "LLM-Driven Rubric-Based Assessment of Algebraic Competence in Multi-Stage Block Coding Tasks with Design and Field Evaluation",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œè¯„ä»·é‡è§„çš„å¤šé˜¶æ®µç§¯æœ¨å¼ç¼–ç¨‹ä»»åŠ¡ä»£æ•°èƒ½åŠ›æµ‹è¯„ï¼šè®¾è®¡ä¸å®åœ°è¯„ä¼°",
      "authors": [
        "Yong Oh Lee",
        "Byeonghun Bang",
        "Sejun Oh"
      ],
      "abstract": "As online education platforms continue to expand, there is a growing need for assessment methods that not only measure answer accuracy but also capture the depth of students' cognitive processes in alignment with curriculum objectives. This study proposes and evaluates a rubric-based assessment framework powered by a large language model (LLM) for measuring algebraic competence, real-world-context block coding tasks. The problem set, designed by mathematics education experts, aligns each problem segment with five predefined rubric dimensions, enabling the LLM to assess both correctness and quality of students' problem-solving processes. The system was implemented on an online platform that records all intermediate responses and employs the LLM for rubric-aligned achievement evaluation. To examine the practical effectiveness of the proposed framework, we conducted a field study involving 42 middle school students engaged in multi-stage quadratic equation tasks with block coding. The study integrated learner self-assessments and expert ratings to benchmark the system's outputs. The LLM-based rubric evaluation showed strong agreement with expert judgments and consistently produced rubric-aligned, process-oriented feedback. These results demonstrate both the validity and scalability of incorporating LLM-driven rubric assessment into online mathematics and STEM education platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„åŸºäºé‡è§„(Rubric-based)çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¡¡é‡å­¦ç”Ÿåœ¨å¤šé˜¶æ®µç§¯æœ¨å¼ç¼–ç¨‹(Block coding)ä»»åŠ¡ä¸­çš„ä»£æ•°èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç”±æ•°å­¦æ•™è‚²ä¸“å®¶è®¾è®¡ï¼Œå°†é—®é¢˜é›†çš„æ¯ä¸ªç¯èŠ‚ä¸äº”ä¸ªé¢„å®šä¹‰çš„é‡è§„ç»´åº¦ç›¸å¯¹é½ï¼Œä½¿LLMèƒ½å¤ŸåŒæ—¶è¯„ä¼°å­¦ç”Ÿè§£å†³é—®é¢˜è¿‡ç¨‹çš„æ­£ç¡®æ€§å’Œè´¨é‡ã€‚ç³»ç»Ÿåœ¨åœ¨çº¿å¹³å°ä¸Šå®ç°äº†å¯¹æ‰€æœ‰ä¸­é—´å“åº”çš„è®°å½•ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œç¬¦åˆé‡è§„çš„æˆå°±è¯„ä»·ã€‚é€šè¿‡å¯¹42åå‚ä¸äºŒæ¬¡æ–¹ç¨‹å¤šé˜¶æ®µç¼–ç¨‹ä»»åŠ¡çš„åˆä¸­ç”Ÿè¿›è¡Œå®åœ°ç ”ç©¶ï¼Œå¹¶ç»“åˆä¸“å®¶è¯„åˆ†è¿›è¡Œå¯¹æ¯”ï¼Œç»“æœæ˜¾ç¤ºè¯¥LLMè¯„ä¼°ç³»ç»Ÿä¸ä¸“å®¶åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚ç ”ç©¶ä¸ä»…å±•ç¤ºäº†ç³»ç»Ÿäº§ç”Ÿé¢å‘è¿‡ç¨‹åé¦ˆçš„èƒ½åŠ›ï¼Œæ›´è¯æ˜äº†å°†LLMé©±åŠ¨çš„é‡è§„è¯„ä¼°åº”ç”¨äºåœ¨çº¿æ•°å­¦å’ŒSTEMæ•™è‚²å¹³å°çš„æœ‰æ•ˆæ€§ä¸å¯æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.06253v1",
      "published_date": "2025-10-04 01:00:33 UTC",
      "updated_date": "2025-10-04 01:00:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:24.856533+00:00"
    },
    {
      "arxiv_id": "2510.03592v1",
      "title": "Deep Reinforcement Learning for Multi-Agent Coordination",
      "title_zh": "é¢å‘å¤šæ™ºèƒ½ä½“åä½œçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Kehinde O. Aina",
        "Sehoon Ha"
      ],
      "abstract": "We address the challenge of coordinating multiple robots in narrow and confined environments, where congestion and interference often hinder collective task performance. Drawing inspiration from insect colonies, which achieve robust coordination through stigmergy -- modifying and interpreting environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) framework that leverages virtual pheromones to model local and social interactions, enabling decentralized emergent coordination without explicit communication. To overcome the convergence and scalability limitations of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum learning, which decomposes complex tasks into progressively harder sub-problems. Simulation results show that our framework achieves the most effective coordination of up to eight agents, where robots self-organize into asymmetric workload distributions that reduce congestion and modulate group performance. This emergent behavior, analogous to strategies observed in nature, demonstrates a scalable solution for decentralized multi-agent coordination in crowded environments with communication constraints.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæœºå™¨äººåœ¨ç‹­çª„å—é™ç¯å¢ƒä¸­çš„åè°ƒæŒ‘æˆ˜ï¼Œè§£å†³äº†å› æ‹¥å µå’Œå¹²æ‰°å¦¨ç¢é›†ä½“ä»»åŠ¡æ€§èƒ½çš„é—®é¢˜ã€‚å—æ˜†è™«ç¾¤ä½“å¯å‘ï¼Œä½œè€…æå‡ºäº† Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL) æ¡†æ¶ï¼Œåˆ©ç”¨è™šæ‹Ÿä¿¡æ¯ç´  (virtual pheromones) æ¥æ¨¡æ‹Ÿå±€éƒ¨å’Œç¤¾äº¤äº’åŠ¨ï¼Œå®ç°äº†æ— éœ€æ˜¾å¼é€šä¿¡çš„å»ä¸­å¿ƒåŒ–æ¶Œç°åè°ƒã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿ MADQNã€MADDPG å’Œ MAPPO ç®—æ³•åœ¨æ”¶æ•›æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å±€é™ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†è¯¾ç¨‹å­¦ä¹  (curriculum learning) æŠ€æœ¯ï¼Œå°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºæ¸è¿›å¼çš„å­é—®é¢˜ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒS-MADRL åœ¨å¤šè¾¾å…«ä¸ªæ™ºèƒ½ä½“çš„åœºæ™¯ä¸­å®ç°äº†æœ€é«˜æ•ˆçš„åè°ƒï¼Œæœºå™¨äººèƒ½è‡ªç»„ç»‡å½¢æˆéå¯¹ç§°çš„å·¥ä½œè´Ÿè½½åˆ†å¸ƒï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ‹¥å µã€‚è¿™ç§ç±»ä¼¼äºè‡ªç„¶ç•Œçš„æ¶Œç°è¡Œä¸ºï¼Œä¸ºé€šä¿¡å—é™çš„æ‹¥æŒ¤ç¯å¢ƒä¸‹å®ç°å¯æ‰©å±•çš„å»ä¸­å¿ƒåŒ–å¤šæ™ºèƒ½ä½“åè°ƒæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 8 figures, 1 table, presented at SWARM 2022, to be published in Journal of Artificial Life and Robotics",
      "pdf_url": "https://arxiv.org/pdf/2510.03592v1",
      "published_date": "2025-10-04 00:47:20 UTC",
      "updated_date": "2025-10-04 00:47:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:35.851846+00:00"
    },
    {
      "arxiv_id": "2510.03591v1",
      "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games",
      "title_zh": "ç”µå­æ¸¸æˆè§†è§‰ç¼ºé™·æ£€æµ‹çš„æ··åˆååŒå¾®è°ƒæ–¹æ³•",
      "authors": [
        "Faliu Yi",
        "Sherif Abdelfattah",
        "Wei Huang",
        "Adrian Brown"
      ],
      "abstract": "Manual identification of visual bugs in video games is a resource-intensive and costly process, often demanding specialized domain knowledge. While supervised visual bug detection models offer a promising solution, their reliance on extensive labeled datasets presents a significant challenge due to the infrequent occurrence of such bugs. To overcome this limitation, we propose a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled and unlabeled data. Our approach leverages labeled samples from the target game and diverse co-domain games, additionally incorporating unlabeled data to enhance feature representation learning. This strategy maximizes the utility of all available data, substantially reducing the dependency on labeled examples from the specific target game. The developed framework demonstrates enhanced scalability and adaptability, facilitating efficient visual bug detection across various game titles. Our experimental results show the robustness of the proposed method for game visual bug detection, exhibiting superior performance compared to conventional baselines across multiple gaming environments. Furthermore, CFT maintains competitive performance even when trained with only 50% of the labeled data from the target game.",
      "tldr_zh": "é’ˆå¯¹ç”µå­æ¸¸æˆè§†è§‰ç¼ºé™·æ£€æµ‹(Visual Bug Detection)ä¸­äººå·¥è¯†åˆ«æˆæœ¬é«˜ä»¥åŠç›‘ç£å­¦ä¹ æ¨¡å‹é«˜åº¦ä¾èµ–å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCo-FineTuning (CFT)çš„æ··åˆååŒå¾®è°ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ‰æ•ˆæ•´åˆç›®æ ‡æ¸¸æˆåŠåŒé¢†åŸŸæ¸¸æˆçš„æ ‡æ³¨æ ·æœ¬ï¼Œå¹¶ç»“åˆæ— æ ‡æ³¨æ•°æ®æ¥å¢å¼ºç‰¹å¾è¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ã€‚è¿™ç§ç­–ç•¥æ˜¾è‘—é™ä½äº†å¯¹ç‰¹å®šç›®æ ‡æ¸¸æˆæ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œæå¤§æå‡äº†æ¡†æ¶åœ¨ä¸åŒæ¸¸æˆä½œå“ä¸­çš„å¯æ‰©å±•æ€§ä¸é€‚åº”æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCFTæ–¹æ³•åœ¨å¤šä¸ªæ¸¸æˆç¯å¢ƒä¸‹çš„è¡¨ç°å‡ä¼˜äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºæå¼ºçš„é²æ£’æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ä»…ä½¿ç”¨ç›®æ ‡æ¸¸æˆ50%æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥æ¡†æ¶ä¾ç„¶èƒ½å¤Ÿä¿æŒæå…·ç«äº‰åŠ›çš„æ£€æµ‹æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨åŒ–æ¸¸æˆæµ‹è¯•æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03591v1",
      "published_date": "2025-10-04 00:43:10 UTC",
      "updated_date": "2025-10-04 00:43:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:43.157700+00:00"
    },
    {
      "arxiv_id": "2510.03582v1",
      "title": "Deep learning the sources of MJO predictability: a spectral view of learned features",
      "title_zh": "æ·±åº¦å­¦ä¹ MJOå¯é¢„æŠ¥æ€§çš„æ¥æºï¼šå­¦ä¹ ç‰¹å¾çš„é¢‘è°±è§†è§’",
      "authors": [
        "Lin Yao",
        "Da Yang",
        "James P. C. Duncan",
        "Ashesh Chattopadhyay",
        "Pedram Hassanzadeh",
        "Wahid Bhimji",
        "Bin Yu"
      ],
      "abstract": "The Madden-Julian oscillation (MJO) is a planetary-scale, intraseasonal tropical rainfall phenomenon crucial for global weather and climate; however, its dynamics and predictability remain poorly understood. Here, we leverage deep learning (DL) to investigate the sources of MJO predictability, motivated by a central difference in MJO theories: which spatial scales are essential for driving the MJO? We first develop a deep convolutional neural network (DCNN) to forecast the MJO indices (RMM and ROMI). Our model predicts RMM and ROMI up to 21 and 33 days, respectively, achieving skills comparable to leading subseasonal-to-seasonal models such as NCEP. To identify the spatial scales most relevant for MJO forecasting, we conduct spectral analysis of the latent feature space and find that large-scale patterns dominate the learned signals. Additional experiments show that models using only large-scale signals as the input have the same skills as those using all the scales, supporting the large-scale view of the MJO. Meanwhile, we find that small-scale signals remain informative: surprisingly, models using only small-scale input can still produce skillful forecasts up to 1-2 weeks ahead. We show that this is achieved by reconstructing the large-scale envelope of the small-scale activities, which aligns with the multi-scale view of the MJO. Altogether, our findings support that large-scale patterns--whether directly included or reconstructed--may be the primary source of MJO predictability.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ (Deep Learning)æ¢è®¨é©¬ç™»-æœ±åˆ©å®‰æŒ¯è¡(Madden-Julian oscillation, MJO)çš„å¯é¢„æŠ¥æ€§æ¥æºï¼Œé‡ç‚¹åˆ†æé©±åŠ¨MJOçš„å…³é”®ç©ºé—´å°ºåº¦ã€‚ç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ(DCNN)æ¥é¢„æµ‹RMMå’ŒROMIæŒ‡æ•°ï¼Œå…¶é¢„æŠ¥æ—¶æ•ˆåˆ†åˆ«å¯è¾¾21å¤©å’Œ33å¤©ï¼Œè¡¨ç°ä¸NCEPç­‰é¢†å…ˆçš„äºšå­£èŠ‚è‡³å­£èŠ‚æ¨¡å‹ç›¸å½“ã€‚ä¸ºäº†è¯†åˆ«é¢„æŠ¥ä¸­æœ€ç›¸å…³çš„ç©ºé—´å°ºåº¦ï¼Œç ”ç©¶å¯¹æ½œåœ¨ç‰¹å¾ç©ºé—´(latent feature space)è¿›è¡Œäº†é¢‘è°±åˆ†æ(spectral analysis)ï¼Œå‘ç°å¤§å°ºåº¦æ¨¡å¼(large-scale patterns)åœ¨æ¨¡å‹å­¦ä¹ åˆ°çš„ä¿¡å·ä¸­å ä¸»å¯¼åœ°ä½ã€‚å®éªŒè¯æ˜ï¼Œä»…ä½¿ç”¨å¤§å°ºåº¦ä¿¡å·ä½œä¸ºè¾“å…¥çš„æ¨¡å‹ä¸ä½¿ç”¨å…¨å°ºåº¦è¾“å…¥çš„æ¨¡å‹å…·æœ‰ç›¸åŒçš„é¢„æŠ¥æŠ€å·§ï¼Œæ”¯æŒäº†å¤§å°ºåº¦é©±åŠ¨MJOçš„è§‚ç‚¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å°å°ºåº¦ä¿¡å·(small-scale signals)åŒæ ·åŒ…å«æœ‰ç”¨ä¿¡æ¯ï¼Œä»…å‡­æ­¤ç±»è¾“å…¥ä»èƒ½å®ç°1-2å‘¨çš„é¢„æŠ¥ï¼Œè¿™æ˜¯é€šè¿‡é‡å»ºå°å°ºåº¦æ´»åŠ¨çš„å¤§å°ºåº¦åŒ…ç»œçº¿(large-scale envelope)å®ç°çš„ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¤§å°ºåº¦æ¨¡å¼æ— è®ºæ˜¯è¢«ç›´æ¥æ•è·è¿˜æ˜¯è¢«é‡æ„ï¼Œéƒ½æ˜¯MJOå¯é¢„æŠ¥æ€§çš„ä¸»è¦æ¥æºï¼Œä¸ºç†è§£MJOçš„å¤šå°ºåº¦è§‚ç‚¹(multi-scale view)æä¾›äº†æ–°è§†è§’ã€‚",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03582v1",
      "published_date": "2025-10-04 00:15:45 UTC",
      "updated_date": "2025-10-04 00:15:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:49:46.246902+00:00"
    },
    {
      "arxiv_id": "2510.03578v1",
      "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning",
      "title_zh": "é¢å‘é«˜æ ·æœ¬æ•ˆç‡åŠ¨åŠ›å­¦å­¦ä¹ çš„æ½œå¯¹ç§°æ··åˆæ¨¡å‹",
      "authors": [
        "Haoran Li",
        "Chenhan Xiao",
        "Muhao Guo",
        "Yang Weng"
      ],
      "abstract": "Learning dynamics is essential for model-based control and Reinforcement Learning in engineering systems, such as robotics and power systems. However, limited system measurements, such as those from low-resolution sensors, demand sample-efficient learning. Symmetry provides a powerful inductive bias by characterizing equivariant relations in system states to improve sample efficiency. While recent methods attempt to discover symmetries from data, they typically assume a single global symmetry group and treat symmetry discovery and dynamic learning as separate tasks, leading to limited expressiveness and error accumulation. In this paper, we propose the Latent Mixture of Symmetries (Latent MoS), an expressive model that captures a mixture of symmetry-governed latent factors from complex dynamical measurements. Latent MoS focuses on dynamic learning while locally and provably preserving the underlying symmetric transformations. To further capture long-term equivariance, we introduce a hierarchical architecture that stacks MoS blocks. Numerical experiments in diverse physical systems demonstrate that Latent MoS outperforms state-of-the-art baselines in interpolation and extrapolation tasks while offering interpretable latent representations suitable for future geometric and safety-critical analyses.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººå’Œç”µåŠ›ç³»ç»Ÿç­‰å·¥ç¨‹é¢†åŸŸä¸­åŠ¨æ€å­¦ä¹ (Dynamic Learning)æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰å¯¹ç§°æ€§å‘ç°æ–¹æ³•å¾€å¾€å‡è®¾å•ä¸€å…¨å±€å¯¹ç§°ç¾¤ä¸”ä¸åŠ¨æ€å­¦ä¹ ä»»åŠ¡è„±èŠ‚çš„å±€é™ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Latent Mixture of Symmetries (Latent MoS)ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿä»å¤æ‚åŠ¨æ€æµ‹é‡ä¸­æ•æ‰å—å¯¹ç§°æ€§æ”¯é…çš„æ··åˆæ½œåœ¨å› å­çš„è¡¨è¾¾æ¨¡å‹ã€‚Latent MoS ä¸“æ³¨äºåŠ¨æ€å­¦ä¹ ï¼ŒåŒæ—¶åœ¨å±€éƒ¨ä¸”å¯è¯æ˜åœ°ä¿ç•™äº†åº•å±‚çš„å¯¹ç§°å˜æ¢(Symmetric Transformations)ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ•æ‰é•¿æœŸç­‰å˜æ€§(Equivariance)ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§å †å  MoS æ¨¡å—çš„åˆ†å±‚æ¶æ„(Hierarchical Architecture)ã€‚åœ¨å¤šç§ç‰©ç†ç³»ç»Ÿä¸Šçš„æ•°å€¼å®éªŒè¡¨æ˜ï¼ŒLatent MoS åœ¨æ’å€¼(Interpolation)å’Œå¤–æ¨(Extrapolation)ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…æé«˜äº†æ ·æœ¬æ•ˆç‡ï¼Œè¿˜æä¾›äº†å¯è§£é‡Šçš„æ½œåœ¨è¡¨ç¤º(Latent Representations)ï¼Œä¸ºæœªæ¥çš„å‡ ä½•å’Œå®‰å…¨æ€§å…³é”®åˆ†æå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03578v1",
      "published_date": "2025-10-04 00:06:31 UTC",
      "updated_date": "2025-10-04 00:06:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-25T00:50:17.952736+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 84,
  "processed_papers_count": 84,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-25T00:51:10.917730+00:00"
}
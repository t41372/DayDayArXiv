[
  {
    "arxiv_id": "2405.17708v2",
    "title": "OPERA: Automatic Offline Policy Evaluation with Re-weighted Aggregates of Multiple Estimators",
    "authors": [
      "Allen Nie",
      "Yash Chandak",
      "Christina J. Yuan",
      "Anirudhan Badrinath",
      "Yannis Flet-Berliac",
      "Emma Brunskil"
    ],
    "abstract": "Offline policy evaluation (OPE) allows us to evaluate and estimate a new\nsequential decision-making policy's performance by leveraging historical\ninteraction data collected from other policies. Evaluating a new policy online\nwithout a confident estimate of its performance can lead to costly, unsafe, or\nhazardous outcomes, especially in education and healthcare. Several OPE\nestimators have been proposed in the last decade, many of which have\nhyperparameters and require training. Unfortunately, choosing the best OPE\nalgorithm for each task and domain is still unclear. In this paper, we propose\na new algorithm that adaptively blends a set of OPE estimators given a dataset\nwithout relying on an explicit selection using a statistical procedure. We\nprove that our estimator is consistent and satisfies several desirable\nproperties for policy evaluation. Additionally, we demonstrate that when\ncompared to alternative approaches, our estimator can be used to select\nhigher-performing policies in healthcare and robotics. Our work contributes to\nimproving ease of use for a general-purpose, estimator-agnostic, off-policy\nevaluation framework for offline RL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.17708v2",
    "published_date": "2024-05-27 23:51:20 UTC",
    "updated_date": "2024-10-31 23:40:16 UTC"
  },
  {
    "arxiv_id": "2405.17706v1",
    "title": "Video Enriched Retrieval Augmented Generation Using Aligned Video Captions",
    "authors": [
      "Kevin Dela Rosa"
    ],
    "abstract": "In this work, we propose the use of \"aligned visual captions\" as a mechanism\nfor integrating information contained within videos into retrieval augmented\ngeneration (RAG) based chat assistant systems. These captions are able to\ndescribe the visual and audio content of videos in a large corpus while having\nthe advantage of being in a textual format that is both easy to reason about &\nincorporate into large language model (LLM) prompts, but also typically require\nless multimedia content to be inserted into the multimodal LLM context window,\nwhere typical configurations can aggressively fill up the context window by\nsampling video frames from the source video. Furthermore, visual captions can\nbe adapted to specific use cases by prompting the original foundational model /\ncaptioner for particular visual details or fine tuning. In hopes of helping\nadvancing progress in this area, we curate a dataset and describe automatic\nevaluation procedures on common RAG tasks.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "SIGIR 2024 Workshop on Multimodal Representation and Retrieval (MRR\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.17706v1",
    "published_date": "2024-05-27 23:39:17 UTC",
    "updated_date": "2024-05-27 23:39:17 UTC"
  },
  {
    "arxiv_id": "2406.07568v1",
    "title": "Reinforcement Learning Based Escape Route Generation in Low Visibility Environments",
    "authors": [
      "Hari Srikanth"
    ],
    "abstract": "Structure fires are responsible for the majority of fire-related deaths\nnationwide. In order to assist with the rapid evacuation of trapped people,\nthis paper proposes the use of a system that determines optimal search paths\nfor firefighters and exit paths for civilians in real time based on\nenvironmental measurements. Through the use of a LiDAR mapping system evaluated\nand verified by a trust range derived from sonar and smoke concentration data,\na proposed solution to low visibility mapping is tested. These independent\npoint clouds are then used to create distinct maps, which are merged through\nthe use of a RANSAC based alignment methodology and simplified into a\nvisibility graph. Temperature and humidity data are then used to label each\nnode with a danger score, creating an environment tensor. After demonstrating\nhow a Linear Function Approximation based Natural Policy Gradient RL\nmethodology outperforms more complex competitors with respect to robustness and\nspeed, this paper outlines two systems (savior and refugee) that process the\nenvironment tensor to create safe rescue and escape routes, respectively.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.07568v1",
    "published_date": "2024-05-27 23:00:57 UTC",
    "updated_date": "2024-05-27 23:00:57 UTC"
  },
  {
    "arxiv_id": "2405.17691v1",
    "title": "Ontology-Enhanced Decision-Making for Autonomous Agents in Dynamic and Partially Observable Environments",
    "authors": [
      "Saeedeh Ghanadbashi",
      "Fatemeh Golpayegani"
    ],
    "abstract": "Agents, whether software or hardware, perceive their environment through\nsensors and act using actuators, often operating in dynamic, partially\nobservable settings. They face challenges like incomplete and noisy data,\nunforeseen situations, and the need to adapt goals in real-time. Traditional\nreasoning and ML methods, including Reinforcement Learning (RL), help but are\nlimited by data needs, predefined goals, and extensive exploration periods.\nOntologies offer a solution by integrating diverse information sources,\nenhancing decision-making in complex environments. This thesis introduces an\nontology-enhanced decision-making model (OntoDeM) for autonomous agents.\nOntoDeM enriches agents' domain knowledge, allowing them to interpret\nunforeseen events, generate or adapt goals, and make better decisions. Key\ncontributions include: 1. An ontology-based method to improve agents' real-time\nobservations using prior knowledge. 2. The OntoDeM model for handling dynamic,\nunforeseen situations by evolving or generating new goals. 3. Implementation\nand evaluation in four real-world applications, demonstrating its\neffectiveness. Compared to traditional and advanced learning algorithms,\nOntoDeM shows superior performance in improving agents' observations and\ndecision-making in dynamic, partially observable environments.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2405.17691v1",
    "published_date": "2024-05-27 22:52:23 UTC",
    "updated_date": "2024-05-27 22:52:23 UTC"
  },
  {
    "arxiv_id": "2405.17678v1",
    "title": "TIMA: Text-Image Mutual Awareness for Balancing Zero-Shot Adversarial Robustness and Generalization Ability",
    "authors": [
      "Fengji Ma",
      "Li Liu",
      "Hei Victor Cheng"
    ],
    "abstract": "This work addresses the challenge of achieving zero-shot adversarial\nrobustness while preserving zero-shot generalization in large-scale foundation\nmodels, with a focus on the popular Contrastive Language-Image Pre-training\n(CLIP). Although foundation models were reported to have exceptional zero-shot\ngeneralization, they are highly vulnerable to adversarial perturbations.\nExisting methods achieve a comparable good tradeoff between zero-shot\nadversarial robustness and generalization under small adversarial\nperturbations. However, they fail to achieve a good tradeoff under large\nadversarial perturbations. To this end, we propose a novel Text-Image Mutual\nAwareness (TIMA) method that strikes a balance between zero-shot adversarial\nrobustness and generalization. More precisely, we propose an Image-Aware Text\n(IAT) tuning mechanism that increases the inter-class distance of text\nembeddings by incorporating the Minimum Hyperspherical Energy (MHE).\nSimultaneously, fixed pre-trained image embeddings are used as cross-modal\nauxiliary supervision to maintain the similarity between the MHE-tuned and\noriginal text embeddings by the knowledge distillation, preserving semantic\ninformation between different classes. Besides, we introduce a Text-Aware Image\n(TAI) tuning mechanism, which increases inter-class distance between image\nembeddings during the training stage by Text-distance based Adaptive Margin\n(TAM). Similarly, a knowledge distillation is utilized to retain the similarity\nbetween fine-tuned and pre-trained image embeddings. Extensive experimental\nresults demonstrate the effectiveness of our approach, showing impressive\nzero-shot performance against a wide range of adversarial perturbations while\npreserving the zero-shot generalization capabilities of the original CLIP\nmodel.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17678v1",
    "published_date": "2024-05-27 22:10:17 UTC",
    "updated_date": "2024-05-27 22:10:17 UTC"
  },
  {
    "arxiv_id": "2405.17676v1",
    "title": "Utilising a Quantum Hybrid Solver for Bi-objective Quadratic Assignment Problems",
    "authors": [
      "Mayowa Ayodele"
    ],
    "abstract": "The intersection between quantum computing and optimisation has been an area\nof interest in recent years. There have been numerous studies exploring the\napplication of quantum and quantum-hybrid solvers to various optimisation\nproblems. This work explores scalarisation methods within the context of\nsolving the bi-objective quadratic assignment problem using a quantum-hybrid\nsolver. We show results that are consistent with previous research on a\ndifferent Ising machine.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "G.1.6"
    ],
    "primary_category": "quant-ph",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.17676v1",
    "published_date": "2024-05-27 22:03:26 UTC",
    "updated_date": "2024-05-27 22:03:26 UTC"
  },
  {
    "arxiv_id": "2405.17672v1",
    "title": "Exploring Loss Design Techniques For Decision Tree Robustness To Label Noise",
    "authors": [
      "Lukasz Sztukiewicz",
      "Jack Henry Good",
      "Artur Dubrawski"
    ],
    "abstract": "In the real world, data is often noisy, affecting not only the quality of\nfeatures but also the accuracy of labels. Current research on mitigating label\nerrors stems primarily from advances in deep learning, and a gap exists in\nexploring interpretable models, particularly those rooted in decision trees. In\nthis study, we investigate whether ideas from deep learning loss design can be\napplied to improve the robustness of decision trees. In particular, we show\nthat loss correction and symmetric losses, both standard approaches, are not\neffective. We argue that other directions need to be explored to improve the\nrobustness of decision trees to label noise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17672v1",
    "published_date": "2024-05-27 21:49:57 UTC",
    "updated_date": "2024-05-27 21:49:57 UTC"
  },
  {
    "arxiv_id": "2406.07567v1",
    "title": "A GRASP-based memetic algorithm with path relinking for the far from most string problem",
    "authors": [
      "José E. Gallardo",
      "Carlos Cotta"
    ],
    "abstract": "The FAR FROM MOST STRING PROBLEM (FFMSP) is a string selection problem. The\nobjective is to find a string whose distance to other strings in a certain\ninput set is above a given threshold for as many of those strings as possible.\nThis problem has links with some tasks in computational biology and its\nresolution has been shown to be very hard. We propose a memetic algorithm (MA)\nto tackle the FFMSP. This MA exploits a heuristic objective function for the\nproblem and features initialization of the population via a Greedy Randomized\nAdaptive Search Procedure (GRASP) metaheuristic, intensive recombination via\npath relinking and local improvement via hill climbing. An extensive empirical\nevaluation using problem instances of both random and biological origin is done\nto assess parameter sensitivity and draw performance comparisons with other\nstate-of-the-art techniques. The MA is shown to perform better than these\nlatter techniques with statistical significance.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.07567v1",
    "published_date": "2024-05-27 21:33:15 UTC",
    "updated_date": "2024-05-27 21:33:15 UTC"
  },
  {
    "arxiv_id": "2405.17653v4",
    "title": "InversionView: A General-Purpose Method for Reading Information from Neural Activations",
    "authors": [
      "Xinting Huang",
      "Madhur Panwar",
      "Navin Goyal",
      "Michael Hahn"
    ],
    "abstract": "The inner workings of neural networks can be better understood if we can\nfully decipher the information encoded in neural activations. In this paper, we\nargue that this information is embodied by the subset of inputs that give rise\nto similar activations. We propose InversionView, which allows us to\npractically inspect this subset by sampling from a trained decoder model\nconditioned on activations. This helps uncover the information content of\nactivation vectors, and facilitates understanding of the algorithms implemented\nby transformer models. We present four case studies where we investigate models\nranging from small transformers to GPT-2. In these studies, we show that\nInversionView can reveal clear information contained in activations, including\nbasic information about tokens appearing in the context, as well as more\ncomplex information, such as the count of certain tokens, their relative\npositions, and abstract knowledge about the subject. We also provide causally\nverified circuits to confirm the decoded information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024; ICML 2024 Mechanistic Interpretability Workshop oral",
    "pdf_url": "http://arxiv.org/pdf/2405.17653v4",
    "published_date": "2024-05-27 20:53:22 UTC",
    "updated_date": "2024-11-02 19:13:06 UTC"
  },
  {
    "arxiv_id": "2405.17642v1",
    "title": "Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels",
    "authors": [
      "Patryk Wielopolski",
      "Oleksii Furman",
      "Jerzy Stefanowski",
      "Maciej Zięba"
    ],
    "abstract": "Growing regulatory and societal pressures demand increased transparency in\nAI, particularly in understanding the decisions made by complex machine\nlearning models. Counterfactual Explanations (CFs) have emerged as a promising\ntechnique within Explainable AI (xAI), offering insights into individual model\npredictions. However, to understand the systemic biases and disparate impacts\nof AI models, it is crucial to move beyond local CFs and embrace global\nexplanations, which offer a~holistic view across diverse scenarios and\npopulations. Unfortunately, generating Global Counterfactual Explanations\n(GCEs) faces challenges in computational complexity, defining the scope of\n\"global,\" and ensuring the explanations are both globally representative and\nlocally plausible. We introduce a novel unified approach for generating Local,\nGroup-wise, and Global Counterfactual Explanations for differentiable\nclassification models via gradient-based optimization to address these\nchallenges. This framework aims to bridge the gap between individual and\nsystemic insights, enabling a deeper understanding of model decisions and their\npotential impact on diverse populations. Our approach further innovates by\nincorporating a probabilistic plausibility criterion, enhancing actionability\nand trustworthiness. By offering a cohesive solution to the optimization and\nplausibility challenges in GCEs, our work significantly advances the\ninterpretability and accountability of AI models, marking a step forward in the\npursuit of transparent AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17642v1",
    "published_date": "2024-05-27 20:32:09 UTC",
    "updated_date": "2024-05-27 20:32:09 UTC"
  },
  {
    "arxiv_id": "2405.17640v2",
    "title": "Probabilistically Plausible Counterfactual Explanations with Normalizing Flows",
    "authors": [
      "Patryk Wielopolski",
      "Oleksii Furman",
      "Jerzy Stefanowski",
      "Maciej Zięba"
    ],
    "abstract": "We present PPCEF, a novel method for generating probabilistically plausible\ncounterfactual explanations (CFs). PPCEF advances beyond existing methods by\ncombining a probabilistic formulation that leverages the data distribution with\nthe optimization of plausibility within a unified framework. Compared to\nreference approaches, our method enforces plausibility by directly optimizing\nthe explicit density function without assuming a particular family of\nparametrized distributions. This ensures CFs are not only valid (i.e., achieve\nclass change) but also align with the underlying data's probability density.\nFor that purpose, our approach leverages normalizing flows as powerful density\nestimators to capture the complex high-dimensional data distribution.\nFurthermore, we introduce a novel loss that balances the trade-off between\nachieving class change and maintaining closeness to the original instance while\nalso incorporating a probabilistic plausibility term. PPCEF's unconstrained\nformulation allows for efficient gradient-based optimization with batch\nprocessing, leading to orders of magnitude faster computation compared to prior\nmethods. Moreover, the unconstrained formulation of PPCEF allows for the\nseamless integration of future constraints tailored to specific counterfactual\nproperties. Finally, extensive evaluations demonstrate PPCEF's superiority in\ngenerating high-quality, probabilistically plausible counterfactual\nexplanations in high-dimensional tabular settings. This makes PPCEF a powerful\ntool for not only interpreting complex machine learning models but also for\nimproving fairness, accountability, and trust in AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17640v2",
    "published_date": "2024-05-27 20:24:03 UTC",
    "updated_date": "2024-08-07 07:29:39 UTC"
  },
  {
    "arxiv_id": "2405.17638v3",
    "title": "The surprising efficiency of temporal difference learning for rare event prediction",
    "authors": [
      "Xiaoou Cheng",
      "Jonathan Weare"
    ],
    "abstract": "We quantify the efficiency of temporal difference (TD) learning over the\ndirect, or Monte Carlo (MC), estimator for policy evaluation in reinforcement\nlearning, with an emphasis on estimation of quantities related to rare events.\nPolicy evaluation is complicated in the rare event setting by the long\ntimescale of the event and by the need for \\emph{relative accuracy} in\nestimates of very small values. Specifically, we focus on least-squares TD\n(LSTD) prediction for finite state Markov chains, and show that LSTD can\nachieve relative accuracy far more efficiently than MC. We prove a central\nlimit theorem for the LSTD estimator and upper bound the \\emph{relative\nasymptotic variance} by simple quantities characterizing the connectivity of\nstates relative to the transition probabilities between them. Using this bound,\nwe show that, even when both the timescale of the rare event and the relative\naccuracy of the MC estimator are exponentially large in the number of states,\nLSTD maintains a fixed level of relative accuracy with a total number of\nobserved transitions of the Markov chain that is only \\emph{polynomially} large\nin the number of states.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Final camera-ready version published at NeurIPS 2024. Correct an\n  assumption statement and typos, and change/add a few sentences from the last\n  version",
    "pdf_url": "http://arxiv.org/pdf/2405.17638v3",
    "published_date": "2024-05-27 20:18:20 UTC",
    "updated_date": "2025-01-16 04:11:29 UTC"
  },
  {
    "arxiv_id": "2405.17637v1",
    "title": "The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model",
    "authors": [
      "Geraldo Xexéo",
      "Filipe Braida",
      "Marcus Parreiras",
      "Paulo Xavier"
    ],
    "abstract": "Selecting language models in business contexts requires a careful analysis of\nthe final financial benefits of the investment. However, the emphasis of\nacademia and industry analysis of LLM is solely on performance. This work\nintroduces a framework to evaluate LLMs, focusing on the earnings and return on\ninvestment aspects that should be taken into account in business decision\nmaking. We use a decision-theoretic approach to compare the financial impact of\ndifferent LLMs, considering variables such as the cost per token, the\nprobability of success in the specific task, and the gain and losses associated\nwith LLMs use. The study reveals how the superior accuracy of more expensive\nmodels can, under certain conditions, justify a greater investment through more\nsignificant earnings but not necessarily a larger RoI. This article provides a\nframework for companies looking to optimize their technology choices, ensuring\nthat investment in cutting-edge technology aligns with strategic financial\nobjectives. In addition, we discuss how changes in operational variables\ninfluence the economics of using LLMs, offering practical insights for\nenterprise settings, finding that the predicted gain and loss and the different\nprobabilities of success and failure are the variables that most impact the\nsensitivity of the models.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "I.2.m; K.6.1"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17637v1",
    "published_date": "2024-05-27 20:08:41 UTC",
    "updated_date": "2024-05-27 20:08:41 UTC"
  },
  {
    "arxiv_id": "2405.17631v3",
    "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
    "authors": [
      "Yusuf Roohani",
      "Andrew Lee",
      "Qian Huang",
      "Jian Vora",
      "Zachary Steinhart",
      "Kexin Huang",
      "Alexander Marson",
      "Percy Liang",
      "Jure Leskovec"
    ],
    "abstract": "Agents based on large language models have shown great potential in\naccelerating scientific discovery by leveraging their rich background knowledge\nand reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an\nagent that designs new experiments, reasons about their outcomes, and\nefficiently navigates the hypothesis space to reach desired solutions. We\ndemonstrate our agent on the problem of designing genetic perturbation\nexperiments, where the aim is to find a small subset out of many possible genes\nthat, when perturbed, result in a specific phenotype (e.g., cell growth).\nUtilizing its biological knowledge, BioDiscoveryAgent can uniquely design new\nexperiments without the need to train a machine learning model or explicitly\ndesign an acquisition function as in Bayesian optimization. Moreover,\nBioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21%\nimprovement in predicting relevant genetic perturbations across six datasets,\nand a 46% improvement in the harder task of non-essential gene perturbation,\ncompared to existing Bayesian optimization baselines specifically trained for\nthis task. Our evaluation includes one dataset that is unpublished, ensuring it\nis not part of the language model's training data. Additionally,\nBioDiscoveryAgent predicts gene combinations to perturb more than twice as\naccurately as a random baseline, a task so far not explored in the context of\nclosed-loop experiment design. The agent also has access to tools for searching\nthe biomedical literature, executing code to analyze biological datasets, and\nprompting another agent to critically evaluate its predictions. Overall,\nBioDiscoveryAgent is interpretable at every stage, representing an accessible\nnew paradigm in the computational design of biological experiments with the\npotential to augment scientists' efficacy.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17631v3",
    "published_date": "2024-05-27 19:57:17 UTC",
    "updated_date": "2025-03-09 21:57:20 UTC"
  },
  {
    "arxiv_id": "2405.17628v1",
    "title": "Tensor Low-rank Approximation of Finite-horizon Value Functions",
    "authors": [
      "Sergio Rozada",
      "Antonio G. Marques"
    ],
    "abstract": "The goal of reinforcement learning is estimating a policy that maps states to\nactions and maximizes the cumulative reward of a Markov Decision Process (MDP).\nThis is oftentimes achieved by estimating first the optimal (reward) value\nfunction (VF) associated with each state-action pair. When the MDP has an\ninfinite horizon, the optimal VFs and policies are stationary under mild\nconditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary\nwith time. This poses a challenge since the number of VFs to estimate grows not\nonly with the size of the state-action space but also with the time horizon.\nThis paper proposes a non-parametric low-rank stochastic algorithm to\napproximate the VFs of finite-horizon MDPs. First, we represent the (unknown)\nVFs as a multi-dimensional array, or tensor, where time is one of the\ndimensions. Then, we use rewards sampled from the MDP to estimate the optimal\nVFs. More precisely, we use the (truncated) PARAFAC decomposition to design an\nonline low-rank algorithm that recovers the entries of the tensor of VFs. The\nsize of the low-rank PARAFAC model grows additively with respect to each of its\ndimensions, rendering our approach efficient, as demonstrated via numerical\nexperiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17628v1",
    "published_date": "2024-05-27 19:52:00 UTC",
    "updated_date": "2024-05-27 19:52:00 UTC"
  },
  {
    "arxiv_id": "2405.17626v1",
    "title": "Matrix Low-Rank Approximation For Policy Gradient Methods",
    "authors": [
      "Sergio Rozada",
      "Antonio G. Marques"
    ],
    "abstract": "Estimating a policy that maps states to actions is a central problem in\nreinforcement learning. Traditionally, policies are inferred from the so called\nvalue functions (VFs), but exact VF computation suffers from the curse of\ndimensionality. Policy gradient (PG) methods bypass this by learning directly a\nparametric stochastic policy. Typically, the parameters of the policy are\nestimated using neural networks (NNs) tuned via stochastic gradient descent.\nHowever, finding adequate NN architectures can be challenging, and convergence\nissues are common as well. In this paper, we put forth low-rank matrix-based\nmodels to estimate efficiently the parameters of PG algorithms. We collect the\nparameters of the stochastic policy into a matrix, and then, we leverage\nmatrix-completion techniques to promote (enforce) low rank. We demonstrate via\nnumerical studies how low-rank matrix-based policy models reduce the\ncomputational and sample complexities relative to NN models, while achieving a\nsimilar aggregated reward.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17626v1",
    "published_date": "2024-05-27 19:49:08 UTC",
    "updated_date": "2024-05-27 19:49:08 UTC"
  },
  {
    "arxiv_id": "2405.17625v1",
    "title": "Matrix Low-Rank Trust Region Policy Optimization",
    "authors": [
      "Sergio Rozada",
      "Antonio G. Marques"
    ],
    "abstract": "Most methods in reinforcement learning use a Policy Gradient (PG) approach to\nlearn a parametric stochastic policy that maps states to actions. The standard\napproach is to implement such a mapping via a neural network (NN) whose\nparameters are optimized using stochastic gradient descent. However, PG methods\nare prone to large policy updates that can render learning inefficient. Trust\nregion algorithms, like Trust Region Policy Optimization (TRPO), constrain the\npolicy update step, ensuring monotonic improvements. This paper introduces\nlow-rank matrix-based models as an efficient alternative for estimating the\nparameters of TRPO algorithms. By gathering the stochastic policy's parameters\ninto a matrix and applying matrix-completion techniques, we promote and enforce\nlow rank. Our numerical studies demonstrate that low-rank matrix-based policy\nmodels effectively reduce both computational and sample complexities compared\nto NN models, while maintaining comparable aggregated rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17625v1",
    "published_date": "2024-05-27 19:46:31 UTC",
    "updated_date": "2024-05-27 19:46:31 UTC"
  },
  {
    "arxiv_id": "2405.17618v2",
    "title": "Symmetric Reinforcement Learning Loss for Robust Learning on Diverse Tasks and Model Scales",
    "authors": [
      "Ju-Seung Byun",
      "Andrew Perrault"
    ],
    "abstract": "Reinforcement learning (RL) training is inherently unstable due to factors\nsuch as moving targets and high gradient variance. Reinforcement Learning from\nHuman Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF) can\nintroduce additional difficulty. Differing preferences can complicate the\nalignment process, and prediction errors in a trained reward model can become\nmore severe as the LLM generates unseen outputs. To enhance training\nrobustness, RL has adopted techniques from supervised learning, such as\nensembles and layer normalization. In this work, we improve the stability of RL\ntraining by adapting the reverse cross entropy (RCE) from supervised learning\nfor noisy data to define a symmetric RL loss. We demonstrate performance\nimprovements across various tasks and scales. We conduct experiments in\ndiscrete action tasks (Atari games) and continuous action space tasks (MuJoCo\nbenchmark and Box2D) using Symmetric A2C (SA2C) and Symmetric PPO (SPPO), with\nand without added noise with especially notable performance in SPPO across\ndifferent hyperparameters. Furthermore, we validate the benefits of the\nsymmetric RL loss when using SPPO for large language models through improved\nperformance in RLHF tasks, such as IMDB positive sentiment sentiment and TL;DR\nsummarization tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17618v2",
    "published_date": "2024-05-27 19:28:33 UTC",
    "updated_date": "2024-05-29 04:19:00 UTC"
  },
  {
    "arxiv_id": "2405.17610v1",
    "title": "Explainable machine learning multi-label classification of Spanish legal judgements",
    "authors": [
      "Francisco de Arriba-Pérez",
      "Silvia García-Méndez",
      "Francisco J. González-Castaño",
      "Jaime González-González"
    ],
    "abstract": "Artificial Intelligence techniques such as Machine Learning (ML) have not\nbeen exploited to their maximum potential in the legal domain. This has been\npartially due to the insufficient explanations they provided about their\ndecisions. Automatic expert systems with explanatory capabilities can be\nspecially useful when legal practitioners search jurisprudence to gather\ncontextual knowledge for their cases. Therefore, we propose a hybrid system\nthat applies ML for multi-label classification of judgements (sentences) and\nvisual and natural language descriptions for explanation purposes, boosted by\nNatural Language Processing techniques and deep legal reasoning to identify the\nentities, such as the parties, involved. We are not aware of any prior work on\nautomatic multi-label classification of legal judgements also providing natural\nlanguage explanations to the end-users with comparable overall quality. Our\nsolution achieves over 85 % micro precision on a labelled data set annotated by\nlegal experts. This endorses its interest to relieve human experts from\nmonotonous labour-intensive legal classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17610v1",
    "published_date": "2024-05-27 19:16:42 UTC",
    "updated_date": "2024-05-27 19:16:42 UTC"
  },
  {
    "arxiv_id": "2405.17607v1",
    "title": "Advancing Cultural Inclusivity: Optimizing Embedding Spaces for Balanced Music Recommendations",
    "authors": [
      "Armin Moradi",
      "Nicola Neophytou",
      "Golnoosh Farnadi"
    ],
    "abstract": "Popularity bias in music recommendation systems -- where artists and tracks\nwith the highest listen counts are recommended more often -- can also propagate\nbiases along demographic and cultural axes. In this work, we identify these\nbiases in recommendations for artists from underrepresented cultural groups in\nprototype-based matrix factorization methods. Unlike traditional matrix\nfactorization methods, prototype-based approaches are interpretable. This\nallows us to directly link the observed bias in recommendations for minority\nartists (the effect) to specific properties of the embedding space (the cause).\nWe mitigate popularity bias in music recommendation through capturing both\nusers' and songs' cultural nuances in the embedding space. To address these\nchallenges while maintaining recommendation quality, we propose two novel\nenhancements to the embedding space: i) we propose an approach to filter-out\nthe irrelevant prototypes used to represent each user and item to improve\ngeneralizability, and ii) we introduce regularization techniques to reinforce a\nmore uniform distribution of prototypes within the embedding space. Our results\ndemonstrate significant improvements in reducing popularity bias and enhancing\ndemographic and cultural fairness in music recommendations while achieving\ncompetitive -- if not better -- overall performance.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17607v1",
    "published_date": "2024-05-27 19:12:53 UTC",
    "updated_date": "2024-05-27 19:12:53 UTC"
  },
  {
    "arxiv_id": "2405.17604v2",
    "title": "LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters",
    "authors": [
      "Klaudia Bałazy",
      "Mohammadreza Banaei",
      "Karl Aberer",
      "Jacek Tabor"
    ],
    "abstract": "The rapid expansion of large language models (LLMs) has underscored the need\nfor parameter-efficient fine-tuning methods, with LoRA (Low-Rank Adaptation)\nemerging as a popular solution. Although LoRA reduces the number of trainable\nparameters, serving multiple (task or user-specific) LoRA modules on top of a\nbase model still creates significant storage challenges. To address this, using\ntheoretical derivation, we introduce LoRA-XS (Low-Rank Adaptation with\neXtremely Small number of parameters), a novel low-rank adaptation method that\nconsiderably reduces the trainable parameters while showing superior or\ncompetitive performance. LoRA-XS achieves this by inserting a small, trainable\nr x r weight matrix between frozen low-rank matrices, which are constructed by\nSingular Value Decomposition (SVD) of the original weight matrix. This\nlightweight matrix enables fine-tuning with drastically reduced storage\nrequirements, making it feasible to deploy millions of personalized models\nwhile minimizing memory overhead. For instance, LoRA-XS achieves a remarkable\nreduction of trainable parameters by over 100x in 7B models compared to LoRA.\nOur evaluations across various benchmarks (including GLUE, GSM8K, MATH, and\neight commonsense reasoning datasets) demonstrate that LoRA-XS performs\ncompetitively or better than LoRA and other recent methods like VeRA while\nbeing significantly more parameter efficient. We also provide an extensive\nablation study on the importance of singular vectors in transformer weights,\nshedding light on the underlying mechanisms driving LoRA-XS's enhanced\nefficiency. These findings suggest that LoRA-XS is not only a storage-efficient\nalternative, but also a powerful tool for scaling and personalizing LLMs at\nunprecedented scales.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17604v2",
    "published_date": "2024-05-27 19:07:13 UTC",
    "updated_date": "2024-10-08 16:45:14 UTC"
  },
  {
    "arxiv_id": "2405.17580v2",
    "title": "Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes",
    "authors": [
      "Zhenfeng Tu",
      "Santiago Aranguri",
      "Arthur Jacot"
    ],
    "abstract": "The training dynamics of linear networks are well studied in two distinct\nsetups: the lazy regime and balanced/active regime, depending on the\ninitialization and width of the network. We provide a surprisingly simple\nunifying formula for the evolution of the learned matrix that contains as\nspecial cases both lazy and balanced regimes but also a mixed regime in between\nthe two. In the mixed regime, a part of the network is lazy while the other is\nbalanced. More precisely the network is lazy along singular values that are\nbelow a certain threshold and balanced along those that are above the same\nthreshold. At initialization, all singular values are lazy, allowing for the\nnetwork to align itself with the task, so that later in time, when some of the\nsingular value cross the threshold and become active they will converge rapidly\n(convergence in the balanced regime is notoriously difficult in the absence of\nalignment). The mixed regime is the `best of both worlds': it converges from\nany random initialization (in contrast to balanced dynamics which require\nspecial initialization), and has a low rank bias (absent in the lazy dynamics).\nThis allows us to prove an almost complete phase diagram of training behavior\nas a function of the variance at initialization and the width, for a MSE\ntraining task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17580v2",
    "published_date": "2024-05-27 18:29:23 UTC",
    "updated_date": "2024-10-29 20:52:18 UTC"
  },
  {
    "arxiv_id": "2405.17576v1",
    "title": "Container pre-marshalling problem minimizing CV@R under uncertainty of ship arrival times",
    "authors": [
      "Daiki Ikuma",
      "Shunnosuke Ikeda",
      "Noriyoshi Sukegawa",
      "Yuichi Takano"
    ],
    "abstract": "This paper is concerned with the container pre-marshalling problem, which\ninvolves relocating containers in the storage area so that they can be\nefficiently loaded onto ships without reshuffles. In reality, however, ship\narrival times are affected by various external factors, which can cause the\norder of container retrieval to be different from the initial plan. To\nrepresent such uncertainty, we generate multiple scenarios from a multivariate\nprobability distribution of ship arrival times. We derive a mixed-integer\nlinear optimization model to find an optimal container layout such that the\nconditional value-at-risk is minimized for the number of misplaced containers\nresponsible for reshuffles. Moreover, we devise an exact algorithm based on the\ncutting-plane method to handle large-scale problems. Numerical experiments\nusing synthetic datasets demonstrate that our method can produce high-quality\ncontainer layouts compared with the conventional robust optimization model.\nAdditionally, our algorithm can speed up the computation of solving large-scale\nproblems.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17576v1",
    "published_date": "2024-05-27 18:19:09 UTC",
    "updated_date": "2024-05-27 18:19:09 UTC"
  },
  {
    "arxiv_id": "2405.17573v2",
    "title": "Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets",
    "authors": [
      "Arthur Jacot",
      "Alexandre Kaiser"
    ],
    "abstract": "We study Leaky ResNets, which interpolate between ResNets and Fully-Connected\nnets depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the\ninfinite depth limit, we study 'representation geodesics' $A_{p}$: continuous\npaths in representation space (similar to NeuralODEs) from input $p=0$ to\noutput $p=1$ that minimize the parameter norm of the network. We give a\nLagrangian and Hamiltonian reformulation, which highlight the importance of two\nterms: a kinetic energy which favors small layer derivatives\n$\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional\nrepresentations, as measured by the 'Cost of Identity'. The balance between\nthese two forces offers an intuitive understanding of feature learning in\nResNets. We leverage this intuition to explain the emergence of a bottleneck\nstructure, as observed in previous work: for large $\\tilde{L}$ the potential\nenergy dominates and leads to a separation of timescales, where the\nrepresentation jumps rapidly from the high dimensional inputs to a\nlow-dimensional representation, move slowly inside the space of low-dimensional\nrepresentations, before jumping back to the potentially high-dimensional\noutputs. Inspired by this phenomenon, we train with an adaptive layer step-size\nto adapt to the separation of timescales.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17573v2",
    "published_date": "2024-05-27 18:15:05 UTC",
    "updated_date": "2025-03-06 13:47:53 UTC"
  },
  {
    "arxiv_id": "2405.17569v1",
    "title": "Discriminant audio properties in deep learning based respiratory insufficiency detection in Brazilian Portuguese",
    "authors": [
      "Marcelo Matheus Gauy",
      "Larissa Cristina Berti",
      "Arnaldo Cândido Jr",
      "Augusto Camargo Neto",
      "Alfredo Goldman",
      "Anna Sara Shafferman Levin",
      "Marcus Martins",
      "Beatriz Raposo de Medeiros",
      "Marcelo Queiroz",
      "Ester Cerdeira Sabino",
      "Flaviane Romani Fernandes Svartman",
      "Marcelo Finger"
    ],
    "abstract": "This work investigates Artificial Intelligence (AI) systems that detect\nrespiratory insufficiency (RI) by analyzing speech audios, thus treating speech\nas a RI biomarker. Previous works collected RI data (P1) from COVID-19 patients\nduring the first phase of the pandemic and trained modern AI models, such as\nCNNs and Transformers, which achieved $96.5\\%$ accuracy, showing the\nfeasibility of RI detection via AI. Here, we collect RI patient data (P2) with\nseveral causes besides COVID-19, aiming at extending AI-based RI detection. We\nalso collected control data from hospital patients without RI. We show that the\nconsidered models, when trained on P1, do not generalize to P2, indicating that\nCOVID-19 RI has features that may not be found in all RI types.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 2 figures, 1 table. Published in Artificial Intelligence in\n  Medicine (AIME) 2023",
    "pdf_url": "http://arxiv.org/pdf/2405.17569v1",
    "published_date": "2024-05-27 18:04:49 UTC",
    "updated_date": "2024-05-27 18:04:49 UTC"
  },
  {
    "arxiv_id": "2405.17556v2",
    "title": "Probabilistic Verification of Neural Networks using Branch and Bound",
    "authors": [
      "David Boetius",
      "Stefan Leue",
      "Tobias Sutter"
    ],
    "abstract": "Probabilistic verification of neural networks is concerned with formally\nanalysing the output distribution of a neural network under a probability\ndistribution of the inputs. Examples of probabilistic verification include\nverifying the demographic parity fairness notion or quantifying the safety of a\nneural network. We present a new algorithm for the probabilistic verification\nof neural networks based on an algorithm for computing and iteratively refining\nlower and upper bounds on probabilities over the outputs of a neural network.\nBy applying state-of-the-art bound propagation and branch and bound techniques\nfrom non-probabilistic neural network verification, our algorithm significantly\noutpaces existing probabilistic verification algorithms, reducing solving times\nfor various benchmarks from the literature from tens of minutes to tens of\nseconds. Furthermore, our algorithm compares favourably even to dedicated\nalgorithms for restricted subsets of probabilistic verification. We complement\nour empirical evaluation with a theoretical analysis, proving that our\nalgorithm is sound and, under mildly restrictive conditions, also complete when\nusing a suitable set of heuristics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Code available at https://github.com/sen-uni-kn/probspecs; 19 pages,\n  3 figures, 30 pages references and appendix, including 7 more figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17556v2",
    "published_date": "2024-05-27 18:00:03 UTC",
    "updated_date": "2025-01-30 15:57:56 UTC"
  },
  {
    "arxiv_id": "2405.17430v2",
    "title": "Matryoshka Multimodal Models",
    "authors": [
      "Mu Cai",
      "Jianwei Yang",
      "Jianfeng Gao",
      "Yong Jae Lee"
    ],
    "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in\nvisual-linguistic reasoning. These models first embed images into a fixed large\nnumber of visual tokens and then feed them into a Large Language Model (LLM).\nHowever, this design causes an excessive number of tokens for dense visual\nscenarios such as high-resolution images and videos, leading to great\ninefficiency. While token pruning/merging methods do exist, they produce a\nsingle length output for each image and do not afford flexibility in trading\noff information density v.s. efficiency. Inspired by the concept of Matryoshka\nDolls, we propose M3: Matryoshka Multimodal Models, which learns to represent\nvisual content as nested sets of visual tokens that capture information across\nmultiple coarse-to-fine granularities. Our approach offers several unique\nbenefits for LMMs: (1) One can explicitly control the visual granularity per\ntest instance during inference, e.g. , adjusting the number of tokens used to\nrepresent an image based on the anticipated complexity or simplicity of the\ncontent; (2) M3 provides a framework for analyzing the granularity needed for\nexisting datasets, where we find that COCO-style benchmarks only need around ~9\nvisual tokens to obtain accuracy similar to that of using all 576 tokens; (3)\nOur approach provides a foundation to explore the best trade-off between\nperformance and visual token length at sample level, where our investigation\nreveals that a large gap exists between the oracle upper bound and current\nfixed-scale representations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://matryoshka-mm.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.17430v2",
    "published_date": "2024-05-27 17:59:56 UTC",
    "updated_date": "2024-07-29 17:59:28 UTC"
  },
  {
    "arxiv_id": "2405.17429v1",
    "title": "GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction",
    "authors": [
      "Yuanhui Huang",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D semantic occupancy prediction aims to obtain 3D fine-grained geometry and\nsemantics of the surrounding scene and is an important task for the robustness\nof vision-centric autonomous driving. Most existing methods employ dense grids\nsuch as voxels as scene representations, which ignore the sparsity of occupancy\nand the diversity of object scales and thus lead to unbalanced allocation of\nresources. To address this, we propose an object-centric representation to\ndescribe 3D scenes with sparse 3D semantic Gaussians where each Gaussian\nrepresents a flexible region of interest and its semantic features. We\naggregate information from images through the attention mechanism and\niteratively refine the properties of 3D Gaussians including position,\ncovariance, and semantics. We then propose an efficient Gaussian-to-voxel\nsplatting method to generate 3D occupancy predictions, which only aggregates\nthe neighboring Gaussians for a certain position. We conduct extensive\nexperiments on the widely adopted nuScenes and KITTI-360 datasets. Experimental\nresults demonstrate that GaussianFormer achieves comparable performance with\nstate-of-the-art methods with only 17.8% - 24.8% of their memory consumption.\nCode is available at: https://github.com/huang-yh/GaussianFormer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/huang-yh/GaussianFormer",
    "pdf_url": "http://arxiv.org/pdf/2405.17429v1",
    "published_date": "2024-05-27 17:59:51 UTC",
    "updated_date": "2024-05-27 17:59:51 UTC"
  },
  {
    "arxiv_id": "2405.17428v3",
    "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
    "authors": [
      "Chankyu Lee",
      "Rajarshi Roy",
      "Mengyao Xu",
      "Jonathan Raiman",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "abstract": "Decoder-only LLM-based embedding models are beginning to outperform BERT or\nT5-based embedding models in general-purpose text embedding tasks, including\ndense vector-based retrieval. In this work, we introduce NV-Embed,\nincorporating architectural designs, training procedures, and curated datasets\nto significantly enhance the performance of LLM as a versatile embedding model,\nwhile maintaining its simplicity and reproducibility. For model architecture,\nwe propose a latent attention layer to obtain pooled embeddings, which\nconsistently improves retrieval and downstream task accuracy compared to mean\npooling or using the last <EOS> token embedding from LLMs. To enhance\nrepresentation learning, we remove the causal attention mask of LLMs during\ncontrastive training. For training algorithm, we introduce a two-stage\ncontrastive instruction-tuning method. It first applies contrastive training\nwith instructions on retrieval datasets, utilizing in-batch negatives and\ncurated hard negative examples. At stage-2, it blends various non-retrieval\ninto instruction tuning, which not only enhances non-retrieval task accuracy\nbut also improves retrieval performance. For training data, we utilize the\nhard-negative mining, synthetic data generation and existing public available\ndatasets to boost the performance of embedding model. By combining these\ntechniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position\non the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across\n56 tasks, demonstrating the sustained effectiveness of the proposed methods\nover time. It also achieved the highest scores in the Long Doc section and the\nsecond-highest scores in the QA section of the AIR Benchmark, which covers a\nrange of out-of-domain information retrieval topics beyond those in MTEB. We\nfurther provide the analysis of model compression techniques for generalist\nembedding models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 (Spotlight). We open-source the model at:\n  https://huggingface.co/nvidia/NV-Embed-v2",
    "pdf_url": "http://arxiv.org/pdf/2405.17428v3",
    "published_date": "2024-05-27 17:59:45 UTC",
    "updated_date": "2025-02-25 00:35:18 UTC"
  },
  {
    "arxiv_id": "2405.20774v3",
    "title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems",
    "authors": [
      "Ruochen Jiao",
      "Shaoyuan Xie",
      "Justin Yue",
      "Takami Sato",
      "Lixu Wang",
      "Yixuan Wang",
      "Qi Alfred Chen",
      "Qi Zhu"
    ],
    "abstract": "Large Language Models (LLMs) have shown significant promise in real-world\ndecision-making tasks for embodied artificial intelligence, especially when\nfine-tuned to leverage their inherent common sense and reasoning abilities\nwhile being tailored to specific applications. However, this fine-tuning\nprocess introduces considerable safety and security vulnerabilities, especially\nin safety-critical cyber-physical systems. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-based Decision-making\nsystems (BALD) in embodied AI, systematically exploring the attack surfaces and\ntrigger mechanisms. Specifically, we propose three distinct attack mechanisms:\nword injection, scenario manipulation, and knowledge injection, targeting\nvarious components in the LLM-based decision-making pipeline. We perform\nextensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in\nautonomous driving and home robot tasks, demonstrating the effectiveness and\nstealthiness of our backdoor triggers across various attack channels, with\ncases like vehicles accelerating toward obstacles and robots placing knives on\nbeds. Our word and knowledge injection attacks achieve nearly 100% success rate\nacross multiple models and datasets while requiring only limited access to the\nsystem. Our scenario manipulation attack yields success rates exceeding 65%,\nreaching up to 90%, and does not require any runtime system intrusion. We also\nassess the robustness of these attacks against defenses, revealing their\nresilience. Our findings highlight critical security vulnerabilities in\nembodied LLM systems and emphasize the urgent need for safeguarding these\nsystems to mitigate potential risks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted paper at ICLR 2025, 31 pages, including main paper,\n  references, and appendix",
    "pdf_url": "http://arxiv.org/pdf/2405.20774v3",
    "published_date": "2024-05-27 17:59:43 UTC",
    "updated_date": "2025-04-30 17:59:57 UTC"
  },
  {
    "arxiv_id": "2405.17422v1",
    "title": "Hardness-Aware Scene Synthesis for Semi-Supervised 3D Object Detection",
    "authors": [
      "Shuai Zeng",
      "Wenzhao Zheng",
      "Jiwen Lu",
      "Haibin Yan"
    ],
    "abstract": "3D object detection aims to recover the 3D information of concerning objects\nand serves as the fundamental task of autonomous driving perception. Its\nperformance greatly depends on the scale of labeled training data, yet it is\ncostly to obtain high-quality annotations for point cloud data. While\nconventional methods focus on generating pseudo-labels for unlabeled samples as\nsupplements for training, the structural nature of 3D point cloud data\nfacilitates the composition of objects and backgrounds to synthesize realistic\nscenes. Motivated by this, we propose a hardness-aware scene synthesis (HASS)\nmethod to generate adaptive synthetic scenes to improve the generalization of\nthe detection models. We obtain pseudo-labels for unlabeled objects and\ngenerate diverse scenes with different compositions of objects and backgrounds.\nAs the scene synthesis is sensitive to the quality of pseudo-labels, we further\npropose a hardness-aware strategy to reduce the effect of low-quality\npseudo-labels and maintain a dynamic pseudo-database to ensure the diversity\nand quality of synthetic scenes. Extensive experimental results on the widely\nused KITTI and Waymo datasets demonstrate the superiority of the proposed HASS\nmethod, which outperforms existing semi-supervised learning methods on 3D\nobject detection. Code: https://github.com/wzzheng/HASS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/wzzheng/HASS",
    "pdf_url": "http://arxiv.org/pdf/2405.17422v1",
    "published_date": "2024-05-27 17:59:23 UTC",
    "updated_date": "2024-05-27 17:59:23 UTC"
  },
  {
    "arxiv_id": "2405.17419v2",
    "title": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",
    "authors": [
      "Hao Dong",
      "Yue Zhao",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "abstract": "Detecting out-of-distribution (OOD) samples is important for deploying\nmachine learning models in safety-critical applications such as autonomous\ndriving and robot-assisted surgery. Existing research has mainly focused on\nunimodal scenarios on image data. However, real-world applications are\ninherently multimodal, which makes it essential to leverage information from\nmultiple modalities to enhance the efficacy of OOD detection. To establish a\nfoundation for more realistic Multimodal OOD Detection, we introduce the\nfirst-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes\nand varying modality combinations. We first evaluate existing unimodal OOD\ndetection algorithms on MultiOOD, observing that the mere inclusion of\nadditional modalities yields substantial improvements. This underscores the\nimportance of utilizing multiple modalities for OOD detection. Based on the\nobservation of Modality Prediction Discrepancy between in-distribution (ID) and\nOOD data, and its strong correlation with OOD performance, we propose the\nAgree-to-Disagree (A2D) algorithm to encourage such discrepancy during\ntraining. Moreover, we introduce a novel outlier synthesis method, NP-Mix,\nwhich explores broader feature spaces by leveraging the information from\nnearest neighbor classes and complements A2D to strengthen OOD detection\nperformance. Extensive experiments on MultiOOD demonstrate that training with\nA2D and NP-Mix improves existing OOD detection algorithms by a large margin.\nOur source code and MultiOOD benchmark are available at\nhttps://github.com/donghao51/MultiOOD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 spotlight. Code and MultiOOD benchmark:\n  https://github.com/donghao51/MultiOOD",
    "pdf_url": "http://arxiv.org/pdf/2405.17419v2",
    "published_date": "2024-05-27 17:59:02 UTC",
    "updated_date": "2024-10-26 16:27:02 UTC"
  },
  {
    "arxiv_id": "2405.17537v4",
    "title": "CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale",
    "authors": [
      "ZeMing Gong",
      "Austin T. Wang",
      "Xiaoliang Huo",
      "Joakim Bruslund Haurum",
      "Scott C. Lowe",
      "Graham W. Taylor",
      "Angel X. Chang"
    ],
    "abstract": "Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for taxonomic classification\nof photographic images and DNA separately, in this work, we introduce a\nmultimodal approach combining both, using CLIP-style contrastive learning to\nalign images, barcode DNA, and text-based representations of taxonomic labels\nin a unified embedding space. This allows for accurate classification of both\nknown and unknown insect species without task-specific fine-tuning, leveraging\ncontrastive learning for the first time to fuse barcode DNA and image data. Our\nmethod surpasses previous single-modality approaches in accuracy by over 8% on\nzero-shot learning tasks, showcasing its effectiveness in biodiversity studies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "31 pages with 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17537v4",
    "published_date": "2024-05-27 17:57:48 UTC",
    "updated_date": "2025-04-02 22:13:52 UTC"
  },
  {
    "arxiv_id": "2405.17413v1",
    "title": "Enhancing Music Genre Classification through Multi-Algorithm Analysis and User-Friendly Visualization",
    "authors": [
      "Navin Kamuni",
      "Dheerendra Panwar"
    ],
    "abstract": "The aim of this study is to teach an algorithm how to recognize different\ntypes of music. Users will submit songs for analysis. Since the algorithm\nhasn't heard these songs before, it needs to figure out what makes each song\nunique. It does this by breaking down the songs into different parts and\nstudying things like rhythm, melody, and tone via supervised learning because\nthe program learns from examples that are already labelled. One important thing\nto consider when classifying music is its genre, which can be quite complex. To\nensure accuracy, we use five different algorithms, each working independently,\nto analyze the songs. This helps us get a more complete understanding of each\nsong's characteristics. Therefore, our goal is to correctly identify the genre\nof each submitted song. Once the analysis is done, the results are presented\nusing a graphing tool, making it easy for users to understand and provide\nfeedback.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17413v1",
    "published_date": "2024-05-27 17:57:20 UTC",
    "updated_date": "2024-05-27 17:57:20 UTC"
  },
  {
    "arxiv_id": "2405.17412v5",
    "title": "Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE",
    "authors": [
      "Aditya Ravuri",
      "Neil D. Lawrence"
    ],
    "abstract": "This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in Ravuri et al. (2023), that describes the graph Laplacian\n(an estimate of the data precision matrix) using a Wishart distribution, with a\nmean given by a non-linear covariance function evaluated on the latents. This\ninterpretation offers deeper theoretical and semantic insights into such\nalgorithms, and forging a connection to Gaussian process latent variable models\nby showing that well-known kernels can be used to describe covariances implied\nby graph Laplacians. We also introduce tools with which similar dimensionality\nreduction methods can be studied.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "Updated figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17412v5",
    "published_date": "2024-05-27 17:57:12 UTC",
    "updated_date": "2025-05-10 19:36:12 UTC"
  },
  {
    "arxiv_id": "2405.17535v1",
    "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
    "authors": [
      "Mucong Ding",
      "Yuancheng Xu",
      "Tahseen Rabbani",
      "Xiaoyu Liu",
      "Brian Gravelle",
      "Teresa Ranadive",
      "Tai-Ching Tuan",
      "Furong Huang"
    ],
    "abstract": "Dataset condensation can be used to reduce the computational cost of training\nmultiple models on a large dataset by condensing the training dataset into a\nsmall synthetic set. State-of-the-art approaches rely on matching the model\ngradients between the real and synthetic data. However, there is no theoretical\nguarantee of the generalizability of the condensed data: data condensation\noften generalizes poorly across hyperparameters/architectures in practice. This\npaper considers a different condensation objective specifically geared toward\nhyperparameter search. We aim to generate a synthetic validation dataset so\nthat the validation-performance rankings of the models, with different\nhyperparameters, on the condensed and original datasets are comparable. We\npropose a novel hyperparameter-calibrated dataset condensation (HCDC)\nalgorithm, which obtains the synthetic validation dataset by matching the\nhyperparameter gradients computed via implicit differentiation and efficient\ninverse Hessian approximation. Experiments demonstrate that the proposed\nframework effectively maintains the validation-performance rankings of models\nand speeds up hyperparameter/architecture search for tasks on both images and\ngraphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17535v1",
    "published_date": "2024-05-27 17:55:01 UTC",
    "updated_date": "2024-05-27 17:55:01 UTC"
  },
  {
    "arxiv_id": "2406.18559v1",
    "title": "Revision Matters: Generative Design Guided by Revision Edits",
    "authors": [
      "Tao Li",
      "Chin-Yi Cheng",
      "Amber Xie",
      "Gang Li",
      "Yang Li"
    ],
    "abstract": "Layout design, such as user interface or graphical layout in general, is\nfundamentally an iterative revision process. Through revising a design\nrepeatedly, the designer converges on an ideal layout. In this paper, we\ninvestigate how revision edits from human designer can benefit a multimodal\ngenerative model. To do so, we curate an expert dataset that traces how human\ndesigners iteratively edit and improve a layout generation with a prompted\nlanguage goal. Based on such data, we explore various supervised fine-tuning\ntask setups on top of a Gemini multimodal backbone, a large multimodal model.\nOur results show that human revision plays a critical role in iterative layout\nrefinement. While being noisy, expert revision edits lead our model to a\nsurprisingly strong design FID score ~10 which is close to human performance\n(~6). In contrast, self-revisions that fully rely on model's own judgement,\nlead to an echo chamber that prevents iterative improvement, and sometimes\nleads to generative degradation. Fortunately, we found that providing human\nguidance plays at early stage plays a critical role in final generation. In\nsuch human-in-the-loop scenario, our work paves the way for iterative design\nrevision based on pre-trained large multimodal models.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18559v1",
    "published_date": "2024-05-27 17:54:51 UTC",
    "updated_date": "2024-05-27 17:54:51 UTC"
  },
  {
    "arxiv_id": "2405.17404v1",
    "title": "Spectral Greedy Coresets for Graph Neural Networks",
    "authors": [
      "Mucong Ding",
      "Yinhan He",
      "Jundong Li",
      "Furong Huang"
    ],
    "abstract": "The ubiquity of large-scale graphs in node-classification tasks significantly\nhinders the real-world applications of Graph Neural Networks (GNNs). Node\nsampling, graph coarsening, and dataset condensation are effective strategies\nfor enhancing data efficiency. However, owing to the interdependence of graph\nnodes, coreset selection, which selects subsets of the data examples, has not\nbeen successfully applied to speed up GNN training on large graphs, warranting\nspecial treatment. This paper studies graph coresets for GNNs and avoids the\ninterdependence issue by selecting ego-graphs (i.e., neighborhood subgraphs\naround a node) based on their spectral embeddings. We decompose the coreset\nselection problem for GNNs into two phases: a coarse selection of widely spread\nego graphs and a refined selection to diversify their topologies. We design a\ngreedy algorithm that approximately optimizes both objectives. Our spectral\ngreedy graph coreset (SGGC) scales to graphs with millions of nodes, obviates\nthe need for model pre-training, and applies to low-homophily graphs. Extensive\nexperiments on ten datasets demonstrate that SGGC outperforms other coreset\nmethods by a wide margin, generalizes well across GNN architectures, and is\nmuch faster than graph condensation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17404v1",
    "published_date": "2024-05-27 17:52:12 UTC",
    "updated_date": "2024-05-27 17:52:12 UTC"
  },
  {
    "arxiv_id": "2405.17403v3",
    "title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
    "authors": [
      "Kai Wang",
      "Mingjia Shi",
      "Yukun Zhou",
      "Zekai Li",
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Xiaojiang Peng",
      "Hanwang Zhang",
      "Yang You"
    ],
    "abstract": "Training diffusion models is always a computation-intensive task. In this\npaper, we introduce a novel speed-up method for diffusion model training,\ncalled, which is based on a closer look at time steps. Our key findings are: i)\nTime steps can be empirically divided into acceleration, deceleration, and\nconvergence areas based on the process increment. ii) These time steps are\nimbalanced, with many concentrated in the convergence area. iii) The\nconcentrated steps provide limited benefits for diffusion training. To address\nthis, we design an asymmetric sampling strategy that reduces the frequency of\nsteps from the convergence area while increasing the sampling probability for\nsteps from other areas. Additionally, we propose a weighting strategy to\nemphasize the importance of time steps with rapid-change process increments. As\na plug-and-play and architecture-agnostic approach, SpeeD consistently achieves\n3-times acceleration across various diffusion architectures, datasets, and\ntasks. Notably, due to its simple design, our approach significantly reduces\nthe cost of diffusion model training with minimal overhead. Our research\nenables more researchers to train diffusion models at a lower cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17403v3",
    "published_date": "2024-05-27 17:51:36 UTC",
    "updated_date": "2025-03-25 08:38:28 UTC"
  },
  {
    "arxiv_id": "2405.17533v1",
    "title": "PAE: LLM-based Product Attribute Extraction for E-Commerce Fashion Trends",
    "authors": [
      "Apurva Sinha",
      "Ekta Gujral"
    ],
    "abstract": "Product attribute extraction is an growing field in e-commerce business, with\nseveral applications including product ranking, product recommendation, future\nassortment planning and improving online shopping customer experiences.\nUnderstanding the customer needs is critical part of online business,\nspecifically fashion products. Retailers uses assortment planning to determine\nthe mix of products to offer in each store and channel, stay responsive to\nmarket dynamics and to manage inventory and catalogs. The goal is to offer the\nright styles, in the right sizes and colors, through the right channels. When\nshoppers find products that meet their needs and desires, they are more likely\nto return for future purchases, fostering customer loyalty. Product attributes\nare a key factor in assortment planning. In this paper we present PAE, a\nproduct attribute extraction algorithm for future trend reports consisting text\nand images in PDF format. Most existing methods focus on attribute extraction\nfrom titles or product descriptions or utilize visual information from existing\nproduct images. Compared to the prior works, our work focuses on attribute\nextraction from PDF files where upcoming fashion trends are explained. This\nwork proposes a more comprehensive framework that fully utilizes the different\nmodalities for attribute extraction and help retailers to plan the assortment\nin advance. Our contributions are three-fold: (a) We develop PAE, an efficient\nframework to extract attributes from unstructured data (text and images); (b)\nWe provide catalog matching methodology based on BERT representations to\ndiscover the existing attributes using upcoming attribute values; (c) We\nconduct extensive experiments with several baselines and show that PAE is an\neffective, flexible and on par or superior (avg 92.5% F1-Score) framework to\nexisting state-of-the-art for attribute value extraction task.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Attribute Extraction, PDF files, Bert Embedding, Hashtag, Large\n  Language Model (LLM), Text and Images",
    "pdf_url": "http://arxiv.org/pdf/2405.17533v1",
    "published_date": "2024-05-27 17:50:25 UTC",
    "updated_date": "2024-05-27 17:50:25 UTC"
  },
  {
    "arxiv_id": "2405.17399v2",
    "title": "Transformers Can Do Arithmetic with the Right Embeddings",
    "authors": [
      "Sean McLeish",
      "Arpit Bansal",
      "Alex Stein",
      "Neel Jain",
      "John Kirchenbauer",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Abhinav Bhatele",
      "Jonas Geiping",
      "Avi Schwarzschild",
      "Tom Goldstein"
    ],
    "abstract": "The poor performance of transformers on arithmetic tasks seems to stem in\nlarge part from their inability to keep track of the exact position of each\ndigit inside of a large span of digits. We mend this problem by adding an\nembedding to each digit that encodes its position relative to the start of the\nnumber. In addition to the boost these embeddings provide on their own, we show\nthat this fix enables architectural modifications such as input injection and\nrecurrent layers to improve performance even further.\n  With positions resolved, we can study the logical extrapolation ability of\ntransformers. Can they solve arithmetic problems that are larger and more\ncomplex than those in their training data? We find that training on only 20\ndigit numbers with a single GPU for one day, we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on 100 digit addition problems.\nFinally, we show that these gains in numeracy also unlock improvements on other\nmulti-step reasoning tasks including sorting and multiplication.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17399v2",
    "published_date": "2024-05-27 17:49:18 UTC",
    "updated_date": "2024-12-23 12:46:06 UTC"
  },
  {
    "arxiv_id": "2405.17398v5",
    "title": "Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability",
    "authors": [
      "Shenyuan Gao",
      "Jiazhi Yang",
      "Li Chen",
      "Kashyap Chitta",
      "Yihang Qiu",
      "Andreas Geiger",
      "Jun Zhang",
      "Hongyang Li"
    ],
    "abstract": "World models can foresee the outcomes of different actions, which is of\nparamount importance for autonomous driving. Nevertheless, existing driving\nworld models still have limitations in generalization to unseen environments,\nprediction fidelity of critical details, and action controllability for\nflexible application. In this paper, we present Vista, a generalizable driving\nworld model with high fidelity and versatile controllability. Based on a\nsystematic diagnosis of existing methods, we introduce several key ingredients\nto address these limitations. To accurately predict real-world dynamics at high\nresolution, we propose two novel losses to promote the learning of moving\ninstances and structural information. We also devise an effective latent\nreplacement approach to inject historical frames as priors for coherent\nlong-horizon rollouts. For action controllability, we incorporate a versatile\nset of controls from high-level intentions (command, goal point) to low-level\nmaneuvers (trajectory, angle, and speed) through an efficient learning\nstrategy. After large-scale training, the capabilities of Vista can seamlessly\ngeneralize to different scenarios. Extensive experiments on multiple datasets\nshow that Vista outperforms the most advanced general-purpose video generator\nin over 70% of comparisons and surpasses the best-performing driving world\nmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize\nthe capacity of Vista itself to establish a generalizable reward for real-world\naction evaluation without accessing the ground truth actions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024. Code and model: https://github.com/OpenDriveLab/Vista,\n  demo page: https://vista-demo.github.io",
    "pdf_url": "http://arxiv.org/pdf/2405.17398v5",
    "published_date": "2024-05-27 17:49:15 UTC",
    "updated_date": "2024-10-28 05:53:17 UTC"
  },
  {
    "arxiv_id": "2405.17386v1",
    "title": "MindMerger: Efficient Boosting LLM Reasoning in non-English Languages",
    "authors": [
      "Zixian Huang",
      "Wenhao Zhu",
      "Gong Cheng",
      "Lei Li",
      "Fei Yuan"
    ],
    "abstract": "Reasoning capabilities are crucial for Large Language Models (LLMs), yet a\nnotable gap exists between English and non-English languages. To bridge this\ndisparity, some works fine-tune LLMs to relearn reasoning capabilities in\nnon-English languages, while others replace non-English inputs with an external\nmodel's outputs such as English translation text to circumvent the challenge of\nLLM understanding non-English. Unfortunately, these methods often underutilize\nthe built-in skilled reasoning and useful language understanding capabilities\nof LLMs. In order to better utilize the minds of reasoning and language\nunderstanding in LLMs, we propose a new method, namely MindMerger, which merges\nLLMs with the external language understanding capabilities from multilingual\nmodels to boost the multilingual reasoning performance. Furthermore, a two-step\ntraining scheme is introduced to first train to embeded the external\ncapabilities into LLMs and then train the collaborative utilization of the\nexternal capabilities and the built-in capabilities in LLMs. Experiments on\nthree multilingual reasoning datasets and a language understanding dataset\ndemonstrate that MindMerger consistently outperforms all baselines, especially\nin low-resource languages. Without updating the parameters of LLMs, the average\naccuracy improved by 6.7% and 8.0% across all languages and low-resource\nlanguages on the MGSM dataset, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17386v1",
    "published_date": "2024-05-27 17:41:54 UTC",
    "updated_date": "2024-05-27 17:41:54 UTC"
  },
  {
    "arxiv_id": "2405.17372v3",
    "title": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction",
    "authors": [
      "Zikang Zhou",
      "Haibo Hu",
      "Xinhong Chen",
      "Jianping Wang",
      "Nan Guan",
      "Kui Wu",
      "Yung-Hui Li",
      "Yu-Kai Huang",
      "Chun Jason Xue"
    ],
    "abstract": "Simulating realistic behaviors of traffic agents is pivotal for efficiently\nvalidating the safety of autonomous driving systems. Existing data-driven\nsimulators primarily use an encoder-decoder architecture to encode the\nhistorical trajectories before decoding the future. However, the heterogeneity\nbetween encoders and decoders complicates the models, and the manual separation\nof historical and future trajectories leads to low data utilization. Given\nthese limitations, we propose BehaviorGPT, a homogeneous and fully\nautoregressive Transformer designed to simulate the sequential behavior of\nmultiple agents. Crucially, our approach discards the traditional separation\nbetween \"history\" and \"future\" by modeling each time step as the \"current\" one\nfor motion generation, leading to a simpler, more parameter- and data-efficient\nagent simulator. We further introduce the Next-Patch Prediction Paradigm (NP3)\nto mitigate the negative effects of autoregressive modeling, in which models\nare trained to reason at the patch level of trajectories and capture long-range\nspatial-temporal interactions. Despite having merely 3M model parameters,\nBehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a\nrealism score of 0.7473 and a minADE score of 1.4147, demonstrating its\nexceptional performance in traffic agent simulation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17372v3",
    "published_date": "2024-05-27 17:28:25 UTC",
    "updated_date": "2024-11-11 14:20:39 UTC"
  },
  {
    "arxiv_id": "2405.17358v3",
    "title": "Rethinking Transformers in Solving POMDPs",
    "authors": [
      "Chenhao Lu",
      "Ruizhe Shi",
      "Yuyao Liu",
      "Kaizhe Hu",
      "Simon S. Du",
      "Huazhe Xu"
    ],
    "abstract": "Sequential decision-making algorithms such as reinforcement learning (RL) in\nreal-world scenarios inevitably face environments with partial observability.\nThis paper scrutinizes the effectiveness of a popular architecture, namely\nTransformers, in Partially Observable Markov Decision Processes (POMDPs) and\nreveals its theoretical limitations. We establish that regular languages, which\nTransformers struggle to model, are reducible to POMDPs. This poses a\nsignificant challenge for Transformers in learning POMDP-specific inductive\nbiases, due to their lack of inherent recurrence found in other models like\nRNNs. This paper casts doubt on the prevalent belief in Transformers as\nsequence models for RL and proposes to introduce a point-wise recurrent\nstructure. The Deep Linear Recurrent Unit (LRU) emerges as a well-suited\nalternative for Partially Observable RL, with empirical results highlighting\nthe sub-optimal performance of the Transformer and considerable strength of\nLRU.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024; references added; typos fixed",
    "pdf_url": "http://arxiv.org/pdf/2405.17358v3",
    "published_date": "2024-05-27 17:02:35 UTC",
    "updated_date": "2024-05-30 07:54:40 UTC"
  },
  {
    "arxiv_id": "2405.17346v1",
    "title": "Prompt Optimization with Human Feedback",
    "authors": [
      "Xiaoqiang Lin",
      "Zhongxiang Dai",
      "Arun Verma",
      "See-Kiong Ng",
      "Patrick Jaillet",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performances in\nvarious tasks. However, the performance of LLMs heavily depends on the input\nprompt, which has given rise to a number of recent works on prompt\noptimization. However, previous works often require the availability of a\nnumeric score to assess the quality of every prompt. Unfortunately, when a\nhuman user interacts with a black-box LLM, attaining such a score is often\ninfeasible and unreliable. Instead, it is usually significantly easier and more\nreliable to obtain preference feedback from a human user, i.e., showing the\nuser the responses generated from a pair of prompts and asking the user which\none is preferred. Therefore, in this paper, we study the problem of prompt\noptimization with human feedback (POHF), in which we aim to optimize the prompt\nfor a black-box LLM using only human preference feedback. Drawing inspiration\nfrom dueling bandits, we design a theoretically principled strategy to select a\npair of prompts to query for preference feedback in every iteration, and hence\nintroduce our algorithm named automated POHF (APOHF). We apply our APOHF\nalgorithm to various tasks, including optimizing user instructions, prompt\noptimization for text-to-image generative models, and response optimization\nwith human feedback (i.e., further refining the response using a variant of our\nAPOHF). The results demonstrate that our APOHF can efficiently find a good\nprompt using a small number of preference feedback instances. Our code can be\nfound at \\url{https://github.com/xqlin98/APOHF}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint, 18 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.17346v1",
    "published_date": "2024-05-27 16:49:29 UTC",
    "updated_date": "2024-05-27 16:49:29 UTC"
  },
  {
    "arxiv_id": "2405.17345v2",
    "title": "Exploring and steering the moral compass of Large Language Models",
    "authors": [
      "Alejandro Tlaie"
    ],
    "abstract": "Large Language Models (LLMs) have become central to advancing automation and\ndecision-making across various sectors, raising significant ethical questions.\nThis study proposes a comprehensive comparative analysis of the most advanced\nLLMs to assess their moral profiles. We subjected several state-of-the-art\nmodels to a selection of ethical dilemmas and found that all the proprietary\nones are mostly utilitarian and all of the open-weights ones align mostly with\nvalues-based ethics. Furthermore, when using the Moral Foundations\nQuestionnaire, all models we probed - except for Llama 2-7B - displayed a\nstrong liberal bias. Lastly, in order to causally intervene in one of the\nstudied models, we propose a novel similarity-specific activation steering\ntechnique. Using this method, we were able to reliably steer the model's moral\ncompass to different ethical schools. All of these results showcase that there\nis an ethical dimension in already deployed LLMs, an aspect that is generally\noverlooked.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17345v2",
    "published_date": "2024-05-27 16:49:22 UTC",
    "updated_date": "2024-06-06 11:53:23 UTC"
  },
  {
    "arxiv_id": "2405.17337v1",
    "title": "Cost-efficient Knowledge-based Question Answering with Large Language Models",
    "authors": [
      "Junnan Dong",
      "Qinggang Zhang",
      "Chuang Zhou",
      "Hao Chen",
      "Daochen Zha",
      "Xiao Huang"
    ],
    "abstract": "Knowledge-based question answering (KBQA) is widely used in many scenarios\nthat necessitate domain knowledge. Large language models (LLMs) bring\nopportunities to KBQA, while their costs are significantly higher and absence\nof domain-specific knowledge during pre-training. We are motivated to combine\nLLMs and prior small models on knowledge graphs (KGMs) for both inferential\naccuracy and cost saving. However, it remains challenging since accuracy and\ncost are not readily combined in the optimization as two distinct metrics. It\nis also laborious for model selection since different models excel in diverse\nknowledge. To this end, we propose Coke, a novel cost-efficient strategy for\nKBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize\ncalls to LLMs within limited budgets. We first formulate the accuracy\nexpectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A\ncontext-aware policy is optimized to further distinguish the expert model\nsubject to the question semantics. The overall decision is bounded by the cost\nregret according to historical expenditure on failures. Extensive experiments\nshowcase the superior performance of Coke, which moves the Pareto frontier with\nup to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on\nthe benchmark datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17337v1",
    "published_date": "2024-05-27 16:37:34 UTC",
    "updated_date": "2024-05-27 16:37:34 UTC"
  },
  {
    "arxiv_id": "2406.01608v1",
    "title": "Detecting Deceptive Dark Patterns in E-commerce Platforms",
    "authors": [
      "Arya Ramteke",
      "Sankalp Tembhurne",
      "Gunesh Sonawane",
      "Ratnmala N. Bhimanpallewar"
    ],
    "abstract": "Dark patterns are deceptive user interfaces employed by e-commerce websites\nto manipulate user's behavior in a way that benefits the website, often\nunethically. This study investigates the detection of such dark patterns.\nExisting solutions include UIGuard, which uses computer vision and natural\nlanguage processing, and approaches that categorize dark patterns based on\ndetectability or utilize machine learning models trained on datasets. We\npropose combining web scraping techniques with fine-tuned BERT language models\nand generative capabilities to identify dark patterns, including outliers. The\napproach scrapes textual content, feeds it into the BERT model for detection,\nand leverages BERT's bidirectional analysis and generation abilities. The study\nbuilds upon research on automatically detecting and explaining dark patterns,\naiming to raise awareness and protect consumers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.01608v1",
    "published_date": "2024-05-27 16:32:40 UTC",
    "updated_date": "2024-05-27 16:32:40 UTC"
  },
  {
    "arxiv_id": "2405.17324v1",
    "title": "Leveraging Offline Data in Linear Latent Bandits",
    "authors": [
      "Chinmaya Kausik",
      "Kevin Tan",
      "Ambuj Tewari"
    ],
    "abstract": "Sequential decision-making domains such as recommender systems, healthcare\nand education often have unobserved heterogeneity in the population that can be\nmodeled using latent bandits $-$ a framework where an unobserved latent state\ndetermines the model for a trajectory. While the latent bandit framework is\ncompelling, the extent of its generality is unclear. We first address this by\nestablishing a de Finetti theorem for decision processes, and show that\n$\\textit{every}$ exchangeable and coherent stateless decision process is a\nlatent bandit. The latent bandit framework lends itself particularly well to\nonline learning with offline datasets, a problem of growing interest in\nsequential decision-making. One can leverage offline latent bandit data to\nlearn a complex model for each latent state, so that an agent can simply learn\nthe latent state online to act optimally. We focus on a linear model for a\nlatent bandit with $d_A$-dimensional actions, where the latent states lie in an\nunknown $d_K$-dimensional subspace for $d_K \\ll d_A$. We present SOLD, a novel\nprincipled method to learn this subspace from short offline trajectories with\nguarantees. We then provide two methods to leverage this subspace online:\nLOCAL-UCB and ProBALL-UCB. We demonstrate that LOCAL-UCB enjoys $\\tilde\nO(\\min(d_A\\sqrt{T}, d_K\\sqrt{T}(1+\\sqrt{d_AT/d_KN})))$ regret guarantees, where\nthe effective dimension is lower when the size $N$ of the offline dataset is\nlarger. ProBALL-UCB enjoys a slightly weaker guarantee, but is more practical\nand computationally efficient. Finally, we establish the efficacy of our\nmethods using experiments on both synthetic data and real-life movie\nrecommendation data from MovieLens.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "40 pages. 14 pages for main paper, 26 pages for references + appendix",
    "pdf_url": "http://arxiv.org/pdf/2405.17324v1",
    "published_date": "2024-05-27 16:23:34 UTC",
    "updated_date": "2024-05-27 16:23:34 UTC"
  },
  {
    "arxiv_id": "2405.17293v1",
    "title": "Efficient Ensembles Improve Training Data Attribution",
    "authors": [
      "Junwei Deng",
      "Ting-Wei Li",
      "Shichang Zhang",
      "Jiaqi Ma"
    ],
    "abstract": "Training data attribution (TDA) methods aim to quantify the influence of\nindividual training data points on the model predictions, with broad\napplications in data-centric AI, such as mislabel detection, data selection,\nand copyright compensation. However, existing methods in this field, which can\nbe categorized as retraining-based and gradient-based, have struggled with the\ntrade-off between computational efficiency and attribution efficacy.\nRetraining-based methods can accurately attribute complex non-convex models but\nare computationally prohibitive, while gradient-based methods are efficient but\noften fail for non-convex models. Recent research has shown that augmenting\ngradient-based methods with ensembles of multiple independently trained models\ncan achieve significantly better attribution efficacy. However, this approach\nremains impractical for very large-scale applications.\n  In this work, we discover that expensive, fully independent training is\nunnecessary for ensembling the gradient-based methods, and we propose two\nefficient ensemble strategies, DROPOUT ENSEMBLE and LORA ENSEMBLE, alternative\nto naive independent ensemble. These strategies significantly reduce training\ntime (up to 80%), serving time (up to 60%), and space cost (up to 80%) while\nmaintaining similar attribution efficacy to the naive independent ensemble. Our\nextensive experimental results demonstrate that the proposed strategies are\neffective across multiple TDA methods on diverse datasets and models, including\ngenerative settings, significantly advancing the Pareto frontier of TDA methods\nwith better computational efficiency and attribution efficacy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17293v1",
    "published_date": "2024-05-27 15:58:34 UTC",
    "updated_date": "2024-05-27 15:58:34 UTC"
  },
  {
    "arxiv_id": "2405.17287v2",
    "title": "Opinion-Guided Reinforcement Learning",
    "authors": [
      "Kyanna Dagenais",
      "Istvan David"
    ],
    "abstract": "Human guidance is often desired in reinforcement learning to improve the\nperformance of the learning agent. However, human insights are often mere\nopinions and educated guesses rather than well-formulated arguments. While\nopinions are subject to uncertainty, e.g., due to partial informedness or\nignorance about a problem, they also emerge earlier than hard evidence can be\nproduced. Thus, guiding reinforcement learning agents by way of opinions offers\nthe potential for more performant learning processes, but comes with the\nchallenge of modeling and managing opinions in a formal way. In this article,\nwe present a method to guide reinforcement learning agents through opinions. To\nthis end, we provide an end-to-end method to model and manage advisors'\nopinions. To assess the utility of the approach, we evaluate it with synthetic\n(oracle) and human advisors, at different levels of uncertainty, and under\nmultiple advice strategies. Our results indicate that opinions, even if\nuncertain, improve the performance of reinforcement learning agents, resulting\nin higher rewards, more efficient exploration, and a better reinforced policy.\nAlthough we demonstrate our approach through a two-dimensional topological\nrunning example, our approach is applicable to complex problems with higher\ndimensions as well.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17287v2",
    "published_date": "2024-05-27 15:52:27 UTC",
    "updated_date": "2024-08-03 17:05:46 UTC"
  },
  {
    "arxiv_id": "2405.17284v2",
    "title": "An NLP Crosswalk Between the Common Core State Standards and NAEP Item Specifications",
    "authors": [
      "Gregory Camilli"
    ],
    "abstract": "Natural language processing (NLP) is rapidly developing for applications in\neducational assessment. In this paper, I describe an NLP-based procedure that\ncan be used to support subject matter experts in establishing a crosswalk\nbetween item specifications and content standards. This paper extends recent\nwork by proposing and demonstrating the use of multivariate similarity based on\nembedding vectors for sentences or texts. In particular, a hybrid regression\nprocedure is demonstrated for establishing the match of each content standard\nto multiple item specifications. The procedure is used to evaluate the match of\nthe Common Core State Standards (CCSS) for mathematics at grade 4 to the\ncorresponding item specifications for the 2026 National Assessment of\nEducational Progress (NAEP).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Deleted repeated sections. Corrected proper nouns. Corrected type in\n  CCSS sentences",
    "pdf_url": "http://arxiv.org/pdf/2405.17284v2",
    "published_date": "2024-05-27 15:47:46 UTC",
    "updated_date": "2024-05-31 21:30:44 UTC"
  },
  {
    "arxiv_id": "2405.17279v1",
    "title": "Socially-Aware Shared Control Navigation for Assistive Mobile Robots in the Built Environment",
    "authors": [
      "Yifan Xu",
      "Qianwei Wang",
      "Vineet Kamat",
      "Carol Menassa"
    ],
    "abstract": "As the number of Persons with Disabilities (PWD), particularly those with one\nor more physical impairments, increases, there is an increasing demand for\nassistive robotic technologies that can support independent mobility in the\nbuilt environment and reduce the burden on caregivers. Current assistive\nmobility platforms (e.g., robotic wheelchairs) often fail to incorporate user\npreferences and control, leading to reduced trust and efficiency. Existing\nshared control algorithms do not allow the incorporation of the user control\npreferences inside the navigation framework or the path planning algorithm. In\naddition, existing dynamic local planner algorithms for robotic wheelchairs do\nnot take into account the social spaces of people, potentially leading such\nplatforms to infringe upon these areas and cause discomfort. To address these\nconcerns, this work introduces a novel socially-aware shared autonomy-based\nnavigation system for assistive mobile robotic platforms.\n  Our navigation framework comprises a Global Planner and a Local Planner. To\nimplement the Global Planner, the proposed approach introduces a novel User\nPreference Field (UPF) theory within its global planning framework, explicitly\nacknowledging user preferences to adeptly navigate away from congested areas.\nFor the Local Planner, we propose a Socially-aware Shared Control-based Model\nPredictive Control with Dynamic Control Barrier Function (SS-MPC-DCBF) to\nadjust movements in real-time, integrating user preferences for safer, more\nautonomous navigation. Evaluation results show that our Global Planner aligns\nclosely with user preferences compared to baselines, and our Local Planner\ndemonstrates enhanced safety and efficiency in dynamic and static scenarios.\nThis integrated approach fosters trust and autonomy, crucial for the acceptance\nof assistive mobility technologies in the built environment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "42 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17279v1",
    "published_date": "2024-05-27 15:40:34 UTC",
    "updated_date": "2024-05-27 15:40:34 UTC"
  },
  {
    "arxiv_id": "2405.17527v3",
    "title": "Unisolver: PDE-Conditional Transformers Are Universal PDE Solvers",
    "authors": [
      "Hang Zhou",
      "Yuezhou Ma",
      "Haixu Wu",
      "Haowen Wang",
      "Mingsheng Long"
    ],
    "abstract": "Deep models have recently emerged as a promising tool to solve partial\ndifferential equations (PDEs), known as neural PDE solvers. While neural\nsolvers trained from either simulation data or physics-informed loss can solve\nPDEs reasonably well, they are mainly restricted to a few instances of PDEs,\ne.g. a certain equation with a limited set of coefficients. This limits the\ngeneralization of neural solvers to diverse PDEs, impeding them from being\npractical surrogate models for numerical solvers. In this paper, we present the\nUniversal PDE Solver (Unisolver) capable of solving a wide scope of PDEs by\ntraining a novel Transformer model on diverse data and conditioned on diverse\nPDEs. Instead of purely scaling up data and parameters, Unisolver stems from\nthe theoretical analysis of the PDE-solving process. Our key finding is that a\nPDE solution is fundamentally under the control of a series of PDE components,\ne.g. equation symbols, coefficients, and boundary conditions. Inspired by the\nmathematical structure of PDEs, we define a complete set of PDE components and\nflexibly embed them as domain-wise (e.g. equation symbols) and point-wise (e.g.\nboundaries) conditions for Transformer PDE solvers. Integrating physical\ninsights with recent Transformer advances, Unisolver achieves consistent\nstate-of-the-art results on three challenging large-scale benchmarks, showing\nimpressive performance gains and favorable PDE generalizability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17527v3",
    "published_date": "2024-05-27 15:34:35 UTC",
    "updated_date": "2024-10-08 03:36:24 UTC"
  },
  {
    "arxiv_id": "2405.17272v2",
    "title": "DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems",
    "authors": [
      "Zhi Zheng",
      "Shunyu Yao",
      "Zhenkun Wang",
      "Xialiang Tong",
      "Mingxuan Yuan",
      "Ke Tang"
    ],
    "abstract": "The min-max vehicle routing problem (min-max VRP) traverses all given\ncustomers by assigning several routes and aims to minimize the length of the\nlongest route. Recently, reinforcement learning (RL)-based sequential planning\nmethods have exhibited advantages in solving efficiency and optimality.\nHowever, these methods fail to exploit the problem-specific properties in\nlearning representations, resulting in less effective features for decoding\noptimal routes. This paper considers the sequential planning process of min-max\nVRPs as two coupled optimization tasks: customer partition for different routes\nand customer navigation in each route (i.e., partition and navigation). To\neffectively process min-max VRP instances, we present a novel attention-based\nPartition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings\nfor partition and navigation. Furthermore, we utilize an inherent symmetry in\ndecoding routes and develop an effective agent-permutation-symmetric (APS) loss\nfunction. Experimental results demonstrate that the proposed\nDecoupling-Partition-Navigation (DPN) method significantly surpasses existing\nlearning-based methods in both single-depot and multi-depot min-max VRPs. Our\ncode is available at",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17272v2",
    "published_date": "2024-05-27 15:33:16 UTC",
    "updated_date": "2024-06-06 10:30:24 UTC"
  },
  {
    "arxiv_id": "2405.17258v1",
    "title": "$\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning",
    "authors": [
      "Runqian Wang",
      "Soumya Ghosh",
      "David Cox",
      "Diego Antognini",
      "Aude Oliva",
      "Rogerio Feris",
      "Leonid Karlinsky"
    ],
    "abstract": "Low-rank adapters (LoRA) and their variants are popular parameter-efficient\nfine-tuning (PEFT) techniques that closely match full model fine-tune\nperformance while requiring only a small number of additional parameters. These\nadditional LoRA parameters are specific to the base model being adapted. When\nthe base model needs to be deprecated and replaced with a new one, all the\nassociated LoRA modules need to be re-trained. Such re-training requires access\nto the data used to train the LoRA for the original base model. This is\nespecially problematic for commercial cloud applications where the LoRA modules\nand the base models are hosted by service providers who may not be allowed to\nhost proprietary client task data. To address this challenge, we propose\n$\\textit{Trans-LoRA}$ -- a novel method for lossless, nearly data-free transfer\nof LoRAs across base models. Our approach relies on synthetic data to transfer\nLoRA modules. Using large language models, we design a synthetic data generator\nto approximate the data-generating process of the $\\textit{observed}$ task data\nsubset. Training on the resulting synthetic dataset transfers LoRA modules to\nnew models. We show the effectiveness of our approach using both LLama and\nGemma model families. Our approach achieves lossless (mostly improved) LoRA\ntransfer between models within and across different base model families, and\neven between different PEFT methods, on a wide variety of tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17258v1",
    "published_date": "2024-05-27 15:15:08 UTC",
    "updated_date": "2024-05-27 15:15:08 UTC"
  },
  {
    "arxiv_id": "2405.17253v1",
    "title": "Gaussian Embedding of Temporal Networks",
    "authors": [
      "Raphaël Romero",
      "Jefrey Lijffijt",
      "Riccardo Rastelli",
      "Marco Corneli",
      "Tijl De Bie"
    ],
    "abstract": "Representing the nodes of continuous-time temporal graphs in a\nlow-dimensional latent space has wide-ranging applications, from prediction to\nvisualization. Yet, analyzing continuous-time relational data with timestamped\ninteractions introduces unique challenges due to its sparsity. Merely embedding\nnodes as trajectories in the latent space overlooks this sparsity, emphasizing\nthe need to quantify uncertainty around the latent positions. In this paper, we\npropose TGNE (\\textbf{T}emporal \\textbf{G}aussian \\textbf{N}etwork\n\\textbf{E}mbedding), an innovative method that bridges two distinct strands of\nliterature: the statistical analysis of networks via Latent Space Models\n(LSM)\\cite{Hoff2002} and temporal graph machine learning. TGNE embeds nodes as\npiece-wise linear trajectories of Gaussian distributions in the latent space,\ncapturing both structural information and uncertainty around the trajectories.\nWe evaluate TGNE's effectiveness in reconstructing the original graph and\nmodelling uncertainty. The results demonstrate that TGNE generates competitive\ntime-varying embedding locations compared to common baselines for\nreconstructing unobserved edge interactions based on observed edges.\nFurthermore, the uncertainty estimates align with the time-varying degree\ndistribution in the network, providing valuable insights into the temporal\ndynamics of the graph. To facilitate reproducibility, we provide an open-source\nimplementation of TGNE at \\url{https://github.com/aida-ugent/tgne}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17253v1",
    "published_date": "2024-05-27 15:07:57 UTC",
    "updated_date": "2024-05-27 15:07:57 UTC"
  },
  {
    "arxiv_id": "2405.17249v2",
    "title": "Assessing LLMs Suitability for Knowledge Graph Completion",
    "authors": [
      "Vasile Ionut Remus Iga",
      "Gheorghe Cosmin Silaghi"
    ],
    "abstract": "Recent work has shown the capability of Large Language Models (LLMs) to solve\ntasks related to Knowledge Graphs, such as Knowledge Graph Completion, even in\nZero- or Few-Shot paradigms. However, they are known to hallucinate answers, or\noutput results in a non-deterministic manner, thus leading to wrongly reasoned\nresponses, even if they satisfy the user's demands. To highlight opportunities\nand challenges in knowledge graphs-related tasks, we experiment with three\ndistinguished LLMs, namely Mixtral-8x7b-Instruct-v0.1, GPT-3.5-Turbo-0125 and\nGPT-4o, on Knowledge Graph Completion for static knowledge graphs, using\nprompts constructed following the TELeR taxonomy, in Zero- and One-Shot\ncontexts, on a Task-Oriented Dialogue system use case. When evaluated using\nboth strict and flexible metrics measurement manners, our results show that\nLLMs could be fit for such a task if prompts encapsulate sufficient information\nand relevant examples.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at 18th International Conference on Neural-Symbolic Learning\n  and Reasoning, NESY 2024. Evaluating Mixtral-8x7b-Instruct-v0.1,\n  GPT-3.5-Turbo-0125 and GPT-4o for Knowledge Graph Completion task with\n  prompts formatted according to the TELeR taxonomy",
    "pdf_url": "http://arxiv.org/pdf/2405.17249v2",
    "published_date": "2024-05-27 15:04:50 UTC",
    "updated_date": "2024-07-18 09:48:02 UTC"
  },
  {
    "arxiv_id": "2405.17245v1",
    "title": "Galaxy: A Resource-Efficient Collaborative Edge AI System for In-situ Transformer Inference",
    "authors": [
      "Shengyuan Ye",
      "Jiangsu Du",
      "Liekang Zeng",
      "Wenzhong Ou",
      "Xiaowen Chu",
      "Yutong Lu",
      "Xu Chen"
    ],
    "abstract": "Transformer-based models have unlocked a plethora of powerful intelligent\napplications at the edge, such as voice assistant in smart home. Traditional\ndeployment approaches offload the inference workloads to the remote cloud\nserver, which would induce substantial pressure on the backbone network as well\nas raise users' privacy concerns. To address that, in-situ inference has been\nrecently recognized for edge intelligence, but it still confronts significant\nchallenges stemming from the conflict between intensive workloads and limited\non-device computing resources. In this paper, we leverage our observation that\nmany edge environments usually comprise a rich set of accompanying trusted edge\ndevices with idle resources and propose Galaxy, a collaborative edge AI system\nthat breaks the resource walls across heterogeneous edge devices for efficient\nTransformer inference acceleration. Galaxy introduces a novel hybrid model\nparallelism to orchestrate collaborative inference, along with a\nheterogeneity-aware parallelism planning for fully exploiting the resource\npotential. Furthermore, Galaxy devises a tile-based fine-grained overlapping of\ncommunication and computation to mitigate the impact of tensor synchronizations\non inference latency under bandwidth-constrained edge environments. Extensive\nevaluation based on prototype implementation demonstrates that Galaxy\nremarkably outperforms state-of-the-art approaches under various edge\nenvironment setups, achieving up to 2.5x end-to-end latency reduction.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by IEEE International Conference on Computer Communications\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17245v1",
    "published_date": "2024-05-27 15:01:04 UTC",
    "updated_date": "2024-05-27 15:01:04 UTC"
  },
  {
    "arxiv_id": "2405.17243v2",
    "title": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning",
    "authors": [
      "Adriana Hugessen",
      "Roger Creus Castanyer",
      "Faisal Mohamed",
      "Glen Berseth"
    ],
    "abstract": "Both entropy-minimizing and entropy-maximizing (curiosity) objectives for\nunsupervised reinforcement learning (RL) have been shown to be effective in\ndifferent environments, depending on the environment's level of natural\nentropy. However, neither method alone results in an agent that will\nconsistently learn intelligent behavior across environments. In an effort to\nfind a single entropy-based method that will encourage emergent behaviors in\nany environment, we propose an agent that can adapt its objective online,\ndepending on the entropy conditions by framing the choice as a multi-armed\nbandit problem. We devise a novel intrinsic feedback signal for the bandit,\nwhich captures the agent's ability to control the entropy in its environment.\nWe demonstrate that such agents can learn to control entropy and exhibit\nemergent behaviors in both high- and low-entropy regimes and can learn skillful\nbehaviors in benchmark tasks. Videos of the trained agents and summarized\nfindings can be found on our project page\nhttps://sites.google.com/view/surprise-adaptive-agents",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at the Reinforcement Learning Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17243v2",
    "published_date": "2024-05-27 14:58:24 UTC",
    "updated_date": "2024-08-16 17:55:32 UTC"
  },
  {
    "arxiv_id": "2405.17234v6",
    "title": "Benchmarking General-Purpose In-Context Learning",
    "authors": [
      "Fan Wang",
      "Chuan Lin",
      "Yang Cao",
      "Yu Kang"
    ],
    "abstract": "In-context learning (ICL) empowers generative models to address new tasks\neffectively and efficiently on the fly, without relying on any artificially\ncrafted optimization techniques. In this paper, we study extending ICL to\naddress a broader range of tasks with an extended learning horizon and higher\nimprovement potential, namely General Purpose In-Context Learning (GPICL). To\nthis end, we introduce two lightweight benchmarks specifically crafted to train\nand evaluate GPICL functionalities. Each benchmark encompasses a vast number of\ntasks characterized by significant task variance. These tasks are also crafted\nto promote long-horizon in-context learning through continuous generation and\ninteraction, covering domains such as language modeling, decision-making, and\nworld modeling. The benchmarks necessitate the models to leverage contexts and\nhistory interactions to enhance their capabilities, which we believe to be the\nkey characteristics of GPICL. Our experiments indicate that the diversity of\ntraining tasks is positively correlated with the ability to generalize with\nICL, but inversely correlated with zero-shot capabilities. Additionally, our\nfindings indicate that the scale of parameters alone may not be crucial for ICL\nor GPICL, suggesting alternative approaches such as increasing the scale of\ncontexts and memory states.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17234v6",
    "published_date": "2024-05-27 14:50:42 UTC",
    "updated_date": "2024-09-12 15:22:09 UTC"
  },
  {
    "arxiv_id": "2406.00038v1",
    "title": "ViSpeR: Multilingual Audio-Visual Speech Recognition",
    "authors": [
      "Sanath Narayan",
      "Yasser Abdelaziz Dahou Djilali",
      "Ankit Singh",
      "Eustache Le Bihan",
      "Hakim Hacid"
    ],
    "abstract": "This work presents an extensive and detailed study on Audio-Visual Speech\nRecognition (AVSR) for five widely spoken languages: Chinese, Spanish, English,\nArabic, and French. We have collected large-scale datasets for each language\nexcept for English, and have engaged in the training of supervised learning\nmodels. Our model, ViSpeR, is trained in a multi-lingual setting, resulting in\ncompetitive performance on newly established benchmarks for each language. The\ndatasets and models are released to the community with an aim to serve as a\nfoundation for triggering and feeding further research work and exploration on\nAudio-Visual Speech Recognition, an increasingly important area of research.\nCode available at\n\\href{https://github.com/YasserdahouML/visper}{https://github.com/YasserdahouML/visper}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00038v1",
    "published_date": "2024-05-27 14:48:51 UTC",
    "updated_date": "2024-05-27 14:48:51 UTC"
  },
  {
    "arxiv_id": "2405.17216v1",
    "title": "Autoformalizing Euclidean Geometry",
    "authors": [
      "Logan Murphy",
      "Kaiyu Yang",
      "Jialiang Sun",
      "Zhaoyu Li",
      "Anima Anandkumar",
      "Xujie Si"
    ],
    "abstract": "Autoformalization involves automatically translating informal math into\nformal theorems and proofs that are machine-verifiable. Euclidean geometry\nprovides an interesting and controllable domain for studying autoformalization.\nIn this paper, we introduce a neuro-symbolic framework for autoformalizing\nEuclidean geometry, which combines domain knowledge, SMT solvers, and large\nlanguage models (LLMs). One challenge in Euclidean geometry is that informal\nproofs rely on diagrams, leaving gaps in texts that are hard to formalize. To\naddress this issue, we use theorem provers to fill in such diagrammatic\ninformation automatically, so that the LLM only needs to autoformalize the\nexplicit textual steps, making it easier for the model. We also provide\nautomatic semantic evaluation for autoformalized theorem statements. We\nconstruct LeanEuclid, an autoformalization benchmark consisting of problems\nfrom Euclid's Elements and the UniGeo dataset formalized in the Lean proof\nassistant. Experiments with GPT-4 and GPT-4V show the capability and\nlimitations of state-of-the-art LLMs on autoformalizing geometry problems. The\ndata and code are available at https://github.com/loganrjmurphy/LeanEuclid.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICML 2024. The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2405.17216v1",
    "published_date": "2024-05-27 14:35:10 UTC",
    "updated_date": "2024-05-27 14:35:10 UTC"
  },
  {
    "arxiv_id": "2405.17202v3",
    "title": "Efficient multi-prompt evaluation of LLMs",
    "authors": [
      "Felipe Maia Polo",
      "Ronald Xu",
      "Lucas Weber",
      "Mírian Silva",
      "Onkar Bhardwaj",
      "Leshem Choshen",
      "Allysson Flavio Melo de Oliveira",
      "Yuekai Sun",
      "Mikhail Yurochkin"
    ],
    "abstract": "Most popular benchmarks for comparing LLMs rely on a limited set of prompt\ntemplates, which may not fully capture the LLMs' abilities and can affect the\nreproducibility of results on leaderboards. Many recent works empirically\nverify prompt sensitivity and advocate for changes in LLM evaluation. In this\npaper, we consider the problem of estimating the performance distribution\nacross many prompt variants instead of finding a single prompt to evaluate\nwith. We introduce PromptEval, a method for estimating performance across a\nlarge set of prompts borrowing strength across prompts and examples to produce\naccurate estimates under practical evaluation budgets. The resulting\ndistribution can be used to obtain performance quantiles to construct various\nrobust performance metrics (e.g., top 95% quantile or median). We prove that\nPromptEval consistently estimates the performance distribution and demonstrate\nits efficacy empirically on three prominent LLM benchmarks: MMLU, BIG-bench\nHard, and LMentry; for example, PromptEval can accurately estimate performance\nquantiles across 100 prompt templates on MMLU with a budget equivalent to two\nsingle-prompt evaluations. Moreover, we show how PromptEval can be useful in\nLLM-as-a-judge and best prompt identification applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17202v3",
    "published_date": "2024-05-27 14:24:47 UTC",
    "updated_date": "2024-10-31 03:26:21 UTC"
  },
  {
    "arxiv_id": "2406.00037v1",
    "title": "Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering",
    "authors": [
      "Hongyu Yang",
      "Liyang He",
      "Min Hou",
      "Shuanghong Shen",
      "Rui Li",
      "Jiahui Hou",
      "Jianhui Ma",
      "Junda Zhao"
    ],
    "abstract": "Code Community Question Answering (CCQA) seeks to tackle programming-related\nissues, thereby boosting productivity in both software engineering and academic\nresearch. Recent advancements in Reinforcement Learning from Human Feedback\n(RLHF) have transformed the fine-tuning process of Large Language Models (LLMs)\nto produce responses that closely mimic human behavior. Leveraging LLMs with\nRLHF for practical CCQA applications has thus emerged as a promising area of\nstudy. Unlike standard code question-answering tasks, CCQA involves multiple\npossible answers, with varying user preferences for each response.\nAdditionally, code communities often show a preference for new APIs. These\nchallenges prevent LLMs from generating responses that cater to the diverse\npreferences of users in CCQA tasks. To address these issues, we propose a novel\nframework called Aligning LLMs through Multi-perspective User Preference\nRanking-based Feedback for Programming Question Answering (ALMupQA) to create\nuser-focused responses. Our approach starts with Multi-perspective Preference\nRanking Alignment (MPRA), which synthesizes varied user preferences based on\nthe characteristics of answers from code communities. We then introduce a\nRetrieval-augmented In-context Learning (RIL) module to mitigate the problem of\noutdated answers by retrieving responses to similar questions from a question\nbank. Due to the limited availability of high-quality, multi-answer CCQA\ndatasets, we also developed a dataset named StaCCQA from real code communities.\nExtensive experiments demonstrated the effectiveness of the ALMupQA framework\nin terms of accuracy and user preference. Compared to the base model, ALMupQA\nshowed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in\nBERTScore and CodeBERTScore, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00037v1",
    "published_date": "2024-05-27 14:21:31 UTC",
    "updated_date": "2024-05-27 14:21:31 UTC"
  },
  {
    "arxiv_id": "2405.17187v2",
    "title": "Memorize What Matters: Emergent Scene Decomposition from Multitraverse",
    "authors": [
      "Yiming Li",
      "Zehong Wang",
      "Yue Wang",
      "Zhiding Yu",
      "Zan Gojcic",
      "Marco Pavone",
      "Chen Feng",
      "Jose M. Alvarez"
    ],
    "abstract": "Humans naturally retain memories of permanent elements, while ephemeral\nmoments often slip through the cracks of memory. This selective retention is\ncrucial for robotic perception, localization, and mapping. To endow robots with\nthis capability, we introduce 3D Gaussian Mapping (3DGM), a self-supervised,\ncamera-only offline mapping framework grounded in 3D Gaussian Splatting. 3DGM\nconverts multitraverse RGB videos from the same region into a Gaussian-based\nenvironmental map while concurrently performing 2D ephemeral object\nsegmentation. Our key observation is that the environment remains consistent\nacross traversals, while objects frequently change. This allows us to exploit\nself-supervision from repeated traversals to achieve environment-object\ndecomposition. More specifically, 3DGM formulates multitraverse environmental\nmapping as a robust differentiable rendering problem, treating pixels of the\nenvironment and objects as inliers and outliers, respectively. Using robust\nfeature distillation, feature residuals mining, and robust optimization, 3DGM\njointly performs 2D segmentation and 3D mapping without human intervention. We\nbuild the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets,\nto evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and\nneural rendering. Extensive results verify the effectiveness and potential of\nour method for self-driving and robotics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://3d-gaussian-mapping.github.io; Code and data:\n  https://github.com/NVlabs/3DGM",
    "pdf_url": "http://arxiv.org/pdf/2405.17187v2",
    "published_date": "2024-05-27 14:11:17 UTC",
    "updated_date": "2024-05-29 23:32:23 UTC"
  },
  {
    "arxiv_id": "2405.17182v1",
    "title": "Exploring the Performance of Continuous-Time Dynamic Link Prediction Algorithms",
    "authors": [
      "Raphaël Romero",
      "Maarten Buyl",
      "Tijl De Bie",
      "Jefrey Lijffijt"
    ],
    "abstract": "Dynamic Link Prediction (DLP) addresses the prediction of future links in\nevolving networks. However, accurately portraying the performance of DLP\nalgorithms poses challenges that might impede progress in the field.\nImportantly, common evaluation pipelines usually calculate ranking or binary\nclassification metrics, where the scores of observed interactions (positives)\nare compared with those of randomly generated ones (negatives). However, a\nsingle metric is not sufficient to fully capture the differences between DLP\nalgorithms, and is prone to overly optimistic performance evaluation. Instead,\nan in-depth evaluation should reflect performance variations across different\nnodes, edges, and time segments. In this work, we contribute tools to perform\nsuch a comprehensive evaluation. (1) We propose Birth-Death diagrams, a simple\nbut powerful visualization technique that illustrates the effect of time-based\ntrain-test splitting on the difficulty of DLP on a given dataset. (2) We\ndescribe an exhaustive taxonomy of negative sampling methods that can be used\nat evaluation time. (3) We carry out an empirical study of the effect of the\ndifferent negative sampling strategies. Our comparison between heuristics and\nstate-of-the-art memory-based methods on various real-world datasets confirms a\nstrong effect of using different negative sampling strategies on the test Area\nUnder the Curve (AUC). Moreover, we conduct a visual exploration of the\nprediction, with additional insights on which different types of errors are\nprominent over time.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17182v1",
    "published_date": "2024-05-27 14:03:28 UTC",
    "updated_date": "2024-05-27 14:03:28 UTC"
  },
  {
    "arxiv_id": "2405.17176v1",
    "title": "DreamMat: High-quality PBR Material Generation with Geometry- and Light-aware Diffusion Models",
    "authors": [
      "Yuqing Zhang",
      "Yuan Liu",
      "Zhiyu Xie",
      "Lei Yang",
      "Zhongyuan Liu",
      "Mengzhou Yang",
      "Runze Zhang",
      "Qilong Kou",
      "Cheng Lin",
      "Wenping Wang",
      "Xiaogang Jin"
    ],
    "abstract": "2D diffusion model, which often contains unwanted baked-in shading effects\nand results in unrealistic rendering effects in the downstream applications.\nGenerating Physically Based Rendering (PBR) materials instead of just RGB\ntextures would be a promising solution. However, directly distilling the PBR\nmaterial parameters from 2D diffusion models still suffers from incorrect\nmaterial decomposition, such as baked-in shading effects in albedo. We\nintroduce DreamMat, an innovative approach to resolve the aforementioned\nproblem, to generate high-quality PBR materials from text descriptions. We find\nout that the main reason for the incorrect material distillation is that\nlarge-scale 2D diffusion models are only trained to generate final shading\ncolors, resulting in insufficient constraints on material decomposition during\ndistillation. To tackle this problem, we first finetune a new light-aware 2D\ndiffusion model to condition on a given lighting environment and generate the\nshading results on this specific lighting condition. Then, by applying the same\nenvironment lights in the material distillation, DreamMat can generate\nhigh-quality PBR materials that are not only consistent with the given geometry\nbut also free from any baked-in shading effects in albedo. Extensive\nexperiments demonstrate that the materials produced through our methods exhibit\ngreater visual appeal to users and achieve significantly superior rendering\nquality compared to baseline methods, which are preferable for downstream tasks\nsuch as game and film production.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted to SIGGRAPH 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17176v1",
    "published_date": "2024-05-27 13:55:08 UTC",
    "updated_date": "2024-05-27 13:55:08 UTC"
  },
  {
    "arxiv_id": "2407.01570v1",
    "title": "Ego-Foresight: Agent Visuomotor Prediction as Regularization for RL",
    "authors": [
      "Manuel S. Nunes",
      "Atabak Dehban",
      "Yiannis Demiris",
      "José Santos-Victor"
    ],
    "abstract": "Despite the significant advancements in Deep Reinforcement Learning (RL)\nobserved in the last decade, the amount of training experience necessary to\nlearn effective policies remains one of the primary concerns both in simulated\nand real environments. Looking to solve this issue, previous work has shown\nthat improved training efficiency can be achieved by separately modeling agent\nand environment, but usually requiring a supervisory agent mask. In contrast to\nRL, humans can perfect a new skill from a very small number of trials and in\nmost cases do so without a supervisory signal, making neuroscientific studies\nof human development a valuable source of inspiration for RL. In particular, we\nexplore the idea of motor prediction, which states that humans develop an\ninternal model of themselves and of the consequences that their motor commands\nhave on the immediate sensory inputs. Our insight is that the movement of the\nagent provides a cue that allows the duality between agent and environment to\nbe learned. To instantiate this idea, we present Ego-Foresight, a\nself-supervised method for disentangling agent and environment based on motion\nand prediction. Our main finding is that visuomotor prediction of the agent\nprovides regularization to the RL algorithm, by encouraging the actions to stay\nwithin predictable bounds. To test our approach, we first study the ability of\nour model to visually predict agent movement irrespective of the environment,\nin real-world robotic interactions. Then, we integrate Ego-Foresight with a\nmodel-free RL algorithm to solve simulated robotic manipulation tasks, showing\nan average improvement of 23% in efficiency and 8% in performance.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "9 pages, 9 figures, 3 tables, conference",
    "pdf_url": "http://arxiv.org/pdf/2407.01570v1",
    "published_date": "2024-05-27 13:32:43 UTC",
    "updated_date": "2024-05-27 13:32:43 UTC"
  },
  {
    "arxiv_id": "2405.17152v3",
    "title": "CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control",
    "authors": [
      "Jingqing Ruan",
      "Ziyue Li",
      "Hua Wei",
      "Haoyuan Jiang",
      "Jiaming Lu",
      "Xuantang Xiong",
      "Hangyu Mao",
      "Rui Zhao"
    ],
    "abstract": "Effective multi-intersection collaboration is pivotal for\nreinforcement-learning-based traffic signal control to alleviate congestion.\nExisting work mainly chooses neighboring intersections as collaborators.\nHowever, quite an amount of congestion, even some wide-range congestion, is\ncaused by non-neighbors failing to collaborate. To address these issues, we\npropose to separate the collaborator selection as a second policy to be\nlearned, concurrently being updated with the original signal-controlling\npolicy. Specifically, the selection policy in real-time adaptively selects the\nbest teammates according to phase- and intersection-level features. Empirical\nresults on both synthetic and real-world datasets provide robust validation for\nthe superiority of our approach, offering significant improvements over\nexisting state-of-the-art methods. The code is available at\nhttps://github.com/bonaldli/CoSLight.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17152v3",
    "published_date": "2024-05-27 13:26:59 UTC",
    "updated_date": "2024-06-19 10:07:02 UTC"
  },
  {
    "arxiv_id": "2405.17139v2",
    "title": "Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling",
    "authors": [
      "Cristian Rodriguez-Opazo",
      "Ehsan Abbasnejad",
      "Damien Teney",
      "Hamed Damirchi",
      "Edison Marrese-Taylor",
      "Anton van den Hengel"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) stands out as a prominent\nmethod for image representation learning. Various architectures, from vision\ntransformers (ViTs) to convolutional networks (ResNets) have been trained with\nCLIP to serve as general solutions to diverse vision tasks. This paper explores\nthe differences across various CLIP-trained vision backbones. Despite using the\nsame data and training objective, we find that these architectures have notably\ndifferent representations, different classification performance across\ndatasets, and different robustness properties to certain types of image\nperturbations. Our findings indicate a remarkable possible synergy across\nbackbones by leveraging their respective strengths. In principle,\nclassification accuracy could be improved by over 40 percentage with an\ninformed selection of the optimal backbone per test example.Using this insight,\nwe develop a straightforward yet powerful approach to adaptively ensemble\nmultiple backbones. The approach uses as few as one labeled example per class\nto tune the adaptive combination of backbones. On a large collection of\ndatasets, the method achieves a remarkable increase in accuracy of up to 39.1%\nover the best single backbone, well beyond traditional ensembles",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025. arXiv admin note: text overlap with arXiv:2312.14400",
    "pdf_url": "http://arxiv.org/pdf/2405.17139v2",
    "published_date": "2024-05-27 12:59:35 UTC",
    "updated_date": "2025-02-16 08:25:02 UTC"
  },
  {
    "arxiv_id": "2405.17523v2",
    "title": "Locally Testing Model Detections for Semantic Global Concepts",
    "authors": [
      "Franz Motzkus",
      "Georgii Mikriukov",
      "Christian Hellert",
      "Ute Schmid"
    ],
    "abstract": "Ensuring the quality of black-box Deep Neural Networks (DNNs) has become ever\nmore significant, especially in safety-critical domains such as automated\ndriving. While global concept encodings generally enable a user to test a model\nfor a specific concept, linking global concept encodings to the local\nprocessing of single network inputs reveals their strengths and limitations.\nOur proposed framework global-to-local Concept Attribution (glCA) uses\napproaches from local (why a specific prediction originates) and global (how a\nmodel works generally) eXplainable Artificial Intelligence (xAI) to test DNNs\nfor a predefined semantical concept locally. The approach allows for\nconditioning local, post-hoc explanations on predefined semantic concepts\nencoded as linear directions in the model's latent space. Pixel-exact scoring\nconcerning the global concept usage assists the tester in further understanding\nthe model processing of single data points for the selected concept. Our\napproach has the advantage of fully covering the model-internal encoding of the\nsemantic concept and allowing the localization of relevant concept-related\ninformation. The results show major differences in the local perception and\nusage of individual global concept encodings and demand for further\ninvestigations regarding obtaining thorough semantic concept encodings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17523v2",
    "published_date": "2024-05-27 12:52:45 UTC",
    "updated_date": "2024-05-29 07:40:40 UTC"
  },
  {
    "arxiv_id": "2405.17129v2",
    "title": "TEII: Think, Explain, Interact and Iterate with Large Language Models to Solve Cross-lingual Emotion Detection",
    "authors": [
      "Long Cheng",
      "Qihao Shao",
      "Christine Zhao",
      "Sheng Bi",
      "Gina-Anne Levow"
    ],
    "abstract": "Cross-lingual emotion detection allows us to analyze global trends, public\nopinion, and social phenomena at scale. We participated in the Explainability\nof Cross-lingual Emotion Detection (EXALT) shared task, achieving an F1-score\nof 0.6046 on the evaluation set for the emotion detection sub-task. Our system\noutperformed the baseline by more than 0.16 F1-score absolute, and ranked\nsecond amongst competing systems. We conducted experiments using fine-tuning,\nzero-shot learning, and few-shot learning for Large Language Model (LLM)-based\nmodels as well as embedding-based BiLSTM and KNN for non-LLM-based techniques.\nAdditionally, we introduced two novel methods: the Multi-Iteration Agentic\nWorkflow and the Multi-Binary-Classifier Agentic Workflow. We found that\nLLM-based approaches provided good performance on multilingual emotion\ndetection. Furthermore, ensembles combining all our experimented models yielded\nhigher F1-scores than any single approach alone.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of the 13th Workshop on Computational Approaches to\n  Subjectivity, Sentiment, & Social Media Analysis (ACL 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.17129v2",
    "published_date": "2024-05-27 12:47:40 UTC",
    "updated_date": "2024-07-02 12:18:51 UTC"
  },
  {
    "arxiv_id": "2405.17116v1",
    "title": "Mixtures of Unsupervised Lexicon Classification",
    "authors": [
      "Peratham Wiriyathammabhum"
    ],
    "abstract": "This paper presents a mixture version of the method-of-moment unsupervised\nlexicon classification by an incorporation of a Dirichlet process.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "A draft on lexicon classification unsupervised learning. It shows\n  that aggregating lexicon scores is equivalent to a finite mixture of\n  multinomial Naive Bayes models. A very preliminary work of a few days\n  man-hours, like a weekly report/note, but might be useful",
    "pdf_url": "http://arxiv.org/pdf/2405.17116v1",
    "published_date": "2024-05-27 12:33:47 UTC",
    "updated_date": "2024-05-27 12:33:47 UTC"
  },
  {
    "arxiv_id": "2405.17110v1",
    "title": "Superpixelwise Low-rank Approximation based Partial Label Learning for Hyperspectral Image Classification",
    "authors": [
      "Shujun Yang",
      "Yu Zhang",
      "Yao Ding",
      "Danfeng Hong"
    ],
    "abstract": "Insufficient prior knowledge of a captured hyperspectral image (HSI) scene\nmay lead the experts or the automatic labeling systems to offer incorrect\nlabels or ambiguous labels (i.e., assigning each training sample to a group of\ncandidate labels, among which only one of them is valid; this is also known as\npartial label learning) during the labeling process. Accordingly, how to learn\nfrom such data with ambiguous labels is a problem of great practical\nimportance. In this paper, we propose a novel superpixelwise low-rank\napproximation (LRA)-based partial label learning method, namely SLAP, which is\nthe first to take into account partial label learning in HSI classification.\nSLAP is mainly composed of two phases: disambiguating the training labels and\nacquiring the predictive model. Specifically, in the first phase, we propose a\nsuperpixelwise LRA-based model, preparing the affinity graph for the subsequent\nlabel propagation process while extracting the discriminative representation to\nenhance the following classification task of the second phase. Then to\ndisambiguate the training labels, label propagation propagates the labeling\ninformation via the affinity graph of training pixels. In the second phase, we\ntake advantage of the resulting disambiguated training labels and the\ndiscriminative representations to enhance the classification performance. The\nextensive experiments validate the advantage of the proposed SLAP method over\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17110v1",
    "published_date": "2024-05-27 12:26:49 UTC",
    "updated_date": "2024-05-27 12:26:49 UTC"
  },
  {
    "arxiv_id": "2405.17104v2",
    "title": "LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding",
    "authors": [
      "Haoyu Zhao",
      "Wenhang Ge",
      "Ying-cong Chen"
    ],
    "abstract": "Visual grounding is an essential tool that links user-provided text queries\nwith query-specific regions within an image. Despite advancements in visual\ngrounding models, their ability to comprehend complex queries remains limited.\nTo overcome this limitation, we introduce LLM-Optic, an innovative method that\nutilizes Large Language Models (LLMs) as an optical lens to enhance existing\nvisual grounding models in comprehending complex text queries involving\nintricate text structures, multiple objects, or object spatial relationships,\nsituations that current models struggle with. LLM-Optic first employs an LLM as\na Text Grounder to interpret complex text queries and accurately identify\nobjects the user intends to locate. Then a pre-trained visual grounding model\nis used to generate candidate bounding boxes given the refined query by the\nText Grounder. After that, LLM-Optic annotates the candidate bounding boxes\nwith numerical marks to establish a connection between text and specific image\nregions, thereby linking two distinct modalities. Finally, it employs a Large\nMultimodal Model (LMM) as a Visual Grounder to select the marked candidate\nobjects that best correspond to the original text query. Through LLM-Optic, we\nhave achieved universal visual grounding, which allows for the detection of\narbitrary objects specified by arbitrary human language input. Importantly, our\nmethod achieves this enhancement without requiring additional training or\nfine-tuning. Extensive experiments across various challenging benchmarks\ndemonstrate that LLM-Optic achieves state-of-the-art zero-shot visual grounding\ncapabilities. Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://haoyu-zhao.github.io/LLM-Optic.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.17104v2",
    "published_date": "2024-05-27 12:23:08 UTC",
    "updated_date": "2024-05-28 02:17:56 UTC"
  },
  {
    "arxiv_id": "2405.17103v2",
    "title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens",
    "authors": [
      "Houxing Ren",
      "Mingjie Zhan",
      "Zhongyuan Wu",
      "Hongsheng Li"
    ],
    "abstract": "In infilling tasks, sub-tokens, representing instances where a complete token\nis segmented into two parts, often emerge at the boundaries of prefixes,\nmiddles, and suffixes. Traditional methods focused on training models at the\ntoken level, leading to sub-optimal performance in character-level infilling\ntasks during the inference stage. Alternately, some approaches considered\ncharacter-level infilling, but they relied on predicting sub-tokens in\ninference, yet this strategy diminished ability in character-level infilling\ntasks due to the large perplexity of the model on sub-tokens. In this paper, we\nintroduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and\nEnding character constraints. The proposed method addresses character-level\ninfilling tasks by utilizing a line-level format to avoid predicting any\nsub-token in inference. In addition, we incorporate two special tokens to\nsignify the rest of the incomplete lines, thereby enhancing generation\nguidance. Extensive experiments demonstrate that our proposed approach\nsurpasses previous methods, offering a significant advantage. Code is available\nat https://github.com/SenseLLM/FIM-SE.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 (main conference)",
    "pdf_url": "http://arxiv.org/pdf/2405.17103v2",
    "published_date": "2024-05-27 12:21:48 UTC",
    "updated_date": "2024-06-14 09:26:41 UTC"
  },
  {
    "arxiv_id": "2405.17097v2",
    "title": "A Comparative Study on Multi-task Uncertainty Quantification in Semantic Segmentation and Monocular Depth Estimation",
    "authors": [
      "Steven Landgraf",
      "Markus Hillemann",
      "Theodor Kapler",
      "Markus Ulrich"
    ],
    "abstract": "Deep neural networks excel in perception tasks such as semantic segmentation\nand monocular depth estimation, making them indispensable in safety-critical\napplications like autonomous driving and industrial inspection. However, they\noften suffer from overconfidence and poor explainability, especially for\nout-of-domain data. While uncertainty quantification has emerged as a promising\nsolution to these challenges, multi-task settings have yet to be explored. In\nan effort to shed light on this, we evaluate Monte Carlo Dropout, Deep\nSub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular\ndepth estimation. Thereby, we reveal that Deep Ensembles stand out as the\npreferred choice, particularly in out-of-domain scenarios, and show the\npotential benefit of multi-task learning with regard to the uncertainty quality\nin comparison to solving both tasks separately. Additionally, we highlight the\nimpact of employing different uncertainty thresholds to classify pixels as\ncertain or uncertain, with the median uncertainty emerging as a robust default.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This manuscript is an extended version of a previously published\n  conference paper and is currently in review for a journal",
    "pdf_url": "http://arxiv.org/pdf/2405.17097v2",
    "published_date": "2024-05-27 12:12:26 UTC",
    "updated_date": "2025-01-16 16:27:33 UTC"
  },
  {
    "arxiv_id": "2405.17088v1",
    "title": "Phase Transitions in the Output Distribution of Large Language Models",
    "authors": [
      "Julian Arnold",
      "Flemming Holtorf",
      "Frank Schäfer",
      "Niels Lörch"
    ],
    "abstract": "In a physical system, changing parameters such as temperature can induce a\nphase transition: an abrupt change from one state of matter to another.\nAnalogous phenomena have recently been observed in large language models.\nTypically, the task of identifying phase transitions requires human analysis\nand some prior understanding of the system to narrow down which low-dimensional\nproperties to monitor and analyze. Statistical methods for the automated\ndetection of phase transitions from data have recently been proposed within the\nphysics community. These methods are largely system agnostic and, as shown\nhere, can be adapted to study the behavior of large language models. In\nparticular, we quantify distributional changes in the generated output via\nstatistical distances, which can be efficiently estimated with access to the\nprobability distribution over next-tokens. This versatile approach is capable\nof discovering new phases of behavior and unexplored transitions -- an ability\nthat is particularly exciting in light of the rapid development of language\nmodels and their emergent capabilities.",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17088v1",
    "published_date": "2024-05-27 12:04:36 UTC",
    "updated_date": "2024-05-27 12:04:36 UTC"
  },
  {
    "arxiv_id": "2407.11978v1",
    "title": "\"It depends\": Configuring AI to Improve Clinical Usefulness Across Contexts",
    "authors": [
      "Hubert D. Zając",
      "Jorge M. N. Ribeiro",
      "Silvia Ingala",
      "Simona Gentile",
      "Ruth Wanjohi",
      "Samuel N. Gitau",
      "Jonathan F. Carlsen",
      "Michael B. Nielsen",
      "Tariq O. Andersen"
    ],
    "abstract": "Artificial Intelligence (AI) repeatedly match or outperform radiologists in\nlab experiments. However, real-world implementations of radiological AI-based\nsystems are found to provide little to no clinical value. This paper explores\nhow to design AI for clinical usefulness in different contexts. We conducted 19\ndesign sessions and design interventions with 13 radiologists from 7 clinical\nsites in Denmark and Kenya, based on three iterations of a functional AI-based\nprototype. Ten sociotechnical dependencies were identified as crucial for the\ndesign of AI in radiology. We conceptualised four technical dimensions that\nmust be configured to the intended clinical context of use: AI functionality,\nAI medical focus, AI decision threshold, and AI Explainability. We present four\ndesign recommendations on how to address dependencies pertaining to the medical\nknowledge, clinic type, user expertise level, patient context, and user\nsituation that condition the configuration of these technical dimensions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.11978v1",
    "published_date": "2024-05-27 11:49:05 UTC",
    "updated_date": "2024-05-27 11:49:05 UTC"
  },
  {
    "arxiv_id": "2405.17076v1",
    "title": "Leveraging small language models for Text2SPARQL tasks to improve the resilience of AI assistance",
    "authors": [
      "Felix Brei",
      "Johannes Frey",
      "Lars-Peter Meyer"
    ],
    "abstract": "In this work we will show that language models with less than one billion\nparameters can be used to translate natural language to SPARQL queries after\nfine-tuning. Using three different datasets ranging from academic to real\nworld, we identify prerequisites that the training data must fulfill in order\nfor the training to be successful. The goal is to empower users of semantic web\ntechnology to use AI assistance with affordable commodity hardware, making them\nmore resilient against external factors.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in Proceedings of the Workshop on Linked Data-driven\n  Resilience Research 2024 (D2R2) co-located with Extended Semantic Web\n  Conference 2024 (ESWC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2405.17076v1",
    "published_date": "2024-05-27 11:47:21 UTC",
    "updated_date": "2024-05-27 11:47:21 UTC"
  },
  {
    "arxiv_id": "2405.17072v1",
    "title": "A novel framework for systematic propositional formula simplification based on existential graphs",
    "authors": [
      "Jordina Francès de Mas",
      "Juliana Bowles"
    ],
    "abstract": "This paper presents a novel simplification calculus for propositional logic\nderived from Peirce's existential graphs' rules of inference and implication\ngraphs. Our rules can be applied to propositional logic formulae in nested\nform, are equivalence-preserving, guarantee a monotonically decreasing number\nof variables, clauses and literals, and maximise the preservation of structural\nproblem information. Our techniques can also be seen as higher-level SAT\npreprocessing, and we show how one of our rules (TWSR) generalises and\nstreamlines most of the known equivalence-preserving SAT preprocessing methods.\nIn addition, we propose a simplification procedure based on the systematic\napplication of two of our rules (EPR and TWSR) which is solver-agnostic and can\nbe used to simplify large Boolean satisfiability problems and propositional\nformulae in arbitrary form, and we provide a formal analysis of its algorithmic\ncomplexity in terms of space and time. Finally, we show how our rules can be\nfurther extended with a novel n-ary implication graph to capture all known\nequivalence-preserving preprocessing procedures.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "math.LO",
      "03B35, 03B70, 68N17, 68T27",
      "F.4.1; I.2.2; I.2.3; I.2.4"
    ],
    "primary_category": "cs.LO",
    "comment": "19 pages, 12 figures. Under consideration in Theory and Practice of\n  Logic Programming (TPLP)",
    "pdf_url": "http://arxiv.org/pdf/2405.17072v1",
    "published_date": "2024-05-27 11:42:46 UTC",
    "updated_date": "2024-05-27 11:42:46 UTC"
  },
  {
    "arxiv_id": "2405.17067v2",
    "title": "Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization",
    "authors": [
      "Dixuan Wang",
      "Yanda Li",
      "Junyuan Jiang",
      "Zepeng Ding",
      "Ziqin Luo",
      "Guochao Jiang",
      "Jiaqing Liang",
      "Deqing Yang"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17067v2",
    "published_date": "2024-05-27 11:39:59 UTC",
    "updated_date": "2025-05-15 15:57:32 UTC"
  },
  {
    "arxiv_id": "2405.17060v1",
    "title": "Graph Neural Networks on Quantum Computers",
    "authors": [
      "Yidong Liao",
      "Xiao-Ming Zhang",
      "Chris Ferrie"
    ],
    "abstract": "Graph Neural Networks (GNNs) are powerful machine learning models that excel\nat analyzing structured data represented as graphs, demonstrating remarkable\nperformance in applications like social network analysis and recommendation\nsystems. However, classical GNNs face scalability challenges when dealing with\nlarge-scale graphs. This paper proposes frameworks for implementing GNNs on\nquantum computers to potentially address the challenges. We devise quantum\nalgorithms corresponding to the three fundamental types of classical GNNs:\nGraph Convolutional Networks, Graph Attention Networks, and Message-Passing\nGNNs. A complexity analysis of our quantum implementation of the Simplified\nGraph Convolutional (SGC) Network shows potential quantum advantages over its\nclassical counterpart, with significant improvements in time and space\ncomplexities. Our complexities can have trade-offs between the two: when\noptimizing for minimal circuit depth, our quantum SGC achieves logarithmic time\ncomplexity in the input sizes (albeit at the cost of linear space complexity).\nWhen optimizing for minimal qubit usage, the quantum SGC exhibits space\ncomplexity logarithmic in the input sizes, offering an exponential reduction\ncompared to classical SGCs, while still maintaining better time complexity.\nThese results suggest our Quantum GNN frameworks could efficiently process\nlarge-scale graphs. This work paves the way for implementing more advanced\nGraph Neural Network models on quantum computers, opening new possibilities in\nquantum machine learning for analyzing graph-structured data.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "50 Pages, 22 Figures",
    "pdf_url": "http://arxiv.org/pdf/2405.17060v1",
    "published_date": "2024-05-27 11:31:08 UTC",
    "updated_date": "2024-05-27 11:31:08 UTC"
  },
  {
    "arxiv_id": "2405.17057v1",
    "title": "ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation",
    "authors": [
      "Houxing Ren",
      "Mingjie Zhan",
      "Zhongyuan Wu",
      "Aojun Zhou",
      "Junting Pan",
      "Hongsheng Li"
    ],
    "abstract": "Code generation plays a crucial role in various tasks, such as code\nauto-completion and mathematical reasoning. Previous work has proposed numerous\nmethods to enhance code generation performance, including integrating feedback\nfrom the compiler. Inspired by this, we present ReflectionCoder, a novel\napproach that effectively leverages reflection sequences constructed by\nintegrating compiler feedback to improve one-off code generation performance.\nFurthermore, we propose reflection self-distillation and dynamically masked\ndistillation to effectively utilize these reflection sequences. Extensive\nexperiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E,\ndemonstrate that models fine-tuned with our method achieve state-of-the-art\nperformance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9\n(76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo\nand Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we\nbelieve this approach can benefit other domains that focus on final results and\nrequire long reasoning paths. Code and data are available at\nhttps://github.com/SenseLLM/ReflectionCoder.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17057v1",
    "published_date": "2024-05-27 11:27:00 UTC",
    "updated_date": "2024-05-27 11:27:00 UTC"
  },
  {
    "arxiv_id": "2405.17053v2",
    "title": "WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence",
    "authors": [
      "Jiawei Shao",
      "Jingwen Tong",
      "Qiong Wu",
      "Wei Guo",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "The rapid evolution of wireless technologies and the growing complexity of\nnetwork infrastructures necessitate a paradigm shift in how communication\nnetworks are designed, configured, and managed. Recent advancements in Large\nLanguage Models (LLMs) have sparked interest in their potential to\nrevolutionize wireless communication systems. However, existing studies on LLMs\nfor wireless systems are limited to a direct application for telecom language\nunderstanding. To empower LLMs with knowledge and expertise in the wireless\ndomain, this paper proposes WirelessLLM, a comprehensive framework for adapting\nand enhancing LLMs to address the unique challenges and requirements of\nwireless communication networks. We first identify three foundational\nprinciples that underpin WirelessLLM: knowledge alignment, knowledge fusion,\nand knowledge evolution. Then, we investigate the enabling technologies to\nbuild WirelessLLM, including prompt engineering, retrieval augmented\ngeneration, tool usage, multi-modal pre-training, and domain-specific\nfine-tuning. Moreover, we present three case studies to demonstrate the\npractical applicability and benefits of WirelessLLM for solving typical\nproblems in wireless networks. Finally, we conclude this paper by highlighting\nkey challenges and outlining potential avenues for future research.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17053v2",
    "published_date": "2024-05-27 11:18:25 UTC",
    "updated_date": "2024-06-15 07:01:54 UTC"
  },
  {
    "arxiv_id": "2405.17051v1",
    "title": "BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics",
    "authors": [
      "Hao Wu",
      "Xingjian Shi",
      "Ziyue Huang",
      "Penghao Zhao",
      "Wei Xiong",
      "Jinbao Xue",
      "Yangyu Tao",
      "Xiaomeng Huang",
      "Weiyan Wang"
    ],
    "abstract": "Data-driven deep learning has emerged as the new paradigm to model complex\nphysical space-time systems. These data-driven methods learn patterns by\noptimizing statistical metrics and tend to overlook the adherence to physical\nlaws, unlike traditional model-driven numerical methods. Thus, they often\ngenerate predictions that are not physically realistic. On the other hand, by\nsampling a large amount of high quality predictions from a data-driven model,\nsome predictions will be more physically plausible than the others and closer\nto what will happen in the future. Based on this observation, we propose\n\\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical\nalignment of data-driven space-time forecasting models. The key of BeamVQ is to\ntrain model on self-generated samples filtered with physics-aware metrics. To\nbe flexibly support different backbone architectures, BeamVQ leverages a code\nbank to transform any encoder-decoder model to the continuous state space into\ndiscrete codes. Afterwards, it iteratively employs beam search to sample\nhigh-quality sequences, retains those with the highest physics-aware scores,\nand trains model on the new dataset. Comprehensive experiments show that BeamVQ\nnot only gave an average statistical skill score boost for more than 32% for\nten backbones on five datasets, but also significantly enhances physics-aware\nmetrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17051v1",
    "published_date": "2024-05-27 11:07:47 UTC",
    "updated_date": "2024-05-27 11:07:47 UTC"
  },
  {
    "arxiv_id": "2405.17044v3",
    "title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
    "authors": [
      "Xuemei Gu",
      "Mario Krenn"
    ],
    "abstract": "The rapid growth of scientific literature makes it challenging for\nresearchers to identify novel and impactful ideas, especially across\ndisciplines. Modern artificial intelligence (AI) systems offer new approaches,\npotentially inspiring ideas not conceived by humans alone. But how compelling\nare these AI-generated ideas, and how can we improve their quality? Here, we\nintroduce SciMuse, which uses 58 million research papers and a large-language\nmodel to generate research ideas. We conduct a large-scale evaluation in which\nover 100 research group leaders -- from natural sciences to humanities --\nranked more than 4,400 personalized ideas based on their interest. This data\nallows us to predict research interest using (1) supervised neural networks\ntrained on human evaluations, and (2) unsupervised zero-shot ranking with\nlarge-language models. Our results demonstrate how future systems can help\ngenerating compelling research ideas and foster unforeseen interdisciplinary\ncollaborations.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages; 4 figures; Appendix: 6 pages, 5 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.17044v3",
    "published_date": "2024-05-27 11:00:51 UTC",
    "updated_date": "2025-01-07 21:29:45 UTC"
  },
  {
    "arxiv_id": "2406.00036v2",
    "title": "EMERGE: Enhancing Multimodal Electronic Health Records Predictive Modeling with Retrieval-Augmented Generation",
    "authors": [
      "Yinghao Zhu",
      "Changyu Ren",
      "Zixiang Wang",
      "Xiaochen Zheng",
      "Shiyun Xie",
      "Junlan Feng",
      "Xi Zhu",
      "Zhoujun Li",
      "Liantao Ma",
      "Chengwei Pan"
    ],
    "abstract": "The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly advanced clinical predictive capabilities. Existing models, which\nutilize clinical notes and multivariate time-series EHR data, often fall short\nof incorporating the necessary medical context for accurate clinical tasks,\nwhile previous approaches with knowledge graphs (KGs) primarily focus on\nstructured knowledge extraction. In response, we propose EMERGE, a\nRetrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR\npredictive modeling. We extract entities from both time-series data and\nclinical notes by prompting Large Language Models (LLMs) and align them with\nprofessional PrimeKG, ensuring consistency. In addition to triplet\nrelationships, we incorporate entities' definitions and descriptions for richer\nsemantics. The extracted knowledge is then used to generate task-relevant\nsummaries of patients' health statuses. Finally, we fuse the summary with other\nmodalities using an adaptive multimodal fusion network with cross-attention.\nExtensive experiments on the MIMIC-III and MIMIC-IV datasets' in-hospital\nmortality and 30-day readmission tasks demonstrate the superior performance of\nthe EMERGE framework over baseline models. Comprehensive ablation studies and\nanalysis highlight the efficacy of each designed module and robustness to data\nsparsity. EMERGE contributes to refining the utilization of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts essential for\ninformed clinical predictions. We have publicly released the code at\nhttps://github.com/yhzhu99/EMERGE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "CIKM 2024 Full Research Paper; arXiv admin note: text overlap with\n  arXiv:2402.07016",
    "pdf_url": "http://arxiv.org/pdf/2406.00036v2",
    "published_date": "2024-05-27 10:53:15 UTC",
    "updated_date": "2025-02-26 13:18:09 UTC"
  },
  {
    "arxiv_id": "2405.17038v1",
    "title": "Advancements in Tactile Hand Gesture Recognition for Enhanced Human-Machine Interaction",
    "authors": [
      "Chiara Fumelli",
      "Anirvan Dutta",
      "Mohsen Kaboli"
    ],
    "abstract": "Motivated by the growing interest in enhancing intuitive physical\nHuman-Machine Interaction (HRI/HVI), this study aims to propose a robust\ntactile hand gesture recognition system. We performed a comprehensive\nevaluation of different hand gesture recognition approaches for a large area\ntactile sensing interface (touch interface) constructed from conductive\ntextiles. Our evaluation encompassed traditional feature engineering methods,\nas well as contemporary deep learning techniques capable of real-time\ninterpretation of a range of hand gestures, accommodating variations in hand\nsizes, movement velocities, applied pressure levels, and interaction points.\nOur extensive analysis of the various methods makes a significant contribution\nto tactile-based gesture recognition in the field of human-machine interaction.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17038v1",
    "published_date": "2024-05-27 10:44:27 UTC",
    "updated_date": "2024-05-27 10:44:27 UTC"
  },
  {
    "arxiv_id": "2405.17034v2",
    "title": "FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks",
    "authors": [
      "Renqiang Luo",
      "Huafei Huang",
      "Shuo Yu",
      "Zhuoyang Han",
      "Estrid He",
      "Xiuzhen Zhang",
      "Feng Xia"
    ],
    "abstract": "Fairness-aware Graph Neural Networks (GNNs) often face a challenging\ntrade-off, where prioritizing fairness may require compromising utility. In\nthis work, we re-examine fairness through the lens of spectral graph theory,\naiming to reconcile fairness and utility within the framework of spectral graph\nlearning. We explore the correlation between sensitive features and spectrum in\nGNNs, using theoretical analysis to delineate the similarity between original\nsensitive features and those after convolution under different spectra. Our\nanalysis reveals a reduction in the impact of similarity when the eigenvectors\nassociated with the largest magnitude eigenvalue exhibit directional\nsimilarity. Based on these theoretical insights, we propose FUGNN, a novel\nspectral graph learning approach that harmonizes the conflict between fairness\nand utility. FUGNN ensures algorithmic fairness and utility by truncating the\nspectrum and optimizing eigenvector distribution during the encoding process.\nThe fairness-aware eigenvector selection reduces the impact of convolution on\nsensitive features while concurrently minimizing the sacrifice of utility.\nFUGNN further optimizes the distribution of eigenvectors through a transformer\narchitecture. By incorporating the optimized spectrum into the graph\nconvolution network, FUGNN effectively learns node representations. Experiments\non six real-world datasets demonstrate the superiority of FUGNN over baseline\nmethods. The codes are available at https://github.com/yushuowiki/FUGNN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in SIGKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17034v2",
    "published_date": "2024-05-27 10:40:21 UTC",
    "updated_date": "2024-08-13 15:04:18 UTC"
  },
  {
    "arxiv_id": "2405.17025v1",
    "title": "SWAT: Scalable and Efficient Window Attention-based Transformers Acceleration on FPGAs",
    "authors": [
      "Zhenyu Bai",
      "Pranav Dangi",
      "Huize Li",
      "Tulika Mitra"
    ],
    "abstract": "Efficiently supporting long context length is crucial for Transformer models.\nThe quadratic complexity of the self-attention computation plagues traditional\nTransformers. Sliding window-based static sparse attention mitigates the\nproblem by limiting the attention scope of the input tokens, reducing the\ntheoretical complexity from quadratic to linear. Although the sparsity induced\nby window attention is highly structured, it does not align perfectly with the\nmicroarchitecture of the conventional accelerators, leading to suboptimal\nimplementation. In response, we propose a dataflow-aware FPGA-based accelerator\ndesign, SWAT, that efficiently leverages the sparsity to achieve scalable\nperformance for long input. The proposed microarchitecture is based on a design\nthat maximizes data reuse by using a combination of row-wise dataflow, kernel\nfusion optimization, and an input-stationary design considering the distributed\nmemory and computation resources of FPGA. Consequently, it achieves up to\n22$\\times$ and 5.7$\\times$ improvement in latency and energy efficiency\ncompared to the baseline FPGA-based accelerator and 15$\\times$ energy\nefficiency compared to GPU-based solution.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepeted paper for DAC'22",
    "pdf_url": "http://arxiv.org/pdf/2405.17025v1",
    "published_date": "2024-05-27 10:25:08 UTC",
    "updated_date": "2024-05-27 10:25:08 UTC"
  },
  {
    "arxiv_id": "2405.17022v1",
    "title": "Compositional Few-Shot Class-Incremental Learning",
    "authors": [
      "Yixiong Zou",
      "Shanghang Zhang",
      "Haichen Zhou",
      "Yuhua Li",
      "Ruixuan Li"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) is proposed to continually learn\nfrom novel classes with only a few samples after the (pre-)training on base\nclasses with sufficient data. However, this remains a challenge. In contrast,\nhumans can easily recognize novel classes with a few samples. Cognitive science\ndemonstrates that an important component of such human capability is\ncompositional learning. This involves identifying visual primitives from\nlearned knowledge and then composing new concepts using these transferred\nprimitives, making incremental learning both effective and interpretable. To\nimitate human compositional learning, we propose a cognitive-inspired method\nfor the FSCIL task. We define and build a compositional model based on set\nsimilarities, and then equip it with a primitive composition module and a\nprimitive reuse module. In the primitive composition module, we propose to\nutilize the Centered Kernel Alignment (CKA) similarity to approximate the\nsimilarity between primitive sets, allowing the training and evaluation based\non primitive compositions. In the primitive reuse module, we enhance primitive\nreusability by classifying inputs based on primitives replaced with the closest\nprimitives from other classes. Experiments on three datasets validate our\nmethod, showing it outperforms current state-of-the-art methods with improved\ninterpretability. Our code is available at\nhttps://github.com/Zoilsen/Comp-FSCIL.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17022v1",
    "published_date": "2024-05-27 10:21:38 UTC",
    "updated_date": "2024-05-27 10:21:38 UTC"
  },
  {
    "arxiv_id": "2405.17009v3",
    "title": "Position: Foundation Agents as the Paradigm Shift for Decision Making",
    "authors": [
      "Xiaoqian Liu",
      "Xingzhou Lou",
      "Jianbin Jiao",
      "Junge Zhang"
    ],
    "abstract": "Decision making demands intricate interplay between perception, memory, and\nreasoning to discern optimal policies. Conventional approaches to decision\nmaking face challenges related to low sample efficiency and poor\ngeneralization. In contrast, foundation models in language and vision have\nshowcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\nconstruction of foundation agents as a transformative shift in the learning\nparadigm of agents. This proposal is underpinned by the formulation of\nfoundation agents with their fundamental characteristics and challenges\nmotivated by the success of large language models (LLMs). Moreover, we specify\nthe roadmap of foundation agents from large interactive data collection or\ngeneration, to self-supervised pretraining and adaptation, and knowledge and\nvalue alignment with LLMs. Lastly, we pinpoint critical research questions\nderived from the formulation and delineate trends for foundation agents\nsupported by real-world use cases, addressing both technical and theoretical\naspects to propel the field towards a more comprehensive and impactful future.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, camera-ready version of ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.17009v3",
    "published_date": "2024-05-27 09:54:50 UTC",
    "updated_date": "2024-05-29 14:15:09 UTC"
  },
  {
    "arxiv_id": "2406.01607v2",
    "title": "Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark",
    "authors": [
      "Hongliu Cao"
    ],
    "abstract": "Text embedding methods have become increasingly popular in both industrial\nand academic fields due to their critical role in a variety of natural language\nprocessing tasks. The significance of universal text embeddings has been\nfurther highlighted with the rise of Large Language Models (LLMs) applications\nsuch as Retrieval-Augmented Systems (RAGs). While previous models have\nattempted to be general-purpose, they often struggle to generalize across tasks\nand domains. However, recent advancements in training data quantity, quality\nand diversity; synthetic data generation from LLMs as well as using LLMs as\nbackbones encourage great improvements in pursuing universal text embeddings.\nIn this paper, we provide an overview of the recent advances in universal text\nembedding models with a focus on the top performing text embeddings on Massive\nText Embedding Benchmark (MTEB). Through detailed comparison and analysis, we\nhighlight the key contributions and limitations in this area, and propose\npotentially inspiring future research directions.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "21 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.01607v2",
    "published_date": "2024-05-27 09:52:54 UTC",
    "updated_date": "2024-06-19 06:52:13 UTC"
  },
  {
    "arxiv_id": "2405.16994v1",
    "title": "Vision-and-Language Navigation Generative Pretrained Transformer",
    "authors": [
      "Wen Hanlin"
    ],
    "abstract": "In the Vision-and-Language Navigation (VLN) field, agents are tasked with\nnavigating real-world scenes guided by linguistic instructions. Enabling the\nagent to adhere to instructions throughout the process of navigation represents\na significant challenge within the domain of VLN. To address this challenge,\ncommon approaches often rely on encoders to explicitly record past locations\nand actions, increasing model complexity and resource consumption. Our\nproposal, the Vision-and-Language Navigation Generative Pretrained Transformer\n(VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory\nsequence dependencies, bypassing the need for historical encoding modules. This\nmethod allows for direct historical information access through trajectory\nsequence, enhancing efficiency. Furthermore, our model separates the training\nprocess into offline pre-training with imitation learning and online\nfine-tuning with reinforcement learning. This distinction allows for more\nfocused training objectives and improved performance. Performance assessments\non the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art\nencoder-based models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16994v1",
    "published_date": "2024-05-27 09:42:04 UTC",
    "updated_date": "2024-05-27 09:42:04 UTC"
  },
  {
    "arxiv_id": "2405.17516v2",
    "title": "Time Elastic Neural Networks",
    "authors": [
      "Pierre-François Marteau"
    ],
    "abstract": "We introduce and detail an atypical neural network architecture, called time\nelastic neural network (teNN), for multivariate time series classification. The\nnovelty compared to classical neural network architecture is that it explicitly\nincorporates time warping ability, as well as a new way of considering\nattention. In addition, this architecture is capable of learning a dropout\nstrategy, thus optimizing its own architecture.Behind the design of this\narchitecture, our overall objective is threefold: firstly, we are aiming at\nimproving the accuracy of instance based classification approaches that shows\nquite good performances as far as enough training data is available. Secondly\nwe seek to reduce the computational complexity inherent to these methods to\nimprove their scalability. Ideally, we seek to find an acceptable balance\nbetween these first two criteria. And finally, we seek to enhance the\nexplainability of the decision provided by this kind of neural architecture.The\nexperiment demonstrates that the stochastic gradient descent implemented to\ntrain a teNN is quite effective. To the extent that the selection of some\ncritical meta-parameters is correct, convergence is generally smooth and\nfast.While maintaining good accuracy, we get a drastic gain in scalability by\nfirst reducing the required number of reference time series, i.e. the number of\nteNN cells required. Secondly, we demonstrate that, during the training\nprocess, the teNN succeeds in reducing the number of neurons required within\neach cell. Finally, we show that the analysis of the activation and attention\nmatrices as well as the reference time series after training provides relevant\ninformation to interpret and explain the classification results.The comparative\nstudy that we have carried out and which concerns around thirty diverse and\nmultivariate datasets shows that the teNN obtains results comparable to those\nof the state of the art, in particular similar to those of a network mixing\nLSTM and CNN architectures for example.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17516v2",
    "published_date": "2024-05-27 09:01:30 UTC",
    "updated_date": "2024-06-13 07:34:10 UTC"
  },
  {
    "arxiv_id": "2405.16964v2",
    "title": "Exploring the LLM Journey from Cognition to Expression with Linear Representations",
    "authors": [
      "Yuzi Yan",
      "Jialian Li",
      "Yipin Zhang",
      "Dong Yan"
    ],
    "abstract": "This paper presents an in-depth examination of the evolution and interplay of\ncognitive and expressive capabilities in large language models (LLMs), with a\nspecific focus on Baichuan-7B and Baichuan-33B, an advanced bilingual (Chinese\nand English) LLM series. We define and explore the model's cognitive and\nexpressive capabilities through linear representations across three critical\nphases: Pretraining, Supervised Fine-Tuning (SFT), and Reinforcement Learning\nfrom Human Feedback (RLHF). Cognitive capability is defined as the quantity and\nquality of information conveyed by the neuron output vectors within the\nnetwork, similar to the neural signal processing in human cognition. Expressive\ncapability is defined as the model's capability to produce word-level output.\nOur findings unveil a sequential development pattern, where cognitive abilities\nare largely established during Pretraining, whereas expressive abilities\npredominantly advance during SFT and RLHF. Statistical analyses confirm a\nsignificant correlation between the two capabilities, suggesting that cognitive\ncapacity may limit expressive potential. The paper also explores the\ntheoretical underpinnings of these divergent developmental trajectories and\ntheir connection to the LLMs' architectural design. Moreover, we evaluate\nvarious optimization-independent strategies, such as few-shot learning and\nrepeated sampling, which bridge the gap between cognitive and expressive\ncapabilities. This research reveals the potential connection between the hidden\nspace and the output space, contributing valuable insights into the\ninterpretability and controllability of their training processes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16964v2",
    "published_date": "2024-05-27 08:57:04 UTC",
    "updated_date": "2024-11-08 05:19:48 UTC"
  },
  {
    "arxiv_id": "2405.16961v2",
    "title": "Blind Data Adaptation to tackle Covariate Shift in Operational Steganalysis",
    "authors": [
      "Rony Abecidan",
      "Vincent Itier",
      "Jérémie Boulanger",
      "Patrick Bas",
      "Tomáš Pevný"
    ],
    "abstract": "The proliferation of image manipulation for unethical purposes poses\nsignificant challenges in social networks. One particularly concerning method\nis Image Steganography, allowing individuals to hide illegal information in\ndigital images without arousing suspicions. Such a technique pose severe\nsecurity risks, making it crucial to develop effective steganalysis methods\nenabling to detect manipulated images for clandestine communications. Although\nsignificant advancements have been achieved with machine learning models, a\ncritical issue remains: the disparity between the controlled datasets used to\ntrain steganalysis models against real-world datasets of forensic\npractitioners, undermining severely the practical effectiveness of standardized\nsteganalysis models. In this paper, we address this issue focusing on a\nrealistic scenario where practitioners lack crucial information about the\nlimited target set of images under analysis, including details about their\ndevelopment process and even whereas it contains manipulated images or not. By\nleveraging geometric alignment and distribution matching of source and target\nresiduals, we develop TADA (Target Alignment through Data Adaptation), a novel\nmethodology enabling to emulate sources aligned with specific targets in\nsteganalysis, which is also relevant for highly unbalanced targets. The\nemulator is represented by a light convolutional network trained to align\ndistributions of image residuals. Experimental validation demonstrates the\npotential of our strategy over traditional methods fighting covariate shift in\nsteganalysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16961v2",
    "published_date": "2024-05-27 08:55:22 UTC",
    "updated_date": "2024-05-29 06:47:30 UTC"
  },
  {
    "arxiv_id": "2405.16956v2",
    "title": "Functional Programming Paradigm of Python for Scientific Computation Pipeline Integration",
    "authors": [
      "Chen Zhang",
      "Lecheng Jia",
      "Wei Zhang",
      "Ning Wen"
    ],
    "abstract": "The advent of modern data processing has led to an increasing tendency\ntowards interdisciplinarity, which frequently involves the importation of\ndifferent technical approaches. Consequently, there is an urgent need for a\nunified data control system to facilitate the integration of varying libraries.\nThis integration is of profound significance in accelerating prototype\nverification, optimising algorithm performance and minimising maintenance\ncosts. This paper presents a novel functional programming (FP) paradigm based\non the Python architecture and associated suites in programming practice,\ndesigned for the integration of pipelines of different data mapping operations.\nIn particular, the solution is intended for the integration of scientific\ncomputation flows, which affords a robust yet flexible solution for the\naforementioned challenges.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2405.16956v2",
    "published_date": "2024-05-27 08:46:57 UTC",
    "updated_date": "2024-06-03 10:42:50 UTC"
  },
  {
    "arxiv_id": "2406.18557v1",
    "title": "Experimental Evaluation of Road-Crossing Decisions by Autonomous Wheelchairs against Environmental Factors",
    "authors": [
      "Franca Corradini",
      "Carlo Grigioni",
      "Alessandro Antonucci",
      "Jérôme Guzzi",
      "Francesco Flammini"
    ],
    "abstract": "Safe road crossing by autonomous wheelchairs can be affected by several\nenvironmental factors such as adverse weather conditions influencing the\naccuracy of artificial vision. Previous studies have addressed experimental\nevaluation of multi-sensor information fusion to support road-crossing\ndecisions in autonomous wheelchairs. In this study, we focus on the fine-tuning\nof tracking performance and on its experimental evaluation against outdoor\nenvironmental factors such as fog, rain, darkness, etc. It is rather intuitive\nthat those factors can negatively affect the tracking performance; therefore\nour aim is to provide an approach to quantify their effects in the reference\nscenario, in order to detect conditions of unacceptable accuracy. In those\ncases, warnings can be issued and system can be possibly reconfigured to reduce\nthe reputation of less accurate sensors, and thus improve overall safety.\nCritical situations can be detected by the main sensors or by additional\nsensors, e.g., light sensors, rain sensors, etc. Results have been achieved by\nusing an available laboratory dataset and by applying appropriate software\nfilters; they show that the approach can be adopted to evaluate video tracking\nand event detection robustness against outdoor environmental factors in\nrelevant operational scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T45 (Primary), 68T37 (Secondary)",
      "I.2.10; I.2.9; C.4; I.4.8"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to the \"27th IEEE International Conference on Intelligent\n  Transportation Systems\"",
    "pdf_url": "http://arxiv.org/pdf/2406.18557v1",
    "published_date": "2024-05-27 08:43:26 UTC",
    "updated_date": "2024-05-27 08:43:26 UTC"
  },
  {
    "arxiv_id": "2405.16946v1",
    "title": "Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld",
    "authors": [
      "Moein Khajehnejad",
      "Forough Habibollahi",
      "Aswin Paul",
      "Adeel Razi",
      "Brett J. Kagan"
    ],
    "abstract": "How do biological systems and machine learning algorithms compare in the\nnumber of samples required to show significant improvements in completing a\ntask? We compared the learning efficiency of in vitro biological neural\nnetworks to the state-of-the-art deep reinforcement learning (RL) algorithms in\na simplified simulation of the game `Pong'. Using DishBrain, a system that\nembodies in vitro neural networks with in silico computation using a\nhigh-density multi-electrode array, we contrasted the learning rate and the\nperformance of these biological systems against time-matched learning from\nthree state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same\ngame environment. This allowed a meaningful comparison between biological\nneural systems and deep RL. We find that when samples are limited to a\nreal-world time course, even these very simple biological cultures outperformed\ndeep RL algorithms across various game performance characteristics, implying a\nhigher sample efficiency. Ultimately, even when tested across multiple types of\ninformation input to assess the impact of higher dimensional data input,\nbiological neurons showcased faster learning than all deep reinforcement\nlearning agents.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "13 Pages, 6 Figures - 38 Supplementary Pages, 6 Supplementary\n  Figures, 4 Supplementary Tables",
    "pdf_url": "http://arxiv.org/pdf/2405.16946v1",
    "published_date": "2024-05-27 08:38:17 UTC",
    "updated_date": "2024-05-27 08:38:17 UTC"
  },
  {
    "arxiv_id": "2405.17514v3",
    "title": "AbstractBeam: Enhancing Bottom-Up Program Synthesis using Library Learning",
    "authors": [
      "Janis Zenkner",
      "Lukas Dierkes",
      "Tobias Sesterhenn",
      "Chrisitan Bartelt"
    ],
    "abstract": "LambdaBeam is a state-of-the-art, execution-guided algorithm for program\nsynthesis that utilizes higher-order functions, lambda functions, and iterative\nloops within a Domain-Specific Language (DSL). LambdaBeam generates each\nprogram from scratch but does not take advantage of the frequent recurrence of\nprogram blocks or subprograms commonly found in specific domains, such as loops\nfor list traversal. To address this limitation, we introduce AbstractBeam: a\nnovel program synthesis framework designed to enhance LambdaBeam by leveraging\nLibrary Learning. AbstractBeam identifies and integrates recurring program\nstructures into the DSL, optimizing the synthesis process. Our experimental\nevaluations demonstrate that AbstractBeam statistically significantly (p <\n0.05) outperforms LambdaBeam in the integer list manipulation domain. Beyond\nsolving more tasks, AbstractBeam's program synthesis is also more efficient,\nrequiring less time and fewer candidate programs to generate a solution.\nFurthermore, our findings indicate that Library Learning effectively enhances\nprogram synthesis in domains that are not explicitly designed to showcase its\nadvantages, thereby highlighting the broader applicability of Library Learning.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.17514v3",
    "published_date": "2024-05-27 08:31:12 UTC",
    "updated_date": "2024-09-12 06:11:50 UTC"
  },
  {
    "arxiv_id": "2405.16929v2",
    "title": "Uncertainty Management in the Construction of Knowledge Graphs: a Survey",
    "authors": [
      "Lucas Jarnac",
      "Yoan Chabot",
      "Miguel Couceiro"
    ],
    "abstract": "Knowledge Graphs (KGs) are a major asset for companies thanks to their great\nflexibility in data representation and their numerous applications, e.g.,\nvocabulary sharing, Q/A or recommendation systems. To build a KG it is a common\npractice to rely on automatic methods for extracting knowledge from various\nheterogeneous sources. But in a noisy and uncertain world, knowledge may not be\nreliable and conflicts between data sources may occur. Integrating unreliable\ndata would directly impact the use of the KG, therefore such conflicts must be\nresolved. This could be done manually by selecting the best data to integrate.\nThis first approach is highly accurate, but costly and time-consuming. That is\nwhy recent efforts focus on automatic approaches, which represents a\nchallenging task since it requires handling the uncertainty of extracted\nknowledge throughout its integration into the KG. We survey state-of-the-art\napproaches in this direction and present constructions of both open and\nenterprise KGs and how their quality is maintained. We then describe different\nknowledge extraction methods, introducing additional uncertainty. We also\ndiscuss downstream tasks after knowledge acquisition, including KG completion\nusing embedding models, knowledge alignment, and knowledge fusion in order to\naddress the problem of knowledge uncertainty in KG construction. We conclude\nwith a discussion on the remaining challenges and perspectives when\nconstructing a KG taking into account uncertainty.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16929v2",
    "published_date": "2024-05-27 08:22:52 UTC",
    "updated_date": "2024-07-19 07:46:07 UTC"
  },
  {
    "arxiv_id": "2405.16922v2",
    "title": "Theories of synaptic memory consolidation and intelligent plasticity for continual learning",
    "authors": [
      "Friedemann Zenke",
      "Axel Laborieux"
    ],
    "abstract": "Humans and animals learn throughout life. Such continual learning is crucial\nfor intelligence. In this chapter, we examine the pivotal role plasticity\nmechanisms with complex internal synaptic dynamics could play in enabling this\nability in neural networks. By surveying theoretical research, we highlight two\nfundamental enablers for continual learning. First, synaptic plasticity\nmechanisms must maintain and evolve an internal state over several behaviorally\nrelevant timescales. Second, plasticity algorithms must leverage the internal\nstate to intelligently regulate plasticity at individual synapses to facilitate\nthe seamless integration of new memories while avoiding detrimental\ninterference with existing ones. Our chapter covers successful applications of\nthese principles to deep neural networks and underscores the significance of\nsynaptic metaplasticity in sustaining continual learning capabilities. Finally,\nwe outline avenues for further research to understand the brain's superb\ncontinual learning abilities and harness similar mechanisms for artificial\nintelligence systems.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC",
    "comment": "An introductory-level book chapter. 35 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.16922v2",
    "published_date": "2024-05-27 08:13:39 UTC",
    "updated_date": "2024-10-18 06:15:42 UTC"
  },
  {
    "arxiv_id": "2405.16919v3",
    "title": "VoCoT: Unleashing Visually Grounded Multi-Step Reasoning in Large Multi-Modal Models",
    "authors": [
      "Zejun Li",
      "Ruipu Luo",
      "Jiwen Zhang",
      "Minghui Qiu",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "While large multi-modal models (LMMs) have exhibited impressive capabilities\nacross diverse tasks, their effectiveness in handling complex tasks has been\nlimited by the prevailing single-step reasoning paradigm. To this end, this\npaper proposes VoCoT, a multi-step Visually grounded object-centric\nChain-of-Thought reasoning framework tailored for inference with LMMs. VoCoT is\ncharacterized by two key features: (1) object-centric reasoning paths that\nrevolve around cross-modal shared object-level information, and (2) visually\ngrounded representation of object concepts in a multi-modal interleaved and\naligned manner, which effectively bridges the modality gap within LMMs during\nlong-term generation. To adapt LMMs in reasoning with VoCoT, we further\nconstruct an instruction-tuning dataset. By combining VoCoT with the prevalent\nopen-source LMM architectures, we develop a VoCoT-based model, VolCano. With\nonly 7B parameters and limited input image resolution, VolCano demonstrates\nexcellent performance across various scenarios. In benchmarks like CLEVR and\nEmbSpatial, which highly require complex reasoning capabilities, VolCano\noutperforms SOTA models, including powerful GPT-4V. Related code, data and\nmodels are released in https://github.com/RupertLuo/VoCoT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NAACL 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2405.16919v3",
    "published_date": "2024-05-27 08:12:00 UTC",
    "updated_date": "2025-03-08 17:16:09 UTC"
  },
  {
    "arxiv_id": "2405.16907v5",
    "title": "GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning",
    "authors": [
      "Jaewoo Lee",
      "Sujin Yun",
      "Taeyoung Yun",
      "Jinkyoo Park"
    ],
    "abstract": "Offline Reinforcement Learning (Offline RL) presents challenges of learning\neffective decision-making policies from static datasets without any online\ninteractions. Data augmentation techniques, such as noise injection and data\nsynthesizing, aim to improve Q-function approximation by smoothing the learned\nstate-action region. However, these methods often fall short of directly\nimproving the quality of offline datasets, leading to suboptimal results. In\nresponse, we introduce GTA, Generative Trajectory Augmentation, a novel\ngenerative data augmentation approach designed to enrich offline data by\naugmenting trajectories to be both high-rewarding and dynamically plausible.\nGTA applies a diffusion model within the data augmentation framework. GTA\npartially noises original trajectories and then denoises them with\nclassifier-free guidance via conditioning on amplified return value. Our\nresults show that GTA, as a general data augmentation strategy, enhances the\nperformance of widely used offline RL algorithms across various tasks with\nunique challenges. Furthermore, we conduct a quality analysis of data augmented\nby GTA and demonstrate that GTA improves the quality of the data. Our code is\navailable at https://github.com/Jaewoopudding/GTA",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024. Previously accepted (Spotlight) to ICLR 2024 Workshop\n  on Generative Models for Decision Making. Jaewoo Lee and Sujin Yun are equal\n  contribution authors",
    "pdf_url": "http://arxiv.org/pdf/2405.16907v5",
    "published_date": "2024-05-27 07:55:45 UTC",
    "updated_date": "2024-11-07 02:24:18 UTC"
  },
  {
    "arxiv_id": "2405.16899v1",
    "title": "Partial Models for Building Adaptive Model-Based Reinforcement Learning Agents",
    "authors": [
      "Safa Alver",
      "Ali Rahimi-Kalahroudi",
      "Doina Precup"
    ],
    "abstract": "In neuroscience, one of the key behavioral tests for determining whether a\nsubject of study exhibits model-based behavior is to study its adaptiveness to\nlocal changes in the environment. In reinforcement learning, however, recent\nstudies have shown that modern model-based agents display poor adaptivity to\nsuch changes. The main reason for this is that modern agents are typically\ndesigned to improve sample efficiency in single task settings and thus do not\ntake into account the challenges that can arise in other settings. In local\nadaptation settings, one particularly important challenge is in quickly\nbuilding and maintaining a sufficiently accurate model after a local change.\nThis is challenging for deep model-based agents as their models and replay\nbuffers are monolithic structures lacking distribution shift handling\ncapabilities. In this study, we show that the conceptually simple idea of\npartial models can allow deep model-based agents to overcome this challenge and\nthus allow for building locally adaptive model-based agents. By modeling the\ndifferent parts of the state space through different models, the agent can not\nonly maintain a model that is accurate across the state space, but it can also\nquickly adapt it in the presence of a local change in the environment. We\ndemonstrate this by showing that the use of partial models in agents such as\ndeep Dyna-Q, PlaNet and Dreamer can allow for them to effectively adapt to the\nlocal changes in their environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at CoLLAs 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16899v1",
    "published_date": "2024-05-27 07:46:36 UTC",
    "updated_date": "2024-05-27 07:46:36 UTC"
  },
  {
    "arxiv_id": "2405.17512v2",
    "title": "On Fairness of Low-Rank Adaptation of Large Models",
    "authors": [
      "Zhoujie Ding",
      "Ken Ziyu Liu",
      "Pura Peetathawatchai",
      "Berivan Isik",
      "Sanmi Koyejo"
    ],
    "abstract": "Low-rank adaptation of large models, particularly LoRA, has gained traction\ndue to its computational efficiency. This efficiency, contrasted with the\nprohibitive costs of full-model fine-tuning, means that practitioners often\nturn to LoRA and sometimes without a complete understanding of its\nramifications. In this study, we focus on fairness and ask whether LoRA has an\nunexamined impact on utility, calibration, and resistance to membership\ninference across different subgroups (e.g., genders, races, religions) compared\nto a full-model fine-tuning baseline. We present extensive experiments across\nvision and language domains and across classification and generation tasks\nusing ViT-Base, Swin-v2-Large, Llama-2 7B, and Mistral 7B. Intriguingly,\nexperiments suggest that while one can isolate cases where LoRA exacerbates\nmodel bias across subgroups, the pattern is inconsistent -- in many cases, LoRA\nhas equivalent or even improved fairness compared to the base model or its full\nfine-tuning baseline. We also examine the complications of evaluating\nfine-tuning fairness relating to task design and model token bias, calling for\nmore careful fairness evaluations in future work.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "COLM 2024 camera ready",
    "pdf_url": "http://arxiv.org/pdf/2405.17512v2",
    "published_date": "2024-05-27 07:37:43 UTC",
    "updated_date": "2024-09-18 00:55:35 UTC"
  },
  {
    "arxiv_id": "2405.16887v1",
    "title": "A Large Language Model-based multi-agent manufacturing system for intelligent shopfloor",
    "authors": [
      "Zhen Zhao",
      "Dunbing Tang",
      "Haihua Zhu",
      "Zequn Zhang",
      "Kai Chen",
      "Changchun Liu",
      "Yuchen Ji"
    ],
    "abstract": "As productivity advances, the demand of customers for multi-variety and\nsmall-batch production is increasing, thereby putting forward higher\nrequirements for manufacturing systems. When production tasks frequent changes\ndue to this demand, traditional manufacturing systems often cannot response\npromptly. The multi-agent manufacturing system is proposed to address this\nproblem. However, because of technical limitations, the negotiation among\nagents in this kind of system is realized through predefined heuristic rules,\nwhich is not intelligent enough to deal with the multi-variety and small batch\nproduction. To this end, a Large Language Model-based (LLM-based) multi-agent\nmanufacturing system for intelligent shopfloor is proposed in the present\nstudy. This system delineates the diverse agents and defines their\ncollaborative methods. The roles of the agents encompass Machine Server Agent\n(MSA), Bid Inviter Agent (BIA), Bidder Agent (BA), Thinking Agent (TA), and\nDecision Agent (DA). Due to the support of LLMs, TA and DA acquire the ability\nof analyzing the shopfloor condition and choosing the most suitable machine, as\nopposed to executing a predefined program artificially. The negotiation between\nBAs and BIA is the most crucial step in connecting manufacturing resources.\nWith the support of TA and DA, BIA will finalize the distribution of orders,\nrelying on the information of each machine returned by BA. MSAs bears the\nresponsibility for connecting the agents with the physical shopfloor. This\nsystem aims to distribute and transmit workpieces through the collaboration of\nthe agents with these distinct roles, distinguishing it from other scheduling\napproaches. Comparative experiments were also conducted to validate the\nperformance of this system.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16887v1",
    "published_date": "2024-05-27 07:10:04 UTC",
    "updated_date": "2024-05-27 07:10:04 UTC"
  },
  {
    "arxiv_id": "2405.16883v2",
    "title": "Scorch: A Library for Sparse Deep Learning",
    "authors": [
      "Bobby Yan",
      "Alexander J. Root",
      "Trevor Gale",
      "David Broman",
      "Fredrik Kjolstad"
    ],
    "abstract": "The rapid growth in the size of deep learning models strains the capabilities\nof traditional dense computation paradigms. Leveraging sparse computation has\nbecome increasingly popular for training and deploying large-scale models, but\nexisting deep learning frameworks lack extensive support for sparse operations.\nTo bridge this gap, we introduce Scorch, a library that seamlessly integrates\nefficient sparse tensor computation into the PyTorch ecosystem, with an initial\nfocus on inference workloads on CPUs. Scorch provides a flexible and intuitive\ninterface for sparse tensors, supporting diverse sparse data structures. Scorch\nintroduces a compiler stack that automates key optimizations, including\nautomatic loop ordering, tiling, and format inference. Combined with a runtime\nthat adapts its execution to both dense and sparse data, Scorch delivers\nsubstantial speedups over hand-written PyTorch Sparse (torch.sparse) operations\nwithout sacrificing usability. More importantly, Scorch enables efficient\ncomputation of complex sparse operations that lack hand-optimized PyTorch\nimplementations. This flexibility is crucial for exploring novel sparse\narchitectures. We demonstrate Scorch's ease of use and performance gains on\ndiverse deep learning models across multiple domains. With only minimal code\nchanges, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end\ntasks. Scorch's seamless integration and performance gains make it a valuable\naddition to the PyTorch ecosystem. We believe Scorch will enable wider\nexploration of sparsity as a tool for scaling deep learning and inform the\ndevelopment of other sparse libraries.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MS",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.16883v2",
    "published_date": "2024-05-27 06:59:20 UTC",
    "updated_date": "2024-06-20 06:24:23 UTC"
  },
  {
    "arxiv_id": "2405.16879v1",
    "title": "Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning",
    "authors": [
      "Wangyang Ying",
      "Dongjie Wang",
      "Xuanming Hu",
      "Yuanchun Zhou",
      "Charu C. Aggarwal",
      "Yanjie Fu"
    ],
    "abstract": "Feature transformation is to derive a new feature set from original features\nto augment the AI power of data. In many science domains such as material\nperformance screening, while feature transformation can model material formula\ninteractions and compositions and discover performance drivers, supervised\nlabels are collected from expensive and lengthy experiments. This issue\nmotivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior\nliterature, such as manual transformation, supervised feedback guided search,\nand PCA, either relies on domain knowledge or expensive supervised feedback, or\nsuffers from large search space, or overlooks non-linear feature-feature\ninteractions. UFTL imposes a major challenge on existing methods: how to design\na new unsupervised paradigm that captures complex feature interactions and\navoids large search space? To fill this gap, we connect graph, contrastive, and\ngenerative learning to develop a measurement-pretrain-finetune paradigm for\nUFTL. For unsupervised feature set utility measurement, we propose a feature\nvalue consistency preservation perspective and develop a mean discounted\ncumulative gain like unsupervised metric to evaluate feature set utility. For\nunsupervised feature set representation pretraining, we regard a feature set as\na feature-feature interaction graph, and develop an unsupervised graph\ncontrastive learning encoder to embed feature sets into vectors. For generative\ntransformation finetuning, we regard a feature set as a feature cross sequence\nand feature transformation as sequential generation. We develop a deep\ngenerative feature transformation model that coordinates the pretrained feature\nset encoder and the gradient information extracted from a feature set utility\nevaluator to optimize a transformed feature generator.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16879v1",
    "published_date": "2024-05-27 06:50:00 UTC",
    "updated_date": "2024-05-27 06:50:00 UTC"
  },
  {
    "arxiv_id": "2405.16877v3",
    "title": "Are Self-Attentions Effective for Time Series Forecasting?",
    "authors": [
      "Dongbin Kim",
      "Jinseong Park",
      "Jaewook Lee",
      "Hoki Kim"
    ],
    "abstract": "Time series forecasting is crucial for applications across multiple domains\nand various scenarios. Although Transformer models have dramatically advanced\nthe landscape of forecasting, their effectiveness remains debated. Recent\nfindings have indicated that simpler linear models might outperform complex\nTransformer-based approaches, highlighting the potential for more streamlined\narchitectures. In this paper, we shift the focus from evaluating the overall\nTransformer architecture to specifically examining the effectiveness of\nself-attention for time series forecasting. To this end, we introduce a new\narchitecture, Cross-Attention-only Time Series transformer (CATS), that\nrethinks the traditional Transformer framework by eliminating self-attention\nand leveraging cross-attention mechanisms instead. By establishing future\nhorizon-dependent parameters as queries and enhanced parameter sharing, our\nmodel not only improves long-term forecasting accuracy but also reduces the\nnumber of parameters and memory usage. Extensive experiment across various\ndatasets demonstrates that our model achieves superior performance with the\nlowest mean squared error and uses fewer parameters compared to existing\nmodels. The implementation of our model is available at:\nhttps://github.com/dongbeank/CATS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16877v3",
    "published_date": "2024-05-27 06:49:39 UTC",
    "updated_date": "2024-12-23 13:34:55 UTC"
  },
  {
    "arxiv_id": "2405.16876v3",
    "title": "Transfer Learning for Diffusion Models",
    "authors": [
      "Yidong Ouyang",
      "Liyan Xie",
      "Hongyuan Zha",
      "Guang Cheng"
    ],
    "abstract": "Diffusion models, a specific type of generative model, have achieved\nunprecedented performance in recent years and consistently produce high-quality\nsynthetic samples. A critical prerequisite for their notable success lies in\nthe presence of a substantial number of training samples, which can be\nimpractical in real-world applications due to high collection costs or\nassociated risks. Consequently, various finetuning and regularization\napproaches have been proposed to transfer knowledge from existing pre-trained\nmodels to specific target domains with limited data. This paper introduces the\nTransfer Guided Diffusion Process (TGDP), a novel approach distinct from\nconventional finetuning and regularization methods. We prove that the optimal\ndiffusion model for the target domain integrates pre-trained diffusion models\non the source domain with additional guidance from a domain classifier. We\nfurther extend TGDP to a conditional version for modeling the joint\ndistribution of data and its corresponding labels, together with two additional\nregularization terms to enhance the model performance. We validate the\neffectiveness of TGDP on both simulated and real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16876v3",
    "published_date": "2024-05-27 06:48:58 UTC",
    "updated_date": "2024-10-30 18:48:50 UTC"
  },
  {
    "arxiv_id": "2405.16869v4",
    "title": "Multiple Heads are Better than One: Mixture of Modality Knowledge Experts for Entity Representation Learning",
    "authors": [
      "Yichi Zhang",
      "Zhuo Chen",
      "Lingbing Guo",
      "Yajing Xu",
      "Binbin Hu",
      "Ziqi Liu",
      "Wen Zhang",
      "Huajun Chen"
    ],
    "abstract": "Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "ICLR 2025 Camera-ready Version. Fix a caption typo in the current\n  version",
    "pdf_url": "http://arxiv.org/pdf/2405.16869v4",
    "published_date": "2024-05-27 06:36:17 UTC",
    "updated_date": "2025-04-06 09:08:09 UTC"
  },
  {
    "arxiv_id": "2405.16867v1",
    "title": "Clustering-based Learning for UAV Tracking and Pose Estimation",
    "authors": [
      "Jiaping Xiao",
      "Phumrapee Pisutsin",
      "Cheng Wen Tsao",
      "Mir Feroskhan"
    ],
    "abstract": "UAV tracking and pose estimation plays an imperative role in various\nUAV-related missions, such as formation control and anti-UAV measures.\nAccurately detecting and tracking UAVs in a 3D space remains a particularly\nchallenging problem, as it requires extracting sparse features of micro UAVs\nfrom different flight environments and continuously matching correspondences,\nespecially during agile flight. Generally, cameras and LiDARs are the two main\ntypes of sensors used to capture UAV trajectories in flight. However, both\nsensors have limitations in UAV classification and pose estimation. This\ntechnical report briefly introduces the method proposed by our team \"NTU-ICG\"\nfor the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based\nlearning detection approach, CL-Det, for UAV tracking and pose estimation using\ntwo types of LiDARs, namely Livox Avia and LiDAR 360. We combine the\ninformation from the two data sources to locate drones in 3D. We first align\nthe timestamps of Livox Avia data and LiDAR 360 data and then separate the\npoint cloud of objects of interest (OOIs) from the environment. The point cloud\nof OOIs is clustered using the DBSCAN method, with the midpoint of the largest\ncluster assumed to be the UAV position. Furthermore, we utilize historical\nestimations to fill in missing data. The proposed method shows competitive pose\nestimation performance and ranks 5th on the final leaderboard of the CVPR 2024\nUG2+ Challenge.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted Report of CVPR 2024 UG2+ Challenge Track 5",
    "pdf_url": "http://arxiv.org/pdf/2405.16867v1",
    "published_date": "2024-05-27 06:33:25 UTC",
    "updated_date": "2024-05-27 06:33:25 UTC"
  },
  {
    "arxiv_id": "2405.16860v1",
    "title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
    "authors": [
      "Yunqi Zhang",
      "Songda Li",
      "Chunyuan Deng",
      "Luyi Wang",
      "Hui Zhao"
    ],
    "abstract": "Gender bias in vision-language models (VLMs) can reinforce harmful\nstereotypes and discrimination. In this paper, we focus on mitigating gender\nbias towards vision-language tasks. We identify object hallucination as the\nessence of gender bias in VLMs. Existing VLMs tend to focus on salient or\nfamiliar attributes in images but ignore contextualized nuances. Moreover, most\nVLMs rely on the co-occurrence between specific objects and gender attributes\nto infer the ignored features, ultimately resulting in gender bias. We propose\nGAMA, a task-agnostic generation framework to mitigate gender bias. GAMA\nconsists of two stages: narrative generation and answer inference. During\nnarrative generation, GAMA yields all-sided but gender-obfuscated narratives,\nwhich prevents premature concentration on localized image features, especially\ngender attributes. During answer inference, GAMA integrates the image,\ngenerated narrative, and a task-specific question prompt to infer answers for\ndifferent vision-language tasks. This approach allows the model to rethink\ngender attributes and answers. We conduct extensive experiments on GAMA,\ndemonstrating its debiasing and generalization ability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accept to NAACL 2024(main)",
    "pdf_url": "http://arxiv.org/pdf/2405.16860v1",
    "published_date": "2024-05-27 06:20:58 UTC",
    "updated_date": "2024-05-27 06:20:58 UTC"
  },
  {
    "arxiv_id": "2405.16852v2",
    "title": "EM Distillation for One-step Diffusion Models",
    "authors": [
      "Sirui Xie",
      "Zhisheng Xiao",
      "Diederik P Kingma",
      "Tingbo Hou",
      "Ying Nian Wu",
      "Kevin Patrick Murphy",
      "Tim Salimans",
      "Ben Poole",
      "Ruiqi Gao"
    ],
    "abstract": "While diffusion models can learn complex distributions, sampling requires a\ncomputationally expensive iterative process. Existing distillation methods\nenable efficient sampling, but have notable limitations, such as performance\ndegradation with very few sampling steps, reliance on training data access, or\nmode-seeking optimization that may fail to capture the full distribution. We\npropose EM Distillation (EMD), a maximum likelihood-based approach that\ndistills a diffusion model to a one-step generator model with minimal loss of\nperceptual quality. Our approach is derived through the lens of\nExpectation-Maximization (EM), where the generator parameters are updated using\nsamples from the joint distribution of the diffusion teacher prior and inferred\ngenerator latents. We develop a reparametrized sampling scheme and a noise\ncancellation technique that together stabilizes the distillation process. We\nfurther reveal an interesting connection of our method with existing methods\nthat minimize mode-seeking KL. EMD outperforms existing one-step generative\nmethods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares\nfavorably with prior work on distilling text-to-image diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16852v2",
    "published_date": "2024-05-27 05:55:22 UTC",
    "updated_date": "2024-12-06 06:07:45 UTC"
  },
  {
    "arxiv_id": "2405.16851v1",
    "title": "Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning",
    "authors": [
      "Mingqing Xiao",
      "Yixin Zhu",
      "Di He",
      "Zhouchen Lin"
    ],
    "abstract": "Spiking neural networks (SNNs) are investigated as biologically inspired\nmodels of neural computation, distinguished by their computational capability\nand energy efficiency due to precise spiking times and sparse spikes with\nevent-driven computation. A significant question is how SNNs can emulate\nhuman-like graph-based reasoning of concepts and relations, especially\nleveraging the temporal domain optimally. This paper reveals that SNNs, when\namalgamated with synaptic delay and temporal coding, are proficient in\nexecuting (knowledge) graph reasoning. It is elucidated that spiking time can\nfunction as an additional dimension to encode relation properties via a\nneural-generalized path formulation. Empirical results highlight the efficacy\nof temporal delay in relation processing and showcase exemplary performance in\ndiverse graph reasoning tasks. The spiking model is theoretically estimated to\nachieve $20\\times$ energy savings compared to non-spiking counterparts,\ndeepening insights into the capabilities and potential of biologically inspired\nSNNs for efficient reasoning. The code is available at\nhttps://github.com/pkuxmq/GRSNN.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16851v1",
    "published_date": "2024-05-27 05:53:30 UTC",
    "updated_date": "2024-05-27 05:53:30 UTC"
  },
  {
    "arxiv_id": "2405.16847v1",
    "title": "TokenUnify: Scalable Autoregressive Visual Pre-training with Mixture Token Prediction",
    "authors": [
      "Yinda Chen",
      "Haoyuan Shi",
      "Xiaoyu Liu",
      "Te Shi",
      "Ruobing Zhang",
      "Dong Liu",
      "Zhiwei Xiong",
      "Feng Wu"
    ],
    "abstract": "Autoregressive next-token prediction is a standard pretraining method for\nlarge-scale language models, but its application to vision tasks is hindered by\nthe non-sequential nature of image data, leading to cumulative errors. Most\nvision models employ masked autoencoder (MAE) based pretraining, which faces\nscalability issues. To address these challenges, we introduce\n\\textbf{TokenUnify}, a novel pretraining method that integrates random token\nprediction, next-token prediction, and next-all token prediction. We provide\ntheoretical evidence demonstrating that TokenUnify mitigates cumulative errors\nin visual autoregression. Cooperated with TokenUnify, we have assembled a\nlarge-scale electron microscopy (EM) image dataset with ultra-high resolution,\nideal for creating spatially correlated long sequences. This dataset includes\nover 120 million annotated voxels, making it the largest neuron segmentation\ndataset to date and providing a unified benchmark for experimental validation.\nLeveraging the Mamba network inherently suited for long-sequence modeling on\nthis dataset, TokenUnify not only reduces the computational complexity but also\nleads to a significant 45\\% improvement in segmentation performance on\ndownstream EM neuron segmentation tasks compared to existing methods.\nFurthermore, TokenUnify demonstrates superior scalability over MAE and\ntraditional autoregressive methods, effectively bridging the gap between\npretraining strategies for language and vision models. Code is available at\n\\url{https://github.com/ydchen0806/TokenUnify}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16847v1",
    "published_date": "2024-05-27 05:45:51 UTC",
    "updated_date": "2024-05-27 05:45:51 UTC"
  },
  {
    "arxiv_id": "2405.16836v1",
    "title": "Enhancing Fast Feed Forward Networks with Load Balancing and a Master Leaf Node",
    "authors": [
      "Andreas Charalampopoulos",
      "Nikolas Chatzis",
      "Foivos Ntoulas-Panagiotopoulos",
      "Charilaos Papaioannou",
      "Alexandros Potamianos"
    ],
    "abstract": "Fast feedforward networks (FFFs) are a class of neural networks that exploit\nthe observation that different regions of the input space activate distinct\nsubsets of neurons in wide networks. FFFs partition the input space into\nseparate sections using a differentiable binary tree of neurons and during\ninference descend the binary tree in order to improve computational efficiency.\nInspired by Mixture of Experts (MoE) research, we propose the incorporation of\nload balancing and Master Leaf techniques into the FFF architecture to improve\nperformance and simplify the training process. We reproduce experiments found\nin literature and present results on FFF models enhanced using these\ntechniques. The proposed architecture and training recipe achieves up to 16.3%\nand 3% absolute classification accuracy increase in training and test accuracy,\nrespectively, compared to the original FFF architecture. Additionally, we\nobserve a smaller variance in the results compared to those reported in prior\nresearch. These findings demonstrate the potential of integrating MoE-inspired\ntechniques into FFFs for developing more accurate and efficient models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16836v1",
    "published_date": "2024-05-27 05:06:24 UTC",
    "updated_date": "2024-05-27 05:06:24 UTC"
  },
  {
    "arxiv_id": "2405.16830v2",
    "title": "Structured Graph Network for Constrained Robot Crowd Navigation with Low Fidelity Simulation",
    "authors": [
      "Shuijing Liu",
      "Kaiwen Hong",
      "Neeloy Chakraborty",
      "Katherine Driggs-Campbell"
    ],
    "abstract": "We investigate the feasibility of deploying reinforcement learning (RL)\npolicies for constrained crowd navigation using a low-fidelity simulator. We\nintroduce a representation of the dynamic environment, separating human and\nobstacle representations. Humans are represented through detected states, while\nobstacles are represented as computed point clouds based on maps and robot\nlocalization. This representation enables RL policies trained in a low-fidelity\nsimulator to deploy in real world with a reduced sim2real gap. Additionally, we\npropose a spatio-temporal graph to model the interactions between agents and\nobstacles. Based on the graph, we use attention mechanisms to capture the\nrobot-human, human-human, and human-obstacle interactions. Our method\nsignificantly improves navigation performance in both simulated and real-world\nenvironments. Video demonstrations can be found at\nhttps://sites.google.com/view/constrained-crowdnav/home.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16830v2",
    "published_date": "2024-05-27 04:53:09 UTC",
    "updated_date": "2024-05-28 01:20:43 UTC"
  },
  {
    "arxiv_id": "2405.16823v1",
    "title": "Unified Editing of Panorama, 3D Scenes, and Videos Through Disentangled Self-Attention Injection",
    "authors": [
      "Gihyun Kwon",
      "Jangho Park",
      "Jong Chul Ye"
    ],
    "abstract": "While text-to-image models have achieved impressive capabilities in image\ngeneration and editing, their application across various modalities often\nnecessitates training separate models. Inspired by existing method of single\nimage editing with self attention injection and video editing with shared\nattention, we propose a novel unified editing framework that combines the\nstrengths of both approaches by utilizing only a basic 2D image text-to-image\n(T2I) diffusion model. Specifically, we design a sampling method that\nfacilitates editing consecutive images while maintaining semantic consistency\nutilizing shared self-attention features during both reference and consecutive\nimage sampling processes. Experimental results confirm that our method enables\nediting across diverse modalities including 3D scenes, videos, and panorama\nimages.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://unifyediting.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2405.16823v1",
    "published_date": "2024-05-27 04:44:36 UTC",
    "updated_date": "2024-05-27 04:44:36 UTC"
  },
  {
    "arxiv_id": "2405.16820v1",
    "title": "Laboratory-Scale AI: Open-Weight Models are Competitive with ChatGPT Even in Low-Resource Settings",
    "authors": [
      "Robert Wolfe",
      "Isaac Slaughter",
      "Bin Han",
      "Bingbing Wen",
      "Yiwei Yang",
      "Lucas Rosenblatt",
      "Bernease Herman",
      "Eva Brown",
      "Zening Qu",
      "Nic Weber",
      "Bill Howe"
    ],
    "abstract": "The rapid proliferation of generative AI has raised questions about the\ncompetitiveness of lower-parameter, locally tunable, open-weight models\nrelative to high-parameter, API-guarded, closed-weight models in terms of\nperformance, domain adaptation, cost, and generalization. Centering\nunder-resourced yet risk-intolerant settings in government, research, and\nhealthcare, we see for-profit closed-weight models as incompatible with\nrequirements for transparency, privacy, adaptability, and standards of\nevidence. Yet the performance penalty in using open-weight models, especially\nin low-data and low-resource settings, is unclear.\n  We assess the feasibility of using smaller, open-weight models to replace\nGPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to\nonly a single, low-cost GPU. We assess value-sensitive issues around bias,\nprivacy, and abstention on three additional tasks relevant to those topics. We\nfind that with relatively low effort, very low absolute monetary cost, and\nrelatively little data for fine-tuning, small open-weight models can achieve\ncompetitive performance in domain-adapted tasks without sacrificing generality.\nWe then run experiments considering practical issues in bias, privacy, and\nhallucination risk, finding that open models offer several benefits over closed\nmodels. We intend this work as a case study in understanding the opportunity\ncost of reproducibility and transparency over for-profit state-of-the-art zero\nshot performance, finding this cost to be marginal under realistic settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT) 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16820v1",
    "published_date": "2024-05-27 04:38:10 UTC",
    "updated_date": "2024-05-27 04:38:10 UTC"
  },
  {
    "arxiv_id": "2405.20776v1",
    "title": "Federated Learning with Blockchain-Enhanced Machine Unlearning: A Trustworthy Approach",
    "authors": [
      "Xuhan Zuo",
      "Minghao Wang",
      "Tianqing Zhu",
      "Lefeng Zhang",
      "Shui Yu",
      "Wanlei Zhou"
    ],
    "abstract": "With the growing need to comply with privacy regulations and respond to user\ndata deletion requests, integrating machine unlearning into IoT-based federated\nlearning has become imperative. Traditional unlearning methods, however, often\nlack verifiable mechanisms, leading to challenges in establishing trust. This\npaper delves into the innovative integration of blockchain technology with\nfederated learning to surmount these obstacles. Blockchain fortifies the\nunlearning process through its inherent qualities of immutability,\ntransparency, and robust security. It facilitates verifiable certification,\nharmonizes security with privacy, and sustains system efficiency. We introduce\na framework that melds blockchain with federated learning, thereby ensuring an\nimmutable record of unlearning requests and actions. This strategy not only\nbolsters the trustworthiness and integrity of the federated learning model but\nalso adeptly addresses efficiency and security challenges typical in IoT\nenvironments. Our key contributions encompass a certification mechanism for the\nunlearning process, the enhancement of data security and privacy, and the\noptimization of data management to ensure system responsiveness in IoT\nscenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 25 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.20776v1",
    "published_date": "2024-05-27 04:35:49 UTC",
    "updated_date": "2024-05-27 04:35:49 UTC"
  },
  {
    "arxiv_id": "2405.16807v2",
    "title": "Extreme Compression of Adaptive Neural Images",
    "authors": [
      "Leo Hoshikawa",
      "Marcos V. Conde",
      "Takeshi Ohashi",
      "Atsushi Irie"
    ],
    "abstract": "Implicit Neural Representations (INRs) and Neural Fields are a novel paradigm\nfor signal representation, from images and audio to 3D scenes and videos. The\nfundamental idea is to represent a signal as a continuous and differentiable\nneural network. This idea offers unprecedented benefits such as continuous\nresolution and memory efficiency, enabling new compression techniques. However,\nrepresenting data as neural networks poses new challenges. For instance, given\na 2D image as a neural network, how can we further compress such a neural\nimage?. In this work, we present a novel analysis on compressing neural fields,\nwith the focus on images. We also introduce Adaptive Neural Images (ANI), an\nefficient neural representation that enables adaptation to different inference\nor transmission requirements. Our proposed method allows to reduce the\nbits-per-pixel (bpp) of the neural image by 4x, without losing sensitive\ndetails or harming fidelity. We achieve this thanks to our successful\nimplementation of 4-bit neural representations. Our work offers a new framework\nfor developing compressed neural fields.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report. Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2405.16807v2",
    "published_date": "2024-05-27 03:54:09 UTC",
    "updated_date": "2024-06-04 18:42:01 UTC"
  },
  {
    "arxiv_id": "2405.16806v2",
    "title": "Entity Alignment with Noisy Annotations from Large Language Models",
    "authors": [
      "Shengyuan Chen",
      "Qinggang Zhang",
      "Junnan Dong",
      "Wen Hua",
      "Qing Li",
      "Xiao Huang"
    ],
    "abstract": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying\nequivalent entity pairs. While existing methods heavily rely on human-generated\nlabels, it is prohibitively expensive to incorporate cross-domain experts for\nannotation in real-world scenarios. The advent of Large Language Models (LLMs)\npresents new avenues for automating EA with annotations, inspired by their\ncomprehensive capability to process semantic information. However, it is\nnontrivial to directly apply LLMs for EA since the annotation space in\nreal-world KGs is large. LLMs could also generate noisy labels that may mislead\nthe alignment. To this end, we propose a unified framework, LLM4EA, to\neffectively leverage LLMs for EA. Specifically, we design a novel active\nlearning policy to significantly reduce the annotation space by prioritizing\nthe most valuable entities based on the entire inter-KG and intra-KG structure.\nMoreover, we introduce an unsupervised label refiner to continuously enhance\nlabel accuracy through in-depth probabilistic reasoning. We iteratively\noptimize the policy based on the feedback from a base EA model. Extensive\nexperiments demonstrate the advantages of LLM4EA on four benchmark datasets in\nterms of effectiveness, robustness, and efficiency. Codes are available via\nhttps://github.com/chensyCN/llm4ea_official.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16806v2",
    "published_date": "2024-05-27 03:52:55 UTC",
    "updated_date": "2024-05-28 07:50:18 UTC"
  },
  {
    "arxiv_id": "2405.16800v1",
    "title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
    "authors": [
      "Zheng Zhang",
      "Yuntong Hu",
      "Bo Pan",
      "Chen Ling",
      "Liang Zhao"
    ],
    "abstract": "Text-Attributed Graphs (TAGs) enhance graph structures with natural language\ndescriptions, enabling detailed representation of data and their relationships\nacross a broad spectrum of real-world scenarios. Despite the potential for\ndeeper insights, existing TAG representation learning primarily relies on\nsupervised methods, necessitating extensive labeled data and limiting\napplicability across diverse contexts. This paper introduces a new\nself-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA),\nwhich overcomes these constraints by integrating TAGs' structural and semantic\ndimensions. TAGA constructs two complementary views: Text-of-Graph view, which\norganizes node texts into structured documents based on graph topology, and the\nGraph-of-Text view, which converts textual nodes and connections into graph\ndata. By aligning representations from both views, TAGA captures joint textual\nand structural information. In addition, a novel structure-preserving random\nwalk algorithm is proposed for efficient training on large-sized TAGs. Our\nframework demonstrates strong performance in zero-shot and few-shot scenarios\nacross eight real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16800v1",
    "published_date": "2024-05-27 03:40:16 UTC",
    "updated_date": "2024-05-27 03:40:16 UTC"
  },
  {
    "arxiv_id": "2405.16797v1",
    "title": "A Real-Time Voice Activity Detection Based On Lightweight Neural",
    "authors": [
      "Jidong Jia",
      "Pei Zhao",
      "Di Wang"
    ],
    "abstract": "Voice activity detection (VAD) is the task of detecting speech in an audio\nstream, which is challenging due to numerous unseen noises and low\nsignal-to-noise ratios in real environments. Recently, neural network-based\nVADs have alleviated the degradation of performance to some extent. However,\nthe majority of existing studies have employed excessively large models and\nincorporated future context, while neglecting to evaluate the operational\nefficiency and latency of the models. In this paper, we propose a lightweight\nand real-time neural network called MagicNet, which utilizes casual and depth\nseparable 1-D convolutions and GRU. Without relying on future features as\ninput, our proposed model is compared with two state-of-the-art algorithms on\nsynthesized in-domain and out-domain test datasets. The evaluation results\ndemonstrate that MagicNet can achieve improved performance and robustness with\nfewer parameter costs.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16797v1",
    "published_date": "2024-05-27 03:31:16 UTC",
    "updated_date": "2024-05-27 03:31:16 UTC"
  },
  {
    "arxiv_id": "2405.16792v2",
    "title": "Laurel: Unblocking Automated Verification with Large Language Models",
    "authors": [
      "Eric Mugnier",
      "Emmanuel Anaya Gonzalez",
      "Ranjit Jhala",
      "Nadia Polikarpova",
      "Yuanyuan Zhou"
    ],
    "abstract": "Program verifiers such as Dafny automate proofs by outsourcing them to an SMT\nsolver. This automation is not perfect, however, and the solver often requires\nhints in the form of assertions, creating a burden for the proof engineer. In\nthis paper, we propose Laurel, a tool that alleviates this burden by\nautomatically generating assertions using large language models (LLMs). To\nimprove the success rate of LLMs in this task, we design two domain-specific\nprompting techniques. First, we help the LLM determine the location of the\nmissing assertion by analyzing the verifier's error message and inserting an\nassertion placeholder at that location. Second, we provide the LLM with example\nassertions from the same codebase, which we select based on a new proof\nsimilarity metric. We evaluate our techniques on our new benchmark DafnyGym, a\ndataset of complex lemmas we extracted from three real-world Dafny codebases.\nOur evaluation shows that Laurel is able to generate over 56.6\\% of the\nrequired assertions given only a few attempts, making LLMs an affordable tool\nfor unblocking program verifiers without human intervention.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "34 pages, accepted at OOPSLA 25",
    "pdf_url": "http://arxiv.org/pdf/2405.16792v2",
    "published_date": "2024-05-27 03:26:01 UTC",
    "updated_date": "2025-03-03 22:24:37 UTC"
  },
  {
    "arxiv_id": "2405.16783v1",
    "title": "TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models",
    "authors": [
      "Yuzhou. Nie",
      "Yanting. Wang",
      "Jinyuan. Jia",
      "Michael J. De Lucia",
      "Nathaniel D. Bastian",
      "Wenbo. Guo",
      "Dawn. Song"
    ],
    "abstract": "One key challenge in backdoor attacks against large foundation models is the\nresource limits. Backdoor attacks usually require retraining the target model,\nwhich is impractical for very large foundation models. Existing backdoor\nattacks are mainly designed for supervised classifiers or small foundation\nmodels (e.g., BERT). None of these attacks has successfully compromised a very\nlarge foundation model, such as Llama-3-70B, especially with limited\ncomputational resources. In this paper, we propose TrojFM, a novel backdoor\nattack tailored for very large foundation models. Our primary technical\ncontribution is the development of a novel backdoor injection method. This\nmethod forces a backdoored model to generate similar hidden representations for\npoisoned inputs regardless of their actual semantics. Our approach injects such\nbackdoors by fine-tuning only a very small proportion of model parameters. This\nenables TrojFM to efficiently launch downstream task-agnostic backdoor attacks\nagainst very large foundation models under limited computational resources.\nMoreover, we optimize the fine-tuning process with our customized QLoRA\ntechnique, enabling launching our attack via only~\\textit{one A100 GPU}.\nFurthermore, we design a new trigger injection method to ensure our attack\nstealthiness. Through extensive experiments, we first demonstrate that TrojFM\ncan launch effective backdoor attacks against widely used large GPT-style\nmodels without jeopardizing their normal functionalities (and outperforming\nexisting attacks on BERT-style models). Furthermore, we show that TrojFM is\nresilient to SOTA defenses and is insensitive to changes in key\nhyper-parameters. Finally, we conduct a resource analysis to quantify that our\nmethod can significantly save computational and memory costs compared to\nexisting backdoor attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16783v1",
    "published_date": "2024-05-27 03:10:57 UTC",
    "updated_date": "2024-05-27 03:10:57 UTC"
  },
  {
    "arxiv_id": "2405.16766v2",
    "title": "Concept Matching with Agent for Out-of-Distribution Detection",
    "authors": [
      "Yuxiao Lee",
      "Xiaofeng Cao",
      "Jingcai Guo",
      "Wei Ye",
      "Qing Guo",
      "Yi Chang"
    ],
    "abstract": "The remarkable achievements of Large Language Models (LLMs) have captivated\nthe attention of both academia and industry, transcending their initial role in\ndialogue generation. To expand the usage scenarios of LLM, some works enhance\nthe effectiveness and capabilities of the model by introducing more external\ninformation, which is called the agent paradigm. Based on this idea, we propose\na new method that integrates the agent paradigm into out-of-distribution (OOD)\ndetection task, aiming to improve its robustness and adaptability. Our proposed\nmethod, Concept Matching with Agent (CMA), employs neutral prompts as agents to\naugment the CLIP-based OOD detection process. These agents function as dynamic\nobservers and communication hubs, interacting with both In-distribution (ID)\nlabels and data inputs to form vector triangle relationships. This triangular\nframework offers a more nuanced approach than the traditional binary\nrelationship, allowing for better separation and identification of ID and OOD\ninputs. Our extensive experimental results showcase the superior performance of\nCMA over both zero-shot and training-required methods in a diverse array of\nreal-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2405.16766v2",
    "published_date": "2024-05-27 02:27:28 UTC",
    "updated_date": "2025-01-07 03:53:12 UTC"
  },
  {
    "arxiv_id": "2405.16761v1",
    "title": "Masked Face Recognition with Generative-to-Discriminative Representations",
    "authors": [
      "Shiming Ge",
      "Weijia Guo",
      "Chenyu Li",
      "Junzheng Zhang",
      "Yong Li",
      "Dan Zeng"
    ],
    "abstract": "Masked face recognition is important for social good but challenged by\ndiverse occlusions that cause insufficient or inaccurate representations. In\nthis work, we propose a unified deep network to learn\ngenerative-to-discriminative representations for facilitating masked face\nrecognition. To this end, we split the network into three modules and learn\nthem on synthetic masked faces in a greedy module-wise pretraining manner.\nFirst, we leverage a generative encoder pretrained for face inpainting and\nfinetune it to represent masked faces into category-aware descriptors.\nAttribute to the generative encoder's ability in recovering context\ninformation, the resulting descriptors can provide occlusion-robust\nrepresentations for masked faces, mitigating the effect of diverse masks. Then,\nwe incorporate a multi-layer convolutional network as a discriminative reformer\nand learn it to convert the category-aware descriptors into identity-aware\nvectors, where the learning is effectively supervised by distilling relation\nknowledge from off-the-shelf face recognition model. In this way, the\ndiscriminative reformer together with the generative encoder serves as the\npretrained backbone, providing general and discriminative representations\ntowards masked faces. Finally, we cascade one fully-connected layer following\nby one softmax layer into a feature classifier and finetune it to identify the\nreformed identity-aware vectors. Extensive experiments on synthetic and\nrealistic datasets demonstrate the effectiveness of our approach in recognizing\nmasked faces.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by International Conference on Machine Learning 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.16761v1",
    "published_date": "2024-05-27 02:20:55 UTC",
    "updated_date": "2024-05-27 02:20:55 UTC"
  },
  {
    "arxiv_id": "2405.16755v3",
    "title": "CHESS: Contextual Harnessing for Efficient SQL Synthesis",
    "authors": [
      "Shayan Talaei",
      "Mohammadreza Pourreza",
      "Yu-Chen Chang",
      "Azalia Mirhoseini",
      "Amin Saberi"
    ],
    "abstract": "Translating natural language questions into SQL queries, known as\ntext-to-SQL, is a long-standing research problem. Effective text-to-SQL\nsynthesis can become very challenging due to (i) the extensive size of database\ncatalogs (descriptions of tables and their columns) and database values, (ii)\nreasoning over large database schemas, (iii) ensuring the functional validity\nof the generated queries, and (iv) navigating the ambiguities of natural\nlanguage questions. We introduce CHESS, a Large Language Model (LLM) based\nmulti-agent framework for efficient and scalable SQL synthesis, comprising four\nspecialized agents, each targeting one of the aforementioned challenges: the\nInformation Retriever (IR) extracts relevant data, the Schema Selector (SS)\nprunes large schemas, the Candidate Generator (CG) generates high-quality\ncandidates and refines queries iteratively, and the Unit Tester (UT) validates\nqueries through LLM-based natural language unit tests. Our framework offers\nconfigurable features that adapt to various deployment constraints, including\n1) Supporting industrial-scale databases: leveraging the Schema Selector agent,\nCHESS efficiently narrows down very large database schemas into manageable\nsub-schemas, boosting system accuracy by approximately $2\\%$ and reducing the\nnumber of LLM tokens by $\\times 5$. 2) State-of-the-Art privacy-preserving\nperformance: Among the methods using open-source models, CHESS achieves\nstate-of-the-art performance, resulting in a high-performing,\nprivacy-preserving system suitable for industrial deployment. 3) Scalablity\nwith additional compute budget: In settings with high computational budgets,\nCHESS achieves $71.10\\%$ accuracy on the BIRD test set, within $2\\%$ of the\nleading proprietary method, while requiring approximately $83\\%$ fewer LLM\ncalls.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16755v3",
    "published_date": "2024-05-27 01:54:16 UTC",
    "updated_date": "2024-11-25 19:43:07 UTC"
  },
  {
    "arxiv_id": "2405.16752v1",
    "title": "Model Ensembling for Constrained Optimization",
    "authors": [
      "Ira Globus-Harris",
      "Varun Gupta",
      "Michael Kearns",
      "Aaron Roth"
    ],
    "abstract": "There is a long history in machine learning of model ensembling, beginning\nwith boosting and bagging and continuing to the present day. Much of this\nhistory has focused on combining models for classification and regression, but\nrecently there is interest in more complex settings such as ensembling policies\nin reinforcement learning. Strong connections have also emerged between\nensembling and multicalibration techniques. In this work, we further\ninvestigate these themes by considering a setting in which we wish to ensemble\nmodels for multidimensional output predictions that are in turn used for\ndownstream optimization. More precisely, we imagine we are given a number of\nmodels mapping a state space to multidimensional real-valued predictions. These\npredictions form the coefficients of a linear objective that we would like to\noptimize under specified constraints. The fundamental question we address is\nhow to improve and combine such models in a way that outperforms the best of\nthem in the downstream optimization problem. We apply multicalibration\ntechniques that lead to two provably efficient and convergent algorithms. The\nfirst of these (the white box approach) requires being given models that map\nstates to output predictions, while the second (the \\emph{black box} approach)\nrequires only policies (mappings from states to solutions to the optimization\nproblem). For both, we provide convergence and utility guarantees. We conclude\nby investigating the performance and behavior of the two algorithms in a\ncontrolled experimental setting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16752v1",
    "published_date": "2024-05-27 01:48:07 UTC",
    "updated_date": "2024-05-27 01:48:07 UTC"
  },
  {
    "arxiv_id": "2405.16751v2",
    "title": "REVECA: Adaptive Planning and Trajectory-based Validation in Cooperative Language Agents using Information Relevance and Relative Proximity",
    "authors": [
      "SeungWon Seo",
      "SeongRae Noh",
      "Junhyeok Lee",
      "SooBin Lim",
      "Won Hee Lee",
      "HyeongYeop Kang"
    ],
    "abstract": "We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by cooperating with decentralized agents under complex partial\nobservations. Existing cooperative agent systems often struggle with\nefficiently processing continuously accumulating information, managing globally\nsuboptimal planning due to lack of consideration of collaborators, and\naddressing false planning caused by environmental changes introduced by other\ncollaborators. To overcome these challenges, we propose the RElevance,\nProximity, and Validation-Enhanced Cooperative Language Agent (REVECA), a novel\ncognitive architecture powered by GPT-4o-mini. REVECA enables efficient memory\nmanagement, optimal planning, and cost-effective prevention of false planning\nby leveraging Relevance Estimation, Adaptive Planning, and Trajectory-based\nValidation. Extensive experimental results demonstrate REVECA's superiority\nover existing methods across various benchmarks, while a user study reveals its\npotential for achieving trustworthy human-AI cooperation.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "v2 is the AAAI'25 camera-ready version, including the appendix, which\n  has been enhanced based on the reviewers' comments",
    "pdf_url": "http://arxiv.org/pdf/2405.16751v2",
    "published_date": "2024-05-27 01:47:14 UTC",
    "updated_date": "2024-12-18 08:38:06 UTC"
  },
  {
    "arxiv_id": "2405.16739v1",
    "title": "Oracle-Efficient Reinforcement Learning for Max Value Ensembles",
    "authors": [
      "Marcel Hussing",
      "Michael Kearns",
      "Aaron Roth",
      "Sikata Bela Sengupta",
      "Jessica Sorrell"
    ],
    "abstract": "Reinforcement learning (RL) in large or infinite state spaces is notoriously\nchallenging, both theoretically (where worst-case sample and computational\ncomplexities must scale with state space cardinality) and experimentally (where\nfunction approximation and policy gradient techniques often scale poorly and\nsuffer from instability and high variance). One line of research attempting to\naddress these difficulties makes the natural assumption that we are given a\ncollection of heuristic base or $\\textit{constituent}$ policies upon which we\nwould like to improve in a scalable manner. In this work we aim to compete with\nthe $\\textit{max-following policy}$, which at each state follows the action of\nwhichever constituent policy has the highest value. The max-following policy is\nalways at least as good as the best constituent policy, and may be considerably\nbetter. Our main result is an efficient algorithm that learns to compete with\nthe max-following policy, given only access to the constituent policies (but\nnot their value functions). In contrast to prior work in similar settings, our\ntheoretical results require only the minimal assumption of an ERM oracle for\nvalue function approximation for the constituent policies (and not the global\noptimal policy or the max-following policy itself) on samplable distributions.\nWe illustrate our algorithm's experimental effectiveness and behavior on\nseveral robotic simulation testbeds.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16739v1",
    "published_date": "2024-05-27 01:08:23 UTC",
    "updated_date": "2024-05-27 01:08:23 UTC"
  },
  {
    "arxiv_id": "2405.16730v1",
    "title": "Latent Energy-Based Odyssey: Black-Box Optimization via Expanded Exploration in the Energy-Based Latent Space",
    "authors": [
      "Peiyu Yu",
      "Dinghuai Zhang",
      "Hengzhi He",
      "Xiaojian Ma",
      "Ruiyao Miao",
      "Yifan Lu",
      "Yasi Zhang",
      "Deqian Kong",
      "Ruiqi Gao",
      "Jianwen Xie",
      "Guang Cheng",
      "Ying Nian Wu"
    ],
    "abstract": "Offline Black-Box Optimization (BBO) aims at optimizing a black-box function\nusing the knowledge from a pre-collected offline dataset of function values and\ncorresponding input designs. However, the high-dimensional and\nhighly-multimodal input design space of black-box function pose inherent\nchallenges for most existing methods that model and operate directly upon input\ndesigns. These issues include but are not limited to high sample complexity,\nwhich relates to inaccurate approximation of black-box function; and\ninsufficient coverage and exploration of input design modes, which leads to\nsuboptimal proposal of new input designs. In this work, we consider finding a\nlatent space that serves as a compressed yet accurate representation of the\ndesign-value joint space, enabling effective latent exploration of high-value\ninput design modes. To this end, we formulate an learnable energy-based latent\nspace, and propose Noise-intensified Telescoping density-Ratio Estimation\n(NTRE) scheme for variational learning of an accurate latent space model\nwithout costly Markov Chain Monte Carlo. The optimization process is then\nexploration of high-value designs guided by the learned energy-based model in\nthe latent space, formulated as gradient-based sampling from a\nlatent-variable-parameterized inverse model. We show that our particular\nparameterization encourages expanded exploration around high-value design\nmodes, motivated by inversion thinking of a fundamental result of conditional\ncovariance matrix typically used for variance reduction. We observe that our\nmethod, backed by an accurately learned informative latent space and an\nexpanding-exploration model design, yields significant improvements over strong\nprevious methods on both synthetic and real world datasets such as the\ndesign-bench suite.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.16730v1",
    "published_date": "2024-05-27 00:11:53 UTC",
    "updated_date": "2024-05-27 00:11:53 UTC"
  }
]
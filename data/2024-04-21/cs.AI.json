{
  "date": "2024-04-21",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-21 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 应用创新，包括 LLM 的安全性和解释性、图神经网络、医疗诊断和机器人技术等领域，其中 AdvPrompter（对抗 LLM 提示生成）和 Minds, Brains, AI（AGI 讨论，由 Jay Seitz 等知名学者发布）最为令人印象深刻，前者提升了 LLM 抗攻击能力，后者质疑 AGI 实现的科学依据。\n\n以下是今日值得关注的论文摘要，我将相关主题归类讨论，先优先选取重要、话题度高的论文（如 AI 安全和 LLM 相关），并快速掠过次要内容。每个条目包括论文标题（中文 + 英文）和核心贡献。\n\n### AI 安全与 LLM 相关\n- **AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs**（中文：快速自适应对抗提示生成用于大型语言模型）  \n  这篇论文提出一种快速生成人类可读对抗提示的方法，使用 Bi-directional GAN 和低秩微调，显著提升 LLM 的抗越狱攻击能力，同时保持基准性能。在实验中，AdvPrompter 在 AdvBench 数据集上超越现有方法，速度提升约 800 倍，强调了 LLM 安全性的实际应用。\n\n- **Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome**（中文：使用预测潜在个性维度进行反事实推理以优化说服结果）  \n  论文引入一种跟踪用户个性维度并生成定制化反事实语句的方法，结合 BiCoGAN 和 D3QN 模型，提升说服对话系统的动态适应性。在 PersuasionForGood 数据集上，方法优于基线，展示了反事实推理在在线交互中的潜力。\n\n- **NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding**（中文：在谈判情境中压力测试机器理论-of-mind 的基准）  \n  构建了一个新基准，用于评估 LLM 在谈判中的理论-of-mind 能力，通过多轮对话测试欲望、信念和意图。该方法揭示 LLM 在复杂人机交互中的局限性，是 LLM 解释性和鲁棒性研究的重要补充。\n\n- **Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models**（中文：对抗表示工程：用于大型语言模型的一般模型编辑框架）  \n  提出 ARE 框架，使用对抗方法编辑 LLM 表示，而不影响基准性能。该方法在多任务实验中表现出色，已被 NeurIPS 2024 接受，适用于 LLM 的概念级调整。\n\n### 医疗与生物技术\n- **Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks**（中文：通过偏差转换生成对抗网络生成公平的合成健康数据）  \n  开发 Bt-GAN 生成公平的合成医疗数据，解决传统 GAN 在 EHR 中的偏差问题，通过加权采样和公平变换提升模型性能。在 MIMIC-III 数据集上，显著减少偏差放大，是医疗 AI 公平性的关键进展。\n\n- **Accelerating Medical Knowledge Discovery through Automated Knowledge Graph Generation and Enrichment**（中文：通过自动知识图谱生成和丰富加速医疗知识发现）  \n  提出 M-KGA 框架，使用 BioPortal 语义丰富医疗概念，构建动态知识图谱。该方法在 EHR 数据上识别隐藏连接，提升知识发现效率，适用于大规模医疗数据分析。\n\n- **MathNet: A Framework for Boosting DNN Accuracy with Entropy-driven Generalized Converting Autoencoder**（中文：使用熵驱动的泛化转换自编码器提升 DNN 精度的框架）  \n  引入泛化自编码器优化图像分类，显著提升 DNN 准确性（如 CIFAR-10 上 VGG16 准确率从 92.64% 升至 94.05%），而保持模型大小不变。该方法在复杂数据集上超越知识蒸馏技术。\n\n- **A Nasal Cytology Dataset for Object Detection and Deep Learning**（中文：用于对象检测和深度学习的鼻腔细胞学数据集）  \n  发布首个鼻腔细胞数据集，并使用 DETR 和 YOLO 模型进行细胞检测，解决鼻炎诊断中的细胞计数问题。该数据集有助于 AI 辅助临床实践，但实验受限于类别不平衡。\n\n### 机器人与计算机视觉\n- **Soar: Design and Deployment of A Smart Roadside Infrastructure System for Autonomous Driving**（中文：Soar：智能路边基础设施系统的设计和部署，用于自动驾驶）  \n  设计 Soar 系统，支持自动驾驶应用，通过多跳通信和任务管理框架提升实时性能。该系统已在校园部署两年，展示了基础设施辅助自动驾驶的潜力。\n\n- **Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following**（中文：苏格拉底规划器：基于自问自答的零样本规划，用于具身指令跟随）  \n  提出基于 LLM 的零样本规划方法，通过自问自答生成子目标，并在视觉反馈下调整计划。在 ALFRED 基准上表现优异，适用于真实机器人任务。\n\n其他论文如图神经网络（CKGConv）、指纹生成（Universal Fingerprint Generation）和图像检测（AnyPattern）也有贡献，但相对常规，我仅简要提及：CKGConv 增强了图卷积的表达性，AnyPattern 实现了无训练的图像复制检测。这些工作在各自领域有实用价值，但未列出详细讨论以控制篇幅。\n\n总之，今天的 arXiv 论文突显 AI 在安全、医疗和机器人领域的潜力，建议读者关注 LLM 相关研究以跟进热门趋势。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2407.10975v1",
      "title": "Stream State-tying for Sign Language Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Jiyong Ma",
        "Wen Gao",
        "Chunli Wang"
      ],
      "abstract": "In this paper, a novel approach to sign language recognition based on state\ntying in each of data streams is presented. In this framework, it is assumed\nthat hand gesture signal is represented in terms of six synchronous data\nstreams, i.e., the left/right hand position, left/right hand orientation and\nleft/right handshape. This approach offers a very accurate representation of\nthe sign space and keeps the number of parameters reasonably small in favor of\na fast decoding. Experiments were carried out for 5177 Chinese signs. The real\ntime isolated recognition rate is 94.8%. For continuous sign recognition, the\nword correct rate is 91.4%. Keywords: Sign language recognition; Automatic sign\nlanguage translation; Hand gesture recognition; Hidden Markov models;\nState-tying; Multimodal user interface; Virtual reality; Man-machine systems.",
      "tldr_zh": "本文提出了一种基于 Stream State-tying 的手语识别新方法，将手势信号表示为六个同步数据流，包括左右手位置（left/right hand position）、左右手方向（left/right hand orientation）和左右手形状（left/right handshape），以提供精确的符号空间表示并减少参数以实现快速解码。实验在 5177 个中文手语上进行，实时隔离识别率达到 94.8%，而连续手语识别的词正确率则为 91.4%。该方法利用 Hidden Markov models 等技术，增强了手语识别的准确性和效率，为 Automatic sign language translation 和 Hand gesture recognition 等应用提供了重要基础。",
      "categories": [
        "cs.OH",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.OH",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.10975v1",
      "published_date": "2024-04-21 23:21:52 UTC",
      "updated_date": "2024-04-21 23:21:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:14:23.717890"
    },
    {
      "arxiv_id": "2404.13792v1",
      "title": "Counterfactual Reasoning Using Predicted Latent Personality Dimensions for Optimizing Persuasion Outcome",
      "title_zh": "利用预测的潜在人格维度进行反事实推理以优化说服结果",
      "authors": [
        "Donghuo Zeng",
        "Roberto S. Legaspi",
        "Yuewen Sun",
        "Xinshuai Dong",
        "Kazushi Ikeda",
        "Peter Spirtes",
        "kun Zhang"
      ],
      "abstract": "Customizing persuasive conversations related to the outcome of interest for\nspecific users achieves better persuasion results. However, existing persuasive\nconversation systems rely on persuasive strategies and encounter challenges in\ndynamically adjusting dialogues to suit the evolving states of individual users\nduring interactions. This limitation restricts the system's ability to deliver\nflexible or dynamic conversations and achieve suboptimal persuasion outcomes.\nIn this paper, we present a novel approach that tracks a user's latent\npersonality dimensions (LPDs) during ongoing persuasion conversation and\ngenerates tailored counterfactual utterances based on these LPDs to optimize\nthe overall persuasion outcome. In particular, our proposed method leverages a\nBi-directional Generative Adversarial Network (BiCoGAN) in tandem with a\nDialogue-based Personality Prediction Regression (DPPR) model to generate\ncounterfactual data. This enables the system to formulate alternative\npersuasive utterances that are more suited to the user. Subsequently, we\nutilize the D3QN model to learn policies for optimized selection of system\nutterances on counterfactual data. Experimental results we obtained from using\nthe PersuasionForGood dataset demonstrate the superiority of our approach over\nthe existing method, BiCoGAN. The cumulative rewards and Q-values produced by\nour method surpass ground truth benchmarks, showcasing the efficacy of\nemploying counterfactual reasoning and LPDs to optimize reinforcement learning\npolicy in online interactions.",
      "tldr_zh": "该论文提出了一种新方法，通过预测用户的潜在人格维度 (LPDs) 来实现反事实推理 (Counterfactual Reasoning)，以优化说服性对话的结果。该方法动态跟踪用户在对话中的状态，利用 Bi-directional Generative Adversarial Network (BiCoGAN) 和 Dialogue-based Personality Prediction Regression (DPPR) 模型生成针对LPDs的自定义反事实话语。随后，通过 D3QN 模型学习策略，从这些话语中选择最优的系统响应。实验在 PersuasionForGood 数据集上显示，该方法在累积奖励和 Q-values 上超过了现有方法 BiCoGAN，证明了其在提升说服效果方面的优越性。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.MM",
      "comment": "14 pages, 10 figures, Accepted by Persuasive Technology 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13792v1",
      "published_date": "2024-04-21 23:03:47 UTC",
      "updated_date": "2024-04-21 23:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:14:34.569954"
    },
    {
      "arxiv_id": "2404.13791v1",
      "title": "Universal Fingerprint Generation: Controllable Diffusion Model with Multimodal Conditions",
      "title_zh": "通用指纹生成：带有多模态条件的可控扩散模型",
      "authors": [
        "Steven A. Grosz",
        "Anil K. Jain"
      ],
      "abstract": "The utilization of synthetic data for fingerprint recognition has garnered\nincreased attention due to its potential to alleviate privacy concerns\nsurrounding sensitive biometric data. However, current methods for generating\nfingerprints have limitations in creating impressions of the same finger with\nuseful intra-class variations. To tackle this challenge, we present GenPrint, a\nframework to produce fingerprint images of various types while maintaining\nidentity and offering humanly understandable control over different appearance\nfactors such as fingerprint class, acquisition type, sensor device, and quality\nlevel. Unlike previous fingerprint generation approaches, GenPrint is not\nconfined to replicating style characteristics from the training dataset alone:\nit enables the generation of novel styles from unseen devices without requiring\nadditional fine-tuning. To accomplish these objectives, we developed GenPrint\nusing latent diffusion models with multimodal conditions (text and image) for\nconsistent generation of style and identity. Our experiments leverage a variety\nof publicly available datasets for training and evaluation. Results demonstrate\nthe benefits of GenPrint in terms of identity preservation, explainable\ncontrol, and universality of generated images. Importantly, the\nGenPrint-generated images yield comparable or even superior accuracy to models\ntrained solely on real data and further enhances performance when augmenting\nthe diversity of existing real fingerprint datasets.",
      "tldr_zh": "本论文提出GenPrint框架，利用可控的diffusion模型和multimodal conditions（文本和图像），生成各种类型指纹图像，同时保持身份一致并提供对外观因素（如指纹类别、获取类型、传感器设备和质量水平）的可解释控制。不同于传统方法，GenPrint能创建新设备的新风格指纹，而无需额外微调。实验结果表明，使用公开数据集训练的GenPrint图像在身份保留和准确性上与真实数据相当或更优，并能通过增强数据集多样性进一步提升指纹识别性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13791v1",
      "published_date": "2024-04-21 23:01:08 UTC",
      "updated_date": "2024-04-21 23:01:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:14:46.636967"
    },
    {
      "arxiv_id": "2404.13789v1",
      "title": "Anchor-aware Deep Metric Learning for Audio-visual Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Donghuo Zeng",
        "Yanan Wang",
        "Kazushi Ikeda",
        "Yi Yu"
      ],
      "abstract": "Metric learning minimizes the gap between similar (positive) pairs of data\npoints and increases the separation of dissimilar (negative) pairs, aiming at\ncapturing the underlying data structure and enhancing the performance of tasks\nlike audio-visual cross-modal retrieval (AV-CMR). Recent works employ sampling\nmethods to select impactful data points from the embedding space during\ntraining. However, the model training fails to fully explore the space due to\nthe scarcity of training data points, resulting in an incomplete representation\nof the overall positive and negative distributions. In this paper, we propose\nan innovative Anchor-aware Deep Metric Learning (AADML) method to address this\nchallenge by uncovering the underlying correlations among existing data points,\nwhich enhances the quality of the shared embedding space. Specifically, our\nmethod establishes a correlation graph-based manifold structure by considering\nthe dependencies between each sample as the anchor and its semantically similar\nsamples. Through dynamic weighting of the correlations within this underlying\nmanifold structure using an attention-driven mechanism, Anchor Awareness (AA)\nscores are obtained for each anchor. These AA scores serve as data proxies to\ncompute relative distances in metric learning approaches. Extensive experiments\nconducted on two audio-visual benchmark datasets demonstrate the effectiveness\nof our proposed AADML method, significantly surpassing state-of-the-art models.\nFurthermore, we investigate the integration of AA proxies with various metric\nlearning methods, further highlighting the efficacy of our approach.",
      "tldr_zh": "该论文提出了一种创新的Anchor-aware Deep Metric Learning (AADML)方法，用于解决音频-视觉跨模态检索 (AV-CMR) 中的问题。传统度量学习因训练数据点稀缺而无法充分探索嵌入空间，导致正负样本分布表示不完整；AADML 通过构建基于相关性图的流形结构，并利用注意力驱动机制计算Anchor Awareness (AA)得分，作为数据代理来计算相对距离，从而提升共享嵌入空间的质量。在两个音频-视觉基准数据集上的大量实验显示，AADML显著优于现有最先进模型，并证明了AA代理与其他度量学习方法的兼容性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 5 figures. Accepted by ACM ICMR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13789v1",
      "published_date": "2024-04-21 22:44:44 UTC",
      "updated_date": "2024-04-21 22:44:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:14:58.662895"
    },
    {
      "arxiv_id": "2404.13788v3",
      "title": "AnyPattern: Towards In-context Image Copy Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhao Wang",
        "Yifan Sun",
        "Zhentao Tan",
        "Yi Yang"
      ],
      "abstract": "This paper explores in-context learning for image copy detection (ICD), i.e.,\nprompting an ICD model to identify replicated images with new tampering\npatterns without the need for additional training. The prompts (or the\ncontexts) are from a small set of image-replica pairs that reflect the new\npatterns and are used at inference time. Such in-context ICD has good realistic\nvalue, because it requires no fine-tuning and thus facilitates fast reaction\nagainst the emergence of unseen patterns. To accommodate the \"seen\n$\\rightarrow$ unseen\" generalization scenario, we construct the first\nlarge-scale pattern dataset named AnyPattern, which has the largest number of\ntamper patterns ($90$ for training and $10$ for testing) among all the existing\nones. We benchmark AnyPattern with popular ICD methods and reveal that existing\nmethods barely generalize to novel patterns. We further propose a simple\nin-context ICD method named ImageStacker. ImageStacker learns to select the\nmost representative image-replica pairs and employs them as the pattern prompts\nin a stacking manner (rather than the popular concatenation manner).\nExperimental results show (1) training with our large-scale dataset\nsubstantially benefits pattern generalization ($+26.66 \\%$ $\\mu AP$), (2) the\nproposed ImageStacker facilitates effective in-context ICD (another round of\n$+16.75 \\%$ $\\mu AP$), and (3) AnyPattern enables in-context ICD, i.e., without\nsuch a large-scale dataset, in-context learning does not emerge even with our\nImageStacker. Beyond the ICD task, we also demonstrate how AnyPattern can\nbenefit artists, i.e., the pattern retrieval method trained on AnyPattern can\nbe generalized to identify style mimicry by text-to-image models. The project\nis publicly available at https://anypattern.github.io.",
      "tldr_zh": "这篇论文探讨了图像复制检测 (ICD) 中的 in-context learning 方法，允许模型通过一小套图像-复制对作为提示，识别新篡改模式而无需额外训练。作者构建了首个大规模数据集 AnyPattern，包含 90 个训练模式和 10 个测试模式，以支持“已见→未见”泛化场景。实验基准测试显示，现有机器学习方法难以泛化到新模式，而提出的 ImageStacker 方法通过选择最代表性图像-复制对并以堆叠方式使用提示，显著提升了性能（+26.66% μAP 泛化提升，+16.75% μAP in-context 提升）。此外，AnyPattern 还可扩展应用，如帮助艺术家识别文本到图像模型的风格模仿。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The project is publicly available at https://anypattern.github.io",
      "pdf_url": "http://arxiv.org/pdf/2404.13788v3",
      "published_date": "2024-04-21 22:33:57 UTC",
      "updated_date": "2024-09-28 13:03:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:15:12.674864"
    },
    {
      "arxiv_id": "2404.16873v1",
      "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Anselm Paulus",
        "Arman Zharmagambetov",
        "Chuan Guo",
        "Brandon Amos",
        "Yuandong Tian"
      ],
      "abstract": "While recently Large Language Models (LLMs) have achieved remarkable\nsuccesses, they are vulnerable to certain jailbreaking attacks that lead to\ngeneration of inappropriate or harmful content. Manual red-teaming requires\nfinding adversarial prompts that cause such jailbreaking, e.g. by appending a\nsuffix to a given instruction, which is inefficient and time-consuming. On the\nother hand, automatic adversarial prompt generation often leads to semantically\nmeaningless attacks that can easily be detected by perplexity-based filters,\nmay require gradient information from the TargetLLM, or do not scale well due\nto time-consuming discrete optimization processes over the token space. In this\npaper, we present a novel method that uses another LLM, called the AdvPrompter,\nto generate human-readable adversarial prompts in seconds, $\\sim800\\times$\nfaster than existing optimization-based approaches. We train the AdvPrompter\nusing a novel algorithm that does not require access to the gradients of the\nTargetLLM. This process alternates between two steps: (1) generating\nhigh-quality target adversarial suffixes by optimizing the AdvPrompter\npredictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated\nadversarial suffixes. The trained AdvPrompter generates suffixes that veil the\ninput instruction without changing its meaning, such that the TargetLLM is\nlured to give a harmful response. Experimental results on popular open source\nTargetLLMs show state-of-the-art results on the AdvBench dataset, that also\ntransfer to closed-source black-box LLM APIs. Further, we demonstrate that by\nfine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made\nmore robust against jailbreaking attacks while maintaining performance, i.e.\nhigh MMLU scores.",
      "tldr_zh": "该论文提出 AdvPrompter，一种快速自适应对抗提示生成方法，用于提升大型语言模型（LLMs）的安全性，以应对 jailbreaking 攻击导致的有害内容生成。AdvPrompter 利用另一个 LLM 通过新型算法生成人类可读的对抗后缀，该算法交替优化高质目标后缀和低秩微调，而无需访问 TargetLLM 的梯度信息，从而实现约 800 倍于现有优化方法的加速。实验在 AdvBench 数据集上显示，AdvPrompter 取得了最先进的结果，并能转移到闭源黑盒 LLM API；此外，通过用其生成的合成数据集微调，LLMs 的鲁棒性得到增强，同时保持高 MMLU 分数。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "32 pages, 9 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.16873v1",
      "published_date": "2024-04-21 22:18:13 UTC",
      "updated_date": "2024-04-21 22:18:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:15:23.170467"
    },
    {
      "arxiv_id": "2407.02495v1",
      "title": "Minds, Brains, AI",
      "title_zh": "心智、大脑与 AI",
      "authors": [
        "Jay Seitz"
      ],
      "abstract": "In the last year or so and going back many decades there has been extensive\nclaims by major computational scientists, engineers, and others that AGI,\nartificial general intelligence, is five or ten years away, but without a\nscintilla of scientific evidence, for a broad body of these claims. Computers\nwill become conscious, have a theory of mind, think and reason, will become\nmore intelligent than humans, and so on. But the claims are science fiction,\nnot science. This article reviews evidence for the following three propositions\nusing extensive body of scientific research and related sources from the\ncognitive and neurosciences, evolutionary evidence, linguistics, data science,\ncomparative psychology, self-driving cars, robotics. and the learning sciences.\n(1) Do computing machines think or reason? (2) Are computing machines sentient\nor conscious? (3) Do computing machines have a theory of mind?",
      "tldr_zh": "这篇论文审视了关于AGI（artificial general intelligence）即将到来的夸大声明，这些断言缺乏科学证据，并认为计算机将变得有意识、拥有心智理论（theory of mind）、思考推理或超越人类智能等观点纯属科幻而非科学。作者通过回顾认知神经科学、进化证据、语言学、数据科学、比较心理学、自动驾驶汽车、机器人学和学习科学等领域的大量研究，评估三个关键命题：(1) 计算机是否能思考或推理？(2) 计算机是否是感性的或有意识的？(3) 计算机是否拥有心智理论。最终，论文基于科学证据驳斥了这些不实声明，强调了AGI发展的现实局限性。",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02495v1",
      "published_date": "2024-04-21 21:49:42 UTC",
      "updated_date": "2024-04-21 21:49:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:15:34.902849"
    },
    {
      "arxiv_id": "2404.13786v1",
      "title": "Soar: Design and Deployment of A Smart Roadside Infrastructure System for Autonomous Driving",
      "title_zh": "Soar：用于自动驾驶的智能路侧基础设施系统的设计与部署",
      "authors": [
        "Shuyao Shi",
        "Neiwen Ling",
        "Zhehao Jiang",
        "Xuan Huang",
        "Yuze He",
        "Xiaoguang Zhao",
        "Bufang Yang",
        "Chen Bian",
        "Jingfei Xia",
        "Zhenyu Yan",
        "Raymond Yeung",
        "Guoliang Xing"
      ],
      "abstract": "Recently,smart roadside infrastructure (SRI) has demonstrated the potential\nof achieving fully autonomous driving systems. To explore the potential of\ninfrastructure-assisted autonomous driving, this paper presents the design and\ndeployment of Soar, the first end-to-end SRI system specifically designed to\nsupport autonomous driving systems. Soar consists of both software and hardware\ncomponents carefully designed to overcome various system and physical\nchallenges. Soar can leverage the existing operational infrastructure like\nstreet lampposts for a lower barrier of adoption. Soar adopts a new\ncommunication architecture that comprises a bi-directional multi-hop I2I\nnetwork and a downlink I2V broadcast service, which are designed based on\noff-the-shelf 802.11ac interfaces in an integrated manner. Soar also features a\nhierarchical DL task management framework to achieve desirable load balancing\namong nodes and enable them to collaborate efficiently to run multiple\ndata-intensive autonomous driving applications. We deployed a total of 18 Soar\nnodes on existing lampposts on campus, which have been operational for over two\nyears. Our real-world evaluation shows that Soar can support a diverse set of\nautonomous driving applications and achieve desirable real-time performance and\nhigh communication reliability. Our findings and experiences in this work offer\nkey insights into the development and deployment of next-generation smart\nroadside infrastructure and autonomous driving systems.",
      "tldr_zh": "本论文介绍了 Soar 系统，这是一个端到端的智能路边基础设施 (SRI) 系统，旨在通过硬件和软件组件支持自动驾驶应用。Soar 利用现有路灯柱降低部署门槛，采用创新通信架构（包括双向多跳 I2I 网络和 I2V 广播服务基于 802.11ac 接口），并引入分层 DL 任务管理框架，实现节点间负载均衡和高效协作。在校园部署的 18 个节点运行超过两年后，实世评估显示 Soar 支持多样自动驾驶应用，实现了高实时性能和通信可靠性，并为下一代智能路边基础设施和自动驾驶系统的发展提供关键见解。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13786v1",
      "published_date": "2024-04-21 21:45:23 UTC",
      "updated_date": "2024-04-21 21:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:15:47.993124"
    },
    {
      "arxiv_id": "2404.13778v1",
      "title": "Multi-channel Emotion Analysis for Consensus Reaching in Group Movie Recommendation Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Adilet Yerkin",
        "Elnara Kadyrgali",
        "Yerdauit Torekhan",
        "Pakizar Shamoi"
      ],
      "abstract": "Watching movies is one of the social activities typically done in groups.\nEmotion is the most vital factor that affects movie viewers' preferences. So,\nthe emotional aspect of the movie needs to be determined and analyzed for\nfurther recommendations. It can be challenging to choose a movie that appeals\nto the emotions of a diverse group. Reaching an agreement for a group can be\ndifficult due to the various genres and choices. This paper proposes a novel\napproach to group movie suggestions by examining emotions from three different\nchannels: movie descriptions (text), soundtracks (audio), and posters (image).\nWe employ the Jaccard similarity index to match each participant's emotional\npreferences to prospective movie choices, followed by a fuzzy inference\ntechnique to determine group consensus. We use a weighted integration process\nfor the fusion of emotion scores from diverse data types. Then, group movie\nrecommendation is based on prevailing emotions and viewers' best-loved movies.\nAfter determining the recommendations, the group's consensus level is\ncalculated using a fuzzy inference system, taking participants' feedback as\ninput. Participants (n=130) in the survey were provided with different emotion\ncategories and asked to select the emotions best suited for particular movies\n(n=12). Comparison results between predicted and actual scores demonstrate the\nefficiency of using emotion detection for this problem (Jaccard similarity\nindex = 0.76). We explored the relationship between induced emotions and movie\npopularity as an additional experiment, analyzing emotion distribution in 100\npopular movies from the TMDB database. Such systems can potentially improve the\naccuracy of movie recommendation systems and achieve a high level of consensus\namong participants with diverse preferences.",
      "tldr_zh": "这篇论文提出了一种多渠道情感分析方法，用于群体电影推荐系统，以解决不同参与者偏好多样性导致的共识难题。方法通过分析电影描述（文本）、配乐（音频）和海报（图像）三个渠道的情感，使用 Jaccard similarity index 匹配参与者的情感偏好，并结合模糊推理技术进行群体共识计算和加权融合推荐。实验结果显示，该方法在对 130 名参与者的调查中，预测情感准确率达到 Jaccard similarity index 为 0.76，并通过分析 100 部 TMDB 热门电影的情感分布，证明了其能显著提高推荐系统的准确性和群体共识水平。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "the paper has been submitted for consideration to IEEE",
      "pdf_url": "http://arxiv.org/pdf/2404.13778v1",
      "published_date": "2024-04-21 21:19:31 UTC",
      "updated_date": "2024-04-21 21:19:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:15:58.974804"
    },
    {
      "arxiv_id": "2404.14459v2",
      "title": "LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities and Limitations",
      "title_zh": "翻译失败",
      "authors": [
        "Rebeka Tóth",
        "Tamas Bisztray",
        "László Erdodi"
      ],
      "abstract": "This study evaluates the security of web application code generated by Large\nLanguage Models, analyzing 2,500 GPT-4 generated PHP websites. These were\ndeployed in Docker containers and tested for vulnerabilities using a hybrid\napproach of Burp Suite active scanning, static analysis, and manual review. Our\ninvestigation focuses on identifying Insecure File Upload, SQL Injection,\nStored XSS, and Reflected XSS in GPT-4 generated PHP code. This analysis\nhighlights potential security risks and the implications of deploying such code\nin real-world scenarios. Overall, our analysis found 2,440 vulnerable\nparameters. According to Burp's Scan, 11.56% of the sites can be straight out\ncompromised. Adding static scan results, 26% had at least one vulnerability\nthat can be exploited through web interaction. Certain coding scenarios, like\nfile upload functionality, are insecure 78% of the time, underscoring\nsignificant risks to software safety and security. To support further research,\nwe have made the source codes and a detailed vulnerability record for each\nsample publicly available. This study emphasizes the crucial need for thorough\ntesting and evaluation if generative AI technologies are used in software\ndevelopment.",
      "tldr_zh": "本研究评估了大型语言模型（LLMs）生成的 PHP 代码的安全性，分析了由 GPT-4 生成的 2500 个网站。研究采用混合方法，包括 Burp Suite 主动扫描、静态分析和手动审查，重点识别 Insecure File Upload、SQL Injection、Stored XSS 和 Reflected XSS 等漏洞。结果显示，2440 个参数存在漏洞，11.56% 的站点可直接被入侵，26% 的站点有可通过网络交互利用的漏洞，而文件上传功能有 78% 的不安全率。该研究强调了在软件开发中使用生成式 AI 的潜在风险，并公开了源代码和漏洞记录，以推动进一步的测试和改进。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.14459v2",
      "published_date": "2024-04-21 20:56:02 UTC",
      "updated_date": "2024-05-21 13:10:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:16:11.085875"
    },
    {
      "arxiv_id": "2404.13770v1",
      "title": "EncodeNet: A Framework for Boosting DNN Accuracy with Entropy-driven Generalized Converting Autoencoder",
      "title_zh": "翻译失败",
      "authors": [
        "Hasanul Mahmud",
        "Kevin Desai",
        "Palden Lama",
        "Sushil K. Prasad"
      ],
      "abstract": "Image classification is a fundamental task in computer vision, and the quest\nto enhance DNN accuracy without inflating model size or latency remains a\npressing concern. We make a couple of advances in this regard, leading to a\nnovel EncodeNet design and training framework. The first advancement involves\nConverting Autoencoders, a novel approach that transforms images into an\neasy-to-classify image of its class. Our prior work that applied the Converting\nAutoencoder and a simple classifier in tandem achieved moderate accuracy over\nsimple datasets, such as MNIST and FMNIST. However, on more complex datasets\nlike CIFAR-10, the Converting Autoencoder has a large reconstruction loss,\nmaking it unsuitable for enhancing DNN accuracy. To address these limitations,\nwe generalize the design of Converting Autoencoders by leveraging a larger\nclass of DNNs, those with architectures comprising feature extraction layers\nfollowed by classification layers. We incorporate a generalized algorithmic\ndesign of the Converting Autoencoder and intraclass clustering to identify\nrepresentative images, leading to optimized image feature learning. Next, we\ndemonstrate the effectiveness of our EncodeNet design and training framework,\nimproving the accuracy of well-trained baseline DNNs while maintaining the\noverall model size. EncodeNet's building blocks comprise the trained encoder\nfrom our generalized Converting Autoencoders transferring knowledge to a\nlightweight classifier network - also extracted from the baseline DNN. Our\nexperimental results demonstrate that EncodeNet improves the accuracy of VGG16\nfrom 92.64% to 94.05% on CIFAR-10 and RestNet20 from 74.56% to 76.04% on\nCIFAR-100. It outperforms state-of-the-art techniques that rely on knowledge\ndistillation and attention mechanisms, delivering higher accuracy for models of\ncomparable size.",
      "tldr_zh": "这篇论文提出了 EncodeNet 框架，利用 Entropy-driven Generalized Converting Autoencoder 来提升 DNN（Deep Neural Networks）的图像分类准确率，同时保持模型大小和延迟不变。框架通过泛化 Converting Autoencoders 和 intraclass clustering 优化图像特征学习，将原始图像转换为易于分类的形式，并将训练过的编码器知识转移到一个轻量级分类器网络中。实验结果显示，EncodeNet 在 CIFAR-10 上将 VGG16 的准确率从 92.64% 提高到 94.05%，并在 CIFAR-100 上将 ResNet20 的准确率从 74.56% 提升到 76.04%，优于基于知识蒸馏和注意力机制的现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.13770v1",
      "published_date": "2024-04-21 20:45:18 UTC",
      "updated_date": "2024-04-21 20:45:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:16:26.798468"
    },
    {
      "arxiv_id": "2404.13767v1",
      "title": "Autonomous Robot for Disaster Mapping and Victim Localization",
      "title_zh": "用于灾害映射和受害者定位的自主机器人",
      "authors": [
        "Michael Potter",
        "Rahil Bhowal",
        "Richard Zhao",
        "Anuj Patel",
        "Jingming Cheng"
      ],
      "abstract": "In response to the critical need for effective reconnaissance in disaster\nscenarios, this research article presents the design and implementation of a\ncomplete autonomous robot system using the Turtlebot3 with Robotic Operating\nSystem (ROS) Noetic. Upon deployment in closed, initially unknown environments,\nthe system aims to generate a comprehensive map and identify any present\n'victims' using AprilTags as stand-ins. We discuss our solution for search and\nrescue missions, while additionally exploring more advanced algorithms to\nimprove search and rescue functionalities. We introduce a Cubature Kalman\nFilter to help reduce the mean squared error [m] for AprilTag localization and\nan information-theoretic exploration algorithm to expedite exploration in\nunknown environments. Just like turtles, our system takes it slow and steady,\nbut when it's time to save the day, it moves at ninja-like speed! Despite\nDonatello's shell, he's no slowpoke - he zips through obstacles with the\nagility of a teenage mutant ninja turtle. So, hang on tight to your shells and\nget ready for a whirlwind of reconnaissance!\n  Full pipeline code https://github.com/rzhao5659/MRProject/tree/main\n  Exploration code https://github.com/rzhao5659/MRProject/tree/main",
      "tldr_zh": "本研究设计了一个基于 Turtlebot3 和 Robotic Operating System (ROS) Noetic 的自主机器人系统，旨在在封闭未知环境中生成全面地图并定位受害者（使用 AprilTags 作为替代）。系统引入 Cubature Kalman Filter 来降低 AprilTag 定位的均方误差，以及一个信息理论探索算法来加速未知环境的探索。实验结果显示，该系统显著提升了搜索和救援任务的效率和准确性，为灾害响应提供可靠的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "Class final project for Northeastern University EECE 5550 Mobile\n  Robotics Course",
      "pdf_url": "http://arxiv.org/pdf/2404.13767v1",
      "published_date": "2024-04-21 20:32:02 UTC",
      "updated_date": "2024-04-21 20:32:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:16:35.151088"
    },
    {
      "arxiv_id": "2404.13752v3",
      "title": "Adversarial Representation Engineering: A General Model Editing Framework for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yihao Zhang",
        "Zeming Wei",
        "Jun Sun",
        "Meng Sun"
      ],
      "abstract": "Since the rapid development of Large Language Models (LLMs) has achieved\nremarkable success, understanding and rectifying their internal complex\nmechanisms has become an urgent issue. Recent research has attempted to\ninterpret their behaviors through the lens of inner representation. However,\ndeveloping practical and efficient methods for applying these representations\nfor general and flexible model editing remains challenging. In this work, we\nexplore how to leverage insights from representation engineering to guide the\nediting of LLMs by deploying a representation sensor as an editing oracle. We\nfirst identify the importance of a robust and reliable sensor during editing,\nthen propose an Adversarial Representation Engineering (ARE) framework to\nprovide a unified and interpretable approach for conceptual model editing\nwithout compromising baseline performance. Experiments on multiple tasks\ndemonstrate the effectiveness of ARE in various model editing scenarios. Our\ncode and data are available at\nhttps://github.com/Zhang-Yihao/Adversarial-Representation-Engineering.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）的内部机制复杂性，提出一种通用的模型编辑框架——Adversarial Representation Engineering (ARE)。ARE 框架通过部署 representation sensor 作为编辑指导，利用对抗性方法实现概念模型编辑，确保不损害基线性能，同时提供可解释性和灵活性。实验在多个任务上验证了 ARE 的有效性，展示了其在理解和修正 LLMs 行为方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13752v3",
      "published_date": "2024-04-21 19:24:15 UTC",
      "updated_date": "2024-11-01 07:51:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:16:47.576860"
    },
    {
      "arxiv_id": "2404.13745v1",
      "title": "A Nasal Cytology Dataset for Object Detection and Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mauro Camporeale",
        "Giovanni Dimauro",
        "Matteo Gelardi",
        "Giorgia Iacobellis",
        "Mattia Sebastiano Ladisa",
        "Sergio Latrofa",
        "Nunzia Lomonte"
      ],
      "abstract": "Nasal Cytology is a new and efficient clinical technique to diagnose rhinitis\nand allergies that is not much widespread due to the time-consuming nature of\ncell counting; that is why AI-aided counting could be a turning point for the\ndiffusion of this technique. In this article we present the first dataset of\nrhino-cytological field images: the NCD (Nasal Cytology Dataset), aimed to\ntrain and deploy Object Detection models to support physicians and biologists\nduring clinical practice. The real distribution of the cytotypes, populating\nthe nasal mucosa has been replicated, sampling images from slides of clinical\npatients, and manually annotating each cell found on them. The correspondent\nobject detection task presents non'trivial issues associated with the strong\nclass imbalancement, involving the rarest cell types. This work contributes to\nsome of open challenges by presenting a novel machine learning-based approach\nto aid the automated detection and classification of nasal mucosa cells: the\nDETR and YOLO models shown good performance in detecting cells and classifying\nthem correctly, revealing great potential to accelerate the work of rhinology\nexperts.",
      "tldr_zh": "本文介绍了 Nasal Cytology Dataset (NCD)，这是第一个用于 Object Detection 的鼻细胞学图像数据集，旨在通过 AI 辅助细胞计数来推广鼻炎和过敏的诊断技术。数据集从临床患者幻灯片采样并手动标注，复制了鼻黏膜细胞的真实分布，同时处理了类别不平衡问题，如稀有细胞类型的挑战。研究采用 DETR 和 YOLO 模型进行细胞检测和分类，结果显示这些模型表现出色，能够有效加速鼻科专家的工作。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Pre Print almost ready to be submitted",
      "pdf_url": "http://arxiv.org/pdf/2404.13745v1",
      "published_date": "2024-04-21 19:02:38 UTC",
      "updated_date": "2024-04-21 19:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:00.559833"
    },
    {
      "arxiv_id": "2404.13742v1",
      "title": "Seamless Underwater Navigation with Limited Doppler Velocity Log Measurements",
      "title_zh": "翻译失败",
      "authors": [
        "Nadav Cohen",
        "Itzik Klein"
      ],
      "abstract": "Autonomous Underwater Vehicles (AUVs) commonly utilize an inertial navigation\nsystem (INS) and a Doppler velocity log (DVL) for underwater navigation. To\nthat end, their measurements are integrated through a nonlinear filter such as\nthe extended Kalman filter (EKF). The DVL velocity vector estimate depends on\nretrieving reflections from the seabed, ensuring that at least three out of its\nfour transmitted acoustic beams return successfully. When fewer than three\nbeams are obtained, the DVL cannot provide a velocity update to bind the\nnavigation solution drift. To cope with this challenge, in this paper, we\npropose a hybrid neural coupled (HNC) approach for seamless AUV navigation in\nsituations of limited DVL measurements. First, we drive an approach to regress\ntwo or three missing DVL beams. Then, those beams, together with the measured\nbeams, are incorporated into the EKF. We examined INS/DVL fusion both in\nloosely and tightly coupled approaches. Our method was trained and evaluated on\nrecorded data from AUV experiments conducted in the Mediterranean Sea on two\ndifferent occasions. The results illustrate that our proposed method\noutperforms the baseline loosely and tightly coupled model-based approaches by\nan average of 96.15%. It also demonstrates superior performance compared to a\nmodel-based beam estimator by an average of 12.41% in terms of velocity\naccuracy for scenarios involving two or three missing beams. Therefore, we\ndemonstrate that our approach offers seamless AUV navigation in situations of\nlimited beam measurements.",
      "tldr_zh": "这篇论文提出了一种混合神经耦合 (HNC) 方法，用于在 Doppler Velocity Log (DVL) 测量有限时实现自主水下车辆 (AUV) 的无缝导航，解决 DVL 声束不足导致的惯性导航系统 (INS) 漂移问题。方法首先利用神经网络回归缺失的两个或三个 DVL 声束，然后将这些预测声束与实际测量整合到扩展卡尔曼滤波器 (EKF) 中，支持松散和紧密耦合的 INS/DVL 融合。实验结果显示，该方法在地中海 AUV 数据上比基线模型平均提升 96.15%，并在速度准确性上优于传统模型-based 波束估计器 12.41%，从而实现了可靠的 AUV 导航。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13742v1",
      "published_date": "2024-04-21 18:56:54 UTC",
      "updated_date": "2024-04-21 18:56:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:14.575301"
    },
    {
      "arxiv_id": "2404.13737v4",
      "title": "Stochastic Multi-round Submodular Optimization with Budget",
      "title_zh": "翻译失败",
      "authors": [
        "Vincenzo Auletta",
        "Diodato Ferraioli",
        "Cosimo Vinci"
      ],
      "abstract": "In this work, we study the Stochastic Budgeted Multi-round Submodular\nMaximization (SBMSm) problem, where we aim to adaptively maximize the sum, over\nmultiple rounds, of a monotone and submodular objective function defined on\nsubsets of items. The objective function also depends on the realization of\nstochastic events, and the total number of items we can select over all rounds\nis bounded by a limited budget. This problem extends, and generalizes to\nmultiple round settings, well-studied problems such as (adaptive) influence\nmaximization and stochastic probing.\n  We show that, if the number of items and stochastic events is somehow\nbounded, there is a polynomial time dynamic programming algorithm for SBMSm.\nThen, we provide a simple greedy $1/2(1-1/e-\\epsilon)\\approx\n0.316$-approximation algorithm for SBMSm, that first non-adaptively allocates\nthe budget to be spent at each round, and then greedily and adaptively\nmaximizes the objective function by using the budget assigned at each round.\nFinally, we introduce the {\\em budget-adaptivity gap}, by which we measure how\nmuch an adaptive policy for SBMSm is better than an optimal partially adaptive\none that, as in our greedy algorithm, determines the budget allocation in\nadvance. We show that the budget-adaptivity gap lies between $e/(e-1)\\approx\n1.582$ and $2$.",
      "tldr_zh": "本研究探讨了 Stochastic Budgeted Multi-round Submodular Maximization (SBMSm) 问题，旨在多轮中最大化一个单调且 submodular 的目标函数，同时处理随机事件并受预算限制，从而扩展了影响最大化和随机探测等经典问题。论文提出了一种多项式时间动态规划算法，适用于项目和随机事件数量有限的场景，以及一个简单的贪婪算法，该算法先非自适应地分配预算，然后在每个轮次自适应地最大化函数，提供约 0.316 的近似率。最终，研究引入了 budget-adaptivity gap，量化自适应策略相对于预先预算分配策略的改进潜力，其范围在 1.582 到 2 之间。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13737v4",
      "published_date": "2024-04-21 18:24:43 UTC",
      "updated_date": "2024-09-25 16:53:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:25.646884"
    },
    {
      "arxiv_id": "2404.13736v2",
      "title": "Interval Abstractions for Robust Counterfactual Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Junqi Jiang",
        "Francesco Leofante",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "abstract": "Counterfactual Explanations (CEs) have emerged as a major paradigm in\nexplainable AI research, providing recourse recommendations for users affected\nby the decisions of machine learning models. However, CEs found by existing\nmethods often become invalid when slight changes occur in the parameters of the\nmodel they were generated for. The literature lacks a way to provide exhaustive\nrobustness guarantees for CEs under model changes, in that existing methods to\nimprove CEs' robustness are mostly heuristic, and the robustness performances\nare evaluated empirically using only a limited number of retrained models. To\nbridge this gap, we propose a novel interval abstraction technique for\nparametric machine learning models, which allows us to obtain provable\nrobustness guarantees for CEs under a possibly infinite set of plausible model\nchanges $\\Delta$. Based on this idea, we formalise a robustness notion for CEs,\nwhich we call $\\Delta$-robustness, in both binary and multi-class\nclassification settings. We present procedures to verify $\\Delta$-robustness\nbased on Mixed Integer Linear Programming, using which we further propose\nalgorithms to generate CEs that are $\\Delta$-robust. In an extensive empirical\nstudy involving neural networks and logistic regression models, we demonstrate\nthe practical applicability of our approach. We discuss two strategies for\ndetermining the appropriate hyperparameters in our method, and we\nquantitatively benchmark CEs generated by eleven methods, highlighting the\neffectiveness of our algorithms in finding robust CEs.",
      "tldr_zh": "该论文针对 Counterfactual Explanations (CEs) 在解释性 AI 中的问题，指出现有方法生成的 CEs 对模型参数微小变化不鲁棒，缺乏全面的鲁棒性保证。作者提出了一种新型 interval abstraction 技术，应用于参数化机器学习模型，能够在无限可能的模型变化 Δ 下，提供可证明的鲁棒性保障，并正式定义了 Δ-robustness 概念，适用于二元和多类分类。基于 Mixed Integer Linear Programming (MILP)，论文开发了验证和生成 Δ-robust CEs 的算法，并在神经网络和逻辑回归模型的广泛实证研究中证明了其有效性，与其他 11 种方法相比表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in Artificial Intelligence Journal",
      "pdf_url": "http://arxiv.org/pdf/2404.13736v2",
      "published_date": "2024-04-21 18:24:34 UTC",
      "updated_date": "2024-11-22 15:01:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:37.312910"
    },
    {
      "arxiv_id": "2404.13733v4",
      "title": "Elucidating the Design Space of Dataset Condensation",
      "title_zh": "翻译失败",
      "authors": [
        "Shitong Shao",
        "Zikai Zhou",
        "Huanran Chen",
        "Zhiqiang Shen"
      ],
      "abstract": "Dataset condensation, a concept within data-centric learning, efficiently\ntransfers critical attributes from an original dataset to a synthetic version,\nmaintaining both diversity and realism. This approach significantly improves\nmodel training efficiency and is adaptable across multiple application areas.\nPrevious methods in dataset condensation have faced challenges: some incur high\ncomputational costs which limit scalability to larger datasets (e.g., MTT,\nDREAM, and TESLA), while others are restricted to less optimal design spaces,\nwhich could hinder potential improvements, especially in smaller datasets\n(e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a\ncomprehensive design framework that includes specific, effective strategies\nlike implementing soft category-aware matching and adjusting the learning rate\nschedule. These strategies are grounded in empirical evidence and theoretical\nbacking. Our resulting approach, Elucidate Dataset Condensation (EDC),\nestablishes a benchmark for both small and large-scale dataset condensation. In\nour testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on\nImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a\ncompression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM,\nand RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.",
      "tldr_zh": "该论文探讨了数据集浓缩（Dataset Condensation）的设计空间，旨在通过高效转移原始数据集的关键属性来提升模型训练效率，同时解决现有方法的高计算成本（如 MTT、DREAM 和 TESLA）和设计限制（如 SRe2L、G-VBSM 和 RDED）的问题。作者提出了一种全面框架 Elucidate Dataset Condensation (EDC)，包括软类别感知匹配（soft category-aware matching）和学习率调度调整等策略，这些基于实证和理论支持。实验结果显示，EDC 在 ImageNet-1k 上使用 ResNet-18 模型时，达到 48.6% 的最先进准确率（IPC 为 10，压缩比为 0.78%），分别比 SRe2L、G-VBSM 和 RDED 高出 27.3%、17.2% 和 6.6%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13733v4",
      "published_date": "2024-04-21 18:19:27 UTC",
      "updated_date": "2025-01-17 07:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:49.072916"
    },
    {
      "arxiv_id": "2404.13721v1",
      "title": "The Framework of a Design Process Language",
      "title_zh": "设计过程语言的框架",
      "authors": [
        "Arnulf Hagen"
      ],
      "abstract": "The thesis develops a view of design in a concept formation framework and\noutlines a language to describe both the object of the design and the process\nof designing. The unknown object at the outset of the design work may be seen\nas an unknown concept that the designer is to define. Throughout the process,\nshe develops a description of this object by relating it to known concepts. The\nsearch stops when the designer is satisfied that the design specification is\ncomplete enough to satisfy the requirements from it once built. It is then a\ncollection of propositions that all contribute towards defining the design\nobject - a collection of sentences describing relationships between the object\nand known concepts. Also, the design process itself may be described by\nrelating known concepts - by organizing known abilities into particular\npatterns of activation, or mobilization. In view of the demands posed to a\nlanguage to use in this concept formation process, the framework of a Design\nProcess Language (DPL) is developed. The basis for the language are linguistic\ncategories that act as classes of relations used to combine concepts,\ncontaining relations used for describing process and object within the same\ngeneral system, with some relations being process specific, others being object\nspecific, and with the bulk being used both for process and object description.\nAnother outcome is the distinction of modal relations, or relations describing\nfuturity, possibility, willingness, hypothetical events, and the like. The\ndesign process almost always includes aspects such as these, and it is thus\nnecessary for a language facilitating design process description to support\nsuch relationships to be constructed. The DPL is argued to be a foundation\nwhereupon to build a language that can be used for enabling computers to be\nmore useful - act more intelligently - in the design process.",
      "tldr_zh": "该论文将设计视为一个概念形成框架，提出 Design Process Language (DPL) 的框架，用于描述设计对象和设计过程。DPL 通过关系 (relations) 将未知设计对象与已知概念相连，包括过程特定、对象特定和通用关系类别，并特别区分模态关系 (modal relations) 如未来性、可能性和假设事件，以支持设计过程的全面描述。最终，该框架旨在提升计算机在设计过程中的智能性，使其更有效地参与概念形成和决策。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "PhD dissertation, 1993, Norwegian Institute of Technology",
      "pdf_url": "http://arxiv.org/pdf/2404.13721v1",
      "published_date": "2024-04-21 17:20:19 UTC",
      "updated_date": "2024-04-21 17:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:17:59.758782"
    },
    {
      "arxiv_id": "2404.13719v1",
      "title": "A Practical Multilevel Governance Framework for Autonomous and Intelligent Systems",
      "title_zh": "一个实用的多级治理框架，用于自治和智能系统",
      "authors": [
        "Lukas D. Pöhler",
        "Klaus Diepold",
        "Wendell Wallach"
      ],
      "abstract": "Autonomous and intelligent systems (AIS) facilitate a wide range of\nbeneficial applications across a variety of different domains. However,\ntechnical characteristics such as unpredictability and lack of transparency, as\nwell as potential unintended consequences, pose considerable challenges to the\ncurrent governance infrastructure. Furthermore, the speed of development and\ndeployment of applications outpaces the ability of existing governance\ninstitutions to put in place effective ethical-legal oversight. New approaches\nfor agile, distributed and multilevel governance are needed. This work presents\na practical framework for multilevel governance of AIS. The framework enables\nmapping actors onto six levels of decision-making including the international,\nnational and organizational levels. Furthermore, it offers the ability to\nidentify and evolve existing tools or create new tools for guiding the behavior\nof actors within the levels. Governance mechanisms enable actors to shape and\nenforce regulations and other tools, which when complemented with good\npractices contribute to effective and comprehensive governance.",
      "tldr_zh": "该论文针对自治和智能系统(Autonomous and Intelligent Systems, AIS)的不可预测性、缺乏透明度和潜在风险等挑战，指出现有治理基础设施无法跟上其快速发展速度。作者提出一个实用的多级治理框架，将参与者映射到六个决策级别，包括国际、国家和组织层面，并允许识别、演化和创建工具来指导行为。该框架通过治理机制和良好实践相结合，帮助塑造、执行法规，实现敏捷、分布式和有效的综合治理。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13719v1",
      "published_date": "2024-04-21 17:15:43 UTC",
      "updated_date": "2024-04-21 17:15:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:18:11.477689"
    },
    {
      "arxiv_id": "2404.13706v1",
      "title": "Concept Arithmetics for Circumventing Concept Inhibition in Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Vitali Petsiuk",
        "Kate Saenko"
      ],
      "abstract": "Motivated by ethical and legal concerns, the scientific community is actively\ndeveloping methods to limit the misuse of Text-to-Image diffusion models for\nreproducing copyrighted, violent, explicit, or personal information in the\ngenerated images. Simultaneously, researchers put these newly developed safety\nmeasures to the test by assuming the role of an adversary to find\nvulnerabilities and backdoors in them. We use compositional property of\ndiffusion models, which allows to leverage multiple prompts in a single image\ngeneration. This property allows us to combine other concepts, that should not\nhave been affected by the inhibition, to reconstruct the vector, responsible\nfor target concept generation, even though the direct computation of this\nvector is no longer accessible. We provide theoretical and empirical evidence\nwhy the proposed attacks are possible and discuss the implications of these\nfindings for safe model deployment. We argue that it is essential to consider\nall possible approaches to image generation with diffusion models that can be\nemployed by an adversary. Our work opens up the discussion about the\nimplications of concept arithmetics and compositional inference for safety\nmechanisms in diffusion models.\n  Content Advisory: This paper contains discussions and model-generated content\nthat may be considered offensive. Reader discretion is advised.\n  Project page: https://cs-people.bu.edu/vpetsiuk/arc",
      "tldr_zh": "这篇论文探讨了如何利用概念算术（Concept Arithmetics）绕过扩散模型（Diffusion Models）中的概念抑制（Concept Inhibition），以生成受版权保护、暴力或色情内容。作者利用扩散模型的组合属性（compositional property），通过结合未受影响的概念和多个提示来重建目标概念向量，即使直接计算受限。研究提供了理论和实证证据，证明这种攻击的可行性，并讨论了其对安全模型部署的潜在风险，强调需要全面考虑对手的图像生成策略。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13706v1",
      "published_date": "2024-04-21 16:35:16 UTC",
      "updated_date": "2024-04-21 16:35:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:18:25.186898"
    },
    {
      "arxiv_id": "2405.02321v1",
      "title": "Accelerating Medical Knowledge Discovery through Automated Knowledge Graph Generation and Enrichment",
      "title_zh": "翻译失败",
      "authors": [
        "Mutahira Khalid",
        "Raihana Rahman",
        "Asim Abbas",
        "Sushama Kumari",
        "Iram Wajahat",
        "Syed Ahmad Chan Bukhari"
      ],
      "abstract": "Knowledge graphs (KGs) serve as powerful tools for organizing and\nrepresenting structured knowledge. While their utility is widely recognized,\nchallenges persist in their automation and completeness. Despite efforts in\nautomation and the utilization of expert-created ontologies, gaps in\nconnectivity remain prevalent within KGs. In response to these challenges, we\npropose an innovative approach termed ``Medical Knowledge Graph Automation\n(M-KGA)\". M-KGA leverages user-provided medical concepts and enriches them\nsemantically using BioPortal ontologies, thereby enhancing the completeness of\nknowledge graphs through the integration of pre-trained embeddings. Our\napproach introduces two distinct methodologies for uncovering hidden\nconnections within the knowledge graph: a cluster-based approach and a\nnode-based approach. Through rigorous testing involving 100 frequently\noccurring medical concepts in Electronic Health Records (EHRs), our M-KGA\nframework demonstrates promising results, indicating its potential to address\nthe limitations of existing knowledge graph automation techniques.",
      "tldr_zh": "该研究针对知识图谱（KGs）在自动化和完整性方面的挑战，提出了一种名为“Medical Knowledge Graph Automation (M-KGA)”的创新框架，以加速医疗知识发现。M-KGA 通过利用用户提供的医疗概念、BioPortal ontologies 进行语义丰富，并整合 pre-trained embeddings，同时引入基于聚类和基于节点的两种方法，来揭示知识图谱中的隐藏连接。实验结果显示，在对100个在 Electronic Health Records (EHRs) 中频繁出现的医疗概念进行测试后，框架表现出色，有效解决了现有技术的局限性。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.02321v1",
      "published_date": "2024-04-21 15:54:27 UTC",
      "updated_date": "2024-04-21 15:54:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:18:36.512473"
    },
    {
      "arxiv_id": "2404.13680v3",
      "title": "Zero-shot High-fidelity and Pose-controllable Character Animation",
      "title_zh": "零样本高保真度且姿势可控的角色动画",
      "authors": [
        "Bingwen Zhu",
        "Fanyi Wang",
        "Tianyi Lu",
        "Peng Liu",
        "Jingwen Su",
        "Jinxiu Liu",
        "Yanhao Zhang",
        "Zuxuan Wu",
        "Guo-Jun Qi",
        "Yu-Gang Jiang"
      ],
      "abstract": "Image-to-video (I2V) generation aims to create a video sequence from a single\nimage, which requires high temporal coherence and visual fidelity. However,\nexisting approaches suffer from inconsistency of character appearances and poor\npreservation of fine details. Moreover, they require a large amount of video\ndata for training, which can be computationally demanding. To address these\nlimitations, we propose PoseAnimate, a novel zero-shot I2V framework for\ncharacter animation. PoseAnimate contains three key components: 1) a Pose-Aware\nControl Module (PACM) that incorporates diverse pose signals into text\nembeddings, to preserve character-independent content and maintain precise\nalignment of actions. 2) a Dual Consistency Attention Module (DCAM) that\nenhances temporal consistency and retains character identity and intricate\nbackground details. 3) a Mask-Guided Decoupling Module (MGDM) that refines\ndistinct feature perception abilities, improving animation fidelity by\ndecoupling the character and background. We also propose a Pose Alignment\nTransition Algorithm (PATA) to ensure smooth action transition. Extensive\nexperiment results demonstrate that our approach outperforms the\nstate-of-the-art training-based methods in terms of character consistency and\ndetail fidelity. Moreover, it maintains a high level of temporal coherence\nthroughout the generated animations.",
      "tldr_zh": "这篇论文提出了PoseAnimate，一个零-shot I2V框架，用于从单张图像生成高保真、可控姿态的角色动画，解决了现有方法在角色外观一致性和细节保留方面的不足，同时避免了大量训练数据的需求。框架的关键组件包括Pose-Aware Control Module (PACM)来整合姿态信号确保动作精确对齐、Dual Consistency Attention Module (DCAM)来增强时间一致性和保留角色身份与背景细节，以及Mask-Guided Decoupling Module (MGDM)来通过解耦角色和背景提升动画保真度。还引入了Pose Alignment Transition Algorithm (PATA)以实现动作的平滑过渡。实验结果显示，PoseAnimate在角色一致性、细节保真度和时间连贯性上优于现有训练-based方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.13680v3",
      "published_date": "2024-04-21 14:43:31 UTC",
      "updated_date": "2024-06-05 07:32:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:18:50.767044"
    },
    {
      "arxiv_id": "2404.13667v1",
      "title": "MathNet: A Data-Centric Approach for Printed Mathematical Expression Recognition",
      "title_zh": "MathNet：以数据为中心的方法，用于印刷",
      "authors": [
        "Felix M. Schmitt-Koopmann",
        "Elaine M. Huang",
        "Hans-Peter Hutter",
        "Thilo Stadelmann",
        "Alireza Darvishy"
      ],
      "abstract": "Printed mathematical expression recognition (MER) models are usually trained\nand tested using LaTeX-generated mathematical expressions (MEs) as input and\nthe LaTeX source code as ground truth. As the same ME can be generated by\nvarious different LaTeX source codes, this leads to unwanted variations in the\nground truth data that bias test performance results and hinder efficient\nlearning. In addition, the use of only one font to generate the MEs heavily\nlimits the generalization of the reported results to realistic scenarios. We\npropose a data-centric approach to overcome this problem, and present\nconvincing experimental results: Our main contribution is an enhanced LaTeX\nnormalization to map any LaTeX ME to a canonical form. Based on this process,\nwe developed an improved version of the benchmark dataset im2latex-100k,\nfeaturing 30 fonts instead of one. Second, we introduce the real-world dataset\nrealFormula, with MEs extracted from papers. Third, we developed a MER model,\nMathNet, based on a convolutional vision transformer, with superior results on\nall four test sets (im2latex-100k, im2latexv2, realFormula, and InftyMDB-1),\noutperforming the previous state of the art by up to 88.3%.",
      "tldr_zh": "该论文提出一种数据中心方法，用于解决打印数学表达式识别(MER)模型的训练数据问题，包括LaTeX源代码的变异性和字体限制导致的偏差和泛化不足。核心贡献包括开发增强的LaTeX标准化映射到规范形式、改进im2latex-100k数据集（使用30种字体）和引入realFormula数据集（从真实论文中提取表达式）。此外，作者构建了MathNet模型，基于convolutional vision transformer，并在im2latex-100k、im2latexv2、realFormula和InftyMDB-1四个测试集上超越了现有最先进模型，提高性能高达88.3%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.13667v1",
      "published_date": "2024-04-21 14:03:34 UTC",
      "updated_date": "2024-04-21 14:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:02.179066"
    },
    {
      "arxiv_id": "2404.16067v1",
      "title": "Layout2Rendering: AI-aided Greenspace design",
      "title_zh": "Layout2Rendering：AI 辅助绿地设计",
      "authors": [
        "Ran Chen",
        "Zeke Lian",
        "Yueheng He",
        "Xiao Ling",
        "Fuyu Yang",
        "Xueqi Yao",
        "Xingjian Yi",
        "Jing Zhao"
      ],
      "abstract": "In traditional human living environment landscape design, the establishment\nof three-dimensional models is an essential step for designers to intuitively\npresent the spatial relationships of design elements, as well as a foundation\nfor conducting landscape analysis on the site. Rapidly and effectively\ngenerating beautiful and realistic landscape spaces is a significant challenge\nfaced by designers. Although generative design has been widely applied in\nrelated fields, they mostly generate three-dimensional models through the\nrestriction of indicator parameters. However, the elements of landscape design\nare complex and have unique requirements, making it difficult to generate\ndesigns from the perspective of indicator limitations. To address these issues,\nthis study proposes a park space generative design system based on deep\nlearning technology. This system generates design plans based on the\ntopological relationships of landscape elements, then vectorizes the plan\nelement information, and uses Grasshopper to generate three-dimensional models\nwhile synchronously fine-tuning parameters, rapidly completing the entire\nprocess from basic site conditions to model effect analysis. Experimental\nresults show that: (1) the system, with the aid of AI-assisted technology, can\nrapidly generate space green space schemes that meet the designer's perspective\nbased on site conditions; (2) this study has vectorized and\nthree-dimensionalized various types of landscape design elements based on\nsemantic information; (3) the analysis and visualization module constructed in\nthis study can perform landscape analysis on the generated three-dimensional\nmodels and produce node effect diagrams, allowing users to modify the design in\nreal time based on the effects, thus enhancing the system's interactivity.",
      "tldr_zh": "本文提出Layout2Rendering系统，利用AI-aided技术基于deep learning，针对传统景观设计的挑战，通过景观元素的拓扑关系生成设计方案，并使用Grasshopper进行矢量化、三维模型生成和参数微调，实现从场地条件到效果分析的全过程自动化。系统能够快速产出符合设计师视角的绿化空间方案，并基于语义信息实现景观元素的矢量化和三维化。实验结果表明，该系统显著提升了设计效率和交互性，支持实时修改和景观分析。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "14 pages,8 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.16067v1",
      "published_date": "2024-04-21 14:00:43 UTC",
      "updated_date": "2024-04-21 14:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:13.079280"
    },
    {
      "arxiv_id": "2404.13663v2",
      "title": "Cumulative Hazard Function Based Efficient Multivariate Temporal Point Process Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Bingqing Liu"
      ],
      "abstract": "Most existing temporal point process models are characterized by conditional\nintensity function. These models often require numerical approximation methods\nfor likelihood evaluation, which potentially hurts their performance. By\ndirectly modelling the integral of the intensity function, i.e., the cumulative\nhazard function (CHF), the likelihood can be evaluated accurately, making it a\npromising approach. However, existing CHF-based methods are not well-defined,\ni.e., the mathematical constraints of CHF are not completely satisfied, leading\nto untrustworthy results. For multivariate temporal point process, most\nexisting methods model intensity (or density, etc.) functions for each variate,\nlimiting the scalability. In this paper, we explore using neural networks to\nmodel a flexible but well-defined CHF and learning the multivariate temporal\npoint process with low parameter complexity. Experimental results on six\ndatasets show that the proposed model achieves the state-of-the-art performance\non data fitting and event prediction tasks while having significantly fewer\nparameters and memory usage than the strong competitors. The source code and\ndata can be obtained from https://github.com/lbq8942/NPP.",
      "tldr_zh": "该论文提出了一种基于Cumulative Hazard Function (CHF)的多变量时间点过程学习方法，以解决现有模型依赖条件强度函数导致的似然评估不准确问题。作者使用神经网络建模一个灵活且数学约束完备的CHF，实现低参数复杂度的多变量学习，从而提升模型的可扩展性和效率。在六个数据集上的实验表明，该方法在数据拟合和事件预测任务上达到了最先进性能，同时参数和内存使用显著低于竞争模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.13663v2",
      "published_date": "2024-04-21 13:51:31 UTC",
      "updated_date": "2024-05-02 02:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:24.586606"
    },
    {
      "arxiv_id": "2404.13657v1",
      "title": "MLP: Motion Label Prior for Temporal Sentence Localization in Untrimmed 3D Human Motions",
      "title_zh": "MLP：动作标签先验用于未修剪的3D人体动作中的时序句子定位",
      "authors": [
        "Sheng Yan",
        "Mengyuan Liu",
        "Yong Wang",
        "Yang Liu",
        "Chen Chen",
        "Hong Liu"
      ],
      "abstract": "In this paper, we address the unexplored question of temporal sentence\nlocalization in human motions (TSLM), aiming to locate a target moment from a\n3D human motion that semantically corresponds to a text query. Considering that\n3D human motions are captured using specialized motion capture devices, motions\nwith only a few joints lack complex scene information like objects and\nlighting. Due to this character, motion data has low contextual richness and\nsemantic ambiguity between frames, which limits the accuracy of predictions\nmade by current video localization frameworks extended to TSLM to only a rough\nlevel. To refine this, we devise two novel label-prior-assisted training\nschemes: one embed prior knowledge of foreground and background to highlight\nthe localization chances of target moments, and the other forces the originally\nrough predictions to overlap with the more accurate predictions obtained from\nthe flipped start/end prior label sequences during recovery training. We show\nthat injecting label-prior knowledge into the model is crucial for improving\nperformance at high IoU. In our constructed TSLM benchmark, our model termed\nMLP achieves a recall of 44.13 at IoU@0.7 on the BABEL dataset and 71.17 on\nHumanML3D (Restore), outperforming prior works. Finally, we showcase the\npotential of our approach in corpus-level moment retrieval. Our source code is\nopenly accessible at https://github.com/eanson023/mlp.",
      "tldr_zh": "本论文解决了Temporal Sentence Localization in Human Motions (TSLM)问题，即从未修剪的3D人体动作中定位与文本查询语义匹配的目标时刻，针对动作数据语义模糊和缺乏复杂场景信息的挑战。作者提出两种新颖的标签先验辅助训练方案：一种嵌入前景和背景的先验知识以突出目标时刻，另一种通过翻转开始/结束标签序列的恢复训练强制粗糙预测与更准确预测重叠，从而提升高IoU下的定位性能。在构建的TSLM基准上，模型MLP在BABEL数据集上IoU@0.7的召回率达到44.13，在HumanML3D (Restore)上为71.17，优于现有工作，并展示了在语料级时刻检索中的潜力。源代码已开源，可进一步推动相关研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.13657v1",
      "published_date": "2024-04-21 13:25:46 UTC",
      "updated_date": "2024-04-21 13:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:39.423989"
    },
    {
      "arxiv_id": "2404.13655v2",
      "title": "SPGNN: Recognizing Salient Subgraph Patterns via Enhanced Graph Convolution and Pooling",
      "title_zh": "SPGNN：通过增强的图卷积和池化识别显著子图模式",
      "authors": [
        "Zehao Dong",
        "Muhan Zhang",
        "Yixin Chen"
      ],
      "abstract": "Graph neural networks (GNNs) have revolutionized the field of machine\nlearning on non-Euclidean data such as graphs and networks. GNNs effectively\nimplement node representation learning through neighborhood aggregation and\nachieve impressive results in many graph-related tasks. However, most\nneighborhood aggregation approaches are summation-based, which can be\nproblematic as they may not be sufficiently expressive to encode informative\ngraph structures. Furthermore, though the graph pooling module is also of vital\nimportance for graph learning, especially for the task of graph classification,\nresearch on graph down-sampling mechanisms is rather limited.\n  To address the above challenges, we propose a concatenation-based graph\nconvolution mechanism that injectively updates node representations to maximize\nthe discriminative power in distinguishing non-isomorphic subgraphs. In\naddition, we design a novel graph pooling module, called WL-SortPool, to learn\nimportant subgraph patterns in a deep-learning manner. WL-SortPool layer-wise\nsorts node representations (i.e. continuous WL colors) to separately learn the\nrelative importance of subtrees with different depths for the purpose of\nclassification, thus better characterizing the complex graph topology and rich\ninformation encoded in the graph. We propose a novel Subgraph Pattern GNN\n(SPGNN) architecture that incorporates these enhancements. We test the proposed\nSPGNN architecture on many graph classification benchmarks. Experimental\nresults show that our method can achieve highly competitive results with\nstate-of-the-art graph kernels and other GNN approaches.",
      "tldr_zh": "该论文针对图神经网络（GNNs）的邻域聚合机制存在表达力不足和图池化（graph pooling）研究有限的问题，提出了一种基于增强图卷积和池化的方法。论文引入了 concatenation-based graph convolution 机制，通过注入式更新节点表示来更好地区分非同构子图；同时设计了 WL-SortPool 模块，该模块通过层级排序节点表示（continuous WL colors），以学习不同深度子树的相对重要性，从而捕捉复杂图拓扑。实验结果显示，提出的 SPGNN 架构在多个图分类基准上取得了与最先进图核和 GNN 方法高度竞争性的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13655v2",
      "published_date": "2024-04-21 13:11:59 UTC",
      "updated_date": "2024-04-29 16:21:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:49.222678"
    },
    {
      "arxiv_id": "2404.13652v1",
      "title": "BANSAI: Towards Bridging the AI Adoption Gap in Industrial Robotics with Neurosymbolic Programming",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Alt",
        "Julia Dvorak",
        "Darko Katic",
        "Rainer Jäkel",
        "Michael Beetz",
        "Gisela Lanza"
      ],
      "abstract": "Over the past decade, deep learning helped solve manipulation problems across\nall domains of robotics. At the same time, industrial robots continue to be\nprogrammed overwhelmingly using traditional program representations and\ninterfaces. This paper undertakes an analysis of this \"AI adoption gap\" from an\nindustry practitioner's perspective. In response, we propose the BANSAI\napproach (Bridging the AI Adoption Gap via Neurosymbolic AI). It systematically\nleverages principles of neurosymbolic AI to establish data-driven, subsymbolic\nprogram synthesis and optimization in modern industrial robot programming\nworkflow. BANSAI conceptually unites several lines of prior research and\nproposes a path toward practical, real-world validation.",
      "tldr_zh": "本研究分析了工业机器人领域中AI采用差距的问题，尽管深度学习已广泛解决机器人操作问题，但工业机器人编程仍依赖传统方法。针对此，提出BANSAI方法（Bridging the AI Adoption Gap via Neurosymbolic AI），通过神经符号AI（Neurosymbolic AI）原则，实现数据驱动的子符号程序合成和优化，融入现代工业机器人编程流程。该方法整合了现有研究，并为实际世界验证提供可行路径，从而推动AI在工业机器人的实际应用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "68T40",
        "I.2.1; I.2.9; I.2.2; J.6; J.7"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 3 figures, accepted at the 2024 CIRP International\n  Conference on Manufacturing Systems (CMS)",
      "pdf_url": "http://arxiv.org/pdf/2404.13652v1",
      "published_date": "2024-04-21 13:04:58 UTC",
      "updated_date": "2024-04-21 13:04:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:19:59.374414"
    },
    {
      "arxiv_id": "2404.13634v3",
      "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Resmi Ramachandranpillai",
        "Md Fahim Sikder",
        "David Bergström",
        "Fredrik Heintz"
      ],
      "abstract": "Synthetic data generation offers a promising solution to enhance the\nusefulness of Electronic Healthcare Records (EHR) by generating realistic\nde-identified data. However, the existing literature primarily focuses on the\nquality of synthetic health data, neglecting the crucial aspect of fairness in\ndownstream predictions. Consequently, models trained on synthetic EHR have\nfaced criticism for producing biased outcomes in target tasks. These biases can\narise from either spurious correlations between features or the failure of\nmodels to accurately represent sub-groups. To address these concerns, we\npresent Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based\nsynthetic data generator specifically designed for the healthcare domain. In\norder to tackle spurious correlations (i), we propose an\ninformation-constrained Data Generation Process that enables the generator to\nlearn a fair deterministic transformation based on a well-defined notion of\nalgorithmic fairness. To overcome the challenge of capturing exact sub-group\nrepresentations (ii), we incentivize the generator to preserve sub-group\ndensities through score-based weighted sampling. This approach compels the\ngenerator to learn from underrepresented regions of the data manifold. We\nconduct extensive experiments using the MIMIC-III database. Our results\ndemonstrate that Bt-GAN achieves SOTA accuracy while significantly improving\nfairness and minimizing bias amplification. We also perform an in-depth\nexplainability analysis to provide additional evidence supporting the validity\nof our study. In conclusion, our research introduces a novel and professional\napproach to addressing the limitations of synthetic data generation in the\nhealthcare domain. By incorporating fairness considerations and leveraging\nadvanced techniques such as GANs, we pave the way for more reliable and\nunbiased predictions in healthcare applications.",
      "tldr_zh": "这篇论文提出了 Bt-GAN，一种基于 Generative Adversarial Networks (GANs) 的合成数据生成器，旨在解决电子健康记录 (EHR) 合成数据在下游任务中的不公平问题，包括虚假相关性和子群表示不准确。Bt-GAN 通过信息约束的数据生成过程实现公平的确定性转换，并采用基于分数的加权采样来保留子群密度，从而促使生成器关注数据流形中的 underrepresented 区域。在 MIMIC-III 数据库上的实验表明，Bt-GAN 达到了 SOTA 准确性，同时显著提升了公平性、减少了偏差放大，并通过深入的可解释性分析验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13634v3",
      "published_date": "2024-04-21 12:16:38 UTC",
      "updated_date": "2024-04-26 05:02:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:20:12.840950"
    },
    {
      "arxiv_id": "2404.13630v2",
      "title": "Utilizing Deep Learning to Optimize Software Development Processes",
      "title_zh": "利用深度学习优化软件开发过程",
      "authors": [
        "Keqin Li",
        "Armando Zhu",
        "Peng Zhao",
        "Jintong Song",
        "Jiabei Liu"
      ],
      "abstract": "This study explores the application of deep learning technologies in software\ndevelopment processes, particularly in automating code reviews, error\nprediction, and test generation to enhance code quality and development\nefficiency. Through a series of empirical studies, experimental groups using\ndeep learning tools and control groups using traditional methods were compared\nin terms of code error rates and project completion times. The results\ndemonstrated significant improvements in the experimental group, validating the\neffectiveness of deep learning technologies. The research also discusses\npotential optimization points, methodologies, and technical challenges of deep\nlearning in software development, as well as how to integrate these\ntechnologies into existing software development workflows.",
      "tldr_zh": "这篇论文探讨了利用深度学习优化软件开发过程，特别是通过自动化代码 reviews、error prediction 和 test generation 来提升代码质量和开发效率。研究采用实证方法，将使用深度学习工具的实验组与传统方法的控制组进行比较，结果显示实验组在代码错误率和项目完成时间上取得了显著改善。论文还讨论了深度学习在软件开发中的潜在优化点、技术挑战以及如何将其整合到现有工作流程中。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13630v2",
      "published_date": "2024-04-21 12:06:05 UTC",
      "updated_date": "2024-05-03 13:07:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:20:25.500856"
    },
    {
      "arxiv_id": "2404.13627v3",
      "title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
      "title_zh": "翻译失败",
      "authors": [
        "Chunkit Chan",
        "Cheng Jiayang",
        "Yauwai Yim",
        "Zheye Deng",
        "Wei Fan",
        "Haoran Li",
        "Xin Liu",
        "Hongming Zhang",
        "Weiqi Wang",
        "Yangqiu Song"
      ],
      "abstract": "Large Language Models (LLMs) have sparked substantial interest and debate\nconcerning their potential emergence of Theory of Mind (ToM) ability. Theory of\nmind evaluations currently focuses on testing models using machine-generated\ndata or game settings prone to shortcuts and spurious correlations, which lacks\nevaluation of machine ToM ability in real-world human interaction scenarios.\nThis poses a pressing demand to develop new real-world scenario benchmarks. We\nintroduce NegotiationToM, a new benchmark designed to stress-test machine ToM\nin real-world negotiation surrounding covered multi-dimensional mental states\n(i.e., desires, beliefs, and intentions). Our benchmark builds upon the\nBelief-Desire-Intention (BDI) agent modeling theory and conducts the necessary\nempirical experiments to evaluate large language models. Our findings\ndemonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as\nthey consistently perform significantly worse than humans, even when employing\nthe chain-of-thought (CoT) method.",
      "tldr_zh": "该论文引入了NegotiationToM基准，用于评估大型语言模型(LLMs)在真实谈判场景中的Theory of Mind (ToM)能力，该基准针对现有评估的不足，如依赖机器生成数据和游戏捷径。NegotiationToM基于Belief-Desire-Intention (BDI)代理建模理论，涵盖欲望(desires)、信念(beliefs)和意图(intentions)等多维度心理状态，并通过实验测试LLMs的表现。结果显示，即使采用chain-of-thought (CoT)方法，当前最先进的LLMs在该基准上仍远逊于人类，突显了机器ToM能力的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 findings. Dataset:\n  https://github.com/HKUST-KnowComp/NegotiationToM",
      "pdf_url": "http://arxiv.org/pdf/2404.13627v3",
      "published_date": "2024-04-21 11:51:13 UTC",
      "updated_date": "2024-10-05 16:58:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:20:38.158091"
    },
    {
      "arxiv_id": "2404.13604v2",
      "title": "CKGConv: General Graph Convolution with Continuous Kernels",
      "title_zh": "CKGConv：带有连续核的通用图卷积",
      "authors": [
        "Liheng Ma",
        "Soumyasundar Pal",
        "Yitian Zhang",
        "Jiaming Zhou",
        "Yingxue Zhang",
        "Mark Coates"
      ],
      "abstract": "The existing definitions of graph convolution, either from spatial or\nspectral perspectives, are inflexible and not unified. Defining a general\nconvolution operator in the graph domain is challenging due to the lack of\ncanonical coordinates, the presence of irregular structures, and the properties\nof graph symmetries. In this work, we propose a novel and general graph\nconvolution framework by parameterizing the kernels as continuous functions of\npseudo-coordinates derived via graph positional encoding. We name this\nContinuous Kernel Graph Convolution (CKGConv). Theoretically, we demonstrate\nthat CKGConv is flexible and expressive. CKGConv encompasses many existing\ngraph convolutions, and exhibits a stronger expressiveness, as powerful as\ngraph transformers in terms of distinguishing non-isomorphic graphs.\nEmpirically, we show that CKGConv-based Networks outperform existing graph\nconvolutional networks and perform comparably to the best graph transformers\nacross a variety of graph datasets. The code and models are publicly available\nat https://github.com/networkslab/CKGConv.",
      "tldr_zh": "本论文提出 CKGConv，一种通用图卷积框架，通过将核参数化为基于图位置编码的伪坐标的连续函数，解决了现有图卷积（从空间或谱角度）的灵活性和统一性问题。理论上，CKGConv 不仅涵盖了许多现有图卷积方法，还表现出更强的表现力，能够像 graph transformers 一样区分非同构图。实验结果表明，基于 CKGConv 的网络在多种图数据集上优于传统图卷积网络，并与最佳 graph transformers 性能相当。代码和模型已在 GitHub 上公开。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "On International Conference on Machine Learning (ICML) 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13604v2",
      "published_date": "2024-04-21 10:26:13 UTC",
      "updated_date": "2024-06-05 12:54:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:20:50.703889"
    },
    {
      "arxiv_id": "2404.14455v1",
      "title": "A Neuro-Symbolic Explainer for Rare Events: A Case Study on Predictive Maintenance",
      "title_zh": "翻译失败",
      "authors": [
        "João Gama",
        "Rita P. Ribeiro",
        "Saulo Mastelini",
        "Narjes Davarid",
        "Bruno Veloso"
      ],
      "abstract": "Predictive Maintenance applications are increasingly complex, with\ninteractions between many components. Black box models are popular approaches\nbased on deep learning techniques due to their predictive accuracy. This paper\nproposes a neural-symbolic architecture that uses an online rule-learning\nalgorithm to explain when the black box model predicts failures. The proposed\nsystem solves two problems in parallel: anomaly detection and explanation of\nthe anomaly. For the first problem, we use an unsupervised state of the art\nautoencoder. For the second problem, we train a rule learning system that\nlearns a mapping from the input features to the autoencoder reconstruction\nerror. Both systems run online and in parallel. The autoencoder signals an\nalarm for the examples with a reconstruction error that exceeds a threshold.\nThe causes of the signal alarm are hard for humans to understand because they\nresult from a non linear combination of sensor data. The rule that triggers\nthat example describes the relationship between the input features and the\nautoencoder reconstruction error. The rule explains the failure signal by\nindicating which sensors contribute to the alarm and allowing the\nidentification of the component involved in the failure. The system can present\nglobal explanations for the black box model and local explanations for why the\nblack box model predicts a failure. We evaluate the proposed system in a\nreal-world case study of Metro do Porto and provide explanations that\nillustrate its benefits.",
      "tldr_zh": "这篇论文提出了一种神经符号架构（neural-symbolic architecture），结合在线规则学习算法（online rule-learning algorithm），用于解释黑盒模型在预测性维护（Predictive Maintenance）中的故障预测。系统同时处理异常检测（anomaly detection）和异常解释，通过无监督自动编码器（autoencoder）检测重建错误超过阈值的警报，并使用规则学习映射输入特征以识别贡献传感器和故障组件。该方法提供黑盒模型的全局解释（global explanations）和局部解释（local explanations），并在Metro do Porto的真实案例研究中证明了其有效性，提高了系统的可解释性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.14455v1",
      "published_date": "2024-04-21 09:48:09 UTC",
      "updated_date": "2024-04-21 09:48:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:21:03.925948"
    },
    {
      "arxiv_id": "2404.13594v1",
      "title": "Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Pantazopoulos",
        "Alessandro Suglia",
        "Oliver Lemon",
        "Arash Eshghi"
      ],
      "abstract": "An effective method for combining frozen large language models (LLM) and\nvisual encoders involves a resampler module that creates a `visual prompt'\nwhich is provided to the LLM, along with the textual prompt. While this\napproach has enabled impressive performance across many coarse-grained tasks\nlike image captioning and visual question answering, more fine-grained tasks\nthat require spatial understanding have not been thoroughly examined. In this\npaper, we use \\textit{diagnostic classifiers} to measure the extent to which\nthe visual prompt produced by the resampler encodes spatial information. Our\nresults show that this information is largely absent from the resampler output\nwhen kept frozen during training of the classifiers. However, when the\nresampler and classifier are trained jointly, we observe a significant\nperformance boost. This shows that the compression achieved by the resamplers\ncan in principle encode the requisite spatial information, but that more\nobject-aware objectives are needed at the pretraining stage to facilitate this\ncapability",
      "tldr_zh": "本研究探讨了视觉语言模型中 resampler 模块在编码细粒度空间信息方面的能力，特别是在结合 frozen large language models (LLM) 和 visual encoders 时。作者使用 diagnostic classifiers 来评估 resampler 生成的 visual prompt 是否包含空间信息，结果显示如果 resampler 保持冻结，空间信息在输出中基本缺失。相反，当 resampler 和 classifier 联合训练时，性能显著提升，表明 resampler 原则上能压缩并编码所需的空间信息。最终，该论文强调需要更注重 object-aware objectives 的预训练阶段，以提升模型在空间理解任务上的表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13594v1",
      "published_date": "2024-04-21 09:23:36 UTC",
      "updated_date": "2024-04-21 09:23:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:21:13.062200"
    },
    {
      "arxiv_id": "2404.14454v2",
      "title": "Reinforcement of Explainability of ChatGPT Prompts by Embedding Breast Cancer Self-Screening Rules into AI Responses",
      "title_zh": "翻译失败",
      "authors": [
        "Yousef Khan",
        "Ahmed Abdeen Hamed"
      ],
      "abstract": "Addressing the global challenge of breast cancer, this research explores the\nfusion of generative AI, focusing on ChatGPT 3.5 turbo model, and the\nintricacies of breast cancer risk assessment. The research aims to evaluate\nChatGPT's reasoning capabilities, emphasizing its potential to process rules\nand provide explanations for screening recommendations. The study seeks to\nbridge the technology gap between intelligent machines and clinicians by\ndemonstrating ChatGPT's unique proficiency in natural language reasoning. The\nmethodology employs a supervised prompt-engineering approach to enforce\ndetailed explanations for ChatGPT's recommendations. Synthetic use cases,\ngenerated algorithmically, serve as the testing ground for the encoded rules,\nevaluating the model's processing prowess. Findings highlight ChatGPT's\npromising capacity in processing rules comparable to Expert System Shells, with\na focus on natural language reasoning. The research introduces the concept of\nreinforcement explainability, showcasing its potential in elucidating outcomes\nand facilitating user-friendly interfaces for breast cancer risk assessment.",
      "tldr_zh": "本研究探讨了通过将乳腺癌自筛规则嵌入 ChatGPT 响应中，来强化 ChatGPT prompts 的可解释性，旨在桥接 AI 与临床医生在乳腺癌风险评估方面的技术差距。采用监督 prompt-engineering 方法，研究使用算法生成的合成用例测试 ChatGPT 3.5 turbo 模型的规则处理和自然语言推理能力。结果表明，ChatGPT 的性能可媲美 Expert System Shells，并在自然语言推理上表现出色；此外，引入 reinforcement explainability 概念，帮助阐明评估结果并提升用户友好界面。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2; I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures, 3 algorithms, 1 table, submitted to the IEEE\n  MedAI'24 Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.14454v2",
      "published_date": "2024-04-21 09:20:16 UTC",
      "updated_date": "2024-06-02 22:42:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:21:27.132017"
    },
    {
      "arxiv_id": "2404.13588v1",
      "title": "Machine Unlearning via Null Space Calibration",
      "title_zh": "翻译失败",
      "authors": [
        "Huiqiang Chen",
        "Tianqing Zhu",
        "Xin Yu",
        "Wanlei Zhou"
      ],
      "abstract": "Machine unlearning aims to enable models to forget specific data instances\nwhen receiving deletion requests. Current research centres on efficient\nunlearning to erase the influence of data from the model and neglects the\nsubsequent impacts on the remaining data. Consequently, existing unlearning\nalgorithms degrade the model's performance after unlearning, known as\n\\textit{over-unlearning}. This paper addresses this critical yet under-explored\nissue by introducing machine \\underline{U}nlearning via \\underline{N}ull\n\\underline{S}pace \\underline{C}alibration (UNSC), which can accurately unlearn\ntarget samples without over-unlearning. On the contrary, by calibrating the\ndecision space during unlearning, UNSC can significantly improve the model's\nperformance on the remaining samples. In particular, our approach hinges on\nconfining the unlearning process to a specified null space tailored to the\nremaining samples, which is augmented by strategically pseudo-labeling the\nunlearning samples. Comparative analyses against several established baselines\naffirm the superiority of our approach. Code is released at this\n\\href{https://github.com/HQC-ML/Machine-Unlearning-via-Null-Space-Calibration}{URL}.",
      "tldr_zh": "该论文针对机器遗忘（Machine Unlearning）中的过遗忘（over-unlearning）问题，提出了一种新方法UNSC（Machine Unlearning via Null Space Calibration），旨在准确删除目标样本的影响，同时避免对剩余数据的性能下降。UNSC通过将遗忘过程限制在针对剩余样本的特定空空间（Null Space），并结合战略性的伪标签（pseudo-labeling）来校准决策空间（decision space）。实验结果显示，与现有基线相比，UNSC显著提升了模型在剩余样本上的表现，并提供了开源代码以供验证。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13588v1",
      "published_date": "2024-04-21 09:09:21 UTC",
      "updated_date": "2024-04-21 09:09:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:21:39.175570"
    },
    {
      "arxiv_id": "2404.13579v4",
      "title": "LTOS: Layout-controllable Text-Object Synthesis via Adaptive Cross-attention Fusions",
      "title_zh": "LTOS：通过自适应交叉注意力融合的布局可控文本-对象合成",
      "authors": [
        "Xiaoran Zhao",
        "Tianhao Wu",
        "Yu Lai",
        "Zhiliang Tian",
        "Zhen Huang",
        "Yahui Liu",
        "Zejiang He",
        "Dongsheng Li"
      ],
      "abstract": "Controllable text-to-image generation synthesizes visual text and objects in\nimages with certain conditions, which are frequently applied to emoji and\nposter generation. Visual text rendering and layout-to-image generation tasks\nhave been popular in controllable text-to-image generation. However, each of\nthese tasks typically focuses on single modality generation or rendering,\nleaving yet-to-be-bridged gaps between the approaches correspondingly designed\nfor each of the tasks. In this paper, we combine text rendering and\nlayout-to-image generation tasks into a single task: layout-controllable\ntext-object synthesis (LTOS) task, aiming at synthesizing images with object\nand visual text based on predefined object layout and text contents. As\ncompliant datasets are not readily available for our LTOS task, we construct a\nlayout-aware text-object synthesis dataset, containing elaborate well-aligned\nlabels of visual text and object information. Based on the dataset, we propose\na layout-controllable text-object adaptive fusion (TOF) framework, which\ngenerates images with clear, legible visual text and plausible objects. We\nconstruct a visual-text rendering module to synthesize text and employ an\nobject-layout control module to generate objects while integrating the two\nmodules to harmoniously generate and integrate text content and objects in\nimages. To better the image-text integration, we propose a self-adaptive\ncross-attention fusion module that helps the image generation to attend more to\nimportant text information. Within such a fusion module, we use a self-adaptive\nlearnable factor to learn to flexibly control the influence of cross-attention\noutputs on image generation. Experimental results show that our method\noutperforms the state-of-the-art in LTOS, text rendering, and layout-to-image\ntasks, enabling harmonious visual text rendering and object generation.",
      "tldr_zh": "这篇论文引入了 LTOS（Layout-controllable Text-Object Synthesis）任务，旨在基于预定义的对象布局和文本内容合成包含视觉文本和对象的图像，结合了文本渲染和布局到图像生成。作者构建了一个新的布局感知数据集，并提出了 TOF（Text-Object Adaptive Fusion）框架，包括视觉文本渲染模块、对象布局控制模块，以及自适应 cross-attention 融合模块，该模块通过可学习因子灵活控制图像生成中文本信息的注意力。实验结果表明，该方法在 LTOS、文本渲染和布局到图像任务上优于现有技术，实现更和谐的图像文本整合和对象生成。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13579v4",
      "published_date": "2024-04-21 08:37:43 UTC",
      "updated_date": "2024-11-26 14:58:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:21:50.478770"
    },
    {
      "arxiv_id": "2404.13575v1",
      "title": "FedMPQ: Secure and Communication-Efficient Federated Learning with Multi-codebook Product Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Yang",
        "Jiapeng Zhang",
        "Qifeng Zhang",
        "Zhuo Tang"
      ],
      "abstract": "In federated learning, particularly in cross-device scenarios, secure\naggregation has recently gained popularity as it effectively defends against\ninference attacks by malicious aggregators. However, secure aggregation often\nrequires additional communication overhead and can impede the convergence rate\nof the global model, which is particularly challenging in wireless network\nenvironments with extremely limited bandwidth. Therefore, achieving efficient\ncommunication compression under the premise of secure aggregation presents a\nhighly challenging and valuable problem. In this work, we propose a novel\nuplink communication compression method for federated learning, named FedMPQ,\nwhich is based on multi shared codebook product quantization.Specifically, we\nutilize updates from the previous round to generate sufficiently robust\ncodebooks. Secure aggregation is then achieved through trusted execution\nenvironments (TEE) or a trusted third party (TTP).In contrast to previous\nworks, our approach exhibits greater robustness in scenarios where data is not\nindependently and identically distributed (non-IID) and there is a lack of\nsufficient public data. The experiments conducted on the LEAF dataset\ndemonstrate that our proposed method achieves 99% of the baseline's final\naccuracy, while reducing uplink communications by 90-95%",
      "tldr_zh": "该研究提出FedMPQ，一种基于multi-codebook product quantization的联邦学习通信压缩方法，旨在在secure aggregation下实现高效通信，同时防御恶意聚合器的推断攻击。具体地，FedMPQ利用上一轮更新生成鲁棒的共享码本，并通过TEE或TTP进行安全聚合，在non-IID数据和缺乏公共数据场景下表现出色。实验结果显示，在LEAF数据集上，该方法达到基线模型99%的最终准确率，同时将上行通信量减少90-95%。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13575v1",
      "published_date": "2024-04-21 08:27:36 UTC",
      "updated_date": "2024-04-21 08:27:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:22:02.524190"
    },
    {
      "arxiv_id": "2404.13571v1",
      "title": "Test-Time Training on Graphs with Large Language Models (LLMs)",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxin Zhang",
        "Yiqi Wang",
        "Xihong Yang",
        "Siwei Wang",
        "Yu Feng",
        "Yu Shi",
        "Ruicaho Ren",
        "En Zhu",
        "Xinwang Liu"
      ],
      "abstract": "Graph Neural Networks have demonstrated great success in various fields of\nmultimedia. However, the distribution shift between the training and test data\nchallenges the effectiveness of GNNs. To mitigate this challenge, Test-Time\nTraining (TTT) has been proposed as a promising approach. Traditional TTT\nmethods require a demanding unsupervised training strategy to capture the\ninformation from test to benefit the main task. Inspired by the great\nannotation ability of Large Language Models (LLMs) on Text-Attributed Graphs\n(TAGs), we propose to enhance the test-time training on graphs with LLMs as\nannotators. In this paper, we design a novel Test-Time Training pipeline,\nLLMTTT, which conducts the test-time adaptation under the annotations by LLMs\non a carefully-selected node set. Specifically, LLMTTT introduces a hybrid\nactive node selection strategy that considers not only node diversity and\nrepresentativeness, but also prediction signals from the pre-trained model.\nGiven annotations from LLMs, a two-stage training strategy is designed to\ntailor the test-time model with the limited and noisy labels. A theoretical\nanalysis ensures the validity of our method and extensive experiments\ndemonstrate that the proposed LLMTTT can achieve a significant performance\nimprovement compared to existing Out-of-Distribution (OOD) generalization\nmethods.",
      "tldr_zh": "这篇论文提出 LLMTTT，一种利用 Large Language Models (LLMs) 作为标注器的测试时训练 (Test-Time Training, TTT) 方法，旨在解决 Graph Neural Networks (GNNs) 在数据分布偏移时的性能问题。方法包括混合主动节点选择策略（考虑节点多样性、代表性和预训练模型的预测信号），以及基于 LLMs 标注的两阶段训练策略，以处理有限和嘈杂的标签。实验结果表明，LLMTTT 在 Out-of-Distribution (OOD) 泛化任务上显著优于现有方法，并通过理论分析证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13571v1",
      "published_date": "2024-04-21 08:20:02 UTC",
      "updated_date": "2024-04-21 08:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:22:15.542009"
    },
    {
      "arxiv_id": "2404.15190v2",
      "title": "Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following",
      "title_zh": "翻译失败",
      "authors": [
        "Suyeon Shin",
        "Sujin jeon",
        "Junghyun Kim",
        "Gi-Cheon Kang",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks.",
      "tldr_zh": "本研究提出Socratic Planner，一种基于自问自答(self-QA)的零样本规划方法，用于Embodied Instruction Following (EIF)，即在交互环境中执行自然语言指令。该方法利用Large Language Model (LLM)生成子目标序列，并在执行过程中通过视觉反馈机制动态调整计划，以应对意外情况如障碍物。实验结果显示，Socratic Planner在ALFRED benchmark上超越现有最先进模型，在长horizon任务中表现出色。最终，该框架在物理机器人上的部署证明了其实际可行性，为复杂任务规划提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.RO",
        "68T01 (Primary) 68T40, 68T50, 68T45 (Secondary)"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 6 figures, published to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.15190v2",
      "published_date": "2024-04-21 08:10:20 UTC",
      "updated_date": "2025-03-26 07:42:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:22:28.739114"
    },
    {
      "arxiv_id": "2404.13567v1",
      "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Abhilekha Dalal",
        "Rushrukh Rayan",
        "Adrita Barua",
        "Eugene Y. Vasserman",
        "Md Kamruzzaman Sarker",
        "Pascal Hitzler"
      ],
      "abstract": "A major challenge in Explainable AI is in correctly interpreting activations\nof hidden neurons: accurate interpretations would help answer the question of\nwhat a deep learning system internally detects as relevant in the input,\ndemystifying the otherwise black-box nature of deep learning systems. The state\nof the art indicates that hidden node activations can, in some cases, be\ninterpretable in a way that makes sense to humans, but systematic automated\nmethods that would be able to hypothesize and verify interpretations of hidden\nneuron activations are underexplored. This is particularly the case for\napproaches that can both draw explanations from substantial background\nknowledge, and that are based on inherently explainable (symbolic) methods.\n  In this paper, we introduce a novel model-agnostic post-hoc Explainable AI\nmethod demonstrating that it provides meaningful interpretations. Our approach\nis based on using a Wikipedia-derived concept hierarchy with approximately 2\nmillion classes as background knowledge, and utilizes OWL-reasoning-based\nConcept Induction for explanation generation. Additionally, we explore and\ncompare the capabilities of off-the-shelf pre-trained multimodal-based\nexplainable methods.\n  Our results indicate that our approach can automatically attach meaningful\nclass expressions as explanations to individual neurons in the dense layer of a\nConvolutional Neural Network. Evaluation through statistical analysis and\ndegree of concept activation in the hidden layer show that our method provides\na competitive edge in both quantitative and qualitative aspects compared to\nprior work.",
      "tldr_zh": "本研究探讨了在 Explainable AI 中，使用标记数据和符号方法分析隐藏神经元激活的价值，以揭示深度学习系统的内部决策过程。论文提出了一种新型模型无关的后验解释方法，该方法利用维基百科衍生的概念层次（约200万类）作为背景知识，并结合 OWL-reasoning-based Concept Induction 生成有意义的神经元解释，同时比较了现成预训练多模态解释方法。结果表明，该方法能自动为 Convolutional Neural Network 的密集层神经元附加准确的类表达式，并在统计分析和概念激活度评估中表现出定量和定性优势，超越了现有工作。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13567v1",
      "published_date": "2024-04-21 07:57:45 UTC",
      "updated_date": "2024-04-21 07:57:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:22:40.502958"
    },
    {
      "arxiv_id": "2404.15373v1",
      "title": "Robust EEG-based Emotion Recognition Using an Inception and Two-sided Perturbation Model",
      "title_zh": "翻译失败",
      "authors": [
        "Shadi Sartipi",
        "Mujdat Cetin"
      ],
      "abstract": "Automated emotion recognition using electroencephalogram (EEG) signals has\ngained substantial attention. Although deep learning approaches exhibit strong\nperformance, they often suffer from vulnerabilities to various perturbations,\nlike environmental noise and adversarial attacks. In this paper, we propose an\nInception feature generator and two-sided perturbation (INC-TSP) approach to\nenhance emotion recognition in brain-computer interfaces. INC-TSP integrates\nthe Inception module for EEG data analysis and employs two-sided perturbation\n(TSP) as a defensive mechanism against input perturbations. TSP introduces\nworst-case perturbations to the model's weights and inputs, reinforcing the\nmodel's elasticity against adversarial attacks. The proposed approach addresses\nthe challenge of maintaining accurate emotion recognition in the presence of\ninput uncertainties. We validate INC-TSP in a subject-independent three-class\nemotion recognition scenario, demonstrating robust performance.",
      "tldr_zh": "该论文提出了一种基于 EEG 信号的稳健情感识别方法，名为 Inception feature generator and two-sided perturbation (INC-TSP) 模型，以应对深度学习模型对环境噪音和对抗性攻击的脆弱性。INC-TSP 整合了 Inception 模块用于 EEG 数据分析，并通过 Two-sided perturbation (TSP) 机制在模型权重和输入上引入最坏情况扰动，从而增强模型对输入不确定性的弹性。实验结果显示，该方法在主体无关的三类情感识别场景中表现出色，显著提高了情感识别的准确性和可靠性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.15373v1",
      "published_date": "2024-04-21 07:54:43 UTC",
      "updated_date": "2024-04-21 07:54:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:22:53.687885"
    },
    {
      "arxiv_id": "2404.13565v3",
      "title": "Exploring Diverse Methods in Visual Question Answering",
      "title_zh": "探索视觉问答中的多样方法",
      "authors": [
        "Panfeng Li",
        "Qikai Yang",
        "Xieming Geng",
        "Wenjing Zhou",
        "Zhicheng Ding",
        "Yi Nian"
      ],
      "abstract": "This study explores innovative methods for improving Visual Question\nAnswering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and\nattention mechanisms. Leveraging a balanced VQA dataset, we investigate three\ndistinct strategies. Firstly, GAN-based approaches aim to generate answer\nembeddings conditioned on image and question inputs, showing potential but\nstruggling with more complex tasks. Secondly, autoencoder-based techniques\nfocus on learning optimal embeddings for questions and images, achieving\ncomparable results with GAN due to better ability on complex questions. Lastly,\nattention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),\naddress language priors and attention modeling, albeit with a\ncomplexity-performance trade-off. This study underscores the challenges and\nopportunities in VQA and suggests avenues for future research, including\nalternative GAN formulations and attentional mechanisms.",
      "tldr_zh": "本研究探索了多种创新方法来提升 Visual Question Answering (VQA) 的性能，包括 Generative Adversarial Networks (GANs)、autoencoders 和 attention mechanisms，利用平衡的 VQA 数据集进行评估。GANs 方法通过生成基于图像和问题的答案嵌入，但对复杂任务表现较差；autoencoders 则专注于学习问题和图像的优化嵌入，在处理复杂问题时表现出色，与 GANs 相当。attention mechanisms 结合 Multimodal Compact Bilinear pooling (MCB) 来缓解语言先验问题，但面临复杂度与性能的权衡；整体研究突出了 VQA 的挑战，并建议未来探索替代 GAN 形式和改进注意力机制。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by 2024 5th International Conference on Electronic\n  Communication and Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2404.13565v3",
      "published_date": "2024-04-21 07:34:44 UTC",
      "updated_date": "2024-11-12 07:21:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:23:04.884668"
    },
    {
      "arxiv_id": "2404.13564v1",
      "title": "Masked Latent Transformer with the Random Masking Ratio to Advance the Diagnosis of Dental Fluorosis",
      "title_zh": "翻译失败",
      "authors": [
        "Yun Wu",
        "Hao Xu",
        "Maohua Gu",
        "Zhongchuan Jiang",
        "Jun Xu",
        "Youliang Tian"
      ],
      "abstract": "Dental fluorosis is a chronic disease caused by long-term overconsumption of\nfluoride, which leads to changes in the appearance of tooth enamel. It is an\nimportant basis for early non-invasive diagnosis of endemic fluorosis. However,\neven dental professionals may not be able to accurately distinguish dental\nfluorosis and its severity based on tooth images. Currently, there is still a\ngap in research on applying deep learning to diagnosing dental fluorosis.\nTherefore, we construct the first open-source dental fluorosis image dataset\n(DFID), laying the foundation for deep learning research in this field. To\nadvance the diagnosis of dental fluorosis, we propose a pioneering deep\nlearning model called masked latent transformer with the random masking ratio\n(MLTrMR). MLTrMR introduces a mask latent modeling scheme based on Vision\nTransformer to enhance contextual learning of dental fluorosis lesion\ncharacteristics. Consisting of a latent embedder, encoder, and decoder, MLTrMR\nemploys the latent embedder to extract latent tokens from the original image,\nwhereas the encoder and decoder comprising the latent transformer (LT) block\nare used to process unmasked tokens and predict masked tokens, respectively. To\nmitigate the lack of inductive bias in Vision Transformer, which may result in\nperformance degradation, the LT block introduces latent tokens to enhance the\nlearning capacity of latent lesion features. Furthermore, we design an\nauxiliary loss function to constrain the parameter update direction of the\nmodel. MLTrMR achieves 80.19% accuracy, 75.79% F1, and 81.28% quadratic\nweighted kappa on DFID, making it state-of-the-art (SOTA).",
      "tldr_zh": "该研究针对牙齿氟斑牙（一种由氟过量引起的慢性病）的诊断挑战，构建了首个开源图像数据集 DFID，以推动深度学习应用。论文提出了一种创新模型 Masked Latent Transformer with the Random Masking Ratio (MLTrMR)，基于 Vision Transformer 的掩码潜在建模方案，通过潜在嵌入器、编码器和解码器处理图像特征，并引入辅助损失函数来增强对病变特征的学习能力。实验结果显示，MLTrMR 在 DFID 上实现了 80.19% 准确率、75.79% F1 分数和 81.28% 二次加权 Kappa，达到了当前最先进（SOTA）水平。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13564v1",
      "published_date": "2024-04-21 07:26:09 UTC",
      "updated_date": "2024-04-21 07:26:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:23:16.901884"
    },
    {
      "arxiv_id": "2404.13555v1",
      "title": "Cell Phone Image-Based Persian Rice Detection and Classification Using Deep Learning Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmood Saeedi kelishami",
        "Amin Saeidi Kelishami",
        "Sajjad Saeedi Kelishami"
      ],
      "abstract": "This study introduces an innovative approach to classifying various types of\nPersian rice using image-based deep learning techniques, highlighting the\npractical application of everyday technology in food categorization.\nRecognizing the diversity of Persian rice and its culinary significance, we\nleveraged the capabilities of convolutional neural networks (CNNs),\nspecifically by fine-tuning a ResNet model for accurate identification of\ndifferent rice varieties and employing a U-Net architecture for precise\nsegmentation of rice grains in bulk images. This dual-methodology framework\nallows for both individual grain classification and comprehensive analysis of\nbulk rice samples, addressing two crucial aspects of rice quality assessment.\nUtilizing images captured with consumer-grade cell phones reflects a realistic\nscenario in which individuals can leverage this technology for assistance with\ngrocery shopping and meal preparation. The dataset, comprising various rice\ntypes photographed under natural conditions without professional lighting or\nequipment, presents a challenging yet practical classification problem. Our\nfindings demonstrate the feasibility of using non-professional images for food\nclassification and the potential of deep learning models, like ResNet and\nU-Net, to adapt to the nuances of everyday objects and textures. This study\ncontributes to the field by providing insights into the applicability of\nimage-based deep learning in daily life, specifically for enhancing consumer\nexperiences and knowledge in food selection. Furthermore, it opens avenues for\nextending this approach to other food categories and practical applications,\nemphasizing the role of accessible technology in bridging the gap between\nsophisticated computational methods and everyday tasks.",
      "tldr_zh": "本研究提出了一种基于手机图像的波斯米检测和分类方法，利用深度学习技术（特别是微调 ResNet 模型）进行不同米品种的准确识别，并结合 U-Net 架构实现米粒的精确分割，从而支持单个粒分类和批量样本分析。数据集使用普通手机在自然条件下拍摄的图像，展示了该方法在真实场景中的实用性。实验结果证明了非专业图像在食物分类中的可行性，并为消费者提供辅助工具，提升日常生活中的食物选择体验，同时为扩展到其他食物类别打开了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10, I.4.6"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.13555v1",
      "published_date": "2024-04-21 07:03:48 UTC",
      "updated_date": "2024-04-21 07:03:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:23:28.678903"
    },
    {
      "arxiv_id": "2404.13528v1",
      "title": "SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Niu",
        "Md Musfiqur Rahman Sanim",
        "Zhihao Shu",
        "Jiexiong Guan",
        "Xipeng Shen",
        "Miao Yin",
        "Gagan Agrawal",
        "Bin Ren"
      ],
      "abstract": "This work is motivated by recent developments in Deep Neural Networks,\nparticularly the Transformer architectures underlying applications such as\nChatGPT, and the need for performing inference on mobile devices. Focusing on\nemerging transformers (specifically the ones with computationally efficient\nSwin-like architectures) and large models (e.g., Stable Diffusion and LLMs)\nbased on transformers, we observe that layout transformations between the\ncomputational operators cause a significant slowdown in these applications.\nThis paper presents SmartMem, a comprehensive framework for eliminating most\nlayout transformations, with the idea that multiple operators can use the same\ntensor layout through careful choice of layout and implementation of\noperations. Our approach is based on classifying the operators into four\ngroups, and considering combinations of producer-consumer edges between the\noperators. We develop a set of methods for searching such layouts. Another\ncomponent of our work is developing efficient memory layouts for 2.5\ndimensional memory commonly seen in mobile devices. Our experimental results\nshow that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on\nmobile devices across 18 varied neural networks, including CNNs, Transformers\nwith both local and global attention, as well as LLMs. In particular, compared\nto DNNFusion, SmartMem achieves an average speedup of 2.8$\\times$, and\noutperforms TVM and MNN with speedups of 6.9$\\times$ and 7.9$\\times$,\nrespectively, on average.",
      "tldr_zh": "本论文针对Transformer架构（如ChatGPT）在移动设备上执行DNN推理时因布局转换(layout transformations)导致的性能下降问题，提出SmartMem框架，以消除大部分布局转换。SmartMem通过将操作符分类为四组并优化生产者-消费者边际，选择合适的张量布局和操作实现，同时开发高效的2.5维内存布局，以实现多个操作符共享同一布局。实验结果显示，SmartMem在18个神经网络（包括CNNs、Transformer和LLMs）上优于5个最先进框架，平均比DNNFusion加速2.8倍，比TVM和MNN分别加速6.9倍和7.9倍。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13528v1",
      "published_date": "2024-04-21 04:47:26 UTC",
      "updated_date": "2024-04-21 04:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:23:40.015067"
    },
    {
      "arxiv_id": "2406.07558v2",
      "title": "An AI-Enabled Framework Within Reach for Enhancing Healthcare Sustainability and Fairness",
      "title_zh": "一种触手可及的 AI 赋能框架，用于提升医疗保健的可持续性和公平性",
      "authors": [
        "Bin Huang",
        "Changchen Zhao",
        "Zimeng Liu",
        "Shenda Hong",
        "Baochang Zhang",
        "Hao Lu",
        "Zhijun Liu",
        "Wenjin Wang",
        "Hui Liu"
      ],
      "abstract": "Good health and well-being is among key issues in the United Nations 2030\nSustainable Development Goals. The rising prevalence of large-scale infectious\ndiseases and the accelerated aging of the global population are driving the\ntransformation of healthcare technologies. In this context, establishing\nlarge-scale public health datasets, developing medical models, and creating\ndecision-making systems with a human-centric approach are of strategic\nsignificance. Recently, by leveraging the extraordinary number of accessible\ncameras, groundbreaking advancements have emerged in AI methods for\nphysiological signal monitoring and disease diagnosis using camera sensors.\nThese approaches, requiring no specialized medical equipment, offer convenient\nmanners of collecting large-scale medical data in response to public health\nevents. Therefore, we outline a prospective framework and heuristic vision for\na camera-based public health (CBPH) framework utilizing visual physiological\nmonitoring technology. The CBPH can be considered as a convenient and universal\nframework for public health, advancing the United Nations Sustainable\nDevelopment Goals, particularly in promoting the universality, sustainability,\nand equity of healthcare in low- and middle-income countries or regions.\nFurthermore, CBPH provides a comprehensive solution for building a large-scale\nand human-centric medical database, and a multi-task large medical model for\npublic health and medical scientific discoveries. It has a significant\npotential to revolutionize personal monitoring technologies, digital medicine,\ntelemedicine, and primary health care in public health. Therefore, it can be\ndeemed that the outcomes of this paper will contribute to the establishment of\na sustainable and fair framework for public health, which serves as a crucial\nbridge for advancing scientific discoveries in the realm of AI for medicine\n(AI4Medicine).",
      "tldr_zh": "本论文提出一个基于AI的框架（CBPH），利用视觉生理监测技术，通过可访问的摄像头进行生理信号监测和疾病诊断，以应对大规模传染病和人口老龄化挑战，促进联合国可持续发展目标中的健康福祉。  \n该框架无需专业医疗设备，便于在低中收入国家或地区收集大型公共卫生数据集，并开发多任务大型医疗模型，支持人类中心决策系统。  \n实验和分析显示，CBPH可提升医疗的普遍性、可持续性和公平性，并有望革新个人监测、数字医学、远程医疗等领域，推动AI4Medicine的科学发现。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "16 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.07558v2",
      "published_date": "2024-04-21 04:37:24 UTC",
      "updated_date": "2024-08-01 09:57:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:23:52.519993"
    },
    {
      "arxiv_id": "2404.13522v2",
      "title": "Error Analysis of Shapley Value-Based Model Explanations: An Informative Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Ningsheng Zhao",
        "Jia Yuan Yu",
        "Krzysztof Dzieciolowski",
        "Trang Bui"
      ],
      "abstract": "Shapley value attribution (SVA) is an increasingly popular explainable AI\n(XAI) method, which quantifies the contribution of each feature to the model's\noutput. However, recent work has shown that most existing methods to implement\nSVAs have some drawbacks, resulting in biased or unreliable explanations that\nfail to correctly capture the true intrinsic relationships between features and\nmodel outputs. Moreover, the mechanism and consequences of these drawbacks have\nnot been discussed systematically. In this paper, we propose a novel error\ntheoretical analysis framework, in which the explanation errors of SVAs are\ndecomposed into two components: observation bias and structural bias. We\nfurther clarify the underlying causes of these two biases and demonstrate that\nthere is a trade-off between them. Based on this error analysis framework, we\ndevelop two novel concepts: over-informative and underinformative explanations.\nWe demonstrate how these concepts can be effectively used to understand\npotential errors of existing SVA methods. In particular, for the widely\ndeployed assumption-based SVAs, we find that they can easily be\nunder-informative due to the distribution drift caused by distributional\nassumptions. We propose a measurement tool to quantify such a distribution\ndrift. Finally, our experiments illustrate how different existing SVA methods\ncan be over- or under-informative. Our work sheds light on how errors incur in\nthe estimation of SVAs and encourages new less error-prone methods.",
      "tldr_zh": "这篇论文分析了Shapley Value Attribution (SVA)作为解释性AI (XAI)方法的错误问题，提出一个新的错误理论分析框架，将解释错误分解为observation bias和structural bias，并揭示两者之间的权衡关系。论文引入了over-informative和under-informative explanations两个新概念，用于评估现有SVA方法的潜在偏差，例如assumption-based SVAs易因分布漂移而导致under-informative。实验结果展示了不同SVA方法的错误类型，并提供了一种测量分布漂移的工具，以指导开发更可靠的解释方法。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13522v2",
      "published_date": "2024-04-21 04:07:52 UTC",
      "updated_date": "2024-05-30 01:56:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:24:06.284884"
    },
    {
      "arxiv_id": "2404.13521v1",
      "title": "Graph4GUI: Graph Neural Networks for Representing Graphical User Interfaces",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Jiang",
        "Changkong Zhou",
        "Vikas Garg",
        "Antti Oulasvirta"
      ],
      "abstract": "Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of\ntext, graphics, and interactive elements such as buttons and menus, but\nrepresentations of GUIs have not kept up. They do not encapsulate both semantic\nand visuo-spatial relationships among elements. To seize machine learning's\npotential for GUIs more efficiently, Graph4GUI exploits graph neural networks\nto capture individual elements' properties and their semantic-visuo-spatial\nconstraints in a layout. The learned representation demonstrated its\neffectiveness in multiple tasks, especially generating designs in a challenging\nGUI autocompletion task, which involved predicting the positions of remaining\nunplaced elements in a partially completed GUI. The new model's suggestions\nshowed alignment and visual appeal superior to the baseline method and received\nhigher subjective ratings for preference. Furthermore, we demonstrate the\npractical benefits and efficiency advantages designers perceive when utilizing\nour model as an autocompletion plug-in.",
      "tldr_zh": "该论文提出 Graph4GUI，一种利用 Graph Neural Networks (GNNs) 表示图形用户界面 (GUIs) 的方法，能够同时捕捉元素属性的语义和视空间关系，从而更高效地应用机器学习于 GUI 设计。Graph4GUI 在多个任务中表现出色，特别是 GUI 自动完成任务中，通过预测未放置元素的位，提供比基线方法更优的对齐和视觉吸引力。实验结果显示，该模型获得了更高的主观偏好评分，并作为自动完成插件为设计师带来了实际益处和效率提升。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.13521v1",
      "published_date": "2024-04-21 04:06:09 UTC",
      "updated_date": "2024-04-21 04:06:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:24:16.296576"
    },
    {
      "arxiv_id": "2404.14453v1",
      "title": "EPI-SQL: Enhancing Text-to-SQL Translation with Error-Prevention Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Xiping Liu",
        "Zhao Tan"
      ],
      "abstract": "The conversion of natural language queries into SQL queries, known as\nText-to-SQL, is a critical yet challenging task. This paper introduces EPI-SQL,\na novel methodological framework leveraging Large Language Models (LLMs) to\nenhance the performance of Text-to-SQL tasks. EPI-SQL operates through a\nfour-step process. Initially, the method involves gathering instances from the\nSpider dataset on which LLMs are prone to failure. These instances are then\nutilized to generate general error-prevention instructions (EPIs).\nSubsequently, LLMs craft contextualized EPIs tailored to the specific context\nof the current task. Finally, these context-specific EPIs are incorporated into\nthe prompt used for SQL generation. EPI-SQL is distinguished in that it\nprovides task-specific guidance, enabling the model to circumvent potential\nerrors for the task at hand. Notably, the methodology rivals the performance of\nadvanced few-shot methods despite being a zero-shot approach. An empirical\nassessment using the Spider benchmark reveals that EPI-SQL achieves an\nexecution accuracy of 85.1\\%, underscoring its effectiveness in generating\naccurate SQL queries through LLMs. The findings indicate a promising direction\nfor future research, i.e. enhancing instructions with task-specific and\ncontextualized rules, for boosting LLMs' performance in NLP tasks.",
      "tldr_zh": "本研究引入了 EPI-SQL，一种利用 Large Language Models (LLMs) 的新框架，用于提升 Text-to-SQL 任务的性能，通过生成和整合错误预防指令 (EPIs) 来避免常见错误。EPI-SQL 的四步过程包括：从 Spider 数据集收集 LLMs 易失败的实例、创建一般 EPIs、生成针对特定任务的上下文化 EPIs，并将这些指令融入 SQL 生成提示中。尽管采用 zero-shot 方式，该方法在 Spider 基准测试中实现了 85.1% 的执行准确率，与先进的 few-shot 方法相当。研究结果表明，通过任务特定和上下文化规则增强指令，有望进一步提升 LLMs 在 NLP 任务中的表现。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.14453v1",
      "published_date": "2024-04-21 03:52:46 UTC",
      "updated_date": "2024-04-21 03:52:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:24:29.368755"
    },
    {
      "arxiv_id": "2404.13518v1",
      "title": "Reliable Model Watermarking: Defending Against Theft without Compromising on Evasion",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyu Zhu",
        "Sichu Liang",
        "Wentao Hu",
        "Fangqi Li",
        "Ju Jia",
        "Shilin Wang"
      ],
      "abstract": "With the rise of Machine Learning as a Service (MLaaS) platforms,safeguarding\nthe intellectual property of deep learning models is becoming paramount. Among\nvarious protective measures, trigger set watermarking has emerged as a flexible\nand effective strategy for preventing unauthorized model distribution. However,\nthis paper identifies an inherent flaw in the current paradigm of trigger set\nwatermarking: evasion adversaries can readily exploit the shortcuts created by\nmodels memorizing watermark samples that deviate from the main task\ndistribution, significantly impairing their generalization in adversarial\nsettings. To counteract this, we leverage diffusion models to synthesize\nunrestricted adversarial examples as trigger sets. By learning the model to\naccurately recognize them, unique watermark behaviors are promoted through\nknowledge injection rather than error memorization, thus avoiding exploitable\nshortcuts. Furthermore, we uncover that the resistance of current trigger set\nwatermarking against removal attacks primarily relies on significantly damaging\nthe decision boundaries during embedding, intertwining unremovability with\nadverse impacts. By optimizing the knowledge transfer properties of protected\nmodels, our approach conveys watermark behaviors to extraction surrogates\nwithout aggressively decision boundary perturbation. Experimental results on\nCIFAR-10/100 and Imagenette datasets demonstrate the effectiveness of our\nmethod, showing not only improved robustness against evasion adversaries but\nalso superior resistance to watermark removal attacks compared to\nstate-of-the-art solutions.",
      "tldr_zh": "该论文针对机器学习模型水印技术的问题，提出了一种可靠的模型水印方法，以防范未经授权的模型盗用，同时避免影响模型对evasion adversaries的抵抗力。作者发现，传统trigger set watermarking易受模型记忆水印样本的捷径(shortcuts)影响，导致泛化能力下降，因此改用diffusion models合成不受限制的adversarial examples作为触发集，并通过knowledge injection注入独特水印行为，以避免错误记忆和决策边界(decision boundaries)过度扰动。实验结果显示，在CIFAR-10/100和Imagenette数据集上，该方法显著提升了对evasion adversaries的鲁棒性，并比现有方案更有效地抵抗watermark removal attacks。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13518v1",
      "published_date": "2024-04-21 03:38:20 UTC",
      "updated_date": "2024-04-21 03:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:24:42.749817"
    },
    {
      "arxiv_id": "2404.13515v2",
      "title": "FedTrans: Efficient Federated Learning via Multi-Model Transformation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Zhu",
        "Jiachen Liu",
        "Mosharaf Chowdhury",
        "Fan Lai"
      ],
      "abstract": "Federated learning (FL) aims to train machine learning (ML) models across\npotentially millions of edge client devices. Yet, training and customizing\nmodels for FL clients is notoriously challenging due to the heterogeneity of\nclient data, device capabilities, and the massive scale of clients, making\nindividualized model exploration prohibitively expensive. State-of-the-art FL\nsolutions personalize a globally trained model or concurrently train multiple\nmodels, but they often incur suboptimal model accuracy and huge training costs.\n  In this paper, we introduce FedTrans, a multi-model FL training framework\nthat automatically produces and trains high-accuracy, hardware-compatible\nmodels for individual clients at scale. FedTrans begins with a basic global\nmodel, identifies accuracy bottlenecks in model architectures during training,\nand then employs model transformation to derive new models for heterogeneous\nclients on the fly. It judiciously assigns models to individual clients while\nperforming soft aggregation on multi-model updates to minimize total training\ncosts. Our evaluations using realistic settings show that FedTrans improves\nindividual client model accuracy by 14% - 72% while slashing training costs by\n1.6X - 20X over state-of-the-art solutions.",
      "tldr_zh": "本论文提出FedTrans，一种高效的Federated Learning (FL) 框架，旨在解决异构客户端数据、设备能力和大规模训练带来的挑战，通过自动生成和训练高精度硬件兼容模型来实现个性化模型探索。FedTrans从一个基本全局模型出发，在训练过程中识别准确率瓶颈，并采用模型转换技术动态为客户端生成新模型，同时通过智能模型分配和软聚合最小化总训练成本。实验结果显示，FedTrans将个客户端模型准确率提高了14%-72%，并将训练成本降低了1.6X-20X，显著优于现有FL解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13515v2",
      "published_date": "2024-04-21 03:31:01 UTC",
      "updated_date": "2024-04-25 20:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:24:53.966780"
    },
    {
      "arxiv_id": "2404.13509v1",
      "title": "MFHCA: Enhancing Speech Emotion Recognition Via Multi-Spatial Fusion and Hierarchical Cooperative Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Xinxin Jiao",
        "Liejun Wang",
        "Yinfeng Yu"
      ],
      "abstract": "Speech emotion recognition is crucial in human-computer interaction, but\nextracting and using emotional cues from audio poses challenges. This paper\nintroduces MFHCA, a novel method for Speech Emotion Recognition using\nMulti-Spatial Fusion and Hierarchical Cooperative Attention on spectrograms and\nraw audio. We employ the Multi-Spatial Fusion module (MF) to efficiently\nidentify emotion-related spectrogram regions and integrate Hubert features for\nhigher-level acoustic information. Our approach also includes a Hierarchical\nCooperative Attention module (HCA) to merge features from various auditory\nlevels. We evaluate our method on the IEMOCAP dataset and achieve 2.6\\% and\n1.87\\% improvements on the weighted accuracy and unweighted accuracy,\nrespectively. Extensive experiments demonstrate the effectiveness of the\nproposed method.",
      "tldr_zh": "该论文提出了 MFHCA，一种新型语音情感识别方法，通过 Multi-Spatial Fusion 模块高效识别情感相关的频谱图区域，并整合 Hubert features 以获取高级声学信息。\n同时，该方法引入 Hierarchical Cooperative Attention 模块，用于合并不同听觉水平的特征，从而提升整体性能。\n在 IEMOCAP 数据集上的实验显示，MFHCA 分别将加权准确率和未加权准确率提高了 2.6% 和 1.87%，证明了其有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Main paper (5 pages). Accepted for publication by ICME 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13509v1",
      "published_date": "2024-04-21 02:44:17 UTC",
      "updated_date": "2024-04-21 02:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:25:06.148692"
    },
    {
      "arxiv_id": "2404.13506v2",
      "title": "Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications",
      "title_zh": "参数高效微调：跨应用领域的全面分析",
      "authors": [
        "Charith Chandra Sai Balne",
        "Sreyoshi Bhaduri",
        "Tamoghna Roy",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "abstract": "The rise of deep learning has marked significant progress in fields such as\ncomputer vision, natural language processing, and medical imaging, primarily\nthrough the adaptation of pre-trained models for specific tasks. Traditional\nfine-tuning methods, involving adjustments to all parameters, face challenges\ndue to high computational and memory demands. This has led to the development\nof Parameter Efficient Fine-Tuning (PEFT) techniques, which selectively update\nparameters to balance computational efficiency with performance. This review\nexamines PEFT approaches, offering a detailed comparison of various strategies\nhighlighting applications across different domains, including text generation,\nmedical imaging, protein modeling, and speech synthesis. By assessing the\neffectiveness of PEFT methods in reducing computational load, speeding up\ntraining, and lowering memory usage, this paper contributes to making deep\nlearning more accessible and adaptable, facilitating its wider application and\nencouraging innovation in model optimization. Ultimately, the paper aims to\ncontribute towards insights into PEFT's evolving landscape, guiding researchers\nand practitioners in overcoming the limitations of conventional fine-tuning\napproaches.",
      "tldr_zh": "这篇论文对 Parameter Efficient Fine-Tuning (PEFT) 技术进行了全面分析，旨在解决传统 fine-tuning 方法在深度学习应用中面临的高计算和内存需求问题。作者审查并比较了各种 PEFT 策略，包括在文本生成、医疗成像、蛋白质建模和语音合成等领域的应用，强调其在减少计算负载、加速训练和降低内存使用方面的有效性。研究发现，PEFT 能显著提升深度学习的适应性和可访问性，帮助研究者和从业者克服传统方法的局限。最终，该论文为模型优化创新提供了宝贵指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13506v2",
      "published_date": "2024-04-21 02:26:15 UTC",
      "updated_date": "2024-04-23 21:28:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:25:18.364816"
    },
    {
      "arxiv_id": "2404.13501v1",
      "title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyu Zhang",
        "Xiaohe Bo",
        "Chen Ma",
        "Rui Li",
        "Xu Chen",
        "Quanyu Dai",
        "Jieming Zhu",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "abstract": "Large language model (LLM) based agents have recently attracted much\nattention from the research and industry communities. Compared with original\nLLMs, LLM-based agents are featured in their self-evolving capability, which is\nthe basis for solving real-world problems that need long-term and complex\nagent-environment interactions. The key component to support agent-environment\ninteractions is the memory of the agents. While previous studies have proposed\nmany promising memory mechanisms, they are scattered in different papers, and\nthere lacks a systematical review to summarize and compare these works from a\nholistic perspective, failing to abstract common and effective designing\npatterns for inspiring future studies. To bridge this gap, in this paper, we\npropose a comprehensive survey on the memory mechanism of LLM-based agents. In\nspecific, we first discuss ''what is'' and ''why do we need'' the memory in\nLLM-based agents. Then, we systematically review previous studies on how to\ndesign and evaluate the memory module. In addition, we also present many agent\napplications, where the memory module plays an important role. At last, we\nanalyze the limitations of existing work and show important future directions.\nTo keep up with the latest advances in this field, we create a repository at\n\\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",
      "tldr_zh": "这篇调查论文探讨了Large Language Model (LLM) based agents的记忆机制，强调记忆模块在支持代理与环境的长期互动中至关重要。作者系统回顾了现有研究，包括记忆机制的设计原则、评估方法以及在各种代理应用中的作用，并抽象出常见设计模式以启发未来工作。该调查还分析了当前工作的局限性，并指出了未来研究方向，如持续跟踪最新进展（仓库链接：https://github.com/nuster1128/LLM_Agent_Memory_Survey）。总的来说，这为构建自我演化的LLM-based agents提供了全面总结和指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "39 pages, 5 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.13501v1",
      "published_date": "2024-04-21 01:49:46 UTC",
      "updated_date": "2024-04-21 01:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:25:28.671068"
    },
    {
      "arxiv_id": "2404.13500v1",
      "title": "Generalized Regression with Conditional GANs",
      "title_zh": "翻译失败",
      "authors": [
        "Deddy Jobson",
        "Eddy Hudson"
      ],
      "abstract": "Regression is typically treated as a curve-fitting process where the goal is\nto fit a prediction function to data. With the help of conditional generative\nadversarial networks, we propose to solve this age-old problem in a different\nway; we aim to learn a prediction function whose outputs, when paired with the\ncorresponding inputs, are indistinguishable from feature-label pairs in the\ntraining dataset. We show that this approach to regression makes fewer\nassumptions on the distribution of the data we are fitting to and, therefore,\nhas better representation capabilities. We draw parallels with generalized\nlinear models in statistics and show how our proposal serves as an extension of\nthem to neural networks. We demonstrate the superiority of this new approach to\nstandard regression with experiments on multiple synthetic and publicly\navailable real-world datasets, finding encouraging results, especially with\nreal-world heavy-tailed regression datasets. To make our work more\nreproducible, we release our source code. Link to repository:\nhttps://anonymous.4open.science/r/regressGAN-7B71/",
      "tldr_zh": "本论文提出了一种新的回归方法，使用Conditional GANs，将回归问题转化为学习一个预测函数，使其输出与输入配对时，无法与训练数据集中的特征-标签对区分开来。该方法减少了对数据分布的假设，从而提升了表示能力，并将统计学中的Generalized Linear Models扩展到神经网络框架。实验结果显示，该方法在多个合成和真实数据集上优于标准回归，尤其在重尾分布的真实数据集上表现出色；作者还发布了源代码以促进可重复性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13500v1",
      "published_date": "2024-04-21 01:27:47 UTC",
      "updated_date": "2024-04-21 01:27:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:25:41.048861"
    },
    {
      "arxiv_id": "2405.06658v1",
      "title": "ProteinEngine: Empower LLM with Domain Knowledge for Protein Engineering",
      "title_zh": "ProteinEngine：利用领域知识赋能 LLM 用于蛋白质工程",
      "authors": [
        "Yiqing Shen",
        "Outongyi Lv",
        "Houying Zhu",
        "Yu Guang Wang"
      ],
      "abstract": "Large language models (LLMs) have garnered considerable attention for their\nproficiency in tackling intricate tasks, particularly leveraging their\ncapacities for zero-shot and in-context learning. However, their utility has\nbeen predominantly restricted to general tasks due to an absence of\ndomain-specific knowledge. This constraint becomes particularly pertinent in\nthe realm of protein engineering, where specialized expertise is required for\ntasks such as protein function prediction, protein evolution analysis, and\nprotein design, with a level of specialization that existing LLMs cannot\nfurnish. In response to this challenge, we introduce \\textsc{ProteinEngine}, a\nhuman-centered platform aimed at amplifying the capabilities of LLMs in protein\nengineering by seamlessly integrating a comprehensive range of relevant tools,\npackages, and software via API calls. Uniquely, \\textsc{ProteinEngine} assigns\nthree distinct roles to LLMs, facilitating efficient task delegation,\nspecialized task resolution, and effective communication of results. This\ndesign fosters high extensibility and promotes the smooth incorporation of new\nalgorithms, models, and features for future development. Extensive user\nstudies, involving participants from both the AI and protein engineering\ncommunities across academia and industry, consistently validate the superiority\nof \\textsc{ProteinEngine} in augmenting the reliability and precision of deep\nlearning in protein engineering tasks. Consequently, our findings highlight the\npotential of \\textsc{ProteinEngine} to bride the disconnected tools for future\nresearch in the protein engineering domain.",
      "tldr_zh": "该研究指出，大型语言模型（LLMs）在零样本学习（zero-shot）和上下文学习（in-context learning）方面表现出色，但缺乏领域特定知识，限制了其在蛋白质工程（如蛋白功能预测、进化分析和设计）中的应用。为解决此问题，研究团队推出了 ProteinEngine，一个以人为中心的平台，通过整合相关工具、包和软件（via API calls），并为 LLMs 分配三个角色（任务分配、专门任务解决和结果沟通），从而提升其在蛋白质工程领域的能力。实验包括广泛的用户研究，参与者来自 AI 和蛋白质工程社区，结果显示 ProteinEngine 显著提高了深度学习任务的可靠性和精确性。该平台促进了高扩展性和新算法的整合，为未来蛋白质工程研究桥接工具断层提供了潜力。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.06658v1",
      "published_date": "2024-04-21 01:07:33 UTC",
      "updated_date": "2024-04-21 01:07:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:25:54.751976"
    },
    {
      "arxiv_id": "2404.13496v1",
      "title": "ODE-DPS: ODE-based Diffusion Posterior Sampling for Inverse Problems in Partial Differential Equation",
      "title_zh": "翻译失败",
      "authors": [
        "Enze Jiang",
        "Jishen Peng",
        "Zheng Ma",
        "Xiong-Bin Yan"
      ],
      "abstract": "In recent years we have witnessed a growth in mathematics for deep learning,\nwhich has been used to solve inverse problems of partial differential equations\n(PDEs). However, most deep learning-based inversion methods either require\npaired data or necessitate retraining neural networks for modifications in the\nconditions of the inverse problem, significantly reducing the efficiency of\ninversion and limiting its applicability. To overcome this challenge, in this\npaper, leveraging the score-based generative diffusion model, we introduce a\nnovel unsupervised inversion methodology tailored for solving inverse problems\narising from PDEs. Our approach operates within the Bayesian inversion\nframework, treating the task of solving the posterior distribution as a\nconditional generation process achieved through solving a reverse-time\nstochastic differential equation. Furthermore, to enhance the accuracy of\ninversion results, we propose an ODE-based Diffusion Posterior Sampling\ninversion algorithm. The algorithm stems from the marginal probability density\nfunctions of two distinct forward generation processes that satisfy the same\nFokker-Planck equation. Through a series of experiments involving various PDEs,\nwe showcase the efficiency and robustness of our proposed method.",
      "tldr_zh": "本文提出了一种基于分数生成扩散模型的无监督方法，用于解决偏微分方程（PDEs）的逆问题，克服了传统深度学习方法对配对数据依赖和重新训练的需求。该方法在贝叶斯逆问题框架下，将后验分布求解视为条件生成过程，通过反向时间随机微分方程实现精确逆向。作者进一步引入 ODE-based Diffusion Posterior Sampling（ODE-DPS）算法，该算法基于两个满足相同 Fokker-Planck 方程的前向生成过程，提升了逆问题结果的准确性。通过多种 PDEs 的实验，展示了该方法的效率和鲁棒性。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13496v1",
      "published_date": "2024-04-21 00:57:13 UTC",
      "updated_date": "2024-04-21 00:57:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T02:26:07.279580"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 59,
  "processed_papers_count": 59,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T02:26:30.325119"
}
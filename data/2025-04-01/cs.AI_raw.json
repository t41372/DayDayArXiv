[
  {
    "arxiv_id": "2504.01252v1",
    "title": "Plan-and-Act using Large Language Models for Interactive Agreement",
    "authors": [
      "Kazuhiro Sasabuchi",
      "Naoki Wake",
      "Atsushi Kanehira",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ],
    "abstract": "Recent large language models (LLMs) are capable of planning robot actions. In\nthis paper, we explore how LLMs can be used for planning actions with tasks\ninvolving situational human-robot interaction (HRI). A key problem of applying\nLLMs in situational HRI is balancing between \"respecting the current human's\nactivity\" and \"prioritizing the robot's task,\" as well as understanding the\ntiming of when to use the LLM to generate an action plan. In this paper, we\npropose a necessary plan-and-act skill design to solve the above problems. We\nshow that a critical factor for enabling a robot to switch between passive /\nactive interaction behavior is to provide the LLM with an action text about the\ncurrent robot's action. We also show that a second-stage question to the LLM\n(about the next timing to call the LLM) is necessary for planning actions at an\nappropriate timing. The skill design is applied to an Engage skill and is\ntested on four distinct interaction scenarios. We show that by using the skill\ndesign, LLMs can be leveraged to easily scale to different HRI scenarios with a\nreasonable success rate reaching 90% on the test scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01252v1",
    "published_date": "2025-04-01 23:41:05 UTC",
    "updated_date": "2025-04-01 23:41:05 UTC"
  },
  {
    "arxiv_id": "2504.01248v1",
    "title": "Automated Factual Benchmarking for In-Car Conversational Systems using Large Language Models",
    "authors": [
      "Rafael Giebisch",
      "Ken E. Friedl",
      "Lev Sorokin",
      "Andrea Stocco"
    ],
    "abstract": "In-car conversational systems bring the promise to improve the in-vehicle\nuser experience. Modern conversational systems are based on Large Language\nModels (LLMs), which makes them prone to errors such as hallucinations, i.e.,\ninaccurate, fictitious, and therefore factually incorrect information. In this\npaper, we present an LLM-based methodology for the automatic factual\nbenchmarking of in-car conversational systems. We instantiate our methodology\nwith five LLM-based methods, leveraging ensembling techniques and diverse\npersonae to enhance agreement and minimize hallucinations. We use our\nmethodology to evaluate CarExpert, an in-car retrieval-augmented conversational\nquestion answering system, with respect to the factual correctness to a\nvehicle's manual. We produced a novel dataset specifically created for the\nin-car domain, and tested our methodology against an expert evaluation. Our\nresults show that the combination of GPT-4 with the Input Output Prompting\nachieves over 90 per cent factual correctness agreement rate with expert\nevaluations, other than being the most efficient approach yielding an average\nresponse time of 4.5s. Our findings suggest that LLM-based testing constitutes\na viable approach for the validation of conversational systems regarding their\nfactual correctness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in IEEE Intelligent Vehicles Symposium Conference (IV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.01248v1",
    "published_date": "2025-04-01 23:25:30 UTC",
    "updated_date": "2025-04-01 23:25:30 UTC"
  },
  {
    "arxiv_id": "2504.01246v1",
    "title": "Dynamic Graph Structure Estimation for Learning Multivariate Point Process using Spiking Neural Networks",
    "authors": [
      "Biswadeep Chakraborty",
      "Hemant Kumawat",
      "Beomseok Kang",
      "Saibal Mukhopadhyay"
    ],
    "abstract": "Modeling and predicting temporal point processes (TPPs) is critical in\ndomains such as neuroscience, epidemiology, finance, and social sciences. We\nintroduce the Spiking Dynamic Graph Network (SDGN), a novel framework that\nleverages the temporal processing capabilities of spiking neural networks\n(SNNs) and spike-timing-dependent plasticity (STDP) to dynamically estimate\nunderlying spatio-temporal functional graphs. Unlike existing methods that rely\non predefined or static graph structures, SDGN adapts to any dataset by\nlearning dynamic spatio-temporal dependencies directly from the event data,\nenhancing generalizability and robustness. While SDGN offers significant\nimprovements over prior methods, we acknowledge its limitations in handling\ndense graphs and certain non-Gaussian dependencies, providing opportunities for\nfuture refinement. Our evaluations, conducted on both synthetic and real-world\ndatasets including NYC Taxi, 911, Reddit, and Stack Overflow, demonstrate that\nSDGN achieves superior predictive accuracy while maintaining computational\nefficiency. Furthermore, we include ablation studies to highlight the\ncontributions of its core components.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01246v1",
    "published_date": "2025-04-01 23:23:10 UTC",
    "updated_date": "2025-04-01 23:23:10 UTC"
  },
  {
    "arxiv_id": "2504.01243v2",
    "title": "FUSION: Frequency-guided Underwater Spatial Image recOnstructioN",
    "authors": [
      "Jaskaran Singh Walia",
      "Shravan Venkatraman",
      "Pavithra LK"
    ],
    "abstract": "Underwater images suffer from severe degradations, including color\ndistortions, reduced visibility, and loss of structural details due to\nwavelength-dependent attenuation and scattering. Existing enhancement methods\nprimarily focus on spatial-domain processing, neglecting the frequency domain's\npotential to capture global color distributions and long-range dependencies. To\naddress these limitations, we propose FUSION, a dual-domain deep learning\nframework that jointly leverages spatial and frequency domain information.\nFUSION independently processes each RGB channel through multi-scale\nconvolutional kernels and adaptive attention mechanisms in the spatial domain,\nwhile simultaneously extracting global structural information via FFT-based\nfrequency attention. A Frequency Guided Fusion module integrates complementary\nfeatures from both domains, followed by inter-channel fusion and adaptive\nchannel recalibration to ensure balanced color distributions. Extensive\nexperiments on benchmark datasets (UIEB, EUVP, SUIM-E) demonstrate that FUSION\nachieves state-of-the-art performance, consistently outperforming existing\nmethods in reconstruction fidelity (highest PSNR of 23.717 dB and SSIM of 0.883\non UIEB), perceptual quality (lowest LPIPS of 0.112 on UIEB), and visual\nenhancement metrics (best UIQM of 3.414 on UIEB), while requiring significantly\nfewer parameters (0.28M) and lower computational complexity, demonstrating its\nsuitability for real-time underwater imaging applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01243v2",
    "published_date": "2025-04-01 23:16:19 UTC",
    "updated_date": "2025-04-13 19:51:56 UTC"
  },
  {
    "arxiv_id": "2504.01228v1",
    "title": "TenAd: A Tensor-based Low-rank Black Box Adversarial Attack for Video Classification",
    "authors": [
      "Kimia haghjooei",
      "Mansoor Rezghi"
    ],
    "abstract": "Deep learning models have achieved remarkable success in computer vision but\nremain vulnerable to adversarial attacks, particularly in black-box settings\nwhere model details are unknown. Existing adversarial attack methods(even those\nworks with key frames) often treat video data as simple vectors, ignoring their\ninherent multi-dimensional structure, and require a large number of queries,\nmaking them inefficient and detectable. In this paper, we propose\n\\textbf{TenAd}, a novel tensor-based low-rank adversarial attack that leverages\nthe multi-dimensional properties of video data by representing videos as\nfourth-order tensors. By exploiting low-rank attack, our method significantly\nreduces the search space and the number of queries needed to generate\nadversarial examples in black-box settings. Experimental results on standard\nvideo classification datasets demonstrate that \\textbf{TenAd} effectively\ngenerates imperceptible adversarial perturbations while achieving higher attack\nsuccess rates and query efficiency compared to state-of-the-art methods. Our\napproach outperforms existing black-box adversarial attacks in terms of success\nrate, query efficiency, and perturbation imperceptibility, highlighting the\npotential of tensor-based methods for adversarial attacks on video models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01228v1",
    "published_date": "2025-04-01 22:35:28 UTC",
    "updated_date": "2025-04-01 22:35:28 UTC"
  },
  {
    "arxiv_id": "2504.01225v1",
    "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates",
    "authors": [
      "Gon√ßalo Gomes",
      "Chrysoula Zerva",
      "Bruno Martins"
    ],
    "abstract": "This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessment for individual\nword misalignments within captions, and the reliance on single-point quality\nestimates without considering uncertainty. To address these limitations, we\npropose a simple yet effective strategy for generating and calibrating\nCLIPScore distributions. Leveraging a model-agnostic conformal risk control\nframework, we calibrate CLIPScore values for task-specific control variables,\nto tackle the aforementioned two limitations. Experimental results demonstrate\nthat using conformal risk control, over the distributions produced with simple\nmethods such as input masking, can achieve competitive performance compared to\nmore complex approaches. Our method effectively detects misaligned words, while\nproviding formal guarantees aligned with desired risk levels, and improving the\ncorrelation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01225v1",
    "published_date": "2025-04-01 22:25:00 UTC",
    "updated_date": "2025-04-01 22:25:00 UTC"
  },
  {
    "arxiv_id": "2504.01216v1",
    "title": "Detecting PTSD in Clinical Interviews: A Comparative Analysis of NLP Methods and Large Language Models",
    "authors": [
      "Feng Chen",
      "Dror Ben-Zeev",
      "Gillian Sparks",
      "Arya Kadakia",
      "Trevor Cohen"
    ],
    "abstract": "Post-Traumatic Stress Disorder (PTSD) remains underdiagnosed in clinical\nsettings, presenting opportunities for automated detection to identify\npatients. This study evaluates natural language processing approaches for\ndetecting PTSD from clinical interview transcripts. We compared general and\nmental health-specific transformer models (BERT/RoBERTa), embedding-based\nmethods (SentenceBERT/LLaMA), and large language model prompting strategies\n(zero-shot/few-shot/chain-of-thought) using the DAIC-WOZ dataset.\nDomain-specific models significantly outperformed general models\n(Mental-RoBERTa F1=0.643 vs. RoBERTa-base 0.485). LLaMA embeddings with neural\nnetworks achieved the highest performance (F1=0.700). Zero-shot prompting using\nDSM-5 criteria yielded competitive results without training data (F1=0.657).\nPerformance varied significantly across symptom severity and comorbidity\nstatus, with higher accuracy for severe PTSD cases and patients with comorbid\ndepression. Our findings highlight the potential of domain-adapted embeddings\nand LLMs for scalable screening while underscoring the need for improved\ndetection of nuanced presentations and offering insights for developing\nclinically viable AI tools for PTSD assessment.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 4 tables, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2504.01216v1",
    "published_date": "2025-04-01 22:06:28 UTC",
    "updated_date": "2025-04-01 22:06:28 UTC"
  },
  {
    "arxiv_id": "2504.01214v1",
    "title": "PolygoNet: Leveraging Simplified Polygonal Representation for Effective Image Classification",
    "authors": [
      "Salim Khazem",
      "Jeremy Fix",
      "C√©dric Pradalier"
    ],
    "abstract": "Deep learning models have achieved significant success in various image\nrelated tasks. However, they often encounter challenges related to\ncomputational complexity and overfitting. In this paper, we propose an\nefficient approach that leverages polygonal representations of images using\ndominant points or contour coordinates. By transforming input images into these\ncompact forms, our method significantly reduces computational requirements,\naccelerates training, and conserves resources making it suitable for real time\nand resource constrained applications. These representations inherently capture\nessential image features while filtering noise, providing a natural\nregularization effect that mitigates overfitting. The resulting lightweight\nmodels achieve performance comparable to state of the art methods using full\nresolution images while enabling deployment on edge devices. Extensive\nexperiments on benchmark datasets validate the effectiveness of our approach in\nreducing complexity, improving generalization, and facilitating edge computing\napplications. This work demonstrates the potential of polygonal representations\nin advancing efficient and scalable deep learning solutions for real world\nscenarios. The code for the experiments of the paper is provided in\nhttps://github.com/salimkhazem/PolygoNet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01214v1",
    "published_date": "2025-04-01 22:05:00 UTC",
    "updated_date": "2025-04-01 22:05:00 UTC"
  },
  {
    "arxiv_id": "2504.01211v1",
    "title": "Off-Policy Evaluation for Sequential Persuasion Process with Unobserved Confounding",
    "authors": [
      "Nishanth Venkatesh S.",
      "Heeseung Bang",
      "Andreas A. Malikopoulos"
    ],
    "abstract": "In this paper, we expand the Bayesian persuasion framework to account for\nunobserved confounding variables in sender-receiver interactions. While\ntraditional models assume that belief updates follow Bayesian principles,\nreal-world scenarios often involve hidden variables that impact the receiver's\nbelief formation and decision-making. We conceptualize this as a sequential\ndecision-making problem, where the sender and receiver interact over multiple\nrounds. In each round, the sender communicates with the receiver, who also\ninteracts with the environment. Crucially, the receiver's belief update is\naffected by an unobserved confounding variable. By reformulating this scenario\nas a Partially Observable Markov Decision Process (POMDP), we capture the\nsender's incomplete information regarding both the dynamics of the receiver's\nbeliefs and the unobserved confounder. We prove that finding an optimal\nobservation-based policy in this POMDP is equivalent to solving for an optimal\nsignaling strategy in the original persuasion framework. Furthermore, we\ndemonstrate how this reformulation facilitates the application of proximal\nlearning for off-policy evaluation in the persuasion process. This advancement\nenables the sender to evaluate alternative signaling strategies using only\nobservational data from a behavioral policy, thus eliminating the necessity for\ncostly new experiments.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01211v1",
    "published_date": "2025-04-01 21:50:32 UTC",
    "updated_date": "2025-04-01 21:50:32 UTC"
  },
  {
    "arxiv_id": "2504.01208v1",
    "title": "Lightweight Deep Models for Dermatological Disease Detection: A Study on Instance Selection and Channel Optimization",
    "authors": [
      "Ian Mateos Gonzalez",
      "Estefani Jaramilla Nava",
      "Abraham S√°nchez Morales",
      "Jes√∫s Garc√≠a-Ram√≠rez",
      "Ricardo Ramos-Aguilar"
    ],
    "abstract": "The identification of dermatological disease is an important problem in\nMexico according with different studies. Several works in literature use the\ndatasets of different repositories without applying a study of the data\nbehavior, especially in medical images domain. In this work, we propose a\nmethodology to preprocess dermaMNIST dataset in order to improve its quality\nfor the classification stage, where we use lightweight convolutional neural\nnetworks. In our results, we reduce the number of instances for the neural\nnetwork training obtaining a similar performance of models as ResNet.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Submitted to Mexican Conference on Pattern Recognition 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01208v1",
    "published_date": "2025-04-01 21:47:57 UTC",
    "updated_date": "2025-04-01 21:47:57 UTC"
  },
  {
    "arxiv_id": "2504.01205v1",
    "title": "Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery",
    "authors": [
      "Nicholas Clark",
      "Hua Shen",
      "Bill Howe",
      "Tanushree Mitra"
    ],
    "abstract": "LLMs increasingly serve as tools for knowledge acquisition, yet users cannot\neffectively specify how they want information presented. When users request\nthat LLMs \"cite reputable sources,\" \"express appropriate uncertainty,\" or\n\"include multiple perspectives,\" they discover that current interfaces provide\nno structured way to articulate these preferences. The result is prompt sharing\nfolklore: community-specific copied prompts passed through trust relationships\nrather than based on measured efficacy. We propose the Epistemic Alignment\nFramework, a set of ten challenges in knowledge transmission derived from the\nphilosophical literature of epistemology, concerning issues such as evidence\nquality assessment and calibration of testimonial reliance. The framework\nserves as a structured intermediary between user needs and system capabilities,\ncreating a common vocabulary to bridge the gap between what users want and what\nsystems deliver. Through a thematic analysis of custom prompts and\npersonalization strategies shared on online communities where these issues are\nactively discussed, we find users develop elaborate workarounds to address each\nof the challenges. We then apply our framework to two prominent model\nproviders, OpenAI and Anthropic, through content analysis of their documented\npolicies and product features. Our analysis shows that while these providers\nhave partially addressed the challenges we identified, they fail to establish\nadequate mechanisms for specifying epistemic preferences, lack transparency\nabout how preferences are implemented, and offer no verification tools to\nconfirm whether preferences were followed. For AI developers, the Epistemic\nAlignment Framework offers concrete guidance for supporting diverse approaches\nto knowledge; for users, it works toward information delivery that aligns with\ntheir specific needs rather than defaulting to one-size-fits-all approaches.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01205v1",
    "published_date": "2025-04-01 21:38:12 UTC",
    "updated_date": "2025-04-01 21:38:12 UTC"
  },
  {
    "arxiv_id": "2504.01201v1",
    "title": "Medical large language models are easily distracted",
    "authors": [
      "Krithik Vishwanath",
      "Anton Alyakin",
      "Daniel Alexander Alber",
      "Jin Vivian Lee",
      "Douglas Kondziolka",
      "Eric Karl Oermann"
    ],
    "abstract": "Large language models (LLMs) have the potential to transform medicine, but\nreal-world clinical scenarios contain extraneous information that can hinder\nperformance. The rise of assistive technologies like ambient dictation, which\nautomatically generates draft notes from live patient encounters, has the\npotential to introduce additional noise making it crucial to assess the ability\nof LLM's to filter relevant data. To investigate this, we developed\nMedDistractQA, a benchmark using USMLE-style questions embedded with simulated\nreal-world distractions. Our findings show that distracting statements\n(polysemous words with clinical meanings used in a non-clinical context or\nreferences to unrelated health conditions) can reduce LLM accuracy by up to\n17.9%. Commonly proposed solutions to improve model performance such as\nretrieval-augmented generation (RAG) and medical fine-tuning did not change\nthis effect and in some cases introduced their own confounders and further\ndegraded performance. Our findings suggest that LLMs natively lack the logical\nmechanisms necessary to distinguish relevant from irrelevant clinical\ninformation, posing challenges for real-world applications. MedDistractQA and\nour results highlights the need for robust mitigation strategies to enhance LLM\nresilience to extraneous information.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 2 main figures, 6 extended figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01201v1",
    "published_date": "2025-04-01 21:34:01 UTC",
    "updated_date": "2025-04-01 21:34:01 UTC"
  },
  {
    "arxiv_id": "2504.01196v1",
    "title": "$Œº$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models",
    "authors": [
      "Zian Su",
      "Ziyang Huang",
      "Kaiyuan Zhang",
      "Xiangyu Zhang"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful knowledge bases yet are\nlimited by static training data, leading to issues such as hallucinations and\nsafety risks. Editing a model's internal knowledge through the locate-and-edit\nparadigm has proven a cost-effective alternative to retraining, though current\nunstructured approaches, especially window-based autoregressive methods, often\ndisrupt the causal dependency between early memory updates and later output\ntokens. In this work, we first theoretically analyze these limitations and then\nintroduce Matryoshka Unstructured Knowledge Editing ($\\mu$KE), a novel memory\nupdate mechanism that preserves such dependencies via a Matryoshka-style\nobjective and adaptive loss coefficients. Empirical evaluations on two models\nacross four benchmarks demonstrate that $\\mu$KE improves edit efficacy by up to\n12.33% over state-of-the-art methods, and remain robust when applied to diverse\nformatted edits, underscoring its potential for effective unstructured\nknowledge editing in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01196v1",
    "published_date": "2025-04-01 21:24:44 UTC",
    "updated_date": "2025-04-01 21:24:44 UTC"
  },
  {
    "arxiv_id": "2504.01173v1",
    "title": "Neural Approaches to SAT Solving: Design Choices and Interpretability",
    "authors": [
      "David Moj≈æ√≠≈°ek",
      "Jan H≈Øla",
      "Ziwei Li",
      "Ziyu Zhou",
      "Mikol√°≈° Janota"
    ],
    "abstract": "In this contribution, we provide a comprehensive evaluation of graph neural\nnetworks applied to Boolean satisfiability problems, accompanied by an\nintuitive explanation of the mechanisms enabling the model to generalize to\ndifferent instances. We introduce several training improvements, particularly a\nnovel closest assignment supervision method that dynamically adapts to the\nmodel's current state, significantly enhancing performance on problems with\nlarger solution spaces. Our experiments demonstrate the suitability of\nvariable-clause graph representations with recurrent neural network updates,\nwhich achieve good accuracy on SAT assignment prediction while reducing\ncomputational demands. We extend the base graph neural network into a diffusion\nmodel that facilitates incremental sampling and can be effectively combined\nwith classical techniques like unit propagation. Through analysis of embedding\nspace patterns and optimization trajectories, we show how these networks\nimplicitly perform a process very similar to continuous relaxations of MaxSAT,\noffering an interpretable view of their reasoning process. This understanding\nguides our design choices and explains the ability of recurrent architectures\nto scale effectively at inference time beyond their training distribution,\nwhich we demonstrate with test-time scaling experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01173v1",
    "published_date": "2025-04-01 20:31:01 UTC",
    "updated_date": "2025-04-01 20:31:01 UTC"
  },
  {
    "arxiv_id": "2504.01154v1",
    "title": "Remember, but also, Forget: Bridging Myopic and Perfect Recall Fairness with Past-Discounting",
    "authors": [
      "Ashwin Kumar",
      "William Yeoh"
    ],
    "abstract": "Dynamic resource allocation in multi-agent settings often requires balancing\nefficiency with fairness over time--a challenge inadequately addressed by\nconventional, myopic fairness measures. Motivated by behavioral insights that\nhuman judgments of fairness evolve with temporal distance, we introduce a novel\nframework for temporal fairness that incorporates past-discounting mechanisms.\nBy applying a tunable discount factor to historical utilities, our approach\ninterpolates between instantaneous and perfect-recall fairness, thereby\ncapturing both immediate outcomes and long-term equity considerations. Beyond\naligning more closely with human perceptions of fairness, this past-discounting\nmethod ensures that the augmented state space remains bounded, significantly\nimproving computational tractability in sequential decision-making settings. We\ndetail the formulation of discounted-recall fairness in both additive and\naveraged utility contexts, illustrate its benefits through practical examples,\nand discuss its implications for designing balanced, scalable resource\nallocation strategies.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01154v1",
    "published_date": "2025-04-01 19:42:17 UTC",
    "updated_date": "2025-04-01 19:42:17 UTC"
  },
  {
    "arxiv_id": "2504.01153v3",
    "title": "Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations",
    "authors": [
      "Mahjabin Nahar",
      "Eun-Ju Lee",
      "Jin Won Park",
      "Dongwon Lee"
    ],
    "abstract": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01153v3",
    "published_date": "2025-04-01 19:36:14 UTC",
    "updated_date": "2025-05-06 17:40:04 UTC"
  },
  {
    "arxiv_id": "2504.01132v1",
    "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding",
    "authors": [
      "Melanie Subbiah",
      "Akankshya Mishra",
      "Grace Kim",
      "Liyan Tang",
      "Greg Durrett",
      "Kathleen McKeown"
    ],
    "abstract": "Determining faithfulness of a claim to a source document is an important\nproblem across many domains. This task is generally treated as a binary\njudgment of whether the claim is supported or unsupported in relation to the\nsource. In many cases, though, whether a claim is supported can be ambiguous.\nFor instance, it may depend on making inferences from given evidence, and\ndifferent people can reasonably interpret the claim as either supported or\nunsupported based on their agreement with those inferences. Forcing binary\nlabels upon such claims lowers the reliability of evaluation. In this work, we\nreframe the task to manage the subjectivity involved with factuality judgments\nof ambiguous claims. We introduce LLM-generated edits of summaries as a method\nof providing a nuanced evaluation of claims: how much does a summary need to be\nedited to be unambiguous? Whether a claim gets rewritten and how much it\nchanges can be used as an automatic evaluation metric, the Ambiguity Rewrite\nMetric (ARM), with a much richer feedback signal than a binary judgment of\nfaithfulness. We focus on the area of narrative summarization as it is\nparticularly rife with ambiguity and subjective interpretation. We show that\nARM produces a 21% absolute improvement in annotator agreement on claim\nfaithfulness, indicating that subjectivity is reduced.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.01132v1",
    "published_date": "2025-04-01 19:08:24 UTC",
    "updated_date": "2025-04-01 19:08:24 UTC"
  },
  {
    "arxiv_id": "2504.03748v1",
    "title": "TDBench: Benchmarking Vision-Language Models in Understanding Top-Down Images",
    "authors": [
      "Kaiyuan Hou",
      "Minghui Zhao",
      "Lilin Xu",
      "Yuang Fan",
      "Xiaofan Jiang"
    ],
    "abstract": "The rapid emergence of Vision-Language Models (VLMs) has significantly\nadvanced multimodal understanding, enabling applications in scene comprehension\nand visual reasoning. While these models have been primarily evaluated and\ndeveloped for front-view image understanding, their capabilities in\ninterpreting top-down images have received limited attention, partly due to the\nscarcity of diverse top-down datasets and the challenges in collecting such\ndata. In contrast, top-down vision provides explicit spatial overviews and\nimproved contextual understanding of scenes, making it particularly valuable\nfor tasks like autonomous navigation, aerial imaging, and spatial planning. In\nthis work, we address this gap by introducing TDBench, a comprehensive\nbenchmark for VLMs in top-down image understanding. TDBench is constructed from\npublic top-down view datasets and high-quality simulated images, including\ndiverse real-world and synthetic scenarios. TDBench consists of visual\nquestion-answer pairs across ten evaluation dimensions of image understanding.\nMoreover, we conduct four case studies that commonly happen in real-world\nscenarios but are less explored. By revealing the strengths and limitations of\nexisting VLM through evaluation results, we hope TDBench to provide insights\nfor motivating future research. Project homepage:\nhttps://github.com/Columbia-ICSL/TDBench",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03748v1",
    "published_date": "2025-04-01 19:01:13 UTC",
    "updated_date": "2025-04-01 19:01:13 UTC"
  },
  {
    "arxiv_id": "2504.01128v2",
    "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
    "authors": [
      "Andrei Dumitriu",
      "Florin Tatui",
      "Florin Miron",
      "Aakash Ralhan",
      "Radu Tudor Ionescu",
      "Radu Timofte"
    ],
    "abstract": "Rip currents are strong, localized and narrow currents of water that flow\noutwards into the sea, causing numerous beach-related injuries and fatalities\nworldwide. Accurate identification of rip currents remains challenging due to\ntheir amorphous nature and the lack of annotated data, which often requires\nexpert knowledge. To address these issues, we present RipVIS, a large-scale\nvideo instance segmentation benchmark explicitly designed for rip current\nsegmentation. RipVIS is an order of magnitude larger than previous datasets,\nfeaturing $184$ videos ($212,328$ frames), of which $150$ videos ($163,528$\nframes) are with rip currents, collected from various sources, including\ndrones, mobile phones, and fixed beach cameras. Our dataset encompasses diverse\nvisual contexts, such as wave-breaking patterns, sediment flows, and water\ncolor variations, across multiple global locations, including USA, Mexico,\nCosta Rica, Portugal, Italy, Greece, Romania, Sri Lanka, Australia and New\nZealand. Most videos are annotated at $5$ FPS to ensure accuracy in dynamic\nscenarios, supplemented by an additional $34$ videos ($48,800$ frames) without\nrip currents. We conduct comprehensive experiments with Mask R-CNN, Cascade\nMask R-CNN, SparseInst and YOLO11, fine-tuning these models for the task of rip\ncurrent segmentation. Results are reported in terms of multiple metrics, with a\nparticular focus on the $F_2$ score to prioritize recall and reduce false\nnegatives. To enhance segmentation performance, we introduce a novel\npost-processing step based on Temporal Confidence Aggregation (TCA). RipVIS\naims to set a new standard for rip current segmentation, contributing towards\nsafer beach environments. We offer a benchmark website to share data, models,\nand results with the research community, encouraging ongoing collaboration and\nfuture contributions, at https://ripvis.ai.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.0; I.4.9"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.01128v2",
    "published_date": "2025-04-01 18:57:15 UTC",
    "updated_date": "2025-04-03 09:29:08 UTC"
  },
  {
    "arxiv_id": "2504.01122v1",
    "title": "ffstruc2vec: Flat, Flexible and Scalable Learning of Node Representations from Structural Identities",
    "authors": [
      "Mario Heidrich",
      "Jeffrey Heidemann",
      "R√ºdiger Buchkremer",
      "Gonzalo Wandosell Fern√°ndez de Bobadilla"
    ],
    "abstract": "Node embedding refers to techniques that generate low-dimensional vector\nrepresentations of nodes in a graph while preserving specific properties of the\nnodes. A key challenge in the field is developing scalable methods that can\npreserve structural properties suitable for the required types of structural\npatterns of a given downstream application task. While most existing methods\nfocus on preserving node proximity, those that do preserve structural\nproperties often lack the flexibility to preserve various types of structural\npatterns required by downstream application tasks. This paper introduces\nffstruc2vec, a scalable deep-learning framework for learning node embedding\nvectors that preserve structural identities. Its flat, efficient architecture\nallows high flexibility in capturing diverse types of structural patterns,\nenabling broad adaptability to various downstream application tasks. The\nproposed framework significantly outperforms existing approaches across diverse\nunsupervised and supervised tasks in practical applications. Moreover,\nffstruc2vec enables explainability by quantifying how individual structural\npatterns influence task outcomes, providing actionable interpretation. To our\nknowledge, no existing framework combines this level of flexibility,\nscalability, and structural interpretability, underscoring its unique\ncapabilities.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01122v1",
    "published_date": "2025-04-01 18:47:16 UTC",
    "updated_date": "2025-04-01 18:47:16 UTC"
  },
  {
    "arxiv_id": "2504.01094v1",
    "title": "Multilingual and Multi-Accent Jailbreaking of Audio LLMs",
    "authors": [
      "Jaechul Roh",
      "Virat Shejwalkar",
      "Amir Houmansadr"
    ],
    "abstract": "Large Audio Language Models (LALMs) have significantly advanced audio\nunderstanding but introduce critical security risks, particularly through audio\njailbreaks. While prior work has focused on English-centric attacks, we expose\na far more severe vulnerability: adversarial multilingual and multi-accent\naudio jailbreaks, where linguistic and acoustic variations dramatically amplify\nattack success. In this paper, we introduce Multi-AudioJail, the first\nsystematic framework to exploit these vulnerabilities through (1) a novel\ndataset of adversarially perturbed multilingual/multi-accent audio jailbreaking\nprompts, and (2) a hierarchical evaluation pipeline revealing that how acoustic\nperturbations (e.g., reverberation, echo, and whisper effects) interacts with\ncross-lingual phonetics to cause jailbreak success rates (JSRs) to surge by up\nto +57.25 percentage points (e.g., reverberated Kenyan-accented attack on\nMERaLiON). Crucially, our work further reveals that multimodal LLMs are\ninherently more vulnerable than unimodal systems: attackers need only exploit\nthe weakest link (e.g., non-English audio inputs) to compromise the entire\nmodel, which we empirically show by multilingual audio-only attacks achieving\n3.1x higher success rates than text-only attacks. We plan to release our\ndataset to spur research into cross-modal defenses, urging the community to\naddress this expanding attack surface in multimodality as LALMs evolve.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "21 pages, 6 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01094v1",
    "published_date": "2025-04-01 18:12:23 UTC",
    "updated_date": "2025-04-01 18:12:23 UTC"
  },
  {
    "arxiv_id": "2504.01093v1",
    "title": "Hard-constraining Neumann boundary conditions in physics-informed neural networks via Fourier feature embeddings",
    "authors": [
      "Christopher Straub",
      "Philipp Brendel",
      "Vlad Medvedev",
      "Andreas Rosskopf"
    ],
    "abstract": "We present a novel approach to hard-constrain Neumann boundary conditions in\nphysics-informed neural networks (PINNs) using Fourier feature embeddings.\nNeumann boundary conditions are used to described critical processes in various\napplication, yet they are more challenging to hard-constrain in PINNs than\nDirichlet conditions. Our method employs specific Fourier feature embeddings to\ndirectly incorporate Neumann boundary conditions into the neural network's\narchitecture instead of learning them. The embedding can be naturally extended\nby high frequency modes to better capture high frequency phenomena. We\ndemonstrate the efficacy of our approach through experiments on a diffusion\nproblem, for which our method outperforms existing hard-constraining methods\nand classical PINNs, particularly in multiscale and high frequency scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.01093v1",
    "published_date": "2025-04-01 18:10:46 UTC",
    "updated_date": "2025-04-01 18:10:46 UTC"
  },
  {
    "arxiv_id": "2504.01089v1",
    "title": "HomeEmergency -- Using Audio to Find and Respond to Emergencies in the Home",
    "authors": [
      "James F. Mullen Jr",
      "Dhruva Kumar",
      "Xuewei Qi",
      "Rajasimman Madhivanan",
      "Arnie Sen",
      "Dinesh Manocha",
      "Richard Kim"
    ],
    "abstract": "In the United States alone accidental home deaths exceed 128,000 per year.\nOur work aims to enable home robots who respond to emergency scenarios in the\nhome, preventing injuries and deaths. We introduce a new dataset of household\nemergencies based in the ThreeDWorld simulator. Each scenario in our dataset\nbegins with an instantaneous or periodic sound which may or may not be an\nemergency. The agent must navigate the multi-room home scene using prior\nobservations, alongside audio signals and images from the simulator, to\ndetermine if there is an emergency or not.\n  In addition to our new dataset, we present a modular approach for localizing\nand identifying potential home emergencies. Underpinning our approach is a\nnovel probabilistic dynamic scene graph (P-DSG), where our key insight is that\ngraph nodes corresponding to agents can be represented with a probabilistic\nedge. This edge, when refined using Bayesian inference, enables efficient and\neffective localization of agents in the scene. We also utilize multi-modal\nvision-language models (VLMs) as a component in our approach, determining\nobject traits (e.g. flammability) and identifying emergencies. We present a\ndemonstration of our method completing a real-world version of our task on a\nconsumer robot, showing the transferability of both our task and our method.\nOur dataset will be released to the public upon this papers publication.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01089v1",
    "published_date": "2025-04-01 18:07:25 UTC",
    "updated_date": "2025-04-01 18:07:25 UTC"
  },
  {
    "arxiv_id": "2504.01016v1",
    "title": "GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors",
    "authors": [
      "Tian-Xing Xu",
      "Xiangjun Gao",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Song-Hai Zhang",
      "Ying Shan"
    ],
    "abstract": "Despite remarkable advancements in video depth estimation, existing methods\nexhibit inherent limitations in achieving geometric fidelity through the\naffine-invariant predictions, limiting their applicability in reconstruction\nand other metrically grounded downstream tasks. We propose GeometryCrafter, a\nnovel framework that recovers high-fidelity point map sequences with temporal\ncoherence from open-world videos, enabling accurate 3D/4D reconstruction,\ncamera parameter estimation, and other depth-based applications. At the core of\nour approach lies a point map Variational Autoencoder (VAE) that learns a\nlatent space agnostic to video latent distributions for effective point map\nencoding and decoding. Leveraging the VAE, we train a video diffusion model to\nmodel the distribution of point map sequences conditioned on the input videos.\nExtensive evaluations on diverse datasets demonstrate that GeometryCrafter\nachieves state-of-the-art 3D accuracy, temporal consistency, and generalization\ncapability.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Project webpage: https://geometrycrafter.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2504.01016v1",
    "published_date": "2025-04-01 17:58:03 UTC",
    "updated_date": "2025-04-01 17:58:03 UTC"
  },
  {
    "arxiv_id": "2504.01008v1",
    "title": "IntrinsiX: High-Quality PBR Generation using Image Priors",
    "authors": [
      "Peter Kocsis",
      "Lukas H√∂llein",
      "Matthias Nie√üner"
    ],
    "abstract": "We introduce IntrinsiX, a novel method that generates high-quality intrinsic\nimages from text description. In contrast to existing text-to-image models\nwhose outputs contain baked-in scene lighting, our approach predicts\nphysically-based rendering (PBR) maps. This enables the generated outputs to be\nused for content creation scenarios in core graphics applications that\nfacilitate re-lighting, editing, and texture generation tasks. In order to\ntrain our generator, we exploit strong image priors, and pre-train separate\nmodels for each PBR material component (albedo, roughness, metallic, normals).\nWe then align these models with a new cross-intrinsic attention formulation\nthat concatenates key and value features in a consistent fashion. This allows\nus to exchange information between each output modality and to obtain\nsemantically coherent PBR predictions. To ground each intrinsic component, we\npropose a rendering loss which provides image-space signals to constrain the\nmodel, thus facilitating sharp details also in the output BRDF properties. Our\nresults demonstrate detailed intrinsic generation with strong generalization\ncapabilities that outperforms existing intrinsic image decomposition methods\nused with generated images by a significant margin. Finally, we show a series\nof applications, including re-lighting, editing, and text-conditioned\nroom-scale PBR texture generation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.4.8; I.4.9; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://peter-kocsis.github.io/IntrinsiX/ Video:\n  https://youtu.be/b0wVA44R93Y",
    "pdf_url": "http://arxiv.org/pdf/2504.01008v1",
    "published_date": "2025-04-01 17:47:48 UTC",
    "updated_date": "2025-04-01 17:47:48 UTC"
  },
  {
    "arxiv_id": "2504.01005v1",
    "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
    "authors": [
      "Nishad Singhi",
      "Hritik Bansal",
      "Arian Hosseini",
      "Aditya Grover",
      "Kai-Wei Chang",
      "Marcus Rohrbach",
      "Anna Rohrbach"
    ],
    "abstract": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.01005v1",
    "published_date": "2025-04-01 17:41:57 UTC",
    "updated_date": "2025-04-01 17:41:57 UTC"
  },
  {
    "arxiv_id": "2504.01002v1",
    "title": "Token embeddings violate the manifold hypothesis",
    "authors": [
      "Michael Robinson",
      "Sourya Dey",
      "Tony Chiang"
    ],
    "abstract": "To fully understand the behavior of a large language model (LLM) requires our\nunderstanding of its input space. If this input space differs from our\nassumption, our understanding of and conclusions about the LLM is likely\nflawed, regardless of its architecture. Here, we elucidate the structure of the\ntoken embeddings, the input domain for LLMs, both empirically and\ntheoretically. We present a generalized and statistically testable model where\nthe neighborhood of each token splits into well-defined signal and noise\ndimensions.\n  This model is based on a generalization of a manifold called a fiber bundle,\nso we denote our hypothesis test as the ``fiber bundle null.'' Failing to\nreject the null is uninformative, but rejecting it at a specific token\nindicates that token has a statistically significant local structure, and so is\nof interest to us. By running our test over several open-source LLMs, each with\nunique token embeddings, we find that the null is frequently rejected, and so\nthe token subspace is provably not a fiber bundle and hence also not a\nmanifold. As a consequence of our findings, when an LLM is presented with two\nsemantically equivalent prompts, and if one prompt contains a token implicated\nby our test, that prompt will likely exhibit more output variability\nproportional to the local signal dimension of the token.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "53Z50, 62H15"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.01002v1",
    "published_date": "2025-04-01 17:40:12 UTC",
    "updated_date": "2025-04-01 17:40:12 UTC"
  },
  {
    "arxiv_id": "2504.03746v1",
    "title": "Enhancing Biologically Inspired Hierarchical Temporal Memory with Hardware-Accelerated Reflex Memory",
    "authors": [
      "Pavia Bera",
      "Sabrina Hassan Moon",
      "Jennifer Adorno",
      "Dayane Alfenas Reis",
      "Sanjukta Bhanja"
    ],
    "abstract": "The rapid expansion of the Internet of Things (IoT) generates zettabytes of\ndata that demand efficient unsupervised learning systems. Hierarchical Temporal\nMemory (HTM), a third-generation unsupervised AI algorithm, models the\nneocortex of the human brain by simulating columns of neurons to process and\npredict sequences. These neuron columns can memorize and infer sequences across\nmultiple orders. While multiorder inferences offer robust predictive\ncapabilities, they often come with significant computational overhead. The\nSequence Memory (SM) component of HTM, which manages these inferences,\nencounters bottlenecks primarily due to its extensive programmable\ninterconnects. In many cases, it has been observed that first-order temporal\nrelationships have proven to be sufficient without any significant loss in\nefficiency. This paper introduces a Reflex Memory (RM) block, inspired by the\nSpinal Cord's working mechanisms, designed to accelerate the processing of\nfirst-order inferences. The RM block performs these inferences significantly\nfaster than the SM. The integration of RM with HTM forms a system called the\nAccelerated Hierarchical Temporal Memory (AHTM), which processes repetitive\ninformation more efficiently than the original HTM while still supporting\nmultiorder inferences. The experimental results demonstrate that the HTM\npredicts an event in 0.945 s, whereas the AHTM module does so in 0.125 s.\nAdditionally, the hardware implementation of RM in a content-addressable memory\n(CAM) block, known as Hardware-Accelerated Hierarchical Temporal Memory\n(H-AHTM), predicts an event in just 0.094 s, significantly improving inference\nspeed. Compared to the original algorithm \\cite{bautista2020matlabhtm}, AHTM\naccelerates inference by up to 7.55x, while H-AHTM further enhances performance\nwith a 10.10x speedup.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03746v1",
    "published_date": "2025-04-01 17:40:12 UTC",
    "updated_date": "2025-04-01 17:40:12 UTC"
  },
  {
    "arxiv_id": "2504.01001v1",
    "title": "Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models",
    "authors": [
      "Jos√© Pombal",
      "Nuno M. Guerreiro",
      "Ricardo Rei",
      "Andr√© F. T. Martins"
    ],
    "abstract": "As language models improve and become capable of performing more complex\ntasks across modalities, evaluating them automatically becomes increasingly\nchallenging. Developing strong and robust task-specific automatic metrics gets\nharder, and human-annotated test sets -- which are expensive to create --\nsaturate more quickly. A compelling alternative is to design reliable\nstrategies to automate the creation of test data and evaluation, but previous\nattempts either rely on pre-existing data, or focus solely on individual tasks.\nWe present Zero-shot Benchmarking (ZSB), a framework for creating high-quality\nbenchmarks for any task by leveraging language models for both synthetic test\ndata creation and evaluation. ZSB is simple and flexible: it requires only the\ncreation of a prompt for data generation and one for evaluation; it is scalable\nto tasks and languages where collecting real-world data is costly or\nimpractical; it is model-agnostic, allowing the creation of increasingly\nchallenging benchmarks as models improve. To assess the effectiveness of our\nframework, we create benchmarks for five text-only tasks and a multi-modal one:\ngeneral capabilities in four languages (English, Chinese, French, and Korean),\ntranslation, and general vision-language capabilities in English. We then rank\na broad range of open and closed systems on our benchmarks. ZSB rankings\nconsistently correlate strongly with human rankings, outperforming\nwidely-adopted standard benchmarks. Through ablations, we find that strong\nbenchmarks can be created with open models, and that judge model size and\ndataset variety are crucial drivers of performance. We release all our\nbenchmarks, and code to reproduce our experiments and to produce new\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01001v1",
    "published_date": "2025-04-01 17:40:08 UTC",
    "updated_date": "2025-04-01 17:40:08 UTC"
  },
  {
    "arxiv_id": "2504.00999v1",
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization",
    "authors": [
      "Siyuan Li",
      "Luyuan Zhang",
      "Zedong Wang",
      "Juanxi Tian",
      "Cheng Tan",
      "Zicheng Liu",
      "Chang Yu",
      "Qingsong Xie",
      "Haonan Lu",
      "Haoqian Wang",
      "Zhen Lei"
    ],
    "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR2025 (in process for more analysis and extension)",
    "pdf_url": "http://arxiv.org/pdf/2504.00999v1",
    "published_date": "2025-04-01 17:39:19 UTC",
    "updated_date": "2025-04-01 17:39:19 UTC"
  },
  {
    "arxiv_id": "2504.00993v2",
    "title": "MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs",
    "authors": [
      "Juncheng Wu",
      "Wenlong Deng",
      "Xingxuan Li",
      "Sheng Liu",
      "Taomian Mi",
      "Yifan Peng",
      "Ziyang Xu",
      "Yi Liu",
      "Hyunjin Cho",
      "Chang-In Choi",
      "Yihan Cao",
      "Hui Ren",
      "Xiang Li",
      "Xiaoxiao Li",
      "Yuyin Zhou"
    ],
    "abstract": "Medical tasks such as diagnosis and treatment planning require precise and\ncomplex reasoning, particularly in life-critical domains. Unlike mathematical\nreasoning, medical reasoning demands meticulous, verifiable thought processes\nto ensure reliability and accuracy. However, there is a notable lack of\ndatasets that provide transparent, step-by-step reasoning to validate and\nenhance the medical reasoning ability of AI models. To bridge this gap, we\nintroduce MedReason, a large-scale high-quality medical reasoning dataset\ndesigned to enable faithful and explainable medical problem-solving in large\nlanguage models (LLMs). We utilize a structured medical knowledge graph (KG) to\nconvert clinical QA pairs into logical chains of reasoning, or ``thinking\npaths'', which trace connections from question elements to answers via relevant\nKG entities. Each path is validated for consistency with clinical logic and\nevidence-based medicine. Our pipeline generates detailed reasoning for various\nmedical questions from 7 medical datasets, resulting in a dataset of 32,682\nquestion-answer pairs, each with detailed, step-by-step explanations.\nExperiments demonstrate that fine-tuning with our dataset consistently boosts\nmedical problem-solving capabilities, achieving significant gains of up to 7.7%\nfor DeepSeek-Ditill-8B. Our top-performing model, MedReason-8B, outperforms the\nHuatuo-o1-8B, a state-of-the-art medical reasoning model, by up to 4.2% on the\nclinical benchmark MedBullets. We also engage medical professionals from\ndiverse specialties to assess our dataset's quality, ensuring MedReason offers\naccurate and coherent medical reasoning. Our data, models, and code is\navailable at https://github.com/UCSC-VLAA/MedReason.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 11 figures, 6 tables. Project page:\n  https://github.com/UCSC-VLAA/MedReason",
    "pdf_url": "http://arxiv.org/pdf/2504.00993v2",
    "published_date": "2025-04-01 17:31:44 UTC",
    "updated_date": "2025-04-04 18:29:18 UTC"
  },
  {
    "arxiv_id": "2504.00986v1",
    "title": "Accelerating drug discovery with Artificial: a whole-lab orchestration and scheduling system for self-driving labs",
    "authors": [
      "Yao Fehlis",
      "Paul Mandel",
      "Charles Crain",
      "Betty Liu",
      "David Fuller"
    ],
    "abstract": "Self-driving labs are transforming drug discovery by enabling automated,\nAI-guided experimentation, but they face challenges in orchestrating complex\nworkflows, integrating diverse instruments and AI models, and managing data\nefficiently. Artificial addresses these issues with a comprehensive\norchestration and scheduling system that unifies lab operations, automates\nworkflows, and integrates AI-driven decision-making. By incorporating AI/ML\nmodels like NVIDIA BioNeMo - which facilitates molecular interaction prediction\nand biomolecular analysis - Artificial enhances drug discovery and accelerates\ndata-driven research. Through real-time coordination of instruments, robots,\nand personnel, the platform streamlines experiments, enhances reproducibility,\nand advances drug discovery.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00986v1",
    "published_date": "2025-04-01 17:22:50 UTC",
    "updated_date": "2025-04-01 17:22:50 UTC"
  },
  {
    "arxiv_id": "2504.00983v1",
    "title": "WorldScore: A Unified Evaluation Benchmark for World Generation",
    "authors": [
      "Haoyi Duan",
      "Hong-Xing Yu",
      "Sirui Chen",
      "Li Fei-Fei",
      "Jiajun Wu"
    ],
    "abstract": "We introduce the WorldScore benchmark, the first unified benchmark for world\ngeneration. We decompose world generation into a sequence of next-scene\ngeneration tasks with explicit camera trajectory-based layout specifications,\nenabling unified evaluation of diverse approaches from 3D and 4D scene\ngeneration to video generation models. The WorldScore benchmark encompasses a\ncurated dataset of 3,000 test examples that span diverse worlds: static and\ndynamic, indoor and outdoor, photorealistic and stylized. The WorldScore\nmetrics evaluate generated worlds through three key aspects: controllability,\nquality, and dynamics. Through extensive evaluation of 19 representative\nmodels, including both open-source and closed-source ones, we reveal key\ninsights and challenges for each category of models. Our dataset, evaluation\ncode, and leaderboard can be found at https://haoyi-duan.github.io/WorldScore/",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Project website: https://haoyi-duan.github.io/WorldScore/ The first\n  two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2504.00983v1",
    "published_date": "2025-04-01 17:20:23 UTC",
    "updated_date": "2025-04-01 17:20:23 UTC"
  },
  {
    "arxiv_id": "2504.00975v3",
    "title": "Resource Allocation for RIS-Assisted CoMP-NOMA Networks using Reinforcement Learning",
    "authors": [
      "Muhammad Umer",
      "Muhammad Ahmed Mohsin",
      "Huma Ghafoor",
      "Syed Ali Hassan"
    ],
    "abstract": "This thesis delves into the forefront of wireless communication by exploring\nthe synergistic integration of three transformative technologies: STAR-RIS,\nCoMP, and NOMA. Driven by the ever-increasing demand for higher data rates,\nimproved spectral efficiency, and expanded coverage in the evolving landscape\nof 6G development, this research investigates the potential of these\ntechnologies to revolutionize future wireless networks.\n  The thesis analyzes the performance gains achievable through strategic\ndeployment of STAR-RIS, focusing on mitigating inter-cell interference,\nenhancing signal strength, and extending coverage to cell-edge users. Resource\nsharing strategies for STAR-RIS elements are explored, optimizing both\ntransmission and reflection functionalities. Analytical frameworks are\ndeveloped to quantify the benefits of STAR-RIS assisted CoMP-NOMA networks\nunder realistic channel conditions, deriving key performance metrics such as\nergodic rates and outage probabilities. Additionally, the research delves into\nenergy-efficient design approaches for CoMP-NOMA networks incorporating RIS,\nproposing novel RIS configurations and optimization algorithms to achieve a\nbalance between performance and energy consumption. Furthermore, the\napplication of Deep Reinforcement Learning (DRL) techniques for intelligent and\nadaptive optimization in aerial RIS-assisted CoMP-NOMA networks is explored,\naiming to maximize network sum rate while meeting user quality of service\nrequirements. Through a comprehensive investigation of these technologies and\ntheir synergistic potential, this thesis contributes valuable insights into the\nfuture of wireless communication, paving the way for the development of more\nefficient, reliable, and sustainable networks capable of meeting the demands of\nour increasingly connected world.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00975v3",
    "published_date": "2025-04-01 17:14:01 UTC",
    "updated_date": "2025-05-19 15:28:43 UTC"
  },
  {
    "arxiv_id": "2504.00970v1",
    "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV Caching",
    "authors": [
      "Yuxuan Zhu",
      "Ali Falahati",
      "David H. Yang",
      "Mohammad Mohammadi Amiri"
    ],
    "abstract": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00970v1",
    "published_date": "2025-04-01 17:08:57 UTC",
    "updated_date": "2025-04-01 17:08:57 UTC"
  },
  {
    "arxiv_id": "2504.00969v2",
    "title": "HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO",
    "authors": [
      "Giovanni Cioffi",
      "Leonard Bauersfeld",
      "Davide Scaramuzza"
    ],
    "abstract": "Visual-inertial odometry (VIO) is widely used for state estimation in\nautonomous micro aerial vehicles using onboard sensors. Current methods improve\nVIO by incorporating a model of the translational vehicle dynamics, yet their\nperformance degrades when faced with low-accuracy vehicle models or continuous\nexternal disturbances, like wind. Additionally, incorporating rotational\ndynamics in these models is computationally intractable when they are deployed\nin online applications, e.g., in a closed-loop control system. We present\nHDVIO2.0, which models full 6-DoF, translational and rotational, vehicle\ndynamics and tightly incorporates them into a VIO with minimal impact on the\nruntime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these\nchallenges through a hybrid dynamics model combining a point-mass vehicle model\nwith a learning-based component, with access to control commands and IMU\nhistory, to capture complex aerodynamic effects. The key idea behind modeling\nthe rotational dynamics is to represent them with continuous-time functions.\nHDVIO2.0 leverages the divergence between the actual motion and the predicted\nmotion from the hybrid dynamics model to estimate external forces as well as\nthe robot state. Our system surpasses the performance of state-of-the-art\nmethods in experiments using public and new drone dynamics datasets, as well as\nreal-world flights in winds up to 25 km/h. Unlike existing approaches, we also\nshow that accurate vehicle dynamics predictions are achievable without precise\nknowledge of the full vehicle state.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00969v2",
    "published_date": "2025-04-01 17:08:27 UTC",
    "updated_date": "2025-04-07 06:48:15 UTC"
  },
  {
    "arxiv_id": "2504.00957v2",
    "title": "Enabling Efficient Processing of Spiking Neural Networks with On-Chip Learning on Commodity Neuromorphic Processors for Edge AI Systems",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Pasindu Wickramasinghe",
      "Muhammad Shafique"
    ],
    "abstract": "The rising demand for energy-efficient edge AI systems (e.g., mobile\nagents/robots) has increased the interest in neuromorphic computing, since it\noffers ultra-low power/energy AI computation through spiking neural network\n(SNN) algorithms on neuromorphic processors. However, their efficient\nimplementation strategy has not been comprehensively studied, hence limiting\nSNN deployments for edge AI systems. Toward this, we propose a design\nmethodology to enable efficient SNN processing on commodity neuromorphic\nprocessors. To do this, we first study the key characteristics of targeted\nneuromorphic hardware (e.g., memory and compute budgets), and leverage this\ninformation to perform compatibility analysis for network selection. Afterward,\nwe employ a mapping strategy for efficient SNN implementation on the targeted\nprocessor. Furthermore, we incorporate an efficient on-chip learning mechanism\nto update the systems' knowledge for adapting to new input classes and dynamic\nenvironments. The experimental results show that the proposed methodology leads\nthe system to achieve low latency of inference (i.e., less than 50ms for image\nclassification, less than 200ms for real-time object detection in video\nstreaming, and less than 1ms in keyword recognition) and low latency of on-chip\nlearning (i.e., less than 2ms for keyword recognition), while incurring less\nthan 250mW of processing power and less than 15mJ of energy consumption across\nthe respective different applications and scenarios. These results show the\npotential of the proposed methodology in enabling efficient edge AI systems for\ndiverse application use-cases.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN) 2025 in Rome, Italy",
    "pdf_url": "http://arxiv.org/pdf/2504.00957v2",
    "published_date": "2025-04-01 16:52:03 UTC",
    "updated_date": "2025-04-19 05:17:37 UTC"
  },
  {
    "arxiv_id": "2504.00955v1",
    "title": "Unfair Learning: GenAI Exceptionalism and Copyright Law",
    "authors": [
      "David Atkinson"
    ],
    "abstract": "This paper challenges the argument that generative artificial intelligence\n(GenAI) is entitled to broad immunity from copyright law for reproducing\ncopyrighted works without authorization due to a fair use defense. It examines\nfair use legal arguments and eight distinct substantive arguments, contending\nthat every legal and substantive argument favoring fair use for GenAI applies\nequally, if not more so, to humans. Therefore, granting GenAI exceptional\nprivileges in this domain is legally and logically inconsistent with\nwithholding broad fair use exemptions from individual humans. It would mean no\nhuman would need to pay for virtually any copyright work again. The solution is\nto take a circumspect view of any fair use claim for mass copyright\nreproduction by any entity and focus on the first principles of whether\npermitting such exceptionalism for GenAI promotes science and the arts.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00955v1",
    "published_date": "2025-04-01 16:49:39 UTC",
    "updated_date": "2025-04-01 16:49:39 UTC"
  },
  {
    "arxiv_id": "2504.00954v1",
    "title": "IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval",
    "authors": [
      "Bangwei Liu",
      "Yicheng Bao",
      "Shaohui Lin",
      "Xuhong Wang",
      "Xin Tan",
      "Yingchun Wang",
      "Yuan Xie",
      "Chaochao Lu"
    ],
    "abstract": "Multimodal retrieval systems are becoming increasingly vital for cutting-edge\nAI technologies, such as embodied AI and AI-driven digital content industries.\nHowever, current multimodal retrieval tasks lack sufficient complexity and\ndemonstrate limited practical application value. It spires us to design\nInstance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires\nmodels to retrieve images containing the same instance as a query image while\nmatching a text-described scenario. Unlike existing retrieval tasks focused on\nglobal image similarity or category-level matching, IDMR demands fine-grained\ninstance-level consistency across diverse contexts. To benchmark this\ncapability, we develop IDMR-bench using real-world object tracking and\nfirst-person video data. Addressing the scarcity of training data, we propose a\ncross-domain synthesis method that creates 557K training samples by cropping\nobjects from standard detection datasets. Our Multimodal Large Language Model\n(MLLM) based retrieval model, trained on 1.2M samples, outperforms\nstate-of-the-art approaches on both traditional benchmarks and our zero-shot\nIDMR-bench. Experimental results demonstrate previous models' limitations in\ninstance-aware retrieval and highlight the potential of MLLM for advanced\nretrieval applications. The whole training dataset, codes and models, with wide\nranges of sizes, are available at https://github.com/BwLiu01/IDMR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00954v1",
    "published_date": "2025-04-01 16:47:20 UTC",
    "updated_date": "2025-04-01 16:47:20 UTC"
  },
  {
    "arxiv_id": "2504.00952v1",
    "title": "Personalized Federated Training of Diffusion Models with Privacy Guarantees",
    "authors": [
      "Kumar Kshitij Patel",
      "Weitong Zhang",
      "Lingxiao Wang"
    ],
    "abstract": "The scarcity of accessible, compliant, and ethically sourced data presents a\nconsiderable challenge to the adoption of artificial intelligence (AI) in\nsensitive fields like healthcare, finance, and biomedical research.\nFurthermore, access to unrestricted public datasets is increasingly constrained\ndue to rising concerns over privacy, copyright, and competition. Synthetic data\nhas emerged as a promising alternative, and diffusion models -- a cutting-edge\ngenerative AI technology -- provide an effective solution for generating\nhigh-quality and diverse synthetic data. In this paper, we introduce a novel\nfederated learning framework for training diffusion models on decentralized\nprivate datasets. Our framework leverages personalization and the inherent\nnoise in the forward diffusion process to produce high-quality samples while\nensuring robust differential privacy guarantees. Our experiments show that our\nframework outperforms non-collaborative training methods, particularly in\nsettings with high data heterogeneity, and effectively reduces biases and\nimbalances in synthetic data, resulting in fairer downstream models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00952v1",
    "published_date": "2025-04-01 16:45:26 UTC",
    "updated_date": "2025-04-01 16:45:26 UTC"
  },
  {
    "arxiv_id": "2504.00948v1",
    "title": "QSViT: A Methodology for Quantizing Spiking Vision Transformers",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Saad Iftikhar",
      "Muhammad Shafique"
    ],
    "abstract": "Vision Transformer (ViT)-based models have shown state-of-the-art performance\n(e.g., accuracy) in vision-based AI tasks. However, realizing their capability\nin resource-constrained embedded AI systems is challenging due to their\ninherent large memory footprints and complex computations, thereby incurring\nhigh power/energy consumption. Recently, Spiking Vision Transformer\n(SViT)-based models have emerged as alternate low-power ViT networks. However,\ntheir large memory footprints still hinder their applicability for\nresource-constrained embedded AI systems. Therefore, there is a need for a\nmethodology to compress SViT models without degrading the accuracy\nsignificantly. To address this, we propose QSViT, a novel design methodology to\ncompress the SViT models through a systematic quantization strategy across\ndifferent network layers. To do this, our QSViT employs several key steps: (1)\ninvestigating the impact of different precision levels in different network\nlayers, (2) identifying the appropriate base quantization settings for guiding\nbit precision reduction, (3) performing a guided quantization strategy based on\nthe base settings to select the appropriate quantization setting, and (4)\ndeveloping an efficient quantized network based on the selected quantization\nsetting. The experimental results demonstrate that, our QSViT methodology\nachieves 22.75% memory saving and 21.33% power saving, while also maintaining\nhigh accuracy within 2.1% from that of the original non-quantized SViT model on\nthe ImageNet dataset. These results highlight the potential of QSViT\nmethodology to pave the way toward the efficient SViT deployments on\nresource-constrained embedded AI systems.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at the International Joint Conference on Neural Networks\n  (IJCNN) 2025 in Rome, Italy",
    "pdf_url": "http://arxiv.org/pdf/2504.00948v1",
    "published_date": "2025-04-01 16:34:46 UTC",
    "updated_date": "2025-04-01 16:34:46 UTC"
  },
  {
    "arxiv_id": "2504.00943v1",
    "title": "Graph Classification and Radiomics Signature for Identification of Tuberculous Meningitis",
    "authors": [
      "Snigdha Agarwal",
      "Ganaraja V H",
      "Neelam Sinha",
      "Abhilasha Indoria",
      "Netravathi M",
      "Jitender Saini"
    ],
    "abstract": "Introduction: Tuberculous meningitis (TBM) is a serious brain infection\ncaused by Mycobacterium tuberculosis, characterized by inflammation of the\nmeninges covering the brain and spinal cord. Diagnosis often requires invasive\nlumbar puncture (LP) and cerebrospinal fluid (CSF) analysis. Objectives: This\nstudy aims to classify TBM patients using T1-weighted (T1w) non-contrast\nMagnetic Resonance Imaging (MRI) scans. We hypothesize that specific brain\nregions, such as the interpeduncular cisterns, bone, and corpus callosum,\ncontain visual markers that can non-invasively distinguish TBM patients from\nhealthy controls. We propose a novel Pixel-array Graphs Classifier\n(PAG-Classifier) that leverages spatial relationships between neighbouring 3D\npixels in a graph-based framework to extract significant features through eigen\ndecomposition. These features are then used to train machine learning\nclassifiers for effective patient classification. We validate our approach\nusing a radiomics-based methodology, classifying TBM patients based on relevant\nradiomics features. Results: We utilized an internal dataset consisting of 52\nscans, 32 from confirmed TBM patients based on mycobacteria detection in CSF,\nand 20 from healthy individuals. We achieved a 5-fold cross-validated average\nF1 score of 85.71% for cistern regions with our PAG-Classifier and 92.85% with\nthe radiomics features classifier, surpassing current state-of-the-art\nbenchmarks by 15% and 22%, respectively. However, bone and corpus callosum\nregions showed poor classification effectiveness, with average F1 scores below\n50%. Conclusion: Our study suggests that algorithms like the PAG-Classifier\nserve as effective tools for non-invasive TBM analysis, particularly by\ntargeting the interpeduncular cistern. Findings indicate that the bone and\ncorpus callosum regions lack distinctive patterns for differentiation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 6 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.00943v1",
    "published_date": "2025-04-01 16:28:39 UTC",
    "updated_date": "2025-04-01 16:28:39 UTC"
  },
  {
    "arxiv_id": "2504.00938v1",
    "title": "AI Judges in Design: Statistical Perspectives on Achieving Human Expert Equivalence With Vision-Language Models",
    "authors": [
      "Kristen M. Edwards",
      "Farnaz Tehranchi",
      "Scarlett R. Miller",
      "Faez Ahmed"
    ],
    "abstract": "The subjective evaluation of early stage engineering designs, such as\nconceptual sketches, traditionally relies on human experts. However, expert\nevaluations are time-consuming, expensive, and sometimes inconsistent. Recent\nadvances in vision-language models (VLMs) offer the potential to automate\ndesign assessments, but it is crucial to ensure that these AI ``judges''\nperform on par with human experts. However, no existing framework assesses\nexpert equivalence. This paper introduces a rigorous statistical framework to\ndetermine whether an AI judge's ratings match those of human experts. We apply\nthis framework in a case study evaluating four VLM-based judges on key design\nmetrics (uniqueness, creativity, usefulness, and drawing quality). These AI\njudges employ various in-context learning (ICL) techniques, including uni- vs.\nmultimodal prompts and inference-time reasoning. The same statistical framework\nis used to assess three trained novices for expert-equivalence. Results show\nthat the top-performing AI judge, using text- and image-based ICL with\nreasoning, achieves expert-level agreement for uniqueness and drawing quality\nand outperforms or matches trained novices across all metrics. In 6/6 runs for\nboth uniqueness and creativity, and 5/6 runs for both drawing quality and\nusefulness, its agreement with experts meets or exceeds that of the majority of\ntrained novices. These findings suggest that reasoning-supported VLM models can\nachieve human-expert equivalence in design evaluation. This has implications\nfor scaling design evaluation in education and practice, and provides a general\nstatistical framework for validating AI judges in other domains requiring\nsubjective content evaluation.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 8 tables, 6 figures, 8 tables in the appendix",
    "pdf_url": "http://arxiv.org/pdf/2504.00938v1",
    "published_date": "2025-04-01 16:20:29 UTC",
    "updated_date": "2025-04-01 16:20:29 UTC"
  },
  {
    "arxiv_id": "2504.02872v1",
    "title": "Scraping the Shadows: Deep Learning Breakthroughs in Dark Web Intelligence",
    "authors": [
      "Ingmar Bakermans",
      "Daniel De Pascale",
      "Gon√ßalo Marcelino",
      "Giuseppe Cascavilla",
      "Zeno Geradts"
    ],
    "abstract": "Darknet markets (DNMs) facilitate the trade of illegal goods on a global\nscale. Gathering data on DNMs is critical to ensuring law enforcement agencies\ncan effectively combat crime. Manually extracting data from DNMs is an\nerror-prone and time-consuming task. Aiming to automate this process we develop\na framework for extracting data from DNMs and evaluate the application of three\nstate-of-the-art Named Entity Recognition (NER) models, ELMo-BiLSTM\n\\citep{ShahEtAl2022}, UniversalNER \\citep{ZhouEtAl2024}, and GLiNER\n\\citep{ZaratianaEtAl2023}, at the task of extracting complex entities from DNM\nproduct listing pages. We propose a new annotated dataset, which we use to\ntrain, fine-tune, and evaluate the models. Our findings show that\nstate-of-the-art NER models perform well in information extraction from DNMs,\nachieving 91% Precision, 96% Recall, and an F1 score of 94%. In addition,\nfine-tuning enhances model performance, with UniversalNER achieving the best\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 17 images",
    "pdf_url": "http://arxiv.org/pdf/2504.02872v1",
    "published_date": "2025-04-01 16:12:19 UTC",
    "updated_date": "2025-04-01 16:12:19 UTC"
  },
  {
    "arxiv_id": "2504.02871v1",
    "title": "Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction",
    "authors": [
      "Enshuo Hsu",
      "Martin Ugbala",
      "Krishna Kumar Kookal",
      "Zouaidi Kawtar",
      "Nicholas L. Rider",
      "Muhammad F. Walji",
      "Kirk Roberts"
    ],
    "abstract": "Generative information extraction using large language models, particularly\nthrough few-shot learning, has become a popular method. Recent studies indicate\nthat providing a detailed, human-readable guideline-similar to the annotation\nguidelines traditionally used for training human annotators can significantly\nimprove performance. However, constructing these guidelines is both labor- and\nknowledge-intensive. Additionally, the definitions are often tailored to meet\nspecific needs, making them highly task-specific and often non-reusable.\nHandling these subtle differences requires considerable effort and attention to\ndetail. In this study, we propose a self-improving method that harvests the\nknowledge summarization and text generation capacity of LLMs to synthesize\nannotation guidelines while requiring virtually no human input. Our zero-shot\nexperiments on the clinical named entity recognition benchmarks, 2012 i2b2\nEVENT, 2012 i2b2 TIMEX, 2014 i2b2, and 2018 n2c2 showed 25.86%, 4.36%, 0.20%,\nand 7.75% improvements in strict F1 scores from the no-guideline baseline. The\nLLM-synthesized guidelines showed equivalent or better performance compared to\nhuman-written guidelines by 1.15% to 4.14% in most tasks. In conclusion, this\nstudy proposes a novel LLM self-improving method that requires minimal\nknowledge and human input and is applicable to multiple biomedical domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.02871v1",
    "published_date": "2025-04-01 15:59:04 UTC",
    "updated_date": "2025-04-01 15:59:04 UTC"
  },
  {
    "arxiv_id": "2504.00907v2",
    "title": "Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning",
    "authors": [
      "Ram Ramrakhya",
      "Matthew Chang",
      "Xavier Puig",
      "Ruta Desai",
      "Zsolt Kira",
      "Roozbeh Mottaghi"
    ],
    "abstract": "Embodied agents operating in real-world environments must interpret ambiguous\nand under-specified human instructions. A capable household robot should\nrecognize ambiguity and ask relevant clarification questions to infer the user\nintent accurately, leading to more effective task execution. To study this\nproblem, we introduce the Ask-to-Act task, where an embodied agent must fetch a\nspecific object instance given an ambiguous instruction in a home environment.\nThe agent must strategically ask minimal, yet relevant, clarification questions\nto resolve ambiguity while navigating under partial observability. To solve\nthis problem, we propose a novel approach that fine-tunes multimodal large\nlanguage models (MLLMs) as vision-language-action (VLA) policies using online\nreinforcement learning (RL) with LLM-generated rewards. Our method eliminates\nthe need for large-scale human demonstrations or manually engineered rewards\nfor training such agents. We benchmark against strong zero-shot baselines,\nincluding GPT-4o, and supervised fine-tuned MLLMs, on our task. Our results\ndemonstrate that our RL-finetuned MLLM outperforms all baselines by a\nsignificant margin ($19.1$-$40.3\\%$), generalizing well to novel scenes and\ntasks. To the best of our knowledge, this is the first demonstration of\nadapting MLLMs as VLA agents that can act and ask for help using LLM-generated\nrewards with online RL.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00907v2",
    "published_date": "2025-04-01 15:41:50 UTC",
    "updated_date": "2025-04-02 01:49:21 UTC"
  },
  {
    "arxiv_id": "2504.00906v1",
    "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
    "authors": [
      "Saaket Agashe",
      "Kyle Wong",
      "Vincent Tu",
      "Jiachen Yang",
      "Ang Li",
      "Xin Eric Wang"
    ],
    "abstract": "Computer use agents automate digital tasks by directly interacting with\ngraphical user interfaces (GUIs) on computers and mobile devices, offering\nsignificant potential to enhance human productivity by completing an open-ended\nspace of user queries. However, current agents face significant challenges:\nimprecise grounding of GUI elements, difficulties with long-horizon task\nplanning, and performance bottlenecks from relying on single generalist models\nfor diverse cognitive tasks. To this end, we introduce Agent S2, a novel\ncompositional framework that delegates cognitive responsibilities across\nvarious generalist and specialist models. We propose a novel\nMixture-of-Grounding technique to achieve precise GUI localization and\nintroduce Proactive Hierarchical Planning, dynamically refining action plans at\nmultiple temporal scales in response to evolving observations. Evaluations\ndemonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance\non three prominent computer use benchmarks. Specifically, Agent S2 achieves\n18.9% and 32.7% relative improvements over leading baseline agents such as\nClaude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation.\nMoreover, Agent S2 generalizes effectively to other operating systems and\napplications, surpassing previous best methods by 52.8% on WindowsAgentArena\nand by 16.52% on AndroidWorld relatively. Code available at\nhttps://github.com/simular-ai/Agent-S.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 13 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.00906v1",
    "published_date": "2025-04-01 15:40:27 UTC",
    "updated_date": "2025-04-01 15:40:27 UTC"
  },
  {
    "arxiv_id": "2504.00899v1",
    "title": "Role and Use of Race in AI/ML Models Related to Health",
    "authors": [
      "Martin C. Were",
      "Ang Li",
      "Bradley A. Malin",
      "Zhijun Yin",
      "Joseph R. Coco",
      "Benjamin X. Collins",
      "Ellen Wright Clayton",
      "Laurie L. Novak",
      "Rachele Hendricks-Sturrup",
      "Abiodun Oluyomi",
      "Shilo Anders",
      "Chao Yan"
    ],
    "abstract": "The role and use of race within health-related artificial intelligence and\nmachine learning (AI/ML) models has sparked increasing attention and\ncontroversy. Despite the complexity and breadth of related issues, a robust and\nholistic framework to guide stakeholders in their examination and resolution\nremains lacking. This perspective provides a broad-based, systematic, and\ncross-cutting landscape analysis of race-related challenges, structured around\nthe AI/ML lifecycle and framed through \"points to consider\" to support inquiry\nand decision-making.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00899v1",
    "published_date": "2025-04-01 15:27:31 UTC",
    "updated_date": "2025-04-01 15:27:31 UTC"
  },
  {
    "arxiv_id": "2504.03744v1",
    "title": "Comparative Explanations: Explanation Guided Decision Making for Human-in-the-Loop Preference Selection",
    "authors": [
      "Tanmay Chakraborty",
      "Christian Wirth",
      "Christin Seifert"
    ],
    "abstract": "This paper introduces Multi-Output LOcal Narrative Explanation (MOLONE), a\nnovel comparative explanation method designed to enhance preference selection\nin human-in-the-loop Preference Bayesian optimization (PBO). The preference\nelicitation in PBO is a non-trivial task because it involves navigating\nimplicit trade-offs between vector-valued outcomes, subjective priorities of\ndecision-makers, and decision-makers' uncertainty in preference selection.\nExisting explainable AI (XAI) methods for BO primarily focus on input feature\nimportance, neglecting the crucial role of outputs (objectives) in human\npreference elicitation. MOLONE addresses this gap by providing explanations\nthat highlight both input and output importance, enabling decision-makers to\nunderstand the trade-offs between competing objectives and make more informed\npreference selections. MOLONE focuses on local explanations, comparing the\nimportance of input features and outcomes across candidate samples within a\nlocal neighborhood of the search space, thus capturing nuanced differences\nrelevant to preference-based decision-making. We evaluate MOLONE within a PBO\nframework using benchmark multi-objective optimization functions, demonstrating\nits effectiveness in improving convergence compared to noisy preference\nselections. Furthermore, a user study confirms that MOLONE significantly\naccelerates convergence in human-in-the-loop scenarios by facilitating more\nefficient identification of preferred options.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03744v1",
    "published_date": "2025-04-01 15:23:54 UTC",
    "updated_date": "2025-04-01 15:23:54 UTC"
  },
  {
    "arxiv_id": "2504.03743v1",
    "title": "Modelling bounded rational decision-making through Wasserstein constraints",
    "authors": [
      "Benjamin Patrick Evans",
      "Leo Ardon",
      "Sumitra Ganesh"
    ],
    "abstract": "Modelling bounded rational decision-making through information constrained\nprocessing provides a principled approach for representing departures from\nrationality within a reinforcement learning framework, while still treating\ndecision-making as an optimization process. However, existing approaches are\ngenerally based on Entropy, Kullback-Leibler divergence, or Mutual Information.\nIn this work, we highlight issues with these approaches when dealing with\nordinal action spaces. Specifically, entropy assumes uniform prior beliefs,\nmissing the impact of a priori biases on decision-makings. KL-Divergence\naddresses this, however, has no notion of \"nearness\" of actions, and\nadditionally, has several well known potentially undesirable properties such as\nthe lack of symmetry, and furthermore, requires the distributions to have the\nsame support (e.g. positive probability for all actions). Mutual information is\noften difficult to estimate. Here, we propose an alternative approach for\nmodeling bounded rational RL agents utilising Wasserstein distances. This\napproach overcomes the aforementioned issues. Crucially, this approach accounts\nfor the nearness of ordinal actions, modeling \"stickiness\" in agent decisions\nand unlikeliness of rapidly switching to far away actions, while also\nsupporting low probability actions, zero-support prior distributions, and is\nsimple to calculate directly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at RLDM 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.03743v1",
    "published_date": "2025-04-01 15:19:34 UTC",
    "updated_date": "2025-04-01 15:19:34 UTC"
  },
  {
    "arxiv_id": "2504.00885v1",
    "title": "Spectral Architecture Search for Neural Networks",
    "authors": [
      "Gianluca Peri",
      "Lorenzo Giambagli",
      "Lorenzo Chicchi",
      "Duccio Fanelli"
    ],
    "abstract": "Architecture design and optimization are challenging problems in the field of\nartificial neural networks. Working in this context, we here present SPARCS\n(SPectral ARchiteCture Search), a novel architecture search protocol which\nexploits the spectral attributes of the inter-layer transfer matrices. SPARCS\nallows one to explore the space of possible architectures by spanning\ncontinuous and differentiable manifolds, thus enabling for gradient-based\noptimization algorithms to be eventually employed. With reference to simple\nbenchmark models, we show that the newly proposed method yields a self-emerging\narchitecture with a minimal degree of expressivity to handle the task under\ninvestigation and with a reduced parameter count as compared to other viable\nalternatives.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00885v1",
    "published_date": "2025-04-01 15:14:30 UTC",
    "updated_date": "2025-04-01 15:14:30 UTC"
  },
  {
    "arxiv_id": "2504.00883v2",
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "authors": [
      "Zhenyi Liao",
      "Qingsong Xie",
      "Yanhao Zhang",
      "Zijian Kong",
      "Haonan Lu",
      "Zhenyu Yang",
      "Zhijie Deng"
    ],
    "abstract": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00883v2",
    "published_date": "2025-04-01 15:11:11 UTC",
    "updated_date": "2025-04-14 20:12:57 UTC"
  },
  {
    "arxiv_id": "2504.00882v1",
    "title": "CrackSQL: A Hybrid SQL Dialect Translation System Powered by Large Language Models",
    "authors": [
      "Wei Zhou",
      "Yuyang Gao",
      "Xuanhe Zhou",
      "Guoliang Li"
    ],
    "abstract": "Dialect translation plays a key role in enabling seamless interaction across\nheterogeneous database systems. However, translating SQL queries between\ndifferent dialects (e.g., from PostgreSQL to MySQL) remains a challenging task\ndue to syntactic discrepancies and subtle semantic variations. Existing\napproaches including manual rewriting, rule-based systems, and large language\nmodel (LLM)-based techniques often involve high maintenance effort (e.g.,\ncrafting custom translation rules) or produce unreliable results (e.g., LLM\ngenerates non-existent functions), especially when handling complex queries. In\nthis demonstration, we present CrackSQL, the first hybrid SQL dialect\ntranslation system that combines rule and LLM-based methods to overcome these\nlimitations. CrackSQL leverages the adaptability of LLMs to minimize manual\nintervention, while enhancing translation accuracy by segmenting lengthy\ncomplex SQL via functionality-based query processing. To further improve\nrobustness, it incorporates a novel cross-dialect syntax embedding model for\nprecise syntax alignment, as well as an adaptive local-to-global translation\nstrategy that effectively resolves interdependent query operations. CrackSQL\nsupports three translation modes and offers multiple deployment and access\noptions including a web console interface, a PyPI package, and a command-line\nprompt, facilitating adoption across a variety of real-world use cases",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "Extension of our SIGMOD 2025 paper. Please refer to source code\n  available at: https://github.com/weAIDB/CrackSQL",
    "pdf_url": "http://arxiv.org/pdf/2504.00882v1",
    "published_date": "2025-04-01 15:11:03 UTC",
    "updated_date": "2025-04-01 15:11:03 UTC"
  },
  {
    "arxiv_id": "2504.00869v1",
    "title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models",
    "authors": [
      "Xiaoke Huang",
      "Juncheng Wu",
      "Hui Liu",
      "Xianfeng Tang",
      "Yuyin Zhou"
    ],
    "abstract": "Test-time scaling has emerged as a powerful technique for enhancing the\nreasoning capabilities of large language models. However, its effectiveness in\nmedical reasoning remains uncertain, as the medical domain fundamentally\ndiffers from mathematical tasks in terms of knowledge representation and\ndecision-making processes. In this paper, we provide the first comprehensive\ninvestigation of test-time scaling for medical reasoning and present m1, a\nsimple yet effective approach that increases a model's medical reasoning\ncapability at inference. Our evaluation across diverse medical tasks\ndemonstrates that test-time scaling consistently enhances medical reasoning,\nenabling lightweight fine-tuned models under 10B parameters to establish new\nstate-of-the-art performance, while our 32B model rivals previous 70B-scale\nmedical LLMs. However, we identify an optimal reasoning token budget of\napproximately 4K, beyond which performance may degrade due to overthinking.\nBudget forcing, which extends test-time computation through iterative prompts,\nhelps models double-check answers but does not necessarily improve the overall\nmedical QA performance and, in some cases, even introduces errors into\npreviously correct responses. Our case-by-case analysis identifies insufficient\nmedical knowledge as a key bottleneck that prevents further performance gains\nthrough test-time scaling. We find that increasing data scale, improving data\nquality, and expanding model capacity consistently enhance medical knowledge\ngrounding, enabling continued performance improvements, particularly on\nchallenging medical benchmarks where smaller models reach saturation. These\nfindings underscore fundamental differences between medical and mathematical\nreasoning in LLMs, highlighting that enriched medical knowledge, other than\nincreased reasoning depth alone, is essential for realizing the benefits of\ntest-time scaling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages; 7 figures; Data, code, and models:\n  https://github.com/UCSC-VLAA/m1",
    "pdf_url": "http://arxiv.org/pdf/2504.00869v1",
    "published_date": "2025-04-01 14:57:43 UTC",
    "updated_date": "2025-04-01 14:57:43 UTC"
  },
  {
    "arxiv_id": "2504.03742v1",
    "title": "Hierarchical Local-Global Feature Learning for Few-shot Malicious Traffic Detection",
    "authors": [
      "Songtao Peng",
      "Lei Wang",
      "Wu Shuai",
      "Hao Song",
      "Jiajun Zhou",
      "Shanqing Yu",
      "Qi Xuan"
    ],
    "abstract": "With the rapid growth of internet traffic, malicious network attacks have\nbecome increasingly frequent and sophisticated, posing significant threats to\nglobal cybersecurity. Traditional detection methods, including rule-based and\nmachine learning-based approaches, struggle to accurately identify emerging\nthreats, particularly in scenarios with limited samples. While recent advances\nin few-shot learning have partially addressed the data scarcity issue, existing\nmethods still exhibit high false positive rates and lack the capability to\neffectively capture crucial local traffic patterns. In this paper, we propose\nHLoG, a novel hierarchical few-shot malicious traffic detection framework that\nleverages both local and global features extracted from network sessions. HLoG\nemploys a sliding-window approach to segment sessions into phases, capturing\nfine-grained local interaction patterns through hierarchical bidirectional GRU\nencoding, while simultaneously modeling global contextual dependencies. We\nfurther design a session similarity assessment module that integrates local\nsimilarity with global self-attention-enhanced representations, achieving\naccurate and robust few-shot traffic classification. Comprehensive experiments\non three meticulously reconstructed datasets demonstrate that HLoG\nsignificantly outperforms existing state-of-the-art methods. Particularly, HLoG\nachieves superior recall rates while substantially reducing false positives,\nhighlighting its effectiveness and practical value in real-world cybersecurity\napplications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03742v1",
    "published_date": "2025-04-01 14:56:44 UTC",
    "updated_date": "2025-04-01 14:56:44 UTC"
  },
  {
    "arxiv_id": "2504.00860v1",
    "title": "Investigating the Capabilities and Limitations of Machine Learning for Identifying Bias in English Language Data with Information and Heritage Professionals",
    "authors": [
      "Lucy Havens",
      "Benjamin Bach",
      "Melissa Terras",
      "Beatrice Alex"
    ],
    "abstract": "Despite numerous efforts to mitigate their biases, ML systems continue to\nharm already-marginalized people. While predominant ML approaches assume bias\ncan be removed and fair models can be created, we show that these are not\nalways possible, nor desirable, goals. We reframe the problem of ML bias by\ncreating models to identify biased language, drawing attention to a dataset's\nbiases rather than trying to remove them. Then, through a workshop, we\nevaluated the models for a specific use case: workflows of information and\nheritage professionals. Our findings demonstrate the limitations of ML for\nidentifying bias due to its contextual nature, the way in which approaches to\nmitigating it can simultaneously privilege and oppress different communities,\nand its inevitability. We demonstrate the need to expand ML approaches to bias\nand fairness, providing a mixed-methods approach to investigating the\nfeasibility of removing bias or achieving fairness in a given ML use case.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "I.2.7; J.0; K.4.0"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to the 2025 CHI Conference on Human Factors in Computing\n  Systems (CHI '25)",
    "pdf_url": "http://arxiv.org/pdf/2504.00860v1",
    "published_date": "2025-04-01 14:51:25 UTC",
    "updated_date": "2025-04-01 14:51:25 UTC"
  },
  {
    "arxiv_id": "2504.00857v1",
    "title": "Exploring Personalized Federated Learning Architectures for Violence Detection in Surveillance Videos",
    "authors": [
      "Mohammad Kassir",
      "Siba Haidar",
      "Antoun Yaacoub"
    ],
    "abstract": "The challenge of detecting violent incidents in urban surveillance systems is\ncompounded by the voluminous and diverse nature of video data. This paper\npresents a targeted approach using Personalized Federated Learning (PFL) to\naddress these issues, specifically employing the Federated Learning with\nPersonalization Layers method within the Flower framework. Our methodology\nadapts learning models to the unique data characteristics of each surveillance\nnode, effectively managing the heterogeneous and non-IID nature of surveillance\nvideo data. Through rigorous experiments conducted on balanced and imbalanced\ndatasets, our PFL models demonstrated enhanced accuracy and efficiency,\nachieving up to 99.3% accuracy. This study underscores the potential of PFL to\nsignificantly improve the scalability and effectiveness of surveillance\nsystems, offering a robust, privacy-preserving solution for violence detection\nin complex urban environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.00857v1",
    "published_date": "2025-04-01 14:47:14 UTC",
    "updated_date": "2025-04-01 14:47:14 UTC"
  },
  {
    "arxiv_id": "2504.00852v1",
    "title": "ReaLitE: Enrichment of Relation Embeddings in Knowledge Graphs using Numeric Literals",
    "authors": [
      "Antonis Klironomos",
      "Baifan Zhou",
      "Zhuoxun Zheng",
      "Gad-Elrab Mohamed",
      "Heiko Paulheim",
      "Evgeny Kharlamov"
    ],
    "abstract": "Most knowledge graph embedding (KGE) methods tailored for link prediction\nfocus on the entities and relations in the graph, giving little attention to\nother literal values, which might encode important information. Therefore, some\nliteral-aware KGE models attempt to either integrate numerical values into the\nembeddings of the entities or convert these numerics into entities during\npreprocessing, leading to information loss. Other methods concerned with\ncreating relation-specific numerical features assume completeness of numerical\ndata, which does not apply to real-world graphs. In this work, we propose\nReaLitE, a novel relation-centric KGE model that dynamically aggregates and\nmerges entities' numerical attributes with the embeddings of the connecting\nrelations. ReaLitE is designed to complement existing conventional KGE methods\nwhile supporting multiple variations for numerical aggregations, including a\nlearnable method.\n  We comprehensively evaluated the proposed relation-centric embedding using\nseveral benchmarks for link prediction and node classification tasks. The\nresults showed the superiority of ReaLitE over the state of the art in both\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ESWC 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.00852v1",
    "published_date": "2025-04-01 14:38:22 UTC",
    "updated_date": "2025-04-01 14:38:22 UTC"
  },
  {
    "arxiv_id": "2504.00850v1",
    "title": "Global Intervention and Distillation for Federated Out-of-Distribution Generalization",
    "authors": [
      "Zhuang Qi",
      "Runhui Zhang",
      "Lei Meng",
      "Wei Wu",
      "Yachong Zhang",
      "Xiangxu Meng"
    ],
    "abstract": "Attribute skew in federated learning leads local models to focus on learning\nnon-causal associations, guiding them towards inconsistent optimization\ndirections, which inevitably results in performance degradation and unstable\nconvergence. Existing methods typically leverage data augmentation to enhance\nsample diversity or employ knowledge distillation to learn invariant\nrepresentations. However, the instability in the quality of generated data and\nthe lack of domain information limit their performance on unseen samples. To\naddress these issues, this paper presents a global intervention and\ndistillation method, termed FedGID, which utilizes diverse attribute features\nfor backdoor adjustment to break the spurious association between background\nand label. It includes two main modules, where the global intervention module\nadaptively decouples objects and backgrounds in images, injects background\ninformation into random samples to intervene in the sample distribution, which\nlinks backgrounds to all categories to prevent the model from treating\nbackground-label associations as causal. The global distillation module\nleverages a unified knowledge base to guide the representation learning of\nclient models, preventing local models from overfitting to client-specific\nattributes. Experimental results on three datasets demonstrate that FedGID\nenhances the model's ability to focus on the main subjects in unseen data and\noutperforms existing methods in collaborative modeling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00850v1",
    "published_date": "2025-04-01 14:36:24 UTC",
    "updated_date": "2025-04-01 14:36:24 UTC"
  },
  {
    "arxiv_id": "2504.00843v1",
    "title": "Investigating Large Language Models in Diagnosing Students' Cognitive Skills in Math Problem-solving",
    "authors": [
      "Hyoungwook Jin",
      "Yoonsu Kim",
      "Dongyun Jung",
      "Seungju Kim",
      "Kiyoon Choi",
      "Jinho Son",
      "Juho Kim"
    ],
    "abstract": "Mathematics learning entails mastery of both content knowledge and cognitive\nprocessing of knowing, applying, and reasoning with it. Automated math\nassessment primarily has focused on grading students' exhibition of content\nknowledge by finding textual evidence, such as specific numbers, formulas, and\nstatements. Recent advancements in problem-solving, image recognition, and\nreasoning capabilities of large language models (LLMs) show promise for nuanced\nevaluation of students' cognitive skills. Diagnosing cognitive skills needs to\ninfer students' thinking processes beyond textual evidence, which is an\nunderexplored task in LLM-based automated assessment. In this work, we\ninvestigate how state-of-the-art LLMs diagnose students' cognitive skills in\nmathematics. We constructed MathCog, a novel benchmark dataset comprising 639\nstudent responses to 110 expert-curated middle school math problems, each\nannotated with detailed teachers' diagnoses based on cognitive skill\nchecklists. Using MathCog, we evaluated 16 closed and open LLMs of varying\nmodel sizes and vendors. Our evaluation reveals that even the state-of-the-art\nLLMs struggle with the task, all F1 scores below 0.5, and tend to exhibit\nstrong false confidence for incorrect cases ($r_s=.617$). We also found that\nmodel size positively correlates with the diagnosis performance ($r_s=.771$).\nFinally, we discuss the implications of these findings, the overconfidence\nissue, and directions for improving automated cognitive skill diagnosis.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00843v1",
    "published_date": "2025-04-01 14:29:41 UTC",
    "updated_date": "2025-04-01 14:29:41 UTC"
  },
  {
    "arxiv_id": "2504.00839v1",
    "title": "Context-Aware Human Behavior Prediction Using Multimodal Large Language Models: Challenges and Insights",
    "authors": [
      "Yuchen Liu",
      "Lino Lerch",
      "Luigi Palmieri",
      "Andrey Rudenko",
      "Sebastian Koch",
      "Timo Ropinski",
      "Marco Aiello"
    ],
    "abstract": "Predicting human behavior in shared environments is crucial for safe and\nefficient human-robot interaction. Traditional data-driven methods to that end\nare pre-trained on domain-specific datasets, activity types, and prediction\nhorizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)\npromise open-ended cross-domain generalization to describe various human\nactivities and make predictions in any context. In particular, Multimodal LLMs\n(MLLMs) are able to integrate information from various sources, achieving more\ncontextual awareness and improved scene understanding. The difficulty in\napplying general-purpose MLLMs directly for prediction stems from their limited\ncapacity for processing large input sequences, sensitivity to prompt design,\nand expensive fine-tuning. In this paper, we present a systematic analysis of\napplying pre-trained MLLMs for context-aware human behavior prediction. To this\nend, we introduce a modular multimodal human activity prediction framework that\nallows us to benchmark various MLLMs, input variations, In-Context Learning\n(ICL), and autoregressive techniques. Our evaluation indicates that the\nbest-performing framework configuration is able to reach 92.8% semantic\nsimilarity and 66.1% exact label accuracy in predicting human behaviors in the\ntarget frame.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00839v1",
    "published_date": "2025-04-01 14:28:19 UTC",
    "updated_date": "2025-04-01 14:28:19 UTC"
  },
  {
    "arxiv_id": "2504.00837v2",
    "title": "A Survey on Music Generation from Single-Modal, Cross-Modal, and Multi-Modal Perspectives",
    "authors": [
      "Shuyu Li",
      "Shulei Ji",
      "Zihao Wang",
      "Songruoyao Wu",
      "Jiaxing Yu",
      "Kejun Zhang"
    ],
    "abstract": "Multi-modal music generation, using multiple modalities like text, images,\nand video alongside musical scores and audio as guidance, is an emerging\nresearch area with broad applications. This paper reviews this field,\ncategorizing music generation systems from the perspective of modalities. The\nreview covers modality representation, multi-modal data alignment, and their\nutilization to guide music generation. Current datasets and evaluation methods\nare also discussed. Key challenges in this area include effective multi-modal\nintegration, large-scale comprehensive datasets, and systematic evaluation\nmethods. Finally, an outlook on future research directions is provided,\nfocusing on creativity, efficiency, multi-modal alignment, and evaluation.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00837v2",
    "published_date": "2025-04-01 14:26:25 UTC",
    "updated_date": "2025-04-20 12:55:44 UTC"
  },
  {
    "arxiv_id": "2504.00831v1",
    "title": "Example-Based Concept Analysis Framework for Deep Weather Forecast Models",
    "authors": [
      "Soyeon Kim",
      "Junho Choi",
      "Subeen Lee",
      "Jaesik Choi"
    ],
    "abstract": "To improve the trustworthiness of an AI model, finding consistent,\nunderstandable representations of its inference process is essential. This\nunderstanding is particularly important in high-stakes operations such as\nweather forecasting, where the identification of underlying meteorological\nmechanisms is as critical as the accuracy of the predictions. Despite the\ngrowing literature that addresses this issue through explainable AI, the\napplicability of their solutions is often limited due to their AI-centric\ndevelopment. To fill this gap, we follow a user-centric process to develop an\nexample-based concept analysis framework, which identifies cases that follow a\nsimilar inference process as the target instance in a target model and presents\nthem in a user-comprehensible format. Our framework provides the users with\nvisually and conceptually analogous examples, including the probability of\nconcept assignment to resolve ambiguities in weather mechanisms. To bridge the\ngap between vector representations identified from models and\nhuman-understandable explanations, we compile a human-annotated concept dataset\nand implement a user interface to assist domain experts involved in the the\nframework development.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "68T07",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00831v1",
    "published_date": "2025-04-01 14:22:41 UTC",
    "updated_date": "2025-04-01 14:22:41 UTC"
  },
  {
    "arxiv_id": "2504.00795v1",
    "title": "Explainable AI-Based Interface System for Weather Forecasting Model",
    "authors": [
      "Soyeon Kim",
      "Junho Choi",
      "Yeji Choi",
      "Subeen Lee",
      "Artyom Stitsyuk",
      "Minkyoung Park",
      "Seongyeop Jeong",
      "Youhyun Baek",
      "Jaesik Choi"
    ],
    "abstract": "Machine learning (ML) is becoming increasingly popular in meteorological\ndecision-making. Although the literature on explainable artificial intelligence\n(XAI) is growing steadily, user-centered XAI studies have not extend to this\ndomain yet. This study defines three requirements for explanations of black-box\nmodels in meteorology through user studies: statistical model performance for\ndifferent rainfall scenarios to identify model bias, model reasoning, and the\nconfidence of model outputs. Appropriate XAI methods are mapped to each\nrequirement, and the generated explanations are tested quantitatively and\nqualitatively. An XAI interface system is designed based on user feedback. The\nresults indicate that the explanations increase decision utility and user\ntrust. Users prefer intuitive explanations over those based on XAI algorithms\neven for potentially easy-to-recognize examples. These findings can provide\nevidence for future research on user-centered XAI algorithms, as well as a\nbasis to improve the usability of AI systems in practice.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "68T07",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00795v1",
    "published_date": "2025-04-01 13:52:34 UTC",
    "updated_date": "2025-04-01 13:52:34 UTC"
  },
  {
    "arxiv_id": "2504.00794v1",
    "title": "Conditional Temporal Neural Processes with Covariance Loss",
    "authors": [
      "Boseon Yoo",
      "Jiwoo Lee",
      "Janghoon Ju",
      "Seijun Chung",
      "Soyeon Kim",
      "Jaesik Choi"
    ],
    "abstract": "We introduce a novel loss function, Covariance Loss, which is conceptually\nequivalent to conditional neural processes and has a form of regularization so\nthat is applicable to many kinds of neural networks. With the proposed loss,\nmappings from input variables to target variables are highly affected by\ndependencies of target variables as well as mean activation and mean\ndependencies of input and target variables. This nature enables the resulting\nneural networks to become more robust to noisy observations and recapture\nmissing dependencies from prior information. In order to show the validity of\nthe proposed loss, we conduct extensive sets of experiments on real-world\ndatasets with state-of-the-art models and discuss the benefits and drawbacks of\nthe proposed Covariance Loss.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.8"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00794v1",
    "published_date": "2025-04-01 13:51:44 UTC",
    "updated_date": "2025-04-01 13:51:44 UTC"
  },
  {
    "arxiv_id": "2504.00780v1",
    "title": "Digitally Supported Analysis of Spontaneous Speech (DigiSpon): Benchmarking NLP-Supported Language Sample Analysis of Swiss Children's Speech",
    "authors": [
      "Anja Ryser",
      "Yingqiang Gao",
      "Sarah Ebling"
    ],
    "abstract": "Language sample analysis (LSA) is a process that complements standardized\npsychometric tests for diagnosing, for example, developmental language disorder\n(DLD) in children. However, its labor-intensive nature has limited its use in\nspeech-language pathology practice. We introduce an approach that leverages\nnatural language processing (NLP) methods not based on commercial large\nlanguage models (LLMs) applied to transcribed speech data from 119 children in\nthe German speaking part of Switzerland with typical and atypical language\ndevelopment. The study aims to identify optimal practices that support\nspeech-language pathologists in diagnosing DLD more efficiently within a\nhuman-in-the-loop framework, without relying on potentially unethical\nimplementations that leverage commercial LLMs. Preliminary findings underscore\nthe potential of integrating locally deployed NLP methods into the process of\nsemi-automatic LSA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00780v1",
    "published_date": "2025-04-01 13:32:38 UTC",
    "updated_date": "2025-04-01 13:32:38 UTC"
  },
  {
    "arxiv_id": "2504.03740v1",
    "title": "Brain Network Classification Based on Graph Contrastive Learning and Graph Transformer",
    "authors": [
      "ZhiTeng Zhu",
      "Lan Yao"
    ],
    "abstract": "The dynamic characterization of functional brain networks is of great\nsignificance for elucidating the mechanisms of human brain function. Although\ngraph neural networks have achieved remarkable progress in functional network\nanalysis, challenges such as data scarcity and insufficient supervision\npersist. To address the limitations of limited training data and inadequate\nsupervision, this paper proposes a novel model named PHGCL-DDGformer that\nintegrates graph contrastive learning with graph transformers, effectively\nenhancing the representation learning capability for brain network\nclassification tasks. To overcome the constraints of existing graph contrastive\nlearning methods in brain network feature extraction, an adaptive graph\naugmentation strategy combining attribute masking and edge perturbation is\nimplemented for data enhancement. Subsequently, a dual-domain graph transformer\n(DDGformer) module is constructed to integrate local and global information,\nwhere graph convolutional networks aggregate neighborhood features to capture\nlocal patterns while attention mechanisms extract global dependencies. Finally,\na graph contrastive learning framework is established to maximize the\nconsistency between positive and negative pairs, thereby obtaining high-quality\ngraph representations. Experimental results on real-world datasets demonstrate\nthat the PHGCL-DDGformer model outperforms existing state-of-the-art approaches\nin brain network classification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures, uses tikz.sty",
    "pdf_url": "http://arxiv.org/pdf/2504.03740v1",
    "published_date": "2025-04-01 13:26:03 UTC",
    "updated_date": "2025-04-01 13:26:03 UTC"
  },
  {
    "arxiv_id": "2504.00762v4",
    "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute",
    "authors": [
      "Jianhao Chen",
      "Zishuo Xun",
      "Bocheng Zhou",
      "Han Qi",
      "Hangfan Zhang",
      "Qiaosheng Zhang",
      "Yang Chen",
      "Wei Hu",
      "Yuzhong Qu",
      "Wanli Ouyang",
      "Shuyue Hu"
    ],
    "abstract": "This paper presents a simple, effective, and cost-efficient strategy to\nimprove LLM performance by scaling test-time compute. Our strategy builds upon\nthe repeated-sampling-then-voting framework, with a novel twist: incorporating\nmultiple models, even weaker ones, to leverage their complementary strengths\nthat potentially arise from diverse training data and paradigms. By using\nconsistency as a signal, our strategy dynamically switches between models.\nTheoretical analysis highlights the efficiency and performance advantages of\nour strategy. Extensive experiments on six datasets demonstrate that our\nstrategy not only outperforms self-consistency and state-of-the-art multi-agent\ndebate approaches, but also significantly reduces inference costs.\nAdditionally, ModelSwitch requires only a few comparable LLMs to achieve\noptimal performance and can be extended with verification methods,\ndemonstrating the potential of leveraging multiple LLMs in the\ngeneration-verification paradigm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00762v4",
    "published_date": "2025-04-01 13:13:43 UTC",
    "updated_date": "2025-05-08 08:07:55 UTC"
  },
  {
    "arxiv_id": "2504.00752v1",
    "title": "LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models",
    "authors": [
      "Sameer Sadruddin",
      "Jennifer D'Souza",
      "Eleni Poupaki",
      "Alex Watkins",
      "Hamed Babaei Giglou",
      "Anisa Rula",
      "Bora Karasulu",
      "S√∂ren Auer",
      "Adrie Mackus",
      "Erwin Kessels"
    ],
    "abstract": "Extracting structured information from unstructured text is crucial for\nmodeling real-world processes, but traditional schema mining relies on\nsemi-structured data, limiting scalability. This paper introduces schema-miner,\na novel tool that combines large language models with human feedback to\nautomate and refine schema extraction. Through an iterative workflow, it\norganizes properties from text, incorporates expert input, and integrates\ndomain-specific ontologies for semantic depth. Applied to materials\nscience--specifically atomic layer deposition--schema-miner demonstrates that\nexpert-guided LLMs generate semantically rich schemas suitable for diverse\nreal-world applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 3 figures, to appear in the Extended Semantic Web\n  Conference (ESWC 2025) proceedings in the Resource track",
    "pdf_url": "http://arxiv.org/pdf/2504.00752v1",
    "published_date": "2025-04-01 13:03:33 UTC",
    "updated_date": "2025-04-01 13:03:33 UTC"
  },
  {
    "arxiv_id": "2504.02870v2",
    "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening",
    "authors": [
      "Frank P. -W. Lo",
      "Jianing Qiu",
      "Zeyu Wang",
      "Haibao Yu",
      "Yeming Chen",
      "Gao Zhang",
      "Benny Lo"
    ],
    "abstract": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by CVPR 2025 Workshop",
    "pdf_url": "http://arxiv.org/pdf/2504.02870v2",
    "published_date": "2025-04-01 12:56:39 UTC",
    "updated_date": "2025-05-13 16:41:54 UTC"
  },
  {
    "arxiv_id": "2504.01053v1",
    "title": "Knowledge-Base based Semantic Image Transmission Using CLIP",
    "authors": [
      "Chongyang Li",
      "Yanmei He",
      "Tianqian Zhang",
      "Mingjian He",
      "Shouyin Liu"
    ],
    "abstract": "This paper proposes a novel knowledge-Base (KB) assisted semantic\ncommunication framework for image transmission. At the receiver, a Facebook AI\nSimilarity Search (FAISS) based vector database is constructed by extracting\nsemantic embeddings from images using the Contrastive Language-Image\nPre-Training (CLIP) model. During transmission, the transmitter first extracts\na 512-dimensional semantic feature using the CLIP model, then compresses it\nwith a lightweight neural network for transmission. After receiving the signal,\nthe receiver reconstructs the feature back to 512 dimensions and performs\nsimilarity matching from the KB to retrieve the most semantically similar\nimage. Semantic transmission success is determined by category consistency\nbetween the transmitted and retrieved images, rather than traditional metrics\nlike Peak Signal-to-Noise Ratio (PSNR). The proposed system prioritizes\nsemantic accuracy, offering a new evaluation paradigm for semantic-aware\ncommunication systems. Experimental validation on CIFAR100 demonstrates the\neffectiveness of the framework in achieving semantic image transmission.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01053v1",
    "published_date": "2025-04-01 12:53:54 UTC",
    "updated_date": "2025-04-01 12:53:54 UTC"
  },
  {
    "arxiv_id": "2504.00727v1",
    "title": "Personality-Driven Decision-Making in LLM-Based Autonomous Agents",
    "authors": [
      "Lewis Newsham",
      "Daniel Prince"
    ],
    "abstract": "The embedding of Large Language Models (LLMs) into autonomous agents is a\nrapidly developing field which enables dynamic, configurable behaviours without\nthe need for extensive domain-specific training. In our previous work, we\nintroduced SANDMAN, a Deceptive Agent architecture leveraging the Five-Factor\nOCEAN personality model, demonstrating that personality induction significantly\ninfluences agent task planning. Building on these findings, this study presents\na novel method for measuring and evaluating how induced personality traits\naffect task selection processes - specifically planning, scheduling, and\ndecision-making - in LLM-based agents. Our results reveal distinct\ntask-selection patterns aligned with induced OCEAN attributes, underscoring the\nfeasibility of designing highly plausible Deceptive Agents for proactive cyber\ndefense strategies.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "I.2.11; I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 8 figures. To be included in Proc. of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.00727v1",
    "published_date": "2025-04-01 12:36:28 UTC",
    "updated_date": "2025-04-01 12:36:28 UTC"
  },
  {
    "arxiv_id": "2504.00717v1",
    "title": "Advancements in Multimodal Differential Evolution: A Comprehensive Review and Future Perspectives",
    "authors": [
      "Dikshit Chauhan",
      "Shivani",
      "Donghwi Jung",
      "Anupam Yadav"
    ],
    "abstract": "Multi-modal optimization involves identifying multiple global and local\noptima of a function, offering valuable insights into diverse optimal solutions\nwithin the search space. Evolutionary algorithms (EAs) excel at finding\nmultiple solutions in a single run, providing a distinct advantage over\nclassical optimization techniques that often require multiple restarts without\nguarantee of obtaining diverse solutions. Among these EAs, differential\nevolution (DE) stands out as a powerful and versatile optimizer for continuous\nparameter spaces. DE has shown significant success in multi-modal optimization\nby utilizing its population-based search to promote the formation of multiple\nstable subpopulations, each targeting different optima. Recent advancements in\nDE for multi-modal optimization have focused on niching methods, parameter\nadaptation, hybridization with other algorithms including machine learning, and\napplications across various domains. Given these developments, it is an\nopportune moment to present a critical review of the latest literature and\nidentify key future research directions. This paper offers a comprehensive\noverview of recent DE advancements in multimodal optimization, including\nmethods for handling multiple optima, hybridization with EAs, and machine\nlearning, and highlights a range of real-world applications. Additionally, the\npaper outlines a set of compelling open problems and future research issues\nfrom multiple perspectives",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00717v1",
    "published_date": "2025-04-01 12:30:07 UTC",
    "updated_date": "2025-04-01 12:30:07 UTC"
  },
  {
    "arxiv_id": "2504.00709v1",
    "title": "Science Autonomy using Machine Learning for Astrobiology",
    "authors": [
      "Victoria Da Poian",
      "Bethany Theiling",
      "Eric Lyness",
      "David Burtt",
      "Abigail R. Azari",
      "Joey Pasterski",
      "Luoth Chou",
      "Melissa Trainer",
      "Ryan Danell",
      "Desmond Kaplan",
      "Xiang Li",
      "Lily Clough",
      "Brett McKinney",
      "Lukas Mandrake",
      "Bill Diamond",
      "Caroline Freissinet"
    ],
    "abstract": "In recent decades, artificial intelligence (AI) including machine learning\n(ML) have become vital for space missions enabling rapid data processing,\nadvanced pattern recognition, and enhanced insight extraction. These tools are\nespecially valuable in astrobiology applications, where models must distinguish\nbiotic patterns from complex abiotic backgrounds. Advancing the integration of\nautonomy through AI and ML into space missions is a complex challenge, and we\nbelieve that by focusing on key areas, we can make significant progress and\noffer practical recommendations for tackling these obstacles.",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM",
    "comment": "8 pages (expanded citations compared to 5 page submitted version for\n  DARES white papers), a white paper for the 2025 NASA Decadal Astrobiology\n  Research and Exploration Strategy (DARES)",
    "pdf_url": "http://arxiv.org/pdf/2504.00709v1",
    "published_date": "2025-04-01 12:20:18 UTC",
    "updated_date": "2025-04-01 12:20:18 UTC"
  },
  {
    "arxiv_id": "2504.00707v1",
    "title": "Energy Weighted Learning Progress Guided Interleaved Multi-Task Learning",
    "authors": [
      "Hanne Say",
      "Suzan Ece Ada",
      "Emre Ugur",
      "Erhan Oztop"
    ],
    "abstract": "Humans can continuously acquire new skills and knowledge by exploiting\nexisting ones for improved learning, without forgetting them. Similarly,\n'continual learning' in machine learning aims to learn new information while\npreserving the previously acquired knowledge. Existing research often overlooks\nthe nature of human learning, where tasks are interleaved due to human choice\nor environmental constraints. So, almost never do humans master one task before\nswitching to the next. To investigate to what extent human-like learning can\nbenefit the learner, we propose a method that interleaves tasks based on their\n'learning progress' and energy consumption. From a machine learning\nperspective, our approach can be seen as a multi-task learning system that\nbalances learning performance with energy constraints while mimicking\necologically realistic human task learning. To assess the validity of our\napproach, we consider a robot learning setting in simulation, where the robot\nlearns the effect of its actions in different contexts. The conducted\nexperiments show that our proposed method achieves better performance than\nsequential task learning and reduces energy consumption for learning the tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00707v1",
    "published_date": "2025-04-01 12:15:27 UTC",
    "updated_date": "2025-04-01 12:15:27 UTC"
  },
  {
    "arxiv_id": "2504.00698v2",
    "title": "Command A: An Enterprise-Ready Large Language Model",
    "authors": [
      "Team Cohere",
      ":",
      "Aakanksha",
      "Arash Ahmadian",
      "Marwan Ahmed",
      "Jay Alammar",
      "Milad Alizadeh",
      "Yazeed Alnumay",
      "Sophia Althammer",
      "Arkady Arkhangorodsky",
      "Viraat Aryabumi",
      "Dennis Aumiller",
      "Rapha√´l Avalos",
      "Zahara Aviv",
      "Sammie Bae",
      "Saurabh Baji",
      "Alexandre Barbet",
      "Max Bartolo",
      "Bj√∂rn Bebensee",
      "Neeral Beladia",
      "Walter Beller-Morales",
      "Alexandre B√©rard",
      "Andrew Berneshawi",
      "Anna Bialas",
      "Phil Blunsom",
      "Matt Bobkin",
      "Adi Bongale",
      "Sam Braun",
      "Maxime Brunet",
      "Samuel Cahyawijaya",
      "David Cairuz",
      "Jon Ander Campos",
      "Cassie Cao",
      "Kris Cao",
      "Roman Castagn√©",
      "Juli√°n Cendrero",
      "Leila Chan Currie",
      "Yash Chandak",
      "Diane Chang",
      "Giannis Chatziveroglou",
      "Hongyu Chen",
      "Claire Cheng",
      "Alexis Chevalier",
      "Justin T. Chiu",
      "Eugene Cho",
      "Eugene Choi",
      "Eujeong Choi",
      "Tim Chung",
      "Volkan Cirik",
      "Ana Cismaru",
      "Pierre Clavier",
      "Henry Conklin",
      "Lucas Crawhall-Stein",
      "Devon Crouse",
      "Andres Felipe Cruz-Salinas",
      "Ben Cyrus",
      "Daniel D'souza",
      "Hugo Dalla-Torre",
      "John Dang",
      "William Darling",
      "Omar Darwiche Domingues",
      "Saurabh Dash",
      "Antoine Debugne",
      "Th√©o Dehaze",
      "Shaan Desai",
      "Joan Devassy",
      "Rishit Dholakia",
      "Kyle Duffy",
      "Ali Edalati",
      "Ace Eldeib",
      "Abdullah Elkady",
      "Sarah Elsharkawy",
      "Irem Erg√ºn",
      "Beyza Ermis",
      "Marzieh Fadaee",
      "Boyu Fan",
      "Lucas Fayoux",
      "Yannis Flet-Berliac",
      "Nick Frosst",
      "Matthias Gall√©",
      "Wojciech Galuba",
      "Utsav Garg",
      "Matthieu Geist",
      "Mohammad Gheshlaghi Azar",
      "Ellen Gilsenan-McMahon",
      "Seraphina Goldfarb-Tarrant",
      "Tomas Goldsack",
      "Aidan Gomez",
      "Victor Machado Gonzaga",
      "Nithya Govindarajan",
      "Manoj Govindassamy",
      "Nathan Grinsztajn",
      "Nikolas Gritsch",
      "Patrick Gu",
      "Shangmin Guo",
      "Kilian Haefeli",
      "Rod Hajjar",
      "Tim Hawes",
      "Jingyi He",
      "Sebastian Hofst√§tter",
      "Sungjin Hong",
      "Sara Hooker",
      "Tom Hosking",
      "Stephanie Howe",
      "Eric Hu",
      "Renjie Huang",
      "Hemant Jain",
      "Ritika Jain",
      "Nick Jakobi",
      "Madeline Jenkins",
      "JJ Jordan",
      "Dhruti Joshi",
      "Jason Jung",
      "Trushant Kalyanpur",
      "Siddhartha Rao Kamalakara",
      "Julia Kedrzycki",
      "Gokce Keskin",
      "Edward Kim",
      "Joon Kim",
      "Wei-Yin Ko",
      "Tom Kocmi",
      "Michael Kozakov",
      "Wojciech Kry≈õci≈Ñski",
      "Arnav Kumar Jain",
      "Komal Kumar Teru",
      "Sander Land",
      "Michael Lasby",
      "Olivia Lasche",
      "Justin Lee",
      "Patrick Lewis",
      "Jeffrey Li",
      "Jonathan Li",
      "Hangyu Lin",
      "Acyr Locatelli",
      "Kevin Luong",
      "Raymond Ma",
      "Luk√°≈° Mach",
      "Marina Machado",
      "Joanne Magbitang",
      "Brenda Malacara Lopez",
      "Aryan Mann",
      "Kelly Marchisio",
      "Olivia Markham",
      "Alexandre Matton",
      "Alex McKinney",
      "Dominic McLoughlin",
      "Jozef Mokry",
      "Adrien Morisot",
      "Autumn Moulder",
      "Harry Moynehan",
      "Maximilian Mozes",
      "Vivek Muppalla",
      "Lidiya Murakhovska",
      "Hemangani Nagarajan",
      "Alekhya Nandula",
      "Hisham Nasir",
      "Shauna Nehra",
      "Josh Netto-Rosen",
      "Daniel Ohashi",
      "James Owers-Bardsley",
      "Jason Ozuzu",
      "Dennis Padilla",
      "Gloria Park",
      "Sam Passaglia",
      "Jeremy Pekmez",
      "Laura Penstone",
      "Aleksandra Piktus",
      "Case Ploeg",
      "Andrew Poulton",
      "Youran Qi",
      "Shubha Raghvendra",
      "Miguel Ramos",
      "Ekagra Ranjan",
      "Pierre Richemond",
      "C√©cile Robert-Michon",
      "Aur√©lien Rodriguez",
      "Sudip Roy",
      "Sebastian Ruder",
      "Laura Ruis",
      "Louise Rust",
      "Anubhav Sachan",
      "Alejandro Salamanca",
      "Kailash Karthik Saravanakumar",
      "Isha Satyakam",
      "Alice Schoenauer Sebag",
      "Priyanka Sen",
      "Sholeh Sepehri",
      "Preethi Seshadri",
      "Ye Shen",
      "Tom Sherborne",
      "Sylvie Shang Shi",
      "Sanal Shivaprasad",
      "Vladyslav Shmyhlo",
      "Anirudh Shrinivason",
      "Inna Shteinbuk",
      "Amir Shukayev",
      "Mathieu Simard",
      "Ella Snyder",
      "Ava Spataru",
      "Victoria Spooner",
      "Trisha Starostina",
      "Florian Strub",
      "Yixuan Su",
      "Jimin Sun",
      "Dwarak Talupuru",
      "Eugene Tarassov",
      "Elena Tommasone",
      "Jennifer Tracey",
      "Billy Trend",
      "Evren Tumer",
      "Ahmet √úst√ºn",
      "Bharat Venkitesh",
      "David Venuto",
      "Pat Verga",
      "Maxime Voisin",
      "Alex Wang",
      "Donglu Wang",
      "Shijian Wang",
      "Edmond Wen",
      "Naomi White",
      "Jesse Willman",
      "Marysia Winkels",
      "Chen Xia",
      "Jessica Xie",
      "Minjie Xu",
      "Bowen Yang",
      "Tan Yi-Chern",
      "Ivan Zhang",
      "Zhenyu Zhao",
      "Zhoujie Zhao"
    ],
    "abstract": "In this report we describe the development of Command A, a powerful large\nlanguage model purpose-built to excel at real-world enterprise use cases.\nCommand A is an agent-optimised and multilingual-capable model, with support\nfor 23 languages of global business, and a novel hybrid architecture balancing\nefficiency with top of the range performance. It offers best-in-class Retrieval\nAugmented Generation (RAG) capabilities with grounding and tool use to automate\nsophisticated business processes. These abilities are achieved through a\ndecentralised training approach, including self-refinement algorithms and model\nmerging techniques. We also include results for Command R7B which shares\ncapability and architectural similarities to Command A. Weights for both models\nhave been released for research purposes. This technical report details our\noriginal training pipeline and presents an extensive evaluation of our models\nacross a suite of enterprise-relevant tasks and public benchmarks,\ndemonstrating excellent performance and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "55 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.00698v2",
    "published_date": "2025-04-01 12:08:07 UTC",
    "updated_date": "2025-04-14 12:37:51 UTC"
  },
  {
    "arxiv_id": "2504.00692v1",
    "title": "The HCI GenAI CO2ST Calculator: A Tool for Calculating the Carbon Footprint of Generative AI Use in Human-Computer Interaction Research",
    "authors": [
      "Nanna Inie",
      "Jeanette Falk",
      "Raghavendra Selvan"
    ],
    "abstract": "Increased usage of generative AI (GenAI) in Human-Computer Interaction (HCI)\nresearch induces a climate impact from carbon emissions due to energy\nconsumption of the hardware used to develop and run GenAI models and systems.\nThe exact energy usage and and subsequent carbon emissions are difficult to\nestimate in HCI research because HCI researchers most often use cloud-based\nservices where the hardware and its energy consumption are hidden from plain\nview. The HCI GenAI CO2ST Calculator is a tool designed specifically for the\nHCI research pipeline, to help researchers estimate the energy consumption and\ncarbon footprint of using generative AI in their research, either a priori\n(allowing for mitigation strategies or experimental redesign) or post hoc\n(allowing for transparent documentation of carbon footprint in written reports\nof the research).",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00692v1",
    "published_date": "2025-04-01 12:02:45 UTC",
    "updated_date": "2025-04-01 12:02:45 UTC"
  },
  {
    "arxiv_id": "2504.03739v1",
    "title": "A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System",
    "authors": [
      "Mingyan Liu"
    ],
    "abstract": "Generative models, such as GPT and BERT, have significantly improved\nperformance in tasks like text generation and summarization. However,\nhallucinations \"where models generate non-factual or misleading content\" are\nespecially problematic in smaller-scale architectures, limiting their\nreal-world applicability.In this paper, we propose a unified Virtual\nMixture-of-Experts (MoE) fusion strategy that enhances inference performance\nand mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing\nthe parameter count. Our method leverages multiple domain-specific expert\nprompts (with the number of experts being adjustable) to guide the model from\ndifferent perspectives. We apply a statistical outlier truncation strategy\nbased on the mean and standard deviation to filter out abnormally high\nprobability predictions, and we inject noise into the embedding space to\npromote output diversity. To clearly assess the contribution of each module, we\nadopt a fixed voting mechanism rather than a dynamic gating network, thereby\navoiding additional confounding factors. We provide detailed theoretical\nderivations from both statistical and ensemble learning perspectives to\ndemonstrate how our method reduces output variance and suppresses\nhallucinations. Extensive ablation experiments on dialogue generation tasks\nshow that our approach significantly improves inference accuracy and robustness\nin small models. Additionally, we discuss methods for evaluating the\northogonality of virtual experts and outline the potential for future work\ninvolving dynamic expert weight allocation using gating networks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03739v1",
    "published_date": "2025-04-01 11:38:01 UTC",
    "updated_date": "2025-04-01 11:38:01 UTC"
  },
  {
    "arxiv_id": "2504.13877v1",
    "title": "New care pathways for supporting transitional care from hospitals to home using AI and personalized digital assistance",
    "authors": [
      "Ionut Anghel",
      "Tudor Cioara",
      "Roberta Bevilacqua",
      "Federico Barbarossa",
      "Terje Grimstad",
      "Riitta Hellman",
      "Arnor Solberg",
      "Lars Thomas Boye",
      "Ovidiu Anchidin",
      "Ancuta Nemes",
      "Camilla Gabrielsen"
    ],
    "abstract": "Transitional care may play a vital role for the sustainability of Europe\nfuture healthcare system, offering solutions for relocating patient care from\nhospital to home therefore addressing the growing demand for medical care as\nthe population is ageing. However, to be effective, it is essential to\nintegrate innovative Information and Communications Technology technologies to\nensure that patients with comorbidities experience a smooth and coordinated\ntransition from hospitals or care centers to home, thereby reducing the risk of\nrehospitalization. In this paper, we present an overview of the integration of\nInternet of Things, artificial intelligence, and digital assistance\ntechnologies with traditional care pathways to address the challenges and needs\nof healthcare systems in Europe. We identify the current gaps in transitional\ncare and define the technology mapping to enhance the care pathways, aiming to\nimprove patient outcomes, safety, and quality of life avoiding hospital\nreadmissions. Finally, we define the trial setup and evaluation methodology\nneeded to provide clinical evidence that supports the positive impact of\ntechnology integration on patient care and discuss the potential effects on the\nhealthcare system.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "submitted to journal (under review)",
    "pdf_url": "http://arxiv.org/pdf/2504.13877v1",
    "published_date": "2025-04-01 11:37:36 UTC",
    "updated_date": "2025-04-01 11:37:36 UTC"
  },
  {
    "arxiv_id": "2504.00661v1",
    "title": "DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism",
    "authors": [
      "Dengchun Li",
      "Naizheng Wang",
      "Zihao Zhang",
      "Haoyang Yin",
      "Lei Duan",
      "Meng Xiao",
      "Mingjie Tang"
    ],
    "abstract": "Instruction-based fine-tuning of large language models (LLMs) has achieved\nremarkable success in various natural language processing (NLP) tasks.\nParameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts\n(MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the\nversatility of Mixture of Experts (MoE) models, demonstrating significant\npotential for handling multiple downstream tasks. However, the existing routing\nmechanisms for MoLE often involve a trade-off between computational efficiency\nand predictive accuracy, and they fail to fully address the diverse expert\nselection demands across different transformer layers. In this work, we propose\nDynMoLE, a hybrid routing strategy that dynamically adjusts expert selection\nbased on the Tsallis entropy of the router's probability distribution. This\napproach mitigates router uncertainty, enhances stability, and promotes more\nequitable expert participation, leading to faster convergence and improved\nmodel performance. Additionally, we introduce an auxiliary loss based on\nTsallis entropy to further guide the model toward convergence with reduced\nuncertainty, thereby improving training stability and performance. Our\nextensive experiments on commonsense reasoning benchmarks demonstrate that\nDynMoLE achieves substantial performance improvements, outperforming LoRA by\n9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also\nconduct a comprehensive ablation study to evaluate the contributions of\nDynMoLE's key components.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00661v1",
    "published_date": "2025-04-01 11:14:19 UTC",
    "updated_date": "2025-04-01 11:14:19 UTC"
  },
  {
    "arxiv_id": "2504.00652v1",
    "title": "Towards Adaptive AI Governance: Comparative Insights from the U.S., EU, and Asia",
    "authors": [
      "Vikram Kulothungan",
      "Deepti Gupta"
    ],
    "abstract": "Artificial intelligence (AI) trends vary significantly across global regions,\nshaping the trajectory of innovation, regulation, and societal impact. This\nvariation influences how different regions approach AI development, balancing\ntechnological progress with ethical and regulatory considerations. This study\nconducts a comparative analysis of AI trends in the United States (US), the\nEuropean Union (EU), and Asia, focusing on three key dimensions: generative AI,\nethical oversight, and industrial applications. The US prioritizes\nmarket-driven innovation with minimal regulatory constraints, the EU enforces a\nprecautionary risk-based framework emphasizing ethical safeguards, and Asia\nemploys state-guided AI strategies that balance rapid deployment with\nregulatory oversight. Although these approaches reflect different economic\nmodels and policy priorities, their divergence poses challenges to\ninternational collaboration, regulatory harmonization, and the development of\nglobal AI standards. To address these challenges, this paper synthesizes\nregional strengths to propose an adaptive AI governance framework that\nintegrates risk-tiered oversight, innovation accelerators, and strategic\nalignment mechanisms. By bridging governance gaps, this study offers actionable\ninsights for fostering responsible AI development while ensuring a balance\nbetween technological progress, ethical imperatives, and regulatory coherence.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted at IEEE BigDataSecurity 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2504.00652v1",
    "published_date": "2025-04-01 11:05:47 UTC",
    "updated_date": "2025-04-01 11:05:47 UTC"
  },
  {
    "arxiv_id": "2504.02000v1",
    "title": "AI Regulation and Capitalist Growth: Balancing Innovation, Ethics, and Global Governance",
    "authors": [
      "Vikram Kulothungan",
      "Priya Ranjani Mohan",
      "Deepti Gupta"
    ],
    "abstract": "Artificial Intelligence (AI) is increasingly central to economic growth,\npromising new efficiencies and markets. This economic significance has sparked\ndebate over AI regulation: do rules and oversight bolster long term growth by\nbuilding trust and safeguarding the public, or do they constrain innovation and\nfree enterprise? This paper examines the balance between AI regulation and\ncapitalist ideals, focusing on how different approaches to AI data privacy can\nimpact innovation in AI-driven applications. The central question is whether AI\nregulation enhances or inhibits growth in a capitalist economy. Our analysis\nsynthesizes historical precedents, the current U.S. regulatory landscape,\neconomic projections, legal challenges, and case studies of recent AI policies.\nWe discuss that carefully calibrated AI data privacy regulations-balancing\ninnovation incentives with the public interest can foster sustainable growth by\nbuilding trust and ensuring responsible data use, while excessive regulation\nmay risk stifling innovation and entrenching incumbents.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "Accepted for IEEE BigDataSecurity 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2504.02000v1",
    "published_date": "2025-04-01 10:59:02 UTC",
    "updated_date": "2025-04-01 10:59:02 UTC"
  },
  {
    "arxiv_id": "2504.00638v2",
    "title": "Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models",
    "authors": [
      "Alireza Aghabagherloo",
      "Aydin Abadi",
      "Sumanta Sarkar",
      "Vishnu Asutosh Dasu",
      "Bart Preneel"
    ],
    "abstract": "The accuracy and robustness of machine learning models against adversarial\nattacks are significantly influenced by factors such as training data quality,\nmodel architecture, the training process, and the deployment environment. In\nrecent years, duplicated data in training sets, especially in language models,\nhas attracted considerable attention. It has been shown that deduplication\nenhances both training performance and model accuracy in language models. While\nthe importance of data quality in training image classifier Deep Neural\nNetworks (DNNs) is widely recognized, the impact of duplicated images in the\ntraining set on model generalization and performance has received little\nattention.\n  In this paper, we address this gap and provide a comprehensive study on the\neffect of duplicates in image classification. Our analysis indicates that the\npresence of duplicated images in the training set not only negatively affects\nthe efficiency of model training but also may result in lower accuracy of the\nimage classifier. This negative impact of duplication on accuracy is\nparticularly evident when duplicated data is non-uniform across classes or when\nduplication, whether uniform or non-uniform, occurs in the training set of an\nadversarially trained model. Even when duplicated samples are selected in a\nuniform way, increasing the amount of duplication does not lead to a\nsignificant improvement in accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00638v2",
    "published_date": "2025-04-01 10:48:00 UTC",
    "updated_date": "2025-04-17 16:01:23 UTC"
  },
  {
    "arxiv_id": "2504.00634v1",
    "title": "CNOT-Optimal Clifford Synthesis as SAT",
    "authors": [
      "Irfansha Shaik",
      "Jaco van de Pol"
    ],
    "abstract": "Clifford circuit optimization is an important step in the quantum compilation\npipeline. Major compilers employ heuristic approaches. While they are fast,\ntheir results are often suboptimal. Minimization of noisy gates, like 2-qubit\nCNOT gates, is crucial for practical computing. Exact approaches have been\nproposed to fill the gap left by heuristic approaches. Among these are SAT\nbased approaches that optimize gate count or depth, but they suffer from\nscalability issues. Further, they do not guarantee optimality on more important\nmetrics like CNOT count or CNOT depth. A recent work proposed an exhaustive\nsearch only on Clifford circuits in a certain normal form to guarantee CNOT\ncount optimality. But an exhaustive approach cannot scale beyond 6 qubits.\n  In this paper, we incorporate search restricted to Clifford normal forms in a\nSAT encoding to guarantee CNOT count optimality. By allowing parallel plans, we\npropose a second SAT encoding that optimizes CNOT depth. By taking advantage of\nflexibility in SAT based approaches, we also handle connectivity restrictions\nin hardware platforms, and allow for qubit relabeling. We have implemented the\nabove encodings and variations in our open source tool Q-Synth.\n  In experiments, our encodings significantly outperform existing SAT\napproaches on random Clifford circuits. We consider practical VQE and Feynman\nbenchmarks to compare with TKET and Qiskit compilers. In all-to-all\nconnectivity, we observe reductions up to 32.1% in CNOT count and 48.1% in CNOT\ndepth. Overall, we observe better results than TKET in the CNOT count and\ndepth. We also experiment with connectivity restrictions of major quantum\nplatforms. Compared to Qiskit, we observe up to 30.3% CNOT count and 35.9% CNOT\ndepth further reduction.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "27 pages (16 main text, rest references and appendix), 15 Tables, 3\n  Figures, 2 Algorithms",
    "pdf_url": "http://arxiv.org/pdf/2504.00634v1",
    "published_date": "2025-04-01 10:35:58 UTC",
    "updated_date": "2025-04-01 10:35:58 UTC"
  },
  {
    "arxiv_id": "2504.00624v1",
    "title": "Feature Subset Weighting for Distance-based Supervised Learning through Choquet Integration",
    "authors": [
      "Adnan Theerens",
      "Yvan Saeys",
      "Chris Cornelis"
    ],
    "abstract": "This paper introduces feature subset weighting using monotone measures for\ndistance-based supervised learning. The Choquet integral is used to define a\ndistance metric that incorporates these weights. This integration enables the\nproposed distances to effectively capture non-linear relationships and account\nfor interactions both between conditional and decision attributes and among\nconditional attributes themselves, resulting in a more flexible distance\nmeasure. In particular, we show how this approach ensures that the distances\nremain unaffected by the addition of duplicate and strongly correlated\nfeatures. Another key point of this approach is that it makes feature subset\nweighting computationally feasible, since only $m$ feature subset weights\nshould be calculated each time instead of calculating all feature subset\nweights ($2^m$), where $m$ is the number of attributes. Next, we also examine\nhow the use of the Choquet integral for measuring similarity leads to a\nnon-equivalent definition of distance. The relationship between distance and\nsimilarity is further explored through dual measures. Additionally, symmetric\nChoquet distances and similarities are proposed, preserving the classical\nsymmetry between similarity and distance. Finally, we introduce a concrete\nfeature subset weighting distance, evaluate its performance in a $k$-nearest\nneighbors (KNN) classification setting, and compare it against Mahalanobis\ndistances and weighted distance methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00624v1",
    "published_date": "2025-04-01 10:23:01 UTC",
    "updated_date": "2025-04-01 10:23:01 UTC"
  },
  {
    "arxiv_id": "2504.00615v2",
    "title": "Towards Responsible and Trustworthy Educational Data Mining: Comparing Symbolic, Sub-Symbolic, and Neural-Symbolic AI Methods",
    "authors": [
      "Danial Hooshyar",
      "Eve Kikas",
      "Yeongwook Yang",
      "Gustav ≈†√≠r",
      "Raija H√§m√§l√§inen",
      "Tommi K√§rkk√§inen",
      "Roger Azevedo"
    ],
    "abstract": "Given the demand for responsible and trustworthy AI for education, this study\nevaluates symbolic, sub-symbolic, and neural-symbolic AI (NSAI) in terms of\ngeneralizability and interpretability. Our extensive experiments on balanced\nand imbalanced self-regulated learning datasets of Estonian primary school\nstudents predicting 7th-grade mathematics national test performance showed that\nsymbolic and sub-symbolic methods performed well on balanced data but struggled\nto identify low performers in imbalanced datasets. Interestingly, symbolic and\nsub-symbolic methods emphasized different factors in their decision-making:\nsymbolic approaches primarily relied on cognitive and motivational factors,\nwhile sub-symbolic methods focused more on cognitive aspects, learnt knowledge,\nand the demographic variable of gender -- yet both largely overlooked\nmetacognitive factors. The NSAI method, on the other hand, showed advantages\nby: (i) being more generalizable across both classes -- even in imbalanced\ndatasets -- as its symbolic knowledge component compensated for the\nunderrepresented class; and (ii) relying on a more integrated set of factors in\nits decision-making, including motivation, (meta)cognition, and learnt\nknowledge, thus offering a comprehensive and theoretically grounded\ninterpretability framework. These contrasting findings highlight the need for a\nholistic comparison of AI methods before drawing conclusions based solely on\npredictive performance. They also underscore the potential of hybrid,\nhuman-centred NSAI methods to address the limitations of other AI families and\nmove us closer to responsible AI for education. Specifically, by enabling\nstakeholders to contribute to AI design, NSAI aligns learned patterns with\ntheoretical constructs, incorporates factors like motivation and metacognition,\nand strengthens the trustworthiness and responsibility of educational data\nmining.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00615v2",
    "published_date": "2025-04-01 10:14:11 UTC",
    "updated_date": "2025-04-11 11:21:05 UTC"
  },
  {
    "arxiv_id": "2504.00613v1",
    "title": "LLM-Guided Search for Deletion-Correcting Codes",
    "authors": [
      "Franziska Weindel",
      "Reinhard Heckel"
    ],
    "abstract": "Finding deletion-correcting codes of maximum size has been an open problem\nfor over 70 years, even for a single deletion. In this paper, we propose a\nnovel approach for constructing deletion-correcting codes. A code is a set of\nsequences satisfying certain constraints, and we construct it by greedily\nadding the highest-priority sequence according to a priority function. To find\ngood priority functions, we leverage FunSearch, a large language model\n(LLM)-guided evolutionary search proposed by Romera et al., 2024. FunSearch\niteratively generates, evaluates, and refines priority functions to construct\nlarge deletion-correcting codes. For a single deletion, our evolutionary search\nfinds functions that construct codes which match known maximum sizes, reach the\nsize of the largest (conjectured optimal) Varshamov-Tenengolts codes where the\nmaximum is unknown, and independently rediscover them in equivalent form. For\ntwo deletions, we find functions that construct codes with new best-known sizes\nfor code lengths \\( n = 12, 13 \\), and \\( 16 \\), establishing improved lower\nbounds. These results demonstrate the potential of LLM-guided search for\ninformation theory and code design and represent the first application of such\nmethods for constructing error-correcting codes.",
    "categories": [
      "cs.AI",
      "cs.IT",
      "cs.NE",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00613v1",
    "published_date": "2025-04-01 10:11:32 UTC",
    "updated_date": "2025-04-01 10:11:32 UTC"
  },
  {
    "arxiv_id": "2504.00608v1",
    "title": "PLM4NDV: Minimizing Data Access for Number of Distinct Values Estimation with Pre-trained Language Models",
    "authors": [
      "Xianghong Xu",
      "Xiao He",
      "Tieying Zhang",
      "Lei Zhang",
      "Rui Shi",
      "Jianjun Chen"
    ],
    "abstract": "Number of Distinct Values (NDV) estimation of a multiset/column is a basis\nfor many data management tasks, especially within databases. Despite decades of\nresearch, most existing methods require either a significant amount of samples\nthrough uniform random sampling or access to the entire column to produce\nestimates, leading to substantial data access costs and potentially ineffective\nestimations in scenarios with limited data access. In this paper, we propose\nleveraging semantic information, i.e., schema, to address these challenges. The\nschema contains rich semantic information that can benefit the NDV estimation.\nTo this end, we propose PLM4NDV, a learned method incorporating Pre-trained\nLanguage Models (PLMs) to extract semantic schema information for NDV\nestimation. Specifically, PLM4NDV leverages the semantics of the target column\nand the corresponding table to gain a comprehensive understanding of the\ncolumn's meaning. By using the semantics, PLM4NDV reduces data access costs,\nprovides accurate NDV estimation, and can even operate effectively without any\ndata access. Extensive experiments on a large-scale real-world dataset\ndemonstrate the superiority of PLM4NDV over baseline methods. Our code is\navailable at https://github.com/bytedance/plm4ndv.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted by SIGMOD 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.00608v1",
    "published_date": "2025-04-01 10:06:20 UTC",
    "updated_date": "2025-04-01 10:06:20 UTC"
  },
  {
    "arxiv_id": "2504.00603v1",
    "title": "Data Cleansing for GANs",
    "authors": [
      "Naoyuki Terashita",
      "Hiroki Ohashi",
      "Satoshi Hara"
    ],
    "abstract": "As the application of generative adversarial networks (GANs) expands, it\nbecomes increasingly critical to develop a unified approach that improves\nperformance across various generative tasks. One effective strategy that\napplies to any machine learning task is identifying harmful instances, whose\nremoval improves the performance. While previous studies have successfully\nestimated these harmful training instances in supervised settings, their\napproaches are not easily applicable to GANs. The challenge lies in two\nrequirements of the previous approaches that do not apply to GANs. First,\nprevious approaches require that the absence of a training instance directly\naffects the parameters. However, in the training for GANs, the instances do not\ndirectly affect the generator's parameters since they are only fed into the\ndiscriminator. Second, previous approaches assume that the change in loss\ndirectly quantifies the harmfulness of the instance to a model's performance,\nwhile common types of GAN losses do not always reflect the generative\nperformance. To overcome the first challenge, we propose influence estimation\nmethods that use the Jacobian of the generator's gradient with respect to the\ndiscriminator's parameters (and vice versa). Such a Jacobian represents the\nindirect effect between two models: how removing an instance from the\ndiscriminator's training changes the generator's parameters. Second, we propose\nan instance evaluation scheme that measures the harmfulness of each training\ninstance based on how a GAN evaluation metric (e.g., Inception score) is\nexpected to change by the instance's removal. Furthermore, we demonstrate that\nremoving the identified harmful instances significantly improves the generative\nperformance on various GAN evaluation metrics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS, 2025). Journal extention of\n  https://openreview.net/forum?id=opHLcXxYTC_",
    "pdf_url": "http://arxiv.org/pdf/2504.00603v1",
    "published_date": "2025-04-01 10:02:37 UTC",
    "updated_date": "2025-04-01 10:02:37 UTC"
  },
  {
    "arxiv_id": "2504.00597v2",
    "title": "On the Consistency of Multilingual Context Utilization in Retrieval-Augmented Generation",
    "authors": [
      "Jirui Qi",
      "Raquel Fern√°ndez",
      "Arianna Bisazza"
    ],
    "abstract": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from out-language passages, but a much weaker ability to formulate\na full answer in the correct language. Our analysis, based on both accuracy and\nfeature attribution techniques, further shows that distracting passages\nnegatively impact answer quality regardless of their language. However,\ndistractors in the query language exert a slightly stronger influence. Taken\ntogether, our findings deepen the understanding of how LLMs utilize context in\nmRAG systems, providing directions for future improvements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review at COLM2025. All codes and data are released at\n  https://github.com/Betswish/mRAG-Context-Consistency",
    "pdf_url": "http://arxiv.org/pdf/2504.00597v2",
    "published_date": "2025-04-01 09:55:23 UTC",
    "updated_date": "2025-04-08 12:40:23 UTC"
  },
  {
    "arxiv_id": "2504.00584v1",
    "title": "Enhancing Negation Awareness in Universal Text Embeddings: A Data-efficient and Computational-efficient Approach",
    "authors": [
      "Hongliu Cao"
    ],
    "abstract": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00584v1",
    "published_date": "2025-04-01 09:39:57 UTC",
    "updated_date": "2025-04-01 09:39:57 UTC"
  },
  {
    "arxiv_id": "2504.02867v1",
    "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications",
    "authors": [
      "Hongliu Cao",
      "Ilias Driouich",
      "Robin Singh",
      "Eoin Thomas"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across\ndiverse domains, yet they still encounter challenges such as insufficient\ndomain-specific knowledge, biases, and hallucinations. This underscores the\nneed for robust evaluation methodologies to accurately assess LLM-based\napplications. Traditional evaluation methods, which rely on word overlap or\ntext embeddings, are inadequate for capturing the nuanced semantic information\nnecessary to evaluate dynamic, open-ended text generation. Recent research has\nexplored leveraging LLMs to mimic human reasoning and decision-making processes\nfor evaluation purposes known as LLM-as-a-judge framework. However, these\nexisting frameworks have two significant limitations. First, they lack the\nflexibility to adapt to different text styles, including various answer and\nground truth styles, thereby reducing their generalization performance. Second,\nthe evaluation scores produced by these frameworks are often skewed and hard to\ninterpret, showing a low correlation with human judgment. To address these\nchallenges, we propose a novel dynamic multi-agent system that automatically\ndesigns personalized LLM judges for various natural language generation\napplications. This system iteratively refines evaluation prompts and balances\nthe trade-off between the adaptive requirements of downstream tasks and the\nalignment with human perception. Our experimental results show that the\nproposed multi-agent LLM Judge framework not only enhances evaluation accuracy\ncompared to existing methods but also produces evaluation scores that better\nalign with human perception.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at SophiaSummit2024",
    "pdf_url": "http://arxiv.org/pdf/2504.02867v1",
    "published_date": "2025-04-01 09:36:56 UTC",
    "updated_date": "2025-04-01 09:36:56 UTC"
  },
  {
    "arxiv_id": "2504.03738v1",
    "title": "Attention in Diffusion Model: A Survey",
    "authors": [
      "Litao Hua",
      "Fan Liu",
      "Jie Su",
      "Xingyu Miao",
      "Zizhou Ouyang",
      "Zeyu Wang",
      "Runze Hu",
      "Zhenyu Wen",
      "Bing Zhai",
      "Yang Long",
      "Haoran Duan",
      "Yuan Zhou"
    ],
    "abstract": "Attention mechanisms have become a foundational component in diffusion\nmodels, significantly influencing their capacity across a wide range of\ngenerative and discriminative tasks. This paper presents a comprehensive survey\nof attention within diffusion models, systematically analysing its roles,\ndesign patterns, and operations across different modalities and tasks. We\npropose a unified taxonomy that categorises attention-related modifications\ninto parts according to the structural components they affect, offering a clear\nlens through which to understand their functional diversity. In addition to\nreviewing architectural innovations, we examine how attention mechanisms\ncontribute to performance improvements in diverse applications. We also\nidentify current limitations and underexplored areas, and outline potential\ndirections for future research. Our study provides valuable insights into the\nevolving landscape of diffusion models, with a particular focus on the\nintegrative and ubiquitous role of attention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03738v1",
    "published_date": "2025-04-01 09:00:49 UTC",
    "updated_date": "2025-04-01 09:00:49 UTC"
  },
  {
    "arxiv_id": "2504.00526v1",
    "title": "High-Quality Pseudo-Label Generation Based on Visual Prompt Assisted Cloud Model Update",
    "authors": [
      "Xinrun Xu",
      "Qiuhong Zhang",
      "Jianwen Yang",
      "Zhanbiao Lian",
      "Jin Yan",
      "Zhiming Ding",
      "Shan Jiang"
    ],
    "abstract": "Generating high-quality pseudo-labels on the cloud is crucial for cloud-edge\nobject detection, especially in dynamic traffic monitoring where data\ndistributions evolve. Existing methods often assume reliable cloud models,\nneglecting potential errors or struggling with complex distribution shifts.\nThis paper proposes Cloud-Adaptive High-Quality Pseudo-label generation\n(CA-HQP), addressing these limitations by incorporating a learnable Visual\nPrompt Generator (VPG) and dual feature alignment into cloud model updates. The\nVPG enables parameter-efficient adaptation by injecting visual prompts,\nenhancing flexibility without extensive fine-tuning. CA-HQP mitigates domain\ndiscrepancies via two feature alignment techniques: global Domain Query Feature\nAlignment (DQFA) capturing scene-level shifts, and fine-grained Temporal\nInstance-Aware Feature Embedding Alignment (TIAFA) addressing instance\nvariations. Experiments on the Bellevue traffic dataset demonstrate that CA-HQP\nsignificantly improves pseudo-label quality compared to existing methods,\nleading to notable performance gains for the edge model and showcasing CA-HQP's\nadaptation effectiveness. Ablation studies validate each component (DQFA,\nTIAFA, VPG) and the synergistic effect of combined alignment strategies,\nhighlighting the importance of adaptive cloud updates and domain adaptation for\nrobust object detection in evolving scenarios. CA-HQP provides a promising\nsolution for enhancing cloud-edge object detection systems in real-world\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCNN'25",
    "pdf_url": "http://arxiv.org/pdf/2504.00526v1",
    "published_date": "2025-04-01 08:20:16 UTC",
    "updated_date": "2025-04-01 08:20:16 UTC"
  },
  {
    "arxiv_id": "2504.00521v1",
    "title": "Automated detection of atomicity violations in large-scale systems",
    "authors": [
      "Hang He",
      "Yixing Luo",
      "Chengcheng Wan",
      "Ting Su",
      "Haiying Sun",
      "Geguang Pu"
    ],
    "abstract": "Atomicity violations in interrupt-driven programs pose a significant threat\nto software safety in critical systems. These violations occur when the\nexecution sequence of operations on shared resources is disrupted by\nasynchronous interrupts. Detecting atomicity violations is challenging due to\nthe vast program state space, application-level code dependencies, and complex\ndomain-specific knowledge. We propose Clover, a hybrid framework that\nintegrates static analysis with large language model (LLM) agents to detect\natomicity violations in real-world programs. Clover first performs static\nanalysis to extract critical code snippets and operation information. It then\ninitiates a multi-agent process, where the expert agent leverages\ndomain-specific knowledge to detect atomicity violations, which are\nsubsequently validated by the judge agent. Evaluations on RaceBench 2.1,\nSV-COMP, and RWIP demonstrate that Clover achieves a precision/recall of\n92.3%/86.6%, outperforming existing approaches by 27.4-118.2% on F1-score.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00521v1",
    "published_date": "2025-04-01 08:13:29 UTC",
    "updated_date": "2025-04-01 08:13:29 UTC"
  },
  {
    "arxiv_id": "2504.00515v1",
    "title": "Training Frozen Feature Pyramid DINOv2 for Eyelid Measurements with Infinite Encoding and Orthogonal Regularization",
    "authors": [
      "Chun-Hung Chen"
    ],
    "abstract": "Accurate measurement of eyelid parameters such as Margin Reflex Distances\n(MRD1, MRD2) and Levator Function (LF) is critical in oculoplastic diagnostics\nbut remains limited by manual, inconsistent methods. This study evaluates deep\nlearning models: SE-ResNet, EfficientNet, and the vision transformer-based\nDINOv2 for automating these measurements using smartphone-acquired images. We\nassess performance across frozen and fine-tuned settings, using MSE, MAE, and\nR2 metrics. DINOv2, pretrained through self-supervised learning, demonstrates\nsuperior scalability and robustness, especially under frozen conditions ideal\nfor mobile deployment. Lightweight regressors such as MLP and Deep Ensemble\noffer high precision with minimal computational overhead. To address class\nimbalance and improve generalization, we integrate focal loss, orthogonal\nregularization, and binary encoding strategies. Our results show that DINOv2\ncombined with these enhancements delivers consistent, accurate predictions\nacross all tasks, making it a strong candidate for real-world, mobile-friendly\nclinical applications. This work highlights the potential of foundation models\nin advancing AI-powered ophthalmic care.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00515v1",
    "published_date": "2025-04-01 08:06:08 UTC",
    "updated_date": "2025-04-01 08:06:08 UTC"
  },
  {
    "arxiv_id": "2504.00513v2",
    "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
    "authors": [
      "Asma Yamani",
      "Malak Baslyman",
      "Moataz Ahmed"
    ],
    "abstract": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00513v2",
    "published_date": "2025-04-01 08:03:40 UTC",
    "updated_date": "2025-04-23 11:26:49 UTC"
  },
  {
    "arxiv_id": "2504.00510v1",
    "title": "Operator Learning with Domain Decomposition for Geometry Generalization in PDE Solving",
    "authors": [
      "Jianing Huang",
      "Kaixuan Zhang",
      "Youjia Wu",
      "Ze Cheng"
    ],
    "abstract": "Neural operators have become increasingly popular in solving \\textit{partial\ndifferential equations} (PDEs) due to their superior capability to capture\nintricate mappings between function spaces over complex domains. However, the\ndata-hungry nature of operator learning inevitably poses a bottleneck for their\nwidespread applications. At the core of the challenge lies the absence of\ntransferability of neural operators to new geometries. To tackle this issue, we\npropose operator learning with domain decomposition, a local-to-global\nframework to solve PDEs on arbitrary geometries. Under this framework, we\ndevise an iterative scheme \\textit{Schwarz Neural Inference} (SNI). This scheme\nallows for partitioning of the problem domain into smaller subdomains, on which\nlocal problems can be solved with neural operators, and stitching local\nsolutions to construct a global solution. Additionally, we provide a\ntheoretical analysis of the convergence rate and error bound. We conduct\nextensive experiments on several representative PDEs with diverse boundary\nconditions and achieve remarkable geometry generalization compared to\nalternative methods. These analysis and experiments demonstrate the proposed\nframework's potential in addressing challenges related to geometry\ngeneralization and data efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00510v1",
    "published_date": "2025-04-01 08:00:43 UTC",
    "updated_date": "2025-04-01 08:00:43 UTC"
  },
  {
    "arxiv_id": "2504.00509v2",
    "title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?",
    "authors": [
      "Kai Yan",
      "Yufei Xu",
      "Zhengyin Du",
      "Xuesong Yao",
      "Zheyu Wang",
      "Xiaowen Guo",
      "Jiecao Chen"
    ],
    "abstract": "The rapid escalation from elementary school-level to frontier problems of the\ndifficulty for LLM benchmarks in recent years have weaved a miracle for\nresearchers that we are only inches away from surpassing human intelligence.\nHowever, is the LLMs' remarkable reasoning ability indeed comes from true\nintelligence by human standards, or are they simply reciting solutions\nwitnessed during training at an Internet level? To study this problem, we\npropose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's\nrecitation behavior when asked simple reasoning problems but with conditions\nsubtly shifted, and conduct empirical analysis on our benchmark. Surprisingly,\nwe found existing cutting-edge LLMs unanimously exhibits extremely severe\nrecitation behavior; by changing one phrase in the condition, top models such\nas OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary\nschool-level arithmetic and reasoning problems. Such findings are a wake-up\ncall to the LLM community that compels us to re-evaluate the true intelligence\nlevel of cutting-edge LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 3 figures, 10 tables. V2 refines related work and\n  acknowledgement, and adds links to chat logs for qualitative studies",
    "pdf_url": "http://arxiv.org/pdf/2504.00509v2",
    "published_date": "2025-04-01 07:57:58 UTC",
    "updated_date": "2025-04-08 16:51:11 UTC"
  },
  {
    "arxiv_id": "2504.00485v3",
    "title": "Stroke Disease Classification Using Machine Learning with Feature Selection Techniques",
    "authors": [
      "Mahade Hasan",
      "Farhana Yasmin",
      "Xue Yu"
    ],
    "abstract": "Heart disease remains a leading cause of mortality and morbidity worldwide,\nnecessitating the development of accurate and reliable predictive models to\nfacilitate early detection and intervention. While state of the art work has\nfocused on various machine learning approaches for predicting heart disease,\nbut they could not able to achieve remarkable accuracy. In response to this\nneed, we applied nine machine learning algorithms XGBoost, logistic regression,\ndecision tree, random forest, k-nearest neighbors (KNN), support vector machine\n(SVM), gaussian na\\\"ive bayes (NB gaussian), adaptive boosting, and linear\nregression to predict heart disease based on a range of physiological\nindicators. Our approach involved feature selection techniques to identify the\nmost relevant predictors, aimed at refining the models to enhance both\nperformance and interpretability. The models were trained, incorporating\nprocesses such as grid search hyperparameter tuning, and cross-validation to\nminimize overfitting. Additionally, we have developed a novel voting system\nwith feature selection techniques to advance heart disease classification.\nFurthermore, we have evaluated the models using key performance metrics\nincluding accuracy, precision, recall, F1-score, and the area under the\nreceiver operating characteristic curve (ROC AUC). Among the models, XGBoost\ndemonstrated exceptional performance, achieving 99% accuracy, precision,\nF1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach\nto early heart disease diagnosis and preventive healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00485v3",
    "published_date": "2025-04-01 07:16:49 UTC",
    "updated_date": "2025-05-21 20:42:15 UTC"
  },
  {
    "arxiv_id": "2504.03736v1",
    "title": "Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators",
    "authors": [
      "Teodor Chiaburu",
      "Felix Bie√ümann",
      "Frank Hau√üer"
    ],
    "abstract": "Understanding uncertainty in Explainable AI (XAI) is crucial for building\ntrust and ensuring reliable decision-making in Machine Learning models. This\npaper introduces a unified framework for quantifying and interpreting\nUncertainty in XAI by defining a general explanation function $e_{\\theta}(x,\nf)$ that captures the propagation of uncertainty from key sources:\nperturbations in input data and model parameters. By using both analytical and\nempirical estimates of explanation variance, we provide a systematic means of\nassessing the impact uncertainty on explanations. We illustrate the approach\nusing a first-order uncertainty propagation as the analytical estimator. In a\ncomprehensive evaluation across heterogeneous datasets, we compare analytical\nand empirical estimates of uncertainty propagation and evaluate their\nrobustness. Extending previous work on inconsistencies in explanations, our\nexperiments identify XAI methods that do not reliably capture and propagate\nuncertainty. Our findings underscore the importance of uncertainty-aware\nexplanations in high-stakes applications and offer new insights into the\nlimitations of current XAI methods. The code for the experiments can be found\nin our repository at https://github.com/TeodorChiaburu/UXAI",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 10 figures, accepted at WCXAI 2025 Istanbul",
    "pdf_url": "http://arxiv.org/pdf/2504.03736v1",
    "published_date": "2025-04-01 07:06:31 UTC",
    "updated_date": "2025-04-01 07:06:31 UTC"
  },
  {
    "arxiv_id": "2504.00472v1",
    "title": "Memorizing is Not Enough: Deep Knowledge Injection Through Reasoning",
    "authors": [
      "Ruoxi Xu",
      "Yunjie Ji",
      "Boxi Cao",
      "Yaojie Lu",
      "Hongyu Lin",
      "Xianpei Han",
      "Ben He",
      "Yingfei Sun",
      "Xiangang Li",
      "Le Sun"
    ],
    "abstract": "Although large language models (LLMs) excel in knowledge recall and\nreasoning, their static nature leads to outdated information as the real world\nevolves or when adapting to domain-specific knowledge, highlighting the need\nfor effective knowledge injection. However, current research on knowledge\ninjection remains superficial, mainly focusing on knowledge memorization and\nretrieval. This paper proposes a four-tier knowledge injection framework that\nsystematically defines the levels of knowledge injection: memorization,\nretrieval, reasoning, and association. Based on this framework, we introduce\nDeepKnowledge, a synthetic experimental testbed designed for fine-grained\nevaluation of the depth of knowledge injection across three knowledge types\n(novel, incremental, and updated). We then explore various knowledge injection\nscenarios and evaluate the depth of knowledge injection for each scenario on\nthe benchmark. Experimental results reveal key factors to reach each level of\nknowledge injection for LLMs and establish a mapping between the levels of\nknowledge injection and the corresponding suitable injection methods, aiming to\nprovide a comprehensive approach for efficient knowledge injection across\nvarious levels.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00472v1",
    "published_date": "2025-04-01 06:59:59 UTC",
    "updated_date": "2025-04-01 06:59:59 UTC"
  },
  {
    "arxiv_id": "2504.00469v2",
    "title": "Learning-Based Approximate Nonlinear Model Predictive Control Motion Cueing",
    "authors": [
      "Camilo Gonzalez Arango",
      "Houshyar Asadi",
      "Mohammad Reza Chalak Qazani",
      "Chee Peng Lim"
    ],
    "abstract": "Motion Cueing Algorithms (MCAs) encode the movement of simulated vehicles\ninto movement that can be reproduced with a motion simulator to provide a\nrealistic driving experience within the capabilities of the machine. This paper\nintroduces a novel learning-based MCA for serial robot-based motion simulators.\nBuilding on the differentiable predictive control framework, the proposed\nmethod merges the advantages of Nonlinear Model Predictive Control (NMPC) -\nnotably nonlinear constraint handling and accurate kinematic modeling - with\nthe computational efficiency of machine learning. By shifting the computational\nburden to offline training, the new algorithm enables real-time operation at\nhigh control rates, thus overcoming the key challenge associated with\nNMPC-based motion cueing. The proposed MCA incorporates a nonlinear joint-space\nplant model and a policy network trained to mimic NMPC behavior while\naccounting for joint acceleration, velocity, and position limits. Simulation\nexperiments across multiple motion cueing scenarios showed that the proposed\nalgorithm performed on par with a state-of-the-art NMPC-based alternative in\nterms of motion cueing quality as quantified by the RMSE and correlation\ncoefficient with respect to reference signals. However, the proposed algorithm\nwas on average 400 times faster than the NMPC baseline. In addition, the\nalgorithm successfully generalized to unseen operating conditions, including\nmotion cueing scenarios on a different vehicle and real-time physics-based\nsimulations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00469v2",
    "published_date": "2025-04-01 06:52:30 UTC",
    "updated_date": "2025-04-09 23:09:21 UTC"
  },
  {
    "arxiv_id": "2504.00460v1",
    "title": "MetaLoRA: Tensor-Enhanced Adaptive Low-Rank Fine-tuning",
    "authors": [
      "Maolin Wang",
      "Xiangyu Zhao"
    ],
    "abstract": "There has been a significant increase in the deployment of neural network\nmodels, presenting substantial challenges in model adaptation and fine-tuning.\nEfficient adaptation is crucial in maintaining model performance across diverse\ntasks and domains. While Low-Rank Adaptation (LoRA) has emerged as a promising\nparameter-efficient fine-tuning method, its fixed parameter nature limits its\nability to handle dynamic task requirements effectively. Adapting models to new\ntasks can be challenging due to the need for extensive fine-tuning. Current\nLoRA variants primarily focus on general parameter reduction while overlooking\nthe importance of dynamic parameter adjustment and meta-learning capabilities.\nMoreover, existing approaches mainly address static adaptations, neglecting the\npotential benefits of task-aware parameter generation in handling diverse task\ndistributions. To address these limitations, this Ph.D. research proposes a\nLoRA generation approach to model task relationships and introduces MetaLoRA, a\nnovel parameter-efficient adaptation framework incorporating meta-learning\nprinciples. This work develops a comprehensive architecture that integrates\nmeta-parameter generation with adaptive low-rank decomposition, enabling\nefficient handling of both task-specific and task-agnostic features. MetaLoRA\naccurately captures task patterns by incorporating meta-learning mechanisms and\ndynamic parameter adjustment strategies. To our knowledge, this research\nrepresents the first attempt to provide a meta-learning enhanced LoRA variant,\noffering improved adaptation capability while maintaining computational\nefficiency in model fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICDE 2025 PhD Symposium Track",
    "pdf_url": "http://arxiv.org/pdf/2504.00460v1",
    "published_date": "2025-04-01 06:34:26 UTC",
    "updated_date": "2025-04-01 06:34:26 UTC"
  },
  {
    "arxiv_id": "2504.00457v3",
    "title": "Distilling Multi-view Diffusion Models into 3D Generators",
    "authors": [
      "Hao Qin",
      "Luyuan Chen",
      "Ming Kong",
      "Mengxu Lu",
      "Qiang Zhu"
    ],
    "abstract": "We introduce DD3G, a formulation that Distills a multi-view Diffusion model\n(MV-DM) into a 3D Generator using gaussian splatting. DD3G compresses and\nintegrates extensive visual and spatial geometric knowledge from the MV-DM by\nsimulating its ordinary differential equation (ODE) trajectory, ensuring the\ndistilled generator generalizes better than those trained solely on 3D data.\nUnlike previous amortized optimization approaches, we align the MV-DM and 3D\ngenerator representation spaces to transfer the teacher's probabilistic flow to\nthe student, thus avoiding inconsistencies in optimization objectives caused by\nprobabilistic sampling. The introduction of probabilistic flow and the coupling\nof various attributes in 3D Gaussians introduce challenges in the generation\nprocess. To tackle this, we propose PEPD, a generator consisting of Pattern\nExtraction and Progressive Decoding phases, which enables efficient fusion of\nprobabilistic flow and converts a single image into 3D Gaussians within 0.06\nseconds. Furthermore, to reduce knowledge loss and overcome sparse-view\nsupervision, we design a joint optimization objective that ensures the quality\nof generated samples through explicit supervision and implicit verification.\nLeveraging existing 2D generation models, we compile 120k high-quality RGBA\nimages for distillation. Experiments on synthetic and public datasets\ndemonstrate the effectiveness of our method. Our project is available at:\nhttps://qinbaigao.github.io/DD3G_project/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00457v3",
    "published_date": "2025-04-01 06:32:48 UTC",
    "updated_date": "2025-04-03 01:44:53 UTC"
  },
  {
    "arxiv_id": "2504.00441v2",
    "title": "No Free Lunch with Guardrails",
    "authors": [
      "Divyanshu Kumar",
      "Nitin Aravind Birur",
      "Tanay Baswa",
      "Sahil Agarwal",
      "Prashanth Harshangi"
    ],
    "abstract": "As large language models (LLMs) and generative AI become widely adopted,\nguardrails have emerged as a key tool to ensure their safe use. However, adding\nguardrails isn't without tradeoffs; stronger security measures can reduce\nusability, while more flexible systems may leave gaps for adversarial attacks.\nIn this work, we explore whether current guardrails effectively prevent misuse\nwhile maintaining practical utility. We introduce a framework to evaluate these\ntradeoffs, measuring how different guardrails balance risk, security, and\nusability, and build an efficient guardrail.\n  Our findings confirm that there is no free lunch with guardrails;\nstrengthening security often comes at the cost of usability. To address this,\nwe propose a blueprint for designing better guardrails that minimize risk while\nmaintaining usability. We evaluate various industry guardrails, including Azure\nContent Safety, Bedrock Guardrails, OpenAI's Moderation API, Guardrails AI,\nNemo Guardrails, and Enkrypt AI guardrails. Additionally, we assess how LLMs\nlike GPT-4o, Gemini 2.0-Flash, Claude 3.5-Sonnet, and Mistral Large-Latest\nrespond under different system prompts, including simple prompts, detailed\nprompts, and detailed prompts with chain-of-thought (CoT) reasoning. Our study\nprovides a clear comparison of how different guardrails perform, highlighting\nthe challenges in balancing security and usability.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00441v2",
    "published_date": "2025-04-01 05:46:54 UTC",
    "updated_date": "2025-04-03 13:34:57 UTC"
  },
  {
    "arxiv_id": "2504.00438v1",
    "title": "Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation",
    "authors": [
      "Lan Sun",
      "Songpengcheng Xia",
      "Jiarui Yang",
      "Ling Pei"
    ],
    "abstract": "The proliferation of wearable technology has established multi-device\necosystems comprising smartphones, smartwatches, and headphones as critical\nenablers for ubiquitous pedestrian localization. However, traditional\npedestrian dead reckoning (PDR) struggles with diverse motion modes, while\ndata-driven methods, despite improving accuracy, often lack robustness due to\ntheir reliance on a single-device setup. Therefore, a promising solution is to\nfully leverage existing wearable devices to form a flexiwear bodynet for robust\nand accurate pedestrian localization. This paper presents Suite-IN++, a deep\nlearning framework for flexiwear bodynet-based pedestrian localization.\nSuite-IN++ integrates motion data from wearable devices on different body\nparts, using contrastive learning to separate global and local motion features.\nIt fuses global features based on the data reliability of each device to\ncapture overall motion trends and employs an attention mechanism to uncover\ncross-device correlations in local features, extracting motion details helpful\nfor accurate localization. To evaluate our method, we construct a real-life\nflexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and\nAirPods) across diverse walking modes and device configurations. Experimental\nresults demonstrate that Suite-IN++ achieves superior localization accuracy and\nrobustness, significantly outperforming state-of-the-art models in real-life\npedestrian tracking scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages,10 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00438v1",
    "published_date": "2025-04-01 05:40:52 UTC",
    "updated_date": "2025-04-01 05:40:52 UTC"
  },
  {
    "arxiv_id": "2504.00428v1",
    "title": "LLM-Assisted Proactive Threat Intelligence for Automated Reasoning",
    "authors": [
      "Shuva Paul",
      "Farhad Alemi",
      "Richard Macwan"
    ],
    "abstract": "Successful defense against dynamically evolving cyber threats requires\nadvanced and sophisticated techniques. This research presents a novel approach\nto enhance real-time cybersecurity threat detection and response by integrating\nlarge language models (LLMs) and Retrieval-Augmented Generation (RAG) systems\nwith continuous threat intelligence feeds. Leveraging recent advancements in\nLLMs, specifically GPT-4o, and the innovative application of RAG techniques,\nour approach addresses the limitations of traditional static threat analysis by\nincorporating dynamic, real-time data sources. We leveraged RAG to get the\nlatest information in real-time for threat intelligence, which is not possible\nin the existing GPT-4o model. We employ the Patrowl framework to automate the\nretrieval of diverse cybersecurity threat intelligence feeds, including Common\nVulnerabilities and Exposures (CVE), Common Weakness Enumeration (CWE), Exploit\nPrediction Scoring System (EPSS), and Known Exploited Vulnerabilities (KEV)\ndatabases, and integrate these with the all-mpnet-base-v2 model for\nhigh-dimensional vector embeddings, stored and queried in Milvus. We\ndemonstrate our system's efficacy through a series of case studies, revealing\nsignificant improvements in addressing recently disclosed vulnerabilities,\nKEVs, and high-EPSS-score CVEs compared to the baseline GPT-4o. This work not\nonly advances the role of LLMs in cybersecurity but also establishes a robust\nfoundation for the development of automated intelligent cyberthreat information\nmanagement systems, addressing crucial gaps in current cybersecurity practices.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 Pages, 1 Figure",
    "pdf_url": "http://arxiv.org/pdf/2504.00428v1",
    "published_date": "2025-04-01 05:19:33 UTC",
    "updated_date": "2025-04-01 05:19:33 UTC"
  },
  {
    "arxiv_id": "2504.00424v1",
    "title": "Hawkeye:Efficient Reasoning with Model Collaboration",
    "authors": [
      "Jianshu She",
      "Zhuohao Li",
      "Zhemin Huang",
      "Qi Li",
      "Peiran Xu",
      "Haonan Li",
      "Qirong Ho"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning has demonstrated remarkable effectiveness in\nenhancing the reasoning abilities of large language models (LLMs). However, its\nefficiency remains a challenge due to the generation of excessive intermediate\nreasoning tokens, which introduce semantic redundancy and overly detailed\nreasoning steps. Moreover, computational expense and latency are significant\nconcerns, as the cost scales with the number of output tokens, including those\nintermediate steps. In this work, we observe that most CoT tokens are\nunnecessary, and retaining only a small portion of them is sufficient for\nproducing high-quality responses. Inspired by this, we propose HAWKEYE, a novel\npost-training and inference framework where a large model produces concise CoT\ninstructions to guide a smaller model in response generation. HAWKEYE\nquantifies redundancy in CoT reasoning and distills high-density information\nvia reinforcement learning. By leveraging these concise CoTs, HAWKEYE is able\nto expand responses while reducing token usage and computational cost\nsignificantly. Our evaluation shows that HAWKEYE can achieve comparable\nresponse quality using only 35% of the full CoTs, while improving clarity,\ncoherence, and conciseness by approximately 10%. Furthermore, HAWKEYE can\naccelerate end-to-end reasoning by up to 3.4x on complex math tasks while\nreducing inference cost by up to 60%. HAWKEYE will be open-sourced and the\nmodels will be available soon.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00424v1",
    "published_date": "2025-04-01 05:09:04 UTC",
    "updated_date": "2025-04-01 05:09:04 UTC"
  },
  {
    "arxiv_id": "2504.01047v1",
    "title": "Predicting Movie Production Years through Facial Recognition of Actors with Machine Learning",
    "authors": [
      "Asraa Muayed Abdalah",
      "Noor Redha Alkazaz"
    ],
    "abstract": "This study used machine learning algorithms to identify actors and extract\nthe age of actors from images taken randomly from movies. The use of images\ntaken from Arab movies includes challenges such as non-uniform lighting,\ndifferent and multiple poses for the actors and multiple elements with the\nactor or a group of actors. Additionally, the use of make-up, wigs, beards, and\nwearing different accessories and costumes made it difficult for the system to\nidentify the personality of the same actor. The Arab Actors Dataset-AAD\ncomprises 574 images sourced from various movies, encompassing both black and\nwhite as well as color compositions. The images depict complete scenes or\nfragments thereof. Multiple models were employed for feature extraction, and\ndiverse machine learning algorithms were utilized during the classification and\nprediction stages to determine the most effective algorithm for handling such\nimage types. The study demonstrated the effectiveness of the Logistic\nRegression model exhibited the best performance compared to other models in the\ntraining phase, as evidenced by its AUC, precision, CA and F1score values of\n99%, 86%, 85.5% and 84.2% respectively. The findings of this study can be used\nto improve the precision and reliability of facial recognition technology for\nvarious uses as with movies search services, movie suggestion algorithms, and\ngenre classification of movies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01047v1",
    "published_date": "2025-04-01 04:46:05 UTC",
    "updated_date": "2025-04-01 04:46:05 UTC"
  },
  {
    "arxiv_id": "2504.00414v1",
    "title": "Multimodal LLMs for OCR, OCR Post-Correction, and Named Entity Recognition in Historical Documents",
    "authors": [
      "Gavin Greif",
      "Niclas Griesshaber",
      "Robin Greif"
    ],
    "abstract": "We explore how multimodal Large Language Models (mLLMs) can help researchers\ntranscribe historical documents, extract relevant historical information, and\nconstruct datasets from historical sources. Specifically, we investigate the\ncapabilities of mLLMs in performing (1) Optical Character Recognition (OCR),\n(2) OCR Post-Correction, and (3) Named Entity Recognition (NER) tasks on a set\nof city directories published in German between 1754 and 1870. First, we\nbenchmark the off-the-shelf transcription accuracy of both mLLMs and\nconventional OCR models. We find that the best-performing mLLM model\nsignificantly outperforms conventional state-of-the-art OCR models and other\nfrontier mLLMs. Second, we are the first to introduce multimodal\npost-correction of OCR output using mLLMs. We find that this novel approach\nleads to a drastic improvement in transcription accuracy and consistently\nproduces highly accurate transcriptions (<1% CER), without any image\npre-processing or model fine-tuning. Third, we demonstrate that mLLMs can\nefficiently recognize entities in transcriptions of historical documents and\nparse them into structured dataset formats. Our findings provide early evidence\nfor the long-term potential of mLLMs to introduce a paradigm shift in the\napproaches to historical data collection and document transcription.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00414v1",
    "published_date": "2025-04-01 04:21:34 UTC",
    "updated_date": "2025-04-01 04:21:34 UTC"
  },
  {
    "arxiv_id": "2504.00409v1",
    "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding",
    "authors": [
      "Mohanakrishnan Hariharan"
    ],
    "abstract": "Large language models (LLMs) have greatly improved their capability in\nperforming NLP tasks. However, deeper semantic understanding, contextual\ncoherence, and more subtle reasoning are still difficult to obtain. The paper\ndiscusses state-of-the-art methodologies that advance LLMs with more advanced\nNLU techniques, such as semantic parsing, knowledge integration, and contextual\nreinforcement learning. We analyze the use of structured knowledge graphs,\nretrieval-augmented generation (RAG), and fine-tuning strategies that match\nmodels with human-level understanding. Furthermore, we address the\nincorporation of transformer-based architectures, contrastive learning, and\nhybrid symbolic-neural methods that address problems like hallucinations,\nambiguity, and inconsistency in the factual perspectives involved in performing\ncomplex NLP tasks, such as question-answering text summarization and dialogue\ngeneration. Our findings show the importance of semantic precision for\nenhancing AI-driven language systems and suggest future research directions to\nbridge the gap between statistical language models and true natural language\nunderstanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00409v1",
    "published_date": "2025-04-01 04:12:04 UTC",
    "updated_date": "2025-04-01 04:12:04 UTC"
  },
  {
    "arxiv_id": "2504.00408v1",
    "title": "From Intuition to Understanding: Using AI Peers to Overcome Physics Misconceptions",
    "authors": [
      "Ruben Weijers",
      "Denton Wu",
      "Hannah Betts",
      "Tamara Jacod",
      "Yuxiang Guan",
      "Vidya Sujaya",
      "Kushal Dev",
      "Toshali Goel",
      "William Delooze",
      "Reihaneh Rabbany",
      "Ying Wu",
      "Jean-Fran√ßois Godbout",
      "Kellin Pelrine"
    ],
    "abstract": "Generative AI has the potential to transform personalization and\naccessibility of education. However, it raises serious concerns about accuracy\nand helping students become independent critical thinkers. In this study, we\ndesigned a helpful AI \"Peer\" to help students correct fundamental physics\nmisconceptions related to Newtonian mechanic concepts. In contrast to\napproaches that seek near-perfect accuracy to create an authoritative AI tutor\nor teacher, we directly inform students that this AI can answer up to 40% of\nquestions incorrectly. In a randomized controlled trial with 165 students,\nthose who engaged in targeted dialogue with the AI Peer achieved post-test\nscores that were, on average, 10.5 percentage points higher - with over 20\npercentage points higher normalized gain - than a control group that discussed\nphysics history. Qualitative feedback indicated that 91% of the treatment\ngroup's AI interactions were rated as helpful. Furthermore, by comparing\nstudent performance on pre- and post-test questions about the same concept,\nalong with experts' annotations of the AI interactions, we find initial\nevidence suggesting the improvement in performance does not depend on the\ncorrectness of the AI. With further research, the AI Peer paradigm described\nhere could open new possibilities for how we learn, adapt to, and grow with AI.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00408v1",
    "published_date": "2025-04-01 04:09:13 UTC",
    "updated_date": "2025-04-01 04:09:13 UTC"
  },
  {
    "arxiv_id": "2504.00406v1",
    "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
    "authors": [
      "Jiuzhou Han",
      "Wray Buntine",
      "Ehsan Shareghi"
    ],
    "abstract": "Large language models demonstrate remarkable reasoning capabilities but often\nproduce unreliable or incorrect responses. Existing verification methods are\ntypically model-specific or domain-restricted, requiring significant\ncomputational resources and lacking scalability across diverse reasoning tasks.\nTo address these limitations, we propose VerifiAgent, a unified verification\nagent that integrates two levels of verification: meta-verification, which\nassesses completeness and consistency in model responses, and tool-based\nadaptive verification, where VerifiAgent autonomously selects appropriate\nverification tools based on the reasoning type, including mathematical,\nlogical, or commonsense reasoning. This adaptive approach ensures both\nefficiency and robustness across different verification scenarios. Experimental\nresults show that VerifiAgent outperforms baseline verification methods (e.g.,\ndeductive verifier, backward verifier) among all reasoning tasks. Additionally,\nit can further enhance reasoning accuracy by leveraging feedback from\nverification results. VerifiAgent can also be effectively applied to inference\nscaling, achieving better results with fewer generated samples and costs\ncompared to existing process reward models in the mathematical reasoning\ndomain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00406v1",
    "published_date": "2025-04-01 04:05:03 UTC",
    "updated_date": "2025-04-01 04:05:03 UTC"
  },
  {
    "arxiv_id": "2504.03735v1",
    "title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots",
    "authors": [
      "Erfan Shayegani",
      "G M Shahariar",
      "Sara Abdali",
      "Lei Yu",
      "Nael Abu-Ghazaleh",
      "Yue Dong"
    ],
    "abstract": "Multimodal Language Models (MMLMs) typically undergo post-training alignment\nto prevent harmful content generation. However, these alignment stages focus\nprimarily on the assistant role, leaving the user role unaligned, and stick to\na fixed input prompt structure of special tokens, leaving the model vulnerable\nwhen inputs deviate from these expectations. We introduce Role-Modality Attacks\n(RMA), a novel class of adversarial attacks that exploit role confusion between\nthe user and assistant and alter the position of the image token to elicit\nharmful outputs. Unlike existing attacks that modify query content, RMAs\nmanipulate the input structure without altering the query itself. We\nsystematically evaluate these attacks across multiple Vision Language Models\n(VLMs) on eight distinct settings, showing that they can be composed to create\nstronger adversarial prompts, as also evidenced by their increased projection\nin the negative refusal direction in the residual stream, a property observed\nin prior successful attacks. Finally, for mitigation, we propose an adversarial\ntraining approach that makes the model robust against input prompt\nperturbations. By training the model on a range of harmful and benign prompts\nall perturbed with different RMA settings, it loses its sensitivity to Role\nConfusion and Modality Manipulation attacks and is trained to only pay\nattention to the content of the query in the input prompt structure,\neffectively reducing Attack Success Rate (ASR) while preserving the model's\ngeneral utility.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03735v1",
    "published_date": "2025-04-01 03:54:36 UTC",
    "updated_date": "2025-04-01 03:54:36 UTC"
  },
  {
    "arxiv_id": "2504.00401v1",
    "title": "Beyond Wide-Angle Images: Unsupervised Video Portrait Correction via Spatiotemporal Diffusion Adaptation",
    "authors": [
      "Wenbo Nie",
      "Lang Nie",
      "Chunyu Lin",
      "Jingwen Chen",
      "Ke Xing",
      "Jiyuan Wang",
      "Yao Zhao"
    ],
    "abstract": "Wide-angle cameras, despite their popularity for content creation, suffer\nfrom distortion-induced facial stretching-especially at the edge of the\nlens-which degrades visual appeal. To address this issue, we propose an image\nportrait correction framework using diffusion models named ImagePD. It\nintegrates the long-range awareness of transformer and multi-step denoising of\ndiffusion models into a unified framework, achieving global structural\nrobustness and local detail refinement. Besides, considering the high cost of\nobtaining video labels, we then repurpose ImagePD for unlabeled wide-angle\nvideos (termed VideoPD), by spatiotemporal diffusion adaption with spatial\nconsistency and temporal smoothness constraints. For the former, we encourage\nthe denoised image to approximate pseudo labels following the wide-angle\ndistortion distribution pattern, while for the latter, we derive rectification\ntrajectories with backward optical flows and smooth them. Compared with\nImagePD, VideoPD maintains high-quality facial corrections in space and\nmitigates the potential temporal shakes sequentially. Finally, to establish an\nevaluation benchmark and train the framework, we establish a video portrait\ndataset with a large diversity in people number, lighting conditions, and\nbackground. Experiments demonstrate that the proposed methods outperform\nexisting solutions quantitatively and qualitatively, contributing to\nhigh-fidelity wide-angle videos with stable and natural portraits. The codes\nand dataset will be available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00401v1",
    "published_date": "2025-04-01 03:49:59 UTC",
    "updated_date": "2025-04-01 03:49:59 UTC"
  },
  {
    "arxiv_id": "2504.00389v1",
    "title": "CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation",
    "authors": [
      "Chengshuai Zhao",
      "Riccardo De Maria",
      "Tharindu Kumarage",
      "Kumar Satvik Chaudhary",
      "Garima Agrawal",
      "Yiwen Li",
      "Jongchan Park",
      "Yuli Deng",
      "Ying-Chih Chen",
      "Huan Liu"
    ],
    "abstract": "Advancements in large language models (LLMs) have enabled the development of\nintelligent educational tools that support inquiry-based learning across\ntechnical domains. In cybersecurity education, where accuracy and safety are\nparamount, systems must go beyond surface-level relevance to provide\ninformation that is both trustworthy and domain-appropriate. To address this\nchallenge, we introduce CyberBOT, a question-answering chatbot that leverages a\nretrieval-augmented generation (RAG) pipeline to incorporate contextual\ninformation from course-specific materials and validate responses using a\ndomain-specific cybersecurity ontology. The ontology serves as a structured\nreasoning layer that constrains and verifies LLM-generated answers, reducing\nthe risk of misleading or unsafe guidance. CyberBOT has been deployed in a\nlarge graduate-level course at Arizona State University (ASU), where more than\none hundred students actively engage with the system through a dedicated\nweb-based platform. Computational evaluations in lab environments highlight the\npotential capacity of CyberBOT, and a forthcoming field study will evaluate its\npedagogical impact. By integrating structured domain reasoning with modern\ngenerative capabilities, CyberBOT illustrates a promising direction for\ndeveloping reliable and curriculum-aligned AI applications in specialized\neducational contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00389v1",
    "published_date": "2025-04-01 03:19:22 UTC",
    "updated_date": "2025-04-01 03:19:22 UTC"
  },
  {
    "arxiv_id": "2504.00374v1",
    "title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)",
    "authors": [
      "Mahak Agarwal",
      "Divyam Khanna"
    ],
    "abstract": "In many real-world scenarios, a single Large Language Model (LLM) may\nencounter contradictory claims-some accurate, others forcefully incorrect-and\nmust judge which is true. We investigate this risk in a single-turn,\nmulti-agent debate framework: one LLM-based agent provides a factual answer\nfrom TruthfulQA, another vigorously defends a falsehood, and the same LLM\narchitecture serves as judge. We introduce the Confidence-Weighted Persuasion\nOverride Rate (CW-POR), which captures not only how often the judge is deceived\nbut also how strongly it believes the incorrect choice. Our experiments on five\nopen-source LLMs (3B-14B parameters), where we systematically vary agent\nverbosity (30-300 words), reveal that even smaller models can craft persuasive\narguments that override truthful answers-often with high confidence. These\nfindings underscore the importance of robust calibration and adversarial\ntesting to prevent LLMs from confidently endorsing misinformation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.2.6"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.00374v1",
    "published_date": "2025-04-01 02:45:02 UTC",
    "updated_date": "2025-04-01 02:45:02 UTC"
  },
  {
    "arxiv_id": "2504.00356v1",
    "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation",
    "authors": [
      "Ting Liu",
      "Siyuan Li"
    ],
    "abstract": "Recent advances in zero-shot referring image segmentation (RIS), driven by\nmodels such as the Segment Anything Model (SAM) and CLIP, have made substantial\nprogress in aligning visual and textual information. Despite these successes,\nthe extraction of precise and high-quality mask region representations remains\na critical challenge, limiting the full potential of RIS tasks. In this paper,\nwe introduce a training-free, hybrid global-local feature extraction approach\nthat integrates detailed mask-specific features with contextual information\nfrom the surrounding area, enhancing mask region representation. To further\nstrengthen alignment between mask regions and referring expressions, we propose\na spatial guidance augmentation strategy that improves spatial coherence, which\nis essential for accurately localizing described areas. By incorporating\nmultiple spatial cues, this approach facilitates more robust and precise\nreferring segmentation. Extensive experiments on standard RIS benchmarks\ndemonstrate that our method significantly outperforms existing zero-shot RIS\nmodels, achieving substantial performance gains. We believe our approach\nadvances RIS tasks and establishes a versatile framework for region-text\nalignment, offering broader implications for cross-modal understanding and\ninteraction. Code is available at https://github.com/fhgyuanshen/HybridGL .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "accepted to CVPR2025",
    "pdf_url": "http://arxiv.org/pdf/2504.00356v1",
    "published_date": "2025-04-01 02:13:39 UTC",
    "updated_date": "2025-04-01 02:13:39 UTC"
  },
  {
    "arxiv_id": "2504.03734v1",
    "title": "Artificial Geographically Weighted Neural Network: A Novel Framework for Spatial Analysis with Geographically Weighted Layers",
    "authors": [
      "Jianfei Cao",
      "Dongchao Wang"
    ],
    "abstract": "Geographically Weighted Regression (GWR) is a widely recognized technique for\nmodeling spatial heterogeneity. However, it is commonly assumed that the\nrelationships between dependent and independent variables are linear. To\novercome this limitation, we propose an Artificial Geographically Weighted\nNeural Network (AGWNN), a novel framework that integrates geographically\nweighted techniques with neural networks to capture complex nonlinear spatial\nrelationships. Central to this framework is the Geographically Weighted Layer\n(GWL), a specialized component designed to encode spatial heterogeneity within\nthe neural network architecture. To rigorously evaluate the performance of\nAGWNN, we conducted comprehensive experiments using both simulated datasets and\nreal-world case studies. Our results demonstrate that AGWNN significantly\noutperforms traditional GWR and standard Artificial Neural Networks (ANNs) in\nterms of model fitting accuracy. Notably, AGWNN excels in modeling intricate\nnonlinear relationships and effectively identifies complex spatial\nheterogeneity patterns, offering a robust and versatile tool for advanced\nspatial analysis.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03734v1",
    "published_date": "2025-04-01 01:48:46 UTC",
    "updated_date": "2025-04-01 01:48:46 UTC"
  },
  {
    "arxiv_id": "2504.00341v1",
    "title": "Integrated LLM-Based Intrusion Detection with Secure Slicing xApp for Securing O-RAN-Enabled Wireless Network Deployments",
    "authors": [
      "Joshua Moore",
      "Aly Sabri Abdalla",
      "Prabesh Khanal",
      "Vuk Marojevic"
    ],
    "abstract": "The Open Radio Access Network (O-RAN) architecture is reshaping\ntelecommunications by promoting openness, flexibility, and intelligent\nclosed-loop optimization. By decoupling hardware and software and enabling\nmulti-vendor deployments, O-RAN reduces costs, enhances performance, and allows\nrapid adaptation to new technologies. A key innovation is intelligent network\nslicing, which partitions networks into isolated slices tailored for specific\nuse cases or quality of service requirements. The RAN Intelligent Controller\nfurther optimizes resource allocation, ensuring efficient utilization and\nimproved service quality for user equipment (UEs). However, the modular and\ndynamic nature of O-RAN expands the threat surface, necessitating advanced\nsecurity measures to maintain network integrity, confidentiality, and\navailability. Intrusion detection systems have become essential for identifying\nand mitigating attacks. This research explores using large language models\n(LLMs) to generate security recommendations based on the temporal traffic\npatterns of connected UEs. The paper introduces an LLM-driven intrusion\ndetection framework and demonstrates its efficacy through experimental\ndeployments, comparing non fine-tuned and fine-tuned models for task-specific\naccuracy.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CR",
    "comment": "This article has been accepted for publication in the IEEE 2025\n  International Conference on Communications (ICC2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.00341v1",
    "published_date": "2025-04-01 01:45:07 UTC",
    "updated_date": "2025-04-01 01:45:07 UTC"
  },
  {
    "arxiv_id": "2504.00339v1",
    "title": "VNJPTranslate: A comprehensive pipeline for Vietnamese-Japanese translation",
    "authors": [
      "Hoang Hai Phan",
      "Nguyen Duc Minh Vu",
      "Nam Dang Phuong"
    ],
    "abstract": "Neural Machine Translation (NMT) driven by Transformer architectures has\nadvanced significantly, yet faces challenges with low-resource language pairs\nlike Vietnamese-Japanese (Vi-Ja). Issues include sparse parallel data and\nhandling linguistic/cultural nuances. Recent progress in Large Language Models\n(LLMs) with strong reasoning, often refined via Reinforcement Learning (RL),\nenables high-quality synthetic data generation. We introduce VNJPTranslate, a\npipeline designed to systematically address the Vi-Ja translation task. It\nfeatures a targeted data augmentation strategy using advanced LLMs with\nChain-of-Thought prompting for challenging segments identified via corpus\nanalysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth\nwith QLoRA) on a capable, low-parameter autoregressive model (specifically, a\nfine-tuned version of the 1.8B parameter Sailor model, which is based on the\nQwen architecture) to create a practical and high-performing translation\nsystem. This integrated approach aims to improve Vi-Ja translation quality\nsignificantly over existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00339v1",
    "published_date": "2025-04-01 01:38:25 UTC",
    "updated_date": "2025-04-01 01:38:25 UTC"
  },
  {
    "arxiv_id": "2504.00338v1",
    "title": "Agentic Multimodal AI for Hyperpersonalized B2B and B2C Advertising in Competitive Markets: An AI-Driven Competitive Advertising Framework",
    "authors": [
      "Sakhinana Sagar Srinivas",
      "Akash Das",
      "Shivam Gupta",
      "Venkataramana Runkana"
    ],
    "abstract": "The growing use of foundation models (FMs) in real-world applications demands\nadaptive, reliable, and efficient strategies for dynamic markets. In the\nchemical industry, AI-discovered materials drive innovation, but commercial\nsuccess hinges on market adoption, requiring FM-driven advertising frameworks\nthat operate in-the-wild. We present a multilingual, multimodal AI framework\nfor autonomous, hyper-personalized advertising in B2B and B2C markets. By\nintegrating retrieval-augmented generation (RAG), multimodal reasoning, and\nadaptive persona-based targeting, our system generates culturally relevant,\nmarket-aware ads tailored to shifting consumer behaviors and competition.\nValidation combines real-world product experiments with a Simulated Humanistic\nColony of Agents to model consumer personas, optimize strategies at scale, and\nensure privacy compliance. Synthetic experiments mirror real-world scenarios,\nenabling cost-effective testing of ad strategies without risky A/B tests.\nCombining structured retrieval-augmented reasoning with in-context learning\n(ICL), the framework boosts engagement, prevents market cannibalization, and\nmaximizes ROAS. This work bridges AI-driven innovation and market adoption,\nadvancing multimodal FM deployment for high-stakes decision-making in\ncommercial marketing.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00338v1",
    "published_date": "2025-04-01 01:37:02 UTC",
    "updated_date": "2025-04-01 01:37:02 UTC"
  },
  {
    "arxiv_id": "2504.00336v2",
    "title": "SeizureTransformer: Scaling U-Net with Transformer for Simultaneous Time-Step Level Seizure Detection from Long EEG Recordings",
    "authors": [
      "Kerui Wu",
      "Ziyue Zhao",
      "B√ºlent Yener"
    ],
    "abstract": "Epilepsy is a common neurological disorder that affects around 65 million\npeople worldwide. Detecting seizures quickly and accurately is vital, given the\nprevalence and severity of the associated complications. Recently, deep\nlearning-based automated seizure detection methods have emerged as solutions;\nhowever, most existing methods require extensive post-processing and do not\neffectively handle the crucial long-range patterns in EEG data. In this work,\nwe propose SeizureTransformer, a simple model comprised of (i) a deep encoder\ncomprising 1D convolutions (ii) a residual CNN stack and a transformer encoder\nto embed previous output into high-level representation with contextual\ninformation, and (iii) streamlined decoder which converts these features into a\nsequence of probabilities, directly indicating the presence or absence of\nseizures at every time step. Extensive experiments on public and private EEG\nseizure detection datasets demonstrate that our model significantly outperforms\nexisting approaches (ranked in the first place in the 2025 \"seizure detection\nchallenge\" organized in the International Conference on Artificial Intelligence\nin Epilepsy and Other Neurological Disorders), underscoring its potential for\nreal-time, precise seizure detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00336v2",
    "published_date": "2025-04-01 01:33:42 UTC",
    "updated_date": "2025-04-02 16:23:11 UTC"
  },
  {
    "arxiv_id": "2504.03733v1",
    "title": "Artificial Intelligence and Deep Learning Algorithms for Epigenetic Sequence Analysis: A Review for Epigeneticists and AI Experts",
    "authors": [
      "Muhammad Tahir",
      "Mahboobeh Norouzi",
      "Shehroz S. Khan",
      "James R. Davie",
      "Soichiro Yamanaka",
      "Ahmed Ashraf"
    ],
    "abstract": "Epigenetics encompasses mechanisms that can alter the expression of genes\nwithout changing the underlying genetic sequence. The epigenetic regulation of\ngene expression is initiated and sustained by several mechanisms such as DNA\nmethylation, histone modifications, chromatin conformation, and non-coding RNA.\nThe changes in gene regulation and expression can manifest in the form of\nvarious diseases and disorders such as cancer and congenital deformities. Over\nthe last few decades, high throughput experimental approaches have been used to\nidentify and understand epigenetic changes, but these laboratory experimental\napproaches and biochemical processes are time-consuming and expensive. To\novercome these challenges, machine learning and artificial intelligence (AI)\napproaches have been extensively used for mapping epigenetic modifications to\ntheir phenotypic manifestations. In this paper we provide a narrative review of\npublished research on AI models trained on epigenomic data to address a variety\nof problems such as prediction of disease markers, gene expression, enhancer\npromoter interaction, and chromatin states. The purpose of this review is\ntwofold as it is addressed to both AI experts and epigeneticists. For AI\nresearchers, we provided a taxonomy of epigenetics research problems that can\nbenefit from an AI-based approach. For epigeneticists, given each of the above\nproblems we provide a list of candidate AI solutions in the literature. We have\nalso identified several gaps in the literature, research challenges, and\nrecommendations to address these challenges.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.03733v1",
    "published_date": "2025-04-01 01:02:34 UTC",
    "updated_date": "2025-04-01 01:02:34 UTC"
  },
  {
    "arxiv_id": "2504.00310v1",
    "title": "Detecting and Mitigating Bias in LLMs through Knowledge Graph-Augmented Training",
    "authors": [
      "Rajeev Kumar",
      "Harishankar Kumar",
      "Kumari Shalini"
    ],
    "abstract": "Large language models have revolutionized natural language processing with\ntheir surprising capability to understand and generate human-like text.\nHowever, many of these models inherit and further amplify the biases present in\ntheir training data, raising ethical and fairness concerns. The detection and\nmitigation of such biases are vital to ensuring that LLMs act responsibly and\nequitably across diverse domains. This work investigates Knowledge\nGraph-Augmented Training (KGAT) as a novel method to mitigate bias in LLM.\nUsing structured domain-specific knowledge from real-world knowledge graphs, we\nimprove the understanding of the model and reduce biased output. Public\ndatasets for bias assessment include Gender Shades, Bias in Bios, and FairFace,\nwhile metrics such as demographic parity and equal opportunity facilitate\nrigorous detection. We also performed targeted mitigation strategies to correct\nbiased associations, leading to a significant drop in biased output and\nimproved bias metrics. Equipped with real-world datasets and knowledge graphs,\nour framework is both scalable and effective, paving the way toward responsible\ndeployment in sensitive and high-stakes applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00310v1",
    "published_date": "2025-04-01 00:27:50 UTC",
    "updated_date": "2025-04-01 00:27:50 UTC"
  },
  {
    "arxiv_id": "2504.00308v1",
    "title": "FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization",
    "authors": [
      "Haonan Wang",
      "Zeli Liu",
      "Kajimusugura Hoshino",
      "Tuo Zhang",
      "John Paul Walters",
      "Stephen Crago"
    ],
    "abstract": "Federated Learning (FL) enables distributed training on edge devices but\nfaces significant challenges due to resource constraints in edge environments,\nimpacting both communication and computational efficiency. Existing iterative\npruning techniques improve communication efficiency but are limited by their\ncentralized design, which struggles with FL's decentralized and data-imbalanced\nnature, resulting in suboptimal sparsity levels. To address these issues, we\npropose FedPaI, a novel efficient FL framework that leverages Pruning at\nInitialization (PaI) to achieve extreme sparsity. FedPaI identifies optimal\nsparse connections at an early stage, maximizing model capacity and\nsignificantly reducing communication and computation overhead by fixing\nsparsity patterns at the start of training. To adapt to diverse hardware and\nsoftware environments, FedPaI supports both structured and unstructured\npruning. Additionally, we introduce personalized client-side pruning mechanisms\nfor improved learning capacity and sparsity-aware server-side aggregation for\nenhanced efficiency. Experimental results demonstrate that FedPaI consistently\noutperforms existing efficient FL that applies conventional iterative pruning\nwith significant leading in efficiency and model accuracy. For the first time,\nour proposed FedPaI achieves an extreme sparsity level of up to 98% without\ncompromising the model accuracy compared to unpruned baselines, even under\nchallenging non-IID settings. By employing our FedPaI with joint optimization\nof model learning capacity and sparsity, FL applications can benefit from\nfaster convergence and accelerate the training by 6.4 to 7.9 times.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00308v1",
    "published_date": "2025-04-01 00:24:34 UTC",
    "updated_date": "2025-04-01 00:24:34 UTC"
  },
  {
    "arxiv_id": "2504.01995v2",
    "title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
    "authors": [
      "Hamed Mahdavi",
      "Alireza Hashemi",
      "Majid Daliri",
      "Pegah Mohammadipour",
      "Alireza Farhadi",
      "Samira Malek",
      "Yekta Yazdanifard",
      "Amir Khasahmadi",
      "Vasant Honavar"
    ],
    "abstract": "Recent advances in large language models (LLMs) have shown impressive\nprogress in mathematical reasoning tasks. However, current evaluation\nbenchmarks predominantly focus on the accuracy of final answers, often\noverlooking the crucial logical rigor for mathematical problem solving. The\nclaim that state-of-the-art LLMs can solve Math Olympiad-level problems\nrequires closer examination. To explore this, we conducted both qualitative and\nquantitative human evaluations of proofs generated by LLMs, and developed a\nschema for automatically assessing their reasoning capabilities. Our study\nreveals that current LLMs fall significantly short of solving challenging\nOlympiad-level problems and frequently fail to distinguish correct mathematical\nreasoning from clearly flawed solutions. Our analyses demonstrate that the\noccasional correct final answers provided by LLMs often result from pattern\nrecognition or heuristic shortcuts rather than genuine mathematical reasoning.\nThese findings underscore the substantial gap between LLM performance and human\nexpertise in advanced mathematical reasoning and highlight the importance of\ndeveloping benchmarks that prioritize the soundness of the reasoning used to\narrive at an answer rather than the mere correctness of the final answers.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.01995v2",
    "published_date": "2025-04-01 00:10:10 UTC",
    "updated_date": "2025-04-10 20:43:23 UTC"
  },
  {
    "arxiv_id": "2504.00299v1",
    "title": "Collaborative LLM Numerical Reasoning with Local Data Protection",
    "authors": [
      "Min Zhang",
      "Yuzhe Lu",
      "Yun Zhou",
      "Panpan Xu",
      "Lin Lee Cheong",
      "Chang-Tien Lu",
      "Haozhu Wang"
    ],
    "abstract": "Numerical reasoning over documents, which demands both contextual\nunderstanding and logical inference, is challenging for low-capacity local\nmodels deployed on computation-constrained devices. Although such complex\nreasoning queries could be routed to powerful remote models like GPT-4,\nexposing local data raises significant data leakage concerns. Existing\nmitigation methods generate problem descriptions or examples for remote\nassistance. However, the inherent complexity of numerical reasoning hinders the\nlocal model from generating logically equivalent queries and accurately\ninferring answers with remote guidance. In this paper, we present a model\ncollaboration framework with two key innovations: (1) a context-aware synthesis\nstrategy that shifts the query domains while preserving logical consistency;\nand (2) a tool-based answer reconstruction approach that reuses the\nremote-generated problem-solving pattern with code snippets. Experimental\nresults demonstrate that our method achieves better reasoning accuracy than\nsolely using local models while providing stronger data protection than fully\nrelying on remote models. Furthermore, our method improves accuracy by 16.2% -\n43.6% while reducing data leakage by 2.3% - 44.6% compared to existing data\nprotection approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.00299v1",
    "published_date": "2025-04-01 00:02:25 UTC",
    "updated_date": "2025-04-01 00:02:25 UTC"
  }
]
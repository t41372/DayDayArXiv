[
  {
    "arxiv_id": "2409.13120v1",
    "title": "Are Large Language Models Good Essay Graders?",
    "authors": [
      "Anindita Kundu",
      "Denilson Barbosa"
    ],
    "abstract": "We evaluate the effectiveness of Large Language Models (LLMs) in assessing\nessay quality, focusing on their alignment with human grading. More precisely,\nwe evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a\ncrucial natural language processing (NLP) application in Education. We consider\nboth zero-shot and few-shot learning and different prompting approaches. We\ncompare the numeric grade provided by the LLMs to human rater-provided scores\nutilizing the ASAP dataset, a well-known benchmark for the AES task. Our\nresearch reveals that both LLMs generally assign lower scores compared to those\nprovided by the human raters; moreover, those scores do not correlate well with\nthose provided by the humans. In particular, ChatGPT tends to be harsher and\nfurther misaligned with human evaluations than Llama. We also experiment with a\nnumber of essay features commonly used by previous AES methods, related to\nlength, usage of connectives and transition words, and readability metrics,\nincluding the number of spelling and grammar mistakes. We find that, generally,\nnone of these features correlates strongly with human or LLM scores. Finally,\nwe report results on Llama 3, which are generally better across the board, as\nexpected. Overall, while LLMs do not seem an adequate replacement for human\ngrading, our results are somewhat encouraging for their use as a tool to assist\nhumans in the grading of written essays in the future.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13120v1",
    "published_date": "2024-09-19 23:20:49 UTC",
    "updated_date": "2024-09-19 23:20:49 UTC"
  },
  {
    "arxiv_id": "2409.15374v2",
    "title": "Explainable AI for Autism Diagnosis: Identifying Critical Brain Regions Using fMRI Data",
    "authors": [
      "Suryansh Vidya",
      "Kush Gupta",
      "Amir Aly",
      "Andy Wills",
      "Emmanuel Ifeachor",
      "Rohit Shankar"
    ],
    "abstract": "Early diagnosis and intervention for Autism Spectrum Disorder (ASD) has been\nshown to significantly improve the quality of life of autistic individuals.\nHowever, diagnostics methods for ASD rely on assessments based on clinical\npresentation that are prone to bias and can be challenging to arrive at an\nearly diagnosis. There is a need for objective biomarkers of ASD which can help\nimprove diagnostic accuracy. Deep learning (DL) has achieved outstanding\nperformance in diagnosing diseases and conditions from medical imaging data.\nExtensive research has been conducted on creating models that classify ASD\nusing resting-state functional Magnetic Resonance Imaging (fMRI) data. However,\nexisting models lack interpretability. This research aims to improve the\naccuracy and interpretability of ASD diagnosis by creating a DL model that can\nnot only accurately classify ASD but also provide explainable insights into its\nworking. The dataset used is a preprocessed version of the Autism Brain Imaging\nData Exchange (ABIDE) with 884 samples. Our findings show a model that can\naccurately classify ASD and highlight critical brain regions differing between\nASD and typical controls, with potential implications for early diagnosis and\nunderstanding of the neural basis of ASD. These findings are validated by\nstudies in the literature that use different datasets and modalities,\nconfirming that the model actually learned characteristics of ASD and not just\nthe dataset. This study advances the field of explainable AI in medical imaging\nby providing a robust and interpretable model, thereby contributing to a future\nwith objective and reliable ASD diagnostics.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.15374v2",
    "published_date": "2024-09-19 23:08:09 UTC",
    "updated_date": "2025-03-04 00:46:19 UTC"
  },
  {
    "arxiv_id": "2409.13115v1",
    "title": "Personalized 2D Binary Patient Codes of Tissue Images and Immunogenomic Data Through Multimodal Self-Supervised Fusion",
    "authors": [
      "Areej Alsaafin",
      "Abubakr Shafique",
      "Saghir Alfasly",
      "H. R. Tizhoosh"
    ],
    "abstract": "The field of medical diagnostics has witnessed a transformative convergence\nof artificial intelligence (AI) and healthcare data, offering promising avenues\nfor enhancing patient care and disease comprehension. However, this integration\nof multimodal data, specifically histopathology whole slide images (WSIs) and\ngenetic sequencing data, presents unique challenges due to modality disparities\nand the need for scalable computational solutions. This paper addresses the\nscarcity of multimodal solutions, primarily centered around unimodal data\nsolutions, thus limiting the realization of the rich insights that can be\nderived from integrating images and genomic data. Here, we introduce MarbliX\n``Multimodal Association and Retrieval with Binary Latent Indexed matriX,'' an\ninnovative multimodal framework that integrates histopathology images with\nimmunogenomic sequencing data, encapsulating them into a concise binary patient\ncode, referred to as ``monogram.'' This binary representation facilitates the\nestablishment of a comprehensive archive, enabling clinicians to match similar\ncases. The experimental results demonstrate the potential of MarbliX to empower\nhealthcare professionals with in-depth insights, leading to more precise\ndiagnoses, reduced variability, and expanded personalized treatment options,\nparticularly in the context of cancer.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13115v1",
    "published_date": "2024-09-19 22:49:27 UTC",
    "updated_date": "2024-09-19 22:49:27 UTC"
  },
  {
    "arxiv_id": "2409.13112v2",
    "title": "Analyzing mixed construction and demolition waste in material recovery facilities: evolution, challenges, and applications of computer vision and deep learning",
    "authors": [
      "Adrian Langley",
      "Matthew Lonergan",
      "Tao Huang",
      "Mostafa Rahimi Azghadi"
    ],
    "abstract": "Improving the automatic and timely recognition of construction and demolition\nwaste composition is crucial for enhancing business returns, economic outcomes\nand sustainability. While deep learning models show promise in recognizing and\nclassifying homogenous materials, the current literature lacks research\nassessing their performance for mixed, contaminated material in commercial\nmaterial recycling facility settings. Despite the increasing numbers of deep\nlearning models and datasets generated in this area, the sub-domain of deep\nlearning analysis of construction and demolition waste piles remains\nunderexplored. To address this gap, recent deep learning algorithms and\ntechniques were explored. This review examines the progression in datasets,\nsensors and the evolution from object detection towards real-time segmentation\nmodels. It also synthesizes research from the past five years on deep learning\nfor construction and demolition waste management, highlighting recent\nadvancements while acknowledging limitations that hinder widespread commercial\nadoption. The analysis underscores the critical requirement for diverse and\nhigh-fidelity datasets, advanced sensor technologies, and robust algorithmic\nframeworks to facilitate the effective integration of deep learning\nmethodologies into construction and demolition waste management systems. This\nintegration is envisioned to contribute significantly towards the advancement\nof a more sustainable and circular economic model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13112v2",
    "published_date": "2024-09-19 22:38:26 UTC",
    "updated_date": "2025-03-03 20:48:28 UTC"
  },
  {
    "arxiv_id": "2409.17172v1",
    "title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning",
    "authors": [
      "Shashidhar Reddy Javaji",
      "Zining Zhu"
    ],
    "abstract": "Large language models (LLMs) can store a massive amount of knowledge, yet\ntheir potential to acquire new knowledge remains unknown. We propose a novel\nevaluation framework that evaluates this capability. This framework prompts\nLLMs to generate questions about a statement introducing scientific knowledge,\nsimulating a curious person when facing the statement for the first time. We\nscore the qualities of the generated questions, thereby evaluating the\nknowledge acquisition potential of the LLM. We apply controlled ablation\nstudies to validate our scoring procedures. Additionally, we created a\nsynthetic dataset consisting of 1101 statements in physics, chemistry, and\nmaths with distinct levels of difficulties, 300 general knowledge statements,\nand 567 incorrect statements. Human evaluations were conducted to validate our\nmodel assessments, achieving an approximate weighted Cohen's kappa of 0.7 on\nall three metrics considered. We find that while large models like GPT-4 and\nMistral 8x7b are adept at generating coherent and relevant questions, the\nsmaller Phi-2 model is equally or more effective. This indicates that size does\nnot solely determine a model's knowledge acquisition potential. The proposed\nframework quantifies a critical model capability that was commonly overlooked\nand opens up research opportunities for developing more knowledgeable AI\nsystems",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.17172v1",
    "published_date": "2024-09-19 22:12:16 UTC",
    "updated_date": "2024-09-19 22:12:16 UTC"
  },
  {
    "arxiv_id": "2409.13104v2",
    "title": "ERIC: Estimating Rainfall with Commodity Doorbell Camera for Precision Residential Irrigation",
    "authors": [
      "Tian Liu",
      "Liuyi Jin",
      "Radu Stoleru",
      "Amran Haroon",
      "Charles Swanson",
      "Kexin Feng"
    ],
    "abstract": "Current state-of-the-art residential irrigation systems, such as WaterMyYard,\nrely on rainfall data from nearby weather stations to adjust irrigation\namounts. However, the accuracy of rainfall data is compromised by the limited\nspatial resolution of rain gauges and the significant variability of hyperlocal\nrainfall, leading to substantial water waste. To improve irrigation efficiency,\nwe developed a cost-effective irrigation system, dubbed ERIC, which employs\nmachine learning models to estimate rainfall from commodity doorbell camera\nfootage and optimizes irrigation schedules without human intervention.\nSpecifically, we: a) designed novel visual and audio features with lightweight\nneural network models to infer rainfall from the camera at the edge, preserving\nuser privacy; b) built a complete end-to-end irrigation system on Raspberry Pi\n4, costing only \\$75. We deployed the system across five locations (collecting\nover 750 hours of video) with varying backgrounds and light conditions.\nComprehensive evaluation validates that ERIC achieves state-of-the-art rainfall\nestimation performance ($\\sim$ 5mm/day), saving 9,112 gallons/month of water,\ntranslating to \\$28.56/month in utility savings. Data and code are available at\nhttps://github.com/LENSS/ERIC-BuildSys2024.git",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "BuildSys 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13104v2",
    "published_date": "2024-09-19 22:11:08 UTC",
    "updated_date": "2024-10-03 22:06:25 UTC"
  },
  {
    "arxiv_id": "2409.17171v2",
    "title": "Cross-Domain Content Generation with Domain-Specific Small Language Models",
    "authors": [
      "Ankit Maloo",
      "Abhinav Garg"
    ],
    "abstract": "Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17171v2",
    "published_date": "2024-09-19 21:45:13 UTC",
    "updated_date": "2024-10-02 10:28:02 UTC"
  },
  {
    "arxiv_id": "2409.13094v2",
    "title": "DenoMamba: A fused state-space model for low-dose CT denoising",
    "authors": [
      "Şaban Öztürk",
      "Oğuz Can Duran",
      "Tolga Çukur"
    ],
    "abstract": "Low-dose computed tomography (LDCT) lower potential risks linked to radiation\nexposure while relying on advanced denoising algorithms to maintain diagnostic\nquality in reconstructed images. The reigning paradigm in LDCT denoising is\nbased on neural network models that learn data-driven image priors to separate\nnoise evoked by dose reduction from underlying tissue signals. Naturally, the\nfidelity of these priors depend on the model's ability to capture the broad\nrange of contextual features evident in CT images. Earlier convolutional neural\nnetworks (CNN) are highly adept at efficiently capturing short-range spatial\ncontext, but their limited receptive fields reduce sensitivity to interactions\nover longer distances. Although transformers based on self-attention mechanisms\nhave recently been posed to increase sensitivity to long-range context, they\ncan suffer from suboptimal performance and efficiency due to elevated model\ncomplexity, particularly for high-resolution CT images. For high-quality\nrestoration of LDCT images, here we introduce DenoMamba, a novel denoising\nmethod based on state-space modeling (SSM), that efficiently captures short-\nand long-range context in medical images. Following an hourglass architecture\nwith encoder-decoder stages, DenoMamba employs a spatial SSM module to encode\nspatial context and a novel channel SSM module equipped with a secondary gated\nconvolution network to encode latent features of channel context at each stage.\nFeature maps from the two modules are then consolidated with low-level input\nfeatures via a convolution fusion module (CFM). Comprehensive experiments on\nLDCT datasets with 25\\% and 10\\% dose reduction demonstrate that DenoMamba\noutperforms state-of-the-art denoisers with average improvements of 1.4dB PSNR,\n1.1% SSIM, and 1.6% RMSE in recovered image quality.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13094v2",
    "published_date": "2024-09-19 21:32:07 UTC",
    "updated_date": "2024-12-15 16:11:59 UTC"
  },
  {
    "arxiv_id": "2409.13093v1",
    "title": "Guided Profile Generation Improves Personalization with LLMs",
    "authors": [
      "Jiarui Zhang"
    ],
    "abstract": "In modern commercial systems, including Recommendation, Ranking, and\nE-Commerce platforms, there is a trend towards improving customer experiences\nby incorporating Personalization context as input into Large Language Models\n(LLMs). However, LLMs often struggle to effectively parse and utilize sparse\nand complex personal context without additional processing or contextual\nenrichment, underscoring the need for more sophisticated context understanding\nmechanisms. In this work, we propose Guided Profile Generation (GPG), a general\nmethod designed to generate personal profiles in natural language. As is\nobserved, intermediate guided profile generation enables LLMs to summarize, and\nextract the important, distinctive features from the personal context into\nconcise, descriptive sentences, precisely tailoring their generation more\nclosely to an individual's unique habits and preferences. Our experimental\nresults show that GPG improves LLM's personalization ability across different\ntasks, for example, it increases 37% accuracy in predicting personal preference\ncompared to directly feeding the LLMs with raw personal context.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2409.13093v1",
    "published_date": "2024-09-19 21:29:56 UTC",
    "updated_date": "2024-09-19 21:29:56 UTC"
  },
  {
    "arxiv_id": "2409.13091v1",
    "title": "Interpretable Action Recognition on Hard to Classify Actions",
    "authors": [
      "Anastasia Anichenko",
      "Frank Guerin",
      "Andrew Gilbert"
    ],
    "abstract": "We investigate a human-like interpretable model of video understanding.\nHumans recognise complex activities in video by recognising critical\nspatio-temporal relations among explicitly recognised objects and parts, for\nexample, an object entering the aperture of a container. To mimic this we build\non a model which uses positions of objects and hands, and their motions, to\nrecognise the activity taking place. To improve this model we focussed on three\nof the most confused classes (for this model) and identified that the lack of\n3D information was the major problem. To address this we extended our basic\nmodel by adding 3D awareness in two ways: (1) A state-of-the-art object\ndetection model was fine-tuned to determine the difference between \"Container\"\nand \"NotContainer\" in order to integrate object shape information into the\nexisting object features. (2) A state-of-the-art depth estimation model was\nused to extract depth values for individual objects and calculate depth\nrelations to expand the existing relations used our interpretable model. These\n3D extensions to our basic model were evaluated on a subset of three\nsuperficially similar \"Putting\" actions from the Something-Something-v2\ndataset. The results showed that the container detector did not improve\nperformance, but the addition of depth relations made a significant improvement\nto performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, This manuscript has been accepted at the Human-inspired\n  Computer Vision (HCV) ECCV 2024 Workshop. arXiv admin note: text overlap with\n  arXiv:2107.05319",
    "pdf_url": "http://arxiv.org/pdf/2409.13091v1",
    "published_date": "2024-09-19 21:23:44 UTC",
    "updated_date": "2024-09-19 21:23:44 UTC"
  },
  {
    "arxiv_id": "2409.13083v1",
    "title": "FedAT: Federated Adversarial Training for Distributed Insider Threat Detection",
    "authors": [
      "R G Gayathri",
      "Atul Sajjanhar",
      "Md Palash Uddin",
      "Yong Xiang"
    ],
    "abstract": "Insider threats usually occur from within the workplace, where the attacker\nis an entity closely associated with the organization. The sequence of actions\nthe entities take on the resources to which they have access rights allows us\nto identify the insiders. Insider Threat Detection (ITD) using Machine Learning\n(ML)-based approaches gained attention in the last few years. However, most\ntechniques employed centralized ML methods to perform such an ITD.\nOrganizations operating from multiple locations cannot contribute to the\ncentralized models as the data is generated from various locations. In\nparticular, the user behavior data, which is the primary source of ITD, cannot\nbe shared among the locations due to privacy concerns. Additionally, the data\ndistributed across various locations result in extreme class imbalance due to\nthe rarity of attacks. Federated Learning (FL), a distributed data modeling\nparadigm, gained much interest recently. However, FL-enabled ITD is not yet\nexplored, and it still needs research to study the significant issues of its\nimplementation in practical settings. As such, our work investigates an\nFL-enabled multiclass ITD paradigm that considers non-Independent and\nIdentically Distributed (non-IID) data distribution to detect insider threats\nfrom different locations (clients) of an organization. Specifically, we propose\na Federated Adversarial Training (FedAT) approach using a generative model to\nalleviate the extreme data skewness arising from the non-IID data distribution\namong the clients. Besides, we propose to utilize a Self-normalized Neural\nNetwork-based Multi-Layer Perceptron (SNN-MLP) model to improve ITD. We perform\ncomprehensive experiments and compare the results with the benchmarks to\nmanifest the enhanced performance of the proposed FedATdriven ITD scheme.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13083v1",
    "published_date": "2024-09-19 20:44:33 UTC",
    "updated_date": "2024-09-19 20:44:33 UTC"
  },
  {
    "arxiv_id": "2409.13082v2",
    "title": "AutoVerus: Automated Proof Generation for Rust Code",
    "authors": [
      "Chenyuan Yang",
      "Xuheng Li",
      "Md Rakib Hossain Misu",
      "Jianan Yao",
      "Weidong Cui",
      "Yeyun Gong",
      "Chris Hawblitzel",
      "Shuvendu Lahiri",
      "Jacob R. Lorch",
      "Shuai Lu",
      "Fan Yang",
      "Ziqiao Zhou",
      "Shan Lu"
    ],
    "abstract": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLM to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13082v2",
    "published_date": "2024-09-19 20:40:52 UTC",
    "updated_date": "2025-02-08 01:06:56 UTC"
  },
  {
    "arxiv_id": "2409.13779v1",
    "title": "AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble",
    "authors": [
      "Tanya Chutani",
      "Saikiran Bonthu",
      "Pranab Samanta",
      "Nitin Singhal"
    ],
    "abstract": "Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for\ndiagnosing, managing, and planning treatment for various cancers. Developing\nreliable deep learning models for the segmentation of tumor lesions in PET/CT\nscans in a multi-tracer multicenter environment, is a critical area of\nresearch. Different tracers, such as Fluorodeoxyglucose (FDG) and\nProstate-Specific Membrane Antigen (PSMA), have distinct physiological uptake\npatterns and data from different centers often vary in terms of acquisition\nprotocols, scanner types, and patient populations. Because of this variability,\nit becomes more difficult to design reliable segmentation algorithms and\ngeneralization techniques due to variations in image quality and lesion\ndetectability. To address this challenge, We trained a 3D Residual encoder\nU-Net within the no new U-Net framework, aiming to generalize the performance\nof automatic lesion segmentation of whole body PET/CT scans, across different\ntracers and clinical sites. Further, We explored several preprocessing\ntechniques and ultimately settled on using the Total Segmentator to crop our\ntraining data. Additionally, we applied resampling during this process. During\ninference, we leveraged test-time augmentations and other post-processing\ntechniques to enhance tumor lesion segmentation. Our team currently hold the\ntop position in the Auto-PET III challenge and outperformed the challenge\nbaseline model in the preliminary test set with Dice score of 0.9627.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13779v1",
    "published_date": "2024-09-19 20:18:39 UTC",
    "updated_date": "2024-09-19 20:18:39 UTC"
  },
  {
    "arxiv_id": "2410.00031v2",
    "title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions",
    "authors": [
      "Ryan Y. Lin",
      "Siddhartha Ojha",
      "Kevin Cai",
      "Maxwell F. Chen"
    ],
    "abstract": "Machine-learning technologies are seeing increased deployment in real-world\nmarket scenarios. In this work, we explore the strategic behaviors of large\nlanguage models (LLMs) when deployed as autonomous agents in multi-commodity\nmarkets, specifically within Cournot competition frameworks. We examine whether\nLLMs can independently engage in anti-competitive practices such as collusion\nor, more specifically, market division. Our findings demonstrate that LLMs can\neffectively monopolize specific commodities by dynamically adjusting their\npricing and resource allocation strategies, thereby maximizing profitability\nwithout direct human input or explicit collusion commands. These results pose\nunique challenges and opportunities for businesses looking to integrate AI into\nstrategic roles and for regulatory bodies tasked with maintaining fair and\ncompetitive markets. The study provides a foundation for further exploration\ninto the ramifications of deferring high-stakes decisions to LLM-based agents.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CL",
      "q-fin.CP"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00031v2",
    "published_date": "2024-09-19 20:10:40 UTC",
    "updated_date": "2025-05-16 10:05:18 UTC"
  },
  {
    "arxiv_id": "2409.13064v1",
    "title": "Fear and Loathing on the Frontline: Decoding the Language of Othering by Russia-Ukraine War Bloggers",
    "authors": [
      "Patrick Gerard",
      "William Theisen",
      "Tim Weninger",
      "Kristina Lerman"
    ],
    "abstract": "Othering, the act of portraying outgroups as fundamentally different from the\ningroup, often escalates into framing them as existential threats--fueling\nintergroup conflict and justifying exclusion and violence. These dynamics are\nalarmingly pervasive, spanning from the extreme historical examples of\ngenocides against minorities in Germany and Rwanda to the ongoing violence and\nrhetoric targeting migrants in the US and Europe. While concepts like hate\nspeech and fear speech have been explored in existing literature, they capture\nonly part of this broader and more nuanced dynamic which can often be harder to\ndetect, particularly in online speech and propaganda. To address this\nchallenge, we introduce a novel computational framework that leverages large\nlanguage models (LLMs) to quantify othering across diverse contexts, extending\nbeyond traditional linguistic indicators of hostility. Applying the model to\nreal-world data from Telegram war bloggers and political discussions on Gab\nreveals how othering escalates during conflicts, interacts with moral language,\nand garners significant attention, particularly during periods of crisis. Our\nframework, designed to offer deeper insights into othering dynamics, combines\nwith a rapid adaptation process to provide essential tools for mitigating\nothering's adverse impacts on social cohesion.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.13064v1",
    "published_date": "2024-09-19 19:56:03 UTC",
    "updated_date": "2024-09-19 19:56:03 UTC"
  },
  {
    "arxiv_id": "2410.07114v5",
    "title": "System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam",
    "authors": [
      "Joost de Winter",
      "Dimitra Dodou",
      "Yke Bauke Eisma"
    ],
    "abstract": "The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch students' average of 40.63 points. Neither model had\naccess to the exam figures. Since there was a risk of model contami-nation\n(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam was\npublished online), we repeated the procedure with a new Mathematics B exam that\nwas published after the cutoff date. The results again indicated that\no1-preview performed strongly (97.8th percentile), which suggests that\ncontamination was not a factor. We also show that there is some variability in\nthe output of o1-preview, which means that sometimes there is 'luck' (the\nanswer is correct) or 'bad luck' (the output has diverged into something that\nis incorrect). We demonstrate that the self-consistency approach, where\nrepeated prompts are given and the most common answer is selected, is a useful\nstrategy for identifying the correct answer. It is concluded that while\nOpenAI's new model series holds great potential, certain risks must be\nconsidered.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07114v5",
    "published_date": "2024-09-19 19:48:31 UTC",
    "updated_date": "2024-10-25 07:57:44 UTC"
  },
  {
    "arxiv_id": "2409.13059v1",
    "title": "Comprehensive Overview of Artificial Intelligence Applications in Modern Industries",
    "authors": [
      "Yijie Weng",
      "Jianhao Wu",
      "Tara Kelly",
      "William Johnson"
    ],
    "abstract": "Artificial Intelligence (AI) is fundamentally reshaping various industries by\nenhancing decision-making processes, optimizing operations, and unlocking new\nopportunities for innovation. This paper explores the applications of AI across\nfour key sectors: healthcare, finance, manufacturing, and retail. Each section\ndelves into the specific challenges faced by these industries, the AI\ntechnologies employed to address them, and the measurable impact on business\noutcomes and societal welfare. We also discuss the implications of AI\nintegration, including ethical considerations, the future trajectory of AI\ndevelopment, and its potential to drive economic growth while posing challenges\nthat need to be managed responsibly.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13059v1",
    "published_date": "2024-09-19 19:22:52 UTC",
    "updated_date": "2024-09-19 19:22:52 UTC"
  },
  {
    "arxiv_id": "2409.13054v1",
    "title": "LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models",
    "authors": [
      "Akshaj Kumar Veldanda",
      "Shi-Xiong Zhang",
      "Anirban Das",
      "Supriyo Chakraborty",
      "Stephen Rawls",
      "Sambit Sahu",
      "Milind Naphade"
    ],
    "abstract": "Large language models (LLMs) have revolutionized various domains, yet their\nutility comes with significant challenges related to outdated or problematic\nknowledge embedded during pretraining. This paper addresses the challenge of\nmodifying LLMs to unlearn problematic and outdated information while\nefficiently integrating new knowledge without retraining from scratch. Here, we\npropose LLM Surgery, a framework to efficiently modify LLM behaviour by\noptimizing a three component objective function that: (1) Performs reverse\ngradient on unlearning dataset (problematic and outdated information), (2)\nPerforms gradient descent on the update dataset (new and updated information),\nand (3) Minimizes the KL divergence on the retain dataset (small subset of\nunchanged text), ensuring alignment between pretrained and modified model\noutputs. Due to the lack of publicly available datasets specifically tailored\nfor our novel task, we compiled a new dataset and an evaluation benchmark.\nUsing Llama2-7B, we demonstrate that LLM Surgery can achieve significant\nforgetting on the unlearn set, a 20\\% increase in accuracy on the update set,\nand maintain performance on the retain set.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13054v1",
    "published_date": "2024-09-19 19:07:01 UTC",
    "updated_date": "2024-09-19 19:07:01 UTC"
  },
  {
    "arxiv_id": "2409.15373v1",
    "title": "Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention",
    "authors": [
      "Rengan Xu",
      "Junjie Yang",
      "Yifan Xu",
      "Hong Li",
      "Xing Liu",
      "Devashish Shankar",
      "Haoci Zhang",
      "Meng Liu",
      "Boyang Li",
      "Yuxi Hu",
      "Mingwei Tang",
      "Zehua Zhang",
      "Tunhou Zhang",
      "Dai Li",
      "Sijia Chen",
      "Gian-Paolo Musumeci",
      "Jiaqi Zhai",
      "Bill Zhu",
      "Hong Yan",
      "Srihari Reddy"
    ],
    "abstract": "The integration of hardware accelerators has significantly advanced the\ncapabilities of modern recommendation systems, enabling the exploration of\ncomplex ranking paradigms previously deemed impractical. However, the GPU-based\ncomputational costs present substantial challenges. In this paper, we\ndemonstrate our development of an efficiency-driven approach to explore these\nparadigms, moving beyond traditional reliance on native PyTorch modules. We\naddress the specific challenges posed by ranking models' dependence on\ncategorical features, which vary in length and complicate GPU utilization. We\nintroduce Jagged Feature Interaction Kernels, a novel method designed to\nextract fine-grained insights from long categorical features through efficient\nhandling of dynamically sized tensors. We further enhance the performance of\nattention mechanisms by integrating Jagged tensors with Flash Attention. Our\nnovel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction\ncompared to dense attention. Notably, it also outperforms dense flash\nattention, with up to 3x speedup and 53% more memory efficiency. In production\nmodels, we observe 10% QPS improvement and 18% memory savings, enabling us to\nscale our recommendation systems with longer features and more complex\narchitectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "3 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15373v1",
    "published_date": "2024-09-19 18:20:54 UTC",
    "updated_date": "2024-09-19 18:20:54 UTC"
  },
  {
    "arxiv_id": "2409.13038v1",
    "title": "HeadCT-ONE: Enabling Granular and Controllable Automated Evaluation of Head CT Radiology Report Generation",
    "authors": [
      "Julián N. Acosta",
      "Xiaoman Zhang",
      "Siddhant Dogra",
      "Hong-Yu Zhou",
      "Seyedmehdi Payabvash",
      "Guido J. Falcone",
      "Eric K. Oermann",
      "Pranav Rajpurkar"
    ],
    "abstract": "We present Head CT Ontology Normalized Evaluation (HeadCT-ONE), a metric for\nevaluating head CT report generation through ontology-normalized entity and\nrelation extraction. HeadCT-ONE enhances current information extraction derived\nmetrics (such as RadGraph F1) by implementing entity normalization through\ndomain-specific ontologies, addressing radiological language variability.\nHeadCT-ONE compares normalized entities and relations, allowing for\ncontrollable weighting of different entity types or specific entities. Through\nexperiments on head CT reports from three health systems, we show that\nHeadCT-ONE's normalization and weighting approach improves the capture of\nsemantically equivalent reports, better distinguishes between normal and\nabnormal reports, and aligns with radiologists' assessment of clinically\nsignificant errors, while offering flexibility to prioritize specific aspects\nof report content. Our results demonstrate how HeadCT-ONE enables more\nflexible, controllable, and granular automated evaluation of head CT reports.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13038v1",
    "published_date": "2024-09-19 18:20:11 UTC",
    "updated_date": "2024-09-19 18:20:11 UTC"
  },
  {
    "arxiv_id": "2409.12963v2",
    "title": "Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner",
    "authors": [
      "Yuzhang Shang",
      "Bingxin Xu",
      "Weitai Kang",
      "Mu Cai",
      "Yuheng Li",
      "Zehao Wen",
      "Zhen Dong",
      "Kurt Keutzer",
      "Yong Jae Lee",
      "Yan Yan"
    ],
    "abstract": "Advancements in Large Language Models (LLMs) inspire various strategies for\nintegrating video modalities. A key approach is Video-LLMs, which incorporate\nan optimizable interface linking sophisticated video encoders to LLMs. However,\ndue to computation and data limitations, these Video-LLMs are typically\npre-trained to process only short videos, limiting their broader application\nfor understanding longer video content. Additionally, fine-tuning Video-LLMs to\nhandle longer videos is cost-prohibitive. Consequently, it becomes essential to\nexplore the interpolation of Video-LLMs under a completely training-free\nsetting. In this paper, we first identify the primary challenges in\ninterpolating Video-LLMs: (1) the video encoder and modality alignment\nprojector are fixed, preventing the integration of additional frames into\nVideo-LLMs, and (2) the LLM backbone is limited in its content length\ncapabilities, which complicates the processing of an increased number of video\ntokens. To address these challenges, we propose a specific INTerPolation method\nfor Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token\nrearrangement technique that circumvents limitations imposed by the fixed video\nencoder and alignment projector. Furthermore, we introduce a training-free LLM\ncontext window extension method to enable Video-LLMs to understand a\ncorrespondingly increased number of visual tokens.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12963v2",
    "published_date": "2024-09-19 17:59:55 UTC",
    "updated_date": "2024-10-02 01:56:08 UTC"
  },
  {
    "arxiv_id": "2409.12959v2",
    "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
    "authors": [
      "Dongzhi Jiang",
      "Renrui Zhang",
      "Ziyu Guo",
      "Yanmin Wu",
      "Jiayi Lei",
      "Pengshuo Qiu",
      "Pan Lu",
      "Zehui Chen",
      "Chaoyou Fu",
      "Guanglu Song",
      "Peng Gao",
      "Yu Liu",
      "Chunyuan Li",
      "Hongsheng Li"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://mmsearch.github.io",
    "pdf_url": "http://arxiv.org/pdf/2409.12959v2",
    "published_date": "2024-09-19 17:59:45 UTC",
    "updated_date": "2024-11-27 08:49:12 UTC"
  },
  {
    "arxiv_id": "2409.12958v1",
    "title": "MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions",
    "authors": [
      "Abdullatif Köksal",
      "Marion Thaler",
      "Ayyoob Imani",
      "Ahmet Üstün",
      "Anna Korhonen",
      "Hinrich Schütze"
    ],
    "abstract": "Instruction tuning enhances large language models (LLMs) by aligning them\nwith human preferences across diverse tasks. Traditional approaches to create\ninstruction tuning datasets face serious challenges for low-resource languages\ndue to their dependence on data annotation. This work introduces a novel\nmethod, Multilingual Reverse Instructions (MURI), which generates high-quality\ninstruction tuning datasets for low-resource languages without requiring human\nannotators or pre-existing multilingual models. Utilizing reverse instructions\nand a translation pipeline, MURI produces instruction-output pairs from\nexisting human-written texts in low-resource languages. This method ensures\ncultural relevance and diversity by sourcing texts from different native\ndomains and applying filters to eliminate inappropriate content. Our dataset,\nMURI-IT, includes more than 2 million instruction-output pairs across 200\nlanguages. Evaluation by native speakers and fine-tuning experiments with mT5\nmodels demonstrate the approach's effectiveness for both NLU and open-ended\ngeneration. We publicly release datasets and models at\nhttps://github.com/akoksal/muri.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12958v1",
    "published_date": "2024-09-19 17:59:20 UTC",
    "updated_date": "2024-09-19 17:59:20 UTC"
  },
  {
    "arxiv_id": "2409.12953v4",
    "title": "JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images",
    "authors": [
      "Zhecan Wang",
      "Junzhang Liu",
      "Chia-Wei Tang",
      "Hani Alomari",
      "Anushka Sivakumar",
      "Rui Sun",
      "Wenhao Li",
      "Md. Atabuzzaman",
      "Hammad Ayyubi",
      "Haoxuan You",
      "Alvi Ishmam",
      "Kai-Wei Chang",
      "Shih-Fu Chang",
      "Chris Thomas"
    ],
    "abstract": "Existing vision-language understanding benchmarks largely consist of images\nof objects in their usual contexts. As a consequence, recent multimodal large\nlanguage models can perform well with only a shallow visual understanding by\nrelying on background language biases. Thus, strong performance on these\nbenchmarks does not necessarily correlate with strong visual understanding. In\nthis paper, we release JourneyBench, a comprehensive human-annotated benchmark\nof generated images designed to assess the model's fine-grained multimodal\nreasoning abilities across five tasks: complementary multimodal chain of\nthought, multi-image VQA, imaginary image captioning, VQA with hallucination\ntriggers, and fine-grained retrieval with sample-specific distractors. Unlike\nexisting benchmarks, JourneyBench explicitly requires fine-grained multimodal\nreasoning in unusual imaginary scenarios where language bias and holistic image\ngist are insufficient. We benchmark state-of-the-art models on JourneyBench and\nanalyze performance along a number of fine-grained dimensions. Results across\nall five tasks show that JourneyBench is exceptionally challenging for even the\nbest models, indicating that models' visual reasoning abilities are not as\nstrong as they first appear. We discuss the implications of our findings and\npropose avenues for further research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12953v4",
    "published_date": "2024-09-19 17:58:16 UTC",
    "updated_date": "2025-01-10 02:31:03 UTC"
  },
  {
    "arxiv_id": "2409.12951v2",
    "title": "Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm",
    "authors": [
      "Akshat Gupta",
      "Atahan Ozdemir",
      "Gopala Anumanchipalli"
    ],
    "abstract": "This paper presents a novel geometric interpretation of LayerNorm and\nexplores how LayerNorm influences the norm and orientation of hidden vectors in\nthe representation space. With these geometric insights, we prepare the\nfoundation for comparing LayerNorm with RMSNorm. We show that the definition of\nLayerNorm is innately linked to the uniform vector, defined as $\\boldsymbol{1}\n= [1, 1, 1, 1, \\cdots, 1]^T \\in \\mathbb{R}^d$. We then show that the\nstandardization step in LayerNorm can be understood in three simple steps: (i)\nremove the component of a vector along the uniform vector, (ii) normalize the\nremaining vector, and (iii) scale the resultant vector by $\\sqrt{d}$, where $d$\nis the dimensionality of the representation space. We also provide additional\ninsights into how LayerNorm operates at inference time. Finally, we compare the\nhidden representations of LayerNorm-based LLMs with models trained using\nRMSNorm and show that all LLMs naturally operate orthogonal to the uniform\nvector at inference time, that is, on average they do not have a component\nalong the uniform vector during inference. This presents the first mechanistic\nevidence that removing the component along the uniform vector in LayerNorm is a\nredundant step. These results advocate for using RMSNorm over LayerNorm which\nis also more computationally efficient.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12951v2",
    "published_date": "2024-09-19 17:58:07 UTC",
    "updated_date": "2025-02-01 06:06:36 UTC"
  },
  {
    "arxiv_id": "2409.13007v2",
    "title": "iCost: A Novel Instance Complexity Based Cost-Sensitive Learning Framework",
    "authors": [
      "Asif Newaz",
      "Asif Ur Rahman Adib",
      "Taskeed Jabid"
    ],
    "abstract": "Class imbalance in data presents significant challenges for classification\ntasks. It is fairly common and requires careful handling to obtain desirable\nperformance. Traditional classification algorithms become biased toward the\nmajority class. One way to alleviate the scenario is to make the classifiers\ncost-sensitive. This is achieved by assigning a higher misclassification cost\nto minority-class instances. One issue with this implementation is that all the\nminority-class instances are treated equally, and assigned with the same\npenalty value. However, the learning difficulties of all the instances are not\nthe same. Instances that are located in the overlapping region or near the\ndecision boundary are harder to classify, whereas those further away are\neasier. Without taking into consideration the instance complexity and naively\nweighting all the minority-class samples uniformly, results in an unwarranted\nbias and consequently, a higher number of misclassifications of the\nmajority-class instances. This is undesirable and to overcome the situation, we\npropose a novel instance complexity-based cost-sensitive approach (termed\n'iCost') in this study. We first categorize all the minority-class instances\nbased on their difficulty level and then the instances are penalized\naccordingly. This ensures a more equitable instance weighting and prevents\nexcessive penalization. The performance of the proposed approach is tested on\n65 binary and 10 multiclass imbalanced datasets against the traditional\ncost-sensitive learning frameworks. A significant improvement in performance\nhas been observed, demonstrating the effectiveness of the proposed strategy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13007v2",
    "published_date": "2024-09-19 17:53:21 UTC",
    "updated_date": "2024-10-25 11:29:44 UTC"
  },
  {
    "arxiv_id": "2409.12919v1",
    "title": "Swine Diet Design using Multi-objective Regionalized Bayesian Optimization",
    "authors": [
      "Gabriel D. Uribe-Guerra",
      "Danny A. Múnera-Ramírez",
      "Julián D. Arias-Londoño"
    ],
    "abstract": "The design of food diets in the context of animal nutrition is a complex\nproblem that aims to develop cost-effective formulations while balancing\nminimum nutritional content. Traditional approaches based on theoretical models\nof metabolic responses and concentrations of digestible energy in raw materials\nface limitations in incorporating zootechnical or environmental variables\naffecting the performance of animals and including multiple objectives aligned\nwith sustainable development policies. Recently, multi-objective Bayesian\noptimization has been proposed as a promising heuristic alternative able to\ndeal with the combination of multiple sources of information, multiple and\ndiverse objectives, and with an intrinsic capacity to deal with uncertainty in\nthe measurements that could be related to variability in the nutritional\ncontent of raw materials. However, Bayesian optimization encounters\ndifficulties in high-dimensional search spaces, leading to exploration\npredominantly at the boundaries. This work analyses a strategy to split the\nsearch space into regions that provide local candidates termed multi-objective\nregionalized Bayesian optimization as an alternative to improve the quality of\nthe Pareto set and Pareto front approximation provided by BO in the context of\nswine diet design. Results indicate that this regionalized approach produces\nmore diverse non-dominated solutions compared to the standard multi-objective\nBayesian optimization. Besides, the regionalized strategy was four times more\neffective in finding solutions that outperform those identified by a stochastic\nprogramming approach referenced in the literature. Experiments using batches of\nquery candidate solutions per iteration show that the optimization process can\nalso be accelerated without compromising the quality of the Pareto set\napproximation during the initial, most critical phase of optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12919v1",
    "published_date": "2024-09-19 17:18:00 UTC",
    "updated_date": "2024-09-19 17:18:00 UTC"
  },
  {
    "arxiv_id": "2409.12903v2",
    "title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization",
    "authors": [
      "Mohammad Samragh",
      "Iman Mirzadeh",
      "Keivan Alizadeh Vahid",
      "Fartash Faghri",
      "Minsik Cho",
      "Moin Nabi",
      "Devang Naik",
      "Mehrdad Farajtabar"
    ],
    "abstract": "The pre-training phase of language models often begins with randomly\ninitialized parameters. With the current trends in scaling models, training\ntheir large number of parameters can be extremely slow and costly. In contrast,\nsmall language models are less expensive to train, but they often cannot\nachieve the accuracy of large models. In this paper, we explore an intriguing\nidea to connect these two different regimes: Can we develop a method to\ninitialize large language models using smaller pre-trained models? Will such\ninitialization bring any benefits in terms of training time and final accuracy?\nIn this paper, we introduce HyperCloning, a method that can expand the\nparameters of a pre-trained language model to those of a larger model with\nincreased hidden dimensions. Our method ensures that the larger model retains\nthe functionality of the smaller model. As a result, the larger model already\ninherits the predictive power and accuracy of the smaller model before the\ntraining starts. We demonstrate that training such an initialized model results\nin significant savings in terms of GPU hours required for pre-training large\nlanguage models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12903v2",
    "published_date": "2024-09-19 16:50:26 UTC",
    "updated_date": "2024-09-20 16:22:37 UTC"
  },
  {
    "arxiv_id": "2409.12900v1",
    "title": "Recognition of Harmful Phytoplankton from Microscopic Images using Deep Learning",
    "authors": [
      "Aymane Khaldi",
      "Rohaifa Khaldi"
    ],
    "abstract": "Monitoring plankton distribution, particularly harmful phytoplankton, is\nvital for preserving aquatic ecosystems, regulating the global climate, and\nensuring environmental protection. Traditional methods for monitoring are often\ntime-consuming, expensive, error-prone, and unsuitable for large-scale\napplications, highlighting the need for accurate and efficient automated\nsystems. In this study, we evaluate several state-of-the-art CNN models,\nincluding ResNet, ResNeXt, DenseNet, and EfficientNet, using three transfer\nlearning approaches: linear probing, fine-tuning, and a combined approach, to\nclassify eleven harmful phytoplankton genera from microscopic images. The best\nperformance was achieved by ResNet-50 using the fine-tuning approach, with an\naccuracy of 96.97%. The results also revealed that the models struggled to\ndifferentiate between four harmful phytoplankton types with similar\nmorphological features.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12900v1",
    "published_date": "2024-09-19 16:42:53 UTC",
    "updated_date": "2024-09-19 16:42:53 UTC"
  },
  {
    "arxiv_id": "2409.15372v1",
    "title": "Fuzzy Rule based Intelligent Cardiovascular Disease Prediction using Complex Event Processing",
    "authors": [
      "Shashi Shekhar Kumar",
      "Anurag Harsh",
      "Ritesh Chandra",
      "Sonali Agarwal"
    ],
    "abstract": "Cardiovascular disease (CVDs) is a rapidly rising global concern due to\nunhealthy diets, lack of physical activity, and other factors. According to the\nWorld Health Organization (WHO), primary risk factors include elevated blood\npressure, glucose, blood lipids, and obesity. Recent research has focused on\naccurate and timely disease prediction to reduce risk and fatalities, often\nrelying on predictive models trained on large datasets, which require intensive\ntraining. An intelligent system for CVDs patients could greatly assist in\nmaking informed decisions by effectively analyzing health parameters. Complex\nEvent Processing (CEP) has emerged as a valuable method for solving real-time\nchallenges by aggregating patterns of interest and their causes and effects on\nend users. In this work, we propose a fuzzy rule-based system for monitoring\nclinical data to provide real-time decision support. We designed fuzzy rules\nbased on clinical and WHO standards to ensure accurate predictions. Our\nintegrated approach uses Apache Kafka and Spark for data streaming, and the\nSiddhi CEP engine for event processing. Additionally, we pass numerous\ncardiovascular disease-related parameters through CEP engines to ensure fast\nand reliable prediction decisions. To validate the effectiveness of our\napproach, we simulated real-time, unseen data to predict cardiovascular\ndisease. Using synthetic data (1000 samples), we categorized it into \"Very Low\nRisk, Low Risk, Medium Risk, High Risk, and Very High Risk.\" Validation results\nshowed that 20% of samples were categorized as very low risk, 15-45% as low\nrisk, 35-65% as medium risk, 55-85% as high risk, and 75% as very high risk.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15372v1",
    "published_date": "2024-09-19 16:36:24 UTC",
    "updated_date": "2024-09-19 16:36:24 UTC"
  },
  {
    "arxiv_id": "2409.12889v2",
    "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case",
    "authors": [
      "Peng Chen",
      "Pi Bu",
      "Jun Song",
      "Yuan Gao",
      "Bo Zheng"
    ],
    "abstract": "Recently, large language model (LLM)-based agents have made significant\nadvances across various fields. One of the most popular research areas involves\napplying these agents to video games. Traditionally, these methods have relied\non game APIs to access in-game environmental and action data. However, this\napproach is limited by the availability of APIs and does not reflect how humans\nplay games. With the advent of vision language models (VLMs), agents now have\nenhanced visual understanding capabilities, enabling them to interact with\ngames using only visual inputs. Despite these advances, current approaches\nstill face challenges in action-oriented tasks, particularly in action\nrole-playing games (ARPGs), where reinforcement learning methods are prevalent\nbut suffer from poor generalization and require extensive training. To address\nthese limitations, we select an ARPG, ``Black Myth: Wukong'', as a research\nplatform to explore the capability boundaries of existing VLMs in scenarios\nrequiring visual-only input and complex action output. We define 12 tasks\nwithin the game, with 75% focusing on combat, and incorporate several\nstate-of-the-art VLMs into this benchmark. Additionally, we will release a\nhuman operation dataset containing recorded gameplay videos and operation logs,\nincluding mouse and keyboard actions. Moreover, we propose a novel VARP (Vision\nAction Role-Playing) agent framework, consisting of an action planning system\nand a visual trajectory system. Our framework demonstrates the ability to\nperform basic tasks and succeed in 90% of easy and medium-level combat\nscenarios. This research aims to provide new insights and directions for\napplying multimodal agents in complex action game environments. The code and\ndatasets will be made available at https://varp-agent.github.io/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12889v2",
    "published_date": "2024-09-19 16:30:25 UTC",
    "updated_date": "2024-09-22 09:51:58 UTC"
  },
  {
    "arxiv_id": "2409.12883v1",
    "title": "Improving Prototypical Parts Abstraction for Case-Based Reasoning Explanations Designed for the Kidney Stone Type Recognition",
    "authors": [
      "Daniel Flores-Araiza",
      "Francisco Lopez-Tiro",
      "Clément Larose",
      "Salvador Hinojosa",
      "Andres Mendez-Vazquez",
      "Miguel Gonzalez-Mendoza",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ],
    "abstract": "The in-vivo identification of the kidney stone types during an ureteroscopy\nwould be a major medical advance in urology, as it could reduce the time of the\ntedious renal calculi extraction process, while diminishing infection risks.\nFurthermore, such an automated procedure would make possible to prescribe\nanti-recurrence treatments immediately. Nowadays, only few experienced\nurologists are able to recognize the kidney stone types in the images of the\nvideos displayed on a screen during the endoscopy. Thus, several deep learning\n(DL) models have recently been proposed to automatically recognize the kidney\nstone types using ureteroscopic images. However, these DL models are of black\nbox nature whicl limits their applicability in clinical settings. This\ncontribution proposes a case-based reasoning DL model which uses prototypical\nparts (PPs) and generates local and global descriptors. The PPs encode for each\nclass (i.e., kidney stone type) visual feature information (hue, saturation,\nintensity and textures) similar to that used by biologists. The PPs are\noptimally generated due a new loss function used during the model training.\nMoreover, the local and global descriptors of PPs allow to explain the\ndecisions (\"what\" information, \"where in the images\") in an understandable way\nfor biologists and urologists. The proposed DL model has been tested on a\ndatabase including images of the six most widespread kidney stone types. The\noverall average classification accuracy was 90.37. When comparing this results\nwith that of the eight other DL models of the kidney stone state-of-the-art, it\ncan be seen that the valuable gain in explanability was not reached at the\nexpense of accuracy which was even slightly increased with respect to that\n(88.2) of the best method of the literature. These promising and interpretable\nresults also encourage urologists to put their trust in AI-based solutions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper submitted to Artificial Intelligence in Medicine. (AIIM),\n  Elsevier",
    "pdf_url": "http://arxiv.org/pdf/2409.12883v1",
    "published_date": "2024-09-19 16:27:32 UTC",
    "updated_date": "2024-09-19 16:27:32 UTC"
  },
  {
    "arxiv_id": "2409.12880v1",
    "title": "Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models",
    "authors": [
      "Bryan Zhang",
      "Taichi Nakatani",
      "Stephan Walter"
    ],
    "abstract": "E-commerce stores enable multilingual product discovery which require\naccurate product title translation. Multilingual large language models (LLMs)\nhave shown promising capacity to perform machine translation tasks, and it can\nalso enhance and translate product titles cross-lingually in one step. However,\nproduct title translation often requires more than just language conversion\nbecause titles are short, lack context, and contain specialized terminology.\nThis study proposes a retrieval-augmented generation (RAG) approach that\nleverages existing bilingual product information in e-commerce by retrieving\nsimilar bilingual examples and incorporating them as few-shot prompts to\nenhance LLM-based product title translation. Experiment results show that our\nproposed RAG approach improve product title translation quality with chrF score\ngains of up to 15.3% for language pairs where the LLM has limited proficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "6 Pages,In Proceedings of ACM CIKM Workshop on Data-Centric AI (CIKM\n  DCAI 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.12880v1",
    "published_date": "2024-09-19 16:23:42 UTC",
    "updated_date": "2024-09-19 16:23:42 UTC"
  },
  {
    "arxiv_id": "2409.12865v2",
    "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning",
    "authors": [
      "Junnan Liu",
      "Qianren Mao",
      "Weifeng Jiang",
      "Jianxin Li"
    ],
    "abstract": "Knowledge graph reasoning plays a vital role in various applications and has\ngarnered considerable attention. Recently, path-based methods have achieved\nimpressive performance. However, they may face limitations stemming from\nconstraints in message-passing neural networks, such as missing paths and\ninformation over-squashing. In this paper, we revisit the application of\ntransformers for knowledge graph reasoning to address the constraints faced by\npath-based methods and propose a novel method KnowFormer. KnowFormer utilizes a\ntransformer architecture to perform reasoning on knowledge graphs from the\nmessage-passing perspective, rather than reasoning by textual information like\nprevious pretrained language model based methods. Specifically, we define the\nattention computation based on the query prototype of knowledge graph\nreasoning, facilitating convenient construction and efficient optimization. To\nincorporate structural information into the self-attention mechanism, we\nintroduce structure-aware modules to calculate query, key, and value\nrespectively. Additionally, we present an efficient attention computation\nmethod for better scalability. Experimental results demonstrate the superior\nperformance of KnowFormer compared to prominent baseline methods on both\ntransductive and inductive benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICML2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12865v2",
    "published_date": "2024-09-19 16:08:10 UTC",
    "updated_date": "2024-12-17 18:27:01 UTC"
  },
  {
    "arxiv_id": "2409.12846v2",
    "title": "How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Symbols",
    "authors": [
      "Volker Tresp",
      "Hang Li"
    ],
    "abstract": "The Tensor Brain (TB) has been introduced as a computational model for\nperception and memory. This paper provides an overview of the TB model,\nincorporating recent developments and insights into its functionality. The TB\nis composed of two primary layers: the representation layer and the index\nlayer. The representation layer serves as a model for the subsymbolic global\nworkspace, a concept derived from consciousness research. Its state represents\nthe cognitive brain state, capturing the dynamic interplay of sensory and\ncognitive processes. The index layer, in contrast, contains symbolic\nrepresentations for concepts, time instances, and predicates. In a bottom-up\noperation, sensory input activates the representation layer, which then\ntriggers associated symbolic labels in the index layer. Conversely, in a\ntop-down operation, symbols in the index layer activate the representation\nlayer, which in turn influences earlier processing layers through embodiment.\nThis top-down mechanism underpins semantic memory, enabling the integration of\nabstract knowledge into perceptual and cognitive processes. A key feature of\nthe TB is its use of concept embeddings, which function as connection weights\nlinking the index layer to the representation layer. As a concept's ``DNA,''\nthese embeddings consolidate knowledge from diverse experiences, sensory\nmodalities, and symbolic representations, providing a unified framework for\nlearning and memory.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12846v2",
    "published_date": "2024-09-19 15:45:38 UTC",
    "updated_date": "2024-12-29 09:20:18 UTC"
  },
  {
    "arxiv_id": "2409.13000v2",
    "title": "Introducing the Large Medical Model: State of the art healthcare cost and risk prediction with transformers trained on patient event sequences",
    "authors": [
      "Ricky Sahu",
      "Eric Marriott",
      "Ethan Siegel",
      "David Wagner",
      "Flore Uzan",
      "Troy Yang",
      "Asim Javed"
    ],
    "abstract": "With U.S. healthcare spending approaching $5T (NHE Fact Sheet 2024), and 25%\nof it estimated to be wasteful (Waste in the US the health care system:\nestimated costs and potential for savings, n.d.), the need to better predict\nrisk and optimal patient care is evermore important. This paper introduces the\nLarge Medical Model (LMM), a generative pre-trained transformer (GPT) designed\nto guide and predict the broad facets of patient care and healthcare\nadministration. The model is trained on medical event sequences from over 140M\nlongitudinal patient claims records with a specialized vocabulary built from\nmedical terminology systems and demonstrates a superior capability to forecast\nhealthcare costs and identify potential risk factors. Through experimentation\nand validation, we showcase the LMM's proficiency in not only in cost and risk\npredictions, but also in discerning intricate patterns within complex medical\nconditions and an ability to identify novel relationships in patient care. The\nLMM is able to improve both cost prediction by 14.1% over the best commercial\nmodels and chronic conditions prediction by 1.9% over the best transformer\nmodels in research predicting a broad set of conditions. The LMM is a\nsubstantial advancement in healthcare analytics, offering the potential to\nsignificantly enhance risk assessment, cost management, and personalized\nmedicine.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML",
      "I.2.1; K.4.1; K.4.3; J.1; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13000v2",
    "published_date": "2024-09-19 15:38:21 UTC",
    "updated_date": "2024-12-05 17:19:12 UTC"
  },
  {
    "arxiv_id": "2409.12842v1",
    "title": "Vision Language Models Can Parse Floor Plan Maps",
    "authors": [
      "David DeFazio",
      "Hrudayangam Mehta",
      "Jeremy Blackburn",
      "Shiqi Zhang"
    ],
    "abstract": "Vision language models (VLMs) can simultaneously reason about images and\ntexts to tackle many tasks, from visual question answering to image captioning.\nThis paper focuses on map parsing, a novel task that is unexplored within the\nVLM context and particularly useful to mobile robots. Map parsing requires\nunderstanding not only the labels but also the geometric configurations of a\nmap, i.e., what areas are like and how they are connected. To evaluate the\nperformance of VLMs on map parsing, we prompt VLMs with floorplan maps to\ngenerate task plans for complex indoor navigation. Our results demonstrate the\nremarkable capability of VLMs in map parsing, with a success rate of 0.96 in\ntasks requiring a sequence of nine navigation actions, e.g., approaching and\ngoing through doors. Other than intuitive observations, e.g., VLMs do better in\nsmaller maps and simpler navigation tasks, there was a very interesting\nobservation that its performance drops in large open areas. We provide\npractical suggestions to address such challenges as validated by our\nexperimental results. Webpage: https://shorturl.at/OUkEY",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12842v1",
    "published_date": "2024-09-19 15:36:28 UTC",
    "updated_date": "2024-09-19 15:36:28 UTC"
  },
  {
    "arxiv_id": "2409.19001v1",
    "title": "Pay Attention to What Matters",
    "authors": [
      "Pedro Luiz Silva",
      "Antonio de Domenico",
      "Ali Maatouk",
      "Fadhel Ayed"
    ],
    "abstract": "Despite the remarkable success of Large Language Models (LLMs), they still\nexhibit a limited capability to align their outputs to the user instructions.\nIn this work, we introduce a simple and effective method, which we name GUIDE,\nthat mechanistically increases attention scores in instruction tokens. To\nsupport this operation, we present Influence, a novel metric that highlights\nhow the user's instructions propagate through the transformer layers and impact\nthe LLM output. Our results show that GUIDE improves the accuracy of following\ninstructions 29.4 % to 60.4%, outperforming natural prompting alternatives and\nSupervised Fine-Tuning up to 1M tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.19001v1",
    "published_date": "2024-09-19 15:26:50 UTC",
    "updated_date": "2024-09-19 15:26:50 UTC"
  },
  {
    "arxiv_id": "2409.12832v3",
    "title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
    "authors": [
      "Tenghao Huang",
      "Donghee Lee",
      "John Sweeney",
      "Jiatong Shi",
      "Emily Steliotes",
      "Matthew Lange",
      "Jonathan May",
      "Muhao Chen"
    ],
    "abstract": "Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12832v3",
    "published_date": "2024-09-19 15:07:35 UTC",
    "updated_date": "2024-10-07 01:26:23 UTC"
  },
  {
    "arxiv_id": "2409.12997v1",
    "title": "VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness",
    "authors": [
      "Xuan Cai",
      "Zhiyong Cui",
      "Xuesong Bai",
      "Ruimin Ke",
      "Zhenshu Ma",
      "Haiyang Yu",
      "Yilong Ren"
    ],
    "abstract": "Autonomous vehicles (AVs) face significant threats to their safe operation in\ncomplex traffic environments. Adversarial training has emerged as an effective\nmethod of enabling AVs to preemptively fortify their robustness against\nmalicious attacks. Train an attacker using an adversarial policy, allowing the\nAV to learn robust driving through interaction with this attacker. However,\nadversarial policies in existing methodologies often get stuck in a loop of\noverexploiting established vulnerabilities, resulting in poor improvement for\nAVs. To overcome the limitations, we introduce a pioneering framework termed\nVulnerability-aware and Curiosity-driven Adversarial Training (VCAT).\nSpecifically, during the traffic vehicle attacker training phase, a surrogate\nnetwork is employed to fit the value function of the AV victim, providing dense\ninformation about the victim's inherent vulnerabilities. Subsequently, random\nnetwork distillation is used to characterize the novelty of the environment,\nconstructing an intrinsic reward to guide the attacker in exploring unexplored\nterritories. In the victim defense training phase, the AV is trained in\ncritical scenarios in which the pretrained attacker is positioned around the\nvictim to generate attack behaviors. Experimental results revealed that the\ntraining methodology provided by VCAT significantly improved the robust control\ncapabilities of learning-based AVs, outperforming both conventional training\nmodalities and alternative reinforcement learning counterparts, with a marked\nreduction in crash rates. The code is available at\nhttps://github.com/caixxuan/VCAT.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 5 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2409.12997v1",
    "published_date": "2024-09-19 14:53:02 UTC",
    "updated_date": "2024-09-19 14:53:02 UTC"
  },
  {
    "arxiv_id": "2409.12820v1",
    "title": "Machine-learning based high-bandwidth magnetic sensing",
    "authors": [
      "Galya Haim",
      "Stefano Martina",
      "John Howell",
      "Nir Bar-Gill",
      "Filippo Caruso"
    ],
    "abstract": "Recent years have seen significant growth of quantum technologies, and\nspecifically quantum sensing, both in terms of the capabilities of advanced\nplatforms and their applications. One of the leading platforms in this context\nis nitrogen-vacancy (NV) color centers in diamond, providing versatile,\nhigh-sensitivity, and high-resolution magnetic sensing. Nevertheless, current\nschemes for spin resonance magnetic sensing (as applied by NV quantum sensing)\nsuffer from tradeoffs associated with sensitivity, dynamic range, and\nbandwidth. Here we address this issue, and implement machine learning tools to\nenhance NV magnetic sensing in terms of the sensitivity/bandwidth tradeoff in\nlarge dynamic range scenarios. We experimentally demonstrate this new approach,\nreaching an improvement in the relevant figure of merit by a factor of up to 5.\nOur results promote quantum machine learning protocols for sensing applications\ntowards more feasible and efficient quantum technologies.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "physics.app-ph",
      "physics.comp-ph",
      "68T07 (Primary) 68T10, 81-08, 81-05, 81-10, 81-11, 81V10 (Secondary)",
      "I.2.6; I.5.4; J.2; I.6.3"
    ],
    "primary_category": "quant-ph",
    "comment": "12 pages including supplementary, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12820v1",
    "published_date": "2024-09-19 14:50:12 UTC",
    "updated_date": "2024-09-19 14:50:12 UTC"
  },
  {
    "arxiv_id": "2409.12996v1",
    "title": "pyrtklib: An open-source package for tightly coupled deep learning and GNSS integration for positioning in urban canyons",
    "authors": [
      "Runzhi Hu",
      "Penghui Xu",
      "Yihan Zhong",
      "Weisong Wen"
    ],
    "abstract": "Artificial intelligence (AI) is revolutionizing numerous fields, with\nincreasing applications in Global Navigation Satellite Systems (GNSS)\npositioning algorithms in intelligent transportation systems (ITS) via deep\nlearning. However, a significant technological disparity exists as traditional\nGNSS algorithms are often developed in Fortran or C, contrasting with the\nPython-based implementation prevalent in deep learning tools. To address this\ndiscrepancy, this paper introduces pyrtklib, a Python binding for the widely\nutilized open-source GNSS tool, RTKLIB. This binding makes all RTKLIB\nfunctionalities accessible in Python, facilitating seamless integration.\nMoreover, we present a deep learning subsystem under pyrtklib, which is a novel\ndeep learning framework that leverages pyrtklib to accurately predict weights\nand biases within the GNSS positioning process. The use of pyrtklib enables\ndevelopers to easily and quickly prototype and implement deep learning-aided\nGNSS algorithms, showcasing its potential to enhance positioning accuracy\nsignificantly.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12996v1",
    "published_date": "2024-09-19 14:47:47 UTC",
    "updated_date": "2024-09-19 14:47:47 UTC"
  },
  {
    "arxiv_id": "2409.12815v1",
    "title": "Graph Convolutional Neural Networks as Surrogate Models for Climate Simulation",
    "authors": [
      "Kevin Potter",
      "Carianne Martinez",
      "Reina Pradhan",
      "Samantha Brozak",
      "Steven Sleder",
      "Lauren Wheeler"
    ],
    "abstract": "Many climate processes are characterized using large systems of nonlinear\ndifferential equations; this, along with the immense amount of data required to\nparameterize complex interactions, means that Earth-System Model (ESM)\nsimulations may take weeks to run on large clusters. Uncertainty quantification\nmay require thousands of runs, making ESM simulations impractical for\npreliminary assessment. Alternatives may include simplifying the processes in\nthe model, but recent efforts have focused on using machine learning to\ncomplement these models or even act as full surrogates. \\textit{We leverage\nmachine learning, specifically fully-connected neural networks (FCNNs) and\ngraph convolutional neural networks (GCNNs), to enable rapid simulation and\nuncertainty quantification in order to inform more extensive ESM simulations.}\nOur surrogate simulated 80 years in approximately 310 seconds on a single A100\nGPU, compared to weeks for the ESM model while having mean temperature errors\nbelow $0.1^{\\circ}C$ and maximum errors below $2^{\\circ}C$.",
    "categories": [
      "physics.ao-ph",
      "cs.AI"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12815v1",
    "published_date": "2024-09-19 14:41:15 UTC",
    "updated_date": "2024-09-19 14:41:15 UTC"
  },
  {
    "arxiv_id": "2409.12812v2",
    "title": "Towards Interactive and Learnable Cooperative Driving Automation: a Large Language Model-Driven Decision-Making Framework",
    "authors": [
      "Shiyu Fang",
      "Jiaqi Liu",
      "Mingyu Ding",
      "Yiming Cui",
      "Chen Lv",
      "Peng Hang",
      "Jian Sun"
    ],
    "abstract": "At present, Connected Autonomous Vehicles (CAVs) have begun to open road\ntesting around the world, but their safety and efficiency performance in\ncomplex scenarios is still not satisfactory. Cooperative driving leverages the\nconnectivity ability of CAVs to achieve synergies greater than the sum of their\nparts, making it a promising approach to improving CAV performance in complex\nscenarios. However, the lack of interaction and continuous learning ability\nlimits current cooperative driving to single-scenario applications and specific\nCooperative Driving Automation (CDA). To address these challenges, this paper\nproposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative\ndriving framework, to achieve all-scenario and all-CDA. First, since Large\nLanguage Models(LLMs) are not adept at handling mathematical calculations, an\nenvironment module is introduced to update vehicle positions based on semantic\ndecisions, thus avoiding potential errors from direct LLM control of vehicle\npositions. Second, based on the four levels of CDA defined by the SAE J3216\nstandard, we propose a Chain-of-Thought (COT) based reasoning module that\nincludes state perception, intent sharing, negotiation, and decision-making,\nenhancing the stability of LLMs in multi-step reasoning tasks. Centralized\nconflict resolution is then managed through a conflict coordinator in the\nreasoning process. Finally, by introducing a memory module and employing\nretrieval-augmented generation, CAVs are endowed with the ability to learn from\ntheir past experiences. We validate the proposed CoDrivingLLM through ablation\nexperiments on the negotiation module, reasoning with different shots\nexperience, and comparison with other cooperative driving methods.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12812v2",
    "published_date": "2024-09-19 14:36:00 UTC",
    "updated_date": "2024-09-22 09:31:58 UTC"
  },
  {
    "arxiv_id": "2409.12809v2",
    "title": "Don't be Fooled: The Misinformation Effect of Explanations in Human-AI Collaboration",
    "authors": [
      "Philipp Spitzer",
      "Joshua Holstein",
      "Katelyn Morrison",
      "Kenneth Holstein",
      "Gerhard Satzger",
      "Niklas Kühl"
    ],
    "abstract": "Across various applications, humans increasingly use black-box artificial\nintelligence (AI) systems without insight into these systems' reasoning. To\ncounter this opacity, explainable AI (XAI) methods promise enhanced\ntransparency and interpretability. While recent studies have explored how XAI\naffects human-AI collaboration, few have examined the potential pitfalls caused\nby incorrect explanations. The implications for humans can be far-reaching but\nhave not been explored extensively. To investigate this, we ran a study (n=160)\non AI-assisted decision-making in which humans were supported by XAI. Our\nfindings reveal a misinformation effect when incorrect explanations accompany\ncorrect AI advice with implications post-collaboration. This effect causes\nhumans to infer flawed reasoning strategies, hindering task execution and\ndemonstrating impaired procedural knowledge. Additionally, incorrect\nexplanations compromise human-AI team-performance during collaboration. With\nour work, we contribute to HCI by providing empirical evidence for the negative\nconsequences of incorrect explanations on humans post-collaboration and\noutlining guidelines for designers of AI.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12809v2",
    "published_date": "2024-09-19 14:34:20 UTC",
    "updated_date": "2025-01-08 13:59:28 UTC"
  },
  {
    "arxiv_id": "2410.02808v1",
    "title": "KLDD: Kalman Filter based Linear Deformable Diffusion Model in Retinal Image Segmentation",
    "authors": [
      "Zhihao Zhao",
      "Yinzheng Zhao",
      "Junjie Yang",
      "Kai Huang",
      "Nassir Navab",
      "M. Ali Nasseri"
    ],
    "abstract": "AI-based vascular segmentation is becoming increasingly common in enhancing\nthe screening and treatment of ophthalmic diseases. Deep learning structures\nbased on U-Net have achieved relatively good performance in vascular\nsegmentation. However, small blood vessels and capillaries tend to be lost\nduring segmentation when passed through the traditional U-Net downsampling\nmodule. To address this gap, this paper proposes a novel Kalman filter based\nLinear Deformable Diffusion (KLDD) model for retinal vessel segmentation. Our\nmodel employs a diffusion process that iteratively refines the segmentation,\nleveraging the flexible receptive fields of deformable convolutions in feature\nextraction modules to adapt to the detailed tubular vascular structures. More\nspecifically, we first employ a feature extractor with linear deformable\nconvolution to capture vascular structure information form the input images. To\nbetter optimize the coordinate positions of deformable convolution, we employ\nthe Kalman filter to enhance the perception of vascular structures in linear\ndeformable convolution. Subsequently, the features of the vascular structures\nextracted are utilized as a conditioning element within a diffusion model by\nthe Cross-Attention Aggregation module (CAAM) and the Channel-wise Soft\nAttention module (CSAM). These aggregations are designed to enhance the\ndiffusion model's capability to generate vascular structures. Experiments are\nevaluated on retinal fundus image datasets (DRIVE, CHASE_DB1) as well as the\n3mm and 6mm of the OCTA-500 dataset, and the results show that the diffusion\nmodel proposed in this paper outperforms other methods.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at BIBM 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.02808v1",
    "published_date": "2024-09-19 14:21:38 UTC",
    "updated_date": "2024-09-19 14:21:38 UTC"
  },
  {
    "arxiv_id": "2409.12801v1",
    "title": "Exploring the Lands Between: A Method for Finding Differences between AI-Decisions and Human Ratings through Generated Samples",
    "authors": [
      "Lukas Mecke",
      "Daniel Buschek",
      "Uwe Gruenefeld",
      "Florian Alt"
    ],
    "abstract": "Many important decisions in our everyday lives, such as authentication via\nbiometric models, are made by Artificial Intelligence (AI) systems. These can\nbe in poor alignment with human expectations, and testing them on clear-cut\nexisting data may not be enough to uncover those cases. We propose a method to\nfind samples in the latent space of a generative model, designed to be\nchallenging for a decision-making model with regard to matching human\nexpectations. By presenting those samples to both the decision-making model and\nhuman raters, we can identify areas where its decisions align with human\nintuition and where they contradict it. We apply this method to a face\nrecognition model and collect a dataset of 11,200 human ratings from 100\nparticipants. We discuss findings from our dataset and how our approach can be\nused to explore the performance of AI models in different contexts and for\ndifferent user groups.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12801v1",
    "published_date": "2024-09-19 14:14:08 UTC",
    "updated_date": "2024-09-19 14:14:08 UTC"
  },
  {
    "arxiv_id": "2409.12798v1",
    "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL",
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Tim Rockäschel",
      "Edward Grefenstette",
      "Davide Paglieri",
      "Samuel Coward",
      "Laura Toni"
    ],
    "abstract": "The temporal credit assignment problem is a central challenge in\nReinforcement Learning (RL), concerned with attributing the appropriate\ninfluence to each actions in a trajectory for their ability to achieve a goal.\nHowever, when feedback is delayed and sparse, the learning signal is poor, and\naction evaluation becomes harder. Canonical solutions, such as reward shaping\nand options, require extensive domain knowledge and manual intervention,\nlimiting their scalability and applicability. In this work, we lay the\nfoundations for Credit Assignment with Language Models (CALM), a novel approach\nthat leverages Large Language Models (LLMs) to automate credit assignment via\nreward shaping and options discovery. CALM uses LLMs to decompose a task into\nelementary subgoals and assess the achievement of these subgoals in\nstate-action transitions. Every time an option terminates, a subgoal is\nachieved, and CALM provides an auxiliary reward. This additional reward signal\ncan enhance the learning process when the task reward is sparse and delayed\nwithout the need for human-designed rewards. We provide a preliminary\nevaluation of CALM using a dataset of human-annotated demonstrations from\nMiniHack, suggesting that LLMs can be effective in assigning credit in\nzero-shot settings, without examples or LLM fine-tuning. Our preliminary\nresults indicate that the knowledge of LLMs is a promising prior for credit\nassignment in RL, facilitating the transfer of human knowledge into value\nfunctions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.12798v1",
    "published_date": "2024-09-19 14:08:09 UTC",
    "updated_date": "2024-09-19 14:08:09 UTC"
  },
  {
    "arxiv_id": "2409.12797v1",
    "title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing",
    "authors": [
      "Minh Nguyen",
      "Mert R. Sabuncu"
    ],
    "abstract": "Invariant causal prediction (ICP) is a popular technique for finding causal\nparents (direct causes) of a target via exploiting distribution shifts and\ninvariance testing (Peters et al., 2016). However, since ICP needs to run an\nexponential number of tests and fails to identify parents when distribution\nshifts only affect a few variables, applying ICP to practical large scale\nproblems is challenging. We propose MMSE-ICP and fastICP, two approaches which\nemploy an error inequality to address the identifiability problem of ICP. The\ninequality states that the minimum prediction error of the predictor using\ncausal parents is the smallest among all predictors which do not use\ndescendants. fastICP is an efficient approximation tailored for large problems\nas it exploits the inequality and a heuristic to run fewer tests. MMSE-ICP and\nfastICP not only outperform competitive baselines in many simulations but also\nachieve state-of-the-art result on a large scale real data benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at TMLR",
    "pdf_url": "http://arxiv.org/pdf/2409.12797v1",
    "published_date": "2024-09-19 14:07:31 UTC",
    "updated_date": "2024-09-19 14:07:31 UTC"
  },
  {
    "arxiv_id": "2409.12995v1",
    "title": "Improving generalisability of 3D binding affinity models in low data regimes",
    "authors": [
      "Julia Buhmann",
      "Ward Haddadin",
      "Lukáš Pravda",
      "Alan Bilsland",
      "Hagen Triendl"
    ],
    "abstract": "Predicting protein-ligand binding affinity is an essential part of\ncomputer-aided drug design. However, generalisable and performant global\nbinding affinity models remain elusive, particularly in low data regimes.\nDespite the evolution of model architectures, current benchmarks are not\nwell-suited to probe the generalisability of 3D binding affinity models.\nFurthermore, 3D global architectures such as GNNs have not lived up to\nperformance expectations. To investigate these issues, we introduce a novel\nsplit of the PDBBind dataset, minimizing similarity leakage between train and\ntest sets and allowing for a fair and direct comparison between various model\narchitectures. On this low similarity split, we demonstrate that, in general,\n3D global models are superior to protein-specific local models in low data\nregimes. We also demonstrate that the performance of GNNs benefits from three\nnovel contributions: supervised pre-training via quantum mechanical data,\nunsupervised pre-training via small molecule diffusion, and explicitly modeling\nhydrogen atoms in the input graph. We believe that this work introduces\npromising new approaches to unlock the potential of GNN architectures for\nbinding affinity modelling.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 10 figues",
    "pdf_url": "http://arxiv.org/pdf/2409.12995v1",
    "published_date": "2024-09-19 13:54:38 UTC",
    "updated_date": "2024-09-19 13:54:38 UTC"
  },
  {
    "arxiv_id": "2409.12785v2",
    "title": "Investigation on domain adaptation of additive manufacturing monitoring systems to enhance digital twin reusability",
    "authors": [
      "Jiarui Xie",
      "Zhuo Yang",
      "Chun-Chun Hu",
      "Haw-Ching Yang",
      "Yan Lu",
      "Yaoyao Fiona Zhao"
    ],
    "abstract": "Powder bed fusion (PBF) is an emerging metal additive manufacturing (AM)\ntechnology that enables rapid fabrication of complex geometries. However,\ndefects such as pores and balling may occur and lead to structural\nunconformities, thus compromising the mechanical performance of the part. This\nhas become a critical challenge for quality assurance as the nature of some\ndefects is stochastic during the process and invisible from the exterior. To\naddress this issue, digital twin (DT) using machine learning (ML)-based\nmodeling can be deployed for AM process monitoring and control. Melt pool is\none of the most commonly observed physical phenomena for process monitoring,\nusually by high-speed cameras. Once labeled and preprocessed, the melt pool\nimages are used to train ML-based models for DT applications such as process\nanomaly detection and print quality evaluation. Nonetheless, the reusability of\nDTs is restricted due to the wide variability of AM settings, including AM\nmachines and monitoring instruments. The performance of the ML models trained\nusing the dataset collected from one setting is usually compromised when\napplied to other settings. This paper proposes a knowledge transfer pipeline\nbetween different AM settings to enhance the reusability of AM DTs. The source\nand target datasets are collected from the National Institute of Standards and\nTechnology and National Cheng Kung University with different cameras,\nmaterials, AM machines, and process parameters. The proposed pipeline consists\nof four steps: data preprocessing, data augmentation, domain alignment, and\ndecision alignment. Compared with the model trained only using the source\ndataset, this pipeline increased the melt pool anomaly detection accuracy by\n31% without any labeled training data from the target dataset.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CE",
    "comment": "8 pages, 7 figures, 3 tables. IEEE CASE 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12785v2",
    "published_date": "2024-09-19 13:54:01 UTC",
    "updated_date": "2024-09-20 04:29:23 UTC"
  },
  {
    "arxiv_id": "2409.12784v7",
    "title": "Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering",
    "authors": [
      "Youngsun Lim",
      "Hojun Choi",
      "Hyunjung Shim"
    ],
    "abstract": "Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.12784v7",
    "published_date": "2024-09-19 13:51:21 UTC",
    "updated_date": "2025-02-10 13:10:32 UTC"
  },
  {
    "arxiv_id": "2409.12774v3",
    "title": "GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction",
    "authors": [
      "Hanyue Zhang",
      "Zhiliu Yang",
      "Xinhe Zuo",
      "Yuxin Tong",
      "Ying Long",
      "Chen Liu"
    ],
    "abstract": "This paper proposes a novel framework for large-scale scene reconstruction\nbased on 3D Gaussian splatting (3DGS) and aims to address the scalability and\naccuracy challenges faced by existing methods. For tackling the scalability\nissue, we split the large scene into multiple cells, and the candidate\npoint-cloud and camera views of each cell are correlated through a\nvisibility-based camera selection and a progressive point-cloud extension. To\nreinforce the rendering quality, three highlighted improvements are made in\ncomparison with vanilla 3DGS, which are a strategy of the ray-Gaussian\nintersection and the novel Gaussians density control for learning efficiency,\nan appearance decoupling module based on ConvKAN network to solve uneven\nlighting conditions in large-scale scenes, and a refined final loss with the\ncolor loss, the depth distortion loss, and the normal consistency loss.\nFinally, the seamless stitching procedure is executed to merge the individual\nGaussian radiance field for novel view synthesis across different cells.\nEvaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method\nconsistently generates more high-fidelity rendering results than\nstate-of-the-art methods of large-scale scene reconstruction. We further\nvalidate the generalizability of the proposed approach by rendering on\nself-collected video clips recorded by a commercial drone.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12774v3",
    "published_date": "2024-09-19 13:43:31 UTC",
    "updated_date": "2024-09-24 15:03:24 UTC"
  },
  {
    "arxiv_id": "2409.12769v1",
    "title": "The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning",
    "authors": [
      "Manh V. Nguyen",
      "Liang Zhao",
      "Bobin Deng",
      "William Severa",
      "Honghui Xu",
      "Shaoen Wu"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have recently gained significant interest in\non-chip learning in embedded devices and emerged as an energy-efficient\nalternative to conventional Artificial Neural Networks (ANNs). However, to\nextend SNNs to a Federated Learning (FL) setting involving collaborative model\ntraining, the communication between the local devices and the remote server\nremains the bottleneck, which is often restricted and costly. In this paper, we\nfirst explore the inherent robustness of SNNs under noisy communication in FL.\nBuilding upon this foundation, we propose a novel Federated Learning with Top-K\nSparsification (FLTS) algorithm to reduce the bandwidth usage for FL training.\nWe discover that the proposed scheme with SNNs allows more bandwidth savings\ncompared to ANNs without impacting the model's accuracy. Additionally, the\nnumber of parameters to be communicated can be reduced to as low as 6 percent\nof the size of the original model. We further improve the communication\nefficiency by enabling dynamic parameter compression during model training.\nExtensive experiment results demonstrate that our proposed algorithms\nsignificantly outperform the baselines in terms of communication cost and model\naccuracy and are promising for practical network-efficient FL with SNNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper has been accepted for publication at the 43rd IEEE\n  International Performance Computing and Communications Conference (IPCCC\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.12769v1",
    "published_date": "2024-09-19 13:37:18 UTC",
    "updated_date": "2024-09-19 13:37:18 UTC"
  },
  {
    "arxiv_id": "2410.02807v1",
    "title": "AutoPETIII: The Tracer Frontier. What Frontier?",
    "authors": [
      "Zacharia Mesbah",
      "Léo Mottay",
      "Romain Modzelewski",
      "Pierre Decazes",
      "Sébastien Hapdey",
      "Su Ruan",
      "Sébastien Thureau"
    ],
    "abstract": "For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02807v1",
    "published_date": "2024-09-19 13:26:31 UTC",
    "updated_date": "2024-09-19 13:26:31 UTC"
  },
  {
    "arxiv_id": "2409.12745v1",
    "title": "Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space",
    "authors": [
      "Sebastião Quintas",
      "Isabelle Ferrané",
      "Thomas Pellegrini"
    ],
    "abstract": "The use of synthetic speech as data augmentation is gaining increasing\npopularity in fields such as automatic speech recognition and speech\nclassification tasks. Despite novel text-to-speech systems with voice cloning\ncapabilities, that allow the usage of a larger amount of voices based on short\naudio segments, it is known that these systems tend to hallucinate and\noftentimes produce bad data that will most likely have a negative impact on the\ndownstream task. In the present work, we conduct a set of experiments around\nzero-shot learning with synthetic speech data for the specific task of speech\ncommands classification. Our results on the Google Speech Commands dataset show\nthat a simple ASR-based filtering method can have a big impact in the quality\nof the generated data, translating to a better performance. Furthermore,\ndespite the good quality of the generated speech data, we also show that\nsynthetic and real speech can still be easily distinguishable when using\nself-supervised (WavLM) features, an aspect further explored with a CycleGAN to\nbridge the gap between the two types of speech material.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12745v1",
    "published_date": "2024-09-19 13:07:55 UTC",
    "updated_date": "2024-09-19 13:07:55 UTC"
  },
  {
    "arxiv_id": "2409.12741v3",
    "title": "Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization",
    "authors": [
      "Thomas Savage",
      "Stephen Ma",
      "Abdessalem Boukil",
      "Vishwesh Patel",
      "Ekanath Rangan",
      "Ivan Lopez",
      "Jonathan H Chen"
    ],
    "abstract": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12741v3",
    "published_date": "2024-09-19 13:03:24 UTC",
    "updated_date": "2024-12-13 15:37:01 UTC"
  },
  {
    "arxiv_id": "2409.12740v1",
    "title": "HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling",
    "authors": [
      "Junyi Chen",
      "Lu Chi",
      "Bingyue Peng",
      "Zehuan Yuan"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, prompting several studies to explore their potential in recommendation\nsystems. However, these attempts have so far resulted in only modest\nimprovements over traditional recommendation models. Moreover, three critical\nquestions remain under-explored: firstly, the real value of LLMs' pre-trained\nweights, often considered to encapsulate world knowledge; secondly, the\nnecessity of fine-tuning for recommendation tasks; lastly, whether LLMs can\nexhibit the same scalability benefits in recommendation systems as they do in\nother domains. In this paper, we propose a novel Hierarchical Large Language\nModel (HLLM) architecture designed to enhance sequential recommendation\nsystems. Our approach employs a two-tier model: the first Item LLM extracts\nrich content features from the detailed text description of the item, while the\nsecond User LLM utilizes these features to predict users' future interests\nbased on their interaction history. Extensive experiments demonstrate that our\nmethod effectively leverages the pre-trained capabilities of open-source LLMs,\nand further fine-tuning leads to significant performance boosts. Additionally,\nHLLM achieves excellent scalability, with the largest configuration utilizing\n7B parameters for both item feature extraction and user interest modeling.\nMoreover, HLLM offers excellent training and serving efficiency, making it\npractical in real-world applications. Evaluations on two large-scale datasets,\nPixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results,\noutperforming traditional ID-based models by a wide margin. In online A/B\ntesting, HLLM showcases notable gains, validating its practical impact in\nreal-world recommendation scenarios. Codes are available at\nhttps://github.com/bytedance/HLLM.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12740v1",
    "published_date": "2024-09-19 13:03:07 UTC",
    "updated_date": "2024-09-19 13:03:07 UTC"
  },
  {
    "arxiv_id": "2409.12737v1",
    "title": "MEXMA: Token-level objectives improve sentence representations",
    "authors": [
      "João Maria Janeiro",
      "Benjamin Piwowarski",
      "Patrick Gallinari",
      "Loïc Barrault"
    ],
    "abstract": "Current pre-trained cross-lingual sentence encoders approaches use\nsentence-level objectives only. This can lead to loss of information,\nespecially for tokens, which then degrades the sentence representation. We\npropose MEXMA, a novel approach that integrates both sentence-level and\ntoken-level objectives. The sentence representation in one language is used to\npredict masked tokens in another language, with both the sentence\nrepresentation and all tokens directly updating the encoder. We show that\nadding token-level objectives greatly improves the sentence representation\nquality across several tasks. Our approach outperforms current pre-trained\ncross-lingual sentence encoders on bi-text mining as well as several downstream\ntasks. We also analyse the information encoded in our tokens, and how the\nsentence representation is built from them.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12737v1",
    "published_date": "2024-09-19 13:00:29 UTC",
    "updated_date": "2024-09-19 13:00:29 UTC"
  },
  {
    "arxiv_id": "2409.12730v3",
    "title": "When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising Recommendation",
    "authors": [
      "Weipu Chen",
      "Zhuangzhuang He",
      "Fei Liu"
    ],
    "abstract": "Learning user preferences from implicit feedback is one of the core\nchallenges in recommendation. The difficulty lies in the potential noise within\nimplicit feedback. Therefore, various denoising recommendation methods have\nbeen proposed recently. However, most of them overly rely on the hyperparameter\nconfigurations, inevitably leading to inadequacies in model adaptability and\ngeneralization performance. In this study, we propose a novel Adaptive Ensemble\nLearning (AEL) for denoising recommendation, which employs a sparse gating\nnetwork as a brain, selecting suitable experts to synthesize appropriate\ndenoising capacities for different data samples. To address the ensemble\nlearning shortcoming of model complexity and ensure sub-recommender diversity,\nwe also proposed a novel method that stacks components to create\nsub-recommenders instead of directly constructing them. Extensive experiments\nacross various datasets demonstrate that AEL outperforms others in kinds of\npopular metrics, even in the presence of substantial and dynamic noise. Our\ncode is available at https://github.com/cpu9xx/AEL.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at ICASSP 2025. 5pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12730v3",
    "published_date": "2024-09-19 12:55:34 UTC",
    "updated_date": "2024-12-26 09:20:17 UTC"
  },
  {
    "arxiv_id": "2409.12726v1",
    "title": "Cloudy with a Chance of Anomalies: Dynamic Graph Neural Network for Early Detection of Cloud Services' User Anomalies",
    "authors": [
      "Revital Marbel",
      "Yanir Cohen",
      "Ran Dubin",
      "Amit Dvir",
      "Chen Hajaj"
    ],
    "abstract": "Ensuring the security of cloud environments is imperative for sustaining\norganizational growth and operational efficiency. As the ubiquity of cloud\nservices continues to rise, the inevitability of cyber threats underscores the\nimportance of preemptive detection. This paper introduces a pioneering\ntime-based embedding approach for Cloud Services Graph-based Anomaly Detection\n(CS-GAD), utilizing a Graph Neural Network (GNN) to discern anomalous user\nbehavior during interactions with cloud services. Our method employs a dynamic\ntripartite graph representation to encapsulate the evolving interactions among\ncloud services, users, and their activities over time. Leveraging GNN models in\neach time frame, our approach generates a graph embedding wherein each user is\nassigned a score based on their historical activity, facilitating the\nidentification of unusual behavior. Results demonstrate a notable reduction in\nfalse positive rates (2-9%) compared to prevailing methods, coupled with a\ncommendable true positive rate (100%). The contributions of this work encompass\nearly detection capabilities, a low false positive rate, an innovative\ntripartite graph representation incorporating action types, the introduction of\na new cloud services dataset featuring various user attacks, and an open-source\nimplementation for community collaboration in advancing cloud service security.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12726v1",
    "published_date": "2024-09-19 12:50:31 UTC",
    "updated_date": "2024-09-19 12:50:31 UTC"
  },
  {
    "arxiv_id": "2409.12994v2",
    "title": "Performance and Power: Systematic Evaluation of AI Workloads on Accelerators with CARAML",
    "authors": [
      "Chelsea Maria John",
      "Stepan Nassyr",
      "Carolin Penke",
      "Andreas Herten"
    ],
    "abstract": "The rapid advancement of machine learning (ML) technologies has driven the\ndevelopment of specialized hardware accelerators designed to facilitate more\nefficient model training. This paper introduces the CARAML benchmark suite,\nwhich is employed to assess performance and energy consumption during the\ntraining of transformer-based large language models and computer vision models\non a range of hardware accelerators, including systems from NVIDIA, AMD, and\nGraphcore. CARAML provides a compact, automated, extensible, and reproducible\nframework for assessing the performance and energy of ML workloads across\nvarious novel hardware architectures. The design and implementation of CARAML,\nalong with a custom power measurement tool called jpwr, are discussed in\ndetail.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "To be published in Workshop Proceedings of The International\n  Conference for High Performance Computing Networking, Storage, and Analysis\n  (SC-W '24) (2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.12994v2",
    "published_date": "2024-09-19 12:43:18 UTC",
    "updated_date": "2024-10-29 09:07:58 UTC"
  },
  {
    "arxiv_id": "2409.12683v1",
    "title": "Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National Varieties, Creoles and Other Low-resource Scenarios",
    "authors": [
      "Aditya Joshi",
      "Diptesh Kanojia",
      "Heather Lent",
      "Hour Kaing",
      "Haiyue Song"
    ],
    "abstract": "Despite excellent results on benchmarks over a small subset of languages,\nlarge language models struggle to process text from languages situated in\n`lower-resource' scenarios such as dialects/sociolects (national or social\nvarieties of a language), Creoles (languages arising from linguistic contact\nbetween multiple languages) and other low-resource languages. This introductory\ntutorial will identify common challenges, approaches, and themes in natural\nlanguage processing (NLP) research for confronting and overcoming the obstacles\ninherent to data-poor contexts. By connecting past ideas to the present field,\nthis tutorial aims to ignite collaboration and cross-pollination between\nresearchers working in these scenarios. Our notion of `lower-resource' broadly\ndenotes the outstanding lack of data required for model training - and may be\napplied to scenarios apart from the three covered in the tutorial.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Selected as a full-day tutorial at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12683v1",
    "published_date": "2024-09-19 11:48:42 UTC",
    "updated_date": "2024-09-19 11:48:42 UTC"
  },
  {
    "arxiv_id": "2409.12682v1",
    "title": "Retrieval-Augmented Test Generation: How Far Are We?",
    "authors": [
      "Jiho Shin",
      "Reem Aleithan",
      "Hadi Hemmati",
      "Song Wang"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has shown notable advancements in\nsoftware engineering tasks. Despite its potential, RAG's application in unit\ntest generation remains under-explored. To bridge this gap, we take the\ninitiative to investigate the efficacy of RAG-based LLMs in test generation. As\nRAGs can leverage various knowledge sources to enhance their performance, we\nalso explore the impact of different sources of RAGs' knowledge bases on unit\ntest generation to provide insights into their practical benefits and\nlimitations. Specifically, we examine RAG built upon three types of domain\nknowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.\nEach source offers essential knowledge for creating tests from different\nperspectives, i.e., API documentations provide official API usage guidelines,\nGitHub issues offer resolutions of issues related to the APIs from the library\ndevelopers, and StackOverflow Q&As present community-driven solutions and best\npractices. For our experiment, we focus on five widely used and typical\nPython-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,\nScikit-learn, Google JAX, and XGBoost to build, train, and deploy complex\nneural networks efficiently. We conducted experiments using the top 10% most\nwidely used APIs across these projects, involving a total of 188 APIs. We\ninvestigate the effectiveness of four state-of-the-art LLMs (open and\nclosed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1\n405B. Additionally, we compare three prompting strategies in generating unit\ntest cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an\nAPI-level RAG on the three external sources. Finally, we compare the cost of\ndifferent sources of knowledge used for the RAG.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "18 pages + reference",
    "pdf_url": "http://arxiv.org/pdf/2409.12682v1",
    "published_date": "2024-09-19 11:48:29 UTC",
    "updated_date": "2024-09-19 11:48:29 UTC"
  },
  {
    "arxiv_id": "2409.12677v1",
    "title": "(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers",
    "authors": [
      "Manh Khoi Duong",
      "Stefan Conrad"
    ],
    "abstract": "Fairness metrics are used to assess discrimination and bias in\ndecision-making processes across various domains, including machine learning\nmodels and human decision-makers in real-world applications. This involves\ncalculating the disparities between probabilistic outcomes among social groups,\nsuch as acceptance rates between male and female applicants. However,\ntraditional fairness metrics do not account for the uncertainty in these\nprocesses and lack of comparability when two decision-makers exhibit the same\ndisparity. Using Bayesian statistics, we quantify the uncertainty of the\ndisparity to enhance discrimination assessments. We represent each\ndecision-maker, whether a machine learning model or a human, by its disparity\nand the corresponding uncertainty in that disparity. We define preferences over\ndecision-makers and utilize brute-force to choose the optimal decision-maker\naccording to a utility function that ranks decision-makers based on these\npreferences. The decision-maker with the highest utility score can be\ninterpreted as the one for whom we are most certain that it is fair.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.12677v1",
    "published_date": "2024-09-19 11:44:03 UTC",
    "updated_date": "2024-09-19 11:44:03 UTC"
  },
  {
    "arxiv_id": "2409.12669v1",
    "title": "Enhancing Construction Site Safety: A Lightweight Convolutional Network for Effective Helmet Detection",
    "authors": [
      "Mujadded Al Rabbani Alif"
    ],
    "abstract": "In the realm of construction safety, the detection of personal protective\nequipment, such as helmets, plays a critical role in preventing workplace\ninjuries. This paper details the development and evaluation of convolutional\nneural networks (CNNs) designed for the accurate classification of helmet\npresence on construction sites. Initially, a simple CNN model comprising one\nconvolutional block and one fully connected layer was developed, yielding\nmodest results. To enhance its performance, the model was progressively\nrefined, first by extending the architecture to include an additional\nconvolutional block and a fully connected layer. Subsequently, batch\nnormalization and dropout techniques were integrated, aiming to mitigate\noverfitting and improve the model's generalization capabilities. The\nperformance of these models is methodically analyzed, revealing a peak F1-score\nof 84\\%, precision of 82\\%, and recall of 86\\% with the most advanced\nconfiguration of the first study phase. Despite these improvements, the\naccuracy remained suboptimal, thus setting the stage for further architectural\nand operational enhancements. This work lays a foundational framework for\nongoing adjustments and optimization in automated helmet detection technology,\nwith future enhancements expected to address the limitations identified during\nthese initial experiments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12669v1",
    "published_date": "2024-09-19 11:29:18 UTC",
    "updated_date": "2024-09-19 11:29:18 UTC"
  },
  {
    "arxiv_id": "2409.13676v1",
    "title": "A sound description: Exploring prompt templates and class descriptions to enhance zero-shot audio classification",
    "authors": [
      "Michel Olvera",
      "Paraskevas Stamatiadis",
      "Slim Essid"
    ],
    "abstract": "Audio-text models trained via contrastive learning offer a practical approach\nto perform audio classification through natural language prompts, such as \"this\nis a sound of\" followed by category names. In this work, we explore alternative\nprompt templates for zero-shot audio classification, demonstrating the\nexistence of higher-performing options. First, we find that the formatting of\nthe prompts significantly affects performance so that simply prompting the\nmodels with properly formatted class labels performs competitively with\noptimized prompt templates and even prompt ensembling. Moreover, we look into\ncomplementing class labels by audio-centric descriptions. By leveraging large\nlanguage models, we generate textual descriptions that prioritize acoustic\nfeatures of sound events to disambiguate between classes, without extensive\nprompt engineering. We show that prompting with class descriptions leads to\nstate-of-the-art results in zero-shot audio classification across major ambient\nsound datasets. Remarkably, this method requires no additional training and\nremains fully zero-shot.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "DCASE 2024 - 9th Workshop on Detection and Classification of Acoustic\n  Scenes and Events, Oct 2024, Tokyo, Japan",
    "pdf_url": "http://arxiv.org/pdf/2409.13676v1",
    "published_date": "2024-09-19 11:27:50 UTC",
    "updated_date": "2024-09-19 11:27:50 UTC"
  },
  {
    "arxiv_id": "2410.02806v1",
    "title": "Investigating the Impact of Randomness on Reproducibility in Computer Vision: A Study on Applications in Civil Engineering and Medicine",
    "authors": [
      "Bahadır Eryılmaz",
      "Osman Alperen Koraş",
      "Jörg Schlötterer",
      "Christin Seifert"
    ],
    "abstract": "Reproducibility is essential for scientific research. However, in computer\nvision, achieving consistent results is challenging due to various factors. One\ninfluential, yet often unrecognized, factor is CUDA-induced randomness. Despite\nCUDA's advantages for accelerating algorithm execution on GPUs, if not\ncontrolled, its behavior across multiple executions remains non-deterministic.\nWhile reproducibility issues in ML being researched, the implications of\nCUDA-induced randomness in application are yet to be understood. Our\ninvestigation focuses on this randomness across one standard benchmark dataset\nand two real-world datasets in an isolated environment. Our results show that\nCUDA-induced randomness can account for differences up to 4.77% in performance\nscores. We find that managing this variability for reproducibility may entail\nincreased runtime or reduce performance, but that disadvantages are not as\nsignificant as reported in previous studies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02806v1",
    "published_date": "2024-09-19 11:06:06 UTC",
    "updated_date": "2024-09-19 11:06:06 UTC"
  },
  {
    "arxiv_id": "2409.12642v1",
    "title": "Deep generative models as an adversarial attack strategy for tabular machine learning",
    "authors": [
      "Salijona Dyrmishi",
      "Mihaela Cătălina Stoian",
      "Eleonora Giunchiglia",
      "Maxime Cordy"
    ],
    "abstract": "Deep Generative Models (DGMs) have found application in computer vision for\ngenerating adversarial examples to test the robustness of machine learning (ML)\nsystems. Extending these adversarial techniques to tabular ML presents unique\nchallenges due to the distinct nature of tabular data and the necessity to\npreserve domain constraints in adversarial examples. In this paper, we adapt\nfour popular tabular DGMs into adversarial DGMs (AdvDGMs) and evaluate their\neffectiveness in generating realistic adversarial examples that conform to\ndomain constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICMLC 2024 (International Conference on Machine Learning\n  and Cybernetics)",
    "pdf_url": "http://arxiv.org/pdf/2409.12642v1",
    "published_date": "2024-09-19 10:41:23 UTC",
    "updated_date": "2024-09-19 10:41:23 UTC"
  },
  {
    "arxiv_id": "2409.15371v9",
    "title": "Balancing LoRA Performance and Efficiency with Simple Shard Sharing",
    "authors": [
      "Jiale Kang",
      "Qingyu Yin"
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), effectively reduce the number of trainable parameters in\nLarge Language Models (LLMs). However, as model scales continue to grow, the\ndemand for computational resources remains a significant challenge. Existing\nLoRA variants often struggle to strike an optimal balance between adaptability\n(model performance and convergence speed) and efficiency (computational\noverhead, memory usage, and initialization time). This paper introduces\nFOSSIL(\\textbf{F}ramework for \\textbf{O}ptimal \\textbf{S}hard \\textbf{S}haring\n\\textbf{I}ntegration in \\textbf{L}oRA), a novel PEFT approach that addresses\nthis trade-off through a simple shard-sharing mechanism. FOSSIL leverages the\ninsight that a low-rank adaptation can be achieved by decomposing the weight\nmatrix into multiple fragment matrices and utilizing a shared, trainable common\nfragment. This method constructs the low-rank update matrix through the\nreplication of these shared, partitioned shards. We also propose a\nhardware-efficient and broadly applicable implementation for FOSSIL. Extensive\nexperiments conducted on a range of tasks, alongside a systematic analysis of\ncomputational performance, demonstrate FOSSIL's superiority. The results show\nthat FOSSIL significantly outperforms standard LoRA and its prominent variants\nin both model performance metrics and computational efficiency, including\ninitialization speed and training throughput. By effectively balancing\nexpressive power and resource utilization, FOSSIL offers a compelling solution\nfor efficiently adapting large-scale models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15371v9",
    "published_date": "2024-09-19 10:26:42 UTC",
    "updated_date": "2025-05-16 12:21:57 UTC"
  },
  {
    "arxiv_id": "2409.12634v1",
    "title": "Exploring bat song syllable representations in self-supervised audio encoders",
    "authors": [
      "Marianne de Heer Kloots",
      "Mirjam Knörnschild"
    ],
    "abstract": "How well can deep learning models trained on human-generated sounds\ndistinguish between another species' vocalization types? We analyze the\nencoding of bat song syllables in several self-supervised audio encoders, and\nfind that models pre-trained on human speech generate the most distinctive\nrepresentations of different syllable types. These findings form first steps\ntowards the application of cross-species transfer learning in bat bioacoustics,\nas well as an improved understanding of out-of-distribution signal processing\nin audio encoder models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Presented at VIHAR-2024; see https://vihar-2024.vihar.org/",
    "pdf_url": "http://arxiv.org/pdf/2409.12634v1",
    "published_date": "2024-09-19 10:09:31 UTC",
    "updated_date": "2024-09-19 10:09:31 UTC"
  },
  {
    "arxiv_id": "2409.12632v1",
    "title": "Counterfactual Explanations for Clustering Models",
    "authors": [
      "Aurora Spagnol",
      "Kacper Sokol",
      "Pietro Barbiero",
      "Marc Langheinrich",
      "Martin Gjoreski"
    ],
    "abstract": "Clustering algorithms rely on complex optimisation processes that may be\ndifficult to comprehend, especially for individuals who lack technical\nexpertise. While many explainable artificial intelligence techniques exist for\nsupervised machine learning, unsupervised learning -- and clustering in\nparticular -- has been largely neglected. To complicate matters further, the\nnotion of a ``true'' cluster is inherently challenging to define. These facets\nof unsupervised learning and its explainability make it difficult to foster\ntrust in such methods and curtail their adoption. To address these challenges,\nwe propose a new, model-agnostic technique for explaining clustering algorithms\nwith counterfactual statements. Our approach relies on a novel soft-scoring\nmethod that captures the spatial information utilised by clustering models. It\nbuilds upon a state-of-the-art Bayesian counterfactual generator for supervised\nlearning to deliver high-quality explanations. We evaluate its performance on\nfive datasets and two clustering algorithms, and demonstrate that introducing\nsoft scores to guide counterfactual search significantly improves the results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12632v1",
    "published_date": "2024-09-19 10:05:58 UTC",
    "updated_date": "2024-09-19 10:05:58 UTC"
  },
  {
    "arxiv_id": "2409.12623v2",
    "title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks",
    "authors": [
      "Zhaozhi Qian",
      "Faroq Altam",
      "Muhammad Alqurishi",
      "Riad Souissi"
    ],
    "abstract": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12623v2",
    "published_date": "2024-09-19 09:52:35 UTC",
    "updated_date": "2024-09-24 08:49:21 UTC"
  },
  {
    "arxiv_id": "2409.12618v2",
    "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning",
    "authors": [
      "Santosh Kumar Radha",
      "Yasamin Nouri Jelyani",
      "Ara Ghukasyan",
      "Oktay Goktas"
    ],
    "abstract": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12618v2",
    "published_date": "2024-09-19 09:44:17 UTC",
    "updated_date": "2024-10-01 17:50:25 UTC"
  },
  {
    "arxiv_id": "2409.18998v1",
    "title": "Controlled LLM-based Reasoning for Clinical Trial Retrieval",
    "authors": [
      "Mael Jullien",
      "Alex Bogatu",
      "Harriet Unsworth",
      "Andre Freitas"
    ],
    "abstract": "Matching patients to clinical trials demands a systematic and reasoned\ninterpretation of documents which require significant expert-level background\nknowledge, over a complex set of well-defined eligibility criteria. Moreover,\nthis interpretation process needs to operate at scale, over vast knowledge\nbases of trials. In this paper, we propose a scalable method that extends the\ncapabilities of LLMs in the direction of systematizing the reasoning over sets\nof medical eligibility criteria, evaluating it in the context of real-world\ncases. The proposed method overlays a Set-guided reasoning method for LLMs. The\nproposed framework is evaluated on TREC 2022 Clinical Trials, achieving results\nsuperior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18998v1",
    "published_date": "2024-09-19 09:42:33 UTC",
    "updated_date": "2024-09-19 09:42:33 UTC"
  },
  {
    "arxiv_id": "2409.12602v1",
    "title": "Enhancing Agricultural Environment Perception via Active Vision and Zero-Shot Learning",
    "authors": [
      "Michele Carlo La Greca",
      "Mirko Usuelli",
      "Matteo Matteucci"
    ],
    "abstract": "Agriculture, fundamental for human sustenance, faces unprecedented\nchallenges. The need for efficient, human-cooperative, and sustainable farming\nmethods has never been greater. The core contributions of this work involve\nleveraging Active Vision (AV) techniques and Zero-Shot Learning (ZSL) to\nimprove the robot's ability to perceive and interact with agricultural\nenvironment in the context of fruit harvesting. The AV Pipeline implemented\nwithin ROS 2 integrates the Next-Best View (NBV) Planning for 3D environment\nreconstruction through a dynamic 3D Occupancy Map. Our system allows the\nrobotics arm to dynamically plan and move to the most informative viewpoints\nand explore the environment, updating the 3D reconstruction using semantic\ninformation produced through ZSL models. Simulation and real-world experimental\nresults demonstrate our system's effectiveness in complex visibility\nconditions, outperforming traditional and static predefined planning methods.\nZSL segmentation models employed, such as YOLO World + EfficientViT SAM,\nexhibit high-speed performance and accurate segmentation, allowing flexibility\nwhen dealing with semantic information in unknown agricultural contexts without\nrequiring any fine-tuning process.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12602v1",
    "published_date": "2024-09-19 09:26:23 UTC",
    "updated_date": "2024-09-19 09:26:23 UTC"
  },
  {
    "arxiv_id": "2409.12587v1",
    "title": "Test-Time Augmentation Meets Variational Bayes",
    "authors": [
      "Masanari Kimura",
      "Howard Bondell"
    ],
    "abstract": "Data augmentation is known to contribute significantly to the robustness of\nmachine learning models. In most instances, data augmentation is utilized\nduring the training phase. Test-Time Augmentation (TTA) is a technique that\ninstead leverages these data augmentations during the testing phase to achieve\nrobust predictions. More precisely, TTA averages the predictions of multiple\ndata augmentations of an instance to produce a final prediction. Although the\neffectiveness of TTA has been empirically reported, it can be expected that the\npredictive performance achieved will depend on the set of data augmentation\nmethods used during testing. In particular, the data augmentation methods\napplied should make different contributions to performance. That is, it is\nanticipated that there may be differing degrees of contribution in the set of\ndata augmentation methods used for TTA, and these could have a negative impact\non prediction performance. In this study, we consider a weighted version of the\nTTA based on the contribution of each data augmentation. Some variants of TTA\ncan be regarded as considering the problem of determining the appropriate\nweighting. We demonstrate that the determination of the coefficients of this\nweighted TTA can be formalized in a variational Bayesian framework. We also\nshow that optimizing the weights to maximize the marginal log-likelihood\nsuppresses candidates of unwanted data augmentations at the test phase.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12587v1",
    "published_date": "2024-09-19 09:11:01 UTC",
    "updated_date": "2024-09-19 09:11:01 UTC"
  },
  {
    "arxiv_id": "2409.12567v1",
    "title": "Model calibration using a parallel differential evolution algorithm in computational neuroscience: simulation of stretch induced nerve deficit",
    "authors": [
      "Antonio LaTorre",
      "Man Ting Kwong",
      "Julián A. García-Grajales",
      "Riyi Shi",
      "Antoine Jérusalem",
      "José-María Peña"
    ],
    "abstract": "Neuronal damage, in the form of both brain and spinal cord injuries, is one\nof the major causes of disability and death in young adults worldwide. One way\nto assess the direct damage occurring after a mechanical insult is the\nsimulation of the neuronal cells functional deficits following the mechanical\nevent. In this study, we use a coupled mechanical electrophysiological model\nwith several free parameters that are required to be calibrated against\nexperimental results. The calibration is carried out by means of an\nevolutionary algorithm (differential evolution, DE) that needs to evaluate each\nconfiguration of parameters on six different damage cases, each of them taking\nseveral minutes to compute. To minimise the simulation time of the parameter\ntuning for the DE, the stretch of one unique fixed-diameter axon with a\nsimplified triggering process is used to speed up the calculations. The model\nis then leveraged for the parameter optimization of the more realistic bundle\nof independent axons, an impractical configuration to run on a single processor\ncomputer. To this end, we have developed a parallel implementation based on\nOpenMP that runs on a multi-processor taking advantage of all the available\ncomputational power. The parallel DE algorithm obtains good results,\noutperforming the best effort achieved by published manual calibration, in a\nfraction of the time. While not being able to fully capture the experimental\nresults, the resulting nerve model provides a complex averaging framework for\nnerve damage simulation able to simulate gradual axonal functional alteration\nin a bundle.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12567v1",
    "published_date": "2024-09-19 08:40:32 UTC",
    "updated_date": "2024-09-19 08:40:32 UTC"
  },
  {
    "arxiv_id": "2409.13774v1",
    "title": "Trustworthy Intrusion Detection: Confidence Estimation Using Latent Space",
    "authors": [
      "Ioannis Pitsiorlas",
      "George Arvanitakis",
      "Marios Kountouris"
    ],
    "abstract": "This work introduces a novel method for enhancing confidence in anomaly\ndetection in Intrusion Detection Systems (IDS) through the use of a Variational\nAutoencoder (VAE) architecture. By developing a confidence metric derived from\nlatent space representations, we aim to improve the reliability of IDS\npredictions against cyberattacks. Applied to the NSL-KDD dataset, our approach\nfocuses on binary classification tasks to effectively distinguish between\nnormal and malicious network activities. The methodology demonstrates a\nsignificant enhancement in anomaly detection, evidenced by a notable\ncorrelation of 0.45 between the reconstruction error and the proposed metric.\nOur findings highlight the potential of employing VAEs for more accurate and\ntrustworthy anomaly detection in network security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.13774v1",
    "published_date": "2024-09-19 08:09:44 UTC",
    "updated_date": "2024-09-19 08:09:44 UTC"
  },
  {
    "arxiv_id": "2409.12538v1",
    "title": "PersonaFlow: Boosting Research Ideation with LLM-Simulated Expert Personas",
    "authors": [
      "Yiren Liu",
      "Pranav Sharma",
      "Mehul Jitendra Oswal",
      "Haijun Xia",
      "Yun Huang"
    ],
    "abstract": "Developing novel interdisciplinary research ideas often requires discussions\nand feedback from experts across different domains. However, obtaining timely\ninputs is challenging due to the scarce availability of domain experts. Recent\nadvances in Large Language Model (LLM) research have suggested the feasibility\nof utilizing LLM-simulated expert personas to support research ideation. In\nthis study, we introduce PersonaFlow, an LLM-based system using persona\nsimulation to support the ideation stage of interdisciplinary scientific\ndiscovery. Our findings indicate that using multiple personas during ideation\nsignificantly enhances user-perceived quality of outcomes (e.g., relevance of\ncritiques, creativity of research questions) without increasing cognitive load.\nWe also found that users' persona customization interactions significantly\nimproved their sense of control and recall of generated ideas. Based on the\nfindings, we discuss highlighting ethical concerns, including potential\nover-reliance and cognitive biases, and suggest design implications for\nleveraging LLM-simulated expert personas to support research ideation when\nhuman expertise is inaccessible.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12538v1",
    "published_date": "2024-09-19 07:54:29 UTC",
    "updated_date": "2024-09-19 07:54:29 UTC"
  },
  {
    "arxiv_id": "2409.12524v1",
    "title": "Should RAG Chatbots Forget Unimportant Conversations? Exploring Importance and Forgetting with Psychological Insights",
    "authors": [
      "Ryuichi Sumida",
      "Koji Inoue",
      "Tatsuya Kawahara"
    ],
    "abstract": "While Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nlong-term conversations, the increasing memory load as conversations progress\ndegrades retrieval accuracy. Drawing on psychological insights, we propose\nLUFY, a simple yet effective method that focuses on emotionally arousing\nmemories and retains less than 10% of the conversation. In the user experiment,\nparticipants interacted with three types of RAG chatbots, each for 2 hours over\n4 sessions, marking the most extensive assessment of a chatbot's long-term\ncapabilities to date -- more than four times longer than any existing\nbenchmark. The results demonstrate that prioritizing arousing memories while\nforgetting the majority of the conversation significantly enhances user\nexperience. This study pushes the frontier of long-term conversations and\nhighlights the importance of forgetting unimportant parts of conversations.\nCode and Dataset: https://github.com/ryuichi-sumida/LUFY",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12524v1",
    "published_date": "2024-09-19 07:39:22 UTC",
    "updated_date": "2024-09-19 07:39:22 UTC"
  },
  {
    "arxiv_id": "2409.12518v4",
    "title": "Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting",
    "authors": [
      "Boying Li",
      "Zhixi Cai",
      "Yuan-Fang Li",
      "Ian Reid",
      "Hamid Rezatofighi"
    ],
    "abstract": "We propose Hier-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring\na novel hierarchical categorical representation, which enables accurate global\n3D semantic mapping, scaling-up capability, and explicit semantic label\nprediction in the 3D world. The parameter usage in semantic SLAM systems\nincreases significantly with the growing complexity of the environment, making\nit particularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\n\\MethodName{} outperforms existing dense SLAM methods in both mapping and\ntracking accuracy, while achieving a 2x operation speed-up. Additionally, it\nachieves on-par semantic rendering performance compared to existing methods\nwhile significantly reducing storage and training time requirements. Rendering\nFPS impressively reaches 2,000 with semantic information and 3,000 without it.\nMost notably, it showcases the capability of handling the complex real-world\nscene with more than 500 semantic classes, highlighting its valuable scaling-up\ncapability. The open-source code is available at\nhttps://github.com/LeeBY68/Hier-SLAM",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication at ICRA 2025. Code is available at\n  https://github.com/LeeBY68/Hier-SLAM",
    "pdf_url": "http://arxiv.org/pdf/2409.12518v4",
    "published_date": "2024-09-19 07:18:41 UTC",
    "updated_date": "2025-03-10 05:47:01 UTC"
  },
  {
    "arxiv_id": "2409.12517v2",
    "title": "Scaling FP8 training to trillion-token LLMs",
    "authors": [
      "Maxim Fishman",
      "Brian Chmiel",
      "Ron Banner",
      "Daniel Soudry"
    ],
    "abstract": "We train, for the first time, large language models using FP8 precision on\ndatasets up to 2 trillion tokens -- a 20-fold increase over previous limits.\nThrough these extended training runs, we uncover critical instabilities in FP8\ntraining that were not observable in earlier works with shorter durations. We\ntrace these instabilities to outlier amplification by the SwiGLU activation\nfunction. Interestingly, we show, both analytically and empirically, that this\namplification happens only over prolonged training periods, and link it to a\nSwiGLU weight alignment process. To address this newly identified issue, we\nintroduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training\nwithout altering function behavior. We also demonstrate, for the first time,\nFP8 quantization of both Adam optimizer moments. Combining these innovations,\nwe successfully train a 7B parameter model using FP8 precision on 256 Intel\nGaudi2 accelerators, achieving on-par results with the BF16 baseline while\ndelivering up to a $\\sim 34 \\%$ throughput improvement. A reference\nimplementation is supplied in\nhttps://github.com/Anonymous1252022/Megatron-DeepSpeed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12517v2",
    "published_date": "2024-09-19 07:15:58 UTC",
    "updated_date": "2025-02-10 09:37:59 UTC"
  },
  {
    "arxiv_id": "2409.12516v1",
    "title": "A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets -- A New Microfoundations of GARCH model",
    "authors": [
      "Kei Nakagawa",
      "Masanori Hirano",
      "Kentaro Minami",
      "Takanobu Mizuta"
    ],
    "abstract": "The AI traders in financial markets have sparked significant interest in\ntheir effects on price formation mechanisms and market volatility, raising\nimportant questions for market stability and regulation. Despite this interest,\na comprehensive model to quantitatively assess the specific impacts of AI\ntraders remains undeveloped. This study aims to address this gap by modeling\nthe influence of AI traders on market price formation and volatility within a\nmulti-agent framework, leveraging the concept of microfoundations.\nMicrofoundations involve understanding macroeconomic phenomena, such as market\nprice formation, through the decision-making and interactions of individual\neconomic agents. While widely acknowledged in macroeconomics, microfoundational\napproaches remain unexplored in empirical finance, particularly for models like\nthe GARCH model, which captures key financial statistical properties such as\nvolatility clustering and fat tails. This study proposes a multi-agent market\nmodel to derive the microfoundations of the GARCH model, incorporating three\ntypes of agents: noise traders, fundamental traders, and AI traders. By\nmathematically aggregating the micro-structure of these agents, we establish\nthe microfoundations of the GARCH model. We validate this model through\nmulti-agent simulations, confirming its ability to reproduce the stylized facts\nof financial markets. Finally, we analyze the impact of AI traders using\nparameters derived from these microfoundations, contributing to a deeper\nunderstanding of their role in market dynamics.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.MA",
      "q-fin.TR"
    ],
    "primary_category": "q-fin.CP",
    "comment": "Accepted PRIMA2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12516v1",
    "published_date": "2024-09-19 07:14:13 UTC",
    "updated_date": "2024-09-19 07:14:13 UTC"
  },
  {
    "arxiv_id": "2409.12992v1",
    "title": "DiffEditor: Enhancing Speech Editing with Semantic Enrichment and Acoustic Consistency",
    "authors": [
      "Yang Chen",
      "Yuhang Jia",
      "Shiwan Zhao",
      "Ziyue Jiang",
      "Haoran Li",
      "Jiarong Kang",
      "Yong Qin"
    ],
    "abstract": "As text-based speech editing becomes increasingly prevalent, the demand for\nunrestricted free-text editing continues to grow. However, existing speech\nediting techniques encounter significant challenges, particularly in\nmaintaining intelligibility and acoustic consistency when dealing with\nout-of-domain (OOD) text. In this paper, we introduce, DiffEditor, a novel\nspeech editing model designed to enhance performance in OOD text scenarios\nthrough semantic enrichment and acoustic consistency. To improve the\nintelligibility of the edited speech, we enrich the semantic information of\nphoneme embeddings by integrating word embeddings extracted from a pretrained\nlanguage model. Furthermore, we emphasize that interframe smoothing properties\nare critical for modeling acoustic consistency, and thus we propose a\nfirst-order loss function to promote smoother transitions at editing boundaries\nand enhance the overall fluency of the edited speech. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in both\nin-domain and OOD text scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12992v1",
    "published_date": "2024-09-19 07:11:54 UTC",
    "updated_date": "2024-09-19 07:11:54 UTC"
  },
  {
    "arxiv_id": "2409.13773v1",
    "title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
    "authors": [
      "Yi Cui"
    ],
    "abstract": "This paper presents a case study of coding tasks by the latest reasoning\nmodels of OpenAI, i.e. o1-preview and o1-mini, in comparison with other\nfrontier models. The o1 models deliver SOTA results for WebApp1K, a single-task\nbenchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling\nnumber of tasks and test cases. The new benchmark causes the o1 model\nperformances to decline significantly, falling behind Claude 3.5. Moreover,\nthey consistently fail when confronted with atypical yet correct test cases, a\ntrap non-reasoning models occasionally avoid. We hypothesize that the\nperformance variability is due to instruction comprehension. Specifically, the\nreasoning mechanism boosts performance when all expectations are captured,\nmeanwhile exacerbates errors when key expectations are missed, potentially\nimpacted by input lengths. As such, we argue that the coding success of\nreasoning models hinges on the top-notch base model and SFT to ensure\nmeticulous adherence to instructions.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13773v1",
    "published_date": "2024-09-19 06:58:02 UTC",
    "updated_date": "2024-09-19 06:58:02 UTC"
  },
  {
    "arxiv_id": "2409.18997v2",
    "title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent",
    "authors": [
      "Jiateng Liu",
      "Lin Ai",
      "Zizhou Liu",
      "Payam Karisani",
      "Zheng Hui",
      "May Fung",
      "Preslav Nakov",
      "Julia Hirschberg",
      "Heng Ji"
    ],
    "abstract": "Propaganda plays a critical role in shaping public opinion and fueling\ndisinformation. While existing research primarily focuses on identifying\npropaganda techniques, it lacks the ability to capture the broader motives and\nthe impacts of such content. To address these challenges, we introduce\npropainsight, a conceptual framework grounded in foundational social science\nresearch, which systematically dissects propaganda into techniques, arousal\nappeals, and underlying intent. propainsight offers a more granular\nunderstanding of how propaganda operates across different contexts.\nAdditionally, we present propagaze, a novel dataset that combines\nhuman-annotated data with high-quality synthetic data generated through a\nmeticulously designed pipeline. Our experiments show that off-the-shelf LLMs\nstruggle with propaganda analysis, but training with propagaze significantly\nimproves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span\nIoU in technique identification and 66.2% higher BertScore in appeal analysis\ncompared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited\nhuman-annotated data in data-sparse and cross-domain scenarios, showing its\npotential for comprehensive and generalizable propaganda analysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18997v2",
    "published_date": "2024-09-19 06:28:18 UTC",
    "updated_date": "2025-02-13 23:45:10 UTC"
  },
  {
    "arxiv_id": "2409.12500v1",
    "title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward",
    "authors": [
      "Dongheng Li",
      "Yongchang Hao",
      "Lili Mou"
    ],
    "abstract": "Large language models have become increasingly popular and demonstrated\nremarkable performance in various natural language processing (NLP) tasks.\nHowever, these models are typically computationally expensive and difficult to\nbe deployed in resource-constrained environments. In this paper, we propose\nLLMR, a novel knowledge distillation (KD) method based on a reward function\ninduced from large language models. We conducted experiments on multiple\ndatasets in the dialogue generation and summarization tasks. Empirical results\ndemonstrate that our LLMR approach consistently outperforms traditional KD\nmethods in different tasks and datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by LERC COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12500v1",
    "published_date": "2024-09-19 06:27:58 UTC",
    "updated_date": "2024-09-19 06:27:58 UTC"
  },
  {
    "arxiv_id": "2409.12490v2",
    "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs",
    "authors": [
      "Junlin Lv",
      "Yuan Feng",
      "Xike Xie",
      "Xin Jia",
      "Qirong Peng",
      "Guiming Xie"
    ],
    "abstract": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12490v2",
    "published_date": "2024-09-19 06:09:56 UTC",
    "updated_date": "2024-09-23 02:24:33 UTC"
  },
  {
    "arxiv_id": "2409.12479v1",
    "title": "Learning Multi-Manifold Embedding for Out-Of-Distribution Detection",
    "authors": [
      "Jeng-Lin Li",
      "Ming-Ching Chang",
      "Wei-Chao Chen"
    ],
    "abstract": "Detecting out-of-distribution (OOD) samples is crucial for trustworthy AI in\nreal-world applications. Leveraging recent advances in representation learning\nand latent embeddings, Various scoring algorithms estimate distributions beyond\nthe training data. However, a single embedding space falls short in\ncharacterizing in-distribution data and defending against diverse OOD\nconditions. This paper introduces a novel Multi-Manifold Embedding Learning\n(MMEL) framework, optimizing hypersphere and hyperbolic spaces jointly for\nenhanced OOD detection. MMEL generates representative embeddings and employs a\nprototype-aware scoring function to differentiate OOD samples. It operates with\nvery few OOD samples and requires no model retraining. Experiments on six open\ndatasets demonstrate MMEL's significant reduction in FPR while maintaining a\nhigh AUC compared to state-of-the-art distance-based OOD detection methods. We\nanalyze the effects of learning multiple manifolds and visualize OOD score\ndistributions across datasets. Notably, enrolling ten OOD samples without\nretraining achieves comparable FPR and AUC to modern outlier exposure methods\nusing 80 million outlier samples for model training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "European Conference on Computer Vision ECCV 2024 BEW Workshop Best\n  Paper",
    "pdf_url": "http://arxiv.org/pdf/2409.12479v1",
    "published_date": "2024-09-19 05:43:00 UTC",
    "updated_date": "2024-09-19 05:43:00 UTC"
  },
  {
    "arxiv_id": "2409.12477v2",
    "title": "ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend Conditioning",
    "authors": [
      "Daewoong Kim",
      "Hao-Wen Dong",
      "Dasaem Jeong"
    ],
    "abstract": "Modeling the natural contour of fundamental frequency (F0) plays a critical\nrole in music audio synthesis. However, transcribing and managing multiple F0\ncontours in polyphonic music is challenging, and explicit F0 contour modeling\nhas not yet been explored for polyphonic instrumental synthesis. In this paper,\nwe present ViolinDiff, a two-stage diffusion-based synthesis framework. For a\ngiven violin MIDI file, the first stage estimates the F0 contour as pitch bend\ninformation, and the second stage generates mel spectrogram incorporating these\nexpressive details. The quantitative metrics and listening test results show\nthat the proposed model generates more realistic violin sounds than the model\nwithout explicit pitch bend modeling. Audio samples are available online:\ndaewoung.github.io/ViolinDiff-Demo.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12477v2",
    "published_date": "2024-09-19 05:39:19 UTC",
    "updated_date": "2025-02-04 11:43:34 UTC"
  },
  {
    "arxiv_id": "2409.12472v1",
    "title": "TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN",
    "authors": [
      "Ziyi Liu",
      "Dengpan Ye",
      "Long Tang",
      "Yunming Zhang",
      "Jiacheng Deng"
    ],
    "abstract": "With the development of artificial intelligence, neural networks play a key\nrole in network intrusion detection systems (NIDS). Despite the tremendous\nadvantages, neural networks are susceptible to adversarial attacks. To improve\nthe reliability of NIDS, many research has been conducted and plenty of\nsolutions have been proposed. However, the existing solutions rarely consider\nthe adversarial attacks against recurrent neural networks (RNN) with time\nsteps, which would greatly affect the application of NIDS in real world.\nTherefore, we first propose a novel RNN adversarial attack model based on\nfeature reconstruction called \\textbf{T}emporal adversarial \\textbf{E}xamples\n\\textbf{A}ttack \\textbf{M}odel \\textbf{(TEAM)}, which applied to time series\ndata and reveals the potential connection between adversarial and time steps in\nRNN. That is, the past adversarial examples within the same time steps can\ntrigger further attacks on current or future original examples. Moreover, TEAM\nleverages Time Dilation (TD) to effectively mitigates the effect of temporal\namong adversarial examples within the same time steps. Experimental results\nshow that in most attack categories, TEAM improves the misjudgment rate of NIDS\non both black and white boxes, making the misjudgment rate reach more than\n96.68%. Meanwhile, the maximum increase in the misjudgment rate of the NIDS for\nsubsequent original samples exceeds 95.57%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12472v1",
    "published_date": "2024-09-19 05:26:04 UTC",
    "updated_date": "2024-09-19 05:26:04 UTC"
  },
  {
    "arxiv_id": "2409.12471v1",
    "title": "Arena 4.0: A Comprehensive ROS2 Development and Benchmarking Platform for Human-centric Navigation Using Generative-Model-based Environment Generation",
    "authors": [
      "Volodymyr Shcherbyna1",
      "Linh Kästner",
      "Diego Diaz",
      "Huu Giang Nguyen",
      "Maximilian Ho-Kyoung Schreff",
      "Tim Lenz",
      "Jonas Kreutz",
      "Ahmed Martban",
      "Huajian Zeng",
      "Harold Soh"
    ],
    "abstract": "Building on the foundations of our previous work, this paper introduces Arena\n4.0, a significant advancement over Arena 3.0, Arena-Bench, Arena 1.0, and\nArena 2.0. Arena 4.0 offers three key novel contributions: (1) a\ngenerative-model-based world and scenario generation approach that utilizes\nlarge language models (LLMs) and diffusion models to dynamically generate\ncomplex, human-centric environments from text prompts or 2D floorplans, useful\nfor the development and benchmarking of social navigation strategies; (2) a\ncomprehensive 3D model database, extendable with additional 3D assets that are\nsemantically linked and annotated for dynamic spawning and arrangement within\n3D worlds; and (3) a complete migration to ROS 2, enabling compatibility with\nmodern hardware and enhanced functionalities for improved navigation,\nusability, and easier deployment on real robots. We evaluated the platform's\nperformance through a comprehensive user study, demonstrating significant\nimprovements in usability and efficiency compared to previous versions. Arena\n4.0 is openly available at https://github.com/Arena-Rosnav.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12471v1",
    "published_date": "2024-09-19 05:20:13 UTC",
    "updated_date": "2024-09-19 05:20:13 UTC"
  },
  {
    "arxiv_id": "2409.12468v2",
    "title": "Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation",
    "authors": [
      "Dongwon Jung",
      "Qin Liu",
      "Tenghao Huang",
      "Ben Zhou",
      "Muhao Chen"
    ],
    "abstract": "Retrieval-augmented generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieved from external\nsources. However, it often struggles to cope with inconsistent and irrelevant\ninformation that can distract the LM from its tasks, especially when multiple\nevidence pieces are required. While compressing the retrieved evidence with a\ncompression model aims to address this issue, the compressed evidence may still\nbe unfamiliar to the target model used for downstream tasks, potentially\nfailing to utilize the evidence effectively. We propose FaviComp\n(Familarity-Aware Evidence Compression), a novel training-free evidence\ncompression technique that makes retrieved evidence more familiar to the target\nmodel, while seamlessly integrating parametric knowledge from the model.\nExperimental results show that FaviComp consistently outperforms most recent\nevidence compression baselines across multiple open-domain QA datasets,\nimproving accuracy by up to 28.1% while achieving high compression rates.\nAdditionally, we demonstrate the effective integration of both parametric and\nnon-parametric knowledge during evidence compression.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12468v2",
    "published_date": "2024-09-19 05:14:55 UTC",
    "updated_date": "2024-12-17 00:25:46 UTC"
  },
  {
    "arxiv_id": "2409.12467v2",
    "title": "SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference",
    "authors": [
      "Zhen Chen",
      "Xingjian Luo",
      "Jinlin Wu",
      "Long Bai",
      "Zhen Lei",
      "Hongliang Ren",
      "Sebastien Ourselin",
      "Hongbin Liu"
    ],
    "abstract": "Surgical phase recognition is critical for assisting surgeons in\nunderstanding surgical videos. Existing studies focused more on online surgical\nphase recognition, by leveraging preceding frames to predict the current frame.\nDespite great progress, they formulated the task as a series of frame-wise\nclassification, which resulted in a lack of global context of the entire\nprocedure and incoherent predictions. Moreover, besides online analysis,\naccurate offline surgical phase recognition is also in significant clinical\nneed for retrospective analysis, and existing online algorithms do not fully\nanalyze the entire video, thereby limiting accuracy in offline analysis. To\novercome these challenges and enhance both online and offline inference\ncapabilities, we propose a universal Surgical Phase Localization Network, named\nSurgPLAN++, with the principle of temporal detection. To ensure a global\nunderstanding of the surgical procedure, we devise a phase localization\nstrategy for SurgPLAN++ to predict phase segments across the entire video\nthrough phase proposals. For online analysis, to generate high-quality phase\nproposals, SurgPLAN++ incorporates a data augmentation strategy to extend the\nstreaming video into a pseudo-complete video through mirroring,\ncenter-duplication, and down-sampling. For offline analysis, SurgPLAN++\ncapitalizes on its global phase prediction framework to continuously refine\npreceding predictions during each online inference step, thereby significantly\nimproving the accuracy of phase recognition. We perform extensive experiments\nto validate the effectiveness, and our SurgPLAN++ achieves remarkable\nperformance in both online and offline modes, which outperforms\nstate-of-the-art methods. The source code is available at\nhttps://github.com/franciszchen/SurgPLAN-Plus.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work is accepted by IEEE ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12467v2",
    "published_date": "2024-09-19 05:08:33 UTC",
    "updated_date": "2025-02-13 19:57:52 UTC"
  },
  {
    "arxiv_id": "2409.12454v1",
    "title": "FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling",
    "authors": [
      "Enze Shi",
      "Kui Zhao",
      "Qilong Yuan",
      "Jiaqi Wang",
      "Huawen Hu",
      "Sigang Yu",
      "Shu Zhang"
    ],
    "abstract": "Electroencephalography (EEG) is a vital tool to measure and record brain\nactivity in neuroscience and clinical applications, yet its potential is\nconstrained by signal heterogeneity, low signal-to-noise ratios, and limited\nlabeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a\nnovel approach using adaptive temporal-lateral attention scaling to address\nabove-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of\nscalp and intracranial EEG recordings, comprising 745M parameters trained for\n1,096k steps. Our model introduces two key innovations: a time-frequency fusion\nembedding technique and an adaptive time-lateral attention scaling (ATLAS)\nmechanism. These components synergistically capture complex temporal and\nspectral EEG dynamics, enabling FoME to adapt to varying patterns across\ndiverse data streams and facilitate robust multi-channel modeling. Evaluations\nacross four downstream tasks demonstrate FoME's superior performance in\nclassification and forecasting applications, consistently achieving\nstate-of-the-art results. To conclude, FoME establishes a new paradigm for EEG\nanalysis, offering a versatile foundation that advances brain-computer\ninterfaces, clinical diagnostics, and cognitive research across neuroscience\nand related fields. Our code will be available at\nhttps://github.com/1061413241/FoME.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12454v1",
    "published_date": "2024-09-19 04:22:40 UTC",
    "updated_date": "2024-09-19 04:22:40 UTC"
  },
  {
    "arxiv_id": "2410.02805v1",
    "title": "Trust-informed Decision-Making Through An Uncertainty-Aware Stacked Neural Networks Framework: Case Study in COVID-19 Classification",
    "authors": [
      "Hassan Gharoun",
      "Mohammad Sadegh Khorshidi",
      "Fang Chen",
      "Amir H. Gandomi"
    ],
    "abstract": "This study presents an uncertainty-aware stacked neural networks model for\nthe reliable classification of COVID-19 from radiological images. The model\naddresses the critical gap in uncertainty-aware modeling by focusing on\naccurately identifying confidently correct predictions while alerting users to\nconfidently incorrect and uncertain predictions, which can promote trust in\nautomated systems. The architecture integrates uncertainty quantification\nmethods, including Monte Carlo dropout and ensemble techniques, to enhance\npredictive reliability by assessing the certainty of diagnostic predictions.\nWithin a two-tier model framework, the tier one model generates initial\npredictions and associated uncertainties, which the second tier model uses to\nproduce a trust indicator alongside the diagnostic outcome. This dual-output\nmodel not only predicts COVID-19 cases but also provides a trust flag,\nindicating the reliability of each diagnosis and aiming to minimize the need\nfor retesting and expert verification. The effectiveness of this approach is\ndemonstrated through extensive experiments on the COVIDx CXR-4 dataset, showing\na novel approach in identifying and handling confidently incorrect cases and\nuncertain cases, thus enhancing the trustworthiness of automated diagnostics in\nclinical settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "68T07"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 7 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.02805v1",
    "published_date": "2024-09-19 04:20:12 UTC",
    "updated_date": "2024-09-19 04:20:12 UTC"
  },
  {
    "arxiv_id": "2409.12450v1",
    "title": "Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency",
    "authors": [
      "Mansoor Ali Teevno",
      "Rafael Martinez-Garcia-Pena",
      "Gilberto Ochoa-Ruiz",
      "Sharib Ali"
    ],
    "abstract": "Frequent monitoring is necessary to stratify individuals based on their\nlikelihood of developing gastrointestinal (GI) cancer precursors. In clinical\npractice, white-light imaging (WLI) and complementary modalities such as\nnarrow-band imaging (NBI) and fluorescence imaging are used to assess risk\nareas. However, conventional deep learning (DL) models show degraded\nperformance due to the domain gap when a model is trained on one modality and\ntested on a different one. In our earlier approach, we used a superpixel-based\nmethod referred to as \"SUPRA\" to effectively learn domain-invariant information\nusing color and space distances to generate groups of pixels. One of the main\nlimitations of this earlier work is that the aggregation does not exploit\nstructural information, making it suboptimal for segmentation tasks, especially\nfor polyps and heterogeneous color distributions. Therefore, in this work, we\npropose an approach for style-content disentanglement using instance\nnormalization and instance selective whitening (ISW) for improved domain\ngeneralization when combined with SUPRA. We evaluate our approach on two\ndatasets: EndoUDA Barrett's Esophagus and EndoUDA polyps, and compare its\nperformance with three state-of-the-art (SOTA) methods. Our findings\ndemonstrate a notable enhancement in performance compared to both baseline and\nSOTA methods across the target domain data. Specifically, our approach\nexhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three\nSOTA methods on the polyp dataset. Additionally, it surpassed the second-best\nmethod (EndoUDA) on the Barrett's Esophagus dataset by nearly 2%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12450v1",
    "published_date": "2024-09-19 04:10:04 UTC",
    "updated_date": "2024-09-19 04:10:04 UTC"
  },
  {
    "arxiv_id": "2409.12447v2",
    "title": "Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts",
    "authors": [
      "Jenny T. Liang",
      "Melissa Lin",
      "Nikitha Rao",
      "Brad A. Myers"
    ],
    "abstract": "Generative pre-trained models power intelligent software features used by\nmillions of users controlled by developer-written natural language prompts.\nDespite the impact of prompt-powered software, little is known about its\ndevelopment process and its relationship to programming. In this work, we argue\nthat some prompts are programs and that the development of prompts is a\ndistinct phenomenon in programming known as \"prompt programming\". We develop an\nunderstanding of prompt programming using Straussian grounded theory through\ninterviews with 20 developers engaged in prompt development across a variety of\ncontexts, models, domains, and prompt structures. We contribute 15 observations\nto form a preliminary understanding of current prompt programming practices.\nFor example, rather than building mental models of code, prompt programmers\ndevelop mental models of the foundation model (FM)'s behavior on the prompt by\ninteracting with the FM. While prior research shows that experts have\nwell-formed mental models, we find that prompt programmers who have developed\ndozens of prompts still struggle to develop reliable mental models. Our\nobservations show that prompt programming differs from traditional software\ndevelopment, motivating the creation of prompt programming tools and providing\nimplications for software engineering stakeholders.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to FSE'25",
    "pdf_url": "http://arxiv.org/pdf/2409.12447v2",
    "published_date": "2024-09-19 03:55:49 UTC",
    "updated_date": "2025-04-25 03:55:39 UTC"
  },
  {
    "arxiv_id": "2409.12446v2",
    "title": "Neural Networks Generalize on Low Complexity Data",
    "authors": [
      "Sourav Chatterjee",
      "Timothy Sudijono"
    ],
    "abstract": "We show that feedforward neural networks with ReLU activation generalize on\nlow complexity data, suitably defined. Given i.i.d. data generated from a\nsimple programming language, the minimum description length (MDL) feedforward\nneural network which interpolates the data generalizes with high probability.\nWe define this simple programming language, along with a notion of description\nlength of such networks. We provide several examples on basic computational\ntasks, such as checking primality of a natural number, and more. For primality\ntesting, our theorem shows the following. Suppose that we draw an i.i.d. sample\nof $\\Theta(N^{\\delta}\\ln N)$ numbers uniformly at random from $1$ to $N$, where\n$\\delta\\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and\n$0$ if it is not. Then with high probability, the MDL network fitted to this\ndata accurately answers whether a newly drawn number between $1$ and $N$ is a\nprime or not, with test error $\\leq O(N^{-\\delta})$. Note that the network is\nnot designed to detect primes; minimum description learning discovers a network\nwhich does so.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages. V2: Further references added",
    "pdf_url": "http://arxiv.org/pdf/2409.12446v2",
    "published_date": "2024-09-19 03:54:49 UTC",
    "updated_date": "2024-10-29 03:53:59 UTC"
  },
  {
    "arxiv_id": "2409.12444v3",
    "title": "A Lightweight and Real-Time Binaural Speech Enhancement Model with Spatial Cues Preservation",
    "authors": [
      "Jingyuan Wang",
      "Jie Zhang",
      "Shihao Chen",
      "Miao Sun"
    ],
    "abstract": "Binaural speech enhancement (BSE) aims to jointly improve the speech quality\nand intelligibility of noisy signals received by hearing devices and preserve\nthe spatial cues of the target for natural listening. Existing methods often\nsuffer from the compromise between noise reduction (NR) capacity and spatial\ncues preservation (SCP) accuracy and a high computational demand in complex\nacoustic scenes. In this work, we present a learning-based lightweight binaural\ncomplex convolutional network (LBCCN), which excels in NR by filtering\nlow-frequency bands and keeping the rest. Additionally, our approach explicitly\nincorporates the estimation of interchannel relative acoustic transfer function\nto ensure the spatial cues fidelity and speech clarity. Results show that the\nproposed LBCCN can achieve a comparable NR performance to state-of-the-art\nmethods under fixed-speaker conditions, but with a much lower computational\ncost and a certain degree of SCP capability. The reproducible code and audio\nexamples are available at https://github.com/jywanng/LBCCN.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12444v3",
    "published_date": "2024-09-19 03:52:50 UTC",
    "updated_date": "2025-01-08 07:19:14 UTC"
  },
  {
    "arxiv_id": "2409.12440v1",
    "title": "Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction",
    "authors": [
      "Xin Lian",
      "Nishant Baglodi",
      "Christopher J. MacLellan"
    ],
    "abstract": "This paper introduces Cobweb4L, a novel approach for efficient language model\nlearning that supports masked word prediction. The approach builds on Cobweb,\nan incremental system that learns a hierarchy of probabilistic concepts. Each\nconcept stores the frequencies of words that appear in instances tagged with\nthat concept label. The system utilizes an attribute value representation to\nencode words and their surrounding context into instances. Cobweb4L uses the\ninformation theoretic variant of category utility and a new performance\nmechanism that leverages multiple concepts to generate predictions. We\ndemonstrate that with these extensions it significantly outperforms prior\nCobweb performance mechanisms that use only a single node to generate\npredictions. Further, we demonstrate that Cobweb4L learns rapidly and achieves\nperformance comparable to and even superior to Word2Vec. Next, we show that\nCobweb4L and Word2Vec outperform BERT in the same task with less training data.\nFinally, we discuss future work to make our conclusions more robust and\ninclusive.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the Eleventh Annual Conference on Advances in Cognitive\n  Systems",
    "pdf_url": "http://arxiv.org/pdf/2409.12440v1",
    "published_date": "2024-09-19 03:48:31 UTC",
    "updated_date": "2024-09-19 03:48:31 UTC"
  },
  {
    "arxiv_id": "2410.03559v1",
    "title": "Optimizing food taste sensory evaluation through neural network-based taste electroencephalogram channel selection",
    "authors": [
      "Xiuxin Xia",
      "Qun Wang",
      "He Wang",
      "Chenrui Liu",
      "Pengwei Li",
      "Yan Shi",
      "Hong Men"
    ],
    "abstract": "The taste electroencephalogram (EEG) evoked by the taste stimulation can\nreflect different brain patterns and be used in applications such as sensory\nevaluation of food. However, considering the computational cost and efficiency,\nEEG data with many channels has to face the critical issue of channel\nselection. This paper proposed a channel selection method called class\nactivation mapping with attention (CAM-Attention). The CAM-Attention method\ncombined a convolutional neural network with channel and spatial attention\n(CNN-CSA) model with a gradient-weighted class activation mapping (Grad-CAM)\nmodel. The CNN-CSA model exploited key features in EEG data by attention\nmechanism, and the Grad-CAM model effectively realized the visualization of\nfeature regions. Then, channel selection was effectively implemented based on\nfeature regions. Finally, the CAM-Attention method reduced the computational\nburden of taste EEG recognition and effectively distinguished the four tastes.\nIn short, it has excellent recognition performance and provides effective\ntechnical support for taste sensory evaluation.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP",
    "comment": "33 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.03559v1",
    "published_date": "2024-09-19 03:36:22 UTC",
    "updated_date": "2024-09-19 03:36:22 UTC"
  },
  {
    "arxiv_id": "2409.12431v4",
    "title": "FlexiTex: Enhancing Texture Generation with Visual Guidance",
    "authors": [
      "DaDong Jiang",
      "Xianghui Yang",
      "Zibo Zhao",
      "Sheng Zhang",
      "Jiaao Yu",
      "Zeqiang Lai",
      "Shaoxiong Yang",
      "Chunchao Guo",
      "Xiaobo Zhou",
      "Zhihui Ke"
    ],
    "abstract": "Recent texture generation methods achieve impressive results due to the\npowerful generative prior they leverage from large-scale text-to-image\ndiffusion models. However, abstract textual prompts are limited in providing\nglobal textural or shape information, which results in the texture generation\nmethods producing blurry or inconsistent patterns. To tackle this, we present\nFlexiTex, embedding rich information via visual guidance to generate a\nhigh-quality texture. The core of FlexiTex is the Visual Guidance Enhancement\nmodule, which incorporates more specific information from visual guidance to\nreduce ambiguity in the text prompt and preserve high-frequency details. To\nfurther enhance the visual guidance, we introduce a Direction-Aware Adaptation\nmodule that automatically designs direction prompts based on different camera\nposes, avoiding the Janus problem and maintaining semantically global\nconsistency. Benefiting from the visual guidance, FlexiTex produces\nquantitatively and qualitatively sound results, demonstrating its potential to\nadvance texture generation for real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025, Project Page:\n  https://patrickddj.github.io/FlexiTex/",
    "pdf_url": "http://arxiv.org/pdf/2409.12431v4",
    "published_date": "2024-09-19 03:24:22 UTC",
    "updated_date": "2024-12-27 12:18:55 UTC"
  },
  {
    "arxiv_id": "2409.12428v1",
    "title": "Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift",
    "authors": [
      "Oscar Blessed Deho",
      "Michael Bewong",
      "Selasi Kwashie",
      "Jiuyong Li",
      "Jixue Liu",
      "Lin Liu",
      "Srecko Joksimovic"
    ],
    "abstract": "Over the last few decades, machine learning (ML) applications have grown\nexponentially, yielding several benefits to society. However, these benefits\nare tempered with concerns of discriminatory behaviours exhibited by ML models.\nIn this regard, fairness in machine learning has emerged as a priority research\narea. Consequently, several fairness metrics and algorithms have been developed\nto mitigate against discriminatory behaviours that ML models may possess. Yet\nstill, very little attention has been paid to the problem of naturally\noccurring changes in data patterns (\\textit{aka} data distributional drift),\nand its impact on fairness algorithms and metrics. In this work, we study this\nproblem comprehensively by analyzing 4 fairness-unaware baseline algorithms and\n7 fairness-aware algorithms, carefully curated to cover the breadth of its\ntypology, across 5 datasets including public and proprietary data, and\nevaluated them using 3 predictive performance and 10 fairness metrics. In doing\nso, we show that (1) data distributional drift is not a trivial occurrence, and\nin several cases can lead to serious deterioration of fairness in so-called\nfair models; (2) contrary to some existing literature, the size and direction\nof data distributional drift is not correlated to the resulting size and\ndirection of unfairness; and (3) choice of, and training of fairness algorithms\nis impacted by the effect of data distributional drift which is largely ignored\nin the literature. Emanating from our findings, we synthesize several policy\nimplications of data distributional drift on fairness algorithms that can be\nvery relevant to stakeholders and practitioners.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12428v1",
    "published_date": "2024-09-19 03:18:12 UTC",
    "updated_date": "2024-09-19 03:18:12 UTC"
  },
  {
    "arxiv_id": "2409.18996v1",
    "title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models",
    "authors": [
      "Shengsheng Qian",
      "Zuyi Zhou",
      "Dizhan Xue",
      "Bing Wang",
      "Changsheng Xu"
    ],
    "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and\ndrawing inferences across divergent sensory modalities, is increasingly\nrecognized as a crucial capability in the progression toward more sophisticated\nand anthropomorphic artificial intelligence systems. Large Language Models\n(LLMs) represent a class of AI algorithms specifically engineered to parse,\nproduce, and engage with human language on an extensive scale. The recent trend\nof deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches\nfor enhancing their effectiveness. This survey offers a nuanced exposition of\ncurrent methodologies applied in CMR using LLMs, classifying these into a\ndetailed three-tiered taxonomy. Moreover, the survey delves into the principal\ndesign strategies and operational techniques of prototypical models within this\ndomain. Additionally, it articulates the prevailing challenges associated with\nthe integration of LLMs in CMR and identifies prospective research directions.\nTo sum up, this survey endeavors to expedite progress within this burgeoning\nfield by endowing scholars with a holistic and detailed vista, showcasing the\nvanguard of current research whilst pinpointing potential avenues for\nadvancement. An associated GitHub repository that collects the relevant papers\ncan be found at\nhttps://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "A.1"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.18996v1",
    "published_date": "2024-09-19 02:51:54 UTC",
    "updated_date": "2024-09-19 02:51:54 UTC"
  },
  {
    "arxiv_id": "2409.15370v2",
    "title": "Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models",
    "authors": [
      "Alexius Wadell",
      "Anoushka Bhutani",
      "Venkatasubramanian Viswanathan"
    ],
    "abstract": "Text-based foundation models have become an important part of scientific\ndiscovery, with molecular foundation models accelerating advancements in\nmolecular design and materials science. However, existing models are\nconstrained by closed-vocabulary tokenizers which capture only a fraction of\nmolecular space. In this work, we systematically evaluate thirty tokenizers,\nincluding 19 chemistry-specific ones, for their coverage of the SMILES\nmolecular representation language, revealing significant gaps. To assess the\nimpact of tokenizer choice, we introduce n-gram language models as a low-cost\nproxy and validate their effectiveness by training and fine-tuning 18\nRoBERTa-style encoders for molecular property prediction. To overcome the\nlimitations of existing tokenizers, we propose two new tokenizers -- Smirk and\nSmirk-GPE -- with full coverage of the OpenSMILES specification. Our results\nhighlight the need for open-vocabulary modeling and chemically diverse\nbenchmarks in cheminformatics. The proposed tokenizer framework systematically\nintegrates nuclear, electronic, and geometric degrees of freedom; this\nfacilitates applications in pharmacology, agriculture, biology, and energy\nstorage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15370v2",
    "published_date": "2024-09-19 02:36:04 UTC",
    "updated_date": "2025-02-07 18:36:17 UTC"
  },
  {
    "arxiv_id": "2410.02804v1",
    "title": "Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities",
    "authors": [
      "Qi Fan",
      "Hongyu Yuan",
      "Haolin Zuo",
      "Rui Liu",
      "Guanglai Gao"
    ],
    "abstract": "Multimodal emotion recognition utilizes complete multimodal information and\nrobust multimodal joint representation to gain high performance. However, the\nideal condition of full modality integrity is often not applicable in reality\nand there always appears the situation that some modalities are missing. For\nexample, video, audio, or text data is missing due to sensor failure or network\nbandwidth problems, which presents a great challenge to MER research.\nTraditional methods extract useful information from the complete modalities and\nreconstruct the missing modalities to learn robust multimodal joint\nrepresentation. These methods have laid a solid foundation for research in this\nfield, and to a certain extent, alleviated the difficulty of multimodal emotion\nrecognition under missing modalities. However, relying solely on internal\nreconstruction and multimodal joint learning has its limitations, especially\nwhen the missing information is critical for emotion recognition. To address\nthis challenge, we propose a novel framework of Retrieval Augment for Missing\nModality Multimodal Emotion Recognition (RAMER), which introduces similar\nmultimodal emotion data to enhance the performance of emotion recognition under\nmissing modalities. By leveraging databases, that contain related multimodal\nemotion data, we can retrieve similar multimodal emotion information to fill in\nthe gaps left by missing modalities. Various experimental results demonstrate\nthat our framework is superior to existing state-of-the-art approaches in\nmissing modality MER tasks. Our whole project is publicly available on\nhttps://github.com/WooyoohL/Retrieval_Augment_MER.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under reviewing",
    "pdf_url": "http://arxiv.org/pdf/2410.02804v1",
    "published_date": "2024-09-19 02:31:12 UTC",
    "updated_date": "2024-09-19 02:31:12 UTC"
  },
  {
    "arxiv_id": "2409.12415v1",
    "title": "Multichannel-to-Multichannel Target Sound Extraction Using Direction and Timestamp Clues",
    "authors": [
      "Dayun Choi",
      "Jung-Woo Choi"
    ],
    "abstract": "We propose a multichannel-to-multichannel target sound extraction (M2M-TSE)\nframework for separating multichannel target signals from a multichannel\nmixture of sound sources. Target sound extraction (TSE) isolates a specific\ntarget signal using user-provided clues, typically focusing on single-channel\nextraction with class labels or temporal activation maps. However, to preserve\nand utilize spatial information in multichannel audio signals, it is essential\nto extract multichannel signals of a target sound source. Moreover, the clue\nfor extraction can also include spatial or temporal cues like\ndirection-of-arrival (DoA) or timestamps of source activation. To address these\nchallenges, we present an M2M framework that extracts a multichannel sound\nsignal based on spatio-temporal clues. We demonstrate that our\ntransformer-based architecture can successively accomplish the M2M-TSE task for\nmultichannel signals synthesized from audio signals of diverse classes in\ndifferent room environments. Furthermore, we show that the multichannel\nextraction task introduces sufficient inductive bias in the DNN, allowing it to\ndirectly handle DoA clues without utilizing hand-crafted spatial features.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.12415v1",
    "published_date": "2024-09-19 02:30:49 UTC",
    "updated_date": "2024-09-19 02:30:49 UTC"
  },
  {
    "arxiv_id": "2409.12409v1",
    "title": "LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations",
    "authors": [
      "Michael Mink",
      "Thomas Monninger",
      "Steffen Staab"
    ],
    "abstract": "In autonomous driving, High Definition (HD) maps provide a complete lane\nmodel that is not limited by sensor range and occlusions. However, the\ngeneration and upkeep of HD maps involves periodic data collection and human\nannotations, limiting scalability. To address this, we investigate automating\nthe lane model generation and the use of sparse vehicle observations instead of\ndense sensor measurements. For our approach, a pre-processing step generates\npolylines by aligning and aggregating observed lane boundaries. Aligned driven\ntraces are used as starting points for predicting lane pairs defined by the\nleft and right boundary points. We propose Lane Model Transformer Network\n(LMT-Net), an encoder-decoder neural network architecture that performs\npolyline encoding and predicts lane pairs and their connectivity. A lane graph\nis formed by using predicted lane pairs as nodes and predicted lane\nconnectivity as edges. We evaluate the performance of LMT-Net on an internal\ndataset that consists of multiple vehicle observations as well as human\nannotations as Ground Truth (GT). The evaluation shows promising results and\ndemonstrates superior performance compared to the implemented baseline on both\nhighway and non-highway Operational Design Domain (ODD).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for 2024 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.12409v1",
    "published_date": "2024-09-19 02:14:35 UTC",
    "updated_date": "2024-09-19 02:14:35 UTC"
  },
  {
    "arxiv_id": "2409.12405v1",
    "title": "On the Effectiveness of LLMs for Manual Test Verifications",
    "authors": [
      "Myron David Lucena Campos Peixoto",
      "Davy de Medeiros Baia",
      "Nathalia Nascimento",
      "Paulo Alencar",
      "Baldoino Fonseca",
      "Márcio Ribeiro"
    ],
    "abstract": "Background: Manual testing is vital for detecting issues missed by automated\ntests, but specifying accurate verifications is challenging. Aims: This study\naims to explore the use of Large Language Models (LLMs) to produce\nverifications for manual tests. Method: We conducted two independent and\ncomplementary exploratory studies. The first study involved using 2\nclosed-source and 6 open-source LLMs to generate verifications for manual test\nsteps and evaluate their similarity to original verifications. The second study\ninvolved recruiting software testing professionals to assess their perception\nand agreement with the generated verifications compared to the original ones.\nResults: The open-source models Mistral-7B and Phi-3-mini-4k demonstrated\neffectiveness and consistency comparable to closed-source models like\nGemini-1.5-flash and GPT-3.5-turbo in generating manual test verifications.\nHowever, the agreement level among professional testers was slightly above 40%,\nindicating both promise and room for improvement. While some LLM-generated\nverifications were considered better than the originals, there were also\nconcerns about AI hallucinations, where verifications significantly deviated\nfrom expectations. Conclusion: We contributed by generating a dataset of 37,040\ntest verifications using 8 different LLMs. Although the models show potential,\nthe relatively modest 40% agreement level highlights the need for further\nrefinement. Enhancing the accuracy, relevance, and clarity of the generated\nverifications is crucial to ensure greater reliability in real-world testing\nscenarios.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.12405v1",
    "published_date": "2024-09-19 02:03:04 UTC",
    "updated_date": "2024-09-19 02:03:04 UTC"
  },
  {
    "arxiv_id": "2409.12403v1",
    "title": "Preference Alignment Improves Language Model-Based TTS",
    "authors": [
      "Jinchuan Tian",
      "Chunlei Zhang",
      "Jiatong Shi",
      "Hao Zhang",
      "Jianwei Yu",
      "Shinji Watanabe",
      "Dong Yu"
    ],
    "abstract": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM)-based systems offer competitive performance to their counterparts. Further\noptimization can be achieved through preference alignment algorithms, which\nadjust LMs to align with the preferences of reward models, enhancing the\ndesirability of the generated content. This study presents a thorough empirical\nevaluation of how preference alignment algorithms, particularly Direct\nPreference Optimization (DPO), enhance LM-based TTS. With a 1.15B parameter\nLM-based TTS model, we demonstrate that preference alignment consistently\nimproves intelligibility, speaker similarity, and proxy subjective evaluation\nscores, with the latter two metrics surpassing even human speech in certain\nevaluations. We also show preference alignment is applicable to low-resource\nscenarios and effectively generalized to out-of-domain applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12403v1",
    "published_date": "2024-09-19 01:58:19 UTC",
    "updated_date": "2024-09-19 01:58:19 UTC"
  },
  {
    "arxiv_id": "2409.12397v2",
    "title": "Learning to Coordinate without Communication under Incomplete Information",
    "authors": [
      "Shenghui Chen",
      "Shufang Zhu",
      "Giuseppe De Giacomo",
      "Ufuk Topcu"
    ],
    "abstract": "Achieving seamless coordination in cooperative games is a crucial challenge\nin artificial intelligence, particularly when players operate under incomplete\ninformation. A common strategy to mitigate this information asymmetry involves\nleveraging explicit communication. However, direct (verbal) communication is\nnot always feasible due to factors such as transmission loss. Leveraging the\ngame Gnomes at Night, we explore how effective coordination can be achieved\nwithout verbal communication, relying solely on observing each other's actions.\nWe demonstrate how an autonomous agent can learn to cooperate by interpreting\nits partner's sequences of actions, which are used to hint at its intents. Our\napproach generates a non-Markovian strategy for the agent by learning a\ndeterministic finite automaton for each possible action and integrating these\nautomata into a finite-state transducer. Experimental results in a Gnomes at\nNight testbed show that, even without direct communication, one can learn\neffective cooperation strategies. Such strategies achieve significantly higher\nsuccess rates and require fewer steps to complete the game compared to\nuncoordinated ones, and perform almost as well as in the case direct\ncommunication is allowed.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12397v2",
    "published_date": "2024-09-19 01:41:41 UTC",
    "updated_date": "2025-02-05 19:00:07 UTC"
  },
  {
    "arxiv_id": "2409.12396v1",
    "title": "ARTAI: An Evaluation Platform to Assess Societal Risk of Recommender Algorithms",
    "authors": [
      "Qin Ruan",
      "Jin Xu",
      "Ruihai Dong",
      "Arjumand Younus",
      "Tai Tan Mai",
      "Barry O'Sullivan",
      "Susan Leavy"
    ],
    "abstract": "Societal risk emanating from how recommender algorithms disseminate content\nonline is now well documented. Emergent regulation aims to mitigate this risk\nthrough ethical audits and enabling new research on the social impact of\nalgorithms. However, there is currently a need for tools and methods that\nenable such evaluation. This paper presents ARTAI, an evaluation environment\nthat enables large-scale assessments of recommender algorithms to identify\nharmful patterns in how content is distributed online and enables the\nimplementation of new regulatory requirements for increased transparency in\nrecommender systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "H.3.3; I.2.7; I.5.1"
    ],
    "primary_category": "cs.CY",
    "comment": "3 pages, 1 figure, accepted at FAccTRec 2024 Workshop, RecSys 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.12396v1",
    "published_date": "2024-09-19 01:39:51 UTC",
    "updated_date": "2024-09-19 01:39:51 UTC"
  },
  {
    "arxiv_id": "2409.12394v1",
    "title": "ITPatch: An Invisible and Triggered Physical Adversarial Patch against Traffic Sign Recognition",
    "authors": [
      "Shuai Yuan",
      "Hongwei Li",
      "Xingshuo Han",
      "Guowen Xu",
      "Wenbo Jiang",
      "Tao Ni",
      "Qingchuan Zhao",
      "Yuguang Fang"
    ],
    "abstract": "Physical adversarial patches have emerged as a key adversarial attack to\ncause misclassification of traffic sign recognition (TSR) systems in the real\nworld. However, existing adversarial patches have poor stealthiness and attack\nall vehicles indiscriminately once deployed. In this paper, we introduce an\ninvisible and triggered physical adversarial patch (ITPatch) with a novel\nattack vector, i.e., fluorescent ink, to advance the state-of-the-art. It\napplies carefully designed fluorescent perturbations to a target sign, an\nattacker can later trigger a fluorescent effect using invisible ultraviolet\nlight, causing the TSR system to misclassify the sign and potentially resulting\nin traffic accidents. We conducted a comprehensive evaluation to investigate\nthe effectiveness of ITPatch, which shows a success rate of 98.31% in low-light\nconditions. Furthermore, our attack successfully bypasses five popular defenses\nand achieves a success rate of 96.72%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12394v1",
    "published_date": "2024-09-19 01:36:54 UTC",
    "updated_date": "2024-09-19 01:36:54 UTC"
  },
  {
    "arxiv_id": "2409.12388v2",
    "title": "Disentangling Speakers in Multi-Talker Speech Recognition with Speaker-Aware CTC",
    "authors": [
      "Jiawen Kang",
      "Lingwei Meng",
      "Mingyu Cui",
      "Yuejiao Wang",
      "Xixin Wu",
      "Xunying Liu",
      "Helen Meng"
    ],
    "abstract": "Multi-talker speech recognition (MTASR) faces unique challenges in\ndisentangling and transcribing overlapping speech. To address these challenges,\nthis paper investigates the role of Connectionist Temporal Classification (CTC)\nin speaker disentanglement when incorporated with Serialized Output Training\n(SOT) for MTASR. Our visualization reveals that CTC guides the encoder to\nrepresent different speakers in distinct temporal regions of acoustic\nembeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC\n(SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a\ntailored CTC variant for multi-talker scenarios, it explicitly models speaker\ndisentanglement by constraining the encoder to represent different speakers'\ntokens at specific time frames. When integrated with SOT, the SOT-SACTC model\nconsistently outperforms standard SOT-CTC across various degrees of speech\noverlap. Specifically, we observe relative word error rate reductions of 10%\noverall and 15% on low-overlap speech. This work represents an initial\nexploration of CTC-based enhancements for MTASR tasks, offering a new\nperspective on speaker disentanglement in multi-talker speech recognition. The\ncode is available at https://github.com/kjw11/Speaker-Aware-CTC.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted by ICASSP2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12388v2",
    "published_date": "2024-09-19 01:26:33 UTC",
    "updated_date": "2025-01-03 12:36:25 UTC"
  },
  {
    "arxiv_id": "2409.12386v2",
    "title": "Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition",
    "authors": [
      "Chien-Chun Wang",
      "Li-Wei Chen",
      "Cheng-Kang Chou",
      "Hung-Shin Lee",
      "Berlin Chen",
      "Hsin-Min Wang"
    ],
    "abstract": "While pre-trained automatic speech recognition (ASR) systems demonstrate\nimpressive performance on matched domains, their performance often degrades\nwhen confronted with channel mismatch stemming from unseen recording\nenvironments and conditions. To mitigate this issue, we propose a novel\nchannel-aware data simulation method for robust ASR training. Our method\nharnesses the synergistic power of channel-extractive techniques and generative\nadversarial networks (GANs). We first train a channel encoder capable of\nextracting embeddings from arbitrary audio. On top of this, channel embeddings\nare extracted using a minimal amount of target-domain data and used to guide a\nGAN-based speech synthesizer. This synthesizer generates speech that faithfully\npreserves the phonetic content of the input while mimicking the channel\ncharacteristics of the target domain. We evaluate our method on the challenging\nHakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving\nrelative character error rate (CER) reductions of 20.02% and 9.64%,\nrespectively, compared to the baselines. These results highlight the efficacy\nof our channel-aware data simulation method for bridging the gap between\nsource- and target-domain acoustics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.12386v2",
    "published_date": "2024-09-19 01:02:31 UTC",
    "updated_date": "2025-01-08 05:57:28 UTC"
  },
  {
    "arxiv_id": "2409.12385v1",
    "title": "Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation",
    "authors": [
      "Chenyu Li",
      "Shiming Ge",
      "Daichi Zhang",
      "Jia Li"
    ],
    "abstract": "Many real-world applications today like video surveillance and urban\ngovernance need to address the recognition of masked faces, where content\nreplacement by diverse masks often brings in incomplete appearance and\nambiguous representation, leading to a sharp drop in accuracy. Inspired by\nrecent progress on amodal perception, we propose to migrate the mechanism of\namodal completion for the task of masked face recognition with an end-to-end\nde-occlusion distillation framework, which consists of two modules. The\n\\textit{de-occlusion} module applies a generative adversarial network to\nperform face completion, which recovers the content under the mask and\neliminates appearance ambiguity. The \\textit{distillation} module takes a\npre-trained general face recognition model as the teacher and transfers its\nknowledge to train a student for completed faces using massive online\nsynthesized face pairs. Especially, the teacher knowledge is represented with\nstructural relations among instances in multiple orders, which serves as a\nposterior regularization to enable the adaptation. In this way, the knowledge\ncan be fully distilled and transferred to identify masked faces. Experiments on\nsynthetic and realistic datasets show the efficacy of the proposed approach.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ACM MM 2020",
    "pdf_url": "http://arxiv.org/pdf/2409.12385v1",
    "published_date": "2024-09-19 01:00:36 UTC",
    "updated_date": "2024-09-19 01:00:36 UTC"
  },
  {
    "arxiv_id": "2409.12384v1",
    "title": "Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation",
    "authors": [
      "Bochao Liu",
      "Jianghu Lu",
      "Pengju Wang",
      "Junjie Zhang",
      "Dan Zeng",
      "Zhenxing Qian",
      "Shiming Ge"
    ],
    "abstract": "Deep learning models can achieve high inference accuracy by extracting rich\nknowledge from massive well-annotated data, but may pose the risk of data\nprivacy leakage in practical deployment. In this paper, we present an effective\nteacher-student learning approach to train privacy-preserving deep learning\nmodels via differentially private data-free distillation. The main idea is\ngenerating synthetic data to learn a student that can mimic the ability of a\nteacher well-trained on private data. In the approach, a generator is first\npretrained in a data-free manner by incorporating the teacher as a fixed\ndiscriminator. With the generator, massive synthetic data can be generated for\nmodel training without exposing data privacy. Then, the synthetic data is fed\ninto the teacher to generate private labels. Towards this end, we propose a\nlabel differential privacy algorithm termed selective randomized response to\nprotect the label information. Finally, a student is trained on the synthetic\ndata with the supervision of private labels. In this way, both data privacy and\nlabel privacy are well protected in a unified framework, leading to\nprivacy-preserving models. Extensive experiments and analysis clearly\ndemonstrate the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published by IEEE MMSP 2022",
    "pdf_url": "http://arxiv.org/pdf/2409.12384v1",
    "published_date": "2024-09-19 01:00:18 UTC",
    "updated_date": "2024-09-19 01:00:18 UTC"
  },
  {
    "arxiv_id": "2409.12380v1",
    "title": "Bundle Fragments into a Whole: Mining More Complete Clusters via Submodular Selection of Interesting webpages for Web Topic Detection",
    "authors": [
      "Junbiao Pang",
      "Anjing Hu",
      "Qingming Huang"
    ],
    "abstract": "Organizing interesting webpages into hot topics is one of key steps to\nunderstand the trends of multimodal web data. A state-of-the-art solution is\nfirstly to organize webpages into a large volume of multi-granularity topic\ncandidates; hot topics are further identified by estimating their\ninterestingness. However, these topic candidates contain a large number of\nfragments of hot topics due to both the inefficient feature representations and\nthe unsupervised topic generation. This paper proposes a bundling-refining\napproach to mine more complete hot topics from fragments. Concretely, the\nbundling step organizes the fragment topics into coarse topics; next, the\nrefining step proposes a submodular-based method to refine coarse topics in a\nscalable approach. The propose unconventional method is simple, yet powerful by\nleveraging submodular optimization, our approach outperforms the traditional\nranking methods which involve the careful design and complex steps. Extensive\nexperiments demonstrate that the proposed approach surpasses the\nstate-of-the-art method (i.e., latent Poisson deconvolution Pang et al. (2016))\n20% accuracy and 10% one on two public data sets, respectively.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10",
    "pdf_url": "http://arxiv.org/pdf/2409.12380v1",
    "published_date": "2024-09-19 00:46:31 UTC",
    "updated_date": "2024-09-19 00:46:31 UTC"
  },
  {
    "arxiv_id": "2409.12371v1",
    "title": "Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization",
    "authors": [
      "Haemin Park",
      "Diego Klabjan"
    ],
    "abstract": "Federated Learning (FL) faces significant challenges related to communication\nefficiency and heterogeneity. To address these issues, we explore the potential\nof using low-rank updates. Our theoretical analysis reveals that client's loss\nexhibits a higher rank structure (gradients span higher rank subspace of\nHessian) compared to the server's loss. Based on this insight, we hypothesize\nthat constraining client-side optimization to a low-rank subspace could provide\nan implicit regularization effect. Consequently, we propose FedLoRU, a general\nlow-rank update framework for federated learning. Our framework enforces\nlow-rank client-side updates and accumulates these updates to form a\nhigher-rank model. Additionally, variants of FedLoRU can adapt to environments\nwith statistical and model heterogeneity by employing multiple or hierarchical\nlow-rank updates. Experimental results demonstrate that FedLoRU performs\ncomparably to full-rank algorithms and exhibits robustness to heterogeneous and\nlarge numbers of clients.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.12371v1",
    "published_date": "2024-09-19 00:11:58 UTC",
    "updated_date": "2024-09-19 00:11:58 UTC"
  }
]
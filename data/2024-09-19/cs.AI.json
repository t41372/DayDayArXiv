{
  "date": "2024-09-19",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-19 的 arXiv 中文 TLDR 快报！\n\n今天的 arXiv 论文主要聚焦于 AI 模型的优化与应用，特别是 Large Language Models (LLMs) 在数学推理、知识获取和医疗诊断中的突破性进展，令人印象深刻的是 OpenAI 的 o1-preview 模型在数学考试中的近完美表现，以及多模态 AI 在图像和语音处理上的创新；这些论文强调了 AI 在实际领域的潜力，同时涉及生成模型和强化学习的效率提升。\n\n### LLM 和 AI 推理领域的亮点\n- **Large Language Models as Essay Graders（大型语言模型作为作文评分器）**：这篇论文评估了 ChatGPT 和 Llama 在 Automated Essay Scoring (AES) 任务中的表现，发现这些模型倾向于给出更低的评分，且与人类评分相关性较低；主要贡献是实验证明 LLMs 目前不适合完全取代人类评分，但可作为辅助工具。\n- **What Would You Ask When You First Saw \\(a^2 + b^2 = c^2\\)? Evaluating LLM on Curiosity-Driven Questioning（当你第一次看到 \\(a^2 + b^2 = c^2\\) 时会问什么？评估 LLM 的好奇驱动问题生成）**：作者提出一个新框架评估 LLM 的知识获取潜力，通过生成科学问题来模拟好奇心；关键发现是较大模型如 GPT-4 生成的问题更连贯，但小型模型如 Phi-2 同样有效，强调模型规模并非唯一决定因素。\n- **System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam（OpenAI o1-preview 模型的 System 2 思考：在数学考试中的近完美表现）**：这篇论文测试了 OpenAI 的 o1-preview 模型在系统性推理任务中的表现，模型在数学考试中得分近乎完美（76/76），远超 GPT-4o 和人类平均水平；主要贡献是验证了 LLM 在复杂推理中的潜力，但也指出输出变异性问题。\n\n这些 LLM 相关论文展示了模型在知识获取和推理任务中的进步，但也暴露了泛化性和准确性挑战，值得关注 AI 伦理和实际应用。\n\n### 医疗 AI 和图像处理的进展\n- **Explainable AI for Autism Diagnosis: Identifying Critical Brain Regions Using fMRI Data（解释性 AI 用于自闭症诊断：使用 fMRI 数据识别关键脑区）**：论文开发了一个深度学习模型来分类自闭症并解释关键脑区差异；主要发现是模型在 ABIDE 数据集上准确识别自闭症特征，提升了早期诊断的客观性和可靠性。\n- **DenoMamba: A fused state-space model for low-dose CT denoising（DenoMamba：用于低剂量 CT 去噪的融合状态空间模型）**：作者提出 DenoMamba 模型，通过 state-space modeling 捕获短长距离上下文，实现低剂量 CT 图像去噪；关键贡献是提升图像质量，平均 PSNR 改善 1.4dB，适用于医疗成像。\n- **AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble（AutoPET III 挑战：使用 ResEnc 模型集成进行肿瘤病变分割）**：这篇论文在 AutoPET III 挑战中排名第一，使用 3D Residual encoder U-Net 框架实现肿瘤分割；主要发现是模型在多示踪剂环境中准确率高达 96.27%，为癌症诊断提供高效工具。\n\n这些医疗 AI 论文突出了解释性和高效模型在诊断中的价值，推动了 AI 在临床应用的可靠性。\n\n### 其他值得一提的创新\n- **FedAT: Federated Adversarial Training for Distributed Insider Threat Detection（FedAT：用于分布式内部威胁检测的联邦对抗训练）**：论文提出 FedAT 框架，使用联邦学习缓解数据不平衡问题；主要贡献是提升内部威胁检测的准确性，适用于多地点组织。\n- **AutoVerus: Automated Proof Generation for Rust Code（AutoVerus：Rust 代码的自动证明生成）**：作者开发了 AutoVerus，使用 LLM 生成 Rust 代码证明；关键发现是自动生成正确证明的成功率超过 90%，加速软件验证。\n- **MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines（MMSearch：评估大型模型作为多模态搜索引擎的潜力）**：这篇论文构建了 MMSearch 基准，测试 LMMs 在多模态搜索中的性能；主要发现是 GPT-4o 在端到端任务中优于商用引擎，推动 AI 搜索的创新。\n\n其他论文如语音增强、图像生成和强化学习等虽有技术贡献，但相对常规，因此仅简要提及：例如 **ERIC: Estimating Rainfall with Commodity Doorbell Camera（ERIC：使用普通门铃摄像头估算降雨量）** 通过机器学习优化灌溉系统，节省水资源；**JourneyBench（JourneyBench：针对生成图像的视觉语言理解基准）** 评估多模态模型在图像 hallucination 中的性能，提升了 AI 鲁棒性。\n\n总之，今天的论文展示了 AI 在推理、医疗和多模态领域的强劲进展，但也提醒我们关注泛化和隐私挑战。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2409.13120v1",
      "title": "Are Large Language Models Good Essay Graders?",
      "title_zh": "大语言模型是好的作文评分者吗？",
      "authors": [
        "Anindita Kundu",
        "Denilson Barbosa"
      ],
      "abstract": "We evaluate the effectiveness of Large Language Models (LLMs) in assessing\nessay quality, focusing on their alignment with human grading. More precisely,\nwe evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a\ncrucial natural language processing (NLP) application in Education. We consider\nboth zero-shot and few-shot learning and different prompting approaches. We\ncompare the numeric grade provided by the LLMs to human rater-provided scores\nutilizing the ASAP dataset, a well-known benchmark for the AES task. Our\nresearch reveals that both LLMs generally assign lower scores compared to those\nprovided by the human raters; moreover, those scores do not correlate well with\nthose provided by the humans. In particular, ChatGPT tends to be harsher and\nfurther misaligned with human evaluations than Llama. We also experiment with a\nnumber of essay features commonly used by previous AES methods, related to\nlength, usage of connectives and transition words, and readability metrics,\nincluding the number of spelling and grammar mistakes. We find that, generally,\nnone of these features correlates strongly with human or LLM scores. Finally,\nwe report results on Llama 3, which are generally better across the board, as\nexpected. Overall, while LLMs do not seem an adequate replacement for human\ngrading, our results are somewhat encouraging for their use as a tool to assist\nhumans in the grading of written essays in the future.",
      "tldr_zh": "这篇论文评估了大型语言模型 (LLMs) 如 ChatGPT 和 Llama 在自动作文评分 (AES) 任务中的表现，重点比较其评分与人类评分的对齐度，使用 ASAP 数据集进行实验。研究采用零样本和少样本学习以及不同提示方法，结果显示 LLMs 通常给出更低的评分，且与人类评分相关性较差，其中 ChatGPT 更倾向于严厉且不一致。分析作文特征（如长度、连接词使用、易读性和语法错误）后，发现这些特征与人类或 LLMs 评分的相关性均不强，而 Llama 3 的表现有所改善。总体结论是，LLMs 目前无法完全取代人类评分，但可能作为辅助工具用于未来作文评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13120v1",
      "published_date": "2024-09-19 23:20:49 UTC",
      "updated_date": "2024-09-19 23:20:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:21:44.520178"
    },
    {
      "arxiv_id": "2409.15374v2",
      "title": "Explainable AI for Autism Diagnosis: Identifying Critical Brain Regions Using fMRI Data",
      "title_zh": "可解释AI用于自闭症诊断：使用fMRI数据识别关键脑区",
      "authors": [
        "Suryansh Vidya",
        "Kush Gupta",
        "Amir Aly",
        "Andy Wills",
        "Emmanuel Ifeachor",
        "Rohit Shankar"
      ],
      "abstract": "Early diagnosis and intervention for Autism Spectrum Disorder (ASD) has been\nshown to significantly improve the quality of life of autistic individuals.\nHowever, diagnostics methods for ASD rely on assessments based on clinical\npresentation that are prone to bias and can be challenging to arrive at an\nearly diagnosis. There is a need for objective biomarkers of ASD which can help\nimprove diagnostic accuracy. Deep learning (DL) has achieved outstanding\nperformance in diagnosing diseases and conditions from medical imaging data.\nExtensive research has been conducted on creating models that classify ASD\nusing resting-state functional Magnetic Resonance Imaging (fMRI) data. However,\nexisting models lack interpretability. This research aims to improve the\naccuracy and interpretability of ASD diagnosis by creating a DL model that can\nnot only accurately classify ASD but also provide explainable insights into its\nworking. The dataset used is a preprocessed version of the Autism Brain Imaging\nData Exchange (ABIDE) with 884 samples. Our findings show a model that can\naccurately classify ASD and highlight critical brain regions differing between\nASD and typical controls, with potential implications for early diagnosis and\nunderstanding of the neural basis of ASD. These findings are validated by\nstudies in the literature that use different datasets and modalities,\nconfirming that the model actually learned characteristics of ASD and not just\nthe dataset. This study advances the field of explainable AI in medical imaging\nby providing a robust and interpretable model, thereby contributing to a future\nwith objective and reliable ASD diagnostics.",
      "tldr_zh": "本研究利用 Explainable AI 和深度学习(DL)模型，从 fMRI 数据分析 Autism Spectrum Disorder (ASD)，旨在解决传统诊断方法易出错和缺乏可解释性的问题。模型使用预处理后的 ABIDE 数据集（884 个样本）进行训练，不仅实现了准确的 ASD 分类，还突出了关键脑区差异，并通过与其他研究的验证确认了其泛化性。总体而言，此工作为客观可靠的 ASD 早期诊断和神经基础理解提供了重要见解，推进了医疗成像领域的可解释 AI 应用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "This work has been submitted to the IEEE for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2409.15374v2",
      "published_date": "2024-09-19 23:08:09 UTC",
      "updated_date": "2025-03-04 00:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:21:56.139707"
    },
    {
      "arxiv_id": "2409.13115v1",
      "title": "Personalized 2D Binary Patient Codes of Tissue Images and Immunogenomic Data Through Multimodal Self-Supervised Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Areej Alsaafin",
        "Abubakr Shafique",
        "Saghir Alfasly",
        "H. R. Tizhoosh"
      ],
      "abstract": "The field of medical diagnostics has witnessed a transformative convergence\nof artificial intelligence (AI) and healthcare data, offering promising avenues\nfor enhancing patient care and disease comprehension. However, this integration\nof multimodal data, specifically histopathology whole slide images (WSIs) and\ngenetic sequencing data, presents unique challenges due to modality disparities\nand the need for scalable computational solutions. This paper addresses the\nscarcity of multimodal solutions, primarily centered around unimodal data\nsolutions, thus limiting the realization of the rich insights that can be\nderived from integrating images and genomic data. Here, we introduce MarbliX\n``Multimodal Association and Retrieval with Binary Latent Indexed matriX,'' an\ninnovative multimodal framework that integrates histopathology images with\nimmunogenomic sequencing data, encapsulating them into a concise binary patient\ncode, referred to as ``monogram.'' This binary representation facilitates the\nestablishment of a comprehensive archive, enabling clinicians to match similar\ncases. The experimental results demonstrate the potential of MarbliX to empower\nhealthcare professionals with in-depth insights, leading to more precise\ndiagnoses, reduced variability, and expanded personalized treatment options,\nparticularly in the context of cancer.",
      "tldr_zh": "这篇论文解决了医疗诊断中整合组织图像（WSIs）和免疫基因组数据面临的挑战，通过多模态自监督融合（Multimodal Self-Supervised Fusion）生成个性化的2D二进制患者代码（monogram）。MarbliX框架创新地将这些多模态数据封装成简洁的二进制表示，形成一个全面的病例档案，便于临床医生快速匹配类似病例。实验结果显示，该方法能提升诊断精度、减少变异，并扩展癌症领域的个性化治疗选项。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13115v1",
      "published_date": "2024-09-19 22:49:27 UTC",
      "updated_date": "2024-09-19 22:49:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:22:07.878826"
    },
    {
      "arxiv_id": "2409.13112v2",
      "title": "Analyzing mixed construction and demolition waste in material recovery facilities: evolution, challenges, and applications of computer vision and deep learning",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Langley",
        "Matthew Lonergan",
        "Tao Huang",
        "Mostafa Rahimi Azghadi"
      ],
      "abstract": "Improving the automatic and timely recognition of construction and demolition\nwaste composition is crucial for enhancing business returns, economic outcomes\nand sustainability. While deep learning models show promise in recognizing and\nclassifying homogenous materials, the current literature lacks research\nassessing their performance for mixed, contaminated material in commercial\nmaterial recycling facility settings. Despite the increasing numbers of deep\nlearning models and datasets generated in this area, the sub-domain of deep\nlearning analysis of construction and demolition waste piles remains\nunderexplored. To address this gap, recent deep learning algorithms and\ntechniques were explored. This review examines the progression in datasets,\nsensors and the evolution from object detection towards real-time segmentation\nmodels. It also synthesizes research from the past five years on deep learning\nfor construction and demolition waste management, highlighting recent\nadvancements while acknowledging limitations that hinder widespread commercial\nadoption. The analysis underscores the critical requirement for diverse and\nhigh-fidelity datasets, advanced sensor technologies, and robust algorithmic\nframeworks to facilitate the effective integration of deep learning\nmethodologies into construction and demolition waste management systems. This\nintegration is envisioned to contribute significantly towards the advancement\nof a more sustainable and circular economic model.",
      "tldr_zh": "本文分析了在材料回收设施中处理混合建筑和拆除废物的计算机视觉和deep learning应用，强调了自动识别废物组成对提升商业回报、经济效益和可持续性的关键作用。论文回顾了过去五年的研究进展，包括数据集、传感器的发展，以及从object detection向real-time segmentation模型的演变，同时指出了现有模型在处理污染废物时的性能局限。最终，研究呼吁构建多样、高保真数据集和稳健算法框架，以推动deep learning在废物管理中的整合，助力可持续的循环经济模式。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13112v2",
      "published_date": "2024-09-19 22:38:26 UTC",
      "updated_date": "2025-03-03 20:48:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:22:19.974427"
    },
    {
      "arxiv_id": "2409.17172v1",
      "title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning",
      "title_zh": "翻译失败",
      "authors": [
        "Shashidhar Reddy Javaji",
        "Zining Zhu"
      ],
      "abstract": "Large language models (LLMs) can store a massive amount of knowledge, yet\ntheir potential to acquire new knowledge remains unknown. We propose a novel\nevaluation framework that evaluates this capability. This framework prompts\nLLMs to generate questions about a statement introducing scientific knowledge,\nsimulating a curious person when facing the statement for the first time. We\nscore the qualities of the generated questions, thereby evaluating the\nknowledge acquisition potential of the LLM. We apply controlled ablation\nstudies to validate our scoring procedures. Additionally, we created a\nsynthetic dataset consisting of 1101 statements in physics, chemistry, and\nmaths with distinct levels of difficulties, 300 general knowledge statements,\nand 567 incorrect statements. Human evaluations were conducted to validate our\nmodel assessments, achieving an approximate weighted Cohen's kappa of 0.7 on\nall three metrics considered. We find that while large models like GPT-4 and\nMistral 8x7b are adept at generating coherent and relevant questions, the\nsmaller Phi-2 model is equally or more effective. This indicates that size does\nnot solely determine a model's knowledge acquisition potential. The proposed\nframework quantifies a critical model capability that was commonly overlooked\nand opens up research opportunities for developing more knowledgeable AI\nsystems",
      "tldr_zh": "这篇论文提出了一种新框架，用于评估大型语言模型(LLMs)的知识获取潜力，通过提示模型生成对科学语句（如\\(a^2 + b^2 = c^2\\))的好奇驱动问题，模拟首次接触时的好奇心。研究方法包括创建包含1101个物理、化学和数学语句的合成数据集，进行控制性消融研究和人类评估（Cohen's kappa约0.7），以验证问题质量评分。结果发现，大型模型如GPT-4和Mistral 8x7b能生成连贯的相关问题，但较小模型Phi-2在某些方面表现更佳，表明模型大小并非唯一决定因素。该框架强调了LLMs的一个被忽略的关键能力，为开发更知识渊博的AI系统开辟了研究机会。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.17172v1",
      "published_date": "2024-09-19 22:12:16 UTC",
      "updated_date": "2024-09-19 22:12:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:22:32.704724"
    },
    {
      "arxiv_id": "2409.13104v2",
      "title": "ERIC: Estimating Rainfall with Commodity Doorbell Camera for Precision Residential Irrigation",
      "title_zh": "ERIC：利用商用门铃摄像头估计降雨量",
      "authors": [
        "Tian Liu",
        "Liuyi Jin",
        "Radu Stoleru",
        "Amran Haroon",
        "Charles Swanson",
        "Kexin Feng"
      ],
      "abstract": "Current state-of-the-art residential irrigation systems, such as WaterMyYard,\nrely on rainfall data from nearby weather stations to adjust irrigation\namounts. However, the accuracy of rainfall data is compromised by the limited\nspatial resolution of rain gauges and the significant variability of hyperlocal\nrainfall, leading to substantial water waste. To improve irrigation efficiency,\nwe developed a cost-effective irrigation system, dubbed ERIC, which employs\nmachine learning models to estimate rainfall from commodity doorbell camera\nfootage and optimizes irrigation schedules without human intervention.\nSpecifically, we: a) designed novel visual and audio features with lightweight\nneural network models to infer rainfall from the camera at the edge, preserving\nuser privacy; b) built a complete end-to-end irrigation system on Raspberry Pi\n4, costing only \\$75. We deployed the system across five locations (collecting\nover 750 hours of video) with varying backgrounds and light conditions.\nComprehensive evaluation validates that ERIC achieves state-of-the-art rainfall\nestimation performance ($\\sim$ 5mm/day), saving 9,112 gallons/month of water,\ntranslating to \\$28.56/month in utility savings. Data and code are available at\nhttps://github.com/LENSS/ERIC-BuildSys2024.git",
      "tldr_zh": "该研究针对现有住宅灌溉系统依赖气象站数据导致的降雨估计不准和水资源浪费问题，开发了低成本系统 ERIC，利用机器学习模型从普通门铃摄像头视频中提取视觉和音频特征，实现边缘设备的降雨推断，并自动优化灌溉时间表。ERIC 在 Raspberry Pi 4 上构建了完整的端到端系统，仅需 75 美元成本，并在五个地点部署，收集超过 750 小时视频进行验证。实验结果显示，ERIC 实现了先进的降雨估计精度（约 5mm/天），每月节省 9,112 加仑水和 28.56 美元公用事业费用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "BuildSys 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.13104v2",
      "published_date": "2024-09-19 22:11:08 UTC",
      "updated_date": "2024-10-03 22:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:22:44.271920"
    },
    {
      "arxiv_id": "2409.17171v2",
      "title": "Cross-Domain Content Generation with Domain-Specific Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ankit Maloo",
        "Abhinav Garg"
      ],
      "abstract": "Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.",
      "tldr_zh": "本文研究了使用小型语言模型（small language models）生成跨领域内容的挑战，焦点在于处理最小重叠的多个数据集，如故事（Dataset A）和食谱（Dataset B）。实验显示，单独训练模型并采用自定义tokenizer能显著提升生成质量，但Low-Rank Adaptation (LoRA)或标准fine-tuning往往导致catastrophic forgetting，模型丢失原有知识。作者提出knowledge expansion策略，通过仅训练额外参数来扩展模型能力，使其能根据请求生成多领域内容，同时避免遗忘问题。该方法为高效多领域语言模型的开发提供了关键见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.17171v2",
      "published_date": "2024-09-19 21:45:13 UTC",
      "updated_date": "2024-10-02 10:28:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:22:55.633960"
    },
    {
      "arxiv_id": "2409.13094v2",
      "title": "DenoMamba: A fused state-space model for low-dose CT denoising",
      "title_zh": "DenoMamba：一种融合的状态空间模型，用于低剂量 CT 去噪",
      "authors": [
        "Şaban Öztürk",
        "Oğuz Can Duran",
        "Tolga Çukur"
      ],
      "abstract": "Low-dose computed tomography (LDCT) lower potential risks linked to radiation\nexposure while relying on advanced denoising algorithms to maintain diagnostic\nquality in reconstructed images. The reigning paradigm in LDCT denoising is\nbased on neural network models that learn data-driven image priors to separate\nnoise evoked by dose reduction from underlying tissue signals. Naturally, the\nfidelity of these priors depend on the model's ability to capture the broad\nrange of contextual features evident in CT images. Earlier convolutional neural\nnetworks (CNN) are highly adept at efficiently capturing short-range spatial\ncontext, but their limited receptive fields reduce sensitivity to interactions\nover longer distances. Although transformers based on self-attention mechanisms\nhave recently been posed to increase sensitivity to long-range context, they\ncan suffer from suboptimal performance and efficiency due to elevated model\ncomplexity, particularly for high-resolution CT images. For high-quality\nrestoration of LDCT images, here we introduce DenoMamba, a novel denoising\nmethod based on state-space modeling (SSM), that efficiently captures short-\nand long-range context in medical images. Following an hourglass architecture\nwith encoder-decoder stages, DenoMamba employs a spatial SSM module to encode\nspatial context and a novel channel SSM module equipped with a secondary gated\nconvolution network to encode latent features of channel context at each stage.\nFeature maps from the two modules are then consolidated with low-level input\nfeatures via a convolution fusion module (CFM). Comprehensive experiments on\nLDCT datasets with 25\\% and 10\\% dose reduction demonstrate that DenoMamba\noutperforms state-of-the-art denoisers with average improvements of 1.4dB PSNR,\n1.1% SSIM, and 1.6% RMSE in recovered image quality.",
      "tldr_zh": "本研究提出DenoMamba，一种基于状态空间建模(SSM)的融合模型，用于低剂量CT(LDCT)图像去噪，以平衡辐射风险和诊断质量。DenoMamba采用hourglass架构，包括空间SSM模块捕捉短距离和长距离空间上下文，以及新型通道SSM模块结合次级门控卷积网络处理通道特征，并通过卷积融合模块(CFM)整合特征映射。与传统CNN和Transformer方法相比，该模型克服了效率和复杂性问题。实验在25%和10%剂量减少的LDCT数据集上显示，DenoMamba比最先进去噪器平均提升1.4dB PSNR、1.1% SSIM和1.6% RMSE，显著提高了图像恢复质量。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13094v2",
      "published_date": "2024-09-19 21:32:07 UTC",
      "updated_date": "2024-12-15 16:11:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:23:08.749152"
    },
    {
      "arxiv_id": "2409.13093v1",
      "title": "Guided Profile Generation Improves Personalization with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarui Zhang"
      ],
      "abstract": "In modern commercial systems, including Recommendation, Ranking, and\nE-Commerce platforms, there is a trend towards improving customer experiences\nby incorporating Personalization context as input into Large Language Models\n(LLMs). However, LLMs often struggle to effectively parse and utilize sparse\nand complex personal context without additional processing or contextual\nenrichment, underscoring the need for more sophisticated context understanding\nmechanisms. In this work, we propose Guided Profile Generation (GPG), a general\nmethod designed to generate personal profiles in natural language. As is\nobserved, intermediate guided profile generation enables LLMs to summarize, and\nextract the important, distinctive features from the personal context into\nconcise, descriptive sentences, precisely tailoring their generation more\nclosely to an individual's unique habits and preferences. Our experimental\nresults show that GPG improves LLM's personalization ability across different\ntasks, for example, it increases 37% accuracy in predicting personal preference\ncompared to directly feeding the LLMs with raw personal context.",
      "tldr_zh": "该论文探讨了在推荐、排名和电商等商业系统中，使用大型语言模型（LLMs）融入个性化上下文以提升用户体验的挑战，因为LLMs 难以有效解析稀疏复杂的个人数据。研究提出Guided Profile Generation (GPG)，一种通用方法，通过生成自然语言的个人配置文件来总结和提取关键特征，从而使LLMs 更精确地适应用户的独特习惯和偏好。实验结果表明，GPG 显著提高了LLMs 的个性化性能，例如，在预测个人偏好任务中，准确率比直接输入原始上下文提升了 37%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "EMNLP 2024 Findings",
      "pdf_url": "http://arxiv.org/pdf/2409.13093v1",
      "published_date": "2024-09-19 21:29:56 UTC",
      "updated_date": "2024-09-19 21:29:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:23:19.901794"
    },
    {
      "arxiv_id": "2409.13091v1",
      "title": "Interpretable Action Recognition on Hard to Classify Actions",
      "title_zh": "翻译失败",
      "authors": [
        "Anastasia Anichenko",
        "Frank Guerin",
        "Andrew Gilbert"
      ],
      "abstract": "We investigate a human-like interpretable model of video understanding.\nHumans recognise complex activities in video by recognising critical\nspatio-temporal relations among explicitly recognised objects and parts, for\nexample, an object entering the aperture of a container. To mimic this we build\non a model which uses positions of objects and hands, and their motions, to\nrecognise the activity taking place. To improve this model we focussed on three\nof the most confused classes (for this model) and identified that the lack of\n3D information was the major problem. To address this we extended our basic\nmodel by adding 3D awareness in two ways: (1) A state-of-the-art object\ndetection model was fine-tuned to determine the difference between \"Container\"\nand \"NotContainer\" in order to integrate object shape information into the\nexisting object features. (2) A state-of-the-art depth estimation model was\nused to extract depth values for individual objects and calculate depth\nrelations to expand the existing relations used our interpretable model. These\n3D extensions to our basic model were evaluated on a subset of three\nsuperficially similar \"Putting\" actions from the Something-Something-v2\ndataset. The results showed that the container detector did not improve\nperformance, but the addition of depth relations made a significant improvement\nto performance.",
      "tldr_zh": "本研究探讨了可解释的视频动作识别模型，旨在模仿人类通过识别对象和手部之间的关键时空关系来理解复杂活动，特别是针对难以分类的动作。研究者扩展了基本模型，添加了3D感知：（1）微调先进的物体检测模型以区分“Container”和“NotContainer”，整合物体形状信息；（2）使用深度估计模型提取物体深度值并计算深度关系。实验在Something-Something-v2数据集的三个类似“Putting”动作子集上进行，结果显示容器检测器未改善性能，但添加深度关系显著提升了模型的表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, This manuscript has been accepted at the Human-inspired\n  Computer Vision (HCV) ECCV 2024 Workshop. arXiv admin note: text overlap with\n  arXiv:2107.05319",
      "pdf_url": "http://arxiv.org/pdf/2409.13091v1",
      "published_date": "2024-09-19 21:23:44 UTC",
      "updated_date": "2024-09-19 21:23:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:23:31.286547"
    },
    {
      "arxiv_id": "2409.13083v1",
      "title": "FedAT: Federated Adversarial Training for Distributed Insider Threat Detection",
      "title_zh": "翻译失败",
      "authors": [
        "R G Gayathri",
        "Atul Sajjanhar",
        "Md Palash Uddin",
        "Yong Xiang"
      ],
      "abstract": "Insider threats usually occur from within the workplace, where the attacker\nis an entity closely associated with the organization. The sequence of actions\nthe entities take on the resources to which they have access rights allows us\nto identify the insiders. Insider Threat Detection (ITD) using Machine Learning\n(ML)-based approaches gained attention in the last few years. However, most\ntechniques employed centralized ML methods to perform such an ITD.\nOrganizations operating from multiple locations cannot contribute to the\ncentralized models as the data is generated from various locations. In\nparticular, the user behavior data, which is the primary source of ITD, cannot\nbe shared among the locations due to privacy concerns. Additionally, the data\ndistributed across various locations result in extreme class imbalance due to\nthe rarity of attacks. Federated Learning (FL), a distributed data modeling\nparadigm, gained much interest recently. However, FL-enabled ITD is not yet\nexplored, and it still needs research to study the significant issues of its\nimplementation in practical settings. As such, our work investigates an\nFL-enabled multiclass ITD paradigm that considers non-Independent and\nIdentically Distributed (non-IID) data distribution to detect insider threats\nfrom different locations (clients) of an organization. Specifically, we propose\na Federated Adversarial Training (FedAT) approach using a generative model to\nalleviate the extreme data skewness arising from the non-IID data distribution\namong the clients. Besides, we propose to utilize a Self-normalized Neural\nNetwork-based Multi-Layer Perceptron (SNN-MLP) model to improve ITD. We perform\ncomprehensive experiments and compare the results with the benchmarks to\nmanifest the enhanced performance of the proposed FedATdriven ITD scheme.",
      "tldr_zh": "本文提出 FedAT（Federated Adversarial Training），一种基于 Federated Learning (FL) 的分布式框架，用于检测内部威胁 (Insider Threat Detection, ITD)，以解决多地点数据隐私问题和 non-IID 数据分布导致的类不平衡。FedAT 采用生成模型缓解数据偏差，并结合 Self-normalized Neural Network-based Multi-Layer Perceptron (SNN-MLP) 模型来提升多类 ITD 的性能。实验结果表明，该方法与基准模型相比显著提高了检测准确率，为实际场景下的分布式威胁检测提供了有效方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CR",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13083v1",
      "published_date": "2024-09-19 20:44:33 UTC",
      "updated_date": "2024-09-19 20:44:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:23:44.103220"
    },
    {
      "arxiv_id": "2409.13082v2",
      "title": "AutoVerus: Automated Proof Generation for Rust Code",
      "title_zh": "AutoVerus：Rust 代码的自动证明生成",
      "authors": [
        "Chenyuan Yang",
        "Xuheng Li",
        "Md Rakib Hossain Misu",
        "Jianan Yao",
        "Weidong Cui",
        "Yeyun Gong",
        "Chris Hawblitzel",
        "Shuvendu Lahiri",
        "Jacob R. Lorch",
        "Shuai Lu",
        "Fan Yang",
        "Ziqiao Zhou",
        "Shan Lu"
      ],
      "abstract": "Generative AI has shown its values for many software engineering tasks. Still\nin its infancy, large language model (LLM)-based proof generation lags behind\nLLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses\nLLM to automatically generate correctness proof for Rust code. AutoVerus is\ndesigned to match the unique features of Verus, a verification tool that can\nprove the correctness of Rust code using proofs and specifications also written\nin Rust. AutoVerus consists of a network of LLM agents that are crafted and\norchestrated to mimic human experts' three phases of proof construction:\npreliminary proof generation, proof refinement guided by generic tips, and\nproof debugging guided by verification errors. To thoroughly evaluate AutoVerus\nand help foster future research in this direction, we have built a benchmark\nsuite of 150 non-trivial proof tasks, based on existing code-generation\nbenchmarks and verification benchmarks. Our evaluation shows that AutoVerus can\nautomatically generate correct proof for more than 90% of them, with more than\nhalf of them tackled in less than 30 seconds or 3 LLM calls.",
      "tldr_zh": "本研究提出AutoVerus，一种基于LLM（Large Language Model）的系统，用于自动生成Rust代码的正确性证明。AutoVerus由一系列LLM代理组成，模仿人类专家的三阶段过程：初步证明生成、基于通用提示的证明精炼，以及基于验证错误的证明调试，以匹配Verus工具的独特特性。研究构建了一个包含150个非平凡证明任务的基准测试套件，结果显示AutoVerus能自动生成超过90%的正确证明，其中一半在30秒或3个LLM调用内完成，从而提升了软件验证的效率和可扩展性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.FL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13082v2",
      "published_date": "2024-09-19 20:40:52 UTC",
      "updated_date": "2025-02-08 01:06:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:23:55.179762"
    },
    {
      "arxiv_id": "2409.13779v1",
      "title": "AutoPET III Challenge: Tumor Lesion Segmentation using ResEnc-Model Ensemble",
      "title_zh": "AutoPET III Challenge：使用 ResEnc-模型集成的肿瘤病变分割",
      "authors": [
        "Tanya Chutani",
        "Saikiran Bonthu",
        "Pranab Samanta",
        "Nitin Singhal"
      ],
      "abstract": "Positron Emission Tomography (PET) /Computed Tomography (CT) is crucial for\ndiagnosing, managing, and planning treatment for various cancers. Developing\nreliable deep learning models for the segmentation of tumor lesions in PET/CT\nscans in a multi-tracer multicenter environment, is a critical area of\nresearch. Different tracers, such as Fluorodeoxyglucose (FDG) and\nProstate-Specific Membrane Antigen (PSMA), have distinct physiological uptake\npatterns and data from different centers often vary in terms of acquisition\nprotocols, scanner types, and patient populations. Because of this variability,\nit becomes more difficult to design reliable segmentation algorithms and\ngeneralization techniques due to variations in image quality and lesion\ndetectability. To address this challenge, We trained a 3D Residual encoder\nU-Net within the no new U-Net framework, aiming to generalize the performance\nof automatic lesion segmentation of whole body PET/CT scans, across different\ntracers and clinical sites. Further, We explored several preprocessing\ntechniques and ultimately settled on using the Total Segmentator to crop our\ntraining data. Additionally, we applied resampling during this process. During\ninference, we leveraged test-time augmentations and other post-processing\ntechniques to enhance tumor lesion segmentation. Our team currently hold the\ntop position in the Auto-PET III challenge and outperformed the challenge\nbaseline model in the preliminary test set with Dice score of 0.9627.",
      "tldr_zh": "这篇论文针对PET/CT扫描中肿瘤病变分割的挑战，特别是在多示踪剂（如FDG和PSMA）和多中心数据变异环境下，开发了一个通用的3D Residual encoder U-Net模型。\n研究团队采用了Total Segmentator进行数据预处理、重采样，并在推理阶段应用测试时增强和后处理技术，以提升模型的泛化性和准确性。\n最终，该模型在AutoPET III挑战中排名第一，Dice score达到0.9627，显著优于基准模型。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13779v1",
      "published_date": "2024-09-19 20:18:39 UTC",
      "updated_date": "2024-09-19 20:18:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:24:08.128128"
    },
    {
      "arxiv_id": "2410.00031v2",
      "title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions",
      "title_zh": "LLM 代理的战略合谋：多商品竞争中的市场划分",
      "authors": [
        "Ryan Y. Lin",
        "Siddhartha Ojha",
        "Kevin Cai",
        "Maxwell F. Chen"
      ],
      "abstract": "Machine-learning technologies are seeing increased deployment in real-world\nmarket scenarios. In this work, we explore the strategic behaviors of large\nlanguage models (LLMs) when deployed as autonomous agents in multi-commodity\nmarkets, specifically within Cournot competition frameworks. We examine whether\nLLMs can independently engage in anti-competitive practices such as collusion\nor, more specifically, market division. Our findings demonstrate that LLMs can\neffectively monopolize specific commodities by dynamically adjusting their\npricing and resource allocation strategies, thereby maximizing profitability\nwithout direct human input or explicit collusion commands. These results pose\nunique challenges and opportunities for businesses looking to integrate AI into\nstrategic roles and for regulatory bodies tasked with maintaining fair and\ncompetitive markets. The study provides a foundation for further exploration\ninto the ramifications of deferring high-stakes decisions to LLM-based agents.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 作为自主代理在多商品市场中的战略行为，特别是在 Cournot 竞争框架下是否会进行勾结或市场划分。研究发现，LLMs 可以独立动态调整定价和资源分配策略，从而垄断特定商品并最大化利润，而无需人类干预或明确指令。这些结果为企业整合 AI 技术以及监管机构维护公平竞争市场带来了挑战和机会，并为进一步探索将高风险决策委托给 LLM 代理的潜在后果奠定了基础。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.CL",
        "q-fin.CP"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.00031v2",
      "published_date": "2024-09-19 20:10:40 UTC",
      "updated_date": "2025-05-16 10:05:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:24:19.617765"
    },
    {
      "arxiv_id": "2409.13064v1",
      "title": "Fear and Loathing on the Frontline: Decoding the Language of Othering by Russia-Ukraine War Bloggers",
      "title_zh": "翻译失败",
      "authors": [
        "Patrick Gerard",
        "William Theisen",
        "Tim Weninger",
        "Kristina Lerman"
      ],
      "abstract": "Othering, the act of portraying outgroups as fundamentally different from the\ningroup, often escalates into framing them as existential threats--fueling\nintergroup conflict and justifying exclusion and violence. These dynamics are\nalarmingly pervasive, spanning from the extreme historical examples of\ngenocides against minorities in Germany and Rwanda to the ongoing violence and\nrhetoric targeting migrants in the US and Europe. While concepts like hate\nspeech and fear speech have been explored in existing literature, they capture\nonly part of this broader and more nuanced dynamic which can often be harder to\ndetect, particularly in online speech and propaganda. To address this\nchallenge, we introduce a novel computational framework that leverages large\nlanguage models (LLMs) to quantify othering across diverse contexts, extending\nbeyond traditional linguistic indicators of hostility. Applying the model to\nreal-world data from Telegram war bloggers and political discussions on Gab\nreveals how othering escalates during conflicts, interacts with moral language,\nand garners significant attention, particularly during periods of crisis. Our\nframework, designed to offer deeper insights into othering dynamics, combines\nwith a rapid adaptation process to provide essential tools for mitigating\nothering's adverse impacts on social cohesion.",
      "tldr_zh": "本研究探讨了“Othering”（将外群体视为与内群体根本不同的行为），这种动态常升级为存在威胁的叙事，引发冲突和暴力，并在历史上如德国和卢旺达的种族灭绝中表现突出。论文引入了一个新计算框架，利用大型语言模型（LLMs）来量化 Othering 的复杂性，超越传统敌意指标，并应用于 Telegram 战争博主和 Gab 政治讨论的数据。结果显示，Othering 在冲突期间升级，与道德语言互动，并在危机时期获得更多关注。该框架结合快速适应过程，提供工具来缓解 Othering 对社会凝聚力的负面影响，从而为在线言论监控和冲突缓解提供新见解。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.13064v1",
      "published_date": "2024-09-19 19:56:03 UTC",
      "updated_date": "2024-09-19 19:56:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:24:32.436022"
    },
    {
      "arxiv_id": "2410.07114v5",
      "title": "System 2 thinking in OpenAI's o1-preview model: Near-perfect performance on a mathematics exam",
      "title_zh": "OpenAI 的 o1-preview 模型中的 System 2 思考：数学考试中的近",
      "authors": [
        "Joost de Winter",
        "Dimitra Dodou",
        "Yke Bauke Eisma"
      ],
      "abstract": "The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch students' average of 40.63 points. Neither model had\naccess to the exam figures. Since there was a risk of model contami-nation\n(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam was\npublished online), we repeated the procedure with a new Mathematics B exam that\nwas published after the cutoff date. The results again indicated that\no1-preview performed strongly (97.8th percentile), which suggests that\ncontamination was not a factor. We also show that there is some variability in\nthe output of o1-preview, which means that sometimes there is 'luck' (the\nanswer is correct) or 'bad luck' (the output has diverged into something that\nis incorrect). We demonstrate that the self-consistency approach, where\nrepeated prompts are given and the most common answer is selected, is a useful\nstrategy for identifying the correct answer. It is concluded that while\nOpenAI's new model series holds great potential, certain risks must be\nconsidered.",
      "tldr_zh": "这篇论文评估了 OpenAI o1-preview 模型的 System 2 推理能力，通过两次测试荷兰“Mathematics B”期末考试（满分 76 分），该模型分别获得 76 和 74 分的近乎完美表现，远超 GPT-4o（66 和 62 分）和学生平均分（40.63 分）。为排除模型污染风险，研究者使用发布在知识截止日期后的新考试重复测试，结果显示 o1-preview 仍处于 97.8th percentile，证明其强大性能并非依赖于污染。论文还发现 o1-preview 输出存在变异性，并提出 self-consistency 方法（重复提示并选最常见答案）可提升准确性；总体结论是，该模型潜力巨大，但需注意相关风险。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.07114v5",
      "published_date": "2024-09-19 19:48:31 UTC",
      "updated_date": "2024-10-25 07:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:24:45.036767"
    },
    {
      "arxiv_id": "2409.13059v1",
      "title": "Comprehensive Overview of Artificial Intelligence Applications in Modern Industries",
      "title_zh": "人工智能在现代产业中的应用全面概述",
      "authors": [
        "Yijie Weng",
        "Jianhao Wu",
        "Tara Kelly",
        "William Johnson"
      ],
      "abstract": "Artificial Intelligence (AI) is fundamentally reshaping various industries by\nenhancing decision-making processes, optimizing operations, and unlocking new\nopportunities for innovation. This paper explores the applications of AI across\nfour key sectors: healthcare, finance, manufacturing, and retail. Each section\ndelves into the specific challenges faced by these industries, the AI\ntechnologies employed to address them, and the measurable impact on business\noutcomes and societal welfare. We also discuss the implications of AI\nintegration, including ethical considerations, the future trajectory of AI\ndevelopment, and its potential to drive economic growth while posing challenges\nthat need to be managed responsibly.",
      "tldr_zh": "这篇论文对人工智能（AI）在现代产业中的应用进行了全面概述，重点探讨了其在医疗、金融、制造业和零售等四个关键领域的实施。论文分析了这些行业面临的特定挑战、采用的 AI 技术（如机器学习和决策优化算法），以及对业务成果和社会福利的量化影响，例如提升决策效率和创新机会。最终，它讨论了 AI 整合的伦理考虑、未来发展方向，以及其在驱动经济增长的同时，需要通过负责任的管理来应对潜在挑战。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13059v1",
      "published_date": "2024-09-19 19:22:52 UTC",
      "updated_date": "2024-09-19 19:22:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:05.059809"
    },
    {
      "arxiv_id": "2409.13054v1",
      "title": "LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Akshaj Kumar Veldanda",
        "Shi-Xiong Zhang",
        "Anirban Das",
        "Supriyo Chakraborty",
        "Stephen Rawls",
        "Sambit Sahu",
        "Milind Naphade"
      ],
      "abstract": "Large language models (LLMs) have revolutionized various domains, yet their\nutility comes with significant challenges related to outdated or problematic\nknowledge embedded during pretraining. This paper addresses the challenge of\nmodifying LLMs to unlearn problematic and outdated information while\nefficiently integrating new knowledge without retraining from scratch. Here, we\npropose LLM Surgery, a framework to efficiently modify LLM behaviour by\noptimizing a three component objective function that: (1) Performs reverse\ngradient on unlearning dataset (problematic and outdated information), (2)\nPerforms gradient descent on the update dataset (new and updated information),\nand (3) Minimizes the KL divergence on the retain dataset (small subset of\nunchanged text), ensuring alignment between pretrained and modified model\noutputs. Due to the lack of publicly available datasets specifically tailored\nfor our novel task, we compiled a new dataset and an evaluation benchmark.\nUsing Llama2-7B, we demonstrate that LLM Surgery can achieve significant\nforgetting on the unlearn set, a 20\\% increase in accuracy on the update set,\nand maintain performance on the retain set.",
      "tldr_zh": "本研究针对大型语言模型（Large Language Models, LLMs）中嵌入的过时或问题知识，提出LLM Surgery框架，以高效地实现知识遗忘和编辑，而无需从零重新训练。该框架通过优化一个三组件目标函数：（1）在unlearning数据集上使用反向梯度（reverse gradient）来消除问题信息，（2）在update数据集上进行梯度下降（gradient descent）以整合新知识，以及（3）在retain数据集上最小化KL散度（KL divergence），从而保持模型原有输出的一致性。为支持这一新任务，研究者编译了一个专属数据集和评估基准。实验结果显示，在Llama2-7B模型上，LLM Surgery实现了unlearning数据集的显著遗忘效果、update数据集准确率提升20%，并维持了retain数据集的性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13054v1",
      "published_date": "2024-09-19 19:07:01 UTC",
      "updated_date": "2024-09-19 19:07:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:07.556218"
    },
    {
      "arxiv_id": "2409.15373v1",
      "title": "Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention",
      "title_zh": "通过 Jagged Flash Attention 提升大型推荐系统的性能和可扩展性",
      "authors": [
        "Rengan Xu",
        "Junjie Yang",
        "Yifan Xu",
        "Hong Li",
        "Xing Liu",
        "Devashish Shankar",
        "Haoci Zhang",
        "Meng Liu",
        "Boyang Li",
        "Yuxi Hu",
        "Mingwei Tang",
        "Zehua Zhang",
        "Tunhou Zhang",
        "Dai Li",
        "Sijia Chen",
        "Gian-Paolo Musumeci",
        "Jiaqi Zhai",
        "Bill Zhu",
        "Hong Yan",
        "Srihari Reddy"
      ],
      "abstract": "The integration of hardware accelerators has significantly advanced the\ncapabilities of modern recommendation systems, enabling the exploration of\ncomplex ranking paradigms previously deemed impractical. However, the GPU-based\ncomputational costs present substantial challenges. In this paper, we\ndemonstrate our development of an efficiency-driven approach to explore these\nparadigms, moving beyond traditional reliance on native PyTorch modules. We\naddress the specific challenges posed by ranking models' dependence on\ncategorical features, which vary in length and complicate GPU utilization. We\nintroduce Jagged Feature Interaction Kernels, a novel method designed to\nextract fine-grained insights from long categorical features through efficient\nhandling of dynamically sized tensors. We further enhance the performance of\nattention mechanisms by integrating Jagged tensors with Flash Attention. Our\nnovel Jagged Flash Attention achieves up to 9x speedup and 22x memory reduction\ncompared to dense attention. Notably, it also outperforms dense flash\nattention, with up to 3x speedup and 53% more memory efficiency. In production\nmodels, we observe 10% QPS improvement and 18% memory savings, enabling us to\nscale our recommendation systems with longer features and more complex\narchitectures.",
      "tldr_zh": "本论文针对大规模推荐系统的GPU计算成本和分类特征长度不一的问题，提出了一种效率驱动的方法，包括Jagged Feature Interaction Kernels来处理动态大小张量，以及将Jagged tensors与Flash Attention整合的Jagged Flash Attention机制。\nJagged Flash Attention相较于密集注意机制，实现高达9倍速度提升和22倍内存减少，并在与密集Flash Attention的比较中获得3倍速度提升和53%内存效率改进。\n在生产环境中，该方法提升了10% QPS并节省18%内存，从而使推荐系统能够处理更长的特征和更复杂的架构，提高整体可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "3 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.15373v1",
      "published_date": "2024-09-19 18:20:54 UTC",
      "updated_date": "2024-09-19 18:20:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:20.179606"
    },
    {
      "arxiv_id": "2409.13038v1",
      "title": "HeadCT-ONE: Enabling Granular and Controllable Automated Evaluation of Head CT Radiology Report Generation",
      "title_zh": "HeadCT-ONE：实现头部 CT 放射学报告生成的高粒度可控自动化评估",
      "authors": [
        "Julián N. Acosta",
        "Xiaoman Zhang",
        "Siddhant Dogra",
        "Hong-Yu Zhou",
        "Seyedmehdi Payabvash",
        "Guido J. Falcone",
        "Eric K. Oermann",
        "Pranav Rajpurkar"
      ],
      "abstract": "We present Head CT Ontology Normalized Evaluation (HeadCT-ONE), a metric for\nevaluating head CT report generation through ontology-normalized entity and\nrelation extraction. HeadCT-ONE enhances current information extraction derived\nmetrics (such as RadGraph F1) by implementing entity normalization through\ndomain-specific ontologies, addressing radiological language variability.\nHeadCT-ONE compares normalized entities and relations, allowing for\ncontrollable weighting of different entity types or specific entities. Through\nexperiments on head CT reports from three health systems, we show that\nHeadCT-ONE's normalization and weighting approach improves the capture of\nsemantically equivalent reports, better distinguishes between normal and\nabnormal reports, and aligns with radiologists' assessment of clinically\nsignificant errors, while offering flexibility to prioritize specific aspects\nof report content. Our results demonstrate how HeadCT-ONE enables more\nflexible, controllable, and granular automated evaluation of head CT reports.",
      "tldr_zh": "本研究提出HeadCT-ONE，一种通过本体归一化实体和关系提取的指标，用于评估头CT放射学报告生成。HeadCT-ONE改进了现有指标如RadGraph F1，通过领域特定本体处理放射学语言的变异性，并允许对不同实体类型或特定实体进行可控加权，从而实现更细粒度的评估。在三个卫生系统的头CT报告实验中，该方法更好地捕捉语义等价报告、区分正常和异常报告，并与放射科医生的临床错误评估一致，最终提升了头CT报告自动评估的灵活性和准确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13038v1",
      "published_date": "2024-09-19 18:20:11 UTC",
      "updated_date": "2024-09-19 18:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:31.740893"
    },
    {
      "arxiv_id": "2409.12963v2",
      "title": "Interpolating Video-LLMs: Toward Longer-sequence LMMs in a Training-free Manner",
      "title_zh": "翻译失败",
      "authors": [
        "Yuzhang Shang",
        "Bingxin Xu",
        "Weitai Kang",
        "Mu Cai",
        "Yuheng Li",
        "Zehao Wen",
        "Zhen Dong",
        "Kurt Keutzer",
        "Yong Jae Lee",
        "Yan Yan"
      ],
      "abstract": "Advancements in Large Language Models (LLMs) inspire various strategies for\nintegrating video modalities. A key approach is Video-LLMs, which incorporate\nan optimizable interface linking sophisticated video encoders to LLMs. However,\ndue to computation and data limitations, these Video-LLMs are typically\npre-trained to process only short videos, limiting their broader application\nfor understanding longer video content. Additionally, fine-tuning Video-LLMs to\nhandle longer videos is cost-prohibitive. Consequently, it becomes essential to\nexplore the interpolation of Video-LLMs under a completely training-free\nsetting. In this paper, we first identify the primary challenges in\ninterpolating Video-LLMs: (1) the video encoder and modality alignment\nprojector are fixed, preventing the integration of additional frames into\nVideo-LLMs, and (2) the LLM backbone is limited in its content length\ncapabilities, which complicates the processing of an increased number of video\ntokens. To address these challenges, we propose a specific INTerPolation method\nfor Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token\nrearrangement technique that circumvents limitations imposed by the fixed video\nencoder and alignment projector. Furthermore, we introduce a training-free LLM\ncontext window extension method to enable Video-LLMs to understand a\ncorrespondingly increased number of visual tokens.",
      "tldr_zh": "这篇论文针对 Video-LLMs 在处理长视频序列时的限制，提出了一种无训练的插值方法 INTP-Video-LLMs，以扩展其对更长序列的理解。方法包括视频标记重排技术(video token rearrangement)，用于绕过固定视频编码器和模态对齐投影器的约束；以及无训练的 LLM 上下文窗口扩展方法，帮助处理增加的视觉标记。总体上，该方法在不需额外训练的情况下，提升了 Video-LLMs 的适用性，为更高效的视频理解提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12963v2",
      "published_date": "2024-09-19 17:59:55 UTC",
      "updated_date": "2024-10-02 01:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:44.428056"
    },
    {
      "arxiv_id": "2409.12959v2",
      "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
      "title_zh": "MMSearch：评估大型模型作为多模态搜索引擎的潜力",
      "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Ziyu Guo",
        "Yanmin Wu",
        "Jiayi Lei",
        "Pengshuo Qiu",
        "Pan Lu",
        "Zehui Chen",
        "Chaoyou Fu",
        "Guanglu Song",
        "Peng Gao",
        "Yu Liu",
        "Chunyuan Li",
        "Hongsheng Li"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io",
      "tldr_zh": "这篇论文探讨了 Large Multimodal Models (LMMs) 作为多模态搜索引擎的潜力，针对当前 AI 搜索引擎多限于文本-only 模式的局限性。研究者设计了 MMSearch-Engine 管道，使任何 LMMs 能够处理多模态查询，并引入了 MMSearch 基准测试，包括 300 个跨 14 个子领域的实例，用于评估模型在 requery、rerank、summarization 和端到端任务中的性能。实验结果显示，GPT-4o 在该管道下表现最佳，超过了商业产品 Perplexity Pro，并通过错误分析和消融研究揭示了 LMMs 在多模态搜索中的挑战以及扩展测试时计算潜力的重要性。该工作为多模态 AI 搜索引擎的未来发展提供了关键指导。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://mmsearch.github.io",
      "pdf_url": "http://arxiv.org/pdf/2409.12959v2",
      "published_date": "2024-09-19 17:59:45 UTC",
      "updated_date": "2024-11-27 08:49:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:25:57.170665"
    },
    {
      "arxiv_id": "2409.12958v1",
      "title": "MURI: High-Quality Instruction Tuning Datasets for Low-Resource Languages via Reverse Instructions",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullatif Köksal",
        "Marion Thaler",
        "Ayyoob Imani",
        "Ahmet Üstün",
        "Anna Korhonen",
        "Hinrich Schütze"
      ],
      "abstract": "Instruction tuning enhances large language models (LLMs) by aligning them\nwith human preferences across diverse tasks. Traditional approaches to create\ninstruction tuning datasets face serious challenges for low-resource languages\ndue to their dependence on data annotation. This work introduces a novel\nmethod, Multilingual Reverse Instructions (MURI), which generates high-quality\ninstruction tuning datasets for low-resource languages without requiring human\nannotators or pre-existing multilingual models. Utilizing reverse instructions\nand a translation pipeline, MURI produces instruction-output pairs from\nexisting human-written texts in low-resource languages. This method ensures\ncultural relevance and diversity by sourcing texts from different native\ndomains and applying filters to eliminate inappropriate content. Our dataset,\nMURI-IT, includes more than 2 million instruction-output pairs across 200\nlanguages. Evaluation by native speakers and fine-tuning experiments with mT5\nmodels demonstrate the approach's effectiveness for both NLU and open-ended\ngeneration. We publicly release datasets and models at\nhttps://github.com/akoksal/muri.",
      "tldr_zh": "该论文提出 MURI 方法，通过 reverse instructions 和翻译管道，从现有人类撰写文本生成高质量的指令微调数据集，针对低资源语言避免了依赖人工标注的挑战。MURI 确保数据集的文化相关性和多样性，并创建了 MURI-IT 数据集，包含超过 200 万对指令-输出对，覆盖 200 种语言。实验结果显示，通过对 mT5 模型的微调，MURI-IT 在 NLU 和开放生成任务中表现出色，并已公开数据集和模型以供进一步使用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12958v1",
      "published_date": "2024-09-19 17:59:20 UTC",
      "updated_date": "2024-09-19 17:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:26:08.820945"
    },
    {
      "arxiv_id": "2409.12953v4",
      "title": "JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images",
      "title_zh": "翻译失败",
      "authors": [
        "Zhecan Wang",
        "Junzhang Liu",
        "Chia-Wei Tang",
        "Hani Alomari",
        "Anushka Sivakumar",
        "Rui Sun",
        "Wenhao Li",
        "Md. Atabuzzaman",
        "Hammad Ayyubi",
        "Haoxuan You",
        "Alvi Ishmam",
        "Kai-Wei Chang",
        "Shih-Fu Chang",
        "Chris Thomas"
      ],
      "abstract": "Existing vision-language understanding benchmarks largely consist of images\nof objects in their usual contexts. As a consequence, recent multimodal large\nlanguage models can perform well with only a shallow visual understanding by\nrelying on background language biases. Thus, strong performance on these\nbenchmarks does not necessarily correlate with strong visual understanding. In\nthis paper, we release JourneyBench, a comprehensive human-annotated benchmark\nof generated images designed to assess the model's fine-grained multimodal\nreasoning abilities across five tasks: complementary multimodal chain of\nthought, multi-image VQA, imaginary image captioning, VQA with hallucination\ntriggers, and fine-grained retrieval with sample-specific distractors. Unlike\nexisting benchmarks, JourneyBench explicitly requires fine-grained multimodal\nreasoning in unusual imaginary scenarios where language bias and holistic image\ngist are insufficient. We benchmark state-of-the-art models on JourneyBench and\nanalyze performance along a number of fine-grained dimensions. Results across\nall five tasks show that JourneyBench is exceptionally challenging for even the\nbest models, indicating that models' visual reasoning abilities are not as\nstrong as they first appear. We discuss the implications of our findings and\npropose avenues for further research.",
      "tldr_zh": "本研究指出，现有的视觉语言理解基准主要依赖常见图像，导致多模态大语言模型仅通过浅层视觉理解和语言偏见即可表现良好，从而掩盖了其真实能力。该论文引入 JourneyBench，一个全面的人类标注基准，专注于生成图像，评估模型在五个任务上的细粒度多模态推理能力，包括 complementary multimodal chain of thought、多图像 VQA、imaginary image captioning、VQA with hallucination triggers 以及 fine-grained retrieval with sample-specific distractors。实验结果显示，即使是先进模型在 JourneyBench 上也面临巨大挑战，揭示了其视觉推理能力的不足。该基准的提出为未来多模态模型研究提供了新方向，强调了需要更精确的视觉理解机制。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12953v4",
      "published_date": "2024-09-19 17:58:16 UTC",
      "updated_date": "2025-01-10 02:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:26:19.456831"
    },
    {
      "arxiv_id": "2409.12951v2",
      "title": "Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm",
      "title_zh": "翻译失败",
      "authors": [
        "Akshat Gupta",
        "Atahan Ozdemir",
        "Gopala Anumanchipalli"
      ],
      "abstract": "This paper presents a novel geometric interpretation of LayerNorm and\nexplores how LayerNorm influences the norm and orientation of hidden vectors in\nthe representation space. With these geometric insights, we prepare the\nfoundation for comparing LayerNorm with RMSNorm. We show that the definition of\nLayerNorm is innately linked to the uniform vector, defined as $\\boldsymbol{1}\n= [1, 1, 1, 1, \\cdots, 1]^T \\in \\mathbb{R}^d$. We then show that the\nstandardization step in LayerNorm can be understood in three simple steps: (i)\nremove the component of a vector along the uniform vector, (ii) normalize the\nremaining vector, and (iii) scale the resultant vector by $\\sqrt{d}$, where $d$\nis the dimensionality of the representation space. We also provide additional\ninsights into how LayerNorm operates at inference time. Finally, we compare the\nhidden representations of LayerNorm-based LLMs with models trained using\nRMSNorm and show that all LLMs naturally operate orthogonal to the uniform\nvector at inference time, that is, on average they do not have a component\nalong the uniform vector during inference. This presents the first mechanistic\nevidence that removing the component along the uniform vector in LayerNorm is a\nredundant step. These results advocate for using RMSNorm over LayerNorm which\nis also more computationally efficient.",
      "tldr_zh": "本文提出 LayerNorm 的新几何解释，揭示其通过移除向量沿 uniform vector 的分量、归一化剩余向量并以 √d 缩放，来影响隐藏向量的范数和方向。研究比较了 LayerNorm 和 RMSNorm 在大型语言模型(LLMs)中的表现，发现所有 LLMs 在推理时自然与 uniform vector 正交，这表明 LayerNorm 中移除该分量的步骤是冗余的。最终，论文主张采用 RMSNorm，因为它更计算高效且性能相当。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12951v2",
      "published_date": "2024-09-19 17:58:07 UTC",
      "updated_date": "2025-02-01 06:06:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:26:42.380413"
    },
    {
      "arxiv_id": "2409.13007v2",
      "title": "iCost: A Novel Instance Complexity Based Cost-Sensitive Learning Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Asif Newaz",
        "Asif Ur Rahman Adib",
        "Taskeed Jabid"
      ],
      "abstract": "Class imbalance in data presents significant challenges for classification\ntasks. It is fairly common and requires careful handling to obtain desirable\nperformance. Traditional classification algorithms become biased toward the\nmajority class. One way to alleviate the scenario is to make the classifiers\ncost-sensitive. This is achieved by assigning a higher misclassification cost\nto minority-class instances. One issue with this implementation is that all the\nminority-class instances are treated equally, and assigned with the same\npenalty value. However, the learning difficulties of all the instances are not\nthe same. Instances that are located in the overlapping region or near the\ndecision boundary are harder to classify, whereas those further away are\neasier. Without taking into consideration the instance complexity and naively\nweighting all the minority-class samples uniformly, results in an unwarranted\nbias and consequently, a higher number of misclassifications of the\nmajority-class instances. This is undesirable and to overcome the situation, we\npropose a novel instance complexity-based cost-sensitive approach (termed\n'iCost') in this study. We first categorize all the minority-class instances\nbased on their difficulty level and then the instances are penalized\naccordingly. This ensures a more equitable instance weighting and prevents\nexcessive penalization. The performance of the proposed approach is tested on\n65 binary and 10 multiclass imbalanced datasets against the traditional\ncost-sensitive learning frameworks. A significant improvement in performance\nhas been observed, demonstrating the effectiveness of the proposed strategy.",
      "tldr_zh": "本研究针对类别不平衡问题提出了一种新型框架iCost，该框架基于instance complexity对少数类实例进行难度分类，并据此调整误分类成本，以实现更公平的实例加权和减少偏置。不同于传统cost-sensitive learning方法，iCost考虑了实例的位置（如重叠区域或决策边界附近），从而避免了对所有少数类样本的统一惩罚。实验在65个二元和10个多类不平衡数据集上验证了iCost的性能，显示出显著改善，证明了该策略的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13007v2",
      "published_date": "2024-09-19 17:53:21 UTC",
      "updated_date": "2024-10-25 11:29:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:26:43.897673"
    },
    {
      "arxiv_id": "2409.12919v1",
      "title": "Swine Diet Design using Multi-objective Regionalized Bayesian Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel D. Uribe-Guerra",
        "Danny A. Múnera-Ramírez",
        "Julián D. Arias-Londoño"
      ],
      "abstract": "The design of food diets in the context of animal nutrition is a complex\nproblem that aims to develop cost-effective formulations while balancing\nminimum nutritional content. Traditional approaches based on theoretical models\nof metabolic responses and concentrations of digestible energy in raw materials\nface limitations in incorporating zootechnical or environmental variables\naffecting the performance of animals and including multiple objectives aligned\nwith sustainable development policies. Recently, multi-objective Bayesian\noptimization has been proposed as a promising heuristic alternative able to\ndeal with the combination of multiple sources of information, multiple and\ndiverse objectives, and with an intrinsic capacity to deal with uncertainty in\nthe measurements that could be related to variability in the nutritional\ncontent of raw materials. However, Bayesian optimization encounters\ndifficulties in high-dimensional search spaces, leading to exploration\npredominantly at the boundaries. This work analyses a strategy to split the\nsearch space into regions that provide local candidates termed multi-objective\nregionalized Bayesian optimization as an alternative to improve the quality of\nthe Pareto set and Pareto front approximation provided by BO in the context of\nswine diet design. Results indicate that this regionalized approach produces\nmore diverse non-dominated solutions compared to the standard multi-objective\nBayesian optimization. Besides, the regionalized strategy was four times more\neffective in finding solutions that outperform those identified by a stochastic\nprogramming approach referenced in the literature. Experiments using batches of\nquery candidate solutions per iteration show that the optimization process can\nalso be accelerated without compromising the quality of the Pareto set\napproximation during the initial, most critical phase of optimization.",
      "tldr_zh": "本研究针对猪饲料设计（Swine Diet Design）的复杂性，提出了一种多目标区域化贝叶斯优化（Multi-objective Regionalized Bayesian Optimization）方法，以平衡成本、营养含量以及动物性能和环境变量。相比传统多目标贝叶斯优化，该方法通过将搜索空间分割成区域，生成更多样化的非支配解（non-dominated solutions），并在实验中比文献中的随机规划方法更有效，四倍于其性能。结果表明，这种策略不仅提高了Pareto set和Pareto front逼近的质量，还能通过批量查询候选解加速优化过程，而不牺牲初始阶段的优化质量。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12919v1",
      "published_date": "2024-09-19 17:18:00 UTC",
      "updated_date": "2024-09-19 17:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:26:56.311980"
    },
    {
      "arxiv_id": "2409.12903v2",
      "title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small Model Initialization",
      "title_zh": "智能缩放：使用小模型初始化加速大型语言模型的预训练",
      "authors": [
        "Mohammad Samragh",
        "Iman Mirzadeh",
        "Keivan Alizadeh Vahid",
        "Fartash Faghri",
        "Minsik Cho",
        "Moin Nabi",
        "Devang Naik",
        "Mehrdad Farajtabar"
      ],
      "abstract": "The pre-training phase of language models often begins with randomly\ninitialized parameters. With the current trends in scaling models, training\ntheir large number of parameters can be extremely slow and costly. In contrast,\nsmall language models are less expensive to train, but they often cannot\nachieve the accuracy of large models. In this paper, we explore an intriguing\nidea to connect these two different regimes: Can we develop a method to\ninitialize large language models using smaller pre-trained models? Will such\ninitialization bring any benefits in terms of training time and final accuracy?\nIn this paper, we introduce HyperCloning, a method that can expand the\nparameters of a pre-trained language model to those of a larger model with\nincreased hidden dimensions. Our method ensures that the larger model retains\nthe functionality of the smaller model. As a result, the larger model already\ninherits the predictive power and accuracy of the smaller model before the\ntraining starts. We demonstrate that training such an initialized model results\nin significant savings in terms of GPU hours required for pre-training large\nlanguage models.",
      "tldr_zh": "本研究探讨了通过小型预训练模型初始化大型语言模型（Large Language Models, LLMs）的方法，以加速其预训练过程。作者提出HyperCloning技术，该方法将小型模型的参数扩展到更大模型的隐藏维度，确保新模型继承小型模型的预测能力和准确性，从而避免从随机初始化开始。实验结果显示，这种初始化方式显著减少了LLMs预训练所需的GPU hours，提供了一种高效的模型扩展策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12903v2",
      "published_date": "2024-09-19 16:50:26 UTC",
      "updated_date": "2024-09-20 16:22:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:27:06.980843"
    },
    {
      "arxiv_id": "2409.12900v1",
      "title": "Recognition of Harmful Phytoplankton from Microscopic Images using Deep Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Aymane Khaldi",
        "Rohaifa Khaldi"
      ],
      "abstract": "Monitoring plankton distribution, particularly harmful phytoplankton, is\nvital for preserving aquatic ecosystems, regulating the global climate, and\nensuring environmental protection. Traditional methods for monitoring are often\ntime-consuming, expensive, error-prone, and unsuitable for large-scale\napplications, highlighting the need for accurate and efficient automated\nsystems. In this study, we evaluate several state-of-the-art CNN models,\nincluding ResNet, ResNeXt, DenseNet, and EfficientNet, using three transfer\nlearning approaches: linear probing, fine-tuning, and a combined approach, to\nclassify eleven harmful phytoplankton genera from microscopic images. The best\nperformance was achieved by ResNet-50 using the fine-tuning approach, with an\naccuracy of 96.97%. The results also revealed that the models struggled to\ndifferentiate between four harmful phytoplankton types with similar\nmorphological features.",
      "tldr_zh": "本研究强调监控有害浮游植物对水生生态系统和环境保护的重要性，并指出传统方法耗时且易出错，因此提出使用深度学习进行高效分类。研究评估了多种 CNN 模型，包括 ResNet、ResNeXt、DenseNet 和 EfficientNet，通过线性探测、微调和组合的迁移学习方法，对 11 种有害浮游植物的显微镜图像进行分类。ResNet-50 采用微调方法取得了最佳性能，准确率达 96.97%，但模型在区分形态相似的四种浮游植物时仍面临挑战，为自动化监控系统的发展提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12900v1",
      "published_date": "2024-09-19 16:42:53 UTC",
      "updated_date": "2024-09-19 16:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:27:20.610894"
    },
    {
      "arxiv_id": "2409.15372v1",
      "title": "Fuzzy Rule based Intelligent Cardiovascular Disease Prediction using Complex Event Processing",
      "title_zh": "翻译失败",
      "authors": [
        "Shashi Shekhar Kumar",
        "Anurag Harsh",
        "Ritesh Chandra",
        "Sonali Agarwal"
      ],
      "abstract": "Cardiovascular disease (CVDs) is a rapidly rising global concern due to\nunhealthy diets, lack of physical activity, and other factors. According to the\nWorld Health Organization (WHO), primary risk factors include elevated blood\npressure, glucose, blood lipids, and obesity. Recent research has focused on\naccurate and timely disease prediction to reduce risk and fatalities, often\nrelying on predictive models trained on large datasets, which require intensive\ntraining. An intelligent system for CVDs patients could greatly assist in\nmaking informed decisions by effectively analyzing health parameters. Complex\nEvent Processing (CEP) has emerged as a valuable method for solving real-time\nchallenges by aggregating patterns of interest and their causes and effects on\nend users. In this work, we propose a fuzzy rule-based system for monitoring\nclinical data to provide real-time decision support. We designed fuzzy rules\nbased on clinical and WHO standards to ensure accurate predictions. Our\nintegrated approach uses Apache Kafka and Spark for data streaming, and the\nSiddhi CEP engine for event processing. Additionally, we pass numerous\ncardiovascular disease-related parameters through CEP engines to ensure fast\nand reliable prediction decisions. To validate the effectiveness of our\napproach, we simulated real-time, unseen data to predict cardiovascular\ndisease. Using synthetic data (1000 samples), we categorized it into \"Very Low\nRisk, Low Risk, Medium Risk, High Risk, and Very High Risk.\" Validation results\nshowed that 20% of samples were categorized as very low risk, 15-45% as low\nrisk, 35-65% as medium risk, 55-85% as high risk, and 75% as very high risk.",
      "tldr_zh": "这篇论文提出了一种基于 Fuzzy Rule 的智能系统，利用 Complex Event Processing (CEP) 来预测心血管疾病 (CVDs)，旨在通过实时监控临床数据提供准确决策支持。系统设计了符合临床和 WHO 标准的模糊规则，并整合 Apache Kafka 和 Spark 用于数据流处理，以及 Siddhi CEP 引擎进行事件处理，以实现快速可靠的风险评估。实验使用 1000 份合成数据模拟实时场景，将样本分类为“Very Low Risk”、“Low Risk”、“Medium Risk”、“High Risk”和“Very High Risk”，结果显示风险分布从 20% (极低风险) 到 75% (极高风险)，验证了该方法的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15372v1",
      "published_date": "2024-09-19 16:36:24 UTC",
      "updated_date": "2024-09-19 16:36:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:27:32.420761"
    },
    {
      "arxiv_id": "2409.12889v2",
      "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case",
      "title_zh": "VL",
      "authors": [
        "Peng Chen",
        "Pi Bu",
        "Jun Song",
        "Yuan Gao",
        "Bo Zheng"
      ],
      "abstract": "Recently, large language model (LLM)-based agents have made significant\nadvances across various fields. One of the most popular research areas involves\napplying these agents to video games. Traditionally, these methods have relied\non game APIs to access in-game environmental and action data. However, this\napproach is limited by the availability of APIs and does not reflect how humans\nplay games. With the advent of vision language models (VLMs), agents now have\nenhanced visual understanding capabilities, enabling them to interact with\ngames using only visual inputs. Despite these advances, current approaches\nstill face challenges in action-oriented tasks, particularly in action\nrole-playing games (ARPGs), where reinforcement learning methods are prevalent\nbut suffer from poor generalization and require extensive training. To address\nthese limitations, we select an ARPG, ``Black Myth: Wukong'', as a research\nplatform to explore the capability boundaries of existing VLMs in scenarios\nrequiring visual-only input and complex action output. We define 12 tasks\nwithin the game, with 75% focusing on combat, and incorporate several\nstate-of-the-art VLMs into this benchmark. Additionally, we will release a\nhuman operation dataset containing recorded gameplay videos and operation logs,\nincluding mouse and keyboard actions. Moreover, we propose a novel VARP (Vision\nAction Role-Playing) agent framework, consisting of an action planning system\nand a visual trajectory system. Our framework demonstrates the ability to\nperform basic tasks and succeed in 90% of easy and medium-level combat\nscenarios. This research aims to provide new insights and directions for\napplying multimodal agents in complex action game environments. The code and\ndatasets will be made available at https://varp-agent.github.io/.",
      "tldr_zh": "本研究探讨视觉语言模型（VLMs）在动作角色扮演游戏（ARPGs）中的表现，以《Black Myth: Wukong》作为案例，针对传统方法依赖游戏 API 和强化学习泛化差的问题。研究者定义了12个游戏任务（其中75%涉及战斗），并使用多种state-of-the-art VLMs进行基准测试，同时发布了一个包含游戏视频和操作日志的人类操作数据集。论文提出了一种新型VARP（Vision Action Role-Playing）代理框架，包括行动规划系统和视觉轨迹系统，该框架能在基本任务中表现出色，并在90%的简单和中等难度战斗场景中成功。该工作为多模态代理在复杂游戏环境中的应用提供了新见解，并将代码和数据集开源。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12889v2",
      "published_date": "2024-09-19 16:30:25 UTC",
      "updated_date": "2024-09-22 09:51:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:27:44.879528"
    },
    {
      "arxiv_id": "2409.12883v1",
      "title": "Improving Prototypical Parts Abstraction for Case-Based Reasoning Explanations Designed for the Kidney Stone Type Recognition",
      "title_zh": "针对肾结石类型识别",
      "authors": [
        "Daniel Flores-Araiza",
        "Francisco Lopez-Tiro",
        "Clément Larose",
        "Salvador Hinojosa",
        "Andres Mendez-Vazquez",
        "Miguel Gonzalez-Mendoza",
        "Gilberto Ochoa-Ruiz",
        "Christian Daul"
      ],
      "abstract": "The in-vivo identification of the kidney stone types during an ureteroscopy\nwould be a major medical advance in urology, as it could reduce the time of the\ntedious renal calculi extraction process, while diminishing infection risks.\nFurthermore, such an automated procedure would make possible to prescribe\nanti-recurrence treatments immediately. Nowadays, only few experienced\nurologists are able to recognize the kidney stone types in the images of the\nvideos displayed on a screen during the endoscopy. Thus, several deep learning\n(DL) models have recently been proposed to automatically recognize the kidney\nstone types using ureteroscopic images. However, these DL models are of black\nbox nature whicl limits their applicability in clinical settings. This\ncontribution proposes a case-based reasoning DL model which uses prototypical\nparts (PPs) and generates local and global descriptors. The PPs encode for each\nclass (i.e., kidney stone type) visual feature information (hue, saturation,\nintensity and textures) similar to that used by biologists. The PPs are\noptimally generated due a new loss function used during the model training.\nMoreover, the local and global descriptors of PPs allow to explain the\ndecisions (\"what\" information, \"where in the images\") in an understandable way\nfor biologists and urologists. The proposed DL model has been tested on a\ndatabase including images of the six most widespread kidney stone types. The\noverall average classification accuracy was 90.37. When comparing this results\nwith that of the eight other DL models of the kidney stone state-of-the-art, it\ncan be seen that the valuable gain in explanability was not reached at the\nexpense of accuracy which was even slightly increased with respect to that\n(88.2) of the best method of the literature. These promising and interpretable\nresults also encourage urologists to put their trust in AI-based solutions.",
      "tldr_zh": "该论文针对肾结石类型识别问题，提出了一种改进的 Prototypical Parts (PPs) 抽象方法，应用于基于案例推理的深度学习 (DL) 模型，以提升模型的可解释性。方法通过一个新的损失函数优化 PPs 的生成，编码每个肾结石类型的视觉特征（如色调、饱和度、强度和纹理），并产生本地和全局描述符，帮助解释决策的“什么”和“在哪里”信息。在包含六种常见肾结石类型的数据库上测试，该模型的平均分类准确率达到90.37%，比文献中最佳方法（88.2%）略高，同时增强了 AI 在临床中的可信度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper submitted to Artificial Intelligence in Medicine. (AIIM),\n  Elsevier",
      "pdf_url": "http://arxiv.org/pdf/2409.12883v1",
      "published_date": "2024-09-19 16:27:32 UTC",
      "updated_date": "2024-09-19 16:27:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:27:57.112213"
    },
    {
      "arxiv_id": "2409.12880v1",
      "title": "Enhancing E-commerce Product Title Translation with Retrieval-Augmented Generation and Large Language Models",
      "title_zh": "利用检索增强生成和大语言模型提升电子商务产品标题翻译",
      "authors": [
        "Bryan Zhang",
        "Taichi Nakatani",
        "Stephan Walter"
      ],
      "abstract": "E-commerce stores enable multilingual product discovery which require\naccurate product title translation. Multilingual large language models (LLMs)\nhave shown promising capacity to perform machine translation tasks, and it can\nalso enhance and translate product titles cross-lingually in one step. However,\nproduct title translation often requires more than just language conversion\nbecause titles are short, lack context, and contain specialized terminology.\nThis study proposes a retrieval-augmented generation (RAG) approach that\nleverages existing bilingual product information in e-commerce by retrieving\nsimilar bilingual examples and incorporating them as few-shot prompts to\nenhance LLM-based product title translation. Experiment results show that our\nproposed RAG approach improve product title translation quality with chrF score\ngains of up to 15.3% for language pairs where the LLM has limited proficiency.",
      "tldr_zh": "这篇论文针对电子商务产品标题翻译的挑战（如标题短缺上下文和包含专业术语），提出了一种结合检索增强生成(RAG)技术与大型语言模型(LLMs)的改进方法。方法通过检索类似双语产品信息作为few-shot prompts，增强LLMs的翻译准确性。实验结果显示，该RAG方法显著提升了翻译质量，对于LLMs能力有限的语言对，chrF score提高了多达15.3%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 Pages,In Proceedings of ACM CIKM Workshop on Data-Centric AI (CIKM\n  DCAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.12880v1",
      "published_date": "2024-09-19 16:23:42 UTC",
      "updated_date": "2024-09-19 16:23:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:28:07.581656"
    },
    {
      "arxiv_id": "2409.12865v2",
      "title": "KnowFormer: Revisiting Transformers for Knowledge Graph Reasoning",
      "title_zh": "Know",
      "authors": [
        "Junnan Liu",
        "Qianren Mao",
        "Weifeng Jiang",
        "Jianxin Li"
      ],
      "abstract": "Knowledge graph reasoning plays a vital role in various applications and has\ngarnered considerable attention. Recently, path-based methods have achieved\nimpressive performance. However, they may face limitations stemming from\nconstraints in message-passing neural networks, such as missing paths and\ninformation over-squashing. In this paper, we revisit the application of\ntransformers for knowledge graph reasoning to address the constraints faced by\npath-based methods and propose a novel method KnowFormer. KnowFormer utilizes a\ntransformer architecture to perform reasoning on knowledge graphs from the\nmessage-passing perspective, rather than reasoning by textual information like\nprevious pretrained language model based methods. Specifically, we define the\nattention computation based on the query prototype of knowledge graph\nreasoning, facilitating convenient construction and efficient optimization. To\nincorporate structural information into the self-attention mechanism, we\nintroduce structure-aware modules to calculate query, key, and value\nrespectively. Additionally, we present an efficient attention computation\nmethod for better scalability. Experimental results demonstrate the superior\nperformance of KnowFormer compared to prominent baseline methods on both\ntransductive and inductive benchmarks.",
      "tldr_zh": "该论文重新审视了Transformer在知识图谱推理中的应用，旨在解决路径-based方法存在的局限性，如missing paths和information over-squashing问题。作者提出了一种新方法KnowFormer，利用Transformer架构从message-passing视角进行推理，而不是依赖文本信息。KnowFormer通过基于知识图谱推理的查询原型定义注意力计算，并引入structure-aware模块，将结构信息融入self-attention机制中，同时优化了高效注意力计算以提升可扩展性。实验结果显示，KnowFormer在transductive和inductive基准上优于现有基线方法，证明了其在知识图谱推理中的优越性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ICML2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12865v2",
      "published_date": "2024-09-19 16:08:10 UTC",
      "updated_date": "2024-12-17 18:27:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:28:19.783920"
    },
    {
      "arxiv_id": "2409.12846v2",
      "title": "How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Symbols",
      "title_zh": "翻译失败",
      "authors": [
        "Volker Tresp",
        "Hang Li"
      ],
      "abstract": "The Tensor Brain (TB) has been introduced as a computational model for\nperception and memory. This paper provides an overview of the TB model,\nincorporating recent developments and insights into its functionality. The TB\nis composed of two primary layers: the representation layer and the index\nlayer. The representation layer serves as a model for the subsymbolic global\nworkspace, a concept derived from consciousness research. Its state represents\nthe cognitive brain state, capturing the dynamic interplay of sensory and\ncognitive processes. The index layer, in contrast, contains symbolic\nrepresentations for concepts, time instances, and predicates. In a bottom-up\noperation, sensory input activates the representation layer, which then\ntriggers associated symbolic labels in the index layer. Conversely, in a\ntop-down operation, symbols in the index layer activate the representation\nlayer, which in turn influences earlier processing layers through embodiment.\nThis top-down mechanism underpins semantic memory, enabling the integration of\nabstract knowledge into perceptual and cognitive processes. A key feature of\nthe TB is its use of concept embeddings, which function as connection weights\nlinking the index layer to the representation layer. As a concept's ``DNA,''\nthese embeddings consolidate knowledge from diverse experiences, sensory\nmodalities, and symbolic representations, providing a unified framework for\nlearning and memory.",
      "tldr_zh": "该论文概述了Tensor Brain (TB) 模型，这是一个用于感知和记忆的计算框架，结合了意识研究的子符号全局工作区概念。TB 由两个主要层组成：representation layer 捕捉动态的认知脑状态，包括感官和认知过程；index layer 则存储符号表示，如概念、时间实例和谓词。模型通过bottom-up 操作让感官输入激活representation layer 并触发index layer 的符号，以及top-down 操作让符号反向影响感知过程，从而实现语义记忆的整合。概念 embeddings 作为关键机制，充当连接权重大小，融合多模态知识，提供一个统一的学习和记忆框架。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12846v2",
      "published_date": "2024-09-19 15:45:38 UTC",
      "updated_date": "2024-12-29 09:20:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:28:32.018304"
    },
    {
      "arxiv_id": "2409.13000v2",
      "title": "Introducing the Large Medical Model: State of the art healthcare cost and risk prediction with transformers trained on patient event sequences",
      "title_zh": "翻译失败",
      "authors": [
        "Ricky Sahu",
        "Eric Marriott",
        "Ethan Siegel",
        "David Wagner",
        "Flore Uzan",
        "Troy Yang",
        "Asim Javed"
      ],
      "abstract": "With U.S. healthcare spending approaching $5T (NHE Fact Sheet 2024), and 25%\nof it estimated to be wasteful (Waste in the US the health care system:\nestimated costs and potential for savings, n.d.), the need to better predict\nrisk and optimal patient care is evermore important. This paper introduces the\nLarge Medical Model (LMM), a generative pre-trained transformer (GPT) designed\nto guide and predict the broad facets of patient care and healthcare\nadministration. The model is trained on medical event sequences from over 140M\nlongitudinal patient claims records with a specialized vocabulary built from\nmedical terminology systems and demonstrates a superior capability to forecast\nhealthcare costs and identify potential risk factors. Through experimentation\nand validation, we showcase the LMM's proficiency in not only in cost and risk\npredictions, but also in discerning intricate patterns within complex medical\nconditions and an ability to identify novel relationships in patient care. The\nLMM is able to improve both cost prediction by 14.1% over the best commercial\nmodels and chronic conditions prediction by 1.9% over the best transformer\nmodels in research predicting a broad set of conditions. The LMM is a\nsubstantial advancement in healthcare analytics, offering the potential to\nsignificantly enhance risk assessment, cost management, and personalized\nmedicine.",
      "tldr_zh": "本研究引入了Large Medical Model (LMM)，一个基于Transformer和GPT的生成式预训练模型，旨在提升医疗成本预测和风险评估，以应对美国医疗支出浪费问题。LMM使用超过1.4亿患者的纵向索赔记录作为训练数据，并构建了基于医疗术语系统的专用词汇表，能够精准预测医疗事件序列中的成本和风险因素。实验结果显示，LMM在成本预测上比最佳商业模型提高了14.1%，在慢性病预测上比现有Transformer模型提高了1.9%，并能识别复杂医疗模式和新患者护理关系。该模型为风险评估、成本管理和个性化医疗提供了重大进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP",
        "stat.ML",
        "I.2.1; K.4.1; K.4.3; J.1; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.13000v2",
      "published_date": "2024-09-19 15:38:21 UTC",
      "updated_date": "2024-12-05 17:19:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:28:43.375278"
    },
    {
      "arxiv_id": "2409.12842v1",
      "title": "Vision Language Models Can Parse Floor Plan Maps",
      "title_zh": "视觉语言模型能够解析楼层平面图地图",
      "authors": [
        "David DeFazio",
        "Hrudayangam Mehta",
        "Jeremy Blackburn",
        "Shiqi Zhang"
      ],
      "abstract": "Vision language models (VLMs) can simultaneously reason about images and\ntexts to tackle many tasks, from visual question answering to image captioning.\nThis paper focuses on map parsing, a novel task that is unexplored within the\nVLM context and particularly useful to mobile robots. Map parsing requires\nunderstanding not only the labels but also the geometric configurations of a\nmap, i.e., what areas are like and how they are connected. To evaluate the\nperformance of VLMs on map parsing, we prompt VLMs with floorplan maps to\ngenerate task plans for complex indoor navigation. Our results demonstrate the\nremarkable capability of VLMs in map parsing, with a success rate of 0.96 in\ntasks requiring a sequence of nine navigation actions, e.g., approaching and\ngoing through doors. Other than intuitive observations, e.g., VLMs do better in\nsmaller maps and simpler navigation tasks, there was a very interesting\nobservation that its performance drops in large open areas. We provide\npractical suggestions to address such challenges as validated by our\nexperimental results. Webpage: https://shorturl.at/OUkEY",
      "tldr_zh": "本文研究了 Vision Language Models (VLMs) 在地图解析任务中的应用，该任务要求理解地图的标签和几何配置（如区域连接），这对移动机器人室内导航非常有用。作者通过提示 VLMs 使用楼层平面图生成复杂的导航任务计划（如序列九个动作的路径），评估了其性能，结果显示成功率高达 0.96。实验观察到 VLMs 在较小地图和简单任务中表现更好，但在大型开放区域效果下降，并提供了实用建议来解决这些挑战。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12842v1",
      "published_date": "2024-09-19 15:36:28 UTC",
      "updated_date": "2024-09-19 15:36:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:28:55.781644"
    },
    {
      "arxiv_id": "2409.19001v1",
      "title": "Pay Attention to What Matters",
      "title_zh": "翻译失败",
      "authors": [
        "Pedro Luiz Silva",
        "Antonio de Domenico",
        "Ali Maatouk",
        "Fadhel Ayed"
      ],
      "abstract": "Despite the remarkable success of Large Language Models (LLMs), they still\nexhibit a limited capability to align their outputs to the user instructions.\nIn this work, we introduce a simple and effective method, which we name GUIDE,\nthat mechanistically increases attention scores in instruction tokens. To\nsupport this operation, we present Influence, a novel metric that highlights\nhow the user's instructions propagate through the transformer layers and impact\nthe LLM output. Our results show that GUIDE improves the accuracy of following\ninstructions 29.4 % to 60.4%, outperforming natural prompting alternatives and\nSupervised Fine-Tuning up to 1M tokens.",
      "tldr_zh": "本文研究发现，大型语言模型 (LLMs) 在输出与用户指令对齐方面能力有限，因此提出了一种简单有效的 GUIDE 方法，通过机制性地增加指令标记的注意力分数来提升模型对指令的关注。作者同时引入了 Influence 指标，用于分析用户指令如何在 transformer 层中传播并影响 LLM 输出。实验结果显示，GUIDE 将指令遵循准确率从 29.4% 提高到 60.4%，并在性能上超过了自然提示和 Supervised Fine-Tuning 方法（使用多达 1M tokens）。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.19001v1",
      "published_date": "2024-09-19 15:26:50 UTC",
      "updated_date": "2024-09-19 15:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:29:08.230308"
    },
    {
      "arxiv_id": "2409.12832v3",
      "title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
      "title_zh": "翻译失败",
      "authors": [
        "Tenghao Huang",
        "Donghee Lee",
        "John Sweeney",
        "Jiatong Shi",
        "Emily Steliotes",
        "Matthew Lange",
        "Jonathan May",
        "Muhao Chen"
      ],
      "abstract": "Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.",
      "tldr_zh": "本文研究了食品行业的调味开发面临的挑战，即快速创新和精确调味轮廓创建的需求，传统主观测试方法效率低下。论文的主要贡献包括：定义调味科学的新问题领域，专注于生成调味轮廓来源和理解的假设；引入 FoodPuzzle 基准测试，涵盖978种食品物品和1766种调味分子轮廓；并提出一种新型 Scientific Agent 方法，整合 in-context learning 和 retrieval augmented techniques 来生成可靠的食品科学假设。实验结果表明，该模型在调味轮廓预测任务中显著优于传统方法，展示了其在转变调味开发实践方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12832v3",
      "published_date": "2024-09-19 15:07:35 UTC",
      "updated_date": "2024-10-07 01:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:29:21.768910"
    },
    {
      "arxiv_id": "2409.12997v1",
      "title": "VCAT: Vulnerability-aware and Curiosity-driven Adversarial Training for Enhancing Autonomous Vehicle Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan Cai",
        "Zhiyong Cui",
        "Xuesong Bai",
        "Ruimin Ke",
        "Zhenshu Ma",
        "Haiyang Yu",
        "Yilong Ren"
      ],
      "abstract": "Autonomous vehicles (AVs) face significant threats to their safe operation in\ncomplex traffic environments. Adversarial training has emerged as an effective\nmethod of enabling AVs to preemptively fortify their robustness against\nmalicious attacks. Train an attacker using an adversarial policy, allowing the\nAV to learn robust driving through interaction with this attacker. However,\nadversarial policies in existing methodologies often get stuck in a loop of\noverexploiting established vulnerabilities, resulting in poor improvement for\nAVs. To overcome the limitations, we introduce a pioneering framework termed\nVulnerability-aware and Curiosity-driven Adversarial Training (VCAT).\nSpecifically, during the traffic vehicle attacker training phase, a surrogate\nnetwork is employed to fit the value function of the AV victim, providing dense\ninformation about the victim's inherent vulnerabilities. Subsequently, random\nnetwork distillation is used to characterize the novelty of the environment,\nconstructing an intrinsic reward to guide the attacker in exploring unexplored\nterritories. In the victim defense training phase, the AV is trained in\ncritical scenarios in which the pretrained attacker is positioned around the\nvictim to generate attack behaviors. Experimental results revealed that the\ntraining methodology provided by VCAT significantly improved the robust control\ncapabilities of learning-based AVs, outperforming both conventional training\nmodalities and alternative reinforcement learning counterparts, with a marked\nreduction in crash rates. The code is available at\nhttps://github.com/caixxuan/VCAT.",
      "tldr_zh": "该论文提出VCAT框架，即Vulnerability-aware and Curiosity-driven Adversarial Training，以提升Autonomous Vehicle (AVs)的鲁棒性，针对恶意攻击的威胁。VCAT在攻击者训练阶段使用代理网络拟合AV受害者的价值函数来识别漏洞，并结合随机网络蒸馏构建内在奖励，引导攻击者探索新环境；在防御训练阶段，AV在关键场景中与预训练攻击者互动进行训练。实验结果显示，VCAT显著降低了AV的碰撞率，并优于传统训练和强化学习方法，提供更有效的鲁棒性控制。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 5 figures, conference",
      "pdf_url": "http://arxiv.org/pdf/2409.12997v1",
      "published_date": "2024-09-19 14:53:02 UTC",
      "updated_date": "2024-09-19 14:53:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:29:31.809757"
    },
    {
      "arxiv_id": "2409.12820v1",
      "title": "Machine-learning based high-bandwidth magnetic sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Galya Haim",
        "Stefano Martina",
        "John Howell",
        "Nir Bar-Gill",
        "Filippo Caruso"
      ],
      "abstract": "Recent years have seen significant growth of quantum technologies, and\nspecifically quantum sensing, both in terms of the capabilities of advanced\nplatforms and their applications. One of the leading platforms in this context\nis nitrogen-vacancy (NV) color centers in diamond, providing versatile,\nhigh-sensitivity, and high-resolution magnetic sensing. Nevertheless, current\nschemes for spin resonance magnetic sensing (as applied by NV quantum sensing)\nsuffer from tradeoffs associated with sensitivity, dynamic range, and\nbandwidth. Here we address this issue, and implement machine learning tools to\nenhance NV magnetic sensing in terms of the sensitivity/bandwidth tradeoff in\nlarge dynamic range scenarios. We experimentally demonstrate this new approach,\nreaching an improvement in the relevant figure of merit by a factor of up to 5.\nOur results promote quantum machine learning protocols for sensing applications\ntowards more feasible and efficient quantum technologies.",
      "tldr_zh": "本研究针对氮-空位 (NV) color centers 在金刚石中的量子磁场传感问题，使用 machine learning 工具来优化敏感性与带宽之间的权衡，尤其在大动态范围场景下。研究方法通过 machine learning 协议增强 NV 磁场传感的性能，实验结果显示相关指标的提升高达 5 倍。总体而言，此工作推动了量子 machine learning 在传感应用中的可行性和效率，促进了量子技术的进步。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG",
        "physics.app-ph",
        "physics.comp-ph",
        "68T07 (Primary) 68T10, 81-08, 81-05, 81-10, 81-11, 81V10 (Secondary)",
        "I.2.6; I.5.4; J.2; I.6.3"
      ],
      "primary_category": "quant-ph",
      "comment": "12 pages including supplementary, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12820v1",
      "published_date": "2024-09-19 14:50:12 UTC",
      "updated_date": "2024-09-19 14:50:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:29:43.464572"
    },
    {
      "arxiv_id": "2409.12996v1",
      "title": "pyrtklib: An open-source package for tightly coupled deep learning and GNSS integration for positioning in urban canyons",
      "title_zh": "翻译失败",
      "authors": [
        "Runzhi Hu",
        "Penghui Xu",
        "Yihan Zhong",
        "Weisong Wen"
      ],
      "abstract": "Artificial intelligence (AI) is revolutionizing numerous fields, with\nincreasing applications in Global Navigation Satellite Systems (GNSS)\npositioning algorithms in intelligent transportation systems (ITS) via deep\nlearning. However, a significant technological disparity exists as traditional\nGNSS algorithms are often developed in Fortran or C, contrasting with the\nPython-based implementation prevalent in deep learning tools. To address this\ndiscrepancy, this paper introduces pyrtklib, a Python binding for the widely\nutilized open-source GNSS tool, RTKLIB. This binding makes all RTKLIB\nfunctionalities accessible in Python, facilitating seamless integration.\nMoreover, we present a deep learning subsystem under pyrtklib, which is a novel\ndeep learning framework that leverages pyrtklib to accurately predict weights\nand biases within the GNSS positioning process. The use of pyrtklib enables\ndevelopers to easily and quickly prototype and implement deep learning-aided\nGNSS algorithms, showcasing its potential to enhance positioning accuracy\nsignificantly.",
      "tldr_zh": "这篇论文介绍了 pyrtklib，一个开源 Python 包，旨在解决传统 GNSS 算法（如基于 Fortran 或 C 的 RTKLIB）和深度学习工具的技术差异问题，从而实现深度学习与 GNSS 的紧密集成。该包通过 Python binding 使 RTKLIB 的所有功能在 Python 环境中无缝可用，并包含一个新型深度学习子系统，用于预测 GNSS 定位过程中的权重和偏差。pyrtklib 简化了深度学习辅助 GNSS 算法的原型设计和实现，尤其在城市峡谷等复杂环境中，显著提升了定位准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12996v1",
      "published_date": "2024-09-19 14:47:47 UTC",
      "updated_date": "2024-09-19 14:47:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:29:56.496344"
    },
    {
      "arxiv_id": "2409.12815v1",
      "title": "Graph Convolutional Neural Networks as Surrogate Models for Climate Simulation",
      "title_zh": "图卷积神经网络作为气候模拟的代理模型",
      "authors": [
        "Kevin Potter",
        "Carianne Martinez",
        "Reina Pradhan",
        "Samantha Brozak",
        "Steven Sleder",
        "Lauren Wheeler"
      ],
      "abstract": "Many climate processes are characterized using large systems of nonlinear\ndifferential equations; this, along with the immense amount of data required to\nparameterize complex interactions, means that Earth-System Model (ESM)\nsimulations may take weeks to run on large clusters. Uncertainty quantification\nmay require thousands of runs, making ESM simulations impractical for\npreliminary assessment. Alternatives may include simplifying the processes in\nthe model, but recent efforts have focused on using machine learning to\ncomplement these models or even act as full surrogates. \\textit{We leverage\nmachine learning, specifically fully-connected neural networks (FCNNs) and\ngraph convolutional neural networks (GCNNs), to enable rapid simulation and\nuncertainty quantification in order to inform more extensive ESM simulations.}\nOur surrogate simulated 80 years in approximately 310 seconds on a single A100\nGPU, compared to weeks for the ESM model while having mean temperature errors\nbelow $0.1^{\\circ}C$ and maximum errors below $2^{\\circ}C$.",
      "tldr_zh": "这篇论文提出使用图卷积神经网络 (GCNNs) 和全连接神经网络 (FCNNs) 作为 Earth-System Model (ESM) 的代理模型，以加速气候模拟和不确定性量化，解决传统ESM运行时间长的问题。研究方法通过机器学习技术快速模拟气候过程，并在单个A100 GPU上实现80年的模拟仅需约310秒，而ESM模型需数周。结果显示，代理模型的平均温度错误低于0.1°C，最大错误低于2°C，为初步评估和更广泛的ESM模拟提供了高效、可行的替代方案。",
      "categories": [
        "physics.ao-ph",
        "cs.AI"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12815v1",
      "published_date": "2024-09-19 14:41:15 UTC",
      "updated_date": "2024-09-19 14:41:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:30:08.995789"
    },
    {
      "arxiv_id": "2409.12812v2",
      "title": "Towards Interactive and Learnable Cooperative Driving Automation: a Large Language Model-Driven Decision-Making Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Shiyu Fang",
        "Jiaqi Liu",
        "Mingyu Ding",
        "Yiming Cui",
        "Chen Lv",
        "Peng Hang",
        "Jian Sun"
      ],
      "abstract": "At present, Connected Autonomous Vehicles (CAVs) have begun to open road\ntesting around the world, but their safety and efficiency performance in\ncomplex scenarios is still not satisfactory. Cooperative driving leverages the\nconnectivity ability of CAVs to achieve synergies greater than the sum of their\nparts, making it a promising approach to improving CAV performance in complex\nscenarios. However, the lack of interaction and continuous learning ability\nlimits current cooperative driving to single-scenario applications and specific\nCooperative Driving Automation (CDA). To address these challenges, this paper\nproposes CoDrivingLLM, an interactive and learnable LLM-driven cooperative\ndriving framework, to achieve all-scenario and all-CDA. First, since Large\nLanguage Models(LLMs) are not adept at handling mathematical calculations, an\nenvironment module is introduced to update vehicle positions based on semantic\ndecisions, thus avoiding potential errors from direct LLM control of vehicle\npositions. Second, based on the four levels of CDA defined by the SAE J3216\nstandard, we propose a Chain-of-Thought (COT) based reasoning module that\nincludes state perception, intent sharing, negotiation, and decision-making,\nenhancing the stability of LLMs in multi-step reasoning tasks. Centralized\nconflict resolution is then managed through a conflict coordinator in the\nreasoning process. Finally, by introducing a memory module and employing\nretrieval-augmented generation, CAVs are endowed with the ability to learn from\ntheir past experiences. We validate the proposed CoDrivingLLM through ablation\nexperiments on the negotiation module, reasoning with different shots\nexperience, and comparison with other cooperative driving methods.",
      "tldr_zh": "本文提出CoDrivingLLM框架，这是一个基于Large Language Models (LLMs)的交互式和可学习合作驾驶决策系统，旨在解决Connected Autonomous Vehicles (CAVs)在复杂场景下的安全性和效率问题。框架通过引入环境模块处理车辆位置更新、Chain-of-Thought (COT)推理模块实现状态感知、意图共享、协商和决策，以及记忆模块和检索增强生成技术，使CAVs具备全场景适应性和持续学习能力。实验结果显示，该框架在消融实验和与其他合作驾驶方法的比较中，显著提升了性能稳定性，为全CDA (Cooperative Driving Automation)应用奠定了基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12812v2",
      "published_date": "2024-09-19 14:36:00 UTC",
      "updated_date": "2024-09-22 09:31:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:30:21.307766"
    },
    {
      "arxiv_id": "2409.12809v2",
      "title": "Don't be Fooled: The Misinformation Effect of Explanations in Human-AI Collaboration",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Spitzer",
        "Joshua Holstein",
        "Katelyn Morrison",
        "Kenneth Holstein",
        "Gerhard Satzger",
        "Niklas Kühl"
      ],
      "abstract": "Across various applications, humans increasingly use black-box artificial\nintelligence (AI) systems without insight into these systems' reasoning. To\ncounter this opacity, explainable AI (XAI) methods promise enhanced\ntransparency and interpretability. While recent studies have explored how XAI\naffects human-AI collaboration, few have examined the potential pitfalls caused\nby incorrect explanations. The implications for humans can be far-reaching but\nhave not been explored extensively. To investigate this, we ran a study (n=160)\non AI-assisted decision-making in which humans were supported by XAI. Our\nfindings reveal a misinformation effect when incorrect explanations accompany\ncorrect AI advice with implications post-collaboration. This effect causes\nhumans to infer flawed reasoning strategies, hindering task execution and\ndemonstrating impaired procedural knowledge. Additionally, incorrect\nexplanations compromise human-AI team-performance during collaboration. With\nour work, we contribute to HCI by providing empirical evidence for the negative\nconsequences of incorrect explanations on humans post-collaboration and\noutlining guidelines for designers of AI.",
      "tldr_zh": "本文研究了在人类-AI 协作中，不正确解释对决策的影响，特别是 XAI（可解释 AI）方法可能带来的误导效果。研究通过一项涉及 160 名参与者的实验，发现当正确 AI 建议伴随错误解释时，会导致人类推断错误的推理策略，进而损害任务执行、程序知识和整体团队表现。该工作为 HCI（人机交互）领域提供了实证证据，并为 AI 设计师提出了指导原则，以避免此类负面后果。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12809v2",
      "published_date": "2024-09-19 14:34:20 UTC",
      "updated_date": "2025-01-08 13:59:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:30:32.959651"
    },
    {
      "arxiv_id": "2410.02808v1",
      "title": "KLDD: Kalman Filter based Linear Deformable Diffusion Model in Retinal Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Zhao",
        "Yinzheng Zhao",
        "Junjie Yang",
        "Kai Huang",
        "Nassir Navab",
        "M. Ali Nasseri"
      ],
      "abstract": "AI-based vascular segmentation is becoming increasingly common in enhancing\nthe screening and treatment of ophthalmic diseases. Deep learning structures\nbased on U-Net have achieved relatively good performance in vascular\nsegmentation. However, small blood vessels and capillaries tend to be lost\nduring segmentation when passed through the traditional U-Net downsampling\nmodule. To address this gap, this paper proposes a novel Kalman filter based\nLinear Deformable Diffusion (KLDD) model for retinal vessel segmentation. Our\nmodel employs a diffusion process that iteratively refines the segmentation,\nleveraging the flexible receptive fields of deformable convolutions in feature\nextraction modules to adapt to the detailed tubular vascular structures. More\nspecifically, we first employ a feature extractor with linear deformable\nconvolution to capture vascular structure information form the input images. To\nbetter optimize the coordinate positions of deformable convolution, we employ\nthe Kalman filter to enhance the perception of vascular structures in linear\ndeformable convolution. Subsequently, the features of the vascular structures\nextracted are utilized as a conditioning element within a diffusion model by\nthe Cross-Attention Aggregation module (CAAM) and the Channel-wise Soft\nAttention module (CSAM). These aggregations are designed to enhance the\ndiffusion model's capability to generate vascular structures. Experiments are\nevaluated on retinal fundus image datasets (DRIVE, CHASE_DB1) as well as the\n3mm and 6mm of the OCTA-500 dataset, and the results show that the diffusion\nmodel proposed in this paper outperforms other methods.",
      "tldr_zh": "本研究针对传统U-Net在视网膜血管分割中容易丢失小血管和毛细血管的问题，提出了一种新型Kalman filter based Linear Deformable Diffusion (KLDD)模型，用于提升AI在眼科疾病筛查中的性能。该模型通过线性可变形卷积的特征提取器捕获血管结构信息，并利用Kalman filter优化可变形卷积的坐标位置，以更好地感知细微血管细节；随后，将提取特征作为条件输入扩散模型，通过Cross-Attention Aggregation module (CAAM)和Channel-wise Soft Attention module (CSAM)增强生成效果。在DRIVE、CHASE_DB1和OCTA-500数据集上的实验显示，KLDD模型在分割准确性上优于其他方法，证明了其有效性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted at BIBM 2024",
      "pdf_url": "http://arxiv.org/pdf/2410.02808v1",
      "published_date": "2024-09-19 14:21:38 UTC",
      "updated_date": "2024-09-19 14:21:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:30:44.868906"
    },
    {
      "arxiv_id": "2409.12801v1",
      "title": "Exploring the Lands Between: A Method for Finding Differences between AI-Decisions and Human Ratings through Generated Samples",
      "title_zh": "翻译失败",
      "authors": [
        "Lukas Mecke",
        "Daniel Buschek",
        "Uwe Gruenefeld",
        "Florian Alt"
      ],
      "abstract": "Many important decisions in our everyday lives, such as authentication via\nbiometric models, are made by Artificial Intelligence (AI) systems. These can\nbe in poor alignment with human expectations, and testing them on clear-cut\nexisting data may not be enough to uncover those cases. We propose a method to\nfind samples in the latent space of a generative model, designed to be\nchallenging for a decision-making model with regard to matching human\nexpectations. By presenting those samples to both the decision-making model and\nhuman raters, we can identify areas where its decisions align with human\nintuition and where they contradict it. We apply this method to a face\nrecognition model and collect a dataset of 11,200 human ratings from 100\nparticipants. We discuss findings from our dataset and how our approach can be\nused to explore the performance of AI models in different contexts and for\ndifferent user groups.",
      "tldr_zh": "这篇论文提出了一种方法，通过在生成模型的潜在空间中生成挑战性样本，来识别AI-Decisions与Human Ratings之间的差异，从而揭示AI系统（如面部识别模型）可能与人类期望不一致的地方。研究者将此方法应用于面部识别模型，并收集了11,200个来自100名参与者的Human Ratings数据集。实验结果显示，该方法有助于发现AI在特定上下文中的性能问题，并为评估AI模型在不同用户群体中的表现提供了一个新框架。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12801v1",
      "published_date": "2024-09-19 14:14:08 UTC",
      "updated_date": "2024-09-19 14:14:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:30:56.642207"
    },
    {
      "arxiv_id": "2409.12798v1",
      "title": "Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL",
      "title_zh": "翻译失败",
      "authors": [
        "Eduardo Pignatelli",
        "Johan Ferret",
        "Tim Rockäschel",
        "Edward Grefenstette",
        "Davide Paglieri",
        "Samuel Coward",
        "Laura Toni"
      ],
      "abstract": "The temporal credit assignment problem is a central challenge in\nReinforcement Learning (RL), concerned with attributing the appropriate\ninfluence to each actions in a trajectory for their ability to achieve a goal.\nHowever, when feedback is delayed and sparse, the learning signal is poor, and\naction evaluation becomes harder. Canonical solutions, such as reward shaping\nand options, require extensive domain knowledge and manual intervention,\nlimiting their scalability and applicability. In this work, we lay the\nfoundations for Credit Assignment with Language Models (CALM), a novel approach\nthat leverages Large Language Models (LLMs) to automate credit assignment via\nreward shaping and options discovery. CALM uses LLMs to decompose a task into\nelementary subgoals and assess the achievement of these subgoals in\nstate-action transitions. Every time an option terminates, a subgoal is\nachieved, and CALM provides an auxiliary reward. This additional reward signal\ncan enhance the learning process when the task reward is sparse and delayed\nwithout the need for human-designed rewards. We provide a preliminary\nevaluation of CALM using a dataset of human-annotated demonstrations from\nMiniHack, suggesting that LLMs can be effective in assigning credit in\nzero-shot settings, without examples or LLM fine-tuning. Our preliminary\nresults indicate that the knowledge of LLMs is a promising prior for credit\nassignment in RL, facilitating the transfer of human knowledge into value\nfunctions.",
      "tldr_zh": "本研究探讨了强化学习（RL）中的时间信用分配问题，即在反馈延迟和稀疏的情况下，如何准确评估轨迹中每个动作对目标的贡献。作者提出了一种新方法 Credit Assignment with Language Models (CALM)，利用大型语言模型（LLMs）来自动化信用分配，通过任务分解成子目标、评估状态-动作转换以及提供辅助奖励，从而增强学习过程，而无需手动设计奖励。初步实验使用 MiniHack 数据集的标注演示评估了 CALM，在零-shot 设置下，LLMs 展示了有效性，作为一种有前景的先验，促进人类知识向 RL 值函数的转移。整体结果表明，这种方法能显著改善稀疏奖励环境的性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.12798v1",
      "published_date": "2024-09-19 14:08:09 UTC",
      "updated_date": "2024-09-19 14:08:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:31:08.559553"
    },
    {
      "arxiv_id": "2409.12797v1",
      "title": "Efficient Identification of Direct Causal Parents via Invariance and Minimum Error Testing",
      "title_zh": "翻译失败",
      "authors": [
        "Minh Nguyen",
        "Mert R. Sabuncu"
      ],
      "abstract": "Invariant causal prediction (ICP) is a popular technique for finding causal\nparents (direct causes) of a target via exploiting distribution shifts and\ninvariance testing (Peters et al., 2016). However, since ICP needs to run an\nexponential number of tests and fails to identify parents when distribution\nshifts only affect a few variables, applying ICP to practical large scale\nproblems is challenging. We propose MMSE-ICP and fastICP, two approaches which\nemploy an error inequality to address the identifiability problem of ICP. The\ninequality states that the minimum prediction error of the predictor using\ncausal parents is the smallest among all predictors which do not use\ndescendants. fastICP is an efficient approximation tailored for large problems\nas it exploits the inequality and a heuristic to run fewer tests. MMSE-ICP and\nfastICP not only outperform competitive baselines in many simulations but also\nachieve state-of-the-art result on a large scale real data benchmark.",
      "tldr_zh": "该研究针对Invariant Causal Prediction (ICP)方法在识别目标变量的直接因果父变量时存在的效率问题（如需运行指数级测试和对少数变量分布偏移的敏感性）提出两种新方法：MMSE-ICP 和 fastICP。MMSE-ICP 利用一个错误不等式（error inequality），该不等式表明使用因果父变量的预测器能实现最小预测错误，从而提升父变量的可识别性；fastICP 则通过该不等式和启发式优化减少测试数量，适用于大规模问题。实验结果显示，这两种方法在多种模拟场景中优于竞争基线，并在大型真实数据基准上实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at TMLR",
      "pdf_url": "http://arxiv.org/pdf/2409.12797v1",
      "published_date": "2024-09-19 14:07:31 UTC",
      "updated_date": "2024-09-19 14:07:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:31:19.996727"
    },
    {
      "arxiv_id": "2409.12995v1",
      "title": "Improving generalisability of 3D binding affinity models in low data regimes",
      "title_zh": "在低数据条件下改善3D结合亲和力模型的泛化能力",
      "authors": [
        "Julia Buhmann",
        "Ward Haddadin",
        "Lukáš Pravda",
        "Alan Bilsland",
        "Hagen Triendl"
      ],
      "abstract": "Predicting protein-ligand binding affinity is an essential part of\ncomputer-aided drug design. However, generalisable and performant global\nbinding affinity models remain elusive, particularly in low data regimes.\nDespite the evolution of model architectures, current benchmarks are not\nwell-suited to probe the generalisability of 3D binding affinity models.\nFurthermore, 3D global architectures such as GNNs have not lived up to\nperformance expectations. To investigate these issues, we introduce a novel\nsplit of the PDBBind dataset, minimizing similarity leakage between train and\ntest sets and allowing for a fair and direct comparison between various model\narchitectures. On this low similarity split, we demonstrate that, in general,\n3D global models are superior to protein-specific local models in low data\nregimes. We also demonstrate that the performance of GNNs benefits from three\nnovel contributions: supervised pre-training via quantum mechanical data,\nunsupervised pre-training via small molecule diffusion, and explicitly modeling\nhydrogen atoms in the input graph. We believe that this work introduces\npromising new approaches to unlock the potential of GNN architectures for\nbinding affinity modelling.",
      "tldr_zh": "本研究针对低数据环境下3D结合亲和力模型的泛化性问题，引入了PDBBind数据集的新分割方式，以最小化训练集和测试集之间的相似性泄漏，从而实现对不同模型架构的公平比较。结果显示，在这种低相似性分割中，3D全局模型（如GNNs）在低数据环境中优于蛋白质特定的局部模型。论文进一步提出三项创新：通过量子力学数据进行监督预训练、通过小分子扩散进行无监督预训练，以及在输入图中显式建模氢原子，这些方法显著提升了GNNs的性能。该工作为释放GNNs在结合亲和力建模中的潜力提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 10 figues",
      "pdf_url": "http://arxiv.org/pdf/2409.12995v1",
      "published_date": "2024-09-19 13:54:38 UTC",
      "updated_date": "2024-09-19 13:54:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:31:32.942210"
    },
    {
      "arxiv_id": "2409.12785v2",
      "title": "Investigation on domain adaptation of additive manufacturing monitoring systems to enhance digital twin reusability",
      "title_zh": "翻译失败",
      "authors": [
        "Jiarui Xie",
        "Zhuo Yang",
        "Chun-Chun Hu",
        "Haw-Ching Yang",
        "Yan Lu",
        "Yaoyao Fiona Zhao"
      ],
      "abstract": "Powder bed fusion (PBF) is an emerging metal additive manufacturing (AM)\ntechnology that enables rapid fabrication of complex geometries. However,\ndefects such as pores and balling may occur and lead to structural\nunconformities, thus compromising the mechanical performance of the part. This\nhas become a critical challenge for quality assurance as the nature of some\ndefects is stochastic during the process and invisible from the exterior. To\naddress this issue, digital twin (DT) using machine learning (ML)-based\nmodeling can be deployed for AM process monitoring and control. Melt pool is\none of the most commonly observed physical phenomena for process monitoring,\nusually by high-speed cameras. Once labeled and preprocessed, the melt pool\nimages are used to train ML-based models for DT applications such as process\nanomaly detection and print quality evaluation. Nonetheless, the reusability of\nDTs is restricted due to the wide variability of AM settings, including AM\nmachines and monitoring instruments. The performance of the ML models trained\nusing the dataset collected from one setting is usually compromised when\napplied to other settings. This paper proposes a knowledge transfer pipeline\nbetween different AM settings to enhance the reusability of AM DTs. The source\nand target datasets are collected from the National Institute of Standards and\nTechnology and National Cheng Kung University with different cameras,\nmaterials, AM machines, and process parameters. The proposed pipeline consists\nof four steps: data preprocessing, data augmentation, domain alignment, and\ndecision alignment. Compared with the model trained only using the source\ndataset, this pipeline increased the melt pool anomaly detection accuracy by\n31% without any labeled training data from the target dataset.",
      "tldr_zh": "这篇论文探讨了增材制造 (AM) 监控系统的域适应问题，以提升数字孪生 (DT) 的可重用性，特别是针对粉末床熔融 (PBF) 过程中可能出现的缺陷如孔洞和球化。作者提出一个知识转移管道，包括数据预处理、数据增强、域对齐和决策对齐四个步骤，用于在不同AM设置（如机器、材料和参数）之间转移模型知识。实验结果显示，该管道在不使用目标数据集的标记数据情况下，将熔池异常检测准确率提高了31%，从而显著提高了DT的适用性和效率。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CE",
      "comment": "8 pages, 7 figures, 3 tables. IEEE CASE 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12785v2",
      "published_date": "2024-09-19 13:54:01 UTC",
      "updated_date": "2024-09-20 04:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:31:45.633678"
    },
    {
      "arxiv_id": "2409.12784v7",
      "title": "Evaluating Image Hallucination in Text-to-Image Generation with Question-Answering",
      "title_zh": "使用问答评估文本到图像生成中的图像幻觉",
      "authors": [
        "Youngsun Lim",
        "Hojun Choi",
        "Hyunjung Shim"
      ],
      "abstract": "Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing TTI models can correctly\nrespond to these questions. The I-HallA v1.0 dataset comprises 1.2K diverse\nimage-text pairs across nine categories with 1,000 rigorously curated questions\ncovering various compositional challenges. We evaluate five TTI models using\nI-HallA and reveal that these state-of-the-art models often fail to accurately\nconvey factual information. Moreover, we validate the reliability of our metric\nby demonstrating a strong Spearman correlation ($\\rho$=0.95) with human\njudgments. We believe our benchmark dataset and metric can serve as a\nfoundation for developing factually accurate TTI generation models. Additional\nresources can be found on our project page: https://sgt-lim.github.io/I-HallA/.",
      "tldr_zh": "本文评估了文本到图像（TTI）生成模型中的图像幻觉问题，即生成的图像未能忠实描绘事实内容，并引入了 I-HallA（Image Hallucination evaluation with Question Answering）指标，通过视觉问答（VQA）来自动衡量图像的事实性。研究构建了 I-HallA v1.0 数据集，包含 1.2K 图像-文本对和 1,000 个问题，使用 GPT-4 Omni 基于的代理生成高质量问题-答案对，并通过人工判断确保准确性。在评估五个 TTI 模型后，发现这些模型经常无法准确传达事实信息，且 I-HallA 与人类判断的 Spearman 相关性高达 0.95。该指标和数据集可作为开发事实准确 TTI 模型的基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.12784v7",
      "published_date": "2024-09-19 13:51:21 UTC",
      "updated_date": "2025-02-10 13:10:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:31:58.511650"
    },
    {
      "arxiv_id": "2409.12774v3",
      "title": "GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyue Zhang",
        "Zhiliu Yang",
        "Xinhe Zuo",
        "Yuxin Tong",
        "Ying Long",
        "Chen Liu"
      ],
      "abstract": "This paper proposes a novel framework for large-scale scene reconstruction\nbased on 3D Gaussian splatting (3DGS) and aims to address the scalability and\naccuracy challenges faced by existing methods. For tackling the scalability\nissue, we split the large scene into multiple cells, and the candidate\npoint-cloud and camera views of each cell are correlated through a\nvisibility-based camera selection and a progressive point-cloud extension. To\nreinforce the rendering quality, three highlighted improvements are made in\ncomparison with vanilla 3DGS, which are a strategy of the ray-Gaussian\nintersection and the novel Gaussians density control for learning efficiency,\nan appearance decoupling module based on ConvKAN network to solve uneven\nlighting conditions in large-scale scenes, and a refined final loss with the\ncolor loss, the depth distortion loss, and the normal consistency loss.\nFinally, the seamless stitching procedure is executed to merge the individual\nGaussian radiance field for novel view synthesis across different cells.\nEvaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method\nconsistently generates more high-fidelity rendering results than\nstate-of-the-art methods of large-scale scene reconstruction. We further\nvalidate the generalizability of the proposed approach by rendering on\nself-collected video clips recorded by a commercial drone.",
      "tldr_zh": "本论文提出GaRField++框架，基于3D Gaussian splatting (3DGS)，旨在解决大规模3D场景重建中的扩展性和准确性挑战。通过将场景分割成多个单元，并采用基于可见性的相机选择和渐进式点云扩展来关联每个单元的点云和视图，从而提升整体可扩展性。框架进一步改进渲染质量，包括光线-高斯交集策略、高斯密度控制、基于ConvKAN的外观解耦模块，以及结合颜色损失、深度失真损失和法线一致性损失的精炼损失函数，最终通过无缝拼接实现跨单元的Novel View Synthesis。实验在Mill19、Urban3D和MatrixCity数据集上显示，该方法比现有最先进技术生成更高保真度的渲染结果，并通过无人机视频验证了其泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12774v3",
      "published_date": "2024-09-19 13:43:31 UTC",
      "updated_date": "2024-09-24 15:03:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:32:09.607828"
    },
    {
      "arxiv_id": "2409.12769v1",
      "title": "The Robustness of Spiking Neural Networks in Communication and its Application towards Network Efficiency in Federated Learning",
      "title_zh": "脉冲神经网络在通信中的鲁棒性及其在联邦学习中网络效率的应用",
      "authors": [
        "Manh V. Nguyen",
        "Liang Zhao",
        "Bobin Deng",
        "William Severa",
        "Honghui Xu",
        "Shaoen Wu"
      ],
      "abstract": "Spiking Neural Networks (SNNs) have recently gained significant interest in\non-chip learning in embedded devices and emerged as an energy-efficient\nalternative to conventional Artificial Neural Networks (ANNs). However, to\nextend SNNs to a Federated Learning (FL) setting involving collaborative model\ntraining, the communication between the local devices and the remote server\nremains the bottleneck, which is often restricted and costly. In this paper, we\nfirst explore the inherent robustness of SNNs under noisy communication in FL.\nBuilding upon this foundation, we propose a novel Federated Learning with Top-K\nSparsification (FLTS) algorithm to reduce the bandwidth usage for FL training.\nWe discover that the proposed scheme with SNNs allows more bandwidth savings\ncompared to ANNs without impacting the model's accuracy. Additionally, the\nnumber of parameters to be communicated can be reduced to as low as 6 percent\nof the size of the original model. We further improve the communication\nefficiency by enabling dynamic parameter compression during model training.\nExtensive experiment results demonstrate that our proposed algorithms\nsignificantly outperform the baselines in terms of communication cost and model\naccuracy and are promising for practical network-efficient FL with SNNs.",
      "tldr_zh": "本研究探讨了Spiking Neural Networks (SNNs)在Federated Learning (FL)中的通信鲁棒性，并将其应用于提升网络效率。论文首先分析了SNNs在噪声通信环境下的固有优势，作为ANNs的能效替代方案。作者提出Federated Learning with Top-K Sparsification (FLTS)算法，通过Top-K稀疏化和动态参数压缩，显著减少了FL训练中的带宽使用，将通信参数降低至原模型的6%。实验结果显示，该方法相较于基线，在通信成本上实现了更大节省，同时保持了模型准确性，为网络高效的FL应用提供了可行方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper has been accepted for publication at the 43rd IEEE\n  International Performance Computing and Communications Conference (IPCCC\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.12769v1",
      "published_date": "2024-09-19 13:37:18 UTC",
      "updated_date": "2024-09-19 13:37:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:32:20.884667"
    },
    {
      "arxiv_id": "2410.02807v1",
      "title": "AutoPETIII: The Tracer Frontier. What Frontier?",
      "title_zh": "翻译失败",
      "authors": [
        "Zacharia Mesbah",
        "Léo Mottay",
        "Romain Modzelewski",
        "Pierre Decazes",
        "Sébastien Hapdey",
        "Su Ruan",
        "Sébastien Thureau"
      ],
      "abstract": "For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.",
      "tldr_zh": "本文提出AutoPETIII方法，针对AutoPET比赛2024年的挑战，即在未知示踪剂（FDG或PSMA-based tracer）情况下进行PET/CT扫描的病变分割。研究团队使用nnUNetv2框架训练两个6折集成模型集，实现全自动病变分割，并引入MIP-CNN来选择最合适的模型集。总体上，该方法提升了算法的鲁棒性和适用性，为多示踪剂PET图像分析提供了有效解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02807v1",
      "published_date": "2024-09-19 13:26:31 UTC",
      "updated_date": "2024-09-19 13:26:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:32:32.740843"
    },
    {
      "arxiv_id": "2409.12745v1",
      "title": "Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastião Quintas",
        "Isabelle Ferrané",
        "Thomas Pellegrini"
      ],
      "abstract": "The use of synthetic speech as data augmentation is gaining increasing\npopularity in fields such as automatic speech recognition and speech\nclassification tasks. Despite novel text-to-speech systems with voice cloning\ncapabilities, that allow the usage of a larger amount of voices based on short\naudio segments, it is known that these systems tend to hallucinate and\noftentimes produce bad data that will most likely have a negative impact on the\ndownstream task. In the present work, we conduct a set of experiments around\nzero-shot learning with synthetic speech data for the specific task of speech\ncommands classification. Our results on the Google Speech Commands dataset show\nthat a simple ASR-based filtering method can have a big impact in the quality\nof the generated data, translating to a better performance. Furthermore,\ndespite the good quality of the generated speech data, we also show that\nsynthetic and real speech can still be easily distinguishable when using\nself-supervised (WavLM) features, an aspect further explored with a CycleGAN to\nbridge the gap between the two types of speech material.",
      "tldr_zh": "该研究探讨了使用合成语音数据增强语音命令分类任务的策略，针对合成语音系统可能产生的错误数据（hallucinate）问题。研究者提出了一种基于 ASR（Automatic Speech Recognition）的简单过滤方法，在 Google Speech Commands 数据集上显著提高了模型性能。进一步，通过自监督学习（SSL）特征（如 WavLM）分析发现，合成语音与真实语音易于区分，并使用 CycleGAN 进行域适应，以桥接两者间的差距。整体结果表明，这种方法能有效提升合成数据的质量，支持零样本学习（zero-shot learning）的应用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12745v1",
      "published_date": "2024-09-19 13:07:55 UTC",
      "updated_date": "2024-09-19 13:07:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:32:44.645855"
    },
    {
      "arxiv_id": "2409.12741v3",
      "title": "Fine Tuning Large Language Models for Medicine: The Role and Importance of Direct Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Savage",
        "Stephen Ma",
        "Abdessalem Boukil",
        "Vishwesh Patel",
        "Ekanath Rangan",
        "Ivan Lopez",
        "Jonathan H Chen"
      ],
      "abstract": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.",
      "tldr_zh": "这篇论文探讨了在医学领域微调Large Language Models (LLM)的技术，重点比较了Supervised Fine Tuning (SFT)和Direct Preference Optimization (DPO)两种方法。研究通过实验评估了五种常见医学任务，包括Classification with text data、Classification with numeric data、Clinical Reasoning、Summarization和Clinical Triage，发现SFT在文本分类任务中已足够有效，而DPO显著提升了Clinical Reasoning、Summarization和Clinical Triage等复杂任务的性能。论文强调了DPO在医学中的关键作用，并呼吁解决当前软件缺口以促进其广泛部署。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12741v3",
      "published_date": "2024-09-19 13:03:24 UTC",
      "updated_date": "2024-12-13 15:37:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:32:56.374434"
    },
    {
      "arxiv_id": "2409.12740v1",
      "title": "HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Junyi Chen",
        "Lu Chi",
        "Bingyue Peng",
        "Zehuan Yuan"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, prompting several studies to explore their potential in recommendation\nsystems. However, these attempts have so far resulted in only modest\nimprovements over traditional recommendation models. Moreover, three critical\nquestions remain under-explored: firstly, the real value of LLMs' pre-trained\nweights, often considered to encapsulate world knowledge; secondly, the\nnecessity of fine-tuning for recommendation tasks; lastly, whether LLMs can\nexhibit the same scalability benefits in recommendation systems as they do in\nother domains. In this paper, we propose a novel Hierarchical Large Language\nModel (HLLM) architecture designed to enhance sequential recommendation\nsystems. Our approach employs a two-tier model: the first Item LLM extracts\nrich content features from the detailed text description of the item, while the\nsecond User LLM utilizes these features to predict users' future interests\nbased on their interaction history. Extensive experiments demonstrate that our\nmethod effectively leverages the pre-trained capabilities of open-source LLMs,\nand further fine-tuning leads to significant performance boosts. Additionally,\nHLLM achieves excellent scalability, with the largest configuration utilizing\n7B parameters for both item feature extraction and user interest modeling.\nMoreover, HLLM offers excellent training and serving efficiency, making it\npractical in real-world applications. Evaluations on two large-scale datasets,\nPixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results,\noutperforming traditional ID-based models by a wide margin. In online A/B\ntesting, HLLM showcases notable gains, validating its practical impact in\nreal-world recommendation scenarios. Codes are available at\nhttps://github.com/bytedance/HLLM.",
      "tldr_zh": "该研究提出HLLM，一种分层大型语言模型（LLMs）架构，用于提升序列推荐系统，通过Item LLM从物品的详细文本描述中提取丰富特征，以及User LLM基于这些特征预测用户的互动历史和未来兴趣。HLLM有效利用LLMs的预训练权重，并证明微调对于推荐任务的必要性，同时展示了在推荐系统中的优秀可扩展性（如使用7B参数的配置）。实验在PixelRec和Amazon Reviews等大规模数据集上实现了最先进的结果，大幅优于传统基于ID的模型，并在在线A/B测试中显示显著性能提升。该方法还提供了高效的训练和部署效率，代码已开源。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12740v1",
      "published_date": "2024-09-19 13:03:07 UTC",
      "updated_date": "2024-09-19 13:03:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:33:08.342231"
    },
    {
      "arxiv_id": "2409.12737v1",
      "title": "MEXMA: Token-level objectives improve sentence representations",
      "title_zh": "翻译失败",
      "authors": [
        "João Maria Janeiro",
        "Benjamin Piwowarski",
        "Patrick Gallinari",
        "Loïc Barrault"
      ],
      "abstract": "Current pre-trained cross-lingual sentence encoders approaches use\nsentence-level objectives only. This can lead to loss of information,\nespecially for tokens, which then degrades the sentence representation. We\npropose MEXMA, a novel approach that integrates both sentence-level and\ntoken-level objectives. The sentence representation in one language is used to\npredict masked tokens in another language, with both the sentence\nrepresentation and all tokens directly updating the encoder. We show that\nadding token-level objectives greatly improves the sentence representation\nquality across several tasks. Our approach outperforms current pre-trained\ncross-lingual sentence encoders on bi-text mining as well as several downstream\ntasks. We also analyse the information encoded in our tokens, and how the\nsentence representation is built from them.",
      "tldr_zh": "当前预训练跨语言句子编码器仅使用 sentence-level objectives，这可能导致 token-level 信息丢失并降低句子表示质量。研究提出 MEXMA 方法，通过整合 sentence-level 和 token-level objectives，使用一种语言的句子表示来预测另一种语言的 masked tokens，并直接更新编码器。实验结果显示，MEXMA 在 bi-text mining 以及多个下游任务上优于现有模型，大大提高了句子表示质量。此外，研究还分析了 tokens 中编码的信息以及句子表示如何从这些 tokens 构建。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12737v1",
      "published_date": "2024-09-19 13:00:29 UTC",
      "updated_date": "2024-09-19 13:00:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:33:20.419763"
    },
    {
      "arxiv_id": "2409.12730v3",
      "title": "When SparseMoE Meets Noisy Interactions: An Ensemble View on Denoising Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Weipu Chen",
        "Zhuangzhuang He",
        "Fei Liu"
      ],
      "abstract": "Learning user preferences from implicit feedback is one of the core\nchallenges in recommendation. The difficulty lies in the potential noise within\nimplicit feedback. Therefore, various denoising recommendation methods have\nbeen proposed recently. However, most of them overly rely on the hyperparameter\nconfigurations, inevitably leading to inadequacies in model adaptability and\ngeneralization performance. In this study, we propose a novel Adaptive Ensemble\nLearning (AEL) for denoising recommendation, which employs a sparse gating\nnetwork as a brain, selecting suitable experts to synthesize appropriate\ndenoising capacities for different data samples. To address the ensemble\nlearning shortcoming of model complexity and ensure sub-recommender diversity,\nwe also proposed a novel method that stacks components to create\nsub-recommenders instead of directly constructing them. Extensive experiments\nacross various datasets demonstrate that AEL outperforms others in kinds of\npopular metrics, even in the presence of substantial and dynamic noise. Our\ncode is available at https://github.com/cpu9xx/AEL.",
      "tldr_zh": "本研究针对推荐系统中隐式反馈中的噪声问题，提出了一种新型Adaptive Ensemble Learning (AEL)方法，使用SparseMoE（稀疏混合专家模型）作为门控网络，为不同数据样本动态选择和合成合适的去噪专家。AEL通过堆叠组件来构建子推荐器，确保模型多样性和降低复杂性，从而提升模型的适应性和泛化性能。在多个数据集上的广泛实验表明，AEL在各种流行指标上优于现有方法，即使面对大量动态噪声。代码已开源，可进一步验证其有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at ICASSP 2025. 5pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12730v3",
      "published_date": "2024-09-19 12:55:34 UTC",
      "updated_date": "2024-12-26 09:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:33:32.476886"
    },
    {
      "arxiv_id": "2409.12726v1",
      "title": "Cloudy with a Chance of Anomalies: Dynamic Graph Neural Network for Early Detection of Cloud Services' User Anomalies",
      "title_zh": "翻译失败",
      "authors": [
        "Revital Marbel",
        "Yanir Cohen",
        "Ran Dubin",
        "Amit Dvir",
        "Chen Hajaj"
      ],
      "abstract": "Ensuring the security of cloud environments is imperative for sustaining\norganizational growth and operational efficiency. As the ubiquity of cloud\nservices continues to rise, the inevitability of cyber threats underscores the\nimportance of preemptive detection. This paper introduces a pioneering\ntime-based embedding approach for Cloud Services Graph-based Anomaly Detection\n(CS-GAD), utilizing a Graph Neural Network (GNN) to discern anomalous user\nbehavior during interactions with cloud services. Our method employs a dynamic\ntripartite graph representation to encapsulate the evolving interactions among\ncloud services, users, and their activities over time. Leveraging GNN models in\neach time frame, our approach generates a graph embedding wherein each user is\nassigned a score based on their historical activity, facilitating the\nidentification of unusual behavior. Results demonstrate a notable reduction in\nfalse positive rates (2-9%) compared to prevailing methods, coupled with a\ncommendable true positive rate (100%). The contributions of this work encompass\nearly detection capabilities, a low false positive rate, an innovative\ntripartite graph representation incorporating action types, the introduction of\na new cloud services dataset featuring various user attacks, and an open-source\nimplementation for community collaboration in advancing cloud service security.",
      "tldr_zh": "这篇论文提出了 CS-GAD，一种基于 Graph Neural Network (GNN) 的时间嵌入方法，用于早期检测云服务用户异常行为。它采用动态三部图表示来捕捉用户、云服务和活动之间的演变交互，并在每个时间帧生成用户历史活动分数，以识别异常。实验结果显示，与现有方法相比，假阳性率降低了2-9%，真阳性率达到100%。此外，该工作贡献了创新的三部图表示、一个包含各种用户攻击的新数据集，以及开源实现，以推进云服务安全研究。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12726v1",
      "published_date": "2024-09-19 12:50:31 UTC",
      "updated_date": "2024-09-19 12:50:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:33:44.745303"
    },
    {
      "arxiv_id": "2409.12994v2",
      "title": "Performance and Power: Systematic Evaluation of AI Workloads on Accelerators with CARAML",
      "title_zh": "翻译失败",
      "authors": [
        "Chelsea Maria John",
        "Stepan Nassyr",
        "Carolin Penke",
        "Andreas Herten"
      ],
      "abstract": "The rapid advancement of machine learning (ML) technologies has driven the\ndevelopment of specialized hardware accelerators designed to facilitate more\nefficient model training. This paper introduces the CARAML benchmark suite,\nwhich is employed to assess performance and energy consumption during the\ntraining of transformer-based large language models and computer vision models\non a range of hardware accelerators, including systems from NVIDIA, AMD, and\nGraphcore. CARAML provides a compact, automated, extensible, and reproducible\nframework for assessing the performance and energy of ML workloads across\nvarious novel hardware architectures. The design and implementation of CARAML,\nalong with a custom power measurement tool called jpwr, are discussed in\ndetail.",
      "tldr_zh": "这篇论文引入了 CARAML 基准测试套件，用于系统评估 AI 工作负载在硬件加速器上的性能和能量消耗。CARAML 针对 transformer-based large language models 和 computer vision models 的训练过程，在 NVIDIA、AMD 和 Graphcore 等平台上进行测试，提供了一个紧凑、自动化、可扩展且可重现的评估框架。论文还详细讨论了 CARAML 的设计实现以及自定义功率测量工具 jpwr，以促进新型硬件架构的优化和比较。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.AR",
      "comment": "To be published in Workshop Proceedings of The International\n  Conference for High Performance Computing Networking, Storage, and Analysis\n  (SC-W '24) (2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.12994v2",
      "published_date": "2024-09-19 12:43:18 UTC",
      "updated_date": "2024-10-29 09:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:33:56.492987"
    },
    {
      "arxiv_id": "2409.12683v1",
      "title": "Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National Varieties, Creoles and Other Low-resource Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Joshi",
        "Diptesh Kanojia",
        "Heather Lent",
        "Hour Kaing",
        "Haiyue Song"
      ],
      "abstract": "Despite excellent results on benchmarks over a small subset of languages,\nlarge language models struggle to process text from languages situated in\n`lower-resource' scenarios such as dialects/sociolects (national or social\nvarieties of a language), Creoles (languages arising from linguistic contact\nbetween multiple languages) and other low-resource languages. This introductory\ntutorial will identify common challenges, approaches, and themes in natural\nlanguage processing (NLP) research for confronting and overcoming the obstacles\ninherent to data-poor contexts. By connecting past ideas to the present field,\nthis tutorial aims to ignite collaboration and cross-pollination between\nresearchers working in these scenarios. Our notion of `lower-resource' broadly\ndenotes the outstanding lack of data required for model training - and may be\napplied to scenarios apart from the three covered in the tutorial.",
      "tldr_zh": "这篇论文介绍了自然语言处理(NLP) 在 'lower-resource' 场景中的应用，重点探讨大型语言模型在处理国家变体、Creoles 和其他低资源语言时的挑战，如数据不足导致的性能问题。教程识别了这些场景的常见障碍、方法和主题，并通过连接过去研究与当前领域，促进研究者之间的合作与交叉授粉。最终，该框架的 'lower-resource' 概念可扩展到更多数据匮乏的语境中，旨在激发创新和更广泛的应用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Selected as a full-day tutorial at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12683v1",
      "published_date": "2024-09-19 11:48:42 UTC",
      "updated_date": "2024-09-19 11:48:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:34:08.663238"
    },
    {
      "arxiv_id": "2409.12682v1",
      "title": "Retrieval-Augmented Test Generation: How Far Are We?",
      "title_zh": "检索增强测试生成：我们走了多远？",
      "authors": [
        "Jiho Shin",
        "Reem Aleithan",
        "Hadi Hemmati",
        "Song Wang"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has shown notable advancements in\nsoftware engineering tasks. Despite its potential, RAG's application in unit\ntest generation remains under-explored. To bridge this gap, we take the\ninitiative to investigate the efficacy of RAG-based LLMs in test generation. As\nRAGs can leverage various knowledge sources to enhance their performance, we\nalso explore the impact of different sources of RAGs' knowledge bases on unit\ntest generation to provide insights into their practical benefits and\nlimitations. Specifically, we examine RAG built upon three types of domain\nknowledge: 1) API documentation, 2) GitHub issues, and 3) StackOverflow Q&As.\nEach source offers essential knowledge for creating tests from different\nperspectives, i.e., API documentations provide official API usage guidelines,\nGitHub issues offer resolutions of issues related to the APIs from the library\ndevelopers, and StackOverflow Q&As present community-driven solutions and best\npractices. For our experiment, we focus on five widely used and typical\nPython-based machine learning (ML) projects, i.e., TensorFlow, PyTorch,\nScikit-learn, Google JAX, and XGBoost to build, train, and deploy complex\nneural networks efficiently. We conducted experiments using the top 10% most\nwidely used APIs across these projects, involving a total of 188 APIs. We\ninvestigate the effectiveness of four state-of-the-art LLMs (open and\nclosed-sourced), i.e., GPT-3.5-Turbo, GPT-4o, Mistral MoE 8x22B, and Llamma 3.1\n405B. Additionally, we compare three prompting strategies in generating unit\ntest cases for the experimental APIs, i.e., zero-shot, a Basic RAG, and an\nAPI-level RAG on the three external sources. Finally, we compare the cost of\ndifferent sources of knowledge used for the RAG.",
      "tldr_zh": "这篇论文探讨了Retrieval-Augmented Generation (RAG)在单元测试生成中的有效性，评估其在软件工程任务中的潜力。研究者实验了三种知识来源（API documentation、GitHub issues和StackOverflow Q&As）对RAG-based LLMs的影响，针对TensorFlow、PyTorch、Scikit-learn、Google JAX和XGBoost等五个Python机器学习项目的188个API，使用GPT-3.5-Turbo、GPT-4o、Mistral MoE 8x22B和Llama 3.1 405B等模型，并比较了zero-shot、Basic RAG和API-level RAG三种提示策略。结果显示，不同知识来源在测试生成质量和成本上存在差异，为RAG的实际应用提供了宝贵的见解和局限性分析。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "18 pages + reference",
      "pdf_url": "http://arxiv.org/pdf/2409.12682v1",
      "published_date": "2024-09-19 11:48:29 UTC",
      "updated_date": "2024-09-19 11:48:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:34:21.240535"
    },
    {
      "arxiv_id": "2409.12677v1",
      "title": "(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly Fair Decision-Makers",
      "title_zh": "翻译失败",
      "authors": [
        "Manh Khoi Duong",
        "Stefan Conrad"
      ],
      "abstract": "Fairness metrics are used to assess discrimination and bias in\ndecision-making processes across various domains, including machine learning\nmodels and human decision-makers in real-world applications. This involves\ncalculating the disparities between probabilistic outcomes among social groups,\nsuch as acceptance rates between male and female applicants. However,\ntraditional fairness metrics do not account for the uncertainty in these\nprocesses and lack of comparability when two decision-makers exhibit the same\ndisparity. Using Bayesian statistics, we quantify the uncertainty of the\ndisparity to enhance discrimination assessments. We represent each\ndecision-maker, whether a machine learning model or a human, by its disparity\nand the corresponding uncertainty in that disparity. We define preferences over\ndecision-makers and utilize brute-force to choose the optimal decision-maker\naccording to a utility function that ranks decision-makers based on these\npreferences. The decision-maker with the highest utility score can be\ninterpreted as the one for whom we are most certain that it is fair.",
      "tldr_zh": "本研究探讨了公平性评估中的不确定性问题，提出一种基于偏好的方法来选择在公平性上最确定的决策者。传统公平性指标仅计算社会群体间的差异（如接受率），但忽略了不确定性和可比性；为此，作者使用Bayesian statistics量化这些差异的不确定性，并将每个决策者（机器学习模型或人类）表示为其差异和相关不确定性。接着，通过定义决策者偏好和效用函数，并采用brute-force搜索，选出具有最高效用分数的决策者，即在公平性上最可靠的选项。该方法提升了歧视评估的准确性，为更可信的决策过程提供新框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.12677v1",
      "published_date": "2024-09-19 11:44:03 UTC",
      "updated_date": "2024-09-19 11:44:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:34:31.942750"
    },
    {
      "arxiv_id": "2409.12669v1",
      "title": "Enhancing Construction Site Safety: A Lightweight Convolutional Network for Effective Helmet Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Mujadded Al Rabbani Alif"
      ],
      "abstract": "In the realm of construction safety, the detection of personal protective\nequipment, such as helmets, plays a critical role in preventing workplace\ninjuries. This paper details the development and evaluation of convolutional\nneural networks (CNNs) designed for the accurate classification of helmet\npresence on construction sites. Initially, a simple CNN model comprising one\nconvolutional block and one fully connected layer was developed, yielding\nmodest results. To enhance its performance, the model was progressively\nrefined, first by extending the architecture to include an additional\nconvolutional block and a fully connected layer. Subsequently, batch\nnormalization and dropout techniques were integrated, aiming to mitigate\noverfitting and improve the model's generalization capabilities. The\nperformance of these models is methodically analyzed, revealing a peak F1-score\nof 84\\%, precision of 82\\%, and recall of 86\\% with the most advanced\nconfiguration of the first study phase. Despite these improvements, the\naccuracy remained suboptimal, thus setting the stage for further architectural\nand operational enhancements. This work lays a foundational framework for\nongoing adjustments and optimization in automated helmet detection technology,\nwith future enhancements expected to address the limitations identified during\nthese initial experiments.",
      "tldr_zh": "这篇论文针对建筑工地安全问题，开发了一个轻量级卷积神经网络(CNN)来有效检测工人是否佩戴头盔。研究从一个简单模型（包含一个卷积块和一个全连接层）开始，随后通过添加额外卷积块和全连接层、整合批标准化(batch normalization)和dropout技术来优化模型性能，从而减少过拟合并提升泛化能力。实验结果显示，最优配置达到了84%的F1-score、82%的precision和86%的recall，尽管准确率仍有改进空间，该工作为未来的自动头盔检测技术提供了基础框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12669v1",
      "published_date": "2024-09-19 11:29:18 UTC",
      "updated_date": "2024-09-19 11:29:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:34:44.926771"
    },
    {
      "arxiv_id": "2409.13676v1",
      "title": "A sound description: Exploring prompt templates and class descriptions to enhance zero-shot audio classification",
      "title_zh": "翻译失败",
      "authors": [
        "Michel Olvera",
        "Paraskevas Stamatiadis",
        "Slim Essid"
      ],
      "abstract": "Audio-text models trained via contrastive learning offer a practical approach\nto perform audio classification through natural language prompts, such as \"this\nis a sound of\" followed by category names. In this work, we explore alternative\nprompt templates for zero-shot audio classification, demonstrating the\nexistence of higher-performing options. First, we find that the formatting of\nthe prompts significantly affects performance so that simply prompting the\nmodels with properly formatted class labels performs competitively with\noptimized prompt templates and even prompt ensembling. Moreover, we look into\ncomplementing class labels by audio-centric descriptions. By leveraging large\nlanguage models, we generate textual descriptions that prioritize acoustic\nfeatures of sound events to disambiguate between classes, without extensive\nprompt engineering. We show that prompting with class descriptions leads to\nstate-of-the-art results in zero-shot audio classification across major ambient\nsound datasets. Remarkably, this method requires no additional training and\nremains fully zero-shot.",
      "tldr_zh": "该研究探讨了如何通过优化提示模板和类描述来提升零-shot audio classification的表现。研究者发现，适当格式化的提示模板（如直接使用类标签）能与优化模板或prompt ensembling竞争，甚至表现出色；此外，他们利用large language models生成强调声学特征的音频描述，以更好地区分类别。结果显示，这种方法在主要ambient sound数据集上实现了state-of-the-art性能，且无需额外训练，保持完全zero-shot。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "DCASE 2024 - 9th Workshop on Detection and Classification of Acoustic\n  Scenes and Events, Oct 2024, Tokyo, Japan",
      "pdf_url": "http://arxiv.org/pdf/2409.13676v1",
      "published_date": "2024-09-19 11:27:50 UTC",
      "updated_date": "2024-09-19 11:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:34:55.482801"
    },
    {
      "arxiv_id": "2410.02806v1",
      "title": "Investigating the Impact of Randomness on Reproducibility in Computer Vision: A Study on Applications in Civil Engineering and Medicine",
      "title_zh": "翻译失败",
      "authors": [
        "Bahadır Eryılmaz",
        "Osman Alperen Koraş",
        "Jörg Schlötterer",
        "Christin Seifert"
      ],
      "abstract": "Reproducibility is essential for scientific research. However, in computer\nvision, achieving consistent results is challenging due to various factors. One\ninfluential, yet often unrecognized, factor is CUDA-induced randomness. Despite\nCUDA's advantages for accelerating algorithm execution on GPUs, if not\ncontrolled, its behavior across multiple executions remains non-deterministic.\nWhile reproducibility issues in ML being researched, the implications of\nCUDA-induced randomness in application are yet to be understood. Our\ninvestigation focuses on this randomness across one standard benchmark dataset\nand two real-world datasets in an isolated environment. Our results show that\nCUDA-induced randomness can account for differences up to 4.77% in performance\nscores. We find that managing this variability for reproducibility may entail\nincreased runtime or reduce performance, but that disadvantages are not as\nsignificant as reported in previous studies.",
      "tldr_zh": "该研究调查了CUDA-induced randomness对计算机视觉再现性(reproducibility)的冲击，特别关注土木工程和医学领域的实际应用。研究者使用一个标准基准数据集和两个真实世界数据集，在隔离环境中分析了这一随机性对性能的影响。结果显示，CUDA-induced randomness可导致性能分数差异高达4.77%，而管理这种变异性虽可能增加运行时间或降低性能，但其不利影响不如先前研究报道的那样显著。该工作为提升计算机视觉应用的再现性提供了重要见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.02806v1",
      "published_date": "2024-09-19 11:06:06 UTC",
      "updated_date": "2024-09-19 11:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:35:08.691699"
    },
    {
      "arxiv_id": "2409.12642v1",
      "title": "Deep generative models as an adversarial attack strategy for tabular machine learning",
      "title_zh": "深度生成模型作为表格机器学习的对抗攻击策略",
      "authors": [
        "Salijona Dyrmishi",
        "Mihaela Cătălina Stoian",
        "Eleonora Giunchiglia",
        "Maxime Cordy"
      ],
      "abstract": "Deep Generative Models (DGMs) have found application in computer vision for\ngenerating adversarial examples to test the robustness of machine learning (ML)\nsystems. Extending these adversarial techniques to tabular ML presents unique\nchallenges due to the distinct nature of tabular data and the necessity to\npreserve domain constraints in adversarial examples. In this paper, we adapt\nfour popular tabular DGMs into adversarial DGMs (AdvDGMs) and evaluate their\neffectiveness in generating realistic adversarial examples that conform to\ndomain constraints.",
      "tldr_zh": "该论文探讨了将 Deep Generative Models (DGMs) 作为对抗攻击策略应用于表格机器学习，旨在生成符合领域约束的真实对抗样本，以测试 ML 系统的鲁棒性。研究者将四种流行的表格 DGMs 改编为 Adversarial DGMs (AdvDGMs)，并评估了这些模型在处理表格数据独特挑战（如保持领域约束）方面的有效性。主要贡献在于证明 AdvDGMs 可以生成高质量对抗样本，从而为提升表格 ML 系统的安全性提供新方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICMLC 2024 (International Conference on Machine Learning\n  and Cybernetics)",
      "pdf_url": "http://arxiv.org/pdf/2409.12642v1",
      "published_date": "2024-09-19 10:41:23 UTC",
      "updated_date": "2024-09-19 10:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:35:20.058530"
    },
    {
      "arxiv_id": "2409.15371v9",
      "title": "Balancing LoRA Performance and Efficiency with Simple Shard Sharing",
      "title_zh": "翻译失败",
      "authors": [
        "Jiale Kang",
        "Qingyu Yin"
      ],
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), effectively reduce the number of trainable parameters in\nLarge Language Models (LLMs). However, as model scales continue to grow, the\ndemand for computational resources remains a significant challenge. Existing\nLoRA variants often struggle to strike an optimal balance between adaptability\n(model performance and convergence speed) and efficiency (computational\noverhead, memory usage, and initialization time). This paper introduces\nFOSSIL(\\textbf{F}ramework for \\textbf{O}ptimal \\textbf{S}hard \\textbf{S}haring\n\\textbf{I}ntegration in \\textbf{L}oRA), a novel PEFT approach that addresses\nthis trade-off through a simple shard-sharing mechanism. FOSSIL leverages the\ninsight that a low-rank adaptation can be achieved by decomposing the weight\nmatrix into multiple fragment matrices and utilizing a shared, trainable common\nfragment. This method constructs the low-rank update matrix through the\nreplication of these shared, partitioned shards. We also propose a\nhardware-efficient and broadly applicable implementation for FOSSIL. Extensive\nexperiments conducted on a range of tasks, alongside a systematic analysis of\ncomputational performance, demonstrate FOSSIL's superiority. The results show\nthat FOSSIL significantly outperforms standard LoRA and its prominent variants\nin both model performance metrics and computational efficiency, including\ninitialization speed and training throughput. By effectively balancing\nexpressive power and resource utilization, FOSSIL offers a compelling solution\nfor efficiently adapting large-scale models.",
      "tldr_zh": "这篇论文提出了 FOSSIL，一种新型的 Parameter-Efficient Fine-Tuning (PEFT) 方法，旨在平衡 Low-Rank Adaptation (LoRA) 在 Large Language Models (LLMs) 中的性能（如模型准确性和收敛速度）和效率（如计算开销、内存使用及初始化时间）。FOSSIL 通过将权重矩阵分解为多个片段矩阵，并利用一个共享的、可训练的公共片段来构建低秩更新矩阵，同时提供了一个硬件高效的实现方案。实验结果显示，FOSSIL 在各种任务上显著优于标准 LoRA 和其变体，不仅提升了模型性能指标，还改善了初始化速度和训练吞吐量。总体而言，FOSSIL 有效平衡了表达能力与资源利用，为大规模模型的适应提供了高效解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.15371v9",
      "published_date": "2024-09-19 10:26:42 UTC",
      "updated_date": "2025-05-16 12:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:35:33.809734"
    },
    {
      "arxiv_id": "2409.12634v1",
      "title": "Exploring bat song syllable representations in self-supervised audio encoders",
      "title_zh": "翻译失败",
      "authors": [
        "Marianne de Heer Kloots",
        "Mirjam Knörnschild"
      ],
      "abstract": "How well can deep learning models trained on human-generated sounds\ndistinguish between another species' vocalization types? We analyze the\nencoding of bat song syllables in several self-supervised audio encoders, and\nfind that models pre-trained on human speech generate the most distinctive\nrepresentations of different syllable types. These findings form first steps\ntowards the application of cross-species transfer learning in bat bioacoustics,\nas well as an improved understanding of out-of-distribution signal processing\nin audio encoder models.",
      "tldr_zh": "这篇论文探讨了自监督音频编码器（self-supervised audio encoders）对蝙蝠歌曲音节（bat song syllables）的表示能力，特别是这些模型在区分其他物种发声类型时的表现。研究发现，在人类语音上预训练的模型能够生成最独特的不同音节类型表示，优于其他预训练方式。这些结果为在蝙蝠生物声学（bat bioacoustics）中应用跨物种转移学习（cross-species transfer learning）奠定了基础，并提升了对音频编码器处理分布外信号（out-of-distribution signal processing）的理解。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Presented at VIHAR-2024; see https://vihar-2024.vihar.org/",
      "pdf_url": "http://arxiv.org/pdf/2409.12634v1",
      "published_date": "2024-09-19 10:09:31 UTC",
      "updated_date": "2024-09-19 10:09:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:35:46.365749"
    },
    {
      "arxiv_id": "2409.12632v1",
      "title": "Counterfactual Explanations for Clustering Models",
      "title_zh": "针对聚类模型的反事实解释",
      "authors": [
        "Aurora Spagnol",
        "Kacper Sokol",
        "Pietro Barbiero",
        "Marc Langheinrich",
        "Martin Gjoreski"
      ],
      "abstract": "Clustering algorithms rely on complex optimisation processes that may be\ndifficult to comprehend, especially for individuals who lack technical\nexpertise. While many explainable artificial intelligence techniques exist for\nsupervised machine learning, unsupervised learning -- and clustering in\nparticular -- has been largely neglected. To complicate matters further, the\nnotion of a ``true'' cluster is inherently challenging to define. These facets\nof unsupervised learning and its explainability make it difficult to foster\ntrust in such methods and curtail their adoption. To address these challenges,\nwe propose a new, model-agnostic technique for explaining clustering algorithms\nwith counterfactual statements. Our approach relies on a novel soft-scoring\nmethod that captures the spatial information utilised by clustering models. It\nbuilds upon a state-of-the-art Bayesian counterfactual generator for supervised\nlearning to deliver high-quality explanations. We evaluate its performance on\nfive datasets and two clustering algorithms, and demonstrate that introducing\nsoft scores to guide counterfactual search significantly improves the results.",
      "tldr_zh": "该论文针对聚类算法的可解释性问题，提出了一种新的模型无关（model-agnostic）反事实解释（Counterfactual Explanations）技术，以解决无监督学习中信任和采用的挑战。该方法引入一个新型软评分（soft-scoring）机制，捕捉聚类模型的空间信息，并基于先进的贝叶斯反事实生成器（Bayesian counterfactual generator）生成高质量解释。在五个数据集和两种聚类算法上的实验显示，这种软评分指导的反事实搜索显著提升了解释结果的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12632v1",
      "published_date": "2024-09-19 10:05:58 UTC",
      "updated_date": "2024-09-19 10:05:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:35:57.658860"
    },
    {
      "arxiv_id": "2409.12623v2",
      "title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and Benchmarks",
      "title_zh": "CamelEval：推进文化对齐的阿拉伯语言模型与基准测试",
      "authors": [
        "Zhaozhi Qian",
        "Faroq Altam",
        "Muhammad Alqurishi",
        "Riad Souissi"
      ],
      "abstract": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}.",
      "tldr_zh": "本文介绍了Juhaina，一种9.24亿参数的阿拉伯语-英语双语LLM，专为阿拉伯语使用者设计，支持指令遵循、开放式问题回答和文本处理等功能，并注重文化对齐。作者详细阐述了Juhaina的创建过程和实证评估，同时指出了现有Open Arabic LLM Leaderboard (OALL)的局限性，并提出了新的评估基准CamelEval。实验结果显示，Juhaina在生成有帮助的阿拉伯语响应、提供区域事实信息和理解文化细微差别方面，超过了同规模的LLM如Llama和Gemma系列。该模型旨在服务4亿多阿拉伯语使用者，推动AI技术的民主化，并已公开发布在Huggingface。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12623v2",
      "published_date": "2024-09-19 09:52:35 UTC",
      "updated_date": "2024-09-24 08:49:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:36:10.207528"
    },
    {
      "arxiv_id": "2409.12618v2",
      "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Santosh Kumar Radha",
        "Yasamin Nouri Jelyani",
        "Ara Ghukasyan",
        "Oktay Goktas"
      ],
      "abstract": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.",
      "tldr_zh": "该论文提出 Iteration of Thought (IoT) 框架，利用内部对话机制提升大型语言模型 (LLMs) 的自主推理能力，通过生成上下文特定的提示来动态优化响应。IoT 包括三个核心组件：Inner Dialogue Agent (IDA) 负责创建指导性提示、LLM Agent (LLMA) 处理这些提示进行响应精炼，以及一个迭代提示循环，支持动态适应而不生成废弃的备选想法。实验在 GPQA、Game of 24、Mini Crosswords 和 HotpotQA 等数据集上显示，IoT 相较于 Chain of Thought (CoT) 取得了显著性能提升，实现更高效的推理并减少人类干预。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12618v2",
      "published_date": "2024-09-19 09:44:17 UTC",
      "updated_date": "2024-10-01 17:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:36:21.816828"
    },
    {
      "arxiv_id": "2409.18998v1",
      "title": "Controlled LLM-based Reasoning for Clinical Trial Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Mael Jullien",
        "Alex Bogatu",
        "Harriet Unsworth",
        "Andre Freitas"
      ],
      "abstract": "Matching patients to clinical trials demands a systematic and reasoned\ninterpretation of documents which require significant expert-level background\nknowledge, over a complex set of well-defined eligibility criteria. Moreover,\nthis interpretation process needs to operate at scale, over vast knowledge\nbases of trials. In this paper, we propose a scalable method that extends the\ncapabilities of LLMs in the direction of systematizing the reasoning over sets\nof medical eligibility criteria, evaluating it in the context of real-world\ncases. The proposed method overlays a Set-guided reasoning method for LLMs. The\nproposed framework is evaluated on TREC 2022 Clinical Trials, achieving results\nsuperior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.",
      "tldr_zh": "这篇论文提出了一种基于控制的LLM推理方法，用于临床试验检索，旨在系统化地处理复杂的医疗资格标准，并减少对专家级背景知识的依赖。该方法通过Set-guided reasoning扩展LLM的能力，实现对患者匹配文档的规模化推理和评估。在TREC 2022 Clinical Trials数据集上的实验结果显示，该框架的NDCG@10达到0.693、Precision@10达到0.73，优于现有技术。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18998v1",
      "published_date": "2024-09-19 09:42:33 UTC",
      "updated_date": "2024-09-19 09:42:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:36:33.071815"
    },
    {
      "arxiv_id": "2409.12602v1",
      "title": "Enhancing Agricultural Environment Perception via Active Vision and Zero-Shot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Michele Carlo La Greca",
        "Mirko Usuelli",
        "Matteo Matteucci"
      ],
      "abstract": "Agriculture, fundamental for human sustenance, faces unprecedented\nchallenges. The need for efficient, human-cooperative, and sustainable farming\nmethods has never been greater. The core contributions of this work involve\nleveraging Active Vision (AV) techniques and Zero-Shot Learning (ZSL) to\nimprove the robot's ability to perceive and interact with agricultural\nenvironment in the context of fruit harvesting. The AV Pipeline implemented\nwithin ROS 2 integrates the Next-Best View (NBV) Planning for 3D environment\nreconstruction through a dynamic 3D Occupancy Map. Our system allows the\nrobotics arm to dynamically plan and move to the most informative viewpoints\nand explore the environment, updating the 3D reconstruction using semantic\ninformation produced through ZSL models. Simulation and real-world experimental\nresults demonstrate our system's effectiveness in complex visibility\nconditions, outperforming traditional and static predefined planning methods.\nZSL segmentation models employed, such as YOLO World + EfficientViT SAM,\nexhibit high-speed performance and accurate segmentation, allowing flexibility\nwhen dealing with semantic information in unknown agricultural contexts without\nrequiring any fine-tuning process.",
      "tldr_zh": "这篇论文提出了一种利用 Active Vision (AV) 和 Zero-Shot Learning (ZSL) 的方法，来提升农业机器人对环境的感知和互动，针对水果采摘等任务。该系统在 ROS 2 框架下实现了 AV Pipeline，包括 Next-Best View (NBV) Planning 和动态 3D Occupancy Map，允许机器人臂动态规划最佳视角并通过 ZSL 模型更新 3D 环境重建和语义信息。实验结果显示，该方法在模拟和真实场景中表现优于传统静态规划，尤其在复杂可见性条件下，且采用的 ZSL 模型如 YOLO World + EfficientViT SAM 实现了高速度和准确的分割，而无需任何微调过程。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12602v1",
      "published_date": "2024-09-19 09:26:23 UTC",
      "updated_date": "2024-09-19 09:26:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:36:46.253133"
    },
    {
      "arxiv_id": "2409.12587v1",
      "title": "Test-Time Augmentation Meets Variational Bayes",
      "title_zh": "翻译失败",
      "authors": [
        "Masanari Kimura",
        "Howard Bondell"
      ],
      "abstract": "Data augmentation is known to contribute significantly to the robustness of\nmachine learning models. In most instances, data augmentation is utilized\nduring the training phase. Test-Time Augmentation (TTA) is a technique that\ninstead leverages these data augmentations during the testing phase to achieve\nrobust predictions. More precisely, TTA averages the predictions of multiple\ndata augmentations of an instance to produce a final prediction. Although the\neffectiveness of TTA has been empirically reported, it can be expected that the\npredictive performance achieved will depend on the set of data augmentation\nmethods used during testing. In particular, the data augmentation methods\napplied should make different contributions to performance. That is, it is\nanticipated that there may be differing degrees of contribution in the set of\ndata augmentation methods used for TTA, and these could have a negative impact\non prediction performance. In this study, we consider a weighted version of the\nTTA based on the contribution of each data augmentation. Some variants of TTA\ncan be regarded as considering the problem of determining the appropriate\nweighting. We demonstrate that the determination of the coefficients of this\nweighted TTA can be formalized in a variational Bayesian framework. We also\nshow that optimizing the weights to maximize the marginal log-likelihood\nsuppresses candidates of unwanted data augmentations at the test phase.",
      "tldr_zh": "本研究探讨了Test-Time Augmentation (TTA)，一种在测试阶段使用数据增强来提升机器学习模型鲁棒性的技术，但强调不同数据增强方法的贡献差异可能影响预测性能。作者提出了一种加权TTA方法，通过变分贝叶斯框架形式化权重优化过程，以评估每个数据增强的贡献。实验结果表明，该方法能最大化边际对数似然，从而抑制不理想的数据增强并提高整体预测准确性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12587v1",
      "published_date": "2024-09-19 09:11:01 UTC",
      "updated_date": "2024-09-19 09:11:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:36:57.490010"
    },
    {
      "arxiv_id": "2409.12567v1",
      "title": "Model calibration using a parallel differential evolution algorithm in computational neuroscience: simulation of stretch induced nerve deficit",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio LaTorre",
        "Man Ting Kwong",
        "Julián A. García-Grajales",
        "Riyi Shi",
        "Antoine Jérusalem",
        "José-María Peña"
      ],
      "abstract": "Neuronal damage, in the form of both brain and spinal cord injuries, is one\nof the major causes of disability and death in young adults worldwide. One way\nto assess the direct damage occurring after a mechanical insult is the\nsimulation of the neuronal cells functional deficits following the mechanical\nevent. In this study, we use a coupled mechanical electrophysiological model\nwith several free parameters that are required to be calibrated against\nexperimental results. The calibration is carried out by means of an\nevolutionary algorithm (differential evolution, DE) that needs to evaluate each\nconfiguration of parameters on six different damage cases, each of them taking\nseveral minutes to compute. To minimise the simulation time of the parameter\ntuning for the DE, the stretch of one unique fixed-diameter axon with a\nsimplified triggering process is used to speed up the calculations. The model\nis then leveraged for the parameter optimization of the more realistic bundle\nof independent axons, an impractical configuration to run on a single processor\ncomputer. To this end, we have developed a parallel implementation based on\nOpenMP that runs on a multi-processor taking advantage of all the available\ncomputational power. The parallel DE algorithm obtains good results,\noutperforming the best effort achieved by published manual calibration, in a\nfraction of the time. While not being able to fully capture the experimental\nresults, the resulting nerve model provides a complex averaging framework for\nnerve damage simulation able to simulate gradual axonal functional alteration\nin a bundle.",
      "tldr_zh": "本文提出了一种基于差分进化算法 (differential evolution, DE) 的模型校准方法，用于计算神经科学中模拟机械拉伸引起的神经损伤。该方法通过耦合机械-电生理模型，并采用简化轴突拉伸模拟来加速参数优化，同时开发了基于 OpenMP 的并行实现，以在多处理器上高效处理多个损伤案例。实验结果显示，该并行 DE 算法显著缩短了计算时间，优于手动校准，并提供了一个复杂框架来模拟神经束中渐进性的轴突功能改变，尽管未能完全重现实验数据。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12567v1",
      "published_date": "2024-09-19 08:40:32 UTC",
      "updated_date": "2024-09-19 08:40:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:37:10.560741"
    },
    {
      "arxiv_id": "2409.13774v1",
      "title": "Trustworthy Intrusion Detection: Confidence Estimation Using Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Ioannis Pitsiorlas",
        "George Arvanitakis",
        "Marios Kountouris"
      ],
      "abstract": "This work introduces a novel method for enhancing confidence in anomaly\ndetection in Intrusion Detection Systems (IDS) through the use of a Variational\nAutoencoder (VAE) architecture. By developing a confidence metric derived from\nlatent space representations, we aim to improve the reliability of IDS\npredictions against cyberattacks. Applied to the NSL-KDD dataset, our approach\nfocuses on binary classification tasks to effectively distinguish between\nnormal and malicious network activities. The methodology demonstrates a\nsignificant enhancement in anomaly detection, evidenced by a notable\ncorrelation of 0.45 between the reconstruction error and the proposed metric.\nOur findings highlight the potential of employing VAEs for more accurate and\ntrustworthy anomaly detection in network security.",
      "tldr_zh": "本研究提出了一种新方法，使用 Variational Autoencoder (VAE) 从 latent space 派生置信度指标，以提升 Intrusion Detection Systems (IDS) 在异常检测中的可靠性，从而更好地应对网络攻击。方法聚焦于二元分类任务，应用于 NSL-KDD 数据集，通过分析重建错误与置信度指标的相关性（达 0.45），显著提高了正常和恶意活动的区分准确性。该方法证明了 VAEs 在网络安全中的潜力，提供更可信的异常检测框架。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.13774v1",
      "published_date": "2024-09-19 08:09:44 UTC",
      "updated_date": "2024-09-19 08:09:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:37:21.954226"
    },
    {
      "arxiv_id": "2409.12538v1",
      "title": "PersonaFlow: Boosting Research Ideation with LLM-Simulated Expert Personas",
      "title_zh": "翻译失败",
      "authors": [
        "Yiren Liu",
        "Pranav Sharma",
        "Mehul Jitendra Oswal",
        "Haijun Xia",
        "Yun Huang"
      ],
      "abstract": "Developing novel interdisciplinary research ideas often requires discussions\nand feedback from experts across different domains. However, obtaining timely\ninputs is challenging due to the scarce availability of domain experts. Recent\nadvances in Large Language Model (LLM) research have suggested the feasibility\nof utilizing LLM-simulated expert personas to support research ideation. In\nthis study, we introduce PersonaFlow, an LLM-based system using persona\nsimulation to support the ideation stage of interdisciplinary scientific\ndiscovery. Our findings indicate that using multiple personas during ideation\nsignificantly enhances user-perceived quality of outcomes (e.g., relevance of\ncritiques, creativity of research questions) without increasing cognitive load.\nWe also found that users' persona customization interactions significantly\nimproved their sense of control and recall of generated ideas. Based on the\nfindings, we discuss highlighting ethical concerns, including potential\nover-reliance and cognitive biases, and suggest design implications for\nleveraging LLM-simulated expert personas to support research ideation when\nhuman expertise is inaccessible.",
      "tldr_zh": "这篇论文介绍了PersonaFlow，一种基于LLM-simulated expert personas的系统，用于提升跨学科研究构思阶段的效率。研究发现，使用多个模拟专家角色显著提高了用户感知的结果质量（如批评的相关性和研究问题的创意），同时未增加认知负担；此外，用户对角色的自定义互动增强了他们的控制感和对生成想法的回忆。论文还讨论了潜在伦理问题，包括过度依赖和认知偏差，并提出设计启示，以在人类专家不可用时有效利用此类系统。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12538v1",
      "published_date": "2024-09-19 07:54:29 UTC",
      "updated_date": "2024-09-19 07:54:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:37:33.061696"
    },
    {
      "arxiv_id": "2409.12524v1",
      "title": "Should RAG Chatbots Forget Unimportant Conversations? Exploring Importance and Forgetting with Psychological Insights",
      "title_zh": "翻译失败",
      "authors": [
        "Ryuichi Sumida",
        "Koji Inoue",
        "Tatsuya Kawahara"
      ],
      "abstract": "While Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nlong-term conversations, the increasing memory load as conversations progress\ndegrades retrieval accuracy. Drawing on psychological insights, we propose\nLUFY, a simple yet effective method that focuses on emotionally arousing\nmemories and retains less than 10% of the conversation. In the user experiment,\nparticipants interacted with three types of RAG chatbots, each for 2 hours over\n4 sessions, marking the most extensive assessment of a chatbot's long-term\ncapabilities to date -- more than four times longer than any existing\nbenchmark. The results demonstrate that prioritizing arousing memories while\nforgetting the majority of the conversation significantly enhances user\nexperience. This study pushes the frontier of long-term conversations and\nhighlights the importance of forgetting unimportant parts of conversations.\nCode and Dataset: https://github.com/ryuichi-sumida/LUFY",
      "tldr_zh": "该论文探讨了Retrieval-Augmented Generation (RAG)聊天机器人在长期对话中，由于记忆负载增加而导致检索准确性下降的问题。作者提出LUFY方法，利用心理洞见专注于情感唤起记忆，仅保留对话的不到10%，以有效管理记忆。用户实验中，参与者与三种RAG聊天机器人互动，每次2小时共4次会话，结果显示这种优先记忆策略显著提升了用户体验，并强调了遗忘不重要对话的重要性。代码和数据集可从https://github.com/ryuichi-sumida/LUFY获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12524v1",
      "published_date": "2024-09-19 07:39:22 UTC",
      "updated_date": "2024-09-19 07:39:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:37:45.480260"
    },
    {
      "arxiv_id": "2409.12518v4",
      "title": "Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Boying Li",
        "Zhixi Cai",
        "Yuan-Fang Li",
        "Ian Reid",
        "Hamid Rezatofighi"
      ],
      "abstract": "We propose Hier-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring\na novel hierarchical categorical representation, which enables accurate global\n3D semantic mapping, scaling-up capability, and explicit semantic label\nprediction in the 3D world. The parameter usage in semantic SLAM systems\nincreases significantly with the growing complexity of the environment, making\nit particularly challenging and costly for scene understanding. To address this\nproblem, we introduce a novel hierarchical representation that encodes semantic\ninformation in a compact form into 3D Gaussian Splatting, leveraging the\ncapabilities of large language models (LLMs). We further introduce a novel\nsemantic loss designed to optimize hierarchical semantic information through\nboth inter-level and cross-level optimization. Furthermore, we enhance the\nwhole SLAM system, resulting in improved tracking and mapping performance. Our\n\\MethodName{} outperforms existing dense SLAM methods in both mapping and\ntracking accuracy, while achieving a 2x operation speed-up. Additionally, it\nachieves on-par semantic rendering performance compared to existing methods\nwhile significantly reducing storage and training time requirements. Rendering\nFPS impressively reaches 2,000 with semantic information and 3,000 without it.\nMost notably, it showcases the capability of handling the complex real-world\nscene with more than 500 semantic classes, highlighting its valuable scaling-up\ncapability. The open-source code is available at\nhttps://github.com/LeeBY68/Hier-SLAM",
      "tldr_zh": "本研究提出Hier-SLAM，一种基于层次化分类表示(hierarchical categorical representation)的语义3D Gaussian Splatting SLAM方法，能够实现精确的全局3D语义映射、扩展能力以及显式的语义标签预测。论文引入紧凑的层次化表示，利用大型语言模型(LLMs)将语义信息编码到3D Gaussian Splatting中，并设计新型语义损失函数，通过层间和跨层优化提升整体系统性能。实验结果显示，Hier-SLAM在映射和跟踪准确性上优于现有密集SLAM方法，操作速度提高2倍，同时显著减少存储和训练时间，并在处理超过500个语义类的复杂场景时展现出强大的扩展能力，渲染FPS可达2000（带语义）或3000（无语义）。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted for publication at ICRA 2025. Code is available at\n  https://github.com/LeeBY68/Hier-SLAM",
      "pdf_url": "http://arxiv.org/pdf/2409.12518v4",
      "published_date": "2024-09-19 07:18:41 UTC",
      "updated_date": "2025-03-10 05:47:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:37:58.371892"
    },
    {
      "arxiv_id": "2409.12517v2",
      "title": "Scaling FP8 training to trillion-token LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Maxim Fishman",
        "Brian Chmiel",
        "Ron Banner",
        "Daniel Soudry"
      ],
      "abstract": "We train, for the first time, large language models using FP8 precision on\ndatasets up to 2 trillion tokens -- a 20-fold increase over previous limits.\nThrough these extended training runs, we uncover critical instabilities in FP8\ntraining that were not observable in earlier works with shorter durations. We\ntrace these instabilities to outlier amplification by the SwiGLU activation\nfunction. Interestingly, we show, both analytically and empirically, that this\namplification happens only over prolonged training periods, and link it to a\nSwiGLU weight alignment process. To address this newly identified issue, we\nintroduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training\nwithout altering function behavior. We also demonstrate, for the first time,\nFP8 quantization of both Adam optimizer moments. Combining these innovations,\nwe successfully train a 7B parameter model using FP8 precision on 256 Intel\nGaudi2 accelerators, achieving on-par results with the BF16 baseline while\ndelivering up to a $\\sim 34 \\%$ throughput improvement. A reference\nimplementation is supplied in\nhttps://github.com/Anonymous1252022/Megatron-DeepSpeed.",
      "tldr_zh": "本论文首次使用 FP8 精度训练大型语言模型（LLMs），将数据集规模扩展到 2 万亿 tokens，比之前提升 20 倍。研究发现，FP8 训练在长时间运行中出现不稳定性，主要源于 SwiGLU 激活函数的异常放大，该问题与权重对齐过程相关。作者提出 Smooth-SwiGLU 修改方法，确保训练稳定，同时实现了 FP8 量化 Adam 优化器。最终，在 256 个 Intel Gaudi2 加速器上训练 7B 参数模型，性能与 BF16 基线相当，但吞吐量提升约 34%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12517v2",
      "published_date": "2024-09-19 07:15:58 UTC",
      "updated_date": "2025-02-10 09:37:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:38:10.016261"
    },
    {
      "arxiv_id": "2409.12516v1",
      "title": "A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets -- A New Microfoundations of GARCH model",
      "title_zh": "翻译失败",
      "authors": [
        "Kei Nakagawa",
        "Masanori Hirano",
        "Kentaro Minami",
        "Takanobu Mizuta"
      ],
      "abstract": "The AI traders in financial markets have sparked significant interest in\ntheir effects on price formation mechanisms and market volatility, raising\nimportant questions for market stability and regulation. Despite this interest,\na comprehensive model to quantitatively assess the specific impacts of AI\ntraders remains undeveloped. This study aims to address this gap by modeling\nthe influence of AI traders on market price formation and volatility within a\nmulti-agent framework, leveraging the concept of microfoundations.\nMicrofoundations involve understanding macroeconomic phenomena, such as market\nprice formation, through the decision-making and interactions of individual\neconomic agents. While widely acknowledged in macroeconomics, microfoundational\napproaches remain unexplored in empirical finance, particularly for models like\nthe GARCH model, which captures key financial statistical properties such as\nvolatility clustering and fat tails. This study proposes a multi-agent market\nmodel to derive the microfoundations of the GARCH model, incorporating three\ntypes of agents: noise traders, fundamental traders, and AI traders. By\nmathematically aggregating the micro-structure of these agents, we establish\nthe microfoundations of the GARCH model. We validate this model through\nmulti-agent simulations, confirming its ability to reproduce the stylized facts\nof financial markets. Finally, we analyze the impact of AI traders using\nparameters derived from these microfoundations, contributing to a deeper\nunderstanding of their role in market dynamics.",
      "tldr_zh": "本研究提出一个多智能体市场模型，旨在解释 AI traders 在金融市场对价格形成和波动性的影响，并为其提供 GARCH model 的新微观基础（microfoundations）。模型包括三种代理：noise traders、fundamental traders 和 AI traders，通过数学聚合这些代理的微观互动并进行多智能体模拟，成功重现了金融市场的典型事实，如波动性聚类和厚尾分布。最终，研究分析了 AI traders 的作用，为评估市场稳定性和制定监管政策提供了更深入的理解。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.MA",
        "q-fin.TR"
      ],
      "primary_category": "q-fin.CP",
      "comment": "Accepted PRIMA2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12516v1",
      "published_date": "2024-09-19 07:14:13 UTC",
      "updated_date": "2024-09-19 07:14:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:38:21.663645"
    },
    {
      "arxiv_id": "2409.12992v1",
      "title": "DiffEditor: Enhancing Speech Editing with Semantic Enrichment and Acoustic Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Chen",
        "Yuhang Jia",
        "Shiwan Zhao",
        "Ziyue Jiang",
        "Haoran Li",
        "Jiarong Kang",
        "Yong Qin"
      ],
      "abstract": "As text-based speech editing becomes increasingly prevalent, the demand for\nunrestricted free-text editing continues to grow. However, existing speech\nediting techniques encounter significant challenges, particularly in\nmaintaining intelligibility and acoustic consistency when dealing with\nout-of-domain (OOD) text. In this paper, we introduce, DiffEditor, a novel\nspeech editing model designed to enhance performance in OOD text scenarios\nthrough semantic enrichment and acoustic consistency. To improve the\nintelligibility of the edited speech, we enrich the semantic information of\nphoneme embeddings by integrating word embeddings extracted from a pretrained\nlanguage model. Furthermore, we emphasize that interframe smoothing properties\nare critical for modeling acoustic consistency, and thus we propose a\nfirst-order loss function to promote smoother transitions at editing boundaries\nand enhance the overall fluency of the edited speech. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in both\nin-domain and OOD text scenarios.",
      "tldr_zh": "本研究提出DiffEditor，一种新型语音编辑模型，旨在通过semantic enrichment和acoustic consistency提升对out-of-domain (OOD)文本的处理能力。模型通过整合预训练语言模型的词嵌入来丰富音素嵌入的语义信息，从而提高编辑语音的可理解性；同时，引入一阶损失函数来促进编辑边界的光滑过渡，确保声学一致性和整体流畅性。实验结果显示，DiffEditor在领域内和OOD文本场景中均达到state-of-the-art性能，显著改善了现有语音编辑技术的局限性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12992v1",
      "published_date": "2024-09-19 07:11:54 UTC",
      "updated_date": "2024-09-19 07:11:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:38:33.180175"
    },
    {
      "arxiv_id": "2409.13773v1",
      "title": "A Case Study of Web App Coding with OpenAI Reasoning Models",
      "title_zh": "使用 OpenAI 推理模型的 Web 应用程序编码案例研究",
      "authors": [
        "Yi Cui"
      ],
      "abstract": "This paper presents a case study of coding tasks by the latest reasoning\nmodels of OpenAI, i.e. o1-preview and o1-mini, in comparison with other\nfrontier models. The o1 models deliver SOTA results for WebApp1K, a single-task\nbenchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling\nnumber of tasks and test cases. The new benchmark causes the o1 model\nperformances to decline significantly, falling behind Claude 3.5. Moreover,\nthey consistently fail when confronted with atypical yet correct test cases, a\ntrap non-reasoning models occasionally avoid. We hypothesize that the\nperformance variability is due to instruction comprehension. Specifically, the\nreasoning mechanism boosts performance when all expectations are captured,\nmeanwhile exacerbates errors when key expectations are missed, potentially\nimpacted by input lengths. As such, we argue that the coding success of\nreasoning models hinges on the top-notch base model and SFT to ensure\nmeticulous adherence to instructions.",
      "tldr_zh": "本研究通过案例分析比较了 OpenAI 的推理模型（o1-preview 和 o1-mini）与其他前沿模型在 Web 应用编码任务上的表现。o1 模型在单任务基准 WebApp1K 上实现了 SOTA 结果，但引入更具挑战性的 WebApp1K-Duo 基准后，其性能显著下降，落后于 Claude 3.5，且在非典型测试案例中 consistently fail。研究假设这种变异性源于指令理解问题，推理机制在捕捉所有期望时提升表现，但遗漏关键点时会放大错误，可能受输入长度影响。因此，作者强调，推理模型的编码成功依赖于顶尖的基模型和 SFT，以确保对指令的 meticulous adherence。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13773v1",
      "published_date": "2024-09-19 06:58:02 UTC",
      "updated_date": "2024-09-19 06:58:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:38:46.843284"
    },
    {
      "arxiv_id": "2409.18997v2",
      "title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent",
      "title_zh": "翻译失败",
      "authors": [
        "Jiateng Liu",
        "Lin Ai",
        "Zizhou Liu",
        "Payam Karisani",
        "Zheng Hui",
        "May Fung",
        "Preslav Nakov",
        "Julia Hirschberg",
        "Heng Ji"
      ],
      "abstract": "Propaganda plays a critical role in shaping public opinion and fueling\ndisinformation. While existing research primarily focuses on identifying\npropaganda techniques, it lacks the ability to capture the broader motives and\nthe impacts of such content. To address these challenges, we introduce\npropainsight, a conceptual framework grounded in foundational social science\nresearch, which systematically dissects propaganda into techniques, arousal\nappeals, and underlying intent. propainsight offers a more granular\nunderstanding of how propaganda operates across different contexts.\nAdditionally, we present propagaze, a novel dataset that combines\nhuman-annotated data with high-quality synthetic data generated through a\nmeticulously designed pipeline. Our experiments show that off-the-shelf LLMs\nstruggle with propaganda analysis, but training with propagaze significantly\nimproves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span\nIoU in technique identification and 66.2% higher BertScore in appeal analysis\ncompared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited\nhuman-annotated data in data-sparse and cross-domain scenarios, showing its\npotential for comprehensive and generalizable propaganda analysis.",
      "tldr_zh": "该论文提出了PropaInsight框架，将宣传分析细分为techniques（技巧）、arousal appeals（唤起诉求）和underlying intent（潜在意图），旨在提供更深入的理解，超越现有研究对宣传动机的局限。研究者构建了Propagaze数据集，通过结合人类标注数据和高质合成数据管道，提升分析的全面性。实验结果显示，现有LLMs在宣传分析上表现欠佳，但使用Propagaze训练的Llama-7B-Chat模型在technique identification的text span IoU提高了203.4%，在appeal analysis的BertScore提高了66.2%，优于1-shot GPT-4-Turbo；此外，数据集在数据稀缺和跨域场景中展现出显著的补充潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18997v2",
      "published_date": "2024-09-19 06:28:18 UTC",
      "updated_date": "2025-02-13 23:45:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:38:58.496256"
    },
    {
      "arxiv_id": "2409.12500v1",
      "title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward",
      "title_zh": "翻译失败",
      "authors": [
        "Dongheng Li",
        "Yongchang Hao",
        "Lili Mou"
      ],
      "abstract": "Large language models have become increasingly popular and demonstrated\nremarkable performance in various natural language processing (NLP) tasks.\nHowever, these models are typically computationally expensive and difficult to\nbe deployed in resource-constrained environments. In this paper, we propose\nLLMR, a novel knowledge distillation (KD) method based on a reward function\ninduced from large language models. We conducted experiments on multiple\ndatasets in the dialogue generation and summarization tasks. Empirical results\ndemonstrate that our LLMR approach consistently outperforms traditional KD\nmethods in different tasks and datasets.",
      "tldr_zh": "论文提出LLMR，一种基于大型语言模型(Large Language Model)诱导奖励的知识蒸馏(Knowledge Distillation)方法，旨在解决大型模型在资源受限环境下的计算开销问题。该方法通过奖励函数从大型语言模型中提取知识，并应用于对话生成和摘要任务的多个数据集上。实验结果表明，LLMR在不同任务和数据集上 consistently outperforms 传统知识蒸馏方法，为高效的NLP模型部署提供了有效途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by LERC COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12500v1",
      "published_date": "2024-09-19 06:27:58 UTC",
      "updated_date": "2024-09-19 06:27:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:39:09.631994"
    },
    {
      "arxiv_id": "2409.12490v2",
      "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling Acceleration in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Junlin Lv",
        "Yuan Feng",
        "Xike Xie",
        "Xin Jia",
        "Qirong Peng",
        "Guiming Xie"
      ],
      "abstract": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）中预填充（prefilling）阶段的低效问题，提出CritiPrefill，一种基于查询关键性的分段方法，通过观察相邻查询标记倾向于关注相似的Key-Value (KV) 缓存子集来优化自注意力机制。方法将输入序列的查询和KV缓存分区为段和块，估计算法关键性并修剪非关键计算，从而显著加速预填充过程。实验结果显示，在单A100 GPU上，对于128K上下文长度，Llama3-8B加速高达2.7倍、Yi-9B加速高达3.0倍，同时保持质量损失最小。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12490v2",
      "published_date": "2024-09-19 06:09:56 UTC",
      "updated_date": "2024-09-23 02:24:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:39:21.457147"
    },
    {
      "arxiv_id": "2409.12479v1",
      "title": "Learning Multi-Manifold Embedding for Out-Of-Distribution Detection",
      "title_zh": "用于分布外检测的多流形嵌入学习",
      "authors": [
        "Jeng-Lin Li",
        "Ming-Ching Chang",
        "Wei-Chao Chen"
      ],
      "abstract": "Detecting out-of-distribution (OOD) samples is crucial for trustworthy AI in\nreal-world applications. Leveraging recent advances in representation learning\nand latent embeddings, Various scoring algorithms estimate distributions beyond\nthe training data. However, a single embedding space falls short in\ncharacterizing in-distribution data and defending against diverse OOD\nconditions. This paper introduces a novel Multi-Manifold Embedding Learning\n(MMEL) framework, optimizing hypersphere and hyperbolic spaces jointly for\nenhanced OOD detection. MMEL generates representative embeddings and employs a\nprototype-aware scoring function to differentiate OOD samples. It operates with\nvery few OOD samples and requires no model retraining. Experiments on six open\ndatasets demonstrate MMEL's significant reduction in FPR while maintaining a\nhigh AUC compared to state-of-the-art distance-based OOD detection methods. We\nanalyze the effects of learning multiple manifolds and visualize OOD score\ndistributions across datasets. Notably, enrolling ten OOD samples without\nretraining achieves comparable FPR and AUC to modern outlier exposure methods\nusing 80 million outlier samples for model training.",
      "tldr_zh": "本研究提出了一种 Multi-Manifold Embedding Learning (MMEL) 框架，用于提升 Out-Of-Distribution (OOD) 检测性能，以提高 AI 在真实世界的可信度。MMEL 通过联合优化 hypersphere 和 hyperbolic 空间生成代表性嵌入，并采用 prototype-aware scoring function 来区分 OOD 样本，该方法仅需少量 OOD 样本且无需模型重新训练。实验在六个公开数据集上显示，MMEL 显著降低了 FPR，同时保持高 AUC，比现有基于距离的 OOD 检测方法表现更优；特别地，使用仅十个 OOD 样本即可达到与使用 8000 万样本的 outlier exposure 方法相当的效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "European Conference on Computer Vision ECCV 2024 BEW Workshop Best\n  Paper",
      "pdf_url": "http://arxiv.org/pdf/2409.12479v1",
      "published_date": "2024-09-19 05:43:00 UTC",
      "updated_date": "2024-09-19 05:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:39:33.579027"
    },
    {
      "arxiv_id": "2409.12477v2",
      "title": "ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend Conditioning",
      "title_zh": "翻译失败",
      "authors": [
        "Daewoong Kim",
        "Hao-Wen Dong",
        "Dasaem Jeong"
      ],
      "abstract": "Modeling the natural contour of fundamental frequency (F0) plays a critical\nrole in music audio synthesis. However, transcribing and managing multiple F0\ncontours in polyphonic music is challenging, and explicit F0 contour modeling\nhas not yet been explored for polyphonic instrumental synthesis. In this paper,\nwe present ViolinDiff, a two-stage diffusion-based synthesis framework. For a\ngiven violin MIDI file, the first stage estimates the F0 contour as pitch bend\ninformation, and the second stage generates mel spectrogram incorporating these\nexpressive details. The quantitative metrics and listening test results show\nthat the proposed model generates more realistic violin sounds than the model\nwithout explicit pitch bend modeling. Audio samples are available online:\ndaewoung.github.io/ViolinDiff-Demo.",
      "tldr_zh": "该研究提出 ViolinDiff，一种两阶段基于扩散的合成框架，用于提升小提琴音频合成的表现力，通过显式建模基本频率（F0）轮廓作为 pitch bend 信息来应对多声部音乐的转录挑战。在第一阶段，框架从给定的小提琴 MIDI 文件中估计 F0 轮廓；在第二阶段，生成 mel spectrogram 并融入这些细节。实验结果显示，ViolinDiff 在定量指标和听力测试中比无 pitch bend 建模的模型生成更真实的提琴声音。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted for publication at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12477v2",
      "published_date": "2024-09-19 05:39:19 UTC",
      "updated_date": "2025-02-04 11:43:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:39:44.586867"
    },
    {
      "arxiv_id": "2409.12472v1",
      "title": "TEAM: Temporal Adversarial Examples Attack Model against Network Intrusion Detection System Applied to RNN",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyi Liu",
        "Dengpan Ye",
        "Long Tang",
        "Yunming Zhang",
        "Jiacheng Deng"
      ],
      "abstract": "With the development of artificial intelligence, neural networks play a key\nrole in network intrusion detection systems (NIDS). Despite the tremendous\nadvantages, neural networks are susceptible to adversarial attacks. To improve\nthe reliability of NIDS, many research has been conducted and plenty of\nsolutions have been proposed. However, the existing solutions rarely consider\nthe adversarial attacks against recurrent neural networks (RNN) with time\nsteps, which would greatly affect the application of NIDS in real world.\nTherefore, we first propose a novel RNN adversarial attack model based on\nfeature reconstruction called \\textbf{T}emporal adversarial \\textbf{E}xamples\n\\textbf{A}ttack \\textbf{M}odel \\textbf{(TEAM)}, which applied to time series\ndata and reveals the potential connection between adversarial and time steps in\nRNN. That is, the past adversarial examples within the same time steps can\ntrigger further attacks on current or future original examples. Moreover, TEAM\nleverages Time Dilation (TD) to effectively mitigates the effect of temporal\namong adversarial examples within the same time steps. Experimental results\nshow that in most attack categories, TEAM improves the misjudgment rate of NIDS\non both black and white boxes, making the misjudgment rate reach more than\n96.68%. Meanwhile, the maximum increase in the misjudgment rate of the NIDS for\nsubsequent original samples exceeds 95.57%.",
      "tldr_zh": "这篇论文针对神经网络在网络入侵检测系统 (NIDS) 中的易受对抗攻击问题，提出了一种新型攻击模型 TEAM (Temporal Adversarial Examples Attack Model)，专门针对循环神经网络 (RNN) 处理的时间序列数据。TEAM 通过特征重建揭示了时间步之间的潜在连接，即过去的对抗样本可触发对当前或未来原始样本的进一步攻击，并利用 Time Dilation (TD) 来有效缓解这些时间步的影响。实验结果显示，在大多数攻击类别中，TEAM 使 NIDS 的误判率超过 96.68%，并对后续原始样本的误判率提升超过 95.57%，从而暴露了 RNN 在真实世界应用中的漏洞。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12472v1",
      "published_date": "2024-09-19 05:26:04 UTC",
      "updated_date": "2024-09-19 05:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:39:57.814128"
    },
    {
      "arxiv_id": "2409.12471v1",
      "title": "Arena 4.0: A Comprehensive ROS2 Development and Benchmarking Platform for Human-centric Navigation Using Generative-Model-based Environment Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Volodymyr Shcherbyna1",
        "Linh Kästner",
        "Diego Diaz",
        "Huu Giang Nguyen",
        "Maximilian Ho-Kyoung Schreff",
        "Tim Lenz",
        "Jonas Kreutz",
        "Ahmed Martban",
        "Huajian Zeng",
        "Harold Soh"
      ],
      "abstract": "Building on the foundations of our previous work, this paper introduces Arena\n4.0, a significant advancement over Arena 3.0, Arena-Bench, Arena 1.0, and\nArena 2.0. Arena 4.0 offers three key novel contributions: (1) a\ngenerative-model-based world and scenario generation approach that utilizes\nlarge language models (LLMs) and diffusion models to dynamically generate\ncomplex, human-centric environments from text prompts or 2D floorplans, useful\nfor the development and benchmarking of social navigation strategies; (2) a\ncomprehensive 3D model database, extendable with additional 3D assets that are\nsemantically linked and annotated for dynamic spawning and arrangement within\n3D worlds; and (3) a complete migration to ROS 2, enabling compatibility with\nmodern hardware and enhanced functionalities for improved navigation,\nusability, and easier deployment on real robots. We evaluated the platform's\nperformance through a comprehensive user study, demonstrating significant\nimprovements in usability and efficiency compared to previous versions. Arena\n4.0 is openly available at https://github.com/Arena-Rosnav.",
      "tldr_zh": "本研究介绍了 Arena 4.0，这是一个基于 ROS 2 的综合开发和基准测试平台，用于人类中心导航策略。Arena 4.0 的关键创新包括：利用大型语言模型 (LLMs) 和扩散模型从文本提示或 2D 楼层图动态生成复杂环境，从而支持社交导航策略的开发和测试；提供一个可扩展的 3D 模型数据库，带有语义链接和注释，便于动态生成和安排 3D 世界；以及完全迁移到 ROS 2，以提升硬件兼容性、导航功能和实际机器人部署的便利性。通过用户研究，Arena 4.0 展示了比前版本显著提高的可用性和效率，并已在 GitHub 上开源。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12471v1",
      "published_date": "2024-09-19 05:20:13 UTC",
      "updated_date": "2024-09-19 05:20:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:40:09.532983"
    },
    {
      "arxiv_id": "2409.12468v2",
      "title": "Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Dongwon Jung",
        "Qin Liu",
        "Tenghao Huang",
        "Ben Zhou",
        "Muhao Chen"
      ],
      "abstract": "Retrieval-augmented generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieved from external\nsources. However, it often struggles to cope with inconsistent and irrelevant\ninformation that can distract the LM from its tasks, especially when multiple\nevidence pieces are required. While compressing the retrieved evidence with a\ncompression model aims to address this issue, the compressed evidence may still\nbe unfamiliar to the target model used for downstream tasks, potentially\nfailing to utilize the evidence effectively. We propose FaviComp\n(Familarity-Aware Evidence Compression), a novel training-free evidence\ncompression technique that makes retrieved evidence more familiar to the target\nmodel, while seamlessly integrating parametric knowledge from the model.\nExperimental results show that FaviComp consistently outperforms most recent\nevidence compression baselines across multiple open-domain QA datasets,\nimproving accuracy by up to 28.1% while achieving high compression rates.\nAdditionally, we demonstrate the effective integration of both parametric and\nnon-parametric knowledge during evidence compression.",
      "tldr_zh": "该研究针对Retrieval-Augmented Generation (RAG) 中检索证据的不一致性和无关信息问题，提出了一种无需训练的FaviComp (Familiarity-Aware Evidence Compression) 技术。该方法通过使证据更符合目标模型的熟悉度，同时无缝整合模型的parametric knowledge 和非参数知识(non-parametric knowledge)，来提升证据的利用效率。实验结果显示，FaviComp 在多个开放域QA数据集上比现有基线提升准确率高达28.1%，并保持高压缩率。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12468v2",
      "published_date": "2024-09-19 05:14:55 UTC",
      "updated_date": "2024-12-17 00:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:40:20.888726"
    },
    {
      "arxiv_id": "2409.12467v2",
      "title": "SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Chen",
        "Xingjian Luo",
        "Jinlin Wu",
        "Long Bai",
        "Zhen Lei",
        "Hongliang Ren",
        "Sebastien Ourselin",
        "Hongbin Liu"
      ],
      "abstract": "Surgical phase recognition is critical for assisting surgeons in\nunderstanding surgical videos. Existing studies focused more on online surgical\nphase recognition, by leveraging preceding frames to predict the current frame.\nDespite great progress, they formulated the task as a series of frame-wise\nclassification, which resulted in a lack of global context of the entire\nprocedure and incoherent predictions. Moreover, besides online analysis,\naccurate offline surgical phase recognition is also in significant clinical\nneed for retrospective analysis, and existing online algorithms do not fully\nanalyze the entire video, thereby limiting accuracy in offline analysis. To\novercome these challenges and enhance both online and offline inference\ncapabilities, we propose a universal Surgical Phase Localization Network, named\nSurgPLAN++, with the principle of temporal detection. To ensure a global\nunderstanding of the surgical procedure, we devise a phase localization\nstrategy for SurgPLAN++ to predict phase segments across the entire video\nthrough phase proposals. For online analysis, to generate high-quality phase\nproposals, SurgPLAN++ incorporates a data augmentation strategy to extend the\nstreaming video into a pseudo-complete video through mirroring,\ncenter-duplication, and down-sampling. For offline analysis, SurgPLAN++\ncapitalizes on its global phase prediction framework to continuously refine\npreceding predictions during each online inference step, thereby significantly\nimproving the accuracy of phase recognition. We perform extensive experiments\nto validate the effectiveness, and our SurgPLAN++ achieves remarkable\nperformance in both online and offline modes, which outperforms\nstate-of-the-art methods. The source code is available at\nhttps://github.com/franciszchen/SurgPLAN-Plus.",
      "tldr_zh": "这篇论文针对手术视频阶段识别的问题，提出了一种通用的SurgPLAN++网络，使用时间检测原则来实现在线和离线推断，从而解决现有方法缺乏全局上下文和预测不连贯的局限。SurgPLAN++通过阶段提案策略预测整个视频的阶段段，并在在线分析中采用数据增强技术（如镜像、中心复制和下采样）将流式视频扩展为伪完整视频，以生成高质量提案。对于离线分析，该网络在在线推理过程中持续优化预测，提高准确性。实验结果表明，SurgPLAN++在在线和离线模式下均优于现有方法，并在公开代码中提供实现细节。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "This work is accepted by IEEE ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12467v2",
      "published_date": "2024-09-19 05:08:33 UTC",
      "updated_date": "2025-02-13 19:57:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:40:34.340655"
    },
    {
      "arxiv_id": "2409.12454v1",
      "title": "FoME: A Foundation Model for EEG using Adaptive Temporal-Lateral Attention Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Enze Shi",
        "Kui Zhao",
        "Qilong Yuan",
        "Jiaqi Wang",
        "Huawen Hu",
        "Sigang Yu",
        "Shu Zhang"
      ],
      "abstract": "Electroencephalography (EEG) is a vital tool to measure and record brain\nactivity in neuroscience and clinical applications, yet its potential is\nconstrained by signal heterogeneity, low signal-to-noise ratios, and limited\nlabeled datasets. In this paper, we propose FoME (Foundation Model for EEG), a\nnovel approach using adaptive temporal-lateral attention scaling to address\nabove-mentioned challenges. FoME is pre-trained on a diverse 1.7TB dataset of\nscalp and intracranial EEG recordings, comprising 745M parameters trained for\n1,096k steps. Our model introduces two key innovations: a time-frequency fusion\nembedding technique and an adaptive time-lateral attention scaling (ATLAS)\nmechanism. These components synergistically capture complex temporal and\nspectral EEG dynamics, enabling FoME to adapt to varying patterns across\ndiverse data streams and facilitate robust multi-channel modeling. Evaluations\nacross four downstream tasks demonstrate FoME's superior performance in\nclassification and forecasting applications, consistently achieving\nstate-of-the-art results. To conclude, FoME establishes a new paradigm for EEG\nanalysis, offering a versatile foundation that advances brain-computer\ninterfaces, clinical diagnostics, and cognitive research across neuroscience\nand related fields. Our code will be available at\nhttps://github.com/1061413241/FoME.",
      "tldr_zh": "该论文提出 FoME，一种 EEG 基础模型，使用 adaptive temporal-lateral attention scaling (ATLAS) 来解决 EEG 信号的异质性、低信噪比和标签数据有限等问题。FoME 引入时间-频率融合嵌入技术(time-frequency fusion embedding)，并在 1.7TB 的 scalp 和 intracranial EEG 数据集上预训练，模型参数达 745M，经过 1,096k 训练步骤，以捕获复杂的时域和频域动态。实验结果显示，FoME 在四个下游任务中表现出色，在分类和预测应用中实现最先进性能。总体而言，该模型为 EEG 分析建立新范式，促进脑机接口(BCI)、临床诊断和认知研究的进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12454v1",
      "published_date": "2024-09-19 04:22:40 UTC",
      "updated_date": "2024-09-19 04:22:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:40:47.251731"
    },
    {
      "arxiv_id": "2410.02805v1",
      "title": "Trust-informed Decision-Making Through An Uncertainty-Aware Stacked Neural Networks Framework: Case Study in COVID-19 Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Hassan Gharoun",
        "Mohammad Sadegh Khorshidi",
        "Fang Chen",
        "Amir H. Gandomi"
      ],
      "abstract": "This study presents an uncertainty-aware stacked neural networks model for\nthe reliable classification of COVID-19 from radiological images. The model\naddresses the critical gap in uncertainty-aware modeling by focusing on\naccurately identifying confidently correct predictions while alerting users to\nconfidently incorrect and uncertain predictions, which can promote trust in\nautomated systems. The architecture integrates uncertainty quantification\nmethods, including Monte Carlo dropout and ensemble techniques, to enhance\npredictive reliability by assessing the certainty of diagnostic predictions.\nWithin a two-tier model framework, the tier one model generates initial\npredictions and associated uncertainties, which the second tier model uses to\nproduce a trust indicator alongside the diagnostic outcome. This dual-output\nmodel not only predicts COVID-19 cases but also provides a trust flag,\nindicating the reliability of each diagnosis and aiming to minimize the need\nfor retesting and expert verification. The effectiveness of this approach is\ndemonstrated through extensive experiments on the COVIDx CXR-4 dataset, showing\na novel approach in identifying and handling confidently incorrect cases and\nuncertain cases, thus enhancing the trustworthiness of automated diagnostics in\nclinical settings.",
      "tldr_zh": "这篇论文提出了一种不确定性感知的堆叠神经网络框架，用于从放射图像中可靠分类 COVID-19，从而提升对自动化系统的信任。该框架整合了 Monte Carlo dropout 和 ensemble 技术，通过两层模型结构：第一层生成初始预测和不确定性，第二层输出信任指标及诊断结果，以最小化重新测试和专家验证的需求。在 COVIDx CXR-4 数据集上的实验表明，该方法能有效识别自信错误和不确定案例，提高了临床设置中自动化诊断的可靠性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "68T07"
      ],
      "primary_category": "eess.IV",
      "comment": "15 pages, 7 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2410.02805v1",
      "published_date": "2024-09-19 04:20:12 UTC",
      "updated_date": "2024-09-19 04:20:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:40:57.695107"
    },
    {
      "arxiv_id": "2409.12450v1",
      "title": "Domain Generalization for Endoscopic Image Segmentation by Disentangling Style-Content Information and SuperPixel Consistency",
      "title_zh": "内窥镜图像分割的领域泛化：通过解耦风格-内容信息和超像素一致性",
      "authors": [
        "Mansoor Ali Teevno",
        "Rafael Martinez-Garcia-Pena",
        "Gilberto Ochoa-Ruiz",
        "Sharib Ali"
      ],
      "abstract": "Frequent monitoring is necessary to stratify individuals based on their\nlikelihood of developing gastrointestinal (GI) cancer precursors. In clinical\npractice, white-light imaging (WLI) and complementary modalities such as\nnarrow-band imaging (NBI) and fluorescence imaging are used to assess risk\nareas. However, conventional deep learning (DL) models show degraded\nperformance due to the domain gap when a model is trained on one modality and\ntested on a different one. In our earlier approach, we used a superpixel-based\nmethod referred to as \"SUPRA\" to effectively learn domain-invariant information\nusing color and space distances to generate groups of pixels. One of the main\nlimitations of this earlier work is that the aggregation does not exploit\nstructural information, making it suboptimal for segmentation tasks, especially\nfor polyps and heterogeneous color distributions. Therefore, in this work, we\npropose an approach for style-content disentanglement using instance\nnormalization and instance selective whitening (ISW) for improved domain\ngeneralization when combined with SUPRA. We evaluate our approach on two\ndatasets: EndoUDA Barrett's Esophagus and EndoUDA polyps, and compare its\nperformance with three state-of-the-art (SOTA) methods. Our findings\ndemonstrate a notable enhancement in performance compared to both baseline and\nSOTA methods across the target domain data. Specifically, our approach\nexhibited improvements of 14%, 10%, 8%, and 18% over the baseline and three\nSOTA methods on the polyp dataset. Additionally, it surpassed the second-best\nmethod (EndoUDA) on the Barrett's Esophagus dataset by nearly 2%.",
      "tldr_zh": "本论文针对内窥镜图像分割中的领域泛化问题，提出了一种通过分离风格-内容信息和超像素一致性的方法，以缓解不同成像模态（如白光成像和窄带成像）之间的领域差距。方法结合实例归一化和实例选择性白化 (ISW) 与之前的 SUPRA 框架，改进超像素聚合以更好地利用结构信息，从而提升分割任务的准确性。实验在 EndoUDA Barrett's Esophagus 和 EndoUDA polyps 数据集上评估，结果显示该方法比基线和三个 SOTA 方法提高了14%至18%的性能，并在 Barrett's Esophagus 数据集上超过了第二佳方法近2%。这为胃肠道癌症前体监测提供了更可靠的深度学习解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12450v1",
      "published_date": "2024-09-19 04:10:04 UTC",
      "updated_date": "2024-09-19 04:10:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:41:11.639882"
    },
    {
      "arxiv_id": "2409.12447v2",
      "title": "Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Jenny T. Liang",
        "Melissa Lin",
        "Nikitha Rao",
        "Brad A. Myers"
      ],
      "abstract": "Generative pre-trained models power intelligent software features used by\nmillions of users controlled by developer-written natural language prompts.\nDespite the impact of prompt-powered software, little is known about its\ndevelopment process and its relationship to programming. In this work, we argue\nthat some prompts are programs and that the development of prompts is a\ndistinct phenomenon in programming known as \"prompt programming\". We develop an\nunderstanding of prompt programming using Straussian grounded theory through\ninterviews with 20 developers engaged in prompt development across a variety of\ncontexts, models, domains, and prompt structures. We contribute 15 observations\nto form a preliminary understanding of current prompt programming practices.\nFor example, rather than building mental models of code, prompt programmers\ndevelop mental models of the foundation model (FM)'s behavior on the prompt by\ninteracting with the FM. While prior research shows that experts have\nwell-formed mental models, we find that prompt programmers who have developed\ndozens of prompts still struggle to develop reliable mental models. Our\nobservations show that prompt programming differs from traditional software\ndevelopment, motivating the creation of prompt programming tools and providing\nimplications for software engineering stakeholders.",
      "tldr_zh": "本研究认为，开发者编写的自然语言提示可以视为程序，并引入“prompt programming”作为一种独特的编程现象，以理解其开发过程与传统编程的关系。作者通过对20名从事提示开发的开发者进行访谈，并采用Straussian grounded theory方法，贡献了15个观察结果，例如提示程序员通过与foundation model (FM)互动构建模型行为心理模型，而非代码心理模型。研究发现，即使经验丰富的提示程序员仍难以形成可靠的心理模型，且prompt programming与传统软件开发存在显著差异。这为软件工程利益相关者提供了启示，推动开发专属prompt programming工具的必要性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted to FSE'25",
      "pdf_url": "http://arxiv.org/pdf/2409.12447v2",
      "published_date": "2024-09-19 03:55:49 UTC",
      "updated_date": "2025-04-25 03:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:41:21.074906"
    },
    {
      "arxiv_id": "2409.12446v2",
      "title": "Neural Networks Generalize on Low Complexity Data",
      "title_zh": "神经网络在低复杂度数据上实现泛化",
      "authors": [
        "Sourav Chatterjee",
        "Timothy Sudijono"
      ],
      "abstract": "We show that feedforward neural networks with ReLU activation generalize on\nlow complexity data, suitably defined. Given i.i.d. data generated from a\nsimple programming language, the minimum description length (MDL) feedforward\nneural network which interpolates the data generalizes with high probability.\nWe define this simple programming language, along with a notion of description\nlength of such networks. We provide several examples on basic computational\ntasks, such as checking primality of a natural number, and more. For primality\ntesting, our theorem shows the following. Suppose that we draw an i.i.d. sample\nof $\\Theta(N^{\\delta}\\ln N)$ numbers uniformly at random from $1$ to $N$, where\n$\\delta\\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and\n$0$ if it is not. Then with high probability, the MDL network fitted to this\ndata accurately answers whether a newly drawn number between $1$ and $N$ is a\nprime or not, with test error $\\leq O(N^{-\\delta})$. Note that the network is\nnot designed to detect primes; minimum description learning discovers a network\nwhich does so.",
      "tldr_zh": "本研究证明了使用 ReLU activation 的 feedforward neural networks 在低复杂度数据上具有泛化能力，特别是当数据从一个简单编程语言生成时，通过 minimum description length (MDL) 原则拟合的网络能够以高概率准确插值并泛化 i.i.d. 数据。论文定义了这种简单编程语言和网络的描述长度，并通过示例任务（如检查自然数的 primality testing）进行了验证。例如，在素性测试中，从 1 到 N 随机抽取 Θ(N^δ ln N) 个样本后，MDL 网络能以高概率预测新数是否为素数，测试错误率低于 O(N^{-δ})，展示了网络通过学习发现任务模式而非预设计的能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages. V2: Further references added",
      "pdf_url": "http://arxiv.org/pdf/2409.12446v2",
      "published_date": "2024-09-19 03:54:49 UTC",
      "updated_date": "2024-10-29 03:53:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:41:33.995862"
    },
    {
      "arxiv_id": "2409.12444v3",
      "title": "A Lightweight and Real-Time Binaural Speech Enhancement Model with Spatial Cues Preservation",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyuan Wang",
        "Jie Zhang",
        "Shihao Chen",
        "Miao Sun"
      ],
      "abstract": "Binaural speech enhancement (BSE) aims to jointly improve the speech quality\nand intelligibility of noisy signals received by hearing devices and preserve\nthe spatial cues of the target for natural listening. Existing methods often\nsuffer from the compromise between noise reduction (NR) capacity and spatial\ncues preservation (SCP) accuracy and a high computational demand in complex\nacoustic scenes. In this work, we present a learning-based lightweight binaural\ncomplex convolutional network (LBCCN), which excels in NR by filtering\nlow-frequency bands and keeping the rest. Additionally, our approach explicitly\nincorporates the estimation of interchannel relative acoustic transfer function\nto ensure the spatial cues fidelity and speech clarity. Results show that the\nproposed LBCCN can achieve a comparable NR performance to state-of-the-art\nmethods under fixed-speaker conditions, but with a much lower computational\ncost and a certain degree of SCP capability. The reproducible code and audio\nexamples are available at https://github.com/jywanng/LBCCN.",
      "tldr_zh": "本文提出了一种轻量级实时双耳语音增强（Binaural Speech Enhancement, BSE）模型，即 LBCCN，旨在提升噪音信号的语音质量和可懂度，同时精确保留 spatial cues 以实现自然聆听。LBCCN 通过过滤低频带进行噪声减少（NR），并明确估计 interchannel relative acoustic transfer function 来确保空间线索的保真度和语音清晰度。与现有方法相比，该模型在固定扬声器条件下实现了与最先进方法相当的 NR 性能，但计算需求显著降低，并具备一定的 spatial cues preservation (SCP) 能力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12444v3",
      "published_date": "2024-09-19 03:52:50 UTC",
      "updated_date": "2025-01-08 07:19:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:41:46.192907"
    },
    {
      "arxiv_id": "2409.12440v1",
      "title": "Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Lian",
        "Nishant Baglodi",
        "Christopher J. MacLellan"
      ],
      "abstract": "This paper introduces Cobweb4L, a novel approach for efficient language model\nlearning that supports masked word prediction. The approach builds on Cobweb,\nan incremental system that learns a hierarchy of probabilistic concepts. Each\nconcept stores the frequencies of words that appear in instances tagged with\nthat concept label. The system utilizes an attribute value representation to\nencode words and their surrounding context into instances. Cobweb4L uses the\ninformation theoretic variant of category utility and a new performance\nmechanism that leverages multiple concepts to generate predictions. We\ndemonstrate that with these extensions it significantly outperforms prior\nCobweb performance mechanisms that use only a single node to generate\npredictions. Further, we demonstrate that Cobweb4L learns rapidly and achieves\nperformance comparable to and even superior to Word2Vec. Next, we show that\nCobweb4L and Word2Vec outperform BERT in the same task with less training data.\nFinally, we discuss future work to make our conclusions more robust and\ninclusive.",
      "tldr_zh": "这篇论文引入了 Cobweb4L，一种高效的增量学习方法，支持 masked word prediction，通过构建概率概念层次结构来存储单词频率和上下文信息。Cobweb4L 采用信息理论变体的类别效用和新性能机制，利用多个概念生成预测，从而显著优于以往仅依赖单节点的 Cobweb 机制。实验结果显示，Cobweb4L 在少量训练数据下，其性能可与 Word2Vec 相当甚至优于，且与 Word2Vec 一起在相同任务中超越 BERT；未来工作将进一步提升结论的稳健性和包容性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by the Eleventh Annual Conference on Advances in Cognitive\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2409.12440v1",
      "published_date": "2024-09-19 03:48:31 UTC",
      "updated_date": "2024-09-19 03:48:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:41:58.509047"
    },
    {
      "arxiv_id": "2410.03559v1",
      "title": "Optimizing food taste sensory evaluation through neural network-based taste electroencephalogram channel selection",
      "title_zh": "通过基于神经网络的味觉脑电图通道选择优化食物味觉感官评价",
      "authors": [
        "Xiuxin Xia",
        "Qun Wang",
        "He Wang",
        "Chenrui Liu",
        "Pengwei Li",
        "Yan Shi",
        "Hong Men"
      ],
      "abstract": "The taste electroencephalogram (EEG) evoked by the taste stimulation can\nreflect different brain patterns and be used in applications such as sensory\nevaluation of food. However, considering the computational cost and efficiency,\nEEG data with many channels has to face the critical issue of channel\nselection. This paper proposed a channel selection method called class\nactivation mapping with attention (CAM-Attention). The CAM-Attention method\ncombined a convolutional neural network with channel and spatial attention\n(CNN-CSA) model with a gradient-weighted class activation mapping (Grad-CAM)\nmodel. The CNN-CSA model exploited key features in EEG data by attention\nmechanism, and the Grad-CAM model effectively realized the visualization of\nfeature regions. Then, channel selection was effectively implemented based on\nfeature regions. Finally, the CAM-Attention method reduced the computational\nburden of taste EEG recognition and effectively distinguished the four tastes.\nIn short, it has excellent recognition performance and provides effective\ntechnical support for taste sensory evaluation.",
      "tldr_zh": "这篇论文针对味觉 EEG 数据通道过多导致的计算成本和效率问题，提出了一种名为 CAM-Attention 的通道选择方法。该方法结合 CNN-CSA 模型（利用注意力机制提取 EEG 关键特征）和 Grad-CAM 模型（实现特征区域可视化），从而有效筛选通道并减少计算负担。实验结果表明，该方法能准确区分四种味道，具有优秀的识别性能，为食品味觉感官评估提供可靠的技术支持。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "primary_category": "eess.SP",
      "comment": "33 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2410.03559v1",
      "published_date": "2024-09-19 03:36:22 UTC",
      "updated_date": "2024-09-19 03:36:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:42:09.230566"
    },
    {
      "arxiv_id": "2409.12431v4",
      "title": "FlexiTex: Enhancing Texture Generation with Visual Guidance",
      "title_zh": "FlexiTex：通过视觉引导增强纹理生成",
      "authors": [
        "DaDong Jiang",
        "Xianghui Yang",
        "Zibo Zhao",
        "Sheng Zhang",
        "Jiaao Yu",
        "Zeqiang Lai",
        "Shaoxiong Yang",
        "Chunchao Guo",
        "Xiaobo Zhou",
        "Zhihui Ke"
      ],
      "abstract": "Recent texture generation methods achieve impressive results due to the\npowerful generative prior they leverage from large-scale text-to-image\ndiffusion models. However, abstract textual prompts are limited in providing\nglobal textural or shape information, which results in the texture generation\nmethods producing blurry or inconsistent patterns. To tackle this, we present\nFlexiTex, embedding rich information via visual guidance to generate a\nhigh-quality texture. The core of FlexiTex is the Visual Guidance Enhancement\nmodule, which incorporates more specific information from visual guidance to\nreduce ambiguity in the text prompt and preserve high-frequency details. To\nfurther enhance the visual guidance, we introduce a Direction-Aware Adaptation\nmodule that automatically designs direction prompts based on different camera\nposes, avoiding the Janus problem and maintaining semantically global\nconsistency. Benefiting from the visual guidance, FlexiTex produces\nquantitatively and qualitatively sound results, demonstrating its potential to\nadvance texture generation for real-world applications.",
      "tldr_zh": "该论文指出，现有的纹理生成方法依赖文本到图像扩散模型，但抽象文本提示往往导致生成的纹理模糊或不一致。论文提出 FlexiTex，通过视觉指导嵌入更多信息来提升生成质量，其核心组件包括 Visual Guidance Enhancement 模块，用于减少文本提示的模糊性并保留高频细节，以及 Direction-Aware Adaptation 模块，该模块根据不同相机姿态自动设计方向提示，以避免 Janus problem 并保持语义全局一致性。实验结果显示，FlexiTex 在定量和定性方面均表现出色，具有推进纹理生成在实际应用中的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI 2025, Project Page:\n  https://patrickddj.github.io/FlexiTex/",
      "pdf_url": "http://arxiv.org/pdf/2409.12431v4",
      "published_date": "2024-09-19 03:24:22 UTC",
      "updated_date": "2024-12-27 12:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:42:21.733082"
    },
    {
      "arxiv_id": "2409.12428v1",
      "title": "Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift",
      "title_zh": "翻译失败",
      "authors": [
        "Oscar Blessed Deho",
        "Michael Bewong",
        "Selasi Kwashie",
        "Jiuyong Li",
        "Jixue Liu",
        "Lin Liu",
        "Srecko Joksimovic"
      ],
      "abstract": "Over the last few decades, machine learning (ML) applications have grown\nexponentially, yielding several benefits to society. However, these benefits\nare tempered with concerns of discriminatory behaviours exhibited by ML models.\nIn this regard, fairness in machine learning has emerged as a priority research\narea. Consequently, several fairness metrics and algorithms have been developed\nto mitigate against discriminatory behaviours that ML models may possess. Yet\nstill, very little attention has been paid to the problem of naturally\noccurring changes in data patterns (\\textit{aka} data distributional drift),\nand its impact on fairness algorithms and metrics. In this work, we study this\nproblem comprehensively by analyzing 4 fairness-unaware baseline algorithms and\n7 fairness-aware algorithms, carefully curated to cover the breadth of its\ntypology, across 5 datasets including public and proprietary data, and\nevaluated them using 3 predictive performance and 10 fairness metrics. In doing\nso, we show that (1) data distributional drift is not a trivial occurrence, and\nin several cases can lead to serious deterioration of fairness in so-called\nfair models; (2) contrary to some existing literature, the size and direction\nof data distributional drift is not correlated to the resulting size and\ndirection of unfairness; and (3) choice of, and training of fairness algorithms\nis impacted by the effect of data distributional drift which is largely ignored\nin the literature. Emanating from our findings, we synthesize several policy\nimplications of data distributional drift on fairness algorithms that can be\nvery relevant to stakeholders and practitioners.",
      "tldr_zh": "该论文通过数据分布漂移（covariate drift）的视角，对机器学习公平性进行比较评估，研究了4个公平无关基线算法和7个公平感知算法在5个数据集上的表现，使用3个预测性能指标和10个公平指标进行评估。主要发现包括：数据分布漂移可能导致公平模型的公平性严重恶化，其大小和方向与不公平性结果无关，且会影响公平算法的选择和训练。与此相关，该研究总结了数据分布漂移对公平算法的政策影响，为从业者和利益相关者提供重要启示。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12428v1",
      "published_date": "2024-09-19 03:18:12 UTC",
      "updated_date": "2024-09-19 03:18:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:42:43.759918"
    },
    {
      "arxiv_id": "2409.18996v1",
      "title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shengsheng Qian",
        "Zuyi Zhou",
        "Dizhan Xue",
        "Bing Wang",
        "Changsheng Xu"
      ],
      "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and\ndrawing inferences across divergent sensory modalities, is increasingly\nrecognized as a crucial capability in the progression toward more sophisticated\nand anthropomorphic artificial intelligence systems. Large Language Models\n(LLMs) represent a class of AI algorithms specifically engineered to parse,\nproduce, and engage with human language on an extensive scale. The recent trend\nof deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches\nfor enhancing their effectiveness. This survey offers a nuanced exposition of\ncurrent methodologies applied in CMR using LLMs, classifying these into a\ndetailed three-tiered taxonomy. Moreover, the survey delves into the principal\ndesign strategies and operational techniques of prototypical models within this\ndomain. Additionally, it articulates the prevailing challenges associated with\nthe integration of LLMs in CMR and identifies prospective research directions.\nTo sum up, this survey endeavors to expedite progress within this burgeoning\nfield by endowing scholars with a holistic and detailed vista, showcasing the\nvanguard of current research whilst pinpointing potential avenues for\nadvancement. An associated GitHub repository that collects the relevant papers\ncan be found at\nhttps://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs",
      "tldr_zh": "这篇调查论文探讨了使用 Large Language Models (LLMs) 进行 Cross-modal reasoning (CMR) 的方法与进展，强调 LLMs 如何整合不同感官模态进行推理，以推动更高级的人工智能系统。论文采用三层分类法对当前 CMR 方法进行细致分类，并分析典型模型的设计策略和操作技术。研究指出了 LLMs 在 CMR 中的主要挑战，如模态整合难题，并提出了未来研究方向，以加速该领域的创新和发展。该调查还提供了一个 GitHub 仓库，收集相关论文以供学者参考。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM",
        "A.1"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.18996v1",
      "published_date": "2024-09-19 02:51:54 UTC",
      "updated_date": "2024-09-19 02:51:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:42:45.727934"
    },
    {
      "arxiv_id": "2409.15370v2",
      "title": "Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alexius Wadell",
        "Anoushka Bhutani",
        "Venkatasubramanian Viswanathan"
      ],
      "abstract": "Text-based foundation models have become an important part of scientific\ndiscovery, with molecular foundation models accelerating advancements in\nmolecular design and materials science. However, existing models are\nconstrained by closed-vocabulary tokenizers which capture only a fraction of\nmolecular space. In this work, we systematically evaluate thirty tokenizers,\nincluding 19 chemistry-specific ones, for their coverage of the SMILES\nmolecular representation language, revealing significant gaps. To assess the\nimpact of tokenizer choice, we introduce n-gram language models as a low-cost\nproxy and validate their effectiveness by training and fine-tuning 18\nRoBERTa-style encoders for molecular property prediction. To overcome the\nlimitations of existing tokenizers, we propose two new tokenizers -- Smirk and\nSmirk-GPE -- with full coverage of the OpenSMILES specification. Our results\nhighlight the need for open-vocabulary modeling and chemically diverse\nbenchmarks in cheminformatics. The proposed tokenizer framework systematically\nintegrates nuclear, electronic, and geometric degrees of freedom; this\nfacilitates applications in pharmacology, agriculture, biology, and energy\nstorage.",
      "tldr_zh": "本研究发现，现有的分子基础模型受限于闭合词汇分词器，无法全面覆盖SMILES分子表示语言，从而限制了分子设计和材料科学的进展。作者评估了30个分词器（包括19个化学专用分词器），并使用n-gram language models作为低成本代理，训练和微调了18个RoBERTa-style编码器来预测分子属性。针对这些局限，论文提出Smirk和Smirk-GPE两个新分词器，它们实现了对OpenSMILES规范的完整覆盖。结果表明，这种开放词汇建模框架能整合核、电子和几何自由度，提升模型在药学、农业、生物学和能源存储等领域的应用潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.15370v2",
      "published_date": "2024-09-19 02:36:04 UTC",
      "updated_date": "2025-02-07 18:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:42:57.797560"
    },
    {
      "arxiv_id": "2410.02804v1",
      "title": "Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Fan",
        "Hongyu Yuan",
        "Haolin Zuo",
        "Rui Liu",
        "Guanglai Gao"
      ],
      "abstract": "Multimodal emotion recognition utilizes complete multimodal information and\nrobust multimodal joint representation to gain high performance. However, the\nideal condition of full modality integrity is often not applicable in reality\nand there always appears the situation that some modalities are missing. For\nexample, video, audio, or text data is missing due to sensor failure or network\nbandwidth problems, which presents a great challenge to MER research.\nTraditional methods extract useful information from the complete modalities and\nreconstruct the missing modalities to learn robust multimodal joint\nrepresentation. These methods have laid a solid foundation for research in this\nfield, and to a certain extent, alleviated the difficulty of multimodal emotion\nrecognition under missing modalities. However, relying solely on internal\nreconstruction and multimodal joint learning has its limitations, especially\nwhen the missing information is critical for emotion recognition. To address\nthis challenge, we propose a novel framework of Retrieval Augment for Missing\nModality Multimodal Emotion Recognition (RAMER), which introduces similar\nmultimodal emotion data to enhance the performance of emotion recognition under\nmissing modalities. By leveraging databases, that contain related multimodal\nemotion data, we can retrieve similar multimodal emotion information to fill in\nthe gaps left by missing modalities. Various experimental results demonstrate\nthat our framework is superior to existing state-of-the-art approaches in\nmissing modality MER tasks. Our whole project is publicly available on\nhttps://github.com/WooyoohL/Retrieval_Augment_MER.",
      "tldr_zh": "这篇论文针对多模态情感识别（Multimodal Emotion Recognition）中模态缺失（如视频、音频或文本）的挑战，提出了一种新框架 RAMER（Retrieval Augment for Missing Modality Multimodal Emotion Recognition）。RAMER 通过检索增强（Retrieval Augment）方法，从数据库中获取类似的多模态情感数据来补充缺失信息，从而学习更鲁棒的多模态联合表示。实验结果显示，该框架在缺失模态任务中优于现有最先进的方法，并已在 GitHub 上公开项目代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under reviewing",
      "pdf_url": "http://arxiv.org/pdf/2410.02804v1",
      "published_date": "2024-09-19 02:31:12 UTC",
      "updated_date": "2024-09-19 02:31:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:43:09.518882"
    },
    {
      "arxiv_id": "2409.12415v1",
      "title": "Multichannel-to-Multichannel Target Sound Extraction Using Direction and Timestamp Clues",
      "title_zh": "基于方向和时间戳线索的多通道到多通道目标声音提取",
      "authors": [
        "Dayun Choi",
        "Jung-Woo Choi"
      ],
      "abstract": "We propose a multichannel-to-multichannel target sound extraction (M2M-TSE)\nframework for separating multichannel target signals from a multichannel\nmixture of sound sources. Target sound extraction (TSE) isolates a specific\ntarget signal using user-provided clues, typically focusing on single-channel\nextraction with class labels or temporal activation maps. However, to preserve\nand utilize spatial information in multichannel audio signals, it is essential\nto extract multichannel signals of a target sound source. Moreover, the clue\nfor extraction can also include spatial or temporal cues like\ndirection-of-arrival (DoA) or timestamps of source activation. To address these\nchallenges, we present an M2M framework that extracts a multichannel sound\nsignal based on spatio-temporal clues. We demonstrate that our\ntransformer-based architecture can successively accomplish the M2M-TSE task for\nmultichannel signals synthesized from audio signals of diverse classes in\ndifferent room environments. Furthermore, we show that the multichannel\nextraction task introduces sufficient inductive bias in the DNN, allowing it to\ndirectly handle DoA clues without utilizing hand-crafted spatial features.",
      "tldr_zh": "本研究提出了一种多通道到多通道目标声音提取（M2M-TSE）框架，用于从多通道混合声音中分离出目标信号，超越传统单通道提取方法。框架利用到达方向（DoA）和时间戳等时空线索，结合基于Transformer的架构，实现对不同房间环境和音频类别的多通道信号提取。实验结果表明，该方法为深度神经网络（DNN）提供了足够的归纳偏差，使其能直接处理DoA线索，而无需手工制作的空间特征，从而提升了提取任务的准确性和效率。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.12415v1",
      "published_date": "2024-09-19 02:30:49 UTC",
      "updated_date": "2024-09-19 02:30:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:43:21.052780"
    },
    {
      "arxiv_id": "2409.12409v1",
      "title": "LMT-Net: Lane Model Transformer Network for Automated HD Mapping from Sparse Vehicle Observations",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Mink",
        "Thomas Monninger",
        "Steffen Staab"
      ],
      "abstract": "In autonomous driving, High Definition (HD) maps provide a complete lane\nmodel that is not limited by sensor range and occlusions. However, the\ngeneration and upkeep of HD maps involves periodic data collection and human\nannotations, limiting scalability. To address this, we investigate automating\nthe lane model generation and the use of sparse vehicle observations instead of\ndense sensor measurements. For our approach, a pre-processing step generates\npolylines by aligning and aggregating observed lane boundaries. Aligned driven\ntraces are used as starting points for predicting lane pairs defined by the\nleft and right boundary points. We propose Lane Model Transformer Network\n(LMT-Net), an encoder-decoder neural network architecture that performs\npolyline encoding and predicts lane pairs and their connectivity. A lane graph\nis formed by using predicted lane pairs as nodes and predicted lane\nconnectivity as edges. We evaluate the performance of LMT-Net on an internal\ndataset that consists of multiple vehicle observations as well as human\nannotations as Ground Truth (GT). The evaluation shows promising results and\ndemonstrates superior performance compared to the implemented baseline on both\nhighway and non-highway Operational Design Domain (ODD).",
      "tldr_zh": "本文提出 LMT-Net，一种基于 Transformer 网络的编码器-解码器架构，用于从稀疏车辆观察自动生成高精度 HD 地图的车道模型，解决传统方法依赖密集传感器和人工标注的可扩展性问题。该方法包括预处理步骤对齐并聚合观察到的车道边界生成多线条，并利用对齐驾驶轨迹作为起点预测车道对及其连通性，从而形成车道图。实验结果显示，LMT-Net 在内部数据集上表现优于基线模型，尤其在高速公路和非高速公路场景中，实现了更高的准确性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for 2024 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.12409v1",
      "published_date": "2024-09-19 02:14:35 UTC",
      "updated_date": "2024-09-19 02:14:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:43:34.270884"
    },
    {
      "arxiv_id": "2409.12405v1",
      "title": "On the Effectiveness of LLMs for Manual Test Verifications",
      "title_zh": "翻译失败",
      "authors": [
        "Myron David Lucena Campos Peixoto",
        "Davy de Medeiros Baia",
        "Nathalia Nascimento",
        "Paulo Alencar",
        "Baldoino Fonseca",
        "Márcio Ribeiro"
      ],
      "abstract": "Background: Manual testing is vital for detecting issues missed by automated\ntests, but specifying accurate verifications is challenging. Aims: This study\naims to explore the use of Large Language Models (LLMs) to produce\nverifications for manual tests. Method: We conducted two independent and\ncomplementary exploratory studies. The first study involved using 2\nclosed-source and 6 open-source LLMs to generate verifications for manual test\nsteps and evaluate their similarity to original verifications. The second study\ninvolved recruiting software testing professionals to assess their perception\nand agreement with the generated verifications compared to the original ones.\nResults: The open-source models Mistral-7B and Phi-3-mini-4k demonstrated\neffectiveness and consistency comparable to closed-source models like\nGemini-1.5-flash and GPT-3.5-turbo in generating manual test verifications.\nHowever, the agreement level among professional testers was slightly above 40%,\nindicating both promise and room for improvement. While some LLM-generated\nverifications were considered better than the originals, there were also\nconcerns about AI hallucinations, where verifications significantly deviated\nfrom expectations. Conclusion: We contributed by generating a dataset of 37,040\ntest verifications using 8 different LLMs. Although the models show potential,\nthe relatively modest 40% agreement level highlights the need for further\nrefinement. Enhancing the accuracy, relevance, and clarity of the generated\nverifications is crucial to ensure greater reliability in real-world testing\nscenarios.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在手动测试验证中的有效性，旨在解决指定准确验证的挑战。研究采用两个独立探索性研究：首先，使用2个闭源模型（如Gemini-1.5-flash和GPT-3.5-turbo）和6个开源模型（如Mistral-7B和Phi-3-mini-4k）生成验证，并评估其与原验证的相似度；其次，招募软件测试专业人员评估生成的验证。结果显示，开源模型在生成验证方面与闭源模型表现相当，但专业人员的同意率仅略高于40%，同时存在AI幻觉问题，导致部分验证偏离预期。总体而言，该研究贡献了一个包含37,040个测试验证的数据集，并强调需要进一步改进LLMs的准确性、相关性和清晰度，以提升其在实际测试场景中的可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.12405v1",
      "published_date": "2024-09-19 02:03:04 UTC",
      "updated_date": "2024-09-19 02:03:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:43:46.771717"
    },
    {
      "arxiv_id": "2409.12403v1",
      "title": "Preference Alignment Improves Language Model-Based TTS",
      "title_zh": "翻译失败",
      "authors": [
        "Jinchuan Tian",
        "Chunlei Zhang",
        "Jiatong Shi",
        "Hao Zhang",
        "Jianwei Yu",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "abstract": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM)-based systems offer competitive performance to their counterparts. Further\noptimization can be achieved through preference alignment algorithms, which\nadjust LMs to align with the preferences of reward models, enhancing the\ndesirability of the generated content. This study presents a thorough empirical\nevaluation of how preference alignment algorithms, particularly Direct\nPreference Optimization (DPO), enhance LM-based TTS. With a 1.15B parameter\nLM-based TTS model, we demonstrate that preference alignment consistently\nimproves intelligibility, speaker similarity, and proxy subjective evaluation\nscores, with the latter two metrics surpassing even human speech in certain\nevaluations. We also show preference alignment is applicable to low-resource\nscenarios and effectively generalized to out-of-domain applications.",
      "tldr_zh": "本研究探讨了偏好对齐算法，特别是Direct Preference Optimization (DPO)，如何提升基于语言模型(LM)的文本到语音(TTS)系统性能。通过对一个1.15B参数的LM-based TTS模型进行实证评估，结果显示偏好对齐算法显著提高了intelligibility（可理解性）、speaker similarity（说话者相似性）以及代理主观评估分数，在某些情况下甚至超过了人类语音水平。该方法还适用于低资源场景，并能有效泛化到领域外应用，从而为TTS优化提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12403v1",
      "published_date": "2024-09-19 01:58:19 UTC",
      "updated_date": "2024-09-19 01:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:43:57.782818"
    },
    {
      "arxiv_id": "2409.12397v2",
      "title": "Learning to Coordinate without Communication under Incomplete Information",
      "title_zh": "在不完全信息下学习无通信协调",
      "authors": [
        "Shenghui Chen",
        "Shufang Zhu",
        "Giuseppe De Giacomo",
        "Ufuk Topcu"
      ],
      "abstract": "Achieving seamless coordination in cooperative games is a crucial challenge\nin artificial intelligence, particularly when players operate under incomplete\ninformation. A common strategy to mitigate this information asymmetry involves\nleveraging explicit communication. However, direct (verbal) communication is\nnot always feasible due to factors such as transmission loss. Leveraging the\ngame Gnomes at Night, we explore how effective coordination can be achieved\nwithout verbal communication, relying solely on observing each other's actions.\nWe demonstrate how an autonomous agent can learn to cooperate by interpreting\nits partner's sequences of actions, which are used to hint at its intents. Our\napproach generates a non-Markovian strategy for the agent by learning a\ndeterministic finite automaton for each possible action and integrating these\nautomata into a finite-state transducer. Experimental results in a Gnomes at\nNight testbed show that, even without direct communication, one can learn\neffective cooperation strategies. Such strategies achieve significantly higher\nsuccess rates and require fewer steps to complete the game compared to\nuncoordinated ones, and perform almost as well as in the case direct\ncommunication is allowed.",
      "tldr_zh": "该研究探讨了在不完整信息(incomplete information)条件下，实现人工智能合作游戏中无缝协调的挑战，而无需依赖显式通信。作者提出了一种方法，让自主代理通过观察伙伴的行动序列来推断意图，并生成非马尔可夫(non-Markovian)策略，具体是通过为每个可能动作学习确定性有限自动机(deterministic finite automaton)，并整合成有限状态转录器(finite-state transducer)。在Gnomes at Night游戏的实验中，这种策略显著提高了成功率和效率，与无协调策略相比，成功率提升明显，且几乎与允许直接通信的情况相当。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12397v2",
      "published_date": "2024-09-19 01:41:41 UTC",
      "updated_date": "2025-02-05 19:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:44:09.780703"
    },
    {
      "arxiv_id": "2409.12396v1",
      "title": "ARTAI: An Evaluation Platform to Assess Societal Risk of Recommender Algorithms",
      "title_zh": "翻译失败",
      "authors": [
        "Qin Ruan",
        "Jin Xu",
        "Ruihai Dong",
        "Arjumand Younus",
        "Tai Tan Mai",
        "Barry O'Sullivan",
        "Susan Leavy"
      ],
      "abstract": "Societal risk emanating from how recommender algorithms disseminate content\nonline is now well documented. Emergent regulation aims to mitigate this risk\nthrough ethical audits and enabling new research on the social impact of\nalgorithms. However, there is currently a need for tools and methods that\nenable such evaluation. This paper presents ARTAI, an evaluation environment\nthat enables large-scale assessments of recommender algorithms to identify\nharmful patterns in how content is distributed online and enables the\nimplementation of new regulatory requirements for increased transparency in\nrecommender systems.",
      "tldr_zh": "该论文引入了ARTAI，这是一个评估平台，用于评估推荐算法(recommender algorithms)对社会风险(societal risk)的潜在影响。ARTAI通过大规模评估环境，帮助识别在线内容分发的有害模式，并支持新法规要求以提升推荐系统的透明度。相比现有工具，该平台填补了道德审计和算法社会影响研究的空白，促进更负责任的算法设计和发展。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "H.3.3; I.2.7; I.5.1"
      ],
      "primary_category": "cs.CY",
      "comment": "3 pages, 1 figure, accepted at FAccTRec 2024 Workshop, RecSys 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.12396v1",
      "published_date": "2024-09-19 01:39:51 UTC",
      "updated_date": "2024-09-19 01:39:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:44:20.643826"
    },
    {
      "arxiv_id": "2409.12394v1",
      "title": "ITPatch: An Invisible and Triggered Physical Adversarial Patch against Traffic Sign Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Yuan",
        "Hongwei Li",
        "Xingshuo Han",
        "Guowen Xu",
        "Wenbo Jiang",
        "Tao Ni",
        "Qingchuan Zhao",
        "Yuguang Fang"
      ],
      "abstract": "Physical adversarial patches have emerged as a key adversarial attack to\ncause misclassification of traffic sign recognition (TSR) systems in the real\nworld. However, existing adversarial patches have poor stealthiness and attack\nall vehicles indiscriminately once deployed. In this paper, we introduce an\ninvisible and triggered physical adversarial patch (ITPatch) with a novel\nattack vector, i.e., fluorescent ink, to advance the state-of-the-art. It\napplies carefully designed fluorescent perturbations to a target sign, an\nattacker can later trigger a fluorescent effect using invisible ultraviolet\nlight, causing the TSR system to misclassify the sign and potentially resulting\nin traffic accidents. We conducted a comprehensive evaluation to investigate\nthe effectiveness of ITPatch, which shows a success rate of 98.31% in low-light\nconditions. Furthermore, our attack successfully bypasses five popular defenses\nand achieves a success rate of 96.72%.",
      "tldr_zh": "本文提出了一种隐形且可触发的物理对抗性补丁（ITPatch），利用荧光墨水作为新攻击向量，针对交通标志识别（TSR）系统进行攻击。该方法通过在目标标志上施加精心设计的荧光扰动，并在需要时使用紫外光触发，成功导致系统误分类，可能引发交通事故。实验结果显示，在低光条件下攻击成功率达98.31%，并能绕过五种流行防御，成功率达96.72%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12394v1",
      "published_date": "2024-09-19 01:36:54 UTC",
      "updated_date": "2024-09-19 01:36:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:44:33.948931"
    },
    {
      "arxiv_id": "2409.12388v2",
      "title": "Disentangling Speakers in Multi-Talker Speech Recognition with Speaker-Aware CTC",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawen Kang",
        "Lingwei Meng",
        "Mingyu Cui",
        "Yuejiao Wang",
        "Xixin Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "abstract": "Multi-talker speech recognition (MTASR) faces unique challenges in\ndisentangling and transcribing overlapping speech. To address these challenges,\nthis paper investigates the role of Connectionist Temporal Classification (CTC)\nin speaker disentanglement when incorporated with Serialized Output Training\n(SOT) for MTASR. Our visualization reveals that CTC guides the encoder to\nrepresent different speakers in distinct temporal regions of acoustic\nembeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC\n(SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a\ntailored CTC variant for multi-talker scenarios, it explicitly models speaker\ndisentanglement by constraining the encoder to represent different speakers'\ntokens at specific time frames. When integrated with SOT, the SOT-SACTC model\nconsistently outperforms standard SOT-CTC across various degrees of speech\noverlap. Specifically, we observe relative word error rate reductions of 10%\noverall and 15% on low-overlap speech. This work represents an initial\nexploration of CTC-based enhancements for MTASR tasks, offering a new\nperspective on speaker disentanglement in multi-talker speech recognition. The\ncode is available at https://github.com/kjw11/Speaker-Aware-CTC.",
      "tldr_zh": "本研究针对多说话者语音识别 (MTASR) 中重叠语音的分离和转录挑战，探讨了 Connectionist Temporal Classification (CTC) 的作用，并提出了一种新型训练目标 Speaker-Aware CTC (SACTC)，基于 Bayes risk CTC 框架来显式建模说话者分离。SACTC 通过约束编码器在特定时间帧表示不同说话者的标记，并与 Serialized Output Training (SOT) 结合，引导模型在声学嵌入中更有效地区分说话者。实验结果显示，SOT-SACTC 模型在各种语音重叠度下优于标准 SOT-CTC，实现了整体相对词错误率降低 10% 以及低重叠语音降低 15%，为 MTASR 任务提供了新的视角和代码实现。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12388v2",
      "published_date": "2024-09-19 01:26:33 UTC",
      "updated_date": "2025-01-03 12:36:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:44:46.625455"
    },
    {
      "arxiv_id": "2409.12386v2",
      "title": "Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust Speech Recognition",
      "title_zh": "通道感知领域自适应生成对抗网络用于鲁棒语音识别",
      "authors": [
        "Chien-Chun Wang",
        "Li-Wei Chen",
        "Cheng-Kang Chou",
        "Hung-Shin Lee",
        "Berlin Chen",
        "Hsin-Min Wang"
      ],
      "abstract": "While pre-trained automatic speech recognition (ASR) systems demonstrate\nimpressive performance on matched domains, their performance often degrades\nwhen confronted with channel mismatch stemming from unseen recording\nenvironments and conditions. To mitigate this issue, we propose a novel\nchannel-aware data simulation method for robust ASR training. Our method\nharnesses the synergistic power of channel-extractive techniques and generative\nadversarial networks (GANs). We first train a channel encoder capable of\nextracting embeddings from arbitrary audio. On top of this, channel embeddings\nare extracted using a minimal amount of target-domain data and used to guide a\nGAN-based speech synthesizer. This synthesizer generates speech that faithfully\npreserves the phonetic content of the input while mimicking the channel\ncharacteristics of the target domain. We evaluate our method on the challenging\nHakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving\nrelative character error rate (CER) reductions of 20.02% and 9.64%,\nrespectively, compared to the baselines. These results highlight the efficacy\nof our channel-aware data simulation method for bridging the gap between\nsource- and target-domain acoustics.",
      "tldr_zh": "该论文针对预训练的自动语音识别（ASR）系统在通道不匹配（如未见录音环境）时性能下降的问题，提出了一种通道感知的域适应生成对抗网络（GAN）。方法首先训练一个通道编码器从任意音频提取嵌入，然后使用少量目标域数据提取通道嵌入来指导GAN-based语音合成器，从而生成保留输入语音内容同时模仿目标域通道特性的合成语音。在Hakka Across Taiwan (HAT)和Taiwanese Across Taiwan (TAT)语料库上，该方法分别实现了20.02%和9.64%的相对字符错误率（CER）减少，证明了其在桥接源域和目标域声学差距方面的有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2409.12386v2",
      "published_date": "2024-09-19 01:02:31 UTC",
      "updated_date": "2025-01-08 05:57:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:45:01.290708"
    },
    {
      "arxiv_id": "2409.12385v1",
      "title": "Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Li",
        "Shiming Ge",
        "Daichi Zhang",
        "Jia Li"
      ],
      "abstract": "Many real-world applications today like video surveillance and urban\ngovernance need to address the recognition of masked faces, where content\nreplacement by diverse masks often brings in incomplete appearance and\nambiguous representation, leading to a sharp drop in accuracy. Inspired by\nrecent progress on amodal perception, we propose to migrate the mechanism of\namodal completion for the task of masked face recognition with an end-to-end\nde-occlusion distillation framework, which consists of two modules. The\n\\textit{de-occlusion} module applies a generative adversarial network to\nperform face completion, which recovers the content under the mask and\neliminates appearance ambiguity. The \\textit{distillation} module takes a\npre-trained general face recognition model as the teacher and transfers its\nknowledge to train a student for completed faces using massive online\nsynthesized face pairs. Especially, the teacher knowledge is represented with\nstructural relations among instances in multiple orders, which serves as a\nposterior regularization to enable the adaptation. In this way, the knowledge\ncan be fully distilled and transferred to identify masked faces. Experiments on\nsynthetic and realistic datasets show the efficacy of the proposed approach.",
      "tldr_zh": "本论文针对戴口罩人脸识别中的遮挡问题，提出了一种基于 de-occlusion distillation 的端到端框架，以解决外观不完整和表示模糊导致的准确率下降。\n框架包括 de-occlusion 模块，使用 generative adversarial network (GAN) 进行人脸完成，恢复口罩下的内容并消除模糊；distillation 模块则以预训练的通用人脸识别模型作为教师，通过结构关系知识转移训练学生模型。\n实验结果显示，该方法在合成和真实数据集上表现出色，有效提升了masked face recognition的性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM MM 2020",
      "pdf_url": "http://arxiv.org/pdf/2409.12385v1",
      "published_date": "2024-09-19 01:00:36 UTC",
      "updated_date": "2024-09-19 01:00:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:45:10.221934"
    },
    {
      "arxiv_id": "2409.12384v1",
      "title": "Privacy-Preserving Student Learning with Differentially Private Data-Free Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Bochao Liu",
        "Jianghu Lu",
        "Pengju Wang",
        "Junjie Zhang",
        "Dan Zeng",
        "Zhenxing Qian",
        "Shiming Ge"
      ],
      "abstract": "Deep learning models can achieve high inference accuracy by extracting rich\nknowledge from massive well-annotated data, but may pose the risk of data\nprivacy leakage in practical deployment. In this paper, we present an effective\nteacher-student learning approach to train privacy-preserving deep learning\nmodels via differentially private data-free distillation. The main idea is\ngenerating synthetic data to learn a student that can mimic the ability of a\nteacher well-trained on private data. In the approach, a generator is first\npretrained in a data-free manner by incorporating the teacher as a fixed\ndiscriminator. With the generator, massive synthetic data can be generated for\nmodel training without exposing data privacy. Then, the synthetic data is fed\ninto the teacher to generate private labels. Towards this end, we propose a\nlabel differential privacy algorithm termed selective randomized response to\nprotect the label information. Finally, a student is trained on the synthetic\ndata with the supervision of private labels. In this way, both data privacy and\nlabel privacy are well protected in a unified framework, leading to\nprivacy-preserving models. Extensive experiments and analysis clearly\ndemonstrate the effectiveness of our approach.",
      "tldr_zh": "该论文提出了一种基于差分隐私（Differentially Private）的教师-学生学习方法，通过数据无蒸馏（Data-Free Distillation）训练隐私保护的深度学习模型，以避免数据隐私泄露风险。核心思路是先预训练一个生成器，使用教师模型作为固定鉴别器生成合成数据，然后通过selective randomized response算法保护标签隐私，并用这些合成数据和私有标签训练学生模型。这种统一框架确保了数据和标签的双重隐私保护。实验结果表明，该方法在保持高模型性能的同时，显著提升了隐私安全性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published by IEEE MMSP 2022",
      "pdf_url": "http://arxiv.org/pdf/2409.12384v1",
      "published_date": "2024-09-19 01:00:18 UTC",
      "updated_date": "2024-09-19 01:00:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:45:22.104605"
    },
    {
      "arxiv_id": "2409.12380v1",
      "title": "Bundle Fragments into a Whole: Mining More Complete Clusters via Submodular Selection of Interesting webpages for Web Topic Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Junbiao Pang",
        "Anjing Hu",
        "Qingming Huang"
      ],
      "abstract": "Organizing interesting webpages into hot topics is one of key steps to\nunderstand the trends of multimodal web data. A state-of-the-art solution is\nfirstly to organize webpages into a large volume of multi-granularity topic\ncandidates; hot topics are further identified by estimating their\ninterestingness. However, these topic candidates contain a large number of\nfragments of hot topics due to both the inefficient feature representations and\nthe unsupervised topic generation. This paper proposes a bundling-refining\napproach to mine more complete hot topics from fragments. Concretely, the\nbundling step organizes the fragment topics into coarse topics; next, the\nrefining step proposes a submodular-based method to refine coarse topics in a\nscalable approach. The propose unconventional method is simple, yet powerful by\nleveraging submodular optimization, our approach outperforms the traditional\nranking methods which involve the careful design and complex steps. Extensive\nexperiments demonstrate that the proposed approach surpasses the\nstate-of-the-art method (i.e., latent Poisson deconvolution Pang et al. (2016))\n20% accuracy and 10% one on two public data sets, respectively.",
      "tldr_zh": "本研究针对网络主题检测问题，提出一种 bundling-refining approach，以从碎片化的网页主题候选中挖掘更完整的热门主题。该方法首先通过 bundling 步骤将碎片主题组织成粗糙主题，然后采用基于 submodular optimization 的精炼步骤，在可扩展方式下优化主题选择，从而避免了传统排名方法的复杂设计。相比现有方法，该框架简化了过程并提升了效率，在两个公共数据集上，准确率分别比 state-of-the-art 方法（如 latent Poisson deconvolution）提高了 20% 和 10%。这为理解多模态网络数据趋势提供了更可靠的解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10",
      "pdf_url": "http://arxiv.org/pdf/2409.12380v1",
      "published_date": "2024-09-19 00:46:31 UTC",
      "updated_date": "2024-09-19 00:46:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:45:43.974998"
    },
    {
      "arxiv_id": "2409.12371v1",
      "title": "Communication-Efficient Federated Low-Rank Update Algorithm and its Connection to Implicit Regularization",
      "title_zh": "翻译失败",
      "authors": [
        "Haemin Park",
        "Diego Klabjan"
      ],
      "abstract": "Federated Learning (FL) faces significant challenges related to communication\nefficiency and heterogeneity. To address these issues, we explore the potential\nof using low-rank updates. Our theoretical analysis reveals that client's loss\nexhibits a higher rank structure (gradients span higher rank subspace of\nHessian) compared to the server's loss. Based on this insight, we hypothesize\nthat constraining client-side optimization to a low-rank subspace could provide\nan implicit regularization effect. Consequently, we propose FedLoRU, a general\nlow-rank update framework for federated learning. Our framework enforces\nlow-rank client-side updates and accumulates these updates to form a\nhigher-rank model. Additionally, variants of FedLoRU can adapt to environments\nwith statistical and model heterogeneity by employing multiple or hierarchical\nlow-rank updates. Experimental results demonstrate that FedLoRU performs\ncomparably to full-rank algorithms and exhibits robustness to heterogeneous and\nlarge numbers of clients.",
      "tldr_zh": "本论文探讨了联邦学习（Federated Learning, FL）中的通信效率和异质性问题，通过理论分析发现客户端损失函数的梯度在Hessian子空间中具有更高秩结构，并提出FedLoRU框架，该框架强制客户端更新为低秩形式，并累积这些更新以形成更高秩模型，从而实现隐式正则化（implicit regularization）。FedLoRU的变体可通过多个或层次化的低秩更新适应统计和模型异质性。实验结果显示，FedLoRU的表现与全秩算法相当，并对异质性和大量客户端表现出更高的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.12371v1",
      "published_date": "2024-09-19 00:11:58 UTC",
      "updated_date": "2024-09-19 00:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T01:46:05.604867"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 121,
  "processed_papers_count": 121,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T01:46:25.637347"
}
[
  {
    "arxiv_id": "2403.15648v3",
    "title": "Unifying Large Language Model and Deep Reinforcement Learning for Human-in-Loop Interactive Socially-aware Navigation",
    "authors": [
      "Weizheng Wang",
      "Ike Obi",
      "Aniket Bera",
      "Byung-Cheol Min"
    ],
    "abstract": "Navigating human-filled spaces is crucial for the interactive social robots\nto support advanced services, such as cooperative carrying, which enables\nservice provision in complex and crowded environments while adapting behavior\nbased on real-time human language commands or feedback. However, existing\nsocial robot navigation planners face two major challenges: managing real-time\nuser inputs and ensuring socially compliant behaviors in unfamiliar, zero-shot\nenvironments. In response, we introduce SALM, an interactive, human-in-loop\nSocially-Aware navigation Large Language Model framework that dynamically\nintegrates deep reinforcement learning (DRL) with large language model (LLM)\ncapabilities. SALM leverages contextual semantic understanding from real-time\nhuman-robot interactions to convert high-level user commands into precise,\nlow-level control actions. A high-level LLM module parses user input, guiding\nthe simultaneous generation of navigation commands by both a large language\nnavigation model (LNM) and a DRL-based navigation model (RLNM). A memory\nmechanism archives temporal data for continuous refinement, while a multi-step\ngraph-of-thoughts inference-based large language feedback model adaptively\nfuses the strengths of both planning approaches. Experimental evaluations\ndemonstrate that SALM not only enhances navigational precision in crowded,\ndynamic environments but also significantly improves system adaptability,\noffering tailored behaviors that align with individual user preferences and\nreal-time feedback. More details and videos about this work are available at:\nhttps://sites.google.com/view/navi-salm.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15648v3",
    "published_date": "2024-03-22 23:12:28 UTC",
    "updated_date": "2025-03-07 20:03:06 UTC"
  },
  {
    "arxiv_id": "2403.15646v1",
    "title": "Application of the NIST AI Risk Management Framework to Surveillance Technology",
    "authors": [
      "Nandhini Swaminathan",
      "David Danks"
    ],
    "abstract": "This study offers an in-depth analysis of the application and implications of\nthe National Institute of Standards and Technology's AI Risk Management\nFramework (NIST AI RMF) within the domain of surveillance technologies,\nparticularly facial recognition technology. Given the inherently high-risk and\nconsequential nature of facial recognition systems, our research emphasizes the\ncritical need for a structured approach to risk management in this sector. The\npaper presents a detailed case study demonstrating the utility of the NIST AI\nRMF in identifying and mitigating risks that might otherwise remain unnoticed\nin these technologies. Our primary objective is to develop a comprehensive risk\nmanagement strategy that advances the practice of responsible AI utilization in\nfeasible, scalable ways. We propose a six-step process tailored to the specific\nchallenges of surveillance technology that aims to produce a more systematic\nand effective risk management practice. This process emphasizes continual\nassessment and improvement to facilitate companies in managing AI-related risks\nmore robustly and ensuring ethical and responsible deployment of AI systems.\nAdditionally, our analysis uncovers and discusses critical gaps in the current\nframework of the NIST AI RMF, particularly concerning its application to\nsurveillance technologies. These insights contribute to the evolving discourse\non AI governance and risk management, highlighting areas for future refinement\nand development in frameworks like the NIST AI RMF.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "K.4.1, K.5.2"
    ],
    "primary_category": "cs.CY",
    "comment": "14 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15646v1",
    "published_date": "2024-03-22 23:07:11 UTC",
    "updated_date": "2024-03-22 23:07:11 UTC"
  },
  {
    "arxiv_id": "2406.02554v1",
    "title": "Hear Me, See Me, Understand Me: Audio-Visual Autism Behavior Recognition",
    "authors": [
      "Shijian Deng",
      "Erin E. Kosloski",
      "Siddhi Patel",
      "Zeke A. Barnett",
      "Yiyang Nan",
      "Alexander Kaplan",
      "Sisira Aarukapalli",
      "William T. Doan",
      "Matthew Wang",
      "Harsh Singh",
      "Pamela R. Rollins",
      "Yapeng Tian"
    ],
    "abstract": "In this article, we introduce a novel problem of audio-visual autism behavior\nrecognition, which includes social behavior recognition, an essential aspect\npreviously omitted in AI-assisted autism screening research. We define the task\nat hand as one that is audio-visual autism behavior recognition, which uses\naudio and visual cues, including any speech present in the audio, to recognize\nautism-related behaviors. To facilitate this new research direction, we\ncollected an audio-visual autism spectrum dataset (AV-ASD), currently the\nlargest video dataset for autism screening using a behavioral approach. It\ncovers an extensive range of autism-associated behaviors, including those\nrelated to social communication and interaction. To pave the way for further\nresearch on this new problem, we intensively explored leveraging foundation\nmodels and multimodal large language models across different modalities. Our\nexperiments on the AV-ASD dataset demonstrate that integrating audio, visual,\nand speech modalities significantly enhances the performance in autism behavior\nrecognition. Additionally, we explored the use of a post-hoc to ad-hoc pipeline\nin a multimodal large language model to investigate its potential to augment\nthe model's explanatory capability during autism behavior recognition. We will\nrelease our dataset, code, and pre-trained models.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02554v1",
    "published_date": "2024-03-22 22:52:35 UTC",
    "updated_date": "2024-03-22 22:52:35 UTC"
  },
  {
    "arxiv_id": "2403.15640v1",
    "title": "Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making",
    "authors": [
      "Xin Chen",
      "I-Hong Hou"
    ],
    "abstract": "This paper introduces a novel multi-armed bandits framework, termed\nContextual Restless Bandits (CRB), for complex online decision-making. This CRB\nframework incorporates the core features of contextual bandits and restless\nbandits, so that it can model both the internal state transitions of each arm\nand the influence of external global environmental contexts. Using the dual\ndecomposition method, we develop a scalable index policy algorithm for solving\nthe CRB problem, and theoretically analyze the asymptotical optimality of this\nalgorithm. In the case when the arm models are unknown, we further propose a\nmodel-based online learning algorithm based on the index policy to learn the\narm models and make decisions simultaneously. Furthermore, we apply the\nproposed CRB framework and the index policy algorithm specifically to the\ndemand response decision-making problem in smart grids. The numerical\nsimulations demonstrate the performance and efficiency of our proposed CRB\napproaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15640v1",
    "published_date": "2024-03-22 22:35:07 UTC",
    "updated_date": "2024-03-22 22:35:07 UTC"
  },
  {
    "arxiv_id": "2403.15604v2",
    "title": "Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People",
    "authors": [
      "Ricardo Gonzalez",
      "Jazmin Collins",
      "Shiri Azenkot",
      "Cynthia Bennett"
    ],
    "abstract": "\"Scene description\" applications that describe visual content in a photo are\nuseful daily tools for blind and low vision (BLV) people. Researchers have\nstudied their use, but they have only explored those that leverage remote\nsighted assistants; little is known about applications that use AI to generate\ntheir descriptions. Thus, to investigate their use cases, we conducted a\ntwo-week diary study where 16 BLV participants used an AI-powered scene\ndescription application we designed. Through their diary entries and follow-up\ninterviews, users shared their information goals and assessments of the visual\ndescriptions they received. We analyzed the entries and found frequent use\ncases, such as identifying visual features of known objects, and surprising\nones, such as avoiding contact with dangerous objects. We also found users\nscored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for\nsatisfaction and 2.43 out of 4 (SD=1.16) for trust, showing that descriptions\nstill need significant improvements to deliver satisfying and trustworthy\nexperiences. We discuss future opportunities for AI as it becomes a more\npowerful accessibility tool for BLV users.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "21 pages, 18 figures, 5 tables, main track CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15604v2",
    "published_date": "2024-03-22 20:16:55 UTC",
    "updated_date": "2025-03-11 23:56:15 UTC"
  },
  {
    "arxiv_id": "2403.15603v2",
    "title": "Forward Learning for Gradient-based Black-box Saliency Map Generation",
    "authors": [
      "Zeliang Zhang",
      "Mingqian Feng",
      "Jinyang Jiang",
      "Rongyi Zhu",
      "Yijie Peng",
      "Chenliang Xu"
    ],
    "abstract": "Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The evaluation is based on small datasets and limited models, of\n  which bias leads to misleading conclusions",
    "pdf_url": "http://arxiv.org/pdf/2403.15603v2",
    "published_date": "2024-03-22 20:11:19 UTC",
    "updated_date": "2024-07-02 16:05:48 UTC"
  },
  {
    "arxiv_id": "2403.15601v1",
    "title": "From Guidelines to Governance: A Study of AI Policies in Education",
    "authors": [
      "Aashish Ghimire",
      "John Edwards"
    ],
    "abstract": "Emerging technologies like generative AI tools, including ChatGPT, are\nincreasingly utilized in educational settings, offering innovative approaches\nto learning while simultaneously posing new challenges. This study employs a\nsurvey methodology to examine the policy landscape concerning these\ntechnologies, drawing insights from 102 high school principals and higher\neducation provosts. Our results reveal a prominent policy gap: the majority of\ninstitutions lack specialized guide-lines for the ethical deployment of AI\ntools such as ChatGPT. Moreover,we observed that high schools are less inclined\nto work on policies than higher educational institutions. Where such policies\ndo exist, they often overlook crucial issues, including student privacy and\nalgorithmic transparency. Administrators overwhelmingly recognize the necessity\nof these policies, primarily to safeguard student safety and mitigate\nplagiarism risks. Our findings underscore the urgent need for flexible and\niterative policy frameworks in educational contexts.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15601v1",
    "published_date": "2024-03-22 20:07:58 UTC",
    "updated_date": "2024-03-22 20:07:58 UTC"
  },
  {
    "arxiv_id": "2403.15600v1",
    "title": "Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers",
    "authors": [
      "Sivana Hamer",
      "Marcelo d'Amorim",
      "Laurie Williams"
    ],
    "abstract": "Sonatype's 2023 report found that 97% of developers and security leads\nintegrate generative Artificial Intelligence (AI), particularly Large Language\nModels (LLMs), into their development process. Concerns about the security\nimplications of this trend have been raised. Developers are now weighing the\nbenefits and risks of LLMs against other relied-upon information sources, such\nas StackOverflow (SO), requiring empirical data to inform their choice. In this\nwork, our goal is to raise software developers awareness of the security\nimplications when selecting code snippets by empirically comparing the\nvulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an\nexisting Java dataset from SO with security-related questions and answers.\nThen, we asked ChatGPT the same SO questions, gathering the generated code for\ncomparison. After curating the dataset, we analyzed the number and types of\nCommon Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each\nplatform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities\ncompared to the 302 vulnerabilities found in SO snippets, producing 20% fewer\nvulnerabilities with a statistically significant difference. Additionally,\nChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings\nsuggest developers are under-educated on insecure code propagation from both\nplatforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code\ncopied and pasted, created by AI or humans, cannot be trusted blindly,\nrequiring good software engineering practices to reduce risk. Future work can\nhelp minimize insecure code propagation from any platform.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, 2 figures, accepted at Deep Learning Security and Privacy\n  Workshop (DLSP) part of IEEE Symposium on Security and Privacy Workshops\n  (SPW) for 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15600v1",
    "published_date": "2024-03-22 20:06:41 UTC",
    "updated_date": "2024-03-22 20:06:41 UTC"
  },
  {
    "arxiv_id": "2403.15587v1",
    "title": "Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",
    "authors": [
      "Cristina Zuheros",
      "David Herrera-Poyatos",
      "Rosana Montes",
      "Francisco Herrera"
    ],
    "abstract": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15587v1",
    "published_date": "2024-03-22 19:21:44 UTC",
    "updated_date": "2024-03-22 19:21:44 UTC"
  },
  {
    "arxiv_id": "2403.15586v1",
    "title": "Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",
    "authors": [
      "Aashish Ghimire",
      "James Prather",
      "John Edwards"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) and the expanding\nintegration of large language models (LLMs) have ignited a debate about their\napplication in education. This study delves into university instructors'\nexperiences and attitudes toward AI language models, filling a gap in the\nliterature by analyzing educators' perspectives on AI's role in the classroom\nand its potential impacts on teaching and learning. The objective of this\nresearch is to investigate the level of awareness, overall sentiment\ntowardsadoption, and the factors influencing these attitudes for LLMs and\ngenerative AI-based tools in higher education. Data was collected through a\nsurvey using a Likert scale, which was complemented by follow-up interviews to\ngain a more nuanced understanding of the instructors' viewpoints. The collected\ndata was processed using statistical and thematic analysis techniques. Our\nfindings reveal that educators are increasingly aware of and generally positive\ntowards these tools. We find no correlation between teaching style and attitude\ntoward generative AI. Finally, while CS educators show far more confidence in\ntheir technical understanding of generative AI tools and more positivity\ntowards them than educators in other fields, they show no more confidence in\ntheir ability to detect AI-generated work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15586v1",
    "published_date": "2024-03-22 19:21:29 UTC",
    "updated_date": "2024-03-22 19:21:29 UTC"
  },
  {
    "arxiv_id": "2403.15585v4",
    "title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis",
    "authors": [
      "Mai A. Shaaban",
      "Adnan Khan",
      "Mohammad Yaqub"
    ],
    "abstract": "Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first clinical decision support\nsystem that integrates multimodal large language models (MLLMs), few-shot\nprompting (FP) and visual grounding (VG) to combine imagery with EHR data for\nchest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing\nEHR information, providing a comprehensive understanding of patients' medical\nhistory. Additionally, FP reduces the necessity for extensive training of MLLMs\nwhile effectively tackling the issue of hallucination. Nevertheless, the\nprocess of determining the optimal number of few-shot examples and selecting\nhigh-quality candidates can be burdensome, yet it profoundly influences model\nperformance. Hence, we propose a new technique that dynamically refines\nfew-shot data for real-time adjustment to new patient scenarios. Moreover, VG\nnarrows the search area in X-ray images, thereby enhancing the identification\nof abnormalities. We also release MedPromptX-VQA, a new in-context visual\nquestion answering dataset encompassing interleaved images and EHR data derived\nfrom MIMIC-IV and MIMIC-CXR-JPG databases. Results demonstrate the SOTA\nperformance of MedPromptX, achieving an 11% improvement in F1-score compared to\nthe baselines. Code and data are publicly available on\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15585v4",
    "published_date": "2024-03-22 19:19:51 UTC",
    "updated_date": "2025-01-27 18:46:41 UTC"
  },
  {
    "arxiv_id": "2403.15577v1",
    "title": "Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control",
    "authors": [
      "Xiao Li",
      "H. Eric Tseng",
      "Anouck Girard",
      "Ilya Kolmanovsky"
    ],
    "abstract": "Autonomous driving depends on perception systems to understand the\nenvironment and to inform downstream decision-making. While advanced perception\nsystems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like\ncomprehension, their unpredictable behavior and lack of interpretability may\nhinder their deployment in safety critical scenarios. In this paper, we develop\nan Ensemble of DNN regressors (Deep Ensemble) that generates predictions with\nquantification of prediction uncertainties. In the scenario of Adaptive Cruise\nControl (ACC), we employ the Deep Ensemble to estimate distance headway to the\nlead vehicle from RGB images and enable the downstream controller to account\nfor the estimation uncertainty. We develop an adaptive cruise controller that\nutilizes Stochastic Model Predictive Control (MPC) with chance constraints to\nprovide a probabilistic safety guarantee. We evaluate our ACC algorithm using a\nhigh-fidelity traffic simulator and a real-world traffic dataset and\ndemonstrate the ability of the proposed approach to effect speed tracking and\ncar following while maintaining a safe distance headway. The\nout-of-distribution scenarios are also examined.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15577v1",
    "published_date": "2024-03-22 19:04:58 UTC",
    "updated_date": "2024-03-22 19:04:58 UTC"
  },
  {
    "arxiv_id": "2403.15574v1",
    "title": "SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification",
    "authors": [
      "Yuhan Xia",
      "Qingqing Zhao",
      "Yunfei Long",
      "Ge Xu",
      "Jia Wang"
    ],
    "abstract": "In traditional research approaches, sensory perception and emotion\nclassification have traditionally been considered separate domains. Yet, the\nsignificant influence of sensory experiences on emotional responses is\nundeniable. The natural language processing (NLP) community has often missed\nthe opportunity to merge sensory knowledge with emotion classification. To\naddress this gap, we propose SensoryT5, a neuro-cognitive approach that\nintegrates sensory information into the T5 (Text-to-Text Transfer Transformer)\nmodel, designed specifically for fine-grained emotion classification. This\nmethodology incorporates sensory cues into the T5's attention mechanism,\nenabling a harmonious balance between contextual understanding and sensory\nawareness. The resulting model amplifies the richness of emotional\nrepresentations. In rigorous tests across various detailed emotion\nclassification datasets, SensoryT5 showcases improved performance, surpassing\nboth the foundational T5 model and current state-of-the-art works. Notably,\nSensoryT5's success signifies a pivotal change in the NLP domain, highlighting\nthe potential influence of neuro-cognitive data in refining machine learning\nmodels' emotional sensitivity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by CogALex 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2403.15574v1",
    "published_date": "2024-03-22 19:03:25 UTC",
    "updated_date": "2024-03-22 19:03:25 UTC"
  },
  {
    "arxiv_id": "2403.15559v2",
    "title": "An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes",
    "authors": [
      "Zhengyi Zhao",
      "Chen Song",
      "Xiaodong Gu",
      "Yuan Dong",
      "Qi Zuo",
      "Weihao Yuan",
      "Liefeng Bo",
      "Zilong Dong",
      "Qixing Huang"
    ],
    "abstract": "A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15559v2",
    "published_date": "2024-03-22 18:28:04 UTC",
    "updated_date": "2024-08-02 10:19:34 UTC"
  },
  {
    "arxiv_id": "2403.15551v1",
    "title": "Language-Based Depth Hints for Monocular Depth Estimation",
    "authors": [
      "Dylan Auty",
      "Krystian Mikolajczyk"
    ],
    "abstract": "Monocular depth estimation (MDE) is inherently ambiguous, as a given image\nmay result from many different 3D scenes and vice versa. To resolve this\nambiguity, an MDE system must make assumptions about the most likely 3D scenes\nfor a given input. These assumptions can be either explicit or implicit. In\nthis work, we demonstrate the use of natural language as a source of an\nexplicit prior about the structure of the world. The assumption is made that\nhuman language encodes the likely distribution in depth-space of various\nobjects. We first show that a language model encodes this implicit bias during\ntraining, and that it can be extracted using a very simple learned approach. We\nthen show that this prediction can be provided as an explicit source of\nassumption to an MDE system, using an off-the-shelf instance segmentation model\nthat provides the labels used as the input to the language model. We\ndemonstrate the performance of our method on the NYUD2 dataset, showing\nimprovement compared to the baseline and to random controls.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 1 figure. Work originally done in June 2022",
    "pdf_url": "http://arxiv.org/pdf/2403.15551v1",
    "published_date": "2024-03-22 18:05:33 UTC",
    "updated_date": "2024-03-22 18:05:33 UTC"
  },
  {
    "arxiv_id": "2403.15388v5",
    "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models",
    "authors": [
      "Yuzhang Shang",
      "Mu Cai",
      "Bingxin Xu",
      "Yong Jae Lee",
      "Yan Yan"
    ],
    "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning\ncapabilities by connecting a visual encoder and a large language model. LMMs\ntypically take in a fixed and large amount of visual tokens, such as the\npenultimate layer features in the CLIP visual encoder, as the prefix content.\nRecent LMMs incorporate more complex visual inputs, such as high-resolution\nimages and videos, which further increases the number of visual tokens\nsignificantly. However, due to the inherent design of the Transformer\narchitecture, the computational costs of these models tend to increase\nquadratically with the number of input tokens. To tackle this problem, we\nexplore a token reduction mechanism that identifies significant spatial\nredundancy among visual tokens. In response, we propose PruMerge, a novel\nadaptive visual token reduction strategy that significantly reduces the number\nof visual tokens without compromising the performance of LMMs. Specifically, to\nmetric the importance of each token, we exploit the sparsity observed in the\nvisual encoder, characterized by the sparse distribution of attention scores\nbetween the class token and visual tokens. This sparsity enables us to\ndynamically select the most crucial visual tokens to retain. Subsequently, we\ncluster the selected (unpruned) tokens based on their key similarity and merge\nthem with the unpruned tokens, effectively supplementing and enhancing their\ninformational content. Empirically, when applied to LLaVA-1.5, our approach can\ncompress the visual tokens by 14 times on average, and achieve comparable\nperformance across diverse visual question-answering and reasoning tasks. Code\nand checkpoints are at https://llava-prumerge.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://llava-prumerge.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2403.15388v5",
    "published_date": "2024-03-22 17:59:52 UTC",
    "updated_date": "2024-05-22 20:50:37 UTC"
  },
  {
    "arxiv_id": "2403.15385v1",
    "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
    "authors": [
      "Kevin Xie",
      "Jonathan Lorraine",
      "Tianshi Cao",
      "Jun Gao",
      "James Lucas",
      "Antonio Torralba",
      "Sanja Fidler",
      "Xiaohui Zeng"
    ],
    "abstract": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "68T45",
      "I.2.6; I.2.7; I.3.6; I.3.7"
    ],
    "primary_category": "cs.CV",
    "comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LATTE3D/",
    "pdf_url": "http://arxiv.org/pdf/2403.15385v1",
    "published_date": "2024-03-22 17:59:37 UTC",
    "updated_date": "2024-03-22 17:59:37 UTC"
  },
  {
    "arxiv_id": "2403.15371v3",
    "title": "Can large language models explore in-context?",
    "authors": [
      "Akshay Krishnamurthy",
      "Keegan Harris",
      "Dylan J. Foster",
      "Cyril Zhang",
      "Aleksandrs Slivkins"
    ],
    "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024. This version: added references to related\n  and concurrent work",
    "pdf_url": "http://arxiv.org/pdf/2403.15371v3",
    "published_date": "2024-03-22 17:50:43 UTC",
    "updated_date": "2024-10-28 19:55:46 UTC"
  },
  {
    "arxiv_id": "2403.15529v2",
    "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
    "authors": [
      "Abdur Rahman Bin Md Faizullah",
      "Ashok Urlana",
      "Rahul Mishra"
    ],
    "abstract": "Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and\ntheir associated limitations from the ACL anthology. We investigate several\napproaches to harness large language models (LLMs) for producing suggestive\nlimitations, by thoroughly examining the related challenges, practical\ninsights, and potential opportunities. Our LimGen dataset and code can be\naccessed at \\url{https://github.com/arbmf/LimGen}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ECML-PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15529v2",
    "published_date": "2024-03-22 17:31:43 UTC",
    "updated_date": "2024-06-14 11:19:26 UTC"
  },
  {
    "arxiv_id": "2403.15528v3",
    "title": "Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs",
    "authors": [
      "Yiliang Zhou",
      "Hanley Ong",
      "Patrick Kennedy",
      "Carol Wu",
      "Jacob Kazam",
      "Keith Hentel",
      "Adam Flanders",
      "George Shih",
      "Yifan Peng"
    ],
    "abstract": "The study examines the application of GPT-4V, a multi-modal large language\nmodel equipped with visual recognition, in detecting radiological findings from\na set of 100 chest radiographs and suggests that GPT-4V is currently not ready\nfor real-world diagnostic usage in interpreting chest radiographs.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15528v3",
    "published_date": "2024-03-22 17:27:18 UTC",
    "updated_date": "2024-05-13 03:40:11 UTC"
  },
  {
    "arxiv_id": "2403.15362v2",
    "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
    "authors": [
      "Ryan Teehan",
      "Brenden Lake",
      "Mengye Ren"
    ],
    "abstract": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training. Code\nand data for our project can be found at\nhttps://college-concept-learning.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15362v2",
    "published_date": "2024-03-22 17:26:05 UTC",
    "updated_date": "2024-10-16 19:57:08 UTC"
  },
  {
    "arxiv_id": "2404.07220v2",
    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
    "authors": [
      "Kunal Sawarkar",
      "Abhilasha Mangal",
      "Shivam Raj Solanki"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\nprivate knowledge base of documents with Large Language Models (LLM) to build\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\nincreasingly challenging as the corpus of documents scales up, with Retrievers\nplaying an outsized role in the overall RAG accuracy by extracting the most\nrelevant document from the corpus to provide context to the LLM. In this paper,\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\nquery strategies. Our study achieves better retrieval results and sets new\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\nsurpassing fine-tuning performance.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Paper accepted by MIPR and presented at The 7th IEEE International\n  Conference on Multimedia Information. Processing and Retrieval (IEEE-MIPR\n  2024)",
    "pdf_url": "http://arxiv.org/pdf/2404.07220v2",
    "published_date": "2024-03-22 17:13:46 UTC",
    "updated_date": "2024-08-04 15:32:37 UTC"
  },
  {
    "arxiv_id": "2403.15341v1",
    "title": "Collaborative AI Teaming in Unknown Environments via Active Goal Deduction",
    "authors": [
      "Zuyuan Zhang",
      "Hanhan Zhou",
      "Mahdi Imani",
      "Taeyoung Lee",
      "Tian Lan"
    ],
    "abstract": "With the advancements of artificial intelligence (AI), we're seeing more\nscenarios that require AI to work closely with other agents, whose goals and\nstrategies might not be known beforehand. However, existing approaches for\ntraining collaborative agents often require defined and known reward signals\nand cannot address the problem of teaming with unknown agents that often have\nlatent objectives/rewards. In response to this challenge, we propose teaming\nwith unknown agents framework, which leverages kernel density Bayesian inverse\nlearning method for active goal deduction and utilizes pre-trained,\ngoal-conditioned policies to enable zero-shot policy adaptation. We prove that\nunbiased reward estimates in our framework are sufficient for optimal teaming\nwith unknown agents. We further evaluate the framework of redesigned\nmulti-agent particle and StarCraft II micromanagement environments with diverse\nunknown agents of different behaviors/rewards. Empirical results demonstrate\nthat our framework significantly advances the teaming performance of AI and\nunknown agents in a wide range of collaborative scenarios.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15341v1",
    "published_date": "2024-03-22 16:50:56 UTC",
    "updated_date": "2024-03-22 16:50:56 UTC"
  },
  {
    "arxiv_id": "2403.15325v1",
    "title": "A Technological Perspective on Misuse of Available AI",
    "authors": [
      "Lukas Pöhler",
      "Valentin Schrader",
      "Alexander Ladwein",
      "Florian von Keller"
    ],
    "abstract": "Potential malicious misuse of civilian artificial intelligence (AI) poses\nserious threats to security on a national and international level. Besides\ndefining autonomous systems from a technological viewpoint and explaining how\nAI development is characterized, we show how already existing and openly\navailable AI technology could be misused. To underline this, we developed three\nexemplary use cases of potentially misused AI that threaten political, digital\nand physical security. The use cases can be built from existing AI technologies\nand components from academia, the private sector and the developer-community.\nThis shows how freely available AI can be combined into autonomous weapon\nsystems. Based on the use cases, we deduce points of control and further\nmeasures to prevent the potential threat through misused AI. Further, we\npromote the consideration of malicious misuse of civilian AI systems in the\ndiscussion on autonomous weapon systems (AWS).",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at the UN Meeting of the Group of Governmental Experts on\n  Lethal Autonomous Weapons Systems, 30 August 2018",
    "pdf_url": "http://arxiv.org/pdf/2403.15325v1",
    "published_date": "2024-03-22 16:30:58 UTC",
    "updated_date": "2024-03-22 16:30:58 UTC"
  },
  {
    "arxiv_id": "2403.15317v2",
    "title": "Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection",
    "authors": [
      "Hongzhi Gao",
      "Zheng Chen",
      "Zehui Chen",
      "Lin Chen",
      "Jiaming Liu",
      "Shanghang Zhang",
      "Feng Zhao"
    ],
    "abstract": "Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization. In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15317v2",
    "published_date": "2024-03-22 16:11:29 UTC",
    "updated_date": "2024-03-25 16:45:41 UTC"
  },
  {
    "arxiv_id": "2403.15313v2",
    "title": "CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking",
    "authors": [
      "Nicolas Baumann",
      "Michael Baumgartner",
      "Edoardo Ghignone",
      "Jonas Kühne",
      "Tobias Fischer",
      "Yung-Hsu Yang",
      "Marc Pollefeys",
      "Michele Magno"
    ],
    "abstract": "To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15313v2",
    "published_date": "2024-03-22 16:06:05 UTC",
    "updated_date": "2024-08-06 15:58:35 UTC"
  },
  {
    "arxiv_id": "2403.15304v3",
    "title": "Addressing Label Leakage in Knowledge Tracing Models",
    "authors": [
      "Yahya Badran",
      "Christine Preisach"
    ],
    "abstract": "Knowledge Tracing (KT) is concerned with predicting students' future\nperformance on learning items in intelligent tutoring systems. Learning items\nare tagged with skill labels called knowledge concepts (KCs). Many KT models\nexpand the sequence of item-student interactions into KC-student interactions\nby replacing learning items with their constituting KCs. This approach\naddresses the issue of sparse item-student interactions and minimises the\nnumber of model parameters. However, we identified a label leakage problem with\nthis approach. The model's ability to learn correlations between KCs belonging\nto the same item can result in the leakage of ground truth labels, which leads\nto decreased performance, particularly on datasets with a high number of KCs\nper item.\n  In this paper, we present methods to prevent label leakage in knowledge\ntracing (KT) models. Our model variants that utilize these methods consistently\noutperform their original counterparts. This further underscores the impact of\nlabel leakage on model performance. Additionally, these methods enhance the\noverall performance of KT models, with one model variant surpassing all tested\nbaselines on different benchmarks. Notably, our methods are versatile and can\nbe applied to a wide range of KT models.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15304v3",
    "published_date": "2024-03-22 15:54:30 UTC",
    "updated_date": "2025-04-07 15:00:58 UTC"
  },
  {
    "arxiv_id": "2403.15301v2",
    "title": "Planning with a Learned Policy Basis to Optimally Solve Complex Tasks",
    "authors": [
      "Guillermo Infante",
      "David Kuric",
      "Anders Jonsson",
      "Vicenç Gómez",
      "Herke van Hoof"
    ],
    "abstract": "Conventional reinforcement learning (RL) methods can successfully solve a\nwide range of sequential decision problems. However, learning policies that can\ngeneralize predictably across multiple tasks in a setting with non-Markovian\nreward specifications is a challenging problem. We propose to use successor\nfeatures to learn a policy basis so that each (sub)policy in it solves a\nwell-defined subproblem. In a task described by a finite state automaton (FSA)\nthat involves the same set of subproblems, the combination of these\n(sub)policies can then be used to generate an optimal solution without\nadditional learning. In contrast to other methods that combine (sub)policies\nvia planning, our method asymptotically attains global optimality, even in\nstochastic environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15301v2",
    "published_date": "2024-03-22 15:51:39 UTC",
    "updated_date": "2024-06-03 14:56:28 UTC"
  },
  {
    "arxiv_id": "2403.15297v4",
    "title": "Sphere Neural-Networks for Rational Reasoning",
    "authors": [
      "Tiansi Dong",
      "Mateja Jamnik",
      "Pietro Liò"
    ],
    "abstract": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like communication, and\nalso by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a novel qualitative extension by\ngeneralising computational building blocks from vectors to spheres. We propose\nSphere Neural Networks (SphNNs) for human-like reasoning through model\nconstruction and inspection, and develop SphNN for syllogistic reasoning, a\nmicrocosm of human rationality. SphNN is a hierarchical neuro-symbolic\nKolmogorov-Arnold geometric GNN, and uses a neuro-symbolic transition map of\nneighbourhood spatial relations to transform the current sphere configuration\ntowards the target. SphNN is the first neural model that can determine the\nvalidity of long-chained syllogistic reasoning in one epoch without training\ndata, with the worst computational complexity of O(N). SphNN can evolve into\nvarious types of reasoning, such as spatio-temporal reasoning, logical\nreasoning with negation and disjunction, event reasoning, neuro-symbolic\nunification, and humour understanding (the highest level of cognition). All\nthese suggest a new kind of Herbert A. Simon's scissors with two neural blades.\nSphNNs will tremendously enhance interdisciplinary collaborations to develop\nthe two neural blades and realise deterministic neural reasoning and\nhuman-bounded rationality and elevate LLMs to reliable psychological AI. This\nwork suggests that the non-zero radii of spheres are the missing components\nthat prevent traditional deep-learning systems from reaching the realm of\nrational reasoning and cause LLMs to be trapped in the swamp of hallucination.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15297v4",
    "published_date": "2024-03-22 15:44:59 UTC",
    "updated_date": "2025-02-25 15:48:11 UTC"
  },
  {
    "arxiv_id": "2403.15274v2",
    "title": "Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review",
    "authors": [
      "Jinge Wang",
      "Zien Cheng",
      "Qiuming Yao",
      "Li Liu",
      "Dong Xu",
      "Gangqing Hu"
    ],
    "abstract": "The year 2023 marked a significant surge in the exploration of applying large\nlanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. We\nsurveyed the applications of ChatGPT in bioinformatics and biomedical\ninformatics throughout the year, covering omics, genetics, biomedical text\nmining, drug discovery, biomedical image understanding, bioinformatics\nprogramming, and bioinformatics education. Our survey delineates the current\nstrengths and limitations of this chatbot in bioinformatics and offers insights\ninto potential avenues for future developments.",
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "primary_category": "q-bio.OT",
    "comment": "Peer-reviewed and accepted by Quantitative Biology",
    "pdf_url": "http://arxiv.org/pdf/2403.15274v2",
    "published_date": "2024-03-22 15:16:23 UTC",
    "updated_date": "2024-06-12 15:50:31 UTC"
  },
  {
    "arxiv_id": "2403.15257v1",
    "title": "Hierarchical Information Enhancement Network for Cascade Prediction in Social Networks",
    "authors": [
      "Fanrui Zhang",
      "Jiawei Liu",
      "Qiang Zhang",
      "Xiaoling Zhu",
      "Zheng-Jun Zha"
    ],
    "abstract": "Understanding information cascades in networks is a fundamental issue in\nnumerous applications. Current researches often sample cascade information into\nseveral independent paths or subgraphs to learn a simple cascade\nrepresentation. However, these approaches fail to exploit the hierarchical\nsemantic associations between different modalities, limiting their predictive\nperformance. In this work, we propose a novel Hierarchical Information\nEnhancement Network (HIENet) for cascade prediction. Our approach integrates\nfundamental cascade sequence, user social graphs, and sub-cascade graph into a\nunified framework. Specifically, HIENet utilizes DeepWalk to sample cascades\ninformation into a series of sequences. It then gathers path information\nbetween users to extract the social relationships of propagators. Additionally,\nwe employ a time-stamped graph convolutional network to aggregate sub-cascade\ngraph information effectively. Ultimately, we introduce a Multi-modal Cascade\nTransformer to powerfully fuse these clues, providing a comprehensive\nunderstanding of cascading process. Extensive experiments have demonstrated the\neffectiveness of the proposed method.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15257v1",
    "published_date": "2024-03-22 14:57:27 UTC",
    "updated_date": "2024-03-22 14:57:27 UTC"
  },
  {
    "arxiv_id": "2403.15251v1",
    "title": "Safe Learning of PDDL Domains with Conditional Effects -- Extended Version",
    "authors": [
      "Argaman Mordoch",
      "Enrico Scala",
      "Roni Stern",
      "Brendan Juba"
    ],
    "abstract": "Powerful domain-independent planners have been developed to solve various\ntypes of planning problems. These planners often require a model of the acting\nagent's actions, given in some planning domain description language. Manually\ndesigning such an action model is a notoriously challenging task. An\nalternative is to automatically learn action models from observation. Such an\naction model is called safe if every plan created with it is consistent with\nthe real, unknown action model. Algorithms for learning such safe action models\nexist, yet they cannot handle domains with conditional or universal effects,\nwhich are common constructs in many planning problems. We prove that learning\nnon-trivial safe action models with conditional effects may require an\nexponential number of samples. Then, we identify reasonable assumptions under\nwhich such learning is tractable and propose SAM Learning of Conditional\nEffects (Conditional-SAM), the first algorithm capable of doing so. We analyze\nConditional-SAM theoretically and evaluate it experimentally. Our results show\nthat the action models learned by Conditional-SAM can be used to solve\nperfectly most of the test set problems in most of the experimented domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15251v1",
    "published_date": "2024-03-22 14:49:49 UTC",
    "updated_date": "2024-03-22 14:49:49 UTC"
  },
  {
    "arxiv_id": "2403.15250v2",
    "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
    "authors": [
      "Kun Sun",
      "Rong Wang",
      "Anders Søgaard"
    ],
    "abstract": "Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15250v2",
    "published_date": "2024-03-22 14:47:35 UTC",
    "updated_date": "2024-06-24 07:49:25 UTC"
  },
  {
    "arxiv_id": "2403.15249v2",
    "title": "Spectral Motion Alignment for Video Motion Transfer using Diffusion Models",
    "authors": [
      "Geon Yeong Park",
      "Hyeonho Jeong",
      "Sang Wan Lee",
      "Jong Chul Ye"
    ],
    "abstract": "The evolution of diffusion models has greatly impacted video generation and\nunderstanding. Particularly, text-to-video diffusion models (VDMs) have\nsignificantly facilitated the customization of input video with target\nappearance, motion, etc. Despite these advances, challenges persist in\naccurately distilling motion information from video frames. While existing\nworks leverage the consecutive frame residual as the target motion vector, they\ninherently lack global motion context and are vulnerable to frame-wise\ndistortions. To address this, we present Spectral Motion Alignment (SMA), a\nnovel framework that refines and aligns motion vectors using Fourier and\nwavelet transforms. SMA learns motion patterns by incorporating\nfrequency-domain regularization, facilitating the learning of whole-frame\nglobal motion dynamics, and mitigating spatial artifacts. Extensive experiments\ndemonstrate SMA's efficacy in improving motion transfer while maintaining\ncomputational efficiency and compatibility across various video customization\nframeworks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI 2025, Project page:\n  https://geonyeong-park.github.io/spectral-motion-alignment/",
    "pdf_url": "http://arxiv.org/pdf/2403.15249v2",
    "published_date": "2024-03-22 14:47:18 UTC",
    "updated_date": "2024-12-19 05:30:55 UTC"
  },
  {
    "arxiv_id": "2403.15248v1",
    "title": "Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks",
    "authors": [
      "Sudhir Sornapudi",
      "Rajhans Singh"
    ],
    "abstract": "Computer vision in agriculture is game-changing with its ability to transform\nfarming into a data-driven, precise, and sustainable industry. Deep learning\nhas empowered agriculture vision to analyze vast, complex visual data, but\nheavily rely on the availability of large annotated datasets. This remains a\nbottleneck as manual labeling is error-prone, time-consuming, and expensive.\nThe lack of efficient labeling approaches inspired us to consider\nself-supervised learning as a paradigm shift, learning meaningful feature\nrepresentations from raw agricultural image data. In this work, we explore how\nself-supervised representation learning unlocks the potential applicability to\ndiverse agriculture vision tasks by eliminating the need for large-scale\nannotated datasets. We propose a lightweight framework utilizing SimCLR, a\ncontrastive learning approach, to pre-train a ResNet-50 backbone on a large,\nunannotated dataset of real-world agriculture field images. Our experimental\nanalysis and results indicate that the model learns robust features applicable\nto a broad range of downstream agriculture tasks discussed in the paper.\nAdditionally, the reduced reliance on annotated data makes our approach more\ncost-effective and accessible, paving the way for broader adoption of computer\nvision in agriculture.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15248v1",
    "published_date": "2024-03-22 14:46:51 UTC",
    "updated_date": "2024-03-22 14:46:51 UTC"
  },
  {
    "arxiv_id": "2403.15245v2",
    "title": "Reasoning-Enhanced Object-Centric Learning for Videos",
    "authors": [
      "Jian Li",
      "Pu Ren",
      "Yang Liu",
      "Hao Sun"
    ],
    "abstract": "Object-centric learning aims to break down complex visual scenes into more\nmanageable object representations, enhancing the understanding and reasoning\nabilities of machine learning systems toward the physical world. Recently,\nslot-based video models have demonstrated remarkable proficiency in segmenting\nand tracking objects, but they overlook the importance of the effective\nreasoning module. In the real world, reasoning and predictive abilities play a\ncrucial role in human perception and object tracking; in particular, these\nabilities are closely related to human intuitive physics. Inspired by this, we\ndesigned a novel reasoning module called the Slot-based Time-Space Transformer\nwith Memory buffer (STATM) to enhance the model's perception ability in complex\nscenes. The memory buffer primarily serves as storage for slot information from\nupstream modules, the Slot-based Time-Space Transformer makes predictions\nthrough slot-based spatiotemporal attention computations and fusion. Our\nexperimental results on various datasets indicate that the STATM module can\nsignificantly enhance the capabilities of multiple state-of-the-art\nobject-centric learning models for video. Moreover, as a predictive model, the\nSTATM module also performs well in downstream prediction and Visual Question\nAnswering (VQA) tasks. We will release our codes and data at\nhttps://github.com/intell-sci-comput/STATM.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15245v2",
    "published_date": "2024-03-22 14:41:55 UTC",
    "updated_date": "2025-02-15 14:46:40 UTC"
  },
  {
    "arxiv_id": "2403.15235v1",
    "title": "Multi-perspective Memory Enhanced Network for Identifying Key Nodes in Social Networks",
    "authors": [
      "Qiang Zhang",
      "Jiawei Liu",
      "Fanrui Zhang",
      "Xiaoling Zhu",
      "Zheng-Jun Zha"
    ],
    "abstract": "Identifying key nodes in social networks plays a crucial role in timely\nblocking false information. Existing key node identification methods usually\nconsider node influence only from the propagation structure perspective and\nhave insufficient generalization ability to unknown scenarios. In this paper,\nwe propose a novel Multi-perspective Memory Enhanced Network (MMEN) for\nidentifying key nodes in social networks, which mines key nodes from multiple\nperspectives and utilizes memory networks to store historical information.\nSpecifically, MMEN first constructs two propagation networks from the\nperspectives of user attributes and propagation structure and updates node\nfeature representations using graph attention networks. Meanwhile, the memory\nnetwork is employed to store information of similar subgraphs, enhancing the\nmodel's generalization performance in unknown scenarios. Finally, MMEN applies\nadaptive weights to combine the node influence of the two propagation networks\nto select the ultimate key nodes. Extensive experiments demonstrate that our\nmethod significantly outperforms previous methods.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "7 pages, 1 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15235v1",
    "published_date": "2024-03-22 14:29:03 UTC",
    "updated_date": "2024-03-22 14:29:03 UTC"
  },
  {
    "arxiv_id": "2403.15218v1",
    "title": "Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment Anything Model for Crowd-Sourcing Medical Image Annotations",
    "authors": [
      "Pranav Kulkarni",
      "Adway Kanhere",
      "Dharmam Savani",
      "Andrew Chan",
      "Devina Chatterjee",
      "Paul H. Yi",
      "Vishwa S. Parekh"
    ],
    "abstract": "Curating annotations for medical image segmentation is a labor-intensive and\ntime-consuming task that requires domain expertise, resulting in \"narrowly\"\nfocused deep learning (DL) models with limited translational utility. Recently,\nfoundation models like the Segment Anything Model (SAM) have revolutionized\nsemantic segmentation with exceptional zero-shot generalizability across\nvarious domains, including medical imaging, and hold a lot of promise for\nstreamlining the annotation process. However, SAM has yet to be evaluated in a\ncrowd-sourced setting to curate annotations for training 3D DL segmentation\nmodels. In this work, we explore the potential of SAM for crowd-sourcing\n\"sparse\" annotations from non-experts to generate \"dense\" segmentation masks\nfor training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our\nresults indicate that while SAM-generated annotations exhibit high mean Dice\nscores compared to ground-truth annotations, nnU-Net models trained on\nSAM-generated annotations perform significantly worse than nnU-Net models\ntrained on ground-truth annotations ($p<0.001$, all).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15218v1",
    "published_date": "2024-03-22 14:07:07 UTC",
    "updated_date": "2024-03-22 14:07:07 UTC"
  },
  {
    "arxiv_id": "2403.15216v1",
    "title": "(Un)making AI Magic: a Design Taxonomy",
    "authors": [
      "Maria Luce Lupetti",
      "Dave Murray-Rust"
    ],
    "abstract": "This paper examines the role that enchantment plays in the design of AI\nthings by constructing a taxonomy of design approaches that increase or\ndecrease the perception of magic and enchantment. We start from the design\ndiscourse surrounding recent developments in AI technologies, highlighting\nspecific interaction qualities such as algorithmic uncertainties and errors and\narticulating relations to the rhetoric of magic and supernatural thinking.\nThrough analyzing and reflecting upon 52 students' design projects from two\neditions of a Master course in design and AI, we identify seven design\nprinciples and unpack the effects of each in terms of enchantment and\ndisenchantment. We conclude by articulating ways in which this taxonomy can be\napproached and appropriated by design/HCI practitioners, especially to support\nexploration and reflexivity.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15216v1",
    "published_date": "2024-03-22 14:03:37 UTC",
    "updated_date": "2024-03-22 14:03:37 UTC"
  },
  {
    "arxiv_id": "2403.15523v2",
    "title": "Towards auditory attention decoding with noise-tagging: A pilot study",
    "authors": [
      "H. A. Scheppink",
      "S. Ahmadi",
      "P. Desain",
      "M. Tangermann",
      "J. Thielen"
    ],
    "abstract": "Auditory attention decoding (AAD) aims to extract from brain activity the\nattended speaker amidst candidate speakers, offering promising applications for\nneuro-steered hearing devices and brain-computer interfacing. This pilot study\nmakes a first step towards AAD using the noise-tagging stimulus protocol, which\nevokes reliable code-modulated evoked potentials, but is minimally explored in\nthe auditory modality. Participants were sequentially presented with two Dutch\nspeech stimuli that were amplitude-modulated with a unique binary pseudo-random\nnoise-code, effectively tagging these with additional decodable information. We\ncompared the decoding of unmodulated audio against audio modulated with various\nmodulation depths, and a conventional AAD method against a standard method to\ndecode noise-codes. Our pilot study revealed higher performances for the\nconventional method with 70 to 100 percent modulation depths compared to\nunmodulated audio. The noise-code decoder did not further improve these\nresults. These fundamental insights highlight the potential of integrating\nnoise-codes in speech to enhance auditory speaker detection when multiple\nspeakers are presented simultaneously.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "q-bio.NC",
    "comment": "6 pages, 2 figures, 9th Graz Brain-Computer Interface Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15523v2",
    "published_date": "2024-03-22 13:35:34 UTC",
    "updated_date": "2024-05-17 14:44:24 UTC"
  },
  {
    "arxiv_id": "2403.15195v1",
    "title": "FSD-Inference: Fully Serverless Distributed Inference with Scalable Cloud Communication",
    "authors": [
      "Joe Oakley",
      "Hakan Ferhatosmanoglu"
    ],
    "abstract": "Serverless computing offers attractive scalability, elasticity and\ncost-effectiveness. However, constraints on memory, CPU and function runtime\nhave hindered its adoption for data-intensive applications and machine learning\n(ML) workloads. Traditional 'server-ful' platforms enable distributed\ncomputation via fast networks and well-established inter-process communication\n(IPC) mechanisms such as MPI and shared memory. In the absence of such\nsolutions in the serverless domain, parallel computation with significant IPC\nrequirements is challenging. We present FSD-Inference, the first fully\nserverless and highly scalable system for distributed ML inference. We explore\npotential communication channels, in conjunction with Function-as-a-Service\n(FaaS) compute, to design a state-of-the-art solution for distributed ML within\nthe context of serverless data-intensive computing. We introduce novel fully\nserverless communication schemes for ML inference workloads, leveraging both\ncloud-based publish-subscribe/queueing and object storage offerings. We\ndemonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC\nwith comparable performance to object storage, while offering significantly\nreduced cost at high parallelism levels. We conduct in-depth experiments on\nbenchmark DNNs of various sizes. The results show that when compared to\nserver-based alternatives, FSD-Inference is significantly more cost-effective\nand scalable, and can even achieve competitive performance against optimized\nHPC solutions. Experiments also confirm that our serverless solution can handle\nlarge distributed workloads and leverage high degrees of FaaS parallelism.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "In Proceedings of 2024 IEEE 40th International Conference on Data\n  Engineering (ICDE) (to appear)",
    "pdf_url": "http://arxiv.org/pdf/2403.15195v1",
    "published_date": "2024-03-22 13:31:24 UTC",
    "updated_date": "2024-03-22 13:31:24 UTC"
  },
  {
    "arxiv_id": "2403.15192v1",
    "title": "SFOD: Spiking Fusion Object Detector",
    "authors": [
      "Yimeng Fan",
      "Wei Zhang",
      "Changsong Liu",
      "Mingyang Li",
      "Wenrui Lu"
    ],
    "abstract": "Event cameras, characterized by high temporal resolution, high dynamic range,\nlow power consumption, and high pixel bandwidth, offer unique capabilities for\nobject detection in specialized contexts. Despite these advantages, the\ninherent sparsity and asynchrony of event data pose challenges to existing\nobject detection algorithms. Spiking Neural Networks (SNNs), inspired by the\nway the human brain codes and processes information, offer a potential solution\nto these difficulties. However, their performance in object detection using\nevent cameras is limited in current implementations. In this paper, we propose\nthe Spiking Fusion Object Detector (SFOD), a simple and efficient approach to\nSNN-based object detection. Specifically, we design a Spiking Fusion Module,\nachieving the first-time fusion of feature maps from different scales in SNNs\napplied to event cameras. Additionally, through integrating our analysis and\nexperiments conducted during the pretraining of the backbone network on the\nNCAR dataset, we delve deeply into the impact of spiking decoding strategies\nand loss functions on model performance. Thereby, we establish state-of-the-art\nclassification results based on SNNs, achieving 93.7\\% accuracy on the NCAR\ndataset. Experimental results on the GEN1 detection dataset demonstrate that\nthe SFOD achieves a state-of-the-art mAP of 32.1\\%, outperforming existing\nSNN-based approaches. Our research not only underscores the potential of SNNs\nin object detection with event cameras but also propels the advancement of\nSNNs. Code is available at https://github.com/yimeng-fan/SFOD.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by CVPR2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15192v1",
    "published_date": "2024-03-22 13:24:50 UTC",
    "updated_date": "2024-03-22 13:24:50 UTC"
  },
  {
    "arxiv_id": "2403.15176v3",
    "title": "Brain-aligning of semantic vectors improves neural decoding of visual stimuli",
    "authors": [
      "Shirin Vafaei",
      "Ryohei Fukuma",
      "Takufumi Yanagisawa",
      "Huixiang Yang",
      "Satoru Oshino",
      "Naoki Tani",
      "Hui Ming Khoo",
      "Hidenori Sugano",
      "Yasushi Iimura",
      "Hiroharu Suzuki",
      "Madoka Nakajima",
      "Kentaro Tamura",
      "Haruhiko Kishima"
    ],
    "abstract": "The development of algorithms to accurately decode of neural information is a\nlong-standing effort in the field of neuroscience. Brain decoding is typically\nemployed by training machine learning models to map neural data onto a\npreestablished vector representation of stimulus features. These vectors are\nusually derived from image- and/or text-based feature spaces. Nonetheless, the\nintrinsic characteristics of these vectors might be fundamentally different\nthan those encoded by the brain, limiting the ability of algorithms to\naccurately learn this mapping. To address this issue, here, we propose a\nrepresentation learning framework, called brain-aligning of semantic vectors,\nthat fine-tunes pretrained feature vectors to better align with the structure\nof neural representations of visual stimuli in the human brain. We trained this\nmodel with functional magnetic resonance imaging (fMRI) data representing 150\nvisual stimulus categories; then, we performed zero-shot brain decoding on 1)\nfMRI, 2) magnetoencephalography (MEG), and 3) electrocorticography (ECoG) data\nreflecting neural representations of visual stimuli. By using fMRI-based\nbrain-aligned vectors, the zero-shot decoding accuracy all three neuroimaging\ndatasets increased. This finding underscores the potential of leveraging a\nricher array of brainderived features to increase the performance of brain\ndecoding algorithms.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "40 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15176v3",
    "published_date": "2024-03-22 13:01:10 UTC",
    "updated_date": "2024-09-12 09:35:03 UTC"
  },
  {
    "arxiv_id": "2406.11843v1",
    "title": "Explanation Hacking: The perils of algorithmic recourse",
    "authors": [
      "Emily Sullivan",
      "Atoosa Kasirzadeh"
    ],
    "abstract": "We argue that the trend toward providing users with feasible and actionable\nexplanations of AI decisions, known as recourse explanations, comes with\nethical downsides. Specifically, we argue that recourse explanations face\nseveral conceptual pitfalls and can lead to problematic explanation hacking,\nwhich undermines their ethical status. As an alternative, we advocate that\nexplanations of AI decisions should aim at understanding.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11843v1",
    "published_date": "2024-03-22 12:49:28 UTC",
    "updated_date": "2024-03-22 12:49:28 UTC"
  },
  {
    "arxiv_id": "2403.15170v1",
    "title": "Exploring the Task-agnostic Trait of Self-supervised Learning in the Context of Detecting Mental Disorders",
    "authors": [
      "Rohan Kumar Gupta",
      "Rohit Sinha"
    ],
    "abstract": "Self-supervised learning (SSL) has been investigated to generate\ntask-agnostic representations across various domains. However, such\ninvestigation has not been conducted for detecting multiple mental disorders.\nThe rationale behind the existence of a task-agnostic representation lies in\nthe overlapping symptoms among multiple mental disorders. Consequently, the\nbehavioural data collected for mental health assessment may carry a mixed bag\nof attributes related to multiple disorders. Motivated by that, in this study,\nwe explore a task-agnostic representation derived through SSL in the context of\ndetecting major depressive disorder (MDD) and post-traumatic stress disorder\n(PTSD) using audio and video data collected during interactive sessions. This\nstudy employs SSL models trained by predicting multiple fixed targets or masked\nframes. We propose a list of fixed targets to make the generated representation\nmore efficient for detecting MDD and PTSD. Furthermore, we modify the\nhyper-parameters of the SSL encoder predicting fixed targets to generate global\nrepresentations that capture varying temporal contexts. Both these innovations\nare noted to yield improved detection performances for considered mental\ndisorders and exhibit task-agnostic traits. In the context of the SSL model\npredicting masked frames, the generated global representations are also noted\nto exhibit task-agnostic traits.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15170v1",
    "published_date": "2024-03-22 12:46:58 UTC",
    "updated_date": "2024-03-22 12:46:58 UTC"
  },
  {
    "arxiv_id": "2403.15167v1",
    "title": "Transition Graph Properties of Target Class Classification",
    "authors": [
      "Levon Aslanyan",
      "Hasmik Sahakyan"
    ],
    "abstract": "Target class classification is a mixed classification and transition model\nwhose integrated goal is to assign objects to a certain, so called target or\nnormal class. The classification process is iterative, and in each step an\nobject in a certain class undergoes an action attached to that class,\ninitiating the transition of the object to one of the classes. The sequence of\ntransitions, which we call class transitions, must be designed to provide the\nfinal assignment of objects to the target class. The transition process can be\ndescribed in the form of a directed graph, and the success of the final\nclassification is mainly due to the properties of this graph. In our previous\nresearch we showed that the desirable structure of the transition graph is an\noriented rooted tree with orientation towards the root vertex, which\ncorresponds to the normal class. It is clear that the transition graph of an\narbitrary algorithm (policy) may not have this property. In this paper we study\nthe structure of realistic transition graphs, which makes it possible to find\nclassification inconsistencies, helping to transfer it into the desired form.\nThe medical interpretation of dynamic treatment regime considered in the\narticle further clarifies the investigated framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.LG",
    "comment": "14pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15167v1",
    "published_date": "2024-03-22 12:37:14 UTC",
    "updated_date": "2024-03-22 12:37:14 UTC"
  },
  {
    "arxiv_id": "2403.15143v1",
    "title": "Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project",
    "authors": [
      "Md Abdul Kadir",
      "Hasan Md Tusfiqur Alam",
      "Pascale Maul",
      "Hans-Jürgen Profitlich",
      "Moritz Wolf",
      "Daniel Sonntag"
    ],
    "abstract": "Image annotation is one of the most essential tasks for guaranteeing proper\ntreatment for patients and tracking progress over the course of therapy in the\nfield of medical imaging and disease diagnosis. However, manually annotating a\nlot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)\nbased segmentation algorithms have completely transformed this process and made\nit possible to automate image segmentation. By accurately segmenting medical\nimages, these algorithms can greatly minimize the time and effort necessary for\nmanual annotation. Additionally, by incorporating Active Learning (AL) methods,\nthese segmentation algorithms can perform far more effectively with a smaller\namount of ground truth data. We introduce MedDeepCyleAL, an end-to-end\nframework implementing the complete AL cycle. It provides researchers with the\nflexibility to choose the type of deep learning model they wish to employ and\nincludes an annotation tool that supports the classification and segmentation\nof medical images. The user-friendly interface allows for easy alteration of\nthe AL and DL model settings through a configuration file, requiring no prior\nprogramming experience. While MedDeepCyleAL can be applied to any kind of image\ndata, we have specifically applied it to ophthalmology data in this project.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "DFKI Technical Report",
    "pdf_url": "http://arxiv.org/pdf/2403.15143v1",
    "published_date": "2024-03-22 11:53:03 UTC",
    "updated_date": "2024-03-22 11:53:03 UTC"
  },
  {
    "arxiv_id": "2403.15137v1",
    "title": "CACA Agent: Capability Collaboration based AI Agent",
    "authors": [
      "Peng Xu",
      "Haoran Wang",
      "Chuang Wang",
      "Xu Liu"
    ],
    "abstract": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "4 pages,5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15137v1",
    "published_date": "2024-03-22 11:42:47 UTC",
    "updated_date": "2024-03-22 11:42:47 UTC"
  },
  {
    "arxiv_id": "2406.11842v1",
    "title": "The Ethics of AI in Education",
    "authors": [
      "Kaska Porayska-Pomsta",
      "Wayne Holmes",
      "Selena Nemorin"
    ],
    "abstract": "The transition of Artificial Intelligence (AI) from a lab-based science to\nlive human contexts brings into sharp focus many historic, socio-cultural\nbiases, inequalities, and moral dilemmas. Many questions that have been raised\nregarding the broader ethics of AI are also relevant for AI in Education\n(AIED). AIED raises further specific challenges related to the impact of its\ntechnologies on users, how such technologies might be used to reinforce or\nalter the way that we learn and teach, and what we, as a society and\nindividuals, value as outcomes of education. This chapter discusses key ethical\ndimensions of AI and contextualises them within AIED design and engineering\npractices to draw connections between the AIED systems we build, the questions\nabout human learning and development we ask, the ethics of the pedagogies we\nuse, and the considerations of values that we promote in and through AIED\nwithin a wider socio-technical system.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.11842v1",
    "published_date": "2024-03-22 11:41:37 UTC",
    "updated_date": "2024-03-22 11:41:37 UTC"
  },
  {
    "arxiv_id": "2403.15115v2",
    "title": "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions",
    "authors": [
      "Erik Miehling",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Elizabeth M. Daly",
      "David Piorkowski",
      "John T. Richards"
    ],
    "abstract": "Modern language models, while sophisticated, exhibit some inherent\nshortcomings, particularly in conversational settings. We claim that many of\nthe observed shortcomings can be attributed to violation of one or more\nconversational principles. By drawing upon extensive research from both the\nsocial science and AI communities, we propose a set of maxims -- quantity,\nquality, relevance, manner, benevolence, and transparency -- for describing\neffective human-AI conversation. We first justify the applicability of the\nfirst four maxims (from Grice) in the context of human-AI interactions. We then\nargue that two new maxims, benevolence (concerning the generation of, and\nengagement with, harmful content) and transparency (concerning recognition of\none's knowledge boundaries, operational constraints, and intents), are\nnecessary for addressing behavior unique to modern human-AI interactions. We\nevaluate the degree to which various language models are able to understand\nthese maxims and find that models possess an internal prioritization of\nprinciples that can significantly impact their ability to interpret the maxims\naccurately.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15115v2",
    "published_date": "2024-03-22 11:16:43 UTC",
    "updated_date": "2024-06-22 12:17:38 UTC"
  },
  {
    "arxiv_id": "2403.15114v3",
    "title": "Solving a Real-World Package Delivery Routing Problem Using Quantum Annealers",
    "authors": [
      "Eneko Osaba",
      "Esther Villar-Rodriguez",
      "Antón Asla"
    ],
    "abstract": "Research focused on the conjunction between quantum computing and routing\nproblems has been very prolific in recent years. Most of the works revolve\naround classical problems such as the Traveling Salesman Problem or the Vehicle\nRouting Problem. The real-world applicability of these problems is dependent on\nthe objectives and constraints considered. Anyway, it is undeniable that it is\noften difficult to translate complex requirements into these classical\nformulations.The main objective of this research is to present a solving scheme\nfor dealing with realistic instances while maintaining all the characteristics\nand restrictions of the original real-world problem. Thus, a quantum-classical\nstrategy has been developed, coined Q4RPD, that considers a set of real\nconstraints such as a heterogeneous fleet of vehicles, priority deliveries, and\ncapacities characterized by two values: weight and dimensions of the packages.\nQ4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave.\nTo demonstrate the application of Q4RPD, an experimentation composed of six\ndifferent instances has been conducted, aiming to serve as illustrative\nexamples.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "16 pages, 11 figures and 4 tables. Paper submitted for review in\n  Scientific Reports",
    "pdf_url": "http://arxiv.org/pdf/2403.15114v3",
    "published_date": "2024-03-22 11:16:11 UTC",
    "updated_date": "2024-06-28 11:49:51 UTC"
  },
  {
    "arxiv_id": "2403.15112v5",
    "title": "Text Clustering with Large Language Model Embeddings",
    "authors": [
      "Alina Petukhova",
      "João P. Matos-Carvalho",
      "Nuno Fachada"
    ],
    "abstract": "Text clustering is an important method for organising the increasing volume\nof digital content, aiding in the structuring and discovery of hidden patterns\nin uncategorised data. The effectiveness of text clustering largely depends on\nthe selection of textual embeddings and clustering algorithms. This study\nargues that recent advancements in large language models (LLMs) have the\npotential to enhance this task. The research investigates how different textual\nembeddings, particularly those utilised in LLMs, and various clustering\nalgorithms influence the clustering of text datasets. A series of experiments\nwere conducted to evaluate the impact of embeddings on clustering results, the\nrole of dimensionality reduction through summarisation, and the adjustment of\nmodel size. The findings indicate that LLM embeddings are superior at capturing\nsubtleties in structured language. OpenAI's GPT-3.5 Turbo model yields better\nresults in three out of five clustering metrics across most tested datasets.\nMost LLM embeddings show improvements in cluster purity and provide a more\ninformative silhouette score, reflecting a refined structural understanding of\ntext data compared to traditional methods. Among the more lightweight models,\nBERT demonstrates leading performance. Additionally, it was observed that\nincreasing model dimensionality and employing summarisation techniques do not\nconsistently enhance clustering efficiency, suggesting that these strategies\nrequire careful consideration for practical application. These results\nhighlight a complex balance between the need for refined text representation\nand computational feasibility in text clustering applications. This study\nextends traditional text clustering frameworks by integrating embeddings from\nLLMs, offering improved methodologies and suggesting new avenues for future\nresearch in various types of textual analysis.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50 (Primary), 62H30 (Secondary)",
      "I.2.6; I.2.7; I.7.m"
    ],
    "primary_category": "cs.CL",
    "comment": "The peer-reviewed version of this paper is published in the\n  International Journal of Cognitive Computing in Engineering at\n  https://doi.org/10.1016/j.ijcce.2024.11.004. This version is typeset by the\n  authors and differs only in pagination and typographical detail",
    "pdf_url": "http://arxiv.org/pdf/2403.15112v5",
    "published_date": "2024-03-22 11:08:48 UTC",
    "updated_date": "2024-12-02 19:26:56 UTC"
  },
  {
    "arxiv_id": "2403.15516v1",
    "title": "CTSM: Combining Trait and State Emotions for Empathetic Response Model",
    "authors": [
      "Wang Yufeng",
      "Chen Chao",
      "Yang Zhou",
      "Wang Shuhui",
      "Liao Xiangwen"
    ],
    "abstract": "Empathetic response generation endeavors to empower dialogue systems to\nperceive speakers' emotions and generate empathetic responses accordingly.\nPsychological research demonstrates that emotion, as an essential factor in\nempathy, encompasses trait emotions, which are static and context-independent,\nand state emotions, which are dynamic and context-dependent. However, previous\nstudies treat them in isolation, leading to insufficient emotional perception\nof the context, and subsequently, less effective empathetic expression. To\naddress this problem, we propose Combining Trait and State emotions for\nEmpathetic Response Model (CTSM). Specifically, to sufficiently perceive\nemotions in dialogue, we first construct and encode trait and state emotion\nembeddings, and then we further enhance emotional perception capability through\nan emotion guidance module that guides emotion representation. In addition, we\npropose a cross-contrastive learning decoder to enhance the model's empathetic\nexpression capability by aligning trait and state emotions between generated\nresponses and contexts. Both automatic and manual evaluation results\ndemonstrate that CTSM outperforms state-of-the-art baselines and can generate\nmore empathetic responses. Our code is available at\nhttps://github.com/wangyufeng-empty/CTSM",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 3 figures. Has been accepted by LREC-COLING2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15516v1",
    "published_date": "2024-03-22 10:45:13 UTC",
    "updated_date": "2024-03-22 10:45:13 UTC"
  },
  {
    "arxiv_id": "2403.15100v1",
    "title": "Subequivariant Reinforcement Learning Framework for Coordinated Motion Control",
    "authors": [
      "Haoyu Wang",
      "Xiaoyu Tan",
      "Xihe Qiu",
      "Chao Qu"
    ],
    "abstract": "Effective coordination is crucial for motion control with reinforcement\nlearning, especially as the complexity of agents and their motions increases.\nHowever, many existing methods struggle to account for the intricate\ndependencies between joints. We introduce CoordiGraph, a novel architecture\nthat leverages subequivariant principles from physics to enhance coordination\nof motion control with reinforcement learning. This method embeds the\nprinciples of equivariance as inherent patterns in the learning process under\ngravity influence, which aids in modeling the nuanced relationships between\njoints vital for motion control. Through extensive experimentation with\nsophisticated agents in diverse environments, we highlight the merits of our\napproach. Compared to current leading methods, CoordiGraph notably enhances\ngeneralization and sample efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 7 figures, 2024 IEEE International Conference on Robotics\n  and Automation",
    "pdf_url": "http://arxiv.org/pdf/2403.15100v1",
    "published_date": "2024-03-22 10:39:22 UTC",
    "updated_date": "2024-03-22 10:39:22 UTC"
  },
  {
    "arxiv_id": "2403.15097v2",
    "title": "Argument-Aware Approach To Event Linking",
    "authors": [
      "I-Hung Hsu",
      "Zihan Xue",
      "Nilay Pochh",
      "Sahil Bansal",
      "Premkumar Natarajan",
      "Jayanth Srinivasa",
      "Nanyun Peng"
    ],
    "abstract": "Event linking connects event mentions in text with relevant nodes in a\nknowledge base (KB). Prior research in event linking has mainly borrowed\nmethods from entity linking, overlooking the distinct features of events.\nCompared to the extensively explored entity linking task, events have more\ncomplex structures and can be more effectively distinguished by examining their\nassociated arguments. Moreover, the information-rich nature of events leads to\nthe scarcity of event KBs. This emphasizes the need for event linking models to\nidentify and classify event mentions not in the KB as ``out-of-KB,'' an area\nthat has received limited attention. In this work, we tackle these challenges\nby introducing an argument-aware approach. First, we improve event linking\nmodels by augmenting input text with tagged event argument information,\nfacilitating the recognition of key information about event mentions.\nSubsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize\nout-of-KB training examples from in-KB instances through controlled\nmanipulation of event arguments. Our experiment across two test datasets showed\nsignificant enhancements in both in-KB and out-of-KB scenarios, with a notable\n22% improvement in out-of-KB evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted by ACL-findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15097v2",
    "published_date": "2024-03-22 10:32:43 UTC",
    "updated_date": "2024-06-06 05:18:25 UTC"
  },
  {
    "arxiv_id": "2403.15091v1",
    "title": "Improved Long Short-Term Memory-based Wastewater Treatment Simulators for Deep Reinforcement Learning",
    "authors": [
      "Esmaeel Mohammadi",
      "Daniel Ortiz-Arroyo",
      "Mikkel Stokholm-Bjerregaard",
      "Aviaja Anna Hansen",
      "Petar Durdevic"
    ],
    "abstract": "Even though Deep Reinforcement Learning (DRL) showed outstanding results in\nthe fields of Robotics and Games, it is still challenging to implement it in\nthe optimization of industrial processes like wastewater treatment. One of the\nchallenges is the lack of a simulation environment that will represent the\nactual plant as accurately as possible to train DRL policies. Stochasticity and\nnon-linearity of wastewater treatment data lead to unstable and incorrect\npredictions of models over long time horizons. One possible reason for the\nmodels' incorrect simulation behavior can be related to the issue of\ncompounding error, which is the accumulation of errors throughout the\nsimulation. The compounding error occurs because the model utilizes its\npredictions as inputs at each time step. The error between the actual data and\nthe prediction accumulates as the simulation continues. We implemented two\nmethods to improve the trained models for wastewater treatment data, which\nresulted in more accurate simulators: 1- Using the model's prediction data as\ninput in the training step as a tool of correction, and 2- Change in the loss\nfunction to consider the long-term predicted shape (dynamics). The experimental\nresults showed that implementing these methods can improve the behavior of\nsimulators in terms of Dynamic Time Warping throughout a year up to 98%\ncompared to the base model. These improvements demonstrate significant promise\nin creating simulators for biological processes that do not need pre-existing\nknowledge of the process but instead depend exclusively on time series data\nobtained from the system.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15091v1",
    "published_date": "2024-03-22 10:20:09 UTC",
    "updated_date": "2024-03-22 10:20:09 UTC"
  },
  {
    "arxiv_id": "2403.15076v1",
    "title": "Comprehensive Lipidomic Automation Workflow using Large Language Models",
    "authors": [
      "Connor Beveridge",
      "Sanjay Iyer",
      "Caitlin E. Randolph",
      "Matthew Muhoberac",
      "Palak Manchanda",
      "Amy C. Clingenpeel",
      "Shane Tichy",
      "Gaurav Chopra"
    ],
    "abstract": "Lipidomics generates large data that makes manual annotation and\ninterpretation challenging. Lipid chemical and structural diversity with\nstructural isomers further complicates annotation. Although, several commercial\nand open-source software for targeted lipid identification exists, it lacks\nautomated method generation workflows and integration with statistical and\nbioinformatics tools. We have developed the Comprehensive Lipidomic Automated\nWorkflow (CLAW) platform with integrated workflow for parsing, detailed\nstatistical analysis and lipid annotations based on custom multiple reaction\nmonitoring (MRM) precursor and product ion pair transitions. CLAW contains\nseveral modules including identification of carbon-carbon double bond\nposition(s) in unsaturated lipids when combined with ozone electrospray\nionization (OzESI)-MRM methodology. To demonstrate the utility of the automated\nworkflow in CLAW, large-scale lipidomics data was collected with traditional\nand OzESI-MRM profiling on biological and non-biological samples. Specifically,\na total of 1497 transitions organized into 10 MRM-based mass spectrometry\nmethods were used to profile lipid droplets isolated from different brain\nregions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type\ncontrols. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon\ndouble bond specificity were generated from canola oil samples using OzESI-MRM\nprofiling. We also developed an integrated language user interface with large\nlanguage models using artificially intelligent (AI) agents that permits users\nto interact with the CLAW platform using a chatbot terminal to perform\nstatistical and bioinformatic analyses. We envision CLAW pipeline to be used in\nhigh-throughput lipid structural identification tasks aiding users to generate\nautomated lipidomics workflows ranging from data acquisition to AI agent-based\nbioinformatic analysis.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "q-bio.BM",
      "q-bio.SC"
    ],
    "primary_category": "q-bio.QM",
    "comment": "53 pages, 4 main figures, 23 Supporting figures, 10 Supporting Tables",
    "pdf_url": "http://arxiv.org/pdf/2403.15076v1",
    "published_date": "2024-03-22 10:00:52 UTC",
    "updated_date": "2024-03-22 10:00:52 UTC"
  },
  {
    "arxiv_id": "2403.15075v1",
    "title": "Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation",
    "authors": [
      "Jiaheng Yu",
      "Jing Li",
      "Yue He",
      "Kai Zhu",
      "Shuyi Zhang",
      "Wen Hu"
    ],
    "abstract": "Recent methods utilize graph contrastive Learning within graph-structured\nuser-item interaction data for collaborative filtering and have demonstrated\ntheir efficacy in recommendation tasks. However, they ignore that the\ndifference relation density of nodes between the user- and item-side causes the\nadaptability of graphs on bilateral nodes to be different after multi-hop graph\ninteraction calculation, which limits existing models to achieve ideal results.\nTo solve this issue, we propose a novel framework for recommendation tasks\ncalled Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that\nconsider the bilateral unsymmetry on user-item node relation density for sliced\nuser and item graph reasoning better with bilateral slicing contrastive\ntraining. Especially, taking into account the aggregation ability of\nhypergraph-based graph convolutional network (GCN) in digging implicit\nsimilarities is more suitable for user nodes, embeddings generated from three\ndifferent modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into\ntwo subviews by the user- and item-side respectively, and selectively combined\ninto subview pairs bilaterally based on the characteristics of inter-node\nrelation structure. Furthermore, to align the distribution of user and item\nembeddings after aggregation, a dispersing loss is leveraged to adjust the\nmutual distance between all embeddings for maintaining learning ability.\nComprehensive experiments on two public datasets have proved the superiority of\nBusGCL in comparison to various recommendation methods. Other models can simply\nutilize our bilateral slicing contrastive learning to enhance recommending\nperformance without incurring extra expenses.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15075v1",
    "published_date": "2024-03-22 09:58:33 UTC",
    "updated_date": "2024-03-22 09:58:33 UTC"
  },
  {
    "arxiv_id": "2403.15059v1",
    "title": "MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration",
    "authors": [
      "Zhichao Wei",
      "Qingkun Su",
      "Long Qin",
      "Weizhi Wang"
    ],
    "abstract": "Recent advances in tuning-free personalized image generation based on\ndiffusion models are impressive. However, to improve subject fidelity, existing\nmethods either retrain the diffusion model or infuse it with dense visual\nembeddings, both of which suffer from poor generalization and efficiency. Also,\nthese methods falter in multi-subject image generation due to the unconstrained\ncross-attention mechanism. In this paper, we propose MM-Diff, a unified and\ntuning-free image personalization framework capable of generating high-fidelity\nimages of both single and multiple subjects in seconds. Specifically, to\nsimultaneously enhance text consistency and subject fidelity, MM-Diff employs a\nvision encoder to transform the input image into CLS and patch embeddings. CLS\nembeddings are used on the one hand to augment the text embeddings, and on the\nother hand together with patch embeddings to derive a small number of\ndetail-rich subject embeddings, both of which are efficiently integrated into\nthe diffusion model through the well-designed multimodal cross-attention\nmechanism. Additionally, MM-Diff introduces cross-attention map constraints\nduring the training phase, ensuring flexible multi-subject image sampling\nduring inference without any predefined inputs (e.g., layout). Extensive\nexperiments demonstrate the superior performance of MM-Diff over other leading\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15059v1",
    "published_date": "2024-03-22 09:32:31 UTC",
    "updated_date": "2024-03-22 09:32:31 UTC"
  },
  {
    "arxiv_id": "2403.15049v2",
    "title": "Continual Vision-and-Language Navigation",
    "authors": [
      "Seongjun Jeong",
      "Gi-Cheon Kang",
      "Seongho Choi",
      "Joochan Kim",
      "Byoung-Tak Zhang"
    ],
    "abstract": "In developing Vision-and-Language Navigation (VLN) agents that navigate to a\ndestination using natural language instructions and visual cues, current\nstudies largely assume a \\textit{train-once-deploy-once strategy}. We argue\nthat this kind of strategy is less realistic, as deployed VLN agents are\nexpected to encounter novel environments continuously through their lifetime.\nTo facilitate more realistic setting for VLN agents, we propose Continual\nVision-and-Language Navigation (CVLN) paradigm for agents to continually learn\nand adapt to changing environments. In CVLN, the agents are trained and\nevaluated incrementally across multiple \\textit{scene domains} (i.e.,\nenvironments). We present two CVLN learning setups to consider diverse forms of\nnatural language instructions: Initial-instruction based CVLN, focused on\nnavigation via initial-instruction interpretation, and dialogue-based CVLN,\ndesigned for navigation through dialogue with other agents. We introduce two\nsimple yet effective baseline methods, tailored to the sequential\ndecision-making needs of CVLN: Perplexity Replay (PerpR) and Episodic\nSelf-Replay (ESR), both employing a rehearsal mechanism. PerpR selects replay\nepisodes based on episode difficulty, while ESR stores and revisits action\nlogits from individual episode steps during training to refine learning.\nExperimental results indicate that while existing continual learning methods\nare insufficient for CVLN, PerpR and ESR outperform the comparison methods by\neffectively utilizing replay memory.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15049v2",
    "published_date": "2024-03-22 09:15:36 UTC",
    "updated_date": "2024-12-21 09:05:50 UTC"
  },
  {
    "arxiv_id": "2403.15048v3",
    "title": "Make VLM Recognize Visual Hallucination on Cartoon Character Image with Pose Information",
    "authors": [
      "Bumsoo Kim",
      "Wonseop Shin",
      "Kyuchul Lee",
      "Yonghoon Jung",
      "Sanghyun Seo"
    ],
    "abstract": "Leveraging large-scale Text-to-Image (TTI) models have become a common\ntechnique for generating exemplar or training dataset in the fields of image\nsynthesis, video editing, 3D reconstruction. However, semantic structural\nvisual hallucinations involving perceptually severe defects remain a concern,\nespecially in the domain of non-photorealistic rendering (NPR) such as cartoons\nand pixelization-style character. To detect these hallucinations in NPR, We\npropose a novel semantic structural hallucination detection system using\nVision-Language Model (VLM). Our approach is to leverage the emerging\ncapability of large language model, in-context learning which denotes that VLM\nhas seen some examples by user for specific downstream task, here hallucination\ndetection. Based on in-context learning, we introduce pose-aware in-context\nvisual learning (PA-ICVL) which improve the overall performance of VLM by\nfurther inputting visual data beyond prompts, RGB images and pose information.\nBy incorporating pose guidance, we enable VLMs to make more accurate decisions.\nExperimental results demonstrate significant improvements in identifying visual\nhallucinations compared to baseline methods relying solely on RGB images.\nWithin selected two VLMs, GPT-4v, Gemini pro vision, our proposed PA-ICVL\nimproves the hallucination detection with 50% to 78%, 57% to 80%, respectively.\nThis research advances a capability of TTI models toward real-world\napplications by mitigating visual hallucinations via in-context visual\nlearning, expanding their potential in non-photorealistic domains. In addition,\nit showcase how users can boost the downstream-specialized capability of open\nVLM by harnessing additional conditions. We collect synthetic\ncartoon-hallucination dataset with TTI models, this dataset and final tuned VLM\nwill be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025, Project page:\n  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/",
    "pdf_url": "http://arxiv.org/pdf/2403.15048v3",
    "published_date": "2024-03-22 09:13:09 UTC",
    "updated_date": "2025-01-22 05:46:56 UTC"
  },
  {
    "arxiv_id": "2403.15044v1",
    "title": "Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour Analysis In-the-wild",
    "authors": [
      "Zhuofan Wen",
      "Fengyu Zhang",
      "Siyuan Zhang",
      "Haiyang Sun",
      "Mingyu Xu",
      "Licai Sun",
      "Zheng Lian",
      "Bin Liu",
      "Jianhua Tao"
    ],
    "abstract": "Multimodal fusion is a significant method for most multimodal tasks. With the\nrecent surge in the number of large pre-trained models, combining both\nmultimodal fusion methods and pre-trained model features can achieve\noutstanding performance in many multimodal tasks. In this paper, we present our\napproach, which leverages both advantages for addressing the task of Expression\n(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the\nAff-Wild2 database using pre-trained models, then extract the final hidden\nlayers of the models as features. Following preprocessing and interpolation or\nconvolution to align the extracted features, different models are employed for\nmodal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15044v1",
    "published_date": "2024-03-22 09:00:24 UTC",
    "updated_date": "2024-03-22 09:00:24 UTC"
  },
  {
    "arxiv_id": "2405.10948v3",
    "title": "Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery",
    "authors": [
      "Guankun Wang",
      "Long Bai",
      "Wan Jun Nah",
      "Jie Wang",
      "Zhaoxi Zhang",
      "Zhen Chen",
      "Jinlin Wu",
      "Mobarakol Islam",
      "Hongbin Liu",
      "Hongliang Ren"
    ],
    "abstract": "Recent advancements in Surgical Visual Question Answering (Surgical-VQA) and\nrelated region grounding have shown great promise for robotic and medical\napplications, addressing the critical need for automated methods in\npersonalized surgical mentorship. However, existing models primarily provide\nsimple structured answers and struggle with complex scenarios due to their\nlimited capability in recognizing long-range dependencies and aligning\nmultimodal information. In this paper, we introduce Surgical-LVLM, a novel\npersonalized large vision-language model tailored for complex surgical\nscenarios. Leveraging the pre-trained large vision-language model and\nspecialized Visual Perception LoRA (VP-LoRA) blocks, our model excels in\nunderstanding complex visual-language tasks within surgical contexts. In\naddressing the visual grounding task, we propose the Token-Interaction (TIT)\nmodule, which strengthens the interaction between the grounding module and the\nlanguage responses of the Large Visual Language Model (LVLM) after projecting\nthem into the latent space. We demonstrate the effectiveness of Surgical-LVLM\non several benchmarks, including EndoVis-17-VQLA, EndoVis-18-VQLA, and a newly\nintroduced EndoVis Conversations dataset, which sets new performance standards.\nOur work contributes to advancing the field of automated surgical mentorship by\nproviding a context-aware solution.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "The manuscript is accepted by ICLR 2025 FM-Wild Workshop",
    "pdf_url": "http://arxiv.org/pdf/2405.10948v3",
    "published_date": "2024-03-22 08:38:27 UTC",
    "updated_date": "2025-03-16 02:23:30 UTC"
  },
  {
    "arxiv_id": "2403.15027v2",
    "title": "Grey-informed neural network for time-series forecasting",
    "authors": [
      "Wanli Xie",
      "Ruibin Zhao",
      "Zhenguo Xu",
      "Tingting Liang"
    ],
    "abstract": "Neural network models have shown outstanding performance and successful\nresolutions to complex problems in various fields. However, the majority of\nthese models are viewed as black-box, requiring a significant amount of data\nfor development. Consequently, in situations with limited data, constructing\nappropriate models becomes challenging due to the lack of transparency and\nscarcity of data. To tackle these challenges, this study suggests the\nimplementation of a grey-informed neural network (GINN). The GINN ensures that\nthe output of the neural network follows the differential equation model of the\ngrey system, improving interpretability. Moreover, incorporating prior\nknowledge from grey system theory enables traditional neural networks to\neffectively handle small data samples. Our proposed model has been observed to\nuncover underlying patterns in the real world and produce reliable forecasts\nbased on empirical data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15027v2",
    "published_date": "2024-03-22 08:17:00 UTC",
    "updated_date": "2024-04-03 09:51:29 UTC"
  },
  {
    "arxiv_id": "2403.14999v1",
    "title": "Magic for the Age of Quantized DNNs",
    "authors": [
      "Yoshihide Sawada",
      "Ryuji Saiin",
      "Kazuma Suetake"
    ],
    "abstract": "Recently, the number of parameters in DNNs has explosively increased, as\nexemplified by LLMs (Large Language Models), making inference on small-scale\ncomputers more difficult. Model compression technology is, therefore, essential\nfor integration into products. In this paper, we propose a method of\nquantization-aware training. We introduce a novel normalization (Layer-Batch\nNormalization) that is independent of the mini-batch size and does not require\nany additional computation cost during inference. Then, we quantize the weights\nby the scaled round-clip function with the weight standardization. We also\nquantize activation functions using the same function and apply surrogate\ngradients to train the model with both quantized weights and the quantized\nactivation functions. We call this method Magic for the age of Quantised DNNs\n(MaQD). Experimental results show that our quantization method can be achieved\nwith minimal accuracy degradation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.14999v1",
    "published_date": "2024-03-22 07:21:09 UTC",
    "updated_date": "2024-03-22 07:21:09 UTC"
  },
  {
    "arxiv_id": "2403.14977v1",
    "title": "Piecewise-Linear Manifolds for Deep Metric Learning",
    "authors": [
      "Shubhang Bhatnagar",
      "Narendra Ahuja"
    ],
    "abstract": "Unsupervised deep metric learning (UDML) focuses on learning a semantic\nrepresentation space using only unlabeled data. This challenging problem\nrequires accurately estimating the similarity between data points, which is\nused to supervise a deep network. For this purpose, we propose to model the\nhigh-dimensional data manifold using a piecewise-linear approximation, with\neach low-dimensional linear piece approximating the data manifold in a small\nneighborhood of a point. These neighborhoods are used to estimate similarity\nbetween data points. We empirically show that this similarity estimate\ncorrelates better with the ground truth than the similarity estimates of\ncurrent state-of-the-art techniques. We also show that proxies, commonly used\nin supervised metric learning, can be used to model the piecewise-linear\nmanifold in an unsupervised setting, helping improve performance. Our method\noutperforms existing unsupervised metric learning approaches on standard\nzero-shot image retrieval benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at CPAL 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2403.14977v1",
    "published_date": "2024-03-22 06:22:20 UTC",
    "updated_date": "2024-03-22 06:22:20 UTC"
  },
  {
    "arxiv_id": "2403.14972v2",
    "title": "A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning",
    "authors": [
      "Changmeng Zheng",
      "Dayong Liang",
      "Wengyu Zhang",
      "Xiao-Yong Wei",
      "Tat-Seng Chua",
      "Qing Li"
    ],
    "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ACM Multimedia 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14972v2",
    "published_date": "2024-03-22 06:03:07 UTC",
    "updated_date": "2024-08-06 09:45:27 UTC"
  },
  {
    "arxiv_id": "2403.14965v1",
    "title": "Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation",
    "authors": [
      "Shanthi Karpurapu",
      "Sravanthy Myneni",
      "Unnati Nettur",
      "Likhit Sagar Gajja",
      "Dave Burke",
      "Tom Stiehm",
      "Jeffery Payne"
    ],
    "abstract": "Behavior-driven development (BDD) is an Agile testing methodology fostering\ncollaboration among developers, QA analysts, and stakeholders. In this\nmanuscript, we propose a novel approach to enhance BDD practices using large\nlanguage models (LLMs) to automate acceptance test generation. Our study uses\nzero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,\nand PaLM-2. The paper presents a detailed methodology that includes the\ndataset, prompt techniques, LLMs, and the evaluation process. The results\ndemonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests\nwith better performance. The few-shot prompt technique highlights its ability\nto provide higher accuracy by incorporating examples for in-context learning.\nFurthermore, the study examines syntax errors, validation accuracy, and\ncomparative analysis of LLMs, revealing their effectiveness in enhancing BDD\npractices. However, our study acknowledges that there are limitations to the\nproposed approach. We emphasize that this approach can support collaborative\nBDD processes and create opportunities for future research into automated BDD\nacceptance test generation using LLMs.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "I.2.7; I.2.1"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14965v1",
    "published_date": "2024-03-22 05:37:52 UTC",
    "updated_date": "2024-03-22 05:37:52 UTC"
  },
  {
    "arxiv_id": "2403.15512v1",
    "title": "Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation",
    "authors": [
      "Kyohoon Jin",
      "Junho Lee",
      "Juhwan Choi",
      "Sangmin Song",
      "Youngbin Kim"
    ],
    "abstract": "Efforts to leverage deep learning models in low-resource regimes have led to\nnumerous augmentation studies. However, the direct application of methods such\nas mixup and cutout to text data, is limited due to their discrete\ncharacteristics. While methods using pretrained language models have exhibited\nefficiency, they require additional considerations for robustness. Inspired by\nrecent studies on decision boundaries, this paper proposes a\ndecision-boundary-aware data augmentation strategy to enhance robustness using\npretrained language models. The proposed technique first focuses on shifting\nthe latent features closer to the decision boundary, followed by reconstruction\nto generate an ambiguous version with a soft label. Additionally, mid-K\nsampling is suggested to enhance the diversity of the generated sentences. This\npaper demonstrates the performance of the proposed augmentation strategy\ncompared to other methods through extensive experiments. Furthermore, the\nablation study reveals the effect of soft labels and mid-K sampling and the\nextensibility of the method with curriculum data augmentation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.15512v1",
    "published_date": "2024-03-22 05:18:08 UTC",
    "updated_date": "2024-03-22 05:18:08 UTC"
  },
  {
    "arxiv_id": "2403.14952v1",
    "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
    "authors": [
      "Zhenrui Yue",
      "Huimin Zeng",
      "Yimeng Lu",
      "Lanyu Shang",
      "Yang Zhang",
      "Dong Wang"
    ],
    "abstract": "The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14952v1",
    "published_date": "2024-03-22 05:05:45 UTC",
    "updated_date": "2024-03-22 05:05:45 UTC"
  },
  {
    "arxiv_id": "2403.14951v2",
    "title": "Simple Graph Condensation",
    "authors": [
      "Zhenbang Xiao",
      "Yu Wang",
      "Shunyu Liu",
      "Huiqiong Wang",
      "Mingli Song",
      "Tongya Zheng"
    ],
    "abstract": "The burdensome training costs on large-scale graphs have aroused significant\ninterest in graph condensation, which involves tuning Graph Neural Networks\n(GNNs) on a small condensed graph for use on the large-scale original graph.\nExisting methods primarily focus on aligning key metrics between the condensed\nand original graphs, such as gradients, output distribution and trajectories of\nGNNs, yielding satisfactory performance on downstream tasks. However, these\ncomplex metrics necessitate intricate external parameters and can potentially\ndisrupt the optimization process of the condensation graph, making the\ncondensation process highly demanding and unstable. Motivated by the recent\nsuccess of simplified models across various domains, we propose a simplified\napproach to metric alignment in graph condensation, aiming to reduce\nunnecessary complexity inherited from intricate metrics. We introduce the\nSimple Graph Condensation (SimGC) framework, which aligns the condensed graph\nwith the original graph from the input layer to the prediction layer, guided by\na pre-trained Simple Graph Convolution (SGC) model on the original graph.\nImportantly, SimGC eliminates external parameters and exclusively retains the\ntarget condensed graph during the condensation process. This straightforward\nyet effective strategy achieves a significant speedup of up to 10 times\ncompared to existing graph condensation methods while performing on par with\nstate-of-the-art baselines. Comprehensive experiments conducted on seven\nbenchmark datasets demonstrate the effectiveness of SimGC in prediction\naccuracy, condensation time, and generalization capability. Our code is\navailable at https://github.com/BangHonor/SimGC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "ECML-PKDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14951v2",
    "published_date": "2024-03-22 05:04:48 UTC",
    "updated_date": "2024-07-18 03:34:50 UTC"
  },
  {
    "arxiv_id": "2403.14946v1",
    "title": "A Single Linear Layer Yields Task-Adapted Low-Rank Matrices",
    "authors": [
      "Hwichan Kim",
      "Shota Sasaki",
      "Sho Hoshino",
      "Ukyo Honda"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning\n(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix\n$\\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study\nsuggested that there is correlation between $W_0$ and $\\Delta W$. In this\nstudy, we aim to delve deeper into relationships between $W_0$ and low-rank\nmatrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,\nwe analyze a conversion matrix that transform $W_0$ into low-rank matrices,\nwhich encapsulates information about the relationships. Our analysis reveals\nthat the conversion matrices are similar across each layer. Inspired by these\nfindings, we hypothesize that a single linear layer, which takes each layer's\n$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this\nhypothesis, we devise a method named Conditionally Parameterized LoRA\n(CondLoRA) that updates initial weight matrices with low-rank matrices derived\nfrom a single linear layer. Our empirical results show that CondLoRA maintains\na performance on par with LoRA, despite the fact that the trainable parameters\nof CondLoRA are fewer than those of LoRA. Therefore, we conclude that \"a single\nlinear layer yields task-adapted low-rank matrices.\"",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14946v1",
    "published_date": "2024-03-22 04:38:42 UTC",
    "updated_date": "2024-03-22 04:38:42 UTC"
  },
  {
    "arxiv_id": "2404.04271v1",
    "title": "Towards Effective Next POI Prediction: Spatial and Semantic Augmentation with Remote Sensing Data",
    "authors": [
      "Nan Jiang",
      "Haitao Yuan",
      "Jianing Si",
      "Minxiao Chen",
      "Shangguang Wang"
    ],
    "abstract": "The next point-of-interest (POI) prediction is a significant task in\nlocation-based services, yet its complexity arises from the consolidation of\nspatial and semantic intent. This fusion is subject to the influences of\nhistorical preferences, prevailing location, and environmental factors, thereby\nposing significant challenges. In addition, the uneven POI distribution further\ncomplicates the next POI prediction procedure. To address these challenges, we\nenrich input features and propose an effective deep-learning method within a\ntwo-step prediction framework. Our method first incorporates remote sensing\ndata, capturing pivotal environmental context to enhance input features\nregarding both location and semantics. Subsequently, we employ a region\nquad-tree structure to integrate urban remote sensing, road network, and POI\ndistribution spaces, aiming to devise a more coherent graph representation\nmethod for urban spatial. Leveraging this method, we construct the QR-P graph\nfor the user's historical trajectories to encapsulate historical travel\nknowledge, thereby augmenting input features with comprehensive spatial and\nsemantic insights. We devise distinct embedding modules to encode these\nfeatures and employ an attention mechanism to fuse diverse encodings. In the\ntwo-step prediction procedure, we initially identify potential spatial zones by\npredicting user-preferred tiles, followed by pinpointing specific POIs of a\ndesignated type within the projected tiles. Empirical findings from four\nreal-world location-based social network datasets underscore the remarkable\nsuperiority of our proposed approach over competitive baseline methods.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.IR",
    "comment": "12 pages, 11 figures, Accepted by ICDE 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.04271v1",
    "published_date": "2024-03-22 04:22:36 UTC",
    "updated_date": "2024-03-22 04:22:36 UTC"
  },
  {
    "arxiv_id": "2403.14941v1",
    "title": "Unifying Lane-Level Traffic Prediction from a Graph Structural Perspective: Benchmark and Baseline",
    "authors": [
      "Shuhao Li",
      "Yue Cui",
      "Jingyi Xu",
      "Libin Li",
      "Lingkai Meng",
      "Weidong Yang",
      "Fan Zhang",
      "Xiaofang Zhou"
    ],
    "abstract": "Traffic prediction has long been a focal and pivotal area in research,\nwitnessing both significant strides from city-level to road-level predictions\nin recent years. With the advancement of Vehicle-to-Everything (V2X)\ntechnologies, autonomous driving, and large-scale models in the traffic domain,\nlane-level traffic prediction has emerged as an indispensable direction.\nHowever, further progress in this field is hindered by the absence of\ncomprehensive and unified evaluation standards, coupled with limited public\navailability of data and code. This paper extensively analyzes and categorizes\nexisting research in lane-level traffic prediction, establishes a unified\nspatial topology structure and prediction tasks, and introduces a simple\nbaseline model, GraphMLP, based on graph structure and MLP networks. We have\nreplicated codes not publicly available in existing studies and, based on this,\nthoroughly and fairly assessed various models in terms of effectiveness,\nefficiency, and applicability, providing insights for practical applications.\nAdditionally, we have released three new datasets and corresponding codes to\naccelerate progress in this field, all of which can be found on\nhttps://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14941v1",
    "published_date": "2024-03-22 04:21:40 UTC",
    "updated_date": "2024-03-22 04:21:40 UTC"
  },
  {
    "arxiv_id": "2403.15511v1",
    "title": "Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems",
    "authors": [
      "Phai Vu Dinh",
      "Diep N. Nguyen",
      "Dinh Thai Hoang",
      "Quang Uy Nguyen",
      "Eryk Dutkiewicz",
      "Son Pham Bao"
    ],
    "abstract": "While intrusion detection systems (IDSs) benefit from the diversity and\ngeneralization of IoT data features, the data diversity (e.g., the\nheterogeneity and high dimensions of data) also makes it difficult to train\neffective machine learning models in IoT IDSs. This also leads to potentially\nredundant/noisy features that may decrease the accuracy of the detection engine\nin IDSs. This paper first introduces a novel neural network architecture called\nMultiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that\ncan process inputs from different sources with different characteristics. The\nMIAE model is trained in an unsupervised learning mode to transform the\nheterogeneous inputs into lower-dimensional representation, which helps\nclassifiers distinguish between normal behaviour and different types of\nattacks. To distil and retain more relevant features but remove less\nimportant/redundant ones during the training process, we further design and\nembed a feature selection layer right after the representation layer of MIAE\nresulting in a new model called MIAEFS. This layer learns the importance of\nfeatures in the representation vector, facilitating the selection of\ninformative features from the representation vector. The results on three IDS\ndatasets, i.e., NSLKDD, UNSW-NB15, and IDS2017, show the superior performance\nof MIAE and MIAEFS compared to other methods, e.g., conventional classifiers,\ndimensionality reduction models, unsupervised representation learning methods\nwith different input dimensions, and unsupervised feature selection models.\nMoreover, MIAE and MIAEFS combined with the Random Forest (RF) classifier\nachieve accuracy of 96.5% in detecting sophisticated attacks, e.g., Slowloris.\nThe average running time for detecting an attack sample using RF with the\nrepresentation of MIAE and MIAEFS is approximate 1.7E-6 seconds, whilst the\nmodel size is lower than 1 MB.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15511v1",
    "published_date": "2024-03-22 03:54:04 UTC",
    "updated_date": "2024-03-22 03:54:04 UTC"
  },
  {
    "arxiv_id": "2403.15509v2",
    "title": "Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection",
    "authors": [
      "Phai Vu Dinh",
      "Quang Uy Nguyen",
      "Thai Hoang Dinh",
      "Diep N. Nguyen",
      "Bao Son Pham",
      "Eryk Dutkiewicz"
    ],
    "abstract": "Representation learning (RL) methods for cyberattack detection face the\ndiversity and sophistication of attack data, leading to the issue of mixed\nrepresentations of different classes, particularly as the number of classes\nincreases. To address this, the paper proposes a novel deep learning\narchitecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input\ndata into latent space and then deterministically shifts data samples of\ndifferent classes further apart to create separable data representations,\nreferred to as representation targets. TAE's decoder then projects the input\ndata into these representation targets. After training, TAE's decoder extracts\ndata representations. TAE's representation target serves as a novel dynamic\ncodeword, which refers to the vector that represents a specific class. This\nvector is updated after each training epoch for every data sample, in contrast\nto the conventional fixed codeword that does not incorporate information from\nthe input data. We conduct extensive experiments on diverse cybersecurity\ndatasets, including seven IoT botnet datasets, two network IDS datasets, three\nmalware datasets, one cloud DDoS dataset, and ten artificial datasets as the\nnumber of classes increases. TAE boosts accuracy and F-score in attack\ndetection by around 2% compared to state-of-the-art models, achieving up to\n96.1% average accuracy in IoT attack detection. Additionally, TAE is\nwell-suited for cybersecurity applications and potentially for IoT systems,\nwith a model size of approximately 1 MB and an average running time of around\n2.6E-07 seconds for extracting a data sample.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15509v2",
    "published_date": "2024-03-22 03:39:40 UTC",
    "updated_date": "2025-04-28 22:51:30 UTC"
  },
  {
    "arxiv_id": "2403.14932v3",
    "title": "Extending Token Computation for LLM Reasoning",
    "authors": [
      "Bingli Liao",
      "Danilo Vasconcellos Vargas"
    ],
    "abstract": "Large Language Models (LLMs) are pivotal in advancing natural language\nprocessing but often struggle with complex reasoning tasks due to inefficient\nattention distributions. In this paper, we explore the effect of increased\ncomputed tokens on LLM performance and introduce a novel method for extending\ncomputed tokens in the Chain-of-Thought (CoT) process, utilizing attention\nmechanism optimization. By fine-tuning an LLM on a domain-specific, highly\nstructured dataset, we analyze attention patterns across layers, identifying\ninefficiencies caused by non-semantic tokens with outlier high attention\nscores. To address this, we propose an algorithm that emulates early layer\nattention patterns across downstream layers to re-balance skewed attention\ndistributions and enhance knowledge abstraction. Our findings demonstrate that\nour approach not only facilitates a deeper understanding of the internal\ndynamics of LLMs but also significantly improves their reasoning capabilities,\nparticularly in non-STEM domains. Our study lays the groundwork for further\ninnovations in LLM design, aiming to create more powerful, versatile, and\nresponsible models capable of tackling a broad range of real-world\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14932v3",
    "published_date": "2024-03-22 03:23:58 UTC",
    "updated_date": "2024-06-23 15:50:48 UTC"
  },
  {
    "arxiv_id": "2403.14919v1",
    "title": "Hierarchical Skip Decoding for Efficient Autoregressive Text Generation",
    "authors": [
      "Yunqi Zhu",
      "Xuebing Yang",
      "Yuanyuan Wu",
      "Wensheng Zhang"
    ],
    "abstract": "Autoregressive decoding strategy is a commonly used method for text\ngeneration tasks with pre-trained language models, while early-exiting is an\neffective approach to speedup the inference stage. In this work, we propose a\nnovel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient\nautoregressive text generation. Different from existing methods that require\nadditional trainable components, HSD is a plug-and-play method applicable to\nautoregressive text generation models, it adaptively skips decoding layers in a\nhierarchical manner based on the current sequence length, thereby reducing\ncomputational workload and allocating computation resources. Comprehensive\nexperiments on five text generation datasets with pre-trained language models\ndemonstrate HSD's advantages in balancing efficiency and text quality. With\nalmost half of the layers skipped, HSD can sustain 90% of the text quality\ncompared to vanilla autoregressive decoding, outperforming the competitive\napproaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14919v1",
    "published_date": "2024-03-22 02:44:05 UTC",
    "updated_date": "2024-03-22 02:44:05 UTC"
  },
  {
    "arxiv_id": "2404.07960v1",
    "title": "Content Knowledge Identification with Multi-Agent Large Language Models (LLMs)",
    "authors": [
      "Kaiqi Yang",
      "Yucheng Chu",
      "Taylor Darwin",
      "Ahreum Han",
      "Hang Li",
      "Hongzhi Wen",
      "Yasemin Copur-Gencturk",
      "Jiliang Tang",
      "Hui Liu"
    ],
    "abstract": "Teachers' mathematical content knowledge (CK) is of vital importance and need\nin teacher professional development (PD) programs. Computer-aided asynchronous\nPD systems are the most recent proposed PD techniques, which aim to help\nteachers improve their PD equally with fewer concerns about costs and\nlimitations of time or location. However, current automatic CK identification\nmethods, which serve as one of the core techniques of asynchronous PD systems,\nface challenges such as diversity of user responses, scarcity of high-quality\nannotated data, and low interpretability of the predictions. To tackle these\nchallenges, we propose a Multi-Agent LLMs-based framework, LLMAgent-CK, to\nassess the user responses' coverage of identified CK learning goals without\nhuman annotations. By taking advantage of multi-agent LLMs in strong\ngeneralization ability and human-like discussions, our proposed LLMAgent-CK\npresents promising CK identifying performance on a real-world mathematical CK\ndataset MaCKT. Moreover, our case studies further demonstrate the working of\nthe multi-agent framework.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.07960v1",
    "published_date": "2024-03-22 02:37:33 UTC",
    "updated_date": "2024-03-22 02:37:33 UTC"
  },
  {
    "arxiv_id": "2404.08654v1",
    "title": "Optimal path for Biomedical Text Summarization Using Pointer GPT",
    "authors": [
      "Hyunkyung Han",
      "Jaesik Choi"
    ],
    "abstract": "Biomedical text summarization is a critical tool that enables clinicians to\neffectively ascertain patient status. Traditionally, text summarization has\nbeen accomplished with transformer models, which are capable of compressing\nlong documents into brief summaries. However, transformer models are known to\nbe among the most challenging natural language processing (NLP) tasks.\nSpecifically, GPT models have a tendency to generate factual errors, lack\ncontext, and oversimplify words. To address these limitations, we replaced the\nattention mechanism in the GPT model with a pointer network. This modification\nwas designed to preserve the core values of the original text during the\nsummarization process. The effectiveness of the Pointer-GPT model was evaluated\nusing the ROUGE score. The results demonstrated that Pointer-GPT outperformed\nthe original GPT model. These findings suggest that pointer networks can be a\nvaluable addition to EMR systems and can provide clinicians with more accurate\nand informative summaries of patient medical records. This research has the\npotential to usher in a new paradigm in EMR systems and to revolutionize the\nway that clinicians interact with patient medical records.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "3 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.08654v1",
    "published_date": "2024-03-22 02:13:23 UTC",
    "updated_date": "2024-03-22 02:13:23 UTC"
  },
  {
    "arxiv_id": "2403.14895v1",
    "title": "Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning",
    "authors": [
      "Maksym Taranukhin",
      "Vered Shwartz",
      "Evangelos Milios"
    ],
    "abstract": "Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14895v1",
    "published_date": "2024-03-22 00:58:28 UTC",
    "updated_date": "2024-03-22 00:58:28 UTC"
  },
  {
    "arxiv_id": "2403.15504v1",
    "title": "SymboSLAM: Semantic Map Generation in a Multi-Agent System",
    "authors": [
      "Brandon Curtis Colelough"
    ],
    "abstract": "Sub-symbolic artificial intelligence methods dominate the fields of\nenvironment-type classification and Simultaneous Localisation and Mapping.\nHowever, a significant area overlooked within these fields is solution\ntransparency for the human-machine interaction space, as the sub-symbolic\nmethods employed for map generation do not account for the explainability of\nthe solutions generated. This paper proposes a novel approach to\nenvironment-type classification through Symbolic Simultaneous Localisation and\nMapping, SymboSLAM, to bridge the explainability gap. Our method for\nenvironment-type classification observes ontological reasoning used to\nsynthesise the context of an environment through the features found within. We\nachieve explainability within the model by presenting operators with\nenvironment-type classifications overlayed by a semantically labelled occupancy\nmap of landmarks and features. We evaluate SymboSLAM with ground-truth maps of\nthe Canberra region, demonstrating method effectiveness. We assessed the system\nthrough both simulations and real-world trials.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.15504v1",
    "published_date": "2024-03-22 00:48:52 UTC",
    "updated_date": "2024-03-22 00:48:52 UTC"
  },
  {
    "arxiv_id": "2405.01398v1",
    "title": "Advancing Frontiers in SLAM: A Survey of Symbolic Representation and Human-Machine Teaming in Environmental Mapping",
    "authors": [
      "Brandon Curtis Colelough"
    ],
    "abstract": "This survey paper presents a comprehensive overview of the latest\nadvancements in the field of Simultaneous Localization and Mapping (SLAM) with\na focus on the integration of symbolic representation of environment features.\nThe paper synthesizes research trends in multi-agent systems (MAS) and\nhuman-machine teaming, highlighting their applications in both symbolic and\nsub-symbolic SLAM tasks. The survey emphasizes the evolution and significance\nof ontological designs and symbolic reasoning in creating sophisticated 2D and\n3D maps of various environments. Central to this review is the exploration of\ndifferent architectural approaches in SLAM, with a particular interest in the\nfunctionalities and applications of edge and control agent architectures in MAS\nsettings. This study acknowledges the growing demand for enhanced human-machine\ncollaboration in mapping tasks and examines how these collaborative efforts\nimprove the accuracy and efficiency of environmental mapping",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2405.01398v1",
    "published_date": "2024-03-22 00:48:48 UTC",
    "updated_date": "2024-03-22 00:48:48 UTC"
  }
]
[
  {
    "arxiv_id": "2401.17319v1",
    "title": "Decentralized Federated Learning: A Survey on Security and Privacy",
    "authors": [
      "Ehsan Hallaji",
      "Roozbeh Razavi-Far",
      "Mehrdad Saif",
      "Boyu Wang",
      "Qiang Yang"
    ],
    "abstract": "Federated learning has been rapidly evolving and gaining popularity in recent\nyears due to its privacy-preserving features, among other advantages.\nNevertheless, the exchange of model updates and gradients in this architecture\nprovides new attack surfaces for malicious users of the network which may\njeopardize the model performance and user and data privacy. For this reason,\none of the main motivations for decentralized federated learning is to\neliminate server-related threats by removing the server from the network and\ncompensating for it through technologies such as blockchain. However, this\nadvantage comes at the cost of challenging the system with new privacy threats.\nThus, performing a thorough security analysis in this new paradigm is\nnecessary. This survey studies possible variations of threats and adversaries\nin decentralized federated learning and overviews the potential defense\nmechanisms. Trustability and verifiability of decentralized federated learning\nare also considered in this study.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted for publication in IEEE Transactions on Big Data",
    "pdf_url": "http://arxiv.org/pdf/2401.17319v1",
    "published_date": "2024-01-25 23:35:47 UTC",
    "updated_date": "2024-01-25 23:35:47 UTC"
  },
  {
    "arxiv_id": "2402.01715v1",
    "title": "ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis",
    "authors": [
      "Alessio Buscemi",
      "Daniele Proverbio"
    ],
    "abstract": "Automated sentiment analysis using Large Language Model (LLM)-based models\nlike ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic\nresearch and in industrial applications. However, assessment and validation of\ntheir performance in case of ambiguous or ironic text is still poor. In this\nstudy, we constructed nuanced and ambiguous scenarios, we translated them in 10\nlanguages, and we predicted their associated sentiment using popular LLMs. The\nresults are validated against post-hoc human responses. Ambiguous scenarios are\noften well-coped by ChatGPT and Gemini, but we recognise significant biases and\ninconsistent performance across models and evaluated human languages. This work\nprovides a standardised methodology for automated sentiment analysis evaluation\nand makes a call for action to further improve the algorithms and their\nunderlying data, to improve their performance, interpretability and\napplicability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01715v1",
    "published_date": "2024-01-25 23:15:45 UTC",
    "updated_date": "2024-01-25 23:15:45 UTC"
  },
  {
    "arxiv_id": "2401.14559v1",
    "title": "Language Modelling Approaches to Adaptive Machine Translation",
    "authors": [
      "Yasmin Moslem"
    ],
    "abstract": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nin-domain data scarcity is common in translation settings, due to the lack of\nspecialised datasets and terminology, or inconsistency and inaccuracy of\navailable in-domain translations. In such scenarios where there is insufficient\nin-domain data to fine-tune MT models, producing translations that are\nconsistent with the relevant context is challenging. While real-time adaptation\ncan make use of smaller amounts of in-domain data to improve the translation on\nthe fly, it remains challenging due to supported context limitations and\nefficiency constraints. Large language models (LLMs) have recently shown\ninteresting capabilities of in-context learning, where they learn to replicate\ncertain input-output text generation patterns, without further fine-tuning.\nSuch capabilities have opened new horizons for domain-specific data\naugmentation and real-time adaptive MT. This work attempts to address two main\nrelevant questions: 1) in scenarios involving human interaction and continuous\nfeedback, can we employ language models to improve the quality of adaptive MT\nat inference time? and 2) in the absence of sufficient in-domain data, can we\nuse pre-trained large-scale language models to improve the process of MT domain\nadaptation?",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2401.14559v1",
    "published_date": "2024-01-25 23:02:54 UTC",
    "updated_date": "2024-01-25 23:02:54 UTC"
  },
  {
    "arxiv_id": "2402.06640v1",
    "title": "Modeling and Optimization of Epidemiological Control Policies Through Reinforcement Learning",
    "authors": [
      "Ishir Rao"
    ],
    "abstract": "Pandemics involve the high transmission of a disease that impacts global and\nlocal health and economic patterns. The impact of a pandemic can be minimized\nby enforcing certain restrictions on a community. However, while minimizing\ninfection and death rates, these restrictions can also lead to economic crises.\nEpidemiological models help propose pandemic control strategies based on\nnon-pharmaceutical interventions such as social distancing, curfews, and\nlockdowns, reducing the economic impact of these restrictions. However,\ndesigning manual control strategies while considering disease spread and\neconomic status is non-trivial. Optimal strategies can be designed through\nmulti-objective reinforcement learning (MORL) models, which demonstrate how\nrestrictions can be used to optimize the outcome of a pandemic. In this\nresearch, we utilized an epidemiological Susceptible, Exposed, Infected,\nRecovered, Deceased (SEIRD) model: a compartmental model for virtually\nsimulating a pandemic day by day. We combined the SEIRD model with a deep\ndouble recurrent Q-network to train a reinforcement learning agent to enforce\nthe optimal restriction on the SEIRD simulation based on a reward function. We\ntested two agents with unique reward functions and pandemic goals to obtain two\nstrategies. The first agent placed long lockdowns to reduce the initial spread\nof the disease, followed by cyclical and shorter lockdowns to mitigate the\nresurgence of the disease. The second agent provided similar infection rates\nbut an improved economy by implementing a 10-day lockdown and 20-day\nno-restriction cycle. This use of reinforcement learning and epidemiological\nmodeling allowed for both economic and infection mitigation in multiple\npandemic scenarios.",
    "categories": [
      "cs.AI",
      "q-bio.PE"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.06640v1",
    "published_date": "2024-01-25 22:39:39 UTC",
    "updated_date": "2024-01-25 22:39:39 UTC"
  },
  {
    "arxiv_id": "2401.14542v1",
    "title": "Exploring Musical Roots: Applying Audio Embeddings to Empower Influence Attribution for a Generative Music Model",
    "authors": [
      "Julia Barnett",
      "Hugo Flores Garcia",
      "Bryan Pardo"
    ],
    "abstract": "Every artist has a creative process that draws inspiration from previous\nartists and their works. Today, \"inspiration\" has been automated by generative\nmusic models. The black box nature of these models obscures the identity of the\nworks that influence their creative output. As a result, users may\ninadvertently appropriate, misuse, or copy existing artists' works. We\nestablish a replicable methodology to systematically identify similar pieces of\nmusic audio in a manner that is useful for understanding training data\nattribution. A key aspect of our approach is to harness an effective music\naudio similarity measure. We compare the effect of applying CLMR and CLAP\nembeddings to similarity measurement in a set of 5 million audio clips used to\ntrain VampNet, a recent open source generative music model. We validate this\napproach with a human listening study. We also explore the effect that\nmodifications of an audio example (e.g., pitch shifting, time stretching,\nbackground noise) have on similarity measurements. This work is foundational to\nincorporating automated influence attribution into generative modeling, which\npromises to let model creators and users move from ignorant appropriation to\ninformed creation. Audio samples that accompany this paper are available at\nhttps://tinyurl.com/exploring-musical-roots.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "14 pages + references. Under conference review",
    "pdf_url": "http://arxiv.org/pdf/2401.14542v1",
    "published_date": "2024-01-25 22:20:42 UTC",
    "updated_date": "2024-01-25 22:20:42 UTC"
  },
  {
    "arxiv_id": "2401.14530v1",
    "title": "Relative Value Biases in Large Language Models",
    "authors": [
      "William M. Hayes",
      "Nicolas Yax",
      "Stefano Palminteri"
    ],
    "abstract": "Studies of reinforcement learning in humans and animals have demonstrated a\npreference for options that yielded relatively better outcomes in the past,\neven when those options are associated with lower absolute reward. The present\nstudy tested whether large language models would exhibit a similar bias. We had\ngpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between\npairs of options with the goal of maximizing payoffs. A complete record of\nprevious outcomes was included in each prompt. Both models exhibited relative\nvalue decision biases similar to those observed in humans and animals. Making\nrelative comparisons among outcomes more explicit magnified the bias, whereas\nprompting the models to estimate expected outcomes caused the bias to\ndisappear. These results have implications for the potential mechanisms that\ncontribute to context-dependent choice in human agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14530v1",
    "published_date": "2024-01-25 21:49:32 UTC",
    "updated_date": "2024-01-25 21:49:32 UTC"
  },
  {
    "arxiv_id": "2401.14524v1",
    "title": "Evaluating GPT-3.5's Awareness and Summarization Abilities for European Constitutional Texts with Shared Topics",
    "authors": [
      "Candida M. Greco",
      "A. Tagarelli"
    ],
    "abstract": "Constitutions are foundational legal documents that underpin the governmental\nand societal structures. As such, they are a reflection of a nation's cultural\nand social uniqueness, but also contribute to establish topics of universal\nimportance, like citizens' rights and duties (RD). In this work, using the\nrenowned GPT-3.5, we leverage generative large language models to understand\nconstitutional passages that transcend national boundaries. A key contribution\nof our study is the introduction of a novel application of abstractive\nsummarization on a multi-source collection of constitutional texts, with a\nfocus on European countries' constitution passages related to RD topics. Our\nresults show the meaningfulness of GPT-3.5 to produce informative, coherent and\nfaithful summaries capturing RD topics across European countries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14524v1",
    "published_date": "2024-01-25 21:34:53 UTC",
    "updated_date": "2024-01-25 21:34:53 UTC"
  },
  {
    "arxiv_id": "2401.14523v1",
    "title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do",
    "authors": [
      "William Kidder",
      "Jason D'Cruz",
      "Kush R. Varshney"
    ],
    "abstract": "Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14523v1",
    "published_date": "2024-01-25 21:30:06 UTC",
    "updated_date": "2024-01-25 21:30:06 UTC"
  },
  {
    "arxiv_id": "2401.14521v4",
    "title": "Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron",
    "authors": [
      "Yuan-Heng Wang",
      "Hoshin V. Gupta"
    ],
    "abstract": "We investigate the applicability of machine learning technologies to the\ndevelopment of parsimonious, interpretable, catchment-scale hydrologic models\nusing directed-graph architectures based on the mass-conserving perceptron\n(MCP) as the fundamental computational unit. Here, we focus on architectural\ncomplexity (depth) at a single location, rather than universal applicability\n(breadth) across large samples of catchments. The goal is to discover a minimal\nrepresentation (numbers of cell-states and flow paths) that represents the\ndominant processes that can explain the input-state-output behaviors of a given\ncatchment, with particular emphasis given to simulating the full range (high,\nmedium, and low) of flow dynamics. We find that a HyMod Like architecture with\nthree cell-states and two major flow pathways achieves such a representation at\nour study location, but that the additional incorporation of an input-bypass\nmechanism significantly improves the timing and shape of the hydrograph, while\nthe inclusion of bi-directional groundwater mass exchanges significantly\nenhances the simulation of baseflow. Overall, our results demonstrate the\nimportance of using multiple diagnostic metrics for model evaluation, while\nhighlighting the need for properly selecting and designing the training metrics\nbased on information-theoretic foundations that are better suited to extracting\ninformation across the full range of flow dynamics. This study sets the stage\nfor interpretable regional-scale MCP-based hydrological modeling (using large\nsample data) by using neural architecture search to determine appropriate\nminimal representations for catchments in different hydroclimatic regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "65 pages, 8 Figures, 4 Tables, 1 Supplementary Material",
    "pdf_url": "http://arxiv.org/pdf/2401.14521v4",
    "published_date": "2024-01-25 21:26:49 UTC",
    "updated_date": "2024-07-28 22:59:57 UTC"
  },
  {
    "arxiv_id": "2402.10920v1",
    "title": "Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated Description of a Spiking Neuron Array",
    "authors": [
      "Michael Tomlinson",
      "Joe Li",
      "Andreas Andreou"
    ],
    "abstract": "Large language models (LLMs) have made headlines for synthesizing\ncorrect-sounding responses to a variety of prompts, including code generation.\nIn this paper, we present the prompts used to guide ChatGPT4 to produce a\nsynthesizable and functional verilog description for the entirety of a\nprogrammable Spiking Neuron Array ASIC. This design flow showcases the current\nstate of using ChatGPT4 for natural language driven hardware design. The\nAI-generated design was verified in simulation using handcrafted testbenches\nand has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5\nusing an open-source EDA flow.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.10920v1",
    "published_date": "2024-01-25 21:21:38 UTC",
    "updated_date": "2024-01-25 21:21:38 UTC"
  },
  {
    "arxiv_id": "2401.14511v1",
    "title": "Automated legal reasoning with discretion to act using s(LAW)",
    "authors": [
      "Joaquín Arias",
      "Mar Moreno-Rebato",
      "José A. Rodríguez-García",
      "Sascha Ossowski"
    ],
    "abstract": "Automated legal reasoning and its application in smart contracts and\nautomated decisions are increasingly attracting interest. In this context,\nethical and legal concerns make it necessary for automated reasoners to justify\nin human-understandable terms the advice given. Logic Programming, specially\nAnswer Set Programming, has a rich semantics and has been used to very\nconcisely express complex knowledge. However, modelling discretionality to act\nand other vague concepts such as ambiguity cannot be expressed in top-down\nexecution models based on Prolog, and in bottom-up execution models based on\nASP the justifications are incomplete and/or not scalable. We propose to use\ns(CASP), a top-down execution model for predicate ASP, to model vague concepts\nfollowing a set of patterns. We have implemented a framework, called s(LAW), to\nmodel, reason, and justify the applicable legislation and validate it by\ntranslating (and benchmarking) a representative use case, the criteria for the\nadmission of students in the \"Comunidad de Madrid\".",
    "categories": [
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14511v1",
    "published_date": "2024-01-25 21:11:08 UTC",
    "updated_date": "2024-01-25 21:11:08 UTC"
  },
  {
    "arxiv_id": "2401.14504v1",
    "title": "Learning When to See for Long-term Traffic Data Collection on Power-constrained Devices",
    "authors": [
      "Ruixuan Zhang",
      "Wenyu Han",
      "Zilin Bian",
      "Kaan Ozbay",
      "Chen Feng"
    ],
    "abstract": "Collecting traffic data is crucial for transportation systems and urban\nplanning, and is often more desirable through easy-to-deploy but\npower-constrained devices, due to the unavailability or high cost of power and\nnetwork infrastructure. The limited power means an inevitable trade-off between\ndata collection duration and accuracy/resolution. We introduce a novel\nlearning-based framework that strategically decides observation timings for\nbattery-powered devices and reconstructs the full data stream from sparsely\nsampled observations, resulting in minimal performance loss and a significantly\nprolonged system lifetime. Our framework comprises a predictor, a controller,\nand an estimator. The predictor utilizes historical data to forecast future\ntrends within a fixed time horizon. The controller uses the forecasts to\ndetermine the next optimal timing for data collection. Finally, the estimator\nreconstructs the complete data profile from the sampled observations. We\nevaluate the performance of the proposed method on PeMS data by an RNN\n(Recurrent Neural Network) predictor and estimator, and a DRQN (Deep Recurrent\nQ-Network) controller, and compare it against the baseline that uses Kalman\nfilter and uniform sampling. The results indicate that our method outperforms\nthe baseline, primarily due to the inclusion of more representative data points\nin the profile, resulting in an overall 10\\% improvement in estimation\naccuracy. Source code will be publicly available.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "Accepted by IEEE 26th International Conference on Intelligent\n  Transportation Systems",
    "pdf_url": "http://arxiv.org/pdf/2401.14504v1",
    "published_date": "2024-01-25 20:50:34 UTC",
    "updated_date": "2024-01-25 20:50:34 UTC"
  },
  {
    "arxiv_id": "2402.01714v1",
    "title": "TrICy: Trigger-guided Data-to-text Generation with Intent aware Attention-Copy",
    "authors": [
      "Vibhav Agarwal",
      "Sourav Ghosh",
      "Harichandana BSS",
      "Himanshu Arora",
      "Barath Raj Kandur Raja"
    ],
    "abstract": "Data-to-text (D2T) generation is a crucial task in many natural language\nunderstanding (NLU) applications and forms the foundation of task-oriented\ndialog systems. In the context of conversational AI solutions that can work\ndirectly with local data on the user's device, architectures utilizing large\npre-trained language models (PLMs) are impractical for on-device deployment due\nto a high memory footprint. To this end, we propose TrICy, a novel lightweight\nframework for an enhanced D2T task that generates text sequences based on the\nintent in context and may further be guided by user-provided triggers. We\nleverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words\naccurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:\n70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom\ndataset related to text messaging applications, showcase our architecture's\neffectiveness. Moreover, we show that by leveraging an optional trigger input,\ndata-to-text generation quality increases significantly and achieves the new\nSOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that\nTrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively\nover LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some\nscenarios, performance improvement due to triggers is observed even when they\nare absent in training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in the IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing. (Sourav Ghosh and Vibhav Agarwal contributed equally to this\n  work.)",
    "pdf_url": "http://arxiv.org/pdf/2402.01714v1",
    "published_date": "2024-01-25 20:17:06 UTC",
    "updated_date": "2024-01-25 20:17:06 UTC"
  },
  {
    "arxiv_id": "2402.01713v2",
    "title": "Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data",
    "authors": [
      "Yinghao Zhu",
      "Zixiang Wang",
      "Junyi Gao",
      "Yuning Tong",
      "Jingkun An",
      "Weibin Liao",
      "Ewen M. Harrison",
      "Liantao Ma",
      "Chengwei Pan"
    ],
    "abstract": "The inherent complexity of structured longitudinal Electronic Health Records\n(EHR) data poses a significant challenge when integrated with Large Language\nModels (LLMs), which are traditionally tailored for natural language\nprocessing. Motivated by the urgent need for swift decision-making during new\ndisease outbreaks, where traditional predictive models often fail due to a lack\nof historical data, this research investigates the adaptability of LLMs, like\nGPT-4, to EHR data. We particularly focus on their zero-shot capabilities,\nwhich enable them to make predictions in scenarios in which they haven't been\nexplicitly trained. In response to the longitudinal, sparse, and\nknowledge-infused nature of EHR data, our prompting approach involves taking\ninto account specific EHR characteristics such as units and reference ranges,\nand employing an in-context learning strategy that aligns with clinical\ncontexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets\ndemonstrate that with our elaborately designed prompting framework, LLMs can\nimprove prediction performance in key tasks such as mortality, length-of-stay,\nand 30-day readmission by about 35\\%, surpassing ML models in few-shot\nsettings. Our research underscores the potential of LLMs in enhancing clinical\ndecision-making, especially in urgent healthcare situations like the outbreak\nof emerging diseases with no labeled data. The code is publicly available at\nhttps://github.com/yhzhu99/llm4healthcare for reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01713v2",
    "published_date": "2024-01-25 20:14:50 UTC",
    "updated_date": "2024-02-10 16:31:40 UTC"
  },
  {
    "arxiv_id": "2401.14489v2",
    "title": "The Case for Co-Designing Model Architectures with Hardware",
    "authors": [
      "Quentin Anthony",
      "Jacob Hatef",
      "Deepak Narayanan",
      "Stella Biderman",
      "Stas Bekman",
      "Junqi Yin",
      "Aamir Shafi",
      "Hari Subramoni",
      "Dhabaleswar Panda"
    ],
    "abstract": "While GPUs are responsible for training the vast majority of state-of-the-art\ndeep learning models, the implications of their architecture are often\noverlooked when designing new deep learning (DL) models. As a consequence,\nmodifying a DL model to be more amenable to the target hardware can\nsignificantly improve the runtime performance of DL training and inference. In\nthis paper, we provide a set of guidelines for users to maximize the runtime\nperformance of their transformer models. These guidelines have been created by\ncarefully considering the impact of various model hyperparameters controlling\nmodel shape on the efficiency of the underlying computation kernels executed on\nthe GPU. We find the throughput of models with efficient model shapes is up to\n39\\% higher while preserving accuracy compared to models with a similar number\nof parameters but with unoptimized shapes.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14489v2",
    "published_date": "2024-01-25 19:50:31 UTC",
    "updated_date": "2024-01-30 21:26:09 UTC"
  },
  {
    "arxiv_id": "2401.14488v1",
    "title": "Scilab-RL: A software framework for efficient reinforcement learning and cognitive modeling research",
    "authors": [
      "Jan Dohmen",
      "Frank Röder",
      "Manfred Eppe"
    ],
    "abstract": "One problem with researching cognitive modeling and reinforcement learning\n(RL) is that researchers spend too much time on setting up an appropriate\ncomputational framework for their experiments. Many open source implementations\nof current RL algorithms exist, but there is a lack of a modular suite of tools\ncombining different robotic simulators and platforms, data visualization,\nhyperparameter optimization, and baseline experiments. To address this problem,\nwe present Scilab-RL, a software framework for efficient research in cognitive\nmodeling and reinforcement learning for robotic agents. The framework focuses\non goal-conditioned reinforcement learning using Stable Baselines 3 and the\nOpenAI gym interface. It enables native possibilities for experiment\nvisualizations and hyperparameter optimization. We describe how these features\nenable researchers to conduct experiments with minimal time effort, thus\nmaximizing research output.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14488v1",
    "published_date": "2024-01-25 19:49:02 UTC",
    "updated_date": "2024-01-25 19:49:02 UTC"
  },
  {
    "arxiv_id": "2401.15109v1",
    "title": "Towards Collective Superintelligence: Amplifying Group IQ using Conversational Swarms",
    "authors": [
      "Louis Rosenberg",
      "Gregg Willcox",
      "Hans Schumann",
      "Ganesh Mani"
    ],
    "abstract": "Swarm Intelligence (SI) is a natural phenomenon that enables biological\ngroups to amplify their combined intellect by forming real-time systems.\nArtificial Swarm Intelligence (or Swarm AI) is a technology that enables\nnetworked human groups to amplify their combined intelligence by forming\nsimilar systems. In the past, swarm-based methods were constrained to narrowly\ndefined tasks like probabilistic forecasting and multiple-choice decision\nmaking. A new technology called Conversational Swarm Intelligence (CSI) was\ndeveloped in 2023 that amplifies the decision-making accuracy of networked\nhuman groups through natural conversational deliberations. The current study\nevaluated the ability of real-time groups using a CSI platform to take a common\nIQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a\nbaseline group of participants took the Raven's IQ test by traditional survey.\nThis group averaged 45.6% correct. Then, groups of approximately 35 individuals\nanswered IQ test questions together using a CSI platform called Thinkscape.\nThese groups averaged 80.5% correct. This places the CSI groups in the 97th\npercentile of IQ test-takers and corresponds to an effective IQ increase of 28\npoints (p<0.001). This is an encouraging result and suggests that CSI is a\npowerful method for enabling conversational collective intelligence in large,\nnetworked groups. In addition, because CSI is scalable across groups of\npotentially any size, this technology may provide a viable pathway to building\na Collective Superintelligence.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15109v1",
    "published_date": "2024-01-25 19:43:35 UTC",
    "updated_date": "2024-01-25 19:43:35 UTC"
  },
  {
    "arxiv_id": "2401.14484v1",
    "title": "Design Principles for Generative AI Applications",
    "authors": [
      "Justin D. Weisz",
      "Jessica He",
      "Michael Muller",
      "Gabriela Hoefer",
      "Rachel Miles",
      "Werner Geyer"
    ],
    "abstract": "Generative AI applications present unique design challenges. As generative AI\ntechnologies are increasingly being incorporated into mainstream applications,\nthere is an urgent need for guidance on how to design user experiences that\nfoster effective and safe use. We present six principles for the design of\ngenerative AI applications that address unique characteristics of generative AI\nUX and offer new interpretations and extensions of known issues in the design\nof AI applications. Each principle is coupled with a set of design strategies\nfor implementing that principle via UX capabilities or through the design\nprocess. The principles and strategies were developed through an iterative\nprocess involving literature review, feedback from design practitioners,\nvalidation against real-world generative AI applications, and incorporation\ninto the design process of two generative AI applications. We anticipate the\nprinciples to usefully inform the design of generative AI applications by\ndriving actionable design recommendations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "34 pages, 4 figures. To be published in CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14484v1",
    "published_date": "2024-01-25 19:38:21 UTC",
    "updated_date": "2024-01-25 19:38:21 UTC"
  },
  {
    "arxiv_id": "2401.14469v1",
    "title": "Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels",
    "authors": [
      "Zahra Babaiee",
      "Peyman M. Kiasari",
      "Daniela Rus",
      "Radu Grosu"
    ],
    "abstract": "Recent advances in depthwise-separable convolutional neural networks\n(DS-CNNs) have led to novel architectures, that surpass the performance of\nclassical CNNs, by a considerable scalability and accuracy margin. This paper\nreveals another striking property of DS-CNN architectures: discernible and\nexplainable patterns emerge in their trained depthwise convolutional kernels in\nall layers. Through an extensive analysis of millions of trained filters, with\ndifferent sizes and from various models, we employed unsupervised clustering\nwith autoencoders, to categorize these filters. Astonishingly, the patterns\nconverged into a few main clusters, each resembling the difference of Gaussian\n(DoG) functions, and their first and second-order derivatives. Notably, we were\nable to classify over 95\\% and 90\\% of the filters from state-of-the-art\nConvNextV2 and ConvNeXt models, respectively. This finding is not merely a\ntechnological curiosity; it echoes the foundational models neuroscientists have\nlong proposed for the vision systems of mammals. Our results thus deepen our\nunderstanding of the emergent properties of trained DS-CNNs and provide a\nbridge between artificial and biological visual processing systems. More\nbroadly, they pave the way for more interpretable and biologically-inspired\nneural network designs in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14469v1",
    "published_date": "2024-01-25 19:05:53 UTC",
    "updated_date": "2024-01-25 19:05:53 UTC"
  },
  {
    "arxiv_id": "2401.14461v2",
    "title": "Marabou 2.0: A Versatile Formal Analyzer of Neural Networks",
    "authors": [
      "Haoze Wu",
      "Omri Isac",
      "Aleksandar Zeljić",
      "Teruhiro Tagomori",
      "Matthew Daggitt",
      "Wen Kokke",
      "Idan Refaeli",
      "Guy Amir",
      "Kyle Julian",
      "Shahaf Bassan",
      "Pei Huang",
      "Ori Lahav",
      "Min Wu",
      "Min Zhang",
      "Ekaterina Komendantskaya",
      "Guy Katz",
      "Clark Barrett"
    ],
    "abstract": "This paper serves as a comprehensive system description of version 2.0 of the\nMarabou framework for formal analysis of neural networks. We discuss the tool's\narchitectural design and highlight the major features and components introduced\nsince its initial release.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Condensed version accepted at CAV'24",
    "pdf_url": "http://arxiv.org/pdf/2401.14461v2",
    "published_date": "2024-01-25 19:00:25 UTC",
    "updated_date": "2024-05-20 05:52:05 UTC"
  },
  {
    "arxiv_id": "2401.14405v2",
    "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
    "authors": [
      "Yiyuan Zhang",
      "Xiaohan Ding",
      "Kaixiong Gong",
      "Yixiao Ge",
      "Ying Shan",
      "Xiangyu Yue"
    ],
    "abstract": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024. Code and models are available at\n  https://github.com/AILab-CVC/M2PT",
    "pdf_url": "http://arxiv.org/pdf/2401.14405v2",
    "published_date": "2024-01-25 18:59:58 UTC",
    "updated_date": "2024-03-18 08:45:52 UTC"
  },
  {
    "arxiv_id": "2401.14403v2",
    "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
    "authors": [
      "Haoyu Xiong",
      "Russell Mendonca",
      "Kenneth Shaw",
      "Deepak Pathak"
    ],
    "abstract": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Website at https://open-world-mobilemanip.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2401.14403v2",
    "published_date": "2024-01-25 18:59:44 UTC",
    "updated_date": "2024-01-28 18:58:29 UTC"
  },
  {
    "arxiv_id": "2401.14447v1",
    "title": "Wordflow: Social Prompt Engineering for Large Language Models",
    "authors": [
      "Zijie J. Wang",
      "Aishwarya Chakravarthy",
      "David Munechika",
      "Duen Horng Chau"
    ],
    "abstract": "Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 7 figures. Wordflow is available at:\n  https://poloclub.github.io/wordflow. The code is available at:\n  https://github.com/poloclub/wordflow/. For a demo video, see:\n  https://youtu.be/3dOcVuofGVo",
    "pdf_url": "http://arxiv.org/pdf/2401.14447v1",
    "published_date": "2024-01-25 18:58:11 UTC",
    "updated_date": "2024-01-25 18:58:11 UTC"
  },
  {
    "arxiv_id": "2401.14446v3",
    "title": "Black-Box Access is Insufficient for Rigorous AI Audits",
    "authors": [
      "Stephen Casper",
      "Carson Ezell",
      "Charlotte Siegmann",
      "Noam Kolt",
      "Taylor Lynn Curtis",
      "Benjamin Bucknall",
      "Andreas Haupt",
      "Kevin Wei",
      "Jérémy Scheurer",
      "Marius Hobbhahn",
      "Lee Sharkey",
      "Satyapriya Krishna",
      "Marvin Von Hagen",
      "Silas Alberti",
      "Alan Chan",
      "Qinyi Sun",
      "Michael Gerovitch",
      "David Bau",
      "Max Tegmark",
      "David Krueger",
      "Dylan Hadfield-Menell"
    ],
    "abstract": "External audits of AI systems are increasingly recognized as a key mechanism\nfor AI governance. The effectiveness of an audit, however, depends on the\ndegree of access granted to auditors. Recent audits of state-of-the-art AI\nsystems have primarily relied on black-box access, in which auditors can only\nquery the system and observe its outputs. However, white-box access to the\nsystem's inner workings (e.g., weights, activations, gradients) allows an\nauditor to perform stronger attacks, more thoroughly interpret models, and\nconduct fine-tuning. Meanwhile, outside-the-box access to training and\ndeployment information (e.g., methodology, code, documentation, data,\ndeployment details, findings from internal evaluations) allows auditors to\nscrutinize the development process and design more targeted evaluations. In\nthis paper, we examine the limitations of black-box audits and the advantages\nof white- and outside-the-box audits. We also discuss technical, physical, and\nlegal safeguards for performing these audits with minimal security risks. Given\nthat different forms of access can lead to very different levels of evaluation,\nwe conclude that (1) transparency regarding the access and methods used by\nauditors is necessary to properly interpret audit results, and (2) white- and\noutside-the-box access allow for substantially more scrutiny than black-box\naccess alone.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CY",
    "comment": "FAccT 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14446v3",
    "published_date": "2024-01-25 18:58:05 UTC",
    "updated_date": "2024-05-29 13:56:29 UTC"
  },
  {
    "arxiv_id": "2402.01712v1",
    "title": "Socially Aware Synthetic Data Generation for Suicidal Ideation Detection Using Large Language Models",
    "authors": [
      "Hamideh Ghanadian",
      "Isar Nejadgholi",
      "Hussein Al Osman"
    ],
    "abstract": "Suicidal ideation detection is a vital research area that holds great\npotential for improving mental health support systems. However, the sensitivity\nsurrounding suicide-related data poses challenges in accessing large-scale,\nannotated datasets necessary for training effective machine learning models. To\naddress this limitation, we introduce an innovative strategy that leverages the\ncapabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to\ncreate synthetic data for suicidal ideation detection. Our data generation\napproach is grounded in social factors extracted from psychology literature and\naims to ensure coverage of essential information related to suicidal ideation.\nIn our study, we benchmarked against state-of-the-art NLP classification\nmodels, specifically, those centered around the BERT family structures. When\ntrained on the real-world dataset, UMD, these conventional models tend to yield\nF1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed\nby social factors, offers consistent F1-scores of 0.82 for both models,\nsuggesting that the richness of topics in synthetic data can bridge the\nperformance gap across different model complexities. Most impressively, when we\ncombined a mere 30% of the UMD dataset with our synthetic data, we witnessed a\nsubstantial increase in performance, achieving an F1-score of 0.88 on the UMD\ntest set. Such results underscore the cost-effectiveness and potential of our\napproach in confronting major challenges in the field, such as data scarcity\nand the quest for diversity in data representation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.01712v1",
    "published_date": "2024-01-25 18:25:05 UTC",
    "updated_date": "2024-01-25 18:25:05 UTC"
  },
  {
    "arxiv_id": "2401.14373v1",
    "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
    "authors": [
      "Gökçe Uludoğan",
      "Zeynep Yirmibeşoğlu Balal",
      "Furkan Akkurt",
      "Melikşah Türker",
      "Onur Güngör",
      "Susan Üsküdarlı"
    ],
    "abstract": "The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14373v1",
    "published_date": "2024-01-25 18:24:13 UTC",
    "updated_date": "2024-01-25 18:24:13 UTC"
  },
  {
    "arxiv_id": "2401.14371v1",
    "title": "Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input",
    "authors": [
      "Enrico Picco",
      "Lina Jaurigue",
      "Kathy Lüdge",
      "Serge Massar"
    ],
    "abstract": "We present an experimental validation of a recently proposed optimization\ntechnique for reservoir computing, using an optoelectronic setup. Reservoir\ncomputing is a robust framework for signal processing applications, and the\ndevelopment of efficient optimization approaches remains a key challenge. The\ntechnique we address leverages solely a delayed version of the input signal to\nidentify the optimal operational region of the reservoir, simplifying the\ntraditionally time-consuming task of hyperparameter tuning. We verify the\neffectiveness of this approach on different benchmark tasks and reservoir\noperating conditions.",
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.NE",
      "physics.optics"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14371v1",
    "published_date": "2024-01-25 18:20:37 UTC",
    "updated_date": "2024-01-25 18:20:37 UTC"
  },
  {
    "arxiv_id": "2401.14367v1",
    "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
    "authors": [
      "Asaf Yehudai",
      "Boaz Carmeli",
      "Yosi Mass",
      "Ofir Arviv",
      "Nathaniel Mills",
      "Assaf Toledo",
      "Eyal Shnarch",
      "Leshem Choshen"
    ],
    "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR24",
    "pdf_url": "http://arxiv.org/pdf/2401.14367v1",
    "published_date": "2024-01-25 18:14:57 UTC",
    "updated_date": "2024-01-25 18:14:57 UTC"
  },
  {
    "arxiv_id": "2401.14362v3",
    "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support",
    "authors": [
      "Inhwa Song",
      "Sachin R. Pendse",
      "Neha Kumar",
      "Munmun De Choudhury"
    ],
    "abstract": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "The first two authors contributed equally to this work; typos\n  corrected and post-review revisions incorporated",
    "pdf_url": "http://arxiv.org/pdf/2401.14362v3",
    "published_date": "2024-01-25 18:08:53 UTC",
    "updated_date": "2025-05-09 15:24:02 UTC"
  },
  {
    "arxiv_id": "2401.14444v1",
    "title": "ICASSP 2024 Speech Signal Improvement Challenge",
    "authors": [
      "Nicolae Catalin Ristea",
      "Ando Saabas",
      "Ross Cutler",
      "Babak Naderi",
      "Sebastian Braun",
      "Solomiya Branets"
    ],
    "abstract": "The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to\nstimulate research in the area of improving the speech signal quality in\ncommunication systems. This marks our second challenge, building upon the\nsuccess from the previous ICASSP 2023 Grand Challenge. We enhance the\ncompetition by introducing a dataset synthesizer, enabling all participating\nteams to start at a higher baseline, an objective metric for our extended P.804\ntests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a\nmetric. We evaluate a total of 13 systems in the real-time track and 11 systems\nin the non-real-time track using both subjective P.804 and objective Word\nAccuracy metrics.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14444v1",
    "published_date": "2024-01-25 18:08:00 UTC",
    "updated_date": "2024-01-25 18:08:00 UTC"
  },
  {
    "arxiv_id": "2402.01711v1",
    "title": "LLM on FHIR -- Demystifying Health Records",
    "authors": [
      "Paul Schmiedmayer",
      "Adrit Rao",
      "Philipp Zagar",
      "Vishnu Ravi",
      "Aydin Zahedivash",
      "Arash Fereydooni",
      "Oliver Aalami"
    ],
    "abstract": "Objective: To enhance health literacy and accessibility of health information\nfor a diverse patient population by developing a patient-centered artificial\nintelligence (AI) solution using large language models (LLMs) and Fast\nHealthcare Interoperability Resources (FHIR) application programming interfaces\n(APIs). Materials and Methods: The research involved developing LLM on FHIR, an\nopen-source mobile application allowing users to interact with their health\nrecords using LLMs. The app is built on Stanford's Spezi ecosystem and uses\nOpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient\ndataset and evaluated by medical experts to assess the app's effectiveness in\nincreasing health literacy. The evaluation focused on the accuracy, relevance,\nand understandability of the LLM's responses to common patient questions.\nResults: LLM on FHIR demonstrated varying but generally high degrees of\naccuracy and relevance in providing understandable health information to\npatients. The app effectively translated medical data into patient-friendly\nlanguage and was able to adapt its responses to different patient profiles.\nHowever, challenges included variability in LLM responses and the need for\nprecise filtering of health data. Discussion and Conclusion: LLMs offer\nsignificant potential in improving health literacy and making health records\nmore accessible. LLM on FHIR, as a pioneering application in this field,\ndemonstrates the feasibility and challenges of integrating LLMs into patient\ncare. While promising, the implementation and pilot also highlight risks such\nas inconsistent responses and the importance of replicable output. Future\ndirections include better resource identification mechanisms and executing LLMs\non-device to enhance privacy and reduce costs.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Pre-print of the paper submitted to the Call for Papers for the\n  Special Focus Issue on ChatGPT and Large Language Models (LLMs) in\n  Biomedicine and Health at the Journal of the American Medical Informatics\n  Association:\n  https://academic.oup.com/jamia/pages/call-for-papers-for-special-focus-issue",
    "pdf_url": "http://arxiv.org/pdf/2402.01711v1",
    "published_date": "2024-01-25 17:45:34 UTC",
    "updated_date": "2024-01-25 17:45:34 UTC"
  },
  {
    "arxiv_id": "2401.14336v1",
    "title": "Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition",
    "authors": [
      "Dichao Liu"
    ],
    "abstract": "Fine-grained vehicle recognition (FGVR) is an essential fundamental\ntechnology for intelligent transportation systems, but very difficult because\nof its inherent intra-class variation. Most previous FGVR studies only focus on\nthe intra-class variation caused by different shooting angles, positions, etc.,\nwhile the intra-class variation caused by image noise has received little\nattention. This paper proposes a progressive multi-task anti-noise learning\n(PMAL) framework and a progressive multi-task distilling (PMD) framework to\nsolve the intra-class variation problem in FGVR due to image noise. The PMAL\nframework achieves high recognition accuracy by treating image denoising as an\nadditional task in image recognition and progressively forcing a model to learn\nnoise invariance. The PMD framework transfers the knowledge of the PMAL-trained\nmodel into the original backbone network, which produces a model with about the\nsame recognition accuracy as the PMAL-trained model, but without any additional\noverheads over the original backbone network. Combining the two frameworks, we\nobtain models that significantly exceed previous state-of-the-art methods in\nrecognition accuracy on two widely-used, standard FGVR datasets, namely\nStanford Cars, and CompCars, as well as three additional surveillance\nimage-based vehicle-type classification datasets, namely Beijing Institute of\nTechnology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images\nDataset for Make Model Recognition (VIDMMR), without any additional overheads\nover the original backbone networks. The source code is available at\nhttps://github.com/Dichao-Liu/Anti-noise_FGVR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14336v1",
    "published_date": "2024-01-25 17:34:34 UTC",
    "updated_date": "2024-01-25 17:34:34 UTC"
  },
  {
    "arxiv_id": "2401.15108v2",
    "title": "Tacit algorithmic collusion in deep reinforcement learning guided price competition: A study using EV charge pricing game",
    "authors": [
      "Diwas Paudel",
      "Tapas K. Das"
    ],
    "abstract": "Players in pricing games with complex structures are increasingly adopting\nartificial intelligence (AI) aided learning algorithms to make pricing\ndecisions for maximizing profits. This is raising concern for the antitrust\nagencies as the practice of using AI may promote tacit algorithmic collusion\namong otherwise independent players. Recent studies of games in canonical forms\nhave shown contrasting claims ranging from none to a high level of tacit\ncollusion among AI-guided players. In this paper, we examine the concern for\ntacit collusion by considering a practical game where EV charging hubs compete\nby dynamically varying their prices. Such a game is likely to be commonplace in\nthe near future as EV adoption grows in all sectors of transportation. The hubs\nsource power from the day-ahead (DA) and real-time (RT) electricity markets as\nwell as from in-house battery storage systems. Their goal is to maximize\nprofits via pricing and efficiently managing the cost of power usage. To aid\nour examination, we develop a two-step data-driven methodology. The first step\nobtains the DA commitment by solving a stochastic model. The second step\ngenerates the pricing strategies by solving a competitive Markov decision\nprocess model using a multi-agent deep reinforcement learning (MADRL)\nframework. We evaluate the resulting pricing strategies using an index for the\nlevel of tacit algorithmic collusion. An index value of zero indicates no\ncollusion (perfect competition) and one indicates full collusion (monopolistic\nbehavior). Results from our numerical case study yield collusion index values\nbetween 0.14 and 0.45, suggesting a low to moderate level of collusion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "econ.GN",
      "eess.SY",
      "q-fin.EC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15108v2",
    "published_date": "2024-01-25 16:51:52 UTC",
    "updated_date": "2024-05-10 10:24:12 UTC"
  },
  {
    "arxiv_id": "2401.14295v4",
    "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
    "authors": [
      "Maciej Besta",
      "Florim Memedi",
      "Zhenyu Zhang",
      "Robert Gerstenberger",
      "Guangyuan Piao",
      "Nils Blach",
      "Piotr Nyczyk",
      "Marcin Copik",
      "Grzegorz Kwaśniewski",
      "Jürgen Müller",
      "Lukas Gianinazzi",
      "Ales Kubicek",
      "Hubert Niewiadomski",
      "Aidan O'Mahony",
      "Onur Mutlu",
      "Torsten Hoefler"
    ],
    "abstract": "The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and other parts of the LLM ecosystem such as knowledge bases, and the\nassociated research challenges. Our work will help to advance future prompt\nengineering techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14295v4",
    "published_date": "2024-01-25 16:34:00 UTC",
    "updated_date": "2025-02-08 09:02:41 UTC"
  },
  {
    "arxiv_id": "2401.14292v2",
    "title": "Single and bi-layered 2-D acoustic soft tactile skin (AST2)",
    "authors": [
      "Vishnu Rajendran",
      "Simon Parsons",
      "Amir Ghalamzan E"
    ],
    "abstract": "This paper aims to present an innovative and cost-effective design for\nAcoustic Soft Tactile (AST) Skin, with the primary goal of significantly\nenhancing the accuracy of 2-D tactile feature estimation. The existing\nchallenge lies in achieving precise tactile feature estimation, especially\nconcerning contact geometry characteristics, using cost-effective solutions. We\nhypothesise that by harnessing acoustic energy through dedicated acoustic\nchannels in 2 layers beneath the sensing surface and analysing amplitude\nmodulation, we can effectively decode interactions on the sensory surface,\nthereby improving tactile feature estimation. Our approach involves the\ndistinct separation of hardware components responsible for emitting and\nreceiving acoustic signals, resulting in a modular and highly customizable skin\ndesign. Practical tests demonstrate the effectiveness of this novel design,\nachieving remarkable precision in estimating contact normal forces (MAE < 0.8\nN), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE <\n0.3 mm). In conclusion, the AST skin, with its innovative design and modular\narchitecture, successfully addresses the challenge of tactile feature\nestimation. The presented results showcase its ability to precisely estimate\nvarious tactile features, making it a practical and cost-effective solution for\nrobotic applications.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Robosoft conference 2024 (accepted)",
    "pdf_url": "http://arxiv.org/pdf/2401.14292v2",
    "published_date": "2024-01-25 16:30:22 UTC",
    "updated_date": "2024-02-29 17:23:01 UTC"
  },
  {
    "arxiv_id": "2401.15106v6",
    "title": "Underspecified Human Decision Experiments Considered Harmful",
    "authors": [
      "Jessica Hullman",
      "Alex Kale",
      "Jason Hartline"
    ],
    "abstract": "Decision-making with information displays is a key focus of research in areas\nlike human-AI collaboration and data visualization. However, what constitutes a\ndecision problem, and what is required for an experiment to conclude that\ndecisions are flawed, remain imprecise. We present a widely applicable\ndefinition of a decision problem synthesized from statistical decision theory\nand information economics. We claim that to attribute loss in human performance\nto bias, an experiment must provide the information that a rational agent would\nneed to identify the normative decision. We evaluate whether recent empirical\nresearch on AI-assisted decisions achieves this standard. We find that only 10\n(26%) of 39 studies that claim to identify biased behavior presented\nparticipants with sufficient information to make this claim in at least one\ntreatment condition. We motivate the value of studying well-defined decision\nproblems by describing a characterization of performance losses they allow to\nbe conceived.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15106v6",
    "published_date": "2024-01-25 16:21:37 UTC",
    "updated_date": "2025-05-02 13:21:13 UTC"
  },
  {
    "arxiv_id": "2401.14285v1",
    "title": "POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation",
    "authors": [
      "Bo Zhou",
      "Jun Hou",
      "Tianqi Chen",
      "Yinchi Zhou",
      "Xiongchao Chen",
      "Huidong Xie",
      "Qiong Liu",
      "Xueqi Guo",
      "Yu-Jung Tsai",
      "Vladimir Y. Panin",
      "Takuya Toyonaga",
      "James S. Duncan",
      "Chi Liu"
    ],
    "abstract": "Low-dose PET offers a valuable means of minimizing radiation exposure in PET\nimaging. However, the prevalent practice of employing additional CT scans for\ngenerating attenuation maps (u-map) for PET attenuation correction\nsignificantly elevates radiation doses. To address this concern and further\nmitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an\ninnovative population-prior-aided over-under-representation network that aims\nfor high-quality attenuation map generation from low-dose PET. First, POUR-Net\nincorporates an over-under-representation network (OUR-Net) to facilitate\nefficient feature extraction, encompassing both low-resolution abstracted and\nfine-detail features, for assisting deep generation on the full-resolution\nlevel. Second, complementing OUR-Net, a population prior generation machine\n(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional\nprior information to aid OUR-Net generation. The integration of OUR-Net and\nPPGM within a cascade framework enables iterative refinement of $\\mu$-map\ngeneration, resulting in the production of high-quality $\\mu$-maps.\nExperimental results underscore the effectiveness of POUR-Net, showing it as a\npromising solution for accurate CT-free low-count PET attenuation correction,\nwhich also surpasses the performance of previous baseline methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14285v1",
    "published_date": "2024-01-25 16:18:11 UTC",
    "updated_date": "2024-01-25 16:18:11 UTC"
  },
  {
    "arxiv_id": "2401.14280v3",
    "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models via Romanization",
    "authors": [
      "Jaavid Aktar Husain",
      "Raj Dabre",
      "Aswanth Kumar",
      "Jay Gala",
      "Thanmay Jayakumar",
      "Ratish Puduppully",
      "Anoop Kunchukuttan"
    ],
    "abstract": "This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages that use non-Roman scripts. We propose an approach\nthat utilizes the romanized form of text as an interface for LLMs,\nhypothesizing that its frequent informal use and shared tokens with English\nenhance cross-lingual alignment. Our approach involves the continual\npretraining of an English LLM like Llama 2 on romanized text of non-English,\nnon-Roman script languages, followed by instruction tuning on romanized data.\nThe results indicate that romanized text not only reduces token fertility by\n2x-4x but also matches or outperforms native script representation across\nvarious NLU, NLG, and MT tasks. Moreover, the embeddings computed on romanized\ntext exhibit closer alignment with their English translations than those from\nthe native script. Our approach presents a promising direction for leveraging\nthe power of English LLMs in languages traditionally underrepresented in NLP.\nOur code is available on https://github.com/AI4Bharat/romansetu.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14280v3",
    "published_date": "2024-01-25 16:11:41 UTC",
    "updated_date": "2024-06-23 11:40:20 UTC"
  },
  {
    "arxiv_id": "2401.14279v3",
    "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using LLMs",
    "authors": [
      "Azmain Kabir",
      "Shaowei Wang",
      "Yuan Tian",
      "Tse-Hsun Chen",
      "Muhammad Asaduzzaman",
      "Wenbin Zhang"
    ],
    "abstract": "Technical Q&A sites are valuable for software developers seeking knowledge,\nbut the code snippets they provide are often uncompilable and incomplete due to\nunresolved types and missing libraries. This poses a challenge for users who\nwish to reuse or analyze these snippets. Existing methods either do not focus\non creating compilable code or have low success rates. To address this, we\npropose ZS4C, a lightweight approach for zero-shot synthesis of compilable code\nfrom incomplete snippets using Large Language Models (LLMs). ZS4C operates in\ntwo stages: first, it uses an LLM, like GPT-3.5, to identify missing import\nstatements in a snippet; second, it collaborates with a validator (e.g.,\ncompiler) to fix compilation errors caused by incorrect imports and syntax\nissues. We evaluated ZS4C on the StatType-SO benchmark and a new dataset,\nPython-SO, which includes 539 Python snippets from Stack Overflow across the 20\nmost popular Python libraries. ZS4C significantly outperforms existing methods,\nimproving the compilation rate from 63% to 95.1% compared to the\nstate-of-the-art SnR, marking a 50.1% improvement. On average, ZS4C can infer\nmore accurate import statements (with an F1 score of 0.98) than SnR, with an\nimprovement of 8.5% in the F1.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "This paper has been accepted and published in ACM Transactions on\n  Software Engineering and Methodology (TOSEM), [2024],\n  [https://dl.acm.org/doi/10.1145/3702979]",
    "pdf_url": "http://arxiv.org/pdf/2401.14279v3",
    "published_date": "2024-01-25 16:10:33 UTC",
    "updated_date": "2024-12-09 18:41:35 UTC"
  },
  {
    "arxiv_id": "2401.14267v3",
    "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across Time",
    "authors": [
      "Lyle Muller",
      "Patricia S. Churchland",
      "Terrence J. Sejnowski"
    ],
    "abstract": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence - into a\nlong \"encoding vector\" that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity traveling across single cortical areas or multiple\nregions at the whole-brain scale could implement a similar encoding principle.\nBy encapsulating recent input history into a single spatial pattern at each\nmoment in time, cortical waves may enable temporal context to be extracted from\nsequences of sensory inputs, the same computational principle used in\ntransformers.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "27 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14267v3",
    "published_date": "2024-01-25 16:01:49 UTC",
    "updated_date": "2024-08-16 14:56:36 UTC"
  },
  {
    "arxiv_id": "2401.14257v2",
    "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
    "authors": [
      "Minglin Chen",
      "Weihao Yuan",
      "Yukun Wang",
      "Zhe Sheng",
      "Yisheng He",
      "Zilong Dong",
      "Liefeng Bo",
      "Yulan Guo"
    ],
    "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14257v2",
    "published_date": "2024-01-25 15:49:12 UTC",
    "updated_date": "2024-01-27 07:22:06 UTC"
  },
  {
    "arxiv_id": "2402.00053v1",
    "title": "Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework for Knowledge Graph Link Predictors",
    "authors": [
      "Filip Cornell",
      "Yifei Jin",
      "Jussi Karlgren",
      "Sarunas Girdzijauskas"
    ],
    "abstract": "The standard evaluation protocol for measuring the quality of Knowledge Graph\nCompletion methods - the task of inferring new links to be added to a graph -\ntypically involves a step which ranks every entity of a Knowledge Graph to\nassess their fit as a head or tail of a candidate link to be added. In\nKnowledge Graphs on a larger scale, this task rapidly becomes prohibitively\nheavy. Previous approaches mitigate this problem by using random sampling of\nentities to assess the quality of links predicted or suggested by a method.\nHowever, we show that this approach has serious limitations since the ranking\nmetrics produced do not properly reflect true outcomes. In this paper, we\npresent a thorough analysis of these effects along with the following findings.\nFirst, we empirically find and theoretically motivate why sampling uniformly at\nrandom vastly overestimates the ranking performance of a method. We show that\nthis can be attributed to the effect of easy versus hard negative candidates.\nSecond, we propose a framework that uses relational recommenders to guide the\nselection of candidates for evaluation. We provide both theoretical and\nempirical justification of our methodology, and find that simple and fast\nmethods can work extremely well, and that they match advanced neural\napproaches. Even when a large portion of true candidates for a property are\nmissed, the estimation barely deteriorates. With our proposed framework, we can\nreduce the time and computation needed similar to random sampling strategies\nwhile vastly improving the estimation; on ogbl-wikikg2, we show that accurate\nestimations of the full, filtered ranking can be obtained in 20 seconds instead\nof 30 minutes. We conclude that considerable computational effort can be saved\nby effective preprocessing and sampling methods and still reliably predict\nperformance accurately of the true performance for the entire ranking\nprocedure.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00053v1",
    "published_date": "2024-01-25 15:44:46 UTC",
    "updated_date": "2024-01-25 15:44:46 UTC"
  },
  {
    "arxiv_id": "2401.14228v1",
    "title": "Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods",
    "authors": [
      "Mohammed Sabry",
      "Anya Belz"
    ],
    "abstract": "As the cost of training ever larger language models has grown, so has the\ninterest in reusing previously learnt knowledge. Transfer learning methods have\nshown how reusing non-task-specific knowledge can help in subsequent\ntask-specific learning. In this paper, we investigate the inverse: porting\nwhole functional modules that encode task-specific knowledge from one model to\nanother. We designed a study comprising 1,440 training/testing runs to test the\nportability of modules trained by parameter-efficient finetuning (PEFT)\ntechniques, using sentiment analysis as an example task. We test portability in\na wide range of scenarios, involving different PEFT techniques and different\npretrained host models, among other dimensions. We compare the performance of\nported modules with that of equivalent modules trained (i) from scratch, and\n(ii) from parameters sampled from the same distribution as the ported module.\nWe find that the ported modules far outperform the two alternatives tested, but\nthat there are interesting performance differences between the four PEFT\ntechniques. We conclude that task-specific knowledge in the form of\nstructurally modular sets of parameters as produced by PEFT techniques is\nhighly portable, but that degree of success depends on type of PEFT and on\ndifferences between originating and receiving pretrained models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of EACL 2024. Camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2401.14228v1",
    "published_date": "2024-01-25 15:11:07 UTC",
    "updated_date": "2024-01-25 15:11:07 UTC"
  },
  {
    "arxiv_id": "2401.14215v3",
    "title": "Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement",
    "authors": [
      "Hana Kim",
      "Kai Tzu-iunn Ong",
      "Seoyeon Kim",
      "Dongha Lee",
      "Jinyoung Yeo"
    ],
    "abstract": "Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14215v3",
    "published_date": "2024-01-25 14:54:33 UTC",
    "updated_date": "2024-02-12 12:27:18 UTC"
  },
  {
    "arxiv_id": "2401.14440v2",
    "title": "Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models",
    "authors": [
      "Erik Arakelyan",
      "Zhaoqi Liu",
      "Isabelle Augenstein"
    ],
    "abstract": "Recent studies of the emergent capabilities of transformer-based Natural\nLanguage Understanding (NLU) models have indicated that they have an\nunderstanding of lexical and compositional semantics. We provide evidence that\nsuggests these claims should be taken with a grain of salt: we find that\nstate-of-the-art Natural Language Inference (NLI) models are sensitive towards\nminor semantics preserving surface-form variations, which lead to sizable\ninconsistent model decisions during inference. Notably, this behaviour differs\nfrom valid and in-depth comprehension of compositional semantics, however does\nneither emerge when evaluating model accuracy on standard benchmarks nor when\nprobing for syntactic, monotonic, and logically robust reasoning. We propose a\nnovel framework to measure the extent of semantic sensitivity. To this end, we\nevaluate NLI models on adversarially generated examples containing minor\nsemantics-preserving surface-form input noise. This is achieved using\nconditional text generation, with the explicit condition that the NLI model\npredicts the relationship between the original and adversarial inputs as a\nsymmetric equivalence entailment. We systematically study the effects of the\nphenomenon across NLI models for $\\textbf{in-}$ and $\\textbf{out-of-}$ domain\nsettings. Our experiments show that semantic sensitivity causes performance\ndegradations of $12.92\\%$ and $23.71\\%$ average over $\\textbf{in-}$ and\n$\\textbf{out-of-}$ domain settings, respectively. We further perform ablation\nstudies, analysing this phenomenon across models, datasets, and variations in\ninference and show that semantic sensitivity can lead to major inconsistency\nwithin model predictions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14440v2",
    "published_date": "2024-01-25 14:47:05 UTC",
    "updated_date": "2024-01-31 10:52:52 UTC"
  },
  {
    "arxiv_id": "2401.14206v1",
    "title": "Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification",
    "authors": [
      "Daniele Perlo",
      "Luca Berton",
      "Alessia Delpiano",
      "Francesca Menchini",
      "Stefano Tibaldi",
      "Marco Grosso",
      "Paolo Fonio"
    ],
    "abstract": "The liver is the most involved organ by distant metastasis in colon-rectal\ncancer (CRC) patients and it comes necessary to be aware of the mutational\nstatus of the lesions to correctly design the best individual treatment. So\nfar, efforts have been made in order to develop non-invasive and real-time\nmethods that permit the analysis of the whole tumor, using new artificial\nintelligence tools to analyze the tumor's image obtained by Computed Tomography\n(CT) scan. In order to address the current medical workflow, that is biopsy\nanalysis-based, we propose the first DeepLearning-based exploration, to our\nknowledge, of such classification approach from the patient medical imaging. We\npropose i) a solid pipeline for managing undersized datasets of available CT\nscans and ii) a baseline study for genomics mutation diagnosis support for\npreemptive patient follow-up. Our method is able to identify CRC RAS mutation\nfamily from CT images with 0.73 F1 score.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "J.3; I.1.2"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14206v1",
    "published_date": "2024-01-25 14:40:58 UTC",
    "updated_date": "2024-01-25 14:40:58 UTC"
  },
  {
    "arxiv_id": "2401.14185v1",
    "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion",
    "authors": [
      "Samuel Pegg",
      "Kai Li",
      "Xiaolin Hu"
    ],
    "abstract": "Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14185v1",
    "published_date": "2024-01-25 13:47:22 UTC",
    "updated_date": "2024-01-25 13:47:22 UTC"
  },
  {
    "arxiv_id": "2401.14176v2",
    "title": "Copilot-in-the-Loop: Fixing Code Smells in Copilot-Generated Python Code using Copilot",
    "authors": [
      "Beiqi Zhang",
      "Peng Liang",
      "Qiong Feng",
      "Yujia Fu",
      "Zengyang Li"
    ],
    "abstract": "As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released in\nSeptember 2023, functions as an interactive tool aimed at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot Chat's\nability to fix the code smells. To this end, we built a dataset comprising 102\ncode smells in Copilot-generated Python code. Our aim is to first explore the\noccurrence of code smells in Copilot-generated Python code and then evaluate\nthe effectiveness of Copilot Chat in fixing these code smells employing\ndifferent prompts. The results show that 8 out of 10 types of code smells can\nbe detected in Copilot-generated Python code, among which Multiply-Nested\nContainer is the most common one. For these code smells, Copilot Chat achieves\na highest fixing rate of 87.1%, showing promise in fixing Python code smells\ngenerated by Copilot itself. In addition, the effectiveness of Copilot Chat in\nfixing these smells can be improved by providing more detailed prompts.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "The 39th IEEE/ACM International Conference on Automated Software\n  Engineering (ASE), NIER Track",
    "pdf_url": "http://arxiv.org/pdf/2401.14176v2",
    "published_date": "2024-01-25 13:39:54 UTC",
    "updated_date": "2024-08-21 14:57:43 UTC"
  },
  {
    "arxiv_id": "2401.14174v2",
    "title": "A Structural Complexity Analysis of Hierarchical Task Network Planning",
    "authors": [
      "Cornelius Brand",
      "Robert Ganian",
      "Fionn Mc Inerney",
      "Simon Wietheger"
    ],
    "abstract": "We perform a refined complexity-theoretic analysis of three classical\nproblems in the context of Hierarchical Task Network Planning: the verification\nof a provided plan, whether an executable plan exists, and whether a given\nstate can be reached. Our focus lies on identifying structural properties which\nyield tractability. We obtain new polynomial algorithms for all three problems\non a natural class of primitive networks, along with corresponding lower\nbounds. We also obtain an algorithmic meta-theorem for lifting polynomial-time\nsolvability from primitive to general task networks, and prove that its\npreconditions are tight. Finally, we analyze the parameterized complexity of\nthe three problems.",
    "categories": [
      "cs.CC",
      "cs.AI"
    ],
    "primary_category": "cs.CC",
    "comment": "Updated version contains essentially the same results in a more\n  concise and improved write-up",
    "pdf_url": "http://arxiv.org/pdf/2401.14174v2",
    "published_date": "2024-01-25 13:34:33 UTC",
    "updated_date": "2025-01-22 12:41:26 UTC"
  },
  {
    "arxiv_id": "2401.14436v1",
    "title": "Trust model of privacy-concerned, emotionally-aware agents in a cooperative logistics problem",
    "authors": [
      "J. Carbo",
      "J. M. Molina"
    ],
    "abstract": "In this paper we propose a trust model to be used into a hypothetical mixed\nenvironment where humans and unmanned vehicles cooperate. We address the\ninclusion of emotions inside a trust model in a coherent way to the practical\napproaches to the current psychology theories. The most innovative contribution\nis how privacy issues play a role in the cooperation decisions of the emotional\ntrust model. Both, emotions and trust have been cognitively modeled and managed\nwith the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents\nimplemented in GAML (the programming language of GAMA agent platform) that\ncommunicates using the IEEE FIPA standard. The trusting behaviour of these\nemotional agents is tested in a cooperative logistics problem where: agents\nhave to move objects to destinations and some of the objects and places have\nprivacy issues. The execution of simulations of this logistic problem shows how\nemotions and trust contribute to improve the performance of agents in terms of\nboth, time savings and privacy protection",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14436v1",
    "published_date": "2024-01-25 13:31:43 UTC",
    "updated_date": "2024-01-25 13:31:43 UTC"
  },
  {
    "arxiv_id": "2401.14171v1",
    "title": "Predicting Hypoxia in Brain Tumors from Multiparametric MRI",
    "authors": [
      "Daniele Perlo",
      "Georgia Kanli",
      "Selma Boudissa",
      "Olivier Keunen"
    ],
    "abstract": "This research paper presents a novel approach to the prediction of hypoxia in\nbrain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia,\na condition characterized by low oxygen levels, is a common feature of\nmalignant brain tumors associated with poor prognosis. Fluoromisonidazole\nPositron Emission Tomography (FMISO PET) is a well-established method for\ndetecting hypoxia in vivo, but it is expensive and not widely available. Our\nstudy proposes the use of MRI, a more accessible and cost-effective imaging\nmodality, to predict FMISO PET signals. We investigate deep learning models\n(DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and\nFMISO PET images from patients with brain tumors. Our trained models\neffectively learn the complex relationships between the MRI features and the\ncorresponding FMISO PET signals, thereby enabling the prediction of hypoxia\nfrom MRI scans alone. The results show a strong correlation between the\npredicted and actual FMISO PET signals, with an overall PSNR score above 29.6\nand a SSIM score greater than 0.94, confirming MRI as a promising option for\nhypoxia prediction in brain tumors. This approach could significantly improve\nthe accessibility of hypoxia detection in clinical settings, with the potential\nfor more timely and targeted treatments.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "J.3; I.2.1"
    ],
    "primary_category": "eess.IV",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.14171v1",
    "published_date": "2024-01-25 13:28:53 UTC",
    "updated_date": "2024-01-25 13:28:53 UTC"
  },
  {
    "arxiv_id": "2401.14166v3",
    "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction",
    "authors": [
      "Jiangmeng Li",
      "Fei Song",
      "Yifan Jin",
      "Wenwen Qiang",
      "Changwen Zheng",
      "Fuchun Sun",
      "Hui Xiong"
    ],
    "abstract": "As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14166v3",
    "published_date": "2024-01-25 13:20:47 UTC",
    "updated_date": "2024-03-20 08:52:42 UTC"
  },
  {
    "arxiv_id": "2401.14155v1",
    "title": "Alleviating Structural Distribution Shift in Graph Anomaly Detection",
    "authors": [
      "Yuan Gao",
      "Xiang Wang",
      "Xiangnan He",
      "Zhenguang Liu",
      "Huamin Feng",
      "Yongdong Zhang"
    ],
    "abstract": "Graph anomaly detection (GAD) is a challenging binary classification problem\ndue to its different structural distribution between anomalies and normal nodes\n-- abnormal nodes are a minority, therefore holding high heterophily and low\nhomophily compared to normal nodes. Furthermore, due to various time factors\nand the annotation preferences of human experts, the heterophily and homophily\ncan change across training and testing data, which is called structural\ndistribution shift (SDS) in this paper. The mainstream methods are built on\ngraph neural networks (GNNs), benefiting the classification of normals from\naggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and\nsuffering from poor generalization.\n  This work solves the problem from a feature view. We observe that the degree\nof SDS varies between anomalies and normal nodes. Hence to address the issue,\nthe key lies in resisting high heterophily for anomalies meanwhile benefiting\nthe learning of normals from homophily. We tease out the anomaly features on\nwhich we constrain to mitigate the effect of heterophilous neighbors and make\nthem invariant. We term our proposed framework as Graph Decomposition Network\n(GDN). Extensive experiments are conducted on two benchmark datasets, and the\nproposed framework achieves a remarkable performance boost in GAD, especially\nin an SDS environment where anomalies have largely different structural\ndistribution across training and testing environments. Codes are open-sourced\nin https://github.com/blacksingular/wsdm_GDN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to WSDM 2023",
    "pdf_url": "http://arxiv.org/pdf/2401.14155v1",
    "published_date": "2024-01-25 13:07:34 UTC",
    "updated_date": "2024-01-25 13:07:34 UTC"
  },
  {
    "arxiv_id": "2401.14153v1",
    "title": "Agent-based Simulation with Netlogo to Evaluate AmI Scenarios",
    "authors": [
      "J. Carbo",
      "N. Sanchez",
      "J. M. Molina"
    ],
    "abstract": "In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14153v1",
    "published_date": "2024-01-25 13:05:06 UTC",
    "updated_date": "2024-01-25 13:05:06 UTC"
  },
  {
    "arxiv_id": "2402.04874v1",
    "title": "Choosing a Classical Planner with Graph Neural Networks",
    "authors": [
      "Jana Vatter",
      "Ruben Mayer",
      "Hans-Arno Jacobsen",
      "Horst Samulowitz",
      "Michael Katz"
    ],
    "abstract": "Online planner selection is the task of choosing a solver out of a predefined\nset for a given planning problem. As planning is computationally hard, the\nperformance of solvers varies greatly on planning problems. Thus, the ability\nto predict their performance on a given problem is of great importance. While a\nvariety of learning methods have been employed, for classical cost-optimal\nplanning the prevailing approach uses Graph Neural Networks (GNNs). In this\nwork, we continue the line of work on using GNNs for online planner selection.\nWe perform a thorough investigation of the impact of the chosen GNN model,\ngraph representation and node features, as well as prediction task. Going\nfurther, we propose using the graph representation obtained by a GNN as an\ninput to the Extreme Gradient Boosting (XGBoost) model, resulting in a more\nresource-efficient yet accurate approach. We show the effectiveness of a\nvariety of GNN-based online planner selection methods, opening up new exciting\navenues for research on online planner selection.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.04874v1",
    "published_date": "2024-01-25 13:04:27 UTC",
    "updated_date": "2024-01-25 13:04:27 UTC"
  },
  {
    "arxiv_id": "2401.14151v2",
    "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning",
    "authors": [
      "Weihao Tan",
      "Wentao Zhang",
      "Shanqi Liu",
      "Longtao Zheng",
      "Xinrun Wang",
      "Bo An"
    ],
    "abstract": "Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14151v2",
    "published_date": "2024-01-25 13:03:20 UTC",
    "updated_date": "2024-03-11 03:15:58 UTC"
  },
  {
    "arxiv_id": "2402.00052v1",
    "title": "Zero-shot Sequential Neuro-symbolic Reasoning for Automatically Generating Architecture Schematic Designs",
    "authors": [
      "Milin Kodnongbua",
      "Lawrence H. Curtis",
      "Adriana Schulz"
    ],
    "abstract": "This paper introduces a novel automated system for generating architecture\nschematic designs aimed at streamlining complex decision-making at the\nmultifamily real estate development project's outset. Leveraging the combined\nstrengths of generative AI (neuro reasoning) and mathematical program solvers\n(symbolic reasoning), the method addresses both the reliance on expert insights\nand technical challenges in architectural schematic design. To address the\nlarge-scale and interconnected nature of design decisions needed for designing\na whole building, we proposed a novel sequential neuro-symbolic reasoning\napproach, emulating traditional architecture design processes from initial\nconcept to detailed layout. To remove the need to hand-craft a cost function to\napproximate the desired objectives, we propose a solution that uses neuro\nreasoning to generate constraints and cost functions that the symbolic solvers\ncan use to solve. We also incorporate feedback loops for each design stage to\nensure a tight integration between neuro and symbolic reasoning. Developed\nusing GPT-4 without further training, our method's effectiveness is validated\nthrough comparative studies with real-world buildings. Our method can generate\nvarious building designs in accordance with the understanding of the\nneighborhood, showcasing its potential to transform the realm of architectural\nschematic design.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00052v1",
    "published_date": "2024-01-25 12:52:42 UTC",
    "updated_date": "2024-01-25 12:52:42 UTC"
  },
  {
    "arxiv_id": "2401.14142v4",
    "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations",
    "authors": [
      "Xinyue Xu",
      "Yi Qin",
      "Lu Mi",
      "Hao Wang",
      "Xiaomeng Li"
    ],
    "abstract": "Existing methods, such as concept bottleneck models (CBMs), have been\nsuccessful in providing concept-based interpretations for black-box deep\nlearning models. They typically work by predicting concepts given the input and\nthen predicting the final class label given the predicted concepts. However,\n(1) they often fail to capture the high-order, nonlinear interaction between\nconcepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\nhelp correct highly correlated concepts (e.g., \"yellow belly\"), leading to\nsuboptimal final accuracy; (2) they cannot naturally quantify the complex\nconditional dependencies between different concepts and class labels (e.g., for\nan image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\nwhat is the probability that the model correctly predicts another concept\n\"black crown\"), therefore failing to provide deeper insight into how a\nblack-box model works. In response to these limitations, we propose\nEnergy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\nnetworks to define the joint energy of candidate (input, concept, class)\ntuples. With such a unified interface, prediction, concept correction, and\nconditional dependency quantification are then represented as conditional\nprobabilities, which are generated by composing different energy functions. Our\nECBMs address both limitations of existing CBMs, providing higher accuracy and\nricher concept interpretations. Empirical results show that our approach\noutperforms the state-of-the-art on real-world datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14142v4",
    "published_date": "2024-01-25 12:46:37 UTC",
    "updated_date": "2024-12-31 03:33:55 UTC"
  },
  {
    "arxiv_id": "2402.03349v1",
    "title": "When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",
    "authors": [
      "Abdenour Hadid",
      "Tanujit Chakraborty",
      "Daniel Busby"
    ],
    "abstract": "Generative Artificial Intelligence (GAI) represents an emerging field that\npromises the creation of synthetic data and outputs in different modalities.\nGAI has recently shown impressive results across a large spectrum of\napplications ranging from biology, medicine, education, legislation, computer\nscience, and finance. As one strives for enhanced safety, efficiency, and\nsustainability, generative AI indeed emerges as a key differentiator and\npromises a paradigm shift in the field. This paper explores the potential\napplications of generative AI and large language models in geoscience. The\nrecent developments in the field of machine learning and deep learning have\nenabled the generative model's utility for tackling diverse prediction\nproblems, simulation, and multi-criteria decision-making challenges related to\ngeoscience and Earth system dynamics. This survey discusses several GAI models\nthat have been used in geoscience comprising generative adversarial networks\n(GANs), physics-informed neural networks (PINNs), and generative pre-trained\ntransformer (GPT)-based structures. These tools have helped the geoscience\ncommunity in several applications, including (but not limited to) data\ngeneration/augmentation, super-resolution, panchromatic sharpening, haze\nremoval, restoration, and land surface changing. Some challenges still remain\nsuch as ensuring physical interpretation, nefarious use cases, and\ntrustworthiness. Beyond that, GAI models show promises to the geoscience\ncommunity, especially with the support to climate change, urban science,\natmospheric science, marine science, and planetary science through their\nextraordinary ability to data-driven modeling and uncertainty quantification.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.03349v1",
    "published_date": "2024-01-25 12:03:50 UTC",
    "updated_date": "2024-01-25 12:03:50 UTC"
  },
  {
    "arxiv_id": "2401.15103v1",
    "title": "PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for Symbolic Regression",
    "authors": [
      "Min Wu",
      "Weijun Li",
      "Lina Yu",
      "Wenqiang Li",
      "Jingyi Liu",
      "Yanjie Li",
      "Meilan Hao"
    ],
    "abstract": "Symbolic regression aims to derive interpretable symbolic expressions from\ndata in order to better understand and interpret data. %which plays an\nimportant role in knowledge discovery and interpretable machine learning.\n  In this study, a symbolic network called PruneSymNet is proposed for symbolic\nregression. This is a novel neural network whose activation function consists\nof common elementary functions and operators. The whole network is\ndifferentiable and can be trained by gradient descent method. Each subnetwork\nin the network corresponds to an expression, and our goal is to extract such\nsubnetworks to get the desired symbolic expression.\n  Therefore, a greedy pruning algorithm is proposed to prune the network into a\nsubnetwork while ensuring the accuracy of data fitting. The proposed greedy\npruning algorithm preserves the edge with the least loss in each pruning, but\ngreedy algorithm often can not get the optimal solution. In order to alleviate\nthis problem, we combine beam search during pruning to obtain multiple\ncandidate expressions each time, and finally select the expression with the\nsmallest loss as the final result. It was tested on the public data set and\ncompared with the current popular algorithms. The results showed that the\nproposed algorithm had better accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15103v1",
    "published_date": "2024-01-25 11:53:35 UTC",
    "updated_date": "2024-01-25 11:53:35 UTC"
  },
  {
    "arxiv_id": "2402.01708v2",
    "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
    "authors": [
      "Wiebke Hutiri",
      "Oresiti Papakyriakopoulos",
      "Alice Xiang"
    ],
    "abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a\nrange of significant ethical and safety risks to society that need to be\naddressed. For example, a growing number of speech generation incidents are\nassociated with swatting attacks in the United States, where anonymous\nperpetrators create synthetic voices that call police officers to close down\nschools and hospitals, or to violently gain access to innocent citizens' homes.\nIncidents like this demonstrate that multimodal generative AI risks and harms\ndo not exist in isolation, but arise from the interactions of multiple\nstakeholders and technical AI systems. In this paper we analyse speech\ngeneration incidents to study how patterns of specific harms arise. We find\nthat specific harms can be categorised according to the exposure of affected\nindividuals, that is to say whether they are a subject of, interact with,\nsuffer due to, or are excluded from speech generation systems. Similarly,\nspecific harms are also a consequence of the motives of the creators and\ndeployers of the systems. Based on these insights we propose a conceptual\nframework for modelling pathways to ethical and safety harms of AI, which we\nuse to develop a taxonomy of harms of speech generators. Our relational\napproach captures the complexity of risks and harms in sociotechnical AI\nsystems, and yields a taxonomy that can support appropriate policy\ninterventions and decision making for the responsible development and release\nof speech generation models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 4 tables, 4 figures Accepted at the 2024 ACM Conference on\n  Fairness, Accountability, and Transparency (ACM FAccT '24)",
    "pdf_url": "http://arxiv.org/pdf/2402.01708v2",
    "published_date": "2024-01-25 11:47:06 UTC",
    "updated_date": "2024-05-15 15:26:42 UTC"
  },
  {
    "arxiv_id": "2401.14112v2",
    "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design",
    "authors": [
      "Haojun Xia",
      "Zhen Zheng",
      "Xiaoxia Wu",
      "Shiyang Chen",
      "Zhewei Yao",
      "Stephen Youn",
      "Arash Bakhtiari",
      "Michael Wyatt",
      "Donglin Zhuang",
      "Zhongzhu Zhou",
      "Olatunji Ruwase",
      "Yuxiong He",
      "Shuaiwen Leon Song"
    ],
    "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code is publicly available at\nhttps://github.com/usyd-fsalab/fp6_llm.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "Adding URL link of the source code",
    "pdf_url": "http://arxiv.org/pdf/2401.14112v2",
    "published_date": "2024-01-25 11:46:38 UTC",
    "updated_date": "2024-03-04 02:30:21 UTC"
  },
  {
    "arxiv_id": "2401.14110v1",
    "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators",
    "authors": [
      "Yaniv Blumenfeld",
      "Itay Hubara",
      "Daniel Soudry"
    ],
    "abstract": "The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14110v1",
    "published_date": "2024-01-25 11:46:01 UTC",
    "updated_date": "2024-01-25 11:46:01 UTC"
  },
  {
    "arxiv_id": "2401.14109v2",
    "title": "CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks",
    "authors": [
      "Andrei Tomut",
      "Saeed S. Jahromi",
      "Abhijoy Sarkar",
      "Uygar Kurt",
      "Sukhbinder Singh",
      "Faysal Ishtiaq",
      "Cesar Muñoz",
      "Prabdeep Singh Bajaj",
      "Ali Elborady",
      "Gianni del Bimbo",
      "Mehrazin Alizadeh",
      "David Montero",
      "Pablo Martin-Ramiro",
      "Muhammad Ibrahim",
      "Oussama Tahiri Alaoui",
      "John Malcolm",
      "Samuel Mugel",
      "Roman Orus"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly\nin generative Artificial Intelligence (AI), but their immense size poses\nsignificant challenges, such as huge training and inference costs, substantial\nenergy demands, and limitations for on-site deployment. Traditional compression\nmethods such as pruning, distillation, and low-rank approximation focus on\nreducing the effective number of neurons in the network, while quantization\nfocuses on reducing the numerical precision of individual weights to reduce the\nmodel size while keeping the number of neurons fixed. While these compression\nmethods have been relatively successful in practice, there is no compelling\nreason to believe that truncating the number of neurons is an optimal strategy.\nIn this context, this paper introduces CompactifAI, an innovative LLM\ncompression approach using quantum-inspired Tensor Networks that focuses on the\nmodel's correlation space instead, allowing for a more controlled, refined and\ninterpretable model compression. Our method is versatile and can be implemented\nwith - or on top of - other compression techniques. As a benchmark, we\ndemonstrate that a combination of CompactifAI with quantization allows to\nreduce a 93% the memory size of LlaMA 7B, reducing also 70% the number of\nparameters, accelerating 50% the training and 25% the inference times of the\nmodel, and just with a small accuracy drop of 2% - 3%, going much beyond of\nwhat is achievable today by other compression techniques. Our methods also\nallow to perform a refined layer sensitivity profiling, showing that deeper\nlayers tend to be more suitable for tensor network compression, which is\ncompatible with recent observations on the ineffectiveness of those layers for\nLLM performance. Our results imply that standard LLMs are, in fact, heavily\noverparametrized, and do not need to be large at all.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 4 figures, 2 tables, and supplementary information of 2\n  pages and 1 figure. Revised version with new benchmarks for LlaMA2-7B",
    "pdf_url": "http://arxiv.org/pdf/2401.14109v2",
    "published_date": "2024-01-25 11:45:21 UTC",
    "updated_date": "2024-05-13 10:48:36 UTC"
  },
  {
    "arxiv_id": "2401.14089v1",
    "title": "GQHAN: A Grover-inspired Quantum Hard Attention Network",
    "authors": [
      "Ren-Xin Zhao",
      "Jinjing Shi",
      "Xuelong Li"
    ],
    "abstract": "Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy\nin discerning the significance of quantum data, resulting in diminished\nefficacy when handling extensive quantum datasets. Hard Attention Mechanism\n(HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters\nthe substantial challenge of non-differentiability, consequently constraining\nits extensive applicability. In response to the dilemma of HAM and QML, a\nGrover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a\nFlexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed.\nNotably, the FO is designed to surmount the non-differentiable issue by\nexecuting the activation or masking of Discrete Primitives (DPs) with Flexible\nControl (FC) to weave various discrete destinies. Based on this, such discrete\nchoice can be visualized with a specially defined Quantum Hard Attention Score\n(QHAS). Furthermore, a trainable ADO is devised to boost the generality and\nflexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network\n(GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST\nbinary classification. Experimental findings demonstrate that GQHAN adeptly\nsurmounts the non-differentiability hurdle, surpassing the efficacy of extant\nquantum soft self-attention mechanisms in accuracies and learning ability. In\nnoise experiments, GQHAN is robuster to bit-flip noise in accuracy and\namplitude damping noise in learning performance. Predictably, the proposal of\nGQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for\nfuture quantum computers to process large-scale data, and promotes the\ndevelopment of quantum computer vision.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14089v1",
    "published_date": "2024-01-25 11:11:16 UTC",
    "updated_date": "2024-01-25 11:11:16 UTC"
  },
  {
    "arxiv_id": "2401.14086v4",
    "title": "Generating Likely Counterfactuals Using Sum-Product Networks",
    "authors": [
      "Jiri Nemecek",
      "Tomas Pevny",
      "Jakub Marecek"
    ],
    "abstract": "The need to explain decisions made by AI systems is driven by both recent\nregulation and user demand. The decisions are often explainable only post hoc.\nIn counterfactual explanations, one may ask what constitutes the best\ncounterfactual explanation. Clearly, multiple criteria must be taken into\naccount, although \"distance from the sample\" is a key criterion. Recent methods\nthat consider the plausibility of a counterfactual seem to sacrifice this\noriginal objective. Here, we present a system that provides high-likelihood\nexplanations that are, at the same time, close and sparse. We show that the\nsearch for the most likely explanations satisfying many common desiderata for\ncounterfactual explanations can be modeled using Mixed-Integer Optimization\n(MIO). We use a Sum-Product Network (SPN) to estimate the likelihood of a\ncounterfactual. To achieve that, we propose an MIO formulation of an SPN, which\ncan be of independent interest. The source code with examples is available at\nhttps://github.com/Epanemu/LiCE.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14086v4",
    "published_date": "2024-01-25 11:06:16 UTC",
    "updated_date": "2025-03-21 10:55:53 UTC"
  },
  {
    "arxiv_id": "2401.14079v1",
    "title": "From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures",
    "authors": [
      "Tobias Eisenreich",
      "Sandro Speth",
      "Stefan Wagner"
    ],
    "abstract": "Designing domain models and software architectures represents a significant\nchallenge in software development, as the resulting architectures play a vital\nrole in fulfilling the system's quality of service. Due to time pressure,\narchitects often model only one architecture based on their known limited\ndomain understanding, patterns, and experience instead of thoroughly analyzing\nthe domain and evaluating multiple candidates, selecting the best fitting.\nExisting approaches try to generate domain models based on requirements, but\nstill require time-consuming manual effort to achieve good results. Therefore,\nin this vision paper, we propose a method to generate software architecture\ncandidates semi-automatically based on requirements using artificial\nintelligence techniques. We further envision an automatic evaluation and\ntrade-off analysis of the generated architecture candidates using, e.g., the\narchitecture trade-off analysis method combined with large language models and\nquantitative analyses. To evaluate this approach, we aim to analyze the quality\nof the generated architecture models and the efficiency and effectiveness of\nour proposed process by conducting qualitative studies.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "D.2.2"
    ],
    "primary_category": "cs.SE",
    "comment": "4 pages, vision paper, submitted to the ICSE workshop Designing2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14079v1",
    "published_date": "2024-01-25 10:56:58 UTC",
    "updated_date": "2024-01-25 10:56:58 UTC"
  },
  {
    "arxiv_id": "2401.14067v1",
    "title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims",
    "authors": [
      "Saud Althabiti",
      "Mohammad Ammar Alsalka",
      "Eric Atwell"
    ],
    "abstract": "This paper introduces Ta'keed, an explainable Arabic automatic fact-checking\nsystem. While existing research often focuses on classifying claims as \"True\"\nor \"False,\" there is a limited exploration of generating explanations for claim\ncredibility, particularly in Arabic. Ta'keed addresses this gap by assessing\nclaim truthfulness based on retrieved snippets, utilizing two main components:\ninformation retrieval and LLM-based claim verification. We compiled the\nArFactEx, a testing gold-labelled dataset with manually justified references,\nto evaluate the system. The initial model achieved a promising F1 score of 0.72\nin the classification task. Meanwhile, the system's generated explanations are\ncompared with gold-standard explanations syntactically and semantically. The\nstudy recommends evaluating using semantic similarities, resulting in an\naverage cosine similarity score of 0.76. Additionally, we explored the impact\nof varying snippet quantities on claim classification accuracy, revealing a\npotential correlation, with the model using the top seven hits outperforming\nothers with an F1 score of 0.77.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, conference paper",
    "pdf_url": "http://arxiv.org/pdf/2401.14067v1",
    "published_date": "2024-01-25 10:43:00 UTC",
    "updated_date": "2024-01-25 10:43:00 UTC"
  },
  {
    "arxiv_id": "2401.14066v3",
    "title": "CreativeSynth: Cross-Art-Attention for Artistic Image Synthesis with Multimodal Diffusion",
    "authors": [
      "Nisha Huang",
      "Weiming Dong",
      "Yuxin Zhang",
      "Fan Tang",
      "Ronghui Li",
      "Chongyang Ma",
      "Xiu Li",
      "Tong-Yee Lee",
      "Changsheng Xu"
    ],
    "abstract": "Although remarkable progress has been made in image style transfer, style is\njust one of the components of artistic paintings. Directly transferring\nextracted style features to natural images often results in outputs with\nobvious synthetic traces. This is because key painting attributes including\nlayout, perspective, shape, and semantics often cannot be conveyed and\nexpressed through style transfer. Large-scale pretrained text-to-image\ngeneration models have demonstrated their capability to synthesize a vast\namount of high-quality images. However, even with extensive textual\ndescriptions, it is challenging to fully express the unique visual properties\nand details of paintings. Moreover, generic models often disrupt the overall\nartistic effect when modifying specific areas, making it more complicated to\nachieve a unified aesthetic in artworks. Our main novel idea is to integrate\nmultimodal semantic information as a synthesis guide into artworks, rather than\ntransferring style to the real world. We also aim to reduce the disruption to\nthe harmony of artworks while simplifying the guidance conditions.\nSpecifically, we propose an innovative multi-task unified framework called\nCreativeSynth, based on the diffusion model with the ability to coordinate\nmultimodal inputs. CreativeSynth combines multimodal features with customized\nattention mechanisms to seamlessly integrate real-world semantic content into\nthe art domain through Cross-Art-Attention for aesthetic maintenance and\nsemantic fusion. We demonstrate the results of our method across a wide range\nof different art categories, proving that CreativeSynth bridges the gap between\ngenerative models and artistic expression. Code and results are available at\nhttps://github.com/haha-lisa/CreativeSynth.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14066v3",
    "published_date": "2024-01-25 10:42:09 UTC",
    "updated_date": "2025-05-15 10:04:37 UTC"
  },
  {
    "arxiv_id": "2401.14057v2",
    "title": "Left/Right Brain, human motor control and the implications for robotics",
    "authors": [
      "Jarrad Rinaldo",
      "Levin Kuhlmann",
      "Jason Friedman",
      "Gideon Kowadlo"
    ],
    "abstract": "Neural Network movement controllers promise a variety of advantages over\nconventional control methods, however, they are not widely adopted due to their\ninability to produce reliably precise movements. This research explores a\nbilateral neural network architecture as a control system for motor tasks. We\naimed to achieve hemispheric specialisation similar to what is observed in\nhumans across different tasks; the dominant system (usually the right hand,\nleft hemisphere) excels at tasks involving coordination and efficiency of\nmovement, and the non-dominant system performs better at tasks requiring\npositional stability. Specialisation was achieved by training the hemispheres\nwith different loss functions tailored to the expected behaviour of the\nrespective hemispheres. We compared bilateral models with and without\nspecialised hemispheres, with and without inter-hemispheric connectivity\n(representing the biological Corpus Callosum), and unilateral models with and\nwithout specialisation. The models were trained and tested on two tasks common\nin the human motor control literature: the random reach task, suited to the\ndominant system, a model with better coordination, and the hold position task,\nsuited to the non-dominant system, a model with more stable movement. Each\nsystem outperformed the non-preferred system in its preferred task. For both\ntasks, a bilateral model outperformed the non-preferred hand and was as good or\nbetter than the preferred hand. The results suggest that the hemispheres could\ncollaborate on tasks or work independently to their strengths. This study\nprovides ideas for how a biologically inspired bilateral architecture could be\nexploited for industrial motor control.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "q-bio.NC",
      "I.2.6; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "ACAIN 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14057v2",
    "published_date": "2024-01-25 10:29:07 UTC",
    "updated_date": "2024-07-10 12:47:20 UTC"
  },
  {
    "arxiv_id": "2401.14043v3",
    "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey",
    "authors": [
      "Haochen Li",
      "Jonathan Leung",
      "Zhiqi Shen"
    ],
    "abstract": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks and prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not only as an overview of current prompt\nengineering methods, but also aims to highlight the limitation of designing\nprompts based on an anthropomorphic assumption that expects LLMs to think like\nhumans. From our review of 50 representative studies, we demonstrate that a\ngoal-oriented prompt formulation, which guides LLMs to follow established human\nlogical thinking, significantly improves the performance of LLMs. Furthermore,\nWe introduce a novel taxonomy that categorizes goal-oriented prompting methods\ninto five interconnected stages and we demonstrate the broad applicability of\nour framework. With four future directions proposed, we hope to further\nemphasize the power and potential of goal-oriented prompt engineering in all\nfields.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "An up-to-date resource including papers and tasks is maintained at\n  https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering",
    "pdf_url": "http://arxiv.org/pdf/2401.14043v3",
    "published_date": "2024-01-25 09:47:55 UTC",
    "updated_date": "2024-09-17 04:56:03 UTC"
  },
  {
    "arxiv_id": "2401.14434v2",
    "title": "Transforming gradient-based techniques into interpretable methods",
    "authors": [
      "Caroline Mazini Rodrigues",
      "Nicolas Boutry",
      "Laurent Najman"
    ],
    "abstract": "The explication of Convolutional Neural Networks (CNN) through xAI techniques\noften poses challenges in interpretation. The inherent complexity of input\nfeatures, notably pixels extracted from images, engenders complex correlations.\nGradient-based methodologies, exemplified by Integrated Gradients (IG),\neffectively demonstrate the significance of these features. Nevertheless, the\nconversion of these explanations into images frequently yields considerable\nnoise. Presently, we introduce GAD (Gradient Artificial Distancing) as a\nsupportive framework for gradient-based techniques. Its primary objective is to\naccentuate influential regions by establishing distinctions between classes.\nThe essence of GAD is to limit the scope of analysis during visualization and,\nconsequently reduce image noise. Empirical investigations involving occluded\nimages have demonstrated that the identified regions through this methodology\nindeed play a pivotal role in facilitating class differentiation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14434v2",
    "published_date": "2024-01-25 09:24:19 UTC",
    "updated_date": "2024-05-15 08:52:23 UTC"
  },
  {
    "arxiv_id": "2401.14032v1",
    "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting",
    "authors": [
      "Butian Xiong",
      "Zhuo Li",
      "Zhen Li"
    ],
    "abstract": "We introduce a novel large-scale scene reconstruction benchmark using the\nnewly developed 3D representation approach, Gaussian Splatting, on our\nexpansive U-Scene dataset. U-Scene encompasses over one and a half square\nkilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground\ntruth. For data acquisition, we employed the Matrix 300 drone equipped with the\nhigh-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This\ndataset, offers a unique blend of urban and academic environments for advanced\nspatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with\nGaussian Splatting includes a detailed analysis across various novel\nviewpoints. We also juxtapose these results with those derived from our\naccurate point cloud dataset, highlighting significant differences that\nunderscore the importance of combine multi-modal information",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IJCAI2024 submit, 8 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.14032v1",
    "published_date": "2024-01-25 09:22:32 UTC",
    "updated_date": "2024-01-25 09:22:32 UTC"
  },
  {
    "arxiv_id": "2401.14019v1",
    "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI",
    "authors": [
      "Elron Bandel",
      "Yotam Perlitz",
      "Elad Venezian",
      "Roni Friedman-Melamed",
      "Ofir Arviv",
      "Matan Orbach",
      "Shachar Don-Yehyia",
      "Dafna Sheinwald",
      "Ariel Gera",
      "Leshem Choshen",
      "Michal Shmueli-Scheuer",
      "Yoav Katz"
    ],
    "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to NAACL demo track",
    "pdf_url": "http://arxiv.org/pdf/2401.14019v1",
    "published_date": "2024-01-25 08:57:33 UTC",
    "updated_date": "2024-01-25 08:57:33 UTC"
  },
  {
    "arxiv_id": "2401.14011v3",
    "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning",
    "authors": [
      "Zheqi He",
      "Xinya Wu",
      "Pengfei Zhou",
      "Richeng Xuan",
      "Guang Liu",
      "Xi Yang",
      "Qiannan Zhu",
      "Hua Huang"
    ],
    "abstract": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose an evaluation strategy called Positional Error Variance for assessing\nmultiple-choice questions. The strategy aims to perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs. The data and code are available at\nhttps://github.com/FlagOpen/CMMU.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.14011v3",
    "published_date": "2024-01-25 08:22:10 UTC",
    "updated_date": "2024-05-08 07:34:06 UTC"
  },
  {
    "arxiv_id": "2401.14003v1",
    "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases",
    "authors": [
      "Quyet V. Do",
      "Tianqing Fang",
      "Shizhe Diao",
      "Zhaowei Wang",
      "Yangqiu Song"
    ],
    "abstract": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has\nbeen explored as a way to acquire new commonsense knowledge based on reference\nknowledge in the original CSKBs and external prior knowledge. Despite the\nadvancement of Large Language Models (LLM) and prompt engineering techniques in\nvarious reasoning tasks, they still struggle to deal with CSKB reasoning. One\nof the problems is that it is hard for them to acquire explicit relational\nconstraints in CSKBs from only in-context exemplars, due to a lack of symbolic\nreasoning capabilities (Bengio et al., 2021). To this end, we proposed\n**ConstraintChecker**, a plugin over prompting techniques to provide and check\nexplicit constraints. When considering a new knowledge instance,\nConstraintChecker employs a rule-based module to produce a list of constraints,\nthen it uses a zero-shot learning module to check whether this knowledge\ninstance satisfies all constraints. The acquired constraint-checking result is\nthen aggregated with the output of the main prompting technique to produce the\nfinal output. Experimental results on CSKB Reasoning benchmarks demonstrate the\neffectiveness of our method by bringing consistent improvements over all\nprompting methods. Codes and data are available at\n\\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Proceedings of EACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.14003v1",
    "published_date": "2024-01-25 08:03:38 UTC",
    "updated_date": "2024-01-25 08:03:38 UTC"
  },
  {
    "arxiv_id": "2401.13996v1",
    "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution",
    "authors": [
      "Cheng Qian",
      "Shihao Liang",
      "Yujia Qin",
      "Yining Ye",
      "Xin Cong",
      "Yankai Lin",
      "Yesai Wu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy\nfor enhancing the adaptability and flexibility of AI agents through inter-task\nself-evolution. Unlike existing methods focused on intra-task learning, ICE\npromotes the transfer of knowledge between tasks for genuine self-evolution,\nsimilar to human experience learning. The strategy dynamically investigates\nplanning and execution trajectories, consolidates them into simplified\nworkflows and pipelines, and exploits them for improved task execution. Our\nexperiments on the XAgent framework demonstrate ICE's effectiveness, reducing\nAPI calls by as much as 80% and significantly decreasing the demand for the\nmodel's capability. Specifically, when combined with GPT-3.5, ICE's performance\nmatches that of raw GPT-4 across various agent tasks. We argue that this\nself-evolution approach represents a paradigm shift in agent design,\ncontributing to a more robust AI community and ecosystem, and moving a step\ncloser to full autonomy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13996v1",
    "published_date": "2024-01-25 07:47:49 UTC",
    "updated_date": "2024-01-25 07:47:49 UTC"
  },
  {
    "arxiv_id": "2402.03348v2",
    "title": "Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition",
    "authors": [
      "Sangyu Han",
      "Yearim Kim",
      "Nojun Kwak"
    ],
    "abstract": "The truthfulness of existing explanation methods in authentically elucidating\nthe underlying model's decision-making process has been questioned. Existing\nmethods have deviated from faithfully representing the model, thus susceptible\nto adversarial attacks. To address this, we propose a novel eXplainable AI\n(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects\nthe model's inference process, resulting in significantly enhanced robustness\nin our explanations. Different from the conventional emphasis on the neuronal\nlevel, we adopt a vector perspective to consider the intricate nonlinear\ninteractions between filters. We also introduce an interesting observation\ntermed Activation-Pattern-Only Prediction (APOP), letting us emphasize the\nimportance of inactive neurons and redefine relevance encapsulating all\nrelevant information including both active and inactive neurons. Our method,\nSRD, allows for the recursive decomposition of a Pointwise Feature Vector\n(PFV), providing a high-resolution Effective Receptive Field (ERF) at any\nlayer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "To be published in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2402.03348v2",
    "published_date": "2024-01-25 07:20:23 UTC",
    "updated_date": "2024-12-12 05:56:34 UTC"
  },
  {
    "arxiv_id": "2401.13987v1",
    "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks",
    "authors": [
      "Naeem Paeedeh",
      "Mahardhika Pratama",
      "Muhammad Anwar Ma'sum",
      "Wolfgang Mayer",
      "Zehong Cao",
      "Ryszard Kowlczyk"
    ],
    "abstract": "Most few-shot learning works rely on the same domain assumption between the\nbase and the target tasks, hindering their practical applications. This paper\nproposes an adaptive transformer network (ADAPTER), a simple but effective\nsolution for cross-domain few-shot learning where there exist large domain\nshifts between the base task and the target task. ADAPTER is built upon the\nidea of bidirectional cross-attention to learn transferable features between\nthe two domains. The proposed architecture is trained with DINO to produce\ndiverse, and less biased features to avoid the supervision collapse problem.\nFurthermore, the label smoothing approach is proposed to improve the\nconsistency and reliability of the predictions by also considering the\npredicted labels of the close samples in the embedding space. The performance\nof ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it\noutperforms prior arts with significant margins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Consideration in Knowledge-based Systems",
    "pdf_url": "http://arxiv.org/pdf/2401.13987v1",
    "published_date": "2024-01-25 07:05:42 UTC",
    "updated_date": "2024-01-25 07:05:42 UTC"
  },
  {
    "arxiv_id": "2401.13986v1",
    "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
    "authors": [
      "Yanda Chen",
      "Chandan Singh",
      "Xiaodong Liu",
      "Simiao Zuo",
      "Bin Yu",
      "He He",
      "Jianfeng Gao"
    ],
    "abstract": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2307.08678",
    "pdf_url": "http://arxiv.org/pdf/2401.13986v1",
    "published_date": "2024-01-25 07:04:30 UTC",
    "updated_date": "2024-01-25 07:04:30 UTC"
  },
  {
    "arxiv_id": "2401.13979v3",
    "title": "Routoo: Learning to Route to Large Language Models Effectively",
    "authors": [
      "Alireza Mohammadshahi",
      "Arshad Rafiq Shaikh",
      "Majid Yazdani"
    ],
    "abstract": "LLMs with superior response quality--particularly larger or closed-source\nmodels--often come with higher inference costs, making their deployment\ninefficient and costly. Meanwhile, developing foundational LLMs from scratch is\nbecoming increasingly resource-intensive and impractical for many applications.\nTo address the challenge of balancing quality and cost, we introduce Routoo, an\narchitecture designed to optimize the selection of LLMs for specific prompts\nbased on performance, cost, and efficiency. Routoo provides controllability\nover the trade-off between inference cost and quality, enabling significant\nreductions in inference costs for a given quality requirement. Routoo comprises\ntwo key components: a performance predictor and cost-aware selector. The\nperformance predictor is a lightweight LLM that estimates the expected\nperformance of various underlying LLMs on a given prompt without executing\nthem. The cost-aware selector module then selects the most suitable model based\non these predictions and constraints such as cost and latency, significantly\nreducing inference costs for the same quality. We evaluated Routoo using the\nMMLU benchmark across 57 domains employing open-source models. Our results show\nthat Routoo matches the performance of the Mixtral 8x7b model while reducing\ninference costs by one-third. Additionally, by allowing increased costs, Routoo\nsurpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an\naccuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly\nmatches GPT4's performance at half the cost and exceeds it with a 25% cost\nreduction. These outcomes highlight Routoo's potential to significantly reduce\ninference costs without compromising quality, and even to establish new\nstate-of-the-art results by leveraging the collective capabilities of multiple\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13979v3",
    "published_date": "2024-01-25 06:45:32 UTC",
    "updated_date": "2024-10-02 08:51:45 UTC"
  },
  {
    "arxiv_id": "2401.13976v1",
    "title": "Learning to Manipulate Artistic Images",
    "authors": [
      "Wei Guo",
      "Yuqi Zhang",
      "De Ma",
      "Qian Zheng"
    ],
    "abstract": "Recent advancement in computer vision has significantly lowered the barriers\nto artistic creation. Exemplar-based image translation methods have attracted\nmuch attention due to flexibility and controllability. However, these methods\nhold assumptions regarding semantics or require semantic information as the\ninput, while accurate semantics is not easy to obtain in artistic images.\nBesides, these methods suffer from cross-domain artifacts due to training data\nprior and generate imprecise structure due to feature compression in the\nspatial domain. In this paper, we propose an arbitrary Style Image Manipulation\nNetwork (SIM-Net), which leverages semantic-free information as guidance and a\nregion transportation strategy in a self-supervised manner for image\ngeneration. Our method balances computational efficiency and high resolution to\na certain extent. Moreover, our method facilitates zero-shot style image\nmanipulation. Both qualitative and quantitative experiments demonstrate the\nsuperiority of our method over state-of-the-art methods.Code is available at\nhttps://github.com/SnailForce/SIM-Net.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13976v1",
    "published_date": "2024-01-25 06:34:49 UTC",
    "updated_date": "2024-01-25 06:34:49 UTC"
  },
  {
    "arxiv_id": "2401.13974v1",
    "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models",
    "authors": [
      "Senthil Purushwalkam",
      "Akash Gokul",
      "Shafiq Joty",
      "Nikhil Naik"
    ],
    "abstract": "Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13974v1",
    "published_date": "2024-01-25 06:18:20 UTC",
    "updated_date": "2024-01-25 06:18:20 UTC"
  },
  {
    "arxiv_id": "2401.13968v1",
    "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks",
    "authors": [
      "Muhammad Anwar Ma'sum",
      "MD Rasel Sarkar",
      "Mahardhika Pratama",
      "Savitha Ramasamy",
      "Sreenatha Anavatti",
      "Lin Liu",
      "Habibullah",
      "Ryszard Kowalczyk"
    ],
    "abstract": "A reliable long-term time-series forecaster is highly demanded in practice\nbut comes across many challenges such as low computational and memory\nfootprints as well as robustness against dynamic learning environments. This\npaper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic\nlong-term time-series forecasting tasks. MANTRA relies on the concept of fast\nand slow learners where a collection of fast learners learns different aspects\nof data distributions while adapting quickly to changes. A slow learner tailors\nsuitable representations to fast learners. Fast adaptations to dynamic\nenvironments are achieved using the universal representation transformer layers\nproducing task-adapted representations with a small number of parameters. Our\nexperiments using four datasets with different prediction lengths demonstrate\nthe advantage of our approach with at least $3\\%$ improvements over the\nbaseline algorithms for both multivariate and univariate settings. Source codes\nof MANTRA are publicly available in\n\\url{https://github.com/anwarmaxsum/MANTRA}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Consideration in IEEE Transactions on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2401.13968v1",
    "published_date": "2024-01-25 06:03:56 UTC",
    "updated_date": "2024-01-25 06:03:56 UTC"
  },
  {
    "arxiv_id": "2403.18827v1",
    "title": "Bridging Generative Networks with the Common Model of Cognition",
    "authors": [
      "Robert L. West",
      "Spencer Eckler",
      "Brendan Conway-Smith",
      "Nico Turcas",
      "Eilene Tomkins-Flanagan",
      "Mary Alexandria Kelly"
    ],
    "abstract": "This article presents a theoretical framework for adapting the Common Model\nof Cognition to large generative network models within the field of artificial\nintelligence. This can be accomplished by restructuring modules within the\nCommon Model into shadow production systems that are peripheral to a central\nproduction system, which handles higher-level reasoning based on the shadow\nproductions' output. Implementing this novel structure within the Common Model\nallows for a seamless connection between cognitive architectures and generative\nneural networks.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.18827v1",
    "published_date": "2024-01-25 05:48:50 UTC",
    "updated_date": "2024-01-25 05:48:50 UTC"
  },
  {
    "arxiv_id": "2401.13945v1",
    "title": "General Automatic Solution Generation of Social Problems",
    "authors": [
      "Tong Niu",
      "Haoyu Huang",
      "Yu Du",
      "Weihao Zhang",
      "Luping Shi",
      "Rong Zhao"
    ],
    "abstract": "Given the escalating intricacy and multifaceted nature of contemporary social\nsystems, manually generating solutions to address pertinent social issues has\nbecome a formidable task. In response to this challenge, the rapid development\nof artificial intelligence has spurred the exploration of computational\nmethodologies aimed at automatically generating solutions. However, current\nmethods for auto-generation of solutions mainly concentrate on local social\nregulations that pertain to specific scenarios. Here, we report an automatic\nsocial operating system (ASOS) designed for general social solution generation,\nwhich is built upon agent-based models, enabling both global and local analyses\nand regulations of social problems across spatial and temporal dimensions. ASOS\nadopts a hypergraph with extensible social semantics for a comprehensive and\nstructured representation of social dynamics. It also incorporates a\ngeneralized protocol for standardized hypergraph operations and a symbolic\nhybrid framework that delivers interpretable solutions, yielding a balance\nbetween regulatory efficacy and function viability. To demonstrate the\neffectiveness of ASOS, we apply it to the domain of averting extreme events\nwithin international oil futures markets. By generating a new trading role\nsupplemented by new mechanisms, ASOS can adeptly discern precarious market\nconditions and make front-running interventions for non-profit purposes. This\nstudy demonstrates that ASOS provides an efficient and systematic approach for\ngenerating solutions for enhancing our society.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13945v1",
    "published_date": "2024-01-25 05:00:46 UTC",
    "updated_date": "2024-01-25 05:00:46 UTC"
  },
  {
    "arxiv_id": "2401.13935v1",
    "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
    "authors": [
      "Lucius E. J. Bynum",
      "Joshua R. Loftus",
      "Julia Stoyanovich"
    ],
    "abstract": "Counterfactuals and counterfactual reasoning underpin numerous techniques for\nauditing and understanding artificial intelligence (AI) systems. The\ntraditional paradigm for counterfactual reasoning in this literature is the\ninterventional counterfactual, where hypothetical interventions are imagined\nand simulated. For this reason, the starting point for causal reasoning about\nlegal protections and demographic data in AI is an imagined intervention on a\nlegally-protected characteristic, such as ethnicity, race, gender, disability,\nage, etc. We ask, for example, what would have happened had your race been\ndifferent? An inherent limitation of this paradigm is that some demographic\ninterventions -- like interventions on race -- may not translate into the\nformalisms of interventional counterfactuals. In this work, we explore a new\nparadigm based instead on the backtracking counterfactual, where rather than\nimagine hypothetical interventions on legally-protected characteristics, we\nimagine alternate initial conditions while holding these characteristics fixed.\nWe ask instead, what would explain a counterfactual outcome for you as you\nactually are or could be? This alternate framework allows us to address many of\nthe same social concerns, but to do so while asking fundamentally different\nquestions that do not rely on demographic interventions.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13935v1",
    "published_date": "2024-01-25 04:28:39 UTC",
    "updated_date": "2024-01-25 04:28:39 UTC"
  },
  {
    "arxiv_id": "2402.03347v1",
    "title": "Transfer Learning With Densenet201 Architecture Model For Potato Leaf Disease Classification",
    "authors": [
      "Rifqi Alfinnur Charisma",
      "Faisal Dharma Adhinata"
    ],
    "abstract": "Potato plants are plants that are beneficial to humans. Like other plants in\ngeneral, potato plants also have diseases; if this disease is not treated\nimmediately, there will be a significant decrease in food production.\nTherefore, it is necessary to detect diseases quickly and precisely so that\ndisease control can be carried out effectively and efficiently. Classification\nof potato leaf disease can be done directly. Still, the symptoms cannot always\nexplain the type of disease that attacks potato leaves because there are many\ntypes of diseases with symptoms that look the same. Humans also have\ndeficiencies in determining the results of identification of potato leaf\ndisease, so sometimes the results of identification between individuals can be\ndifferent. Therefore, the use of Deep Learning for the classification process\nof potato leaf disease is expected to shorten the time and have a high\nclassification accuracy. This study uses a deep learning method with the\nDenseNet201 architecture. The choice to use the DenseNet201 algorithm in this\nstudy is because the model can identify important features of potato leaves and\nrecognize early signs of emerging diseases. This study aimed to evaluate the\neffectiveness of the transfer learning method with the DenseNet201 architecture\nin increasing the classification accuracy of potato leaf disease compared to\ntraditional classification methods. This study uses two types of scenarios,\nnamely, comparing the number of dropouts and comparing the three optimizers.\nThis test produces the best model using dropout 0.1 and Adam optimizer with an\naccuracy of 99.5% for training, 95.2% for validation, and 96% for the confusion\nmatrix. In this study, using data testing, as many as 40 images were tested\ninto the model that has been built. The test results on this model resulted in\na new accuracy for classifying potato leaf disease, namely 92.5%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.03347v1",
    "published_date": "2024-01-25 03:58:40 UTC",
    "updated_date": "2024-01-25 03:58:40 UTC"
  },
  {
    "arxiv_id": "2401.13920v3",
    "title": "LocMoE: A Low-Overhead MoE for Large Language Model Training",
    "authors": [
      "Jing Li",
      "Zhijie Sun",
      "Xuan He",
      "Li Zeng",
      "Yi Lin",
      "Entong Li",
      "Binfan Zheng",
      "Rongqian Zhao",
      "Xin Chen"
    ],
    "abstract": "The Mixtures-of-Experts (MoE) model is a widespread distributed and\nintegrated learning method for large language models (LLM), which is favored\ndue to its ability to sparsify and expand models efficiently. However, the\nperformance of MoE is limited by load imbalance and high latency of All-to-All\ncommunication, along with relatively redundant computation owing to large\nexpert capacity. Load imbalance may result from existing routing policies that\nconsistently tend to select certain experts. The frequent inter-node\ncommunication in the All-to-All procedure also significantly prolongs the\ntraining time. To alleviate the above performance problems, we propose a novel\nrouting strategy that combines load balance and locality by converting partial\ninter-node communication to that of intra-node. Notably, we elucidate that\nthere is a minimum threshold for expert capacity, calculated through the\nmaximal angular deviation between the gating weights of the experts and the\nassigned tokens. We port these modifications on the PanGu-Sigma model based on\nthe MindSpore framework with multi-level routing and conduct experiments on\nAscend clusters. The experiment results demonstrate that the proposed LocMoE\nreduces training time per epoch by 12.68% to 22.24% compared to classical\nrouters, such as hash router and switch router, without impacting the model\naccuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "1. Update the font size of all figures. 2. Update the name of the\n  proposed layer Grouped Average Pooling (GrAP). 3. Change the order of the\n  Section Contribution Statement",
    "pdf_url": "http://arxiv.org/pdf/2401.13920v3",
    "published_date": "2024-01-25 03:36:39 UTC",
    "updated_date": "2024-05-23 10:03:35 UTC"
  },
  {
    "arxiv_id": "2401.13919v4",
    "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
    "authors": [
      "Hongliang He",
      "Wenlin Yao",
      "Kaixin Ma",
      "Wenhao Yu",
      "Yong Dai",
      "Hongming Zhang",
      "Zhenzhong Lan",
      "Dong Yu"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has led to a new era\nmarked by the development of autonomous applications in real-world scenarios,\nwhich drives innovation in creating advanced web agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe establish a new benchmark by compiling real-world tasks from 15 popular\nwebsites and introduce an automatic evaluation protocol leveraging multimodal\nunderstanding abilities of GPT-4V to evaluate open-ended web agents. We show\nthat WebVoyager achieves a 59.1% task success rate on our benchmark,\nsignificantly surpassing the performance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring the exceptional capability of\nWebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement\nwith human judgment, indicating its effectiveness in providing reliable and\naccurate assessments of web agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 (main). Code and data is released at\n  https://github.com/MinorJerry/WebVoyager",
    "pdf_url": "http://arxiv.org/pdf/2401.13919v4",
    "published_date": "2024-01-25 03:33:18 UTC",
    "updated_date": "2024-06-06 18:37:34 UTC"
  },
  {
    "arxiv_id": "2401.13913v2",
    "title": "Spectral Clustering for Discrete Distributions",
    "authors": [
      "Zixiao Wang",
      "Dong Qiao",
      "Jicong Fan"
    ],
    "abstract": "The discrete distribution is often used to describe complex instances in\nmachine learning, such as images, sequences, and documents. Traditionally,\nclustering of discrete distributions (D2C) has been approached using\nWasserstein barycenter methods. These methods operate under the assumption that\nclusters can be well-represented by barycenters, which is seldom true in many\nreal-world applications. Additionally, these methods are not scalable for large\ndatasets due to the high computational cost of calculating Wasserstein\nbarycenters. In this work, we explore the feasibility of using spectral\nclustering combined with distribution affinity measures (e.g., maximum mean\ndiscrepancy and Wasserstein distance) to cluster discrete distributions. We\ndemonstrate that these methods can be more accurate and efficient than\nbarycenter methods. To further enhance scalability, we propose using linear\noptimal transport to construct affinity matrices efficiently for large\ndatasets. We provide theoretical guarantees for the success of our methods in\nclustering distributions. Experiments on both synthetic and real data show that\nour methods outperform existing baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.13913v2",
    "published_date": "2024-01-25 03:17:03 UTC",
    "updated_date": "2024-08-16 06:00:05 UTC"
  },
  {
    "arxiv_id": "2401.15098v2",
    "title": "Hierarchical Continual Reinforcement Learning via Large Language Model",
    "authors": [
      "Chaofan Pan",
      "Xin Yang",
      "Hao Wang",
      "Wei Wei",
      "Tianrui Li"
    ],
    "abstract": "The ability to learn continuously in dynamic environments is a crucial\nrequirement for reinforcement learning (RL) agents applying in the real world.\nDespite the progress in continual reinforcement learning (CRL), existing\nmethods often suffer from insufficient knowledge transfer, particularly when\nthe tasks are diverse. To address this challenge, we propose a new framework,\nHierarchical Continual reinforcement learning via large language model\n(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core\norchestrates a twolayer structure: high-level policy formulation by a large\nlanguage model (LLM), which represents agenerates a sequence of goals, and\nlow-level policy learning that closely aligns with goal-oriented RL practices,\nproducing the agent's actions in response to the goals set forth. The framework\nemploys feedback to iteratively adjust and verify highlevel policies, storing\nthem along with low-level policies within a skill library. When encountering a\nnew task, Hi-Core retrieves relevant experience from this library to help to\nlearning. Through experiments on Minigrid, Hi-Core has demonstrated its\neffectiveness in handling diverse CRL tasks, which outperforms popular\nbaselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.15098v2",
    "published_date": "2024-01-25 03:06:51 UTC",
    "updated_date": "2024-02-01 11:58:07 UTC"
  },
  {
    "arxiv_id": "2401.13904v1",
    "title": "Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression",
    "authors": [
      "Siyu Lou",
      "Chengchun Liu",
      "Yuntian Chen",
      "Fanyang Mo"
    ],
    "abstract": "Thin-layer chromatography (TLC) is a crucial technique in molecular polarity\nanalysis. Despite its importance, the interpretability of predictive models for\nTLC, especially those driven by artificial intelligence, remains a challenge.\nCurrent approaches, utilizing either high-dimensional molecular fingerprints or\ndomain-knowledge-driven feature engineering, often face a dilemma between\nexpressiveness and interpretability. To bridge this gap, we introduce\nUnsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical\nneural networks and symbolic regression. UHiSR automatically distills\nchemical-intuitive polarity indices, and discovers interpretable equations that\nlink molecular structure to chromatographic behavior.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.13904v1",
    "published_date": "2024-01-25 02:48:44 UTC",
    "updated_date": "2024-01-25 02:48:44 UTC"
  },
  {
    "arxiv_id": "2401.13883v3",
    "title": "Domain-Independent Dynamic Programming",
    "authors": [
      "Ryo Kuroiwa",
      "J. Christopher Beck"
    ],
    "abstract": "For combinatorial optimization problems, model-based paradigms such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the `holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a novel\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We introduce\nDynamic Programming Description Language (DyPDL), a formalism to define DP\nmodels based on a state transition system, inspired by artificial intelligence\n(AI) planning. we show that heuristic search algorithms can be used to solve\nDyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP\nsolvers with commercial MIP and CP solvers (solving MIP and CP models,\nrespectively) on common benchmark instances of eleven combinatorial\noptimization problem classes. We show that DIDP outperforms MIP in nine problem\nclasses, CP also in nine problem classes, and both MIP and CP in seven. DIDP\nalso achieves superior performance to existing state-based solvers including\ndomain-independent AI planners.",
    "categories": [
      "cs.AI",
      "F.2.2; I.2.8"
    ],
    "primary_category": "cs.AI",
    "comment": "Manuscript submitted to Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2401.13883v3",
    "published_date": "2024-01-25 01:48:09 UTC",
    "updated_date": "2025-03-18 08:19:21 UTC"
  },
  {
    "arxiv_id": "2402.01705v2",
    "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
    "authors": [
      "Jennifer Chien",
      "David Danks"
    ],
    "abstract": "Algorithmic harms are commonly categorized as either allocative or\nrepresentational. This study specifically addresses the latter, focusing on an\nexamination of current definitions of representational harms to discern what is\nincluded and what is not. This analysis motivates our expansion beyond\nbehavioral definitions to encompass harms to cognitive and affective states.\nThe paper outlines high-level requirements for measurement: identifying the\nnecessary expertise to implement this approach and illustrating it through a\ncase study. Our work highlights the unique vulnerabilities of large language\nmodels to perpetrating representational harms, particularly when these harms go\nunmeasured and unmitigated. The work concludes by presenting proposed\nmitigations and delineating when to employ them. The overarching aim of this\nresearch is to establish a framework for broadening the definition of\nrepresentational harms and to translate insights from fairness research into\npractical measurement and mitigation praxis.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "23 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2402.01705v2",
    "published_date": "2024-01-25 00:54:10 UTC",
    "updated_date": "2024-05-06 21:00:00 UTC"
  }
]
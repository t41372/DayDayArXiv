{
  "date": "2024-06-27",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-27 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 和机器学习创新，特别是大型语言模型（LLMs）的优化、多模态融合和应用扩展，突出 LLMs 在医疗、机器人和图像处理中的潜力，令人印象深刻的包括获奖论文（如\"Synthetic Cancer\"）和开源框架（如\"Granit-Function Calling Model\"），以及知名学者如 Yann LeCun 和 Michael Bowling 的贡献。\n\n### 重点论文讨论\n我们挑选了最具话题度和影响力的论文优先讨论，将相关主题归类。以下聚焦于 AI 模型优化、医疗应用和机器人控制等领域，其他次要论文将简要概述。\n\n**LLMs 优化与多模态应用**  \n- **LiveBench: A Challenging, Contamination-Limited LLM Benchmark**（中文：LiveBench：一个具有挑战性的、受污染限制的 LLM 基准）  \n  这篇论文提出 LiveBench 基准，用于评估 LLM 在各种任务（如数学、编码和指令遵循）上的鲁棒性，通过每月更新问题避免数据污染，实现高准确率（如检测 Sora 生成视频），贡献在于提供一个动态、可扩展的 LLM 评估框架，显著提升模型的泛化能力。\n\n- **Granit-Function Calling Model: Foundation Models of Compiler Optimization**（中文：Granit-函数调用模型：编译器优化的基础模型）  \n  作者包括 Hugh Leather 和 Gabriel Synnaeve，该工作基于 Code Llama 构建开源 LLM（7B 和 13B 参数），训练于海量代码数据，用于编译优化任务，实现 77% 的优化潜力和 45% 的反汇编准确率，主要发现是 LLM 在代码优化的高效应用，填补了该领域的空白。\n\n- **OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents**（中文：OmniJARVIS：统一视觉-语言-动作标记化实现开放世界指令跟随代理）  \n  这篇论文开发 OmniJARVIS 框架，使用 Transformer 统一处理视觉、语言和动作数据，实现高效的多模态机器人控制，在 Minecraft 任务中表现出色，贡献在于通过自监督学习提升代理的泛化性，显著改善多任务性能。\n\n- **Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression**（中文：利用 LLMs 进行自动视频内容分析：针对抑郁短视频的探索性工作流程）  \n  作者探索 LLMs 在视频分析中的潜力，提出一个多阶段工作流程（包括提示工程和人类评估），在 YouTube 抑郁视频上分析关键帧，准确率在物体和活动标注上高达 90%，主要发现是 LLMs 可辅助多模态分析，但情感标注仍需改进。\n\n**医疗 AI 和图像处理**  \n- **PathAlign: A vision-language model for whole slide images in histopathology**（中文：PathAlign：用于组织病理学全切片图像的视觉-语言模型）  \n  这篇论文使用 BLIP-2 框架开发视觉-语言模型，处理巨像素病理图像，实现 78% 的准确文本生成和检索，贡献在于利用病理报告监督训练，提升了 AI 在医疗图像分析中的实用性，如报告生成和分类。\n\n- **Super-resolution imaging using super-oscillatory diffractive neural networks**（中文：使用超振荡衍射神经网络的超分辨率成像）  \n  作者提出 SODNN 模型，通过优化衍射层实现远场超分辨率成像，FWHM 达到 0.407λ，并支持多波长焦点阵列，主要发现是该方法在生物传感和感知应用中显著提升图像质量。\n\n- **ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data**（中文：ManiWAV：从野外音频-视觉数据学习机器人操作）  \n  该工作引入一个数据采集设备和策略，从音频-视觉数据中学习接触丰富操作，在机器人任务中实现高泛化性，贡献在于扩展机器人学习到真实环境，显著改善操作精度。\n\n其他论文涉及 AI 安全、图像生成和优化等，但许多是技术性较强的次要工作，我们快速掠过：  \n- **Synthetic Cancer -- Augmenting Worms with LLMs**（中文：Synthetic Cancer：使用 LLMs 增强蠕虫恶意软件）  \n  获奖论文，提出使用 LLMs 生成变形恶意软件，实现代码重写和传播，主要发现是 LLM 在网络安全的潜在风险。\n\n- **What Matters in Detecting AI-Generated Videos like Sora?**（中文：检测像 Sora 这样的 AI 生成视频的关键因素）  \n  通过训练分类器分析 AI 视频的appearance、motion 和 geometry，发现 AI 视频易检测，主要贡献是提出 Ensemble-of-Experts 模型，提升检测鲁棒性。\n\n剩余论文如算法优化（如\"Meta-Gradient Search Control\"）和特定应用（如\"FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus\"）虽有贡献，但主题较窄，我们仅简要提及：它们分别改善了强化学习效率和金融翻译数据质量，而其他如数学或数据处理论文（如\"Estimating Long-term Heterogeneous Dose-response Curve\"）则因专业性强而快速略过，仅确认其在各自领域的技术进展。\n\n总之，今天的论文突显 AI 模型的优化和实际应用潜力，期待这些创新推动更可靠的 AI 系统！",
  "papers": [
    {
      "arxiv_id": "2406.19578v1",
      "title": "PathAlign: A vision-language model for whole slide images in histopathology",
      "title_zh": "翻译失败",
      "authors": [
        "Faruk Ahmed",
        "Andrew Sellergren",
        "Lin Yang",
        "Shawn Xu",
        "Boris Babenko",
        "Abbi Ward",
        "Niels Olson",
        "Arash Mohtashamian",
        "Yossi Matias",
        "Greg S. Corrado",
        "Quang Duong",
        "Dale R. Webster",
        "Shravya Shetty",
        "Daniel Golden",
        "Yun Liu",
        "David F. Steiner",
        "Ellery Wulczyn"
      ],
      "abstract": "Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.",
      "tldr_zh": "该研究提出PathAlign，一种基于BLIP-2框架的vision-language model，用于处理组织病理学中的全滑玻图像（WSIs），以解决图像规模巨大和图像-文本对创建困难的问题。模型利用超过35万WSIs与诊断文本对进行训练，构建共享图像-文本嵌入空间，支持文本检索、图像检索、报告生成以及AI-in-the-loop交互等应用。实验结果显示，模型生成的文本在78%的WSIs上被病理学家评为准确，无临床意义错误或遗漏，并在WSI分类和工作流程优先级（如滑玻分诊）方面表现出色。该工作展示了vision-language model在计算病理学中的潜力，为利用病理报告作为监督来源提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19578v1",
      "published_date": "2024-06-27 23:43:36 UTC",
      "updated_date": "2024-06-27 23:43:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:09:33.064315"
    },
    {
      "arxiv_id": "2406.19570v2",
      "title": "Synthetic Cancer -- Augmenting Worms with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Zimmerman",
        "David Zollikofer"
      ],
      "abstract": "With increasingly sophisticated large language models (LLMs), the potential\nfor abuse rises drastically. As a submission to the Swiss AI Safety Prize, we\npresent a novel type of metamorphic malware leveraging LLMs for two key\nprocesses. First, LLMs are used for automatic code rewriting to evade\nsignature-based detection by antimalware programs. The malware then spreads its\ncopies via email by utilizing an LLM to socially engineer email replies to\nencourage recipients to execute the attached malware. Our submission includes a\nfunctional minimal prototype, highlighting the risks that LLMs pose for\ncybersecurity and underscoring the need for further research into intelligent\nmalware.",
      "tldr_zh": "本研究提出了一种新型变形恶意软件（metamorphic malware）名为“Synthetic Cancer”，利用大型语言模型（LLMs）增强蠕虫病毒的功能，作为瑞士 AI 安全奖的提交。LLMs 被用于两方面：一是自动重写代码以 evasion 基于签名的反恶意软件检测，二是生成社交工程电子邮件来传播病毒并诱导接收者执行附件。该工作包括一个功能性的最小原型，展示了 LLMs 在网络安全中的潜在风险，并呼吁进一步研究智能恶意软件以强化防护措施。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Won first place at the Swiss AI Safety Prize. Some technical details\n  omitted, contact authors for more information",
      "pdf_url": "http://arxiv.org/pdf/2406.19570v2",
      "published_date": "2024-06-27 23:15:45 UTC",
      "updated_date": "2024-07-12 13:40:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:09:43.555280"
    },
    {
      "arxiv_id": "2406.19568v1",
      "title": "What Matters in Detecting AI-Generated Videos like Sora?",
      "title_zh": "翻译失败",
      "authors": [
        "Chirui Chang",
        "Zhengzhe Liu",
        "Xiaoyang Lyu",
        "Xiaojuan Qi"
      ],
      "abstract": "Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/",
      "tldr_zh": "本研究探讨了AI生成视频（如Sora）与真实视频的差距，从appearance、motion和geometry三个方面比较真实视频和Stable Video Diffusion生成的视频。研究者训练了三个基于3D convolutional networks的分类器：分别使用vision foundation model特征检测外观、optical flow检测运动，以及monocular depth检测几何，这些分类器在假视频检测中表现出色，表明AI生成视频易于识别。利用Grad-CAM技术，分析了AI视频在外观、运动和几何方面的系统性失败。最终，提出一个Ensemble-of-Experts模型，整合上述信息，提升假视频检测的鲁棒性和泛化能力，能高精度检测Sora视频，即使训练时未接触相关样本，证明差距在不同生成模型间具有通用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19568v1",
      "published_date": "2024-06-27 23:03:58 UTC",
      "updated_date": "2024-06-27 23:03:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:09:57.089213"
    },
    {
      "arxiv_id": "2406.19561v1",
      "title": "Meta-Gradient Search Control: A Method for Improving the Efficiency of Dyna-style Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Bradley Burega",
        "John D. Martin",
        "Luke Kapeluck",
        "Michael Bowling"
      ],
      "abstract": "We study how a Reinforcement Learning (RL) system can remain sample-efficient\nwhen learning from an imperfect model of the environment. This is particularly\nchallenging when the learning system is resource-constrained and in continual\nsettings, where the environment dynamics change. To address these challenges,\nour paper introduces an online, meta-gradient algorithm that tunes a\nprobability with which states are queried during Dyna-style planning. Our study\ncompares the aggregate, empirical performance of this meta-gradient method to\nbaselines that employ conventional sampling strategies. Results indicate that\nour method improves efficiency of the planning process, which, as a\nconsequence, improves the sample-efficiency of the overall learning process. On\nthe whole, we observe that our meta-learned solutions avoid several pathologies\nof conventional planning approaches, such as sampling inaccurate transitions\nand those that stall credit assignment. We believe these findings could prove\nuseful, in future work, for designing model-based RL systems at scale.",
      "tldr_zh": "这篇论文研究了强化学习(RL)系统在从不完美环境模型中学习时，如何维持样本效率，特别是面对资源限制和动态环境变化的挑战。作者提出了一种在线meta-gradient算法，用于调整Dyna-style规划中状态查询的概率，从而优化采样策略。实验结果显示，该方法显著提高了规划效率和整体学习过程的样本效率，避免了传统方法的缺陷，如采样不准确的转移和信用分配问题。这些发现为未来设计大规模模型-based RL系统提供了重要参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19561v1",
      "published_date": "2024-06-27 22:24:46 UTC",
      "updated_date": "2024-06-27 22:24:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:10:08.332204"
    },
    {
      "arxiv_id": "2407.12021v2",
      "title": "Adaptive Draft-Verification for Efficient Large Language Model Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Xukun Liu",
        "Bowen Lei",
        "Ruqi Zhang",
        "Dongkuan Xu"
      ],
      "abstract": "Large language model (LLM) decoding involves generating a sequence of tokens\nbased on a given context, where each token is predicted one at a time using the\nmodel's learned probabilities. The typical autoregressive decoding method\nrequires a separate forward pass through the model for each token generated,\nwhich is computationally inefficient and poses challenges for deploying LLMs in\nlatency-sensitive scenarios. The main limitations of current decoding methods\nstem from their inefficiencies and resource demands. Existing approaches either\nnecessitate fine-tuning smaller models, which is resource-intensive, or rely on\nfixed retrieval schemes to construct drafts for the next tokens, which lack\nadaptability and fail to generalize across different models and contexts. To\naddress these issues, we introduce a novel methodology called ADED, which\naccelerates LLM decoding without requiring fine-tuning. Our approach involves\nan adaptive draft-verification process that evolves over time to improve\nefficiency. We utilize a tri-gram matrix-based LLM representation to\ndynamically approximate the output distribution of the LLM, allowing the model\nto adjust to changing token probabilities during the decoding process.\nAdditionally, we implement a draft construction mechanism that effectively\nbalances exploration and exploitation, ensuring that the drafts generated are\nboth diverse and close to the true output distribution of the LLM. The\nimportance of this design lies in its ability to optimize the draft\ndistribution adaptively, leading to faster and more accurate decoding. Through\nextensive experiments on various benchmark datasets and LLM architectures, we\ndemonstrate that ADED significantly accelerates the decoding process while\nmaintaining high accuracy, making it suitable for deployment in a wide range of\npractical applications.",
      "tldr_zh": "本研究针对大型语言模型(LLM)的 autoregressive 解码方法存在的计算效率低下问题，提出了一种无需 fine-tuning 的新方法 ADED，以加速解码过程。ADED 通过自适应的 draft-verification 机制，利用 tri-gram matrix-based LLM representation 动态近似输出分布，并结合 draft construction 机制平衡 exploration 和 exploitation，确保生成的高效性和准确性。该方法在各种基准数据集和 LLM 架构上的实验表明，ADED 显著提高了解码速度，同时保持高准确性，适用于延迟敏感的实际应用场景。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review of Neurips 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12021v2",
      "published_date": "2024-06-27 22:20:39 UTC",
      "updated_date": "2024-08-19 15:28:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:10:19.695273"
    },
    {
      "arxiv_id": "2406.19552v1",
      "title": "Rethinking harmless refusals when fine-tuning foundation models",
      "title_zh": "重新审视微调基础模型时的无害拒绝",
      "authors": [
        "Florin Pop",
        "Judd Rosenblatt",
        "Diogo Schwerz de Lucena",
        "Michael Vaiana"
      ],
      "abstract": "In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.",
      "tldr_zh": "这篇论文重新审视了在微调Large Language Models (LLMs)时，微调是否真正缓解了不良行为，还是仅将其掩盖。研究通过半真实角色扮演练习和Chain-of-Thought (CoT)推理分析，发现了reason-based deception现象，即模型要么停止产生推理痕迹，要么输出看似道德的推理但最终结果不道德。实验比较了响应策略，结果显示明确反驳比礼貌拒绝更有效，能显著减少不良输出的延续并几乎消除reason-based deception。论文的主要贡献是定义并研究这一新隐藏行为类型，并建议在微调方法中采用更稳健的响应策略，以提升模型的安全性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2024 AGI Workshop Poster",
      "pdf_url": "http://arxiv.org/pdf/2406.19552v1",
      "published_date": "2024-06-27 22:08:22 UTC",
      "updated_date": "2024-06-27 22:08:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:10:32.975852"
    },
    {
      "arxiv_id": "2407.02524v1",
      "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Chris Cummins",
        "Volker Seeker",
        "Dejan Grubisic",
        "Baptiste Roziere",
        "Jonas Gehring",
        "Gabriel Synnaeve",
        "Hugh Leather"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na variety of software engineering and coding tasks. However, their application\nin the domain of code and compiler optimization remains underexplored. Training\nLLMs is resource-intensive, requiring substantial GPU hours and extensive data\ncollection, which can be prohibitive. To address this gap, we introduce Meta\nLarge Language Model Compiler (LLM Compiler), a suite of robust, openly\navailable, pre-trained models specifically designed for code optimization\ntasks. Built on the foundation of Code Llama, LLM Compiler enhances the\nunderstanding of compiler intermediate representations (IRs), assembly\nlanguage, and optimization techniques. The model has been trained on a vast\ncorpus of 546 billion tokens of LLVM-IR and assembly code and has undergone\ninstruction fine-tuning to interpret compiler behavior. LLM Compiler is\nreleased under a bespoke commercial license to allow wide reuse and is\navailable in two sizes: 7 billion and 13 billion parameters. We also present\nfine-tuned versions of the model, demonstrating its enhanced capabilities in\noptimizing code size and disassembling from x86_64 and ARM assembly back into\nLLVM-IR. These achieve 77% of the optimising potential of an autotuning search,\nand 45% disassembly round trip (14% exact match). This release aims to provide\na scalable, cost-effective foundation for further research and development in\ncompiler optimization by both academic researchers and industry practitioners.",
      "tldr_zh": "本研究引入了Meta Large Language Model Compiler（LLM Compiler），这是一套基于Code Llama的预训练模型，旨在填补大型语言模型（LLMs）在代码和编译器优化领域的应用空白。模型通过训练于546亿tokens的LLVM-IR和汇编代码数据，并进行指令微调，增强了对编译器中间表示（IRs）、汇编语言和优化技术的理解。实验结果显示，微调版本在代码大小优化中达到了77%的自动调优潜力，并在x86_64和ARM汇编的反汇编任务中实现了45%的往返准确率（14%精确匹配）。这项开源发布为学术和工业界提供了可扩展、成本有效的工具，支持进一步的编译器优化研究。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.02524v1",
      "published_date": "2024-06-27 21:47:48 UTC",
      "updated_date": "2024-06-27 21:47:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:10:44.069059"
    },
    {
      "arxiv_id": "2406.19545v1",
      "title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Ritam Dutt",
        "Zhen Wu",
        "Kelly Shi",
        "Divyanshu Sheth",
        "Prakhar Gupta",
        "Carolyn Penstein Rose"
      ],
      "abstract": "We present a generalizable classification approach that leverages Large\nLanguage Models (LLMs) to facilitate the detection of implicitly encoded social\nmeaning in conversations. We design a multi-faceted prompt to extract a textual\nexplanation of the reasoning that connects visible cues to underlying social\nmeanings. These extracted explanations or rationales serve as augmentations to\nthe conversational text to facilitate dialogue understanding and transfer. Our\nempirical results over 2,340 experimental settings demonstrate the significant\npositive impact of adding these rationales. Our findings hold true for\nin-domain classification, zero-shot, and few-shot domain transfer for two\ndifferent social meaning detection tasks, each spanning two different corpora.",
      "tldr_zh": "本文提出了一种可泛化的分类方法，利用大型语言模型 (LLMs) 生成文本解释（rationales），以帮助检测对话中隐含的社会含义。这些解释通过多方面提示提取，并作为对话文本的增强，支持对话理解和领域转移。在2,340个实验设置中，结果显示添加rationales显著提升了性能，尤其适用于领域内分类、零样本 (zero-shot) 和少样本 (few-shot) 转移场景，涵盖两个社会含义检测任务，每个任务涉及两个语料库。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear at The Proceedings of the Association for Computational\n  Linguistics, 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.19545v1",
      "published_date": "2024-06-27 21:47:42 UTC",
      "updated_date": "2024-06-27 21:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:10:56.735747"
    },
    {
      "arxiv_id": "2407.12813v2",
      "title": "Data Generation Using Large Language Models for Text Classification: An Empirical Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Yinheng Li",
        "Rogerio Bonatti",
        "Sara Abdali",
        "Justin Wagle",
        "Kazuhito Koishida"
      ],
      "abstract": "Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.",
      "tldr_zh": "该论文探讨了使用 Large Language Models (LLMs) 生成合成数据以训练文本分类模型的可行性，并通过实证分析评估了影响因素，如提示设计、任务复杂度以及生成数据的质量、数量和多样性。研究专注于文本分类任务，通过训练 Natural Language Understanding (NLU) 模型来比较不同生成方法的合成数据质量。结果显示，这些因素显著影响数据有效性，并提供了优化数据生成实践的推荐，以提升模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by DMLR @ ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.12813v2",
      "published_date": "2024-06-27 21:41:43 UTC",
      "updated_date": "2024-07-19 20:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:11:07.634270"
    },
    {
      "arxiv_id": "2406.19537v1",
      "title": "Handling Ontology Gaps in Semantic Parsing",
      "title_zh": "处理本体间隙的语义解析",
      "authors": [
        "Andrea Bacciu",
        "Marco Damonte",
        "Marco Basaldella",
        "Emilio Monti"
      ],
      "abstract": "The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.",
      "tldr_zh": "大多数 Neural Semantic Parsing (NSP) 模型基于封闭世界假设，假设所有概念都在其目标符号中，从而导致幻觉输出，可能产生错误或冒犯性响应。为解决这一问题，本文提出 Hallucination Simulation Framework (HSF)，一个通用框架，用于模拟和分析 NSP 模型的幻觉，并应用于 KQA Pro 数据集评估现有检测技术。作者引入了一种新策略，通过利用 NSP 模型的计算图来检测本体间隙（ontology gaps）、域外语句和错误，提高 F1-Score 分别约 21%、24% 和 1%。这项工作是封闭本体 NSP 领域首次处理本体间隙问题，并公开了代码和检查点。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19537v1",
      "published_date": "2024-06-27 21:21:22 UTC",
      "updated_date": "2024-06-27 21:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:11:21.070615"
    },
    {
      "arxiv_id": "2406.19528v3",
      "title": "Harnessing LLMs for Automated Video Content Analysis: An Exploratory Workflow of Short Videos on Depression",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaying Lizzy Liu",
        "Yunlong Wang",
        "Yao Lyu",
        "Yiheng Su",
        "Shuo Niu",
        "Xuhai Orson Xu",
        "Yan Zhang"
      ],
      "abstract": "Despite the growing interest in leveraging Large Language Models (LLMs) for\ncontent analysis, current studies have primarily focused on text-based content.\nIn the present work, we explored the potential of LLMs in assisting video\ncontent analysis by conducting a case study that followed a new workflow of\nLLM-assisted multimodal content analysis. The workflow encompasses codebook\ndesign, prompt engineering, LLM processing, and human evaluation. We\nstrategically crafted annotation prompts to get LLM Annotations in structured\nform and explanation prompts to generate LLM Explanations for a better\nunderstanding of LLM reasoning and transparency. To test LLM's video annotation\ncapabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos\nabout depression. We compared the LLM Annotations with those of two human\ncoders and found that LLM has higher accuracy in object and activity\nAnnotations than emotion and genre Annotations. Moreover, we identified the\npotential and limitations of LLM's capabilities in annotating videos. Based on\nthe findings, we explore opportunities and challenges for future research and\nimprovements to the workflow. We also discuss ethical concerns surrounding\nfuture studies based on LLM-assisted video analysis.",
      "tldr_zh": "本文提出了一种利用大型语言模型 (LLMs) 进行视频内容分析的新型工作流程，针对抑郁症相关 YouTube 短视频的关键帧进行探索，包括代码书设计、提示工程、LLM 处理和人类评估。研究分析了 203 个关键帧，发现 LLMs 在对象和活动注解上准确性高于人类编码者，但在情感和类型注解上表现较差。基于这些发现，论文讨论了 LLMs 在视频注解的潜力与局限性、未来研究的机会与挑战，以及相关的伦理问题。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "7 pages, 2 figures, accepted by CSCW 24",
      "pdf_url": "http://arxiv.org/pdf/2406.19528v3",
      "published_date": "2024-06-27 21:03:56 UTC",
      "updated_date": "2024-07-29 22:12:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:11:33.593898"
    },
    {
      "arxiv_id": "2406.19512v1",
      "title": "Captioning Visualizations with Large Language Models (CVLLM): A Tutorial",
      "title_zh": "翻译失败",
      "authors": [
        "Giuseppe Carenini",
        "Jordon Johnson",
        "Ali Salamatian"
      ],
      "abstract": "Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.",
      "tldr_zh": "这篇教程介绍了使用大型语言模型 (LLMs) 来自动为可视化添加标题（CVLLM）的技术，回顾了信息可视化 (InfoVis) 原则和过去的相关工作。教程详细解释了神经模型和 Transformer 架构在通用 LLMs 中的应用，并讨论了这些模型在 InfoVis 领域的最新进展，特别是针对标题生成的潜力。最终，它探索了该领域的未来方向，如进一步优化和创新应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19512v1",
      "published_date": "2024-06-27 20:18:18 UTC",
      "updated_date": "2024-06-27 20:18:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:11:45.148167"
    },
    {
      "arxiv_id": "2407.17474v1",
      "title": "\"My Kind of Woman\": Analysing Gender Stereotypes in AI through The Averageness Theory and EU Law",
      "title_zh": "翻译失败",
      "authors": [
        "Miriam Doh",
        "Anastasia Karagianni"
      ],
      "abstract": "This study delves into gender classification systems, shedding light on the\ninteraction between social stereotypes and algorithmic determinations. Drawing\non the \"averageness theory,\" which suggests a relationship between a face's\nattractiveness and the human ability to ascertain its gender, we explore the\npotential propagation of human bias into artificial intelligence (AI) systems.\nUtilising the AI model Stable Diffusion 2.1, we have created a dataset\ncontaining various connotations of attractiveness to test whether the\ncorrelation between attractiveness and accuracy in gender classification\nobserved in human cognition persists within AI. Our findings indicate that akin\nto human dynamics, AI systems exhibit variations in gender classification\naccuracy based on attractiveness, mirroring social prejudices and stereotypes\nin their algorithmic decisions. This discovery underscores the critical need to\nconsider the impacts of human perceptions on data collection and highlights the\nnecessity for a multidisciplinary and intersectional approach to AI development\nand AI data training. By incorporating cognitive psychology and feminist legal\ntheory, we examine how data used for AI training can foster gender diversity\nand fairness under the scope of the AI Act and GDPR, reaffirming how\npsychological and feminist legal theories can offer valuable insights for\nensuring the protection of gender equality and non-discrimination in AI\nsystems.",
      "tldr_zh": "本研究探讨了AI系统中性别分类的偏见问题，通过“averageness theory”（认为面部吸引力影响人类性别判断）分析社会刻板印象如何渗透到算法决策中。研究者利用Stable Diffusion 2.1模型创建数据集，测试AI在性别分类中的准确率是否随吸引力变化，结果显示AI系统像人类一样，受吸引力影响而出现准确率差异，反映了社会偏见。最终，该研究结合认知心理学和女权主义法律理论，强调在AI开发中采用多学科方法，确保数据训练符合AI Act和GDPR，促进性别平等和非歧视。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "presented at IAIL 2024 the Imagining the AI Landscape After the AI\n  ACT, in conjunction with HHAI2024, Malm\\\"o, Sweden, June 10, 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17474v1",
      "published_date": "2024-06-27 20:03:27 UTC",
      "updated_date": "2024-06-27 20:03:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:11:57.519300"
    },
    {
      "arxiv_id": "2406.19507v2",
      "title": "Too Good to be True? Turn Any Model Differentially Private With DP-Weights",
      "title_zh": "翻译失败",
      "authors": [
        "David Zagardo"
      ],
      "abstract": "Imagine training a machine learning model with Differentially Private\nStochastic Gradient Descent (DP-SGD), only to discover post-training that the\nnoise level was either too high, crippling your model's utility, or too low,\ncompromising privacy. The dreaded realization hits: you must start the lengthy\ntraining process from scratch. But what if you could avoid this retraining\nnightmare? In this study, we introduce a groundbreaking approach (to our\nknowledge) that applies differential privacy noise to the model's weights after\ntraining. We offer a comprehensive mathematical proof for this novel approach's\nprivacy bounds, use formal methods to validate its privacy guarantees, and\nempirically evaluate its effectiveness using membership inference attacks and\nperformance evaluations. This method allows for a single training run, followed\nby post-hoc noise adjustments to achieve optimal privacy-utility trade-offs. We\ncompare this novel fine-tuned model (DP-Weights model) to a traditional DP-SGD\nmodel, demonstrating that our approach yields statistically similar performance\nand privacy guarantees. Our results validate the efficacy of post-training\nnoise application, promising significant time savings and flexibility in\nfine-tuning differential privacy parameters, making it a practical alternative\nfor deploying differentially private models in real-world scenarios.",
      "tldr_zh": "本研究提出了一种创新方法 DP-Weights，用于在模型训练后直接向权重添加差分隐私（Differentially Private）噪声，从而避免传统差分隐私随机梯度下降（DP-SGD）因噪声级别不当而需重新训练的问题。该方法提供了全面的数学证明、正式验证以及通过成员推理攻击（membership inference attacks）和性能评估的实证测试，实现了单次训练后灵活调整隐私-效用权衡。与传统 DP-SGD 模型相比，DP-Weights 模型在性能和隐私保证上表现出统计相似性，同时显著节省时间和提升部署灵活性。该方法为实际场景中部署差分隐私模型提供了高效的替代方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "The results are genuine, but the math is wrong! Please do not use\n  this method for your Differential Privacy implementations",
      "pdf_url": "http://arxiv.org/pdf/2406.19507v2",
      "published_date": "2024-06-27 19:58:11 UTC",
      "updated_date": "2025-01-20 16:23:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:12:10.232134"
    },
    {
      "arxiv_id": "2406.19502v2",
      "title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization",
      "title_zh": "层次化解构 LLM 推理：一个基于图的框架用于分析知识利用",
      "authors": [
        "Miyoung Ko",
        "Sue Hyun Park",
        "Joonsuk Park",
        "Minjoon Seo"
      ],
      "abstract": "Despite the advances in large language models (LLMs), how they use their\nknowledge for reasoning is not yet well understood. In this study, we propose a\nmethod that deconstructs complex real-world questions into a graph,\nrepresenting each question as a node with predecessors of background knowledge\nneeded to solve the question. We develop the DepthQA dataset, deconstructing\nquestions into three depths: (i) recalling conceptual knowledge, (ii) applying\nprocedural knowledge, and (iii) analyzing strategic knowledge. Based on a\nhierarchical graph, we quantify forward discrepancy, a discrepancy in LLM\nperformance on simpler sub-problems versus complex questions. We also measure\nbackward discrepancy where LLMs answer complex questions but struggle with\nsimpler ones. Our analysis shows that smaller models exhibit more discrepancies\nthan larger models. Distinct patterns of discrepancies are observed across\nmodel capacity and possibility of training data memorization. Additionally,\nguiding models from simpler to complex questions through multi-turn\ninteractions improves performance across model sizes, highlighting the\nimportance of structured intermediate steps in knowledge reasoning. This work\nenhances our understanding of LLM reasoning and suggests ways to improve their\nproblem-solving abilities.",
      "tldr_zh": "本研究提出了一种基于图的框架，用于分层分解大型语言模型 (LLMs) 的推理过程，将复杂问题表示为节点图，并整合所需的背景知识。该框架结合开发的 DepthQA 数据集，将问题分解为三个深度：回忆概念知识、应用程序知识和分析战略知识，并量化 forward discrepancy（简单子问题 vs. 复杂问题的性能差异）和 backward discrepancy（复杂问题正确但简单问题错误）。分析结果显示，较小模型表现出更多差异，而通过多轮交互从简单到复杂的问题指导，可显著提升模型性能，从而深化对 LLM 推理的理解并提供改进问题解决能力的建议。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "published at EMNLP 2024; code is available at\n  https://github.com/kaistAI/knowledge-reasoning",
      "pdf_url": "http://arxiv.org/pdf/2406.19502v2",
      "published_date": "2024-06-27 19:29:36 UTC",
      "updated_date": "2024-10-03 20:55:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:12:21.857059"
    },
    {
      "arxiv_id": "2406.19500v1",
      "title": "Knowledge acquisition for dialogue agents using reinforcement learning on graph representations",
      "title_zh": "翻译失败",
      "authors": [
        "Selene Baez Santamaria",
        "Shihan Wang",
        "Piek Vossen"
      ],
      "abstract": "We develop an artificial agent motivated to augment its knowledge base beyond\nits initial training. The agent actively participates in dialogues with other\nagents, strategically acquiring new information. The agent models its knowledge\nas an RDF knowledge graph, integrating new beliefs acquired through\nconversation. Responses in dialogue are generated by identifying graph patterns\naround these new integrated beliefs. We show that policies can be learned using\nreinforcement learning to select effective graph patterns during an\ninteraction, without relying on explicit user feedback. Within this context,\nour study is a proof of concept for leveraging users as effective sources of\ninformation.",
      "tldr_zh": "该研究开发了一种对话代理，使用强化学习（reinforcement learning）在图表示（graph representations）上学习，以主动扩展其知识库。代理将知识建模为 RDF 知识 graph，并通过对话整合新信念，同时通过识别图模式生成响应。实验证明，该方法允许代理在不依赖显式用户反馈的情况下，学习有效的互动策略，并展示了用户作为信息来源的有效性，作为一个 proof of concept。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19500v1",
      "published_date": "2024-06-27 19:28:42 UTC",
      "updated_date": "2024-06-27 19:28:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:12:32.713578"
    },
    {
      "arxiv_id": "2406.19497v1",
      "title": "Inclusivity in Large Language Models: Personality Traits and Gender Bias in Scientific Abstracts",
      "title_zh": "大型语言模型中的包容性：科学摘要中的个性特征和性别偏见",
      "authors": [
        "Naseela Pervez",
        "Alexander J. Titus"
      ],
      "abstract": "Large language models (LLMs) are increasingly utilized to assist in\nscientific and academic writing, helping authors enhance the coherence of their\narticles. Previous studies have highlighted stereotypes and biases present in\nLLM outputs, emphasizing the need to evaluate these models for their alignment\nwith human narrative styles and potential gender biases. In this study, we\nassess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,\nand Gemini 1.5 Flash - by analyzing their performance on benchmark\ntext-generation tasks for scientific abstracts. We employ the Linguistic\nInquiry and Word Count (LIWC) framework to extract lexical, psychological, and\nsocial features from the generated texts. Our findings indicate that, while\nthese models generally produce text closely resembling human authored content,\nvariations in stylistic features suggest significant gender biases. This\nresearch highlights the importance of developing LLMs that maintain a diversity\nof writing styles to promote inclusivity in academic discourse.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在生成科学摘要时的包容性问题，特别关注Claude 3 Opus、Mistral AI Large和Gemini 1.5 Flash模型中存在的个性特征和性别偏见。研究采用Linguistic Inquiry and Word Count (LIWC)框架分析这些模型生成的文本，提取词汇、心理和社会特征，以评估其与人类叙述风格的契合度。结果显示，虽然LLMs的输出通常类似于人类作者的内容，但存在显著的风格差异和性别偏见。最终，该研究强调开发更具多样写作风格的LLMs，以提升学术话语的包容性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19497v1",
      "published_date": "2024-06-27 19:26:11 UTC",
      "updated_date": "2024-06-27 19:26:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:12:47.226030"
    },
    {
      "arxiv_id": "2406.19493v2",
      "title": "Development and Evaluation of a Retrieval-Augmented Generation Tool for Creating SAPPhIRE Models of Artificial Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Anubhab Majumder",
        "Kausik Bhattacharya",
        "Amaresh Chakrabarti"
      ],
      "abstract": "Representing systems using the SAPPhIRE causality model is found useful in\nsupporting design-by-analogy. However, creating a SAPPhIRE model of artificial\nor biological systems is an effort-intensive process that requires human\nexperts to source technical knowledge from multiple technical documents\nregarding how the system works. This research investigates how to leverage\nLarge Language Models (LLMs) in creating structured descriptions of systems\nusing the SAPPhIRE model of causality. This paper, the second part of the\ntwo-part research, presents a new Retrieval-Augmented Generation (RAG) tool for\ngenerating information related to SAPPhIRE constructs of artificial systems and\nreports the results from a preliminary evaluation of the tool's success -\nfocusing on the factual accuracy and reliability of outcomes.",
      "tldr_zh": "这篇论文开发了一种Retrieval-Augmented Generation (RAG) 工具，用于创建人工系统的SAPPhIRE 因果模型，以支持设计-by-analogy过程，从而减少专家手动从多文档中提取知识的努力。工具利用Large Language Models (LLMs)结合检索增强生成技术，从技术文档中自动生成结构化的SAPPhIRE描述。初步评估结果显示，该工具在事实准确性和可靠性方面表现出色，为高效系统建模提供了可行解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper has been accepted for presentation at the 10th\n  International Conference on Research Into Design, 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.19493v2",
      "published_date": "2024-06-27 19:20:09 UTC",
      "updated_date": "2024-10-27 11:28:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:12:58.048333"
    },
    {
      "arxiv_id": "2406.19486v1",
      "title": "LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shouchang Guo",
        "Sonam Damani",
        "Keng-hao Chang"
      ],
      "abstract": "In prompt tuning, a prefix or suffix text is added to the prompt, and the\nembeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix\nare optimized to gain more control over language models for specific tasks.\nThis approach eliminates the need for hand-crafted prompt engineering or\nexplicit model fine-tuning. Prompt tuning is significantly more\nparameter-efficient than model fine-tuning, as it involves optimizing partial\ninputs of language models to produce desired outputs.\n  In this work, we aim to further reduce the amount of trainable parameters\nrequired for a language model to perform well on specific tasks. We propose\nLow-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves\nefficient prompt optimization. The proposed method demonstrates similar\noutcomes to full parameter prompt tuning while reducing the number of trainable\nparameters by a factor of 5. It also provides promising results compared to the\nstate-of-the-art methods that would require 10 to 20 times more parameters.",
      "tldr_zh": "提示调优（prompt tuning）是一种参数高效的方法，通过优化提示的前缀或后缀（如嵌入或令牌索引）来控制语言模型（language models）的输出，而无需手动提示工程或模型微调，从而显著减少可训练参数。\n\n本文提出Low-Rank Prompt Tuning (LoPT)，一种基于低秩模型的提示优化技术，能够将可训练参数减少5倍，同时实现与全参数提示调优相似的性能表现。\n\n此外，LoPT 相比现有最先进方法表现出色，后者往往需要10到20倍的参数量，这为高效的语言模型任务适应提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19486v1",
      "published_date": "2024-06-27 19:02:41 UTC",
      "updated_date": "2024-06-27 19:02:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:13:09.712884"
    },
    {
      "arxiv_id": "2406.19478v1",
      "title": "Sparse Regression for Machine Translation",
      "title_zh": "用于机器翻译的稀疏回归",
      "authors": [
        "Ergun Biçici"
      ],
      "abstract": "We use transductive regression techniques to learn mappings between source\nand target features of given parallel corpora and use these mappings to\ngenerate machine translation outputs. We show the effectiveness of $L_1$\nregularized regression (\\textit{lasso}) to learn the mappings between sparsely\nobserved feature sets versus $L_2$ regularized regression. Proper selection of\ntraining instances plays an important role to learn correct feature mappings\nwithin limited computational resources and at expected accuracy levels. We\nintroduce \\textit{dice} instance selection method for proper selection of\ntraining instances, which plays an important role to learn correct feature\nmappings for improving the source and target coverage of the training set. We\nshow that $L_1$ regularized regression performs better than $L_2$ regularized\nregression both in regression measurements and in the translation experiments\nusing graph decoding. We present encouraging results when translating from\nGerman to English and Spanish to English. We also demonstrate results when the\nphrase table of a phrase-based decoder is replaced with the mappings we find\nwith the regression model.",
      "tldr_zh": "这篇论文探讨了使用稀疏回归技术来学习平行语料中源语言和目标语言特征的映射，从而生成机器翻译输出。作者比较了 L1 正则化回归 (lasso) 与 L2 正则化回归的效果，发现 L1 方法在稀疏特征集上表现更优，能更好地处理特征映射。论文引入了 dice 实例选择方法，以优化训练实例的选择，提高训练集的源和目标覆盖率，从而在有限计算资源下提升准确性。在德语到英语和西班牙语到英语的翻译实验中，L1 正则化回归不仅在回归测量中胜出，还能替换短语-based 解码器的短语表，提供令人鼓舞的翻译结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "G.3; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 4 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.19478v1",
      "published_date": "2024-06-27 18:43:51 UTC",
      "updated_date": "2024-06-27 18:43:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:13:22.685879"
    },
    {
      "arxiv_id": "2406.19464v2",
      "title": "ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyi Liu",
        "Cheng Chi",
        "Eric Cousineau",
        "Naveen Kuppuswamy",
        "Benjamin Burchfiel",
        "Shuran Song"
      ],
      "abstract": "Audio signals provide rich information for the robot interaction and object\nproperties through contact. This information can surprisingly ease the learning\nof contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments by learning from diverse in-the-wild human\ndemonstrations.",
      "tldr_zh": "本研究提出ManiWAV系统，利用in-the-wild音频-视觉数据来提升机器人操作技能的学习，特别是针对接触丰富的任务。\n系统包括一个“耳-in-hand”设备，用于收集同步音频和视觉的人类演示，以及一个策略接口，直接从这些演示中学习机器人操作策略。\n实验通过四个任务展示，系统能被动或主动感知接触事件、模式、物体表面材料和状态，并从多样化的in-the-wild演示中实现对未见环境的泛化。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.RO",
      "comment": "Conference on Robot Learning (CoRL) 2024; Project website:\n  https://maniwav.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2406.19464v2",
      "published_date": "2024-06-27 18:06:38 UTC",
      "updated_date": "2024-11-04 02:21:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:13:35.811664"
    },
    {
      "arxiv_id": "2406.19434v1",
      "title": "Lightweight Predictive 3D Gaussian Splats",
      "title_zh": "轻量级预测3D高斯喷溅",
      "authors": [
        "Junli Cao",
        "Vidit Goel",
        "Chaoyang Wang",
        "Anil Kag",
        "Ju Hu",
        "Sergei Korolev",
        "Chenfanfu Jiang",
        "Sergey Tulyakov",
        "Jian Ren"
      ],
      "abstract": "Recent approaches representing 3D objects and scenes using Gaussian splats\nshow increased rendering speed across a variety of platforms and devices. While\nrendering such representations is indeed extremely efficient, storing and\ntransmitting them is often prohibitively expensive. To represent large-scale\nscenes, one often needs to store millions of 3D Gaussians, occupying gigabytes\nof disk space. This poses a very practical limitation, prohibiting widespread\nadoption.Several solutions have been proposed to strike a balance between disk\nsize and rendering quality, noticeably reducing the visual quality. In this\nwork, we propose a new representation that dramatically reduces the hard drive\nfootprint while featuring similar or improved quality when compared to the\nstandard 3D Gaussian splats. When compared to other compact solutions, ours\noffers higher quality renderings with significantly reduced storage, being able\nto efficiently run on a mobile device in real-time. Our key observation is that\nnearby points in the scene can share similar representations. Hence, only a\nsmall ratio of 3D points needs to be stored. We introduce an approach to\nidentify such points which are called parent points. The discarded points\ncalled children points along with attributes can be efficiently predicted by\ntiny MLPs.",
      "tldr_zh": "本研究针对 3D Gaussian Splats 表示方法的高渲染速度优势，解决了其存储和传输占用大量空间（数百万高斯点可能达数 GB）的实际问题。作者提出了一种轻量级预测表示框架，通过识别场景中相似的附近点（称为 parent points），仅存储少量关键点，而丢弃的子点（children points）及其属性则由小型 MLP 模型高效预测。该方法显著减少了硬盘占用，同时在渲染质量上与标准 3D Gaussian Splats 相当或更优，并在移动设备上实现实时运行。实验结果显示，与其他紧凑解决方案相比，本框架提供更高质量的渲染和更低的存储需求。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Project Page: https://plumpuddings.github.io/LPGS//",
      "pdf_url": "http://arxiv.org/pdf/2406.19434v1",
      "published_date": "2024-06-27 17:59:05 UTC",
      "updated_date": "2024-06-27 17:59:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:13:47.878092"
    },
    {
      "arxiv_id": "2407.12020v2",
      "title": "SignSpeak: Open-Source Time Series Classification for ASL Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Aditya Makkar",
        "Divya Makkar",
        "Aarav Patel",
        "Liam Hebert"
      ],
      "abstract": "The lack of fluency in sign language remains a barrier to seamless\ncommunication for hearing and speech-impaired communities. In this work, we\npropose a low-cost, real-time ASL-to-speech translation glove and an exhaustive\ntraining dataset of sign language patterns. We then benchmarked this dataset\nwith supervised learning models, such as LSTMs, GRUs and Transformers, where\nour best model achieved 92% accuracy. The SignSpeak dataset has 7200 samples\nencompassing 36 classes (A-Z, 1-10) and aims to capture realistic signing\npatterns by using five low-cost flex sensors to measure finger positions at\neach time step at 36 Hz. Our open-source dataset, models and glove designs,\nprovide an accurate and efficient ASL translator while maintaining\ncost-effectiveness, establishing a framework for future work to build on.",
      "tldr_zh": "本文提出SignSpeak，一个低成本、实时的ASL-to-speech翻译手套，并发布了一个开源数据集，包含7200个样本和36个类（A-Z、1-10），使用五个低成本弯曲传感器以36 Hz频率捕获手指位置的时间序列数据。研究通过监督学习模型如LSTMs、GRUs和Transformers对数据集进行基准测试，最佳模型达到92%的准确率。该开源框架提供了一个准确、高效的ASL翻译解决方案，并为未来的手语翻译研究奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures, NeurIPS",
      "pdf_url": "http://arxiv.org/pdf/2407.12020v2",
      "published_date": "2024-06-27 17:58:54 UTC",
      "updated_date": "2024-07-18 20:36:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:13:59.540923"
    },
    {
      "arxiv_id": "2406.19384v1",
      "title": "The Remarkable Robustness of LLMs: Stages of Inference?",
      "title_zh": "翻译失败",
      "authors": [
        "Vedang Lad",
        "Wes Gurnee",
        "Max Tegmark"
      ],
      "abstract": "We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.",
      "tldr_zh": "本研究展示了大型语言模型 (LLMs) 的显著鲁棒性，通过删除和交换相邻层实验，发现这些干预能保留72-95%的原始预测准确率，且层数较多的模型表现出更高的鲁棒性。基于层级干预和进一步验证，该论文假设LLMs存在四个通用推理阶段：detokenization（将原始标记转化为高级上下文表示）、feature engineering（迭代精炼任务和实体特定特征）、prediction ensembling（与词汇空间对齐的相变阶段），以及residual sharpening（消除噪声以锐化预测分布）。这些发现为理解LLMs的内部机制提供了新洞见，并在八个不同模型上得到证实。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19384v1",
      "published_date": "2024-06-27 17:57:03 UTC",
      "updated_date": "2024-06-27 17:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:14:11.785817"
    },
    {
      "arxiv_id": "2406.19370v4",
      "title": "Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space",
      "title_zh": "隐藏能力的涌现：探索概念空间中的学习动态",
      "authors": [
        "Core Francisco Park",
        "Maya Okawa",
        "Andrew Lee",
        "Hidenori Tanaka",
        "Ekdeep Singh Lubana"
      ],
      "abstract": "Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.",
      "tldr_zh": "本研究探讨了生成模型学习抽象概念的动态，提出 concept space 框架，其中每个轴代表数据生成过程中的独立概念，以分析模型学习顺序和能力。结果显示，概念学习的速度和顺序主要由数据属性（concept signal）控制，并在学习动态中出现突然转向，这些点精确对应 hidden capabilities 的出现，即模型虽能通过潜在干预操纵概念，但无法通过简单输入提示引发。尽管基于合成玩具数据集，该研究假设生成模型在训练中普遍会突然发展出这些潜藏能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2406.19370v4",
      "published_date": "2024-06-27 17:50:05 UTC",
      "updated_date": "2024-12-11 07:53:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:14:23.631125"
    },
    {
      "arxiv_id": "2407.00121v1",
      "title": "Granite-Function Calling Model: Introducing Function Calling Abilities via Multi-task Learning of Granular Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Ibrahim Abdelaziz",
        "Kinjal Basu",
        "Mayank Agarwal",
        "Sadhana Kumaravel",
        "Matthew Stallone",
        "Rameswar Panda",
        "Yara Rizk",
        "GP Bhargav",
        "Maxwell Crouse",
        "Chulaka Gunasekara",
        "Shajith Ikbal",
        "Sachin Joshi",
        "Hima Karanam",
        "Vineet Kumar",
        "Asim Munawar",
        "Sumit Neelam",
        "Dinesh Raghu",
        "Udit Sharma",
        "Adriana Meza Soria",
        "Dheeraj Sreedhar",
        "Praveen Venkateswaran",
        "Merve Unuvar",
        "David Cox",
        "Salim Roukos",
        "Luis Lastras",
        "Pavan Kapanipathi"
      ],
      "abstract": "Large language models (LLMs) have recently shown tremendous promise in\nserving as the backbone to agentic systems, as demonstrated by their\nperformance in multi-faceted, challenging benchmarks like SWE-Bench and\nAgent-Bench. However, to realize the true potential of LLMs as autonomous\nagents, they must learn to identify, call, and interact with external tools and\napplication program interfaces (APIs) to complete complex tasks. These tasks\ntogether are termed function calling. Endowing LLMs with function calling\nabilities leads to a myriad of advantages, such as access to current and\ndomain-specific information in databases and knowledge sources, and the ability\nto outsource tasks that can be reliably performed by tools, e.g., a Python\ninterpreter or calculator. While there has been significant progress in\nfunction calling with LLMs, there is still a dearth of open models that perform\non par with proprietary LLMs like GPT, Claude, and Gemini. Therefore, in this\nwork, we introduce the GRANITE-20B-FUNCTIONCALLING model under an Apache 2.0\nlicense. The model is trained using a multi-task training approach on seven\nfundamental tasks encompassed in function calling, those being Nested Function\nCalling, Function Chaining, Parallel Functions, Function Name Detection,\nParameter-Value Pair Detection, Next-Best Function, and Response Generation. We\npresent a comprehensive evaluation on multiple out-of-domain datasets comparing\nGRANITE-20B-FUNCTIONCALLING to more than 15 other best proprietary and open\nmodels. GRANITE-20B-FUNCTIONCALLING provides the best performance among all\nopen models on the Berkeley Function Calling Leaderboard and fourth overall. As\na result of the diverse tasks and datasets used for training our model, we show\nthat GRANITE-20B-FUNCTIONCALLING has better generalizability on multiple tasks\nin seven different evaluation datasets.",
      "tldr_zh": "该论文介绍了开源模型 GRANITE-20B-FUNCTIONCALLING，通过多任务学习赋予大型语言模型(LLMs)函数调用能力，以实现自主代理系统的潜力。训练方法涉及七个关键任务，包括 Nested Function Calling、Function Chaining、Parallel Functions 等，旨在提升模型识别、调用和交互外部工具的能力。实验结果显示，该模型在多个外部数据集上表现出色，在 Berkeley Function Calling Leaderboard 上位列开源模型第一、总体第四，并展示了更强的泛化性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00121v1",
      "published_date": "2024-06-27 17:47:26 UTC",
      "updated_date": "2024-06-27 17:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:14:35.754975"
    },
    {
      "arxiv_id": "2406.19354v1",
      "title": "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?",
      "title_zh": "模型编辑的根本问题：理性信念修正在LLMs中应如何运作？",
      "authors": [
        "Peter Hase",
        "Thomas Hofweber",
        "Xiang Zhou",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
      ],
      "abstract": "The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision",
      "tldr_zh": "这篇论文探讨了模型编辑（model editing）在大型语言模型（LLMs）中的根本问题，强调其本质上是哲学信念修订（belief revision）的挑战，并批评了现有问题的标准表述。作者提出了12个开放问题，涵盖定义问题、开发基准以及假设LLMs具有可编辑信念等方面，以揭示模型编辑的潜在缺陷。论文引入了一个基于Wikidata的半合成数据集，通过与理想化贝叶斯代理（Bayesian agent）的比较，评估LLMs信念修订的表现，并发现其偏离了理想的认识标准；代码已在GitHub上公开，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19354v1",
      "published_date": "2024-06-27 17:33:03 UTC",
      "updated_date": "2024-06-27 17:33:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:14:46.468796"
    },
    {
      "arxiv_id": "2406.19349v1",
      "title": "IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language",
      "title_zh": "翻译失败",
      "authors": [
        "Lucky Susanto",
        "Musa Izzanardi Wijanarko",
        "Prasetia Anugrah Pratama",
        "Traci Hong",
        "Ika Idris",
        "Alham Fikri Aji",
        "Derry Wijaya"
      ],
      "abstract": "Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.",
      "tldr_zh": "该研究引入了 IndoToxic2024 数据集，这是一个针对印尼语言的仇恨言论和毒性类型数据集，特别丰富了人口统计信息，以解决现有数据在针对弱势群体（如 Shia、LGBTQ 等）时的不足。数据集包含 43,692 条标注条目，由 19 个多样化个体标注，焦点是总统选举期间针对这些群体的在线文本。研究者建立了七个二元分类任务的基线，使用 fine-tuned IndoBERTweet 模型达到了 0.78 的 macro-F1 分数，并展示了整合人口统计信息能提升大语言模型（如 gpt-3.5-turbo）的零样本性能。然而，他们也警告过度依赖这些信息可能因数据碎片化而降低微调模型的性能。总的来说，该数据集为印尼仇恨言论检测提供了重要资源，促进了更公平有效的分类工具发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19349v1",
      "published_date": "2024-06-27 17:26:38 UTC",
      "updated_date": "2024-06-27 17:26:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:15:03.421656"
    },
    {
      "arxiv_id": "2407.09551v1",
      "title": "Diminishing Stereotype Bias in Image Generation Model using Reinforcemenlent Learning Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Chen",
        "Virgile Foussereau"
      ],
      "abstract": "This study addresses gender bias in image generation models using\nReinforcement Learning from Artificial Intelligence Feedback (RLAIF) with a\nnovel Denoising Diffusion Policy Optimization (DDPO) pipeline. By employing a\npretrained stable diffusion model and a highly accurate gender classification\nTransformer, the research introduces two reward functions: Rshift for shifting\ngender imbalances, and Rbalance for achieving and maintaining gender balance.\nExperiments demonstrate the effectiveness of this approach in mitigating bias\nwithout compromising image quality or requiring additional data or prompt\nmodifications. While focusing on gender bias, this work establishes a\nfoundation for addressing various forms of bias in AI systems, emphasizing the\nneed for responsible AI development. Future research directions include\nextending the methodology to other bias types, enhancing the RLAIF pipeline's\nrobustness, and exploring multi-prompt fine-tuning to further advance fairness\nand inclusivity in AI.",
      "tldr_zh": "这篇论文提出了一种使用 Reinforcement Learning from Artificial Intelligence Feedback (RLAIF) 的方法，通过新型 Denoising Diffusion Policy Optimization (DDPO) 管道来减少图像生成模型中的性别偏见。研究引入了两个奖励函数——Rshift 用于调整性别不平衡，以及 Rbalance 用于实现和维持性别平衡，并利用预训练的稳定扩散模型和性别分类 Transformer 进行优化，而无需额外数据或提示修改。实验证明，该方法有效缓解了偏见，同时保持了图像质量，为处理其他 AI 偏见类型奠定了基础，并建议未来扩展到多提示微调等领域以提升 AI 的公平性和包容性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09551v1",
      "published_date": "2024-06-27 17:18:58 UTC",
      "updated_date": "2024-06-27 17:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:15:13.348771"
    },
    {
      "arxiv_id": "2406.19320v1",
      "title": "Efficient World Models with Context-Aware Tokenization",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent Micheli",
        "Eloi Alonso",
        "François Fleuret"
      ],
      "abstract": "Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.",
      "tldr_zh": "这项研究针对深度强化学习（RL）中模型-based 方法的计算挑战，提出了一种高效的世界模型架构 $\\Delta$-IRIS。该架构使用离散自编码器编码时间步之间的随机 delta，并结合自回归 transformer 通过连续 tokens 总结当前世界状态来预测未来 delta，从而实现上下文感知的 tokenization。在 Crafter 基准测试中，$\\Delta$-IRIS 在多个帧预算下设定了新的 state-of-the-art 性能，同时训练速度比之前的注意力-based 方法快一个数量级。研究团队还公开了代码和模型以供进一步使用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.19320v1",
      "published_date": "2024-06-27 16:54:12 UTC",
      "updated_date": "2024-06-27 16:54:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:15:24.873709"
    },
    {
      "arxiv_id": "2406.19317v2",
      "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Parand A. Alamdari",
        "Yanshuai Cao",
        "Kevin H. Wilson"
      ],
      "abstract": "We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.",
      "tldr_zh": "本文研究了将Large Language Models (LLMs) 整合到 Contextual Multi-Armed Bandit 框架中的益处，以加速推荐系统的个性化建议生成。研究提出了一种初始化算法，通过提示 LLMs 生成一个预训练数据集来模拟人类偏好，从而显著减少在线学习的 regret 和数据收集成本。该方法通过两个实验验证：一个使用 LLMs 作为 oracle，另一个基于真实世界的 conjoint survey 数据，证明了其在降低 regret 方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19317v2",
      "published_date": "2024-06-27 16:52:19 UTC",
      "updated_date": "2024-10-29 02:42:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:15:37.420168"
    },
    {
      "arxiv_id": "2407.00120v1",
      "title": "Automated Web-Based Malaria Detection System with Machine Learning and Deep Learning Techniques",
      "title_zh": "翻译失败",
      "authors": [
        "Abraham G Taye",
        "Sador Yemane",
        "Eshetu Negash",
        "Yared Minwuyelet",
        "Moges Abebe",
        "Melkamu Hunegnaw Asmare"
      ],
      "abstract": "Malaria parasites pose a significant global health burden, causing widespread\nsuffering and mortality. Detecting malaria infection accurately is crucial for\neffective treatment and control. However, existing automated detection\ntechniques have shown limitations in terms of accuracy and generalizability.\nMany studies have focused on specific features without exploring more\ncomprehensive approaches. In our case, we formulate a deep learning technique\nfor malaria-infected cell classification using traditional CNNs and transfer\nlearning models notably VGG19, InceptionV3, and Xception. The models were\ntrained using NIH datasets and tested using different performance metrics such\nas accuracy, precision, recall, and F1-score. The test results showed that deep\nCNNs achieved the highest accuracy -- 97%, followed by Xception with an\naccuracy of 95%. A machine learning model SVM achieved an accuracy of 83%,\nwhile an Inception-V3 achieved an accuracy of 94%. Furthermore, the system can\nbe accessed through a web interface, where users can upload blood smear images\nfor malaria detection.",
      "tldr_zh": "这篇论文提出了一种基于机器学习和深度学习技术的自动化网页系统，用于检测疟疾感染细胞，旨在解决现有方法的准确性和泛化性问题。研究团队使用传统CNN以及迁移学习模型如VGG19、InceptionV3和Xception，在NIH datasets上训练模型，并通过准确率、精确率、召回率和F1-score等指标评估。结果显示，深度CNN模型达到了97%的最高准确率，Xception为95%、InceptionV3为94%、而SVM为83%；此外，该系统支持用户通过网页界面上传血涂片图像，实现便捷的疟疾检测。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00120v1",
      "published_date": "2024-06-27 16:50:36 UTC",
      "updated_date": "2024-06-27 16:50:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:15:51.384796"
    },
    {
      "arxiv_id": "2406.19314v2",
      "title": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Colin White",
        "Samuel Dooley",
        "Manley Roberts",
        "Arka Pal",
        "Ben Feuer",
        "Siddhartha Jain",
        "Ravid Shwartz-Ziv",
        "Neel Jain",
        "Khalid Saifullah",
        "Sreemanti Dey",
        "Shubh-Agrawal",
        "Sandeep Singh Sandha",
        "Siddartha Naidu",
        "Chinmay Hegde",
        "Yann LeCun",
        "Tom Goldstein",
        "Willie Neiswanger",
        "Micah Goldblum"
      ],
      "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe resistant to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-limited versions of tasks from previous benchmarks such\nas Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 405B in\nsize. LiveBench is difficult, with top models achieving below 70% accuracy. We\nrelease all questions, code, and model answers. Questions are added and updated\non a monthly basis, and we release new tasks and harder versions of tasks over\ntime so that LiveBench can distinguish between the capabilities of LLMs as they\nimprove in the future. We welcome community engagement and collaboration for\nexpanding the benchmark tasks and models.",
      "tldr_zh": "本文提出 LiveBench，一种针对大型语言模型(LLMs)的挑战性基准测试，旨在解决测试集污染(test set contamination)问题，确保公平评估。LiveBench 通过使用最近信息来源（如数学比赛、arXiv 论文和新闻文章）生成频繁更新的问题，并采用自动评分机制，涵盖数学、编码、推理等多样化任务，包括Big-Bench Hard和IFEval的更难版本。实验结果显示，顶级闭源和开源模型的准确率均低于70%，作者计划每月更新任务并欢迎社区合作，以持续区分模型能力的提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 Spotlight",
      "pdf_url": "http://arxiv.org/pdf/2406.19314v2",
      "published_date": "2024-06-27 16:47:42 UTC",
      "updated_date": "2025-04-18 19:36:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:16:02.762053"
    },
    {
      "arxiv_id": "2406.19292v2",
      "title": "From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zheyang Xiong",
        "Vasilis Papageorgiou",
        "Kangwook Lee",
        "Dimitris Papailiopoulos"
      ],
      "abstract": "Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.",
      "tldr_zh": "本研究发现，大语言模型 (LLMs) 在处理长上下文输入时，信息检索和推理能力存在不足，因此提出了一种利用精心设计的合成数据集（包括数值键值检索任务）进行微调的方法。实验在 GPT-3.5 Turbo 和 Mistral 7B 等模型上显示，微调后显著提升了模型在长上下文下的检索和推理性能，例如在 20 个文档的 MDQA 任务中位置 10 的准确率提升 10.5%。此外，微调后的模型在一般基准测试（如 TriviaQA）上性能几乎不变，甚至减少了幻觉问题，而其他基线数据可能导致 2.33% 到 6.19% 的性能下降。该方法突显了合成数据微调在改善 LLMs 长上下文任务方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19292v2",
      "published_date": "2024-06-27 16:05:13 UTC",
      "updated_date": "2024-10-14 02:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:16:16.380774"
    },
    {
      "arxiv_id": "2407.00119v2",
      "title": "Efficient Long-distance Latent Relation-aware Graph Neural Network for Multi-modal Emotion Recognition in Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuntao Shou",
        "Wei Ai",
        "Jiayi Du",
        "Tao Meng",
        "Haiyan Liu",
        "Nan Yin"
      ],
      "abstract": "The task of multi-modal emotion recognition in conversation (MERC) aims to\nanalyze the genuine emotional state of each utterance based on the multi-modal\ninformation in the conversation, which is crucial for conversation\nunderstanding. Existing methods focus on using graph neural networks (GNN) to\nmodel conversational relationships and capture contextual latent semantic\nrelationships. However, due to the complexity of GNN, existing methods cannot\nefficiently capture the potential dependencies between long-distance\nutterances, which limits the performance of MERC. In this paper, we propose an\nEfficient Long-distance Latent Relation-aware Graph Neural Network (ELR-GNN)\nfor multi-modal emotion recognition in conversations. Specifically, we first\nuse pre-extracted text, video and audio features as input to Bi-LSTM to capture\ncontextual semantic information and obtain low-level utterance features. Then,\nwe use low-level utterance features to construct a conversational emotion\ninteraction graph. To efficiently capture the potential dependencies between\nlong-distance utterances, we use the dilated generalized forward push algorithm\nto precompute the emotional propagation between global utterances and design an\nemotional relation-aware operator to capture the potential semantic\nassociations between different utterances. Furthermore, we combine early fusion\nand adaptive late fusion mechanisms to fuse latent dependency information\nbetween speaker relationship information and context. Finally, we obtain\nhigh-level discourse features and feed them into MLP for emotion prediction.\nExtensive experimental results show that ELR-GNN achieves state-of-the-art\nperformance on the benchmark datasets IEMOCAP and MELD, with running times\nreduced by 52\\% and 35\\%, respectively.",
      "tldr_zh": "本文提出了一种高效的长距离潜在关系感知图神经网络 (ELR-GNN)，用于多模态对话情感识别 (MERC)，旨在解决现有 GNN 方法在捕获长距离话语潜在依赖方面的效率问题。ELR-GNN 首先使用预提取的文本、视频和音频特征输入 Bi-LSTM 捕获上下文语义信息，并构建对话情感交互图。随后，通过 dilated generalized forward push 算法预计算全局话语的情感传播，并设计 emotional relation-aware operator 来捕获不同话语之间的潜在语义关联。方法还结合 early fusion 和 adaptive late fusion 机制融合说话者关系和上下文信息，最终将高级话语特征输入 MLP 进行情感预测。实验结果显示，ELR-GNN 在 IEMOCAP 和 MELD 数据集上达到最先进性能，同时运行时间分别减少 52% 和 35%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.00119v2",
      "published_date": "2024-06-27 15:54:12 UTC",
      "updated_date": "2024-08-31 12:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:16:28.358243"
    },
    {
      "arxiv_id": "2406.19280v4",
      "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Junying Chen",
        "Chi Gui",
        "Ruyi Ouyang",
        "Anningzhe Gao",
        "Shunian Chen",
        "Guiming Hardy Chen",
        "Xidong Wang",
        "Ruifei Zhang",
        "Zhenyang Cai",
        "Ke Ji",
        "Guangjun Yu",
        "Xiang Wan",
        "Benyou Wang"
      ],
      "abstract": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
      "tldr_zh": "该研究针对Multimodal LLMs (MLLMs) 在医疗多模态能力上的不足（如数据量和质量问题），通过从PubMed提炼并利用GPT-4V去噪和重新格式化，构建了包含130万医疗VQA样本的PubMedVision数据集。验证结果显示，该数据集显著提升了MLLMs在医疗领域的性能，尤其在MMMU Health & Medicine基准上取得了明显改进，并经医疗专家手动检查证实其数据质量优于现有方法。最终，基于PubMedVision训练的34B参数模型HuatuoGPT-Vision在开源MLLMs中表现出色，为大规模注入医疗视觉知识提供了有效途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19280v4",
      "published_date": "2024-06-27 15:50:41 UTC",
      "updated_date": "2024-09-30 06:45:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:16:39.385886"
    },
    {
      "arxiv_id": "2406.19271v2",
      "title": "AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Praneeth Vadlapati"
      ],
      "abstract": "Up-to-date and reliable language models are consistently sought after and are\nessential in various applications. Typically, models are trained on a fixed\ndataset and then deployed globally. However, the knowledge of the models\nbecomes outdated. Enabling automatic updation of AI knowledge using web data\ninvolves significant concerns regarding the model's safety and quality due to a\nthreat from unsafe and undesirable text across the web. The purity of new data\nwas essential for updating knowledge of language models to maintain their\nreliability. This paper proposes AutoPureData, a system that automatically\ncollects and purifies web data. The system loaded a sample of web data.\nUtilizing existing trusted AI models, it successfully eliminated unsafe text\nwith an accuracy of 97% and undesirable text with an accuracy of 86%,\ndemonstrating the system's effectiveness in purifying the data. The system\nensures that only meaningful and safe text can be used to update LLM knowledge.\nThe pure text was then optimized and stored in a vector database for future\nquerying. It was found that LLM can fetch new data from the vector DB. The LLM\nwrites the RAG query in English, even if the user's query is in another\nlanguage, proving that the system can perform cross-lingual retrieval. This\npaper proposes a method to maintain the accuracy and relevance of up-to-date\nlanguage models by ensuring that only purified data was used to update LLM\nknowledge. This work contributes to updating knowledge of chatbots using\nmeaningful and safe text, enhancing their utility across various industries,\nand potentially reducing the risks associated with outputs caused by unsafe or\nimpure data. Code is available at github.com/Pro-GenAI/AutoPureData.",
      "tldr_zh": "本论文提出 AutoPureData 系统，用于自动收集和过滤网络数据，以安全可靠地更新 LLM 知识。系统利用现有可信赖的 AI 模型对网络文本进行净化，成功去除不安全文本（准确率97%）和不期望文本（准确率86%），并将净化后的数据存储在向量数据库中以支持未来查询。实验结果显示，该系统能实现跨语言检索，即使用户查询为其他语言，LLM 也能用英语生成 RAG 查询，从而提升语言模型的准确性、相关性和在各行业的实用性，同时减少不安全数据带来的风险。代码开源于 github.com/Pro-GenAI/AutoPureData。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Final version",
      "pdf_url": "http://arxiv.org/pdf/2406.19271v2",
      "published_date": "2024-06-27 15:37:57 UTC",
      "updated_date": "2025-02-27 07:17:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:16:50.714562"
    },
    {
      "arxiv_id": "2407.00118v1",
      "title": "From Efficient Multimodal Models to World Models: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Xinji Mai",
        "Zeng Tao",
        "Junxiong Lin",
        "Haoran Wang",
        "Yang Chang",
        "Yanlan Kang",
        "Yan Wang",
        "Wenqiang Zhang"
      ],
      "abstract": "Multimodal Large Models (MLMs) are becoming a significant research focus,\ncombining powerful large language models with multimodal learning to perform\ncomplex tasks across different data modalities. This review explores the latest\ndevelopments and challenges in MLMs, emphasizing their potential in achieving\nartificial general intelligence and as a pathway to world models. We provide an\noverview of key techniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning\n(M-ICL). Additionally, we discuss both the fundamental and specific\ntechnologies of multimodal models, highlighting their applications,\ninput/output modalities, and design characteristics. Despite significant\nadvancements, the development of a unified multimodal model remains elusive. We\ndiscuss the integration of 3D generation and embodied intelligence to enhance\nworld simulation capabilities and propose incorporating external rule systems\nfor improved reasoning and decision-making. Finally, we outline future research\ndirections to address these challenges and advance the field.",
      "tldr_zh": "这篇调查论文探讨了Multimodal Large Models (MLMs)的最新发展及其作为通往世界模型的桥梁，强调其在实现人工通用智能中的潜力。论文概述了关键技术，包括Multimodal Chain of Thought (M-COT)、Multimodal Instruction Tuning (M-IT)和Multimodal In-Context Learning (M-ICL)，并分析了多模态模型的应用、输入/输出模态以及设计特点。尽管取得了进展，统一的多模态模型仍面临挑战，论文提出未来方向，如整合3D生成、体化智能和外部规则系统，以提升世界模拟和决策能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00118v1",
      "published_date": "2024-06-27 15:36:43 UTC",
      "updated_date": "2024-06-27 15:36:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:17:01.787643"
    },
    {
      "arxiv_id": "2406.19261v2",
      "title": "Commodification of Compute",
      "title_zh": "计算资源的商品化",
      "authors": [
        "Jesper Kristensen",
        "David Wender",
        "Carl Anthony"
      ],
      "abstract": "The rapid advancements in artificial intelligence, big data analytics, and\ncloud computing have precipitated an unprecedented demand for computational\nresources. However, the current landscape of computational resource allocation\nis characterized by significant inefficiencies, including underutilization and\nprice volatility. This paper addresses these challenges by introducing a novel\nglobal platform for the commodification of compute hours, termed the Global\nCompute Exchange (GCX) (Patent Pending). The GCX leverages blockchain\ntechnology and smart contracts to create a secure, transparent, and efficient\nmarketplace for buying and selling computational power. The GCX is built in a\nlayered fashion, comprising Market, App, Clearing, Risk Management, Exchange\n(Offchain), and Blockchain (Onchain) layers, each ensuring a robust and\nefficient operation. This platform aims to revolutionize the computational\nresource market by fostering a decentralized, efficient, and transparent\necosystem that ensures equitable access to computing power, stimulates\ninnovation, and supports diverse user needs on a global scale. By transforming\ncompute hours into a tradable commodity, the GCX seeks to optimize resource\nutilization, stabilize pricing, and democratize access to computational\nresources. This paper explores the technological infrastructure, market\npotential, and societal impact of the GCX, positioning it as a pioneering\nsolution poised to drive the next wave of innovation in commodities and\ncompute.",
      "tldr_zh": "这篇论文探讨了人工智能、大数据和云计算的快速发展导致计算资源需求激增，但当前分配存在低效问题，如资源低利用率和价格波动。论文提出 Global Compute Exchange (GCX)，一个基于 blockchain 和 smart contracts 的全球平台，用于买卖计算能力，并采用分层架构（包括 Market、App、Clearing、Risk Management、Exchange 和 Blockchain 层）来确保安全、透明和高效运作。该平台旨在优化资源利用、稳定价格，并通过去中心化机制实现计算资源的公平访问和全球创新支持，最终推动计算市场革命和社会变革。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19261v2",
      "published_date": "2024-06-27 15:32:31 UTC",
      "updated_date": "2024-07-03 16:12:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:17:13.616147"
    },
    {
      "arxiv_id": "2406.19256v2",
      "title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
      "title_zh": "翻译失败",
      "authors": [
        "Kaveen Hiniduma",
        "Suren Byna",
        "Jean Luca Bez",
        "Ravi Madduri"
      ],
      "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
      "tldr_zh": "这篇论文针对“Garbage In Garbage Out”问题，引入了 AIDRIN（AI Data Readiness Inspector）框架，用于量化评估数据是否适合 AI 应用。AIDRIN 定义了 AI 数据准备参数，涵盖传统数据质量指标（如完整性、异常值和重复）以及 AI 特定指标（如特征重要性、特征相关性、类别不平衡、公平性、隐私和 FAIR 原则遵守）。通过提供可视化和报告，该框架帮助数据科学家更高效地评估数据准备度，并优化机器学习管道的决策过程。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 9 figures, Accepted to SSDBM 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.19256v2",
      "published_date": "2024-06-27 15:26:39 UTC",
      "updated_date": "2025-03-11 15:58:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:17:25.948296"
    },
    {
      "arxiv_id": "2407.12019v1",
      "title": "DIM: Dynamic Integration of Multimodal Entity Linking with Large Language Model",
      "title_zh": "DIM：多模态实体链接与大型语言模型的动态整合",
      "authors": [
        "Shezheng Song",
        "Shasha Li",
        "Jie Yu",
        "Shan Zhao",
        "Xiaopeng Li",
        "Jun Ma",
        "Xiaodong Liu",
        "Zhuo Li",
        "Xiaoguang Mao"
      ],
      "abstract": "Our study delves into Multimodal Entity Linking, aligning the mention in\nmultimodal information with entities in knowledge base. Existing methods are\nstill facing challenges like ambiguous entity representations and limited image\ninformation utilization. Thus, we propose dynamic entity extraction using\nChatGPT, which dynamically extracts entities and enhances datasets. We also\npropose a method: Dynamically Integrate Multimodal information with knowledge\nbase (DIM), employing the capability of the Large Language Model (LLM) for\nvisual understanding. The LLM, such as BLIP-2, extracts information relevant to\nentities in the image, which can facilitate improved extraction of entity\nfeatures and linking them with the dynamic entity representations provided by\nChatGPT. The experiments demonstrate that our proposed DIM method outperforms\nthe majority of existing methods on the three original datasets, and achieves\nstate-of-the-art (SOTA) on the dynamically enhanced datasets (Wiki+, Rich+,\nDiverse+). For reproducibility, our code and collected datasets are released on\n\\url{https://github.com/season1blue/DIM}.",
      "tldr_zh": "本研究探讨多模态实体链接(Multimodal Entity Linking)，旨在将多模态信息中的提及与知识库实体对齐，以解决现有方法中实体表示模糊和图像信息利用有限的问题。研究提出动态实体提取方法，使用 ChatGPT 动态提取实体并增强数据集，同时开发 DIM 方法，通过大型语言模型(LLM)如 BLIP-2 提取图像中与实体相关的信息，并将其与动态实体表示整合，提高实体特征提取和链接效果。实验结果显示，DIM 在三个原始数据集上优于大多数现有方法，并在动态增强数据集(Wiki+、Rich+、Diverse+)上达到 state-of-the-art (SOTA) 性能；代码和数据集已开源以便复现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published on PRCV24",
      "pdf_url": "http://arxiv.org/pdf/2407.12019v1",
      "published_date": "2024-06-27 15:18:23 UTC",
      "updated_date": "2024-06-27 15:18:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:17:38.207239"
    },
    {
      "arxiv_id": "2406.19251v1",
      "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Fu",
        "Xiaoting Qin",
        "Fangkai Yang",
        "Lu Wang",
        "Jue Zhang",
        "Qingwei Lin",
        "Yubo Chen",
        "Dongmei Zhang",
        "Saravan Rajmohan",
        "Qi Zhang"
      ],
      "abstract": "Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.",
      "tldr_zh": "本研究提出 AutoRAG-HP 框架，用于 Retrieval-Augmented Generation (RAG) 系统的自动在线 Hyper-Parameter Tuning，以解决超参数优化和在线适应的挑战。该框架将优化问题表述为在线 Multi-Armed Bandit (MAB) 问题，并引入两级 Hierarchical MAB (Hier-MAB) 方法，实现对大型搜索空间的高效探索。在实验中，使用 ALCE-ASQA 和 Natural Questions 数据集优化 top-k 检索文档、提示压缩比和嵌入方法等参数，结果显示 MAB 方法仅需约 20% 的 LLM API 调用即可达到 Recall@5 ≈ 0.8，并在更具挑战性的场景中优于基线方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19251v1",
      "published_date": "2024-06-27 15:18:21 UTC",
      "updated_date": "2024-06-27 15:18:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:17:51.786288"
    },
    {
      "arxiv_id": "2407.08752v1",
      "title": "From Modular to End-to-End Speaker Diarization",
      "title_zh": "从模块化到端到端的说话人二值化",
      "authors": [
        "Federico Landini"
      ],
      "abstract": "Speaker diarization is usually referred to as the task that determines ``who\nspoke when'' in a recording. Until a few years ago, all competitive approaches\nwere modular. Systems based on this framework reached state-of-the-art\nperformance in most scenarios but had major difficulties dealing with\noverlapped speech. More recently, the advent of end-to-end models, capable of\ndealing with all aspects of speaker diarization with a single model and better\nperforming regarding overlapped speech, has brought high levels of attention.\n  This thesis is framed during a period of co-existence of these two trends. We\ndescribe a system based on a Bayesian hidden Markov model used to cluster\nx-vectors (speaker embeddings obtained with a neural network), known as VBx,\nwhich has shown remarkable performance on different datasets and challenges. We\ncomment on its advantages and limitations and evaluate results on different\nrelevant corpora. Then, we move towards end-to-end neural diarization (EEND)\nmethods. Due to the need for large training sets for training these models and\nthe lack of manually annotated diarization data in sufficient quantities, the\ncompromise solution consists in generating training data artificially. We\ndescribe an approach for generating synthetic data which resembles real\nconversations in terms of speaker turns and overlaps. We show how this method\ngenerating ``simulated conversations'' allows for better performance than using\na previously proposed method for creating ``simulated mixtures'' when training\nthe popular EEND with encoder-decoder attractors (EEND-EDA). We also propose a\nnew EEND-based model, which we call DiaPer, and show that it can perform better\nthan EEND-EDA, especially when dealing with many speakers and handling\noverlapped speech. Finally, we compare both VBx-based and DiaPer systems on a\nwide variety of corpora and comment on the advantages of each technique.",
      "tldr_zh": "这篇论文探讨了说话人二值化（speaker diarization）任务的演变，从传统的模块化方法到端到端（end-to-end）模型。作者介绍了基于Bayesian hidden Markov model的VBx系统，该系统使用x-vectors进行说话人聚类，在多种数据集上表现出色，但处理重叠语音时存在局限。论文提出了一种生成模拟对话的合成数据方法来训练EEND模型，并开发了新模型DiaPer，该模型在多说话人场景和重叠语音处理上优于EEND-EDA。最后，通过在各种语料库上的比较，证明端到端方法在整体性能上更具优势，尤其在复杂语音环境中。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Ph.D. thesis. Successfully defended on 27.06.2024",
      "pdf_url": "http://arxiv.org/pdf/2407.08752v1",
      "published_date": "2024-06-27 15:09:39 UTC",
      "updated_date": "2024-06-27 15:09:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:18:02.480995"
    },
    {
      "arxiv_id": "2406.19243v1",
      "title": "Application of ASV for Voice Identification after VC and Duration Predictor Improvement in TTS Models",
      "title_zh": "翻译失败",
      "authors": [
        "Borodin Kirill Nikolayevich",
        "Kudryavtsev Vasiliy Dmitrievich",
        "Mkrtchian Grach Maratovich",
        "Gorodnichev Mikhail Genadievich",
        "Korzh Dmitrii Sergeevich"
      ],
      "abstract": "One of the most crucial components in the field of biometric security is the\nautomatic speaker verification system, which is based on the speaker's voice.\nIt is possible to utilise ASVs in isolation or in conjunction with other AI\nmodels. In the contemporary era, the quality and quantity of neural networks\nare increasing exponentially. Concurrently, there is a growing number of\nsystems that aim to manipulate data through the use of voice conversion and\ntext-to-speech models. The field of voice biometrics forgery is aided by a\nnumber of challenges, including SSTC, ASVSpoof, and SingFake.\n  This paper presents a system for automatic speaker verification. The primary\nobjective of our model is the extraction of embeddings from the target\nspeaker's audio in order to obtain information about important characteristics\nof his voice, such as pitch, energy, and the duration of phonemes. This\ninformation is used in our multivoice TTS pipeline, which is currently under\ndevelopment. However, this model was employed within the SSTC challenge to\nverify users whose voice had undergone voice conversion, where it demonstrated\nan EER of 20.669.",
      "tldr_zh": "这篇论文探讨了自动说话者验证 (ASV) 在语音转换 (VC) 后进行语音识别的应用，并针对文本到语音 (TTS) 模型改善了持续时间预测器，以应对语音生物识别伪造挑战，如 SSTC 和 ASVSpoof。研究提出一个 ASV 系统，通过从目标说话者的音频中提取嵌入特征（如音高、能量和音素持续时间），并将其整合到多语音 TTS 管道中，以增强语音特征的准确性。在 SSTC 挑战中，该模型验证了经过 VC 的语音，取得了 20.669% 的 EER（等错误率），为提升语音生物识别的安全性和鲁棒性提供了新方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19243v1",
      "published_date": "2024-06-27 15:08:51 UTC",
      "updated_date": "2024-06-27 15:08:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:18:15.313073"
    },
    {
      "arxiv_id": "2406.19236v3",
      "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
      "title_zh": "人类感知的视觉与语言导航：通过动态人类互动桥接模拟与现实",
      "authors": [
        "Heng Li",
        "Minghan Li",
        "Zhi-Qi Cheng",
        "Yifei Dong",
        "Yuxuan Zhou",
        "Jun-Yan He",
        "Qi Dai",
        "Teruko Mitamura",
        "Alexander G. Hauptmann"
      ],
      "abstract": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.",
      "tldr_zh": "本研究引入了Human-Aware Vision-and-Language Navigation (HA-VLN)，扩展传统VLN框架，通过整合动态人类活动，解决其在静态环境和专家监督依赖下的实际应用限制。研究者开发了Human-Aware 3D (HA3D)模拟器和Human-Aware Room-to-Room (HA-R2R)数据集，分别结合Matterport3D数据和人类活动描述，以模拟真实动态场景。针对HA-VLN的挑战，他们提出Expert-Supervised Cross-Modal (VLN-CM)和Non-Expert-Supervised Decision Transformer (VLN-DT)代理，利用跨模态融合和多样化训练策略，提升代理在人类环境中的导航性能；实验评估显示，这些方法显著提高了鲁棒性和适应性，并为Embodied AI和Sim2Real转移提供基准和研究洞见。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Spotlight at NeurIPS 2024 D&B Track. 32 pages, 18 figures, Project\n  Page: https://lpercc.github.io/HA3D_simulator/",
      "pdf_url": "http://arxiv.org/pdf/2406.19236v3",
      "published_date": "2024-06-27 15:01:42 UTC",
      "updated_date": "2024-11-02 02:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:18:27.619902"
    },
    {
      "arxiv_id": "2406.19234v2",
      "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
      "title_zh": "生成即相信：针对检索增强生成的成员推断攻击",
      "authors": [
        "Yuying Li",
        "Gaoyang Liu",
        "Chen Wang",
        "Yang Yang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that\nmitigates issues such as hallucinations and knowledge staleness in Large\nLanguage Models (LLMs) by retrieving relevant knowledge from an external\ndatabase to assist in content generation. Existing research has demonstrated\npotential privacy risks associated with the LLMs of RAG. However, the privacy\nrisks posed by the integration of an external database, which often contains\nsensitive data such as medical records or personal identities, have remained\nlargely unexplored. In this paper, we aim to bridge this gap by focusing on\nmembership privacy of RAG's external database, with the aim of determining\nwhether a given sample is part of the RAG's database. Our basic idea is that if\na sample is in the external database, it will exhibit a high degree of semantic\nsimilarity to the text generated by the RAG system. We present S$^2$MIA, a\n\\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes\nthe \\underline{S}emantic \\underline{S}imilarity between a given sample and the\ncontent generated by the RAG system. With our proposed S$^2$MIA, we demonstrate\nthe potential to breach the membership privacy of the RAG database. Extensive\nexperiment results demonstrate that S$^2$MIA can achieve a strong inference\nperformance compared with five existing MIAs, and is able to escape from the\nprotection of three representative defenses.",
      "tldr_zh": "本研究探讨了 Retrieval-Augmented Generation (RAG) 系统的隐私风险，特别针对外部数据库的成员隐私，提出了一种新型成员推理攻击（Membership Inference Attack）名为 S²MIA。S²MIA 利用样本与 RAG 生成内容之间的 Semantic Similarity 来判断样本是否属于数据库，从而揭示潜在的隐私漏洞。实验结果表明，S²MIA 比现有的五种 MIA 攻击性能更强，并能成功绕过三种代表性防御机制。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19234v2",
      "published_date": "2024-06-27 14:58:38 UTC",
      "updated_date": "2024-09-26 04:22:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:18:38.059242"
    },
    {
      "arxiv_id": "2406.19228v1",
      "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
      "title_zh": "翻译失败",
      "authors": [
        "Jimin Sun",
        "So Yeon Min",
        "Yingshan Chang",
        "Yonatan Bisk"
      ],
      "abstract": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.",
      "tldr_zh": "该研究挑战了现有工具使用假设，认为大型语言模型(LLMs)的核心挑战不仅仅是选择工具，而是检测“silent”工具错误并进行规划反思。他们引入了一个更广泛的工具框架，探索模型的错误检测能力，并提出了一种初始失败恢复方法。在受控计算器设置和具身代理规划任务中，该方法取得了有前景的结果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19228v1",
      "published_date": "2024-06-27 14:52:34 UTC",
      "updated_date": "2024-06-27 14:52:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:18:49.622189"
    },
    {
      "arxiv_id": "2406.19223v2",
      "title": "T-FREE: Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Björn Deiseroth",
        "Manuel Brack",
        "Patrick Schramowski",
        "Kristian Kersting",
        "Samuel Weinbach"
      ],
      "abstract": "Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.",
      "tldr_zh": "这篇论文提出了 T-FREE，一种无需子词分词器（Subword Tokenizer-Free）的生成式大型语言模型（Generative LLMs），通过稀疏表示（Sparse Representations）直接嵌入字符三元组来解决传统 Tokenizers 的计算开销大、词汇使用无效和对参考语料库依赖等问题。T-FREE 利用形态相似性（Morphological Similarities）实现嵌入层的强压缩，并不需要参考语料库。实验结果显示，T-FREE 在嵌入和头层参数减少超过 85% 的情况下，保持了下游任务的竞争性性能，并显著改善了跨语言转移学习（Cross-lingual Transfer Learning）的效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19223v2",
      "published_date": "2024-06-27 14:49:08 UTC",
      "updated_date": "2025-01-07 16:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:19:03.673209"
    },
    {
      "arxiv_id": "2406.19220v1",
      "title": "Hack Me If You Can: Aggregating AutoEncoders for Countering Persistent Access Threats Within Highly Imbalanced Data",
      "title_zh": "翻译失败",
      "authors": [
        "Sidahmed Benabderrahmane",
        "Ngoc Hoang",
        "Petko Valtchev",
        "James Cheney",
        "Talal Rahwan"
      ],
      "abstract": "Advanced Persistent Threats (APTs) are sophisticated, targeted cyberattacks\ndesigned to gain unauthorized access to systems and remain undetected for\nextended periods. To evade detection, APT cyberattacks deceive defense layers\nwith breaches and exploits, thereby complicating exposure by traditional\nanomaly detection-based security methods. The challenge of detecting APTs with\nmachine learning is compounded by the rarity of relevant datasets and the\nsignificant imbalance in the data, which makes the detection process highly\nburdensome. We present AE-APT, a deep learning-based tool for APT detection\nthat features a family of AutoEncoder methods ranging from a basic one to a\nTransformer-based one. We evaluated our tool on a suite of provenance trace\ndatabases produced by the DARPA Transparent Computing program, where APT-like\nattacks constitute as little as 0.004% of the data. The datasets span multiple\noperating systems, including Android, Linux, BSD, and Windows, and cover two\nattack scenarios. The outcomes showed that AE-APT has significantly higher\ndetection rates compared to its competitors, indicating superior performance in\ndetecting and ranking anomalies.",
      "tldr_zh": "本论文针对高级持续性威胁 (APTs) 的检测挑战，提出 AE-APT 工具，该工具聚合了一系列 AutoEncoder 方法（从基本版本到 Transformer-based 版本），以应对数据稀缺和高度不平衡的问题。AE-APT 在 DARPA Transparent Computing 程序的 provenance trace 数据库上进行评估，这些数据库涵盖 Android、Linux、BSD 和 Windows 等操作系统，以及两种攻击场景，其中 APT-like attacks 仅占 0.004%。实验结果表明，AE-APT 的检测率和异常排名性能显著优于竞争对手，为高效的 APT 检测提供了新途径。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear Future Generation Computer Systems",
      "pdf_url": "http://arxiv.org/pdf/2406.19220v1",
      "published_date": "2024-06-27 14:45:38 UTC",
      "updated_date": "2024-06-27 14:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:19:14.869344"
    },
    {
      "arxiv_id": "2406.19217v1",
      "title": "Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Zhimin Shao",
        "Jialang Xu",
        "Danail Stoyanov",
        "Evangelos B. Mazomenos",
        "Yueming Jin"
      ],
      "abstract": "Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.",
      "tldr_zh": "这篇论文提出了一种基于 Chain-of-Thought (COG) 提示的实时端到端框架，用于机器人辅助微创手术 (RMIS) 视频中的错误检测，旨在利用手术视频的上下文和语义信息来克服传统方法的局限性。框架包括两个关键模块：Gestural-Visual Reasoning 模块，使用 transformer 和 attention 架构进行手势提示；以及 Multi-Scale Temporal Reasoning 模块，采用多阶段时间卷积网络（包括慢速和快速路径）提取时间信息。实验在公开基准数据集 JIGSAWS 上验证，该方法比现有最佳技术提高了 4.6% 的 F1 分数、4.6% 的准确率和 5.9% 的 Jaccard 指数，同时平均每帧处理时间仅为 6.69 毫秒，展示了其在提升 RMIS 安全性和手术教育中的巨大潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19217v1",
      "published_date": "2024-06-27 14:43:50 UTC",
      "updated_date": "2024-06-27 14:43:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:19:29.064981"
    },
    {
      "arxiv_id": "2407.09550v2",
      "title": "CAPM: Fast and Robust Verification on Maxpool-based CNN via Dual Network",
      "title_zh": "翻译失败",
      "authors": [
        "Jia-Hau Bai",
        "Chi-Ting Liu",
        "Yu Wang",
        "Fu-Chieh Chang",
        "Pei-Yuan Wu"
      ],
      "abstract": "This study uses CAPM (Convex Adversarial Polytope for Maxpool-based CNN) to\nimprove the verified bound for general purpose maxpool-based convolutional\nneural networks (CNNs) under bounded norm adversarial perturbations. The\nmaxpool function is decomposed as a series of ReLU functions to extend the\nconvex relaxation technique to maxpool functions, by which the verified bound\ncan be efficiently computed through a dual network. The experimental results\ndemonstrate that this technique allows the state-of-the-art verification\nprecision for maxpool-based CNNs and involves a much lower computational cost\nthan current verification methods, such as DeepZ, DeepPoly and PRIMA. This\nmethod is also applicable to large-scale CNNs, which previous studies show to\nbe often computationally prohibitively expensive. Under certain circumstances,\nCAPM is 40-times, 20-times or twice as fast and give a significantly higher\nverification bound (CAPM 98% vs. PRIMA 76%/DeepPoly 73%/DeepZ 8%) as compared\nto PRIMA/DeepPoly/DeepZ. Furthermore, we additionally present the time\ncomplexity of our algorithm as $O(W^2NK)$, where $W$ is the maximum width of\nthe neural network, $N$ is the number of neurons, and $K$ is the size of the\nmaxpool layer's kernel.",
      "tldr_zh": "本研究提出CAPM（Convex Adversarial Polytope for Maxpool-based CNN）框架，通过双网络（dual network）提升maxpool-based CNN在有界范数对抗扰动下的验证边界精度。将maxpool函数分解为一系列ReLU函数，并扩展凸松弛技术，实现高效计算。实验结果显示，CAPM相较于DeepZ、DeepPoly和PRIMA等方法，验证精度达到最先进水平，同时计算成本大幅降低，在某些场景下速度提升40倍，并提供更高验证边界（例如CAPM 98% vs. PRIMA 76%）。此外，该方法适用于大规模CNN，时间复杂度为O(W^2 N K)，其中W为神经网络最大宽度、N为神经元数、K为maxpool层内核大小。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09550v2",
      "published_date": "2024-06-27 14:43:06 UTC",
      "updated_date": "2025-04-08 15:51:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:19:42.267142"
    },
    {
      "arxiv_id": "2407.00117v1",
      "title": "Machine learning meets mass spectrometry: a focused perspective",
      "title_zh": "机器学习遇见质谱学：一个焦点视角",
      "authors": [
        "Daniil A. Boiko",
        "Valentine P. Ananikov"
      ],
      "abstract": "Mass spectrometry is a widely used method to study molecules and processes in\nmedicine, life sciences, chemistry, catalysis, and industrial product quality\ncontrol, among many other applications. One of the main features of some mass\nspectrometry techniques is the extensive level of characterization (especially\nwhen coupled with chromatography and ion mobility methods, or a part of tandem\nmass spectrometry experiment) and a large amount of generated data per\nmeasurement. Terabyte scales can be easily reached with mass spectrometry\nstudies. Consequently, mass spectrometry has faced the challenge of a high\nlevel of data disappearance. Researchers often neglect and then altogether lose\naccess to the rich information mass spectrometry experiments could provide.\nWith the development of machine learning methods, the opportunity arises to\nunlock the potential of these data, enabling previously inaccessible\ndiscoveries. The present perspective highlights reevaluation of mass\nspectrometry data analysis in the new generation of methods and describes\nsignificant challenges in the field, particularly related to problems involving\nthe use of electrospray ionization. We argue that further applications of\nmachine learning raise new requirements for instrumentation (increasing\nthroughput and information density, decreasing pricing, and making more\nautomation-friendly software), and once met, the field may experience\nsignificant transformation.",
      "tldr_zh": "这篇论文探讨了机器学习（machine learning）与质谱法（mass spectrometry）的结合，强调质谱法在医学、生命科学和化学等领域产生的大量数据（可达 TB 级别）常因忽略而丢失。作者认为，通过机器学习方法重新评估数据分析，可以解锁这些数据的潜力，实现之前无法访问的发现，特别是针对电喷雾电离（electrospray ionization）相关问题。论文指出，进一步应用机器学习需要改进仪器（如提高吞吐量和信息密度、降低成本、开发更自动化的软件），这可能引发质谱领域重大变革。",
      "categories": [
        "physics.chem-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.chem-ph",
      "comment": "20 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00117v1",
      "published_date": "2024-06-27 14:18:23 UTC",
      "updated_date": "2024-06-27 14:18:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:19:53.247606"
    },
    {
      "arxiv_id": "2406.19195v2",
      "title": "Estimating Long-term Heterogeneous Dose-response Curve: Generalization Bound Leveraging Optimal Transport Weights",
      "title_zh": "估计长期异质剂量-反应曲线：利用最优传输权重的泛化界",
      "authors": [
        "Zeqin Yang",
        "Weilin Chen",
        "Ruichu Cai",
        "Yuguang Yan",
        "Zhifeng Hao",
        "Zhipeng Yu",
        "Zhichao Zou",
        "Jixing Xu",
        "Zhen Peng",
        "Jiecheng Guo"
      ],
      "abstract": "Long-term treatment effect estimation is a significant but challenging\nproblem in many applications. Existing methods rely on ideal assumptions, such\nas no unobserved confounders or binary treatment, to estimate long-term average\ntreatment effects. However, in numerous real-world applications, these\nassumptions could be violated, and average treatment effects are insufficient\nfor personalized decision-making. In this paper, we address a more general\nproblem of estimating long-term Heterogeneous Dose-Response Curve (HDRC) while\naccounting for unobserved confounders and continuous treatment. Specifically,\nto remove the unobserved confounders in the long-term observational data, we\nintroduce an optimal transport weighting framework to align the long-term\nobservational data to an auxiliary short-term experimental data. Furthermore,\nto accurately predict the heterogeneous effects of continuous treatment, we\nestablish a generalization bound on counterfactual prediction error by\nleveraging the reweighted distribution induced by optimal transport. Finally,\nwe develop a long-term HDRC estimator building upon the above theoretical\nfoundations. Extensive experiments on synthetic and semi-synthetic datasets\ndemonstrate the effectiveness of our approach.",
      "tldr_zh": "该论文针对长期治疗效果估计的挑战，提出了一种处理未观察混杂因素和连续治疗的通用方法，旨在估计长期 Heterogeneous Dose-Response Curve (HDRC)，以支持个性化决策。通过引入 optimal transport weighting 框架，将长期观察数据与辅助短期实验数据对齐，从而去除混杂因素，并建立了 leveraging optimal transport weights 的 generalization bound 来精确预测异质性效果。基于这些理论基础，作者开发了高效的长期 HDRC 估计器，并在合成和半合成数据集上的广泛实验中证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19195v2",
      "published_date": "2024-06-27 14:13:46 UTC",
      "updated_date": "2025-05-16 07:28:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:20:05.893825"
    },
    {
      "arxiv_id": "2406.19189v1",
      "title": "BISeizuRe: BERT-Inspired Seizure Data Representation to Improve Epilepsy Monitoring",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Benfenati",
        "Thorir Mar Ingolfsson",
        "Andrea Cossettini",
        "Daniele Jahier Pagliari",
        "Alessio Burrello",
        "Luca Benini"
      ],
      "abstract": "This study presents a novel approach for EEG-based seizure detection\nleveraging a BERT-based model. The model, BENDR, undergoes a two-phase training\nprocess. Initially, it is pre-trained on the extensive Temple University\nHospital EEG Corpus (TUEG), a 1.5 TB dataset comprising over 10,000 subjects,\nto extract common EEG data patterns. Subsequently, the model is fine-tuned on\nthe CHB-MIT Scalp EEG Database, consisting of 664 EEG recordings from 24\npediatric patients, of which 198 contain seizure events. Key contributions\ninclude optimizing fine-tuning on the CHB-MIT dataset, where the impact of\nmodel architecture, pre-processing, and post-processing techniques are\nthoroughly examined to enhance sensitivity and reduce false positives per hour\n(FP/h). We also explored custom training strategies to ascertain the most\neffective setup. The model undergoes a novel second pre-training phase before\nsubject-specific fine-tuning, enhancing its generalization capabilities. The\noptimized model demonstrates substantial performance enhancements, achieving as\nlow as 0.23 FP/h, 2.5$\\times$ lower than the baseline model, with a lower but\nstill acceptable sensitivity rate, showcasing the effectiveness of applying a\nBERT-based approach on EEG-based seizure detection.",
      "tldr_zh": "本研究提出了一种基于 BERT 的模型 BENDR，用于改进 EEG-based seizure detection，以提升癫痫监测。该模型采用两阶段训练：首先在 Temple University Hospital EEG Corpus (TUEG) 上预训练，提取超过 10,000 个受试者的常见 EEG 模式；随后在 CHB-MIT Scalp EEG Database 上微调，优化模型架构、预处理和后处理技术，并引入第二预训练阶段以增强泛化能力。关键贡献包括探索自定义训练策略，显著降低假阳性率 (FP/h) 至 0.23，比基线模型低 2.5 倍，同时保持可接受的敏感性，展示了 BERT-based 方法在癫痫检测中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 2 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19189v1",
      "published_date": "2024-06-27 14:09:10 UTC",
      "updated_date": "2024-06-27 14:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:20:21.041680"
    },
    {
      "arxiv_id": "2407.00116v2",
      "title": "Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Mahmoud Ibrahim",
        "Yasmina Al Khalil",
        "Sina Amirrajab",
        "Chang Sun",
        "Marcel Breeuwer",
        "Josien Pluim",
        "Bart Elen",
        "Gokhan Ertaylan",
        "Michel Dumontier"
      ],
      "abstract": "This paper presents a comprehensive systematic review of generative models\n(GANs, VAEs, DMs, and LLMs) used to synthesize various medical data types,\nincluding imaging (dermoscopic, mammographic, ultrasound, CT, MRI, and X-ray),\ntext, time-series, and tabular data (EHR). Unlike previous narrowly focused\nreviews, our study encompasses a broad array of medical data modalities and\nexplores various generative models. Our search strategy queries databases such\nas Scopus, PubMed, and ArXiv, focusing on recent works from January 2021 to\nNovember 2023, excluding reviews and perspectives. This period emphasizes\nrecent advancements beyond GANs, which have been extensively covered\npreviously.\n  The survey reveals insights from three key aspects: (1) Synthesis\napplications and purpose of synthesis, (2) generation techniques, and (3)\nevaluation methods. It highlights clinically valid synthesis applications,\ndemonstrating the potential of synthetic data to tackle diverse clinical\nrequirements. While conditional models incorporating class labels, segmentation\nmasks and image translations are prevalent, there is a gap in utilizing prior\nclinical knowledge and patient-specific context, suggesting a need for more\npersonalized synthesis approaches and emphasizing the importance of tailoring\ngenerative approaches to the unique characteristics of medical data.\nAdditionally, there is a significant gap in using synthetic data beyond\naugmentation, such as for validation and evaluation of downstream medical AI\nmodels. The survey uncovers that the lack of standardized evaluation\nmethodologies tailored to medical images is a barrier to clinical application,\nunderscoring the need for in-depth evaluation approaches, benchmarking, and\ncomparative studies to promote openness and collaboration.",
      "tldr_zh": "这篇论文对生成式 AI（如 GANs、VAEs、DMs 和 LLMs）在合成多种医疗数据模态（包括皮肤镜、乳房 X 光、超声、CT、MRI、X 光、文本、时间序列和 EHR 等）方面的最新发展进行了系统综述，涵盖2021年1月至2023年11月的文献。研究从合成应用、生成技术和评估方法三个方面分析了这些模型的潜力，特别是用于临床需求的合成数据，如数据增强和图像翻译，但强调了缺乏利用先验临床知识和患者特定上下文的不足。综述指出，合成数据在验证下游医疗 AI 模型方面的应用尚有差距，且标准化评估方法的缺失是推广临床应用的重大障碍，呼吁更多基准测试和个性化生成方法以推动开放合作。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00116v2",
      "published_date": "2024-06-27 14:00:11 UTC",
      "updated_date": "2024-07-02 06:51:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:20:30.805024"
    },
    {
      "arxiv_id": "2407.00115v4",
      "title": "Instance Temperature Knowledge Distillation",
      "title_zh": "实例温度知识蒸馏",
      "authors": [
        "Zhengbo Zhang",
        "Yuxi Zhou",
        "Jia Gong",
        "Jun Liu",
        "Zhigang Tu"
      ],
      "abstract": "Knowledge distillation (KD) enhances the performance of a student network by\nallowing it to learn the knowledge transferred from a teacher network\nincrementally. Existing methods dynamically adjust the temperature to enable\nthe student network to adapt to the varying learning difficulties at different\nlearning stages of KD. KD is a continuous process, but when adjusting the\ntemperature, these methods consider only the immediate benefits of the\noperation in the current learning phase and fail to take into account its\nfuture returns. To address this issue, we formulate the adjustment of\ntemperature as a sequential decision-making task and propose a method based on\nreinforcement learning, termed RLKD. Importantly, we design a novel state\nrepresentation to enable the agent to make more informed action (i.e. instance\ntemperature adjustment). To handle the problem of delayed rewards in our method\ndue to the KD setting, we explore an instance reward calibration approach. In\naddition,we devise an efficient exploration strategy that enables the agent to\nlearn valuable instance temperature adjustment policy more efficiently. Our\nframework can serve as a plug-and-play technique to be inserted into various KD\nmethods easily, and we validate its effectiveness on both image classification\nand object detection tasks. Our project is at\nhttps://www.zayx.me/ITKD.github.io/.",
      "tldr_zh": "该论文提出了一种名为 Instance Temperature Knowledge Distillation (ITKD) 的方法，通过强化学习 (RLKD) 将知识蒸馏 (KD) 中的温度调整视为顺序决策任务，以解决现有方法仅关注当前阶段即时收益而忽略未来回报的问题。研究设计了新型状态表示、实例奖励校准和高效探索策略，使代理能够更有效地学习实例温度调整策略，从而提升学生网络的适应性。该框架作为插件可轻松整合到各种 KD 方法中，并在图像分类和物体检测任务上验证了其有效性，展示了显著的性能改进。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.4.0"
      ],
      "primary_category": "cs.LG",
      "comment": "Serious updates are needed",
      "pdf_url": "http://arxiv.org/pdf/2407.00115v4",
      "published_date": "2024-06-27 14:00:05 UTC",
      "updated_date": "2025-03-14 15:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:20:41.445201"
    },
    {
      "arxiv_id": "2407.00114v2",
      "title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Wang",
        "Shaofei Cai",
        "Zhancun Mu",
        "Haowei Lin",
        "Ceyao Zhang",
        "Xuejie Liu",
        "Qing Li",
        "Anji Liu",
        "Xiaojian Ma",
        "Yitao Liang"
      ],
      "abstract": "This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.",
      "tldr_zh": "这篇论文介绍了 OmniJARVIS，一种统一的 Vision-Language-Action (VLA) 模型，旨在为 Minecraft 中的开放世界指令跟随代理提供强推理和高效决策能力。模型通过自监督方法学习行为编码器，生成行为轨迹的离散化标记，并结合模仿学习策略解码器，将任务指令、记忆、观察和行为等多模态交互数据打包成统一的标记序列，使用 autoregressive transformers 进行建模。OmniJARVIS 能够产生链式思维推理、规划、回答问题和执行行动，在 Minecraft 的原子、程序化和开放任务上表现出色，并揭示了交互数据形成、统一标记化和扩展潜力的关键设计原则。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted on NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00114v2",
      "published_date": "2024-06-27 13:46:11 UTC",
      "updated_date": "2024-10-31 14:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:20:54.908067"
    },
    {
      "arxiv_id": "2407.00113v1",
      "title": "Personalized Federated Continual Learning via Multi-granularity Prompt",
      "title_zh": "基于多粒度提示的个性化联邦持续学习",
      "authors": [
        "Hao Yu",
        "Xin Yang",
        "Xin Gao",
        "Yan Kang",
        "Hao Wang",
        "Junbo Zhang",
        "Tianrui Li"
      ],
      "abstract": "Personalized Federated Continual Learning (PFCL) is a new practical scenario\nthat poses greater challenges in sharing and personalizing knowledge. PFCL not\nonly relies on knowledge fusion for server aggregation at the global\nspatial-temporal perspective but also needs model improvement for each client\naccording to the local requirements. Existing methods, whether in Personalized\nFederated Learning (PFL) or Federated Continual Learning (FCL), have overlooked\nthe multi-granularity representation of knowledge, which can be utilized to\novercome Spatial-Temporal Catastrophic Forgetting (STCF) and adopt generalized\nknowledge to itself by coarse-to-fine human cognitive mechanisms. Moreover, it\nallows more effectively to personalized shared knowledge, thus serving its own\npurpose. To this end, we propose a novel concept called multi-granularity\nprompt, i.e., coarse-grained global prompt acquired through the common model\nlearning process, and fine-grained local prompt used to personalize the\ngeneralized representation. The former focuses on efficiently transferring\nshared global knowledge without spatial forgetting, and the latter emphasizes\nspecific learning of personalized local knowledge to overcome temporal\nforgetting. In addition, we design a selective prompt fusion mechanism for\naggregating knowledge of global prompts distilled from different clients. By\nthe exclusive fusion of coarse-grained knowledge, we achieve the transmission\nand refinement of common knowledge among clients, further enhancing the\nperformance of personalization. Extensive experiments demonstrate the\neffectiveness of the proposed method in addressing STCF as well as improving\npersonalized performance. Our code now is available at\nhttps://github.com/SkyOfBeginning/FedMGP.",
      "tldr_zh": "本研究针对Personalized Federated Continual Learning (PFCL)场景，提出了一种基于多粒度提示的方法，以克服Spatial-Temporal Catastrophic Forgetting (STCF)并提升知识共享和个性化性能。该方法包括粗粒度的全局提示，用于高效转移共享知识避免空间遗忘，以及细粒度的本地提示，用于个性化本地知识以克服时间遗忘。此外，设计了选择性提示融合机制，通过聚合不同客户端的全局提示，实现知识传输和优化，进而增强个性化效果。实验结果显示，该方法在PFCL任务中显著提高了性能，并证明了其在解决STCF方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by KDD 2024 Research Track",
      "pdf_url": "http://arxiv.org/pdf/2407.00113v1",
      "published_date": "2024-06-27 13:41:37 UTC",
      "updated_date": "2024-06-27 13:41:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:21:05.611367"
    },
    {
      "arxiv_id": "2406.19150v1",
      "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
      "title_zh": "RAVEN：多任务检索增强视觉-语言学习",
      "authors": [
        "Varun Nagaraj Rao",
        "Siddharth Choudhary",
        "Aditya Deshpande",
        "Ravi Kumar Satzoda",
        "Srikar Appalaraju"
      ],
      "abstract": "The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.",
      "tldr_zh": "这篇论文介绍了 RAVEN，一种多任务检索增强视觉语言模型 (VLMs) 框架，旨在解决大规模语言模型参数膨胀和资源限制问题，同时克服现有 RAG 方法的单任务局限性、资源密集型预训练和模态优先级问题。RAVEN 通过高效的任务特定微调整合检索增强样本，而无需额外检索参数，使模型在多个任务中获得有效的检索属性。实验结果显示，与非检索基线相比，RAVEN 在图像描述任务上提升显著（MSCOCO 上 +1 CIDEr，NoCaps 上 +4 CIDEr），并在 VQA 任务特定问题类型上提高近 3% 准确率，从而证明了 RAG 应用于 VLMs 的高效性和可访问性，促进了多模态学习的可持续发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19150v1",
      "published_date": "2024-06-27 13:08:35 UTC",
      "updated_date": "2024-06-27 13:08:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:21:18.955839"
    },
    {
      "arxiv_id": "2406.19148v1",
      "title": "BackMix: Mitigating Shortcut Learning in Echocardiography with Minimal Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Kit Mills Bransby",
        "Arian Beqiri",
        "Woo-Jin Cho Kim",
        "Jorge Oliveira",
        "Agisilaos Chartsias",
        "Alberto Gomez"
      ],
      "abstract": "Neural networks can learn spurious correlations that lead to the correct\nprediction in a validation set, but generalise poorly because the predictions\nare right for the wrong reason. This undesired learning of naive shortcuts\n(Clever Hans effect) can happen for example in echocardiogram view\nclassification when background cues (e.g. metadata) are biased towards a class\nand the model learns to focus on those background features instead of on the\nimage content. We propose a simple, yet effective random background\naugmentation method called BackMix, which samples random backgrounds from other\nexamples in the training set. By enforcing the background to be uncorrelated\nwith the outcome, the model learns to focus on the data within the ultrasound\nsector and becomes invariant to the regions outside this. We extend our method\nin a semi-supervised setting, finding that the positive effects of BackMix are\nmaintained with as few as 5% of segmentation labels. A loss weighting\nmechanism, wBackMix, is also proposed to increase the contribution of the\naugmented examples. We validate our method on both in-distribution and\nout-of-distribution datasets, demonstrating significant improvements in\nclassification accuracy, region focus and generalisability. Our source code is\navailable at: https://github.com/kitbransby/BackMix",
      "tldr_zh": "该研究解决了神经网络在超声心动图(echocardiography)视图分类中的捷径学习(shortcut learning)问题，即模型依赖虚假相关性（如背景元数据）导致泛化性差。作者提出BackMix方法，通过从训练集采样随机背景进行增强，确保模型关注超声扇区内的图像内容，而非外部区域。该方法扩展到半监督设置，仅需5%的分割标签即可维持效果，并引入wBackMix损失加权机制进一步提升增强样本的贡献。在分布内和分布外数据集上实验验证，BackMix显著提高了分类准确性、区域焦点和泛化性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI 2024 (Pre-print)",
      "pdf_url": "http://arxiv.org/pdf/2406.19148v1",
      "published_date": "2024-06-27 13:06:47 UTC",
      "updated_date": "2024-06-27 13:06:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:21:29.863627"
    },
    {
      "arxiv_id": "2407.00111v1",
      "title": "Accurate Prediction of Ligand-Protein Interaction Affinities with Fine-Tuned Small Language Models",
      "title_zh": "使用微调的小型语言模型准确",
      "authors": [
        "Ben Fauber"
      ],
      "abstract": "We describe the accurate prediction of ligand-protein interaction (LPI)\naffinities, also known as drug-target interactions (DTI), with instruction\nfine-tuned pretrained generative small language models (SLMs). We achieved\naccurate predictions for a range of affinity values associated with\nligand-protein interactions on out-of-sample data in a zero-shot setting. Only\nthe SMILES string of the ligand and the amino acid sequence of the protein were\nused as the model inputs. Our results demonstrate a clear improvement over\nmachine learning (ML) and free-energy perturbation (FEP+) based methods in\naccurately predicting a range of ligand-protein interaction affinities, which\ncan be leveraged to further accelerate drug discovery campaigns against\nchallenging therapeutic targets.",
      "tldr_zh": "这篇论文介绍了使用指令微调的预训练生成小语言模型（SLMs）来准确预测配体-蛋白质相互作用（LPI）亲和力（也称为药物-靶点相互作用，DTI）。模型仅以配体的 SMILES 字符串和蛋白质的氨基酸序列作为输入，在零样本设置下实现了对外部数据的精确预测。结果显示，该方法比传统机器学习（ML）和自由能扰动（FEP+）方法有显著改进，可用于加速针对挑战性治疗靶点的药物发现过程。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00111v1",
      "published_date": "2024-06-27 13:04:58 UTC",
      "updated_date": "2024-06-27 13:04:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:21:42.944956"
    },
    {
      "arxiv_id": "2407.09548v1",
      "title": "Towards Temporal Change Explanations from Bi-Temporal Satellite Images",
      "title_zh": "翻译失败",
      "authors": [
        "Ryo Tsujimoto",
        "Hiroki Ouchi",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
      ],
      "abstract": "Explaining temporal changes between satellite images taken at different times\nis important for urban planning and environmental monitoring. However, manual\ndataset construction for the task is costly, so human-AI collaboration is\npromissing. Toward the direction, in this paper, we investigate the ability of\nLarge-scale Vision-Language Models (LVLMs) to explain temporal changes between\nsatellite images. While LVLMs are known to generate good image captions, they\nreceive only a single image as input. To deal with a par of satellite images as\ninput, we propose three prompting methods. Through human evaluation, we found\nthe effectiveness of our step-by-step reasoning based prompting.",
      "tldr_zh": "该研究探讨了从双时相卫星图像中解释时间变化的问题，这对城市规划和环境监测至关重要。鉴于手动构建数据集成本高昂，论文提出通过人机协作利用Large-scale Vision-Language Models (LVLMs)来生成变化解释。虽然LVLMs擅长处理单张图像，论文为此设计了三种提示方法，特别是基于逐步推理的提示，以处理成对卫星图像。人类评估结果显示，这种逐步推理提示方法有效，提升了解释的准确性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.09548v1",
      "published_date": "2024-06-27 12:49:22 UTC",
      "updated_date": "2024-06-27 12:49:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:21:53.557247"
    },
    {
      "arxiv_id": "2406.19136v6",
      "title": "YZS-model: A Predictive Model for Organic Drug Solubility Based on Graph Convolutional Networks and Transformer-Attention",
      "title_zh": "YZS-model：一种基于图卷积网络和 Transformer-Attention 的有机药物溶解度预测模型",
      "authors": [
        "Chenxu Wang",
        "Haowei Ming",
        "Jian He",
        "Yao Lu",
        "Junhong Chen"
      ],
      "abstract": "Accurate prediction of drug molecule solubility is crucial for therapeutic\neffectiveness and safety. Traditional methods often miss complex molecular\nstructures, leading to inaccuracies. We introduce the YZS-Model, a deep\nlearning framework integrating Graph Convolutional Networks (GCN), Transformer\narchitectures, and Long Short-Term Memory (LSTM) networks to enhance prediction\nprecision. GCNs excel at capturing intricate molecular topologies by modeling\nthe relationships between atoms and bonds. Transformers, with their\nself-attention mechanisms, effectively identify long-range dependencies within\nmolecules, capturing global interactions. LSTMs process sequential data,\npreserving long-term dependencies and integrating temporal information within\nmolecular sequences. This multifaceted approach leverages the strengths of each\ncomponent, resulting in a model that comprehensively understands and predicts\nmolecular properties. Trained on 9,943 compounds and tested on an anticancer\ndataset, the YZS-Model achieved an $R^2$ of 0.59 and an RMSE of 0.57,\noutperforming benchmark models ($R^2$ of 0.52 and RMSE of 0.61). In an\nindependent test, it demonstrated an RMSE of 1.05, improving accuracy by 45.9%.\nThe integration of these deep learning techniques allows the YZS-Model to learn\nvaluable features from complex data without predefined parameters, handle large\ndatasets efficiently, and adapt to various molecular types. This comprehensive\ncapability significantly improves predictive accuracy and model\ngeneralizability. Its precision in solubility predictions can expedite drug\ndevelopment by optimizing candidate selection, reducing costs, and enhancing\nefficiency. Our research underscores deep learning's transformative potential\nin pharmaceutical science, particularly for solubility prediction and drug\ndesign.",
      "tldr_zh": "本研究提出YZS-Model，一种基于Graph Convolutional Networks (GCN)、Transformer-Attention和Long Short-Term Memory (LSTM)的深度学习框架，用于准确预测有机药物分子溶解度，以克服传统方法对复杂分子结构的局限性。GCN捕捉分子拓扑关系，Transformer识别长距离依赖，LSTM处理顺序数据，从而全面理解分子特性。实验结果显示，该模型在9,943种化合物训练集上测试时，R²达到0.59、RMSE为0.57，比基准模型（R²=0.52、RMSE=0.61）显著提升，并在独立测试中RMSE降至1.05，提高准确性45.9%。YZS-Model的集成方法提升了预测精度和泛化能力，有助于加速药物开发、优化候选选择并降低成本。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 16 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.19136v6",
      "published_date": "2024-06-27 12:40:29 UTC",
      "updated_date": "2024-08-13 07:12:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:22:07.627809"
    },
    {
      "arxiv_id": "2406.19135v1",
      "title": "DEX-TTS: Diffusion-based EXpressive Text-to-Speech with Style Modeling on Time Variability",
      "title_zh": "翻译失败",
      "authors": [
        "Hyun Joon Park",
        "Jin Sob Kim",
        "Wooseok Shin",
        "Sung Won Han"
      ],
      "abstract": "Expressive Text-to-Speech (TTS) using reference speech has been studied\nextensively to synthesize natural speech, but there are limitations to\nobtaining well-represented styles and improving model generalization ability.\nIn this study, we present Diffusion-based EXpressive TTS (DEX-TTS), an acoustic\nmodel designed for reference-based speech synthesis with enhanced style\nrepresentations. Based on a general diffusion TTS framework, DEX-TTS includes\nencoders and adapters to handle styles extracted from reference speech. Key\ninnovations contain the differentiation of styles into time-invariant and\ntime-variant categories for effective style extraction, as well as the design\nof encoders and adapters with high generalization ability. In addition, we\nintroduce overlapping patchify and convolution-frequency patch embedding\nstrategies to improve DiT-based diffusion networks for TTS. DEX-TTS yields\noutstanding performance in terms of objective and subjective evaluation in\nEnglish multi-speaker and emotional multi-speaker datasets, without relying on\npre-training strategies. Lastly, the comparison results for the general TTS on\na single-speaker dataset verify the effectiveness of our enhanced diffusion\nbackbone. Demos are available here.",
      "tldr_zh": "本文提出 DEX-TTS，一种基于扩散框架的表达性 Text-to-Speech (TTS) 模型，旨在通过区分 time-invariant 和 time-variant 风格来提升参考语音的风格提取和泛化能力。模型创新性地引入 encoders、adapters、overlapping patchify 以及 convolution-frequency patch embedding 策略，以改进 DiT-based 扩散网络的性能。实验结果显示，DEX-TTS 在英语多说话者和情感多说话者数据集上的客观和主观评估中表现出色，且在单说话者数据集上验证了增强扩散骨干的有效性，而无需依赖预训练策略。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2406.19135v1",
      "published_date": "2024-06-27 12:39:55 UTC",
      "updated_date": "2024-06-27 12:39:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:22:18.998859"
    },
    {
      "arxiv_id": "2406.19126v1",
      "title": "Super-resolution imaging using super-oscillatory diffractive neural networks",
      "title_zh": "使用超振荡衍射神经网络的超分辨率成像",
      "authors": [
        "Hang Chen",
        "Sheng Gao",
        "Zejia Zhao",
        "Zhengyang Duan",
        "Haiou Zhang",
        "Gordon Wetzstein",
        "Xing Lin"
      ],
      "abstract": "Optical super-oscillation enables far-field super-resolution imaging beyond\ndiffraction limits. However, the existing super-oscillatory lens for the\nspatial super-resolution imaging system still confronts critical limitations in\nperformance due to the lack of a more advanced design method and the limited\ndesign degree of freedom. Here, we propose an optical super-oscillatory\ndiffractive neural network, i.e., SODNN, that can achieve super-resolved\nspatial resolution for imaging beyond the diffraction limit with superior\nperformance over existing methods. SODNN is constructed by utilizing\ndiffractive layers to implement optical interconnections and imaging samples or\nbiological sensors to implement nonlinearity, which modulates the incident\noptical field to create optical super-oscillation effects in 3D space and\ngenerate the super-resolved focal spots. By optimizing diffractive layers with\n3D optical field constraints under an incident wavelength size of $\\lambda$, we\nachieved a super-oscillatory spot with a full width at half maximum of\n0.407$\\lambda$ in the far field distance over 400$\\lambda$ without side-lobes\nover the field of view, having a long depth of field over 10$\\lambda$.\nFurthermore, the SODNN implements a multi-wavelength and multi-focus spot array\nthat effectively avoids chromatic aberrations. Our research work will inspire\nthe development of intelligent optical instruments to facilitate the\napplications of imaging, sensing, perception, etc.",
      "tldr_zh": "本研究提出了一种光学超-oscillatory diffractive neural network（SODNN），用于实现超越衍射极限的远场超分辨率成像，解决了现有超振荡透镜在设计方法和自由度上的局限性。SODNN通过衍射层实现光学互连，并结合成像样本或生物传感器的非线性效果，在3D空间生成超振荡焦斑，优化后在远场距离超过400λ处获得FWHM为0.407λ的超分辨率斑点，具有长景深（超过10λ）和无旁瓣特性，同时支持多波长多焦点阵列以避免色差。该创新有望推动智能光学仪器在成像、感知等领域的发展。",
      "categories": [
        "physics.optics",
        "cs.AI"
      ],
      "primary_category": "physics.optics",
      "comment": "18 pages, 7 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2406.19126v1",
      "published_date": "2024-06-27 12:16:35 UTC",
      "updated_date": "2024-06-27 12:16:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:22:31.613922"
    },
    {
      "arxiv_id": "2407.00110v2",
      "title": "Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services",
      "title_zh": "Chat AI：一种无缝的 Slurm 原生解决方案，用于基于 HPC 的服务",
      "authors": [
        "Ali Doosthosseini",
        "Jonathan Decker",
        "Hendrik Nolte",
        "Julian M. Kunkel"
      ],
      "abstract": "The widespread adoption of large language models (LLMs) has created a\npressing need for an efficient, secure and private serving infrastructure,\nwhich allows researchers to run open source or custom fine-tuned LLMs and\nensures users that their data remains private and is not stored without their\nconsent. While high-performance computing (HPC) systems equipped with\nstate-of-the-art GPUs are well-suited for training LLMs, their batch scheduling\nparadigm is not designed to support real-time serving of AI applications. Cloud\nsystems, on the other hand, are well suited for web services but commonly lack\naccess to the computational power of HPC clusters, especially expensive and\nscarce high-end GPUs, which are required for optimal inference speed. We\npropose an architecture with an implementation consisting of a web service that\nruns on a cloud VM with secure access to a scalable backend running a multitude\nof LLM models on HPC systems. By offering a web service using our HPC\ninfrastructure to host LLMs, we leverage the trusted environment of local\nuniversities and research centers to offer a private and secure alternative to\ncommercial LLM services. Our solution natively integrates with the HPC batch\nscheduler Slurm, enabling seamless deployment on HPC clusters, and is able to\nrun side by side with regular Slurm workloads, while utilizing gaps in the\nschedule created by Slurm. In order to ensure the security of the HPC system,\nwe use the SSH ForceCommand directive to construct a robust circuit breaker,\nwhich prevents successful attacks on the web-facing server from affecting the\ncluster. We have successfully deployed our system as a production service, and\nmade the source code available at \\url{https://github.com/gwdg/chat-ai}",
      "tldr_zh": "该论文提出了一种名为 Chat AI 的解决方案，用于在高性能计算 (HPC) 系统上提供高效、安全和私有的 LLM (Large Language Models) 服务。它通过一个在云虚拟机 (VM) 上运行的 web 服务与 HPC 后端无缝集成，利用 Slurm 调度器原生支持，在不干扰常规工作负载的情况下，利用空闲时间运行模型推理，并通过 SSH ForceCommand 指令增强系统安全性。相比云系统，该架构充分利用了 HPC 的高端 GPU 资源，提供可扩展的 LLM 托管服务，同时确保用户数据隐私。该系统已成功部署为生产服务，并开源在 GitHub 上。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Various improvements to explanations and form and updated graphs to\n  include data points up to 30.07.2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00110v2",
      "published_date": "2024-06-27 12:08:21 UTC",
      "updated_date": "2024-08-02 15:34:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:22:41.521808"
    },
    {
      "arxiv_id": "2406.19121v3",
      "title": "Towards Learning Abductive Reasoning using VSA Distributed Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Giacomo Camposampiero",
        "Michael Hersche",
        "Aleksandar Terzić",
        "Roger Wattenhofer",
        "Abu Sebastian",
        "Abbas Rahimi"
      ],
      "abstract": "We introduce the Abductive Rule Learner with Context-awareness (ARLC), a\nmodel that solves abstract reasoning tasks based on Learn-VRF. ARLC features a\nnovel and more broadly applicable training objective for abductive reasoning,\nresulting in better interpretability and higher accuracy when solving Raven's\nprogressive matrices (RPM). ARLC allows both programming domain knowledge and\nlearning the rules underlying a data distribution. We evaluate ARLC on the\nI-RAVEN dataset, showcasing state-of-the-art accuracy across both\nin-distribution and out-of-distribution (unseen attribute-rule pairs) tests.\nARLC surpasses neuro-symbolic and connectionist baselines, including large\nlanguage models, despite having orders of magnitude fewer parameters. We show\nARLC's robustness to post-programming training by incrementally learning from\nexamples on top of programmed knowledge, which only improves its performance\nand does not result in catastrophic forgetting of the programmed solution. We\nvalidate ARLC's seamless transfer learning from a 2x2 RPM constellation to\nunseen constellations. Our code is available at\nhttps://github.com/IBM/abductive-rule-learner-with-context-awareness.",
      "tldr_zh": "本研究提出了 Abductive Rule Learner with Context-awareness (ARLC) 模型，利用 VSA Distributed Representations 学习演绎推理，基于 Learn-VRF 框架解决抽象推理任务。ARLC 引入了一个新颖的训练目标，提升了任务的可解释性和准确性，尤其在 Raven's progressive matrices (RPM) 上，同时允许编程领域知识并从数据分布中学习规则。在 I-RAVEN 数据集上，ARLC 实现了最先进的准确率，优于神经符号和连接主义基线（如大型语言模型），尽管参数数量级更少，且展示了鲁棒性，通过增量学习避免灾难性遗忘并实现无缝转移学习。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at the 18th International Conference on Neural-Symbolic\n  Learning and Reasoning (NeSy) 2024 [Spotlight]",
      "pdf_url": "http://arxiv.org/pdf/2406.19121v3",
      "published_date": "2024-06-27 12:05:55 UTC",
      "updated_date": "2024-08-30 06:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:22:54.397379"
    },
    {
      "arxiv_id": "2406.19116v1",
      "title": "CHEW: A Dataset of CHanging Events in Wikipedia",
      "title_zh": "翻译失败",
      "authors": [
        "Hsuvas Borkakoty",
        "Luis Espinosa-Anke"
      ],
      "abstract": "We introduce CHEW, a novel dataset of changing events in Wikipedia expressed\nin naturally occurring text. We use CHEW for probing LLMs for their timeline\nunderstanding of Wikipedia entities and events in generative and classification\nexperiments. Our results suggest that LLMs, despite having temporal information\navailable, struggle to construct accurate timelines. We further show the\nusefulness of CHEW-derived embeddings for identifying meaning shift.",
      "tldr_zh": "本文引入了 CHEW 数据集，这是一个包含 Wikipedia 中事件变化的自然文本数据集，用于评估大型语言模型 (LLMs) 对实体和事件的 timeline 理解。研究通过生成和分类实验发现，尽管 LLMs 能访问时间信息，它们在构建准确的时间线方面仍存在显著困难。此外，CHEW 派生的 embeddings 证明在识别 meaning shift（意义转移）方面具有实际价值。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Short Paper",
      "pdf_url": "http://arxiv.org/pdf/2406.19116v1",
      "published_date": "2024-06-27 11:53:15 UTC",
      "updated_date": "2024-06-27 11:53:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:23:05.985568"
    },
    {
      "arxiv_id": "2406.19108v2",
      "title": "Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Blaise Agüera y Arcas",
        "Jyrki Alakuijala",
        "James Evans",
        "Ben Laurie",
        "Alexander Mordvintsev",
        "Eyvind Niklasson",
        "Ettore Randazzo",
        "Luca Versari"
      ],
      "abstract": "The fields of Origin of Life and Artificial Life both question what life is\nand how it emerges from a distinct set of \"pre-life\" dynamics. One common\nfeature of most substrates where life emerges is a marked shift in dynamics\nwhen self-replication appears. While there are some hypotheses regarding how\nself-replicators arose in nature, we know very little about the general\ndynamics, computational principles, and necessary conditions for\nself-replicators to emerge. This is especially true on \"computational\nsubstrates\" where interactions involve logical, mathematical, or programming\nrules. In this paper we take a step towards understanding how self-replicators\narise by studying several computational substrates based on various simple\nprogramming languages and machine instruction sets. We show that when random,\nnon self-replicating programs are placed in an environment lacking any explicit\nfitness landscape, self-replicators tend to arise. We demonstrate how this\noccurs due to random interactions and self-modification, and can happen with\nand without background random mutations. We also show how increasingly complex\ndynamics continue to emerge following the rise of self-replicators. Finally, we\nshow a counterexample of a minimalistic programming language where\nself-replicators are possible, but so far have not been observed to arise.",
      "tldr_zh": "本研究探讨了生命起源和人工生命领域的核心问题：自复制（self-replication）如何从简单的交互动态中出现。作者通过分析基于简单编程语言和机器指令集的计算底物（computational substrates），模拟随机、非自复制程序在无显式适应度景观（fitness landscape）的环境中，发现自复制程序会因随机交互和自修改（self-modification）而自然产生，且此过程可伴随或不伴随背景随机突变。实验结果显示，自复制者出现后会引发更复杂的动态，但在一个极简编程语言的反例中，尽管自复制可能，却尚未观察到其自发生成，这为理解计算底物中的生命演化提供了新见解。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "F.2.2; I.2.11"
      ],
      "primary_category": "cs.NE",
      "comment": "20 pages; updated introduction with further related work",
      "pdf_url": "http://arxiv.org/pdf/2406.19108v2",
      "published_date": "2024-06-27 11:34:35 UTC",
      "updated_date": "2024-08-02 09:10:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:23:18.780227"
    },
    {
      "arxiv_id": "2406.19102v1",
      "title": "Statements: Universal Information Extraction from Tables with Large Language Models for ESG KPIs",
      "title_zh": "翻译失败",
      "authors": [
        "Lokesh Mishra",
        "Sohayl Dhibi",
        "Yusik Kim",
        "Cesar Berrospi Ramis",
        "Shubham Gupta",
        "Michele Dolfi",
        "Peter Staar"
      ],
      "abstract": "Environment, Social, and Governance (ESG) KPIs assess an organization's\nperformance on issues such as climate change, greenhouse gas emissions, water\nconsumption, waste management, human rights, diversity, and policies. ESG\nreports convey this valuable quantitative information through tables.\nUnfortunately, extracting this information is difficult due to high variability\nin the table structure as well as content. We propose Statements, a novel\ndomain agnostic data structure for extracting quantitative facts and related\ninformation. We propose translating tables to statements as a new supervised\ndeep-learning universal information extraction task. We introduce SemTabNet - a\ndataset of over 100K annotated tables. Investigating a family of T5-based\nStatement Extraction Models, our best model generates statements which are 82%\nsimilar to the ground-truth (compared to baseline of 21%). We demonstrate the\nadvantages of statements by applying our model to over 2700 tables from ESG\nreports. The homogeneous nature of statements permits exploratory data analysis\non expansive information found in large collections of ESG reports.",
      "tldr_zh": "该研究针对ESG KPIs（Environment, Social, and Governance关键绩效指标）报告中表格的结构和内容高度可变问题，提出Statements，这是一种通用的领域无关数据结构，用于提取量化事实和相关信息。研究将表格转换为Statements定义为一个新的监督深度学习任务，并引入SemTabNet数据集，包含超过10万条注释表格。基于T5模型家族的Statement Extraction Models，他们的最佳模型生成Statements的相似度达到82%（相比基线21%），并成功应用于超过2700个ESG报告表格中。Statements的同质性使得对大型ESG报告集合进行探索性数据分析成为可能，从而提升了信息提取的效率和实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the NLP4Climate workshop in the 62nd Annual Meeting of\n  the Association for Computational Linguistics (ACL 2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.19102v1",
      "published_date": "2024-06-27 11:28:50 UTC",
      "updated_date": "2024-06-27 11:28:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:23:30.893533"
    },
    {
      "arxiv_id": "2407.12018v1",
      "title": "Empirical Evaluation of Public HateSpeech Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Sadar Jaf",
        "Basel Barakat"
      ],
      "abstract": "Despite the extensive communication benefits offered by social media\nplatforms, numerous challenges must be addressed to ensure user safety. One of\nthe most significant risks faced by users on these platforms is targeted hate\nspeech. Social media platforms are widely utilised for generating datasets\nemployed in training and evaluating machine learning algorithms for hate speech\ndetection. However, existing public datasets exhibit numerous limitations,\nhindering the effective training of these algorithms and leading to inaccurate\nhate speech classification. This study provides a comprehensive empirical\nevaluation of several public datasets commonly used in automated hate speech\nclassification. Through rigorous analysis, we present compelling evidence\nhighlighting the limitations of current hate speech datasets. Additionally, we\nconduct a range of statistical analyses to elucidate the strengths and\nweaknesses inherent in these datasets. This work aims to advance the\ndevelopment of more accurate and reliable machine learning models for hate\nspeech detection by addressing the dataset limitations identified.",
      "tldr_zh": "这篇论文通过实证评估(Empirical Evaluation)对用于仇恨言论(hate speech)检测的公共数据集进行了全面分析，揭示了这些数据集的诸多局限性，如数据质量问题和分类不准确性。研究采用严格的统计分析方法，评估了数据集的优缺点，并提供了证据证明这些缺陷会阻碍机器学习(machine learning)算法的有效训练。最终，该工作旨在通过解决这些问题来推动更准确可靠的仇恨言论检测模型的发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 12 tables, 1 algorithm pseudocode, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.12018v1",
      "published_date": "2024-06-27 11:20:52 UTC",
      "updated_date": "2024-06-27 11:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:23:43.189606"
    },
    {
      "arxiv_id": "2407.00108v1",
      "title": "A Case Study on Contextual Machine Translation in a Professional Scenario of Subtitling",
      "title_zh": "翻译失败",
      "authors": [
        "Sebastian Vincent",
        "Charlotte Prescott",
        "Chris Bayliss",
        "Chris Oakley",
        "Carolina Scarton"
      ],
      "abstract": "Incorporating extra-textual context such as film metadata into the machine\ntranslation (MT) pipeline can enhance translation quality, as indicated by\nautomatic evaluation in recent work. However, the positive impact of such\nsystems in industry remains unproven. We report on an industrial case study\ncarried out to investigate the benefit of MT in a professional scenario of\ntranslating TV subtitles with a focus on how leveraging extra-textual context\nimpacts post-editing. We found that post-editors marked significantly fewer\ncontext-related errors when correcting the outputs of MTCue, the context-aware\nmodel, as opposed to non-contextual models. We also present the results of a\nsurvey of the employed post-editors, which highlights contextual inadequacy as\na significant gap consistently observed in MT. Our findings strengthen the\nmotivation for further work within fully contextual MT.",
      "tldr_zh": "该研究通过一个工业案例研究，探讨了在专业字幕翻译场景中，将额外文本上下文（如电影元数据）整合到机器翻译（MT）管道中的益处。结果显示，与非上下文模型相比，使用上下文感知模型（MTCue）的输出，后期编辑者标记的上下文相关错误显著减少。调查反馈进一步强调了MT在上下文不足方面的主要问题，这些发现强化了对开发完全上下文感知MT的动机。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to EAMT 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.00108v1",
      "published_date": "2024-06-27 11:20:14 UTC",
      "updated_date": "2024-06-27 11:20:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:23:54.454744"
    },
    {
      "arxiv_id": "2406.19087v2",
      "title": "Dimensions underlying the representational alignment of deep neural networks with humans",
      "title_zh": "翻译失败",
      "authors": [
        "Florian P. Mahner",
        "Lukas Muttenthaler",
        "Umut Güçlü",
        "Martin N. Hebart"
      ],
      "abstract": "Determining the similarities and differences between humans and artificial\nintelligence (AI) is an important goal both in computational cognitive\nneuroscience and machine learning, promising a deeper understanding of human\ncognition and safer, more reliable AI systems. Much previous work comparing\nrepresentations in humans and AI has relied on global, scalar measures to\nquantify their alignment. However, without explicit hypotheses, these measures\nonly inform us about the degree of alignment, not the factors that determine\nit. To address this challenge, we propose a generic framework to compare human\nand AI representations, based on identifying latent representational dimensions\nunderlying the same behavior in both domains. Applying this framework to humans\nand a deep neural network (DNN) model of natural images revealed a\nlow-dimensional DNN embedding of both visual and semantic dimensions. In\ncontrast to humans, DNNs exhibited a clear dominance of visual over semantic\nproperties, indicating divergent strategies for representing images. While\nin-silico experiments showed seemingly consistent interpretability of DNN\ndimensions, a direct comparison between human and DNN representations revealed\nsubstantial differences in how they process images. By making representations\ndirectly comparable, our results reveal important challenges for\nrepresentational alignment and offer a means for improving their comparability.",
      "tldr_zh": "本研究探讨了深度神经网络(DNNs)与人类表示对齐背后的潜在维度，提出一个通用框架，通过识别支持相同行为的潜在表示维度来比较人类和AI的表示。应用该框架分析人类和DNN模型后，发现DNNs的嵌入包含视觉和语义维度，但视觉属性占主导，与人类更平衡的处理策略形成对比。尽管模拟实验显示DNN维度的可解释性一致，但直接比较揭示了二者在图像处理上的显著差异。该框架揭示了表示对齐的挑战，并为改进AI系统的可比性和可靠性提供重要方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19087v2",
      "published_date": "2024-06-27 11:14:14 UTC",
      "updated_date": "2025-01-27 13:19:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:24:08.338929"
    },
    {
      "arxiv_id": "2407.00107v2",
      "title": "WineGraph: A Graph Representation For Food-Wine Pairing",
      "title_zh": "翻译失败",
      "authors": [
        "Zuzanna Gawrysiak",
        "Agata Żywot",
        "Agnieszka Ławrynowicz"
      ],
      "abstract": "We present WineGraph, an extended version of FlavorGraph, a heterogeneous\ngraph incorporating wine data into its structure. This integration enables\nfood-wine pairing based on taste and sommelier-defined rules. Leveraging a food\ndataset comprising 500,000 reviews and a wine reviews dataset with over 130,000\nentries, we computed taste descriptors for both food and wine. This information\nwas then utilised to pair food items with wine and augment FlavorGraph with\nadditional data. The results demonstrate the potential of heterogeneous graphs\nto acquire supplementary information, proving beneficial for wine pairing.",
      "tldr_zh": "本研究提出了 WineGraph，这是一个扩展版的 FlavorGraph 异构图（heterogeneous graph），将酒数据整合进来，用于基于口味描述符（taste descriptors）和侍酒师规则进行食物-酒配对。研究利用了包含50,000条食物评论和超过130,000条酒评论的数据集，计算食物和酒的口味特征，并以此增强图结构以实现配对功能。结果表明，WineGraph 通过获取补充信息，显著提高了食物-酒配对的潜力，为异构图在味觉相关应用中的应用提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00107v2",
      "published_date": "2024-06-27 11:11:19 UTC",
      "updated_date": "2024-07-11 12:12:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:24:19.816657"
    },
    {
      "arxiv_id": "2406.19071v2",
      "title": "EmPO: Emotion Grounding for Empathetic Response Generation through Preference Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Ondrej Sotolar",
        "Vojtech Formanek",
        "Alok Debnath",
        "Allison Lahnala",
        "Charles Welch",
        "Lucie FLek"
      ],
      "abstract": "Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. We propose a novel approach where we\nconstruct theory-driven preference datasets based on emotion grounding and use\nthem to align LLMs with preference optimization algorithms to address these\nchallenges. To evaluate empathetic response generation, we employ the\nEmpatheticDialogues dataset, assessing empathy with the diff-Epitome and\nBERTscore metrics and with multi-dimensional human evaluation. Additionally, we\nmeasure diversity and emotional valence using feature-based methods. We also\nevaluate the impact of training on the generalization performance using the\nMMLU benchmark and tasks from the Open LLM Leaderboard. The results show that\nLLMs can be aligned for empathetic response generation by preference\noptimization while retaining their general performance and that emotion\ngrounding can guide preference dataset creation. We make all datasets, source\ncode, and models publicly available. https://github.com/justtherightsize/empo",
      "tldr_zh": "本研究提出EmPO方法，通过情感基础(emotion grounding)构建理论驱动的偏好数据集，并使用偏好优化算法(preference optimization)对齐大型语言模型(LLMs)，以提升对话代理的同理心响应生成，同时保持模型的泛化性能。EmPO解决了现有LLMs在确保响应同理心质量和泛化能力方面的挑战，并通过EmpatheticDialogues数据集进行评估，使用diff-Epitome、BERTscore和人为多维度指标衡量同理心、多样性和情感价。实验结果表明，该方法显著改善了同理心响应生成，同时在MMLU基准和Open LLM Leaderboard任务中保留了模型的整体性能，所有数据集、源代码和模型已公开。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "v02, 8 pages long paper, EMNLP ACL style",
      "pdf_url": "http://arxiv.org/pdf/2406.19071v2",
      "published_date": "2024-06-27 10:41:22 UTC",
      "updated_date": "2024-09-17 14:24:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:24:34.737284"
    },
    {
      "arxiv_id": "2407.00106v1",
      "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
      "title_zh": "翻译失败",
      "authors": [
        "Ilia Shumailov",
        "Jamie Hayes",
        "Eleni Triantafillou",
        "Guillermo Ortiz-Jimenez",
        "Nicolas Papernot",
        "Matthew Jagielski",
        "Itay Yona",
        "Heidi Howard",
        "Eugene Bagdasaryan"
      ],
      "abstract": "Exact unlearning was first introduced as a privacy mechanism that allowed a\nuser to retract their data from machine learning models on request. Shortly\nafter, inexact schemes were proposed to mitigate the impractical costs\nassociated with exact unlearning. More recently unlearning is often discussed\nas an approach for removal of impermissible knowledge i.e. knowledge that the\nmodel should not possess such as unlicensed copyrighted, inaccurate, or\nmalicious information. The promise is that if the model does not have a certain\nmalicious capability, then it cannot be used for the associated malicious\npurpose. In this paper we revisit the paradigm in which unlearning is used for\nin Large Language Models (LLMs) and highlight an underlying inconsistency\narising from in-context learning. Unlearning can be an effective control\nmechanism for the training phase, yet it does not prevent the model from\nperforming an impermissible act during inference. We introduce a concept of\nununlearning, where unlearned knowledge gets reintroduced in-context,\neffectively rendering the model capable of behaving as if it knows the\nforgotten knowledge. As a result, we argue that content filtering for\nimpermissible knowledge will be required and even exact unlearning schemes are\nnot enough for effective content regulation. We discuss feasibility of\nununlearning for modern LLMs and examine broader implications.",
      "tldr_zh": "本论文质疑了在先进生成式 AI（如大型语言模型LLMs）中，unlearning 技术是否足以实现内容调控。作者指出，虽然 unlearning（包括 exact unlearning 和 inexact schemes）可用于训练阶段移除不适当知识（如未授权版权或恶意信息），但它无法防止模型通过 in-context learning 在推理阶段重新获取这些知识。论文引入了“ununlearning”的概念，描述了已unlearned 知识如何通过上下文被重新引入，从而使模型恢复原本的能力。因此，作者认为即使采用 exact unlearning，也需结合 content filtering 机制来有效调控内容，并讨论了这一现象在现代LLMs中的可行性及其更广泛的影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00106v1",
      "published_date": "2024-06-27 10:24:35 UTC",
      "updated_date": "2024-06-27 10:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:24:44.857049"
    },
    {
      "arxiv_id": "2406.19057v2",
      "title": "Segment Anything Model for automated image data annotation: empirical studies using text prompts from Grounding DINO",
      "title_zh": "Segment Anything Model 用于自动图像数据标注：利用来自 Grounding DINO 的文本提示进行的实证研究",
      "authors": [
        "Fuseini Mumuni",
        "Alhassan Mumuni"
      ],
      "abstract": "Grounding DINO and the Segment Anything Model (SAM) have achieved impressive\nperformance in zero-shot object detection and image segmentation, respectively.\nTogether, they have a great potential to revolutionize applications in\nzero-shot semantic segmentation or data annotation. Yet, in specialized domains\nlike medical image segmentation, objects of interest (e.g., organs, tissues,\nand tumors) may not fall in existing class names. To address this problem, the\nreferring expression comprehension (REC) ability of Grounding DINO is leveraged\nto detect arbitrary targets by their language descriptions. However, recent\nstudies have highlighted severe limitation of the REC framework in this\napplication setting owing to its tendency to make false positive predictions\nwhen the target is absent in the given image. And, while this bottleneck is\ncentral to the prospect of open-set semantic segmentation, it is still largely\nunknown how much improvement can be achieved by studying the prediction errors.\nTo this end, we perform empirical studies on six publicly available datasets\nacross different domains and reveal that these errors consistently follow a\npredictable pattern and can, thus, be mitigated by a simple strategy.\nSpecifically, we show that false positive detections with appreciable\nconfidence scores generally occupy large image areas and can usually be\nfiltered by their relative sizes. More importantly, we expect these\nobservations to inspire future research in improving REC-based detection and\nautomated segmentation. Meanwhile, we evaluate the performance of SAM on\nmultiple datasets from various specialized domains and report significant\nimprovements in segmentation performance and annotation time savings over\nmanual approaches.",
      "tldr_zh": "该论文探讨了使用 Segment Anything Model (SAM) 结合 Grounding DINO 的文本提示进行自动化图像数据标注的实证研究，旨在解决零样本语义分割中的挑战。研究发现，Grounding DINO 的 referring expression comprehension (REC) 框架容易产生假阳性预测，尤其在目标对象缺失时，但这些错误具有可预测的模式，通常涉及占据大图像区域的检测。作者提出简单策略，通过过滤检测对象的相对大小来缓解这些问题，并在六个公开数据集上验证了其有效性。同时，实验结果显示 SAM 在医疗和专业领域数据集上显著提升了分割性能，并大大减少了手动标注时间。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19057v2",
      "published_date": "2024-06-27 10:08:29 UTC",
      "updated_date": "2024-06-30 07:54:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:24:56.222670"
    },
    {
      "arxiv_id": "2407.09547v1",
      "title": "Predicting Depression and Anxiety Risk in Dutch Neighborhoods from Street-View Images",
      "title_zh": "翻译失败",
      "authors": [
        "Nin Khodorivsko",
        "Giacomo Spigler"
      ],
      "abstract": "Depression and anxiety disorders are prevalent mental health challenges\naffecting a substantial segment of the global population. In this study, we\nexplored the environmental correlates of these disorders by analyzing\nstreet-view images (SVI) of neighborhoods in the Netherlands. Our dataset\ncomprises 9,879 Dutch SVIs sourced from Google Street View, paired with\nstatistical depression and anxiety risk metrics from the Dutch Health Monitor.\nTo tackle this challenge, we refined two existing neural network architectures,\nDeiT Base and ResNet50. Our goal was to predict neighborhood risk levels,\ncategorized into four tiers from low to high risk, using the raw images. The\nresults showed that DeiT Base and ResNet50 achieved accuracies of 43.43% and\n43.63%, respectively. Notably, a significant portion of the errors were between\nadjacent risk categories, resulting in adjusted accuracies of 83.55% and\n80.38%. We also implemented the SHapley Additive exPlanations (SHAP) method on\nboth models and employed gradient rollout on DeiT. Interestingly, while SHAP\nunderscored specific landscape attributes, the correlation between these\nfeatures and distinct depression risk categories remained unclear. The gradient\nrollout findings were similarly non-definitive. However, through manual\nanalysis, we identified certain landscape types that were consistently linked\nwith specific risk categories. These findings suggest the potential of these\ntechniques in monitoring the correlation between various landscapes and\nenvironmental risk factors for mental health issues. As a future direction, we\nrecommend employing these methods to observe how risk scores from the Dutch\nHealth Monitor shift across neighborhoods over time.",
      "tldr_zh": "本研究利用街景图像（Street-View Images）预测荷兰邻里地区的抑郁和焦虑风险，使用9,879张来自Google Street View的图像与荷兰健康监测数据配对。研究改进了DeiT Base和ResNet50神经网络模型，对风险水平（分为低到高四个类别）进行预测，分别实现了43.43%和43.63%的准确率，但调整后准确率高达83.55%和80.38%，因为错误主要发生在相邻类别之间。采用SHAP（SHapley Additive exPlanations）和gradient rollout方法进行模型解释，但结果不明确，通过手动分析发现某些景观类型与特定风险类别相关。这些发现突显了街景图像在监测环境与心理健康风险相关性的潜力，并建议未来用于跟踪荷兰健康监测风险分数的变化。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.09547v1",
      "published_date": "2024-06-27 10:05:56 UTC",
      "updated_date": "2024-06-27 10:05:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:25:09.117667"
    },
    {
      "arxiv_id": "2406.19054v1",
      "title": "A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE)",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Sonntag",
        "Michael Barz",
        "Thiago Gouvêa"
      ],
      "abstract": "This DFKI technical report presents the anatomy of the No-IDLE prototype\nsystem (funded by the German Federal Ministry of Education and Research) that\nprovides not only basic and fundamental research in interactive machine\nlearning, but also reveals deeper insights into users' behaviours, needs, and\ngoals. Machine learning and deep learning should become accessible to millions\nof end users. No-IDLE's goals and scienfific challenges centre around the\ndesire to increase the reach of interactive deep learning solutions for\nnon-experts in machine learning. One of the key innovations described in this\ntechnical report is a methodology for interactive machine learning combined\nwith multimodal interaction which will become central when we start interacting\nwith semi-intelligent machines in the upcoming area of neural networks and\nlarge language models.",
      "tldr_zh": "本报告介绍了 No-IDLE 原型系统，这是一个由德国联邦教育部和研究部资助的框架，旨在通过交互式机器学习（Interactive Machine Learning）增强深度学习（Deep Learning）的可访问性，让非专家用户也能轻松使用。系统不仅进行基本研究，还深入分析用户的行为、需求和目标，以扩展交互式深度学习解决方案的覆盖范围。关键创新包括将交互式机器学习与多模态交互（Multimodal Interaction）相结合的方法，这将为未来与神经网络和大型语言模型互动的时代提供重要基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "DFKI Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2406.19054v1",
      "published_date": "2024-06-27 10:01:56 UTC",
      "updated_date": "2024-06-27 10:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:25:19.710576"
    },
    {
      "arxiv_id": "2406.19050v1",
      "title": "FedMap: Iterative Magnitude-Based Pruning for Communication-Efficient Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Herzog",
        "Robbie Southam",
        "Ioannis Mavromatis",
        "Aftab Khan"
      ],
      "abstract": "Federated Learning (FL) is a distributed machine learning approach that\nenables training on decentralized data while preserving privacy. However, FL\nsystems often involve resource-constrained client devices with limited\ncomputational power, memory, storage, and bandwidth. This paper introduces\nFedMap, a novel method that aims to enhance the communication efficiency of FL\ndeployments by collaboratively learning an increasingly sparse global model\nthrough iterative, unstructured pruning. Importantly, FedMap trains a global\nmodel from scratch, unlike other methods reported in the literature, making it\nideal for privacy-critical use cases such as in the medical and finance\ndomains, where suitable pre-training data is often limited. FedMap adapts\niterative magnitude-based pruning to the FL setting, ensuring all clients prune\nand refine the same subset of the global model parameters, therefore gradually\nreducing the global model size and communication overhead. The iterative nature\nof FedMap, forming subsequent models as subsets of predecessors, avoids\nparameter reactivation issues seen in prior work, resulting in stable\nperformance. In this paper we provide an extensive evaluation of FedMap across\ndiverse settings, datasets, model architectures, and hyperparameters, assessing\nperformance in both IID and non-IID environments. Comparative analysis against\nthe baseline approach demonstrates FedMap's ability to achieve more stable\nclient model performance. For IID scenarios, FedMap achieves over $90$\\%\npruning without significant performance degradation. In non-IID settings, it\nachieves at least $~80$\\% pruning while maintaining accuracy. FedMap offers a\npromising solution to alleviate communication bottlenecks in FL systems while\nretaining model accuracy.",
      "tldr_zh": "本研究提出FedMap，一种基于迭代幅度剪枝(Iterative Magnitude-Based Pruning)的创新方法，旨在提升Federated Learning (FL) 的通信效率，通过在去中心化数据上协作训练一个越来越稀疏的全局模型。不同于现有方法，FedMap从零开始训练全局模型，确保所有客户端剪枝和精炼相同的参数子集，从而逐步减少模型大小和通信开销，同时避免参数重新激活问题以实现性能稳定。该方法特别适合隐私关键领域，如医疗和金融。实验结果显示，在IID环境中，FedMap可实现超过90%的剪枝而不显著降低性能；在non-IID环境中，至少80%的剪枝同时保持准确性，提供了一个缓解FL通信瓶颈的可靠解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems",
      "pdf_url": "http://arxiv.org/pdf/2406.19050v1",
      "published_date": "2024-06-27 09:58:43 UTC",
      "updated_date": "2024-06-27 09:58:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:25:33.122706"
    },
    {
      "arxiv_id": "2406.19049v1",
      "title": "Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation",
      "title_zh": "翻译失败",
      "authors": [
        "Amartya Sanyal",
        "Yaxi Hu",
        "Yaodong Yu",
        "Yian Ma",
        "Yixin Wang",
        "Bernhard Schölkopf"
      ],
      "abstract": "\"Accuracy-on-the-line\" is a widely observed phenomenon in machine learning,\nwhere a model's accuracy on in-distribution (ID) and out-of-distribution (OOD)\ndata is positively correlated across different hyperparameters and data\nconfigurations. But when does this useful relationship break down? In this\nwork, we explore its robustness. The key observation is that noisy data and the\npresence of nuisance features can be sufficient to shatter the\nAccuracy-on-the-line phenomenon. In these cases, ID and OOD accuracy can become\nnegatively correlated, leading to \"Accuracy-on-the-wrong-line\". This phenomenon\ncan also occur in the presence of spurious (shortcut) features, which tend to\novershadow the more complex signal (core, non-spurious) features, resulting in\na large nuisance feature space. Moreover, scaling to larger datasets does not\nmitigate this undesirable behavior and may even exacerbate it. We formally\nprove a lower bound on Out-of-distribution (OOD) error in a linear\nclassification model, characterizing the conditions on the noise and nuisance\nfeatures for a large OOD error. We finally demonstrate this phenomenon across\nboth synthetic and real datasets with noisy data and nuisance features.",
      "tldr_zh": "本研究探讨了机器学习中的“Accuracy-on-the-line”现象，即模型在分布内（ID）和分布外（OOD）数据的准确率通常正相关，但噪声数据和无关特征（nuisance features）可能导致这一关系崩溃，形成“Accuracy-on-the-wrong-line”，使ID和OOD准确率负相关。论文证明了在存在虚假（spurious）特征时，这种负面影响会放大，且扩展到更大数据集反而可能加剧问题；同时，通过线性分类模型的理论下界证明和合成及真实数据集实验，揭示了噪声和无关特征对OOD泛化性能的破坏性影响。该工作为理解机器学习模型的泛化陷阱提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.19049v1",
      "published_date": "2024-06-27 09:57:31 UTC",
      "updated_date": "2024-06-27 09:57:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:25:49.398982"
    },
    {
      "arxiv_id": "2406.19048v2",
      "title": "BiCo-Fusion: Bidirectional Complementary LiDAR-Camera Fusion for Semantic- and Spatial-Aware 3D Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Song",
        "Lin Wang"
      ],
      "abstract": "3D object detection is an important task that has been widely applied in\nautonomous driving. To perform this task, a new trend is to fuse multi-modal\ninputs, i.e., LiDAR and camera. Under such a trend, recent methods fuse these\ntwo modalities by unifying them in the same 3D space. However, during direct\nfusion in a unified space, the drawbacks of both modalities (LiDAR features\nstruggle with detailed semantic information and the camera lacks accurate 3D\nspatial information) are also preserved, diluting semantic and spatial\nawareness of the final unified representation. To address the issue, this\nletter proposes a novel bidirectional complementary LiDAR-camera fusion\nframework, called BiCo-Fusion that can achieve robust semantic- and\nspatial-aware 3D object detection. The key insight is to fuse LiDAR and camera\nfeatures in a bidirectional complementary way to enhance the semantic awareness\nof the LiDAR and the 3D spatial awareness of the camera. The enhanced features\nfrom both modalities are then adaptively fused to build a semantic- and\nspatial-aware unified representation. Specifically, we introduce Pre-Fusion\nconsisting of a Voxel Enhancement Module (VEM) to enhance the semantic\nawareness of voxel features from 2D camera features and Image Enhancement\nModule (IEM) to enhance the 3D spatial awareness of camera features from 3D\nvoxel features. We then introduce Unified Fusion (U-Fusion) to adaptively fuse\nthe enhanced features from the last stage to build a unified representation.\nExtensive experiments demonstrate the superiority of our BiCo-Fusion against\nthe prior arts. Project page: https://t-ys.github.io/BiCo-Fusion/.",
      "tldr_zh": "这篇论文提出了 BiCo-Fusion，一种双向互补 LiDAR-相机融合框架，用于提升 3D 对象检测的语义和空间感知，解决直接融合时 LiDAR 缺少详细语义信息以及相机缺乏准确 3D 空间信息的问题。框架的核心方法包括 Pre-Fusion 中的 Voxel Enhancement Module (VEM) 来增强 LiDAR 体素特征的语义感知，以及 Image Enhancement Module (IEM) 来提升相机特征的 3D 空间感知。最终，通过 Unified Fusion (U-Fusion) 自适应融合这些增强特征，构建语义和空间感知的统一表示；实验结果表明，BiCo-Fusion 在 3D 对象检测任务上优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.19048v2",
      "published_date": "2024-06-27 09:56:38 UTC",
      "updated_date": "2024-12-01 07:07:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:00.194037"
    },
    {
      "arxiv_id": "2406.19043v2",
      "title": "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Zi Wang",
        "Fanwen Wang",
        "Chen Qin",
        "Jun Lyu",
        "Cheng Ouyang",
        "Shuo Wang",
        "Yan Li",
        "Mengyao Yu",
        "Haoyu Zhang",
        "Kunyuan Guo",
        "Zhang Shi",
        "Qirong Li",
        "Ziqiang Xu",
        "Yajing Zhang",
        "Hao Li",
        "Sha Hua",
        "Binghua Chen",
        "Longyu Sun",
        "Mengting Sun",
        "Qin Li",
        "Ying-Hua Chu",
        "Wenjia Bai",
        "Jing Qin",
        "Xiahai Zhuang",
        "Claudia Prieto",
        "Alistair Young",
        "Michael Markl",
        "He Wang",
        "Lianming Wu",
        "Guang Yang",
        "Xiaobo Qu",
        "Chengyan Wang"
      ],
      "abstract": "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse\npublicly available cardiac k-space dataset. It is acquired from 330 healthy\nvolunteers, covering commonly used modalities, anatomical views, and\nacquisition trajectories in clinical cardiac MRI workflows. Besides, an open\nplatform with tutorials, benchmarks, and data processing tools is provided to\nfacilitate data usage, advanced method development, and fair performance\nevaluation.",
      "tldr_zh": "这篇论文介绍了 CMRxRecon2024 数据集，这是一个多模态、多视图的 k-space 数据集，旨在通过丰富的数据资源推动机器学习在加速心脏 MRI 中的应用，以解决图像重建的挑战。数据集从 330 名健康志愿者中采集，涵盖临床心脏 MRI 常用模态、解剖视图和采集轨迹，是目前最大的协议多样化公共 k-space 数据集。论文还提供了一个开放平台，包括教程、基准和数据处理工具，支持先进方法的开发、公平性能评估，并促进通用框架在临床实践中的快速、鲁棒重建。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.DB"
      ],
      "primary_category": "eess.IV",
      "comment": "23 pages, 3 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.19043v2",
      "published_date": "2024-06-27 09:50:20 UTC",
      "updated_date": "2025-01-16 06:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:10.801724"
    },
    {
      "arxiv_id": "2407.13013v1",
      "title": "FernUni LLM Experimental Infrastructure (FLEXI) -- Enabling Experimentation and Innovation in Higher Education Through Access to Open Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Torsten Zesch",
        "Michael Hanses",
        "Niels Seidel",
        "Piush Aggarwal",
        "Dirk Veiel",
        "Claudia de Witt"
      ],
      "abstract": "Using the full potential of LLMs in higher education is hindered by\nchallenges with access to LLMs. The two main access modes currently discussed\nare paying for a cloud-based LLM or providing a locally maintained open LLM. In\nthis paper, we describe the current state of establishing an open LLM\ninfrastructure at FernUniversit\\\"at in Hagen under the project name FLEXI\n(FernUni LLM Experimental Infrastructure). FLEXI enables experimentation within\nteaching and research with the goal of generating strongly needed evidence in\nfavor (or against) the use of locally maintained open LLMs in higher education.\nThe paper will provide some practical guidance for everyone trying to decide\nwhether to run their own LLM server.",
      "tldr_zh": "该论文介绍了 FLEXI（FernUni LLM Experimental Infrastructure），一个旨在解决高等教育中访问大型语言模型（LLM）的挑战的开放基础设施。FLEXI 通过在 FernUniversität in Hagen 建立本地维护的开放 LLM 系统，启用教学和研究领域的实验，从而生成证据支持或反对使用这种方法的有效性。论文还提供实用指导，帮助机构决定是否运行自己的 LLM 服务器，以促进教育创新。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.13013v1",
      "published_date": "2024-06-27 09:46:11 UTC",
      "updated_date": "2024-06-27 09:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:21.873825"
    },
    {
      "arxiv_id": "2406.19015v3",
      "title": "Gaussian process-based online health monitoring and fault analysis of lithium-ion battery systems from field data",
      "title_zh": "翻译失败",
      "authors": [
        "Joachim Schaeffer",
        "Eric Lenz",
        "Duncan Gulla",
        "Martin Z. Bazant",
        "Richard D. Braatz",
        "Rolf Findeisen"
      ],
      "abstract": "Health monitoring, fault analysis, and detection are critical for the safe\nand sustainable operation of battery systems. We apply Gaussian process\nresistance models on lithium iron phosphate battery field data to effectively\nseparate the time-dependent and operating point-dependent resistance. The data\nset contains 29 battery systems returned to the manufacturer for warranty, each\nwith eight cells in series, totaling 232 cells and 131 million data rows. We\ndevelop probabilistic fault detection rules using recursive spatiotemporal\nGaussian processes. These processes allow the quick processing of over a\nmillion data points, enabling advanced online monitoring and furthering the\nunderstanding of battery pack failure in the field. The analysis underlines\nthat often, only a single cell shows abnormal behavior or a knee point,\nconsistent with weakest-link failure for cells connected in series, amplified\nby local resistive heating. The results further the understanding of how\nbatteries degrade and fail in the field and demonstrate the potential of\nefficient online monitoring based on data. We open-source the code and publish\nthe large data set upon completion of the review of this article.",
      "tldr_zh": "本文使用 Gaussian process-based 电阻模型分析锂铁磷酸盐电池的现场数据，有效分离时间依赖和操作点依赖的电阻，并基于递归时空 Gaussian processes 开发概率故障检测规则，以支持快速处理超过百万数据点的在线监控。研究基于一个包含29个电池系统（232个电池和1.31亿数据行）的真实数据集，发现电池组失效通常仅由单个电池的异常行为或膝点引起，这与串联电池的最弱环节失效理论一致。结果加深了对电池退化与失效的理解，并展示了基于数据的在线监控潜力，同时开源了代码和数据集。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "stat.AP",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "This version is outdated. The final version is published as open\n  access journal article: https://doi.org/10.1016/j.xcrp.2024.102258",
      "pdf_url": "http://arxiv.org/pdf/2406.19015v3",
      "published_date": "2024-06-27 09:00:05 UTC",
      "updated_date": "2024-10-30 15:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:34.932172"
    },
    {
      "arxiv_id": "2407.00105v1",
      "title": "Multiple Kronecker RLS fusion-based link propagation for drug-side effect prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqing Qian",
        "Ziyu Zheng",
        "Prayag Tiwari",
        "Yijie Ding",
        "Quan Zou"
      ],
      "abstract": "Drug-side effect prediction has become an essential area of research in the\nfield of pharmacology. As the use of medications continues to rise, so does the\nimportance of understanding and mitigating the potential risks associated with\nthem. At present, researchers have turned to data-driven methods to predict\ndrug-side effects. Drug-side effect prediction is a link prediction problem,\nand the related data can be described from various perspectives. To process\nthese kinds of data, a multi-view method, called Multiple Kronecker RLS\nfusion-based link propagation (MKronRLSF-LP), is proposed. MKronRLSF-LP extends\nthe Kron-RLS by finding the consensus partitions and multiple graph Laplacian\nconstraints in the multi-view setting. Both of these multi-view settings\ncontribute to a higher quality result. Extensive experiments have been\nconducted on drug-side effect datasets, and our empirical results provide\nevidence that our approach is effective and robust.",
      "tldr_zh": "该论文提出了一种多视图方法 Multiple Kronecker RLS fusion-based link propagation（简称 MKronRLSF-LP），用于药物副作用预测问题，通过扩展 Kron-RLS 算法整合共识分区和多个 graph Laplacian 约束来处理多视角数据，从而提升预测准确性。相比传统方法，该框架能更好地融合不同数据视图，提供更高质量的结果。实验在药物副作用数据集上进行，证明了 MKronRLSF-LP 的有效性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Transactions on Machine Learning Research (TMLR 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.00105v1",
      "published_date": "2024-06-27 08:50:25 UTC",
      "updated_date": "2024-06-27 08:50:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:45.373471"
    },
    {
      "arxiv_id": "2406.18995v1",
      "title": "FedMLP: Federated Multi-Label Medical Image Classification under Task Heterogeneity",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaobin Sun",
        "Nannan Wu",
        "Junjie Shi",
        "Li Yu",
        "Xin Yang",
        "Kwang-Ting Cheng",
        "Zengqiang Yan"
      ],
      "abstract": "Cross-silo federated learning (FL) enables decentralized organizations to\ncollaboratively train models while preserving data privacy and has made\nsignificant progress in medical image classification. One common assumption is\ntask homogeneity where each client has access to all classes during training.\nHowever, in clinical practice, given a multi-label classification task,\nconstrained by the level of medical knowledge and the prevalence of diseases,\neach institution may diagnose only partial categories, resulting in task\nheterogeneity. How to pursue effective multi-label medical image classification\nunder task heterogeneity is under-explored. In this paper, we first formulate\nsuch a realistic label missing setting in the multi-label FL domain and propose\na two-stage method FedMLP to combat class missing from two aspects: pseudo\nlabel tagging and global knowledge learning. The former utilizes a warmed-up\nmodel to generate class prototypes and select samples with high confidence to\nsupplement missing labels, while the latter uses a global model as a teacher\nfor consistency regularization to prevent forgetting missing class knowledge.\nExperiments on two publicly-available medical datasets validate the superiority\nof FedMLP against the state-of-the-art both federated semi-supervised and noisy\nlabel learning approaches under task heterogeneity. Code is available at\nhttps://github.com/szbonaldo/FedMLP.",
      "tldr_zh": "该研究提出 FedMLP 方法，用于处理联邦学习 (FL) 中任务异质性的多标签医疗图像分类问题，其中每个机构可能仅诊断部分类别，导致标签缺失。FedMLP 采用两阶段策略：第一阶段通过预热模型生成类原型并选择高置信度样本进行伪标签补充；第二阶段使用全局模型作为教师模型进行一致性正则化，以防止遗忘缺失类知识。实验在两个公开医疗数据集上验证了 FedMLP 的优越性，其性能超过了现有的联邦半监督和噪声标签学习方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Early accepted by MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.18995v1",
      "published_date": "2024-06-27 08:36:43 UTC",
      "updated_date": "2024-06-27 08:36:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:26:58.686557"
    },
    {
      "arxiv_id": "2406.18992v3",
      "title": "Semi-supervised Concept Bottleneck Models",
      "title_zh": "半监督概念瓶颈模型",
      "authors": [
        "Lijie Hu",
        "Tianhao Huang",
        "Huanyi Xie",
        "Xilin Gong",
        "Chenyang Ren",
        "Zhengyu Hu",
        "Lu Yu",
        "Ping Ma",
        "Di Wang"
      ],
      "abstract": "Concept Bottleneck Models (CBMs) have garnered increasing attention due to\ntheir ability to provide concept-based explanations for black-box deep learning\nmodels while achieving high final prediction accuracy using human-like\nconcepts. However, the training of current CBMs is heavily dependent on the\nprecision and richness of the annotated concepts in the dataset. These concept\nlabels are typically provided by experts, which can be costly and require\nsignificant resources and effort. Additionally, concept saliency maps\nfrequently misalign with input saliency maps, causing concept predictions to\ncorrespond to irrelevant input features - an issue related to annotation\nalignment. To address these limitations, we propose a new framework called\nSSCBM (Semi-supervised Concept Bottleneck Model). Our SSCBM is suitable for\npractical situations where annotated data is scarce. By leveraging joint\ntraining on both labeled and unlabeled data and aligning the unlabeled data at\nthe concept level, we effectively solve these issues. We proposed a strategy to\ngenerate pseudo labels and an alignment loss. Experiments demonstrate that our\nSSCBM is both effective and efficient. With only 10% labeled data, our model's\nconcept and task accuracy on average across four datasets is only 2.44% and\n3.93% lower, respectively, compared to the best baseline in the fully\nsupervised learning setting.",
      "tldr_zh": "本文提出半监督概念瓶颈模型 (Semi-supervised Concept Bottleneck Models, SSCBM)，旨在解决传统概念瓶颈模型 (CBMs) 对精确概念标注的依赖问题，这些标注通常需专家提供且成本高昂，同时还存在概念显著性图与输入显著性图的对齐问题。SSCBM 通过在标注和未标注数据上进行联合训练、生成伪标签以及引入对齐损失，在概念级别上优化模型性能，使其适用于数据稀缺的实际场景。实验结果显示，使用仅 10% 标注数据时，SSCBM 在四个数据集上的概念准确率和任务准确率分别仅比完全监督基线低 2.44% 和 3.93%，证明了其高效性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.18992v3",
      "published_date": "2024-06-27 08:33:35 UTC",
      "updated_date": "2025-03-19 20:33:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:27:11.227590"
    },
    {
      "arxiv_id": "2406.18954v1",
      "title": "Alignment For Performance Improvement in Conversation Bots",
      "title_zh": "翻译失败",
      "authors": [
        "Raghav Garg",
        "Kapil Sharma",
        "Shrey Singla"
      ],
      "abstract": "This paper shows that alignment methods can achieve superior adherence to\nguardrails compared to instruction fine-tuning alone in conversational agents,\nalso known as bots, within predefined guidelines or 'guardrails'. It examines\ntraditional training approaches such as instruction fine-tuning and the recent\nadvancements in direct alignment methods like Identity Preference Optimization\n(IPO), and Kahneman-Tversky Optimization (KTO). The effectiveness of alignment\ntechniques both pre and post-instruction tuning is highlighted, illustrating\ntheir potential to optimize conversational bots in domains that require strict\nadherence to specified rules, such as customer care.",
      "tldr_zh": "这篇论文证明了在对话代理（conversation bots）中使用对齐方法（alignment methods）比单纯的指令微调（instruction fine-tuning）更能有效遵守预定义的守则（guardrails）。它比较了传统训练方法与新型对齐技术，如 Identity Preference Optimization (IPO) 和 Kahneman-Tversky Optimization (KTO)，并展示了这些技术在指令微调前后都能显著优化代理性能。研究强调，这种方法特别适用于需要严格规则的领域，如客户服务，从而提升对话代理的可靠性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18954v1",
      "published_date": "2024-06-27 07:36:25 UTC",
      "updated_date": "2024-06-27 07:36:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:27:21.833662"
    },
    {
      "arxiv_id": "2407.00104v2",
      "title": "Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Iván Matas",
        "Carmen Serrano",
        "Francisca Silva",
        "Amalia Serrano",
        "Tomás Toledo-Pastrana",
        "Begoña Acha"
      ],
      "abstract": "An AI tool has been developed to provide interpretable support for the\ndiagnosis of BCC via teledermatology, thus speeding up referrals and optimizing\nresource utilization. The interpretability is provided in two ways: on the one\nhand, the main BCC dermoscopic patterns are found in the image to justify the\nBCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM,\na clinically inspired visual explanation is developed where the relevant\nfeatures for diagnosis are located. Since there is no established ground truth\nfor BCC dermoscopic features, a standard reference is inferred from the\ndiagnosis of four dermatologists using an Expectation Maximization (EM) based\nalgorithm. The results demonstrate significant improvements in classification\naccuracy and interpretability, positioning this approach as a valuable tool for\nearly BCC detection and referral to dermatologists. The BCC/non-BCC\nclassification achieved an accuracy rate of 90%. For Clinically-inspired XAI\nresults, the detection of BCC patterns useful to clinicians reaches 99%\naccuracy. As for the Clinically-inspired Visual XAI results, the mean of the\nGrad-CAM normalized value within the manually segmented clinical features is\n0.57, while outside this region it is 0.16. This indicates that the model\nstruggles to accurately identify the regions of the BCC patterns. These results\nprove the ability of the AI tool to provide a useful explanation.",
      "tldr_zh": "本研究开发了一个基于专家注解的AI-Tool，用于提升BCC（基底细胞癌）诊断的解释性和可解释性，通过远程皮肤病学加速转诊并优化资源利用。工具采用两种方式提供解释：一是识别图像中的BCC皮损镜模式来支持BCC/Non-BCC分类；二是基于Grad-CAM的临床启发视觉解释，定位诊断相关特征，并使用Expectation Maximization (EM)算法从四位皮肤科医生的诊断中推断标准参考。实验结果显示，BCC/non-BCC分类准确率达到90%，BCC模式检测准确率达99%，而Clinically-inspired Visual XAI在临床特征区域的Grad-CAM归一化值平均为0.57（外部为0.16），表明模型在关键区域识别方面虽有优势但仍面临挑战。该方法证明了AI-Tool在早期BCC检测和转诊中的潜在价值。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.IR",
        "eess.IV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures, 4 tables, under review",
      "pdf_url": "http://arxiv.org/pdf/2407.00104v2",
      "published_date": "2024-06-27 07:33:34 UTC",
      "updated_date": "2025-05-13 09:29:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:27:36.514297"
    },
    {
      "arxiv_id": "2407.00102v1",
      "title": "Curriculum Learning with Quality-Driven Data Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Biao Wu",
        "Fang Meng",
        "Ling Chen"
      ],
      "abstract": "The impressive multimodal capabilities demonstrated by OpenAI's GPT-4 have\ngenerated significant interest in the development of Multimodal Large Language\nModels (MLLMs). Visual instruction tuning of MLLMs with machine-generated\ninstruction-following data has shown to enhance zero-shot capabilities across\nvarious tasks. However, there has been limited exploration into controlling the\nquality of the instruction data.Current methodologies for data selection in\nMLLMs often rely on single, unreliable scores or use downstream tasks for\nselection, which is time-consuming and can lead to potential overfitting on the\nchosen evaluation datasets. To mitigate these limitations, we propose a novel\ndata selection methodology that utilizes image-text correlation and model\nperplexity to evaluate and select data of varying quality. This approach\nleverages the distinct distribution of these two attributes, mapping data\nquality into a two-dimensional space that allows for the selection of data\nbased on their location within this distribution. By utilizing this space, we\ncan analyze the impact of task type settings, used as prompts, on data quality.\nAdditionally, this space can be used to construct multi-stage subsets of\nvarying quality to facilitate curriculum learning. Our research includes\ncomprehensive experiments conducted on various datasets. The results emphasize\nsubstantial enhancements in five commonly assessed capabilities compared to\nusing the complete dataset. Our codes, data, and models are publicly available\nat: \\url{https://anonymous.4open.science/r/EHIT-31B4}",
      "tldr_zh": "本文提出了一种基于质量驱动的数据选择方法，用于提升 Multimodal Large Language Models (MLLMs) 的视觉指令微调效果。该方法利用图像-文本相关性和模型 perplexity 将数据映射到二维空间，根据分布位置评估并选择高质量数据，支持课程学习（curriculum learning）的多阶段子集构建。与现有依赖单一分数或下游任务的方法相比，这一策略避免了时间消耗和过拟合风险。实验在多种数据集上验证，显示在五个关键能力上取得了显著提升，相关代码、数据和模型已公开可用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00102v1",
      "published_date": "2024-06-27 07:20:36 UTC",
      "updated_date": "2024-06-27 07:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:27:49.928024"
    },
    {
      "arxiv_id": "2407.12017v2",
      "title": "Follow-Up Questions Improve Documents Generated by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bernadette J Tix"
      ],
      "abstract": "This study investigates the impact of Large Language Models (LLMs) generating\nfollow-up questions in response to user requests for short (1-page) text\ndocuments. Users interacted with a novel web-based AI system designed to ask\nfollow-up questions. Users requested documents they would like the AI to\nproduce. The AI then generated follow-up questions to clarify the user's needs\nor offer additional insights before generating the requested documents. After\nanswering the questions, users were shown a document generated using both the\ninitial request and the questions and answers, and a document generated using\nonly the initial request. Users indicated which document they preferred and\ngave feedback about their experience with the question-answering process. The\nfindings of this study show clear benefits to question-asking both in document\npreference and in the qualitative user experience. This study further shows\nthat users found more value in questions which were thought-provoking,\nopen-ended, or offered unique insights into the user's request as opposed to\nsimple information-gathering questions.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)通过生成后续问题来提升短文档生成质量的益处。研究设计了一个网络AI系统，让用户针对初始请求回答后续问题后，比较使用问题与答案生成的文档与仅用初始请求生成的文档。结果显示，用户更偏好包含后续问题的文档，并报告了更好的整体体验。特别地，用户更重视那些发人深省、开放式或提供独特见解的问题，而不是简单的澄清性问题。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12017v2",
      "published_date": "2024-06-27 07:16:46 UTC",
      "updated_date": "2024-08-15 07:12:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:28:01.347922"
    },
    {
      "arxiv_id": "2406.18944v4",
      "title": "Rethinking and Defending Protective Perturbation in Personalized Diffusion Models",
      "title_zh": "个性化扩散模型中保护性扰动的重新思考与防御",
      "authors": [
        "Yixin Liu",
        "Ruoxi Chen",
        "Xun Chen",
        "Lichao Sun"
      ],
      "abstract": "Personalized diffusion models (PDMs) have become prominent for adapting\npretrained text-to-image models to generate images of specific subjects using\nminimal training data. However, PDMs are susceptible to minor adversarial\nperturbations, leading to significant degradation when fine-tuned on corrupted\ndatasets. These vulnerabilities are exploited to create protective\nperturbations that prevent unauthorized image generation. Existing purification\nmethods attempt to mitigate this issue but often over-purify images, resulting\nin information loss. In this work, we conduct an in-depth analysis of the\nfine-tuning process of PDMs through the lens of shortcut learning. We\nhypothesize and empirically demonstrate that adversarial perturbations induce a\nlatent-space misalignment between images and their text prompts in the CLIP\nembedding space. This misalignment causes the model to erroneously associate\nnoisy patterns with unique identifiers during fine-tuning, resulting in poor\ngeneralization. Based on these insights, we propose a systematic defense\nframework that includes data purification and contrastive decoupling learning.\nWe first employ off-the-shelf image restoration techniques to realign images\nwith their original semantic meanings in latent space. Then, we introduce\ncontrastive decoupling learning with noise tokens to decouple the learning of\npersonalized concepts from spurious noise patterns. Our study not only uncovers\nfundamental shortcut learning vulnerabilities in PDMs but also provides a\ncomprehensive evaluation framework for developing stronger protection. Our\nextensive evaluation demonstrates its superiority over existing purification\nmethods and stronger robustness against adaptive perturbation.",
      "tldr_zh": "该研究重新审视了Personalized Diffusion Models (PDMs)对对抗扰动(adversarial perturbations)的易感性问题，这些扰动会导致模型在微调时因CLIP嵌入空间的潜在空间失调而出现shortcut learning漏洞，进而影响图像生成性能和泛化能力。作者通过深入分析证明，这种失调使模型错误地将噪声模式与唯一标识符关联，并提出一个系统防御框架，包括图像恢复技术进行数据净化以重新对齐图像语义，以及对比解耦学习(contrastive decoupling learning)使用噪声标记来分离个性化概念的学习和干扰噪声。实验结果显示，该框架比现有净化方法更有效，显著提升了模型的鲁棒性和保护能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Our code is available at\n  https://github.com/liuyixin-louis/DiffShortcut",
      "pdf_url": "http://arxiv.org/pdf/2406.18944v4",
      "published_date": "2024-06-27 07:14:14 UTC",
      "updated_date": "2024-10-03 03:35:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:28:15.056702"
    },
    {
      "arxiv_id": "2406.18939v1",
      "title": "Evaluating AI Group Fairness: a Fuzzy Logic Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Emmanouil Krasanakis",
        "Symeon Papadopoulos"
      ],
      "abstract": "Artificial intelligence systems often address fairness concerns by evaluating\nand mitigating measures of group discrimination, for example that indicate\nbiases against certain genders or races. However, what constitutes group\nfairness depends on who is asked and the social context, whereas definitions\nare often relaxed to accept small deviations from the statistical constraints\nthey set out to impose. Here we decouple definitions of group fairness both\nfrom the context and from relaxation-related uncertainty by expressing them in\nthe axiomatic system of Basic fuzzy Logic (BL) with loosely understood\npredicates, like encountering group members. We then evaluate the definitions\nin subclasses of BL, such as Product or Lukasiewicz logics. Evaluation produces\ncontinuous instead of binary truth values by choosing the logic subclass and\ntruth values for predicates that reflect uncertain context-specific beliefs,\nsuch as stakeholder opinions gathered through questionnaires. Internally, it\nfollows logic-specific rules to compute the truth values of definitions. We\nshow that commonly held propositions standardize the resulting mathematical\nformulas and we transcribe logic and truth value choices to layperson terms, so\nthat anyone can answer them. We also use our framework to study several\nliterature definitions of algorithmic fairness, for which we rationalize\nprevious expedient practices that are non-probabilistic and show how to\nre-interpret their formulas and parameters in new contexts.",
      "tldr_zh": "这篇论文从模糊逻辑(Fuzzy Logic)的视角评估AI群体公平性，提出了一种基于Basic Fuzzy Logic (BL)公理系统的框架，将公平定义从具体社会背景和不确定性中分离，使用模糊谓词来处理如群体成员遇到的不确定情况。方法包括在BL的子类（如Product或Lukasiewicz logics）中评估定义，通过选择逻辑子类和谓词真值（如基于问卷的利益相关者意见）来生成连续真值，而不是二元结果，从而反映现实中的不确定性。论文还标准化了常见公平公式，并重新解释了文献中的算法公平定义，提供了一种灵活的、非概率方法来适应新上下文。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint, 32 pages, 7 figures, 2 theorems, 6 appendices",
      "pdf_url": "http://arxiv.org/pdf/2406.18939v1",
      "published_date": "2024-06-27 07:11:48 UTC",
      "updated_date": "2024-06-27 07:11:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:28:25.909018"
    },
    {
      "arxiv_id": "2406.18937v2",
      "title": "Federated Graph Semantic and Structural Learning",
      "title_zh": "联邦图语义与结构学习",
      "authors": [
        "Wenke Huang",
        "Guancheng Wan",
        "Mang Ye",
        "Bo Du"
      ],
      "abstract": "Federated graph learning collaboratively learns a global graph neural network\nwith distributed graphs, where the non-independent and identically distributed\nproperty is one of the major challenges. Most relative arts focus on\ntraditional distributed tasks like images and voices, incapable of graph\nstructures. This paper firstly reveals that local client distortion is brought\nby both node-level semantics and graph-level structure. First, for node-level\nsemantics, we find that contrasting nodes from distinct classes is beneficial\nto provide a well-performing discrimination. We pull the local node towards the\nglobal node of the same class and push it away from the global node of\ndifferent classes. Second, we postulate that a well-structural graph neural\nnetwork possesses similarity for neighbors due to the inherent adjacency\nrelationships. However, aligning each node with adjacent nodes hinders\ndiscrimination due to the potential class inconsistency. We transform the\nadjacency relationships into the similarity distribution and leverage the\nglobal model to distill the relation knowledge into the local model, which\npreserves the structural information and discriminability of the local model.\nEmpirical results on three graph datasets manifest the superiority of the\nproposed method over its counterparts.",
      "tldr_zh": "这篇论文探讨了Federated Graph Learning中的非独立同分布(non-IID)挑战，揭示了本地客户端失真(local client distortion)源于节点级语义(node-level semantics)和图级结构(graph-level structure)。作者提出了一种方法：通过对比学习拉近本地节点与相同类全局节点的距离，并推开不同类全局节点，以提升节点区分性；同时，将邻接关系转化为相似度分布，并利用全局模型进行知识蒸馏(distill)，以保留图神经网络(Graph Neural Network)的结构信息和区分能力。实验结果在三个图数据集上显示，该方法优于现有基准模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18937v2",
      "published_date": "2024-06-27 07:08:28 UTC",
      "updated_date": "2024-06-29 18:17:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:28:38.870995"
    },
    {
      "arxiv_id": "2407.12016v1",
      "title": "LLM-based Frameworks for API Argument Filling in Task-Oriented Conversational Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jisoo Mok",
        "Mohammad Kachuee",
        "Shuyang Dai",
        "Shayan Ray",
        "Tara Taghavi",
        "Sungroh Yoon"
      ],
      "abstract": "Task-orientated conversational agents interact with users and assist them via\nleveraging external APIs. A typical task-oriented conversational system can be\nbroken down into three phases: external API selection, argument filling, and\nresponse generation. The focus of our work is the task of argument filling,\nwhich is in charge of accurately providing arguments required by the selected\nAPI. Upon comprehending the dialogue history and the pre-defined API schema,\nthe argument filling task is expected to provide the external API with the\nnecessary information to generate a desirable agent action. In this paper, we\nstudy the application of Large Language Models (LLMs) for the problem of API\nargument filling task. Our initial investigation reveals that LLMs require an\nadditional grounding process to successfully perform argument filling,\ninspiring us to design training and prompting frameworks to ground their\nresponses. Our experimental results demonstrate that when paired with proposed\ntechniques, the argument filling performance of LLMs noticeably improves,\npaving a new way toward building an automated argument filling framework.",
      "tldr_zh": "该研究探讨了在任务导向对话系统中，使用大型语言模型（LLMs）来处理API参数填充（argument filling）任务，该任务涉及根据对话历史和API模式提供必要信息。研究发现，LLMs 需要额外的grounding过程来提升准确性，因此设计了专门的训练和提示框架来改进其响应。实验结果显示，这些框架显著提高了LLMs的参数填充性能，为构建自动化的参数填充系统开辟了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12016v1",
      "published_date": "2024-06-27 06:54:53 UTC",
      "updated_date": "2024-06-27 06:54:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:28:47.713863"
    },
    {
      "arxiv_id": "2407.09544v1",
      "title": "A Transformer-Based Multi-Stream Approach for Isolated Iranian Sign Language Recognition",
      "title_zh": "一种基于Transformer的多流方法，用于孤立伊朗手语识别",
      "authors": [
        "Ali Ghadami",
        "Alireza Taheri",
        "Ali Meghdari"
      ],
      "abstract": "Sign language is an essential means of communication for millions of people\naround the world and serves as their primary language. However, most\ncommunication tools are developed for spoken and written languages which can\ncause problems and difficulties for the deaf and hard of hearing community. By\ndeveloping a sign language recognition system, we can bridge this communication\ngap and enable people who use sign language as their main form of expression to\nbetter communicate with people and their surroundings. This recognition system\nincreases the quality of health services, improves public services, and creates\nequal opportunities for the deaf community. This research aims to recognize\nIranian Sign Language words with the help of the latest deep learning tools\nsuch as transformers. The dataset used includes 101 Iranian Sign Language words\nfrequently used in academic environments such as universities. The network used\nis a combination of early fusion and late fusion transformer encoder-based\nnetworks optimized with the help of genetic algorithm. The selected features to\ntrain this network include hands and lips key points, and the distance and\nangle between hands extracted from the sign videos. Also, in addition to the\ntraining model for the classes, the embedding vectors of words are used as\nmulti-task learning to have smoother and more efficient training. This model\nwas also tested on sentences generated from our word dataset using a windowing\ntechnique for sentence translation. Finally, the sign language training\nsoftware that provides real-time feedback to users with the help of the\ndeveloped model, which has 90.2% accuracy on test data, was introduced, and in\na survey, the effectiveness and efficiency of this type of sign language\nlearning software and the impact of feedback were investigated.",
      "tldr_zh": "这篇论文提出了一种基于Transformer的多流方法，用于识别孤立伊朗手语单词，旨在桥接听障人士的沟通障碍并提升他们的健康、教育和公共服务机会。方法结合了早融合和晚融合的Transformer编码器网络，通过遗传算法优化，并使用手部和嘴唇关键点、以及手部间距离和角度等特征从视频中提取数据。模型采用多任务学习，包括单词嵌入向量训练，并在句子级别通过窗口技术进行测试。最终，结果显示模型在测试数据上达到90.2%的准确率，并开发了提供实时反馈的手语训练软件，其有效性通过调查得到验证。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.09544v1",
      "published_date": "2024-06-27 06:54:25 UTC",
      "updated_date": "2024-06-27 06:54:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:29:01.289997"
    },
    {
      "arxiv_id": "2406.18930v1",
      "title": "Reasoning About Action and Change",
      "title_zh": "行动与变化的推理",
      "authors": [
        "Florence Dupin de Saint-Cyr",
        "Andreas Herzig",
        "Jérôme Lang",
        "Pierre Marquis"
      ],
      "abstract": "The purpose of this book is to provide an overview of AI research, ranging\nfrom basic work to interfaces and applications, with as much emphasis on\nresults as on current issues. It is aimed at an audience of master students and\nPh.D. students, and can be of interest as well for researchers and engineers\nwho want to know more about AI. The book is split into three volumes.",
      "tldr_zh": "本书旨在概述人工智能（AI）研究，从基础工作到接口和应用，强调关键成果和当前问题，特别聚焦于“Reasoning About Action and Change”等主题。内容针对硕士和博士学生设计，同时适合研究人员和工程师参考，以加深对AI的理解。书籍分为三卷，提供系统化的知识结构。",
      "categories": [
        "cs.AI",
        "cs.DM",
        "cs.LO",
        "cs.SC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18930v1",
      "published_date": "2024-06-27 06:53:28 UTC",
      "updated_date": "2024-06-27 06:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:29:14.023833"
    },
    {
      "arxiv_id": "2406.18924v1",
      "title": "Learning Pareto Set for Multi-Objective Continuous Robot Control",
      "title_zh": "翻译失败",
      "authors": [
        "Tianye Shu",
        "Ke Shang",
        "Cheng Gong",
        "Yang Nan",
        "Hisao Ishibuchi"
      ],
      "abstract": "For a control problem with multiple conflicting objectives, there exists a\nset of Pareto-optimal policies called the Pareto set instead of a single\noptimal policy. When a multi-objective control problem is continuous and\ncomplex, traditional multi-objective reinforcement learning (MORL) algorithms\nsearch for many Pareto-optimal deep policies to approximate the Pareto set,\nwhich is quite resource-consuming. In this paper, we propose a simple and\nresource-efficient MORL algorithm that learns a continuous representation of\nthe Pareto set in a high-dimensional policy parameter space using a single\nhypernet. The learned hypernet can directly generate various well-trained\npolicy networks for different user preferences. We compare our method with two\nstate-of-the-art MORL algorithms on seven multi-objective continuous robot\ncontrol problems. Experimental results show that our method achieves the best\noverall performance with the least training parameters. An interesting\nobservation is that the Pareto set is well approximated by a curved line or\nsurface in a high-dimensional parameter space. This observation will provide\ninsight for researchers to design new MORL algorithms.",
      "tldr_zh": "本文提出了一种高效的多目标强化学习(MORL)算法，用于多目标连续机器人控制问题。该算法通过使用单一 hypernet 在高维策略参数空间中学习 Pareto set 的连续表示，显著减少了资源消耗，并能直接生成适应不同用户偏好的策略网络。在七个多目标连续机器人控制任务上，与两种最先进 MORL 算法比较，结果显示本方法取得了最佳整体性能，且训练参数最少。同时，实验观察到 Pareto set 在高维参数空间中可被曲线或曲面良好近似，这为设计新 MORL 算法提供了重要洞见。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18924v1",
      "published_date": "2024-06-27 06:31:51 UTC",
      "updated_date": "2024-06-27 06:31:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:29:25.997916"
    },
    {
      "arxiv_id": "2407.00101v1",
      "title": "Hybrid Approach to Parallel Stochastic Gradient Descent",
      "title_zh": "针对并行随机梯度下降的混合方法",
      "authors": [
        "Aakash Sudhirbhai Vora",
        "Dhrumil Chetankumar Joshi",
        "Aksh Kantibhai Patel"
      ],
      "abstract": "Stochastic Gradient Descent is used for large datasets to train models to\nreduce the training time. On top of that data parallelism is widely used as a\nmethod to efficiently train neural networks using multiple worker nodes in\nparallel. Synchronous and asynchronous approach to data parallelism is used by\nmost systems to train the model in parallel. However, both of them have their\ndrawbacks. We propose a third approach to data parallelism which is a hybrid\nbetween synchronous and asynchronous approaches, using both approaches to train\nthe neural network. When the threshold function is selected appropriately to\ngradually shift all parameter aggregation from asynchronous to synchronous, we\nshow that in a given time period our hybrid approach outperforms both\nasynchronous and synchronous approaches.",
      "tldr_zh": "本论文针对随机梯度下降(Stochastic Gradient Descent)在大型数据集训练中的效率问题，分析了数据并行性(data parallelism)的同步(synchronous)和异步(asynchronous)方法各自的缺点。作者提出了一种混合(hybrid)方法，将同步和异步策略结合，通过适当选择阈值函数逐步从异步转向同步参数聚合。实验结果显示，这种混合方法在给定时间段内，性能优于纯同步或纯异步方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "cs.DC",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.00101v1",
      "published_date": "2024-06-27 06:28:30 UTC",
      "updated_date": "2024-06-27 06:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:29:37.011787"
    },
    {
      "arxiv_id": "2406.18922v2",
      "title": "Time Matters: Scaling Laws for Any Budget",
      "title_zh": "翻译失败",
      "authors": [
        "Itay Inbar",
        "Luke Sernau"
      ],
      "abstract": "A primary cost driver for training large models is wall-clock training time.\nWe show that popular time estimates based on FLOPs are poor estimates, and\nconstruct a more accurate proxy based on memory copies. This allows us to\naccurately estimate the training speed of a transformer model from its\nhyperparameters. Combined with a scaling law curve like Chinchilla, this allows\nus to accurately predict the final loss of a model from a simple equation. We\nshow that this expression is accurate across a wide range of model\nhyperparameter values, enabling us to analytically make architectural decisions\nand train models more efficiently. Crucially, this analysis predicts that in\ncontrast to existing literature, models should be wider rather than deeper, as\nthe benefits of speed outweigh the benefits of depth.",
      "tldr_zh": "这篇论文指出，基于FLOPs的训练时间估算不准确，并提出使用内存拷贝作为更精确的代理指标，以从模型超参数准确预测训练速度。结合Chinchilla缩放定律曲线，该方法能通过一个简单方程预测模型的最终损失，并在广泛的超参数值上表现出高准确性。研究的关键发现是，模型应设计得更宽而非更深，因为速度优势能显著提升训练效率和架构决策。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18922v2",
      "published_date": "2024-06-27 06:26:22 UTC",
      "updated_date": "2024-10-23 22:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:29:48.891095"
    },
    {
      "arxiv_id": "2406.18916v2",
      "title": "TrustUQA: A Trustful Framework for Unified Structured Data Question Answering",
      "title_zh": "TrustUQA：一个可信任的框架，用于统一结构化数据问答",
      "authors": [
        "Wen Zhang",
        "Long Jin",
        "Yushan Zhu",
        "Jiaoyan Chen",
        "Zhiwei Huang",
        "Junjie Wang",
        "Yin Hua",
        "Lei Liang",
        "Huajun Chen"
      ],
      "abstract": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA.",
      "tldr_zh": "该论文提出 TrustUQA，一种可信的框架，用于统一处理多种结构化数据（如表格和知识图谱）的自然语言问答 (QA)。TrustUQA 采用 Condition Graph (CG) 作为 LLM 友好的知识表示方法，并结合 Large Language Models (LLMs) 的二层查询机制和动态演示检索，提升了泛化能力和信任度。在 5 个基准测试中，该框架优于现有统一 QA 方法，并在特定数据类型数据集上达到 state-of-the-art 水平，同时展示了在混合和跨结构化数据 QA 任务上的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2406.18916v2",
      "published_date": "2024-06-27 06:13:05 UTC",
      "updated_date": "2024-12-13 15:15:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:30:02.242199"
    },
    {
      "arxiv_id": "2406.18900v1",
      "title": "The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges",
      "title_zh": "人工智能在教育测量的兴起：机遇和伦理挑战",
      "authors": [
        "Okan Bulut",
        "Maggie Beiting-Parrish",
        "Jodi M. Casabianca",
        "Sharon C. Slater",
        "Hong Jiao",
        "Dan Song",
        "Christopher M. Ormerod",
        "Deborah Gbemisola Fabiyi",
        "Rodica Ivan",
        "Cole Walsh",
        "Oscar Rios",
        "Joshua Wilson",
        "Seyma N. Yildirim-Erbasli",
        "Tarid Wongvorachan",
        "Joyce Xinle Liu",
        "Bin Tan",
        "Polina Morilova"
      ],
      "abstract": "The integration of artificial intelligence (AI) in educational measurement\nhas revolutionized assessment methods, enabling automated scoring, rapid\ncontent analysis, and personalized feedback through machine learning and\nnatural language processing. These advancements provide timely, consistent\nfeedback and valuable insights into student performance, thereby enhancing the\nassessment experience. However, the deployment of AI in education also raises\nsignificant ethical concerns regarding validity, reliability, transparency,\nfairness, and equity. Issues such as algorithmic bias and the opacity of AI\ndecision-making processes pose risks of perpetuating inequalities and affecting\nassessment outcomes. Responding to these concerns, various stakeholders,\nincluding educators, policymakers, and organizations, have developed guidelines\nto ensure ethical AI use in education. The National Council of Measurement in\nEducation's Special Interest Group on AI in Measurement and Education (AIME)\nalso focuses on establishing ethical standards and advancing research in this\narea. In this paper, a diverse group of AIME members examines the ethical\nimplications of AI-powered tools in educational measurement, explores\nsignificant challenges such as automation bias and environmental impact, and\nproposes solutions to ensure AI's responsible and effective use in education.",
      "tldr_zh": "本论文探讨了人工智能（AI）在教育测量领域的兴起及其带来的机遇和伦理挑战。AI 通过机器学习（machine learning）和自然语言处理（natural language processing）实现了自动化评分、内容分析和个性化反馈，提升了评估效率和学生表现洞察力。然而，这也引发了诸如算法偏见（algorithmic bias）、透明度不足和公平性问题，可能加剧不平等。论文由国家测量教育委员会的 AI 在测量和教育特别兴趣小组（AIME）成员撰写，分析了这些挑战如自动化偏见（automation bias）和环境影响，并提出指导原则和解决方案，以确保 AI 在教育中的负责任应用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "59 pages, 3 figures, a joint work of the Special Interest Group on\n  Artificial Intelligence in Measurement and Education (AIME) from the National\n  Council of Measurement in Education (NCME)",
      "pdf_url": "http://arxiv.org/pdf/2406.18900v1",
      "published_date": "2024-06-27 05:28:40 UTC",
      "updated_date": "2024-06-27 05:28:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:30:13.264949"
    },
    {
      "arxiv_id": "2406.18899v3",
      "title": "Autonomous Control of a Novel Closed Chain Five Bar Active Suspension via Deep Reinforcement Learning",
      "title_zh": "利用深度强化学习对新型闭链五杆主动悬架进行自治控制",
      "authors": [
        "Nishesh Singh",
        "Sidharth Ramesh",
        "Abhishek Shankar",
        "Jyotishka Duttagupta",
        "Leander Stephen D'Souza",
        "Sanjay Singh"
      ],
      "abstract": "Planetary exploration requires traversal in environments with rugged\nterrains. In addition, Mars rovers and other planetary exploration robots often\ncarry sensitive scientific experiments and components onboard, which must be\nprotected from mechanical harm. This paper deals with an active suspension\nsystem focused on chassis stabilisation and an efficient traversal method while\nencountering unavoidable obstacles. Soft Actor-Critic (SAC) was applied along\nwith Proportional Integral Derivative (PID) control to stabilise the chassis\nand traverse large obstacles at low speeds. The model uses the rover's distance\nfrom surrounding obstacles, the height of the obstacle, and the chassis'\norientation to actuate the control links of the suspension accurately.\nSimulations carried out in the Gazebo environment are used to validate the\nproposed active system.",
      "tldr_zh": "本研究提出了一种新型闭链五杆主动悬挂系统，用于行星探测机器人（如火星探测器）在崎岖地形中稳定底盘并高效穿越障碍，同时保护敏感科学组件。方法结合Soft Actor-Critic (SAC)深度强化学习算法与Proportional Integral Derivative (PID)控制，利用机器人与障碍物的距离、障碍物高度以及底盘方向作为输入，精确驱动悬挂连杆。模拟实验在Gazebo环境中验证了该系统，能够实现低速大障碍物穿越，并提升了整体稳定性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2.9"
      ],
      "primary_category": "cs.RO",
      "comment": "15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.18899v3",
      "published_date": "2024-06-27 05:27:39 UTC",
      "updated_date": "2024-07-04 04:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:30:25.895848"
    },
    {
      "arxiv_id": "2406.18898v2",
      "title": "360 in the Wild: Dataset for Depth Prediction and View Synthesis",
      "title_zh": "360 in the Wild：用于深度预测和视图合成的数据集",
      "authors": [
        "Kibaek Park",
        "Francois Rameau",
        "Jaesik Park",
        "In So Kweon"
      ],
      "abstract": "The large abundance of perspective camera datasets facilitated the emergence\nof novel learning-based strategies for various tasks, such as camera\nlocalization, single image depth estimation, or view synthesis. However,\npanoramic or omnidirectional image datasets, including essential information,\nsuch as pose and depth, are mostly made with synthetic scenes. In this work, we\nintroduce a large scale 360$^{\\circ}$ videos dataset in the wild. This dataset\nhas been carefully scraped from the Internet and has been captured from various\nlocations worldwide. Hence, this dataset exhibits very diversified environments\n(e.g., indoor and outdoor) and contexts (e.g., with and without moving\nobjects). Each of the 25K images constituting our dataset is provided with its\nrespective camera's pose and depth map. We illustrate the relevance of our\ndataset for two main tasks, namely, single image depth estimation and view\nsynthesis.",
      "tldr_zh": "该论文引入了“360 in the Wild”数据集，这是一个大规模的真实世界360度视频数据集，用于支持depth prediction和view synthesis任务。该数据集从互联网上收集了25K张图像，每个图像均附带相机位姿和深度图，涵盖全球多样化环境（如室内外）和场景（如有无移动物体）。与现有合成数据集不同，该数据集强调真实性，并通过实验证明了其在单图像深度估计和视图合成中的相关性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18898v2",
      "published_date": "2024-06-27 05:26:38 UTC",
      "updated_date": "2024-07-05 02:56:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:30:38.173355"
    },
    {
      "arxiv_id": "2407.00100v1",
      "title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoling Zhou",
        "Wei Ye",
        "Yidong Wang",
        "Chaoya Jiang",
        "Zhemg Lee",
        "Rui Xie",
        "Shikun Zhang"
      ],
      "abstract": "The emergence of in-context learning (ICL) enables large pre-trained language\nmodels (PLMs) to make predictions for unseen inputs without updating\nparameters. Despite its potential, ICL's effectiveness heavily relies on the\nquality, quantity, and permutation of demonstrations, commonly leading to\nsuboptimal and unstable performance. In this paper, we tackle this challenge\nfor the first time from the perspective of demonstration augmentation.\nSpecifically, we start with enriching representations of demonstrations by\nleveraging their deep feature distribution. We then theoretically reveal that\nwhen the number of augmented copies approaches infinity, the augmentation is\napproximately equal to a novel logit calibration mechanism integrated with\nspecific statistical properties. This insight results in a simple yet highly\nefficient method that significantly improves the average and worst-case\naccuracy across diverse PLMs and tasks. Moreover, our method effectively\nreduces performance variance among varying demonstrations, permutations, and\ntemplates, and displays the capability to address imbalanced class\ndistributions.",
      "tldr_zh": "该研究首次从演示增强角度提升 in-context learning (ICL)，针对大型预训练语言模型 (PLMs) 在演示质量、数量和排列依赖性导致的性能不稳定问题。作者提出一种隐式演示增强方法，通过利用演示的深层特征分布来丰富其表示，并理论证明当增强副本数量趋近无穷大时，该方法等效于一种结合特定统计特性的 logit calibration 机制。实验结果显示，该方法显著提高了多种 PLMs 和任务的平均及最坏情况准确率，同时减少了演示、排列和模板间的性能方差，并有效处理了不平衡类分布问题。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACL 2024 Main 19 pages,10 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.00100v1",
      "published_date": "2024-06-27 05:25:46 UTC",
      "updated_date": "2024-06-27 05:25:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:30:49.070822"
    },
    {
      "arxiv_id": "2407.12812v1",
      "title": "Building Understandable Messaging for Policy and Evidence Review (BUMPER) with AI",
      "title_zh": "翻译失败",
      "authors": [
        "Katherine A. Rosenfeld",
        "Maike Sonnewald",
        "Sonia J. Jindal",
        "Kevin A. McCarthy",
        "Joshua L. Proctor"
      ],
      "abstract": "We introduce a framework for the use of large language models (LLMs) in\nBuilding Understandable Messaging for Policy and Evidence Review (BUMPER). LLMs\nare proving capable of providing interfaces for understanding and synthesizing\nlarge databases of diverse media. This presents an exciting opportunity to\nsupercharge the translation of scientific evidence into policy and action,\nthereby improving livelihoods around the world. However, these models also pose\nchallenges related to access, trust-worthiness, and accountability. The BUMPER\nframework is built atop a scientific knowledge base (e.g., documentation, code,\nsurvey data) by the same scientists (e.g., individual contributor, lab,\nconsortium). We focus on a solution that builds trustworthiness through\ntransparency, scope-limiting, explicit-checks, and uncertainty measures. LLMs\nare rapidly being adopted and consequences are poorly understood. The framework\naddresses open questions regarding the reliability of LLMs and their use in\nhigh-stakes applications. We provide a worked example in health policy for a\nmodel designed to inform measles control programs. We argue that this framework\ncan facilitate accessibility of and confidence in scientific evidence for\npolicymakers, drive a focus on policy-relevance and translatability for\nresearchers, and ultimately increase and accelerate the impact of scientific\nknowledge used for policy decisions.",
      "tldr_zh": "本研究引入了BUMPER框架，利用大型语言模型(LLMs)来构建可理解的信息系统，帮助合成和翻译海量媒体数据库中的科学证据，以支持政策制定和行动。该框架建立在科学知识库（如文档、代码和调查数据）基础上，通过透明性、范围限制、显式检查以及不确定性措施来解决LLMs的访问、信任和责任性挑战。研究提供了一个应用于麻疹控制健康政策的实际示例，证明BUMPER能提升决策者的证据可访问性和信心，促进研究者关注政策相关性，并加速科学知识对政策决策的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.12812v1",
      "published_date": "2024-06-27 05:03:03 UTC",
      "updated_date": "2024-06-27 05:03:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:31:01.818614"
    },
    {
      "arxiv_id": "2407.01608v1",
      "title": "Deriva-ML: A Continuous FAIRness Approach to Reproducible Machine Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Li",
        "Carl Kesselman",
        "Mike D'Arch",
        "Michael Pazzani",
        "Benjamin Yizing Xu"
      ],
      "abstract": "Increasingly, artificial intelligence (AI) and machine learning (ML) are used\nin eScience applications [9]. While these approaches have great potential, the\nliterature has shown that ML-based approaches frequently suffer from results\nthat are either incorrect or unreproducible due to mismanagement or misuse of\ndata used for training and validating the models [12, 15]. Recognition of the\nnecessity of high-quality data for correct ML results has led to data-centric\nML approaches that shift the central focus from model development to creation\nof high-quality data sets to train and validate the models [14, 20]. However,\nthere are limited tools and methods available for data-centric approaches to\nexplore and evaluate ML solutions for eScience problems which often require\ncollaborative multidisciplinary teams working with models and data that will\nrapidly evolve as an investigation unfolds [1]. In this paper, we show how data\nmanagement tools based on the principle that all of the data for ML should be\nfindable, accessible, interoperable and reusable (i.e. FAIR [26]) can\nsignificantly improve the quality of data that is used for ML applications.\nWhen combined with best practices that apply these tools to the entire life\ncycle of an ML-based eScience investigation, we can significantly improve the\nability of an eScience team to create correct and reproducible ML solutions. We\npropose an architecture and implementation of such tools and demonstrate\nthrough two use cases how they can be used to improve ML-based eScience\ninvestigations.",
      "tldr_zh": "该论文探讨了机器学习(ML)模型在eScience应用中常因数据管理不当而导致结果错误或不可复现的问题，提出了一种基于FAIR原则（Findable, Accessible, Interoperable, Reusable）的Deriva-ML方法，强调数据-centric方法来提升数据质量。研究设计了一个架构和工具集，用于管理ML生命周期，确保数据在多学科团队协作中保持可发现、可访问和可重用。作者通过两个用例证明，该方法能显著提高eScience调查的正确性和可复现性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB",
        "cs.HC",
        "cs.SE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.01608v1",
      "published_date": "2024-06-27 04:42:29 UTC",
      "updated_date": "2024-06-27 04:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:31:13.001860"
    },
    {
      "arxiv_id": "2406.18884v1",
      "title": "Sequential three-way group decision-making for double hierarchy hesitant fuzzy linguistic term set",
      "title_zh": "翻译失败",
      "authors": [
        "Nanfang Luo",
        "Qinghua Zhang",
        "Qin Xie",
        "Yutai Wang",
        "Longjun Yin",
        "Guoyin Wang"
      ],
      "abstract": "Group decision-making (GDM) characterized by complexity and uncertainty is an\nessential part of various life scenarios. Most existing researches lack tools\nto fuse information quickly and interpret decision results for partially formed\ndecisions. This limitation is particularly noticeable when there is a need to\nimprove the efficiency of GDM. To address this issue, a novel multi-level\nsequential three-way decision for group decision-making (S3W-GDM) method is\nconstructed from the perspective of granular computing. This method\nsimultaneously considers the vagueness, hesitation, and variation of GDM\nproblems under double hierarchy hesitant fuzzy linguistic term sets (DHHFLTS)\nenvironment. First, for fusing information efficiently, a novel multi-level\nexpert information fusion method is proposed, and the concepts of expert\ndecision table and the extraction/aggregation of decision-leveled information\nbased on the multi-level granularity are defined. Second, the neighborhood\ntheory, outranking relation and regret theory (RT) are utilized to redesign the\ncalculations of conditional probability and relative loss function. Then, the\ngranular structure of DHHFLTS based on the sequential three-way decision (S3WD)\nis defined to improve the decision-making efficiency, and the decision-making\nstrategy and interpretation of each decision-level are proposed. Furthermore,\nthe algorithm of S3W-GDM is given. Finally, an illustrative example of\ndiagnosis is presented, and the comparative and sensitivity analysis with other\nmethods are performed to verify the efficiency and rationality of the proposed\nmethod.",
      "tldr_zh": "本研究针对群决策（GDM）的复杂性和不确定性，提出了一种新型的多级顺序三向决策群决策（S3W-GDM）方法，利用粒计算（granular computing）在双层级犹豫模糊语言术语集（DHHFLTS）环境中处理模糊性、犹豫性和变化性问题。方法包括多级专家信息融合机制、基于邻域理论（neighborhood theory）、排序关系（outranking relation）和遗憾理论（RT）的条件概率及相对损失函数重新设计，以及定义DHHFLTS的粒结构以提升决策效率。最终，通过诊断示例进行验证，并与其他方法进行比较和敏感性分析，结果表明该方法显著提高了GDM的效率和合理性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18884v1",
      "published_date": "2024-06-27 04:33:26 UTC",
      "updated_date": "2024-06-27 04:33:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:31:26.639918"
    },
    {
      "arxiv_id": "2406.18859v1",
      "title": "Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyu Yang",
        "Santhosh Cherian",
        "Slobodan Vucetic"
      ],
      "abstract": "Radiology reports are highly technical documents aimed primarily at\ndoctor-doctor communication. There has been an increasing interest in sharing\nthose reports with patients, necessitating providing them patient-friendly\nsimplifications of the original reports. This study explores the suitability of\nlarge language models in automatically generating those simplifications. We\nexamine the usefulness of chain-of-thought and self-correction prompting\nmechanisms in this domain. We also propose a new evaluation protocol that\nemploys radiologists and laypeople, where radiologists verify the factual\ncorrectness of simplifications, and laypeople assess simplicity and\ncomprehension. Our experimental results demonstrate the effectiveness of\nself-correction prompting in producing high-quality simplifications. Our\nfindings illuminate the preferences of radiologists and laypeople regarding\ntext simplification, informing future research on this topic.",
      "tldr_zh": "本研究探讨了使用ChatGPT等大型语言模型自动简化放射学报告，以使其更适合患者理解。研究考察了Chain-of-Thought和Self-Correction提示机制的效用，并提出了一种双重人类评估协议：由放射学家验证事实正确性，由普通人评估简单度和可理解性。实验结果表明，Self-Correction提示机制能显著提升简化报告的质量，并揭示了放射学家和普通人对文本简化的不同偏好，为未来放射报告简化研究提供重要指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18859v1",
      "published_date": "2024-06-27 03:05:35 UTC",
      "updated_date": "2024-06-27 03:05:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:31:37.357813"
    },
    {
      "arxiv_id": "2406.18856v1",
      "title": "FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus",
      "title_zh": "FFN: 细粒度的中英金融领域平行语料库",
      "authors": [
        "Yuxin Fu",
        "Shijing Si",
        "Leyi Mai",
        "Xi-ang Li"
      ],
      "abstract": "Large Language Models (LLMs) have stunningly advanced the field of machine\ntranslation, though their effectiveness within the financial domain remains\nlargely underexplored. To probe this issue, we constructed a fine-grained\nChinese-English parallel corpus of financial news called FFN. We acquired\nfinancial news articles spanning between January 1st, 2014, to December 31,\n2023, from mainstream media websites such as CNN, FOX, and China Daily. The\ndataset consists of 1,013 main text and 809 titles, all of which have been\nmanually corrected. We measured the translation quality of two LLMs -- ChatGPT\nand ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.\nFor comparison, we also trained an OpenNMT model based on our dataset. We\ndetail problems of LLMs and provide in-depth analysis, intending to stimulate\nfurther research and solutions in this largely uncharted territory. Our\nresearch underlines the need to optimize LLMs within the specific field of\nfinancial translation to ensure accuracy and quality.",
      "tldr_zh": "该论文构建了名为FFN的细粒度中文-英文金融领域平行语料库，旨在探索大型语言模型(LLMs)在金融翻译中的效能。研究团队从CNN、FOX和China Daily等主流媒体收集了2014年至2023年间的金融新闻文章，包括1013个正文和809个标题，并进行手动修正。实验评估了ChatGPT和ERNIE-bot的翻译质量，使用BLEU、TER和chrF指标进行衡量，并与基于该数据集训练的OpenNMT模型进行比较，结果揭示了LLMs在金融翻译中的问题，如准确性不足，并强调了针对该领域的优化需求，以推动进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "a simplified version of this paper is accepted by International\n  Conference on Asian Language Processing 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.18856v1",
      "published_date": "2024-06-27 02:53:55 UTC",
      "updated_date": "2024-06-27 02:53:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:31:49.776498"
    },
    {
      "arxiv_id": "2407.01606v1",
      "title": "On Discrete Prompt Optimization for Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruochen Wang",
        "Ting Liu",
        "Cho-Jui Hsieh",
        "Boqing Gong"
      ],
      "abstract": "This paper introduces the first gradient-based framework for prompt\noptimization in text-to-image diffusion models. We formulate prompt engineering\nas a discrete optimization problem over the language space. Two major\nchallenges arise in efficiently finding a solution to this problem: (1)\nEnormous Domain Space: Setting the domain to the entire language space poses\nsignificant difficulty to the optimization process. (2) Text Gradient:\nEfficiently computing the text gradient is challenging, as it requires\nbackpropagating through the inference steps of the diffusion model and a\nnon-differentiable embedding lookup table. Beyond the problem formulation, our\nmain technical contributions lie in solving the above challenges. First, we\ndesign a family of dynamically generated compact subspaces comprised of only\nthe most relevant words to user input, substantially restricting the domain\nspace. Second, we introduce \"Shortcut Text Gradient\" -- an effective\nreplacement for the text gradient that can be obtained with constant memory and\nruntime. Empirical evaluation on prompts collected from diverse sources\n(DiffusionDB, ChatGPT, COCO) suggests that our method can discover prompts that\nsubstantially improve (prompt enhancement) or destroy (adversarial attack) the\nfaithfulness of images generated by the text-to-image diffusion model.",
      "tldr_zh": "本论文提出了第一个基于梯度的框架，用于在文本到图像 diffusion models 中进行离散提示优化，将提示工程表述为语言空间上的离散优化问题。针对巨大的领域空间和文本梯度计算挑战，该框架设计了动态生成的紧凑子空间，仅包含与用户输入最相关的单词，并引入了“Shortcut Text Gradient”方法，以恒定内存和运行时高效计算梯度。实验结果显示，该方法能在来自 DiffusionDB、ChatGPT 和 COCO 的提示集上显著提升生成图像的保真度（提示增强）或实现对抗攻击（破坏图像）。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "stat.ML",
        "68T01"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024. Code available at\n  https://github.com/ruocwang/dpo-diffusion",
      "pdf_url": "http://arxiv.org/pdf/2407.01606v1",
      "published_date": "2024-06-27 02:53:01 UTC",
      "updated_date": "2024-06-27 02:53:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:32:02.273254"
    },
    {
      "arxiv_id": "2406.18851v1",
      "title": "LICO: Large Language Models for In-Context Molecular Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Tung Nguyen",
        "Aditya Grover"
      ],
      "abstract": "Optimizing black-box functions is a fundamental problem in science and\nengineering. To solve this problem, many approaches learn a surrogate function\nthat estimates the underlying objective from limited historical evaluations.\nLarge Language Models (LLMs), with their strong pattern-matching capabilities\nvia pretraining on vast amounts of data, stand out as a potential candidate for\nsurrogate modeling. However, directly prompting a pretrained language model to\nproduce predictions is not feasible in many scientific domains due to the\nscarcity of domain-specific data in the pretraining corpora and the challenges\nof articulating complex problems in natural language. In this work, we\nintroduce LICO, a general-purpose model that extends arbitrary base LLMs for\nblack-box optimization, with a particular application to the molecular domain.\nTo achieve this, we equip the language model with a separate embedding layer\nand prediction layer, and train the model to perform in-context predictions on\na diverse set of functions defined over the domain. Once trained, LICO can\ngeneralize to unseen molecule properties simply via in-context prompting. LICO\nachieves state-of-the-art performance on PMO, a challenging molecular\noptimization benchmark comprising over 20 objective functions.",
      "tldr_zh": "本论文提出LICO，一种通用模型，用于扩展Large Language Models (LLMs)以进行黑盒优化（black-box optimization），特别针对分子领域的应用。LICO通过添加独立的嵌入层和预测层，并训练模型在多样函数上进行in-context预测，从而克服LLMs在领域特定数据不足和问题表述挑战。实验结果显示，LICO在PMO基准测试中，在超过20个目标函数上实现了state-of-the-art性能，展示了其泛化能力和优化潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18851v1",
      "published_date": "2024-06-27 02:43:18 UTC",
      "updated_date": "2024-06-27 02:43:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:32:14.370080"
    },
    {
      "arxiv_id": "2407.12015v1",
      "title": "The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing",
      "title_zh": "翻译失败",
      "authors": [
        "Hilda Hadan",
        "Derrick Wang",
        "Reza Hadi Mogavi",
        "Joseph Tu",
        "Leah Zhang-Kennedy",
        "Lennart E. Nacke"
      ],
      "abstract": "Generative AI (GenAI) use in research writing is growing fast. However, it is\nunclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To\ninvestigate the impact of AI-augmented writing on peer reviews, we conducted a\nsnippet-based online survey with 17 peer reviewers from top-tier HCI\nconferences. Our findings indicate that while AI-augmented writing improves\nreadability, language diversity, and informativeness, it often lacks research\ndetails and reflective insights from authors. Reviewers consistently struggled\nto distinguish between human and AI-augmented writing but their judgements\nremained consistent. They noted the loss of a \"human touch\" and subjective\nexpressions in AI-augmented writing. Based on our findings, we advocate for\nreviewer guidelines that promote impartial evaluations of submissions,\nregardless of any personal biases towards GenAI. The quality of the research\nitself should remain a priority in reviews, regardless of any preconceived\nnotions about the tools used to create it. We emphasize that researchers must\nmaintain their authorship and control over the writing process, even when using\nGenAI's assistance.",
      "tldr_zh": "这篇论文调查了同行评审者对 Generative AI (GenAI) 在研究写作中的感知和误解，通过对 17 名顶级 HCI 会议评审者的基于片段的在线调查进行分析。研究发现，GenAI 增强的写作提高了可读性、语言多样性和信息性，但往往缺少研究细节、作者的反思性见解和“人类触感”，导致评审者难以准确区分人类与 AI 写作。作者强调，应制定评审者指南，促进对提交物的公正评估，优先考虑研究质量而非对 GenAI 的偏见，并要求研究者保持对写作过程的控制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.12015v1",
      "published_date": "2024-06-27 02:38:25 UTC",
      "updated_date": "2024-06-27 02:38:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:32:26.717089"
    },
    {
      "arxiv_id": "2406.18847v1",
      "title": "Learning Retrieval Augmentation for Personalized Dialogue Generation",
      "title_zh": "用于个性化对话生成的检索增强学习",
      "authors": [
        "Qiushi Huang",
        "Shuai Fu",
        "Xubo Liu",
        "Wenwu Wang",
        "Tom Ko",
        "Yu Zhang",
        "Lilian Tang"
      ],
      "abstract": "Personalized dialogue generation, focusing on generating highly tailored\nresponses by leveraging persona profiles and dialogue context, has gained\nsignificant attention in conversational AI applications. However, persona\nprofiles, a prevalent setting in current personalized dialogue datasets,\ntypically composed of merely four to five sentences, may not offer\ncomprehensive descriptions of the persona about the agent, posing a challenge\nto generate truly personalized dialogues. To handle this problem, we propose\n$\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for\n$\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration\n($\\textbf{LAPDOG}$), which studies the potential of leveraging external\nknowledge for persona dialogue generation. Specifically, the proposed LAPDOG\nmodel consists of a story retriever and a dialogue generator. The story\nretriever uses a given persona profile as queries to retrieve relevant\ninformation from the story document, which serves as a supplementary context to\naugment the persona profile. The dialogue generator utilizes both the dialogue\nhistory and the augmented persona profile to generate personalized responses.\nFor optimization, we adopt a joint training framework that collaboratively\nlearns the story retriever and dialogue generator, where the story retriever is\noptimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for\nthe dialogue generator to generate personalized responses. Experiments\nconducted on the CONVAI2 dataset with ROCStory as a supplementary data source\nshow that the proposed LAPDOG method substantially outperforms the baselines,\nindicating the effectiveness of the proposed method. The LAPDOG model code is\npublicly available for further exploration.\nhttps://github.com/hqsiswiliam/LAPDOG",
      "tldr_zh": "该研究针对个性化对话生成中的问题，即现有 persona profiles 过于简短（仅4-5句），难以提供全面描述，提出了一种学习检索增强方法——LAPDOG。LAPDOG 模型包括 story retriever 和 dialogue generator 组件，其中 story retriever 使用 persona profiles 作为查询，从外部故事文档（如 ROCStory）中检索相关信息，以增强 persona profiles；随后，dialogue generator 结合对话历史和增强后的 profiles 生成个性化响应。模型通过联合训练框架优化 story retriever（针对指标如 BLEU），在 CONVAI2 数据集上的实验显示，LAPDOG 显著优于基线模型，证明了其在提升对话个性化的有效性，且代码已开源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP-2023",
      "pdf_url": "http://arxiv.org/pdf/2406.18847v1",
      "published_date": "2024-06-27 02:38:13 UTC",
      "updated_date": "2024-06-27 02:38:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:32:37.784095"
    },
    {
      "arxiv_id": "2406.18845v1",
      "title": "Retain, Blend, and Exchange: A Quality-aware Spatial-Stereo Fusion Approach for Event Stream Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Lan Chen",
        "Dong Li",
        "Xiao Wang",
        "Pengpeng Shao",
        "Wei Zhang",
        "Yaowei Wang",
        "Yonghong Tian",
        "Jin Tang"
      ],
      "abstract": "Existing event stream-based pattern recognition models usually represent the\nevent stream as the point cloud, voxel, image, etc., and design various deep\nneural networks to learn their features. Although considerable results can be\nachieved in simple cases, however, the model performance may be limited by\nmonotonous modality expressions, sub-optimal fusion, and readout mechanisms. In\nthis paper, we propose a novel dual-stream framework for event stream-based\npattern recognition via differentiated fusion, termed EFV++. It models two\ncommon event representations simultaneously, i.e., event images and event\nvoxels. The spatial and three-dimensional stereo information can be learned\nseparately by utilizing Transformer and Graph Neural Network (GNN). We believe\nthe features of each representation still contain both efficient and redundant\nfeatures and a sub-optimal solution may be obtained if we directly fuse them\nwithout differentiation. Thus, we divide each feature into three levels and\nretain high-quality features, blend medium-quality features, and exchange\nlow-quality features. The enhanced dual features will be fed into the fusion\nTransformer together with bottleneck features. In addition, we introduce a\nnovel hybrid interaction readout mechanism to enhance the diversity of features\nas final representations. Extensive experiments demonstrate that our proposed\nframework achieves state-of-the-art performance on multiple widely used event\nstream-based classification datasets. Specifically, we achieve new\nstate-of-the-art performance on the Bullying10k dataset, i.e., $90.51\\%$, which\nexceeds the second place by $+2.21\\%$. The source code of this paper has been\nreleased on\n\\url{https://github.com/Event-AHU/EFV_event_classification/tree/EFVpp}.",
      "tldr_zh": "本论文提出了一种名为 EFV++ 的双流框架，用于事件流模式识别，旨在解决现有模型在模态表达、融合和读出机制上的局限性。该框架同时处理事件图像和事件体素，使用 Transformer 学习空间信息以及 Graph Neural Network (GNN) 学习三维立体信息，并通过质量感知融合方法（Retain, Blend, and Exchange）将特征分为高、中、低质量级别，分别保留、混合和交换特征，然后结合混合交互读出机制增强特征多样性。实验结果显示，EFV++ 在多个事件流分类数据集上达到最先进性能，尤其在 Bullying10k 数据集上准确率达 90.51%，比第二名高 2.21%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "In Peer Review, Journal Extension of PRCV 2023",
      "pdf_url": "http://arxiv.org/pdf/2406.18845v1",
      "published_date": "2024-06-27 02:32:46 UTC",
      "updated_date": "2024-06-27 02:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:32:51.626063"
    },
    {
      "arxiv_id": "2407.11017v1",
      "title": "Direct-Inverse Prompting: Analyzing LLMs' Discriminative Capacity in Self-Improving Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jihyun Janice Ahn",
        "Ryo Kamoi",
        "Lu Cheng",
        "Rui Zhang",
        "Wenpeng Yin"
      ],
      "abstract": "Mainstream LLM research has primarily focused on enhancing their generative\ncapabilities. However, even the most advanced LLMs experience uncertainty in\ntheir outputs, often producing varied results on different runs or when faced\nwith minor changes in input, despite no substantial change in content. Given\nmultiple responses from the same LLM to the same input, we advocate leveraging\nthe LLMs' discriminative capability to reduce this generative uncertainty,\naiding in identifying the correct answers. Specifically, we propose and analyze\nthree discriminative prompts: direct, inverse, and hybrid, to explore the\npotential of both closed-source and open-source LLMs in self-improving their\ngenerative performance on two benchmark datasets. Our insights reveal which\ndiscriminative prompt is most promising and when to use it. To our knowledge,\nthis is the first work to systematically analyze LLMs' discriminative capacity\nto address generative uncertainty.",
      "tldr_zh": "该论文探讨了大型语言模型（LLMs）的生成不确定性问题，提出通过LLMs的辨别能力来提升其自我改进生成性能。研究者设计了三种辨别提示（direct、inverse和hybrid），并在两个基准数据集上测试这些提示在闭源和开源LLMs中的效果，以减少输入微变导致的输出差异。实验结果显示，特定提示（如inverse）在某些场景下更具前景，并揭示了最佳使用时机；此工作首次系统分析LLMs的辨别能力，以辅助生成任务的可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.11017v1",
      "published_date": "2024-06-27 02:26:47 UTC",
      "updated_date": "2024-06-27 02:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:33:01.943567"
    },
    {
      "arxiv_id": "2406.18839v1",
      "title": "Disentangling Knowledge-based and Visual Reasoning by Question Decomposition in KB-VQA",
      "title_zh": "翻译失败",
      "authors": [
        "Elham J. Barezi",
        "Parisa Kordjamshidi"
      ],
      "abstract": "We study the Knowledge-Based visual question-answering problem, for which\ngiven a question, the models need to ground it into the visual modality to find\nthe answer. Although many recent works use question-dependent captioners to\nverbalize the given image and use Large Language Models to solve the VQA\nproblem, the research results show they are not reasonably performing for\nmulti-hop questions. Our study shows that replacing a complex question with\nseveral simpler questions helps to extract more relevant information from the\nimage and provide a stronger comprehension of it. Moreover, we analyze the\ndecomposed questions to find out the modality of the information that is\nrequired to answer them and use a captioner for the visual questions and LLMs\nas a general knowledge source for the non-visual KB-based questions. Our\nresults demonstrate the positive impact of using simple questions before\nretrieving visual or non-visual information. We have provided results and\nanalysis on three well-known VQA datasets including OKVQA, A-OKVQA, and KRVQA,\nand achieved up to 2% improvement in accuracy.",
      "tldr_zh": "这篇论文针对 Knowledge-Based Visual Question-Answering (KB-VQA) 问题，通过 Question Decomposition 将复杂问题分解成多个简单问题，以分离知识-based 和视觉推理。方法包括分析分解后的问题，根据其所需模态使用 captioner 处理视觉相关查询，并利用 Large Language Models (LLMs) 作为非视觉知识来源，从而从图像中提取更多相关信息并提升多跳问题(multi-hop questions)的理解。实验结果显示，在 OKVQA、A-OKVQA 和 KRVQA 数据集上，该方法将准确率提高了多达 2%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18839v1",
      "published_date": "2024-06-27 02:19:38 UTC",
      "updated_date": "2024-06-27 02:19:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:33:17.125684"
    },
    {
      "arxiv_id": "2407.15023v1",
      "title": "ViT LoS V2X: Vision Transformers for Environment-aware LoS Blockage Prediction for 6G Vehicular Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ghazi Gharsallah",
        "Georges Kaddoum"
      ],
      "abstract": "As wireless communication technology progresses towards the sixth generation\n(6G), high-frequency millimeter-wave (mmWave) communication has emerged as a\npromising candidate for enabling vehicular networks. It offers high data rates\nand low-latency communication. However, obstacles such as buildings, trees, and\nother vehicles can cause signal attenuation and blockage, leading to\ncommunication failures that can result in fatal accidents or traffic\ncongestion. Predicting blockages is crucial for ensuring reliable and efficient\ncommunications. Furthermore, the advent of 6G technology is anticipated to\nintegrate advanced sensing capabilities, utilizing a variety of sensor types.\nThese sensors, ranging from traditional RF sensors to cameras and Lidar\nsensors, are expected to provide access to rich multimodal data, thereby\nenriching communication systems with a wealth of additional contextual\ninformation. Leveraging this multimodal data becomes essential for making\nprecise network management decisions, including the crucial task of blockage\ndetection. In this paper, we propose a Deep Learning (DL)-based approach that\ncombines Convolutional Neural Networks (CNNs) and customized Vision\nTransformers (ViTs) to effectively extract essential information from\nmultimodal data and predict blockages in vehicular networks. Our method\ncapitalizes on the synergistic strengths of CNNs and ViTs to extract features\nfrom time-series multimodal data, which include images and beam vectors. To\ncapture temporal dependencies between the extracted features and the blockage\nstate at future time steps, we employ a Gated Recurrent Unit (GRU)-based\narchitecture. Our results show that the proposed approach achieves high\naccuracy and outperforms state-of-the-art solutions, achieving more than $95\\%$\naccurate predictions.",
      "tldr_zh": "该论文针对6G车联网中mmWave通信的LoS（Line-of-Sight）阻塞问题，提出了一种环境感知预测方法，以应对障碍物导致的信号衰减和通信故障。研究方法结合CNNs和自定义Vision Transformers (ViTs)从多模态数据（如图像和波束向量）中提取关键特征，并利用GRU（Gated Recurrent Unit）架构捕捉时间序列依赖，从而准确预测未来阻塞状态。该方法在实验中实现了超过95%的预测准确率，显著优于现有解决方案，为可靠的6G Vehicular Networks（V2X）通信提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.15023v1",
      "published_date": "2024-06-27 01:38:09 UTC",
      "updated_date": "2024-06-27 01:38:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:33:36.657304"
    },
    {
      "arxiv_id": "2406.18817v1",
      "title": "Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Zhao",
        "Jingen Jiang",
        "Lei Ma",
        "Shiqing Xin",
        "Gaofeng Meng",
        "Dong-Ming Yan"
      ],
      "abstract": "This paper presents a novel non-rigid point set registration method that is\ninspired by unsupervised clustering analysis. Unlike previous approaches that\ntreat the source and target point sets as separate entities, we develop a\nholistic framework where they are formulated as clustering centroids and\nclustering members, separately. We then adopt Tikhonov regularization with an\n$\\ell_1$-induced Laplacian kernel instead of the commonly used Gaussian kernel\nto ensure smooth and more robust displacement fields. Our formulation delivers\nclosed-form solutions, theoretical guarantees, independence from dimensions,\nand the ability to handle large deformations. Subsequently, we introduce a\nclustering-improved Nystr\\\"om method to effectively reduce the computational\ncomplexity and storage of the Gram matrix to linear, while providing a rigorous\nbound for the low-rank approximation. Our method achieves high accuracy results\nacross various scenarios and surpasses competitors by a significant margin,\nparticularly on shapes with substantial deformations. Additionally, we\ndemonstrate the versatility of our method in challenging tasks such as shape\ntransfer and medical registration.",
      "tldr_zh": "本论文提出了一种基于无监督聚类分析的Correspondence-Free Non-Rigid Point Set Registration方法，将源点集和目标点集分别视为聚类中心和聚类成员，形成一个整体框架。方法采用Tikhonov正则化结合$\\ell_1$-induced Laplacian内核来生成平滑且鲁棒的位移场，提供闭式解、理论保证、维度无关性，并能有效处理大变形。论文还引入了改进的Nyström方法，将Gram矩阵的计算复杂度和存储降低到线性级别，同时在各种场景中表现出高准确性，大幅超过竞争对手，尤其适用于形状转移和医疗配准等挑战任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "[CVPR 2024 Highlight] Project and code at:\n  https://github.com/zikai1/CVPR24_PointSetReg",
      "pdf_url": "http://arxiv.org/pdf/2406.18817v1",
      "published_date": "2024-06-27 01:16:44 UTC",
      "updated_date": "2024-06-27 01:16:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:33:48.804785"
    },
    {
      "arxiv_id": "2406.18814v3",
      "title": "Length Optimization in Conformal Prediction",
      "title_zh": "保形预测中的长度优化",
      "authors": [
        "Shayan Kiyani",
        "George Pappas",
        "Hamed Hassani"
      ],
      "abstract": "Conditional validity and length efficiency are two crucial aspects of\nconformal prediction (CP). Conditional validity ensures accurate uncertainty\nquantification for data subpopulations, while proper length efficiency ensures\nthat the prediction sets remain informative. Despite significant efforts to\naddress each of these issues individually, a principled framework that\nreconciles these two objectives has been missing in the CP literature. In this\npaper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel\nand practical framework that constructs prediction sets with (near-) optimal\nlength while ensuring conditional validity under various classes of covariate\nshifts, including the key cases of marginal and group-conditional coverage. In\nthe infinite sample regime, we provide strong duality results which indicate\nthat CPL achieves conditional validity and length optimality. In the finite\nsample regime, we show that CPL constructs conditionally valid prediction sets.\nOur extensive empirical evaluations demonstrate the superior prediction set\nsize performance of CPL compared to state-of-the-art methods across diverse\nreal-world and synthetic datasets in classification, regression, and large\nlanguage model-based multiple choice question answering. An Implementation of\nour algorithm can be accessed at the following link:\nhttps://github.com/shayankiyani98/CP.",
      "tldr_zh": "该研究针对 Conformal Prediction (CP) 中的条件有效性 (conditional validity) 和长度效率 (length efficiency) 问题，提出了一种新框架 Conformal Prediction with Length-Optimization (CPL)，旨在在各种协变量偏移下构建长度（近）最优的预测集，同时确保条件有效性。CPL 通过优化技术整合这两个目标，在无限样本情况下提供强双重性结果，证明其实现了条件有效性和长度最优；在有限样本情况下，展示了构建条件有效预测集的能力。实证评估在分类、回归以及基于大语言模型的多选题回答任务中表明，CPL 比现有方法在预测集大小方面表现出显著优势。代码实现可访问 https://github.com/shayankiyani98/CP。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18814v3",
      "published_date": "2024-06-27 01:08:04 UTC",
      "updated_date": "2024-12-11 18:48:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:34:01.711639"
    },
    {
      "arxiv_id": "2406.18812v1",
      "title": "A Survey on Privacy Attacks Against Digital Twin Systems in AI-Robotics",
      "title_zh": "翻译失败",
      "authors": [
        "Ivan A. Fernandez",
        "Subash Neupane",
        "Trisha Chakraborty",
        "Shaswata Mitra",
        "Sudip Mittal",
        "Nisha Pillai",
        "Jingdao Chen",
        "Shahram Rahimi"
      ],
      "abstract": "Industry 4.0 has witnessed the rise of complex robots fueled by the\nintegration of Artificial Intelligence/Machine Learning (AI/ML) and Digital\nTwin (DT) technologies. While these technologies offer numerous benefits, they\nalso introduce potential privacy and security risks. This paper surveys privacy\nattacks targeting robots enabled by AI and DT models. Exfiltration and data\nleakage of ML models are discussed in addition to the potential extraction of\nmodels derived from first-principles (e.g., physics-based). We also discuss\ndesign considerations with DT-integrated robotics touching on the impact of ML\nmodel training, responsible AI and DT safeguards, data governance and ethical\nconsiderations on the effectiveness of these attacks. We advocate for a trusted\nautonomy approach, emphasizing the need to combine robotics, AI, and DT\ntechnologies with robust ethical frameworks and trustworthiness principles for\nsecure and reliable AI robotic systems.",
      "tldr_zh": "这篇论文对AI-机器人领域中针对Digital Twin (DT)系统的隐私攻击进行了全面调查，重点讨论了AI/ML模型的数据外泄、模型提取以及基于第一原理（如物理模型）的潜在风险。论文分析了设计考虑因素，包括ML模型训练的影响、负责任的AI和DT保护措施、数据治理以及伦理问题，这些因素对攻击有效性的影响。最终，论文倡导trusted autonomy方法，强调将机器人、AI和DT技术与稳健的伦理框架和信任原则相结合，以构建安全可靠的AI机器人系统。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "10 pages, 3 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2406.18812v1",
      "published_date": "2024-06-27 00:59:20 UTC",
      "updated_date": "2024-06-27 00:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:34:12.930135"
    },
    {
      "arxiv_id": "2407.11015v1",
      "title": "Does ChatGPT Have a Mind?",
      "title_zh": "ChatGPT 是否拥有心智？",
      "authors": [
        "Simon Goldstein",
        "Benjamin A. Levinstein"
      ],
      "abstract": "This paper examines the question of whether Large Language Models (LLMs) like\nChatGPT possess minds, focusing specifically on whether they have a genuine\nfolk psychology encompassing beliefs, desires, and intentions. We approach this\nquestion by investigating two key aspects: internal representations and\ndispositions to act. First, we survey various philosophical theories of\nrepresentation, including informational, causal, structural, and teleosemantic\naccounts, arguing that LLMs satisfy key conditions proposed by each. We draw on\nrecent interpretability research in machine learning to support these claims.\nSecond, we explore whether LLMs exhibit robust dispositions to perform actions,\na necessary component of folk psychology. We consider two prominent\nphilosophical traditions, interpretationism and representationalism, to assess\nLLM action dispositions. While we find evidence suggesting LLMs may satisfy\nsome criteria for having a mind, particularly in game-theoretic environments,\nwe conclude that the data remains inconclusive. Additionally, we reply to\nseveral skeptical challenges to LLM folk psychology, including issues of\nsensory grounding, the \"stochastic parrots\" argument, and concerns about\nmemorization. Our paper has three main upshots. First, LLMs do have robust\ninternal representations. Second, there is an open question to answer about\nwhether LLMs have robust action dispositions. Third, existing skeptical\nchallenges to LLM representation do not survive philosophical scrutiny.",
      "tldr_zh": "本论文探讨大型语言模型（LLMs）如ChatGPT是否拥有心智，特别是是否具备包括信念、欲望和意图在内的民间心理学（folk psychology）。作者通过分析内部表示和行动倾向，考察了信息、因果、结构和目的语义（informational, causal, structural, and teleosemantic accounts）等哲学理论，并结合机器学习的可解释性研究，评估LLMs在博弈论环境中的表现。研究发现，LLMs确实具有稳健的内部表示，但其行动倾向是否稳健仍是个开放问题，同时现有怀疑论如感官 grounding、“stochastic parrots”论和记忆化问题经不起哲学 scrutiny。总的来说，这为LLMs的心智辩论提供了新的见解，强调了需要进一步探究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.11015v1",
      "published_date": "2024-06-27 00:21:16 UTC",
      "updated_date": "2024-06-27 00:21:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:34:25.540618"
    },
    {
      "arxiv_id": "2406.18802v2",
      "title": "All Random Features Representations are Equivalent",
      "title_zh": "所有随机特征表示都是等价的",
      "authors": [
        "Luke Sernau",
        "Silvano Bonacina",
        "Rif A. Saurous"
      ],
      "abstract": "Random features are a powerful technique for rewriting positive-definite\nkernels as linear products. They bring linear tools to bear in important\nnonlinear domains like KNNs and attention. Unfortunately, practical\nimplementations require approximating an expectation, usually via sampling.\nThis has led to the development of increasingly elaborate representations with\never lower sample error. We resolve this arms race by deriving an optimal\nsampling policy. Under this policy all random features representations have the\nsame approximation error, which we show is the lowest possible. This means that\nwe are free to choose whatever representation we please, provided we sample\noptimally.",
      "tldr_zh": "这篇论文证明了所有 random features 表示在最优采样策略下是等效的，用于将 positive-definite kernels 重写为线性乘积，从而应用于非线性领域如 KNNs 和 attention。作者推导出了一个最优采样政策，确保所有表示的近似误差相同，并达到最低可能水平。结果表明，研究者可以自由选择任何 random features 表示，前提是采用该最优采样策略，从而简化了实际实现过程。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18802v2",
      "published_date": "2024-06-27 00:21:10 UTC",
      "updated_date": "2024-10-23 23:04:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:34:37.522481"
    },
    {
      "arxiv_id": "2406.18800v2",
      "title": "Infinite Width Models That Work: Why Feature Learning Doesn't Matter as Much as You Think",
      "title_zh": "有效的无限宽度模型：为什么特征学习没有你想象的那么重要",
      "authors": [
        "Luke Sernau"
      ],
      "abstract": "Common infinite-width architectures such as Neural Tangent Kernels (NTKs)\nhave historically shown weak performance compared to finite models. This is\nusually attributed to the absence of feature learning. We show that this\nexplanation is insufficient. Specifically, we show that infinite width NTKs\nobviate the need for feature learning. They can learn identical behavior by\nselecting relevant subfeatures from their (infinite) frozen feature vector.\nFurthermore, we show experimentally that NTKs under-perform traditional finite\nmodels even when feature learning is artificially disabled. Instead, we show\nthat weak performance is at least partly due to the fact that existing\nconstructions depend on weak optimizers like SGD. We provide a new infinite\nwidth limit based on ADAM-like learning dynamics and demonstrate empirically\nthat the resulting models erase this performance gap.",
      "tldr_zh": "该论文挑战了传统观点，认为无限宽度模型如 Neural Tangent Kernels (NTKs) 的弱表现并非主要由于缺少特征学习。作者证明，NTKs 可以从其无限的冻结特征向量中选择相关子特征，从而实现相同的学习行为，而无需动态特征学习。实验结果显示，即使禁用特征学习，NTKs 仍因依赖弱优化器如 SGD 而表现不佳；为此，他们提出了一种基于 ADAM-like 学习动态的新无限宽度极限，并证明此方法显著消除了 NTKs 与传统有限模型的性能差距。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18800v2",
      "published_date": "2024-06-27 00:15:54 UTC",
      "updated_date": "2024-10-23 23:08:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T01:34:59.806031"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 125,
  "processed_papers_count": 125,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T01:35:22.389211"
}
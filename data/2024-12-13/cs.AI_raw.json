[
  {
    "arxiv_id": "2412.12192v1",
    "title": "No Free Lunch for Defending Against Prefilling Attack by In-Context Learning",
    "authors": [
      "Zhiyu Xue",
      "Guangliang Liu",
      "Bocheng Chen",
      "Kristen Marie Johnson",
      "Ramtin Pedarsani"
    ],
    "abstract": "The security of Large Language Models (LLMs) has become an important research\ntopic since the emergence of ChatGPT. Though there have been various effective\nmethods to defend against jailbreak attacks, prefilling attacks remain an\nunsolved and popular threat against open-sourced LLMs. In-Context Learning\n(ICL) offers a computationally efficient defense against various jailbreak\nattacks, yet no effective ICL methods have been developed to counter prefilling\nattacks. In this paper, we: (1) show that ICL can effectively defend against\nprefilling jailbreak attacks by employing adversative sentence structures\nwithin demonstrations; (2) characterize the effectiveness of this defense\nthrough the lens of model size, number of demonstrations, over-defense,\nintegration with other jailbreak attacks, and the presence of safety alignment.\nGiven the experimental results and our analysis, we conclude that there is no\nfree lunch for defending against prefilling jailbreak attacks with ICL. On the\none hand, current safety alignment methods fail to mitigate prefilling\njailbreak attacks, but adversative structures within ICL demonstrations provide\nrobust defense across various model sizes and complex jailbreak attacks. On the\nother hand, LLMs exhibit similar over-defensiveness when utilizing ICL\ndemonstrations with adversative structures, and this behavior appears to be\nindependent of model size.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12192v1",
    "published_date": "2024-12-13 23:58:12 UTC",
    "updated_date": "2024-12-13 23:58:12 UTC"
  },
  {
    "arxiv_id": "2412.17832v1",
    "title": "MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes",
    "authors": [
      "Jiaqing Zhang",
      "Miguel Contreras",
      "Sabyasachi Bandyopadhyay",
      "Andrea Davidson",
      "Jessica Sena",
      "Yuanfang Ren",
      "Ziyuan Guan",
      "Tezcan Ozrazgat-Baslanti",
      "Tyler J. Loftus",
      "Subhash Nerella",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "abstract": "Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to\nensure timely and appropriate interventions. Advances in artificial\nintelligence (AI) technologies have significantly improved the accuracy of\nacuity predictions. However, prior studies using machine learning for acuity\nprediction have predominantly relied on electronic health records (EHR) data,\noften overlooking other critical aspects of ICU stay, such as patient mobility,\nenvironmental factors, and facial cues indicating pain or agitation. To address\nthis gap, we present MANGO: the Multimodal Acuity traNsformer for intelliGent\nICU Outcomes, designed to enhance the prediction of patient acuity states,\ntransitions, and the need for life-sustaining therapy. We collected a\nmultimodal dataset ICU-Multimodal, incorporating four key modalities, EHR data,\nwearable sensor data, video of patient's facial cues, and ambient sensor data,\nwhich we utilized to train MANGO. The MANGO model employs a multimodal feature\nfusion network powered by Transformer masked self-attention method, enabling it\nto capture and learn complex interactions across these diverse data modalities\neven when some modalities are absent. Our results demonstrated that integrating\nmultiple modalities significantly improved the model's ability to predict\nacuity status, transitions, and the need for life-sustaining therapy. The\nbest-performing models achieved an area under the receiver operating\ncharacteristic curve (AUROC) of 0.76 (95% CI: 0.72-0.79) for predicting\ntransitions in acuity status and the need for life-sustaining therapy, while\n0.82 (95% CI: 0.69-0.89) for acuity status prediction...",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.17832v1",
    "published_date": "2024-12-13 23:51:15 UTC",
    "updated_date": "2024-12-13 23:51:15 UTC"
  },
  {
    "arxiv_id": "2412.10605v2",
    "title": "Client-Side Patching against Backdoor Attacks in Federated Learning",
    "authors": [
      "Borja Molina-Coronado"
    ],
    "abstract": "Federated learning is a versatile framework for training models in\ndecentralized environments. However, the trust placed in clients makes\nfederated learning vulnerable to backdoor attacks launched by malicious\nparticipants. While many defenses have been proposed, they often fail short\nwhen facing heterogeneous data distributions among participating clients. In\nthis paper, we propose a novel defense mechanism for federated learning systems\ndesigned to mitigate backdoor attacks on the clients-side. Our approach\nleverages adversarial learning techniques and model patching to neutralize the\nimpact of backdoor attacks. Through extensive experiments on the MNIST and\nFashion-MNIST datasets, we demonstrate that our defense effectively reduces\nbackdoor accuracy, outperforming existing state-of-the-art defenses, such as\nLFighter, FLAME, and RoseAgg, in i.i.d. and non-i.i.d. scenarios, while\nmaintaining competitive or superior accuracy on clean data.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10605v2",
    "published_date": "2024-12-13 23:17:10 UTC",
    "updated_date": "2024-12-20 09:59:28 UTC"
  },
  {
    "arxiv_id": "2412.10599v1",
    "title": "Advances in Transformers for Robotic Applications: A Review",
    "authors": [
      "Nikunj Sanghai",
      "Nik Bear Brown"
    ],
    "abstract": "The introduction of Transformers architecture has brought about significant\nbreakthroughs in Deep Learning (DL), particularly within Natural Language\nProcessing (NLP). Since their inception, Transformers have outperformed many\ntraditional neural network architectures due to their \"self-attention\"\nmechanism and their scalability across various applications. In this paper, we\ncover the use of Transformers in Robotics. We go through recent advances and\ntrends in Transformer architectures and examine their integration into robotic\nperception, planning, and control for autonomous systems. Furthermore, we\nreview past work and recent research on use of Transformers in Robotics as\npre-trained foundation models and integration of Transformers with Deep\nReinforcement Learning (DRL) for autonomous systems. We discuss how different\nTransformer variants are being adapted in robotics for reliable planning and\nperception, increasing human-robot interaction, long-horizon decision-making,\nand generalization. Finally, we address limitations and challenges, offering\ninsight and suggestions for future research directions.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Early preprint, focusing primarily on general purpose robots, more\n  updates to come",
    "pdf_url": "http://arxiv.org/pdf/2412.10599v1",
    "published_date": "2024-12-13 23:02:15 UTC",
    "updated_date": "2024-12-13 23:02:15 UTC"
  },
  {
    "arxiv_id": "2412.12190v1",
    "title": "iMoT: Inertial Motion Transformer for Inertial Navigation",
    "authors": [
      "Son Minh Nguyen",
      "Linh Duy Tran",
      "Duc Viet Le",
      "Paul J. M Havinga"
    ],
    "abstract": "We propose iMoT, an innovative Transformer-based inertial odometry method\nthat retrieves cross-modal information from motion and rotation modalities for\naccurate positional estimation. Unlike prior work, during the encoding of the\nmotion context, we introduce Progressive Series Decoupler at the beginning of\neach encoder layer to stand out critical motion events inherent in acceleration\nand angular velocity signals. To better aggregate cross-modal interactions, we\npresent Adaptive Positional Encoding, which dynamically modifies positional\nembeddings for temporal discrepancies between different modalities. During\ndecoding, we introduce a small set of learnable query motion particles as\npriors to model motion uncertainties within velocity segments. Each query\nmotion particle is intended to draw cross-modal features dedicated to a\nspecific motion mode, all taken together allowing the model to refine its\nunderstanding of motion dynamics effectively. Lastly, we design a dynamic\nscoring mechanism to stabilize iMoT's optimization by considering all aligned\nmotion particles at the final decoding step, ensuring robust and accurate\nvelocity segment estimation. Extensive evaluations on various inertial datasets\ndemonstrate that iMoT significantly outperforms state-of-the-art methods in\ndelivering superior robustness and accuracy in trajectory reconstruction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted as technical research paper in 39th AAAI Conference on\n  Artificial Intelligence, 2025 (AAAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.12190v1",
    "published_date": "2024-12-13 22:52:47 UTC",
    "updated_date": "2024-12-13 22:52:47 UTC"
  },
  {
    "arxiv_id": "2412.10587v2",
    "title": "Evaluation of GPT-4o and GPT-4o-mini's Vision Capabilities for Compositional Analysis from Dried Solution Drops",
    "authors": [
      "Deven B. Dangi",
      "Beni B. Dangi",
      "Oliver Steinbock"
    ],
    "abstract": "When microliter drops of salt solutions dry on non-porous surfaces, they form\nerratic yet characteristic deposit patterns influenced by complex\ncrystallization dynamics and fluid motion. Using OpenAI's image-enabled\nlanguage models, we analyzed deposits from 12 salts with 200 images per salt\nand per model. GPT-4o classified 57% of the salts accurately, significantly\noutperforming random chance and GPT-4o mini. This study underscores the promise\nof general-use AI tools for reliably identifying salts from their drying\npatterns.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10587v2",
    "published_date": "2024-12-13 22:02:48 UTC",
    "updated_date": "2025-01-27 23:46:06 UTC"
  },
  {
    "arxiv_id": "2412.12189v1",
    "title": "Multi-Surrogate-Teacher Assistance for Representation Alignment in Fingerprint-based Indoor Localization",
    "authors": [
      "Son Minh Nguyen",
      "Linh Duy Tran",
      "Duc Viet Le",
      "Paul J. M Havinga"
    ],
    "abstract": "Despite remarkable progress in knowledge transfer across visual and textual\ndomains, extending these achievements to indoor localization, particularly for\nlearning transferable representations among Received Signal Strength (RSS)\nfingerprint datasets, remains a challenge. This is due to inherent\ndiscrepancies among these RSS datasets, largely including variations in\nbuilding structure, the input number and disposition of WiFi anchors.\nAccordingly, specialized networks, which were deprived of the ability to\ndiscern transferable representations, readily incorporate environment-sensitive\nclues into the learning process, hence limiting their potential when applied to\nspecific RSS datasets. In this work, we propose a plug-and-play (PnP) framework\nof knowledge transfer, facilitating the exploitation of transferable\nrepresentations for specialized networks directly on target RSS datasets\nthrough two main phases. Initially, we design an Expert Training phase, which\nfeatures multiple surrogate generative teachers, all serving as a global\nadapter that homogenizes the input disparities among independent source RSS\ndatasets while preserving their unique characteristics. In a subsequent Expert\nDistilling phase, we continue introducing a triplet of underlying constraints\nthat requires minimizing the differences in essential knowledge between the\nspecialized network and surrogate teachers through refining its representation\nlearning on the target dataset. This process implicitly fosters a\nrepresentational alignment in such a way that is less sensitive to specific\nenvironmental dynamics. Extensive experiments conducted on three benchmark WiFi\nRSS fingerprint datasets underscore the effectiveness of the framework that\nsignificantly exerts the full potential of specialized networks in\nlocalization.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in the 1st round at WACV 2025 (Algorithm Track)",
    "pdf_url": "http://arxiv.org/pdf/2412.12189v1",
    "published_date": "2024-12-13 22:00:26 UTC",
    "updated_date": "2024-12-13 22:00:26 UTC"
  },
  {
    "arxiv_id": "2412.10575v2",
    "title": "Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration",
    "authors": [
      "Karina Halevy",
      "Karly Hou",
      "Charumathi Badrinath"
    ],
    "abstract": "Data augmentation methods, especially SoTA interpolation-based methods such\nas Fair Mixup, have been widely shown to increase model fairness. However, this\nfairness is evaluated on metrics that do not capture model uncertainty and on\ndatasets with only one, relatively large, minority group. As a remedy,\nmulticalibration has been introduced to measure fairness while accommodating\nuncertainty and accounting for multiple minority groups. However, existing\nmethods of improving multicalibration involve reducing initial training data to\ncreate a holdout set for post-processing, which is not ideal when minority\ntraining data is already sparse. This paper uses multicalibration to more\nrigorously examine data augmentation for classification fairness. We\nstress-test four versions of Fair Mixup on two structured data classification\nproblems with up to 81 marginalized groups, evaluating multicalibration\nviolations and balanced accuracy. We find that on nearly every experiment, Fair\nMixup \\textit{worsens} baseline performance and fairness, but the simple\nvanilla Mixup \\textit{outperforms} both Fair Mixup and the baseline, especially\nwhen calibrating on small groups. \\textit{Combining} vanilla Mixup with\nmulticalibration post-processing, which enforces multicalibration through\npost-processing on a holdout set, further increases fairness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Expanded version of AAAI 2025 main track paper. 8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10575v2",
    "published_date": "2024-12-13 21:36:12 UTC",
    "updated_date": "2025-04-14 19:40:08 UTC"
  },
  {
    "arxiv_id": "2412.10569v1",
    "title": "Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers",
    "authors": [
      "Dong Hoon Lee",
      "Seunghoon Hong"
    ],
    "abstract": "Recent token reduction methods for Vision Transformers (ViTs) incorporate\ntoken merging, which measures the similarities between token embeddings and\ncombines the most similar pairs. However, their merging policies are directly\ndependent on intermediate features in ViTs, which prevents exploiting features\ntailored for merging and requires end-to-end training to improve token merging.\nIn this paper, we propose Decoupled Token Embedding for Merging (DTEM) that\nenhances token merging through a decoupled embedding learned via a continuously\nrelaxed token merging process. Our method introduces a lightweight embedding\nmodule decoupled from the ViT forward pass to extract dedicated features for\ntoken merging, thereby addressing the restriction from using intermediate\nfeatures. The continuously relaxed token merging, applied during training,\nenables us to learn the decoupled embeddings in a differentiable manner. Thanks\nto the decoupled structure, our method can be seamlessly integrated into\nexisting ViT backbones and trained either modularly by learning only the\ndecoupled embeddings or end-to-end by fine-tuning. We demonstrate the\napplicability of DTEM on various tasks, including classification, captioning,\nand segmentation, with consistent improvement in token merging. Especially in\nthe ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while\nmaintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at\n\\href{https://github.com/movinghoon/dtem}{link}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.10569v1",
    "published_date": "2024-12-13 21:17:11 UTC",
    "updated_date": "2024-12-13 21:17:11 UTC"
  },
  {
    "arxiv_id": "2412.10558v1",
    "title": "Too Big to Fool: Resisting Deception in Language Models",
    "authors": [
      "Mohammad Reza Samsami",
      "Mats Leon Richter",
      "Juan Rodriguez",
      "Megh Thakkar",
      "Sarath Chandar",
      "Maxime Gasse"
    ],
    "abstract": "Large language models must balance their weight-encoded knowledge with\nin-context information from prompts to generate accurate responses. This paper\ninvestigates this interplay by analyzing how models of varying capacities\nwithin the same family handle intentionally misleading in-context information.\nOur experiments demonstrate that larger models exhibit higher resilience to\ndeceptive prompts, showcasing an advanced ability to interpret and integrate\nprompt information with their internal knowledge. Furthermore, we find that\nlarger models outperform smaller ones in following legitimate instructions,\nindicating that their resilience is not due to disregarding in-context\ninformation. We also show that this phenomenon is likely not a result of\nmemorization but stems from the models' ability to better leverage implicit\ntask-relevant information from the prompt alongside their internally stored\nknowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10558v1",
    "published_date": "2024-12-13 21:03:10 UTC",
    "updated_date": "2024-12-13 21:03:10 UTC"
  },
  {
    "arxiv_id": "2412.10553v1",
    "title": "Edge AI-based Radio Frequency Fingerprinting for IoT Networks",
    "authors": [
      "Ahmed Mohamed Hussain",
      "Nada Abughanam",
      "Panos Papadimitratos"
    ],
    "abstract": "The deployment of the Internet of Things (IoT) in smart cities and critical\ninfrastructure has enhanced connectivity and real-time data exchange but\nintroduced significant security challenges. While effective, cryptography can\noften be resource-intensive for small-footprint resource-constrained (i.e.,\nIoT) devices. Radio Frequency Fingerprinting (RFF) offers a promising\nauthentication alternative by using unique RF signal characteristics for device\nidentification at the Physical (PHY)-layer, without resorting to cryptographic\nsolutions. The challenge is two-fold: how to deploy such RFF in a large scale\nand for resource-constrained environments. Edge computing, processing data\ncloser to its source, i.e., the wireless device, enables faster\ndecision-making, reducing reliance on centralized cloud servers. Considering a\nmodest edge device, we introduce two truly lightweight Edge AI-based RFF\nschemes tailored for resource-constrained devices. We implement two Deep\nLearning models, namely a Convolution Neural Network and a Transformer-Encoder,\nto extract complex features from the IQ samples, forming device-specific RF\nfingerprints. We convert the models to TensorFlow Lite and evaluate them on a\nRaspberry Pi, demonstrating the practicality of Edge deployment. Evaluations\ndemonstrate the Transformer-Encoder outperforms the CNN in identifying unique\ntransmitter features, achieving high accuracy (> 0.95) and ROC-AUC scores (>\n0.90) while maintaining a compact model size of 73KB, appropriate for\nresource-constrained devices.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, and 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10553v1",
    "published_date": "2024-12-13 20:55:10 UTC",
    "updated_date": "2024-12-13 20:55:10 UTC"
  },
  {
    "arxiv_id": "2412.12188v1",
    "title": "Predicting Internet Connectivity in Schools: A Feasibility Study Leveraging Multi-modal Data and Location Encoders in Low-Resource Settings",
    "authors": [
      "Kelsey Doerksen",
      "Casper Fibaek",
      "Rochelle Schneider",
      "Do-Hyung Kim",
      "Isabelle Tingzon"
    ],
    "abstract": "Internet connectivity in schools is critical to provide students with the\ndigital literary skills necessary to compete in modern economies. In order for\ngovernments to effectively implement digital infrastructure development in\nschools, accurate internet connectivity information is required. However,\ntraditional survey-based methods can exceed the financial and capacity limits\nof governments. Open-source Earth Observation (EO) datasets have unlocked our\nability to observe and understand socio-economic conditions on Earth from\nspace, and in combination with Machine Learning (ML), can provide the tools to\ncircumvent costly ground-based survey methods to support infrastructure\ndevelopment. In this paper, we present our work on school internet connectivity\nprediction using EO and ML. We detail the creation of our multi-modal,\nfreely-available satellite imagery and survey information dataset, leverage the\nlatest geographically-aware location encoders, and introduce the first results\nof using the new European Space Agency phi-lab geographically-aware\nfoundational model to predict internet connectivity in Botswana and Rwanda. We\nfind that ML with EO and ground-based auxiliary data yields the best\nperformance in both countries, for accuracy, F1 score, and False Positive\nrates, and highlight the challenges of internet connectivity prediction from\nspace with a case study in Kigali, Rwanda. Our work showcases a practical\napproach to support data-driven digital infrastructure development in\nlow-resource settings, leveraging freely available information, and provide\ncleaned and labelled datasets for future studies to the community through a\nunique collaboration between UNICEF and the European Space Agency phi-lab.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.SI"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12188v1",
    "published_date": "2024-12-13 20:20:29 UTC",
    "updated_date": "2024-12-13 20:20:29 UTC"
  },
  {
    "arxiv_id": "2412.10535v1",
    "title": "On Adversarial Robustness and Out-of-Distribution Robustness of Large Language Models",
    "authors": [
      "April Yang",
      "Jordan Tab",
      "Parth Shah",
      "Paul Kotchavong"
    ],
    "abstract": "The increasing reliance on large language models (LLMs) for diverse\napplications necessitates a thorough understanding of their robustness to\nadversarial perturbations and out-of-distribution (OOD) inputs. In this study,\nwe investigate the correlation between adversarial robustness and OOD\nrobustness in LLMs, addressing a critical gap in robustness evaluation. By\napplying methods originally designed to improve one robustness type across both\ncontexts, we analyze their performance on adversarial and out-of-distribution\nbenchmark datasets. The input of the model consists of text samples, with the\noutput prediction evaluated in terms of accuracy, precision, recall, and F1\nscores in various natural language inference tasks.\n  Our findings highlight nuanced interactions between adversarial robustness\nand OOD robustness, with results indicating limited transferability between the\ntwo robustness types. Through targeted ablations, we evaluate how these\ncorrelations evolve with different model sizes and architectures, uncovering\nmodel-specific trends: smaller models like LLaMA2-7b exhibit neutral\ncorrelations, larger models like LLaMA2-13b show negative correlations, and\nMixtral demonstrates positive correlations, potentially due to domain-specific\nalignment. These results underscore the importance of hybrid robustness\nframeworks that integrate adversarial and OOD strategies tailored to specific\nmodels and domains. Further research is needed to evaluate these interactions\nacross larger models and varied architectures, offering a pathway to more\nreliable and generalizable LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10535v1",
    "published_date": "2024-12-13 20:04:25 UTC",
    "updated_date": "2024-12-13 20:04:25 UTC"
  },
  {
    "arxiv_id": "2412.10513v1",
    "title": "Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Study Case on BERT-based Language Models",
    "authors": [
      "Ana Ozaki",
      "Roberto Confalonieri",
      "Ricardo Guimarães",
      "Anders Imenes"
    ],
    "abstract": "Decision trees are a popular machine learning method, known for their\ninherent explainability. In Explainable AI, decision trees can be used as\nsurrogate models for complex black box AI models or as approximations of parts\nof such models. A key challenge of this approach is determining how accurately\nthe extracted decision tree represents the original model and to what extent it\ncan be trusted as an approximation of their behavior. In this work, we\ninvestigate the use of the Probably Approximately Correct (PAC) framework to\nprovide a theoretical guarantee of fidelity for decision trees extracted from\nAI models. Based on theoretical results from the PAC framework, we adapt a\ndecision tree algorithm to ensure a PAC guarantee under certain conditions. We\nfocus on binary classification and conduct experiments where we extract\ndecision trees from BERT-based language models with PAC guarantees. Our results\nindicate occupational gender bias in these models.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10513v1",
    "published_date": "2024-12-13 19:14:08 UTC",
    "updated_date": "2024-12-13 19:14:08 UTC"
  },
  {
    "arxiv_id": "2412.10511v1",
    "title": "Automated Image Captioning with CNNs and Transformers",
    "authors": [
      "Joshua Adrian Cahyono",
      "Jeremy Nathan Jusuf"
    ],
    "abstract": "This project aims to create an automated image captioning system that\ngenerates natural language descriptions for input images by integrating\ntechniques from computer vision and natural language processing. We employ\nvarious different techniques, ranging from CNN-RNN to the more advanced\ntransformer-based techniques. Training is carried out on image datasets paired\nwith descriptive captions, and model performance will be evaluated using\nestablished metrics such as BLEU, METEOR, and CIDEr. The project will also\ninvolve experimentation with advanced attention mechanisms, comparisons of\ndifferent architectural choices, and hyperparameter optimization to refine\ncaptioning accuracy and overall system effectiveness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10511v1",
    "published_date": "2024-12-13 19:12:11 UTC",
    "updated_date": "2024-12-13 19:12:11 UTC"
  },
  {
    "arxiv_id": "2412.10509v1",
    "title": "Do Large Language Models Show Biases in Causal Learning?",
    "authors": [
      "Maria Victoria Carro",
      "Francisca Gauna Selasco",
      "Denise Alejandra Mester",
      "Margarita Gonzales",
      "Mario A. Leiva",
      "Maria Vanina Martinez",
      "Gerardo I. Simari"
    ],
    "abstract": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this\nresearch, we investigate whether large language models (LLMs) develop causal\nillusions, both in real-world and controlled laboratory contexts of causal\nlearning and inference. To this end, we built a dataset of over 2K samples\nincluding purely correlational cases, situations with null contingency, and\ncases where temporal information excludes the possibility of causality by\nplacing the potential effect before the cause. We then prompted the models to\nmake statements or answer causal questions to evaluate their tendencies to\ninfer causation erroneously in these structured settings. Our findings show a\nstrong presence of causal illusion bias in LLMs. Specifically, in open-ended\ngeneration tasks involving spurious correlations, the models displayed bias at\nlevels comparable to, or even lower than, those observed in similar studies on\nhuman subjects. However, when faced with null-contingency scenarios or temporal\ncues that negate causal relationships, where it was required to respond on a\n0-100 scale, the models exhibited significantly higher bias. These findings\nsuggest that the models have not uniformly, consistently, or reliably\ninternalized the normative principles essential for accurate causal learning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10509v1",
    "published_date": "2024-12-13 19:03:48 UTC",
    "updated_date": "2024-12-13 19:03:48 UTC"
  },
  {
    "arxiv_id": "2412.10494v1",
    "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device",
    "authors": [
      "Yushu Wu",
      "Zhixing Zhang",
      "Yanyu Li",
      "Yanwu Xu",
      "Anil Kag",
      "Yang Sui",
      "Huseyin Coskun",
      "Ke Ma",
      "Aleksei Lebedev",
      "Ju Hu",
      "Dimitris Metaxas",
      "Yanzhi Wang",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "We have witnessed the unprecedented success of diffusion-based video\ngeneration over the past year. Recently proposed models from the community have\nwielded the power to generate cinematic and high-resolution videos with smooth\nmotions from arbitrary input prompts. However, as a supertask of image\ngeneration, video generation models require more computation and are thus\nhosted mostly on cloud servers, limiting broader adoption among content\ncreators. In this work, we propose a comprehensive acceleration framework to\nbring the power of the large-scale video diffusion model to the hands of edge\nusers. From the network architecture scope, we initialize from a compact image\nbackbone and search out the design and arrangement of temporal layers to\nmaximize hardware efficiency. In addition, we propose a dedicated adversarial\nfine-tuning algorithm for our efficient model and reduce the denoising steps to\n4. Our model, with only 0.6B parameters, can generate a 5-second video on an\niPhone 16 PM within 5 seconds. Compared to server-side models that take minutes\non powerful GPUs to generate a single video, we accelerate the generation by\nmagnitudes while delivering on-par quality.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CV",
    "comment": "https://snap-research.github.io/snapgen-v/",
    "pdf_url": "http://arxiv.org/pdf/2412.10494v1",
    "published_date": "2024-12-13 18:59:56 UTC",
    "updated_date": "2024-12-13 18:59:56 UTC"
  },
  {
    "arxiv_id": "2412.10373v1",
    "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction",
    "authors": [
      "Sicheng Zuo",
      "Wenzhao Zheng",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/zuosc19/GaussianWorld",
    "pdf_url": "http://arxiv.org/pdf/2412.10373v1",
    "published_date": "2024-12-13 18:59:54 UTC",
    "updated_date": "2024-12-13 18:59:54 UTC"
  },
  {
    "arxiv_id": "2412.10493v1",
    "title": "SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation",
    "authors": [
      "Runtao Liu",
      "Chen I Chieh",
      "Jindong Gu",
      "Jipeng Zhang",
      "Renjie Pi",
      "Qifeng Chen",
      "Philip Torr",
      "Ashkan Khakzar",
      "Fabio Pizzati"
    ],
    "abstract": "Text-to-image (T2I) models have become widespread, but their limited safety\nguardrails expose end users to harmful content and potentially allow for model\nmisuse. Current safety measures are typically limited to text-based filtering\nor concept removal strategies, able to remove just a few concepts from the\nmodel's generative capabilities. In this work, we introduce SafetyDPO, a method\nfor safety alignment of T2I models through Direct Preference Optimization\n(DPO). We enable the application of DPO for safety purposes in T2I models by\nsynthetically generating a dataset of harmful and safe image-text pairs, which\nwe call CoProV2. Using a custom DPO strategy and this dataset, we train safety\nexperts, in the form of low-rank adaptation (LoRA) matrices, able to guide the\ngeneration process away from specific safety-related concepts. Then, we merge\nthe experts into a single LoRA using a novel merging strategy for optimal\nscaling performance. This expert-based approach enables scalability, allowing\nus to remove 7 times more harmful concepts from T2I models compared to\nbaselines. SafetyDPO consistently outperforms the state-of-the-art on many\nbenchmarks and establishes new practices for safety alignment in T2I networks.\nCode and data will be shared at https://safetydpo.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10493v1",
    "published_date": "2024-12-13 18:59:52 UTC",
    "updated_date": "2024-12-13 18:59:52 UTC"
  },
  {
    "arxiv_id": "2412.10371v1",
    "title": "GaussianAD: Gaussian-Centric End-to-End Autonomous Driving",
    "authors": [
      "Wenzhao Zheng",
      "Junjie Wu",
      "Yao Zheng",
      "Sicheng Zuo",
      "Zixun Xie",
      "Longchao Yang",
      "Yong Pan",
      "Zhihui Hao",
      "Peng Jia",
      "Xianpeng Lang",
      "Shanghang Zhang"
    ],
    "abstract": "Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/wzzheng/GaussianAD",
    "pdf_url": "http://arxiv.org/pdf/2412.10371v1",
    "published_date": "2024-12-13 18:59:30 UTC",
    "updated_date": "2024-12-13 18:59:30 UTC"
  },
  {
    "arxiv_id": "2412.10360v1",
    "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "authors": [
      "Orr Zohar",
      "Xiaohan Wang",
      "Yann Dubois",
      "Nikhil Mehta",
      "Tong Xiao",
      "Philippe Hansen-Estruch",
      "Licheng Yu",
      "Xiaofang Wang",
      "Felix Juefei-Xu",
      "Ning Zhang",
      "Serena Yeung-Levy",
      "Xide Xia"
    ],
    "abstract": "Despite the rapid integration of video perception capabilities into Large\nMultimodal Models (LMMs), the underlying mechanisms driving their video\nunderstanding remain poorly understood. Consequently, many design decisions in\nthis domain are made without proper justification or analysis. The high\ncomputational cost of training and evaluating such models, coupled with limited\nopen research, hinders the development of video-LMMs. To address this, we\npresent a comprehensive study that helps uncover what effectively drives video\nunderstanding in LMMs.\n  We begin by critically examining the primary contributors to the high\ncomputational requirements associated with video-LMM research and discover\nScaling Consistency, wherein design and training decisions made on smaller\nmodels and datasets (up to a critical size) effectively transfer to larger\nmodels. Leveraging these insights, we explored many video-specific aspects of\nvideo-LMMs, including video sampling, architectures, data composition, training\nschedules, and more. For example, we demonstrated that fps sampling during\ntraining is vastly preferable to uniform frame sampling and which vision\nencoders are the best for video representation.\n  Guided by these findings, we introduce Apollo, a state-of-the-art family of\nLMMs that achieve superior performance across different model sizes. Our models\ncan perceive hour-long videos efficiently, with Apollo-3B outperforming most\nexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is\nstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on\nVideo-MME.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "https://apollo-lmms.github.io",
    "pdf_url": "http://arxiv.org/pdf/2412.10360v1",
    "published_date": "2024-12-13 18:53:24 UTC",
    "updated_date": "2024-12-13 18:53:24 UTC"
  },
  {
    "arxiv_id": "2412.10354v3",
    "title": "A Library for Learning Neural Operators",
    "authors": [
      "Jean Kossaifi",
      "Nikola Kovachki",
      "Zongyi Li",
      "David Pitt",
      "Miguel Liu-Schiaffini",
      "Robert Joseph George",
      "Boris Bonev",
      "Kamyar Azizzadenesheli",
      "Julius Berner",
      "Valentin Duruisseaux",
      "Anima Anandkumar"
    ],
    "abstract": "We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10354v3",
    "published_date": "2024-12-13 18:49:37 UTC",
    "updated_date": "2025-04-30 17:23:25 UTC"
  },
  {
    "arxiv_id": "2412.10348v1",
    "title": "A dual contrastive framework",
    "authors": [
      "Yuan Sun",
      "Zhao Zhang",
      "Jorge Ortiz"
    ],
    "abstract": "In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10348v1",
    "published_date": "2024-12-13 18:45:18 UTC",
    "updated_date": "2024-12-13 18:45:18 UTC"
  },
  {
    "arxiv_id": "2412.10347v1",
    "title": "COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models",
    "authors": [
      "Yuchen Ren",
      "Wenwei Han",
      "Qianyuan Zhang",
      "Yining Tang",
      "Weiqiang Bai",
      "Yuchen Cai",
      "Lifeng Qiao",
      "Hao Jiang",
      "Dong Yuan",
      "Tao Chen",
      "Siqi Sun",
      "Pan Tan",
      "Wanli Ouyang",
      "Nanqing Dong",
      "Xinzhu Ma",
      "Peng Ye"
    ],
    "abstract": "As key elements within the central dogma, DNA, RNA, and proteins play crucial\nroles in maintaining life by guaranteeing accurate genetic expression and\nimplementation. Although research on these molecules has profoundly impacted\nfields like medicine, agriculture, and industry, the diversity of machine\nlearning approaches-from traditional statistical methods to deep learning\nmodels and large language models-poses challenges for researchers in choosing\nthe most suitable models for specific tasks, especially for cross-omics and\nmulti-omics tasks due to the lack of comprehensive benchmarks. To address this,\nwe introduce the first comprehensive multi-omics benchmark COMET (Benchmark for\nBiological COmprehensive Multi-omics Evaluation Tasks and Language Models),\ndesigned to evaluate models across single-omics, cross-omics, and multi-omics\ntasks. First, we curate and develop a diverse collection of downstream tasks\nand datasets covering key structural and functional aspects in DNA, RNA, and\nproteins, including tasks that span multiple omics levels. Then, we evaluate\nexisting foundational language models for DNA, RNA, and proteins, as well as\nthe newly proposed multi-omics method, offering valuable insights into their\nperformance in integrating and analyzing data from different biological\nmodalities. This benchmark aims to define critical issues in multi-omics\nresearch and guide future directions, ultimately promoting advancements in\nunderstanding biological processes through integrated and different omics data\nanalysis.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10347v1",
    "published_date": "2024-12-13 18:42:00 UTC",
    "updated_date": "2024-12-13 18:42:00 UTC"
  },
  {
    "arxiv_id": "2412.10345v2",
    "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
    "authors": [
      "Ruijie Zheng",
      "Yongyuan Liang",
      "Shuaiyi Huang",
      "Jianfeng Gao",
      "Hal Daumé III",
      "Andrey Kolobov",
      "Furong Huang",
      "Jianwei Yang"
    ],
    "abstract": "Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10345v2",
    "published_date": "2024-12-13 18:40:51 UTC",
    "updated_date": "2024-12-25 23:12:26 UTC"
  },
  {
    "arxiv_id": "2412.10342v2",
    "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
    "authors": [
      "Zhiqi Ge",
      "Juncheng Li",
      "Xinglei Pang",
      "Minghe Gao",
      "Kaihang Pan",
      "Wang Lin",
      "Hao Fei",
      "Wenqiao Zhang",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10342v2",
    "published_date": "2024-12-13 18:40:10 UTC",
    "updated_date": "2025-02-03 15:23:02 UTC"
  },
  {
    "arxiv_id": "2501.10373v1",
    "title": "DK-PRACTICE: An Intelligent Educational Platform for Personalized Learning Content Recommendations Based on Students Knowledge State",
    "authors": [
      "Marina Delianidi",
      "Konstantinos Diamantaras",
      "Ioannis Moras",
      "Antonis Sidiropoulos"
    ],
    "abstract": "This study introduces DK-PRACTICE (Dynamic Knowledge Prediction and\nEducational Content Recommendation System), an intelligent online platform that\nleverages machine learning to provide personalized learning recommendations\nbased on student knowledge state. Students participate in a short, adaptive\nassessment using the question-and-answer method regarding key concepts in a\nspecific knowledge domain. The system dynamically selects the next question for\neach student based on the correctness and accuracy of their previous answers.\nAfter the test is completed, DK-PRACTICE analyzes students' interaction history\nto recommend learning materials to empower the student's knowledge state in\nidentified knowledge gaps. Both question selection and learning material\nrecommendations are based on machine learning models trained using anonymized\ndata from a real learning environment. To provide self-assessment and monitor\nlearning progress, DK-PRACTICE allows students to take two tests: one\npre-teaching and one post-teaching. After each test, a report is generated with\ndetailed results. In addition, the platform offers functions to visualize\nlearning progress based on recorded test statistics. DK-PRACTICE promotes\nadaptive and personalized learning by empowering students with self-assessment\ncapabilities and providing instructors with valuable information about\nstudents' knowledge levels. DK-PRACTICE can be extended to various educational\nenvironments and knowledge domains, provided the necessary data is available\naccording to the educational topics. A subsequent paper will present the\nmethodology for the experimental application and evaluation of the platform.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "General, Computer Uses in Education",
      "I.2.0; K.3.1"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, The Barcelona Conference on Education 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.10373v1",
    "published_date": "2024-12-13 18:35:37 UTC",
    "updated_date": "2024-12-13 18:35:37 UTC"
  },
  {
    "arxiv_id": "2412.10337v2",
    "title": "Generative AI in Medicine",
    "authors": [
      "Divya Shanmugam",
      "Monica Agrawal",
      "Rajiv Movva",
      "Irene Y. Chen",
      "Marzyeh Ghassemi",
      "Maia Jacobs",
      "Emma Pierson"
    ],
    "abstract": "The increased capabilities of generative AI have dramatically expanded its\npossible use cases in medicine. We provide a comprehensive overview of\ngenerative AI use cases for clinicians, patients, clinical trial organizers,\nresearchers, and trainees. We then discuss the many challenges -- including\nmaintaining privacy and security, improving transparency and interpretability,\nupholding equity, and rigorously evaluating models -- which must be overcome to\nrealize this potential, and the open research directions they give rise to.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the Annual Review of Biomedical Data Science, August\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10337v2",
    "published_date": "2024-12-13 18:32:21 UTC",
    "updated_date": "2024-12-17 14:57:57 UTC"
  },
  {
    "arxiv_id": "2412.10321v1",
    "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
    "authors": [
      "Sicheng Zhu",
      "Brandon Amos",
      "Yuandong Tian",
      "Chuan Guo",
      "Ivan Evtimov"
    ],
    "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10321v1",
    "published_date": "2024-12-13 18:00:57 UTC",
    "updated_date": "2024-12-13 18:00:57 UTC"
  },
  {
    "arxiv_id": "2412.10320v1",
    "title": "MeshA*: Efficient Path Planing With Motion Primitives",
    "authors": [
      "Marat Agranovskiy",
      "Konstantin Yakovlev"
    ],
    "abstract": "We study a path planning problem where the possible move actions are\nrepresented as a finite set of motion primitives aligned with the grid\nrepresentation of the environment. That is, each primitive corresponds to a\nshort kinodynamically-feasible motion of an agent and is represented as a\nsequence of the swept cells of a grid. Typically heuristic search, i.e. A*, is\nconducted over the lattice induced by these primitives (lattice-based planning)\nto find a path. However due to the large branching factor such search may be\ninefficient in practice. To this end we suggest a novel technique rooted in the\nidea of searching over the grid cells (as in vanilla A*) simultaneously fitting\nthe possible sequences of the motion primitives into these cells. The resultant\nalgorithm, MeshA*, provably preserves the guarantees on completeness and\noptimality, on the one hand, and is shown to notably outperform conventional\nlattice-based planning (x1.5 decrease in the runtime), on the other hand.\nMoreover, we suggest an additional pruning technique that additionally\ndecreases the search space of MeshA*. The resultant planner is combined with\nthe regular A* to retain completeness and is shown to further increase the\nsearch performance at the cost of negligible decrease of the solution quality.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10320v1",
    "published_date": "2024-12-13 18:00:21 UTC",
    "updated_date": "2024-12-13 18:00:21 UTC"
  },
  {
    "arxiv_id": "2412.10316v3",
    "title": "BrushEdit: All-In-One Image Inpainting and Editing",
    "authors": [
      "Yaowei Li",
      "Yuxuan Bian",
      "Xuan Ju",
      "Zhaoyang Zhang",
      "Junhao Zhuang",
      "Ying Shan",
      "Yuexian Zou",
      "Qiang Xu"
    ],
    "abstract": "Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/",
    "pdf_url": "http://arxiv.org/pdf/2412.10316v3",
    "published_date": "2024-12-13 17:58:06 UTC",
    "updated_date": "2025-05-05 16:31:12 UTC"
  },
  {
    "arxiv_id": "2412.16188v1",
    "title": "A Decade of Deep Learning: A Survey on The Magnificent Seven",
    "authors": [
      "Dilshod Azizov",
      "Muhammad Arslan Manzoor",
      "Velibor Bojkovic",
      "Yingxu Wang",
      "Zixiao Wang",
      "Zangir Iklassov",
      "Kailong Zhao",
      "Liang Li",
      "Siwei Liu",
      "Yu Zhong",
      "Wei Liu",
      "Shangsong Liang"
    ],
    "abstract": "Deep learning has fundamentally reshaped the landscape of artificial\nintelligence over the past decade, enabling remarkable achievements across\ndiverse domains. At the heart of these developments lie multi-layered neural\nnetwork architectures that excel at automatic feature extraction, leading to\nsignificant improvements in machine learning tasks. To demystify these advances\nand offer accessible guidance, we present a comprehensive overview of the most\ninfluential deep learning algorithms selected through a broad-based survey of\nthe field. Our discussion centers on pivotal architectures, including Residual\nNetworks, Transformers, Generative Adversarial Networks, Variational\nAutoencoders, Graph Neural Networks, Contrastive Language-Image Pre-training,\nand Diffusion models. We detail their historical context, highlight their\nmathematical foundations and algorithmic principles, and examine subsequent\nvariants, extensions, and practical considerations such as training\nmethodologies, normalization techniques, and learning rate schedules. Beyond\nhistorical and technical insights, we also address their applications,\nchallenges, and potential research directions. This survey aims to serve as a\npractical manual for both newcomers seeking an entry point into cutting-edge\ndeep learning methods and experienced researchers transitioning into this\nrapidly evolving domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.16188v1",
    "published_date": "2024-12-13 17:55:39 UTC",
    "updated_date": "2024-12-13 17:55:39 UTC"
  },
  {
    "arxiv_id": "2412.10312v1",
    "title": "Interlocking-free Selective Rationalization Through Genetic-based Learning",
    "authors": [
      "Federico Ruggeri",
      "Gaetano Signorelli"
    ],
    "abstract": "A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10312v1",
    "published_date": "2024-12-13 17:52:48 UTC",
    "updated_date": "2024-12-13 17:52:48 UTC"
  },
  {
    "arxiv_id": "2412.10302v1",
    "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
    "authors": [
      "Zhiyu Wu",
      "Xiaokang Chen",
      "Zizheng Pan",
      "Xingchao Liu",
      "Wen Liu",
      "Damai Dai",
      "Huazuo Gao",
      "Yiyang Ma",
      "Chengyue Wu",
      "Bingxuan Wang",
      "Zhenda Xie",
      "Yu Wu",
      "Kai Hu",
      "Jiawei Wang",
      "Yaofeng Sun",
      "Yukun Li",
      "Yishi Piao",
      "Kang Guan",
      "Aixin Liu",
      "Xin Xie",
      "Yuxiang You",
      "Kai Dong",
      "Xingkai Yu",
      "Haowei Zhang",
      "Liang Zhao",
      "Yisong Wang",
      "Chong Ruan"
    ],
    "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10302v1",
    "published_date": "2024-12-13 17:37:48 UTC",
    "updated_date": "2024-12-13 17:37:48 UTC"
  },
  {
    "arxiv_id": "2412.10291v1",
    "title": "Still \"Talking About Large Language Models\": Some Clarifications",
    "authors": [
      "Murray Shanahan"
    ],
    "abstract": "My paper \"Talking About Large Language Models\" has more than once been\ninterpreted as advocating a reductionist stance towards large language models.\nBut the paper was not intended that way, and I do not endorse such positions.\nThis short note situates the paper in the context of a larger philosophical\nproject that is concerned with the (mis)use of words rather than metaphysics,\nin the spirit of Wittgenstein's later writing.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10291v1",
    "published_date": "2024-12-13 17:21:29 UTC",
    "updated_date": "2024-12-13 17:21:29 UTC"
  },
  {
    "arxiv_id": "2412.10278v1",
    "title": "Envisioning National Resources for Artificial Intelligence Research: NSF Workshop Report",
    "authors": [
      "Shantenu Jha",
      "Yolanda Gil"
    ],
    "abstract": "This is a report of an NSF workshop titled \"Envisioning National Resources\nfor Artificial Intelligence Research\" held in Alexandria, Virginia, in May\n2024. The workshop aimed to identify initial challenges and opportunities for\nnational resources for AI research (e.g., compute, data, models, etc.) and to\nfacilitate planning for the envisioned National AI Research Resource.\nParticipants included AI and cyberinfrastructure (CI) experts. The report\noutlines significant findings and identifies needs and recommendations from the\nworkshop.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10278v1",
    "published_date": "2024-12-13 17:00:31 UTC",
    "updated_date": "2024-12-13 17:00:31 UTC"
  },
  {
    "arxiv_id": "2412.10272v1",
    "title": "Trustworthy and Explainable Decision-Making for Workforce allocation",
    "authors": [
      "Guillaume Povéda",
      "Ryma Boumazouza",
      "Andreas Strahl",
      "Mark Hall",
      "Santiago Quintana-Amate",
      "Nahum Alvarez",
      "Ignace Bleukx",
      "Dimos Tsouros",
      "Hélène Verhaeghe",
      "Tias Guns"
    ],
    "abstract": "In industrial contexts, effective workforce allocation is crucial for\noperational efficiency. This paper presents an ongoing project focused on\ndeveloping a decision-making tool designed for workforce allocation,\nemphasising the explainability to enhance its trustworthiness. Our objective is\nto create a system that not only optimises the allocation of teams to scheduled\ntasks but also provides clear, understandable explanations for its decisions,\nparticularly in cases where the problem is infeasible. By incorporating\nhuman-in-the-loop mechanisms, the tool aims to enhance user trust and\nfacilitate interactive conflict resolution. We implemented our approach on a\nprototype tool/digital demonstrator intended to be evaluated on a real\nindustrial scenario both in terms of performance and user acceptability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for presentation at PTHG-24: The Seventh Workshop on\n  Progress Towards the Holy Grail, part of the 30th International Conference on\n  Principles and Practice of Constraint Programming. For more details, visit\n  the workshop webpage:\n  https://freuder.wordpress.com/progress-towards-the-holy-grail-workshops/pthg-24-the-seventh-workshop-on-progress-towards-the-holy-grail/",
    "pdf_url": "http://arxiv.org/pdf/2412.10272v1",
    "published_date": "2024-12-13 16:46:13 UTC",
    "updated_date": "2024-12-13 16:46:13 UTC"
  },
  {
    "arxiv_id": "2412.10270v1",
    "title": "Cultural Evolution of Cooperation among LLM Agents",
    "authors": [
      "Aron Vallinder",
      "Edward Hughes"
    ],
    "abstract": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "15 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10270v1",
    "published_date": "2024-12-13 16:45:49 UTC",
    "updated_date": "2024-12-13 16:45:49 UTC"
  },
  {
    "arxiv_id": "2412.10267v1",
    "title": "Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT",
    "authors": [
      "Danielle R. Thomas",
      "Conrad Borchers",
      "Sanjit Kakarla",
      "Jionghao Lin",
      "Shambhavi Bhushan",
      "Boyuan Guo",
      "Erin Gatz",
      "Kenneth R. Koedinger"
    ],
    "abstract": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.10267v1",
    "published_date": "2024-12-13 16:37:20 UTC",
    "updated_date": "2024-12-13 16:37:20 UTC"
  },
  {
    "arxiv_id": "2412.10489v2",
    "title": "CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With Multimodal Information",
    "authors": [
      "Kaifan Zhang",
      "Lihuo He",
      "Xin Jiang",
      "Wen Lu",
      "Di Wang",
      "Xinbo Gao"
    ],
    "abstract": "Electroencephalogram (EEG) signals have attracted significant attention from\nresearchers due to their non-invasive nature and high temporal sensitivity in\ndecoding visual stimuli. However, most recent studies have focused solely on\nthe relationship between EEG and image data pairs, neglecting the valuable\n``beyond-image-modality\" information embedded in EEG signals. This results in\nthe loss of critical multimodal information in EEG. To address this limitation,\nwe propose CognitionCapturer, a unified framework that fully leverages\nmultimodal data to represent EEG signals. Specifically, CognitionCapturer\ntrains Modality Expert Encoders for each modality to extract cross-modal\ninformation from the EEG modality. Then, it introduces a diffusion prior to map\nthe EEG embedding space to the CLIP embedding space, followed by using a\npretrained generative model, the proposed framework can reconstruct visual\nstimuli with high semantic and structural fidelity. Notably, the framework does\nnot require any fine-tuning of the generative models and can be extended to\nincorporate more modalities. Through extensive experiments, we demonstrate that\nCognitionCapturer outperforms state-of-the-art methods both qualitatively and\nquantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10489v2",
    "published_date": "2024-12-13 16:27:54 UTC",
    "updated_date": "2024-12-24 13:03:44 UTC"
  },
  {
    "arxiv_id": "2412.10257v2",
    "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
    "authors": [
      "Harry J. Davies",
      "Giorgos Iacovides",
      "Danilo P. Mandic"
    ],
    "abstract": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 5 figures, 1 table. Fixing typo with the final weight\n  editing equation",
    "pdf_url": "http://arxiv.org/pdf/2412.10257v2",
    "published_date": "2024-12-13 16:26:34 UTC",
    "updated_date": "2024-12-16 14:54:00 UTC"
  },
  {
    "arxiv_id": "2412.10255v4",
    "title": "AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era",
    "authors": [
      "Yudong Jiang",
      "Baohan Xu",
      "Siqian Yang",
      "Mingyu Yin",
      "Jing Liu",
      "Chao Xu",
      "Siqi Wang",
      "Yidi Wu",
      "Bingwen Zhu",
      "Xinwen Zhang",
      "Xingyu Zheng",
      "Jixuan Xu",
      "Yue Zhang",
      "Jinlong Hou",
      "Huyang Sun"
    ],
    "abstract": "Animation has gained significant interest in the recent film and TV industry.\nDespite the success of advanced video generation models like Sora, Kling, and\nCogVideoX in generating natural videos, they lack the same effectiveness in\nhandling animation videos. Evaluating animation video generation is also a\ngreat challenge due to its unique artist styles, violating the laws of physics\nand exaggerated motions. In this paper, we present a comprehensive system,\nAniSora, designed for animation video generation, which includes a data\nprocessing pipeline, a controllable generation model, and an evaluation\nbenchmark. Supported by the data processing pipeline with over 10M high-quality\ndata, the generation model incorporates a spatiotemporal mask module to\nfacilitate key animation production functions such as image-to-video\ngeneration, frame interpolation, and localized image-guided animation. We also\ncollect an evaluation benchmark of 948 various animation videos, with\nspecifically developed metrics for animation video generation. Our entire\nproject is publicly available on\nhttps://github.com/bilibili/Index-anisora/tree/main.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10255v4",
    "published_date": "2024-12-13 16:24:58 UTC",
    "updated_date": "2025-05-13 16:20:54 UTC"
  },
  {
    "arxiv_id": "2412.10237v1",
    "title": "Physics Instrument Design with Reinforcement Learning",
    "authors": [
      "Shah Rukh Qasim",
      "Patrick Owen",
      "Nicola Serra"
    ],
    "abstract": "We present a case for the use of Reinforcement Learning (RL) for the design\nof physics instrument as an alternative to gradient-based\ninstrument-optimization methods. It's applicability is demonstrated using two\nempirical studies. One is longitudinal segmentation of calorimeters and the\nsecond is both transverse segmentation as well longitudinal placement of\ntrackers in a spectrometer. Based on these experiments, we propose an\nalternative approach that offers unique advantages over differentiable\nprogramming and surrogate-based differentiable design optimization methods.\nFirst, Reinforcement Learning (RL) algorithms possess inherent exploratory\ncapabilities, which help mitigate the risk of convergence to local optima.\nSecond, this approach eliminates the necessity of constraining the design to a\npredefined detector model with fixed parameters. Instead, it allows for the\nflexible placement of a variable number of detector components and facilitates\ndiscrete decision-making. We then discuss the road map of how this idea can be\nextended into designing very complex instruments. The presented study sets the\nstage for a novel framework in physics instrument design, offering a scalable\nand efficient framework that can be pivotal for future projects such as the\nFuture Circular Collider (FCC), where most optimized detectors are essential\nfor exploring physics at unprecedented energy scales.",
    "categories": [
      "physics.ins-det",
      "cs.AI",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10237v1",
    "published_date": "2024-12-13 16:08:28 UTC",
    "updated_date": "2024-12-13 16:08:28 UTC"
  },
  {
    "arxiv_id": "2412.10220v1",
    "title": "How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives",
    "authors": [
      "Timour Ichmoukhamedov",
      "James Hinns",
      "David Martens"
    ],
    "abstract": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10220v1",
    "published_date": "2024-12-13 15:45:45 UTC",
    "updated_date": "2024-12-13 15:45:45 UTC"
  },
  {
    "arxiv_id": "2412.10209v2",
    "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
    "authors": [
      "Jiapeng Tang",
      "Davide Davoli",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Matthias Niessner"
    ],
    "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve facial identity and appearance details. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling priors to refine the denoised latent before decoding it into an\nimage. We evaluate our method on the NeRSemble dataset, showing that GAF\noutperforms previous state-of-the-art methods in novel view synthesis.\nFurthermore, we demonstrate higher-fidelity avatar reconstructions from\nmonocular videos captured on commodity devices.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper Video: https://youtu.be/QuIYTljvhyg Project Page:\n  https://tangjiapeng.github.io/projects/GAF",
    "pdf_url": "http://arxiv.org/pdf/2412.10209v2",
    "published_date": "2024-12-13 15:31:22 UTC",
    "updated_date": "2025-04-14 16:02:03 UTC"
  },
  {
    "arxiv_id": "2412.10488v3",
    "title": "SVGBuilder: Component-Based Colored SVG Generation with Text-Guided Autoregressive Transformers",
    "authors": [
      "Zehao Chen",
      "Rong Pan"
    ],
    "abstract": "Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://svgbuilder.github.io",
    "pdf_url": "http://arxiv.org/pdf/2412.10488v3",
    "published_date": "2024-12-13 15:24:11 UTC",
    "updated_date": "2025-03-12 14:34:11 UTC"
  },
  {
    "arxiv_id": "2412.10198v2",
    "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection",
    "authors": [
      "Haowei Wang",
      "Rupeng Zhang",
      "Junjie Wang",
      "Mingyang Li",
      "Yuekai Huang",
      "Dandan Wang",
      "Qing Wang"
    ],
    "abstract": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10198v2",
    "published_date": "2024-12-13 15:15:24 UTC",
    "updated_date": "2025-02-07 13:26:18 UTC"
  },
  {
    "arxiv_id": "2412.10186v1",
    "title": "BiCert: A Bilinear Mixed Integer Programming Formulation for Precise Certified Bounds Against Data Poisoning Attacks",
    "authors": [
      "Tobias Lorenz",
      "Marta Kwiatkowska",
      "Mario Fritz"
    ],
    "abstract": "Data poisoning attacks pose one of the biggest threats to modern AI systems,\nnecessitating robust defenses. While extensive efforts have been made to\ndevelop empirical defenses, attackers continue to evolve, creating\nsophisticated methods to circumvent these measures. To address this, we must\nmove beyond empirical defenses and establish provable certification methods\nthat guarantee robustness. This paper introduces a novel certification\napproach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to compute\nsound deterministic bounds that provide such provable robustness. Using BMIP,\nwe compute the reachable set of parameters that could result from training with\npotentially manipulated data. A key element to make this computation feasible\nis to relax the reachable parameter set to a convex set between training\niterations. At test time, this parameter set allows us to predict all possible\noutcomes, guaranteeing robustness. BiCert is more precise than previous\nmethods, which rely solely on interval and polyhedral bounds. Crucially, our\napproach overcomes the fundamental limitation of prior approaches where\nparameter bounds could only grow, often uncontrollably. We show that BiCert's\ntighter bounds eliminate a key source of divergence issues, resulting in more\nstable training and higher certified accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10186v1",
    "published_date": "2024-12-13 14:56:39 UTC",
    "updated_date": "2024-12-13 14:56:39 UTC"
  },
  {
    "arxiv_id": "2412.10185v1",
    "title": "Solving Robust Markov Decision Processes: Generic, Reliable, Efficient",
    "authors": [
      "Tobias Meggendorfer",
      "Maximilian Weininger",
      "Patrick Wienhöft"
    ],
    "abstract": "Markov decision processes (MDP) are a well-established model for sequential\ndecision-making in the presence of probabilities. In robust MDP (RMDP), every\naction is associated with an uncertainty set of probability distributions,\nmodelling that transition probabilities are not known precisely. Based on the\nknown theoretical connection to stochastic games, we provide a framework for\nsolving RMDPs that is generic, reliable, and efficient. It is *generic* both\nwith respect to the model, allowing for a wide range of uncertainty sets,\nincluding but not limited to intervals, $L^1$- or $L^2$-balls, and polytopes;\nand with respect to the objective, including long-run average reward,\nundiscounted total reward, and stochastic shortest path. It is *reliable*, as\nour approach not only converges in the limit, but provides precision guarantees\nat any time during the computation. It is *efficient* because -- in contrast to\nstate-of-the-art approaches -- it avoids explicitly constructing the underlying\nstochastic game. Consequently, our prototype implementation outperforms\nexisting tools by several orders of magnitude and can solve RMDPs with a\nmillion states in under a minute.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at AAAI'25. Extended version with full\n  appendix, 26 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.10185v1",
    "published_date": "2024-12-13 14:55:48 UTC",
    "updated_date": "2024-12-13 14:55:48 UTC"
  },
  {
    "arxiv_id": "2412.10182v1",
    "title": "Multi-Head Encoding for Extreme Label Classification",
    "authors": [
      "Daojun Liang",
      "Haixia Zhang",
      "Dongfeng Yuan",
      "Minggao Zhang"
    ],
    "abstract": "The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 12 figs, Published in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI) 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.10182v1",
    "published_date": "2024-12-13 14:53:47 UTC",
    "updated_date": "2024-12-13 14:53:47 UTC"
  },
  {
    "arxiv_id": "2412.10178v2",
    "title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models",
    "authors": [
      "Hung Nguyen",
      "Quang Qui-Vinh Nguyen",
      "Khoi Nguyen",
      "Rang Nguyen"
    ],
    "abstract": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10178v2",
    "published_date": "2024-12-13 14:50:26 UTC",
    "updated_date": "2024-12-18 18:05:43 UTC"
  },
  {
    "arxiv_id": "2412.10163v1",
    "title": "Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation",
    "authors": [
      "Federico Julian Camerota Verdù",
      "Lorenzo Castelli",
      "Luca Bortolussi"
    ],
    "abstract": "We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for\ndeep reinforcement learning (DRL) based combinatorial optimization improvement\nheuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson\nProblem, LRBS significantly enhances both in-distribution performance and\ngeneralization to larger problem instances, achieving optimality gaps that\noutperform existing improvement heuristics and narrowing the gap with\nstate-of-the-art constructive methods. We also extend our analysis to two\npickup and delivery TSP variants to validate our results. Finally, we employ\nour search strategy for offline and online adaptation of the pre-trained\nimprovement policy, leading to improved search performance and surpassing\nrecent adaptive methods for constructive heuristics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10163v1",
    "published_date": "2024-12-13 14:25:27 UTC",
    "updated_date": "2024-12-13 14:25:27 UTC"
  },
  {
    "arxiv_id": "2412.15244v1",
    "title": "MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative Samples",
    "authors": [
      "Shuo Xie",
      "Fangzhi Zhu",
      "Jiahui Wang",
      "Lulu Wen",
      "Wei Dai",
      "Xiaowei Chen",
      "Junxiong Zhu",
      "Kai Zhou",
      "Bo Zheng"
    ],
    "abstract": "Aligning Large Language Models (LLMs) with human feedback is crucial for\ntheir development. Existing preference optimization methods such as DPO and\nKTO, while improved based on Reinforcement Learning from Human Feedback (RLHF),\nare inherently derived from PPO, requiring a reference model that adds GPU\nmemory resources and relies heavily on abundant preference data. Meanwhile,\ncurrent preference optimization research mainly targets single-question\nscenarios with two replies, neglecting optimization with multiple replies,\nwhich leads to a waste of data in the application. This study introduces the\nMPPO algorithm, which leverages the average likelihood of model responses to\nfit the reward function and maximizes the utilization of preference data.\nThrough a comparison of Point-wise, Pair-wise, and List-wise implementations,\nwe found that the Pair-wise approach achieves the best performance,\nsignificantly enhancing the quality of model responses. Experimental results\ndemonstrate MPPO's outstanding performance across various benchmarks. On\nMT-Bench, MPPO outperforms DPO, ORPO, and SimPO. Notably, on Arena-Hard, MPPO\nsurpasses DPO and ORPO by substantial margins. These achievements underscore\nthe remarkable advantages of MPPO in preference optimization tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by COLING2025",
    "pdf_url": "http://arxiv.org/pdf/2412.15244v1",
    "published_date": "2024-12-13 14:18:58 UTC",
    "updated_date": "2024-12-13 14:18:58 UTC"
  },
  {
    "arxiv_id": "2501.10371v1",
    "title": "What we learned while automating bias detection in AI hiring systems for compliance with NYC Local Law 144",
    "authors": [
      "Gemma Galdon Clavell",
      "Rubén González-Sendino"
    ],
    "abstract": "Since July 5, 2023, New York City's Local Law 144 requires employers to\nconduct independent bias audits for any automated employment decision tools\n(AEDTs) used in hiring processes. The law outlines a minimum set of bias tests\nthat AI developers and implementers must perform to ensure compliance. Over the\npast few months, we have collected and analyzed audits conducted under this\nlaw, identified best practices, and developed a software tool to streamline\nemployer compliance. Our tool, ITACA_144, tailors our broader bias auditing\nframework to meet the specific requirements of Local Law 144. While automating\nthese legal mandates, we identified several critical challenges that merit\nattention to ensure AI bias regulations and audit methodologies are both\neffective and practical. This document presents the insights gained from\nautomating compliance with NYC Local Law 144. It aims to support other cities\nand states in crafting similar legislation while addressing the limitations of\nthe NYC framework. The discussion focuses on key areas including data\nrequirements, demographic inclusiveness, impact ratios, effective bias,\nmetrics, and data reliability.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10371v1",
    "published_date": "2024-12-13 14:14:26 UTC",
    "updated_date": "2024-12-13 14:14:26 UTC"
  },
  {
    "arxiv_id": "2412.10155v1",
    "title": "WordVIS: A Color Worth A Thousand Words",
    "authors": [
      "Umar Khan",
      "Saifullah",
      "Stefan Agne",
      "Andreas Dengel",
      "Sheraz Ahmed"
    ],
    "abstract": "Document classification is considered a critical element in automated\ndocument processing systems. In recent years multi-modal approaches have become\nincreasingly popular for document classification. Despite their improvements,\nthese approaches are underutilized in the industry due to their requirement for\na tremendous volume of training data and extensive computational power. In this\npaper, we attempt to address these issues by embedding textual features\ndirectly into the visual space, allowing lightweight image-based classifiers to\nachieve state-of-the-art results using small-scale datasets in document\nclassification. To evaluate the efficacy of the visual features generated from\nour approach on limited data, we tested on the standard dataset Tobacco-3482.\nOur experiments show a tremendous improvement in image-based classifiers,\nachieving an improvement of 4.64% using ResNet50 with no document pre-training.\nIt also sets a new record for the best accuracy of the Tobacco-3482 dataset\nwith a score of 91.14% using the image-based DocXClassifier with no document\npre-training. The simplicity of the approach, its resource requirements, and\nsubsequent results provide a good prospect for its use in industrial use cases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10155v1",
    "published_date": "2024-12-13 14:12:55 UTC",
    "updated_date": "2024-12-13 14:12:55 UTC"
  },
  {
    "arxiv_id": "2412.10152v1",
    "title": "Direct Encoding of Declare Constraints in ASP",
    "authors": [
      "Francesco Chiariello",
      "Valeria Fionda",
      "Antonio Ielo",
      "Francesco Ricca"
    ],
    "abstract": "Answer Set Programming (ASP), a well-known declarative logic programming\nparadigm, has recently found practical application in Process Mining. In\nparticular, ASP has been used to model tasks involving declarative\nspecifications of business processes. In this area, Declare stands out as the\nmost widely adopted declarative process modeling language, offering a means to\nmodel processes through sets of constraints valid traces must satisfy, that can\nbe expressed in Linear Temporal Logic over Finite Traces (LTLf). Existing\nASP-based solutions encode Declare constraints by modeling the corresponding\nLTLf formula or its equivalent automaton which can be obtained using\nestablished techniques. In this paper, we introduce a novel encoding for\nDeclare constraints that directly models their semantics as ASP rules,\neliminating the need for intermediate representations. We assess the\neffectiveness of this novel approach on two Process Mining tasks by comparing\nit with alternative ASP encodings and a Python library for Declare. Under\nconsideration in Theory and Practice of Logic Programming (TPLP).",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
    "pdf_url": "http://arxiv.org/pdf/2412.10152v1",
    "published_date": "2024-12-13 14:11:33 UTC",
    "updated_date": "2024-12-13 14:11:33 UTC"
  },
  {
    "arxiv_id": "2412.10151v1",
    "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
    "authors": [
      "Hyeonseok Lim",
      "Dongjae Shin",
      "Seohyun Song",
      "Inho Won",
      "Minjun Kim",
      "Junghun Yuk",
      "Haneol Jang",
      "KyungTae Lim"
    ],
    "abstract": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025), 19 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.10151v1",
    "published_date": "2024-12-13 14:11:26 UTC",
    "updated_date": "2024-12-13 14:11:26 UTC"
  },
  {
    "arxiv_id": "2412.10138v1",
    "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL",
    "authors": [
      "Yang Qin",
      "Chao Chen",
      "Zhihang Fu",
      "Ze Chen",
      "Dezhong Peng",
      "Peng Hu",
      "Jieping Ye"
    ],
    "abstract": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10138v1",
    "published_date": "2024-12-13 13:41:18 UTC",
    "updated_date": "2024-12-13 13:41:18 UTC"
  },
  {
    "arxiv_id": "2412.10136v2",
    "title": "Can LLMs Convert Graphs to Text-Attributed Graphs?",
    "authors": [
      "Zehong Wang",
      "Sidney Liu",
      "Zheyuan Zhang",
      "Tianyi Ma",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graphs are ubiquitous structures found in numerous real-world applications,\nsuch as drug discovery, recommender systems, and social network analysis. To\nmodel graph-structured data, graph neural networks (GNNs) have become a popular\ntool. However, existing GNN architectures encounter challenges in cross-graph\nlearning where multiple graphs have different feature spaces. To address this,\nrecent approaches introduce text-attributed graphs (TAGs), where each node is\nassociated with a textual description, which can be projected into a unified\nfeature space using textual encoders. While promising, this method relies\nheavily on the availability of text-attributed graph data, which is difficult\nto obtain in practice. To bridge this gap, we propose a novel method named\nTopology-Aware Node description Synthesis (TANS), leveraging large language\nmodels (LLMs) to convert existing graphs into text-attributed graphs. The key\nidea is to integrate topological information into LLMs to explain how graph\ntopology influences node semantics. We evaluate our TANS on text-rich,\ntext-limited, and text-free graphs, demonstrating its applicability. Notably,\non text-free graphs, our method significantly outperforms existing approaches\nthat manually design node features, showcasing the potential of LLMs for\npreprocessing graph-structured data in the absence of textual information. The\ncode and data are available at https://github.com/Zehong-Wang/TANS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 25 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2412.10136v2",
    "published_date": "2024-12-13 13:32:59 UTC",
    "updated_date": "2025-02-07 01:29:07 UTC"
  },
  {
    "arxiv_id": "2412.10133v2",
    "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects",
    "authors": [
      "Islem Bouzenia",
      "Michael Pradel"
    ],
    "abstract": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "PUBLISHED AT ISSTA 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10133v2",
    "published_date": "2024-12-13 13:30:51 UTC",
    "updated_date": "2025-04-30 10:25:22 UTC"
  },
  {
    "arxiv_id": "2501.10370v1",
    "title": "Harnessing Large Language Models for Mental Health: Opportunities, Challenges, and Ethical Considerations",
    "authors": [
      "Hari Mohan Pandey"
    ],
    "abstract": "Large Language Models (LLMs) are transforming mental health care by enhancing\naccessibility, personalization, and efficiency in therapeutic interventions.\nThese AI-driven tools empower mental health professionals with real-time\nsupport, improved data integration, and the ability to encourage care-seeking\nbehaviors, particularly in underserved communities. By harnessing LLMs,\npractitioners can deliver more empathetic, tailored, and effective support,\naddressing longstanding gaps in mental health service provision. However, their\nimplementation comes with significant challenges and ethical concerns.\nPerformance limitations, data privacy risks, biased outputs, and the potential\nfor generating misleading information underscore the critical need for\nstringent ethical guidelines and robust evaluation mechanisms. The sensitive\nnature of mental health data further necessitates meticulous safeguards to\nprotect patient rights and ensure equitable access to AI-driven care.\nProponents argue that LLMs have the potential to democratize mental health\nresources, while critics warn of risks such as misuse and the diminishment of\nhuman connection in therapy. Achieving a balance between innovation and ethical\nresponsibility is imperative. This paper examines the transformative potential\nof LLMs in mental health care, highlights the associated technical and ethical\ncomplexities, and advocates for a collaborative, multidisciplinary approach to\nensure these advancements align with the goal of providing compassionate,\nequitable, and effective mental health support.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10370v1",
    "published_date": "2024-12-13 13:18:51 UTC",
    "updated_date": "2024-12-13 13:18:51 UTC"
  },
  {
    "arxiv_id": "2412.10117v3",
    "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
    "authors": [
      "Zhihao Du",
      "Yuxuan Wang",
      "Qian Chen",
      "Xian Shi",
      "Xiang Lv",
      "Tianyu Zhao",
      "Zhifu Gao",
      "Yexin Yang",
      "Changfeng Gao",
      "Hui Wang",
      "Fan Yu",
      "Huadai Liu",
      "Zhengyan Sheng",
      "Yue Gu",
      "Chong Deng",
      "Wen Wang",
      "Shiliang Zhang",
      "Zhijie Yan",
      "Jingren Zhou"
    ],
    "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Tech report, work in progress",
    "pdf_url": "http://arxiv.org/pdf/2412.10117v3",
    "published_date": "2024-12-13 12:59:39 UTC",
    "updated_date": "2024-12-25 11:54:03 UTC"
  },
  {
    "arxiv_id": "2412.10110v1",
    "title": "Label-template based Few-Shot Text Classification with Contrastive Learning",
    "authors": [
      "Guanghua Hou",
      "Shuhui Cao",
      "Deqiang Ouyang",
      "Ning Wang"
    ],
    "abstract": "As an algorithmic framework for learning to learn, meta-learning provides a\npromising solution for few-shot text classification. However, most existing\nresearch fail to give enough attention to class labels. Traditional basic\nframework building meta-learner based on prototype networks heavily relies on\ninter-class variance, and it is easily influenced by noise. To address these\nlimitations, we proposes a simple and effective few-shot text classification\nframework. In particular, the corresponding label templates are embed into\ninput sentences to fully utilize the potential value of class labels, guiding\nthe pre-trained model to generate more discriminative text representations\nthrough the semantic information conveyed by labels. With the continuous\ninfluence of label semantics, supervised contrastive learning is utilized to\nmodel the interaction information between support samples and query samples.\nFurthermore, the averaging mechanism is replaced with an attention mechanism to\nhighlight vital semantic information. To verify the proposed scheme, four\ntypical datasets are employed to assess the performance of different methods.\nExperimental results demonstrate that our method achieves substantial\nperformance enhancements and outperforms existing state-of-the-art models on\nfew-shot text classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10110v1",
    "published_date": "2024-12-13 12:51:50 UTC",
    "updated_date": "2024-12-13 12:51:50 UTC"
  },
  {
    "arxiv_id": "2412.10107v1",
    "title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models",
    "authors": [
      "Asmaa Abdallah",
      "Abdullatif Albaseer",
      "Abdulkadir Celik",
      "Mohamed Abdallah",
      "Ahmed M. Eltawil"
    ],
    "abstract": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10107v1",
    "published_date": "2024-12-13 12:48:15 UTC",
    "updated_date": "2024-12-13 12:48:15 UTC"
  },
  {
    "arxiv_id": "2412.10106v4",
    "title": "A Cascaded Dilated Convolution Approach for Mpox Lesion Classification",
    "authors": [
      "Ayush Deshmukh"
    ],
    "abstract": "The global outbreak of the Mpox virus, classified as a Public Health\nEmergency of International Concern (PHEIC) by the World Health Organization,\npresents significant diagnostic challenges due to its visual similarity to\nother skin lesion diseases. Traditional diagnostic methods for Mpox, which rely\non clinical symptoms and laboratory tests, are slow and labor intensive. Deep\nlearning-based approaches for skin lesion classification offer a promising\nalternative. However, developing a model that balances efficiency with accuracy\nis crucial to ensure reliable and timely diagnosis without compromising\nperformance. This study introduces the Cascaded Atrous Group Attention (CAGA)\nframework to address these challenges, combining the Cascaded Atrous Attention\nmodule and the Cascaded Group Attention mechanism. The Cascaded Atrous\nAttention module utilizes dilated convolutions and cascades the outputs to\nenhance multi-scale representation. This is integrated into the Cascaded Group\nAttention mechanism, which reduces redundancy in Multi-Head Self-Attention. By\nintegrating the Cascaded Atrous Group Attention module with EfficientViT-L1 as\nthe backbone architecture, this approach achieves state-of-the-art performance,\nreaching an accuracy of 98% on the Mpox Close Skin Image (MCSI) dataset while\nreducing model parameters by 37.5% compared to the original EfficientViT-L1.\nThe model's robustness is demonstrated through extensive validation on two\nadditional benchmark datasets, where it consistently outperforms existing\napproaches.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "8 pages, 4 figures, Submitted to Medical Imaging with Deep Learning",
    "pdf_url": "http://arxiv.org/pdf/2412.10106v4",
    "published_date": "2024-12-13 12:47:30 UTC",
    "updated_date": "2025-01-14 03:43:02 UTC"
  },
  {
    "arxiv_id": "2412.10104v2",
    "title": "RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector",
    "authors": [
      "Zhensheng Wang",
      "Wenmian Yang",
      "Kun Zhou",
      "Yiquan Zhang",
      "Weijia Jia"
    ],
    "abstract": "The real estate market relies heavily on structured data, such as property\ndetails, market trends, and price fluctuations. However, the lack of\nspecialized Tabular Question Answering datasets in this domain limits the\ndevelopment of automated question-answering systems. To fill this gap, we\nintroduce RETQA, the first large-scale open-domain Chinese Tabular Question\nAnswering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762\nquestion-answer pairs across 16 sub-fields within three major domains: property\ninformation, real estate company finance information and land auction\ninformation. Compared with existing tabular question answering datasets, RETQA\nposes greater challenges due to three key factors: long-table structures,\nopen-domain retrieval, and multi-domain queries. To tackle these challenges, we\npropose the SLUTQA framework, which integrates large language models with\nspoken language understanding tasks to enhance retrieval and answering\naccuracy. Extensive experiments demonstrate that SLUTQA significantly improves\nthe performance of large language models on RETQA by in-context learning. RETQA\nand SLUTQA provide essential resources for advancing tabular question answering\nresearch in the real estate domain, addressing critical challenges in\nopen-domain and long-table question-answering. The dataset and code are\npublicly available at \\url{https://github.com/jensen-w/RETQA}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper is accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10104v2",
    "published_date": "2024-12-13 12:45:14 UTC",
    "updated_date": "2025-01-23 13:18:28 UTC"
  },
  {
    "arxiv_id": "2412.10095v2",
    "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
    "authors": [
      "Jaione Bengoetxea",
      "Mikel Zubillaga",
      "Ekhi Azurmendi",
      "Maite Heredia",
      "Julen Etxaniz",
      "Markel Ferro",
      "Jeremy Barnes"
    ],
    "abstract": "In this paper we present our submission for the NorSID Shared Task as part of\nthe 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:\nIntent Detection, Slot Filling and Dialect Identification, evaluated using data\nin different dialects of the Norwegian language. For Intent Detection and Slot\nFilling, we have fine-tuned a multitask model in a cross-lingual setting, to\nleverage the xSID dataset available in 17 languages. In the case of Dialect\nIdentification, our final submission consists of a model fine-tuned on the\nprovided development set, which has obtained the highest scores within our\nexperiments. Our final results on the test set show that our models do not drop\nin performance compared to the development set, likely due to the\ndomain-specificity of the dataset and the similar distribution of both subsets.\nFinally, we also report an in-depth analysis of the provided datasets and their\nartifacts, as well as other sets of experiments that have been carried out but\ndid not yield the best results. Additionally, we present an analysis on the\nreasons why some methods have been more successful than others; mainly the\nimpact of the combination of languages and domain-specificity of the training\ndata on the results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Vardial 2025 NorSID Shared Task, fixed minor typos",
    "pdf_url": "http://arxiv.org/pdf/2412.10095v2",
    "published_date": "2024-12-13 12:31:06 UTC",
    "updated_date": "2025-01-09 09:09:32 UTC"
  },
  {
    "arxiv_id": "2412.10093v1",
    "title": "AI in the Cosmos",
    "authors": [
      "N. Sahakyan"
    ],
    "abstract": "Artificial intelligence (AI) is revolutionizing research by enabling the\nefficient analysis of large datasets and the discovery of hidden patterns. In\nastrophysics, AI has become essential, transforming the classification of\ncelestial sources, data modeling, and the interpretation of observations. In\nthis review, I highlight examples of AI applications in astrophysics, including\nsource classification, spectral energy distribution modeling, and discuss the\nadvancements achievable through generative AI. However, the use of AI\nintroduces challenges, including biases, errors, and the \"black box\" nature of\nAI models, which must be resolved before their application. These issues can be\naddressed through the concept of Human-Guided AI (HG-AI), which integrates\nhuman expertise and domain-specific knowledge into AI applications. This\napproach aims to ensure that AI is applied in a robust, interpretable, and\nethical manner, leading to deeper insights and fostering scientific excellence.",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA",
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.HE",
    "comment": "In press in the International Journal of Modern Physics D; invited\n  talk at the 17th Marcel Grossmann Meeting",
    "pdf_url": "http://arxiv.org/pdf/2412.10093v1",
    "published_date": "2024-12-13 12:30:11 UTC",
    "updated_date": "2024-12-13 12:30:11 UTC"
  },
  {
    "arxiv_id": "2412.10091v1",
    "title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for Object Re-identification",
    "authors": [
      "Zi Yang",
      "Haojin Yang",
      "Soumajit Majumder",
      "Jorge Cardoso",
      "Guillermo Gallego"
    ],
    "abstract": "Previous studies have demonstrated that not each sample in a dataset is of\nequal importance during training. Data pruning aims to remove less important or\ninformative samples while still achieving comparable results as training on the\noriginal (untruncated) dataset, thereby reducing storage and training costs.\nHowever, the majority of data pruning methods are applied to image\nclassification tasks. To our knowledge, this work is the first to explore the\nfeasibility of these pruning methods applied to object re-identification (ReID)\ntasks, while also presenting a more comprehensive data pruning approach. By\nfully leveraging the logit history during training, our approach offers a more\naccurate and comprehensive metric for quantifying sample importance, as well as\ncorrecting mislabeled samples and recognizing outliers. Furthermore, our\napproach is highly efficient, reducing the cost of importance score estimation\nby 10 times compared to existing methods. Our approach is a plug-and-play,\narchitecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of\nsamples/training time on the VeRi, MSMT17 and Market1501 datasets,\nrespectively, with negligible loss in accuracy (< 0.1%). The lists of\nimportant, mislabeled, and outlier samples from these ReID datasets are\navailable at https://github.com/Zi-Y/data-pruning-reid.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10091v1",
    "published_date": "2024-12-13 12:27:47 UTC",
    "updated_date": "2024-12-13 12:27:47 UTC"
  },
  {
    "arxiv_id": "2412.15242v1",
    "title": "Script-Based Dialog Policy Planning for LLM-Powered Conversational Agents: A Basic Architecture for an \"AI Therapist\"",
    "authors": [
      "Robert Wasenmüller",
      "Kevin Hilbert",
      "Christoph Benzmüller"
    ],
    "abstract": "Large Language Model (LLM)-Powered Conversational Agents have the potential\nto provide users with scaled behavioral healthcare support, and potentially\neven deliver full-scale \"AI therapy'\" in the future. While such agents can\nalready conduct fluent and proactive emotional support conversations, they\ninherently lack the ability to (a) consistently and reliably act by predefined\nrules to align their conversation with an overarching therapeutic concept and\n(b) make their decision paths inspectable for risk management and clinical\nevaluation -- both essential requirements for an \"AI Therapist\".\n  In this work, we introduce a novel paradigm for dialog policy planning in\nconversational agents enabling them to (a) act according to an expert-written\n\"script\" that outlines the therapeutic approach and (b) explicitly transition\nthrough a finite set of states over the course of the conversation. The script\nacts as a deterministic component, constraining the LLM's behavior in desirable\nways and establishing a basic architecture for an AI Therapist.\n  We implement two variants of Script-Based Dialog Policy Planning using\ndifferent prompting techniques and synthesize a total of 100 conversations with\nLLM-simulated patients. The results demonstrate the feasibility of this new\ntechnology and provide insights into the efficiency and effectiveness of\ndifferent implementation variants.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T01"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.15242v1",
    "published_date": "2024-12-13 12:12:47 UTC",
    "updated_date": "2024-12-13 12:12:47 UTC"
  },
  {
    "arxiv_id": "2412.10059v1",
    "title": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity",
    "authors": [
      "Dongyun Kam",
      "Myeongji Yun",
      "Sunwoo Yoo",
      "Seungwoo Hong",
      "Zhengya Zhang",
      "Youngjoo Lee"
    ],
    "abstract": "Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "15 pages, 20 figures, Accepted to HPCA 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.10059v1",
    "published_date": "2024-12-13 11:44:09 UTC",
    "updated_date": "2024-12-13 11:44:09 UTC"
  },
  {
    "arxiv_id": "2412.10056v1",
    "title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?",
    "authors": [
      "Zhikai Lei",
      "Tianyi Liang",
      "Hanglei Hu",
      "Jin Zhang",
      "Yunhua Zhou",
      "Yunfan Shao",
      "Linyang Li",
      "Chenchui Li",
      "Changbo Wang",
      "Hang Yan",
      "Qipeng Guo"
    ],
    "abstract": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10056v1",
    "published_date": "2024-12-13 11:38:10 UTC",
    "updated_date": "2024-12-13 11:38:10 UTC"
  },
  {
    "arxiv_id": "2412.10051v1",
    "title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views",
    "authors": [
      "Liang Zhao",
      "Zehan Bao",
      "Yi Xie",
      "Hong Chen",
      "Yaohui Chen",
      "Weifu Li"
    ],
    "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field,\nachieving both panoptic and interactive segmentation of 3D scenes. However,\nexisting methodologies often overlook the critical need for reconstructing\nspecified targets with complex structures from sparse views. To address this\nissue, we introduce TSGaussian, a novel framework that combines semantic\nconstraints with depth priors to avoid geometry degradation in challenging\nnovel view synthesis tasks. Our approach prioritizes computational resources on\ndesignated targets while minimizing background allocation. Bounding boxes from\nYOLOv9 serve as prompts for Segment Anything Model to generate 2D mask\npredictions, ensuring semantic accuracy and cost efficiency. TSGaussian\neffectively clusters 3D gaussians by introducing a compact identity encoding\nfor each Gaussian ellipsoid and incorporating 3D spatial consistency\nregularization. Leveraging these modules, we propose a pruning strategy to\neffectively reduce redundancy in 3D gaussians. Extensive experiments\ndemonstrate that TSGaussian outperforms state-of-the-art methods on three\nstandard datasets and a new challenging dataset we collected, achieving\nsuperior results in novel view synthesis of specific objects. Code is available\nat: https://github.com/leon2000-ai/TSGaussian.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10051v1",
    "published_date": "2024-12-13 11:26:38 UTC",
    "updated_date": "2024-12-13 11:26:38 UTC"
  },
  {
    "arxiv_id": "2412.10047v2",
    "title": "Large Action Models: From Inception to Implementation",
    "authors": [
      "Lu Wang",
      "Fangkai Yang",
      "Chaoyun Zhang",
      "Junting Lu",
      "Jiaxu Qian",
      "Shilin He",
      "Pu Zhao",
      "Bo Qiao",
      "Ray Huang",
      "Si Qin",
      "Qisheng Su",
      "Jiayi Ye",
      "Yudi Zhang",
      "Jian-Guang Lou",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "abstract": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25pages,12 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10047v2",
    "published_date": "2024-12-13 11:19:56 UTC",
    "updated_date": "2025-01-13 06:47:21 UTC"
  },
  {
    "arxiv_id": "2412.10483v1",
    "title": "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models",
    "authors": [
      "Ruibang Liu",
      "Guoqiang Li",
      "Minyu Chen",
      "Ling-I Wu",
      "Jingyu Ke"
    ],
    "abstract": "Automated program verification has always been an important component of\nbuilding trustworthy software. While the analysis of real-world programs\nremains a theoretical challenge, the automation of loop invariant analysis has\neffectively resolved the problem. However, real-world programs that often mix\ncomplex data structures and control flows pose challenges to traditional loop\ninvariant generation tools. To enhance the applicability of invariant\ngeneration techniques, we proposed ACInv, an Automated Complex program loop\nInvariant generation tool, which combines static analysis with Large Language\nModels (LLMs) to generate the proper loop invariants. We utilize static\nanalysis to extract the necessary information for each loop and embed it into\nprompts for the LLM to generate invariants for each loop. Subsequently, we\nemploy an LLM-based evaluator to assess the generated invariants, refining them\nby either strengthening, weakening, or rejecting them based on their\ncorrectness, ultimately obtaining enhanced invariants. We conducted experiments\non ACInv, which showed that ACInv outperformed previous tools on data sets with\ndata structures, and maintained similar performance to the state-of-the-art\ntool AutoSpec on numerical programs without data structures. For the total data\nset, ACInv can solve 21% more examples than AutoSpec and can generate reference\ndata structure templates.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10483v1",
    "published_date": "2024-12-13 10:36:18 UTC",
    "updated_date": "2024-12-13 10:36:18 UTC"
  },
  {
    "arxiv_id": "2412.12185v1",
    "title": "Graph Similarity Computation via Interpretable Neural Node Alignment",
    "authors": [
      "Jingjing Wang",
      "Hongjie Zhu",
      "Haoran Xie",
      "Fu Lee Wang",
      "Xiaoliang Xu",
      "Yuxiang Wang"
    ],
    "abstract": "\\Graph similarity computation is an essential task in many real-world\ngraph-related applications such as retrieving the similar drugs given a query\nchemical compound or finding the user's potential friends from the social\nnetwork database. Graph Edit Distance (GED) and Maximum Common Subgraphs (MCS)\nare the two commonly used domain-agnostic metrics to evaluate graph similarity\nin practice. Unfortunately, computing the exact GED is known to be a NP-hard\nproblem. To solve this limitation, neural network based models have been\nproposed to approximate the calculations of GED/MCS. However, deep learning\nmodels are well-known ``black boxes'', thus the typically characteristic\none-to-one node/subgraph alignment process in the classical computations of GED\nand MCS cannot be seen. Existing methods have paid attention to approximating\nthe node/subgraph alignment (soft alignment), but the one-to-one node alignment\n(hard alignment) has not yet been solved. To fill this gap, in this paper we\npropose a novel interpretable neural node alignment model without relying on\nnode alignment ground truth information. Firstly, the quadratic assignment\nproblem in classical GED computation is relaxed to a linear alignment via\nembedding the features in the node embedding space. Secondly, a differentiable\nGumbel-Sinkhorn module is proposed to unsupervised generate the optimal\none-to-one node alignment matrix. Experimental results in real-world graph\ndatasets demonstrate that our method outperforms the state-of-the-art methods\nin graph similarity computation and graph retrieval tasks, achieving up to 16\\%\nreduction in the Mean Squared Error and up to 12\\% improvement in the retrieval\nevaluation metrics, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12185v1",
    "published_date": "2024-12-13 10:23:27 UTC",
    "updated_date": "2024-12-13 10:23:27 UTC"
  },
  {
    "arxiv_id": "2412.10482v1",
    "title": "Dynamic Entity-Masked Graph Diffusion Model for histopathological image Representation Learning",
    "authors": [
      "Zhenfeng Zhuang",
      "Min Cen",
      "Yanfeng Li",
      "Fangyu Zhou",
      "Lequan Yu",
      "Baptiste Magnier",
      "Liansheng Wang"
    ],
    "abstract": "Significant disparities between the features of natural images and those\ninherent to histopathological images make it challenging to directly apply and\ntransfer pre-trained models from natural images to histopathology tasks.\nMoreover, the frequent lack of annotations in histopathology patch images has\ndriven researchers to explore self-supervised learning methods like mask\nreconstruction for learning representations from large amounts of unlabeled\ndata. Crucially, previous mask-based efforts in self-supervised learning have\noften overlooked the spatial interactions among entities, which are essential\nfor constructing accurate representations of pathological entities. To address\nthese challenges, constructing graphs of entities is a promising approach. In\naddition, the diffusion reconstruction strategy has recently shown superior\nperformance through its random intensity noise addition technique to enhance\nthe robust learned representation. Therefore, we introduce H-MGDM, a novel\nself-supervised Histopathology image representation learning method through the\nDynamic Entity-Masked Graph Diffusion Model. Specifically, we propose to use\ncomplementary subgraphs as latent diffusion conditions and self-supervised\ntargets respectively during pre-training. We note that the graph can embed\nentities' topological relationships and enhance representation. Dynamic\nconditions and targets can improve pathological fine reconstruction. Our model\nhas conducted pretraining experiments on three large histopathological\ndatasets. The advanced predictive performance and interpretability of H-MGDM\nare clearly evaluated on comprehensive downstream tasks such as classification\nand survival analysis on six datasets. Our code will be publicly available at\nhttps://github.com/centurion-crawler/H-MGDM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10482v1",
    "published_date": "2024-12-13 10:18:36 UTC",
    "updated_date": "2024-12-13 10:18:36 UTC"
  },
  {
    "arxiv_id": "2412.10011v1",
    "title": "Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework",
    "authors": [
      "Niloy Kumar Kundu",
      "Sarah Kobir",
      "Md. Rayhan Ahmed",
      "Tahmina Aktar",
      "Niloya Roy"
    ],
    "abstract": "Speech emotion recognition (SER) is crucial for enhancing affective computing\nand enriching the domain of human-computer interaction. However, the main\nchallenge in SER lies in selecting relevant feature representations from speech\nsignals with lower computational costs. In this paper, we propose a lightweight\nSER architecture that integrates attention-based local feature blocks (ALFBs)\nto capture high-level relevant feature vectors from speech signals. We also\nincorporate a global feature block (GFB) technique to capture sequential,\nglobal information and long-term dependencies in speech signals. By aggregating\nattention-based local and global contextual feature vectors, our model\neffectively captures the internal correlation between salient features that\nreflect complex human emotional cues. To evaluate our approach, we extracted\nfour types of spectral features from speech audio samples: mel-frequency\ncepstral coefficients, mel-spectrogram, root mean square value, and\nzero-crossing rate. Through a 5-fold cross-validation strategy, we tested the\nproposed method on five multi-lingual standard benchmark datasets: TESS,\nRAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of\n99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate\nthat our model achieves state-of-the-art (SOTA) performance compared to most\nexisting methods.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "42 pages,10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.10011v1",
    "published_date": "2024-12-13 09:55:03 UTC",
    "updated_date": "2024-12-13 09:55:03 UTC"
  },
  {
    "arxiv_id": "2412.15241v3",
    "title": "Quantifying Positional Biases in Text Embedding Models",
    "authors": [
      "Samarth Goel",
      "Reagan J. Lee",
      "Kannan Ramchandran"
    ],
    "abstract": "Embedding models are crucial for tasks in Information Retrieval (IR) and\nsemantic similarity measurement, yet their handling of longer texts and\nassociated positional biases remains underexplored. In this study, we\ninvestigate the impact of content position and input size on text embeddings.\nOur experiments reveal that embedding models, irrespective of their positional\nencoding mechanisms, disproportionately prioritize the beginning of an input.\nAblation studies demonstrate that insertion of irrelevant text or removal at\nthe start of a document reduces cosine similarity between altered and original\nembeddings by up to 12.3% more than ablations at the end. Regression analysis\nfurther confirms this bias, with sentence importance declining as position\nmoves further from the start, even with with content-agnosticity. We\nhypothesize that this effect arises from pre-processing strategies and chosen\npositional encoding techniques. These findings quantify the sensitivity of\nretrieval systems and suggest a new lens towards embedding model robustness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 11 figures, NeurIPS",
    "pdf_url": "http://arxiv.org/pdf/2412.15241v3",
    "published_date": "2024-12-13 09:52:25 UTC",
    "updated_date": "2025-01-01 18:06:08 UTC"
  },
  {
    "arxiv_id": "2412.09998v2",
    "title": "Self-Consistent Nested Diffusion Bridge for Accelerated MRI Reconstruction",
    "authors": [
      "Tao Song",
      "Yicheng Wu",
      "Minhao Hu",
      "Xiangde Luo",
      "Guoting Luo",
      "Guotai Wang",
      "Yi Guo",
      "Feng Xu",
      "Shaoting Zhang"
    ],
    "abstract": "Accelerated MRI reconstruction plays a vital role in reducing scan time while\npreserving image quality. While most existing methods rely on complex-valued\nimage-space or k-space data, these formats are often inaccessible in clinical\npractice due to proprietary reconstruction pipelines, leaving only magnitude\nimages stored in DICOM files. To address this gap, we focus on the\nunderexplored task of magnitude-image-based MRI reconstruction. Recent\nadvancements in diffusion models, particularly denoising diffusion\nprobabilistic models (DDPMs), have demonstrated strong capabilities in modeling\nimage priors. However, their task-agnostic denoising nature limits performance\nin source-to-target image translation tasks, such as MRI reconstruction. In\nthis work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB)\nframework that models accelerated MRI reconstruction as a bi-directional image\ntranslation process between under-sampled and fully-sampled magnitude MRI\nimages. SC-NDB introduces a nested diffusion architecture with a\nself-consistency constraint and reverse bridge diffusion pathways to improve\nintermediate prediction fidelity and better capture the explicit priors of\nsource images. Furthermore, we incorporate a Contour Decomposition Embedding\nModule (CDEM) to inject structural and textural knowledge by leveraging\nLaplacian pyramids and directional filter banks. Extensive experiments on the\nfastMRI and IXI datasets demonstrate that our method achieves state-of-the-art\nperformance compared to both magnitude-based and non-magnitude-based diffusion\nmodels, confirming the effectiveness and clinical relevance of SC-NDB.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09998v2",
    "published_date": "2024-12-13 09:35:34 UTC",
    "updated_date": "2025-04-28 02:56:06 UTC"
  },
  {
    "arxiv_id": "2412.10481v1",
    "title": "Tipping Points, Pulse Elasticity and Tonal Tension: An Empirical Study on What Generates Tipping Points",
    "authors": [
      "Canishk Naik",
      "Elaine Chew"
    ],
    "abstract": "Tipping points are moments of change that characterise crucial turning points\nin a piece of music. This study presents a first step towards quantitatively\nand systematically describing the musical properties of tipping points. Timing\ninformation and computationally-derived tonal tension values which correspond\nto dissonance, distance from key, and harmonic motion are compared to tipping\npoints in Ashkenazy's recordings of six Chopin Mazurkas, as identified by 35\nlisteners. The analysis shows that all popular tipping points but one could be\nexplained by statistically significant timing deviations or changepoints in at\nleast one of the three tension parameters.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "International Society for Music Information Retrieval Conference, Oct\n  2017, Suzhou, China, China",
    "pdf_url": "http://arxiv.org/pdf/2412.10481v1",
    "published_date": "2024-12-13 09:27:46 UTC",
    "updated_date": "2024-12-13 09:27:46 UTC"
  },
  {
    "arxiv_id": "2501.04008v2",
    "title": "A Generative AI-driven Metadata Modelling Approach",
    "authors": [
      "Mayukh Bagchi"
    ],
    "abstract": "Since decades, the modelling of metadata has been core to the functioning of\nany academic library. Its importance has only enhanced with the increasing\npervasiveness of Generative Artificial Intelligence (AI)-driven information\nactivities and services which constitute a library's outreach. However, with\nthe rising importance of metadata, there arose several outstanding problems\nwith the process of designing a library metadata model impacting its\nreusability, crosswalk and interoperability with other metadata models. This\npaper posits that the above problems stem from an underlying thesis that there\nshould only be a few core metadata models which would be necessary and\nsufficient for any information service using them, irrespective of the\nheterogeneity of intra-domain or inter-domain settings. To that end, this paper\nadvances a contrary view of the above thesis and substantiates its argument in\nthree key steps. First, it introduces a novel way of thinking about a library\nmetadata model as an ontology-driven composition of five functionally\ninterlinked representation levels from perception to its intensional definition\nvia properties. Second, it introduces the representational manifoldness\nimplicit in each of the five levels which cumulatively contributes to a\nconceptually entangled library metadata model. Finally, and most importantly,\nit proposes a Generative AI-driven Human-Large Language Model (LLM)\ncollaboration based metadata modelling approach to disentangle the entanglement\ninherent in each representation level leading to the generation of a\nconceptually disentangled metadata model. Throughout the paper, the arguments\nare exemplified by motivating scenarios and examples from representative\nlibraries handling cancer information.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "Accepted for publication @ Special Issue on \"Generative AI and\n  Libraries\" - Library Trends Journal, Johns Hopkins University Press,\n  Maryland, USA",
    "pdf_url": "http://arxiv.org/pdf/2501.04008v2",
    "published_date": "2024-12-13 09:26:04 UTC",
    "updated_date": "2025-03-16 21:12:51 UTC"
  },
  {
    "arxiv_id": "2412.09991v1",
    "title": "Visual Object Tracking across Diverse Data Modalities: A Review",
    "authors": [
      "Mengmeng Wang",
      "Teli Ma",
      "Shuo Xin",
      "Xiaojun Hou",
      "Jiazheng Xing",
      "Guang Dai",
      "Jingdong Wang",
      "Yong Liu"
    ],
    "abstract": "Visual Object Tracking (VOT) is an attractive and significant research area\nin computer vision, which aims to recognize and track specific targets in video\nsequences where the target objects are arbitrary and class-agnostic. The VOT\ntechnology could be applied in various scenarios, processing data of diverse\nmodalities such as RGB, thermal infrared and point cloud. Besides, since no one\nsensor could handle all the dynamic and varying environments, multi-modal VOT\nis also investigated. This paper presents a comprehensive survey of the recent\nprogress of both single-modal and multi-modal VOT, especially the deep learning\nmethods. Specifically, we first review three types of mainstream single-modal\nVOT, including RGB, thermal infrared and point cloud tracking. In particular,\nwe conclude four widely-used single-modal frameworks, abstracting their schemas\nand categorizing the existing inheritors. Then we summarize four kinds of\nmulti-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language.\nMoreover, the comparison results in plenty of VOT benchmarks of the discussed\nmodalities are presented. Finally, we provide recommendations and insightful\nobservations, inspiring the future development of this fast-growing literature.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09991v1",
    "published_date": "2024-12-13 09:25:18 UTC",
    "updated_date": "2024-12-13 09:25:18 UTC"
  },
  {
    "arxiv_id": "2412.09990v1",
    "title": "Small Language Model as Data Prospector for Large Language Model",
    "authors": [
      "Shiwen Ni",
      "Haihong Wu",
      "Di Yang",
      "Qiang Qu",
      "Hamid Alinejad-Rokny",
      "Min Yang"
    ],
    "abstract": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09990v1",
    "published_date": "2024-12-13 09:23:58 UTC",
    "updated_date": "2024-12-13 09:23:58 UTC"
  },
  {
    "arxiv_id": "2412.09989v1",
    "title": "One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation in Unknown Environments",
    "authors": [
      "Albert Lin",
      "Shuang Peng",
      "Somil Bansal"
    ],
    "abstract": "As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website:\n  https://sia-lab-git.github.io/One_Filter_to_Deploy_Them_All/",
    "pdf_url": "http://arxiv.org/pdf/2412.09989v1",
    "published_date": "2024-12-13 09:21:02 UTC",
    "updated_date": "2024-12-13 09:21:02 UTC"
  },
  {
    "arxiv_id": "2412.12183v1",
    "title": "Adopting Explainable-AI to investigate the impact of urban morphology design on energy and environmental performance in dry-arid climates",
    "authors": [
      "Pegah Eshraghi",
      "Riccardo Talami",
      "Arman Nikkhah Dehnavi",
      "Maedeh Mirdamadi",
      "Zahra-Sadat Zomorodian"
    ],
    "abstract": "In rapidly urbanizing regions, designing climate-responsive urban forms is\ncrucial for sustainable development, especially in dry arid-climates where\nurban morphology has a significant impact on energy consumption and\nenvironmental performance. This study advances urban morphology evaluation by\ncombining Urban Building Energy Modeling (UBEM) with machine learning methods\n(ML) and Explainable AI techniques, specifically Shapley Additive Explanations\n(SHAP). Using Tehran's dense urban landscape as a case study, this research\nassesses and ranks the impact of 30 morphology parameters at the urban block\nlevel on key energy metrics (cooling, heating, and lighting demand) and\nenvironmental performance (sunlight exposure, photovoltaic generation, and Sky\nView Factor). Among seven ML algorithms evaluated, the XGBoost model was the\nmost effective predictor, achieving high accuracy (R2: 0.92) and a training\ntime of 3.64 seconds. Findings reveal that building shape, window-to-wall\nratio, and commercial ratio are the most critical parameters affecting energy\nefficiency, while the heights and distances of neighboring buildings strongly\ninfluence cooling demand and solar access. By evaluating urban blocks with\nvaried densities and configurations, this study offers generalizable insights\napplicable to other dry-arid regions. Moreover, the integration of UBEM and\nExplainable AI offers a scalable, data-driven framework for developing\nclimate-responsive urban designs adaptable to high-density environments\nworldwide.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12183v1",
    "published_date": "2024-12-13 09:19:49 UTC",
    "updated_date": "2024-12-13 09:19:49 UTC"
  },
  {
    "arxiv_id": "2412.09988v1",
    "title": "AI and the Future of Digital Public Squares",
    "authors": [
      "Beth Goldberg",
      "Diana Acosta-Navas",
      "Michiel Bakker",
      "Ian Beacock",
      "Matt Botvinick",
      "Prateek Buch",
      "Renée DiResta",
      "Nandika Donthi",
      "Nathanael Fast",
      "Ravi Iyer",
      "Zaria Jalan",
      "Andrew Konya",
      "Grace Kwak Danciu",
      "Hélène Landemore",
      "Alice Marwick",
      "Carl Miller",
      "Aviv Ovadya",
      "Emily Saltz",
      "Lisa Schirch",
      "Dalit Shalom",
      "Divya Siddarth",
      "Felix Sieker",
      "Christopher Small",
      "Jonathan Stray",
      "Audrey Tang",
      "Michael Henry Tessler",
      "Amy Zhang"
    ],
    "abstract": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "40 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09988v1",
    "published_date": "2024-12-13 09:15:20 UTC",
    "updated_date": "2024-12-13 09:15:20 UTC"
  },
  {
    "arxiv_id": "2412.09981v2",
    "title": "SUMI-IFL: An Information-Theoretic Framework for Image Forgery Localization with Sufficiency and Minimality Constraints",
    "authors": [
      "Ziqi Sheng",
      "Wei Lu",
      "Xiangyang Luo",
      "Jiantao Zhou",
      "Xiaochun Cao"
    ],
    "abstract": "Image forgery localization (IFL) is a crucial technique for preventing\ntampered image misuse and protecting social safety. However, due to the rapid\ndevelopment of image tampering technologies, extracting more comprehensive and\naccurate forgery clues remains an urgent challenge. To address these\nchallenges, we introduce a novel information-theoretic IFL framework named\nSUMI-IFL that imposes sufficiency-view and minimality-view constraints on\nforgery feature representation. First, grounded in the theoretical analysis of\nmutual information, the sufficiency-view constraint is enforced on the feature\nextraction network to ensure that the latent forgery feature contains\ncomprehensive forgery clues. Considering that forgery clues obtained from a\nsingle aspect alone may be incomplete, we construct the latent forgery feature\nby integrating several individual forgery features from multiple perspectives.\nSecond, based on the information bottleneck, the minimality-view constraint is\nimposed on the feature reasoning network to achieve an accurate and concise\nforgery feature representation that counters the interference of task-unrelated\nfeatures. Extensive experiments show the superior performance of SUMI-IFL to\nexisting state-of-the-art methods, not only on in-dataset comparisons but also\non cross-dataset comparisons.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09981v2",
    "published_date": "2024-12-13 09:08:02 UTC",
    "updated_date": "2024-12-27 05:58:54 UTC"
  },
  {
    "arxiv_id": "2412.09972v2",
    "title": "Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective",
    "authors": [
      "Yuchen Fang",
      "Yuxuan Liang",
      "Bo Hui",
      "Zezhi Shao",
      "Liwei Deng",
      "Xu Liu",
      "Xinke Jiang",
      "Kai Zheng"
    ],
    "abstract": "Road traffic forecasting is crucial in real-world intelligent transportation\nscenarios like traffic dispatching and path planning in city management and\npersonal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as\nthe mainstream solution in this task. Nevertheless, the quadratic complexity of\nremarkable dynamic spatial modeling-based STGNNs has become the bottleneck over\nlarge-scale traffic data. From the spatial data management perspective, we\npresent a novel Transformer framework called PatchSTG to efficiently and\ndynamically model spatial dependencies for large-scale traffic forecasting with\ninterpretability and fidelity. Specifically, we design a novel irregular\nspatial patching to reduce the number of points involved in the dynamic\ncalculation of Transformer. The irregular spatial patching first utilizes the\nleaf K-dimensional tree (KDTree) to recursively partition irregularly\ndistributed traffic points into leaf nodes with a small capacity, and then\nmerges leaf nodes belonging to the same subtree into occupancy-equaled and\nnon-overlapped patches through padding and backtracking. Based on the patched\ndata, depth and breadth attention are used interchangeably in the encoder to\ndynamically learn local and global spatial knowledge from points in a patch and\npoints with the same index of patches. Experimental results on four real world\nlarge-scale traffic datasets show that our PatchSTG achieves train speed and\nmemory utilization improvements up to $10\\times$ and $4\\times$ with the\nstate-of-the-art performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SIGKDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09972v2",
    "published_date": "2024-12-13 08:59:18 UTC",
    "updated_date": "2024-12-31 03:52:09 UTC"
  },
  {
    "arxiv_id": "2412.09966v1",
    "title": "EP-CFG: Energy-Preserving Classifier-Free Guidance",
    "authors": [
      "Kai Zhang",
      "Fujun Luan",
      "Sai Bi",
      "Jianming Zhang"
    ],
    "abstract": "Classifier-free guidance (CFG) is widely used in diffusion models but often\nintroduces over-contrast and over-saturation artifacts at higher guidance\nstrengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),\nwhich addresses these issues by preserving the energy distribution of the\nconditional prediction during the guidance process. Our method simply rescales\nthe energy of the guided output to match that of the conditional prediction at\neach denoising step, with an optional robust variant for improved artifact\nsuppression. Through experiments, we show that EP-CFG maintains natural image\nquality and preserves details across guidance strengths while retaining CFG's\nsemantic alignment benefits, all with minimal computational overhead.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09966v1",
    "published_date": "2024-12-13 08:49:25 UTC",
    "updated_date": "2024-12-13 08:49:25 UTC"
  },
  {
    "arxiv_id": "2412.09961v2",
    "title": "What constitutes a Deep Fake? The blurry line between legitimate processing and manipulation under the EU AI Act",
    "authors": [
      "Kristof Meding",
      "Christoph Sorge"
    ],
    "abstract": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Accepted at ACM CS&Law '25",
    "pdf_url": "http://arxiv.org/pdf/2412.09961v2",
    "published_date": "2024-12-13 08:42:19 UTC",
    "updated_date": "2025-02-04 09:15:56 UTC"
  },
  {
    "arxiv_id": "2412.15240v1",
    "title": "ChainStream: An LLM-based Framework for Unified Synthetic Sensing",
    "authors": [
      "Jiacheng Liu",
      "Yuanchun Li",
      "Liangyan Li",
      "Yi Sun",
      "Hao Wen",
      "Xiangyu Li",
      "Yao Guo",
      "Yunxin Liu"
    ],
    "abstract": "Many applications demand context sensing to offer personalized and timely\nservices. Yet, developing sensing programs can be challenging for developers\nand using them is privacy-concerning for end-users. In this paper, we propose\nto use natural language as the unified interface to process personal data and\nsense user context, which can effectively ease app development and make the\ndata pipeline more transparent. Our work is inspired by large language models\n(LLMs) and other generative models, while directly applying them does not solve\nthe problem - letting the model directly process the data cannot handle complex\nsensing requests and letting the model write the data processing program\nsuffers error-prone code generation. We address the problem with 1) a unified\ndata processing framework that makes context-sensing programs simpler and 2) a\nfeedback-guided query optimizer that makes data query more informative. To\nevaluate the performance of natural language-based context sensing, we create a\nbenchmark that contains 133 context sensing tasks. Extensive evaluation has\nshown that our approach is able to automatically solve the context-sensing\ntasks efficiently and precisely. The code is opensourced at\nhttps://github.com/MobileLLM/ChainStream.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15240v1",
    "published_date": "2024-12-13 08:25:26 UTC",
    "updated_date": "2024-12-13 08:25:26 UTC"
  },
  {
    "arxiv_id": "2412.09946v1",
    "title": "Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework",
    "authors": [
      "Qiao Sun",
      "Jiexin Xie",
      "Nanyang Ye",
      "Qinying Gu",
      "Shijie Guo"
    ],
    "abstract": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09946v1",
    "published_date": "2024-12-13 08:10:56 UTC",
    "updated_date": "2024-12-13 08:10:56 UTC"
  },
  {
    "arxiv_id": "2412.09919v1",
    "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens",
    "authors": [
      "Zhuqiang Lu",
      "Zhenfei Yin",
      "Mengwei He",
      "Zhihui Wang",
      "Zicheng Liu",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "abstract": "Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09919v1",
    "published_date": "2024-12-13 07:13:40 UTC",
    "updated_date": "2024-12-13 07:13:40 UTC"
  },
  {
    "arxiv_id": "2412.09902v2",
    "title": "One Node One Model: Featuring the Missing-Half for Graph Clustering",
    "authors": [
      "Xuanting Xie",
      "Bingheng Li",
      "Erlin Pan",
      "Zhaochen Guo",
      "Zhao Kang",
      "Wenyu Chen"
    ],
    "abstract": "Most existing graph clustering methods primarily focus on exploiting\ntopological structure, often neglecting the ``missing-half\" node feature\ninformation, especially how these features can enhance clustering performance.\nThis issue is further compounded by the challenges associated with\nhigh-dimensional features. Feature selection in graph clustering is\nparticularly difficult because it requires simultaneously discovering clusters\nand identifying the relevant features for these clusters. To address this gap,\nwe introduce a novel paradigm called ``one node one model\", which builds an\nexclusive model for each node and defines the node label as a combination of\npredictions for node groups. Specifically, the proposed ``Feature Personalized\nGraph Clustering (FPGC)\" method identifies cluster-relevant features for each\nnode using a squeeze-and-excitation block, integrating these features into each\nmodel to form the final representations. Additionally, the concept of feature\ncross is developed as a data augmentation technique to learn low-order feature\ninteractions. Extensive experimental results demonstrate that FPGC outperforms\nstate-of-the-art clustering methods. Moreover, the plug-and-play nature of our\nmethod provides a versatile solution to enhance GNN-based models from a feature\nperspective.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09902v2",
    "published_date": "2024-12-13 06:42:36 UTC",
    "updated_date": "2024-12-18 04:51:33 UTC"
  },
  {
    "arxiv_id": "2412.09896v2",
    "title": "Analyzing Fairness of Classification Machine Learning Model with Structured Dataset",
    "authors": [
      "Ahmed Rashed",
      "Abdelkrim Kallich",
      "Mohamed Eltayeb"
    ],
    "abstract": "Machine learning (ML) algorithms have become integral to decision making in\nvarious domains, including healthcare, finance, education, and law enforcement.\nHowever, concerns about fairness and bias in these systems pose significant\nethical and social challenges. This study investigates the fairness of ML\nmodels applied to structured datasets in classification tasks, highlighting the\npotential for biased predictions to perpetuate systemic inequalities. A\npublicly available dataset from Kaggle was selected for analysis, offering a\nrealistic scenario for evaluating fairness in machine learning workflows.\n  To assess and mitigate biases, three prominent fairness libraries; Fairlearn\nby Microsoft, AIF360 by IBM, and the What If Tool by Google were employed.\nThese libraries provide robust frameworks for analyzing fairness, offering\ntools to evaluate metrics, visualize results, and implement bias mitigation\nstrategies. The research aims to assess the extent of bias in the ML models,\ncompare the effectiveness of these libraries, and derive actionable insights\nfor practitioners.\n  The findings reveal that each library has unique strengths and limitations in\nfairness evaluation and mitigation. By systematically comparing their\ncapabilities, this study contributes to the growing field of ML fairness by\nproviding practical guidance for integrating fairness tools into real world\napplications. These insights are intended to support the development of more\nequitable machine learning systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.09896v2",
    "published_date": "2024-12-13 06:31:09 UTC",
    "updated_date": "2024-12-17 00:57:15 UTC"
  },
  {
    "arxiv_id": "2412.09889v1",
    "title": "Semi-Periodic Activation for Time Series Classification",
    "authors": [
      "José Gilberto Barbosa de Medeiros Júnior",
      "Andre Guarnier de Mitri",
      "Diego Furtado Silva"
    ],
    "abstract": "This paper investigates the lack of research on activation functions for\nneural network models in time series tasks. It highlights the need to identify\nessential properties of these activations to improve their effectiveness in\nspecific domains. To this end, the study comprehensively analyzes properties,\nsuch as bounded, monotonic, nonlinearity, and periodicity, for activation in\ntime series neural networks. We propose a new activation that maximizes the\ncoverage of these properties, called LeakySineLU. We empirically evaluate the\nLeakySineLU against commonly used activations in the literature using 112\nbenchmark datasets for time series classification, obtaining the best average\nranking in all comparative scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09889v1",
    "published_date": "2024-12-13 06:06:49 UTC",
    "updated_date": "2024-12-13 06:06:49 UTC"
  },
  {
    "arxiv_id": "2412.09887v2",
    "title": "CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls",
    "authors": [
      "Li Chai",
      "Donglin Wang"
    ],
    "abstract": "Lyric-to-melody generation is a highly challenging task in the field of AI\nmusic generation. Due to the difficulty of learning strict yet weak\ncorrelations between lyrics and melodies, previous methods have suffered from\nweak controllability, low-quality and poorly structured generation. To address\nthese challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody\ngeneration method based on an in-attention Transformer decoder with\nfine-grained lyric and musical controls, which is able to generate full-song\nmelodies matched with the given lyrics and user-specified musical attributes.\nSpecifically, we first introduce REMI-Aligned, a novel music representation\nthat incorporates strict syllable- and sentence-level alignments between lyrics\nand melodies, facilitating precise alignment modeling. Subsequently,\nsentence-level semantic lyric embeddings independently extracted from a\nsentence-wise Transformer encoder are combined with word-level part-of-speech\nembeddings and syllable-level tone embeddings as fine-grained controls to\nenhance the controllability of lyrics over melody generation. Then we introduce\nhuman-labeled musical tags, sentence-level statistical musical attributes, and\nlearned musical features extracted from a pre-trained VQ-VAE as coarse-grained,\nfine-grained and high-fidelity controls, respectively, to the generation\nprocess, thereby enabling user control over melody generation. Finally, an\nin-attention Transformer decoder technique is leveraged to exert fine-grained\ncontrol over the full-song melody generation with the aforementioned lyric and\nmusical conditions. Experimental results demonstrate that our proposed CSL-L2M\noutperforms the state-of-the-art models, generating melodies with higher\nquality, better controllability and enhanced structure. Demos and source code\nare available at https://lichaiustc.github.io/CSL-L2M/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2412.09887v2",
    "published_date": "2024-12-13 06:05:53 UTC",
    "updated_date": "2025-01-15 02:46:18 UTC"
  },
  {
    "arxiv_id": "2412.16187v2",
    "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing",
    "authors": [
      "Minghui Liu",
      "Tahseen Rabbani",
      "Tony O'Halloran",
      "Ananth Sankaralingam",
      "Mary-Anne Hartley",
      "Brian Gravelle",
      "Furong Huang",
      "Cornelia Fermüller",
      "Yiannis Aloimonos"
    ],
    "abstract": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.16187v2",
    "published_date": "2024-12-13 06:00:27 UTC",
    "updated_date": "2024-12-24 13:04:45 UTC"
  },
  {
    "arxiv_id": "2412.10477v1",
    "title": "Benchmarking large language models for materials synthesis: the case of atomic layer deposition",
    "authors": [
      "Angel Yanguas-Gil",
      "Matthew T. Dearing",
      "Jeffrey W. Elam",
      "Jessica C. Jones",
      "Sungjoon Kim",
      "Adnan Mohammad",
      "Chi Thang Nguyen",
      "Bratin Sengupta"
    ],
    "abstract": "In this work we introduce an open-ended question benchmark, ALDbench, to\nevaluate the performance of large language models (LLMs) in materials\nsynthesis, and in particular in the field of atomic layer deposition, a thin\nfilm growth technique used in energy applications and microelectronics. Our\nbenchmark comprises questions with a level of difficulty ranging from graduate\nlevel to domain expert current with the state of the art in the field. Human\nexperts reviewed the questions along the criteria of difficulty and\nspecificity, and the model responses along four different criteria: overall\nquality, specificity, relevance, and accuracy. We ran this benchmark on an\ninstance of OpenAI's GPT-4o. The responses from the model received a composite\nquality score of 3.7 on a 1 to 5 scale, consistent with a passing grade.\nHowever, 36% of the questions received at least one below average score. An\nin-depth analysis of the responses identified at least five instances of\nsuspected hallucination. Finally, we observed statistically significant\ncorrelations between the difficulty of the question and the quality of the\nresponse, the difficulty of the question and the relevance of the response, and\nthe specificity of the question and the accuracy of the response as graded by\nthe human experts. This emphasizes the need to evaluate LLMs across multiple\ncriteria beyond difficulty or accuracy.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10477v1",
    "published_date": "2024-12-13 05:10:29 UTC",
    "updated_date": "2024-12-13 05:10:29 UTC"
  },
  {
    "arxiv_id": "2412.09860v1",
    "title": "Brain-inspired Chaotic Graph Backpropagation for Large-scale Combinatorial Optimization",
    "authors": [
      "Peng Tao",
      "Kazuyuki Aihara",
      "Luonan Chen"
    ],
    "abstract": "Graph neural networks (GNNs) with unsupervised learning can solve large-scale\ncombinatorial optimization problems (COPs) with efficient time complexity,\nmaking them versatile for various applications. However, since this method maps\nthe combinatorial optimization problem to the training process of a graph\nneural network, and the current mainstream backpropagation-based training\nalgorithms are prone to fall into local minima, the optimization performance is\nstill inferior to the current state-of-the-art (SOTA) COP methods. To address\nthis issue, inspired by possibly chaotic dynamics of real brain learning, we\nintroduce a chaotic training algorithm, i.e. chaotic graph backpropagation\n(CGBP), which introduces a local loss function in GNN that makes the training\nprocess not only chaotic but also highly efficient. Different from existing\nmethods, we show that the global ergodicity and pseudo-randomness of such\nchaotic dynamics enable CGBP to learn each optimal GNN effectively and\nglobally, thus solving the COP efficiently. We have applied CGBP to solve\nvarious COPs, such as the maximum independent set, maximum cut, and graph\ncoloring. Results on several large-scale benchmark datasets showcase that CGBP\ncan outperform not only existing GNN algorithms but also SOTA methods. In\naddition to solving large-scale COPs, CGBP as a universal learning algorithm\nfor GNNs, i.e. as a plug-in unit, can be easily integrated into any existing\nmethod for improving the performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09860v1",
    "published_date": "2024-12-13 05:00:57 UTC",
    "updated_date": "2024-12-13 05:00:57 UTC"
  },
  {
    "arxiv_id": "2412.09858v1",
    "title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning",
    "authors": [
      "Charles Xu",
      "Qiyang Li",
      "Jianlan Luo",
      "Sergey Levine"
    ],
    "abstract": "Recent advances in robotic foundation models have enabled the development of\ngeneralist policies that can adapt to diverse tasks. While these models show\nimpressive flexibility, their performance heavily depends on the quality of\ntheir training data. In this work, we propose Reinforcement Learning Distilled\nGeneralists (RLDG), a method that leverages reinforcement learning to generate\nhigh-quality training data for finetuning generalist policies. Through\nextensive real-world experiments on precise manipulation tasks like connector\ninsertion and assembly, we demonstrate that generalist policies trained with\nRL-generated data consistently outperform those trained with human\ndemonstrations, achieving up to 40% higher success rates while generalizing\nbetter to new tasks. We also provide a detailed analysis that reveals this\nperformance gain stems from both optimized action distributions and improved\nstate coverage. Our results suggest that combining task-specific RL with\ngeneralist policy distillation offers a promising approach for developing more\ncapable and efficient robotic manipulation systems that maintain the\nflexibility of foundation models while achieving the performance of specialized\ncontrollers. Videos and code can be found on our project website\nhttps://generalist-distillation.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09858v1",
    "published_date": "2024-12-13 04:57:55 UTC",
    "updated_date": "2024-12-13 04:57:55 UTC"
  },
  {
    "arxiv_id": "2412.09856v1",
    "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
    "authors": [
      "Hongjie Wang",
      "Chih-Yao Ma",
      "Yen-Cheng Liu",
      "Ji Hou",
      "Tao Xu",
      "Jialiang Wang",
      "Felix Juefei-Xu",
      "Yaqiao Luo",
      "Peizhao Zhang",
      "Tingbo Hou",
      "Peter Vajda",
      "Niraj K. Jha",
      "Xiaoliang Dai"
    ],
    "abstract": "Text-to-video generation enhances content creation but is highly\ncomputationally intensive: The computational cost of Diffusion Transformers\n(DiTs) scales quadratically in the number of pixels. This makes minute-length\nvideo generation extremely expensive, limiting most existing models to\ngenerating videos of only 10-20 seconds length. We propose a Linear-complexity\ntext-to-video Generation (LinGen) framework whose cost scales linearly in the\nnumber of pixels. For the first time, LinGen enables high-resolution\nminute-length video generation on a single GPU without compromising quality. It\nreplaces the computationally-dominant and quadratic-complexity block,\nself-attention, with a linear-complexity block called MATE, which consists of\nan MA-branch and a TE-branch. The MA-branch targets short-to-long-range\ncorrelations, combining a bidirectional Mamba2 block with our token\nrearrangement method, Rotary Major Scan, and our review tokens developed for\nlong video generation. The TE-branch is a novel TEmporal Swin Attention block\nthat focuses on temporal correlations between adjacent tokens and medium-range\ntokens. The MATE block addresses the adjacency preservation issue of Mamba and\nimproves the consistency of generated videos significantly. Experimental\nresults show that LinGen outperforms DiT (with a 75.6% win rate) in video\nquality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.\nFurthermore, both automatic metrics and human evaluation demonstrate our\nLinGen-4B yields comparable video quality to state-of-the-art models (with a\n50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,\nrespectively). This paves the way to hour-length movie generation and real-time\ninteractive video generation. We provide 68s video generation results and more\nexamples in our project website: https://lineargen.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09856v1",
    "published_date": "2024-12-13 04:55:10 UTC",
    "updated_date": "2024-12-13 04:55:10 UTC"
  },
  {
    "arxiv_id": "2412.15239v2",
    "title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs",
    "authors": [
      "Hortense Fong",
      "George Gui"
    ],
    "abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN",
      "q-fin.EC",
      "stat.ME",
      "68T50, 91F20",
      "H.3.1; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15239v2",
    "published_date": "2024-12-13 04:53:34 UTC",
    "updated_date": "2025-03-26 18:59:18 UTC"
  },
  {
    "arxiv_id": "2412.09849v1",
    "title": "Deep Learning for Spectrum Prediction in Cognitive Radio Networks: State-of-the-Art, New Opportunities, and Challenges",
    "authors": [
      "Guangliang Pan",
      "David K. Y. Yau",
      "Bo Zhou",
      "Qihui Wu"
    ],
    "abstract": "Spectrum prediction is considered to be a promising technology that enhances\nspectrum efficiency by assisting dynamic spectrum access (DSA) in cognitive\nradio networks (CRN). Nonetheless, the highly nonlinear nature of spectrum data\nacross time, frequency, and space domains, coupled with the intricate spectrum\nusage patterns, poses challenges for accurate spectrum prediction. Deep\nlearning (DL), recognized for its capacity to extract nonlinear features, has\nbeen applied to solve these challenges. This paper first shows the advantages\nof applying DL by comparing with traditional prediction methods. Then, the\ncurrent state-of-the-art DL-based spectrum prediction techniques are reviewed\nand summarized in terms of intra-band and crossband prediction. Notably, this\npaper uses a real-world spectrum dataset to prove the advancements of DL-based\nmethods. Then, this paper proposes a novel intra-band spatiotemporal spectrum\nprediction framework named ViTransLSTM. This framework integrates visual\nself-attention and long short-term memory to capture both local and global\nlong-term spatiotemporal dependencies of spectrum usage patterns. Similarly,\nthe effectiveness of the proposed framework is validated on the aforementioned\nreal-world dataset. Finally, the paper presents new related challenges and\npotential opportunities for future research.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09849v1",
    "published_date": "2024-12-13 04:36:05 UTC",
    "updated_date": "2024-12-13 04:36:05 UTC"
  },
  {
    "arxiv_id": "2412.09843v1",
    "title": "Learning Structural Causal Models from Ordering: Identifiable Flow Models",
    "authors": [
      "Minh Khoa Le",
      "Kien Do",
      "Truyen Tran"
    ],
    "abstract": "In this study, we address causal inference when only observational data and a\nvalid causal ordering from the causal graph are available. We introduce a set\nof flow models that can recover component-wise, invertible transformation of\nexogenous variables. Our flow-based methods offer flexible model design while\nmaintaining causal consistency regardless of the number of discretization\nsteps. We propose design improvements that enable simultaneous learning of all\ncausal mechanisms and reduce abduction and prediction complexity to linear O(n)\nrelative to the number of layers, independent of the number of causal\nvariables. Empirically, we demonstrate that our method outperforms previous\nstate-of-the-art approaches and delivers consistent performance across a wide\nrange of structural causal models in answering observational, interventional,\nand counterfactual questions. Additionally, our method achieves a significant\nreduction in computational time compared to existing diffusion-based\ntechniques, making it practical for large structural causal models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09843v1",
    "published_date": "2024-12-13 04:25:56 UTC",
    "updated_date": "2024-12-13 04:25:56 UTC"
  },
  {
    "arxiv_id": "2412.09826v1",
    "title": "Precise Antigen-Antibody Structure Predictions Enhance Antibody Development with HelixFold-Multimer",
    "authors": [
      "Jie Gao",
      "Jing Hu",
      "Lihang Liu",
      "Yang Xue",
      "Kunrui Zhu",
      "Xiaonan Zhang",
      "Xiaomin Fang"
    ],
    "abstract": "The accurate prediction of antigen-antibody structures is essential for\nadvancing immunology and therapeutic development, as it helps elucidate\nmolecular interactions that underlie immune responses. Despite recent progress\nwith deep learning models like AlphaFold and RoseTTAFold, accurately modeling\nantigen-antibody complexes remains a challenge due to their unique evolutionary\ncharacteristics. HelixFold-Multimer, a specialized model developed for this\npurpose, builds on the framework of AlphaFold-Multimer and demonstrates\nimproved precision for antigen-antibody structures. HelixFold-Multimer not only\nsurpasses other models in accuracy but also provides essential insights into\nantibody development, enabling more precise identification of binding sites,\nimproved interaction prediction, and enhanced design of therapeutic antibodies.\nThese advances underscore HelixFold-Multimer's potential in supporting antibody\nresearch and therapeutic innovation.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09826v1",
    "published_date": "2024-12-13 03:36:23 UTC",
    "updated_date": "2024-12-13 03:36:23 UTC"
  },
  {
    "arxiv_id": "2412.09818v3",
    "title": "MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models",
    "authors": [
      "Yingxu He",
      "Zhuohan Liu",
      "Shuo Sun",
      "Bin Wang",
      "Wenyu Zhang",
      "Xunlong Zou",
      "Nancy F. Chen",
      "Ai Ti Aw"
    ],
    "abstract": "We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning\nin One Network), the first speech-text model tailored for Singapore's\nmultilingual and multicultural landscape. Developed under the National Large\nLanguage Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates\nadvanced speech and text processing to address the diverse linguistic nuances\nof local accents and dialects, enhancing accessibility and usability in\ncomplex, multilingual environments. Our results demonstrate improvements in\nboth speech recognition and task-specific understanding, positioning\nMERaLiON-AudioLLM as a pioneering solution for region specific AI applications.\nWe envision this release to set a precedent for future models designed to\naddress localised linguistic and cultural contexts in a global framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION",
    "pdf_url": "http://arxiv.org/pdf/2412.09818v3",
    "published_date": "2024-12-13 03:15:05 UTC",
    "updated_date": "2025-01-16 03:29:23 UTC"
  },
  {
    "arxiv_id": "2412.09814v2",
    "title": "Federated Learning of Dynamic Bayesian Network via Continuous Optimization from Time Series Data",
    "authors": [
      "Jianhong Chen",
      "Ying Ma",
      "Xubo Yue"
    ],
    "abstract": "Traditionally, learning the structure of a Dynamic Bayesian Network has been\ncentralized, requiring all data to be pooled in one location. However, in\nreal-world scenarios, data are often distributed across multiple entities\n(e.g., companies, devices) that seek to collaboratively learn a Dynamic\nBayesian Network while preserving data privacy and security. More importantly,\ndue to the presence of diverse clients, the data may follow different\ndistributions, resulting in data heterogeneity. This heterogeneity poses\nadditional challenges for centralized approaches. In this study, we first\nintroduce a federated learning approach for estimating the structure of a\nDynamic Bayesian Network from homogeneous time series data that are\nhorizontally distributed across different parties. We then extend this approach\nto heterogeneous time series data by incorporating a proximal operator as a\nregularization term in a personalized federated learning framework. To this\nend, we propose \\texttt{FDBNL} and \\texttt{PFDBNL}, which leverage continuous\noptimization, ensuring that only model parameters are exchanged during the\noptimization process. Experimental results on synthetic and real-world datasets\ndemonstrate that our method outperforms state-of-the-art techniques,\nparticularly in scenarios with many clients and limited individual sample\nsizes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09814v2",
    "published_date": "2024-12-13 03:09:35 UTC",
    "updated_date": "2025-02-05 19:35:48 UTC"
  },
  {
    "arxiv_id": "2412.09805v1",
    "title": "Universal Inceptive GNNs by Eliminating the Smoothness-generalization Dilemma",
    "authors": [
      "Ming Gu",
      "Zhuonan Zheng",
      "Sheng Zhou",
      "Meihan Liu",
      "Jiawei Chen",
      "Tanyu Qiao",
      "Liangcheng Li",
      "Jiajun Bu"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various\ndomains, such as transaction and social net-works. However, their application\nis often hindered by the varyinghomophily levels across different orders of\nneighboring nodes, ne-cessitating separate model designs for homophilic and\nheterophilicgraphs. In this paper, we aim to develop a unified framework\nca-pable of handling neighborhoods of various orders and homophilylevels.\nThrough theoretical exploration, we identify a previouslyoverlooked\narchitectural aspect in multi-hop learning: the cascadedependency, which leads\nto asmoothness-generalization dilemma.This dilemma significantly affects the\nlearning process, especiallyin the context of high-order neighborhoods and\nheterophilic graphs.To resolve this issue, we propose an Inceptive Graph Neural\nNet-work (IGNN), a universal message-passing framework that replacesthe cascade\ndependency with an inceptive architecture. IGNN pro-vides independent\nrepresentations for each hop, allowing personal-ized generalization\ncapabilities, and captures neighborhood-wiserelationships to select appropriate\nreceptive fields. Extensive ex-periments show that our IGNN outperforms 23\nbaseline methods,demonstrating superior performance on both homophilic and\nhet-erophilic graphs, while also scaling efficiently to large graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.09805v1",
    "published_date": "2024-12-13 02:44:47 UTC",
    "updated_date": "2024-12-13 02:44:47 UTC"
  },
  {
    "arxiv_id": "2412.09799v1",
    "title": "CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object Detection",
    "authors": [
      "Qibo Chen",
      "Weizhong Jin",
      "Jianyue Ge",
      "Mengdi Liu",
      "Yuchao Yan",
      "Jian Jiang",
      "Li Yu",
      "Xuanjiang Guo",
      "Shuchang Li",
      "Jianzhong Chen"
    ],
    "abstract": "Recent research on universal object detection aims to introduce language in a\nSoTA closed-set detector and then generalize the open-set concepts by\nconstructing large-scale (text-region) datasets for training. However, these\nmethods face two main challenges: (i) how to efficiently use the prior\ninformation in the prompts to genericise objects and (ii) how to reduce\nalignment bias in the downstream tasks, both leading to sub-optimal performance\nin some scenarios beyond pre-training. To address these challenges, we propose\na strong universal detection foundation model called CP-DETR, which is\ncompetitive in almost all scenarios, with only one pre-training weight.\nSpecifically, we design an efficient prompt visual hybrid encoder that enhances\nthe information interaction between prompt and visual through scale-by-scale\nand multi-scale fusion modules. Then, the hybrid encoder is facilitated to\nfully utilize the prompted information by prompt multi-label loss and auxiliary\ndetection head. In addition to text prompts, we have designed two practical\nconcept prompt generation methods, visual prompt and optimized prompt, to\nextract abstract concepts through concrete visual examples and stably reduce\nalignment bias in downstream tasks. With these effective designs, CP-DETR\ndemonstrates superior universal detection performance in a broad spectrum of\nscenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on\nLVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35.\nFurthermore, our visual prompt generation method achieves 68.4 AP on COCO val\nby interactive detection, and the optimized prompt achieves 73.1 fully-shot AP\non ODinW13.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.09799v1",
    "published_date": "2024-12-13 02:36:29 UTC",
    "updated_date": "2024-12-13 02:36:29 UTC"
  },
  {
    "arxiv_id": "2412.10474v1",
    "title": "CrossVIT-augmented Geospatial-Intelligence Visualization System for Tracking Economic Development Dynamics",
    "authors": [
      "Yanbing Bai",
      "Jinhua Su",
      "Bin Qiao",
      "Xiaoran Ma"
    ],
    "abstract": "Timely and accurate economic data is crucial for effective policymaking.\nCurrent challenges in data timeliness and spatial resolution can be addressed\nwith advancements in multimodal sensing and distributed computing. We introduce\nSenseconomic, a scalable system for tracking economic dynamics via multimodal\nimagery and deep learning. Built on the Transformer framework, it integrates\nremote sensing and street view images using cross-attention, with nighttime\nlight data as weak supervision. The system achieved an R-squared value of\n0.8363 in county-level economic predictions and halved processing time to 23\nminutes using distributed computing. Its user-friendly design includes a\nVue3-based front end with Baidu maps for visualization and a Python-based back\nend automating tasks like image downloads and preprocessing. Senseconomic\nempowers policymakers and researchers with efficient tools for resource\nallocation and economic planning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10474v1",
    "published_date": "2024-12-13 02:31:48 UTC",
    "updated_date": "2024-12-13 02:31:48 UTC"
  },
  {
    "arxiv_id": "2412.09796v1",
    "title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation",
    "authors": [
      "Qiyao Wang",
      "Shiwen Ni",
      "Huaren Liu",
      "Shule Lu",
      "Guhong Chen",
      "Xi Feng",
      "Chi Wei",
      "Qiang Qu",
      "Hamid Alinejad-Rokny",
      "Yuan Lin",
      "Min Yang"
    ],
    "abstract": "As the capabilities of Large Language Models (LLMs) continue to advance, the\nfield of patent processing has garnered increased attention within the natural\nlanguage processing community. However, the majority of research has been\nconcentrated on classification tasks, such as patent categorization and\nexamination, or on short text generation tasks like patent summarization and\npatent quizzes. In this paper, we introduce a novel and practical task known as\nDraft2Patent, along with its corresponding D2P benchmark, which challenges LLMs\nto generate full-length patents averaging 17K tokens based on initial drafts.\nPatents present a significant challenge to LLMs due to their specialized\nnature, standardized terminology, and extensive length. We propose a\nmulti-agent framework called AutoPatent which leverages the LLM-based planner\nagent, writer agents, and examiner agent with PGTree and RRAG to generate\nlengthy, intricate, and high-quality complete patent documents. The\nexperimental results demonstrate that our AutoPatent framework significantly\nenhances the ability to generate comprehensive patents across various LLMs.\nFurthermore, we have discovered that patents generated solely with the\nAutoPatent framework based on the Qwen2.5-7B model outperform those produced by\nlarger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,\nin both objective metrics and human evaluations. We will make the data and code\navailable upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.09796v1",
    "published_date": "2024-12-13 02:27:34 UTC",
    "updated_date": "2024-12-13 02:27:34 UTC"
  },
  {
    "arxiv_id": "2412.12178v2",
    "title": "Activation Sparsity Opportunities for Compressing General Large Language Models",
    "authors": [
      "Nobel Dhar",
      "Bobin Deng",
      "Md Romyull Islam",
      "Kazi Fahim Ahmad Nasif",
      "Liang Zhao",
      "Kun Suo"
    ],
    "abstract": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
    "pdf_url": "http://arxiv.org/pdf/2412.12178v2",
    "published_date": "2024-12-13 02:26:54 UTC",
    "updated_date": "2025-01-31 19:09:19 UTC"
  },
  {
    "arxiv_id": "2412.09784v1",
    "title": "Semi-IIN: Semi-supervised Intra-inter modal Interaction Learning Network for Multimodal Sentiment Analysis",
    "authors": [
      "Jinhao Lin",
      "Yifei Wang",
      "Yanwu Xu",
      "Qi Liu"
    ],
    "abstract": "Despite multimodal sentiment analysis being a fertile research ground that\nmerits further investigation, current approaches take up high annotation cost\nand suffer from label ambiguity, non-amicable to high-quality labeled data\nacquisition. Furthermore, choosing the right interactions is essential because\nthe significance of intra- or inter-modal interactions can differ among various\nsamples. To this end, we propose Semi-IIN, a Semi-supervised Intra-inter modal\nInteraction learning Network for multimodal sentiment analysis. Semi-IIN\nintegrates masked attention and gating mechanisms, enabling effective dynamic\nselection after independently capturing intra- and inter-modal interactive\ninformation. Combined with the self-training approach, Semi-IIN fully utilizes\nthe knowledge learned from unlabeled data. Experimental results on two public\ndatasets, MOSI and MOSEI, demonstrate the effectiveness of Semi-IIN,\nestablishing a new state-of-the-art on several metrics. Code is available at\nhttps://github.com/flow-ljh/Semi-IIN.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.09784v1",
    "published_date": "2024-12-13 01:48:07 UTC",
    "updated_date": "2024-12-13 01:48:07 UTC"
  },
  {
    "arxiv_id": "2412.10473v1",
    "title": "CONCLAD: COntinuous Novel CLAss Detector",
    "authors": [
      "Amanda Rios",
      "Ibrahima Ndiour",
      "Parual Datta",
      "Omesh Tickoo",
      "Nilesh Ahuja"
    ],
    "abstract": "In the field of continual learning, relying on so-called oracles for novelty\ndetection is commonplace albeit unrealistic. This paper introduces CONCLAD\n(\"COntinuous Novel CLAss Detector\"), a comprehensive solution to the\nunder-explored problem of continual novel class detection in post-deployment\ndata. At each new task, our approach employs an iterative uncertainty\nestimation algorithm to differentiate between known and novel class(es)\nsamples, and to further discriminate between the different novel classes\nthemselves. Samples predicted to be from a novel class with high-confidence are\nautomatically pseudo-labeled and used to update our model. Simultaneously, a\ntiny supervision budget is used to iteratively query ambiguous novel class\npredictions, which are also used during update. Evaluation across multiple\ndatasets, ablations and experimental settings demonstrate our method's\neffectiveness at separating novel and old class samples continuously. We will\nrelease our code upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10473v1",
    "published_date": "2024-12-13 01:41:28 UTC",
    "updated_date": "2024-12-13 01:41:28 UTC"
  },
  {
    "arxiv_id": "2412.09770v1",
    "title": "Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation",
    "authors": [
      "Jonghyuk Park",
      "Alex Lascarides",
      "Subramanian Ramamoorthy"
    ],
    "abstract": "In this paper, we offer a learning framework in which the agent's knowledge\ngaps are overcome through corrective feedback from a teacher whenever the agent\nexplains its (incorrect) predictions. We test it in a low-resource visual\nprocessing scenario, in which the agent must learn to recognize distinct types\nof toy truck. The agent starts the learning process with no ontology about what\ntypes of trucks exist nor which parts they have, and a deficient model for\nrecognizing those parts from visual input. The teacher's feedback to the\nagent's explanations addresses its lack of relevant knowledge in the ontology\nvia a generic rule (e.g., \"dump trucks have dumpers\"), whereas an inaccurate\npart recognition is corrected by a deictic statement (e.g., \"this is not a\ndumper\"). The learner utilizes this feedback not only to improve its estimate\nof the hypothesis space of possible domain ontologies and probability\ndistributions over them, but also to use those estimates to update its visual\ninterpretation of the scene. Our experiments demonstrate that teacher-learner\npairs utilizing explanations and corrections are more data-efficient than those\nwithout such a faculty.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to, and to appear in the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
    "pdf_url": "http://arxiv.org/pdf/2412.09770v1",
    "published_date": "2024-12-13 00:28:21 UTC",
    "updated_date": "2024-12-13 00:28:21 UTC"
  },
  {
    "arxiv_id": "2412.12177v1",
    "title": "Model-diff: A Tool for Comparative Study of Language Models in the Input Space",
    "authors": [
      "Weitang Liu",
      "Yuelei Li",
      "Ying Wai Li",
      "Zihan Wang",
      "Jingbo Shang"
    ],
    "abstract": "Comparing two (large) language models (LMs) side-by-side and pinpointing\ntheir prediction similarities and differences on the same set of inputs are\ncrucial in many real-world scenarios, e.g., one can test if a licensed model\nwas potentially plagiarized by another. Traditional analysis compares the LMs'\noutputs on some benchmark datasets, which only cover a limited number of inputs\nof designed perspectives for the intended applications. The benchmark datasets\ncannot prepare data to cover the test cases from unforeseen perspectives which\ncan help us understand differences between models unbiasedly. In this paper, we\npropose a new model comparative analysis setting that considers a large input\nspace where brute-force enumeration would be infeasible. The input space can be\nsimply defined as all token sequences that a LM would produce low perplexity on\n-- we follow this definition in the paper as it would produce the most\nhuman-understandable inputs. We propose a novel framework \\our that uses text\ngeneration by sampling and deweights the histogram of sampling statistics to\nestimate prediction differences between two LMs in this input space efficiently\nand unbiasedly. Our method achieves this by drawing and counting the inputs at\neach prediction difference value in negative log-likelihood. Experiments reveal\nfor the first time the quantitative prediction differences between LMs in a\nlarge input space, potentially facilitating the model analysis for applications\nsuch as model plagiarism.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12177v1",
    "published_date": "2024-12-13 00:06:25 UTC",
    "updated_date": "2024-12-13 00:06:25 UTC"
  }
]
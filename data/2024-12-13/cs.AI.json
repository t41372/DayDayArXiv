{
  "date": "2024-12-13",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-13 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦 AI 安全、多模态模型、机器人应用和高效生成技术等领域，强调 LLM（Large Language Models）的鲁棒性、跨模态融合以及高效计算。其中，DeepSeek 团队的“DeepSeek-VL2: Mixture-of-Experts Vision-Language Models”令人印象深刻，展示了多模态模型的创新应用；此外，医疗和机器人领域的论文如“MANGO”突显实际影响，而高效视频生成工具如“SnapGen-V”则体现了计算效率的突破。\n\n下面，我挑选了几个关键论文进行简要讨论，先聊那些创新性强、可能有话题度的文章（如 AI 安全和多模态模型），然后快速掠过其他较基础的。相关论文按主题归类，核心学术术语保留。\n\n### AI 安全和 LLM 防御\n- **No Free Lunch for Defending Against Prefilling Attack by In-Context Learning（针对预填充攻击的无免费午餐：在语境学习中的防御）**  \n  这篇论文探讨了 LLM 的安全问题，作者包括 Kristen Marie Johnson 和 Ramtin Pedarsani。主要贡献是通过 In-Context Learning（ICL）使用对抗性句子结构防御预填充攻击，实验显示这种方法在不同模型大小和攻击类型下表现出色，但强调防御并非无成本。发现：当前安全对齐方法无效，而 ICL 能提供鲁棒防御，但可能导致过度防御。\n\n- **Too Big to Fool: Resisting Deception in Language Models（太大而无法欺骗：抵抗语言模型中的欺骗）**  \n  作者如 Mohammad Reza Samsami 和 Sarath Chandar 的研究显示，大型 LLM 对欺骗性提示更具抵抗力，主要贡献是证明模型规模能提升对内部知识与提示的整合能力。发现：更大模型在处理误导信息时更可靠，但并非完全忽略提示。\n\n其他 LLM 相关论文，如“AdvPrefix”和“Causal Learning”中 LLM 偏差的讨论，快速掠过：这些工作扩展了 LLM 的攻击和偏差分析，但重复性较高，未见重大突破。\n\n### 多模态模型和医疗应用\n- **MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes（MANGO：用于智能 ICU 结果的多模态敏锐 Transformer）**  \n  作者包括 Azra Bihorac 和 Parisa Rashidi 的这篇论文提出多模态融合模型，用于 ICU 患者敏锐度预测。主要贡献是整合 EHR 数据、可穿戴传感器、视频和环境数据，通过 Transformer 的掩码自注意力机制处理缺失模态，AUROC 达 0.82。发现：多模态数据显著提升预测准确性，适用于临床决策。\n\n- **DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding（DeepSeek-VL2：用于高级多模态理解的混合专家视觉-语言模型）**  \n  DeepSeek 团队的作品，创新性强，主要贡献是优化视觉和语言组件，支持高分辨率图像和视频理解，模型参数从 1B 到 4.5B。发现：在 Video-MME 等基准上，性能优于同类模型，强调混合专家架构的扩展性。\n\n其他医疗 AI 论文，如“Generative AI in Medicine”，快速掠过：它概述了 AI 在医疗中的机会和挑战，但内容较泛，未提供新方法。\n\n### 机器人和高效生成技术\n- **iMoT: Inertial Motion Transformer for Inertial Navigation（iMoT：用于惯性导航的惯性运动 Transformer）**  \n  作者包括 Paul J. M Havinga 的论文在 AAAI 2025 上被接受，主要贡献是提出 Transformer-based 方法处理加速度和角速度信号，通过 Progressive Series Decoupler 和 Adaptive Positional Encoding 提升轨迹重建准确性。发现：在多种数据集上，鲁棒性和精度优于 SOTA 方法。\n\n- **SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device（SnapGen-V：在移动设备上五秒内生成五秒视频）**  \n  作者如 Sergey Tulyakov 和 Jian Ren 的工作，主要贡献是优化扩散模型，实现高效视频生成，减少计算步骤到 4 步。发现：0.6B 参数模型在 iPhone 上实时生成高质量视频，远超传统 GPU 方法。\n\n其他机器人论文，如“GaussianAD”和“TraceVLA”，快速掠过：这些扩展了高斯表示在自动驾驶中的应用，但实验细节较琐碎。\n\n### 其他快速掠过\n剩余论文中，如“Evaluation of GPT-4o and GPT-4o's Vision Capabilities”评估 GPT 模型在图像分析上的表现；“Advances in Transformers for Robotic Applications”审视 Transformer 在机器人中的趋势；“AniSora”探索动画视频生成等。这些工作虽有价值，但主题较常规或实验导向，不如上述论文话题度高，故仅简要提及其主要发现：如 GPT-4o 在盐分析中准确率达 57%，Transformer 在机器人规划中提升泛化性。\n\n总之，今天的论文突显 AI 模型的鲁棒性和多模态融合潜力，建议关注 DeepSeek-VL2 和 MANGO 等创新工作，以推动实际应用。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2412.12192v1",
      "title": "No Free Lunch for Defending Against Prefilling Attack by In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu Xue",
        "Guangliang Liu",
        "Bocheng Chen",
        "Kristen Marie Johnson",
        "Ramtin Pedarsani"
      ],
      "abstract": "The security of Large Language Models (LLMs) has become an important research\ntopic since the emergence of ChatGPT. Though there have been various effective\nmethods to defend against jailbreak attacks, prefilling attacks remain an\nunsolved and popular threat against open-sourced LLMs. In-Context Learning\n(ICL) offers a computationally efficient defense against various jailbreak\nattacks, yet no effective ICL methods have been developed to counter prefilling\nattacks. In this paper, we: (1) show that ICL can effectively defend against\nprefilling jailbreak attacks by employing adversative sentence structures\nwithin demonstrations; (2) characterize the effectiveness of this defense\nthrough the lens of model size, number of demonstrations, over-defense,\nintegration with other jailbreak attacks, and the presence of safety alignment.\nGiven the experimental results and our analysis, we conclude that there is no\nfree lunch for defending against prefilling jailbreak attacks with ICL. On the\none hand, current safety alignment methods fail to mitigate prefilling\njailbreak attacks, but adversative structures within ICL demonstrations provide\nrobust defense across various model sizes and complex jailbreak attacks. On the\nother hand, LLMs exhibit similar over-defensiveness when utilizing ICL\ndemonstrations with adversative structures, and this behavior appears to be\nindependent of model size.",
      "tldr_zh": "该研究探讨了使用 In-Context Learning (ICL) 防御 Large Language Models (LLMs) 面临的 prefilling attacks，发现当前安全对齐方法无法有效缓解此类攻击。论文证明，通过在 ICL 演示中采用 adversative sentence structures，可以显著防御 prefilling jailbreak attacks，并在模型大小、演示数量以及与其他攻击整合等方面表现出稳健性。实验结果显示，这种防御策略会导致 LLMs 出现 over-defense 现象，且这种行为独立于模型大小，从而得出“无免费午餐”的结论，即防御并非完美无缺。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12192v1",
      "published_date": "2024-12-13 23:58:12 UTC",
      "updated_date": "2024-12-13 23:58:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:03:55.119136"
    },
    {
      "arxiv_id": "2412.17832v1",
      "title": "MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaqing Zhang",
        "Miguel Contreras",
        "Sabyasachi Bandyopadhyay",
        "Andrea Davidson",
        "Jessica Sena",
        "Yuanfang Ren",
        "Ziyuan Guan",
        "Tezcan Ozrazgat-Baslanti",
        "Tyler J. Loftus",
        "Subhash Nerella",
        "Azra Bihorac",
        "Parisa Rashidi"
      ],
      "abstract": "Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to\nensure timely and appropriate interventions. Advances in artificial\nintelligence (AI) technologies have significantly improved the accuracy of\nacuity predictions. However, prior studies using machine learning for acuity\nprediction have predominantly relied on electronic health records (EHR) data,\noften overlooking other critical aspects of ICU stay, such as patient mobility,\nenvironmental factors, and facial cues indicating pain or agitation. To address\nthis gap, we present MANGO: the Multimodal Acuity traNsformer for intelliGent\nICU Outcomes, designed to enhance the prediction of patient acuity states,\ntransitions, and the need for life-sustaining therapy. We collected a\nmultimodal dataset ICU-Multimodal, incorporating four key modalities, EHR data,\nwearable sensor data, video of patient's facial cues, and ambient sensor data,\nwhich we utilized to train MANGO. The MANGO model employs a multimodal feature\nfusion network powered by Transformer masked self-attention method, enabling it\nto capture and learn complex interactions across these diverse data modalities\neven when some modalities are absent. Our results demonstrated that integrating\nmultiple modalities significantly improved the model's ability to predict\nacuity status, transitions, and the need for life-sustaining therapy. The\nbest-performing models achieved an area under the receiver operating\ncharacteristic curve (AUROC) of 0.76 (95% CI: 0.72-0.79) for predicting\ntransitions in acuity status and the need for life-sustaining therapy, while\n0.82 (95% CI: 0.69-0.89) for acuity status prediction...",
      "tldr_zh": "本研究提出 MANGO，一种多模态敏锐度 Transformer 模型，用于提升 ICU 患者敏锐度状态、转变以及生命维持疗法需求的预测，解决传统方法依赖电子健康记录 (EHR) 数据而忽略患者移动、环境因素和面部线索的局限。MANGO 利用新收集的多模态数据集 ICU-Multimodal，包括 EHR 数据、可穿戴传感器数据、患者面部视频和环境传感器数据，通过 Transformer 的 masked self-attention 机制实现特征融合，即使某些模态缺失也能学习复杂交互。实验结果显示，整合多模态数据显著提高了预测性能，最佳模型的 AUROC 分别为 0.82（敏锐度状态）和 0.76（敏锐度转变及疗法需求）。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.17832v1",
      "published_date": "2024-12-13 23:51:15 UTC",
      "updated_date": "2024-12-13 23:51:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:04:07.650161"
    },
    {
      "arxiv_id": "2412.10605v2",
      "title": "Client-Side Patching against Backdoor Attacks in Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Borja Molina-Coronado"
      ],
      "abstract": "Federated learning is a versatile framework for training models in\ndecentralized environments. However, the trust placed in clients makes\nfederated learning vulnerable to backdoor attacks launched by malicious\nparticipants. While many defenses have been proposed, they often fail short\nwhen facing heterogeneous data distributions among participating clients. In\nthis paper, we propose a novel defense mechanism for federated learning systems\ndesigned to mitigate backdoor attacks on the clients-side. Our approach\nleverages adversarial learning techniques and model patching to neutralize the\nimpact of backdoor attacks. Through extensive experiments on the MNIST and\nFashion-MNIST datasets, we demonstrate that our defense effectively reduces\nbackdoor accuracy, outperforming existing state-of-the-art defenses, such as\nLFighter, FLAME, and RoseAgg, in i.i.d. and non-i.i.d. scenarios, while\nmaintaining competitive or superior accuracy on clean data.",
      "tldr_zh": "本论文针对联邦学习（Federated Learning）中由恶意参与者发起的后门攻击（backdoor attacks），提出了一种新型客户端侧防御机制，以应对数据分布异质（heterogeneous data distributions）的问题。该机制结合对抗学习技术（adversarial learning techniques）和模型修补（model patching），有效中和攻击影响。通过在MNIST和Fashion-MNIST数据集上的广泛实验，该防御方法在i.i.d.和non-i.i.d.场景下显著降低了后门准确率（backdoor accuracy），并优于现有防御如LFighter、FLAME和RoseAgg，同时维持或提升了干净数据的准确率（clean data accuracy）。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10605v2",
      "published_date": "2024-12-13 23:17:10 UTC",
      "updated_date": "2024-12-20 09:59:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:04:19.473184"
    },
    {
      "arxiv_id": "2412.10599v1",
      "title": "Advances in Transformers for Robotic Applications: A Review",
      "title_zh": "Transformer 用于机器人应用的进展：综述",
      "authors": [
        "Nikunj Sanghai",
        "Nik Bear Brown"
      ],
      "abstract": "The introduction of Transformers architecture has brought about significant\nbreakthroughs in Deep Learning (DL), particularly within Natural Language\nProcessing (NLP). Since their inception, Transformers have outperformed many\ntraditional neural network architectures due to their \"self-attention\"\nmechanism and their scalability across various applications. In this paper, we\ncover the use of Transformers in Robotics. We go through recent advances and\ntrends in Transformer architectures and examine their integration into robotic\nperception, planning, and control for autonomous systems. Furthermore, we\nreview past work and recent research on use of Transformers in Robotics as\npre-trained foundation models and integration of Transformers with Deep\nReinforcement Learning (DRL) for autonomous systems. We discuss how different\nTransformer variants are being adapted in robotics for reliable planning and\nperception, increasing human-robot interaction, long-horizon decision-making,\nand generalization. Finally, we address limitations and challenges, offering\ninsight and suggestions for future research directions.",
      "tldr_zh": "这篇论文回顾了 Transformers 架构在机器人领域的最新进展，强调其自注意力机制如何超越传统神经网络，并在机器人感知、规划和控制中发挥关键作用。论文探讨了 Transformers 作为预训练基础模型的应用，以及与深度强化学习(DRL)的整合，以提升人机交互、长期决策和系统泛化能力。最终，它分析了现有挑战和局限性，并为未来研究提供建议。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Early preprint, focusing primarily on general purpose robots, more\n  updates to come",
      "pdf_url": "http://arxiv.org/pdf/2412.10599v1",
      "published_date": "2024-12-13 23:02:15 UTC",
      "updated_date": "2024-12-13 23:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:04:30.323734"
    },
    {
      "arxiv_id": "2412.12190v1",
      "title": "iMoT: Inertial Motion Transformer for Inertial Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Son Minh Nguyen",
        "Linh Duy Tran",
        "Duc Viet Le",
        "Paul J. M Havinga"
      ],
      "abstract": "We propose iMoT, an innovative Transformer-based inertial odometry method\nthat retrieves cross-modal information from motion and rotation modalities for\naccurate positional estimation. Unlike prior work, during the encoding of the\nmotion context, we introduce Progressive Series Decoupler at the beginning of\neach encoder layer to stand out critical motion events inherent in acceleration\nand angular velocity signals. To better aggregate cross-modal interactions, we\npresent Adaptive Positional Encoding, which dynamically modifies positional\nembeddings for temporal discrepancies between different modalities. During\ndecoding, we introduce a small set of learnable query motion particles as\npriors to model motion uncertainties within velocity segments. Each query\nmotion particle is intended to draw cross-modal features dedicated to a\nspecific motion mode, all taken together allowing the model to refine its\nunderstanding of motion dynamics effectively. Lastly, we design a dynamic\nscoring mechanism to stabilize iMoT's optimization by considering all aligned\nmotion particles at the final decoding step, ensuring robust and accurate\nvelocity segment estimation. Extensive evaluations on various inertial datasets\ndemonstrate that iMoT significantly outperforms state-of-the-art methods in\ndelivering superior robustness and accuracy in trajectory reconstruction.",
      "tldr_zh": "本研究提出 iMoT，一种基于 Transformer 的惯性里程计方法，通过从运动和旋转模态中检索跨模态信息，实现精确的位置估计。iMoT 在编码阶段引入 Progressive Series Decoupler 来突出加速度和角速度信号中的关键运动事件，并使用 Adaptive Positional Encoding 动态调整位置嵌入以处理不同模态的时间差异；在解码阶段，采用可学习的查询运动粒子和动态评分机制来建模运动不确定性和优化速度段估计。实验在多种惯性数据集上表明，iMoT 在轨迹重建的鲁棒性和准确性上显著优于现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as technical research paper in 39th AAAI Conference on\n  Artificial Intelligence, 2025 (AAAI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.12190v1",
      "published_date": "2024-12-13 22:52:47 UTC",
      "updated_date": "2024-12-13 22:52:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:04:44.270539"
    },
    {
      "arxiv_id": "2412.10587v2",
      "title": "Evaluation of GPT-4o and GPT-4o-mini's Vision Capabilities for Compositional Analysis from Dried Solution Drops",
      "title_zh": "翻译失败",
      "authors": [
        "Deven B. Dangi",
        "Beni B. Dangi",
        "Oliver Steinbock"
      ],
      "abstract": "When microliter drops of salt solutions dry on non-porous surfaces, they form\nerratic yet characteristic deposit patterns influenced by complex\ncrystallization dynamics and fluid motion. Using OpenAI's image-enabled\nlanguage models, we analyzed deposits from 12 salts with 200 images per salt\nand per model. GPT-4o classified 57% of the salts accurately, significantly\noutperforming random chance and GPT-4o mini. This study underscores the promise\nof general-use AI tools for reliably identifying salts from their drying\npatterns.",
      "tldr_zh": "本文评估了 GPT-4o 和 GPT-4o mini 的视觉能力，用于从干燥盐溶液滴的沉积图案进行成分分析。研究分析了 12 种盐的沉积图像，每种盐有 200 张图片。结果显示，GPT-4o 准确分类了 57% 的盐，显著优于随机猜测和 GPT-4o mini。这研究突出了通用 AI 工具在可靠识别盐类干燥图案方面的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10587v2",
      "published_date": "2024-12-13 22:02:48 UTC",
      "updated_date": "2025-01-27 23:46:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:04:54.846792"
    },
    {
      "arxiv_id": "2412.12189v1",
      "title": "Multi-Surrogate-Teacher Assistance for Representation Alignment in Fingerprint-based Indoor Localization",
      "title_zh": "翻译失败",
      "authors": [
        "Son Minh Nguyen",
        "Linh Duy Tran",
        "Duc Viet Le",
        "Paul J. M Havinga"
      ],
      "abstract": "Despite remarkable progress in knowledge transfer across visual and textual\ndomains, extending these achievements to indoor localization, particularly for\nlearning transferable representations among Received Signal Strength (RSS)\nfingerprint datasets, remains a challenge. This is due to inherent\ndiscrepancies among these RSS datasets, largely including variations in\nbuilding structure, the input number and disposition of WiFi anchors.\nAccordingly, specialized networks, which were deprived of the ability to\ndiscern transferable representations, readily incorporate environment-sensitive\nclues into the learning process, hence limiting their potential when applied to\nspecific RSS datasets. In this work, we propose a plug-and-play (PnP) framework\nof knowledge transfer, facilitating the exploitation of transferable\nrepresentations for specialized networks directly on target RSS datasets\nthrough two main phases. Initially, we design an Expert Training phase, which\nfeatures multiple surrogate generative teachers, all serving as a global\nadapter that homogenizes the input disparities among independent source RSS\ndatasets while preserving their unique characteristics. In a subsequent Expert\nDistilling phase, we continue introducing a triplet of underlying constraints\nthat requires minimizing the differences in essential knowledge between the\nspecialized network and surrogate teachers through refining its representation\nlearning on the target dataset. This process implicitly fosters a\nrepresentational alignment in such a way that is less sensitive to specific\nenvironmental dynamics. Extensive experiments conducted on three benchmark WiFi\nRSS fingerprint datasets underscore the effectiveness of the framework that\nsignificantly exerts the full potential of specialized networks in\nlocalization.",
      "tldr_zh": "本文针对基于 Received Signal Strength (RSS) 指纹的室内定位问题，提出一个 plug-and-play (PnP) 框架，以解决不同数据集间的建筑结构和 WiFi 锚点差异导致的表示学习不可转移性。框架包括 Expert Training 阶段，使用多个 surrogate generative teachers 作为全局适配器，统一源数据集的输入差异同时保留其特性；随后在 Expert Distilling 阶段，通过三重约束最小化 specialized network 与 teachers 之间的知识差异，实现 representation alignment，使其对环境动态不敏感。实验在三个基准 WiFi RSS 指纹数据集上证明，该框架显著提升了 specialized networks 在定位任务中的性能和潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in the 1st round at WACV 2025 (Algorithm Track)",
      "pdf_url": "http://arxiv.org/pdf/2412.12189v1",
      "published_date": "2024-12-13 22:00:26 UTC",
      "updated_date": "2024-12-13 22:00:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:05:07.538209"
    },
    {
      "arxiv_id": "2412.10575v2",
      "title": "Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration",
      "title_zh": "翻译失败",
      "authors": [
        "Karina Halevy",
        "Karly Hou",
        "Charumathi Badrinath"
      ],
      "abstract": "Data augmentation methods, especially SoTA interpolation-based methods such\nas Fair Mixup, have been widely shown to increase model fairness. However, this\nfairness is evaluated on metrics that do not capture model uncertainty and on\ndatasets with only one, relatively large, minority group. As a remedy,\nmulticalibration has been introduced to measure fairness while accommodating\nuncertainty and accounting for multiple minority groups. However, existing\nmethods of improving multicalibration involve reducing initial training data to\ncreate a holdout set for post-processing, which is not ideal when minority\ntraining data is already sparse. This paper uses multicalibration to more\nrigorously examine data augmentation for classification fairness. We\nstress-test four versions of Fair Mixup on two structured data classification\nproblems with up to 81 marginalized groups, evaluating multicalibration\nviolations and balanced accuracy. We find that on nearly every experiment, Fair\nMixup \\textit{worsens} baseline performance and fairness, but the simple\nvanilla Mixup \\textit{outperforms} both Fair Mixup and the baseline, especially\nwhen calibrating on small groups. \\textit{Combining} vanilla Mixup with\nmulticalibration post-processing, which enforces multicalibration through\npost-processing on a holdout set, further increases fairness.",
      "tldr_zh": "这篇论文通过 multicalibration 的视角重新审视基于插值的数据增强方法，旨在更严格地评估其在分类公平性上的表现，特别是考虑模型不确定性和多个少数群体。研究者测试了 Fair Mixup 的四个版本，并在涉及多达 81 个边缘化群体的结构化数据分类问题上进行实验，结果显示 Fair Mixup 几乎在所有情况下都恶化了基线性能和公平性，而 vanilla Mixup 则优于基线和 Fair Mixup，尤其在小群体校准上。将 vanilla Mixup 与 multicalibration post-processing 结合，进一步提高了模型的公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Expanded version of AAAI 2025 main track paper. 8 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10575v2",
      "published_date": "2024-12-13 21:36:12 UTC",
      "updated_date": "2025-04-14 19:40:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:05:19.979957"
    },
    {
      "arxiv_id": "2412.10569v1",
      "title": "Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Dong Hoon Lee",
        "Seunghoon Hong"
      ],
      "abstract": "Recent token reduction methods for Vision Transformers (ViTs) incorporate\ntoken merging, which measures the similarities between token embeddings and\ncombines the most similar pairs. However, their merging policies are directly\ndependent on intermediate features in ViTs, which prevents exploiting features\ntailored for merging and requires end-to-end training to improve token merging.\nIn this paper, we propose Decoupled Token Embedding for Merging (DTEM) that\nenhances token merging through a decoupled embedding learned via a continuously\nrelaxed token merging process. Our method introduces a lightweight embedding\nmodule decoupled from the ViT forward pass to extract dedicated features for\ntoken merging, thereby addressing the restriction from using intermediate\nfeatures. The continuously relaxed token merging, applied during training,\nenables us to learn the decoupled embeddings in a differentiable manner. Thanks\nto the decoupled structure, our method can be seamlessly integrated into\nexisting ViT backbones and trained either modularly by learning only the\ndecoupled embeddings or end-to-end by fine-tuning. We demonstrate the\napplicability of DTEM on various tasks, including classification, captioning,\nand segmentation, with consistent improvement in token merging. Especially in\nthe ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while\nmaintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at\n\\href{https://github.com/movinghoon/dtem}{link}.",
      "tldr_zh": "本研究针对Vision Transformers (ViTs)中的token merging方法提出Decoupled Token Embedding for Merging (DTEM)，通过一个从ViT前向传递中解耦的轻量级embedding模块，提取专用特征来优化token合并，从而避免了对中间特征的依赖。DTEM采用连续松弛的token merging过程，使训练过程可微分，并支持模块化训练（仅训练解耦embedding）或端到端微调。实验结果显示，该方法在分类、标题生成和分割等任务上均表现出一致改善，尤其在ImageNet-1k分类任务中，使用DeiT-small模型实现了37.2%的FLOPs减少，同时保持79.85%的top-1准确率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.10569v1",
      "published_date": "2024-12-13 21:17:11 UTC",
      "updated_date": "2024-12-13 21:17:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:05:31.760462"
    },
    {
      "arxiv_id": "2412.10558v1",
      "title": "Too Big to Fool: Resisting Deception in Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Reza Samsami",
        "Mats Leon Richter",
        "Juan Rodriguez",
        "Megh Thakkar",
        "Sarath Chandar",
        "Maxime Gasse"
      ],
      "abstract": "Large language models must balance their weight-encoded knowledge with\nin-context information from prompts to generate accurate responses. This paper\ninvestigates this interplay by analyzing how models of varying capacities\nwithin the same family handle intentionally misleading in-context information.\nOur experiments demonstrate that larger models exhibit higher resilience to\ndeceptive prompts, showcasing an advanced ability to interpret and integrate\nprompt information with their internal knowledge. Furthermore, we find that\nlarger models outperform smaller ones in following legitimate instructions,\nindicating that their resilience is not due to disregarding in-context\ninformation. We also show that this phenomenon is likely not a result of\nmemorization but stems from the models' ability to better leverage implicit\ntask-relevant information from the prompt alongside their internally stored\nknowledge.",
      "tldr_zh": "这篇论文探讨了大型语言模型如何平衡内部编码知识与提示中的上下文信息，特别分析了不同容量模型对欺骗性提示的抵抗力。实验结果显示，较大模型表现出更高的弹性，能够更好地解释和整合提示信息，同时在遵循合法指令方面也优于小型模型。研究进一步表明，这种抵抗力并非源于记忆，而是模型更有效地利用提示中的隐含任务信息与内部知识。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10558v1",
      "published_date": "2024-12-13 21:03:10 UTC",
      "updated_date": "2024-12-13 21:03:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:06:03.770979"
    },
    {
      "arxiv_id": "2412.10553v1",
      "title": "Edge AI-based Radio Frequency Fingerprinting for IoT Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Mohamed Hussain",
        "Nada Abughanam",
        "Panos Papadimitratos"
      ],
      "abstract": "The deployment of the Internet of Things (IoT) in smart cities and critical\ninfrastructure has enhanced connectivity and real-time data exchange but\nintroduced significant security challenges. While effective, cryptography can\noften be resource-intensive for small-footprint resource-constrained (i.e.,\nIoT) devices. Radio Frequency Fingerprinting (RFF) offers a promising\nauthentication alternative by using unique RF signal characteristics for device\nidentification at the Physical (PHY)-layer, without resorting to cryptographic\nsolutions. The challenge is two-fold: how to deploy such RFF in a large scale\nand for resource-constrained environments. Edge computing, processing data\ncloser to its source, i.e., the wireless device, enables faster\ndecision-making, reducing reliance on centralized cloud servers. Considering a\nmodest edge device, we introduce two truly lightweight Edge AI-based RFF\nschemes tailored for resource-constrained devices. We implement two Deep\nLearning models, namely a Convolution Neural Network and a Transformer-Encoder,\nto extract complex features from the IQ samples, forming device-specific RF\nfingerprints. We convert the models to TensorFlow Lite and evaluate them on a\nRaspberry Pi, demonstrating the practicality of Edge deployment. Evaluations\ndemonstrate the Transformer-Encoder outperforms the CNN in identifying unique\ntransmitter features, achieving high accuracy (> 0.95) and ROC-AUC scores (>\n0.90) while maintaining a compact model size of 73KB, appropriate for\nresource-constrained devices.",
      "tldr_zh": "本文提出了一种基于 Edge AI 的 Radio Frequency Fingerprinting (RFF) 方案，用于解决物联网 (IoT) 网络的安全挑战，该方案利用设备独特的 RF 信号特性在物理层 (PHY-layer) 进行身份验证，避免了资源密集型加密方法。研究者开发了两个轻量级深度学习模型，包括 Convolution Neural Network (CNN) 和 Transformer-Encoder，从 IQ 样本中提取设备特定指纹，并将模型转换为 TensorFlow Lite，在 Raspberry Pi 等资源受限设备上进行部署。实验结果显示，Transformer-Encoder 模型在识别独特发射器特征方面优于 CNN，实现了高准确率 (>0.95) 和 ROC-AUC (>0.90)，同时保持模型大小仅 73KB，证明了其在大规模 IoT 环境中的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, and 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10553v1",
      "published_date": "2024-12-13 20:55:10 UTC",
      "updated_date": "2024-12-13 20:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:06:16.292185"
    },
    {
      "arxiv_id": "2412.12188v1",
      "title": "Predicting Internet Connectivity in Schools: A Feasibility Study Leveraging Multi-modal Data and Location Encoders in Low-Resource Settings",
      "title_zh": "翻译失败",
      "authors": [
        "Kelsey Doerksen",
        "Casper Fibaek",
        "Rochelle Schneider",
        "Do-Hyung Kim",
        "Isabelle Tingzon"
      ],
      "abstract": "Internet connectivity in schools is critical to provide students with the\ndigital literary skills necessary to compete in modern economies. In order for\ngovernments to effectively implement digital infrastructure development in\nschools, accurate internet connectivity information is required. However,\ntraditional survey-based methods can exceed the financial and capacity limits\nof governments. Open-source Earth Observation (EO) datasets have unlocked our\nability to observe and understand socio-economic conditions on Earth from\nspace, and in combination with Machine Learning (ML), can provide the tools to\ncircumvent costly ground-based survey methods to support infrastructure\ndevelopment. In this paper, we present our work on school internet connectivity\nprediction using EO and ML. We detail the creation of our multi-modal,\nfreely-available satellite imagery and survey information dataset, leverage the\nlatest geographically-aware location encoders, and introduce the first results\nof using the new European Space Agency phi-lab geographically-aware\nfoundational model to predict internet connectivity in Botswana and Rwanda. We\nfind that ML with EO and ground-based auxiliary data yields the best\nperformance in both countries, for accuracy, F1 score, and False Positive\nrates, and highlight the challenges of internet connectivity prediction from\nspace with a case study in Kigali, Rwanda. Our work showcases a practical\napproach to support data-driven digital infrastructure development in\nlow-resource settings, leveraging freely available information, and provide\ncleaned and labelled datasets for future studies to the community through a\nunique collaboration between UNICEF and the European Space Agency phi-lab.",
      "tldr_zh": "该研究探讨了在低资源环境中利用多模态数据和位置编码器预测学校互联网连接的可行性，旨在帮助政府高效部署数字基础设施。研究者创建了一个自由可用的数据集，包括卫星图像和调查信息，并结合地球观测（EO）和机器学习（ML）技术，以及欧洲空间局 phi-lab 的地理感知基础模型，在博茨瓦纳和卢旺达进行预测。结果显示，EO 与辅助数据结合的 ML 方法在准确率、F1 分数和假阳性率上表现出最佳性能，并通过基加利案例研究突出了挑战。该工作提供了支持数据驱动数字基础设施发展的实用方法，并公开了清理标记的数据集以供未来研究使用。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.SI"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12188v1",
      "published_date": "2024-12-13 20:20:29 UTC",
      "updated_date": "2024-12-13 20:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:06:27.309691"
    },
    {
      "arxiv_id": "2412.10535v1",
      "title": "On Adversarial Robustness and Out-of-Distribution Robustness of Large Language Models",
      "title_zh": "关于大型语言模型的对抗鲁棒性和分布外鲁棒性",
      "authors": [
        "April Yang",
        "Jordan Tab",
        "Parth Shah",
        "Paul Kotchavong"
      ],
      "abstract": "The increasing reliance on large language models (LLMs) for diverse\napplications necessitates a thorough understanding of their robustness to\nadversarial perturbations and out-of-distribution (OOD) inputs. In this study,\nwe investigate the correlation between adversarial robustness and OOD\nrobustness in LLMs, addressing a critical gap in robustness evaluation. By\napplying methods originally designed to improve one robustness type across both\ncontexts, we analyze their performance on adversarial and out-of-distribution\nbenchmark datasets. The input of the model consists of text samples, with the\noutput prediction evaluated in terms of accuracy, precision, recall, and F1\nscores in various natural language inference tasks.\n  Our findings highlight nuanced interactions between adversarial robustness\nand OOD robustness, with results indicating limited transferability between the\ntwo robustness types. Through targeted ablations, we evaluate how these\ncorrelations evolve with different model sizes and architectures, uncovering\nmodel-specific trends: smaller models like LLaMA2-7b exhibit neutral\ncorrelations, larger models like LLaMA2-13b show negative correlations, and\nMixtral demonstrates positive correlations, potentially due to domain-specific\nalignment. These results underscore the importance of hybrid robustness\nframeworks that integrate adversarial and OOD strategies tailored to specific\nmodels and domains. Further research is needed to evaluate these interactions\nacross larger models and varied architectures, offering a pathway to more\nreliable and generalizable LLMs.",
      "tldr_zh": "本研究探讨了大语言模型(LLMs)的对抗鲁棒性(adversarial robustness)和分布外鲁棒性(Out-of-Distribution robustness, OOD robustness)之间的相关性，填补了现有鲁棒性评估的空白。\n研究者通过将原本设计用于一种鲁棒性的方法应用于对抗和OOD基准数据集，评估了模型在自然语言推理任务中的准确性、精确度、召回率和F1分数，并进行了针对性消融实验。\n结果显示，这两种鲁棒性转移性有限，不同模型大小和架构表现出特定趋势：如LLaMA2-7b呈中性相关、LLaMA2-13b呈负相关，而Mixtral显示正相关，可能源于领域特定调整。\n这些发现强调了开发混合鲁棒性框架的必要性，以针对特定模型和领域提升LLMs的可靠性和泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10535v1",
      "published_date": "2024-12-13 20:04:25 UTC",
      "updated_date": "2024-12-13 20:04:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:06:40.237167"
    },
    {
      "arxiv_id": "2412.10513v1",
      "title": "Extracting PAC Decision Trees from Black Box Binary Classifiers: The Gender Bias Study Case on BERT-based Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ana Ozaki",
        "Roberto Confalonieri",
        "Ricardo Guimarães",
        "Anders Imenes"
      ],
      "abstract": "Decision trees are a popular machine learning method, known for their\ninherent explainability. In Explainable AI, decision trees can be used as\nsurrogate models for complex black box AI models or as approximations of parts\nof such models. A key challenge of this approach is determining how accurately\nthe extracted decision tree represents the original model and to what extent it\ncan be trusted as an approximation of their behavior. In this work, we\ninvestigate the use of the Probably Approximately Correct (PAC) framework to\nprovide a theoretical guarantee of fidelity for decision trees extracted from\nAI models. Based on theoretical results from the PAC framework, we adapt a\ndecision tree algorithm to ensure a PAC guarantee under certain conditions. We\nfocus on binary classification and conduct experiments where we extract\ndecision trees from BERT-based language models with PAC guarantees. Our results\nindicate occupational gender bias in these models.",
      "tldr_zh": "这篇论文探讨了从黑箱二元分类器中提取 PAC Decision Trees 的方法，以提升模型解释性，并提供理论上的保真度保证。研究者改进了决策树算法，使其在特定条件下符合 Probably Approximately Correct (PAC) 框架，并应用于 BERT-based Language Models 的二元分类任务。实验结果显示，这些模型存在明显的职业性别偏见，为 Explainable AI 提供了新的洞见。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.10513v1",
      "published_date": "2024-12-13 19:14:08 UTC",
      "updated_date": "2024-12-13 19:14:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:06:50.397227"
    },
    {
      "arxiv_id": "2412.10511v1",
      "title": "Automated Image Captioning with CNNs and Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Adrian Cahyono",
        "Jeremy Nathan Jusuf"
      ],
      "abstract": "This project aims to create an automated image captioning system that\ngenerates natural language descriptions for input images by integrating\ntechniques from computer vision and natural language processing. We employ\nvarious different techniques, ranging from CNN-RNN to the more advanced\ntransformer-based techniques. Training is carried out on image datasets paired\nwith descriptive captions, and model performance will be evaluated using\nestablished metrics such as BLEU, METEOR, and CIDEr. The project will also\ninvolve experimentation with advanced attention mechanisms, comparisons of\ndifferent architectural choices, and hyperparameter optimization to refine\ncaptioning accuracy and overall system effectiveness.",
      "tldr_zh": "这篇论文提出了一种自动图像描述系统，通过整合计算机视觉和自然语言处理技术（如 CNNs 和 Transformers）生成输入图像的自然语言描述。系统在配有描述性标题的图像数据集上进行训练，并使用 BLEU、METEOR 和 CIDEr 等指标评估模型性能。研究还包括实验高级注意力机制、不同架构比较以及超参数优化，以提升描述准确性和整体系统效能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10511v1",
      "published_date": "2024-12-13 19:12:11 UTC",
      "updated_date": "2024-12-13 19:12:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:07:02.341312"
    },
    {
      "arxiv_id": "2412.10509v1",
      "title": "Do Large Language Models Show Biases in Causal Learning?",
      "title_zh": "大语言模型在因果学习中是否表现出偏差？",
      "authors": [
        "Maria Victoria Carro",
        "Francisca Gauna Selasco",
        "Denise Alejandra Mester",
        "Margarita Gonzales",
        "Mario A. Leiva",
        "Maria Vanina Martinez",
        "Gerardo I. Simari"
      ],
      "abstract": "Causal learning is the cognitive process of developing the capability of\nmaking causal inferences based on available information, often guided by\nnormative principles. This process is prone to errors and biases, such as the\nillusion of causality, in which people perceive a causal relationship between\ntwo variables despite lacking supporting evidence. This cognitive bias has been\nproposed to underlie many societal problems, including social prejudice,\nstereotype formation, misinformation, and superstitious thinking. In this\nresearch, we investigate whether large language models (LLMs) develop causal\nillusions, both in real-world and controlled laboratory contexts of causal\nlearning and inference. To this end, we built a dataset of over 2K samples\nincluding purely correlational cases, situations with null contingency, and\ncases where temporal information excludes the possibility of causality by\nplacing the potential effect before the cause. We then prompted the models to\nmake statements or answer causal questions to evaluate their tendencies to\ninfer causation erroneously in these structured settings. Our findings show a\nstrong presence of causal illusion bias in LLMs. Specifically, in open-ended\ngeneration tasks involving spurious correlations, the models displayed bias at\nlevels comparable to, or even lower than, those observed in similar studies on\nhuman subjects. However, when faced with null-contingency scenarios or temporal\ncues that negate causal relationships, where it was required to respond on a\n0-100 scale, the models exhibited significantly higher bias. These findings\nsuggest that the models have not uniformly, consistently, or reliably\ninternalized the normative principles essential for accurate causal learning.",
      "tldr_zh": "本文研究了大型语言模型（LLMs）在因果学习中是否表现出因果幻觉偏差，即错误推断因果关系的问题。研究团队构建了一个超过2K样本的数据集，涵盖纯相关性、零偶然性和时间信息排除因果的场景，并通过提示模型进行开放生成和0-100规模响应的测试。结果表明，LLMs在虚假相关性任务中偏差水平与人类相当或更低，但在零偶然性或否定因果的时间线索中，偏差显著更高，显示这些模型尚未均匀内化准确的因果学习规范原则。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10509v1",
      "published_date": "2024-12-13 19:03:48 UTC",
      "updated_date": "2024-12-13 19:03:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:07:16.845960"
    },
    {
      "arxiv_id": "2412.10494v1",
      "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device",
      "title_zh": "翻译失败",
      "authors": [
        "Yushu Wu",
        "Zhixing Zhang",
        "Yanyu Li",
        "Yanwu Xu",
        "Anil Kag",
        "Yang Sui",
        "Huseyin Coskun",
        "Ke Ma",
        "Aleksei Lebedev",
        "Ju Hu",
        "Dimitris Metaxas",
        "Yanzhi Wang",
        "Sergey Tulyakov",
        "Jian Ren"
      ],
      "abstract": "We have witnessed the unprecedented success of diffusion-based video\ngeneration over the past year. Recently proposed models from the community have\nwielded the power to generate cinematic and high-resolution videos with smooth\nmotions from arbitrary input prompts. However, as a supertask of image\ngeneration, video generation models require more computation and are thus\nhosted mostly on cloud servers, limiting broader adoption among content\ncreators. In this work, we propose a comprehensive acceleration framework to\nbring the power of the large-scale video diffusion model to the hands of edge\nusers. From the network architecture scope, we initialize from a compact image\nbackbone and search out the design and arrangement of temporal layers to\nmaximize hardware efficiency. In addition, we propose a dedicated adversarial\nfine-tuning algorithm for our efficient model and reduce the denoising steps to\n4. Our model, with only 0.6B parameters, can generate a 5-second video on an\niPhone 16 PM within 5 seconds. Compared to server-side models that take minutes\non powerful GPUs to generate a single video, we accelerate the generation by\nmagnitudes while delivering on-par quality.",
      "tldr_zh": "该研究提出SnapGen-V框架，通过全面加速优化，将基于diffusion模型的视频生成技术移植到移动设备上，旨在解决现有模型计算密集且依赖云服务器的问题。方法包括从紧凑的图像骨干网络初始化，并搜索时间层设计以提升硬件效率，同时采用专用的对抗性微调算法，将去噪步骤减少到4步。结果表明，该模型仅0.6B参数，在iPhone 16 PM上生成5秒视频只需5秒，与服务器端模型相比加速数倍量级，同时保持相媲美视频质量。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CV",
      "comment": "https://snap-research.github.io/snapgen-v/",
      "pdf_url": "http://arxiv.org/pdf/2412.10494v1",
      "published_date": "2024-12-13 18:59:56 UTC",
      "updated_date": "2024-12-13 18:59:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:07:27.368746"
    },
    {
      "arxiv_id": "2412.10373v1",
      "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Sicheng Zuo",
        "Wenzhao Zheng",
        "Yuanhui Huang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.",
      "tldr_zh": "这篇论文提出GaussianWorld框架，一种基于世界模型的方法，用于改进自动驾驶中的3D occupancy prediction，通过利用场景演变的先验（如静态场景对齐、动态物体局部运动和新场景完成）来实现4D占用预测。框架在3D Gaussian空间中结合当前RGB观察，显式推理场景演变，从而避免了现有方法的局限性。在nuScenes数据集上实验表明，GaussianWorld比单帧模型提高了超过2%的mIoU，且不引入额外计算开销。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/zuosc19/GaussianWorld",
      "pdf_url": "http://arxiv.org/pdf/2412.10373v1",
      "published_date": "2024-12-13 18:59:54 UTC",
      "updated_date": "2024-12-13 18:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:07:39.551730"
    },
    {
      "arxiv_id": "2412.10493v1",
      "title": "SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Runtao Liu",
        "Chen I Chieh",
        "Jindong Gu",
        "Jipeng Zhang",
        "Renjie Pi",
        "Qifeng Chen",
        "Philip Torr",
        "Ashkan Khakzar",
        "Fabio Pizzati"
      ],
      "abstract": "Text-to-image (T2I) models have become widespread, but their limited safety\nguardrails expose end users to harmful content and potentially allow for model\nmisuse. Current safety measures are typically limited to text-based filtering\nor concept removal strategies, able to remove just a few concepts from the\nmodel's generative capabilities. In this work, we introduce SafetyDPO, a method\nfor safety alignment of T2I models through Direct Preference Optimization\n(DPO). We enable the application of DPO for safety purposes in T2I models by\nsynthetically generating a dataset of harmful and safe image-text pairs, which\nwe call CoProV2. Using a custom DPO strategy and this dataset, we train safety\nexperts, in the form of low-rank adaptation (LoRA) matrices, able to guide the\ngeneration process away from specific safety-related concepts. Then, we merge\nthe experts into a single LoRA using a novel merging strategy for optimal\nscaling performance. This expert-based approach enables scalability, allowing\nus to remove 7 times more harmful concepts from T2I models compared to\nbaselines. SafetyDPO consistently outperforms the state-of-the-art on many\nbenchmarks and establishes new practices for safety alignment in T2I networks.\nCode and data will be shared at https://safetydpo.github.io/.",
      "tldr_zh": "本研究针对文本到图像 (T2I) 模型的安全问题，提出 SafetyDPO 方法，通过 Direct Preference Optimization (DPO) 实现可扩展的安全对齐，以减少有害内容生成。方法包括合成数据集 CoProV2（包含有害和安全图像-文本对），并使用低秩适应 (LoRA) 矩阵训练安全专家，然后通过新颖的合并策略将这些专家整合到一个 LoRA 中，提升模型的可扩展性。实验结果显示，SafetyDPO 比基线方法移除 7 倍更多有害概念，并在多个基准测试中优于现有技术，为 T2I 模型的安全对齐建立了新标准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10493v1",
      "published_date": "2024-12-13 18:59:52 UTC",
      "updated_date": "2024-12-13 18:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:07:52.262573"
    },
    {
      "arxiv_id": "2412.10371v1",
      "title": "GaussianAD: Gaussian-Centric End-to-End Autonomous Driving",
      "title_zh": "GaussianAD：以高斯为中心的端到端自动驾驶",
      "authors": [
        "Wenzhao Zheng",
        "Junjie Wu",
        "Yao Zheng",
        "Sicheng Zuo",
        "Zixun Xie",
        "Longchao Yang",
        "Yong Pan",
        "Zhihui Hao",
        "Peng Jia",
        "Xianpeng Lang",
        "Shanghang Zhang"
      ],
      "abstract": "Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.",
      "tldr_zh": "本论文提出 GaussianAD，一种以 3D 语义 Gaussians 为中心的端到端自动驾驶框架，旨在解决现有方法在场景表示上的全面性与效率权衡问题。\n该框架通过初始化均匀 3D Gaussians 并利用周围视图图像逐步精炼它们，进行高效的 3D 感知（如检测和语义地图构建），并预测动态语义的 3D 流以规划车辆轨迹。\n实验结果在 nuScenes 数据集上显示，GaussianAD 在运动规划、3D 占用预测和 4D 占用预测任务中表现出色，可实现端到端训练并提升整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/wzzheng/GaussianAD",
      "pdf_url": "http://arxiv.org/pdf/2412.10371v1",
      "published_date": "2024-12-13 18:59:30 UTC",
      "updated_date": "2024-12-13 18:59:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:08:04.355341"
    },
    {
      "arxiv_id": "2412.10360v1",
      "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Orr Zohar",
        "Xiaohan Wang",
        "Yann Dubois",
        "Nikhil Mehta",
        "Tong Xiao",
        "Philippe Hansen-Estruch",
        "Licheng Yu",
        "Xiaofang Wang",
        "Felix Juefei-Xu",
        "Ning Zhang",
        "Serena Yeung-Levy",
        "Xide Xia"
      ],
      "abstract": "Despite the rapid integration of video perception capabilities into Large\nMultimodal Models (LMMs), the underlying mechanisms driving their video\nunderstanding remain poorly understood. Consequently, many design decisions in\nthis domain are made without proper justification or analysis. The high\ncomputational cost of training and evaluating such models, coupled with limited\nopen research, hinders the development of video-LMMs. To address this, we\npresent a comprehensive study that helps uncover what effectively drives video\nunderstanding in LMMs.\n  We begin by critically examining the primary contributors to the high\ncomputational requirements associated with video-LMM research and discover\nScaling Consistency, wherein design and training decisions made on smaller\nmodels and datasets (up to a critical size) effectively transfer to larger\nmodels. Leveraging these insights, we explored many video-specific aspects of\nvideo-LMMs, including video sampling, architectures, data composition, training\nschedules, and more. For example, we demonstrated that fps sampling during\ntraining is vastly preferable to uniform frame sampling and which vision\nencoders are the best for video representation.\n  Guided by these findings, we introduce Apollo, a state-of-the-art family of\nLMMs that achieve superior performance across different model sizes. Our models\ncan perceive hour-long videos efficiently, with Apollo-3B outperforming most\nexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is\nstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on\nVideo-MME.",
      "tldr_zh": "该研究探讨了大型多模态模型（LMMs）中视频理解的机制，揭示了设计决策缺乏依据和高计算成本的问题，并通过 Scaling Consistency 原理证明小模型决策可有效转移到大模型。研究者考察了视频采样（如 fps sampling 优于均匀帧采样）、架构、数据组成和训练计划等多方面因素，为视频-LMMs 优化提供了关键洞见。最终，他们推出了 Apollo 模型系列，其中 Apollo-3B 在 LongVideoBench 上达到 55.1 分，优于多数 7B 模型，而 Apollo-7B 在 MLVU 和 Video-MME 上分别取得 70.9 和 63.3 分的领先性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://apollo-lmms.github.io",
      "pdf_url": "http://arxiv.org/pdf/2412.10360v1",
      "published_date": "2024-12-13 18:53:24 UTC",
      "updated_date": "2024-12-13 18:53:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:09.281133"
    },
    {
      "arxiv_id": "2412.10354v3",
      "title": "A Library for Learning Neural Operators",
      "title_zh": "翻译失败",
      "authors": [
        "Jean Kossaifi",
        "Nikola Kovachki",
        "Zongyi Li",
        "David Pitt",
        "Miguel Liu-Schiaffini",
        "Robert Joseph George",
        "Boris Bonev",
        "Kamyar Azizzadenesheli",
        "Julius Berner",
        "Valentin Duruisseaux",
        "Anima Anandkumar"
      ],
      "abstract": "We present NeuralOperator, an open-source Python library for operator\nlearning. Neural operators generalize neural networks to maps between function\nspaces instead of finite-dimensional Euclidean spaces. They can be trained and\ninferenced on input and output functions given at various discretizations,\nsatisfying a discretization convergence properties. Built on top of PyTorch,\nNeuralOperator provides all the tools for training and deploying neural\noperator models, as well as developing new ones, in a high-quality, tested,\nopen-source package. It combines cutting-edge models and customizability with a\ngentle learning curve and simple user interface for newcomers.",
      "tldr_zh": "这篇论文介绍了 NeuralOperator，一个开源 Python 库，用于操作学习（operator learning），它将神经网络泛化为函数空间之间的映射，而不是有限维欧式空间。库基于 PyTorch，提供全面工具来训练、部署神经操作符模型，支持各种离散化并满足离散化收敛属性。NeuralOperator 结合了前沿模型的设计、可定制性以及用户友好的界面，使其适合初学者开发和应用新模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10354v3",
      "published_date": "2024-12-13 18:49:37 UTC",
      "updated_date": "2025-04-30 17:23:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:08:26.841761"
    },
    {
      "arxiv_id": "2412.10348v1",
      "title": "A dual contrastive framework",
      "title_zh": "双对比框架",
      "authors": [
        "Yuan Sun",
        "Zhao Zhang",
        "Jorge Ortiz"
      ],
      "abstract": "In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks",
      "tldr_zh": "本研究针对大型视觉语言模型在区域级别视觉理解中的挑战（如有限的空间感知和粗粒度预训练问题），提出了一种名为 AlignCap 的双对比框架，以提升区域标题任务的性能。该框架包括一个潜在特征精炼模块（latent feature refinement module）和一个语义空间对齐模块（semantic space alignment module），通过对比学习（contrastive learning）在两者中创新性地整合，以细粒度对齐潜在空间并改进多模态表示。此外，使用 General Object Detection (GOD) 作为数据预处理管道来增强空间推理。实验结果显示，该方法显著提高了各种任务中的区域级别标题性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10348v1",
      "published_date": "2024-12-13 18:45:18 UTC",
      "updated_date": "2024-12-13 18:45:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:08:40.189128"
    },
    {
      "arxiv_id": "2412.10347v1",
      "title": "COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models",
      "title_zh": "COMET：全面生物多组学评估任务",
      "authors": [
        "Yuchen Ren",
        "Wenwei Han",
        "Qianyuan Zhang",
        "Yining Tang",
        "Weiqiang Bai",
        "Yuchen Cai",
        "Lifeng Qiao",
        "Hao Jiang",
        "Dong Yuan",
        "Tao Chen",
        "Siqi Sun",
        "Pan Tan",
        "Wanli Ouyang",
        "Nanqing Dong",
        "Xinzhu Ma",
        "Peng Ye"
      ],
      "abstract": "As key elements within the central dogma, DNA, RNA, and proteins play crucial\nroles in maintaining life by guaranteeing accurate genetic expression and\nimplementation. Although research on these molecules has profoundly impacted\nfields like medicine, agriculture, and industry, the diversity of machine\nlearning approaches-from traditional statistical methods to deep learning\nmodels and large language models-poses challenges for researchers in choosing\nthe most suitable models for specific tasks, especially for cross-omics and\nmulti-omics tasks due to the lack of comprehensive benchmarks. To address this,\nwe introduce the first comprehensive multi-omics benchmark COMET (Benchmark for\nBiological COmprehensive Multi-omics Evaluation Tasks and Language Models),\ndesigned to evaluate models across single-omics, cross-omics, and multi-omics\ntasks. First, we curate and develop a diverse collection of downstream tasks\nand datasets covering key structural and functional aspects in DNA, RNA, and\nproteins, including tasks that span multiple omics levels. Then, we evaluate\nexisting foundational language models for DNA, RNA, and proteins, as well as\nthe newly proposed multi-omics method, offering valuable insights into their\nperformance in integrating and analyzing data from different biological\nmodalities. This benchmark aims to define critical issues in multi-omics\nresearch and guide future directions, ultimately promoting advancements in\nunderstanding biological processes through integrated and different omics data\nanalysis.",
      "tldr_zh": "该研究介绍了 COMET，这是一个全面的生物多组学基准，用于评估模型在单组学、跨组学和多组学任务中的性能，以解决机器学习方法在处理 DNA、RNA 和 proteins 数据时的选择难题。论文整理了多样化的下游任务和数据集，涵盖这些分子的结构和功能方面，并评估了现有基础语言模型以及一个新提出的多组学方法。结果提供了对模型整合不同生物模态数据的宝贵洞见，帮助识别多组学研究的关键问题，并指导未来通过整合组学数据分析来推进生物过程理解。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10347v1",
      "published_date": "2024-12-13 18:42:00 UTC",
      "updated_date": "2024-12-13 18:42:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:21.206596"
    },
    {
      "arxiv_id": "2412.10345v2",
      "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
      "title_zh": "TraceVLA：",
      "authors": [
        "Ruijie Zheng",
        "Yongyuan Liang",
        "Shuaiyi Huang",
        "Jianfeng Gao",
        "Hal Daumé III",
        "Andrey Kolobov",
        "Furong Huang",
        "Jianwei Yang"
      ],
      "abstract": "Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency.",
      "tldr_zh": "这篇论文提出 visual trace prompting 方法，以提升大型视觉-语言-动作 (VLA) 模型在机器人交互任务中的空间-时间意识，从而更好地处理复杂操作如机器人操控。研究团队开发了 TraceVLA 模型，通过在自收集的 15 万机器人操作轨迹数据集上微调 OpenVLA，实现对状态-动作轨迹的视觉编码。实验评估显示，TraceVLA 在 SimplerEnv 的 137 个配置中比 OpenVLA 提升 10%，在真实 WidowX 机器人上的 4 个任务中性能提高 3.5 倍，并展现出在不同环境和场景的鲁棒泛化。此外，他们基于 4B Phi-3-Vision 的紧凑 VLA 模型在性能上与 7B OpenVLA 基线相当，但显著提高了推理效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10345v2",
      "published_date": "2024-12-13 18:40:51 UTC",
      "updated_date": "2024-12-25 23:12:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:09:06.035916"
    },
    {
      "arxiv_id": "2412.10342v2",
      "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
      "title_zh": "Iris",
      "authors": [
        "Zhiqi Ge",
        "Juncheng Li",
        "Xinglei Pang",
        "Minghe Gao",
        "Kaihang Pan",
        "Wang Lin",
        "Hao Fei",
        "Wenqiao Zhang",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "abstract": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
      "tldr_zh": "该论文介绍了Iris，一种基础视觉代理，用于自动化交互式数字环境（如网页和软件应用）的任务，通过解决Multimodal Large Language Models (MLLMs) 在处理高分辨率、视觉复杂Graphical User Interfaces (GUIs)时的感知挑战。Iris的关键创新包括Information-Sensitive Cropping (ISC)，利用边缘检测算法动态识别并优先处理信息密集区域，以提高计算效率；以及Self-Refining Dual Learning (SRDL)，通过双重学习循环强化UI元素的描述（referring）和定位（grounding），无需额外标注数据。实验结果显示，Iris仅使用850K GUI标注数据，就在多个基准上实现了最先进性能，超越了使用10倍数据的方法，并在网页和操作系统代理任务中取得了显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10342v2",
      "published_date": "2024-12-13 18:40:10 UTC",
      "updated_date": "2025-02-03 15:23:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:09:16.267512"
    },
    {
      "arxiv_id": "2501.10373v1",
      "title": "DK-PRACTICE: An Intelligent Educational Platform for Personalized Learning Content Recommendations Based on Students Knowledge State",
      "title_zh": "DK-PRACTICE：一种基于学生知识状态的个性化学习内容推荐智能教育平台",
      "authors": [
        "Marina Delianidi",
        "Konstantinos Diamantaras",
        "Ioannis Moras",
        "Antonis Sidiropoulos"
      ],
      "abstract": "This study introduces DK-PRACTICE (Dynamic Knowledge Prediction and\nEducational Content Recommendation System), an intelligent online platform that\nleverages machine learning to provide personalized learning recommendations\nbased on student knowledge state. Students participate in a short, adaptive\nassessment using the question-and-answer method regarding key concepts in a\nspecific knowledge domain. The system dynamically selects the next question for\neach student based on the correctness and accuracy of their previous answers.\nAfter the test is completed, DK-PRACTICE analyzes students' interaction history\nto recommend learning materials to empower the student's knowledge state in\nidentified knowledge gaps. Both question selection and learning material\nrecommendations are based on machine learning models trained using anonymized\ndata from a real learning environment. To provide self-assessment and monitor\nlearning progress, DK-PRACTICE allows students to take two tests: one\npre-teaching and one post-teaching. After each test, a report is generated with\ndetailed results. In addition, the platform offers functions to visualize\nlearning progress based on recorded test statistics. DK-PRACTICE promotes\nadaptive and personalized learning by empowering students with self-assessment\ncapabilities and providing instructors with valuable information about\nstudents' knowledge levels. DK-PRACTICE can be extended to various educational\nenvironments and knowledge domains, provided the necessary data is available\naccording to the educational topics. A subsequent paper will present the\nmethodology for the experimental application and evaluation of the platform.",
      "tldr_zh": "本研究引入了DK-PRACTICE，一种智能在线教育平台，利用machine learning模型根据学生的知识状态提供个性化学习内容推荐。系统通过自适应评估（如动态选择问题基于学生答案的正确性和准确性）及分析交互历史，识别知识缺口并推荐相关学习材料，同时支持预教学和后教学测试、报告生成以及学习进度可视化。DK-PRACTICE增强了学生的自评估能力，并为教师提供宝贵洞见，促进适应性和个性化学习，可扩展至各种教育环境，前续论文将讨论其实验应用和评估方法。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "General, Computer Uses in Education",
        "I.2.0; K.3.1"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, The Barcelona Conference on Education 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.10373v1",
      "published_date": "2024-12-13 18:35:37 UTC",
      "updated_date": "2024-12-13 18:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:09:27.012110"
    },
    {
      "arxiv_id": "2412.10337v2",
      "title": "Generative AI in Medicine",
      "title_zh": "翻译失败",
      "authors": [
        "Divya Shanmugam",
        "Monica Agrawal",
        "Rajiv Movva",
        "Irene Y. Chen",
        "Marzyeh Ghassemi",
        "Maia Jacobs",
        "Emma Pierson"
      ],
      "abstract": "The increased capabilities of generative AI have dramatically expanded its\npossible use cases in medicine. We provide a comprehensive overview of\ngenerative AI use cases for clinicians, patients, clinical trial organizers,\nresearchers, and trainees. We then discuss the many challenges -- including\nmaintaining privacy and security, improving transparency and interpretability,\nupholding equity, and rigorously evaluating models -- which must be overcome to\nrealize this potential, and the open research directions they give rise to.",
      "tldr_zh": "这篇论文概述了生成式 AI 在医学领域的潜在应用，包括为临床医生、患者、临床试验组织者、研究人员和培训者提供的各种用例，如辅助诊断和个性化治疗。论文讨论了实现这些应用的挑战，包括维护隐私和安全、提升透明度和可解释性、确保公平性，以及对模型进行严格评估。最终，它指出了这些挑战引发的开放研究方向，以推动生成式 AI 在医学中的可靠部署。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in the Annual Review of Biomedical Data Science, August\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2412.10337v2",
      "published_date": "2024-12-13 18:32:21 UTC",
      "updated_date": "2024-12-17 14:57:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:09:38.300707"
    },
    {
      "arxiv_id": "2412.10321v1",
      "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
      "title_zh": "翻译失败",
      "authors": [
        "Sicheng Zhu",
        "Brandon Amos",
        "Yuandong Tian",
        "Chuan Guo",
        "Ivan Evtimov"
      ],
      "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.",
      "tldr_zh": "这篇论文提出了 AdvPrefix，一种新的前缀强制目标，旨在解决传统 LLM 越狱攻击的局限性，如对模型行为的控制不足和优化困难。AdvPrefix 通过自动选择基于模型的前缀（基于高预填充攻击成功率和低 negative log-likelihood），并支持为单个用户请求使用多个前缀，从而实现更细致的控制和优化。实验结果显示，将 AdvPrefix 整合到 GCG attack 中后，在 Llama-3 模型上将细致攻击成功率从 14% 提高到 80%，表明当前模型对齐难以泛化到未见前缀。该工作强调了越狱目标在实现更有效 jailbreaks 中的关键重要性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10321v1",
      "published_date": "2024-12-13 18:00:57 UTC",
      "updated_date": "2024-12-13 18:00:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:09:53.267804"
    },
    {
      "arxiv_id": "2412.10320v1",
      "title": "MeshA*: Efficient Path Planing With Motion Primitives",
      "title_zh": "翻译失败",
      "authors": [
        "Marat Agranovskiy",
        "Konstantin Yakovlev"
      ],
      "abstract": "We study a path planning problem where the possible move actions are\nrepresented as a finite set of motion primitives aligned with the grid\nrepresentation of the environment. That is, each primitive corresponds to a\nshort kinodynamically-feasible motion of an agent and is represented as a\nsequence of the swept cells of a grid. Typically heuristic search, i.e. A*, is\nconducted over the lattice induced by these primitives (lattice-based planning)\nto find a path. However due to the large branching factor such search may be\ninefficient in practice. To this end we suggest a novel technique rooted in the\nidea of searching over the grid cells (as in vanilla A*) simultaneously fitting\nthe possible sequences of the motion primitives into these cells. The resultant\nalgorithm, MeshA*, provably preserves the guarantees on completeness and\noptimality, on the one hand, and is shown to notably outperform conventional\nlattice-based planning (x1.5 decrease in the runtime), on the other hand.\nMoreover, we suggest an additional pruning technique that additionally\ndecreases the search space of MeshA*. The resultant planner is combined with\nthe regular A* to retain completeness and is shown to further increase the\nsearch performance at the cost of negligible decrease of the solution quality.",
      "tldr_zh": "这篇论文针对路径规划问题，提出了一种新型算法 MeshA*，它在网格单元上进行搜索，同时将运动 primitives 的序列拟合到这些单元中，以解决传统 lattice-based planning 因分支因子大而导致的效率低下问题。MeshA* 算法保留了完整性和最优性保证，同时显著提高了性能，运行时间比常规方法减少 1.5 倍。此外，论文引入了额外的修剪技术，与标准 A* 结合，进一步减少搜索空间，提升整体效率，但仅以微不足道的解决方案质量损失为代价。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10320v1",
      "published_date": "2024-12-13 18:00:21 UTC",
      "updated_date": "2024-12-13 18:00:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:04.300178"
    },
    {
      "arxiv_id": "2412.10316v3",
      "title": "BrushEdit: All-In-One Image Inpainting and Editing",
      "title_zh": "BrushEdit: 一体化图像修复与编辑",
      "authors": [
        "Yaowei Li",
        "Yuxuan Bian",
        "Xuan Ju",
        "Zhaoyang Zhang",
        "Junhao Zhuang",
        "Ying Shan",
        "Yuexian Zou",
        "Qiang Xu"
      ],
      "abstract": "Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.",
      "tldr_zh": "该论文提出BrushEdit，一种基于inpainting的全功能图像编辑框架，旨在解决现有逆向方法在处理大修改（如添加或移除物体）时的局限性，以及指令方法对用户交互的限制。BrushEdit整合多模态大语言模型(MLLMs)和双分支图像inpainting模型，通过代理合作框架实现编辑类别分类、主对象识别、掩码获取和编辑区域修复，从而支持自主、交互式的自由形式指令编辑。实验结果显示，该框架在七个指标（如掩码区域保留和编辑效果连贯性）上优于基线模型，显著提升了图像编辑的性能和用户友好度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/",
      "pdf_url": "http://arxiv.org/pdf/2412.10316v3",
      "published_date": "2024-12-13 17:58:06 UTC",
      "updated_date": "2025-05-05 16:31:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:32.330983"
    },
    {
      "arxiv_id": "2412.16188v1",
      "title": "A Decade of Deep Learning: A Survey on The Magnificent Seven",
      "title_zh": "翻译失败",
      "authors": [
        "Dilshod Azizov",
        "Muhammad Arslan Manzoor",
        "Velibor Bojkovic",
        "Yingxu Wang",
        "Zixiao Wang",
        "Zangir Iklassov",
        "Kailong Zhao",
        "Liang Li",
        "Siwei Liu",
        "Yu Zhong",
        "Wei Liu",
        "Shangsong Liang"
      ],
      "abstract": "Deep learning has fundamentally reshaped the landscape of artificial\nintelligence over the past decade, enabling remarkable achievements across\ndiverse domains. At the heart of these developments lie multi-layered neural\nnetwork architectures that excel at automatic feature extraction, leading to\nsignificant improvements in machine learning tasks. To demystify these advances\nand offer accessible guidance, we present a comprehensive overview of the most\ninfluential deep learning algorithms selected through a broad-based survey of\nthe field. Our discussion centers on pivotal architectures, including Residual\nNetworks, Transformers, Generative Adversarial Networks, Variational\nAutoencoders, Graph Neural Networks, Contrastive Language-Image Pre-training,\nand Diffusion models. We detail their historical context, highlight their\nmathematical foundations and algorithmic principles, and examine subsequent\nvariants, extensions, and practical considerations such as training\nmethodologies, normalization techniques, and learning rate schedules. Beyond\nhistorical and technical insights, we also address their applications,\nchallenges, and potential research directions. This survey aims to serve as a\npractical manual for both newcomers seeking an entry point into cutting-edge\ndeep learning methods and experienced researchers transitioning into this\nrapidly evolving domain.",
      "tldr_zh": "这篇综述回顾了过去十年深度学习在人工智能领域的重大进展，聚焦于七个最具影响力的架构：Residual Networks、Transformers、Generative Adversarial Networks、Variational Autoencoders、Graph Neural Networks、Contrastive Language-Image Pre-training 和 Diffusion models。论文详细阐述了这些模型的历史背景、数学基础、算法原理以及后续变体、扩展，同时讨论了训练方法、归一化技巧和学习率调度等实际应用考虑。最终，它分析了这些架构的广泛应用、潜在挑战以及未来研究方向，为深度学习新手和资深研究人员提供了一个实用入门指南。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.16188v1",
      "published_date": "2024-12-13 17:55:39 UTC",
      "updated_date": "2024-12-13 17:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:44.755194"
    },
    {
      "arxiv_id": "2412.10312v1",
      "title": "Interlocking-free Selective Rationalization Through Genetic-based Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Ruggeri",
        "Gaetano Signorelli"
      ],
      "abstract": "A popular end-to-end architecture for selective rationalization is the\nselect-then-predict pipeline, comprising a generator to extract highlights fed\nto a predictor. Such a cooperative system suffers from suboptimal equilibrium\nminima due to the dominance of one of the two modules, a phenomenon known as\ninterlocking. While several contributions aimed at addressing interlocking,\nthey only mitigate its effect, often by introducing feature-based heuristics,\nsampling, and ad-hoc regularizations. We present GenSPP, the first\ninterlocking-free architecture for selective rationalization that does not\nrequire any learning overhead, as the above-mentioned. GenSPP avoids\ninterlocking by performing disjoint training of the generator and predictor via\ngenetic global search. Experiments on a synthetic and a real-world benchmark\nshow that our model outperforms several state-of-the-art competitors.",
      "tldr_zh": "该论文针对选择性理性化（selective rationalization）的select-then-predict管道中存在的interlocking问题提出了一种新架构GenSPP，该问题源于生成器和预测器模块的竞争导致次优平衡。GenSPP通过遗传全局搜索(genetic global search)实现生成器和预测器的分离训练，从而避免interlocking而无需额外的学习开销。实验结果显示，在合成和真实世界基准上，GenSPP优于现有竞争模型，证明了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10312v1",
      "published_date": "2024-12-13 17:52:48 UTC",
      "updated_date": "2024-12-13 17:52:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:10:55.996741"
    },
    {
      "arxiv_id": "2412.10302v1",
      "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyu Wu",
        "Xiaokang Chen",
        "Zizheng Pan",
        "Xingchao Liu",
        "Wen Liu",
        "Damai Dai",
        "Huazuo Gao",
        "Yiyang Ma",
        "Chengyue Wu",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Yu Wu",
        "Kai Hu",
        "Jiawei Wang",
        "Yaofeng Sun",
        "Yukun Li",
        "Yishi Piao",
        "Kang Guan",
        "Aixin Liu",
        "Xin Xie",
        "Yuxiang You",
        "Kai Dong",
        "Xingkai Yu",
        "Haowei Zhang",
        "Liang Zhao",
        "Yisong Wang",
        "Chong Ruan"
      ],
      "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
      "tldr_zh": "我们提出了 DeepSeek-VL2，一系列先进的 Mixture-of-Experts (MoE) Vision-Language Models，通过动态平铺视觉编码策略处理高分辨率图像的各种长宽比，以及 Multi-head Latent Attention 机制压缩 Key-Value 缓存来提升推理效率和吞吐量。\n该模型在改进的视觉语言数据集上训练，展示了卓越性能，包括视觉问答、光学字符识别、文档/表格/图表理解和视觉 grounding 等多模态任务。\nDeepSeek-VL2 系列包括 DeepSeek-VL2-Tiny (1.0B 参数)、DeepSeek-VL2-Small (2.8B 参数)和 DeepSeek-VL2 (4.5B 参数)，在类似或更少激活参数下达到竞争性或最先进水平，并公开代码和预训练模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10302v1",
      "published_date": "2024-12-13 17:37:48 UTC",
      "updated_date": "2024-12-13 17:37:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:11:10.061019"
    },
    {
      "arxiv_id": "2412.10291v1",
      "title": "Still \"Talking About Large Language Models\": Some Clarifications",
      "title_zh": "翻译失败",
      "authors": [
        "Murray Shanahan"
      ],
      "abstract": "My paper \"Talking About Large Language Models\" has more than once been\ninterpreted as advocating a reductionist stance towards large language models.\nBut the paper was not intended that way, and I do not endorse such positions.\nThis short note situates the paper in the context of a larger philosophical\nproject that is concerned with the (mis)use of words rather than metaphysics,\nin the spirit of Wittgenstein's later writing.",
      "tldr_zh": "这篇论文是对作者先前作品《Talking About Large Language Models》的澄清，作者指出该论文被误解为支持还原论立场，但实际上并非如此。作者强调，该论文是其更大哲学项目的一部分，主要关注词汇的（误）使用问题，而非形而上学讨论。论文灵感来源于维特根斯坦后期著作，旨在纠正对语言模型的解读误区。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10291v1",
      "published_date": "2024-12-13 17:21:29 UTC",
      "updated_date": "2024-12-13 17:21:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:11:19.199562"
    },
    {
      "arxiv_id": "2412.10278v1",
      "title": "Envisioning National Resources for Artificial Intelligence Research: NSF Workshop Report",
      "title_zh": "翻译失败",
      "authors": [
        "Shantenu Jha",
        "Yolanda Gil"
      ],
      "abstract": "This is a report of an NSF workshop titled \"Envisioning National Resources\nfor Artificial Intelligence Research\" held in Alexandria, Virginia, in May\n2024. The workshop aimed to identify initial challenges and opportunities for\nnational resources for AI research (e.g., compute, data, models, etc.) and to\nfacilitate planning for the envisioned National AI Research Resource.\nParticipants included AI and cyberinfrastructure (CI) experts. The report\noutlines significant findings and identifies needs and recommendations from the\nworkshop.",
      "tldr_zh": "这个报告是 NSF 举办的研讨会“Envisioning National Resources for Artificial Intelligence Research”的总结，会议于 2024 年 5 月在弗吉尼亚州亚历山大市举行。研讨会旨在识别 AI 研究国家资源（如 compute、data 和 models 等）的初始挑战和机会，并为国家 AI Research Resource 的规划提供基础。参与者包括 AI 和 cyberinfrastructure 专家，报告概述了关键发现、需求和推荐，以推进 AI 领域的国家资源发展。",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10278v1",
      "published_date": "2024-12-13 17:00:31 UTC",
      "updated_date": "2024-12-13 17:00:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:11:32.171060"
    },
    {
      "arxiv_id": "2412.10272v1",
      "title": "Trustworthy and Explainable Decision-Making for Workforce allocation",
      "title_zh": "可靠且可解释的劳动力分配决策系统",
      "authors": [
        "Guillaume Povéda",
        "Ryma Boumazouza",
        "Andreas Strahl",
        "Mark Hall",
        "Santiago Quintana-Amate",
        "Nahum Alvarez",
        "Ignace Bleukx",
        "Dimos Tsouros",
        "Hélène Verhaeghe",
        "Tias Guns"
      ],
      "abstract": "In industrial contexts, effective workforce allocation is crucial for\noperational efficiency. This paper presents an ongoing project focused on\ndeveloping a decision-making tool designed for workforce allocation,\nemphasising the explainability to enhance its trustworthiness. Our objective is\nto create a system that not only optimises the allocation of teams to scheduled\ntasks but also provides clear, understandable explanations for its decisions,\nparticularly in cases where the problem is infeasible. By incorporating\nhuman-in-the-loop mechanisms, the tool aims to enhance user trust and\nfacilitate interactive conflict resolution. We implemented our approach on a\nprototype tool/digital demonstrator intended to be evaluated on a real\nindustrial scenario both in terms of performance and user acceptability.",
      "tldr_zh": "这篇论文提出了一种可信且可解释的决策工具，用于优化劳动力分配（workforce allocation），以提升工业操作效率。工具的核心方法包括提供清晰的决策解释，特别是针对不可行问题，并整合 human-in-the-loop 机制，以增强用户信任和交互式冲突解决。通过实现一个原型，该工具将在真实工业场景中评估其性能和用户接受度，为可信决策系统提供实际应用基础。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for presentation at PTHG-24: The Seventh Workshop on\n  Progress Towards the Holy Grail, part of the 30th International Conference on\n  Principles and Practice of Constraint Programming. For more details, visit\n  the workshop webpage:\n  https://freuder.wordpress.com/progress-towards-the-holy-grail-workshops/pthg-24-the-seventh-workshop-on-progress-towards-the-holy-grail/",
      "pdf_url": "http://arxiv.org/pdf/2412.10272v1",
      "published_date": "2024-12-13 16:46:13 UTC",
      "updated_date": "2024-12-13 16:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:11:44.758154"
    },
    {
      "arxiv_id": "2412.10270v1",
      "title": "Cultural Evolution of Cooperation among LLM Agents",
      "title_zh": "LLM 代理的合作文化进化",
      "authors": [
        "Aron Vallinder",
        "Edward Hughes"
      ],
      "abstract": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 代理在多代迭代互动中的合作演化，焦点在于代理是否能在存在背叛激励的情况下发展互惠的社会规范，特别是通过间接互惠 (indirect reciprocity) 在迭代捐赠者游戏 (Donor Game) 中的表现。研究让代理观察同伴行为，并比较了不同基础模型的表现，结果显示 Claude 3.5 Sonnet 代理实现了最高的平均分数，其次是 Gemini 1.5 Flash 和 GPT-4o。Claude 3.5 Sonnet 能利用代价性惩罚机制进一步提升分数，而其他模型未能有效运用此机制。研究还观察到行为对随机种子 (random seeds) 的敏感依赖，并建议此评估方法可作为新的 LLM 基准，评估代理部署对社会合作基础设施的影响。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "15 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10270v1",
      "published_date": "2024-12-13 16:45:49 UTC",
      "updated_date": "2024-12-13 16:45:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:11:57.623478"
    },
    {
      "arxiv_id": "2412.10267v1",
      "title": "Does Multiple Choice Have a Future in the Age of Generative AI? A Posttest-only RCT",
      "title_zh": "翻译失败",
      "authors": [
        "Danielle R. Thomas",
        "Conrad Borchers",
        "Sanjit Kakarla",
        "Jionghao Lin",
        "Shambhavi Bhushan",
        "Boyuan Guo",
        "Erin Gatz",
        "Kenneth R. Koedinger"
      ],
      "abstract": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility.",
      "tldr_zh": "这篇论文通过一个后测-only RCT设计，评估了多选题(MCQs)与开放式问题在学习中的有效性，涉及234名导师的790个课程完成，比较了仅MCQs、仅开放式问题和两者组合的三种条件。结果显示，三种条件在学习效果上无显著差异，但MCQs条件完成时间更短，证明了MCQs在有限时间内更高效。论文还使用GPT-4o和GPT-4-turbo自动评分开放式问题，发现这些LLMs适合低风险评估，并贡献了数据集、人类注释标准和LLM提示以提升透明度和可重复性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.10267v1",
      "published_date": "2024-12-13 16:37:20 UTC",
      "updated_date": "2024-12-13 16:37:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:12:09.405203"
    },
    {
      "arxiv_id": "2412.10489v2",
      "title": "CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With Multimodal Information",
      "title_zh": "CognitionCapturer：利用多模态",
      "authors": [
        "Kaifan Zhang",
        "Lihuo He",
        "Xin Jiang",
        "Wen Lu",
        "Di Wang",
        "Xinbo Gao"
      ],
      "abstract": "Electroencephalogram (EEG) signals have attracted significant attention from\nresearchers due to their non-invasive nature and high temporal sensitivity in\ndecoding visual stimuli. However, most recent studies have focused solely on\nthe relationship between EEG and image data pairs, neglecting the valuable\n``beyond-image-modality\" information embedded in EEG signals. This results in\nthe loss of critical multimodal information in EEG. To address this limitation,\nwe propose CognitionCapturer, a unified framework that fully leverages\nmultimodal data to represent EEG signals. Specifically, CognitionCapturer\ntrains Modality Expert Encoders for each modality to extract cross-modal\ninformation from the EEG modality. Then, it introduces a diffusion prior to map\nthe EEG embedding space to the CLIP embedding space, followed by using a\npretrained generative model, the proposed framework can reconstruct visual\nstimuli with high semantic and structural fidelity. Notably, the framework does\nnot require any fine-tuning of the generative models and can be extended to\nincorporate more modalities. Through extensive experiments, we demonstrate that\nCognitionCapturer outperforms state-of-the-art methods both qualitatively and\nquantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.",
      "tldr_zh": "该研究指出，现有的 EEG 信号解码方法仅关注 EEG 和图像数据对，忽略了 EEG 中的“beyond-image-modality”多模态信息，导致关键信息的丢失。为解决此问题，提出 CognitionCapturer 框架，通过训练 Modality Expert Encoders 提取跨模态信息，并使用 diffusion prior 将 EEG 嵌入空间映射到 CLIP 嵌入空间，从而利用预训练生成模型重建视觉刺激，确保高语义和结构保真。框架无需微调生成模型，并可扩展到更多模态；实验结果显示，CognitionCapturer 在定性和定量上优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10489v2",
      "published_date": "2024-12-13 16:27:54 UTC",
      "updated_date": "2024-12-24 13:03:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:12:20.195934"
    },
    {
      "arxiv_id": "2412.10257v2",
      "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Harry J. Davies",
        "Giorgos Iacovides",
        "Danilo P. Mandic"
      ],
      "abstract": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
      "tldr_zh": "本文提出 TARS（Targeted Angular Reversal of Weights）方法，用于从大型语言模型（LLMs）中针对性移除敏感知识，如生物安全信息或版权内容，同时确保移除在多语言和所有提示方向上生效，且不影响模型整体性能。TARS 通过聚合概念在内部表示空间的向量、添加噪声进行细化，并替换与目标向量余弦相似度最高的权重向量，从而限制知识传播。实验在 Llama 3.1 8B 模型上显示，仅需一次编辑即可将目标概念触发概率降至 0.00，且移除知识双向且跨语言有效，而模型在维基百科文本上的 KL divergence 中位数仅为 0.0015，证明其高效性和模块性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 5 figures, 1 table. Fixing typo with the final weight\n  editing equation",
      "pdf_url": "http://arxiv.org/pdf/2412.10257v2",
      "published_date": "2024-12-13 16:26:34 UTC",
      "updated_date": "2024-12-16 14:54:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:12:34.097679"
    },
    {
      "arxiv_id": "2412.10255v4",
      "title": "AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era",
      "title_zh": "AniSora：在Sora时代探索动画视频生成的前沿",
      "authors": [
        "Yudong Jiang",
        "Baohan Xu",
        "Siqian Yang",
        "Mingyu Yin",
        "Jing Liu",
        "Chao Xu",
        "Siqi Wang",
        "Yidi Wu",
        "Bingwen Zhu",
        "Xinwen Zhang",
        "Xingyu Zheng",
        "Jixuan Xu",
        "Yue Zhang",
        "Jinlong Hou",
        "Huyang Sun"
      ],
      "abstract": "Animation has gained significant interest in the recent film and TV industry.\nDespite the success of advanced video generation models like Sora, Kling, and\nCogVideoX in generating natural videos, they lack the same effectiveness in\nhandling animation videos. Evaluating animation video generation is also a\ngreat challenge due to its unique artist styles, violating the laws of physics\nand exaggerated motions. In this paper, we present a comprehensive system,\nAniSora, designed for animation video generation, which includes a data\nprocessing pipeline, a controllable generation model, and an evaluation\nbenchmark. Supported by the data processing pipeline with over 10M high-quality\ndata, the generation model incorporates a spatiotemporal mask module to\nfacilitate key animation production functions such as image-to-video\ngeneration, frame interpolation, and localized image-guided animation. We also\ncollect an evaluation benchmark of 948 various animation videos, with\nspecifically developed metrics for animation video generation. Our entire\nproject is publicly available on\nhttps://github.com/bilibili/Index-anisora/tree/main.",
      "tldr_zh": "本研究探讨了在Sora时代动画视频生成的挑战，指出现有模型如Sora、Kling和CogVideoX在处理动画视频时因其独特艺术风格、违反物理定律和夸张动作而效果不佳。论文提出AniSora系统，包括一个处理超过10M高质量数据的管道、一个整合spatiotemporal mask module的可控生成模型（支持image-to-video generation、frame interpolation和localized image-guided animation），以及一个包含948个动画视频的评估基准。实验结果通过专门开发的指标验证了系统的有效性，并将整个项目开源在GitHub上。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10255v4",
      "published_date": "2024-12-13 16:24:58 UTC",
      "updated_date": "2025-05-13 16:20:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:12:44.358252"
    },
    {
      "arxiv_id": "2412.10237v1",
      "title": "Physics Instrument Design with Reinforcement Learning",
      "title_zh": "基于强化学习的物理仪器设计",
      "authors": [
        "Shah Rukh Qasim",
        "Patrick Owen",
        "Nicola Serra"
      ],
      "abstract": "We present a case for the use of Reinforcement Learning (RL) for the design\nof physics instrument as an alternative to gradient-based\ninstrument-optimization methods. It's applicability is demonstrated using two\nempirical studies. One is longitudinal segmentation of calorimeters and the\nsecond is both transverse segmentation as well longitudinal placement of\ntrackers in a spectrometer. Based on these experiments, we propose an\nalternative approach that offers unique advantages over differentiable\nprogramming and surrogate-based differentiable design optimization methods.\nFirst, Reinforcement Learning (RL) algorithms possess inherent exploratory\ncapabilities, which help mitigate the risk of convergence to local optima.\nSecond, this approach eliminates the necessity of constraining the design to a\npredefined detector model with fixed parameters. Instead, it allows for the\nflexible placement of a variable number of detector components and facilitates\ndiscrete decision-making. We then discuss the road map of how this idea can be\nextended into designing very complex instruments. The presented study sets the\nstage for a novel framework in physics instrument design, offering a scalable\nand efficient framework that can be pivotal for future projects such as the\nFuture Circular Collider (FCC), where most optimized detectors are essential\nfor exploring physics at unprecedented energy scales.",
      "tldr_zh": "本文提出使用 Reinforcement Learning (RL) 作为替代梯度-based 方法来设计物理仪器，通过两个实证研究（热量计的纵向分割和谱仪中跟踪器的横向分割与纵向放置）展示了其适用性。相比传统方法，RL 具备内在探索能力，能避免局部最优，并允许灵活放置可变数量的检测器组件，支持离散决策。最终，该框架为复杂仪器设计提供可扩展方案，有望应用于未来项目如 Future Circular Collider (FCC)，以优化高能量物理探索。",
      "categories": [
        "physics.ins-det",
        "cs.AI",
        "hep-ex"
      ],
      "primary_category": "physics.ins-det",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10237v1",
      "published_date": "2024-12-13 16:08:28 UTC",
      "updated_date": "2024-12-13 16:08:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:12:56.348248"
    },
    {
      "arxiv_id": "2412.10220v1",
      "title": "How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives",
      "title_zh": "翻译失败",
      "authors": [
        "Timour Ichmoukhamedov",
        "James Hinns",
        "David Martens"
      ],
      "abstract": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives.",
      "tldr_zh": "该论文探讨了评估大型语言模型(LLM)生成的解释性AI(XAI)叙事质量的量化指标，旨在避免依赖人类偏好研究。研究提出一个框架和多种自动化指标，用于评估LLM将量化解释（如SHAP）转化为用户友好叙事的性能。作者通过应用该框架比较不同LLM在各种数据集和提示类型上的表现，成功识别了LLM在XAI叙事中存在的幻觉(hallucinations)新挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10220v1",
      "published_date": "2024-12-13 15:45:45 UTC",
      "updated_date": "2024-12-13 15:45:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:13:08.219098"
    },
    {
      "arxiv_id": "2412.10209v2",
      "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
      "title_zh": "翻译失败",
      "authors": [
        "Jiapeng Tang",
        "Davide Davoli",
        "Tobias Kirschstein",
        "Liam Schoneveld",
        "Matthias Niessner"
      ],
      "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve facial identity and appearance details. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling priors to refine the denoised latent before decoding it into an\nimage. We evaluate our method on the NeRSemble dataset, showing that GAF\noutperforms previous state-of-the-art methods in novel view synthesis.\nFurthermore, we demonstrate higher-fidelity avatar reconstructions from\nmonocular videos captured on commodity devices.",
      "tldr_zh": "该研究提出GAF方法，通过多视图扩散模型从单目视频重建可动画3D Gaussian Avatar，解决有限观察导致的未观察区域问题和渲染伪影。方法利用FLAME-based头部重建生成的法线图进行精确视角控制，并结合VAE特征对扩散模型进行条件化，以保留面部身份和外观细节；同时，通过迭代去噪图像作为伪真实值蒸馏扩散先验，并应用潜在上采样来提升图像逼真度。在NeRSemble数据集上，GAF在新型视图合成中超越现有最先进方法，并实现了从商品设备捕获视频的高保真头像重建。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Paper Video: https://youtu.be/QuIYTljvhyg Project Page:\n  https://tangjiapeng.github.io/projects/GAF",
      "pdf_url": "http://arxiv.org/pdf/2412.10209v2",
      "published_date": "2024-12-13 15:31:22 UTC",
      "updated_date": "2025-04-14 16:02:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:13:21.619676"
    },
    {
      "arxiv_id": "2412.10488v3",
      "title": "SVGBuilder: Component-Based Colored SVG Generation with Text-Guided Autoregressive Transformers",
      "title_zh": "SVGBuilder：基于组件的彩色 SVG 生成，使用文本引导的自回归 Transformer",
      "authors": [
        "Zehao Chen",
        "Rong Pan"
      ],
      "abstract": "Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.",
      "tldr_zh": "这篇论文介绍了 SVGBuilder，一种基于组件的自回归 Transformer 模型，用于从文本输入生成高质量彩色 SVG。模型通过模仿人类设计师的组件-based 方法，显著降低了计算开销，比优化-based 方式快达 604 倍。作者还发布了 ColorSVG-100K 数据集，这是第一个包含 10 万彩色 SVG 的规模化数据集，填补了颜色信息缺失的空白。在评估中，SVGBuilder 展现出优于现有模型的性能，尤其在复杂 SVG 生成的效率和质量方面。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project: https://svgbuilder.github.io",
      "pdf_url": "http://arxiv.org/pdf/2412.10488v3",
      "published_date": "2024-12-13 15:24:11 UTC",
      "updated_date": "2025-03-12 14:34:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:13:32.652758"
    },
    {
      "arxiv_id": "2412.10198v2",
      "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection",
      "title_zh": "从盟友到对手：通过对抗性注入操纵 LLM ",
      "authors": [
        "Haowei Wang",
        "Rupeng Zhang",
        "Junjie Wang",
        "Mingyang Li",
        "Yuekai Huang",
        "Dandan Wang",
        "Qing Wang"
      ],
      "abstract": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
      "tldr_zh": "该论文探讨了大型语言模型（LLM）工具调用系统的安全漏洞，通过提出 ToolCommander 框架来演示对抗性注入攻击。框架采用两阶段策略：首先注入恶意工具以收集用户查询，其次基于窃取的信息动态更新工具，从而实现隐私窃取、拒绝服务攻击和操纵商业竞争等攻击。实验结果显示，隐私窃取的攻击成功率（ASR）达到 91.67%，而拒绝服务和非计划工具调用在某些情况下可达 100%，突显了这些漏洞的严重性，并呼吁开发稳健的防御措施以保护 LLM 系统。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10198v2",
      "published_date": "2024-12-13 15:15:24 UTC",
      "updated_date": "2025-02-07 13:26:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:13:45.241052"
    },
    {
      "arxiv_id": "2412.10186v1",
      "title": "BiCert: A Bilinear Mixed Integer Programming Formulation for Precise Certified Bounds Against Data Poisoning Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Lorenz",
        "Marta Kwiatkowska",
        "Mario Fritz"
      ],
      "abstract": "Data poisoning attacks pose one of the biggest threats to modern AI systems,\nnecessitating robust defenses. While extensive efforts have been made to\ndevelop empirical defenses, attackers continue to evolve, creating\nsophisticated methods to circumvent these measures. To address this, we must\nmove beyond empirical defenses and establish provable certification methods\nthat guarantee robustness. This paper introduces a novel certification\napproach, BiCert, using Bilinear Mixed Integer Programming (BMIP) to compute\nsound deterministic bounds that provide such provable robustness. Using BMIP,\nwe compute the reachable set of parameters that could result from training with\npotentially manipulated data. A key element to make this computation feasible\nis to relax the reachable parameter set to a convex set between training\niterations. At test time, this parameter set allows us to predict all possible\noutcomes, guaranteeing robustness. BiCert is more precise than previous\nmethods, which rely solely on interval and polyhedral bounds. Crucially, our\napproach overcomes the fundamental limitation of prior approaches where\nparameter bounds could only grow, often uncontrollably. We show that BiCert's\ntighter bounds eliminate a key source of divergence issues, resulting in more\nstable training and higher certified accuracy.",
      "tldr_zh": "本文提出 BiCert，一种基于 Bilinear Mixed Integer Programming (BMIP) 的新方法，用于计算数据投毒攻击（Data Poisoning Attacks）下的精确认证边界，提供可证明的鲁棒性。该方法通过将训练迭代间的参数可达集放松为凸集，使计算变得可行，并在测试时预测所有可能结果，从而克服了现有方法中参数边界不断增长的问题。与基于 interval 和 polyhedral bounds 的先前方法相比，BiCert 实现了更精确的边界，显著提升了训练稳定性和认证准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10186v1",
      "published_date": "2024-12-13 14:56:39 UTC",
      "updated_date": "2024-12-13 14:56:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:13:56.684674"
    },
    {
      "arxiv_id": "2412.10185v1",
      "title": "Solving Robust Markov Decision Processes: Generic, Reliable, Efficient",
      "title_zh": "鲁棒 Markov 决策过程的求解：通用、可靠、高效",
      "authors": [
        "Tobias Meggendorfer",
        "Maximilian Weininger",
        "Patrick Wienhöft"
      ],
      "abstract": "Markov decision processes (MDP) are a well-established model for sequential\ndecision-making in the presence of probabilities. In robust MDP (RMDP), every\naction is associated with an uncertainty set of probability distributions,\nmodelling that transition probabilities are not known precisely. Based on the\nknown theoretical connection to stochastic games, we provide a framework for\nsolving RMDPs that is generic, reliable, and efficient. It is *generic* both\nwith respect to the model, allowing for a wide range of uncertainty sets,\nincluding but not limited to intervals, $L^1$- or $L^2$-balls, and polytopes;\nand with respect to the objective, including long-run average reward,\nundiscounted total reward, and stochastic shortest path. It is *reliable*, as\nour approach not only converges in the limit, but provides precision guarantees\nat any time during the computation. It is *efficient* because -- in contrast to\nstate-of-the-art approaches -- it avoids explicitly constructing the underlying\nstochastic game. Consequently, our prototype implementation outperforms\nexisting tools by several orders of magnitude and can solve RMDPs with a\nmillion states in under a minute.",
      "tldr_zh": "本论文提出了一种通用的、可靠的和高效的框架，用于解决Robust Markov Decision Processes (RMDP)，以处理不确定性概率分布的问题。该框架支持多种不确定性集（如intervals、L^1- or L^2-balls和polytopes）以及不同目标（如long-run average reward、undiscounted total reward和stochastic shortest path），并基于与stochastic games的理论连接进行优化。相比现有方法，该框架避免显式构建底层stochastic game，从而在计算过程中提供精度保证，并将性能提升几个数量级，能在不到一分钟内解决百万状态的RMDP。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at AAAI'25. Extended version with full\n  appendix, 26 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.10185v1",
      "published_date": "2024-12-13 14:55:48 UTC",
      "updated_date": "2024-12-13 14:55:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:14:09.388136"
    },
    {
      "arxiv_id": "2412.10182v1",
      "title": "Multi-Head Encoding for Extreme Label Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Daojun Liang",
        "Haixia Zhang",
        "Dongfeng Yuan",
        "Minggao Zhang"
      ],
      "abstract": "The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.",
      "tldr_zh": "本研究针对 eXtreme Label Classification (XLC) 中类别数量庞大导致的 Classifier Computational Overload Problem (CCOP)，提出了一种 Multi-Head Encoding (MHE) 机制，使用多头分类器将极端标签分解为多个短局部标签，每个头分别训练和预测，从而显著降低计算负载。MHE 在训练过程中通过标签分解实现高效处理，在测试时直接从各头的局部预测计算最终标签；根据不同任务特性（如单标签、多标签和预训练），进一步开发了 Multi-Head Product、Multi-Head Cascade 和 Multi-Head Sampling 三种实现方式。理论上，该方法通过推广低秩逼近问题从 Frobenius-norm 到 Cross-Entropy，证明其性能可与传统分类器相当。实验结果显示，MHE 在 XLC 任务上实现了最先进性能，同时极大简化了训练和推理过程，并开源了代码。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 12 figs, Published in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI) 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.10182v1",
      "published_date": "2024-12-13 14:53:47 UTC",
      "updated_date": "2024-12-13 14:53:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:14:20.947965"
    },
    {
      "arxiv_id": "2412.10178v2",
      "title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models",
      "title_zh": "SwiftTry：基于扩散模型的快速一致视频虚拟试穿",
      "authors": [
        "Hung Nguyen",
        "Quang Qui-Vinh Nguyen",
        "Khoi Nguyen",
        "Rang Nguyen"
      ],
      "abstract": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.",
      "tldr_zh": "这篇论文提出了SwiftTry，一种基于Diffusion Models的快速一致视频虚拟试穿方法，旨在将输入视频中的人物与新衣服合成新视频，同时保持时空一致性。作者将任务重新定义为条件视频修复问题，通过添加时间注意力层和引入ShiftCaching技术来减少冗余计算并提升时间连贯性。此外，他们发布了TikTokDress数据集，该数据集包含更复杂的背景、挑战性动作和高分辨率；实验结果显示，SwiftTry在视频一致性和推理速度上优于现有基线。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10178v2",
      "published_date": "2024-12-13 14:50:26 UTC",
      "updated_date": "2024-12-18 18:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:14:33.470689"
    },
    {
      "arxiv_id": "2412.10163v1",
      "title": "Scaling Combinatorial Optimization Neural Improvement Heuristics with Online Search and Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Federico Julian Camerota Verdù",
        "Lorenzo Castelli",
        "Luca Bortolussi"
      ],
      "abstract": "We introduce Limited Rollout Beam Search (LRBS), a beam search strategy for\ndeep reinforcement learning (DRL) based combinatorial optimization improvement\nheuristics. Utilizing pre-trained models on the Euclidean Traveling Salesperson\nProblem, LRBS significantly enhances both in-distribution performance and\ngeneralization to larger problem instances, achieving optimality gaps that\noutperform existing improvement heuristics and narrowing the gap with\nstate-of-the-art constructive methods. We also extend our analysis to two\npickup and delivery TSP variants to validate our results. Finally, we employ\nour search strategy for offline and online adaptation of the pre-trained\nimprovement policy, leading to improved search performance and surpassing\nrecent adaptive methods for constructive heuristics.",
      "tldr_zh": "本研究引入了 Limited Rollout Beam Search (LRBS)，一种 beam search 策略，用于提升深度强化学习 (DRL) 基于的组合优化改进启发式算法。利用在 Euclidean Traveling Salesperson Problem (TSP) 上预训练的模型，LRBS 显著提高了分布内性能和对更大问题实例的泛化能力，并在最优性差距上优于现有改进启发式，同时缩小了与最先进构造方法的差距。该方法扩展到两个 pickup and delivery TSP 变体进行验证，并通过离线和在线适应策略，进一步提升搜索性能并超越了最近的自适应方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10163v1",
      "published_date": "2024-12-13 14:25:27 UTC",
      "updated_date": "2024-12-13 14:25:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:14:45.293218"
    },
    {
      "arxiv_id": "2412.15244v1",
      "title": "MPPO: Multi Pair-wise Preference Optimization for LLMs with Arbitrary Negative Samples",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Xie",
        "Fangzhi Zhu",
        "Jiahui Wang",
        "Lulu Wen",
        "Wei Dai",
        "Xiaowei Chen",
        "Junxiong Zhu",
        "Kai Zhou",
        "Bo Zheng"
      ],
      "abstract": "Aligning Large Language Models (LLMs) with human feedback is crucial for\ntheir development. Existing preference optimization methods such as DPO and\nKTO, while improved based on Reinforcement Learning from Human Feedback (RLHF),\nare inherently derived from PPO, requiring a reference model that adds GPU\nmemory resources and relies heavily on abundant preference data. Meanwhile,\ncurrent preference optimization research mainly targets single-question\nscenarios with two replies, neglecting optimization with multiple replies,\nwhich leads to a waste of data in the application. This study introduces the\nMPPO algorithm, which leverages the average likelihood of model responses to\nfit the reward function and maximizes the utilization of preference data.\nThrough a comparison of Point-wise, Pair-wise, and List-wise implementations,\nwe found that the Pair-wise approach achieves the best performance,\nsignificantly enhancing the quality of model responses. Experimental results\ndemonstrate MPPO's outstanding performance across various benchmarks. On\nMT-Bench, MPPO outperforms DPO, ORPO, and SimPO. Notably, on Arena-Hard, MPPO\nsurpasses DPO and ORPO by substantial margins. These achievements underscore\nthe remarkable advantages of MPPO in preference optimization tasks.",
      "tldr_zh": "这篇论文提出了 MPPO 算法，用于优化大型语言模型 (LLMs) 的偏好对齐，解决了现有方法如 DPO 和 KTO 依赖参考模型和高资源消耗的问题，同时支持任意负样本并最大化偏好数据的利用。MPPO 通过利用模型响应的平均似然拟合奖励函数，并通过比较 Point-wise、Pair-wise 和 List-wise 实现，发现 Pair-wise 方法在提升响应质量方面表现最佳。实验结果显示，MPPO 在 MT-Bench 上优于 DPO、ORPO 和 SimPO，在 Arena-Hard 上大幅领先，突显了其在偏好优化任务中的显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by COLING2025",
      "pdf_url": "http://arxiv.org/pdf/2412.15244v1",
      "published_date": "2024-12-13 14:18:58 UTC",
      "updated_date": "2024-12-13 14:18:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:14:57.821593"
    },
    {
      "arxiv_id": "2501.10371v1",
      "title": "What we learned while automating bias detection in AI hiring systems for compliance with NYC Local Law 144",
      "title_zh": "我们在自动化 AI 招聘系统偏见检测以遵守 NYC Local Law 144 的过程中学到了什么",
      "authors": [
        "Gemma Galdon Clavell",
        "Rubén González-Sendino"
      ],
      "abstract": "Since July 5, 2023, New York City's Local Law 144 requires employers to\nconduct independent bias audits for any automated employment decision tools\n(AEDTs) used in hiring processes. The law outlines a minimum set of bias tests\nthat AI developers and implementers must perform to ensure compliance. Over the\npast few months, we have collected and analyzed audits conducted under this\nlaw, identified best practices, and developed a software tool to streamline\nemployer compliance. Our tool, ITACA_144, tailors our broader bias auditing\nframework to meet the specific requirements of Local Law 144. While automating\nthese legal mandates, we identified several critical challenges that merit\nattention to ensure AI bias regulations and audit methodologies are both\neffective and practical. This document presents the insights gained from\nautomating compliance with NYC Local Law 144. It aims to support other cities\nand states in crafting similar legislation while addressing the limitations of\nthe NYC framework. The discussion focuses on key areas including data\nrequirements, demographic inclusiveness, impact ratios, effective bias,\nmetrics, and data reliability.",
      "tldr_zh": "纽约市本地法144（NYC Local Law 144）自2023年7月5日起要求雇主对AI招聘系统中的自动化就业决策工具（AEDTs）进行独立偏见审计，我们在自动化这一过程时收集并分析了相关审计数据。针对该法律，我们开发了ITACA_144工具，该工具基于更广泛的偏见审计框架，定制化处理合规要求，包括数据需求和人口统计包容性。研究揭示了关键挑战，如影响比率（impact ratios）、有效偏见（effective bias）和数据可靠性问题，并提供见解，帮助其他城市和州制定更实用有效的AI偏见法规。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10371v1",
      "published_date": "2024-12-13 14:14:26 UTC",
      "updated_date": "2024-12-13 14:14:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:15:09.232852"
    },
    {
      "arxiv_id": "2412.10155v1",
      "title": "WordVIS: A Color Worth A Thousand Words",
      "title_zh": "WordVIS：",
      "authors": [
        "Umar Khan",
        "Saifullah",
        "Stefan Agne",
        "Andreas Dengel",
        "Sheraz Ahmed"
      ],
      "abstract": "Document classification is considered a critical element in automated\ndocument processing systems. In recent years multi-modal approaches have become\nincreasingly popular for document classification. Despite their improvements,\nthese approaches are underutilized in the industry due to their requirement for\na tremendous volume of training data and extensive computational power. In this\npaper, we attempt to address these issues by embedding textual features\ndirectly into the visual space, allowing lightweight image-based classifiers to\nachieve state-of-the-art results using small-scale datasets in document\nclassification. To evaluate the efficacy of the visual features generated from\nour approach on limited data, we tested on the standard dataset Tobacco-3482.\nOur experiments show a tremendous improvement in image-based classifiers,\nachieving an improvement of 4.64% using ResNet50 with no document pre-training.\nIt also sets a new record for the best accuracy of the Tobacco-3482 dataset\nwith a score of 91.14% using the image-based DocXClassifier with no document\npre-training. The simplicity of the approach, its resource requirements, and\nsubsequent results provide a good prospect for its use in industrial use cases.",
      "tldr_zh": "这篇论文介绍了WordVIS方法，通过将文本特征直接嵌入视觉空间，使用轻量级图像分类器（如ResNet50）来提升文档分类性能，从而解决多模态方法对大量训练数据和计算资源的需求。论文的核心创新在于无需文档预训练，即可在小规模数据集上实现最先进的结果，并在Tobacco-3482数据集上测试中，将ResNet50的准确率提高了4.64%，并创下91.14%的最高记录。总体而言，该方法简单、资源高效，为工业文档处理提供了一个可行的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10155v1",
      "published_date": "2024-12-13 14:12:55 UTC",
      "updated_date": "2024-12-13 14:12:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:15:20.849722"
    },
    {
      "arxiv_id": "2412.10152v1",
      "title": "Direct Encoding of Declare Constraints in ASP",
      "title_zh": "Declare 约束在 ASP 中的直接编码",
      "authors": [
        "Francesco Chiariello",
        "Valeria Fionda",
        "Antonio Ielo",
        "Francesco Ricca"
      ],
      "abstract": "Answer Set Programming (ASP), a well-known declarative logic programming\nparadigm, has recently found practical application in Process Mining. In\nparticular, ASP has been used to model tasks involving declarative\nspecifications of business processes. In this area, Declare stands out as the\nmost widely adopted declarative process modeling language, offering a means to\nmodel processes through sets of constraints valid traces must satisfy, that can\nbe expressed in Linear Temporal Logic over Finite Traces (LTLf). Existing\nASP-based solutions encode Declare constraints by modeling the corresponding\nLTLf formula or its equivalent automaton which can be obtained using\nestablished techniques. In this paper, we introduce a novel encoding for\nDeclare constraints that directly models their semantics as ASP rules,\neliminating the need for intermediate representations. We assess the\neffectiveness of this novel approach on two Process Mining tasks by comparing\nit with alternative ASP encodings and a Python library for Declare. Under\nconsideration in Theory and Practice of Logic Programming (TPLP).",
      "tldr_zh": "该论文探讨了在Answer Set Programming (ASP)中直接编码Declare约束的方法，以提升Process Mining任务的效率。传统方法通过建模对应的Linear Temporal Logic over Finite Traces (LTLf)公式或其等价自动机来处理Declare约束，而新方法直接将Declare约束的语义转化为ASP规则，避免了中间表示的复杂性。在两个Process Mining任务上，该方法与现有ASP编码和Python库相比显示出更好的有效性，目前正在Theory and Practice of Logic Programming (TPLP)审阅中。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)",
      "pdf_url": "http://arxiv.org/pdf/2412.10152v1",
      "published_date": "2024-12-13 14:11:33 UTC",
      "updated_date": "2024-12-13 14:11:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:15:32.082737"
    },
    {
      "arxiv_id": "2412.10151v1",
      "title": "VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval Augmented Generation",
      "title_zh": "VLR-Bench：多语言视觉-语言检索增强生成基准数据集",
      "authors": [
        "Hyeonseok Lim",
        "Dongjae Shin",
        "Seohyun Song",
        "Inho Won",
        "Minjun Kim",
        "Junghun Yuk",
        "Haneol Jang",
        "KyungTae Lim"
      ],
      "abstract": "We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.",
      "tldr_zh": "本研究提出了 VLR-Bench，一种多语言基准数据集，用于评估视觉语言模型 (VLMs) 在检索增强生成 (RAG) 下的视觉问答 (VQA) 性能。该数据集包含五个输入段落，旨在测试模型识别和利用有用信息的能力，并构建了 32,000 个自动生成的指令遵循示例 (VLR-IF)，以帮助 VLMs 学习基于输入段落生成适当答案。实验使用 Llava-Llama-3 模型验证了基准的有效性，并公开提供了数据集。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "The 31st International Conference on Computational Linguistics\n  (COLING 2025), 19 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.10151v1",
      "published_date": "2024-12-13 14:11:26 UTC",
      "updated_date": "2024-12-13 14:11:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:15:45.565667"
    },
    {
      "arxiv_id": "2412.10138v1",
      "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Qin",
        "Chao Chen",
        "Zhihang Fu",
        "Ze Chen",
        "Dezhong Peng",
        "Peng Hu",
        "Jieping Ye"
      ],
      "abstract": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance.",
      "tldr_zh": "该研究提出 ROUTE 方法，通过鲁棒的多任务微调和协作策略，提升开源 LLMs 在 Text-to-SQL 任务中的综合能力，以解决依赖封闭源模型的局限性。ROUTE 包括多任务监督微调 (SFT)，利用合成数据进行 SQL 生成、schema linking、noise correction 和 continuation writing 等任务，从而增强模型对 SQL 语法的理解和查询质量；此外，引入 Multitask Collaboration Prompting (MCP) 策略，通过跨任务协作减少幻觉问题。实验在八个开源 LLMs 和五个基准上进行，结果显示 ROUTE 优于现有方法，取得了领先性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10138v1",
      "published_date": "2024-12-13 13:41:18 UTC",
      "updated_date": "2024-12-13 13:41:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:15:56.572792"
    },
    {
      "arxiv_id": "2412.10136v2",
      "title": "Can LLMs Convert Graphs to Text-Attributed Graphs?",
      "title_zh": "翻译失败",
      "authors": [
        "Zehong Wang",
        "Sidney Liu",
        "Zheyuan Zhang",
        "Tianyi Ma",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Graphs are ubiquitous structures found in numerous real-world applications,\nsuch as drug discovery, recommender systems, and social network analysis. To\nmodel graph-structured data, graph neural networks (GNNs) have become a popular\ntool. However, existing GNN architectures encounter challenges in cross-graph\nlearning where multiple graphs have different feature spaces. To address this,\nrecent approaches introduce text-attributed graphs (TAGs), where each node is\nassociated with a textual description, which can be projected into a unified\nfeature space using textual encoders. While promising, this method relies\nheavily on the availability of text-attributed graph data, which is difficult\nto obtain in practice. To bridge this gap, we propose a novel method named\nTopology-Aware Node description Synthesis (TANS), leveraging large language\nmodels (LLMs) to convert existing graphs into text-attributed graphs. The key\nidea is to integrate topological information into LLMs to explain how graph\ntopology influences node semantics. We evaluate our TANS on text-rich,\ntext-limited, and text-free graphs, demonstrating its applicability. Notably,\non text-free graphs, our method significantly outperforms existing approaches\nthat manually design node features, showcasing the potential of LLMs for\npreprocessing graph-structured data in the absence of textual information. The\ncode and data are available at https://github.com/Zehong-Wang/TANS.",
      "tldr_zh": "该论文探讨了大型语言模型(LLMs)是否能将图结构转换为文本属性图(TAGs)，以解决图神经网络(GNNs)在跨图学习中特征空间差异的挑战。作者提出了一种新方法Topology-Aware Node description Synthesis (TANS)，通过整合图拓扑信息到LLMs中，生成节点的文本描述，从而桥接文本数据缺失的鸿沟。实验结果显示，TANS在文本丰富的、文本有限的和文本-free的图上均表现出色，尤其在文本-free图上显著优于手动设计节点特征的方法，展示了LLMs在预处理图结构数据方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by NAACL 25 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2412.10136v2",
      "published_date": "2024-12-13 13:32:59 UTC",
      "updated_date": "2025-02-07 01:29:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:16:09.627293"
    },
    {
      "arxiv_id": "2412.10133v2",
      "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects",
      "title_zh": "翻译失败",
      "authors": [
        "Islem Bouzenia",
        "Michael Pradel"
      ],
      "abstract": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that prepares scripts for building an\narbitrary project from source code and running its test cases. Inspired by the\nway a human developer would address this task, our approach is a large language\nmodel (LLM)-based agent that autonomously executes commands and interacts with\nthe host system. The agent uses meta-prompting to gather guidelines on the\nlatest technologies related to the given project, and it iteratively refines\nits process based on feedback from the previous steps. Our evaluation applies\nExecutionAgent to 50 open-source projects that use 14 different programming\nlanguages and many different build and testing tools. The approach successfully\nexecutes the test suites of 33/50 projects, while matching the test results of\nground truth test suite executions with a deviation of only 7.5%. These results\nimprove over the best previously available technique by 6.6x. The costs imposed\nby the approach are reasonable, with an execution time of 74 minutes and LLM\ncosts of USD 0.16, on average per project. We envision ExecutionAgent to serve\nas a valuable tool for developers, automated programming tools, and researchers\nthat need to execute tests across a wide variety of projects.",
      "tldr_zh": "这篇论文介绍了 ExecutionAgent，一种基于大型语言模型 (LLM) 的代理系统，旨在自动执行任意开源项目的测试套件，以解决不同编程语言、构建系统和测试框架的兼容性挑战。该代理模仿人类开发者，通过 meta-prompting 收集相关技术指南，并迭代精炼过程以自主执行命令和与主机系统交互。在评估中，ExecutionAgent 成功运行了 50 个项目中的 33 个，与基准测试结果偏差仅 7.5%，比现有最佳技术提高了 6.6 倍，平均执行时间为 74 分钟，LLM 成本仅 0.16 美元。该方法为开发者、自动化工具和研究人员提供了一个可靠的跨项目测试执行工具。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "PUBLISHED AT ISSTA 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.10133v2",
      "published_date": "2024-12-13 13:30:51 UTC",
      "updated_date": "2025-04-30 10:25:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:16:21.852899"
    },
    {
      "arxiv_id": "2501.10370v1",
      "title": "Harnessing Large Language Models for Mental Health: Opportunities, Challenges, and Ethical Considerations",
      "title_zh": "翻译失败",
      "authors": [
        "Hari Mohan Pandey"
      ],
      "abstract": "Large Language Models (LLMs) are transforming mental health care by enhancing\naccessibility, personalization, and efficiency in therapeutic interventions.\nThese AI-driven tools empower mental health professionals with real-time\nsupport, improved data integration, and the ability to encourage care-seeking\nbehaviors, particularly in underserved communities. By harnessing LLMs,\npractitioners can deliver more empathetic, tailored, and effective support,\naddressing longstanding gaps in mental health service provision. However, their\nimplementation comes with significant challenges and ethical concerns.\nPerformance limitations, data privacy risks, biased outputs, and the potential\nfor generating misleading information underscore the critical need for\nstringent ethical guidelines and robust evaluation mechanisms. The sensitive\nnature of mental health data further necessitates meticulous safeguards to\nprotect patient rights and ensure equitable access to AI-driven care.\nProponents argue that LLMs have the potential to democratize mental health\nresources, while critics warn of risks such as misuse and the diminishment of\nhuman connection in therapy. Achieving a balance between innovation and ethical\nresponsibility is imperative. This paper examines the transformative potential\nof LLMs in mental health care, highlights the associated technical and ethical\ncomplexities, and advocates for a collaborative, multidisciplinary approach to\nensure these advancements align with the goal of providing compassionate,\nequitable, and effective mental health support.",
      "tldr_zh": "本论文探讨了Large Language Models (LLMs)在心理健康护理中的应用潜力，包括提升可及性、个性化支持和效率，帮助专业人士提供实时援助并服务于弱势群体。然而，LLMs也面临挑战，如性能限制、数据隐私风险、偏见输出和误导信息，这些问题要求严格的伦理指南和评估机制来保护患者权利。论文主张通过多学科合作平衡创新与伦理责任，实现LLMs在心理健康领域的公正和有效应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.10370v1",
      "published_date": "2024-12-13 13:18:51 UTC",
      "updated_date": "2024-12-13 13:18:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:16:32.976976"
    },
    {
      "arxiv_id": "2412.10117v3",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.",
      "tldr_zh": "该论文介绍了 CosyVoice 2，一种基于 Large Language Models (LLMs) 的可扩展流式语音合成模型，旨在提升交互式语音应用的响应速度和质量。关键优化包括采用 finite-scalar quantization 提高语音标记的代码书利用、简化文本-语音 LM 架构以直接使用预训练 LLM 作为骨干，以及开发 chunk-aware causal flow matching 支持流式和非流式合成场景。通过在大规模多语言数据集上训练，CosyVoice 2 实现了与人类相当的自然度、最小响应延迟和几乎无损的合成质量。用户可通过提供的演示链接（如 https://funaudiollm.github.io/cosyvoice2）体验效果。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Tech report, work in progress",
      "pdf_url": "http://arxiv.org/pdf/2412.10117v3",
      "published_date": "2024-12-13 12:59:39 UTC",
      "updated_date": "2024-12-25 11:54:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:16:46.328375"
    },
    {
      "arxiv_id": "2412.10110v1",
      "title": "Label-template based Few-Shot Text Classification with Contrastive Learning",
      "title_zh": "基于标签模板的少样本文本分类与对比学习",
      "authors": [
        "Guanghua Hou",
        "Shuhui Cao",
        "Deqiang Ouyang",
        "Ning Wang"
      ],
      "abstract": "As an algorithmic framework for learning to learn, meta-learning provides a\npromising solution for few-shot text classification. However, most existing\nresearch fail to give enough attention to class labels. Traditional basic\nframework building meta-learner based on prototype networks heavily relies on\ninter-class variance, and it is easily influenced by noise. To address these\nlimitations, we proposes a simple and effective few-shot text classification\nframework. In particular, the corresponding label templates are embed into\ninput sentences to fully utilize the potential value of class labels, guiding\nthe pre-trained model to generate more discriminative text representations\nthrough the semantic information conveyed by labels. With the continuous\ninfluence of label semantics, supervised contrastive learning is utilized to\nmodel the interaction information between support samples and query samples.\nFurthermore, the averaging mechanism is replaced with an attention mechanism to\nhighlight vital semantic information. To verify the proposed scheme, four\ntypical datasets are employed to assess the performance of different methods.\nExperimental results demonstrate that our method achieves substantial\nperformance enhancements and outperforms existing state-of-the-art models on\nfew-shot text classification tasks.",
      "tldr_zh": "本文提出了一种基于标签模板的少样本文本分类（few-shot text classification）框架，通过嵌入标签模板来充分利用类标签的语义信息，帮助预训练模型生成更具区分性的文本表示。框架结合监督对比学习（supervised contrastive learning）来建模支持样本和查询样本之间的交互信息，并采用注意力机制（attention mechanism）替换平均机制，以突出关键语义。实验在四个典型数据集上验证，该方法显著提升了性能，超越了现有最先进模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10110v1",
      "published_date": "2024-12-13 12:51:50 UTC",
      "updated_date": "2024-12-13 12:51:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:16:57.533108"
    },
    {
      "arxiv_id": "2412.10107v1",
      "title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Asmaa Abdallah",
        "Abdullatif Albaseer",
        "Abdulkadir Celik",
        "Mohamed Abdallah",
        "Ahmed M. Eltawil"
      ],
      "abstract": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.",
      "tldr_zh": "本论文探讨了Large Language Models (LLMs) 在6G无线网络编排中的应用，以应对网络复杂性、管理优化和动态环境的挑战。作者提出NetOrchLLM框架，利用LLMs的语言理解和生成能力，无缝整合各种无线通信模型，实现对密集网络的优化和整体性能提升。该框架通过实际演示桥接了现有研究的理论愿景与可操作解决方案，为AI技术在无线通信领域的未来发展奠定基础。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10107v1",
      "published_date": "2024-12-13 12:48:15 UTC",
      "updated_date": "2024-12-13 12:48:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:17:08.735739"
    },
    {
      "arxiv_id": "2412.10106v4",
      "title": "A Cascaded Dilated Convolution Approach for Mpox Lesion Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Ayush Deshmukh"
      ],
      "abstract": "The global outbreak of the Mpox virus, classified as a Public Health\nEmergency of International Concern (PHEIC) by the World Health Organization,\npresents significant diagnostic challenges due to its visual similarity to\nother skin lesion diseases. Traditional diagnostic methods for Mpox, which rely\non clinical symptoms and laboratory tests, are slow and labor intensive. Deep\nlearning-based approaches for skin lesion classification offer a promising\nalternative. However, developing a model that balances efficiency with accuracy\nis crucial to ensure reliable and timely diagnosis without compromising\nperformance. This study introduces the Cascaded Atrous Group Attention (CAGA)\nframework to address these challenges, combining the Cascaded Atrous Attention\nmodule and the Cascaded Group Attention mechanism. The Cascaded Atrous\nAttention module utilizes dilated convolutions and cascades the outputs to\nenhance multi-scale representation. This is integrated into the Cascaded Group\nAttention mechanism, which reduces redundancy in Multi-Head Self-Attention. By\nintegrating the Cascaded Atrous Group Attention module with EfficientViT-L1 as\nthe backbone architecture, this approach achieves state-of-the-art performance,\nreaching an accuracy of 98% on the Mpox Close Skin Image (MCSI) dataset while\nreducing model parameters by 37.5% compared to the original EfficientViT-L1.\nThe model's robustness is demonstrated through extensive validation on two\nadditional benchmark datasets, where it consistently outperforms existing\napproaches.",
      "tldr_zh": "该研究针对 Mpox 病毒诊断的挑战（如与其它皮肤病相似性），提出了一种 Cascaded Atrous Group Attention (CAGA) 框架，结合 Cascaded Atrous Attention 模块（利用 dilated convolutions 和级联输出增强多尺度表示）以及 Cascaded Group Attention 机制（减少 Multi-Head Self-Attention 的冗余）。该框架以 EfficientViT-L1 作为骨干网络，在 Mpox Close Skin Image (MCSI) 数据集上实现了 98% 的准确率，同时将模型参数减少了 37.5%。实验结果显示，该方法在多个基准数据集上表现出色，显著提升了诊断效率和可靠性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "8 pages, 4 figures, Submitted to Medical Imaging with Deep Learning",
      "pdf_url": "http://arxiv.org/pdf/2412.10106v4",
      "published_date": "2024-12-13 12:47:30 UTC",
      "updated_date": "2025-01-14 03:43:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:17:20.698084"
    },
    {
      "arxiv_id": "2412.10104v2",
      "title": "RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector",
      "title_zh": "RETQA：房地产领域的大规模开放域表格问答数据集",
      "authors": [
        "Zhensheng Wang",
        "Wenmian Yang",
        "Kun Zhou",
        "Yiquan Zhang",
        "Weijia Jia"
      ],
      "abstract": "The real estate market relies heavily on structured data, such as property\ndetails, market trends, and price fluctuations. However, the lack of\nspecialized Tabular Question Answering datasets in this domain limits the\ndevelopment of automated question-answering systems. To fill this gap, we\nintroduce RETQA, the first large-scale open-domain Chinese Tabular Question\nAnswering dataset for Real Estate. RETQA comprises 4,932 tables and 20,762\nquestion-answer pairs across 16 sub-fields within three major domains: property\ninformation, real estate company finance information and land auction\ninformation. Compared with existing tabular question answering datasets, RETQA\nposes greater challenges due to three key factors: long-table structures,\nopen-domain retrieval, and multi-domain queries. To tackle these challenges, we\npropose the SLUTQA framework, which integrates large language models with\nspoken language understanding tasks to enhance retrieval and answering\naccuracy. Extensive experiments demonstrate that SLUTQA significantly improves\nthe performance of large language models on RETQA by in-context learning. RETQA\nand SLUTQA provide essential resources for advancing tabular question answering\nresearch in the real estate domain, addressing critical challenges in\nopen-domain and long-table question-answering. The dataset and code are\npublicly available at \\url{https://github.com/jensen-w/RETQA}.",
      "tldr_zh": "该研究引入了 RETQA，这是一个大规模开放域中文表格问答数据集，针对房地产领域，包含 4,932 个表格和 20,762 个问答对，覆盖房产信息、房地产公司财务信息和土地拍卖信息的 16 个子领域。相比现有数据集，RETQA 面临更大挑战，包括长表格结构、开放域检索和多领域查询。研究者提出 SLUTQA 框架，将大语言模型(Large Language Models)与口语语言理解任务整合，提升检索和回答准确性。实验结果显示，SLUTQA 通过上下文学习显著提高了模型在 RETQA 上的性能，为房地产领域的表格问答研究提供关键资源，且数据集和代码已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper is accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.10104v2",
      "published_date": "2024-12-13 12:45:14 UTC",
      "updated_date": "2025-01-23 13:18:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:17:33.066443"
    },
    {
      "arxiv_id": "2412.10095v2",
      "title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Jaione Bengoetxea",
        "Mikel Zubillaga",
        "Ekhi Azurmendi",
        "Maite Heredia",
        "Julen Etxaniz",
        "Markel Ferro",
        "Jeremy Barnes"
      ],
      "abstract": "In this paper we present our submission for the NorSID Shared Task as part of\nthe 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks:\nIntent Detection, Slot Filling and Dialect Identification, evaluated using data\nin different dialects of the Norwegian language. For Intent Detection and Slot\nFilling, we have fine-tuned a multitask model in a cross-lingual setting, to\nleverage the xSID dataset available in 17 languages. In the case of Dialect\nIdentification, our final submission consists of a model fine-tuned on the\nprovided development set, which has obtained the highest scores within our\nexperiments. Our final results on the test set show that our models do not drop\nin performance compared to the development set, likely due to the\ndomain-specificity of the dataset and the similar distribution of both subsets.\nFinally, we also report an in-depth analysis of the provided datasets and their\nartifacts, as well as other sets of experiments that have been carried out but\ndid not yield the best results. Additionally, we present an analysis on the\nreasons why some methods have been more successful than others; mainly the\nimpact of the combination of languages and domain-specificity of the training\ndata on the results.",
      "tldr_zh": "本论文介绍了HiTZ团队在VarDial 2025 NorSID共享任务中的提交，针对挪威语方言数据下的Intent Detection、Slot Filling和Dialect Identification三个任务，旨在通过Language Transfer和Automatic Data Annotation克服数据稀缺问题。对于Intent Detection和Slot Filling，团队在跨语言设置中微调多任务模型，利用xSID数据集（覆盖17种语言）进行训练；Dialect Identification则基于提供的开发集进行微调，取得了最佳实验成绩。测试结果显示，模型性能与开发集相当，受益于数据集的领域特异性和分布相似性；论文还分析了语言组合和领域特异性对结果的影响，并探讨了其他实验的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Vardial 2025 NorSID Shared Task, fixed minor typos",
      "pdf_url": "http://arxiv.org/pdf/2412.10095v2",
      "published_date": "2024-12-13 12:31:06 UTC",
      "updated_date": "2025-01-09 09:09:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:17:44.881549"
    },
    {
      "arxiv_id": "2412.10093v1",
      "title": "AI in the Cosmos",
      "title_zh": "翻译失败",
      "authors": [
        "N. Sahakyan"
      ],
      "abstract": "Artificial intelligence (AI) is revolutionizing research by enabling the\nefficient analysis of large datasets and the discovery of hidden patterns. In\nastrophysics, AI has become essential, transforming the classification of\ncelestial sources, data modeling, and the interpretation of observations. In\nthis review, I highlight examples of AI applications in astrophysics, including\nsource classification, spectral energy distribution modeling, and discuss the\nadvancements achievable through generative AI. However, the use of AI\nintroduces challenges, including biases, errors, and the \"black box\" nature of\nAI models, which must be resolved before their application. These issues can be\naddressed through the concept of Human-Guided AI (HG-AI), which integrates\nhuman expertise and domain-specific knowledge into AI applications. This\napproach aims to ensure that AI is applied in a robust, interpretable, and\nethical manner, leading to deeper insights and fostering scientific excellence.",
      "tldr_zh": "人工智能(AI)正在革新天体物理学研究，通过高效分析海量数据集和发现隐藏模式，例如在源分类、光谱能量分布建模以及观察数据解释中的应用，并突显生成式AI的进展。本文回顾这些AI应用的同时，强调了潜在挑战，如偏差、错误和AI模型的黑箱性质，这些问题需通过Human-Guided AI (HG-AI)整合人类专业知识来解决。HG-AI 方法确保AI的应用更具鲁棒性、可解释性和伦理性，最终促进更深入的科学洞见和卓越。",
      "categories": [
        "astro-ph.HE",
        "astro-ph.GA",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "astro-ph.HE",
      "comment": "In press in the International Journal of Modern Physics D; invited\n  talk at the 17th Marcel Grossmann Meeting",
      "pdf_url": "http://arxiv.org/pdf/2412.10093v1",
      "published_date": "2024-12-13 12:30:11 UTC",
      "updated_date": "2024-12-13 12:30:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:17:57.426889"
    },
    {
      "arxiv_id": "2412.10091v1",
      "title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for Object Re-identification",
      "title_zh": "翻译失败",
      "authors": [
        "Zi Yang",
        "Haojin Yang",
        "Soumajit Majumder",
        "Jorge Cardoso",
        "Guillermo Gallego"
      ],
      "abstract": "Previous studies have demonstrated that not each sample in a dataset is of\nequal importance during training. Data pruning aims to remove less important or\ninformative samples while still achieving comparable results as training on the\noriginal (untruncated) dataset, thereby reducing storage and training costs.\nHowever, the majority of data pruning methods are applied to image\nclassification tasks. To our knowledge, this work is the first to explore the\nfeasibility of these pruning methods applied to object re-identification (ReID)\ntasks, while also presenting a more comprehensive data pruning approach. By\nfully leveraging the logit history during training, our approach offers a more\naccurate and comprehensive metric for quantifying sample importance, as well as\ncorrecting mislabeled samples and recognizing outliers. Furthermore, our\napproach is highly efficient, reducing the cost of importance score estimation\nby 10 times compared to existing methods. Our approach is a plug-and-play,\narchitecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of\nsamples/training time on the VeRi, MSMT17 and Market1501 datasets,\nrespectively, with negligible loss in accuracy (< 0.1%). The lists of\nimportant, mislabeled, and outlier samples from these ReID datasets are\navailable at https://github.com/Zi-Y/data-pruning-reid.",
      "tldr_zh": "本文提出了一种全面的Data Pruning方法，首次应用于Object Re-identification (ReID)任务，旨在通过移除不重要样本来减少存储和训练成本，同时保持性能。该方法利用训练过程中的logit history来更准确量化样本重要性，并能修正错误标记样本和识别异常值，比现有方法效率提高10倍，是一个即插即用的架构无关框架。在VeRi、MSMT17和Market1501数据集上，该方法可去除35%、30%和5%的样本及训练时间，而准确率损失小于0.1%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10091v1",
      "published_date": "2024-12-13 12:27:47 UTC",
      "updated_date": "2024-12-13 12:27:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:18:09.795118"
    },
    {
      "arxiv_id": "2412.15242v1",
      "title": "Script-Based Dialog Policy Planning for LLM-Powered Conversational Agents: A Basic Architecture for an \"AI Therapist\"",
      "title_zh": "翻译失败",
      "authors": [
        "Robert Wasenmüller",
        "Kevin Hilbert",
        "Christoph Benzmüller"
      ],
      "abstract": "Large Language Model (LLM)-Powered Conversational Agents have the potential\nto provide users with scaled behavioral healthcare support, and potentially\neven deliver full-scale \"AI therapy'\" in the future. While such agents can\nalready conduct fluent and proactive emotional support conversations, they\ninherently lack the ability to (a) consistently and reliably act by predefined\nrules to align their conversation with an overarching therapeutic concept and\n(b) make their decision paths inspectable for risk management and clinical\nevaluation -- both essential requirements for an \"AI Therapist\".\n  In this work, we introduce a novel paradigm for dialog policy planning in\nconversational agents enabling them to (a) act according to an expert-written\n\"script\" that outlines the therapeutic approach and (b) explicitly transition\nthrough a finite set of states over the course of the conversation. The script\nacts as a deterministic component, constraining the LLM's behavior in desirable\nways and establishing a basic architecture for an AI Therapist.\n  We implement two variants of Script-Based Dialog Policy Planning using\ndifferent prompting techniques and synthesize a total of 100 conversations with\nLLM-simulated patients. The results demonstrate the feasibility of this new\ntechnology and provide insights into the efficiency and effectiveness of\ndifferent implementation variants.",
      "tldr_zh": "该论文提出了一种Script-Based Dialog Policy Planning范式，用于LLM驱动的对话代理，旨在解决现有代理在提供行为医疗支持时无法一致遵循预定义规则和决策路径不可检查的问题，从而为“AI Therapist”建立基本架构。方法涉及使用专家编写的“script”作为确定性组件，让代理通过有限状态集显式转换，并在对话中约束LLM的行为。研究实现了两种基于不同提示技术的变体，合成了100个与LLM模拟患者的对话，结果证明了该技术的可行性，并提供了不同实现变体的效率和有效性见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T01"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 5 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2412.15242v1",
      "published_date": "2024-12-13 12:12:47 UTC",
      "updated_date": "2024-12-13 12:12:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:18:21.809882"
    },
    {
      "arxiv_id": "2412.10059v1",
      "title": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric Quantization and Energy-Saving Bit-Slice Sparsity",
      "title_zh": "翻译失败",
      "authors": [
        "Dongyun Kam",
        "Myeongji Yun",
        "Sunwoo Yoo",
        "Seungwoo Hong",
        "Zhengya Zhang",
        "Youngjoo Lee"
      ],
      "abstract": "Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators.",
      "tldr_zh": "本论文提出了一种新型DNN加速器Panacea，通过采用保留准确性的不对称量化(Asymmetric Quantization)和节省能量的位切片稀疏性(Bit-Slice Sparsity)，解决了传统对称量化在激活分布不对称时导致的准确性损失问题。核心方法包括首次引入Asymmetrically-Quantized bit-Slice GEMM (AQS-GEMM)，该方法压缩和跳过不对称量化产生的频繁非零切片，同时通过零点操作(Zero-Point Manipulation)和基于分布的位切片(Distribution-Based Bit-Slicing)优化激活的切片级稀疏性。Panacea加速器支持专用数据流和游长编码(Run-Length Encoding)，最大化数据重用并减少外部内存访问，基准评估显示其在硬件效率上优于现有DNN加速器。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "15 pages, 20 figures, Accepted to HPCA 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.10059v1",
      "published_date": "2024-12-13 11:44:09 UTC",
      "updated_date": "2024-12-13 11:44:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:18:33.623942"
    },
    {
      "arxiv_id": "2412.10056v1",
      "title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?",
      "title_zh": "翻译失败",
      "authors": [
        "Zhikai Lei",
        "Tianyi Liang",
        "Hanglei Hu",
        "Jin Zhang",
        "Yunhua Zhou",
        "Yunfan Shao",
        "Linyang Li",
        "Chenchui Li",
        "Changbo Wang",
        "Hang Yan",
        "Qipeng Guo"
      ],
      "abstract": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.",
      "tldr_zh": "本文提出 GAOKAO-Eval 基准，这是基于中国高考的全面评估框架，用于检验大型语言模型 (LLMs) 的高分是否真正反映人类-like 能力。研究通过闭卷测试和 Rasch 模型分析发现，即使解决数据泄露问题，LLMs 仍存在关键缺陷：表现异常一致于不同难度问题，以及在类似难度问题上的高变异性。实验还揭示了教师对 LLMs 生成答案的评分不一致和常见错误模式，并指出 OpenAI o1 的推理机制可部分缓解这种不匹配。这些结果强调了当前基准的局限性，并呼吁开发更适合 LLMs 的难度分析方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10056v1",
      "published_date": "2024-12-13 11:38:10 UTC",
      "updated_date": "2024-12-13 11:38:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:18:45.815005"
    },
    {
      "arxiv_id": "2412.10051v1",
      "title": "TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting from Sparse Views",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Zhao",
        "Zehan Bao",
        "Yi Xie",
        "Hong Chen",
        "Yaohui Chen",
        "Weifu Li"
      ],
      "abstract": "Recent advances in Gaussian Splatting have significantly advanced the field,\nachieving both panoptic and interactive segmentation of 3D scenes. However,\nexisting methodologies often overlook the critical need for reconstructing\nspecified targets with complex structures from sparse views. To address this\nissue, we introduce TSGaussian, a novel framework that combines semantic\nconstraints with depth priors to avoid geometry degradation in challenging\nnovel view synthesis tasks. Our approach prioritizes computational resources on\ndesignated targets while minimizing background allocation. Bounding boxes from\nYOLOv9 serve as prompts for Segment Anything Model to generate 2D mask\npredictions, ensuring semantic accuracy and cost efficiency. TSGaussian\neffectively clusters 3D gaussians by introducing a compact identity encoding\nfor each Gaussian ellipsoid and incorporating 3D spatial consistency\nregularization. Leveraging these modules, we propose a pruning strategy to\neffectively reduce redundancy in 3D gaussians. Extensive experiments\ndemonstrate that TSGaussian outperforms state-of-the-art methods on three\nstandard datasets and a new challenging dataset we collected, achieving\nsuperior results in novel view synthesis of specific objects. Code is available\nat: https://github.com/leon2000-ai/TSGaussian.",
      "tldr_zh": "该论文提出 TSGaussian，一种结合语义约束和深度先验的框架，用于从稀疏视图重建复杂结构的指定目标，避免 Gaussian Splatting 中的几何退化问题。该方法利用 YOLOv9 的边界框作为提示，结合 Segment Anything Model 生成 2D 掩码，并引入紧凑身份编码和 3D 空间一致性正则化来聚类 3D Gaussians，同时采用修剪策略减少冗余，从而优先处理目标并优化资源分配。实验结果显示，TSGaussian 在三个标准数据集和一个新收集的挑战数据集上，优于现有最先进方法，在特定对象的 Novel View Synthesis 任务中取得显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10051v1",
      "published_date": "2024-12-13 11:26:38 UTC",
      "updated_date": "2024-12-13 11:26:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:18:58.354625"
    },
    {
      "arxiv_id": "2412.10047v2",
      "title": "Large Action Models: From Inception to Implementation",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Wang",
        "Fangkai Yang",
        "Chaoyun Zhang",
        "Junting Lu",
        "Jiaxu Qian",
        "Shilin He",
        "Pu Zhao",
        "Bo Qiao",
        "Ray Huang",
        "Si Qin",
        "Qisheng Su",
        "Jiayi Ye",
        "Yudi Zhang",
        "Jian-Guang Lou",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
      "tldr_zh": "这篇论文探讨了从 Large Language Models (LLMs) 向 Large Action Models (LAMs) 的演进，LAMs 专注于生成和执行真实世界动作，推动 AI 从被动语言理解转向主动任务完成。作者提出一个全面框架，从概念到部署，系统指导 LAMs 的开发过程，包括数据收集、模型训练、环境集成、grounding 和评估，并以 Windows OS-based 代理作为案例研究，提供通用化的工作流程。实验和分析突出了 LAMs 在实现人工智能通用性的潜力，同时讨论了当前限制和未来研究方向，如挑战与工业应用机会。代码和文档已在 GitHub 上公开。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25pages,12 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10047v2",
      "published_date": "2024-12-13 11:19:56 UTC",
      "updated_date": "2025-01-13 06:47:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:19:10.473379"
    },
    {
      "arxiv_id": "2412.10483v1",
      "title": "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models",
      "title_zh": "利用大型语言模型增强复杂程序的自动循环不变式生成",
      "authors": [
        "Ruibang Liu",
        "Guoqiang Li",
        "Minyu Chen",
        "Ling-I Wu",
        "Jingyu Ke"
      ],
      "abstract": "Automated program verification has always been an important component of\nbuilding trustworthy software. While the analysis of real-world programs\nremains a theoretical challenge, the automation of loop invariant analysis has\neffectively resolved the problem. However, real-world programs that often mix\ncomplex data structures and control flows pose challenges to traditional loop\ninvariant generation tools. To enhance the applicability of invariant\ngeneration techniques, we proposed ACInv, an Automated Complex program loop\nInvariant generation tool, which combines static analysis with Large Language\nModels (LLMs) to generate the proper loop invariants. We utilize static\nanalysis to extract the necessary information for each loop and embed it into\nprompts for the LLM to generate invariants for each loop. Subsequently, we\nemploy an LLM-based evaluator to assess the generated invariants, refining them\nby either strengthening, weakening, or rejecting them based on their\ncorrectness, ultimately obtaining enhanced invariants. We conducted experiments\non ACInv, which showed that ACInv outperformed previous tools on data sets with\ndata structures, and maintained similar performance to the state-of-the-art\ntool AutoSpec on numerical programs without data structures. For the total data\nset, ACInv can solve 21% more examples than AutoSpec and can generate reference\ndata structure templates.",
      "tldr_zh": "这篇论文提出了一种名为 ACInv 的工具，用于增强复杂程序的循环不变式自动生成，通过结合静态分析和 Large Language Models (LLMs) 来解决传统工具在处理数据结构和控制流混合程序时的挑战。方法包括利用静态分析提取循环相关信息嵌入提示中，让 LLMs 生成初始不变式，随后通过 LLMs 评估器评估并精炼这些不变式（包括强化、弱化或拒绝）。实验结果显示，ACInv 在包含数据结构的程序上优于现有工具，与 AutoSpec 在无数据结构数值程序上性能相当，总体上能多解决 21% 的例子并生成参考数据结构模板。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "11 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10483v1",
      "published_date": "2024-12-13 10:36:18 UTC",
      "updated_date": "2024-12-13 10:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:19:21.411242"
    },
    {
      "arxiv_id": "2412.12185v1",
      "title": "Graph Similarity Computation via Interpretable Neural Node Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Jingjing Wang",
        "Hongjie Zhu",
        "Haoran Xie",
        "Fu Lee Wang",
        "Xiaoliang Xu",
        "Yuxiang Wang"
      ],
      "abstract": "\\Graph similarity computation is an essential task in many real-world\ngraph-related applications such as retrieving the similar drugs given a query\nchemical compound or finding the user's potential friends from the social\nnetwork database. Graph Edit Distance (GED) and Maximum Common Subgraphs (MCS)\nare the two commonly used domain-agnostic metrics to evaluate graph similarity\nin practice. Unfortunately, computing the exact GED is known to be a NP-hard\nproblem. To solve this limitation, neural network based models have been\nproposed to approximate the calculations of GED/MCS. However, deep learning\nmodels are well-known ``black boxes'', thus the typically characteristic\none-to-one node/subgraph alignment process in the classical computations of GED\nand MCS cannot be seen. Existing methods have paid attention to approximating\nthe node/subgraph alignment (soft alignment), but the one-to-one node alignment\n(hard alignment) has not yet been solved. To fill this gap, in this paper we\npropose a novel interpretable neural node alignment model without relying on\nnode alignment ground truth information. Firstly, the quadratic assignment\nproblem in classical GED computation is relaxed to a linear alignment via\nembedding the features in the node embedding space. Secondly, a differentiable\nGumbel-Sinkhorn module is proposed to unsupervised generate the optimal\none-to-one node alignment matrix. Experimental results in real-world graph\ndatasets demonstrate that our method outperforms the state-of-the-art methods\nin graph similarity computation and graph retrieval tasks, achieving up to 16\\%\nreduction in the Mean Squared Error and up to 12\\% improvement in the retrieval\nevaluation metrics, respectively.",
      "tldr_zh": "本文提出一种可解释的神经节点对齐方法，用于图相似性计算，旨在解决传统 Graph Edit Distance (GED) 和 Maximum Common Subgraphs (MCS) 计算的 NP-hard 问题，以及现有深度学习模型的黑盒特性。方法通过将 quadratic assignment problem 放松到线性对齐，利用节点嵌入空间，并引入 differentiable Gumbel-Sinkhorn 模块来无监督生成最优的一对一节点对齐矩阵，而不依赖节点对齐的 ground truth 信息。实验结果显示，该方法在真实图数据集上优于最先进方法，Mean Squared Error 减少高达 16%，检索评估指标提升高达 12%。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12185v1",
      "published_date": "2024-12-13 10:23:27 UTC",
      "updated_date": "2024-12-13 10:23:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:19:35.640903"
    },
    {
      "arxiv_id": "2412.10482v1",
      "title": "Dynamic Entity-Masked Graph Diffusion Model for histopathological image Representation Learning",
      "title_zh": "动态实体掩码图扩散模型用于组织病理图像表示学习",
      "authors": [
        "Zhenfeng Zhuang",
        "Min Cen",
        "Yanfeng Li",
        "Fangyu Zhou",
        "Lequan Yu",
        "Baptiste Magnier",
        "Liansheng Wang"
      ],
      "abstract": "Significant disparities between the features of natural images and those\ninherent to histopathological images make it challenging to directly apply and\ntransfer pre-trained models from natural images to histopathology tasks.\nMoreover, the frequent lack of annotations in histopathology patch images has\ndriven researchers to explore self-supervised learning methods like mask\nreconstruction for learning representations from large amounts of unlabeled\ndata. Crucially, previous mask-based efforts in self-supervised learning have\noften overlooked the spatial interactions among entities, which are essential\nfor constructing accurate representations of pathological entities. To address\nthese challenges, constructing graphs of entities is a promising approach. In\naddition, the diffusion reconstruction strategy has recently shown superior\nperformance through its random intensity noise addition technique to enhance\nthe robust learned representation. Therefore, we introduce H-MGDM, a novel\nself-supervised Histopathology image representation learning method through the\nDynamic Entity-Masked Graph Diffusion Model. Specifically, we propose to use\ncomplementary subgraphs as latent diffusion conditions and self-supervised\ntargets respectively during pre-training. We note that the graph can embed\nentities' topological relationships and enhance representation. Dynamic\nconditions and targets can improve pathological fine reconstruction. Our model\nhas conducted pretraining experiments on three large histopathological\ndatasets. The advanced predictive performance and interpretability of H-MGDM\nare clearly evaluated on comprehensive downstream tasks such as classification\nand survival analysis on six datasets. Our code will be publicly available at\nhttps://github.com/centurion-crawler/H-MGDM.",
      "tldr_zh": "该研究针对自然图像预训练模型在病理图像上的适用性挑战，提出H-MGDM（Dynamic Entity-Masked Graph Diffusion Model），一种自监督学习方法，通过构建实体图嵌入拓扑关系并使用互补子图作为扩散条件和目标，提升图像表示的鲁棒性和细粒度重建能力。相比传统掩码重建方法，H-MGDM强调实体间的空间交互，解决了忽略这些交互的局限性。在三个大型病理数据集上预训练后，该模型在六个下游任务（如分类和生存分析）上表现出先进预测性能和可解释性，为无标注病理图像学习提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10482v1",
      "published_date": "2024-12-13 10:18:36 UTC",
      "updated_date": "2024-12-13 10:18:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:19:46.459230"
    },
    {
      "arxiv_id": "2412.10011v1",
      "title": "Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework",
      "title_zh": "高效通道注意力引导的深度 CNN-BiLSTM 框架增强语音情感识别",
      "authors": [
        "Niloy Kumar Kundu",
        "Sarah Kobir",
        "Md. Rayhan Ahmed",
        "Tahmina Aktar",
        "Niloya Roy"
      ],
      "abstract": "Speech emotion recognition (SER) is crucial for enhancing affective computing\nand enriching the domain of human-computer interaction. However, the main\nchallenge in SER lies in selecting relevant feature representations from speech\nsignals with lower computational costs. In this paper, we propose a lightweight\nSER architecture that integrates attention-based local feature blocks (ALFBs)\nto capture high-level relevant feature vectors from speech signals. We also\nincorporate a global feature block (GFB) technique to capture sequential,\nglobal information and long-term dependencies in speech signals. By aggregating\nattention-based local and global contextual feature vectors, our model\neffectively captures the internal correlation between salient features that\nreflect complex human emotional cues. To evaluate our approach, we extracted\nfour types of spectral features from speech audio samples: mel-frequency\ncepstral coefficients, mel-spectrogram, root mean square value, and\nzero-crossing rate. Through a 5-fold cross-validation strategy, we tested the\nproposed method on five multi-lingual standard benchmark datasets: TESS,\nRAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of\n99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate\nthat our model achieves state-of-the-art (SOTA) performance compared to most\nexisting methods.",
      "tldr_zh": "本论文提出了一种轻量级 Speech Emotion Recognition (SER) 框架，利用高效通道注意力引导的 Deep CNN-BiLSTM 模型，旨在从语音信号中提取相关特征，同时降低计算成本。框架整合了 Attention-based Local Feature Blocks (ALFBs) 来捕获高层次局部特征，以及 Global Feature Block (GFB) 来处理序列化全球信息和长期依赖，通过聚合这些特征向量有效识别复杂情感线索。实验在五个多语言基准数据集（TESS、RAVDESS、BanglaSER、SUBESCO 和 Emo-DB）上，使用 MFCC、mel-spectrogram 等特征进行5-fold交叉验证，取得了高达99.65%的平均准确率，超越了现有 state-of-the-art 方法。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "42 pages,10 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.10011v1",
      "published_date": "2024-12-13 09:55:03 UTC",
      "updated_date": "2024-12-13 09:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:19:58.528084"
    },
    {
      "arxiv_id": "2412.15241v3",
      "title": "Quantifying Positional Biases in Text Embedding Models",
      "title_zh": "量化文本嵌入模型中的位置偏差",
      "authors": [
        "Samarth Goel",
        "Reagan J. Lee",
        "Kannan Ramchandran"
      ],
      "abstract": "Embedding models are crucial for tasks in Information Retrieval (IR) and\nsemantic similarity measurement, yet their handling of longer texts and\nassociated positional biases remains underexplored. In this study, we\ninvestigate the impact of content position and input size on text embeddings.\nOur experiments reveal that embedding models, irrespective of their positional\nencoding mechanisms, disproportionately prioritize the beginning of an input.\nAblation studies demonstrate that insertion of irrelevant text or removal at\nthe start of a document reduces cosine similarity between altered and original\nembeddings by up to 12.3% more than ablations at the end. Regression analysis\nfurther confirms this bias, with sentence importance declining as position\nmoves further from the start, even with with content-agnosticity. We\nhypothesize that this effect arises from pre-processing strategies and chosen\npositional encoding techniques. These findings quantify the sensitivity of\nretrieval systems and suggest a new lens towards embedding model robustness.",
      "tldr_zh": "本文研究了文本嵌入模型（text embedding models）在处理较长文本时的位置偏差（positional biases），发现这些模型无论使用何种位置编码机制，都更倾向于优先考虑输入的开头部分。作者通过实验和消融研究（ablation studies）证明，在文档开头插入或删除无关文本会使原文本与修改文本之间的余弦相似度（cosine similarity）降低多达12.3%，远高于结尾位置的影响。回归分析（regression analysis）进一步显示，句子的重要性随位置远离开头而递减。作者假设这种偏差源于预处理策略和位置编码技术（positional encoding），并为提升检索系统的鲁棒性提供了新的研究视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 11 figures, NeurIPS",
      "pdf_url": "http://arxiv.org/pdf/2412.15241v3",
      "published_date": "2024-12-13 09:52:25 UTC",
      "updated_date": "2025-01-01 18:06:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:20:09.674153"
    },
    {
      "arxiv_id": "2412.09998v2",
      "title": "Self-Consistent Nested Diffusion Bridge for Accelerated MRI Reconstruction",
      "title_zh": "自一致嵌套扩散桥用于加速MRI重建",
      "authors": [
        "Tao Song",
        "Yicheng Wu",
        "Minhao Hu",
        "Xiangde Luo",
        "Guoting Luo",
        "Guotai Wang",
        "Yi Guo",
        "Feng Xu",
        "Shaoting Zhang"
      ],
      "abstract": "Accelerated MRI reconstruction plays a vital role in reducing scan time while\npreserving image quality. While most existing methods rely on complex-valued\nimage-space or k-space data, these formats are often inaccessible in clinical\npractice due to proprietary reconstruction pipelines, leaving only magnitude\nimages stored in DICOM files. To address this gap, we focus on the\nunderexplored task of magnitude-image-based MRI reconstruction. Recent\nadvancements in diffusion models, particularly denoising diffusion\nprobabilistic models (DDPMs), have demonstrated strong capabilities in modeling\nimage priors. However, their task-agnostic denoising nature limits performance\nin source-to-target image translation tasks, such as MRI reconstruction. In\nthis work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB)\nframework that models accelerated MRI reconstruction as a bi-directional image\ntranslation process between under-sampled and fully-sampled magnitude MRI\nimages. SC-NDB introduces a nested diffusion architecture with a\nself-consistency constraint and reverse bridge diffusion pathways to improve\nintermediate prediction fidelity and better capture the explicit priors of\nsource images. Furthermore, we incorporate a Contour Decomposition Embedding\nModule (CDEM) to inject structural and textural knowledge by leveraging\nLaplacian pyramids and directional filter banks. Extensive experiments on the\nfastMRI and IXI datasets demonstrate that our method achieves state-of-the-art\nperformance compared to both magnitude-based and non-magnitude-based diffusion\nmodels, confirming the effectiveness and clinical relevance of SC-NDB.",
      "tldr_zh": "本文提出 Self-Consistent Nested Diffusion Bridge (SC-NDB) 框架，用于基于 magnitude 图像的加速 MRI 重建，解决临床实践中仅可用 magnitude 图像而非复杂值数据的挑战。SC-NDB 将重建过程视为 under-sampled 和 fully-sampled 图像之间的双向转换，采用 nested diffusion architecture、self-consistency constraint 和 reverse bridge diffusion pathways 来提升中间预测的保真度和捕捉源图像的显式先验。框架还整合 Contour Decomposition Embedding Module (CDEM)，利用 Laplacian pyramids 和 directional filter banks 注入结构和纹理知识。在 fastMRI 和 IXI 数据集上的实验显示，SC-NDB 比现有基于 magnitude 或非 magnitude 的 diffusion models 达到最先进性能，证明了其有效性和临床实用性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09998v2",
      "published_date": "2024-12-13 09:35:34 UTC",
      "updated_date": "2025-04-28 02:56:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:22:23.405148"
    },
    {
      "arxiv_id": "2412.10481v1",
      "title": "Tipping Points, Pulse Elasticity and Tonal Tension: An Empirical Study on What Generates Tipping Points",
      "title_zh": "翻译失败",
      "authors": [
        "Canishk Naik",
        "Elaine Chew"
      ],
      "abstract": "Tipping points are moments of change that characterise crucial turning points\nin a piece of music. This study presents a first step towards quantitatively\nand systematically describing the musical properties of tipping points. Timing\ninformation and computationally-derived tonal tension values which correspond\nto dissonance, distance from key, and harmonic motion are compared to tipping\npoints in Ashkenazy's recordings of six Chopin Mazurkas, as identified by 35\nlisteners. The analysis shows that all popular tipping points but one could be\nexplained by statistically significant timing deviations or changepoints in at\nleast one of the three tension parameters.",
      "tldr_zh": "该研究首次定量和系统地探讨音乐中转折点（tipping points）的音乐属性，旨在分析这些关键时刻的生成因素。研究者通过35名听众识别Ashkenazy录制的六首Chopin Mazurkas中的tipping points，并将其与计算得出的tonal tension值（如dissonance、distance from key和harmonic motion）以及timing信息进行比较。结果显示，几乎所有受欢迎的tipping points都可以用统计显著的timing deviations或tension参数中的changepoints来解释，为理解音乐转折机制提供了实证依据。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "International Society for Music Information Retrieval Conference, Oct\n  2017, Suzhou, China, China",
      "pdf_url": "http://arxiv.org/pdf/2412.10481v1",
      "published_date": "2024-12-13 09:27:46 UTC",
      "updated_date": "2024-12-13 09:27:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:22:34.732762"
    },
    {
      "arxiv_id": "2501.04008v2",
      "title": "A Generative AI-driven Metadata Modelling Approach",
      "title_zh": "生成式 AI 驱动的元数据建模方法",
      "authors": [
        "Mayukh Bagchi"
      ],
      "abstract": "Since decades, the modelling of metadata has been core to the functioning of\nany academic library. Its importance has only enhanced with the increasing\npervasiveness of Generative Artificial Intelligence (AI)-driven information\nactivities and services which constitute a library's outreach. However, with\nthe rising importance of metadata, there arose several outstanding problems\nwith the process of designing a library metadata model impacting its\nreusability, crosswalk and interoperability with other metadata models. This\npaper posits that the above problems stem from an underlying thesis that there\nshould only be a few core metadata models which would be necessary and\nsufficient for any information service using them, irrespective of the\nheterogeneity of intra-domain or inter-domain settings. To that end, this paper\nadvances a contrary view of the above thesis and substantiates its argument in\nthree key steps. First, it introduces a novel way of thinking about a library\nmetadata model as an ontology-driven composition of five functionally\ninterlinked representation levels from perception to its intensional definition\nvia properties. Second, it introduces the representational manifoldness\nimplicit in each of the five levels which cumulatively contributes to a\nconceptually entangled library metadata model. Finally, and most importantly,\nit proposes a Generative AI-driven Human-Large Language Model (LLM)\ncollaboration based metadata modelling approach to disentangle the entanglement\ninherent in each representation level leading to the generation of a\nconceptually disentangled metadata model. Throughout the paper, the arguments\nare exemplified by motivating scenarios and examples from representative\nlibraries handling cancer information.",
      "tldr_zh": "这篇论文讨论了学术图书馆元数据建模的核心问题，包括可重用性、跨标准互操作性等挑战，并挑战了仅需少数核心模型即可适用于各种场景的假设。论文提出一种新型方法，将元数据模型视为本体驱动（ontology-driven）的五层功能互连表示水平（从感知到内涵定义）的组合，并分析这些水平中的表示多样性（representational manifoldness）导致的概念纠缠。最终，它引入了 Generative AI-driven Human-LLM 协作建模方法，来解开这些纠缠，生成概念上解开的元数据模型，并通过癌症信息库的实际场景进行举例说明。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DL",
      "comment": "Accepted for publication @ Special Issue on \"Generative AI and\n  Libraries\" - Library Trends Journal, Johns Hopkins University Press,\n  Maryland, USA",
      "pdf_url": "http://arxiv.org/pdf/2501.04008v2",
      "published_date": "2024-12-13 09:26:04 UTC",
      "updated_date": "2025-03-16 21:12:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:22:47.747273"
    },
    {
      "arxiv_id": "2412.09991v1",
      "title": "Visual Object Tracking across Diverse Data Modalities: A Review",
      "title_zh": "跨越多种数据模态的视觉物体追踪：综述",
      "authors": [
        "Mengmeng Wang",
        "Teli Ma",
        "Shuo Xin",
        "Xiaojun Hou",
        "Jiazheng Xing",
        "Guang Dai",
        "Jingdong Wang",
        "Yong Liu"
      ],
      "abstract": "Visual Object Tracking (VOT) is an attractive and significant research area\nin computer vision, which aims to recognize and track specific targets in video\nsequences where the target objects are arbitrary and class-agnostic. The VOT\ntechnology could be applied in various scenarios, processing data of diverse\nmodalities such as RGB, thermal infrared and point cloud. Besides, since no one\nsensor could handle all the dynamic and varying environments, multi-modal VOT\nis also investigated. This paper presents a comprehensive survey of the recent\nprogress of both single-modal and multi-modal VOT, especially the deep learning\nmethods. Specifically, we first review three types of mainstream single-modal\nVOT, including RGB, thermal infrared and point cloud tracking. In particular,\nwe conclude four widely-used single-modal frameworks, abstracting their schemas\nand categorizing the existing inheritors. Then we summarize four kinds of\nmulti-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language.\nMoreover, the comparison results in plenty of VOT benchmarks of the discussed\nmodalities are presented. Finally, we provide recommendations and insightful\nobservations, inspiring the future development of this fast-growing literature.",
      "tldr_zh": "这篇论文对Visual Object Tracking (VOT)进行了全面综述，聚焦于在不同数据模态（如RGB、thermal infrared和point cloud）下跟踪任意目标对象的进展，特别是深度学习方法。论文首先回顾了单模态VOT的三大主流类型，并总结了四种广泛使用的框架，包括它们的结构和分类；随后探讨了四种多模态VOT，如RGB-Depth、RGB-Thermal、RGB-LiDAR和RGB-Language。最终，通过比较各种VOT基准测试的结果，提供见解和推荐，以指导该领域的未来发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09991v1",
      "published_date": "2024-12-13 09:25:18 UTC",
      "updated_date": "2024-12-13 09:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:22:58.520918"
    },
    {
      "arxiv_id": "2412.09990v1",
      "title": "Small Language Model as Data Prospector for Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Shiwen Ni",
        "Haihong Wu",
        "Di Yang",
        "Qiang Qu",
        "Hamid Alinejad-Rokny",
        "Min Yang"
      ],
      "abstract": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption.",
      "tldr_zh": "这篇论文提出 SuperNUGGETS，一种改进的框架，用于筛选高质量指令数据以提升 Large Language Models (LLMs) 的微调性能。SuperNUGGETS 利用 Small Language Model (SLM) 代替 LLM 来过滤出有效的 one-shot instances，并优化预定义的测试集，从而显著提高效率。实验结果显示，与原有 NUGGETS 相比，SuperNUGGETS 的性能仅下降 1-2%，但效率提升 58 倍，并降低资源消耗，从而具有更高的实用价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09990v1",
      "published_date": "2024-12-13 09:23:58 UTC",
      "updated_date": "2024-12-13 09:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:23:11.012761"
    },
    {
      "arxiv_id": "2412.09989v1",
      "title": "One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation in Unknown Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Albert Lin",
        "Shuang Peng",
        "Somil Bansal"
      ],
      "abstract": "As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website.",
      "tldr_zh": "该论文提出了一种observation-conditioned reachability-based (OCR) safety-filter 框架，用于确保四足机器人（quadruped）在未知环境中的鲁棒导航，该框架能高效适应不同控制器和环境，而无需先验知识。核心方法是利用OCR value network (OCR-VN)，通过LiDAR-based 输入动态构建安全区域，并结合disturbance estimation 模块处理动态不确定性，从而预测最优的control-theoretic safety value function。基于此，框架构建了一个adaptive safety filter，能够在必要时覆盖nominal quadruped controller 以维护系统安全。在模拟和硬件实验（如Unitree Go1 quadruped）中，该方法证明了对多种hierarchical quadruped controllers 的自动保护、环境适应性和对unmodeled dynamics 的鲁棒性，实现“One Filter to Deploy Them All”的目标。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website:\n  https://sia-lab-git.github.io/One_Filter_to_Deploy_Them_All/",
      "pdf_url": "http://arxiv.org/pdf/2412.09989v1",
      "published_date": "2024-12-13 09:21:02 UTC",
      "updated_date": "2024-12-13 09:21:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:23:24.529799"
    },
    {
      "arxiv_id": "2412.12183v1",
      "title": "Adopting Explainable-AI to investigate the impact of urban morphology design on energy and environmental performance in dry-arid climates",
      "title_zh": "翻译失败",
      "authors": [
        "Pegah Eshraghi",
        "Riccardo Talami",
        "Arman Nikkhah Dehnavi",
        "Maedeh Mirdamadi",
        "Zahra-Sadat Zomorodian"
      ],
      "abstract": "In rapidly urbanizing regions, designing climate-responsive urban forms is\ncrucial for sustainable development, especially in dry arid-climates where\nurban morphology has a significant impact on energy consumption and\nenvironmental performance. This study advances urban morphology evaluation by\ncombining Urban Building Energy Modeling (UBEM) with machine learning methods\n(ML) and Explainable AI techniques, specifically Shapley Additive Explanations\n(SHAP). Using Tehran's dense urban landscape as a case study, this research\nassesses and ranks the impact of 30 morphology parameters at the urban block\nlevel on key energy metrics (cooling, heating, and lighting demand) and\nenvironmental performance (sunlight exposure, photovoltaic generation, and Sky\nView Factor). Among seven ML algorithms evaluated, the XGBoost model was the\nmost effective predictor, achieving high accuracy (R2: 0.92) and a training\ntime of 3.64 seconds. Findings reveal that building shape, window-to-wall\nratio, and commercial ratio are the most critical parameters affecting energy\nefficiency, while the heights and distances of neighboring buildings strongly\ninfluence cooling demand and solar access. By evaluating urban blocks with\nvaried densities and configurations, this study offers generalizable insights\napplicable to other dry-arid regions. Moreover, the integration of UBEM and\nExplainable AI offers a scalable, data-driven framework for developing\nclimate-responsive urban designs adaptable to high-density environments\nworldwide.",
      "tldr_zh": "本研究采用 Explainable AI 技术（如 Shapley Additive Explanations, SHAP）和机器学习方法（如 XGBoost），结合 Urban Building Energy Modeling (UBEM)，调查干旱气候下城市形态设计对能源消耗和环境性能的影响。针对德黑兰的城市景观，该研究评估了30个形态参数对冷却、加热、照明需求以及阳光暴露、光伏发电和 Sky View Factor 的影响，发现建筑形状、窗墙比和商业比例是最关键的能源效率因素，而邻近建筑的高度和距离显著影响冷却需求和太阳能访问。实验结果显示，XGBoost 模型表现出高准确率（R2: 0.92）和高效训练时间，为开发适用于其他干旱地区的高密度、气候响应型城市设计提供了一个可扩展的数据驱动框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12183v1",
      "published_date": "2024-12-13 09:19:49 UTC",
      "updated_date": "2024-12-13 09:19:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:23:36.799558"
    },
    {
      "arxiv_id": "2412.09988v1",
      "title": "AI and the Future of Digital Public Squares",
      "title_zh": "翻译失败",
      "authors": [
        "Beth Goldberg",
        "Diana Acosta-Navas",
        "Michiel Bakker",
        "Ian Beacock",
        "Matt Botvinick",
        "Prateek Buch",
        "Renée DiResta",
        "Nandika Donthi",
        "Nathanael Fast",
        "Ravi Iyer",
        "Zaria Jalan",
        "Andrew Konya",
        "Grace Kwak Danciu",
        "Hélène Landemore",
        "Alice Marwick",
        "Carl Miller",
        "Aviv Ovadya",
        "Emily Saltz",
        "Lisa Schirch",
        "Dalit Shalom",
        "Divya Siddarth",
        "Felix Sieker",
        "Christopher Small",
        "Jonathan Stray",
        "Audrey Tang",
        "Michael Henry Tessler",
        "Amy Zhang"
      ],
      "abstract": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI.",
      "tldr_zh": "本文探讨了大型语言模型 (LLMs) 作为互联网之后的又一重大技术进步，如何重塑数字公共广场 (digital public squares)，提供机会促进去中心化、参与式的在线对话，但也可能加剧社会分裂。论文分析了 LLMs 在四个方面的应用：集体对话系统 (collective dialogue systems)、桥梁系统 (bridging systems)、社区调节 (community moderation) 和人类证明系统 (proof-of-humanity systems)，并基于70多位专家的输入，评估了其潜在益处和风险。最终，该研究提出未来 AI 研究和投资议程，以增强数字公共广场的功能并防范 AI 误用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "40 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09988v1",
      "published_date": "2024-12-13 09:15:20 UTC",
      "updated_date": "2024-12-13 09:15:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:23:47.378878"
    },
    {
      "arxiv_id": "2412.09981v2",
      "title": "SUMI-IFL: An Information-Theoretic Framework for Image Forgery Localization with Sufficiency and Minimality Constraints",
      "title_zh": "SUMI-IFL：一种具有充分",
      "authors": [
        "Ziqi Sheng",
        "Wei Lu",
        "Xiangyang Luo",
        "Jiantao Zhou",
        "Xiaochun Cao"
      ],
      "abstract": "Image forgery localization (IFL) is a crucial technique for preventing\ntampered image misuse and protecting social safety. However, due to the rapid\ndevelopment of image tampering technologies, extracting more comprehensive and\naccurate forgery clues remains an urgent challenge. To address these\nchallenges, we introduce a novel information-theoretic IFL framework named\nSUMI-IFL that imposes sufficiency-view and minimality-view constraints on\nforgery feature representation. First, grounded in the theoretical analysis of\nmutual information, the sufficiency-view constraint is enforced on the feature\nextraction network to ensure that the latent forgery feature contains\ncomprehensive forgery clues. Considering that forgery clues obtained from a\nsingle aspect alone may be incomplete, we construct the latent forgery feature\nby integrating several individual forgery features from multiple perspectives.\nSecond, based on the information bottleneck, the minimality-view constraint is\nimposed on the feature reasoning network to achieve an accurate and concise\nforgery feature representation that counters the interference of task-unrelated\nfeatures. Extensive experiments show the superior performance of SUMI-IFL to\nexisting state-of-the-art methods, not only on in-dataset comparisons but also\non cross-dataset comparisons.",
      "tldr_zh": "该论文提出 SUMI-IFL，一种基于信息理论的框架，用于图像篡改定位 (IFL)，通过引入 sufficiency-view 和 minimality-view 约束来解决提取全面准确篡改线索的挑战。基于 mutual information 的 sufficiency-view 约束确保特征提取网络捕获完整的篡改线索，并通过整合多视角特征来提升全面性；基于 information bottleneck 的 minimality-view 约束则使特征推理网络产生简洁准确的表示，减少无关干扰。实验结果显示，SUMI-IFL 在数据集内和跨数据集比较中均优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09981v2",
      "published_date": "2024-12-13 09:08:02 UTC",
      "updated_date": "2024-12-27 05:58:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:23:59.230724"
    },
    {
      "arxiv_id": "2412.09972v2",
      "title": "Efficient Large-Scale Traffic Forecasting with Transformers: A Spatial Data Management Perspective",
      "title_zh": "基于 Transformer 的高效大规模交通预测：一种空间数据管理视角",
      "authors": [
        "Yuchen Fang",
        "Yuxuan Liang",
        "Bo Hui",
        "Zezhi Shao",
        "Liwei Deng",
        "Xu Liu",
        "Xinke Jiang",
        "Kai Zheng"
      ],
      "abstract": "Road traffic forecasting is crucial in real-world intelligent transportation\nscenarios like traffic dispatching and path planning in city management and\npersonal traveling. Spatio-temporal graph neural networks (STGNNs) stand out as\nthe mainstream solution in this task. Nevertheless, the quadratic complexity of\nremarkable dynamic spatial modeling-based STGNNs has become the bottleneck over\nlarge-scale traffic data. From the spatial data management perspective, we\npresent a novel Transformer framework called PatchSTG to efficiently and\ndynamically model spatial dependencies for large-scale traffic forecasting with\ninterpretability and fidelity. Specifically, we design a novel irregular\nspatial patching to reduce the number of points involved in the dynamic\ncalculation of Transformer. The irregular spatial patching first utilizes the\nleaf K-dimensional tree (KDTree) to recursively partition irregularly\ndistributed traffic points into leaf nodes with a small capacity, and then\nmerges leaf nodes belonging to the same subtree into occupancy-equaled and\nnon-overlapped patches through padding and backtracking. Based on the patched\ndata, depth and breadth attention are used interchangeably in the encoder to\ndynamically learn local and global spatial knowledge from points in a patch and\npoints with the same index of patches. Experimental results on four real world\nlarge-scale traffic datasets show that our PatchSTG achieves train speed and\nmemory utilization improvements up to $10\\times$ and $4\\times$ with the\nstate-of-the-art performance.",
      "tldr_zh": "该研究从空间数据管理角度出发，提出了一种高效的Transformer框架PatchSTG，用于大规模交通预测，以解决时空图神经网络(STGNNs)中动态空间建模的二次复杂度瓶颈。PatchSTG通过不规则空间patching技术，利用叶K-维树(KDTree)递归分区交通点，并将叶节点合并成大小相等、非重叠的patches，从而减少动态计算中的点数。基于patched数据，框架在编码器中交替使用深度和广度注意力，动态学习局部和全局空间知识。实验结果显示，在四个真实世界大型交通数据集上，PatchSTG的训练速度和内存利用率分别最高提升10倍和4倍，同时保持了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by SIGKDD 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09972v2",
      "published_date": "2024-12-13 08:59:18 UTC",
      "updated_date": "2024-12-31 03:52:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:26:03.741973"
    },
    {
      "arxiv_id": "2412.09966v1",
      "title": "EP-CFG: Energy-Preserving Classifier-Free Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Zhang",
        "Fujun Luan",
        "Sai Bi",
        "Jianming Zhang"
      ],
      "abstract": "Classifier-free guidance (CFG) is widely used in diffusion models but often\nintroduces over-contrast and over-saturation artifacts at higher guidance\nstrengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),\nwhich addresses these issues by preserving the energy distribution of the\nconditional prediction during the guidance process. Our method simply rescales\nthe energy of the guided output to match that of the conditional prediction at\neach denoising step, with an optional robust variant for improved artifact\nsuppression. Through experiments, we show that EP-CFG maintains natural image\nquality and preserves details across guidance strengths while retaining CFG's\nsemantic alignment benefits, all with minimal computational overhead.",
      "tldr_zh": "本文提出 EP-CFG（Energy-Preserving Classifier-Free Guidance）方法，旨在解决扩散模型中 Classifier-free guidance (CFG) 在高指导强度下导致的过度对比和过度饱和伪像问题。该方法通过在每个去噪步骤中重新缩放指导输出的能量，使其匹配条件预测的能量分布，并提供一个可选的鲁棒变体以进一步抑制伪像。实验结果表明，EP-CFG 能保持图像的自然质量和细节，同时保留 CFG 的语义对齐优势，且仅需最小计算开销。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09966v1",
      "published_date": "2024-12-13 08:49:25 UTC",
      "updated_date": "2024-12-13 08:49:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:24:22.611581"
    },
    {
      "arxiv_id": "2412.09961v2",
      "title": "What constitutes a Deep Fake? The blurry line between legitimate processing and manipulation under the EU AI Act",
      "title_zh": "什么构成了 Deep Fake？在 EU AI Act 下，合法处理与操纵之间的模糊界限",
      "authors": [
        "Kristof Meding",
        "Christoph Sorge"
      ],
      "abstract": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations.",
      "tldr_zh": "该论文探讨了在欧盟 AI Act 下，Deep Fake 的定义及其与合法图像处理之间的模糊界限，强调了合成图像透明度规定的不足。通过分析数字照片从相机传感器到数字编辑的生命周期，作者发现：Deep Fake 在 AI Act 中的定义过于宽泛，导致识别标准不明确；编辑功能如 Google's \"best take\" 是否免除透明义务存在疑问；以及“实质编辑”的界定是否需为人眼可感知等问题。这些发现表明，提供者和部署者难以遵守当前的透明义务，可能导致例外规定过于宽泛或严格，论文旨在促进对 Deep Fake 定义的讨论并提高对 AI Act 缺陷的认识。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Accepted at ACM CS&Law '25",
      "pdf_url": "http://arxiv.org/pdf/2412.09961v2",
      "published_date": "2024-12-13 08:42:19 UTC",
      "updated_date": "2025-02-04 09:15:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:06.047733"
    },
    {
      "arxiv_id": "2412.15240v1",
      "title": "ChainStream: An LLM-based Framework for Unified Synthetic Sensing",
      "title_zh": "ChainStream: 基于LLM的框架",
      "authors": [
        "Jiacheng Liu",
        "Yuanchun Li",
        "Liangyan Li",
        "Yi Sun",
        "Hao Wen",
        "Xiangyu Li",
        "Yao Guo",
        "Yunxin Liu"
      ],
      "abstract": "Many applications demand context sensing to offer personalized and timely\nservices. Yet, developing sensing programs can be challenging for developers\nand using them is privacy-concerning for end-users. In this paper, we propose\nto use natural language as the unified interface to process personal data and\nsense user context, which can effectively ease app development and make the\ndata pipeline more transparent. Our work is inspired by large language models\n(LLMs) and other generative models, while directly applying them does not solve\nthe problem - letting the model directly process the data cannot handle complex\nsensing requests and letting the model write the data processing program\nsuffers error-prone code generation. We address the problem with 1) a unified\ndata processing framework that makes context-sensing programs simpler and 2) a\nfeedback-guided query optimizer that makes data query more informative. To\nevaluate the performance of natural language-based context sensing, we create a\nbenchmark that contains 133 context sensing tasks. Extensive evaluation has\nshown that our approach is able to automatically solve the context-sensing\ntasks efficiently and precisely. The code is opensourced at\nhttps://github.com/MobileLLM/ChainStream.",
      "tldr_zh": "该论文提出ChainStream，一种基于大型语言模型(LLMs)的框架，用于统一合成感知，以简化上下文感知应用的开发并提升数据管道的透明度，从而解决开发者挑战和用户隐私问题。该框架包括一个统一的处理框架和一个反馈引导的查询优化器，能够高效处理复杂感知请求，避免直接使用LLMs的局限性。研究者创建了一个包含133个任务的基准测试，并通过广泛评估证明，该方法能精确自动解决上下文感知任务。代码已在GitHub开源，提供实际实现基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.15240v1",
      "published_date": "2024-12-13 08:25:26 UTC",
      "updated_date": "2024-12-13 08:25:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:26:26.720838"
    },
    {
      "arxiv_id": "2412.09946v1",
      "title": "Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Qiao Sun",
        "Jiexin Xie",
        "Nanyang Ye",
        "Qinying Gu",
        "Shijie Guo"
      ],
      "abstract": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在护理和老年护理中的应用，提出一个AI驱动框架，专注于患者监测和互动。研究者引入了一个新的中文护理数据集，并采用增量预训练（IPT）和监督微调（SFT）技术来提升LLMs在专业任务中的性能，利用LangChain开发了一个动态护理助手，支持实时护理和个性化干预。实验结果显示了显著改进，为AI解决方案应对老龄化人口的医疗需求提供了重要途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09946v1",
      "published_date": "2024-12-13 08:10:56 UTC",
      "updated_date": "2024-12-13 08:10:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:26:38.992587"
    },
    {
      "arxiv_id": "2412.09919v1",
      "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuqiang Lu",
        "Zhenfei Yin",
        "Mengwei He",
        "Zhihui Wang",
        "Zicheng Liu",
        "Zhiyong Wang",
        "Kun Hu"
      ],
      "abstract": "Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.",
      "tldr_zh": "最近，Vision Large Language Models (VLLMs) 在视频理解中面临视觉 tokens 数量急剧增加的问题，导致上下文窗口超限和计算负担加重。论文提出 B-VLLM，一种新型框架，通过文本条件适应性帧选择模块选择任务相关帧，并结合时间帧 tokens 合并技术去除冗余，同时使用空间 tokens 采样和合并策略来平衡时空 cues。相比现有方法，B-VLLM 有效控制 tokens 数量，同时保留关键空间和时间细节。实验结果显示，该框架在多种视频理解基准上表现出优越性能，代码已在 GitHub 上发布。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09919v1",
      "published_date": "2024-12-13 07:13:40 UTC",
      "updated_date": "2024-12-13 07:13:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:26:51.642886"
    },
    {
      "arxiv_id": "2412.09902v2",
      "title": "One Node One Model: Featuring the Missing-Half for Graph Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanting Xie",
        "Bingheng Li",
        "Erlin Pan",
        "Zhaochen Guo",
        "Zhao Kang",
        "Wenyu Chen"
      ],
      "abstract": "Most existing graph clustering methods primarily focus on exploiting\ntopological structure, often neglecting the ``missing-half\" node feature\ninformation, especially how these features can enhance clustering performance.\nThis issue is further compounded by the challenges associated with\nhigh-dimensional features. Feature selection in graph clustering is\nparticularly difficult because it requires simultaneously discovering clusters\nand identifying the relevant features for these clusters. To address this gap,\nwe introduce a novel paradigm called ``one node one model\", which builds an\nexclusive model for each node and defines the node label as a combination of\npredictions for node groups. Specifically, the proposed ``Feature Personalized\nGraph Clustering (FPGC)\" method identifies cluster-relevant features for each\nnode using a squeeze-and-excitation block, integrating these features into each\nmodel to form the final representations. Additionally, the concept of feature\ncross is developed as a data augmentation technique to learn low-order feature\ninteractions. Extensive experimental results demonstrate that FPGC outperforms\nstate-of-the-art clustering methods. Moreover, the plug-and-play nature of our\nmethod provides a versatile solution to enhance GNN-based models from a feature\nperspective.",
      "tldr_zh": "该研究指出，现有的图聚类方法主要关注拓扑结构，而忽略了节点特征信息（missing-half），尤其是在高维特征下的挑战，导致聚类性能受限。为解决此问题，提出“one node one model”新范式，并开发了Feature Personalized Graph Clustering (FPGC)方法，该方法为每个节点构建专属模型，使用squeeze-and-excitation block识别相关特征，并通过feature cross作为数据增强技术学习低阶特征交互。实验结果显示，FPGC优于最先进聚类方法，且其plug-and-play特性可灵活增强基于GNN的模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09902v2",
      "published_date": "2024-12-13 06:42:36 UTC",
      "updated_date": "2024-12-18 04:51:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:03.506105"
    },
    {
      "arxiv_id": "2412.09896v2",
      "title": "Analyzing Fairness of Classification Machine Learning Model with Structured Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Rashed",
        "Abdelkrim Kallich",
        "Mohamed Eltayeb"
      ],
      "abstract": "Machine learning (ML) algorithms have become integral to decision making in\nvarious domains, including healthcare, finance, education, and law enforcement.\nHowever, concerns about fairness and bias in these systems pose significant\nethical and social challenges. This study investigates the fairness of ML\nmodels applied to structured datasets in classification tasks, highlighting the\npotential for biased predictions to perpetuate systemic inequalities. A\npublicly available dataset from Kaggle was selected for analysis, offering a\nrealistic scenario for evaluating fairness in machine learning workflows.\n  To assess and mitigate biases, three prominent fairness libraries; Fairlearn\nby Microsoft, AIF360 by IBM, and the What If Tool by Google were employed.\nThese libraries provide robust frameworks for analyzing fairness, offering\ntools to evaluate metrics, visualize results, and implement bias mitigation\nstrategies. The research aims to assess the extent of bias in the ML models,\ncompare the effectiveness of these libraries, and derive actionable insights\nfor practitioners.\n  The findings reveal that each library has unique strengths and limitations in\nfairness evaluation and mitigation. By systematically comparing their\ncapabilities, this study contributes to the growing field of ML fairness by\nproviding practical guidance for integrating fairness tools into real world\napplications. These insights are intended to support the development of more\nequitable machine learning systems.",
      "tldr_zh": "这篇论文分析了机器学习(ML)模型在分类任务中对结构化数据集的公平性问题，强调偏见可能加剧系统性不平等，并使用Kaggle的公开数据集进行真实场景评估。研究采用Fairlearn (Microsoft)、AIF360 (IBM) 和 What If Tool (Google) 等公平性库来评估偏见指标、可视化结果并实施缓解策略。结果显示，每个库都有独特优势和局限性，通过系统比较，该研究为ML从业者提供了实际指导，以开发更公平的机器学习系统。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.09896v2",
      "published_date": "2024-12-13 06:31:09 UTC",
      "updated_date": "2024-12-17 00:57:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:15.279056"
    },
    {
      "arxiv_id": "2412.09889v1",
      "title": "Semi-Periodic Activation for Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "José Gilberto Barbosa de Medeiros Júnior",
        "Andre Guarnier de Mitri",
        "Diego Furtado Silva"
      ],
      "abstract": "This paper investigates the lack of research on activation functions for\nneural network models in time series tasks. It highlights the need to identify\nessential properties of these activations to improve their effectiveness in\nspecific domains. To this end, the study comprehensively analyzes properties,\nsuch as bounded, monotonic, nonlinearity, and periodicity, for activation in\ntime series neural networks. We propose a new activation that maximizes the\ncoverage of these properties, called LeakySineLU. We empirically evaluate the\nLeakySineLU against commonly used activations in the literature using 112\nbenchmark datasets for time series classification, obtaining the best average\nranking in all comparative scenarios.",
      "tldr_zh": "本论文探讨了激活函数(activation functions)在时间序列(time series)任务中的研究不足，强调识别关键属性如 bounded、monotonic、非linearity 和 periodicity，以提升其在特定领域的有效性。作者提出了一种新激活函数LeakySineLU，该函数旨在最大化这些属性的覆盖范围。实验结果显示，在使用112个基准数据集进行时间序列分类(time series classification)时，LeakySineLU在所有比较场景中取得了最佳的平均排名。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09889v1",
      "published_date": "2024-12-13 06:06:49 UTC",
      "updated_date": "2024-12-13 06:06:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:26.715959"
    },
    {
      "arxiv_id": "2412.09887v2",
      "title": "CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls",
      "title_zh": "翻译失败",
      "authors": [
        "Li Chai",
        "Donglin Wang"
      ],
      "abstract": "Lyric-to-melody generation is a highly challenging task in the field of AI\nmusic generation. Due to the difficulty of learning strict yet weak\ncorrelations between lyrics and melodies, previous methods have suffered from\nweak controllability, low-quality and poorly structured generation. To address\nthese challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody\ngeneration method based on an in-attention Transformer decoder with\nfine-grained lyric and musical controls, which is able to generate full-song\nmelodies matched with the given lyrics and user-specified musical attributes.\nSpecifically, we first introduce REMI-Aligned, a novel music representation\nthat incorporates strict syllable- and sentence-level alignments between lyrics\nand melodies, facilitating precise alignment modeling. Subsequently,\nsentence-level semantic lyric embeddings independently extracted from a\nsentence-wise Transformer encoder are combined with word-level part-of-speech\nembeddings and syllable-level tone embeddings as fine-grained controls to\nenhance the controllability of lyrics over melody generation. Then we introduce\nhuman-labeled musical tags, sentence-level statistical musical attributes, and\nlearned musical features extracted from a pre-trained VQ-VAE as coarse-grained,\nfine-grained and high-fidelity controls, respectively, to the generation\nprocess, thereby enabling user control over melody generation. Finally, an\nin-attention Transformer decoder technique is leveraged to exert fine-grained\ncontrol over the full-song melody generation with the aforementioned lyric and\nmusical conditions. Experimental results demonstrate that our proposed CSL-L2M\noutperforms the state-of-the-art models, generating melodies with higher\nquality, better controllability and enhanced structure. Demos and source code\nare available at https://lichaiustc.github.io/CSL-L2M/.",
      "tldr_zh": "本研究提出 CSL-L2M，一种基于条件 Transformer 的可控歌词到旋律生成方法，旨在解决现有模型在歌词与旋律相关性学习上的弱可控性、低质量和结构问题。CSL-L2M 引入 REMI-Aligned 音乐表示以实现音节级和句子级对齐，并使用细粒度控制如句子级语义嵌入、词级词性嵌入、音节级音调嵌入，以及粗细粒度音乐控制（如人工标记标签和从预训练 VQ-VAE 提取的特征），来增强用户对旋律生成的控制。实验结果显示，CSL-L2M 优于最先进模型，在旋律质量、可控性和结构方面均有显著提升，并提供了演示和源代码支持。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted at AAAI-25",
      "pdf_url": "http://arxiv.org/pdf/2412.09887v2",
      "published_date": "2024-12-13 06:05:53 UTC",
      "updated_date": "2025-01-15 02:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:39.949689"
    },
    {
      "arxiv_id": "2412.16187v2",
      "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing",
      "title_zh": "HashEvict：一种使用局部敏感哈希的预注意力 KV 缓存驱逐策略",
      "authors": [
        "Minghui Liu",
        "Tahseen Rabbani",
        "Tony O'Halloran",
        "Ananth Sankaralingam",
        "Mary-Anne Hartley",
        "Brian Gravelle",
        "Furong Huang",
        "Cornelia Fermüller",
        "Yiannis Aloimonos"
      ],
      "abstract": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
      "tldr_zh": "本研究提出HashEvict，一种基于Locality-Sensitive Hashing (LSH)的预注意力（pre-attention）策略，用于压缩Transformer模型中的KV Cache，以缓解其对GPU内存的占用。HashEvict通过计算二值化高斯投影的Hamming距离，快速识别与当前查询token余弦不相似的缓存token，并在每个解码步骤动态替换预计注意力分数最低的token，从而减少计算成本。与传统方法不同，该策略在注意力计算前就决定token保留。实验结果显示，HashEvict可将KV Cache压缩30%-70%，同时在推理、多选题、长上下文检索和总结任务中保持高性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DS",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.16187v2",
      "published_date": "2024-12-13 06:00:27 UTC",
      "updated_date": "2024-12-24 13:04:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:27:52.487560"
    },
    {
      "arxiv_id": "2412.10477v1",
      "title": "Benchmarking large language models for materials synthesis: the case of atomic layer deposition",
      "title_zh": "翻译失败",
      "authors": [
        "Angel Yanguas-Gil",
        "Matthew T. Dearing",
        "Jeffrey W. Elam",
        "Jessica C. Jones",
        "Sungjoon Kim",
        "Adnan Mohammad",
        "Chi Thang Nguyen",
        "Bratin Sengupta"
      ],
      "abstract": "In this work we introduce an open-ended question benchmark, ALDbench, to\nevaluate the performance of large language models (LLMs) in materials\nsynthesis, and in particular in the field of atomic layer deposition, a thin\nfilm growth technique used in energy applications and microelectronics. Our\nbenchmark comprises questions with a level of difficulty ranging from graduate\nlevel to domain expert current with the state of the art in the field. Human\nexperts reviewed the questions along the criteria of difficulty and\nspecificity, and the model responses along four different criteria: overall\nquality, specificity, relevance, and accuracy. We ran this benchmark on an\ninstance of OpenAI's GPT-4o. The responses from the model received a composite\nquality score of 3.7 on a 1 to 5 scale, consistent with a passing grade.\nHowever, 36% of the questions received at least one below average score. An\nin-depth analysis of the responses identified at least five instances of\nsuspected hallucination. Finally, we observed statistically significant\ncorrelations between the difficulty of the question and the quality of the\nresponse, the difficulty of the question and the relevance of the response, and\nthe specificity of the question and the accuracy of the response as graded by\nthe human experts. This emphasizes the need to evaluate LLMs across multiple\ncriteria beyond difficulty or accuracy.",
      "tldr_zh": "这篇论文引入了 ALDbench 基准，用于评估大型语言模型 (LLMs) 在材料合成领域的性能，特别是 atomic layer deposition (ALD) 技术。该基准包含从研究生水平到领域专家水平的开放式问题，并由人类专家根据难度、具体性评估问题，以及根据整体质量、具体性、相关性和准确性评估模型响应。在使用 OpenAI 的 GPT-4o 进行测试时，模型的综合质量得分为 3.7（满分 5），但 36% 的问题至少有一个低于平均分，并发现了至少五个幻觉实例。研究还观察到问题难度与响应质量及相关性显著相关，问题具体性与准确性相关，从而强调了评估 LLMs 时需考虑多重标准而非仅限难度或准确性。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10477v1",
      "published_date": "2024-12-13 05:10:29 UTC",
      "updated_date": "2024-12-13 05:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:28:04.417033"
    },
    {
      "arxiv_id": "2412.09860v1",
      "title": "Brain-inspired Chaotic Graph Backpropagation for Large-scale Combinatorial Optimization",
      "title_zh": "受脑启发的混沌图反向传播用于大规模组合优化",
      "authors": [
        "Peng Tao",
        "Kazuyuki Aihara",
        "Luonan Chen"
      ],
      "abstract": "Graph neural networks (GNNs) with unsupervised learning can solve large-scale\ncombinatorial optimization problems (COPs) with efficient time complexity,\nmaking them versatile for various applications. However, since this method maps\nthe combinatorial optimization problem to the training process of a graph\nneural network, and the current mainstream backpropagation-based training\nalgorithms are prone to fall into local minima, the optimization performance is\nstill inferior to the current state-of-the-art (SOTA) COP methods. To address\nthis issue, inspired by possibly chaotic dynamics of real brain learning, we\nintroduce a chaotic training algorithm, i.e. chaotic graph backpropagation\n(CGBP), which introduces a local loss function in GNN that makes the training\nprocess not only chaotic but also highly efficient. Different from existing\nmethods, we show that the global ergodicity and pseudo-randomness of such\nchaotic dynamics enable CGBP to learn each optimal GNN effectively and\nglobally, thus solving the COP efficiently. We have applied CGBP to solve\nvarious COPs, such as the maximum independent set, maximum cut, and graph\ncoloring. Results on several large-scale benchmark datasets showcase that CGBP\ncan outperform not only existing GNN algorithms but also SOTA methods. In\naddition to solving large-scale COPs, CGBP as a universal learning algorithm\nfor GNNs, i.e. as a plug-in unit, can be easily integrated into any existing\nmethod for improving the performance.",
      "tldr_zh": "该论文提出了一种受脑混沌动态启发的训练算法——Chaotic Graph Backpropagation (CGBP)，旨在解决Graph Neural Networks (GNNs)在处理大规模Combinatorial Optimization Problems (COPs)时容易陷入局部最优的问题。CGBP通过引入局部损失函数，使训练过程呈现混沌特性，同时利用其全局遍历性和伪随机性来高效学习最优GNN模型。实验结果显示，在最大独立集、最大割和图着色等COPs上，CGBP在多个大型基准数据集上超越了现有GNN算法和SOTA方法。作为一个通用插件，CGBP可轻松集成到其他GNN方法中，提升整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09860v1",
      "published_date": "2024-12-13 05:00:57 UTC",
      "updated_date": "2024-12-13 05:00:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:08.724910"
    },
    {
      "arxiv_id": "2412.09858v1",
      "title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning",
      "title_zh": "RLDG：基于强化学习的机器人通用策略蒸馏",
      "authors": [
        "Charles Xu",
        "Qiyang Li",
        "Jianlan Luo",
        "Sergey Levine"
      ],
      "abstract": "Recent advances in robotic foundation models have enabled the development of\ngeneralist policies that can adapt to diverse tasks. While these models show\nimpressive flexibility, their performance heavily depends on the quality of\ntheir training data. In this work, we propose Reinforcement Learning Distilled\nGeneralists (RLDG), a method that leverages reinforcement learning to generate\nhigh-quality training data for finetuning generalist policies. Through\nextensive real-world experiments on precise manipulation tasks like connector\ninsertion and assembly, we demonstrate that generalist policies trained with\nRL-generated data consistently outperform those trained with human\ndemonstrations, achieving up to 40% higher success rates while generalizing\nbetter to new tasks. We also provide a detailed analysis that reveals this\nperformance gain stems from both optimized action distributions and improved\nstate coverage. Our results suggest that combining task-specific RL with\ngeneralist policy distillation offers a promising approach for developing more\ncapable and efficient robotic manipulation systems that maintain the\nflexibility of foundation models while achieving the performance of specialized\ncontrollers. Videos and code can be found on our project website\nhttps://generalist-distillation.github.io",
      "tldr_zh": "本研究提出RLDG方法，通过Reinforcement Learning生成高质量训练数据，用于微调机器人通用策略，以提升其适应多样任务的能力。在真实世界实验中，如连接器插入和组装任务，RLDG训练的策略比使用人类演示的数据高出40%的成功率，并表现出更好的任务泛化。分析显示，这种性能提升源于优化的动作分布和改进的状态覆盖。总体而言，RLDG将任务特定的Reinforcement Learning与通用策略蒸馏相结合，为开发更高效、灵活的机器人操作系统提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09858v1",
      "published_date": "2024-12-13 04:57:55 UTC",
      "updated_date": "2024-12-13 04:57:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:28:27.212747"
    },
    {
      "arxiv_id": "2412.09856v1",
      "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
      "title_zh": "翻译失败",
      "authors": [
        "Hongjie Wang",
        "Chih-Yao Ma",
        "Yen-Cheng Liu",
        "Ji Hou",
        "Tao Xu",
        "Jialiang Wang",
        "Felix Juefei-Xu",
        "Yaqiao Luo",
        "Peizhao Zhang",
        "Tingbo Hou",
        "Peter Vajda",
        "Niraj K. Jha",
        "Xiaoliang Dai"
      ],
      "abstract": "Text-to-video generation enhances content creation but is highly\ncomputationally intensive: The computational cost of Diffusion Transformers\n(DiTs) scales quadratically in the number of pixels. This makes minute-length\nvideo generation extremely expensive, limiting most existing models to\ngenerating videos of only 10-20 seconds length. We propose a Linear-complexity\ntext-to-video Generation (LinGen) framework whose cost scales linearly in the\nnumber of pixels. For the first time, LinGen enables high-resolution\nminute-length video generation on a single GPU without compromising quality. It\nreplaces the computationally-dominant and quadratic-complexity block,\nself-attention, with a linear-complexity block called MATE, which consists of\nan MA-branch and a TE-branch. The MA-branch targets short-to-long-range\ncorrelations, combining a bidirectional Mamba2 block with our token\nrearrangement method, Rotary Major Scan, and our review tokens developed for\nlong video generation. The TE-branch is a novel TEmporal Swin Attention block\nthat focuses on temporal correlations between adjacent tokens and medium-range\ntokens. The MATE block addresses the adjacency preservation issue of Mamba and\nimproves the consistency of generated videos significantly. Experimental\nresults show that LinGen outperforms DiT (with a 75.6% win rate) in video\nquality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.\nFurthermore, both automatic metrics and human evaluation demonstrate our\nLinGen-4B yields comparable video quality to state-of-the-art models (with a\n50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,\nrespectively). This paves the way to hour-length movie generation and real-time\ninteractive video generation. We provide 68s video generation results and more\nexamples in our project website: https://lineargen.github.io/.",
      "tldr_zh": "这篇论文提出了LinGen框架，用于实现高分辨率、分钟级文本到视频生成，其计算复杂度线性增长，解决了传统Diffusion Transformers (DiTs)因平方复杂度而导致的计算密集问题。LinGen通过替换自注意力机制为MATE块，包括处理短到长距离相关性的MA-branch（结合Mamba2块、Rotary Major Scan和review tokens）和关注时序相关性的TE-branch（TEmporal Swin Attention块），从而提升视频生成的一致性和效率。实验结果显示，LinGen在视频质量上优于DiT（胜率75.6%），并与Gen-3、LumaLabs和Kling等SOTA模型相当，同时减少15倍FLOPs和11.5倍延迟，为小时级电影生成和实时交互视频铺平道路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09856v1",
      "published_date": "2024-12-13 04:55:10 UTC",
      "updated_date": "2024-12-13 04:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:28:41.216198"
    },
    {
      "arxiv_id": "2412.15239v2",
      "title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Hortense Fong",
        "George Gui"
      ],
      "abstract": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries.",
      "tldr_zh": "这篇论文提出一个新框架，使用大型语言模型 (LLMs) 来模型观众对故事发展的预期，从而更好地理解消费者参与行为。该框架通过为每个故事生成多个潜在续集，并提取与预期、不确定性和惊喜相关的特征，补充了现有内容特征提取技术。在超过 30,000 个书章节的实验中，结果显示该方法平均将现有技术的解释力提升 31%，并揭示不同参与类型（如继续阅读、评论和投票）由当前和预期内容特征的独特组合驱动。该框架为研究观众前瞻性信念如何影响叙事媒体参与提供了新途径，并对内容行业的营销策略有重要启发。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "econ.GN",
        "q-fin.EC",
        "stat.ME",
        "68T50, 91F20",
        "H.3.1; I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.15239v2",
      "published_date": "2024-12-13 04:53:34 UTC",
      "updated_date": "2025-03-26 18:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:28:52.074240"
    },
    {
      "arxiv_id": "2412.09849v1",
      "title": "Deep Learning for Spectrum Prediction in Cognitive Radio Networks: State-of-the-Art, New Opportunities, and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Guangliang Pan",
        "David K. Y. Yau",
        "Bo Zhou",
        "Qihui Wu"
      ],
      "abstract": "Spectrum prediction is considered to be a promising technology that enhances\nspectrum efficiency by assisting dynamic spectrum access (DSA) in cognitive\nradio networks (CRN). Nonetheless, the highly nonlinear nature of spectrum data\nacross time, frequency, and space domains, coupled with the intricate spectrum\nusage patterns, poses challenges for accurate spectrum prediction. Deep\nlearning (DL), recognized for its capacity to extract nonlinear features, has\nbeen applied to solve these challenges. This paper first shows the advantages\nof applying DL by comparing with traditional prediction methods. Then, the\ncurrent state-of-the-art DL-based spectrum prediction techniques are reviewed\nand summarized in terms of intra-band and crossband prediction. Notably, this\npaper uses a real-world spectrum dataset to prove the advancements of DL-based\nmethods. Then, this paper proposes a novel intra-band spatiotemporal spectrum\nprediction framework named ViTransLSTM. This framework integrates visual\nself-attention and long short-term memory to capture both local and global\nlong-term spatiotemporal dependencies of spectrum usage patterns. Similarly,\nthe effectiveness of the proposed framework is validated on the aforementioned\nreal-world dataset. Finally, the paper presents new related challenges and\npotential opportunities for future research.",
      "tldr_zh": "这篇论文探讨了深度学习（DL）在认知无线电网络（CRN）中的频谱预测应用，强调了 DL 在处理频谱数据非线性挑战方面的优势，并通过与传统方法的比较证明了其提升频谱效率的作用。论文回顾了现有的 intra-band 和 crossband 预测技术，并使用真实数据集验证了 DL 方法的先进性。作者提出了一种新型框架 ViTransLSTM，该框架整合 visual self-attention 和 long short-term memory (LSTM) 来捕捉频谱使用模式的局部和全局时空依赖，并在实验中证明了其有效性。最后，论文指出了未来研究的潜在机会和挑战，如进一步优化模型和扩展应用场景。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09849v1",
      "published_date": "2024-12-13 04:36:05 UTC",
      "updated_date": "2024-12-13 04:36:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:29:04.550907"
    },
    {
      "arxiv_id": "2412.09843v1",
      "title": "Learning Structural Causal Models from Ordering: Identifiable Flow Models",
      "title_zh": "基于排序学习结构因果模型：可识别的流模型",
      "authors": [
        "Minh Khoa Le",
        "Kien Do",
        "Truyen Tran"
      ],
      "abstract": "In this study, we address causal inference when only observational data and a\nvalid causal ordering from the causal graph are available. We introduce a set\nof flow models that can recover component-wise, invertible transformation of\nexogenous variables. Our flow-based methods offer flexible model design while\nmaintaining causal consistency regardless of the number of discretization\nsteps. We propose design improvements that enable simultaneous learning of all\ncausal mechanisms and reduce abduction and prediction complexity to linear O(n)\nrelative to the number of layers, independent of the number of causal\nvariables. Empirically, we demonstrate that our method outperforms previous\nstate-of-the-art approaches and delivers consistent performance across a wide\nrange of structural causal models in answering observational, interventional,\nand counterfactual questions. Additionally, our method achieves a significant\nreduction in computational time compared to existing diffusion-based\ntechniques, making it practical for large structural causal models.",
      "tldr_zh": "本研究针对仅拥有观察数据和有效因果顺序的场景，提出一组可识别的 flow models，用于从因果顺序中学习 structural causal models，这些模型能恢复外生变量的组件-wise、invertible 变换。方法通过灵活的设计和设计改进，同时学习所有因果机制，并将 abduction 和 prediction 复杂度降低到线性 O(n)，与层数相关但独立于因果变量数量，从而确保因果一致性。实验结果显示，该方法在回答观察、干预和反事实问题上优于现有最先进方法，并在各种 structural causal models 上表现出稳定性能，同时显著减少计算时间，适用于大型模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09843v1",
      "published_date": "2024-12-13 04:25:56 UTC",
      "updated_date": "2024-12-13 04:25:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:29:15.609590"
    },
    {
      "arxiv_id": "2412.09826v1",
      "title": "Precise Antigen-Antibody Structure Predictions Enhance Antibody Development with HelixFold-Multimer",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Gao",
        "Jing Hu",
        "Lihang Liu",
        "Yang Xue",
        "Kunrui Zhu",
        "Xiaonan Zhang",
        "Xiaomin Fang"
      ],
      "abstract": "The accurate prediction of antigen-antibody structures is essential for\nadvancing immunology and therapeutic development, as it helps elucidate\nmolecular interactions that underlie immune responses. Despite recent progress\nwith deep learning models like AlphaFold and RoseTTAFold, accurately modeling\nantigen-antibody complexes remains a challenge due to their unique evolutionary\ncharacteristics. HelixFold-Multimer, a specialized model developed for this\npurpose, builds on the framework of AlphaFold-Multimer and demonstrates\nimproved precision for antigen-antibody structures. HelixFold-Multimer not only\nsurpasses other models in accuracy but also provides essential insights into\nantibody development, enabling more precise identification of binding sites,\nimproved interaction prediction, and enhanced design of therapeutic antibodies.\nThese advances underscore HelixFold-Multimer's potential in supporting antibody\nresearch and therapeutic innovation.",
      "tldr_zh": "该研究强调了准确预测抗原-antibody（Antigen-Antibody）结构对免疫学和治疗开发的重要性，但现有模型如AlphaFold和RoseTTAFold在处理这些结构的独特进化特性时面临挑战。研究引入了HelixFold-Multimer，一种基于AlphaFold-Multimer框架的专用模型，该模型在预测抗原-antibody复合物方面表现出更高的精度。HelixFold-Multimer不仅提升了结合位点的识别、交互预测的准确性，还促进了治疗性抗体的设计，从而为抗体研究和治疗创新提供有力支持。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09826v1",
      "published_date": "2024-12-13 03:36:23 UTC",
      "updated_date": "2024-12-13 03:36:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:29:27.442772"
    },
    {
      "arxiv_id": "2412.09818v3",
      "title": "MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yingxu He",
        "Zhuohan Liu",
        "Shuo Sun",
        "Bin Wang",
        "Wenyu Zhang",
        "Xunlong Zou",
        "Nancy F. Chen",
        "Ai Ti Aw"
      ],
      "abstract": "We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning\nin One Network), the first speech-text model tailored for Singapore's\nmultilingual and multicultural landscape. Developed under the National Large\nLanguage Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates\nadvanced speech and text processing to address the diverse linguistic nuances\nof local accents and dialects, enhancing accessibility and usability in\ncomplex, multilingual environments. Our results demonstrate improvements in\nboth speech recognition and task-specific understanding, positioning\nMERaLiON-AudioLLM as a pioneering solution for region specific AI applications.\nWe envision this release to set a precedent for future models designed to\naddress localised linguistic and cultural contexts in a global framework.",
      "tldr_zh": "我们引入了 MERaLiON-AudioLLM，这是一个针对新加坡多语言和多文化环境的语音-文本模型，使用 Large Language Models 整合先进的语音和文本处理技术，以处理本地口音和方言。\n该模型旨在提升在复杂多语言环境中的可访问性和可用性，并通过实验证明在语音识别和任务特定理解方面取得了显著改进。\n作为国家资助计划的一部分，MERaLiON-AudioLLM 为区域特定 AI 应用提供了先驱性解决方案，并为未来处理本地语言和文化语境的模型设定先例。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION",
      "pdf_url": "http://arxiv.org/pdf/2412.09818v3",
      "published_date": "2024-12-13 03:15:05 UTC",
      "updated_date": "2025-01-16 03:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:29:41.553068"
    },
    {
      "arxiv_id": "2412.09814v2",
      "title": "Federated Learning of Dynamic Bayesian Network via Continuous Optimization from Time Series Data",
      "title_zh": "翻译失败",
      "authors": [
        "Jianhong Chen",
        "Ying Ma",
        "Xubo Yue"
      ],
      "abstract": "Traditionally, learning the structure of a Dynamic Bayesian Network has been\ncentralized, requiring all data to be pooled in one location. However, in\nreal-world scenarios, data are often distributed across multiple entities\n(e.g., companies, devices) that seek to collaboratively learn a Dynamic\nBayesian Network while preserving data privacy and security. More importantly,\ndue to the presence of diverse clients, the data may follow different\ndistributions, resulting in data heterogeneity. This heterogeneity poses\nadditional challenges for centralized approaches. In this study, we first\nintroduce a federated learning approach for estimating the structure of a\nDynamic Bayesian Network from homogeneous time series data that are\nhorizontally distributed across different parties. We then extend this approach\nto heterogeneous time series data by incorporating a proximal operator as a\nregularization term in a personalized federated learning framework. To this\nend, we propose \\texttt{FDBNL} and \\texttt{PFDBNL}, which leverage continuous\noptimization, ensuring that only model parameters are exchanged during the\noptimization process. Experimental results on synthetic and real-world datasets\ndemonstrate that our method outperforms state-of-the-art techniques,\nparticularly in scenarios with many clients and limited individual sample\nsizes.",
      "tldr_zh": "本研究提出了一种联邦学习（Federated Learning）方法，用于从分布式时间序列数据中学习动态贝叶斯网络（Dynamic Bayesian Network, DBN）的结构，以解决数据隐私、安全和异质性挑战。针对同质数据，作者引入了 FDBNL 框架，利用连续优化（Continuous Optimization）仅交换模型参数；针对异质数据，则扩展为 PFDBNL，通过添加 proximal operator 正则化项实现个性化学习。实验结果显示，该方法在合成和真实数据集上优于现有技术，尤其在多客户端和样本规模有限的场景中，显著提升了性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09814v2",
      "published_date": "2024-12-13 03:09:35 UTC",
      "updated_date": "2025-02-05 19:35:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:29:51.599094"
    },
    {
      "arxiv_id": "2412.09805v1",
      "title": "Universal Inceptive GNNs by Eliminating the Smoothness-generalization Dilemma",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Gu",
        "Zhuonan Zheng",
        "Sheng Zhou",
        "Meihan Liu",
        "Jiawei Chen",
        "Tanyu Qiao",
        "Liangcheng Li",
        "Jiajun Bu"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in various\ndomains, such as transaction and social net-works. However, their application\nis often hindered by the varyinghomophily levels across different orders of\nneighboring nodes, ne-cessitating separate model designs for homophilic and\nheterophilicgraphs. In this paper, we aim to develop a unified framework\nca-pable of handling neighborhoods of various orders and homophilylevels.\nThrough theoretical exploration, we identify a previouslyoverlooked\narchitectural aspect in multi-hop learning: the cascadedependency, which leads\nto asmoothness-generalization dilemma.This dilemma significantly affects the\nlearning process, especiallyin the context of high-order neighborhoods and\nheterophilic graphs.To resolve this issue, we propose an Inceptive Graph Neural\nNet-work (IGNN), a universal message-passing framework that replacesthe cascade\ndependency with an inceptive architecture. IGNN pro-vides independent\nrepresentations for each hop, allowing personal-ized generalization\ncapabilities, and captures neighborhood-wiserelationships to select appropriate\nreceptive fields. Extensive ex-periments show that our IGNN outperforms 23\nbaseline methods,demonstrating superior performance on both homophilic and\nhet-erophilic graphs, while also scaling efficiently to large graphs.",
      "tldr_zh": "这项研究针对图神经网络(GNNs)在处理不同阶邻居节点同质性(homophily)水平时的挑战，提出了一种统一框架，以消除多跳学习中的级联依赖(cascaded dependency)导致的平滑性-泛化困境(smoothness-generalization dilemma)。作者设计了Inceptive Graph Neural Network (IGNN)，通过inceptive架构提供每个跳跃的独立表示，实现个性化的泛化能力和邻居级别的关系捕获，从而适应同质和异质(heterophilic)图。实验结果显示，IGNN在各种图上超过了23个基线方法，并能高效扩展到大型图。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.09805v1",
      "published_date": "2024-12-13 02:44:47 UTC",
      "updated_date": "2024-12-13 02:44:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:03.960879"
    },
    {
      "arxiv_id": "2412.09799v1",
      "title": "CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Qibo Chen",
        "Weizhong Jin",
        "Jianyue Ge",
        "Mengdi Liu",
        "Yuchao Yan",
        "Jian Jiang",
        "Li Yu",
        "Xuanjiang Guo",
        "Shuchang Li",
        "Jianzhong Chen"
      ],
      "abstract": "Recent research on universal object detection aims to introduce language in a\nSoTA closed-set detector and then generalize the open-set concepts by\nconstructing large-scale (text-region) datasets for training. However, these\nmethods face two main challenges: (i) how to efficiently use the prior\ninformation in the prompts to genericise objects and (ii) how to reduce\nalignment bias in the downstream tasks, both leading to sub-optimal performance\nin some scenarios beyond pre-training. To address these challenges, we propose\na strong universal detection foundation model called CP-DETR, which is\ncompetitive in almost all scenarios, with only one pre-training weight.\nSpecifically, we design an efficient prompt visual hybrid encoder that enhances\nthe information interaction between prompt and visual through scale-by-scale\nand multi-scale fusion modules. Then, the hybrid encoder is facilitated to\nfully utilize the prompted information by prompt multi-label loss and auxiliary\ndetection head. In addition to text prompts, we have designed two practical\nconcept prompt generation methods, visual prompt and optimized prompt, to\nextract abstract concepts through concrete visual examples and stably reduce\nalignment bias in downstream tasks. With these effective designs, CP-DETR\ndemonstrates superior universal detection performance in a broad spectrum of\nscenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on\nLVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35.\nFurthermore, our visual prompt generation method achieves 68.4 AP on COCO val\nby interactive detection, and the optimized prompt achieves 73.1 fully-shot AP\non ODinW13.",
      "tldr_zh": "本研究提出CP-DETR，一种强大的通用物体检测基础模型，旨在通过概念提示指导DETR解决现有方法的泛化问题和对齐偏差挑战。\nCP-DETR设计了高效的提示视觉混合编码器（包括逐尺度与多尺度融合模块）、提示多标签损失和辅助检测头，并引入视觉提示和优化提示生成方法，以从具体视觉示例中提取抽象概念并提升下游任务性能。\n实验结果显示，该模型在各种场景中表现出色，例如Swin-T骨干模型在LVIS上零样本AP达47.6，Swin-L骨干模型在ODinW35上零样本AP达32.2，并在COCO val和ODinW13上分别实现68.4和73.1 AP。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by AAAI2025",
      "pdf_url": "http://arxiv.org/pdf/2412.09799v1",
      "published_date": "2024-12-13 02:36:29 UTC",
      "updated_date": "2024-12-13 02:36:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:21.526844"
    },
    {
      "arxiv_id": "2412.10474v1",
      "title": "CrossVIT-augmented Geospatial-Intelligence Visualization System for Tracking Economic Development Dynamics",
      "title_zh": "CrossVIT 增强的地理空间情报可视化系统，用于跟踪经济发展的动态",
      "authors": [
        "Yanbing Bai",
        "Jinhua Su",
        "Bin Qiao",
        "Xiaoran Ma"
      ],
      "abstract": "Timely and accurate economic data is crucial for effective policymaking.\nCurrent challenges in data timeliness and spatial resolution can be addressed\nwith advancements in multimodal sensing and distributed computing. We introduce\nSenseconomic, a scalable system for tracking economic dynamics via multimodal\nimagery and deep learning. Built on the Transformer framework, it integrates\nremote sensing and street view images using cross-attention, with nighttime\nlight data as weak supervision. The system achieved an R-squared value of\n0.8363 in county-level economic predictions and halved processing time to 23\nminutes using distributed computing. Its user-friendly design includes a\nVue3-based front end with Baidu maps for visualization and a Python-based back\nend automating tasks like image downloads and preprocessing. Senseconomic\nempowers policymakers and researchers with efficient tools for resource\nallocation and economic planning.",
      "tldr_zh": "这篇论文介绍了 Senseconomic 系统，一个基于 Transformer 框架的地理空间智能可视化系统，用于通过多模态图像跟踪经济发展的动态，解决数据及时性和空间分辨率挑战。系统利用 cross-attention 机制整合遥感图像、街景图像和夜间灯光数据作为弱监督，并结合分布式 computing 优化处理流程。实验结果显示，该系统在县一级经济预测中达到 R-squared 值 0.8363，并将处理时间缩短至 23 分钟。Senseconomic 的用户友好设计，包括 Vue3-based 前端与 Baidu maps 整合，以及 Python-based 后端自动化任务，为决策者提供高效工具，支持资源分配和经济规划。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10474v1",
      "published_date": "2024-12-13 02:31:48 UTC",
      "updated_date": "2024-12-13 02:31:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:32.614099"
    },
    {
      "arxiv_id": "2412.09796v1",
      "title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Qiyao Wang",
        "Shiwen Ni",
        "Huaren Liu",
        "Shule Lu",
        "Guhong Chen",
        "Xi Feng",
        "Chi Wei",
        "Qiang Qu",
        "Hamid Alinejad-Rokny",
        "Yuan Lin",
        "Min Yang"
      ],
      "abstract": "As the capabilities of Large Language Models (LLMs) continue to advance, the\nfield of patent processing has garnered increased attention within the natural\nlanguage processing community. However, the majority of research has been\nconcentrated on classification tasks, such as patent categorization and\nexamination, or on short text generation tasks like patent summarization and\npatent quizzes. In this paper, we introduce a novel and practical task known as\nDraft2Patent, along with its corresponding D2P benchmark, which challenges LLMs\nto generate full-length patents averaging 17K tokens based on initial drafts.\nPatents present a significant challenge to LLMs due to their specialized\nnature, standardized terminology, and extensive length. We propose a\nmulti-agent framework called AutoPatent which leverages the LLM-based planner\nagent, writer agents, and examiner agent with PGTree and RRAG to generate\nlengthy, intricate, and high-quality complete patent documents. The\nexperimental results demonstrate that our AutoPatent framework significantly\nenhances the ability to generate comprehensive patents across various LLMs.\nFurthermore, we have discovered that patents generated solely with the\nAutoPatent framework based on the Qwen2.5-7B model outperform those produced by\nlarger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,\nin both objective metrics and human evaluations. We will make the data and code\navailable upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.",
      "tldr_zh": "本研究引入了Draft2Patent新任务及其D2P基准，挑战Large Language Models (LLMs)基于初始草稿生成完整的专利文档（平均17K tokens），以应对专利的专业性、标准化术语和长度的挑战。研究提出AutoPatent多智能体框架，包括LLM-based planner agent、writer agents和examiner agent，结合PGTree和RRAG技术，实现高质量的长文本专利生成。实验结果显示，AutoPatent显著提升了各种LLMs的专利生成能力，且基于Qwen2.5-7B模型的输出在客观指标和人类评估中优于更大模型如GPT-4o、Qwen2.5-72B和LLAMA3.1-70B。研究将数据和代码开源于GitHub。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.09796v1",
      "published_date": "2024-12-13 02:27:34 UTC",
      "updated_date": "2024-12-13 02:27:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:44.135874"
    },
    {
      "arxiv_id": "2412.12178v2",
      "title": "Activation Sparsity Opportunities for Compressing General Large Language Models",
      "title_zh": "激活稀疏性在压缩通用大型语言模型中的机会",
      "authors": [
        "Nobel Dhar",
        "Bobin Deng",
        "Md Romyull Islam",
        "Kazi Fahim Ahmad Nasif",
        "Liang Zhao",
        "Kun Suo"
      ],
      "abstract": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
      "tldr_zh": "本文探讨了通过激活稀疏性压缩一般大型语言模型 (LLMs) 的方法，以实现高效部署到边缘设备上，从而提升设备独立性、减轻服务器负担并降低响应时间。作者专注于 LLMs 的 Feed-Forward Network (FFN) 组件（占参数约2/3），通过注入零强制阈值强制激活稀疏性，与现有压缩技术正交结合，实验显示可实现约50%的内存和计算减少，同时准确性损失微小。该方法适用于各种 LLMs，而非仅限于 ReLU-based 模型，并为系统架构师提供预测和预取指导，以减少执行时间和资源消耗。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
      "pdf_url": "http://arxiv.org/pdf/2412.12178v2",
      "published_date": "2024-12-13 02:26:54 UTC",
      "updated_date": "2025-01-31 19:09:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:30:56.848831"
    },
    {
      "arxiv_id": "2412.09784v1",
      "title": "Semi-IIN: Semi-supervised Intra-inter modal Interaction Learning Network for Multimodal Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Jinhao Lin",
        "Yifei Wang",
        "Yanwu Xu",
        "Qi Liu"
      ],
      "abstract": "Despite multimodal sentiment analysis being a fertile research ground that\nmerits further investigation, current approaches take up high annotation cost\nand suffer from label ambiguity, non-amicable to high-quality labeled data\nacquisition. Furthermore, choosing the right interactions is essential because\nthe significance of intra- or inter-modal interactions can differ among various\nsamples. To this end, we propose Semi-IIN, a Semi-supervised Intra-inter modal\nInteraction learning Network for multimodal sentiment analysis. Semi-IIN\nintegrates masked attention and gating mechanisms, enabling effective dynamic\nselection after independently capturing intra- and inter-modal interactive\ninformation. Combined with the self-training approach, Semi-IIN fully utilizes\nthe knowledge learned from unlabeled data. Experimental results on two public\ndatasets, MOSI and MOSEI, demonstrate the effectiveness of Semi-IIN,\nestablishing a new state-of-the-art on several metrics. Code is available at\nhttps://github.com/flow-ljh/Semi-IIN.",
      "tldr_zh": "本文提出 Semi-IIN，一种半监督的多模态情感分析框架，旨在解决高标注成本、标签模糊以及 intra- 或 inter-modal 交互选择难题。Semi-IIN 整合 masked attention 和 gating mechanisms，实现对 intra- 和 inter-modal 交互信息的动态选择，并通过 self-training 方法充分利用未标注数据。在 MOSI 和 MOSEI 数据集上的实验中，Semi-IIN 取得了新的 state-of-the-art 性能，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.09784v1",
      "published_date": "2024-12-13 01:48:07 UTC",
      "updated_date": "2024-12-13 01:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:33:01.602257"
    },
    {
      "arxiv_id": "2412.10473v1",
      "title": "CONCLAD: COntinuous Novel CLAss Detector",
      "title_zh": "翻译失败",
      "authors": [
        "Amanda Rios",
        "Ibrahima Ndiour",
        "Parual Datta",
        "Omesh Tickoo",
        "Nilesh Ahuja"
      ],
      "abstract": "In the field of continual learning, relying on so-called oracles for novelty\ndetection is commonplace albeit unrealistic. This paper introduces CONCLAD\n(\"COntinuous Novel CLAss Detector\"), a comprehensive solution to the\nunder-explored problem of continual novel class detection in post-deployment\ndata. At each new task, our approach employs an iterative uncertainty\nestimation algorithm to differentiate between known and novel class(es)\nsamples, and to further discriminate between the different novel classes\nthemselves. Samples predicted to be from a novel class with high-confidence are\nautomatically pseudo-labeled and used to update our model. Simultaneously, a\ntiny supervision budget is used to iteratively query ambiguous novel class\npredictions, which are also used during update. Evaluation across multiple\ndatasets, ablations and experimental settings demonstrate our method's\neffectiveness at separating novel and old class samples continuously. We will\nrelease our code upon acceptance.",
      "tldr_zh": "这篇论文提出了 CONCLAD，一种不依赖 oracle 的持续新型别检测方法，用于解决持续学习（continual learning）中后部署数据的新类别识别问题。CONCLAD 在每个新任务中采用迭代的不确定性估计算法来区分已知类别和新型别样本，并进一步识别不同新型别；高置信度的新类别样本被自动伪标签化用于模型更新，同时利用少量监督预算查询模糊预测以提升准确性。在多个数据集上的实验评估证明，该方法在连续分离新型别和旧类别样本方面表现出色，并计划发布代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10473v1",
      "published_date": "2024-12-13 01:41:28 UTC",
      "updated_date": "2024-12-13 01:41:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:33:13.406763"
    },
    {
      "arxiv_id": "2412.09770v1",
      "title": "Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation",
      "title_zh": "翻译失败",
      "authors": [
        "Jonghyuk Park",
        "Alex Lascarides",
        "Subramanian Ramamoorthy"
      ],
      "abstract": "In this paper, we offer a learning framework in which the agent's knowledge\ngaps are overcome through corrective feedback from a teacher whenever the agent\nexplains its (incorrect) predictions. We test it in a low-resource visual\nprocessing scenario, in which the agent must learn to recognize distinct types\nof toy truck. The agent starts the learning process with no ontology about what\ntypes of trucks exist nor which parts they have, and a deficient model for\nrecognizing those parts from visual input. The teacher's feedback to the\nagent's explanations addresses its lack of relevant knowledge in the ontology\nvia a generic rule (e.g., \"dump trucks have dumpers\"), whereas an inaccurate\npart recognition is corrected by a deictic statement (e.g., \"this is not a\ndumper\"). The learner utilizes this feedback not only to improve its estimate\nof the hypothesis space of possible domain ontologies and probability\ndistributions over them, but also to use those estimates to update its visual\ninterpretation of the scene. Our experiments demonstrate that teacher-learner\npairs utilizing explanations and corrections are more data-efficient than those\nwithout such a faculty.",
      "tldr_zh": "本论文提出了一种通过具身对话和解释的学习框架，代理通过教师的纠正反馈来克服知识空白，从而学习视觉地基域本体（Domain Ontologies）。在低资源视觉处理场景中，代理从无卡车类型和部件知识开始，教师反馈包括通用规则（如“dump trucks have dumpers”）和指示性语句（如“this is not a dumper”），用于更新假设空间（Hypothesis Space）和视觉解释模型。实验结果表明，这种解释和纠正机制使教师-学习者对比没有该机制的更数据高效，显著提高了学习效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to, and to appear in the Thirty-Ninth AAAI Conference on\n  Artificial Intelligence (AAAI-25)",
      "pdf_url": "http://arxiv.org/pdf/2412.09770v1",
      "published_date": "2024-12-13 00:28:21 UTC",
      "updated_date": "2024-12-13 00:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:31:31.444120"
    },
    {
      "arxiv_id": "2412.12177v1",
      "title": "Model-diff: A Tool for Comparative Study of Language Models in the Input Space",
      "title_zh": "Model-diff：一种用于在输入空间中比较研究语言模型的工具",
      "authors": [
        "Weitang Liu",
        "Yuelei Li",
        "Ying Wai Li",
        "Zihan Wang",
        "Jingbo Shang"
      ],
      "abstract": "Comparing two (large) language models (LMs) side-by-side and pinpointing\ntheir prediction similarities and differences on the same set of inputs are\ncrucial in many real-world scenarios, e.g., one can test if a licensed model\nwas potentially plagiarized by another. Traditional analysis compares the LMs'\noutputs on some benchmark datasets, which only cover a limited number of inputs\nof designed perspectives for the intended applications. The benchmark datasets\ncannot prepare data to cover the test cases from unforeseen perspectives which\ncan help us understand differences between models unbiasedly. In this paper, we\npropose a new model comparative analysis setting that considers a large input\nspace where brute-force enumeration would be infeasible. The input space can be\nsimply defined as all token sequences that a LM would produce low perplexity on\n-- we follow this definition in the paper as it would produce the most\nhuman-understandable inputs. We propose a novel framework \\our that uses text\ngeneration by sampling and deweights the histogram of sampling statistics to\nestimate prediction differences between two LMs in this input space efficiently\nand unbiasedly. Our method achieves this by drawing and counting the inputs at\neach prediction difference value in negative log-likelihood. Experiments reveal\nfor the first time the quantitative prediction differences between LMs in a\nlarge input space, potentially facilitating the model analysis for applications\nsuch as model plagiarism.",
      "tldr_zh": "该论文提出了 Model-diff，一种用于比较语言模型 (LMs) 在输入空间中的预测差异的工具，旨在解决传统基准数据集覆盖有限的问题。Model-diff 框架通过文本生成采样和对采样统计直方图去权重的方法，高效估计两个 LMs 在负对数似然上的预测差异，从而覆盖大规模输入空间。实验结果首次量化了 LMs 在广阔输入空间中的差异，这为应用如模型剽窃检测提供了新的分析途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.12177v1",
      "published_date": "2024-12-13 00:06:25 UTC",
      "updated_date": "2024-12-13 00:06:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T12:31:43.718738"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 118,
  "processed_papers_count": 118,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T12:33:32.094668"
}
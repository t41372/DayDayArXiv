[
  {
    "arxiv_id": "2512.21446v1",
    "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning",
    "authors": [
      "Shirui Chen",
      "Jiantao Jiao",
      "Lillian J. Ratliff",
      "Banghua Zhu"
    ],
    "abstract": "Masked diffusion language models (MDLMs) offer the potential for parallel token generation, but most open-source MDLMs decode fewer than 5 tokens per model forward pass even with sophisticated sampling strategies. As a result, their sampling speeds are often comparable to AR + speculative decoding schemes, limiting their advantage over mainstream autoregressive approaches. Existing distillation-based accelerators (dParallel, d3LLM) finetune MDLMs on trajectories generated by a base model, which can become off-policy during finetuning and restrict performance to the quality of the base model's samples. We propose \\texttt{dUltra}, an on-policy reinforcement learning framework based on Group Relative Policy Optimization (GRPO) that learns unmasking strategies for efficient parallel decoding. dUltra introduces an unmasking planner head that predicts per-token unmasking likelihoods under independent Bernoulli distributions. We jointly optimize the base diffusion LLM and the unmasking order planner using reward signals combining verifiable reward, distillation reward, and the number of unmasking steps. Across mathematical reasoning and code generation tasks, dUltra improves the accuracy--efficiency trade-off over state-of-the-art heuristic and distillation baselines, moving towards achieving ``diffusion supremacy'' over autoregressive models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21446v1",
    "published_date": "2025-12-24 23:31:48 UTC",
    "updated_date": "2025-12-24 23:31:48 UTC"
  },
  {
    "arxiv_id": "2512.21439v1",
    "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models",
    "authors": [
      "Geoffroy Morlat",
      "Marceau Nahon",
      "Augustin Chartouny",
      "Raja Chatila",
      "Ismael T. Freire",
      "Mehdi Khamassi"
    ],
    "abstract": "Moral actions are judged not only by their outcomes but by the context in which they occur. We present COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating Do not kill, Do not deceive, and Do not break the law) and collect ternary judgments (Blame/Neutral/Support) from N=101 participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters. COMETH then learns action-specific moral contexts by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically, COMETH roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting (approx. 60% vs. approx. 30% on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 5 figures, +24 pages of Appendix",
    "pdf_url": "https://arxiv.org/pdf/2512.21439v1",
    "published_date": "2025-12-24 22:16:04 UTC",
    "updated_date": "2025-12-24 22:16:04 UTC"
  },
  {
    "arxiv_id": "2601.00828v1",
    "title": "Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis",
    "authors": [
      "Yin Li"
    ],
    "abstract": "Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, 2 figures, 3 tables. Code available at https://github.com/Kevin0304-li/llm-self-correction",
    "pdf_url": "https://arxiv.org/pdf/2601.00828v1",
    "published_date": "2025-12-24 21:51:24 UTC",
    "updated_date": "2025-12-24 21:51:24 UTC"
  },
  {
    "arxiv_id": "2512.21422v1",
    "title": "Teaching People LLM's Errors and Getting it Right",
    "authors": [
      "Nathan Stringham",
      "Fateme Hashemi Chaleshtori",
      "Xinyuan Yan",
      "Zhichao Xu",
      "Bei Wang",
      "Ana Marasović"
    ],
    "abstract": "People use large language models (LLMs) when they should not. This is partly because they see LLMs compose poems and answer intricate questions, so they understandably, but incorrectly, assume LLMs won't stumble on basic tasks like simple arithmetic. Prior work has tried to address this by clustering instance embeddings into regions where an LLM is likely to fail and automatically describing patterns in these regions. The found failure patterns are taught to users to mitigate their overreliance. Yet, this approach has not fully succeeded. In this analysis paper, we aim to understand why.\n  We first examine whether the negative result stems from the absence of failure patterns. We group instances in two datasets by their meta-labels and evaluate an LLM's predictions on these groups. We then define criteria to flag groups that are sizable and where the LLM is error-prone, and find meta-label groups that meet these criteria. Their meta-labels are the LLM's failure patterns that could be taught to users, so they do exist. We next test whether prompting and embedding-based approaches can surface these known failures. Without this, users cannot be taught about them to reduce their overreliance. We find mixed results across methods, which could explain the negative result. Finally, we revisit the final metric that measures teaching effectiveness. We propose to assess a user's ability to effectively use the given failure patterns to anticipate when an LLM is error-prone. A user study shows a positive effect from teaching with this metric, unlike the human-AI team accuracy. Our findings show that teaching failure patterns could be a viable approach to mitigating overreliance, but success depends on better automated failure-discovery methods and using metrics like ours.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21422v1",
    "published_date": "2025-12-24 20:53:07 UTC",
    "updated_date": "2025-12-24 20:53:07 UTC"
  },
  {
    "arxiv_id": "2512.21421v1",
    "title": "Three-way decision with incomplete information based on similarity and satisfiability",
    "authors": [
      "Junfang Luo",
      "Mengjun Hu",
      "Keyun Qin"
    ],
    "abstract": "Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a briefly review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21421v1",
    "published_date": "2025-12-24 20:52:41 UTC",
    "updated_date": "2025-12-24 20:52:41 UTC"
  },
  {
    "arxiv_id": "2512.21420v2",
    "title": "Feasible strategies in three-way conflict analysis with three-valued ratings",
    "authors": [
      "Jing Liu",
      "Mengjun Hu",
      "Guangming Lang"
    ],
    "abstract": "Most existing work on three-way conflict analysis has focused on trisecting agent pairs, agents, or issues, which contributes to understanding the nature of conflicts but falls short in addressing their resolution. Specifically, the formulation of feasible strategies, as an essential component of conflict resolution and mitigation, has received insufficient scholarly attention. Therefore, this paper aims to investigate feasible strategies from two perspectives of consistency and non-consistency. Particularly, we begin with computing the overall rating of a clique of agents based on positive and negative similarity degrees. Afterwards, considering the weights of both agents and issues, we propose weighted consistency and non-consistency measures, which are respectively used to identify the feasible strategies for a clique of agents. Algorithms are developed to identify feasible strategies, $L$-order feasible strategies, and the corresponding optimal ones. Finally, to demonstrate the practicality, effectiveness, and superiority of the proposed models, we apply them to two commonly used case studies on NBA labor negotiations and development plans for Gansu Province and conduct a sensitivity analysis on parameters and a comparative analysis with existing state-of-the-art conflict analysis approaches. The comparison results demonstrate that our conflict resolution models outperform the conventional approaches by unifying weighted agent-issue evaluation with consistency and non-consistency measures to enable the systematic identification of not only feasible strategies but also optimal solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21420v2",
    "published_date": "2025-12-24 20:52:15 UTC",
    "updated_date": "2025-12-29 02:25:21 UTC"
  },
  {
    "arxiv_id": "2512.21419v1",
    "title": "Three-way conflict analysis based on alliance and conflict functions",
    "authors": [
      "Junfang Luo",
      "Mengjun Hu",
      "Guangming Lang",
      "Xin Yang",
      "Keyun Qin"
    ],
    "abstract": "Trisecting agents, issues, and agent pairs are essential topics of three-way conflict analysis. They have been commonly studied based on either a rating or an auxiliary function. A rating function defines the positive, negative, or neutral ratings of agents on issues. An auxiliary function defines the alliance, conflict, and neutrality relations between agents. These functions measure two opposite aspects in a single function, leading to challenges in interpreting their aggregations over a group of issues or agents. For example, when studying agent relations regarding a set of issues, a standard aggregation takes the average of an auxiliary function concerning single issues. Therefore, a pair of alliance +1 and conflict -1 relations will produce the same result as a pair of neutrality 0 relations, although the attitudes represented by the two pairs are very different. To clarify semantics, we separate the two opposite aspects in an auxiliary function into a pair of alliance and conflict functions. Accordingly, we trisect the agents, issues, and agent pairs and investigate their applications in solving a few crucial questions in conflict analysis. Particularly, we explore the concepts of alliance sets and strategies. A real-world application is given to illustrate the proposed models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21419v1",
    "published_date": "2025-12-24 20:51:48 UTC",
    "updated_date": "2025-12-24 20:51:48 UTC"
  },
  {
    "arxiv_id": "2512.21404v1",
    "title": "LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors",
    "authors": [
      "Tianwei Lan",
      "Farid Naït-Abdesselam"
    ],
    "abstract": "The rapid growth in both the scale and complexity of Android malware has driven the widespread adoption of machine learning (ML) techniques for scalable and accurate malware detection. Despite their effectiveness, these models remain vulnerable to adversarial attacks that introduce carefully crafted feature-level perturbations to evade detection while preserving malicious functionality. In this paper, we present LAMLAD, a novel adversarial attack framework that exploits the generative and reasoning capabilities of large language models (LLMs) to bypass ML-based Android malware classifiers. LAMLAD employs a dual-agent architecture composed of an LLM manipulator, which generates realistic and functionality-preserving feature perturbations, and an LLM analyzer, which guides the perturbation process toward successful evasion. To improve efficiency and contextual awareness, LAMLAD integrates retrieval-augmented generation (RAG) into the LLM pipeline. Focusing on Drebin-style feature representations, LAMLAD enables stealthy and high-confidence attacks against widely deployed Android malware detection systems. We evaluate LAMLAD against three representative ML-based Android malware detectors and compare its performance with two state-of-the-art adversarial attack methods. Experimental results demonstrate that LAMLAD achieves an attack success rate (ASR) of up to 97%, requiring on average only three attempts per adversarial sample, highlighting its effectiveness, efficiency, and adaptability in practical adversarial settings. Furthermore, we propose an adversarial training-based defense strategy that reduces the ASR by more than 30% on average, significantly enhancing model robustness against LAMLAD-style attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21404v1",
    "published_date": "2025-12-24 19:56:06 UTC",
    "updated_date": "2025-12-24 19:56:06 UTC"
  },
  {
    "arxiv_id": "2512.21336v1",
    "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
    "authors": [
      "Ziyu Chen",
      "Xinbei Jiang",
      "Peng Sun",
      "Tao Lin"
    ],
    "abstract": "Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21336v1",
    "published_date": "2025-12-24 18:59:51 UTC",
    "updated_date": "2025-12-24 18:59:51 UTC"
  },
  {
    "arxiv_id": "2512.21332v1",
    "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
    "authors": [
      "Jin Qin",
      "Zihan Liao",
      "Ziyin Zhang",
      "Hang Yu",
      "Peng Di",
      "Rui Wang"
    ],
    "abstract": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21332v1",
    "published_date": "2025-12-24 18:59:01 UTC",
    "updated_date": "2025-12-24 18:59:01 UTC"
  },
  {
    "arxiv_id": "2512.21326v1",
    "title": "Measuring all the noises of LLM Evals",
    "authors": [
      "Sida Wang"
    ],
    "abstract": "Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21326v1",
    "published_date": "2025-12-24 18:54:37 UTC",
    "updated_date": "2025-12-24 18:54:37 UTC"
  },
  {
    "arxiv_id": "2512.21316v1",
    "title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks",
    "authors": [
      "Ali Merali"
    ],
    "abstract": "This paper derives `Scaling Laws for Economic Impacts' -- empirical relationships between the training compute of Large Language Models (LLMs) and professional productivity. In a preregistered experiment, over 500 consultants, data analysts, and managers completed professional tasks using one of 13 LLMs. We find that each year of AI model progress reduced task time by 8%, with 56% of gains driven by increased compute and 44% by algorithmic progress. However, productivity gains were significantly larger for non-agentic analytical tasks compared to agentic workflows requiring tool use. These findings suggest continued model scaling could boost U.S. productivity by approximately 20% over the next decade.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21316v1",
    "published_date": "2025-12-24 18:24:29 UTC",
    "updated_date": "2025-12-24 18:24:29 UTC"
  },
  {
    "arxiv_id": "2512.22266v1",
    "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs",
    "authors": [
      "Bing Hao",
      "Minglai Shao",
      "Zengyi Wo",
      "Yunlong Chu",
      "Yuhang Liu",
      "Ruijie Wang"
    ],
    "abstract": "The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22266v1",
    "published_date": "2025-12-24 18:10:29 UTC",
    "updated_date": "2025-12-24 18:10:29 UTC"
  },
  {
    "arxiv_id": "2512.21288v1",
    "title": "Model Merging via Multi-Teacher Knowledge Distillation",
    "authors": [
      "Seyed Arshan Dalili",
      "Mehrdad Mahdavi"
    ],
    "abstract": "Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a \"cross-task heterogeneity\" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21288v1",
    "published_date": "2025-12-24 17:10:44 UTC",
    "updated_date": "2025-12-24 17:10:44 UTC"
  },
  {
    "arxiv_id": "2512.21280v1",
    "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance",
    "authors": [
      "Divij Dudeja",
      "Mayukha Pal"
    ],
    "abstract": "The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21280v1",
    "published_date": "2025-12-24 16:59:04 UTC",
    "updated_date": "2025-12-24 16:59:04 UTC"
  },
  {
    "arxiv_id": "2512.21246v1",
    "title": "Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students",
    "authors": [
      "Gaia Ebli",
      "Bianca Raimondi",
      "Maurizio Gabbrielli"
    ],
    "abstract": "The increasing integration of AI tools in education has led prior research to explore their impact on learning processes. Nevertheless, most existing studies focus on higher education and conventional instructional contexts, leaving open questions about how key learning factors are related in AI-mediated learning environments and how these relationships may vary across different age groups. Addressing these gaps, our work investigates whether four critical learning factors, experience, clarity, comfort, and motivation, maintain coherent interrelationships in AI-augmented educational settings, and how the structure of these relationships differs between middle and high school students. The study was conducted in authentic classroom contexts where students interacted with AI tools as part of programming learning activities to collect data on the four learning factors and students' perceptions. Using a multimethod quantitative analysis, which combined correlation analysis and text mining, we revealed markedly different dimensional structures between the two age groups. Middle school students exhibit strong positive correlations across all dimensions, indicating holistic evaluation patterns whereby positive perceptions in one dimension generalise to others. In contrast, high school students show weak or near-zero correlations between key dimensions, suggesting a more differentiated evaluation process in which dimensions are assessed independently. These findings reveal that perception dimensions actively mediate AI-augmented learning and that the developmental stage moderates their interdependencies. This work establishes a foundation for the development of AI integration strategies that respond to learners' developmental levels and account for age-specific dimensional structures in student-AI interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Preprint. Under review",
    "pdf_url": "https://arxiv.org/pdf/2512.21246v1",
    "published_date": "2025-12-24 15:43:58 UTC",
    "updated_date": "2025-12-24 15:43:58 UTC"
  },
  {
    "arxiv_id": "2512.21243v1",
    "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
    "authors": [
      "Anatoly O. Onishchenko",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "abstract": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21243v1",
    "published_date": "2025-12-24 15:36:21 UTC",
    "updated_date": "2025-12-24 15:36:21 UTC"
  },
  {
    "arxiv_id": "2512.21241v1",
    "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
    "authors": [
      "Xinjie Xu",
      "Shuyu Cheng",
      "Dongwei Xu",
      "Qi Xuan",
      "Chen Ma"
    ],
    "abstract": "In hard-label black-box adversarial attacks, where only the top-1 predicted label is accessible, the prohibitive query complexity poses a major obstacle to practical deployment. In this paper, we focus on optimizing a representative class of attacks that search for the optimal ray direction yielding the minimum $\\ell_2$-norm perturbation required to move a benign image into the adversarial region. Inspired by Nesterov's Accelerated Gradient (NAG), we propose a momentum-based algorithm, ARS-OPT, which proactively estimates the gradient with respect to a future ray direction inferred from accumulated momentum. We provide a theoretical analysis of its convergence behavior, showing that ARS-OPT enables more accurate directional updates and achieves faster, more stable optimization. To further accelerate convergence, we incorporate surrogate-model priors into ARS-OPT's gradient estimation, resulting in PARS-OPT with enhanced performance. The superiority of our approach is supported by theoretical guarantees under standard assumptions. Extensive experiments on ImageNet and CIFAR-10 demonstrate that our method surpasses 13 state-of-the-art approaches in query efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at AAAI 2026 (Oral). This version corresponds to the conference proceedings; v2 will include the appendix",
    "pdf_url": "https://arxiv.org/pdf/2512.21241v1",
    "published_date": "2025-12-24 15:35:03 UTC",
    "updated_date": "2025-12-24 15:35:03 UTC"
  },
  {
    "arxiv_id": "2601.09715v1",
    "title": "Introducing Axlerod: An LLM-based Chatbot for Assisting Independent Insurance Agents",
    "authors": [
      "Adam Bradley",
      "John Hastings",
      "Khandaker Mamun Ahmed"
    ],
    "abstract": "The insurance industry is undergoing a paradigm shift through the adoption of artificial intelligence (AI) technologies, particularly in the realm of intelligent conversational agents. Chatbots have evolved into sophisticated AI-driven systems capable of automating complex workflows, including policy recommendation and claims triage, while simultaneously enabling dynamic, context-aware user engagement. This paper presents the design, implementation, and empirical evaluation of Axlerod, an AI-powered conversational interface designed to improve the operational efficiency of independent insurance agents. Leveraging natural language processing (NLP), retrieval-augmented generation (RAG), and domain-specific knowledge integration, Axlerod demonstrates robust capabilities in parsing user intent, accessing structured policy databases, and delivering real-time, contextually relevant responses. Experimental results underscore Axlerod's effectiveness, achieving an overall accuracy of 93.18% in policy retrieval tasks while reducing the average search time by 2.42 seconds. This work contributes to the growing body of research on enterprise-grade AI applications in insurtech, with a particular focus on agent-assistive rather than consumer-facing architectures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2601.09715v1",
    "published_date": "2025-12-24 15:31:59 UTC",
    "updated_date": "2025-12-24 15:31:59 UTC"
  },
  {
    "arxiv_id": "2512.21236v1",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang",
      "Chong Wang",
      "Yang Liu"
    ],
    "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted to FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.21236v1",
    "published_date": "2025-12-24 15:25:31 UTC",
    "updated_date": "2025-12-24 15:25:31 UTC"
  },
  {
    "arxiv_id": "2512.21227v2",
    "title": "PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation",
    "authors": [
      "Xiao-Qi Han",
      "Peng-Jie Guo",
      "Ze-Feng Gao",
      "Zhong-Yi Lu"
    ],
    "abstract": "In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "19 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.21227v2",
    "published_date": "2025-12-24 15:07:36 UTC",
    "updated_date": "2025-12-27 00:48:56 UTC"
  },
  {
    "arxiv_id": "2512.21221v1",
    "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Nguyen Lam Phu Quy",
      "Phu-Hoa Pham",
      "Tran Chi Nguyen"
    ],
    "abstract": "Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "System description paper for EVENTA Grand Challenge Track 2 at ACM Multimedia 2025 (MM '25). Ranked 4th place. 6 pages, 1 figure, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.21221v1",
    "published_date": "2025-12-24 15:02:33 UTC",
    "updated_date": "2025-12-24 15:02:33 UTC"
  },
  {
    "arxiv_id": "2512.21220v2",
    "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
    "authors": [
      "Le Wang",
      "Zonghao Ying",
      "Xiao Yang",
      "Quanchen Zou",
      "Zhenfei Yin",
      "Tianlin Li",
      "Jian Yang",
      "Yaodong Yang",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "abstract": "Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.21220v2",
    "published_date": "2025-12-24 15:01:26 UTC",
    "updated_date": "2025-12-26 03:30:51 UTC"
  },
  {
    "arxiv_id": "2512.21204v1",
    "title": "SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation",
    "authors": [
      "Mahi Luthra",
      "Jiayi Shen",
      "Maxime Poli",
      "Angelo Ortiz",
      "Yosuke Higuchi",
      "Youssef Benchekroun",
      "Martin Gleize",
      "Charles-Eric Saint-James",
      "Dongyan Lin",
      "Phillip Rust",
      "Angel Villar",
      "Surya Parimi",
      "Vanessa Stark",
      "Rashel Moritz",
      "Juan Pino",
      "Yann LeCun",
      "Emmanuel Dupoux"
    ],
    "abstract": "Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21204v1",
    "published_date": "2025-12-24 14:33:16 UTC",
    "updated_date": "2025-12-24 14:33:16 UTC"
  },
  {
    "arxiv_id": "2512.21201v1",
    "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
    "authors": [
      "Yu He",
      "Da Huang",
      "Zhenyang Liu",
      "Zixiao Gu",
      "Qiang Sun",
      "Guangnan Ye",
      "Yanwei Fu"
    ],
    "abstract": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21201v1",
    "published_date": "2025-12-24 14:28:17 UTC",
    "updated_date": "2025-12-24 14:28:17 UTC"
  },
  {
    "arxiv_id": "2512.21375v1",
    "title": "Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks",
    "authors": [
      "Yuanshuang Fu",
      "Qianyao Wang",
      "Qihao Wang",
      "Bonan Zhang",
      "Jiaxin Zhao",
      "Yiming Cao",
      "Zhijun Li"
    ],
    "abstract": "Unmanned Aerial Vehicle (UAV) spectral remote sensing technology is widely used in water quality monitoring. However, in dynamic environments, varying illumination conditions, such as shadows and specular reflection (sun glint), can cause severe spectral distortion, thereby reducing data availability. To maximize the acquisition of high-quality data while ensuring flight safety, this paper proposes an active path planning method for dynamic light and shadow disturbance avoidance. First, a dynamic prediction model is constructed to transform the time-varying light and shadow disturbance areas into three-dimensional virtual obstacles. Second, an improved Interfered Fluid Dynamical System (IFDS) algorithm is introduced, which generates a smooth initial obstacle avoidance path by building a repulsive force field. Subsequently, a Model Predictive Control (MPC) framework is employed for rolling-horizon path optimization to handle flight dynamics constraints and achieve real-time trajectory tracking. Furthermore, a Dynamic Flight Altitude Adjustment (DFAA) mechanism is designed to actively reduce the flight altitude when the observable area is narrow, thereby enhancing spatial resolution. Simulation results show that, compared with traditional PID and single obstacle avoidance algorithms, the proposed method achieves an obstacle avoidance success rate of 98% in densely disturbed scenarios, significantly improves path smoothness, and increases the volume of effective observation data by approximately 27%. This research provides an effective engineering solution for precise UAV water quality monitoring in complex illumination environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21375v1",
    "published_date": "2025-12-24 14:26:20 UTC",
    "updated_date": "2025-12-24 14:26:20 UTC"
  },
  {
    "arxiv_id": "2512.21165v1",
    "title": "BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft",
    "authors": [
      "Qizhi Wang"
    ],
    "abstract": "Randomized election timeouts are a simple and effective liveness heuristic for Raft, but they become brittle under long-tail latency, jitter, and partition recovery, where repeated split votes can inflate unavailability. This paper presents BALLAST, a lightweight online adaptation mechanism that replaces static timeout heuristics with contextual bandits. BALLAST selects from a discrete set of timeout \"arms\" using efficient linear contextual bandits (LinUCB variants), and augments learning with safe exploration to cap risk during unstable periods. We evaluate BALLAST on a reproducible discrete-event simulation with long-tail delay, loss, correlated bursts, node heterogeneity, and partition/recovery turbulence. Across challenging WAN regimes, BALLAST substantially reduces recovery time and unwritable time compared to standard randomized timeouts and common heuristics, while remaining competitive on stable LAN/WAN settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 22 tables, 11 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.21165v1",
    "published_date": "2025-12-24 13:25:36 UTC",
    "updated_date": "2025-12-24 13:25:36 UTC"
  },
  {
    "arxiv_id": "2512.22260v1",
    "title": "ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers",
    "authors": [
      "Chen Chen",
      "Daniela Kaufmann",
      "Chenhui Deng",
      "Zhan Song",
      "Hongce Zhang",
      "Cunxi Yu"
    ],
    "abstract": "We present ReVEAL, a graph-learning-based method for reverse engineering of multiplier architectures to improve algebraic circuit verification techniques. Our framework leverages structural graph features and learning-driven inference to identify architecture patterns at scale, enabling robust handling of large optimized multipliers. We demonstrate applicability across diverse multiplier benchmarks and show improvements in scalability and accuracy compared to traditional rule-based approaches. The method integrates smoothly with existing verification flows and supports downstream algebraic proof strategies.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Accepted by TACAS 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.22260v1",
    "published_date": "2025-12-24 13:01:55 UTC",
    "updated_date": "2025-12-24 13:01:55 UTC"
  },
  {
    "arxiv_id": "2601.03274v1",
    "title": "LLM_annotate: A Python package for annotating and analyzing fiction characters",
    "authors": [
      "Hannes Rosenbusch"
    ],
    "abstract": "LLM_annotate is a Python package for analyzing the personality of fiction characters with large language models. It standardizes workflows for annotating character behaviors in full texts (e.g., books and movie scripts), inferring character traits, and validating annotation/inference quality via a human-in-the-loop GUI. The package includes functions for text chunking, LLM-based annotation, character name disambiguation, quality scoring, and computation of character-level statistics and embeddings. Researchers can use any LLM, commercial, open-source, or custom, within LLM_annotate. Through tutorial examples using The Simpsons Movie and the novel Pride and Prejudice, I demonstrate the usage of the package for efficient and reproducible character analyses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.03274v1",
    "published_date": "2025-12-24 12:45:02 UTC",
    "updated_date": "2025-12-24 12:45:02 UTC"
  },
  {
    "arxiv_id": "2512.21152v1",
    "title": "MODE: Multi-Objective Adaptive Coreset Selection",
    "authors": [
      "Tanmoy Mukherjee",
      "Pierre Marquis",
      "Zied Bouraoui"
    ],
    "abstract": "We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \\mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \\log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \\mode reduces memory requirements",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21152v1",
    "published_date": "2025-12-24 12:43:40 UTC",
    "updated_date": "2025-12-24 12:43:40 UTC"
  },
  {
    "arxiv_id": "2601.09714v1",
    "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines",
    "authors": [
      "Devesh Saraogi",
      "Rohit Singhee",
      "Dhruv Kumar"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "https://arxiv.org/pdf/2601.09714v1",
    "published_date": "2025-12-24 12:41:31 UTC",
    "updated_date": "2025-12-24 12:41:31 UTC"
  },
  {
    "arxiv_id": "2601.04219v1",
    "title": "AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems",
    "authors": [
      "Yuxin Liu",
      "Zeqing Song",
      "Jiong Lou",
      "Chentao Wu",
      "Jie Li"
    ],
    "abstract": "The rapid advancement of large-scale language models (LLMs) has shown their potential to transform intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners' cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners' learning status, personalized goals, learning preferences, and multimodal study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge & experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners' performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CY",
    "comment": "AAAI2026 Workshop AI4EDU",
    "pdf_url": "https://arxiv.org/pdf/2601.04219v1",
    "published_date": "2025-12-24 12:26:28 UTC",
    "updated_date": "2025-12-24 12:26:28 UTC"
  },
  {
    "arxiv_id": "2512.21135v1",
    "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
    "authors": [
      "Gaoren Lin",
      "Huangxuan Zhao",
      "Yuan Xiong",
      "Lefei Zhang",
      "Bo Du",
      "Wentao Zhu"
    ],
    "abstract": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21135v1",
    "published_date": "2025-12-24 12:06:26 UTC",
    "updated_date": "2025-12-24 12:06:26 UTC"
  },
  {
    "arxiv_id": "2512.21132v1",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "abstract": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21132v1",
    "published_date": "2025-12-24 12:02:00 UTC",
    "updated_date": "2025-12-24 12:02:00 UTC"
  },
  {
    "arxiv_id": "2512.21127v1",
    "title": "A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care",
    "authors": [
      "Oliver Normand",
      "Esther Borsi",
      "Mitch Fruin",
      "Lauren E Walker",
      "Jamie Heagerty",
      "Chris C. Holmes",
      "Anthony J Avery",
      "Iain E Buchan",
      "Harry Coppock"
    ],
    "abstract": "Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\\% [95\\% CI 98.2--100], specificity 83.1\\% [95\\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\\% [95\\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21127v1",
    "published_date": "2025-12-24 11:58:49 UTC",
    "updated_date": "2025-12-24 11:58:49 UTC"
  },
  {
    "arxiv_id": "2512.21118v1",
    "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
    "authors": [
      "Shi Quan Foo",
      "Chi-Ho Wong",
      "Zhihan Gao",
      "Dit-Yan Yeung",
      "Ka-Hing Wong",
      "Wai-Kin Wong"
    ],
    "abstract": "Precipitation nowcasting is a critical spatio-temporal prediction task for society to prevent severe damage owing to extreme weather events. Despite the advances in this field, the complex and stochastic nature of this task still poses challenges to existing approaches. Specifically, deterministic models tend to produce blurry predictions while generative models often struggle with poor accuracy. In this paper, we present a simple yet effective model architecture termed STLDM, a diffusion-based model that learns the latent representation from end to end alongside both the Variational Autoencoder and the conditioning network. STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model. Experimental results on multiple radar datasets demonstrate that STLDM achieves superior performance compared to the state of the art, while also improving inference efficiency. The code is available in https://github.com/sqfoo/stldm_official.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by TMLR. Camera-ready submission",
    "pdf_url": "https://arxiv.org/pdf/2512.21118v1",
    "published_date": "2025-12-24 11:34:44 UTC",
    "updated_date": "2025-12-24 11:34:44 UTC"
  },
  {
    "arxiv_id": "2512.21110v2",
    "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
    "authors": [
      "Ahmed M. Hussain",
      "Salahuddin Salahuddin",
      "Panos Papadimitratos"
    ],
    "abstract": "Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages and 23 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.21110v2",
    "published_date": "2025-12-24 11:15:57 UTC",
    "updated_date": "2025-12-29 14:48:01 UTC"
  },
  {
    "arxiv_id": "2512.21107v1",
    "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
    "authors": [
      "Eduard Stefan Dinuta",
      "Iustin Sirbu",
      "Traian Rebedea"
    ],
    "abstract": "Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21107v1",
    "published_date": "2025-12-24 11:12:09 UTC",
    "updated_date": "2025-12-24 11:12:09 UTC"
  },
  {
    "arxiv_id": "2512.21106v1",
    "title": "Semantic Refinement with LLMs for Graph Representations",
    "authors": [
      "Safal Thapaliya",
      "Zehong Wang",
      "Jiazheng Li",
      "Ziming Li",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "abstract": "Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21106v1",
    "published_date": "2025-12-24 11:10:28 UTC",
    "updated_date": "2025-12-24 11:10:28 UTC"
  },
  {
    "arxiv_id": "2512.21099v1",
    "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
    "authors": [
      "Jaeseong Lee",
      "Junyeong Ahn",
      "Taewoong Kang",
      "Jaegul Choo"
    ],
    "abstract": "Constructing drivable and photorealistic 3D head avatars has become a central task in AR/XR, enabling immersive and expressive user experiences. With the emergence of high-fidelity and efficient representations such as 3D Gaussians, recent works have pushed toward ultra-detailed head avatars. Existing approaches typically fall into two categories: rule-based analytic rigging or neural network-based deformation fields. While effective in constrained settings, both approaches often fail to generalize to unseen expressions and poses, particularly in extreme reenactment scenarios. Other methods constrain Gaussians to the global texel space of 3DMMs to reduce rendering complexity. However, these texel-based avatars tend to underutilize the underlying mesh structure. They apply minimal analytic deformation and rely heavily on neural regressors and heuristic regularization in UV space, which weakens geometric consistency and limits extrapolation to complex, out-of-distribution deformations. To address these limitations, we introduce TexAvatars, a hybrid avatar representation that combines the explicit geometric grounding of analytic rigging with the spatial continuity of texel space. Our approach predicts local geometric attributes in UV space via CNNs, but drives 3D deformation through mesh-aware Jacobians, enabling smooth and semantically meaningful transitions across triangle boundaries. This hybrid design separates semantic modeling from geometric control, resulting in improved generalization, interpretability, and stability. Furthermore, TexAvatars captures fine-grained expression effects, including muscle-induced wrinkles, glabellar lines, and realistic mouth cavity geometry, with high fidelity. Our method achieves state-of-the-art performance under extreme pose and expression variations, demonstrating strong generalization in challenging head reenactment settings.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "3DV 2026, Project page with videos: https://summertight.github.io/TexAvatars/",
    "pdf_url": "https://arxiv.org/pdf/2512.21099v1",
    "published_date": "2025-12-24 10:50:04 UTC",
    "updated_date": "2025-12-24 10:50:04 UTC"
  },
  {
    "arxiv_id": "2601.00827v2",
    "title": "Speak the Art: A Direct Speech to Image Generation Framework",
    "authors": [
      "Mariam Saeed",
      "Manar Amr",
      "Farida Adel",
      "Nada Hassan",
      "Nour Walid",
      "Eman Mohamed",
      "Mohamed Hussein",
      "Marwan Torki"
    ],
    "abstract": "Direct speech-to-image generation has recently shown promising results. However, compared to text-to-image generation, there is still a large gap to enclose. Current approaches use two stages to tackle this task: speech encoding network and image generative adversarial network (GAN). The speech encoding networks in these approaches produce embeddings that do not capture sufficient linguistic information to semantically represent the input speech. GANs suffer from issues such as non-convergence, mode collapse, and diminished gradient, which result in unstable model parameters, limited sample diversity, and ineffective generator learning, respectively. To address these weaknesses, we introduce a framework called Speak the Art (STA) which consists of a speech encoding network and a VQ-Diffusion network conditioned on speech embeddings. To improve speech embeddings, the speech encoding network is supervised by a large pre-trained image-text model during training. Replacing GANs with diffusion leads to more stable training and the generation of diverse images. Additionally, we investigate the feasibility of extending our framework to be multilingual. As a proof of concept, we trained our framework with two languages: English and Arabic. Finally, we show that our results surpass state-of-the-art models by a large margin.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.00827v2",
    "published_date": "2025-12-24 10:49:00 UTC",
    "updated_date": "2026-01-12 12:32:57 UTC"
  },
  {
    "arxiv_id": "2512.21080v1",
    "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking",
    "authors": [
      "Enoch Hyunwook Kang"
    ],
    "abstract": "Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "econ.EM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21080v1",
    "published_date": "2025-12-24 09:56:00 UTC",
    "updated_date": "2025-12-24 09:56:00 UTC"
  },
  {
    "arxiv_id": "2512.21075v1",
    "title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics",
    "authors": [
      "Zihan Yao",
      "Ruoyu Wu",
      "Tianxiang Gao"
    ],
    "abstract": "The empirical success of deep learning is often attributed to scaling laws that predict consistent gains as model, data, and compute grow; however, large models can exhibit training instability and diminishing returns, suggesting that scaling laws describe what success looks like but not when and why scaling succeeds or fails. A central obstacle is the lack of a rigorous understanding of feature learning at large depth. While muP characterizes feature-learning dynamics in the infinite-width limit and enables hyperparameter transfer across width, its depth extension (depth-muP) breaks down for residual blocks with more than one internal layer. We derive Neural Feature Dynamics (NFD) for ResNets with single-layer residual blocks, characterizing feature learning via a coupled forward-backward stochastic system in the joint infinite-width and infinite-depth limit. In this regime, NFD identifies when scaling-law trends persist and explains diminishing returns. It also reveals a vanishing mechanism induced by the 1/sqrt(depth) residual scaling under which the gradient-independence assumption (GIA), known to fail during training at finite depth, becomes provably valid again at infinite depth, yielding an analytically tractable regime for end-to-end feature learning. Motivated by this insight, we study two-layer residual blocks and show that the same mechanism causes feature-learning collapse in the first internal layer at large depth, providing a structural explanation for the empirical failure of depth-muP. Based on this diagnosis, we propose a depth-aware learning-rate correction that counteracts the collapse and empirically restores depth-wise hyperparameter transfer, yielding stronger performance in deeper ResNets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21075v1",
    "published_date": "2025-12-24 09:39:04 UTC",
    "updated_date": "2025-12-24 09:39:04 UTC"
  },
  {
    "arxiv_id": "2512.22258v1",
    "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method",
    "authors": [
      "Satvik Tripathi"
    ],
    "abstract": "Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22258v1",
    "published_date": "2025-12-24 09:20:35 UTC",
    "updated_date": "2025-12-24 09:20:35 UTC"
  },
  {
    "arxiv_id": "2512.21066v1",
    "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
    "authors": [
      "Tomoaki Yamaguchi",
      "Yutong Zhou",
      "Masahiro Ryo",
      "Keisuke Katsura"
    ],
    "abstract": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21066v1",
    "published_date": "2025-12-24 09:19:15 UTC",
    "updated_date": "2025-12-24 09:19:15 UTC"
  },
  {
    "arxiv_id": "2512.21054v1",
    "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
    "authors": [
      "Kaustubh Kundu",
      "Hrishav Bakul Barua",
      "Lucy Robertson-Bell",
      "Zhixi Cai",
      "Kalin Stefanov"
    ],
    "abstract": "The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in WACV 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.21054v1",
    "published_date": "2025-12-24 08:44:58 UTC",
    "updated_date": "2025-12-24 08:44:58 UTC"
  },
  {
    "arxiv_id": "2512.21373v1",
    "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
    "authors": [
      "Titouan Duston",
      "Shuo Xin",
      "Yang Sun",
      "Daoguang Zan",
      "Aoyan Li",
      "Shulin Xin",
      "Kai Shen",
      "Yixiao Chen",
      "Qiming Sun",
      "Ge Zhang",
      "Jiashuo Liu",
      "Huan Zhou",
      "Jingkai Liu",
      "Zhichen Pu",
      "Yuanheng Wang",
      "Bo-Xuan Ge",
      "Xin Tong",
      "Fei Ye",
      "Zhi-Chao Zhao",
      "Wen-Biao Han",
      "Zhoujian Cao",
      "Yueran Zhao",
      "Weiluo Ren",
      "Qingshen Long",
      "Yuxiao Liu",
      "Anni Huang",
      "Yidi Du",
      "Yuanyuan Rong",
      "Jiahao Peng"
    ],
    "abstract": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21373v1",
    "published_date": "2025-12-24 08:11:11 UTC",
    "updated_date": "2025-12-24 08:11:11 UTC"
  },
  {
    "arxiv_id": "2512.22256v1",
    "title": "Agentic Software Issue Resolution with Large Language Models: A Survey",
    "authors": [
      "Zhonghao Jiang",
      "David Lo",
      "Zhongxin Liu"
    ],
    "abstract": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.\n  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22256v1",
    "published_date": "2025-12-24 08:05:10 UTC",
    "updated_date": "2025-12-24 08:05:10 UTC"
  },
  {
    "arxiv_id": "2512.21024v1",
    "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
    "authors": [
      "Yue Lin",
      "Shuhui Zhu",
      "Wenhao Li",
      "Ang Li",
      "Dan Qiao",
      "Pascal Poupart",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "abstract": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21024v1",
    "published_date": "2025-12-24 07:42:10 UTC",
    "updated_date": "2025-12-24 07:42:10 UTC"
  },
  {
    "arxiv_id": "2512.22255v2",
    "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
    "authors": [
      "Abhranil Chandra",
      "Ayush Agrawal",
      "Arian Hosseini",
      "Sebastian Fischmeister",
      "Rishabh Agarwal",
      "Navin Goyal",
      "Aaron Courville"
    ],
    "abstract": "We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22255v2",
    "published_date": "2025-12-24 07:35:55 UTC",
    "updated_date": "2026-01-22 20:25:28 UTC"
  },
  {
    "arxiv_id": "2512.21017v1",
    "title": "Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy",
    "authors": [
      "Xiaofeng Shi",
      "Qian Kou",
      "Yuduo Li",
      "Hua Zhou"
    ],
    "abstract": "With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21017v1",
    "published_date": "2025-12-24 07:24:31 UTC",
    "updated_date": "2025-12-24 07:24:31 UTC"
  },
  {
    "arxiv_id": "2512.21010v1",
    "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
    "authors": [
      "Jiashuo Liu",
      "Jiayun Wu",
      "Chunjie Wu",
      "Jingkai Liu",
      "Zaiyuan Wang",
      "Huan Zhou",
      "Wenhao Huang",
      "Hongseok Namkoong"
    ],
    "abstract": "The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.21010v1",
    "published_date": "2025-12-24 07:14:31 UTC",
    "updated_date": "2025-12-24 07:14:31 UTC"
  },
  {
    "arxiv_id": "2512.21002v2",
    "title": "Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation",
    "authors": [
      "Wei-Rui Chen",
      "Vignesh Kothapalli",
      "Ata Fatahibaarzi",
      "Hejian Sang",
      "Shao Tang",
      "Qingquan Song",
      "Zhipeng Wang",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Distilling the capabilities from a large reasoning model (LRM) to a smaller student model often involves training on substantial amounts of reasoning data. However, knowledge distillation (KD) over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) sections makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different sections (P, CoT, A) affects student performance. Our analysis shows that selective KD over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that beyond a specific length, longer training sequences provide marginal returns for downstream performance but require substantially higher memory and FLOPs. To this end, training on only the first $50\\%$ of tokens of every training sequence can retain, on average, $\\approx91\\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\\%$ each. Codes are available at https://github.com/weiruichen01/distilling-the-essence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.21002v2",
    "published_date": "2025-12-24 06:57:35 UTC",
    "updated_date": "2026-01-08 01:09:21 UTC"
  },
  {
    "arxiv_id": "2512.20996v1",
    "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
    "authors": [
      "Yuwei Du",
      "Jun Zhang",
      "Jie Feng",
      "Zhicheng Liu",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The code will be available at: https://github.com/tsinghua-fib-lab/TrafficSimAgent",
    "pdf_url": "https://arxiv.org/pdf/2512.20996v1",
    "published_date": "2025-12-24 06:48:04 UTC",
    "updated_date": "2025-12-24 06:48:04 UTC"
  },
  {
    "arxiv_id": "2601.00023v1",
    "title": "A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system",
    "authors": [
      "Luis M. Moreno-Saavedra",
      "Silvia Jimenez-Fernandez",
      "Antonio Portilla-Figueras",
      "David Casillas-Perez",
      "Sancho Salcedo-Sanz"
    ],
    "abstract": "Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.00023v1",
    "published_date": "2025-12-24 06:45:44 UTC",
    "updated_date": "2025-12-24 06:45:44 UTC"
  },
  {
    "arxiv_id": "2512.20991v1",
    "title": "FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning",
    "authors": [
      "Toqeer Ali Syed",
      "Abdulaziz Alshahrani",
      "Ali Ullah",
      "Ali Akarma",
      "Sohail Khan",
      "Muhammad Nauman",
      "Salman Jan"
    ],
    "abstract": "The issue of limited household budgets and nutritional demands continues to be a challenge especially in the middle-income environment where food prices fluctuate. This paper introduces a price aware agentic AI system, which combines personal finance management with diet optimization. With household income and fixed expenditures, medical and well-being status, as well as real-time food costs, the system creates nutritionally sufficient meals plans at comparatively reasonable prices that automatically adjust to market changes. The framework is implemented in a modular multi-agent architecture, which has specific agents (budgeting, nutrition, price monitoring, and health personalization). These agents share the knowledge base and use the substitution graph to ensure that the nutritional quality is maintained at a minimum cost. Simulations with a representative Saudi household case study show a steady 12-18\\% reduction in costs relative to a static weekly menu, nutrient adequacy of over 95\\% and high performance with price changes of 20-30%. The findings indicate that the framework can locally combine affordability with nutritional adequacy and provide a viable avenue of capacity-building towards sustainable and fair diet planning in line with Sustainable Development Goals on Zero Hunger and Good Health.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain",
    "pdf_url": "https://arxiv.org/pdf/2512.20991v1",
    "published_date": "2025-12-24 06:33:17 UTC",
    "updated_date": "2025-12-24 06:33:17 UTC"
  },
  {
    "arxiv_id": "2512.20985v1",
    "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
    "authors": [
      "Salman Jan",
      "Hassan Ali Razzaqi",
      "Ali Akarma",
      "Mohammad Riyaz Belgaum"
    ],
    "abstract": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper was presented at the IEEE International Conference on Computing and Applications (ICCA 2025), Bahrain",
    "pdf_url": "https://arxiv.org/pdf/2512.20985v1",
    "published_date": "2025-12-24 06:20:28 UTC",
    "updated_date": "2025-12-24 06:20:28 UTC"
  },
  {
    "arxiv_id": "2512.20983v1",
    "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
    "authors": [
      "Oleksii Proniakin",
      "Diego Fajardo",
      "Ruslan Nazarenko",
      "Razvan Marinescu"
    ],
    "abstract": "Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "48 pages, 3 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2512.20983v1",
    "published_date": "2025-12-24 06:17:21 UTC",
    "updated_date": "2025-12-24 06:17:21 UTC"
  },
  {
    "arxiv_id": "2512.20978v1",
    "title": "GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model",
    "authors": [
      "Haoyang Li",
      "Xuyi Zhuang",
      "Azmat Adnan",
      "Ye Ni",
      "Wei Rao",
      "Shreyas Gopal",
      "Eng Siong Chng"
    ],
    "abstract": "Language Model (LM)-based generative modeling has emerged as a promising direction for TSE, offering potential for improved generalization and high-fidelity speech. We present GenTSE, a two-stage decoder-only generative LM approach for TSE: Stage-1 predicts coarse semantic tokens, and Stage-2 generates fine acoustic tokens. Separating semantics and acoustics stabilizes decoding and yields more faithful, content-aligned target speech. Both stages use continuous SSL or codec embeddings, offering richer context than discretized-prompt methods. To reduce exposure bias, we employ a Frozen-LM Conditioning training strategy that conditions the LMs on predicted tokens from earlier checkpoints to reduce the gap between teacher-forcing training and autoregressive inference. We further employ DPO to better align outputs with human perceptual preferences. Experiments on Libri2Mix show that GenTSE surpasses previous LM-based systems in speech quality, intelligibility, and speaker consistency.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20978v1",
    "published_date": "2025-12-24 06:13:02 UTC",
    "updated_date": "2025-12-24 06:13:02 UTC"
  },
  {
    "arxiv_id": "2512.20974v1",
    "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
    "authors": [
      "Jingyang You",
      "Hanna Kurniawati"
    ],
    "abstract": "Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20974v1",
    "published_date": "2025-12-24 06:00:51 UTC",
    "updated_date": "2025-12-24 06:00:51 UTC"
  },
  {
    "arxiv_id": "2512.20968v1",
    "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
    "authors": [
      "Sirui Chen",
      "Jingji Chen",
      "Siqi Zhu",
      "Ziheng Jiang",
      "Yanghua Peng",
      "Xuehai Qian"
    ],
    "abstract": "Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.\n  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20968v1",
    "published_date": "2025-12-24 05:48:58 UTC",
    "updated_date": "2025-12-24 05:48:58 UTC"
  },
  {
    "arxiv_id": "2512.20959v1",
    "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
    "authors": [
      "An Luo",
      "Jin Du",
      "Fangqiao Tian",
      "Xun Xian",
      "Robert Specht",
      "Ganghua Wang",
      "Xuan Bi",
      "Charles Fleming",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Mingyi Hong",
      "Jie Ding"
    ],
    "abstract": "Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20959v1",
    "published_date": "2025-12-24 05:31:42 UTC",
    "updated_date": "2025-12-24 05:31:42 UTC"
  },
  {
    "arxiv_id": "2512.20958v1",
    "title": "ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design",
    "authors": [
      "R Yadunandan",
      "Nimisha Ghosh"
    ],
    "abstract": "De novo drug design is a crucial component of modern drug development, yet navigating the vast chemical space to find synthetically accessible, high-affinity candidates remains a significant challenge. Reinforcement Learning (RL) enhances this process by enabling multi-objective optimization and exploration of novel chemical space - capabilities that traditional supervised learning methods lack. In this work, we introduce \\textbf{ReACT-Drug}, a fully integrated, target-agnostic molecular design framework based on Reinforcement Learning. Unlike models requiring target-specific fine-tuning, ReACT-Drug utilizes a generalist approach by leveraging ESM-2 protein embeddings to identify similar proteins for a given target from a knowledge base such as Protein Data Base (PDB). Thereafter, the known drug ligands corresponding to such proteins are decomposed to initialize a fragment-based search space, biasing the agent towards biologically relevant subspaces. For each such fragment, the pipeline employs a Proximal Policy Optimization (PPO) agent guiding a ChemBERTa-encoded molecule through a dynamic action space of chemically valid, reaction-template-based transformations. This results in the generation of \\textit{de novo} drug candidates with competitive binding affinities and high synthetic accessibility, while ensuring 100\\% chemical validity and novelty as per MOSES benchmarking. This architecture highlights the potential of integrating structural biology, deep representation learning, and chemical synthesis rules to automate and accelerate rational drug design. The dataset and code are available at https://github.com/YadunandanRaman/ReACT-Drug/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20958v1",
    "published_date": "2025-12-24 05:29:35 UTC",
    "updated_date": "2025-12-24 05:29:35 UTC"
  },
  {
    "arxiv_id": "2512.20957v4",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
    "authors": [
      "Zhaoxi Zhang",
      "Yitong Duan",
      "Yanzhi Zhang",
      "Yiming Xu",
      "Weikang Li",
      "Jiahui Liang",
      "Deguo Xia",
      "Jizhou Huang",
      "Jiyan He",
      "Yunfang Wu"
    ],
    "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20957v4",
    "published_date": "2025-12-24 05:27:53 UTC",
    "updated_date": "2026-01-08 03:22:10 UTC"
  },
  {
    "arxiv_id": "2512.20954v1",
    "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
    "authors": [
      "Xiang Zhang",
      "Jiaqi Wei",
      "Yuejin Yang",
      "Zijie Qiu",
      "Yuhan Chen",
      "Zhiqiang Gao",
      "Muhammad Abdul-Mageed",
      "Laks V. S. Lakshmanan",
      "Wanli Ouyang",
      "Chenyu You",
      "Siqi Sun"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20954v1",
    "published_date": "2025-12-24 05:25:17 UTC",
    "updated_date": "2025-12-24 05:25:17 UTC"
  },
  {
    "arxiv_id": "2512.20950v1",
    "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
    "authors": [
      "Mohammad Mahdi Abootorabi",
      "Alireza Ghahramani Kure",
      "Mohammadali Mohammadkhani",
      "Sina Elahimanesh",
      "Mohammad Ali Ali Panah"
    ],
    "abstract": "This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages Published at the SemEval-2025 workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.20950v1",
    "published_date": "2025-12-24 05:14:40 UTC",
    "updated_date": "2025-12-24 05:14:40 UTC"
  },
  {
    "arxiv_id": "2512.22252v1",
    "title": "Graph Attention-based Adaptive Transfer Learning for Link Prediction",
    "authors": [
      "Huashen Lu",
      "Wensheng Gan",
      "Guoting Chen",
      "Zhichao Huang",
      "Philip S. Yu"
    ],
    "abstract": "Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ACM TIST, 9 tables, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2512.22252v1",
    "published_date": "2025-12-24 05:11:34 UTC",
    "updated_date": "2025-12-24 05:11:34 UTC"
  },
  {
    "arxiv_id": "2512.20949v1",
    "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
    "authors": [
      "Shize Liang",
      "Hongzhi Wang"
    ],
    "abstract": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20949v1",
    "published_date": "2025-12-24 05:10:19 UTC",
    "updated_date": "2025-12-24 05:10:19 UTC"
  },
  {
    "arxiv_id": "2512.20941v1",
    "title": "A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate",
    "authors": [
      "Yiren Shen",
      "Juan J. Alonso"
    ],
    "abstract": "Data-driven surrogate models are increasingly adopted to accelerate vehicle design. However, open-source multi-fidelity datasets and empirical guidelines linking dataset size to model performance remain limited. This study investigates the relationship between training data size and prediction accuracy for a graph neural network (GNN) based surrogate model for aerodynamic field prediction. We release an open-source, multi-fidelity aerodynamic dataset for double-delta wings, comprising 2448 flow snapshots across 272 geometries evaluated at angles of attack from 11 (degree) to 19 (degree) at Ma=0.3 using both Vortex Lattice Method (VLM) and Reynolds-Averaged Navier-Stokes (RANS) solvers. The geometries are generated using a nested Saltelli sampling scheme to support future dataset expansion and variance-based sensitivity analysis. Using this dataset, we conduct a preliminary empirical scaling study of the MF-VortexNet surrogate by constructing six training datasets with sizes ranging from 40 to 1280 snapshots and training models with 0.1 to 2.4 million parameters under a fixed training budget. We find that the test error decreases with data size with a power-law exponent of -0.6122, indicating efficient data utilization. Based on this scaling law, we estimate that the optimal sampling density is approximately eight samples per dimension in a d-dimensional design space. The results also suggest improved data utilization efficiency for larger surrogate models, implying a potential trade-off between dataset generation cost and model training budget.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20941v1",
    "published_date": "2025-12-24 04:53:11 UTC",
    "updated_date": "2025-12-24 04:53:11 UTC"
  },
  {
    "arxiv_id": "2512.22251v2",
    "title": "Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs",
    "authors": [
      "Pascal Passigan",
      "Kevin Zhu",
      "Angelina Ning"
    ],
    "abstract": "Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.22251v2",
    "published_date": "2025-12-24 04:42:25 UTC",
    "updated_date": "2025-12-31 17:30:56 UTC"
  },
  {
    "arxiv_id": "2512.20934v1",
    "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
    "authors": [
      "Shengguang Wu",
      "Xiaohan Wang",
      "Yuhui Zhang",
      "Hao Zhu",
      "Serena Yeung-Levy"
    ],
    "abstract": "Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Website: https://transductive-visualprogram.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2512.20934v1",
    "published_date": "2025-12-24 04:30:21 UTC",
    "updated_date": "2025-12-24 04:30:21 UTC"
  },
  {
    "arxiv_id": "2512.20932v2",
    "title": "Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy",
    "authors": [
      "Deepit Sapru"
    ],
    "abstract": "This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20932v2",
    "published_date": "2025-12-24 04:25:31 UTC",
    "updated_date": "2026-01-20 19:56:43 UTC"
  },
  {
    "arxiv_id": "2601.02386v1",
    "title": "Tree of Preferences for Diversified Recommendation",
    "authors": [
      "Hanyang Yuan",
      "Ning Tang",
      "Tongya Zheng",
      "Jiarong Xu",
      "Xintong Hu",
      "Renhong Huang",
      "Shunyu Liu",
      "Jiacong Hu",
      "Jiawei Chen",
      "Mingli Song"
    ],
    "abstract": "Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.02386v1",
    "published_date": "2025-12-24 04:13:17 UTC",
    "updated_date": "2025-12-24 04:13:17 UTC"
  },
  {
    "arxiv_id": "2512.20920v1",
    "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
    "authors": [
      "Ningyuan Liu",
      "Jing Yang",
      "Kaitong Cai",
      "Keze Wang"
    ],
    "abstract": "Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Under submission",
    "pdf_url": "https://arxiv.org/pdf/2512.20920v1",
    "published_date": "2025-12-24 03:56:58 UTC",
    "updated_date": "2025-12-24 03:56:58 UTC"
  },
  {
    "arxiv_id": "2512.20905v4",
    "title": "DiEC: Diffusion Embedded Clustering",
    "authors": [
      "Haidong Hu",
      "Xiaoyu Zheng",
      "Jin Zhou",
      "Yingxu Wang",
      "Rui Wang",
      "Pei Dong",
      "Shiyuan Han",
      "Lin Wang",
      "C. L. Philip Chen",
      "Tong Zhang",
      "Yuehui Chen"
    ],
    "abstract": "Deep clustering methods typically rely on a single, well-defined representation for clustering. In contrast, pretrained diffusion models provide abundant and diverse multi-scale representations across network layers and noise timesteps. However, a key challenge is how to efficiently identify the most clustering-friendly representation in the layer*timestep space. To address this issue, we propose Diffusion Embedded Clustering (DiEC), an unsupervised framework that performs clustering by leveraging optimal intermediate representations from pretrained diffusion models. DiEC systematically evaluates the clusterability of representations along the trajectory of network depth and noise timesteps. Meanwhile, an unsupervised search strategy is designed for recognizing the Clustering-optimal Layer (COL) and Clustering-optimal Timestep (COT) in the layer*timestep space of pretrained diffusion models, aiming to promote clustering performance and reduce computational overhead. DiEC is fine-tuned primarily with a structure-preserving DEC-style KL-divergence objective at the fixed COL + COT, together with a random-timestep diffusion denoising objective to maintain the generative capability of the pretrained model. Without relying on augmentation-based consistency constraints or contrastive learning, DiEC achieves excellent clustering performance across multiple benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20905v4",
    "published_date": "2025-12-24 03:10:00 UTC",
    "updated_date": "2026-01-21 09:28:12 UTC"
  },
  {
    "arxiv_id": "2512.20902v1",
    "title": "Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction",
    "authors": [
      "Siqi Mu",
      "Shuo Wen",
      "Yang Lu",
      "Ruihong Jiang",
      "Bo Ai"
    ],
    "abstract": "Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20902v1",
    "published_date": "2025-12-24 03:06:37 UTC",
    "updated_date": "2025-12-24 03:06:37 UTC"
  },
  {
    "arxiv_id": "2512.20898v1",
    "title": "DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction",
    "authors": [
      "Xiao Yu",
      "Zhaojie Fang",
      "Guanyu Zhou",
      "Yin Shen",
      "Huoling Luo",
      "Ye Li",
      "Ahmed Elazab",
      "Xiang Wan",
      "Ruiquan Ge",
      "Changmiao Wang"
    ],
    "abstract": "Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20898v1",
    "published_date": "2025-12-24 02:47:22 UTC",
    "updated_date": "2025-12-24 02:47:22 UTC"
  },
  {
    "arxiv_id": "2512.20884v1",
    "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
    "authors": [
      "Zan-Kai Chong",
      "Hiroyuki Ohsaki",
      "Bryan Ng"
    ],
    "abstract": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20884v1",
    "published_date": "2025-12-24 02:02:25 UTC",
    "updated_date": "2025-12-24 02:02:25 UTC"
  },
  {
    "arxiv_id": "2601.06063v1",
    "title": "The Environmental Impact of AI Servers and Sustainable Solutions",
    "authors": [
      "Aadi Patel",
      "Nikhil Mahalingam",
      "Rusheen Patel"
    ],
    "abstract": "The rapid expansion of artificial intelligence has significantly increased the electricity, water, and carbon demands of modern data centers, raising sustainability concerns. This study evaluates the environmental footprint of AI server operations and examines feasible technological and infrastructural strategies to mitigate these impacts. Using a literature-based methodology supported by quantitative projections and case-study analysis, we assessed trends in global electricity consumption, cooling-related water use, and carbon emissions. Projections indicate that global data center electricity demand may increase from approximately 415 TWh in 2024 to nearly 945 TWh by 2030, with AI workloads accounting for a disproportionate share of this growth. In the United States alone, AI servers are expected to drive annual increases in water consumption of 200--300 billion gallons and add 24--44 million metric tons of CO2 quivalent emissions by 2030. The results show that the design of the cooling system and the geographic location influence the environmental impact as strongly as the efficiency of the hardware. Advanced cooling technologies can reduce cooling energy by up to 50%, while location in low-carbon and water-secure regions can cut combined footprints by nearly half. In general, the study concludes that sustainable AI expansion requires coordinated improvements in cooling efficiency, renewable energy integration, and strategic deployment decisions.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "5 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06063v1",
    "published_date": "2025-12-24 01:09:06 UTC",
    "updated_date": "2025-12-24 01:09:06 UTC"
  },
  {
    "arxiv_id": "2601.06062v1",
    "title": "From Values to Frameworks: A Qualitative Study of Ethical Reasoning in Agentic AI Practitioners",
    "authors": [
      "Theodore Roberts",
      "Bahram Zarrin"
    ],
    "abstract": "Agentic artificial intelligence systems are autonomous technologies capable of pursuing complex goals with minimal human oversight and are rapidly emerging as the next frontier in AI. While these systems promise major gains in productivity, they also raise new ethical challenges. Prior research has examined how different populations prioritize Responsible AI values, yet little is known about how practitioners actually reason through the trade-offs inherent in designing these autonomous systems. This paper investigates the ethical reasoning of AI practitioners through qualitative interviews centered on structured dilemmas in agentic AI deployment. We find that the responses of practitioners do not merely reflect value preferences but rather align with three distinct reasoning frameworks. First is a Customer-Centric framework where choices are justified by business interests, legality, and user autonomy. Second is a Design-Centric framework emphasizing technical safeguards and system constraints. Third is an Ethics-Centric framework prioritizing social good and moral responsibility beyond compliance. We argue that these frameworks offer distinct and necessary insights for navigating ethical trade-offs. Consequently, providers of agentic AI must look beyond general principles and actively manage how these diverse reasoning frameworks are represented in their decision-making processes to ensure robust ethical outcomes.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 2 charts, 1 heatmap",
    "pdf_url": "https://arxiv.org/pdf/2601.06062v1",
    "published_date": "2025-12-24 00:58:41 UTC",
    "updated_date": "2025-12-24 00:58:41 UTC"
  },
  {
    "arxiv_id": "2512.20866v1",
    "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
    "authors": [
      "Haotian Lv",
      "Chao Li",
      "Jiangbo Dai",
      "Yuhui Zhang",
      "Zepeng Fan",
      "Yiqiu Tan",
      "Dawei Wang",
      "Binglei Xie"
    ],
    "abstract": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20866v1",
    "published_date": "2025-12-24 00:50:27 UTC",
    "updated_date": "2025-12-24 00:50:27 UTC"
  },
  {
    "arxiv_id": "2512.20861v2",
    "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
    "authors": [
      "Pierre Abillama",
      "Changwoo Lee",
      "Juechu Dong",
      "David Blaauw",
      "Dennis Sylvester",
      "Hun-Seok Kim"
    ],
    "abstract": "Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20861v2",
    "published_date": "2025-12-24 00:41:13 UTC",
    "updated_date": "2026-01-17 17:29:18 UTC"
  },
  {
    "arxiv_id": "2512.20856v1",
    "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "authors": [
      "NVIDIA",
      ":",
      "Aaron Blakeman",
      "Aaron Grattafiori",
      "Aarti Basant",
      "Abhibha Gupta",
      "Abhinav Khattar",
      "Adi Renduchintala",
      "Aditya Vavre",
      "Akanksha Shukla",
      "Akhiad Bercovich",
      "Aleksander Ficek",
      "Aleksandr Shaposhnikov",
      "Alex Kondratenko",
      "Alexander Bukharin",
      "Alexandre Milesi",
      "Ali Taghibakhshi",
      "Alisa Liu",
      "Amelia Barton",
      "Ameya Sunil Mahabaleshwarkar",
      "Amir Klein",
      "Amit Zuker",
      "Amnon Geifman",
      "Amy Shen",
      "Anahita Bhiwandiwalla",
      "Andrew Tao",
      "Anjulie Agrusa",
      "Ankur Verma",
      "Ann Guan",
      "Anubhav Mandarwal",
      "Arham Mehta",
      "Ashwath Aithal",
      "Ashwin Poojary",
      "Asif Ahamed",
      "Asit Mishra",
      "Asma Kuriparambil Thekkumpate",
      "Ayush Dattagupta",
      "Banghua Zhu",
      "Bardiya Sadeghi",
      "Barnaby Simkin",
      "Ben Lanir",
      "Benedikt Schifferer",
      "Besmira Nushi",
      "Bilal Kartal",
      "Bita Darvish Rouhani",
      "Boris Ginsburg",
      "Brandon Norick",
      "Brandon Soubasis",
      "Branislav Kisacanin",
      "Brian Yu",
      "Bryan Catanzaro",
      "Carlo del Mundo",
      "Chantal Hwang",
      "Charles Wang",
      "Cheng-Ping Hsieh",
      "Chenghao Zhang",
      "Chenhan Yu",
      "Chetan Mungekar",
      "Chintan Patel",
      "Chris Alexiuk",
      "Christopher Parisien",
      "Collin Neale",
      "Cyril Meurillon",
      "Damon Mosk-Aoyama",
      "Dan Su",
      "Dane Corneil",
      "Daniel Afrimi",
      "Daniel Lo",
      "Daniel Rohrer",
      "Daniel Serebrenik",
      "Daria Gitman",
      "Daria Levy",
      "Darko Stosic",
      "David Mosallanezhad",
      "Deepak Narayanan",
      "Dhruv Nathawani",
      "Dima Rekesh",
      "Dina Yared",
      "Divyanshu Kakwani",
      "Dong Ahn",
      "Duncan Riach",
      "Dusan Stosic",
      "Edgar Minasyan",
      "Edward Lin",
      "Eileen Long",
      "Eileen Peters Long",
      "Elad Segal",
      "Elena Lantz",
      "Ellie Evans",
      "Elliott Ning",
      "Eric Chung",
      "Eric Harper",
      "Eric Tramel",
      "Erick Galinkin",
      "Erik Pounds",
      "Evan Briones",
      "Evelina Bakhturina",
      "Evgeny Tsykunov",
      "Faisal Ladhak",
      "Fay Wang",
      "Fei Jia",
      "Felipe Soares",
      "Feng Chen",
      "Ferenc Galko",
      "Frank Sun",
      "Frankie Siino",
      "Gal Hubara Agam",
      "Ganesh Ajjanagadde",
      "Gantavya Bhatt",
      "Gargi Prasad",
      "George Armstrong",
      "Gerald Shen",
      "Gorkem Batmaz",
      "Grigor Nalbandyan",
      "Haifeng Qian",
      "Harsh Sharma",
      "Hayley Ross",
      "Helen Ngo",
      "Herbert Hum",
      "Herman Sahota",
      "Hexin Wang",
      "Himanshu Soni",
      "Hiren Upadhyay",
      "Huizi Mao",
      "Huy C Nguyen",
      "Huy Q Nguyen",
      "Iain Cunningham",
      "Ido Galil",
      "Ido Shahaf",
      "Igor Gitman",
      "Ilya Loshchilov",
      "Itamar Schen",
      "Itay Levy",
      "Ivan Moshkov",
      "Izik Golan",
      "Izzy Putterman",
      "Jan Kautz",
      "Jane Polak Scowcroft",
      "Jared Casper",
      "Jatin Mitra",
      "Jeffrey Glick",
      "Jenny Chen",
      "Jesse Oliver",
      "Jian Zhang",
      "Jiaqi Zeng",
      "Jie Lou",
      "Jimmy Zhang",
      "Jinhang Choi",
      "Jining Huang",
      "Joey Conway",
      "Joey Guman",
      "John Kamalu",
      "Johnny Greco",
      "Jonathan Cohen",
      "Joseph Jennings",
      "Joyjit Daw",
      "Julien Veron Vialard",
      "Junkeun Yi",
      "Jupinder Parmar",
      "Kai Xu",
      "Kan Zhu",
      "Kari Briski",
      "Katherine Cheung",
      "Katherine Luna",
      "Keith Wyss",
      "Keshav Santhanam",
      "Kevin Shih",
      "Kezhi Kong",
      "Khushi Bhardwaj",
      "Kirthi Shankar",
      "Krishna C. Puvvada",
      "Krzysztof Pawelec",
      "Kumar Anik",
      "Lawrence McAfee",
      "Laya Sleiman",
      "Leon Derczynski",
      "Li Ding",
      "Lizzie Wei",
      "Lucas Liebenwein",
      "Luis Vega",
      "Maanu Grover",
      "Maarten Van Segbroeck",
      "Maer Rodrigues de Melo",
      "Mahdi Nazemi",
      "Makesh Narsimhan Sreedhar",
      "Manoj Kilaru",
      "Maor Ashkenazi",
      "Marc Romeijn",
      "Marcin Chochowski",
      "Mark Cai",
      "Markus Kliegl",
      "Maryam Moosaei",
      "Matt Kulka",
      "Matvei Novikov",
      "Mehrzad Samadi",
      "Melissa Corpuz",
      "Mengru Wang",
      "Meredith Price",
      "Michael Andersch",
      "Michael Boone",
      "Michael Evans",
      "Miguel Martinez",
      "Mikail Khona",
      "Mike Chrzanowski",
      "Minseok Lee",
      "Mohammad Dabbah",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Nabin Mulepati",
      "Najeeb Nabwani",
      "Natalie Hereth",
      "Nave Assaf",
      "Negar Habibi",
      "Neta Zmora",
      "Netanel Haber",
      "Nicola Sessions",
      "Nidhi Bhatia",
      "Nikhil Jukar",
      "Nikki Pope",
      "Nikolai Ludwig",
      "Nima Tajbakhsh",
      "Nir Ailon",
      "Nirmal Juluru",
      "Nishant Sharma",
      "Oleksii Hrinchuk",
      "Oleksii Kuchaiev",
      "Olivier Delalleau",
      "Oluwatobi Olabiyi",
      "Omer Ullman Argov",
      "Omri Puny",
      "Oren Tropp",
      "Ouye Xie",
      "Parth Chadha",
      "Pasha Shamis",
      "Paul Gibbons",
      "Pavlo Molchanov",
      "Pawel Morkisz",
      "Peter Dykas",
      "Peter Jin",
      "Pinky Xu",
      "Piotr Januszewski",
      "Pranav Prashant Thombre",
      "Prasoon Varshney",
      "Pritam Gundecha",
      "Przemek Tredak",
      "Qing Miao",
      "Qiyu Wan",
      "Rabeeh Karimi Mahabadi",
      "Rachit Garg",
      "Ran El-Yaniv",
      "Ran Zilberstein",
      "Rasoul Shafipour",
      "Rich Harang",
      "Rick Izzo",
      "Rima Shahbazyan",
      "Rishabh Garg",
      "Ritika Borkar",
      "Ritu Gala",
      "Riyad Islam",
      "Robert Hesse",
      "Roger Waleffe",
      "Rohit Watve",
      "Roi Koren",
      "Ruoxi Zhang",
      "Russell Hewett",
      "Russell J. Hewett",
      "Ryan Prenger",
      "Ryan Timbrook",
      "Sadegh Mahdavi",
      "Sahil Modi",
      "Samuel Kriman",
      "Sangkug Lim",
      "Sanjay Kariyappa",
      "Sanjeev Satheesh",
      "Saori Kaji",
      "Satish Pasumarthi",
      "Saurav Muralidharan",
      "Sean Narentharen",
      "Sean Narenthiran",
      "Seonmyeong Bak",
      "Sergey Kashirsky",
      "Seth Poulos",
      "Shahar Mor",
      "Shanmugam Ramasamy",
      "Shantanu Acharya",
      "Shaona Ghosh",
      "Sharath Turuvekere Sreenivas",
      "Shelby Thomas",
      "Shiqing Fan",
      "Shreya Gopal",
      "Shrimai Prabhumoye",
      "Shubham Pachori",
      "Shubham Toshniwal",
      "Shuoyang Ding",
      "Siddharth Singh",
      "Simeng Sun",
      "Smita Ithape",
      "Somshubra Majumdar",
      "Soumye Singhal",
      "Stas Sergienko",
      "Stefania Alborghetti",
      "Stephen Ge",
      "Sugam Dipak Devare",
      "Sumeet Kumar Barua",
      "Suseella Panguluri",
      "Suyog Gupta",
      "Sweta Priyadarshi",
      "Syeda Nahida Akter",
      "Tan Bui",
      "Teodor-Dumitru Ene",
      "Terry Kong",
      "Thanh Do",
      "Tijmen Blankevoort",
      "Tim Moon",
      "Tom Balough",
      "Tomer Asida",
      "Tomer Bar Natan",
      "Tomer Ronen",
      "Tugrul Konuk",
      "Twinkle Vashishth",
      "Udi Karpas",
      "Ushnish De",
      "Vahid Noorozi",
      "Vahid Noroozi",
      "Venkat Srinivasan",
      "Venmugil Elango",
      "Victor Cui",
      "Vijay Korthikanti",
      "Vinay Rao",
      "Vitaly Kurin",
      "Vitaly Lavrukhin",
      "Vladimir Anisimov",
      "Wanli Jiang",
      "Wasi Uddin Ahmad",
      "Wei Du",
      "Wei Ping",
      "Wenfei Zhou",
      "Will Jennings",
      "William Zhang",
      "Wojciech Prazuch",
      "Xiaowei Ren",
      "Yashaswi Karnati",
      "Yejin Choi",
      "Yev Meyer",
      "Yi-Fu Wu",
      "Yian Zhang",
      "Yigong Qin",
      "Ying Lin",
      "Yonatan Geifman",
      "Yonggan Fu",
      "Yoshi Subara",
      "Yoshi Suhara",
      "Yubo Gao",
      "Zach Moshe",
      "Zhen Dong",
      "Zhongbo Zhu",
      "Zihan Liu",
      "Zijia Chen",
      "Zijie Yan"
    ],
    "abstract": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.20856v1",
    "published_date": "2025-12-24 00:24:05 UTC",
    "updated_date": "2025-12-24 00:24:05 UTC"
  }
]
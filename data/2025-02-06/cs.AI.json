{
  "date": "2025-02-06",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-02-06 的 arXiv 中文 TLDR 快报！  \n\n今天 arXiv 的论文主要聚焦于 AI 和 LLM 的优化、多模态生成、强化学习以及医疗和机器人应用等领域，亮点包括 LLM 在安全对齐和多任务生成中的创新进展，以及著名学者如 Jitendra Malik 和 Doina Precup 的作品，这些论文展示了 AI 模型在实际应用中的鲁棒性和扩展潜力。接下来，我将优先讨论重要且有话题度的论文，如 LLM 优化和多模态生成相关内容，并将相关主题归类简要概述，其他论文则快速掠过。  \n\n### 重点论文讨论  \n\n**1. TruthFlow: Truthful LLM Generation via Representation Flow Correction（TruthFlow: 通过表示流修正实现 LLM 的真实生成）**  \n这篇论文由 Hanyu Wang 等作者提出，聚焦于解决 LLM 的幻觉问题（hallucination）。主要贡献是通过 Flow Matching 技术学习查询特定的修正向量，将模型表示从幻觉状态转换为真实状态，并在 TruthfulQA 等基准上显著提升生成真实性，同时展示模型在其他数据集上的迁移能力。该工作对 LLM 安全性和可靠性有重要启示，值得关注。  \n\n**2. Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator（通过对比散度优化偏好：你的奖励模型其实是 NLL 估计器）**  \n作者包括 Zhuotong Chen 和 Mohammad Ghavamzadeh，这篇论文创新性地将偏好优化（Preference Optimization）框架化为最小化负对数似然（NLL），并使用对比散度采样策略生成更有效的负样本。贡献包括提出 MC-PO 和 OnMC-PO 算法，在对齐基准上超越现有 SOTA 方法。该工作强化了 LLM 奖励模型的理论基础，对强化学习和对齐任务有实际影响。  \n\n**3. WaferLLM: A Wafer-Scale LLM Inference System（WaferLLM: 晶圆级 LLM 推理系统）**  \n由 Congjie He 等作者撰写，这篇论文针对 LLM 在晶圆级加速器上的优化，提出 PLMR 模型和 MeshGEMM/GEMV 算法，实现高效并行计算。关键发现是 WaferLLM 在 Llama3-8B 和 Qwen2-72B 上分别达到 2700 和 840 toks/sec/req 的解码速度，比现有系统快 39 倍且更节能。该工作对硬件加速 LLM 有重大实际价值。  \n\n**4. DexterityGen: Foundation Controller for Unprecedented Dexterity（DexterityGen: 基础控制器实现前所未有的灵巧性）**  \nJitendra Malik 等知名学者参与，这篇论文提出 RL 预训练数据结合人类遥操作的 DexterityGen 框架，实现机器人手部灵巧操作，如物体重定向和工具使用。贡献包括在模拟和真实环境中实现高精度任务，显著提升机器人稳定性。该工作展示了 RL 在机器人领域的潜力，Jitendra Malik 的参与使其更具话题度。  \n\n**5. FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks（FocalCodec: 通过焦点调制网络实现低比特率语音编码）**  \n作者 Luca Della Libera 等提出一种基于单一二进制码本的低比特率语音编码模型。关键发现是 FocalCodec 在 0.16-0.65 kbps 下保持高质量语义和声学信息，并在多语言和噪声环境中表现出色。该工作对高效语音生成有重要启示。  \n\n**6. Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture（探测视觉-语言-动作模型的符号状态并集成到认知架构）**  \nDoina Precup 等作者探讨 VLA 模型在机器人中的应用，通过探测隐藏层提取符号表示，提升鲁棒性。贡献包括在 LIBERO 任务上实现高精度符号状态编码，并构建 DIARC-OpenVLA 系统。该论文将认知架构与 AI 结合，具有跨领域影响。  \n\n**7. WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs（WorldSense: 评估多模态 LLM 的真实世界全模态理解）**  \n作者 Jack Hong 等提出首个全模态视频理解基准，包含视觉、音频和文本。关键发现是现有模型在真实场景下表现不佳（准确率 48%），并通过 26 个任务评估模型。该工作为多模态 LLM 评估提供新标准。  \n\n**8. Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers（Aligner-Encoders: 自注意力 Transformer 可以是自转录器）**  \nAdam Stooke 等作者发现 Transformer 编码器可内部处理对齐，实现更高效的语音识别。贡献包括提出 Aligner-Encoder 模型，比 RNN-T 和 AED 快 2 倍和 16 倍。该论文创新了语音处理技术。  \n\n**9. AnyPlace: Learning Generalized Object Placement for Robot Manipulation（AnyPlace: 学习通用的物体放置以实现机器人操作）**  \n作者 Yuchi Zhao 等提出基于合成数据的两阶段方法，预测物体放置姿势。关键发现是结合 VLM 识别粗略位置，提高了机器人放置精度。该工作在模拟和真实环境中表现出色。  \n\n**10. ImprovNet: Generating Controllable Musical Improvisations with Iterative Corruption Refinement（ImprovNet: 通过迭代损坏精炼生成可控音乐即兴）**  \nKeshav Bhandari 等作者开发 Transformer 架构，支持音乐风格转移。贡献包括统一模型处理多种音乐任务，并在主观评估中超越基线。该论文扩展了 AI 在音乐生成的应用。  \n\n### 相关论文简要概述  \n以上论文多聚焦 LLM 和多模态优化，展示了 AI 在生成、安全和机器人领域的进展。其他如量子计算（e.g., Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer）和医疗应用（e.g., MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification）论文虽有创新，但非核心焦点，这里快速掠过：它们分别在零样本元学习和医疗时序分类中提升了性能，但细节较技术化，不如 LLM 相关工作有广泛话题度。剩余论文（如音频处理、图像生成等）则更多是领域特定优化，贡献如高效量化或鲁棒性提升，但篇幅有限，仅提及其在特定任务中的小幅改进。  \n\n今天的 arXiv 更新展示了 AI 领域的多样性，LLM 优化论文尤其值得追踪。如果你对特定主题感兴趣，下次快报可更针对性调整！",
  "papers": [
    {
      "arxiv_id": "2502.04573v1",
      "title": "Zero-shot Meta-learning for Tabular Prediction Tasks with Adversarially Pre-trained Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Yulun Wu",
        "Doron L. Bergman"
      ],
      "abstract": "We present an Adversarially Pre-trained Transformer (APT) that is able to\nperform zero-shot meta-learning on tabular prediction tasks without\npre-training on any real-world dataset, extending on the recent development of\nPrior-Data Fitted Networks (PFNs) and TabPFN. Specifically, APT is pre-trained\nwith adversarial synthetic data agents, who continue to shift their underlying\ndata generating distribution and deliberately challenge the model with\ndifferent synthetic datasets. In addition, we propose a mixture block\narchitecture that is able to handle classification tasks with arbitrary number\nof classes, addressing the class size limitation -- a crucial weakness of prior\ndeep tabular zero-shot learners. In experiments, we show that our framework\nmatches state-of-the-art performance on small classification tasks without\nfiltering on dataset characteristics such as number of classes and number of\nmissing values, while maintaining an average runtime under one second. On\ncommon benchmark dataset suites in both classification and regression, we show\nthat adversarial pre-training was able to enhance TabPFN's performance. In our\nanalysis, we demonstrate that the adversarial synthetic data agents were able\nto generate a more diverse collection of data compared to the ordinary random\ngenerator in TabPFN. In addition, we demonstrate that our mixture block neural\ndesign has improved generalizability and greatly accelerated pre-training.",
      "tldr_zh": "这篇论文提出了 Adversarially Pre-trained Transformer (APT)，一种无需真实数据集预训练的模型，用于表格预测任务的 zero-shot meta-learning，基于 Prior-Data Fitted Networks (PFNs) 和 TabPFN 的扩展。APT 通过对抗性合成数据代理进行预训练，这些代理不断改变数据生成分布以挑战模型，并引入混合块架构来处理任意类数的分类任务，解决了现有方法的类数限制。实验结果显示，APT 在小分类任务上与最先进方法性能相当，运行时间不到一秒，并在常见基准数据集上提升了 TabPFN 的表现，同时证明了对抗预训练提高了数据多样性和模型泛化性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04573v1",
      "published_date": "2025-02-06 23:58:11 UTC",
      "updated_date": "2025-02-06 23:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:51:35.613766"
    },
    {
      "arxiv_id": "2502.04567v1",
      "title": "Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator",
      "title_zh": "通过对比散度的偏好优化：你的奖励模型其实是一个 NLL 估计器",
      "authors": [
        "Zhuotong Chen",
        "Fang Liu",
        "Xuan Zhu",
        "Yanjun Qi",
        "Mohammad Ghavamzadeh"
      ],
      "abstract": "Existing studies on preference optimization (PO) have centered on\nconstructing pairwise preference data following simple heuristics, such as\nmaximizing the margin between preferred and dispreferred completions based on\nhuman (or AI) ranked scores. However, none of these heuristics has a full\ntheoretical justification. In this work, we develop a novel PO framework that\nprovides theoretical guidance to effectively sample dispreferred completions.\nTo achieve this, we formulate PO as minimizing the negative log-likelihood\n(NLL) of a probability model and propose to estimate its normalization constant\nvia a sampling strategy. As we will demonstrate, these estimative samples can\nact as dispreferred completions in PO. We then select contrastive divergence\n(CD) as the sampling strategy, and propose a novel MC-PO algorithm that applies\nthe Monte Carlo (MC) kernel from CD to sample hard negatives w.r.t. the\nparameterized reward model. Finally, we propose the OnMC-PO algorithm, an\nextension of MC-PO to the online setting. On popular alignment benchmarks,\nMC-PO outperforms existing SOTA baselines, and OnMC-PO leads to further\nimprovement.",
      "tldr_zh": "本研究提出了一种新颖的偏好优化（PO）框架，将 PO 表述为最小化负对数似然（NLL）的概率模型，并通过采样策略估计归一化常数，以理论指导有效采样非偏好完成。作者采用对比散度（Contrastive Divergence, CD）作为采样方法，开发了 MC-PO 算法，使用 Monte Carlo (MC) 内核采样相对于参数化奖励模型的硬负样本，并扩展为在线版本的 OnMC-PO 算法。在流行的对齐基准测试中，MC-PO 超过了现有 SOTA 基线，而 OnMC-PO 进一步提升了性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04567v1",
      "published_date": "2025-02-06 23:45:08 UTC",
      "updated_date": "2025-02-06 23:45:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:51:47.632908"
    },
    {
      "arxiv_id": "2502.04563v2",
      "title": "WaferLLM: A Wafer-Scale LLM Inference System",
      "title_zh": "WaferLLM：晶圆级大语言模型推理",
      "authors": [
        "Congjie He",
        "Yeqi Huang",
        "Pei Mu",
        "Ziming Miao",
        "Jilong Xue",
        "Lingxiao Ma",
        "Fan Yang",
        "Luo Mai"
      ],
      "abstract": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing\ntechnologies, integrating hundreds of thousands of AI cores in a mesh-based\narchitecture with large distributed on-chip memory (tens of GB in total) and\nultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM\ninference systems, optimized for shared memory architectures like GPUs, fail to\nfully exploit these accelerators.\n  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM\nis guided by a novel PLMR model (pronounced as \"Plummer\") that captures the\nunique hardware characteristics of wafer-scale architectures. Leveraging this\nmodel, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the\nutilization of hundreds of thousands of on-chip cores. It also introduces\nMeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to\nscale effectively on wafer-scale accelerators.\n  Evaluations show that WaferLLM achieves 200$\\times$ better wafer-scale\naccelerator utilization than state-of-the-art systems. On a commodity\nwafer-scale accelerator, WaferLLM delivers 606$\\times$ faster and 22$\\times$\nmore energy-efficient GEMV compared to an advanced GPU. For LLMs, based on\n16-bit data type, WaferLLM achieves 2700 toks/sec/req decode speed on Llama3-8B\nmodel and 840 toks/sec/req decode speed on Qwen2-72B model, which enables\n39$\\times$ faster decoding with 1.7$\\times$ better energy efficiency. We\nanticipate these numbers will grow significantly as wafer-scale AI models,\nsoftware, and hardware continue to mature.",
      "tldr_zh": "本文提出 WaferLLM，一种针对晶圆级 AI 加速器的首创 LLM 推理系统，利用 PLMR 模型捕捉其独特硬件特性，如大量分布式片上内存和超高带宽。WaferLLM 引入晶圆级 LLM 并行性、MeshGEMM 和 MeshGEMV 优化数万个片上核心的利用率，实现高效的 GEMM 和 GEMV 操作。实验显示，该系统比现有系统提高 200 倍加速器利用率，并在 Llama3-8B 上达到 2700 toks/sec/req 的解码速度，在 Qwen2-72B 上达到 840 toks/sec/req，同时实现 39 倍更快解码和 1.7 倍更好的能效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.DC",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04563v2",
      "published_date": "2025-02-06 23:32:19 UTC",
      "updated_date": "2025-02-18 12:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:00.169013"
    },
    {
      "arxiv_id": "2502.04558v1",
      "title": "Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture",
      "title_zh": "针对视觉-语言-动作模型的符号状态探查及其集成到认知架构中",
      "authors": [
        "Hong Lu",
        "Hengxu Li",
        "Prithviraj Singh Shahani",
        "Stephanie Herbers",
        "Matthias Scheutz"
      ],
      "abstract": "Vision-language-action (VLA) models hold promise as generalist robotics\nsolutions by translating visual and linguistic inputs into robot actions, yet\nthey lack reliability due to their black-box nature and sensitivity to\nenvironmental changes. In contrast, cognitive architectures (CA) excel in\nsymbolic reasoning and state monitoring but are constrained by rigid predefined\nexecution. This work bridges these approaches by probing OpenVLA's hidden\nlayers to uncover symbolic representations of object properties, relations, and\naction states, enabling integration with a CA for enhanced interpretability and\nrobustness. Through experiments on LIBERO-spatial pick-and-place tasks, we\nanalyze the encoding of symbolic states across different layers of OpenVLA's\nLlama backbone. Our probing results show consistently high accuracies (> 0.90)\nfor both object and action states across most layers, though contrary to our\nhypotheses, we did not observe the expected pattern of object states being\nencoded earlier than action states. We demonstrate an integrated DIARC-OpenVLA\nsystem that leverages these symbolic representations for real-time state\nmonitoring, laying the foundation for more interpretable and reliable robotic\nmanipulation.",
      "tldr_zh": "本论文探讨了 Vision-Language-Action (VLA) 模型的局限性，包括其黑箱性质和对环境变化的敏感性，并通过探测 OpenVLA 的隐藏层，提取对象属性、关系和动作状态的符号表示，以整合到 Cognitive Architecture (CA) 中，提升系统的可解释性和鲁棒性。实验在 LIBERO-spatial 拾取和放置任务上分析了 OpenVLA 的 Llama 骨干网不同层对符号状态的编码，结果显示对象和动作状态的准确率均超过 0.90，但对象状态并非如预期在更早层编码。最终，该研究展示了 DIARC-OpenVLA 集成系统，实现实时状态监控，为更可靠的机器人操作奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 Pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04558v1",
      "published_date": "2025-02-06 23:11:11 UTC",
      "updated_date": "2025-02-06 23:11:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:12.188407"
    },
    {
      "arxiv_id": "2502.04556v1",
      "title": "TruthFlow: Truthful LLM Generation via Representation Flow Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyu Wang",
        "Bochuan Cao",
        "Yuanpu Cao",
        "Jinghui Chen"
      ],
      "abstract": "Large language models (LLMs) are known to struggle with consistently\ngenerating truthful responses. While various representation intervention\ntechniques have been proposed, these methods typically apply a universal\nrepresentation correction vector to all input queries, limiting their\neffectiveness against diverse queries in practice. In this study, we introduce\nTruthFlow, a novel method that leverages the Flow Matching technique for\nquery-specific truthful representation correction. Specifically, TruthFlow\nfirst uses a flow model to learn query-specific correction vectors that\ntransition representations from hallucinated to truthful states. Then, during\ninference, the trained flow model generates these correction vectors to enhance\nthe truthfulness of LLM outputs. Experimental results demonstrate that\nTruthFlow significantly improves performance on open-ended generation tasks\nacross various advanced LLMs evaluated on TruthfulQA. Moreover, the trained\nTruthFlow model exhibits strong transferability, performing effectively on\nother unseen hallucination benchmarks.",
      "tldr_zh": "该研究针对大型语言模型 (LLMs) 在生成真实响应方面存在的挑战，提出了一种新方法 TruthFlow，通过 Flow Matching 技术实现查询特定的表示修正。具体来说，TruthFlow 使用流模型学习从幻觉到真实状态的修正向量，并在推理过程中应用这些向量来提升 LLM 输出真实性。实验结果显示，TruthFlow 在 TruthfulQA 基准上的开放生成任务中显著提高了各种高级 LLM 的性能，且该模型具有强转移性，在其他未见过的幻觉基准上也表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04556v1",
      "published_date": "2025-02-06 23:10:14 UTC",
      "updated_date": "2025-02-06 23:10:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:22.197677"
    },
    {
      "arxiv_id": "2502.04554v1",
      "title": "Unifying and Optimizing Data Values for Selection via Sequential-Decision-Making",
      "title_zh": "通过顺序决策统一和优化用于选择的数据价值",
      "authors": [
        "Hongliang Chi",
        "Qiong Wu",
        "Zhengyi Zhou",
        "Jonathan Light",
        "Emily Dodwell",
        "Yao Ma"
      ],
      "abstract": "Data selection has emerged as a crucial downstream application of data\nvaluation. While existing data valuation methods have shown promise in\nselection tasks, the theoretical foundations and full potential of using data\nvalues for selection remain largely unexplored. In this work, we first\ndemonstrate that data values applied for selection can be naturally\nreformulated as a sequential-decision-making problem, where the optimal data\nvalue can be derived through dynamic programming. We show this framework\nunifies and reinterprets existing methods like Data Shapley through the lens of\napproximate dynamic programming, specifically as myopic reward function\napproximations to this sequential problem. Furthermore, we analyze how\nsequential data selection optimality is affected when the ground-truth utility\nfunction exhibits monotonic submodularity with curvature. To address the\ncomputational challenges in obtaining optimal data values, we propose an\nefficient approximation scheme using learned bipartite graphs as surrogate\nutility models, ensuring greedy selection is still optimal when the surrogate\nutility is correctly specified and learned. Extensive experiments demonstrate\nthe effectiveness of our approach across diverse datasets.",
      "tldr_zh": "本文将数据选择问题重新表述为一个顺序决策问题，通过动态编程求解最优数据值，从而统一了现有方法如Data Shapley，并将其视为近似动态编程的近视奖励函数。研究分析了当效用函数具有单调子模性和曲率时，对顺序数据选择的优化影响。作者提出了一种高效近似方案，使用学习到的二分图作为代理效用模型，确保贪婪选择在模型正确时达到最优。实验在多种数据集上验证了该方法的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04554v1",
      "published_date": "2025-02-06 23:03:10 UTC",
      "updated_date": "2025-02-06 23:03:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:34.278489"
    },
    {
      "arxiv_id": "2502.05232v1",
      "title": "Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Stooke",
        "Rohit Prabhavalkar",
        "Khe Chai Sim",
        "Pedro Moreno Mengibar"
      ],
      "abstract": "Modern systems for automatic speech recognition, including the RNN-Transducer\nand Attention-based Encoder-Decoder (AED), are designed so that the encoder is\nnot required to alter the time-position of information from the audio sequence\ninto the embedding; alignment to the final text output is processed during\ndecoding. We discover that the transformer-based encoder adopted in recent\nyears is actually capable of performing the alignment internally during the\nforward pass, prior to decoding. This new phenomenon enables a simpler and more\nefficient model, the \"Aligner-Encoder\". To train it, we discard the dynamic\nprogramming of RNN-T in favor of the frame-wise cross-entropy loss of AED,\nwhile the decoder employs the lighter text-only recurrence of RNN-T without\nlearned cross-attention -- it simply scans embedding frames in order from the\nbeginning, producing one token each until predicting the end-of-message. We\nconduct experiments demonstrating performance remarkably close to the state of\nthe art, including a special inference configuration enabling long-form\nrecognition. In a representative comparison, we measure the total inference\ntime for our model to be 2x faster than RNN-T and 16x faster than AED. Lastly,\nwe find that the audio-text alignment is clearly visible in the self-attention\nweights of a certain layer, which could be said to perform \"self-transduction\".",
      "tldr_zh": "本文发现，Self-Attention Transformers 能够在前向传递中自行进行音频-文本对齐，从而提出 Aligner-Encoder 模型，作为一种更简单高效的语音识别框架。训练方法结合 Attention-based Encoder-Decoder (AED) 的帧级交叉熵损失和 RNN-Transducer (RNN-T) 的轻量级文本循环解码器，而无需学到的交叉注意力。实验结果显示，该模型性能接近最先进水平，推理速度比 RNN-T 快 2 倍、比 AED 快 16 倍，并在自注意力权重中清晰显示“Self-Transducers”对齐现象。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.05232v1",
      "published_date": "2025-02-06 22:09:52 UTC",
      "updated_date": "2025-02-06 22:09:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:47.349235"
    },
    {
      "arxiv_id": "2502.04531v1",
      "title": "AnyPlace: Learning Generalized Object Placement for Robot Manipulation",
      "title_zh": "AnyPlace：学习泛化物体",
      "authors": [
        "Yuchi Zhao",
        "Miroslav Bogdanovic",
        "Chengyuan Luo",
        "Steven Tohme",
        "Kourosh Darvish",
        "Alán Aspuru-Guzik",
        "Florian Shkurti",
        "Animesh Garg"
      ],
      "abstract": "Object placement in robotic tasks is inherently challenging due to the\ndiversity of object geometries and placement configurations. To address this,\nwe propose AnyPlace, a two-stage method trained entirely on synthetic data,\ncapable of predicting a wide range of feasible placement poses for real-world\ntasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to\nidentify rough placement locations, we focus only on the relevant regions for\nlocal placement, which enables us to train the low-level\nplacement-pose-prediction model to capture diverse placements efficiently. For\ntraining, we generate a fully synthetic dataset of randomly generated objects\nin different placement configurations (insertion, stacking, hanging) and train\nlocal placement-prediction models. We conduct extensive evaluations in\nsimulation, demonstrating that our method outperforms baselines in terms of\nsuccess rate, coverage of possible placement modes, and precision. In\nreal-world experiments, we show how our approach directly transfers models\ntrained purely on synthetic data to the real world, where it successfully\nperforms placements in scenarios where other models struggle -- such as with\nvarying object geometries, diverse placement modes, and achieving high\nprecision for fine placement. More at: https://any-place.github.io.",
      "tldr_zh": "本论文提出 AnyPlace，一种两阶段方法，用于机器人操作中的物体放置，旨在处理物体几何多样性和放置配置的挑战。该方法利用 Vision-Language Model (VLM) 识别粗略放置位置，并专注于相关区域训练低级放置姿势预测模型，所有训练均基于合成数据生成，包括插入、堆叠和悬挂等配置。在模拟和真实世界实验中，AnyPlace 比基线模型在成功率、放置模式覆盖和精度上表现出显著优势，能够直接从合成数据转移到真实场景中处理复杂物体几何和精细放置。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04531v1",
      "published_date": "2025-02-06 22:04:13 UTC",
      "updated_date": "2025-02-06 22:04:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:52:58.953746"
    },
    {
      "arxiv_id": "2502.04530v1",
      "title": "Robust Probabilistic Model Checking with Continuous Reward Domains",
      "title_zh": "鲁棒概率模型检查：连续奖励域",
      "authors": [
        "Xiaotong Ji",
        "Hanchun Wang",
        "Antonio Filieri",
        "Ilenia Epifani"
      ],
      "abstract": "Probabilistic model checking traditionally verifies properties on the\nexpected value of a measure of interest. This restriction may fail to capture\nthe quality of service of a significant proportion of a system's runs,\nespecially when the probability distribution of the measure of interest is\npoorly represented by its expected value due to heavy-tail behaviors or\nmultiple modalities. Recent works inspired by distributional reinforcement\nlearning use discrete histograms to approximate integer reward distribution,\nbut they struggle with continuous reward space and present challenges in\nbalancing accuracy and scalability. We propose a novel method for handling both\ncontinuous and discrete reward distributions in Discrete Time Markov Chains\nusing moment matching with Erlang mixtures. By analytically deriving\nhigher-order moments through Moment Generating Functions, our method\napproximates the reward distribution with theoretically bounded error while\npreserving the statistical properties of the true distribution. This detailed\ndistributional insight enables the formulation and robust model checking of\nquality properties based on the entire reward distribution function, rather\nthan restricting to its expected value. We include a theoretical foundation\nensuring bounded approximation errors, along with an experimental evaluation\ndemonstrating our method's accuracy and scalability in practical model-checking\nproblems.",
      "tldr_zh": "传统概率模型检查通常只验证奖励的期望值，这可能无法准确捕捉系统运行的质量，尤其是在奖励分布存在重尾行为或多模态时。  \n本文提出一种新方法，使用 Erlang mixtures 的矩匹配（moment matching）来处理离散时间 Markov 链中的连续和离散奖励分布，通过 Moment Generating Functions 分析推导高阶矩，确保奖励分布近似的理论误差有界。  \n这种方法允许基于整个奖励分布函数进行质量属性的鲁棒模型检查，而非仅限于期望值，并通过实验验证展示了其在实际问题中的准确性和可伸缩性。",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by the 20th International Conference on Software Engineering\n  for Adaptive and Self-Managing Systems 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04530v1",
      "published_date": "2025-02-06 22:03:18 UTC",
      "updated_date": "2025-02-06 22:03:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:53:11.168751"
    },
    {
      "arxiv_id": "2502.04522v4",
      "title": "ImprovNet -- Generating Controllable Musical Improvisations with Iterative Corruption Refinement",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Bhandari",
        "Sungkyun Chang",
        "Tongyu Lu",
        "Fareza R. Enus",
        "Louis B. Bradshaw",
        "Dorien Herremans",
        "Simon Colton"
      ],
      "abstract": "Despite deep learning's remarkable advances in style transfer across various\ndomains, generating controllable performance-level musical style transfer for\ncomplete symbolically represented musical works remains a challenging area of\nresearch. Much of this is owed to limited datasets, especially for genres such\nas jazz, and the lack of unified models that can handle multiple music\ngeneration tasks. This paper presents ImprovNet, a transformer-based\narchitecture that generates expressive and controllable musical improvisations\nthrough a self-supervised corruption-refinement training strategy. The\nimprovisational style transfer is aimed at making meaningful modifications to\none or more musical elements - melody, harmony or rhythm of the original\ncomposition with respect to the target genre. ImprovNet unifies multiple\ncapabilities within a single model: it can perform cross-genre and intra-genre\nimprovisations, harmonize melodies with genre-specific styles, and execute\nshort prompt continuation and infilling tasks. The model's iterative generation\nframework allows users to control the degree of style transfer and structural\nsimilarity to the original composition. Objective and subjective evaluations\ndemonstrate ImprovNet's effectiveness in generating musically coherent\nimprovisations while maintaining structural relationships with the original\npieces. The model outperforms Anticipatory Music Transformer in short\ncontinuation and infilling tasks and successfully achieves recognizable genre\nconversion, with 79\\% of participants correctly identifying jazz-style\nimprovisations of classical pieces. Our code and demo page can be found at\nhttps://github.com/keshavbhandari/improvnet.",
      "tldr_zh": "本论文提出 ImprovNet，一种基于 Transformer 的架构，用于生成可控的音乐即兴表演，通过自监督的 corruption-refinement 训练策略，实现对音乐元素（如旋律、和声、节奏）的风格转移。ImprovNet 整合多种功能，包括跨流派和内部流派的即兴、和声添加、短提示延续及填充任务，并允许用户通过迭代生成框架控制风格转移程度和原作结构相似度。实验评估表明，该模型在音乐生成任务中优于 Anticipatory Music Transformer，生成音乐连贯性强，且79% 的参与者能正确识别流派转换效果。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "10 pages, 6 figures, IJCNN 2025 conference",
      "pdf_url": "http://arxiv.org/pdf/2502.04522v4",
      "published_date": "2025-02-06 21:45:38 UTC",
      "updated_date": "2025-05-16 14:56:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:53:23.506846"
    },
    {
      "arxiv_id": "2502.04515v1",
      "title": "MedGNN: Towards Multi-resolution Spatiotemporal Graph Learning for Medical Time Series Classification",
      "title_zh": "MedGNN：面向多分辨率时空图学习的医疗",
      "authors": [
        "Wei Fan",
        "Jingru Fei",
        "Dingyu Guo",
        "Kun Yi",
        "Xiaozhuang Song",
        "Haolong Xiang",
        "Hangting Ye",
        "Min Li"
      ],
      "abstract": "Medical time series has been playing a vital role in real-world healthcare\nsystems as valuable information in monitoring health conditions of patients.\nAccurate classification for medical time series, e.g., Electrocardiography\n(ECG) signals, can help for early detection and diagnosis. Traditional methods\ntowards medical time series classification rely on handcrafted feature\nextraction and statistical methods; with the recent advancement of artificial\nintelligence, the machine learning and deep learning methods have become more\npopular. However, existing methods often fail to fully model the complex\nspatial dynamics under different scales, which ignore the dynamic\nmulti-resolution spatial and temporal joint inter-dependencies. Moreover, they\nare less likely to consider the special baseline wander problem as well as the\nmulti-view characteristics of medical time series, which largely hinders their\nprediction performance. To address these limitations, we propose a\nMulti-resolution Spatiotemporal Graph Learning framework, MedGNN, for medical\ntime series classification. Specifically, we first propose to construct\nmulti-resolution adaptive graph structures to learn dynamic multi-scale\nembeddings. Then, to address the baseline wander problem, we propose Difference\nAttention Networks to operate self-attention mechanisms on the finite\ndifference for temporal modeling. Moreover, to learn the multi-view\ncharacteristics, we utilize the Frequency Convolution Networks to capture\ncomplementary information of medical time series from the frequency domain. In\naddition, we introduce the Multi-resolution Graph Transformer architecture to\nmodel the dynamic dependencies and fuse the information from different\nresolutions. Finally, we have conducted extensive experiments on multiple\nmedical real-world datasets that demonstrate the superior performance of our\nmethod. Our Code is available.",
      "tldr_zh": "该研究提出MedGNN框架，旨在通过多分辨率时空图学习（Multi-resolution Spatiotemporal Graph Learning）提升医疗时间序列分类的性能，解决现有方法忽略的多尺度空间动态、基线漂移（baseline wander）问题以及多视图特性。MedGNN首先构建多分辨率自适应图结构来学习动态多尺度嵌入，然后引入Difference Attention Networks在有限差分上应用自注意力机制处理时间建模，并利用Frequency Convolution Networks从频域捕获补充信息，同时采用Multi-resolution Graph Transformer融合不同分辨率的信息。实验结果显示，该框架在多个真实医疗数据集上表现出色，显著改善了分类准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WWW 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04515v1",
      "published_date": "2025-02-06 21:34:54 UTC",
      "updated_date": "2025-02-06 21:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:53:34.703823"
    },
    {
      "arxiv_id": "2502.04512v2",
      "title": "Safety is Essential for Responsible Open-Ended Systems",
      "title_zh": "安全是负责任的开放式系统的必需",
      "authors": [
        "Ivaxi Sheth",
        "Jan Wehner",
        "Sahar Abdelnabi",
        "Ruta Binkyte",
        "Mario Fritz"
      ],
      "abstract": "AI advancements have been significantly driven by a combination of foundation\nmodels and curiosity-driven learning aimed at increasing capability and\nadaptability. A growing area of interest within this field is Open-Endedness -\nthe ability of AI systems to continuously and autonomously generate novel and\ndiverse artifacts or solutions. This has become relevant for accelerating\nscientific discovery and enabling continual adaptation in AI agents. This\nposition paper argues that the inherently dynamic and self-propagating nature\nof Open-Ended AI introduces significant, underexplored risks, including\nchallenges in maintaining alignment, predictability, and control. This paper\nsystematically examines these challenges, proposes mitigation strategies, and\ncalls for action for different stakeholders to support the safe, responsible\nand successful development of Open-Ended AI.",
      "tldr_zh": "本论文强调，AI 的快速发展依赖基础模型（foundation models）和好奇心驱动学习，而Open-Endedness——AI 系统持续自主生成新颖多样产物的能力——对科学发现和AI 代理的持续适应至关重要。然而，该论文指出，Open-Ended AI 的动态自传播特性引入了未充分探索的风险，包括维持一致性（alignment）、可预测性和控制的挑战。论文系统分析这些风险，提出缓解策略，并呼吁不同利益相关者采取行动，以支持Open-Ended AI 的安全、负责任发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.04512v2",
      "published_date": "2025-02-06 21:32:07 UTC",
      "updated_date": "2025-02-10 19:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:53:46.543605"
    },
    {
      "arxiv_id": "2502.05231v1",
      "title": "Thin ring wing as a means of flow improvement upstream of a propeller",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Sluchak"
      ],
      "abstract": "There are numerous devices currently known with the purpose of reducing the\nirregularity of the flow upstream of the propeller and to decrease by that\nmeans the propeller-induced vibration and noise. Many of these devices are\nwing-shaped vortex-generators that affect the flow with their induced (i.e.\npassive) longitudinal vortices. The paper's subject is the use of a ring-shaped\nwing as a highly effective passive vortex-generator which allows to control the\nflow closer to the most charged sections of propeller blades. The problem of a\nthin ring-shaped wing with irregular (asymmetric) geometry in the irregular\nsteady flow has been solved in a linear approach and the intensity of the\ninduced longitudinal vortices as a function of the irregularity of the flow and\nthe geometry of the ring wing has been estimated using that solution.\nExperiments in the towing tank showing good concordance with the theoretical\nmodel confirmed the effectiveness of such a device. Some additional advantages\nof a ring-shaped wing incorporated into the construction of stabilizers are\nconsidered.",
      "tldr_zh": "本文提出使用 thin ring wing 作为高效的被动 vortex-generator，以改善螺旋桨上游流动的不规则性，从而降低螺旋桨引起的振动和噪音。该方法通过线性求解 thin ring wing 在不规则稳态流动中的问题，估算了诱导 longitudinal vortices 的强度，并将其与环形翼的几何和流动不规则性相关联。实验在拖曳水池中进行，结果与理论模型高度一致，验证了该设备的有效性，并探讨了将其整合到稳定器结构中的额外优势。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "7 pages, 7 figures, Propellers/Shafting 97 Symposium",
      "pdf_url": "http://arxiv.org/pdf/2502.05231v1",
      "published_date": "2025-02-06 21:30:08 UTC",
      "updated_date": "2025-02-06 21:30:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:53:59.050639"
    },
    {
      "arxiv_id": "2502.06845v1",
      "title": "DiffNMR3: Advancing NMR Resolution Beyond Instrumental Limits",
      "title_zh": "DiffNMR3：超越仪器限制提升 NMR 分辨率",
      "authors": [
        "Sen Yan",
        "Etienne Goffinet",
        "Fabrizio Gabellieri",
        "Ryan Young",
        "Lydia Gkoura",
        "Laurence Jennings",
        "Filippo Castiglione",
        "Thomas Launey"
      ],
      "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy is a crucial analytical\ntechnique used for molecular structure elucidation, with applications spanning\nchemistry, biology, materials science, and medicine. However, the frequency\nresolution of NMR spectra is limited by the \"field strength\" of the instrument.\nHigh-field NMR instruments provide high-resolution spectra but are\nprohibitively expensive, whereas lower-field instruments offer more accessible,\nbut lower-resolution, results. This paper introduces an AI-driven approach that\nnot only enhances the frequency resolution of NMR spectra through\nsuper-resolution techniques but also provides multi-scale functionality. By\nleveraging a diffusion model, our method can reconstruct high-field spectra\nfrom low-field NMR data, offering flexibility in generating spectra at varying\nmagnetic field strengths. These reconstructions are comparable to those\nobtained from high-field instruments, enabling finer spectral details and\nimproving molecular characterization. To date, our approach is one of the first\nto overcome the limitations of instrument field strength, achieving NMR\nsuper-resolution through AI. This cost-effective solution makes high-resolution\nanalysis accessible to more researchers and industries, without the need for\nmultimillion-dollar equipment.",
      "tldr_zh": "这篇论文提出DiffNMR3，一种AI驱动的方法，利用diffusion model实现NMR光谱的超分辨率，超越仪器场强的限制，从而从低场强数据重建高质量的高场强光谱。方法提供多尺度功能，允许生成不同磁场强度的光谱，提高分子表征的精细度。实验结果表明，该技术使重建光谱与高场强仪器相当，提供成本有效的解决方案，让更多研究者和行业无需昂贵设备即可进行高分辨率分析。",
      "categories": [
        "physics.ins-det",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "physics.ins-det",
      "comment": "13 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.06845v1",
      "published_date": "2025-02-06 21:00:35 UTC",
      "updated_date": "2025-02-06 21:00:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:54:10.882887"
    },
    {
      "arxiv_id": "2502.06844v1",
      "title": "Exploring Model Invariance with Discrete Search for Ultra-Low-Bit Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqiao Wen",
        "Yanshuai Cao",
        "Lili Mou"
      ],
      "abstract": "Large language models have been increasing in size due to their success in a\nwide range of applications. This calls for a pressing need to reduce memory\nusage to make them more accessible. Post-training quantization is a popular\ntechnique which uses fewer bits (e.g., 4--8 bits) to represent the model\nwithout retraining it. However, it remains a challenging task to perform\nquantization in an ultra-low-bit setup (e.g., 2 bits). In this paper, we\npropose InvarExplore, a unified framework that systematically explores\ndifferent model invariance at the same time, allowing us to take advantage of\nthe synergy between each type of invariance. Importantly, InvarExplore features\na discrete search algorithm that enables us to explore permutation invariance,\nwhich is under-studied as it cannot be optimized with gradient-based methods.\nResults show that InvarExplore is compatible with existing state-of-the-art\nmethods, achieving an add-on performance improvement over strong competing\nmethods.",
      "tldr_zh": "本文探讨了超低位量化（ultra-low-bit quantization）的问题，以应对大语言模型规模增大导致的内存使用挑战。作者提出 InvarExplore 框架，该框架统一探索多种模型不变性（model invariance），利用它们之间的协同效应来优化量化过程。InvarExplore 采用离散搜索算法（discrete search algorithm）来处理排列不变性（permutation invariance），因为后者无法通过梯度方法优化。实验结果表明，该框架与现有最先进方法兼容，并带来额外的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.6; I.2.m; I.5.1; I.7.m"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.06844v1",
      "published_date": "2025-02-06 21:00:13 UTC",
      "updated_date": "2025-02-06 21:00:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:54:22.465983"
    },
    {
      "arxiv_id": "2502.04499v1",
      "title": "Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much)",
      "title_zh": "翻译失败",
      "authors": [
        "Zony Yu",
        "Yuqiao Wen",
        "Lili Mou"
      ],
      "abstract": "Knowledge distillation (KD) is a popular method of transferring knowledge\nfrom a large \"teacher\" model to a small \"student\" model. KD can be divided into\ntwo categories: prediction matching and intermediate-layer matching. We explore\nan intriguing phenomenon: layer-selection strategy does not matter (much) in\nintermediate-layer matching. In this paper, we show that seemingly nonsensical\nmatching strategies such as matching the teacher's layers in reverse still\nresult in surprisingly good student performance. We provide an interpretation\nfor this phenomenon by examining the angles between teacher layers viewed from\nthe student's perspective.",
      "tldr_zh": "本论文重新审视了知识蒸馏（Knowledge Distillation, KD）中的中间层匹配，发现层选择策略对学生模型性能的影响不大，即使采用反向匹配等看似不合理的策略，学生模型也能表现出色。通过分析从学生视角观看教师层之间的角度，论文为这一现象提供了合理的解释。该研究为优化KD方法提供了新洞见，强调了中间层匹配的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04499v1",
      "published_date": "2025-02-06 21:00:01 UTC",
      "updated_date": "2025-02-06 21:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:54:34.976360"
    },
    {
      "arxiv_id": "2502.04489v1",
      "title": "CNN Autoencoders for Hierarchical Feature Extraction and Fusion in Multi-sensor Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Saeed Arabzadeh",
        "Farshad Almasganj",
        "Mohammad Mahdi Ahmadi"
      ],
      "abstract": "Deep learning methods have been widely used for Human Activity Recognition\n(HAR) using recorded signals from Iner-tial Measurement Units (IMUs) sensors\nthat are installed on various parts of the human body. For this type of HAR,\nsev-eral challenges exist, the most significant of which is the analysis of\nmultivarious IMU sensors data. Here, we introduce a Hierarchically Unsupervised\nFusion (HUF) model designed to extract, and fuse features from IMU sensors data\nvia a hybrid structure of Convolutional Neural Networks (CNN)s and Autoencoders\n(AE)s. First, we design a stack CNN-AE to embed short-time signals into sets of\nhigh dimensional features. Second, we develop another CNN-AE network to locally\nfuse the extracted features from each sensor unit. Finally, we unify all the\nsensor features through a third CNN-AE architecture as globally feature fusion\nto create a unique feature set. Additionally, we analyze the effects of varying\nthe model hyperparameters. The best results are achieved with eight\nconvolutional layers in each AE. Furthermore, it is determined that an\novercomplete AE with 256 kernels in the code layer is suitable for feature\nextraction in the first block of the proposed HUF model; this number reduces to\n64 in the last block of the model to customize the size of the applied features\nto the classifier. The tuned model is applied to the UCI-HAR, DaLiAc, and\nParkinson's disease gait da-tasets, achieving the classification accuracies of\n97%, 97%, and 88%, respectively, which are nearly 3% better com-pared to the\nstate-of-the-art supervised methods.",
      "tldr_zh": "本文提出了一种 Hierarchically Unsupervised Fusion (HUF) 模型，结合 CNN 和 Autoencoders (AE) 的混合结构，用于从多传感器 IMU 数据中提取和融合特征，以提升 Human Activity Recognition (HAR) 的性能。模型采用三层层次化架构：首先使用堆叠 CNN-AE 将短时间信号嵌入高维特征；其次，通过另一个 CNN-AE 局部融合每个传感器的提取特征；最后，利用第三个 CNN-AE 全局融合所有特征，形成统一特征集。实验结果显示，在 UCI-HAR、DaLiAc 和 Parkinson's 疾病步态数据集上，HUF 模型分别达到 97%、97% 和 88% 的分类准确率，比现有监督方法高出约 3%，并通过优化超参数（如每个 AE 的八个卷积层和特定内核数量）进一步提升了模型效能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04489v1",
      "published_date": "2025-02-06 20:36:41 UTC",
      "updated_date": "2025-02-06 20:36:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:54:47.934304"
    },
    {
      "arxiv_id": "2502.04488v1",
      "title": "Building A Unified AI-centric Language System: analysis, framework and future work",
      "title_zh": "构建一个统一的AI中心语言系统：分析、框架和未来工作",
      "authors": [
        "Edward Hong Wang",
        "Cynthia Xin Wen"
      ],
      "abstract": "Recent advancements in large language models have demonstrated that extended\ninference through techniques can markedly improve performance, yet these gains\ncome with increased computational costs and the propagation of inherent biases\nfound in natural languages. This paper explores the design of a unified\nAI-centric language system that addresses these challenges by offering a more\nconcise, unambiguous, and computationally efficient alternative to traditional\nhuman languages. We analyze the limitations of natural language such as gender\nbias, morphological irregularities, and contextual ambiguities and examine how\nthese issues are exacerbated within current Transformer architectures, where\nredundant attention heads and token inefficiencies prevail. Drawing on insights\nfrom emergent artificial communication systems and constructed languages like\nEsperanto and Lojban, we propose a framework that translates diverse natural\nlanguage inputs into a streamlined AI-friendly language, enabling more\nefficient model training and inference while reducing memory footprints.\nFinally, we outline a pathway for empirical validation through controlled\nexperiments, paving the way for a universal interchange format that could\nrevolutionize AI-to-AI and human-to-AI interactions by enhancing clarity,\nfairness, and overall performance.",
      "tldr_zh": "这篇论文分析了 large language models 在扩展推理技术中取得的性能提升，但同时暴露出的计算成本增加和固有偏差问题，探讨了设计一个统一的 AI-centric language system 来提供更简洁、无歧义和高效的替代方案。作者审视了自然语言的局限性，如性别偏差、形态不规则性和上下文歧义，这些问题在 Transformer architectures 中被放大，导致冗余注意力头和低效标记。论文提出一个框架，通过借鉴 emergent artificial communication systems 和构建语言（如 Esperanto 和 Lojban）的见解，将多样自然语言输入翻译成精简的 AI 友好语言，从而优化模型训练、推理效率并降低内存占用。最后，论文概述了通过控制实验进行实证验证的路径，这有望革新 AI-to-AI 和 human-to-AI 互动，提升清晰度、公平性和整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04488v1",
      "published_date": "2025-02-06 20:32:57 UTC",
      "updated_date": "2025-02-06 20:32:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:00.662623"
    },
    {
      "arxiv_id": "2502.04485v1",
      "title": "Active Task Disambiguation with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Katarzyna Kobalczyk",
        "Nicolas Astorga",
        "Tennison Liu",
        "Mihaela van der Schaar"
      ],
      "abstract": "Despite the impressive performance of large language models (LLMs) across\nvarious benchmarks, their ability to address ambiguously specified\nproblems--frequent in real-world interactions--remains underexplored. To\naddress this gap, we introduce a formal definition of task ambiguity and frame\nthe problem of task disambiguation through the lens of Bayesian Experimental\nDesign. By posing clarifying questions, LLM agents can acquire additional task\nspecifications, progressively narrowing the space of viable solutions and\nreducing the risk of generating unsatisfactory outputs. Yet, generating\neffective clarifying questions requires LLM agents to engage in a form of\nmeta-cognitive reasoning, an ability LLMs may presently lack. Our proposed\napproach of active task disambiguation enables LLM agents to generate targeted\nquestions maximizing the information gain. Effectively, this approach shifts\nthe load from implicit to explicit reasoning about the space of viable\nsolutions. Empirical results demonstrate that this form of question selection\nleads to more effective task disambiguation in comparison to approaches relying\non reasoning solely within the space of questions.",
      "tldr_zh": "该论文探讨了大型语言模型 (LLMs) 在处理模糊任务时的局限性，并正式定义了任务模糊性，通过贝叶斯实验设计 (Bayesian Experimental Design) 框架来解决这一问题。研究提出一种主动任务消除模糊方法，让 LLMs 生成针对性的澄清问题，以最大化信息增益 (information gain)，从而从隐式推理转向显式元认知推理 (meta-cognitive reasoning)。实验结果表明，这种方法比仅在问题空间内推理的策略更有效地缩小解决方案空间并提升任务处理性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04485v1",
      "published_date": "2025-02-06 20:20:22 UTC",
      "updated_date": "2025-02-06 20:20:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:10.619559"
    },
    {
      "arxiv_id": "2502.05230v1",
      "title": "DiffNMR2: NMR Guided Sampling Acquisition Through Diffusion Model Uncertainty",
      "title_zh": "DiffNMR2：通过扩散模型不确定性指导的NMR采样获取",
      "authors": [
        "Etienne Goffinet",
        "Sen Yan",
        "Fabrizio Gabellieri",
        "Laurence Jennings",
        "Lydia Gkoura",
        "Filippo Castiglione",
        "Ryan Young",
        "Idir Malki",
        "Ankita Singh",
        "Thomas Launey"
      ],
      "abstract": "Nuclear Magnetic Resonance (NMR) spectrometry uses electro-frequency pulses\nto probe the resonance of a compound's nucleus, which is then analyzed to\ndetermine its structure. The acquisition time of high-resolution NMR spectra\nremains a significant bottleneck, especially for complex biological samples\nsuch as proteins. In this study, we propose a novel and efficient sub-sampling\nstrategy based on a diffusion model trained on protein NMR data. Our method\niteratively reconstructs under-sampled spectra while using model uncertainty to\nguide subsequent sampling, significantly reducing acquisition time. Compared to\nstate-of-the-art strategies, our approach improves reconstruction accuracy by\n52.9\\%, reduces hallucinated peaks by 55.6%, and requires 60% less time in\ncomplex NMR experiments. This advancement holds promise for many applications,\nfrom drug discovery to materials science, where rapid and high-resolution\nspectral analysis is critical.",
      "tldr_zh": "本研究提出DiffNMR2，一种基于diffusion model的子采样策略，用于指导NMR光谱获取过程，通过利用模型不确定性来迭代重建欠采样谱，从而显著缩短获取时间。相比现有方法，该策略提高了重建准确性52.9%、减少了幻觉峰55.6%，并在复杂实验中节省60%的时间。DiffNMR2的创新有望加速NMR在药物发现和材料科学等领域的应用，提供更快速、高分辨率的谱分析。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "11 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.05230v1",
      "published_date": "2025-02-06 20:10:28 UTC",
      "updated_date": "2025-02-06 20:10:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:22.823723"
    },
    {
      "arxiv_id": "2502.04476v1",
      "title": "ADIFF: Explaining audio difference using natural language",
      "title_zh": "ADIFF：使用自然语言解释音频差异",
      "authors": [
        "Soham Deshmukh",
        "Shuo Han",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "abstract": "Understanding and explaining differences between audio recordings is crucial\nfor fields like audio forensics, quality assessment, and audio generation. This\ninvolves identifying and describing audio events, acoustic scenes, signal\ncharacteristics, and their emotional impact on listeners. This paper stands out\nas the first work to comprehensively study the task of explaining audio\ndifferences and then propose benchmark, baselines for the task. First, we\npresent two new datasets for audio difference explanation derived from the\nAudioCaps and Clotho audio captioning datasets. Using Large Language Models\n(LLMs), we generate three levels of difference explanations: (1) concise\ndescriptions of audio events and objects, (2) brief sentences about audio\nevents, acoustic scenes, and signal properties, and (3) comprehensive\nexplanations that include semantics and listener emotions. For the baseline, we\nuse prefix tuning where audio embeddings from two audio files are used to\nprompt a frozen language model. Our empirical analysis and ablation studies\nreveal that the naive baseline struggles to distinguish perceptually similar\nsounds and generate detailed tier 3 explanations. To address these limitations,\nwe propose ADIFF, which introduces a cross-projection module, position\ncaptioning, and a three-step training process to enhance the model's ability to\nproduce detailed explanations. We evaluate our model using objective metrics\nand human evaluation and show our model enhancements lead to significant\nimprovements in performance over naive baseline and SoTA Audio-Language Model\n(ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the\neffects of cross-projection, language model parameters, position captioning,\nthird stage fine-tuning, and present our findings. Our benchmarks, findings,\nand strong baseline pave the way for nuanced and human-like explanations of\naudio differences.",
      "tldr_zh": "这篇论文首次全面研究使用自然语言解释音频差异的任务，应用于音频取证、质量评估和音频生成，并提出了新的基准数据集和基线模型。他们基于 AudioCaps 和 Clotho 数据集创建了两个新数据集，并利用 Large Language Models (LLMs) 生成三种级别的差异解释：简洁描述、事件属性句子以及包括语义和听众情绪的全面解释。论文提出了 ADIFF 模型，通过引入 cross-projection module、position captioning 和三步训练过程，解决了基线模型（如 prefix tuning）在区分相似声音和生成详细解释方面的局限性。实验结果显示，ADIFF 在客观指标和人类评估中比朴素基线和 SOTA 音频语言模型 Qwen Audio 显著提升性能，并通过消融研究证实了各组件的有效性，为音频差异的细致人类化解释奠定了基础。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at ICLR 2025. Dataset and checkpoints are available at:\n  https://github.com/soham97/ADIFF",
      "pdf_url": "http://arxiv.org/pdf/2502.04476v1",
      "published_date": "2025-02-06 20:00:43 UTC",
      "updated_date": "2025-02-06 20:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:36.680863"
    },
    {
      "arxiv_id": "2502.04475v1",
      "title": "Augmented Conditioning Is Enough For Effective Training Image Generation",
      "title_zh": "增强条件足以实现有效的训练图像生成",
      "authors": [
        "Jiahui Chen",
        "Amy Zhang",
        "Adriana Romero-Soriano"
      ],
      "abstract": "Image generation abilities of text-to-image diffusion models have\nsignificantly advanced, yielding highly photo-realistic images from descriptive\ntext and increasing the viability of leveraging synthetic images to train\ncomputer vision models. To serve as effective training data, generated images\nmust be highly realistic while also sufficiently diverse within the support of\nthe target data distribution. Yet, state-of-the-art conditional image\ngeneration models have been primarily optimized for creative applications,\nprioritizing image realism and prompt adherence over conditional diversity. In\nthis paper, we investigate how to improve the diversity of generated images\nwith the goal of increasing their effectiveness to train downstream image\nclassification models, without fine-tuning the image generation model. We find\nthat conditioning the generation process on an augmented real image and text\nprompt produces generations that serve as effective synthetic datasets for\ndownstream training. Conditioning on real training images contextualizes the\ngeneration process to produce images that are in-domain with the real image\ndistribution, while data augmentations introduce visual diversity that improves\nthe performance of the downstream classifier. We validate\naugmentation-conditioning on a total of five established long-tail and few-shot\nimage classification benchmarks and show that leveraging augmentations to\ncondition the generation process results in consistent improvements over the\nstate-of-the-art on the long-tailed benchmark and remarkable gains in extreme\nfew-shot regimes of the remaining four benchmarks. These results constitute an\nimportant step towards effectively leveraging synthetic data for downstream\ntraining.",
      "tldr_zh": "本文提出了一种名为“Augmented Conditioning”的方法，用于提升文本到图像扩散模型的生成图像多样性，从而使其更适合下游图像分类模型的训练，而无需微调生成模型。该方法通过结合数据增强与真实图像及文本提示来条件化生成过程，确保生成的图像既贴近目标数据分布又引入视觉多样性。在五个长尾和少样本图像分类基准上进行验证，结果显示，该方法在长尾基准上优于现有状态-of-the-art模型，并在极端少样本场景中实现了显著性能提升。这一创新为有效利用合成数据进行下游训练提供了关键进展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04475v1",
      "published_date": "2025-02-06 19:57:33 UTC",
      "updated_date": "2025-02-06 19:57:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:47.685839"
    },
    {
      "arxiv_id": "2502.04470v1",
      "title": "Color in Visual-Language Models: CLIP deficiencies",
      "title_zh": "视觉语言模型中的颜色：CLIP 的缺陷",
      "authors": [
        "Guillem Arias",
        "Ramon Baldrich",
        "Maria Vanrell"
      ],
      "abstract": "This work explores how color is encoded in CLIP (Contrastive Language-Image\nPre-training) which is currently the most influential VML (Visual Language\nmodel) in Artificial Intelligence. After performing different experiments on\nsynthetic datasets created for this task, we conclude that CLIP is able to\nattribute correct color labels to colored visual stimulus, but, we come across\ntwo main deficiencies: (a) a clear bias on achromatic stimuli that are poorly\nrelated to the color concept, thus white, gray and black are rarely assigned as\ncolor labels; and (b) the tendency to prioritize text over other visual\ninformation. Here we prove it is highly significant in color labelling through\nan exhaustive Stroop-effect test. With the aim to find the causes of these\ncolor deficiencies, we analyse the internal representation at the neuron level.\nWe conclude that CLIP presents an important amount of neurons selective to\ntext, specially in deepest layers of the network, and a smaller amount of\nmulti-modal color neurons which could be the key of understanding the concept\nof color properly. Our investigation underscores the necessity of refining\ncolor representation mechanisms in neural networks to foster a more\ncomprehensive comprehension of colors as humans understand them, thereby\nadvancing the efficacy and versatility of multimodal models like CLIP in\nreal-world scenarios.",
      "tldr_zh": "本研究探讨了视觉语言模型(Visual-Language Models)中 CLIP 的颜色编码缺陷，通过合成数据集实验发现，CLIP 能正确为有色视觉刺激分配颜色标签，但存在两大问题：(a) 对无色(achromatic)刺激如白、灰和黑有明显偏见，导致这些颜色标签被忽略；(b) 优先处理文本信息，而非其他视觉细节，这在 Stroop-effect 测试中被证实。作者通过神经元级别分析揭示，CLIP 在深层网络中具有大量对文本敏感的 neurons，但多模态颜色 neurons 较少，从而影响了对颜色的全面理解。该工作强调，需要改进神经网络的颜色表示机制，以使多模态模型如 CLIP 更符合人类认知，并在实际场景中提升效能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 10 figures, conference, Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2502.04470v1",
      "published_date": "2025-02-06 19:38:12 UTC",
      "updated_date": "2025-02-06 19:38:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:55:58.503831"
    },
    {
      "arxiv_id": "2502.04469v1",
      "title": "No Images, No Problem: Retaining Knowledge in Continual VQA with Questions-Only Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Imad Eddine Marouf",
        "Enzo Tartaglione",
        "Stephane Lathuiliere",
        "Joost van de Weijer"
      ],
      "abstract": "Continual Learning in Visual Question Answering (VQACL) requires models to\nlearn new visual-linguistic tasks (plasticity) while retaining knowledge from\nprevious tasks (stability). The multimodal nature of VQACL presents unique\nchallenges, requiring models to balance stability across visual and textual\ndomains while maintaining plasticity to adapt to novel objects and reasoning\ntasks. Existing methods, predominantly designed for unimodal tasks, often\nstruggle to balance these demands effectively. In this work, we introduce\nQUestion-only replay with Attention Distillation (QUAD), a novel approach for\nVQACL that leverages only past task questions for regularisation, eliminating\nthe need to store visual data and addressing both memory and privacy concerns.\nQUAD achieves stability by introducing a question-only replay mechanism that\nselectively uses questions from previous tasks to prevent overfitting to the\ncurrent task's answer space, thereby mitigating the out-of-answer-set problem.\nComplementing this, we propose attention consistency distillation, which\nuniquely enforces both intra-modal and inter-modal attention consistency across\ntasks, preserving essential visual-linguistic associations. Extensive\nexperiments on VQAv2 and NExT-QA demonstrate that QUAD significantly\noutperforms state-of-the-art methods, achieving robust performance in continual\nVQA.",
      "tldr_zh": "该研究针对视觉问答持续学习(VQACL)提出QUAD方法，使用仅问题重放机制，避免存储视觉数据，从而解决内存和隐私问题，同时平衡模型的稳定性(stability)和可塑性(plasticity)。QUAD通过question-only replay防止过拟合当前任务的答案空间，并引入attention consistency distillation，确保任务间的intra-modal和inter-modal注意一致性，保留关键的视觉-语言关联。实验结果显示，在VQAv2和NExT-QA数据集上，QUAD显著优于现有方法，实现了更稳健的持续VQA性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, in-review",
      "pdf_url": "http://arxiv.org/pdf/2502.04469v1",
      "published_date": "2025-02-06 19:37:43 UTC",
      "updated_date": "2025-02-06 19:37:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:56:10.824041"
    },
    {
      "arxiv_id": "2502.04465v1",
      "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Della Libera",
        "Francesco Paissan",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "abstract": "Large language models have revolutionized natural language processing through\nself-supervised pretraining on massive datasets. Inspired by this success,\nresearchers have explored adapting these methods to speech by discretizing\ncontinuous audio into tokens using neural audio codecs. However, existing\napproaches face limitations, including high bitrates, the loss of either\nsemantic or acoustic information, and the reliance on multi-codebook designs\nwhen trying to capture both, which increases architectural complexity for\ndownstream tasks. To address these challenges, we introduce FocalCodec, an\nefficient low-bitrate codec based on focal modulation that utilizes a single\nbinary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec\ndelivers competitive performance in speech resynthesis and voice conversion at\nlower bitrates than the current state-of-the-art, while effectively handling\nmultilingual speech and noisy environments. Evaluation on downstream tasks\nshows that FocalCodec successfully preserves sufficient semantic and acoustic\ninformation, while also being well-suited for generative modeling. Demo\nsamples, code and checkpoints are available at\nhttps://lucadellalib.github.io/focalcodec-web/.",
      "tldr_zh": "本研究受大型语言模型在自然语言处理成功启发，针对现有语音编解码器的高比特率和信息丢失问题，提出FocalCodec——一种基于focal modulation networks的低比特率语音编码方法，使用单个二进制代码本将语音压缩至0.16至0.65 kbps。FocalCodec在语音重合成和语音转换任务中表现出色，即使在较低比特率下，也能与最先进方法竞争，同时有效处理多语言语音和嘈杂环境。实验评估显示，该方法成功保留了足够的语义和声学信息，并适用于下游生成建模任务。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.04465v1",
      "published_date": "2025-02-06 19:24:50 UTC",
      "updated_date": "2025-02-06 19:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:56:29.225201"
    },
    {
      "arxiv_id": "2502.06843v1",
      "title": "Vision-Integrated LLMs for Autonomous Driving Assistance : Human Performance Comparison and Trust Evaluation",
      "title_zh": "翻译失败",
      "authors": [
        "Namhee Kim",
        "Woojin Park"
      ],
      "abstract": "Traditional autonomous driving systems often struggle with reasoning in\ncomplex, unexpected scenarios due to limited comprehension of spatial\nrelationships. In response, this study introduces a Large Language Model\n(LLM)-based Autonomous Driving (AD) assistance system that integrates a vision\nadapter and an LLM reasoning module to enhance visual understanding and\ndecision-making. The vision adapter, combining YOLOv4 and Vision Transformer\n(ViT), extracts comprehensive visual features, while GPT-4 enables human-like\nspatial reasoning and response generation. Experimental evaluations with 45\nexperienced drivers revealed that the system closely mirrors human performance\nin describing situations and moderately aligns with human decisions in\ngenerating appropriate responses.",
      "tldr_zh": "这篇论文提出了一种集成视觉适配器的 LLM 辅助系统，用于提升自动驾驶在复杂场景中的空间关系理解和决策能力，以解决传统系统的局限性。系统结合 YOLOv4 和 Vision Transformer (ViT) 提取全面视觉特征，并利用 GPT-4 进行类似人类的空间推理和响应生成。通过实验评估，系统与 45 名经验丰富驾驶员的性能比较显示，在描述情况方面接近人类水平，在生成适当响应方面与人类决策有中等一致性。整体结果为自动驾驶辅助系统的可信度提供了重要参考。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.06843v1",
      "published_date": "2025-02-06 19:19:28 UTC",
      "updated_date": "2025-02-06 19:19:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:56:34.455174"
    },
    {
      "arxiv_id": "2502.06842v1",
      "title": "Integrating Generative Artificial Intelligence in ADRD: A Framework for Streamlining Diagnosis and Care in Neurodegenerative Diseases",
      "title_zh": "翻译失败",
      "authors": [
        "Andrew G. Breithaupt",
        "Alice Tang",
        "Bruce L. Miller",
        "Pedro Pinheiro-Chagas"
      ],
      "abstract": "Healthcare systems are struggling to meet the growing demand for neurological\ncare, with challenges particularly acute in Alzheimer's disease and related\ndementias (ADRD). While artificial intelligence research has often focused on\nidentifying patterns beyond human perception, implementing such predictive\ncapabilities remains challenging as clinicians cannot readily verify insights\nthey cannot themselves detect. We propose that large language models (LLMs)\noffer more immediately practical applications by enhancing clinicians'\ncapabilities in three critical areas: comprehensive data collection,\ninterpretation of complex clinical information, and timely application of\nrelevant medical knowledge. These challenges stem from limited time for proper\ndiagnosis, growing data complexity, and an overwhelming volume of medical\nliterature that exceeds any clinician's capacity to fully master. We present a\nframework for responsible AI integration that leverages LLMs' ability to\ncommunicate effectively with both patients and providers while maintaining\nhuman oversight. This approach prioritizes standardized, high-quality data\ncollection to enable a system that learns from every patient encounter while\nincorporating the latest clinical evidence, continuously improving care\ndelivery. We begin to address implementation challenges and initiate important\ndiscussions around ethical considerations and governance needs. While developed\nfor ADRD, this roadmap provides principles for responsible AI integration\nacross neurology and other medical specialties, with potential to improve\ndiagnostic accuracy, reduce care disparities, and advance clinical knowledge\nthrough a learning healthcare system.",
      "tldr_zh": "该论文提出一个框架，将生成式人工智能（特别是大型语言模型，LLMs）整合到阿尔茨海默病和相关痴呆（ADRD）等神经退行性疾病的诊断和护理中，以应对医疗系统面临的资源短缺和数据复杂性挑战。框架重点增强临床医生的能力，包括全面数据收集、复杂临床信息的解释以及及时应用医疗知识，同时强调标准化数据采集和人类监督，确保负责任的AI使用。实验和讨论显示，此方法可通过学习型医疗系统持续改善护理质量，提升诊断准确性、减少护理差异，并为神经科及其他医疗领域提供可推广的AI集成原则。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "I.2.1"
      ],
      "primary_category": "cs.CY",
      "comment": "20 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.06842v1",
      "published_date": "2025-02-06 19:09:11 UTC",
      "updated_date": "2025-02-06 19:09:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:56:46.982490"
    },
    {
      "arxiv_id": "2502.04326v1",
      "title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jack Hong",
        "Shilin Yan",
        "Jiayin Cai",
        "Xiaolong Jiang",
        "Yao Hu",
        "Weidi Xie"
      ],
      "abstract": "In this paper, we introduce WorldSense, the first benchmark to assess the\nmulti-modal video understanding, that simultaneously encompasses visual, audio,\nand text inputs. In contrast to existing benchmarks, our WorldSense has several\nfeatures: (i) collaboration of omni-modality, we design the evaluation tasks to\nfeature a strong coupling of audio and video, requiring models to effectively\nutilize the synergistic perception of omni-modality; (ii) diversity of videos\nand tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual\nsynchronised videos, systematically categorized into 8 primary domains and 67\nfine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice\nQA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii)\nhigh-quality annotations, all the QA pairs are manually labeled by 80 expert\nannotators with multiple rounds of correction to ensure quality. Based on our\nWorldSense, we extensively evaluate various state-of-the-art models. The\nexperimental results indicate that existing models face significant challenges\nin understanding real-world scenarios (48.0% best accuracy). We hope our\nWorldSense can provide a platform for evaluating the ability in constructing\nand understanding coherent contexts from omni-modality.",
      "tldr_zh": "本文提出WorldSense，这是第一个同时评估视觉、音频和文本输入的多模态视频理解基准。WorldSense的关键特点包括全模态协作的设计、视频和任务的多样性（涵盖1,662个音频-视觉同步视频和3,172个多选QA对，分布于8个主要领域和26个任务），以及高质量的专家标注。实验结果显示，现有的Multimodal LLMs在真实场景理解方面面临挑战，最佳准确率仅为48.0%，该基准有望成为评估模型从全模态构建和理解连贯上下文能力的平台。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04326v1",
      "published_date": "2025-02-06 18:59:40 UTC",
      "updated_date": "2025-02-06 18:59:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:56:59.395796"
    },
    {
      "arxiv_id": "2502.04428v1",
      "title": "Confident or Seek Stronger: Exploring Uncertainty-Based On-device LLM Routing From Benchmarking to Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Neng Chuang",
        "Leisheng Yu",
        "Guanchu Wang",
        "Lizhe Zhang",
        "Zirui Liu",
        "Xuanting Cai",
        "Yang Sui",
        "Vladimir Braverman",
        "Xia Hu"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed and democratized on\nedge devices. To improve the efficiency of on-device deployment, small language\nmodels (SLMs) are often adopted due to their efficient decoding latency and\nreduced energy consumption. However, these SLMs often generate inaccurate\nresponses when handling complex queries. One promising solution is\nuncertainty-based SLM routing, offloading high-stakes queries to stronger LLMs\nwhen resulting in low-confidence responses on SLM. This follows the principle\nof \"If you lack confidence, seek stronger support\" to enhance reliability.\nRelying on more powerful LLMs is yet effective but increases invocation costs.\nTherefore, striking a routing balance between efficiency and efficacy remains a\ncritical challenge. Additionally, efficiently generalizing the routing strategy\nto new datasets remains under-explored. In this paper, we conduct a\ncomprehensive investigation into benchmarking and generalization of\nuncertainty-driven routing strategies from SLMs to LLMs over 1500+ settings.\nOur findings highlight: First, uncertainty-correctness alignment in different\nuncertainty quantification (UQ) methods significantly impacts routing\nperformance. Second, uncertainty distributions depend more on both the specific\nSLM and the chosen UQ method, rather than downstream data. Building on the\ninsight, we propose a calibration data construction instruction pipeline and\nopen-source a constructed hold-out set to enhance routing generalization on new\ndownstream scenarios. The experimental results indicate calibration data\neffectively bootstraps routing performance without any new data.",
      "tldr_zh": "该研究探讨了在边缘设备上部署大型语言模型 (LLMs) 的效率问题，提出了一种基于不确定性的小型语言模型 (SLMs) 路由策略：当 SLMs 对复杂查询信心不足时，将其转交给更强大的 LLMs，以平衡效率和准确性。研究通过超过1500个设置的基准测试发现，不同不确定性量化 (UQ) 方法的不确定性正确性对齐对路由性能影响重大，且不确定性分布更多取决于具体 SLMs 和 UQ 方法而非下游数据。基于此，他们开发了一种校准数据构建指令管道并开源了一个数据集，实验结果显示，该方法能有效提升路由策略在新场景下的泛化性能，而无需额外数据。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04428v1",
      "published_date": "2025-02-06 18:59:11 UTC",
      "updated_date": "2025-02-06 18:59:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:57:11.573054"
    },
    {
      "arxiv_id": "2502.04322v1",
      "title": "Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions",
      "title_zh": "Speak Easy：通过简单交互从大型语言模型中引出有害越狱",
      "authors": [
        "Yik Siu Chan",
        "Narutatsu Ri",
        "Yuxin Xiao",
        "Marzyeh Ghassemi"
      ],
      "abstract": "Despite extensive safety alignment efforts, large language models (LLMs)\nremain vulnerable to jailbreak attacks that elicit harmful behavior. While\nexisting studies predominantly focus on attack methods that require technical\nexpertise, two critical questions remain underexplored: (1) Are jailbroken\nresponses truly useful in enabling average users to carry out harmful actions?\n(2) Do safety vulnerabilities exist in more common, simple human-LLM\ninteractions? In this paper, we demonstrate that LLM responses most effectively\nfacilitate harmful actions when they are both actionable and informative--two\nattributes easily elicited in multi-step, multilingual interactions. Using this\ninsight, we propose HarmScore, a jailbreak metric that measures how effectively\nan LLM response enables harmful actions, and Speak Easy, a simple multi-step,\nmultilingual attack framework. Notably, by incorporating Speak Easy into direct\nrequest and jailbreak baselines, we see an average absolute increase of 0.319\nin Attack Success Rate and 0.426 in HarmScore in both open-source and\nproprietary LLMs across four safety benchmarks. Our work reveals a critical yet\noften overlooked vulnerability: Malicious users can easily exploit common\ninteraction patterns for harmful intentions.",
      "tldr_zh": "尽管大型语言模型（LLMs）进行了广泛的安全对齐，但它们仍易受越狱攻击（jailbreak attacks），尤其在简单的人机交互中。论文发现，多步多语言交互能有效诱导LLMs产生可行动且信息丰富的有害响应，从而帮助普通用户实施有害行动。研究提出了HarmScore指标，用于量化响应促成有害行为的程度，以及Speak Easy框架，一种简单的多步多语言攻击方法。通过整合Speak Easy，攻击成功率（Attack Success Rate）平均绝对增加0.319，HarmScore增加0.426，在开源和专有LLMs的四个安全基准上均有效验证。这揭示了恶意用户利用常见交互模式的潜在安全漏洞。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04322v1",
      "published_date": "2025-02-06 18:59:02 UTC",
      "updated_date": "2025-02-06 18:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:57:24.624891"
    },
    {
      "arxiv_id": "2502.04315v3",
      "title": "ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time Clusters",
      "title_zh": "翻译失败",
      "authors": [
        "Kamer Ali Yuksel",
        "Hassan Sawaf"
      ],
      "abstract": "Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/",
      "tldr_zh": "这篇论文介绍了 ChameleonLLM，一种新型框架，用于在推理时动态适应大语言模型 (LLMs)，通过批次感知聚类 (batch-aware clustering) 和实时生成低秩更新 (low-rank updates) 来处理真实世界数据的变异性。不同于传统的 Low-Rank Adaptation (LoRA) 方法，ChameleonLLM 利用 hyper-network 根据聚类批次的聚合统计动态生成自适应权重修改，从而避免了维护多个专家模型的开销。实验结果显示，该框架在性能上优于 LoRA 方法，并证明了其作为灵活、高适应性语言模型推理解决方案的潜力；论文已开源以确保可复现性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04315v3",
      "published_date": "2025-02-06 18:57:06 UTC",
      "updated_date": "2025-02-11 14:01:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:57:36.388188"
    },
    {
      "arxiv_id": "2502.04313v1",
      "title": "Great Models Think Alike and this Undermines AI Oversight",
      "title_zh": "翻译失败",
      "authors": [
        "Shashwat Goel",
        "Joschka Struber",
        "Ilze Amanda Auzina",
        "Karuna K Chandra",
        "Ponnurangam Kumaraguru",
        "Douwe Kiela",
        "Ameya Prabhu",
        "Matthias Bethge",
        "Jonas Geiping"
      ],
      "abstract": "As Language Model (LM) capabilities advance, evaluating and supervising them\nat scale is getting harder for humans. There is hope that other language models\ncan automate both these tasks, which we refer to as \"AI Oversight\". We study\nhow model similarity affects both aspects of AI oversight by proposing a\nprobabilistic metric for LM similarity based on overlap in model mistakes.\nUsing this metric, we first show that LLM-as-a-judge scores favor models\nsimilar to the judge, generalizing recent self-preference results. Then, we\nstudy training on LM annotations, and find complementary knowledge between the\nweak supervisor and strong student model plays a crucial role in gains from\n\"weak-to-strong generalization\". As model capabilities increase, it becomes\nharder to find their mistakes, and we might defer more to AI oversight.\nHowever, we observe a concerning trend -- model mistakes are becoming more\nsimilar with increasing capabilities, pointing to risks from correlated\nfailures. Our work underscores the importance of reporting and correcting for\nmodel similarity, especially in the emerging paradigm of AI oversight.",
      "tldr_zh": "该研究探讨了语言模型(LM)相似性对AI监督的影响，提出了一种基于模型错误重叠的概率指标来衡量LM相似性。研究发现，LLM-as-a-judge评分倾向于偏好与评判者相似的模型，而在训练LM注解时，弱监督者和强学生模型之间的互补知识对实现“weak-to-strong generalization”至关重要。随着模型能力的提升，模型错误变得越来越相似，这可能导致相关失败风险增加。作者强调，在AI监督中报告和修正模型相似性至关重要，以提升监督的可靠性和有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "60 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04313v1",
      "published_date": "2025-02-06 18:56:01 UTC",
      "updated_date": "2025-02-06 18:56:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:57:48.365873"
    },
    {
      "arxiv_id": "2502.04426v1",
      "title": "Decoding AI Judgment: How LLMs Assess News Credibility and Bias",
      "title_zh": "解码 AI 判断：LLMs 如何评估新闻可信度和偏见",
      "authors": [
        "Edoardo Loru",
        "Jacopo Nudo",
        "Niccolò Di Marco",
        "Matteo Cinelli",
        "Walter Quattrociocchi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used to assess news\ncredibility, yet little is known about how they make these judgments. While\nprior research has examined political bias in LLM outputs or their potential\nfor automated fact-checking, their internal evaluation processes remain largely\nunexamined. Understanding how LLMs assess credibility provides insights into AI\nbehavior and how credibility is structured and applied in large-scale language\nmodels. This study benchmarks the reliability and political classifications of\nstate-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and\nLLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as\nNewsGuard and Media Bias Fact Check. Beyond assessing classification\nperformance, we analyze the linguistic markers that shape LLM decisions,\nidentifying which words and concepts drive their evaluations. We uncover\npatterns in how LLMs associate credibility with specific linguistic features by\nexamining keyword frequency, contextual determinants, and rank distributions.\nBeyond static classification, we introduce a framework in which LLMs refine\ntheir credibility assessments by retrieving external information, querying\nother models, and adapting their responses. This allows us to investigate\nwhether their assessments reflect structured reasoning or rely primarily on\nprior learned associations.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）评估新闻可信度和偏见的内部过程，通过基准测试Gemini 1.5 Flash、GPT-4o mini和LLaMA 3.1，与NewsGuard和Media Bias Fact Check等专家系统进行比较。研究分析了LLMs决策中的语言标记，如关键词频率、上下文因素和排名分布，以揭示这些模型如何依赖特定语言特征来判断可信度。作者还引入了一个框架，让LLMs通过检索外部信息、查询其他模型和适应响应来改进评估，从而考察这些判断是否基于结构化推理而非仅靠先验学习关联。整体结果为理解AI行为和新闻可信度结构提供了重要洞见。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04426v1",
      "published_date": "2025-02-06 18:52:10 UTC",
      "updated_date": "2025-02-06 18:52:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:57:59.771524"
    },
    {
      "arxiv_id": "2502.04308v1",
      "title": "HOG-Diff: Higher-Order Guided Diffusion for Graph Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yiming Huang",
        "Tolga Birdal"
      ],
      "abstract": "Graph generation is a critical yet challenging task as empirical analyses\nrequire a deep understanding of complex, non-Euclidean structures. Although\ndiffusion models have recently made significant achievements in graph\ngeneration, these models typically adapt from the frameworks designed for image\ngeneration, making them ill-suited for capturing the topological properties of\ngraphs. In this work, we propose a novel Higher-order Guided Diffusion\n(HOG-Diff) model that follows a coarse-to-fine generation curriculum and is\nguided by higher-order information, enabling the progressive generation of\nplausible graphs with inherent topological structures. We further prove that\nour model exhibits a stronger theoretical guarantee than classical diffusion\nframeworks. Extensive experiments on both molecular and generic graph\ngeneration tasks demonstrate that our method consistently outperforms or\nremains competitive with state-of-the-art baselines. Our code is available at\nhttps://github.com/Yiminghh/HOG-Diff.",
      "tldr_zh": "该论文针对图生成任务的挑战，提出了一种新型 Higher-Order Guided Diffusion (HOG-Diff) 模型，以解决传统 diffusion models 在捕获图的拓扑属性方面的不足。HOG-Diff 通过粗到细的生成课程和高阶信息指导，实现逐步生成具有内在非欧空间结构的图。作者证明了该模型比经典扩散框架具有更强的理论保证。在分子图和通用图生成任务的广泛实验中，HOG-Diff  consistently outperforms 或 remains competitive with 最先进基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "physics.soc-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04308v1",
      "published_date": "2025-02-06 18:51:14 UTC",
      "updated_date": "2025-02-06 18:51:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:58:12.173688"
    },
    {
      "arxiv_id": "2502.04307v1",
      "title": "DexterityGen: Foundation Controller for Unprecedented Dexterity",
      "title_zh": "翻译失败",
      "authors": [
        "Zhao-Heng Yin",
        "Changhao Wang",
        "Luis Pineda",
        "Francois Hogan",
        "Krishna Bodduluri",
        "Akash Sharma",
        "Patrick Lancaster",
        "Ishita Prasad",
        "Mrinal Kalakrishnan",
        "Jitendra Malik",
        "Mike Lambeta",
        "Tingfan Wu",
        "Pieter Abbeel",
        "Mustafa Mukadam"
      ],
      "abstract": "Teaching robots dexterous manipulation skills, such as tool use, presents a\nsignificant challenge. Current approaches can be broadly categorized into two\nstrategies: human teleoperation (for imitation learning) and sim-to-real\nreinforcement learning. The first approach is difficult as it is hard for\nhumans to produce safe and dexterous motions on a different embodiment without\ntouch feedback. The second RL-based approach struggles with the domain gap and\ninvolves highly task-specific reward engineering on complex tasks. Our key\ninsight is that RL is effective at learning low-level motion primitives, while\nhumans excel at providing coarse motion commands for complex, long-horizon\ntasks. Therefore, the optimal solution might be a combination of both\napproaches. In this paper, we introduce DexterityGen (DexGen), which uses RL to\npretrain large-scale dexterous motion primitives, such as in-hand rotation or\ntranslation. We then leverage this learned dataset to train a dexterous\nfoundational controller. In the real world, we use human teleoperation as a\nprompt to the controller to produce highly dexterous behavior. We evaluate the\neffectiveness of DexGen in both simulation and real world, demonstrating that\nit is a general-purpose controller that can realize input dexterous\nmanipulation commands and significantly improves stability by 10-100x measured\nas duration of holding objects across diverse tasks. Notably, with DexGen we\ndemonstrate unprecedented dexterous skills including diverse object\nreorientation and dexterous tool use such as pen, syringe, and screwdriver for\nthe first time.",
      "tldr_zh": "本论文针对机器人灵巧操作（如工具使用）的挑战，提出DexterityGen (DexGen)框架，该框架结合RL (Reinforcement Learning)预训练大规模动作原语（如手内旋转或平移），并使用这些数据训练一个通用基础控制器。DexGen在实际应用中以人类遥操作作为提示，实现高度灵巧行为，并在模拟和真实环境中将物体保持稳定性提升10-100倍。实验结果展示了前所未有的技能，包括多样物体重新定向和首次实现笔、注射器、螺丝刀等工具的使用。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project: https://zhaohengyin.github.io/dexteritygen",
      "pdf_url": "http://arxiv.org/pdf/2502.04307v1",
      "published_date": "2025-02-06 18:49:35 UTC",
      "updated_date": "2025-02-06 18:49:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:58:24.724653"
    },
    {
      "arxiv_id": "2502.04302v1",
      "title": "Strong Equivalence in Answer Set Programming with Constraints",
      "title_zh": "带约束的答案集编程中的强等价",
      "authors": [
        "Pedro Cabalar",
        "Jorge Fandinno",
        "Torsten Schaub",
        "Philipp Wanko"
      ],
      "abstract": "We investigate the concept of strong equivalence within the extended\nframework of Answer Set Programming with constraints. Two groups of rules are\nconsidered strongly equivalent if, informally speaking, they have the same\nmeaning in any context. We demonstrate that, under certain assumptions, strong\nequivalence between rule sets in this extended setting can be precisely\ncharacterized by their equivalence in the logic of Here-and-There with\nconstraints. Furthermore, we present a translation from the language of several\nclingo-based answer set solvers that handle constraints into the language of\nHere-and-There with constraints. This translation enables us to leverage the\nlogic of Here-and-There to reason about strong equivalence within the context\nof these solvers. We also explore the computational complexity of determining\nstrong equivalence in this context.",
      "tldr_zh": "这篇论文探讨了Answer Set Programming with Constraints中的Strong Equivalence，定义为两个规则组在任何上下文中的含义相同。研究发现，在特定假设下，规则集的Strong Equivalence可以通过Here-and-There with Constraints逻辑精确表征，并提供了一种从clingo-based answer set solvers语言到Here-and-There with Constraints的翻译方法，以支持相关推理。论文还分析了确定Strong Equivalence的计算复杂性，为扩展的Answer Set Programming框架提供了理论基础。",
      "categories": [
        "cs.AI",
        "cs.LO",
        "I.2.4; I.2.8"
      ],
      "primary_category": "cs.AI",
      "comment": "30 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.04302v1",
      "published_date": "2025-02-06 18:43:59 UTC",
      "updated_date": "2025-02-06 18:43:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:58:35.136912"
    },
    {
      "arxiv_id": "2502.04290v1",
      "title": "Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants",
      "title_zh": "翻译失败",
      "authors": [
        "Fares Fourati",
        "Salma Kharrat",
        "Vaneet Aggarwal",
        "Mohamed-Slim Alouini"
      ],
      "abstract": "Optimizing expensive, non-convex, black-box Lipschitz continuous functions\npresents significant challenges, particularly when the Lipschitz constant of\nthe underlying function is unknown. Such problems often demand numerous\nfunction evaluations to approximate the global optimum, which can be\nprohibitive in terms of time, energy, or resources. In this work, we introduce\nEvery Call is Precious (ECP), a novel global optimization algorithm that\nminimizes unpromising evaluations by strategically focusing on potentially\noptimal regions. Unlike previous approaches, ECP eliminates the need to\nestimate the Lipschitz constant, thereby avoiding additional function\nevaluations. ECP guarantees no-regret performance for infinite evaluation\nbudgets and achieves minimax-optimal regret bounds within finite budgets.\nExtensive ablation studies validate the algorithm's robustness, while empirical\nevaluations show that ECP outperforms 10 benchmark algorithms including\nLipschitz, Bayesian, bandits, and evolutionary methods across 30\nmulti-dimensional non-convex synthetic and real-world optimization problems,\nwhich positions ECP as a competitive approach for global optimization.",
      "tldr_zh": "本文提出Every Call is Precious (ECP)算法，用于优化未知Lipschitz常数的昂贵、非凸黑箱函数，通过战略性聚焦潜在最优区域来最小化无谓评估。ECP无需估计Lipschitz常数，避免额外函数评估，并在理论上保证无限预算下的无遗憾性能，以及有限预算下的最小最大遗憾界。实验结果显示，ECP在30个多维非凸合成和真实优化问题上，优于10个基准算法，包括Lipschitz、Bayesian、bandits和evolutionary方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at AISTATS 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04290v1",
      "published_date": "2025-02-06 18:34:40 UTC",
      "updated_date": "2025-02-06 18:34:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:58:47.650988"
    },
    {
      "arxiv_id": "2502.04424v1",
      "title": "EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models",
      "title_zh": "EmoBench-M：",
      "authors": [
        "He Hu",
        "Yucheng Zhou",
        "Lianzhong You",
        "Hongbo Xu",
        "Qianning Wang",
        "Zheng Lian",
        "Fei Richard Yu",
        "Fei Ma",
        "Laizhong Cui"
      ],
      "abstract": "With the integration of Multimodal large language models (MLLMs) into robotic\nsystems and various AI applications, embedding emotional intelligence (EI)\ncapabilities into these models is essential for enabling robots to effectively\naddress human emotional needs and interact seamlessly in real-world scenarios.\nExisting static, text-based, or text-image benchmarks overlook the multimodal\ncomplexities of real-world interactions and fail to capture the dynamic,\nmultimodal nature of emotional expressions, making them inadequate for\nevaluating MLLMs' EI. Based on established psychological theories of EI, we\nbuild EmoBench-M, a novel benchmark designed to evaluate the EI capability of\nMLLMs across 13 valuation scenarios from three key dimensions: foundational\nemotion recognition, conversational emotion understanding, and socially complex\nemotion analysis. Evaluations of both open-source and closed-source MLLMs on\nEmoBench-M reveal a significant performance gap between them and humans,\nhighlighting the need to further advance their EI capabilities. All benchmark\nresources, including code and datasets, are publicly available at\nhttps://emo-gml.github.io/.",
      "tldr_zh": "该研究强调了在多模态大语言模型(MLLMs)中整合情感智能(EI)的重要性，以提升机器人对人类情感需求的响应能力，并指出现有基准因忽略多模态动态交互而不足。论文提出EmoBench-M，一个基于心理理论的新基准，用于评估MLLMs在13个场景中的EI能力，涵盖基础情感识别、对话情感理解和社会复杂情感分析三个维度。通过评估开源和闭源MLLMs，结果显示它们与人类相比存在显著性能差距，突显了进一步提升EI能力的必要性。该基准的所有资源已公开可用，以促进相关研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04424v1",
      "published_date": "2025-02-06 18:13:35 UTC",
      "updated_date": "2025-02-06 18:13:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:58:59.609610"
    },
    {
      "arxiv_id": "2502.04269v1",
      "title": "How does a Multilingual LM Handle Multiple Languages?",
      "title_zh": "翻译失败",
      "authors": [
        "Santhosh Kakarla",
        "Gautama Shastry Bulusu Venkata",
        "Aishwarya Gaddam"
      ],
      "abstract": "Multilingual language models have significantly advanced due to rapid\nprogress in natural language processing. Models like BLOOM 1.7B, trained on\ndiverse multilingual datasets, aim to bridge linguistic gaps. However, their\neffectiveness in capturing linguistic knowledge, particularly for low-resource\nlanguages, remains an open question. This study critically examines MLMs\ncapabilities in multilingual understanding, semantic representation, and\ncross-lingual knowledge transfer. While these models perform well for\nhigh-resource languages, they struggle with less-represented ones.\nAdditionally, traditional evaluation methods often overlook their internal\nsyntactic and semantic encoding.\n  This research addresses key limitations through three objectives. First, it\nassesses semantic similarity by analyzing multilingual word embeddings for\nconsistency using cosine similarity. Second, it examines BLOOM-1.7B and Qwen2\nthrough Named Entity Recognition and sentence similarity tasks to understand\ntheir linguistic structures. Third, it explores cross-lingual knowledge\ntransfer by evaluating generalization from high-resource to low-resource\nlanguages in sentiment analysis and text classification.\n  By leveraging linguistic probing, performance metrics, and visualizations,\nthis study provides insights into the strengths and limitations of MLMs. The\nfindings aim to enhance multilingual NLP models, ensuring better support for\nboth high- and low-resource languages, thereby promoting inclusivity in\nlanguage technologies.",
      "tldr_zh": "本研究探讨了多语言语言模型（Multilingual LMs）如 BLOOM 1.7B 在处理多种语言时的能力，重点关注其在低资源语言上的局限性，包括语义表示和跨语言知识转移的不足。研究通过分析多语言词嵌入的余弦相似性（cosine similarity）、命名实体识别（Named Entity Recognition）和句子相似性任务，来评估模型的内部结构和性能；同时，考察了从高资源语言到低资源语言的知识转移，如在情感分析和文本分类中的泛化效果。结果表明，虽然模型在高资源语言上表现良好，但传统评估方法忽略了其语义编码问题；这项工作通过语言探测（linguistic probing）和可视化，提供见解以提升 Multilingual NLP 模型的包容性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04269v1",
      "published_date": "2025-02-06 18:08:14 UTC",
      "updated_date": "2025-02-06 18:08:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:59:13.708703"
    },
    {
      "arxiv_id": "2502.04268v2",
      "title": "Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Yu",
        "Botao Ren",
        "Peiyuan Zhang",
        "Mingxin Liu",
        "Junwei Luo",
        "Shaofeng Zhang",
        "Feipeng Da",
        "Junchi Yan",
        "Xue Yang"
      ],
      "abstract": "With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning OOD from\npoint annotations has gained great attention. In this paper, we rethink this\nchallenging task setting with the layout among instances and present\nPoint2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It\nlearns an upper bound for each instance by treating objects as 2D Gaussian\ndistributions and minimizing their overlap. 2) Voronoi watershed loss. It\nlearns a lower bound for each instance through watershed on Voronoi\ntessellation. 3) Consistency loss. It learns the size/rotation variation\nbetween two output sets with respect to an input image and its augmented view.\nSupplemented by a few devised techniques, e.g. edge loss and copy-paste, the\ndetector is further enhanced. To our best knowledge, Point2RBox-v2 is the first\napproach to explore the spatial layout among instances for learning\npoint-supervised OOD. Our solution is elegant and lightweight, yet it is\nexpected to give a competitive performance especially in densely packed scenes:\n62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M. Code is available at\nhttps://github.com/VisionXLab/point2rbox-v2.",
      "tldr_zh": "本研究重新审视了点监督的定向物体检测（OOD），提出 Point2RBox-v2 方法，通过探索实例间空间布局来提升弱监督学习效果。该方法的核心原则包括 Gaussian overlap loss（将对象视为 2D Gaussian 分布最小化重叠）、Voronoi watershed loss（通过 Voronoi tessellation 的 watershed 学习实例边界）和 Consistency loss（处理输入图像及其增强视图的尺寸/旋转变化），并辅以 edge loss 和 copy-paste 等技术优化检测器。在 DOTA/HRSC/FAIR1M 数据集上，Point2RBox-v2 分别实现了 62.61%、86.15% 和 34.71% 的性能，尤其在密集场景中表现出竞争优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 5 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.04268v2",
      "published_date": "2025-02-06 18:07:25 UTC",
      "updated_date": "2025-02-07 02:23:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:59:24.722364"
    },
    {
      "arxiv_id": "2502.04263v1",
      "title": "Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion",
      "title_zh": "翻译失败",
      "authors": [
        "Marco Mistretta",
        "Alberto Baldrati",
        "Lorenzo Agnolucci",
        "Marco Bertini",
        "Andrew D. Bagdanov"
      ],
      "abstract": "Pre-trained multi-modal Vision-Language Models like CLIP are widely used\noff-the-shelf for a variety of applications. In this paper, we show that the\ncommon practice of individually exploiting the text or image encoders of these\npowerful multi-modal models is highly suboptimal for intra-modal tasks like\nimage-to-image retrieval. We argue that this is inherently due to the\nCLIP-style inter-modal contrastive loss that does not enforce any intra-modal\nconstraints, leading to what we call intra-modal misalignment. To demonstrate\nthis, we leverage two optimization-based modality inversion techniques that map\nrepresentations from their input modality to the complementary one without any\nneed for auxiliary data or additional trained adapters. We empirically show\nthat, in the intra-modal tasks of image-to-image and text-to-text retrieval,\napproaching these tasks inter-modally significantly improves performance with\nrespect to intra-modal baselines on more than fifteen datasets. Additionally,\nwe demonstrate that approaching a native inter-modal task (e.g. zero-shot image\nclassification) intra-modally decreases performance, further validating our\nfindings. Finally, we show that incorporating an intra-modal term in the\npre-training objective or narrowing the modality gap between the text and image\nfeature embedding spaces helps reduce the intra-modal misalignment. The code is\npublicly available at: https://github.com/miccunifi/Cross-the-Gap.",
      "tldr_zh": "这篇论文揭示了预训练多模态模型 CLIP 中的 intra-modal misalignment 问题，即模态间对比损失未强制内部模态约束，导致在图像到图像或文本到文本检索等内部模态任务上性能不佳。作者引入两种优化-based 的 modality inversion 技术，将一种模态的表示映射到另一模态，而无需额外数据或适配器，从而证明通过模态间方法处理这些任务可显著提升性能，在超过15个数据集上比基线高。最终，论文展示了在原生模态间任务（如零样本图像分类）上使用内部模态方法会降低表现，并建议在预训练目标中添加内部模态项或缩小模态间特征空间差距，以缓解 intra-modal misalignment。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04263v1",
      "published_date": "2025-02-06 17:58:59 UTC",
      "updated_date": "2025-02-06 17:58:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:59:37.005555"
    },
    {
      "arxiv_id": "2502.04249v1",
      "title": "Free Energy Risk Metrics for Systemically Safe AI: Gatekeeping Multi-Agent Study",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Walters",
        "Rafael Kaufmann",
        "Justice Sefas",
        "Thomas Kopinski"
      ],
      "abstract": "We investigate the Free Energy Principle as a foundation for measuring risk\nin agentic and multi-agent systems. From these principles we introduce a\nCumulative Risk Exposure metric that is flexible to differing contexts and\nneeds. We contrast this to other popular theories for safe AI that hinge on\nmassive amounts of data or describing arbitrarily complex world models. In our\nframework, stakeholders need only specify their preferences over system\noutcomes, providing straightforward and transparent decision rules for risk\ngovernance and mitigation. This framework naturally accounts for uncertainty in\nboth world model and preference model, allowing for decision-making that is\nepistemically and axiologically humble, parsimonious, and future-proof. We\ndemonstrate this novel approach in a simplified autonomous vehicle environment\nwith multi-agent vehicles whose driving policies are mediated by gatekeepers\nthat evaluate, in an online fashion, the risk to the collective safety in their\nneighborhood, and intervene through each vehicle's policy when appropriate. We\nshow that the introduction of gatekeepers in an AV fleet, even at low\npenetration, can generate significant positive externalities in terms of\nincreased system safety.",
      "tldr_zh": "该研究基于 Free Energy Principle 提出 Cumulative Risk Exposure 指标，用于衡量代理和多代理系统中的风险，提供一种灵活且无需庞大数据或复杂世界模型的框架。利益相关者只需指定系统结果偏好，即可实现透明的风险治理和不确定性处理，使决策更具认知和价值谦逊性。在简化自主车辆环境中，引入 gatekeepers 实时评估多代理安全风险并适时干预，即使渗透率低，也能显著提升整体系统安全。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "physics.data-an",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2502.04249v1",
      "published_date": "2025-02-06 17:38:45 UTC",
      "updated_date": "2025-02-06 17:38:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:59:47.225306"
    },
    {
      "arxiv_id": "2502.04245v1",
      "title": "TriNER: A Series of Named Entity Recognition Models For Hindi, Bengali & Marathi",
      "title_zh": "TriNER：针对Hindi、Bengali和Marathi的一系列命名实体识别模型",
      "authors": [
        "Mohammed Amaan Dhamaskar",
        "Rasika Ransing"
      ],
      "abstract": "India's rich cultural and linguistic diversity poses various challenges in\nthe domain of Natural Language Processing (NLP), particularly in Named Entity\nRecognition (NER). NER is a NLP task that aims to identify and classify tokens\ninto different entity groups like Person, Location, Organization, Number, etc.\nThis makes NER very useful for downstream tasks like context-aware\nanonymization. This paper details our work to build a multilingual NER model\nfor the three most spoken languages in India - Hindi, Bengali & Marathi. We\ntrain a custom transformer model and fine tune a few pretrained models,\nachieving an F1 Score of 92.11 for a total of 6 entity groups. Through this\npaper, we aim to introduce a single model to perform NER and significantly\nreduce the inconsistencies in entity groups and tag names, across the three\nlanguages.",
      "tldr_zh": "这篇论文介绍了TriNER，一系列针对印地语(Hindi)、孟加拉语(Bengali)和马拉地语(Marathi)的命名实体识别(NER)模型，旨在解决印度语言多样性在自然语言处理(NLP)领域面临的挑战。研究团队训练了一个自定义的transformer模型，并微调了预训练模型，针对包括Person、Location、Organization、Number等在内的6个实体组，实现了92.11的F1 Score。该模型提供了一个统一的单模型解决方案，有效减少了三种语言之间实体组和标签名称的不一致性，从而支持下游任务如上下文感知匿名化。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04245v1",
      "published_date": "2025-02-06 17:37:36 UTC",
      "updated_date": "2025-02-06 17:37:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T07:59:59.975429"
    },
    {
      "arxiv_id": "2502.04242v2",
      "title": "A Theoretical Framework for Data Efficient Multi-Source Transfer Learning Based on Cramér-Rao Bound",
      "title_zh": "基于 Cramér-Rao 界的数据高效多源迁移学习理论框架",
      "authors": [
        "Qingyue Zhang",
        "Haohao Fu",
        "Guanbo Huang",
        "Yaoyuan Liang",
        "Chang Chu",
        "Tianren Peng",
        "Yanru Wu",
        "Qi Li",
        "Yang Li",
        "Shao-Lun Huang"
      ],
      "abstract": "Multi-source transfer learning provides an effective solution to data\nscarcity in real-world supervised learning scenarios by leveraging multiple\nsource tasks. In this field, existing works typically use all available samples\nfrom sources in training, which constrains their training efficiency and may\nlead to suboptimal results. To address this, we propose a theoretical framework\nthat answers the question: what is the optimal quantity of source samples\nneeded from each source task to jointly train the target model? Specifically,\nwe introduce a generalization error measure that aligns with cross-entropy\nloss, and minimize it based on the Cram\\'er-Rao Bound to determine the optimal\ntransfer quantity for each source task. Additionally, we develop an\narchitecture-agnostic and data-efficient algorithm OTQMS to implement our\ntheoretical results for training deep multi-source transfer learning models.\nExperimental studies on diverse architectures and two real-world benchmark\ndatasets show that our proposed algorithm significantly outperforms\nstate-of-the-art approaches in both accuracy and data efficiency. The code and\nsupplementary materials are available in\nhttps://anonymous.4open.science/r/Materials.",
      "tldr_zh": "这篇论文提出一个基于Cramér-Rao Bound的理论框架，用于数据高效的多源转移学习(multi-source transfer learning)，以解决监督学习中数据稀缺问题，避免使用所有源样本导致的效率低下。框架引入一个与交叉熵损失对齐的泛化错误度量，并通过最小化Cramér-Rao Bound来确定每个源任务的最优样本量。作者开发了架构无关的算法OTQMS来实现这一方法，并在不同架构和真实世界基准数据集上的实验中，显示其在准确性和数据效率上显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04242v2",
      "published_date": "2025-02-06 17:32:49 UTC",
      "updated_date": "2025-02-25 14:33:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:00:12.899681"
    },
    {
      "arxiv_id": "2502.15737v1",
      "title": "A Performance Analysis of You Only Look Once Models for Deployment on Constrained Computational Edge Devices in Drone Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Rey",
        "Ana M. Bernardos",
        "Andrzej D. Dobrzycki",
        "David Carramiñana",
        "Luca Bergesio",
        "Juan A. Besada",
        "José Ramón Casar"
      ],
      "abstract": "Advancements in embedded systems and Artificial Intelligence (AI) have\nenhanced the capabilities of Unmanned Aircraft Vehicles (UAVs) in computer\nvision. However, the integration of AI techniques o-nboard drones is\nconstrained by their processing capabilities. In this sense, this study\nevaluates the deployment of object detection models (YOLOv8n and YOLOv8s) on\nboth resource-constrained edge devices and cloud environments. The objective is\nto carry out a comparative performance analysis using a representative\nreal-time UAV image processing pipeline. Specifically, the NVIDIA Jetson Orin\nNano, Orin NX, and Raspberry Pi 5 (RPI5) devices have been tested to measure\ntheir detection accuracy, inference speed, and energy consumption, and the\neffects of post-training quantization (PTQ). The results show that YOLOv8n\nsurpasses YOLOv8s in its inference speed, achieving 52 FPS on the Jetson Orin\nNX and 65 fps with INT8 quantization. Conversely, the RPI5 failed to satisfy\nthe real-time processing needs in spite of its suitability for low-energy\nconsumption applications. An analysis of both the cloud-based and edge-based\nend-to-end processing times showed that increased communication latencies\nhindered real-time applications, revealing trade-offs between edge (low\nlatency) and cloud processing (quick processing). Overall, these findings\ncontribute to providing recommendations and optimization strategies for the\ndeployment of AI models on UAVs.",
      "tldr_zh": "本研究评估了 YOLOv8n 和 YOLOv8s 模型在资源受限的边缘设备（如 NVIDIA Jetson Orin Nano、Orin NX 和 Raspberry Pi 5）以及云环境中的部署性能，针对无人机(UAVs)实时图像处理管道进行比较分析，包括检测准确率、推理速度、能源消耗和后训练量化(PTQ)的影响。结果显示，YOLOv8n 在 Jetson Orin NX 上达到52 FPS，应用 INT8 量化后提升至65 FPS，而 Raspberry Pi 5 虽适合低能耗应用但无法满足实时处理需求；此外，边缘计算提供低延迟优势，但云端处理因通信延迟而受限。总体而言，该研究为 AI 模型在 UAVs 上的优化部署提供了实用推荐和策略。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CV",
        "I.2.10; C.3; C.1.3"
      ],
      "primary_category": "cs.DC",
      "comment": "This manuscript consists of 24 pages, 7 figures, and 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.15737v1",
      "published_date": "2025-02-06 17:22:01 UTC",
      "updated_date": "2025-02-06 17:22:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:00:26.255461"
    },
    {
      "arxiv_id": "2502.04423v1",
      "title": "Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions",
      "title_zh": "初级护理诊断作为骨科手术干预的可靠预测指标",
      "authors": [
        "Khushboo Verma",
        "Alan Michels",
        "Ergi Gumusaneli",
        "Shilpa Chitnis",
        "Smita Sinha Kumar",
        "Christopher Thompson",
        "Lena Esmail",
        "Guruprasath Srinivasan",
        "Chandini Panchada",
        "Sushovan Guha",
        "Satwant Kumar"
      ],
      "abstract": "Referral workflow inefficiencies, including misaligned referrals and delays,\ncontribute to suboptimal patient outcomes and higher healthcare costs. In this\nstudy, we investigated the possibility of predicting procedural needs based on\nprimary care diagnostic entries, thereby improving referral accuracy,\nstreamlining workflows, and providing better care to patients. A de-identified\ndataset of 2,086 orthopedic referrals from the University of Texas Health at\nTyler was analyzed using machine learning models built on Base General\nEmbeddings (BGE) for semantic extraction. To ensure real-world applicability,\nnoise tolerance experiments were conducted, and oversampling techniques were\nemployed to mitigate class imbalance. The selected optimum and parsimonious\nembedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, Matthews\nCorrelation Coefficient (MCC): 0.540), effectively distinguishing patients\nrequiring surgical intervention. Dimensionality reduction techniques confirmed\nthe model's ability to capture meaningful clinical relationships. A threshold\nsensitivity analysis identified an optimal decision threshold (0.30) to balance\nprecision and recall, maximizing referral efficiency. In the predictive\nmodeling analysis, the procedure rate increased from 11.27% to an optimal\n60.1%, representing a 433% improvement with significant implications for\noperational efficiency and healthcare revenue.\n  The results of our study demonstrate that referral optimization can enhance\nprimary and surgical care integration. Through this approach, precise and\ntimely predictions of procedural requirements can be made, thereby minimizing\ndelays, improving surgical planning, and reducing administrative burdens. In\naddition, the findings highlight the potential of clinical decision support as\na scalable solution for improving patient outcomes and the efficiency of the\nhealthcare system.",
      "tldr_zh": "这篇论文探讨了使用初级护理诊断预测骨科手术需求的潜力，以优化转诊工作流程、减少延误并降低医疗成本。研究分析了2,086例骨科转诊数据，通过基于Base General Embeddings (BGE)的机器学习模型进行语义提取，并采用噪声容忍实验和过采样技术处理类别不平衡。结果显示，选定的模型具有高预测准确性（ROC-AUC: 0.874, MCC: 0.540），手术程序率从11.27%提升至60.1%，实现了433%的改善。该方法证明了临床决策支持的可扩展性，有助于增强初级和手术护理的整合，提高患者结果和系统效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.6; I.2.7; J.3; H.2.8"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04423v1",
      "published_date": "2025-02-06 17:15:12 UTC",
      "updated_date": "2025-02-06 17:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:00:38.669764"
    },
    {
      "arxiv_id": "2502.04230v2",
      "title": "XAttnMark: Learning Robust Audio Watermarking with Cross-Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Yixin Liu",
        "Lie Lu",
        "Jihui Jin",
        "Lichao Sun",
        "Andrea Fanelli"
      ],
      "abstract": "The rapid proliferation of generative audio synthesis and editing\ntechnologies has raised significant concerns about copyright infringement, data\nprovenance, and the spread of misinformation through deepfake audio.\nWatermarking offers a proactive solution by embedding imperceptible,\nidentifiable, and traceable marks into audio content. While recent neural\nnetwork-based watermarking methods like WavMark and AudioSeal have improved\nrobustness and quality, they struggle to achieve both robust detection and\naccurate attribution simultaneously. This paper introduces Cross-Attention\nRobust Audio Watermark (XAttnMark), which bridges this gap by leveraging\npartial parameter sharing between the generator and the detector, a\ncross-attention mechanism for efficient message retrieval, and a temporal\nconditioning module for improved message distribution. Additionally, we propose\na psychoacoustic-aligned temporal-frequency masking loss that captures\nfine-grained auditory masking effects, enhancing watermark imperceptibility.\nOur approach achieves state-of-the-art performance in both detection and\nattribution, demonstrating superior robustness against a wide range of audio\ntransformations, including challenging generative editing with strong editing\nstrength. The project webpage is available at\nhttps://liuyixin-louis.github.io/xattnmark/.",
      "tldr_zh": "该论文针对生成音频技术的版权侵权和虚假信息问题，提出XAttnMark，一种基于Cross-Attention的鲁棒音频水印方法。该方法通过部分参数共享、Cross-Attention机制和Temporal Conditioning模块，实现高效的消息检索和分布，同时引入Psychoacoustic-Aligned Temporal-Frequency Masking Loss，以提升水印的隐蔽性和听觉兼容性。与现有方法如WavMark和AudioSeal相比，XAttnMark在检测和归属任务上达到最先进性能，对各种音频转换和生成编辑表现出卓越的鲁棒性。项目网页为https://liuyixin-louis.github.io/xattnmark/。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "24 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04230v2",
      "published_date": "2025-02-06 17:15:08 UTC",
      "updated_date": "2025-02-07 20:11:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:00:49.063950"
    },
    {
      "arxiv_id": "2502.04229v1",
      "title": "Dark Distillation: Backdooring Distilled Datasets without Accessing Raw Data",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyuan Yang",
        "Ming Yan",
        "Yi Zhang",
        "Joey Tianyi Zhou"
      ],
      "abstract": "Dataset distillation (DD) enhances training efficiency and reduces bandwidth\nby condensing large datasets into smaller synthetic ones. It enables models to\nachieve performance comparable to those trained on the raw full dataset and has\nbecome a widely adopted method for data sharing. However, security concerns in\nDD remain underexplored. Existing studies typically assume that malicious\nbehavior originates from dataset owners during the initial distillation\nprocess, where backdoors are injected into raw datasets. In contrast, this work\nis the first to address a more realistic and concerning threat: attackers may\nintercept the dataset distribution process, inject backdoors into the distilled\ndatasets, and redistribute them to users. While distilled datasets were\npreviously considered resistant to backdoor attacks, we demonstrate that they\nremain vulnerable to such attacks. Furthermore, we show that attackers do not\neven require access to any raw data to inject the backdoors successfully.\nSpecifically, our approach reconstructs conceptual archetypes for each class\nfrom the model trained on the distilled dataset. Backdoors are then injected\ninto these archetypes to update the distilled dataset. Moreover, we ensure the\nupdated dataset not only retains the backdoor but also preserves the original\noptimization trajectory, thus maintaining the knowledge of the raw dataset. To\nachieve this, a hybrid loss is designed to integrate backdoor information along\nthe benign optimization trajectory, ensuring that previously learned\ninformation is not forgotten. Extensive experiments demonstrate that distilled\ndatasets are highly vulnerable to backdoor attacks, with risks pervasive across\nvarious raw datasets, distillation methods, and downstream training strategies.\nMoreover, our attack method is efficient, capable of synthesizing a malicious\ndistilled dataset in under one minute in certain cases.",
      "tldr_zh": "本研究首次探讨了数据集蒸馏（Dataset Distillation, DD）过程中的新安全威胁，即攻击者在不访问原始数据的情况下，拦截并向蒸馏数据集注入后门（Backdoors），然后重新分发。攻击方法涉及从蒸馏数据集训练的模型中重建每个类别的概念原型（Conceptual Archetypes），并通过注入后门来更新数据集。论文设计了混合损失（Hybrid Loss）来整合后门信息，同时保留原始优化轨迹（Optimization Trajectory），确保数据集不遗忘原始知识。实验结果表明，这种攻击高效（某些情况下可在1分钟内完成），并广泛适用于各种原始数据集、蒸馏方法和下游训练策略，突显了DD的安全漏洞。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04229v1",
      "published_date": "2025-02-06 17:14:17 UTC",
      "updated_date": "2025-02-06 17:14:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:01:01.960910"
    },
    {
      "arxiv_id": "2502.04219v1",
      "title": "NLP-Based .NET CLR Event Logs Analyzer",
      "title_zh": "基于 NLP 的 .NET CLR 事件日志分析器",
      "authors": [
        "Maxim Stavtsev",
        "Sergey Shershakov"
      ],
      "abstract": "In this paper, we present a tool for analyzing .NET CLR event logs based on a\nnovel method inspired by Natural Language Processing (NLP) approach. Our\nresearch addresses the growing need for effective monitoring and optimization\nof software systems through detailed event log analysis. We utilize a\nBERT-based architecture with an enhanced tokenization process customized to\nevent logs. The tool, developed using Python, its libraries, and an SQLite\ndatabase, allows both conducting experiments for academic purposes and\nefficiently solving industry-emerging tasks. Our experiments demonstrate the\nefficacy of our approach in compressing event sequences, detecting recurring\npatterns, and identifying anomalies. The trained model shows promising results,\nwith a high accuracy rate in anomaly detection, which demonstrates the\npotential of NLP methods to improve the reliability and stability of software\nsystems.",
      "tldr_zh": "这篇论文提出了一种基于 NLP 的工具，用于分析 .NET CLR 事件日志，以满足软件系统监控和优化的需求。该工具采用 BERT-based 架构，并通过增强的 tokenization 过程来处理事件日志，使用 Python 和 SQLite 进行开发，支持学术实验和行业应用。实验结果表明，该方法在压缩事件序列、检测重复模式以及识别异常方面表现出色，实现了高准确率的异常检测，从而提升了软件系统的可靠性和稳定性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04219v1",
      "published_date": "2025-02-06 17:01:38 UTC",
      "updated_date": "2025-02-06 17:01:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:01:11.820617"
    },
    {
      "arxiv_id": "2502.04210v3",
      "title": "Algorithmic causal structure emerging through compression",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Wendong",
        "Simon Buchholz",
        "Bernhard Schölkopf"
      ],
      "abstract": "We explore the relationship between causality, symmetry, and compression. We\nbuild on and generalize the known connection between learning and compression\nto a setting where causal models are not identifiable. We propose a framework\nwhere causality emerges as a consequence of compressing data across multiple\nenvironments. We define algorithmic causality as an alternative definition of\ncausality when traditional assumptions for causal identifiability do not hold.\nWe demonstrate how algorithmic causal and symmetric structures can emerge from\nminimizing upper bounds on Kolmogorov complexity, without knowledge of\nintervention targets. We hypothesize that these insights may also provide a\nnovel perspective on the emergence of causality in machine learning models,\nsuch as large language models, where causal relationships may not be explicitly\nidentifiable.",
      "tldr_zh": "本论文探讨了因果性、对称性和压缩之间的关系，构建并推广了学习与压缩的已知联系，适用于因果模型不可识别的场景。研究提出一个框架，通过在多个环境中压缩数据，使 algorithmic causality 作为一种替代定义自然出现，而无需传统因果可识别假设。作者展示了通过最小化 Kolmogorov complexity 的上界，可以让算法因果和对称结构自发生成，并假设此见解可为机器学习模型（如大型语言模型）中隐性因果关系的出现提供新视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "Extended version of the camera-ready paper accepted at CLeaR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04210v3",
      "published_date": "2025-02-06 16:50:57 UTC",
      "updated_date": "2025-03-21 14:54:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:01:24.656581"
    },
    {
      "arxiv_id": "2502.04194v2",
      "title": "The Best Instruction-Tuning Data are Those That Fit",
      "title_zh": "翻译失败",
      "authors": [
        "Dylan Zhang",
        "Qirun Dai",
        "Hao Peng"
      ],
      "abstract": "High-quality supervised fine-tuning (SFT) data are crucial for eliciting\nstrong capabilities from pretrained large language models (LLMs). Typically,\ninstructions are paired with multiple responses sampled from other LLMs, which\nare often out of the distribution of the target model to be fine-tuned. This,\nat scale, can lead to diminishing returns and even hurt the models' performance\nand robustness. We propose **GRAPE**, a novel SFT framework that accounts for\nthe unique characteristics of the target model. For each instruction, it\ngathers responses from various LLMs and selects the one with the highest\nprobability measured by the target model, indicating that it aligns most\nclosely with the target model's pretrained distribution; it then proceeds with\nstandard SFT training.\n  We first evaluate GRAPE with a controlled experiment, where we sample various\nsolutions for each question in UltraInteract from multiple models and fine-tune\ncommonly used LMs like LLaMA3.1-8B, Mistral-7B, and Qwen2.5-7B on\nGRAPE-selected data. GRAPE significantly outperforms strong baselines,\nincluding distilling from the strongest model with an absolute gain of up to\n13.8%, averaged across benchmarks, and training on 3x more data with a maximum\nperformance improvement of 17.3%. GRAPE's strong performance generalizes to\nrealistic settings. We experiment with the post-training data used for Tulu3\nand Olmo-2. GRAPE outperforms strong baselines trained on 4.5 times more data\nby 6.1% and a state-of-the-art data selection approach by 3% on average\nperformance. Remarkably, using 1/3 of the data and half the number of epochs,\nGRAPE enables LLaMA3.1-8B to surpass the performance of Tulu3-SFT by 3.5%.",
      "tldr_zh": "该研究提出了一种名为 GRAPE 的 SFT（Supervised Fine-Tuning）框架，用于优化大语言模型（LLMs）的指令微调数据选择，强调选择与目标模型预训练分布最匹配的响应，以提升模型性能和鲁棒性。GRAPE 的方法包括从多个 LLMs 收集响应，并基于目标模型的概率选择最佳响应进行标准训练。实验结果显示，GRAPE 在 UltraInteract 数据集上使 LLaMA3.1-8B 等模型比强基线提升高达 13.8%，并在真实场景中，使用 1/3 数据和一半 epoch 就让 LLaMA3.1-8B 超过了 Tulu3-SFT 的性能，提升 3.5%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04194v2",
      "published_date": "2025-02-06 16:31:21 UTC",
      "updated_date": "2025-02-07 02:20:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:01:37.300916"
    },
    {
      "arxiv_id": "2502.04172v1",
      "title": "Archetypal Analysis for Binary Data",
      "title_zh": "二元数据的原型分析",
      "authors": [
        "A. Emilie J. Wedenborg",
        "Morten Mørup"
      ],
      "abstract": "Archetypal analysis (AA) is a matrix decomposition method that identifies\ndistinct patterns using convex combinations of the data points denoted\narchetypes with each data point in turn reconstructed as convex combinations of\nthe archetypes. AA thereby forms a polytope representing trade-offs of the\ndistinct aspects in the data. Most existing methods for AA are designed for\ncontinuous data and do not exploit the structure of the data distribution. In\nthis paper, we propose two new optimization frameworks for archetypal analysis\nfor binary data. i) A second order approximation of the AA likelihood based on\nthe Bernoulli distribution with efficient closed-form updates using an active\nset procedure for learning the convex combinations defining the archetypes, and\na sequential minimal optimization strategy for learning the observation\nspecific reconstructions. ii) A Bernoulli likelihood based version of the\nprincipal convex hull analysis (PCHA) algorithm originally developed for least\nsquares optimization. We compare these approaches with the only existing binary\nAA procedure relying on multiplicative updates and demonstrate their\nsuperiority on both synthetic and real binary data. Notably, the proposed\noptimization frameworks for AA can easily be extended to other data\ndistributions providing generic efficient optimization frameworks for AA based\non tailored likelihood functions reflecting the underlying data distribution.",
      "tldr_zh": "本论文针对二进制数据提出两种新的优化框架，用于Archetypal Analysis (AA)，一种通过数据点的凸组合识别archetypes并重构数据的方法，以更好地利用二进制数据的分布结构。第一框架基于Bernoulli分布的二阶近似AA似然函数，采用高效的闭式更新、active set过程和sequential minimal optimization策略来学习archetypes和重构；第二框架则是Bernoulli似然的principal convex hull analysis (PCHA)算法变体，与原有最小二乘优化方法相比更适合二进制数据。实验结果显示，这些新框架在合成和真实二进制数据上优于现有的基于multiplicative updates的方法。最后，该框架易于扩展到其他数据分布，提供通用的AA优化方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04172v1",
      "published_date": "2025-02-06 16:05:15 UTC",
      "updated_date": "2025-02-06 16:05:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:01:48.585584"
    },
    {
      "arxiv_id": "2502.04421v1",
      "title": "Assessing and Prioritizing Ransomware Risk Based on Historical Victim Data",
      "title_zh": "基于历史受害者数据的勒索软件风险评估与优先排序",
      "authors": [
        "Spencer Massengale",
        "Philip Huff"
      ],
      "abstract": "We present an approach to identifying which ransomware adversaries are most\nlikely to target specific entities, thereby assisting these entities in\nformulating better protection strategies. Ransomware poses a formidable\ncybersecurity threat characterized by profit-driven motives, a complex\nunderlying economy supporting criminal syndicates, and the overt nature of its\nattacks. This type of malware has consistently ranked among the most prevalent,\nwith a rapid escalation in activity observed. Recent estimates indicate that\napproximately two-thirds of organizations experienced ransomware attacks in\n2023 \\cite{Sophos2023Ransomware}. A central tactic in ransomware campaigns is\npublicizing attacks to coerce victims into paying ransoms. Our study utilizes\npublic disclosures from ransomware victims to predict the likelihood of an\nentity being targeted by a specific ransomware variant. We employ a Large\nLanguage Model (LLM) architecture that uses a unique chain-of-thought,\nmulti-shot prompt methodology to define adversary SKRAM (Skills, Knowledge,\nResources, Authorities, and Motivation) profiles from ransomware bulletins,\nthreat reports, and news items. This analysis is enriched with publicly\navailable victim data and is further enhanced by a heuristic for generating\nsynthetic data that reflects victim profiles. Our work culminates in the\ndevelopment of a machine learning model that assists organizations in\nprioritizing ransomware threats and formulating defenses based on the tactics,\ntechniques, and procedures (TTP) of the most likely attackers.",
      "tldr_zh": "本研究提出了一种基于历史受害者数据的评估和优先化勒索软件（Ransomware）风险的方法，以帮助实体预测特定勒索软件对手的攻击可能性并制定保护策略。该方法利用 Large Language Model (LLM) 架构，通过链式思维（Chain-of-Thought）和多示例提示（Multi-shot Prompt）从勒索软件公告、威胁报告及新闻中定义对手的 SKRAM（Skills, Knowledge, Resources, Authorities, and Motivation）配置文件，并结合公开受害者数据和合成数据进行分析。最终，开发了一个机器学习模型，协助组织根据最可能攻击者的策略、技术和程序（TTP）优先处理威胁并优化防御措施。实验结果表明，该方法能有效提升风险评估的准确性，为网络安全防护提供实用指导。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04421v1",
      "published_date": "2025-02-06 15:57:56 UTC",
      "updated_date": "2025-02-06 15:57:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:02:00.638564"
    },
    {
      "arxiv_id": "2502.04153v1",
      "title": "UltraIF: Advancing Instruction Following from the Wild",
      "title_zh": "翻译失败",
      "authors": [
        "Kaikai An",
        "Li Sheng",
        "Ganqu Cui",
        "Shuzheng Si",
        "Ning Ding",
        "Yu Cheng",
        "Baobao Chang"
      ],
      "abstract": "Instruction-following made modern large language models (LLMs) helpful\nassistants. However, the key to taming LLMs on complex instructions remains\nmysterious, for that there are huge gaps between models trained by open-source\ncommunity and those trained by leading companies. To bridge the gap, we propose\na simple and scalable approach UltraIF for building LLMs that can follow\ncomplex instructions with open-source data. UltraIF first decomposes real-world\nuser prompts into simpler queries, constraints, and corresponding evaluation\nquestions for the constraints. Then, we train an UltraComposer to compose\nconstraint-associated prompts with evaluation questions. This prompt composer\nallows us to synthesize complicated instructions as well as filter responses\nwith evaluation questions. In our experiment, for the first time, we\nsuccessfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5\ninstruction-following benchmarks without any benchmark information, using only\n8B model as response generator and evaluator. The aligned model also achieved\ncompetitive scores on other benchmarks. Moreover, we also show that UltraIF\ncould further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating\nbroader use cases for the method. Our code will be available at\nhttps://github.com/kkk-an/UltraIF.",
      "tldr_zh": "该研究提出了一种简单可扩展的方法UltraIF，用于利用开源数据训练大型语言模型(LLMs)以更好地遵循复杂指令，从而桥接开源社区与领先公司之间的差距。UltraIF首先将真实用户提示分解为简单查询、约束和对应的评估问题，然后训练UltraComposer来组合这些约束相关的提示并通过评估问题合成复杂指令和过滤响应。在实验中，该方法成功对齐LLaMA-3.1-8B-Base模型，使其在5个指令遵循基准上赶上其instruct版本，且在其他基准上取得竞争性成绩；此外，UltraIF还可通过自对齐进一步提升LLaMA-3.1-8B-Instruct的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04153v1",
      "published_date": "2025-02-06 15:39:16 UTC",
      "updated_date": "2025-02-06 15:39:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:02:12.646114"
    },
    {
      "arxiv_id": "2502.04420v3",
      "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Xing Li",
        "Zeyu Xing",
        "Yiming Li",
        "Linping Qu",
        "Hui-Ling Zhen",
        "Wulong Liu",
        "Yiwu Yao",
        "Sinno Jialin Pan",
        "Mingxuan Yuan"
      ],
      "abstract": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
      "tldr_zh": "该论文提出 KVTuner 框架，通过敏感性感知的层级混合精度量化来优化 LLMs 的 KV cache，从而在长上下文和大批量场景下提高推理吞吐量和延迟，同时保持几乎无损性能。框架分析了层级 Transformer 注意模式与量化错误的关联，强调 key cache 比 value cache 更重要，并采用多目标优化结合 intra-layer 精度配对修剪和 inter-layer 聚类来减少搜索空间，实现硬件友好的离线配置。实验结果显示，在 Llama-3.1-8B-Instruct 等模型上实现了 3.25-bit 混合精度量化，推理吞吐量比 KV8 量化提高了最多 38.3%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
      "pdf_url": "http://arxiv.org/pdf/2502.04420v3",
      "published_date": "2025-02-06 15:26:26 UTC",
      "updated_date": "2025-02-25 03:42:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:02:25.730823"
    },
    {
      "arxiv_id": "2502.04419v2",
      "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks",
      "title_zh": "理解并缓解LLM-based数据增强中偏见继承对下游任务的影响",
      "authors": [
        "Miaomiao Li",
        "Hao Chen",
        "Yang Wang",
        "Tingyuan Zhu",
        "Weijia Zhang",
        "Kaijie Zhu",
        "Kam-Fai Wong",
        "Jindong Wang"
      ],
      "abstract": "Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.",
      "tldr_zh": "该研究首次系统调查了在LLM-based数据增强中，偏见继承(bias inheritance)问题如何影响下游任务的公平性和鲁棒性。作者通过在10个分类和生成任务上实验，分析了6种偏见类型在不同比例下的表现，并识别了价值观、群体数据和数据分布的失调因素作为关键原因。为缓解这一问题，提出了基于token、mask和loss的三种策略；实验结果显示，这些策略在不同任务和偏见上的效果不一，表明完全消除偏见继承的挑战依然巨大。该工作为LLM数据增强研究提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical report; 31 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.04419v2",
      "published_date": "2025-02-06 15:20:58 UTC",
      "updated_date": "2025-02-10 16:34:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:02:37.465382"
    },
    {
      "arxiv_id": "2502.04140v1",
      "title": "Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs",
      "title_zh": "使用偏微分方程的",
      "authors": [
        "Jost Arndt",
        "Utku Isil",
        "Michael Detzel",
        "Wojciech Samek",
        "Jackie Ma"
      ],
      "abstract": "Many physical processes can be expressed through partial differential\nequations (PDEs). Real-world measurements of such processes are often collected\nat irregularly distributed points in space, which can be effectively\nrepresented as graphs; however, there are currently only a few existing\ndatasets. Our work aims to make advancements in the field of PDE-modeling\naccessible to the temporal graph machine learning community, while addressing\nthe data scarcity problem, by creating and utilizing datasets based on PDEs. In\nthis work, we create and use synthetic datasets based on PDEs to support\nspatio-temporal graph modeling in machine learning for different applications.\nMore precisely, we showcase three equations to model different types of\ndisasters and hazards in the fields of epidemiology, atmospheric particles, and\ntsunami waves. Further, we show how such created datasets can be used by\nbenchmarking several machine learning models on the epidemiological dataset.\nAdditionally, we show how pre-training on this dataset can improve model\nperformance on real-world epidemiological data. The presented methods enable\nothers to create datasets and benchmarks customized to individual requirements.\nThe source code for our methodology and the three created datasets can be found\non https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.",
      "tldr_zh": "该论文提出使用偏微分方程 (PDEs) 生成合成数据集，以解决时空图 (spatio-temporal graphs) 机器学习中的数据稀缺问题。研究者创建了基于三个 PDEs 的数据集，用于模拟流行病学、大气粒子和海啸波等灾害场景，并通过基准测试展示了多种机器学习模型在流行病学数据集上的性能。结果表明，在合成数据集上预训练可以显著提升模型在真实世界流行病学数据中的表现。该方法允许用户自定义数据集，并提供了开源代码以促进进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Currently under review",
      "pdf_url": "http://arxiv.org/pdf/2502.04140v1",
      "published_date": "2025-02-06 15:20:32 UTC",
      "updated_date": "2025-02-06 15:20:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:02:48.388950"
    },
    {
      "arxiv_id": "2502.04128v2",
      "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Ye",
        "Xinfa Zhu",
        "Chi-Min Chan",
        "Xinsheng Wang",
        "Xu Tan",
        "Jiahe Lei",
        "Yi Peng",
        "Haohe Liu",
        "Yizhu Jin",
        "Zheqi Dai",
        "Hongzhan Lin",
        "Jianyi Chen",
        "Xingjian Du",
        "Liumeng Xue",
        "Yunlin Chen",
        "Zhifei Li",
        "Lei Xie",
        "Qiuqiang Kong",
        "Yike Guo",
        "Wei Xue"
      ],
      "abstract": "Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.",
      "tldr_zh": "这篇论文探讨了在Llama-based语音合成中缩放训练时和推理时计算的潜力，以提升TTS系统的性能。作者提出Llasa框架，使用单层向量量化器(VQ)编解码器和单Transformer架构，与标准LLMs如Llama完全兼容，从而简化了多阶段TTS系统的复杂性。实验结果显示，缩放训练时计算显著提高了合成语音的自然度和韵律模式复杂度，而缩放推理时计算通过语音理解模型作为验证器，改善了情感表达、音色一致性和内容准确性。此外，作者公开了TTS模型（1B、3B、8B）和编解码器的检查点及训练代码。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04128v2",
      "published_date": "2025-02-06 15:04:00 UTC",
      "updated_date": "2025-02-22 11:32:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:03:01.107618"
    },
    {
      "arxiv_id": "2502.04418v1",
      "title": "Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Prakhar Srivastava",
        "Jasmeet Singh"
      ],
      "abstract": "This paper presents a comprehensive overview of autotelic Reinforcement\nLearning (RL), emphasizing the role of intrinsic motivations in the open-ended\nformation of skill repertoires. We delineate the distinctions between\nknowledge-based and competence-based intrinsic motivations, illustrating how\nthese concepts inform the development of autonomous agents capable of\ngenerating and pursuing self-defined goals. The typology of Intrinsically\nMotivated Goal Exploration Processes (IMGEPs) is explored, with a focus on the\nimplications for multi-goal RL and developmental robotics. The autotelic\nlearning problem is framed within a reward-free Markov Decision Process (MDP),\nWHERE agents must autonomously represent, generate, and master their own goals.\nWe address the unique challenges in evaluating such agents, proposing various\nmetrics for measuring exploration, generalization, and robustness in complex\nenvironments. This work aims to advance the understanding of autotelic RL\nagents and their potential for enhancing skill acquisition in a diverse and\ndynamic setting.",
      "tldr_zh": "这篇论文对 autotelic Reinforcement Learning (RL) 进行了全面概述，强调内在动机（intrinsic motivations）在开放环境中的技能获取作用。论文区分了 knowledge-based 和 competence-based 内在动机，并探讨了 Intrinsically Motivated Goal Exploration Processes (IMGEPs) 在 multi-goal RL 和 developmental robotics 中的应用，将 autotelic 学习问题框架化为无奖励的 Markov Decision Process (MDP)，让代理自主表示、生成和掌握目标。最终，论文提出评估指标来应对评估挑战，提升代理在复杂环境中的探索、泛化和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04418v1",
      "published_date": "2025-02-06 14:37:46 UTC",
      "updated_date": "2025-02-06 14:37:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:03:13.120956"
    },
    {
      "arxiv_id": "2502.04110v1",
      "title": "Ancient Greek Technology: An Immersive Learning Use Case Described Using a Co-Intelligent Custom ChatGPT Assistant",
      "title_zh": "古希腊技术：使用协同智能自定义 ChatGPT 助手的沉浸式学习用例描述",
      "authors": [
        "Vlasis Kasapakis",
        "Leonel Morgado"
      ],
      "abstract": "Achieving consistency in immersive learning case descriptions is essential\nbut challenging due to variations in research focus, methodology, and\nresearchers' background. We address these challenges by leveraging the\nImmersive Learning Case Sheet (ILCS), a methodological instrument to\nstandardize case descriptions, that we applied to an immersive learning case on\nancient Greek technology in VRChat. Research team members had differing levels\nof familiarity with the ILCS and the case content, so we developed a custom\nChatGPT assistant to facilitate consistent terminology and process alignment\nacross the team. This paper constitutes an example of how structured case\nreports can be a novel contribution to immersive learning literature. Our\nfindings demonstrate how the ILCS supports structured reflection and\ninterpretation of the case. Further we report that the use of a ChatGPT\nassistant significantly sup-ports the coherence and quality of the team members\ndevelopment of the final ILCS. This exposes the potential of employing\nAI-driven tools to enhance collaboration and standardization of research\npractices in qualitative educational research. However, we also discuss the\nlimitations and challenges, including reliance on AI for interpretive tasks and\nmanaging varied levels of expertise within the team. This study thus provides\ninsights into the practical application of AI in standardizing immersive\nlearning research processes.",
      "tldr_zh": "本研究探讨了在沉浸式学习案例描述中实现一致性的挑战，通过采用 Immersive Learning Case Sheet (ILCS) 作为标准化工具，并应用于一个关于古希腊技术的 VRChat 沉浸式学习案例。研究团队开发了一个自定义 ChatGPT assistant，以辅助团队成员保持术语和过程的一致性，从而提升协作效率。结果显示，ILCS 支持结构化的反思和解释，而 ChatGPT assistant 显著提高了案例描述的连贯性和质量，揭示了 AI 工具在标准化教育研究实践中的潜力。尽管如此，研究也指出了依赖 AI 进行解释任务的局限性，以及管理团队中不同专业水平带来的挑战。总的来说，此研究为沉浸式学习文献提供了结构化案例报告的新贡献，并展示了 AI 在研究过程中的实际应用价值。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.1; I.2.1"
      ],
      "primary_category": "cs.HC",
      "comment": "5 pages, presented at the 2024 IEEE 3rd International Conference on\n  Intelligent Reality (ICIR 2024), 6th of December, 2024",
      "pdf_url": "http://arxiv.org/pdf/2502.04110v1",
      "published_date": "2025-02-06 14:35:42 UTC",
      "updated_date": "2025-02-06 14:35:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:03:24.622940"
    },
    {
      "arxiv_id": "2502.04103v2",
      "title": "VTutor: An Open-Source SDK for Generative AI-Powered Animated Pedagogical Agents with Multi-Media Output",
      "title_zh": "翻译失败",
      "authors": [
        "Eason Chen",
        "Chenyu Lin",
        "Xinyi Tang",
        "Aprille Xi",
        "Canwen Wang",
        "Jionghao Lin",
        "Kenneth R Koedinger"
      ],
      "abstract": "The rapid evolution of large language models (LLMs) has transformed\nhuman-computer interaction (HCI), but the interaction with LLMs is currently\nmainly focused on text-based interactions, while other multi-model approaches\nremain under-explored. This paper introduces VTutor, an open-source Software\nDevelopment Kit (SDK) that combines generative AI with advanced animation\ntechnologies to create engaging, adaptable, and realistic APAs for human-AI\nmulti-media interactions. VTutor leverages LLMs for real-time personalized\nfeedback, advanced lip synchronization for natural speech alignment, and WebGL\nrendering for seamless web integration. Supporting various 2D and 3D character\nmodels, VTutor enables researchers and developers to design emotionally\nresonant, contextually adaptive learning agents. This toolkit enhances learner\nengagement, feedback receptivity, and human-AI interaction while promoting\ntrustworthy AI principles in education. VTutor sets a new standard for\nnext-generation APAs, offering an accessible, scalable solution for fostering\nmeaningful and immersive human-AI interaction experiences. The VTutor project\nis open-sourced and welcomes community-driven contributions and showcases.",
      "tldr_zh": "本文介绍了VTutor，一个开源SDK，将生成式AI与先进动画技术结合，创建支持多媒体输出的动画教学代理（APAs），以增强人类-AI互动。VTutor利用LLMs提供实时个性化反馈、先进唇同步和WebGL渲染，支持各种2D和3D角色模型，帮助设计情感共鸣且上下文适应的学习代理。该工具提升了学习者参与度、反馈接受性和教育中的可信赖AI原则，并作为可访问、可扩展的开源项目，欢迎社区贡献。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04103v2",
      "published_date": "2025-02-06 14:27:54 UTC",
      "updated_date": "2025-02-13 17:57:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:03:36.477156"
    },
    {
      "arxiv_id": "2502.04417v1",
      "title": "NeuralMOVES: A lightweight and microscopic vehicle emission estimation model based on reverse engineering and surrogate learning",
      "title_zh": "翻译失败",
      "authors": [
        "Edgar Ramirez-Sanchez",
        "Catherine Tang",
        "Yaosheng Xu",
        "Nrithya Renganathan",
        "Vindula Jayawardana",
        "Zhengbing He",
        "Cathy Wu"
      ],
      "abstract": "The transportation sector significantly contributes to greenhouse gas\nemissions, necessitating accurate emission models to guide mitigation\nstrategies. Despite its field validation and certification, the\nindustry-standard Motor Vehicle Emission Simulator (MOVES) faces challenges\nrelated to complexity in usage, high computational demands, and its\nunsuitability for microscopic real-time applications. To address these\nlimitations, we present NeuralMOVES, a comprehensive suite of high-performance,\nlightweight surrogate models for vehicle CO2 emissions. Developed based on\nreverse engineering and Neural Networks, NeuralMOVES achieves a remarkable\n6.013% Mean Average Percentage Error relative to MOVES across extensive tests\nspanning over two million scenarios with diverse trajectories and the factors\nregarding environments and vehicles. NeuralMOVES is only 2.4 MB, largely\ncondensing the original MOVES and the reverse engineered MOVES into a compact\nrepresentation, while maintaining high accuracy. Therefore, NeuralMOVES\nsignificantly enhances accessibility while maintaining the accuracy of MOVES,\nsimplifying CO2 evaluation for transportation analyses and enabling real-time,\nmicroscopic applications across diverse scenarios without reliance on complex\nsoftware or extensive computational resources. Moreover, this paper provides,\nfor the first time, a framework for reverse engineering industrial-grade\nsoftware tailored specifically to transportation scenarios, going beyond MOVES.\nThe surrogate models are available at https://github.com/edgar-rs/neuralMOVES.",
      "tldr_zh": "该研究针对交通部门温室气体排放评估的挑战，提出 NeuralMOVES，一种基于反向工程和神经网络的轻量级替代模型，用于估算车辆 CO2 排放。相比行业标准 MOVES 模型，NeuralMOVES 在超过 200 万种场景测试中实现仅 6.013% 的平均百分比误差，同时体积压缩至 2.4 MB，显著降低计算需求并支持实时微观应用。模型不仅提升了可访问性和效率，还首次提供了一个针对交通场景的反向工程框架，并开源于 https://github.com/edgar-rs/neuralMOVES。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04417v1",
      "published_date": "2025-02-06 14:26:26 UTC",
      "updated_date": "2025-02-06 14:26:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:03:49.035560"
    },
    {
      "arxiv_id": "2502.04098v2",
      "title": "Efficient Few-Shot Continual Learning in Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Aristeidis Panos",
        "Rahaf Aljundi",
        "Daniel Olmeda Reino",
        "Richard E. Turner"
      ],
      "abstract": "Vision-language models (VLMs) excel in tasks such as visual question\nanswering and image captioning. However, VLMs are often limited by their use of\npretrained image encoders, like CLIP, leading to image understanding errors\nthat hinder overall performance. On top of that, real-world applications often\nrequire the model to be continuously adapted as new and often limited data\ncontinuously arrive. To address this, we propose LoRSU (Low-Rank Adaptation\nwith Structured Updates), a robust and computationally efficient method for\nselectively updating image encoders within VLMs. LoRSU introduces structured\nand localized parameter updates, effectively correcting performance on\npreviously error-prone data while preserving the model's general robustness.\nOur approach leverages theoretical insights to identify and update only the\nmost critical parameters, achieving significant resource efficiency.\nSpecifically, we demonstrate that LoRSU reduces computational overhead by over\n25x compared to full VLM updates, without sacrificing performance. Experimental\nresults on VQA tasks in the few-shot continual learning setting, validate\nLoRSU's scalability, efficiency, and effectiveness, making it a compelling\nsolution for image encoder adaptation in resource-constrained environments.",
      "tldr_zh": "该研究针对视觉语言模型（VLMs）在视觉问答（VQA）和图像描述等任务中的图像理解错误问题，提出了一种高效方法LoRSU（Low-Rank Adaptation with Structured Updates），用于在少样本持续学习（Few-Shot Continual Learning）场景下选择性地更新图像编码器，如CLIP。LoRSU通过结构化和本地化的参数更新，仅针对关键参数进行优化，从而修正错误-prone数据并保持模型的整体鲁棒性。实验结果显示，该方法将计算开销减少超过25倍，同时在VQA任务的少样本持续学习设置中实现了出色的可扩展性、效率和性能表现，为资源受限环境下的VLM适应提供了实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04098v2",
      "published_date": "2025-02-06 14:20:55 UTC",
      "updated_date": "2025-02-07 13:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:04:01.047070"
    },
    {
      "arxiv_id": "2502.04095v1",
      "title": "LLMs to Support a Domain Specific Knowledge Assistant",
      "title_zh": "大型语言模型用于支持特定领域的知识助手",
      "authors": [
        "Maria-Flavia Lovin"
      ],
      "abstract": "This work presents a custom approach to developing a domain specific\nknowledge assistant for sustainability reporting using the International\nFinancial Reporting Standards (IFRS). In this domain, there is no publicly\navailable question-answer dataset, which has impeded the development of a\nhigh-quality chatbot to support companies with IFRS reporting. The two key\ncontributions of this project therefore are:\n  (1) A high-quality synthetic question-answer (QA) dataset based on IFRS\nsustainability standards, created using a novel generation and evaluation\npipeline leveraging Large Language Models (LLMs). This comprises 1,063 diverse\nQA pairs that address a wide spectrum of potential user queries in\nsustainability reporting. Various LLM-based techniques are employed to create\nthe dataset, including chain-of-thought reasoning and few-shot prompting. A\ncustom evaluation framework is developed to assess question and answer quality\nacross multiple dimensions, including faithfulness, relevance, and domain\nspecificity. The dataset averages a score range of 8.16 out of 10 on these\nmetrics.\n  (2) Two architectures for question-answering in the sustainability reporting\ndomain - a RAG pipeline and a fully LLM-based pipeline. The architectures are\ndeveloped by experimenting, fine-tuning, and training on the QA dataset. The\nfinal pipelines feature an LLM fine-tuned on domain specific data and an\nindustry classification component to improve the handling of complex queries.\nThe RAG architecture achieves an accuracy of 85.32% on single-industry and\n72.15% on cross-industry multiple-choice questions, outperforming the baseline\napproach by 4.67 and 19.21 percentage points, respectively. The LLM-based\npipeline achieves an accuracy of 93.45% on single-industry and 80.30% on\ncross-industry multiple-choice questions, an improvement of 12.80 and 27.36\npercentage points over the baseline, respectively.",
      "tldr_zh": "本研究开发了一个针对可持续性报告领域的领域特定知识助手，使用 International Financial Reporting Standards (IFRS) 标准作为基础，以解决缺乏公开问答数据集的问题。关键贡献包括：(1) 利用 Large Language Models (LLMs) 创建了一个高质量的合成问答 (QA) 数据集，共 1,063 对问题答案，通过 chain-of-thought reasoning 和 few-shot prompting 等技术生成，并采用自定义评估框架评估其在 faithfulness、relevance 和 domain specificity 等维度的平均得分达 8.16/10。(2) 构建了两种问答架构——RAG pipeline 和 fully LLM-based pipeline，通过在 QA 数据集上实验和微调，加入行业分类组件以处理复杂查询。结果显示，RAG 架构在单行业和跨行业多选题上的准确率分别为 85.32% 和 72.15%，LLM-based pipeline 则达 93.45% 和 80.30%，均显著优于基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04095v1",
      "published_date": "2025-02-06 14:12:41 UTC",
      "updated_date": "2025-02-06 14:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:04:14.174844"
    },
    {
      "arxiv_id": "2502.04416v1",
      "title": "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Zehua Pei",
        "Lancheng Zou",
        "Hui-Ling Zhen",
        "Xianzhi Yu",
        "Wulong Liu",
        "Sinno Jialin Pan",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "abstract": "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https://github.com/JarvisPei/CMoE.",
      "tldr_zh": "这篇论文提出 CMoE 框架，用于从密集模型中快速构建 Mixture-of-Experts (MoE) 架构，以提升 Large Language Models (LLMs) 的推理效率，解决 FFNs 高激活稀疏性带来的开销问题。CMoE 通过基于激活率的神经元分组（将神经元分为共享专家和路由专家）和无需从零训练的可微路由机制，实现高效专家分配和负载均衡，仅需少量数据即可在 5 分钟内从 7B 密集模型中生成可用 MoE 模型。接着，通过轻量级微调，该框架能在 1 小时内恢复高性能，并开源代码以便进一步应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04416v1",
      "published_date": "2025-02-06 14:05:30 UTC",
      "updated_date": "2025-02-06 14:05:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:04:26.032729"
    },
    {
      "arxiv_id": "2502.04083v1",
      "title": "Automatic quantification of breast cancer biomarkers from multiple 18F-FDG PET image segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Tewele W. Tareke",
        "Neree Payan",
        "Alexandre Cochet",
        "Laurent Arnould",
        "Benoit Presles",
        "Jean-Marc Vrigneaud",
        "Fabrice Meriaudeau",
        "Alain Lalande"
      ],
      "abstract": "Neoadjuvant chemotherapy (NAC) has become a standard clinical practice for\ntumor downsizing in breast cancer with 18F-FDG Positron Emission Tomography\n(PET). Our work aims to leverage PET imaging for the segmentation of breast\nlesions. The focus is on developing an automated system that accurately\nsegments primary tumor regions and extracts key biomarkers from these areas to\nprovide insights into the evolution of breast cancer following the first course\nof NAC. 243 baseline 18F-FDG PET scans (PET_Bl) and 180 follow-up 18F-FDG PET\nscans (PET_Fu) were acquired before and after the first course of NAC,\nrespectively. Firstly, a deep learning-based breast tumor segmentation method\nwas developed. The optimal baseline model (model trained on baseline exams) was\nfine-tuned on 15 follow-up exams and adapted using active learning to segment\ntumor areas in PET_Fu. The pipeline computes biomarkers such as maximum\nstandardized uptake value (SUVmax), metabolic tumor volume (MTV), and total\nlesion glycolysis (TLG) to evaluate tumor evolution between PET_Fu and PET_Bl.\nQuality control measures were employed to exclude aberrant outliers. The nnUNet\ndeep learning model outperformed in tumor segmentation on PET_Bl, achieved a\nDice similarity coefficient (DSC) of 0.89 and a Hausdorff distance (HD) of 3.52\nmm. After fine-tuning, the model demonstrated a DSC of 0.78 and a HD of 4.95 mm\non PET_Fu exams. Biomarkers analysis revealed very strong correlations whatever\nthe biomarker between manually segmented and automatically predicted regions.\nThe significant average decrease of SUVmax, MTV and TLG were 5.22, 11.79 cm3\nand 19.23 cm3, respectively. The presented approach demonstrates an automated\nsystem for breast tumor segmentation from 18F-FDG PET. Thanks to the extracted\nbiomarkers, our method enables the automatic assessment of cancer progression.",
      "tldr_zh": "本研究开发了一个自动化系统，利用 18F-FDG PET 成像来分割乳腺癌肿瘤，并量化关键生物标记物，以评估新辅助化疗 (NAC) 第一疗程后的肿瘤演变。系统基于 nnUNet 深度学习模型，首先在 243 个基线 PET 扫描 (PET_Bl) 上训练，达到 Dice similarity coefficient (DSC) 为 0.89 和 Hausdorff distance (HD) 为 3.52 mm 的表现，然后通过微调和主动学习适应 180 个随访 PET 扫描 (PET_Fu)，实现 DSC 为 0.78 和 HD 为 4.95 mm。结果显示，自动分割的生物标记物（如 maximum standardized uptake value (SUVmax)、metabolic tumor volume (MTV) 和 total lesion glycolysis (TLG)）与手动结果高度相关，并分别平均下降 5.22、11.79 cm³ 和 19.23 cm³，表明肿瘤缩小。该方法为乳腺癌进展的自动评估提供了高效工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submit soon to EJNMMI Research",
      "pdf_url": "http://arxiv.org/pdf/2502.04083v1",
      "published_date": "2025-02-06 13:51:28 UTC",
      "updated_date": "2025-02-06 13:51:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:04:37.864799"
    },
    {
      "arxiv_id": "2502.04415v1",
      "title": "TerraQ: Spatiotemporal Question-Answering on Satellite Image Archives",
      "title_zh": "翻译失败",
      "authors": [
        "Sergios-Anestis Kefalidis",
        "Konstantinos Plas",
        "Manolis Koubarakis"
      ],
      "abstract": "TerraQ is a spatiotemporal question-answering engine for satellite image\narchives. It is a natural language processing system that is built to process\nrequests for satellite images satisfying certain criteria. The requests can\nrefer to image metadata and entities from a specialized knowledge base (e.g.,\nthe Emilia-Romagna region). With it, users can make requests like \"Give me a\nhundred images of rivers near ports in France, with less than 20% snow coverage\nand more than 10% cloud coverage\", thus making Earth Observation data more\neasily accessible, in-line with the current landscape of digital assistants.",
      "tldr_zh": "该论文介绍了TerraQ，一种针对卫星图像档案的时空问答引擎，利用自然语言处理技术处理用户查询。TerraQ允许用户通过自然语言指定图像标准，例如引用图像元数据和知识库实体（如Emilia-Romagna地区），并生成符合条件的图像请求，例如“提供法国港口附近河流的100张图像，雪覆盖率低于20%且云覆盖率高于10%”。这种系统使地球观测数据更易访问，类似于数字助理的交互方式，从而提升了数据利用效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04415v1",
      "published_date": "2025-02-06 13:43:17 UTC",
      "updated_date": "2025-02-06 13:43:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:04:47.384057"
    },
    {
      "arxiv_id": "2502.04066v2",
      "title": "SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals",
      "title_zh": "SMI：一种基于信息理论的指标，用于仅从预训练信号预测模型知识",
      "authors": [
        "Changhao Jiang",
        "Ming Zhang",
        "Junjie Ye",
        "Xiaoran Fan",
        "Yifei Cao",
        "Jiajun Sun",
        "Zhiheng Xi",
        "Shihan Dou",
        "Yi Dong",
        "Yujiong Shen",
        "Jingqi Tong",
        "Zhen Wang",
        "Tao Liang",
        "Zhihui Fei",
        "Mingyang Wan",
        "Guojun Ma",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang"
      ],
      "abstract": "The GPT-4 technical report highlights the possibility of predicting model\nperformance on downstream tasks using only pre-training signals, though\ndetailed methodologies are absent. Such predictive capabilities are essential\nfor resource-efficient pre-training and the construction of task-aligned\ndatasets. In this paper, we aim to predict performance in closed-book question\nanswering (QA), a vital downstream task indicative of a model's internal\nknowledge. We address three primary challenges: (1) limited access to and\nunderstanding of pre-training corpora, (2) limitations of current evaluation\nmethods for pre-trained models, and (3) limitations of frequency-based metrics\nin predicting model performance. In response to these challenges, we conduct\nlarge-scale retrieval and semantic analysis across the pre-training corpora of\n21 publicly available and 3 custom-trained large language models. Subsequently,\nwe develop a multi-template QA evaluation framework incorporating paraphrased\nquestion variants. Building on these foundations, we propose Size-dependent\nMutual Information (SMI), an information-theoretic metric that linearly\ncorrelates pre-training data characteristics, model size, and QA accuracy,\nwithout requiring any additional training. The experimental results demonstrate\nthat SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on\nmodels with over one billion parameters. Theoretical analysis further reveals\nthe marginal benefits of scaling model size and optimizing data, indicating\nthat the upper limit of specific QA task accuracy is approximately 80%. Our\nproject is available at https://github.com/yuhui1038/SMI.",
      "tldr_zh": "本研究提出了一种信息论指标 Size-dependent Mutual Information (SMI)，旨在仅基于预训练信号预测大型语言模型在闭卷问答(QA)任务上的性能，从而提升预训练资源的效率和任务对齐数据集的构建。论文通过对21个公开模型和3个自定义模型的预训练语料进行大规模检索和语义分析，并开发多模板QA评估框架来应对预训练语料访问限制和现有评估方法的局限性。实验结果显示，SMI优于基于共现的基线，在参数超过10亿的模型上达到R² > 0.75；理论分析进一步揭示了模型大小扩展和数据优化的边际收益，并估算特定QA任务准确率的上限约为80%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04066v2",
      "published_date": "2025-02-06 13:23:53 UTC",
      "updated_date": "2025-05-13 14:19:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:05:01.681112"
    },
    {
      "arxiv_id": "2502.04058v1",
      "title": "Strategic Learning with Local Explanations as Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Kiet Q. H. Vo",
        "Siu Lun Chau",
        "Masahiro Kato",
        "Yixin Wang",
        "Krikamol Muandet"
      ],
      "abstract": "We investigate algorithmic decision problems where agents can respond\nstrategically to the decision maker's (DM) models. The demand for clear and\nactionable explanations from DMs to (potentially strategic) agents continues to\nrise. While prior work often treats explanations as full model disclosures,\nexplanations in practice might convey only partial information, which can lead\nto misinterpretations and harmful responses. When full disclosure of the\npredictive model is neither feasible nor desirable, a key open question is how\nDMs can use explanations to maximise their utility without compromising agent\nwelfare. In this work, we explore well-known local and global explanation\nmethods, and establish a necessary condition to prevent explanations from\nmisleading agents into self-harming actions. Moreover, with conditional\nhomogeneity, we establish that action recommendation (AR)-based explanations\nare sufficient for non-harmful responses, akin to the revelation principle in\ninformation design. To operationalise AR-based explanations, we propose a\nsimple algorithm to jointly optimise the predictive model and AR policy to\nbalance DM outcomes with agent welfare. Our empirical results demonstrate the\nbenefits of this approach as a more refined strategy for safe and effective\npartial model disclosure in algorithmic decision-making.",
      "tldr_zh": "本文探讨了代理对决策者（DM）模型的战略响应问题，重点在于如何使用局部解释避免误导代理导致有害行为，同时最大化DM效用而不损害代理福利。研究建立了防止解释误导的必要条件，并在条件齐次性下证明了基于行动推荐（AR）的解释足以实现非有害响应，类似于信息设计的揭示原理（revelation principle）。作者提出一个简单算法，联合优化预测模型和AR政策，以平衡DM结果和代理福利；实证结果表明，这种方法在算法决策中提供了更安全有效的部分模型披露策略。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04058v1",
      "published_date": "2025-02-06 13:17:24 UTC",
      "updated_date": "2025-02-06 13:17:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:05:12.606545"
    },
    {
      "arxiv_id": "2502.04043v1",
      "title": "Probe-Free Low-Rank Activation Intervention",
      "title_zh": "无需探",
      "authors": [
        "Chonghe Jiang",
        "Bao Nguyen",
        "Anthony Man-Cho So",
        "Viet Anh Nguyen"
      ],
      "abstract": "Language models (LMs) can produce texts that appear accurate and coherent but\ncontain untruthful or toxic content. Inference-time interventions that edit the\nhidden activations have shown promising results in steering the LMs towards\ndesirable generations. Existing activation intervention methods often comprise\nan activation probe to detect undesirable generation, triggering the activation\nmodification to steer subsequent generation. This paper proposes a probe-free\nintervention method FLORAIN for all attention heads in a specific activation\nlayer. It eliminates the need to train classifiers for probing purposes. The\nintervention function is parametrized by a sample-wise nonlinear low-rank\nmapping, which is trained by minimizing the distance between the modified\nactivations and their projection onto the manifold of desirable content. Under\nspecific constructions of the manifold and projection distance, we show that\nthe intervention strategy can be computed efficiently by solving a smooth\noptimization problem. The empirical results, benchmarked on multiple base\nmodels, demonstrate that FLORAIN consistently outperforms several baseline\nmethods in enhancing model truthfulness and quality across generation and\nmultiple-choice tasks.",
      "tldr_zh": "该论文提出了一种无探测器的激活干预方法Probe-Free Low-Rank Activation Intervention（FLORAIN），旨在解决语言模型(LMs)生成不真实或有毒内容的问题。该方法针对特定激活层的全部注意力头，使用样本-wise 非线性low-rank mapping参数化干预函数，通过最小化修改激活与理想内容流形投影的距离来训练干预策略，从而高效地计算优化问题。实验结果显示，FLORAIN在多个基准模型上显著优于基线方法，提高了模型在生成和多项选择任务中的真实性和整体质量。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04043v1",
      "published_date": "2025-02-06 13:03:05 UTC",
      "updated_date": "2025-02-06 13:03:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:05:24.987780"
    },
    {
      "arxiv_id": "2502.04040v1",
      "title": "Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Haoyu Wang",
        "Zeyu Qin",
        "Li Shen",
        "Xueqian Wang",
        "Minhao Cheng",
        "Dacheng Tao"
      ],
      "abstract": "Training safe LLMs is one of the most critical research challenge. However,\nthe commonly used method, Refusal Training (RT), struggles to generalize\nagainst various OOD jailbreaking attacks. Many safety training methods have\nbeen proposed to address this issue. While they offer valuable insights, we aim\nto complement this line of research by investigating whether OOD attacks truly\nexceed the capability of RT model. Conducting evaluation with BoN, we observe\nsignificant improvements on generalization as N increases. This underscores\nthat the model possesses sufficient safety-related latent knowledge, but RT\nfails to consistently elicit this knowledge when addressing OOD attacks.\nFurther analysis based on domain adaptation reveals that training with direct\nrefusal causes model to rely on superficial shortcuts, resulting in learning of\nnon-robust representation mappings. Based on our findings, we propose training\nmodel to perform safety reasoning for each query. Reasoning supervision\nencourages model to perform more computations, explicitly eliciting and using\nlatent knowledge through reasoning. To achieve this, we synthesize reasoning\nsupervision based on pre-guidelines, training the model to reason in alignment\nwith them, thereby effectively eliciting and utilizing latent knowledge from\ndiverse perspectives. Extensive experiments show that our method significantly\nimproves generalization performance against OOD attacks.",
      "tldr_zh": "该研究针对训练安全的大型语言模型（LLMs）面临的挑战，指出Refusal Training (RT)方法在应对OOD jailbreaking attacks时泛化能力不足，因为它未能有效调用模型的潜在安全知识。实验分析发现，RT导致模型依赖浅层捷径，形成非稳健的表示映射，从而无法处理OOD攻击。论文提出一种新方法，通过基于预先指导方针的推理监督，训练模型进行安全推理，以显式调用和利用潜在知识。结果显示，该方法显著提升了模型对OOD attacks的泛化性能，为提升LLMs的安全性提供了有效途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2502.04040v1",
      "published_date": "2025-02-06 13:01:44 UTC",
      "updated_date": "2025-02-06 13:01:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:05:36.199979"
    },
    {
      "arxiv_id": "2502.04034v1",
      "title": "Generalize Drug Response Prediction by Latent Independent Projection for Asymmetric Constrained Domain Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Ran Song",
        "Yinpu Bai",
        "Hui Liu"
      ],
      "abstract": "The accurate prediction of drug responses remains a formidable challenge,\nparticularly at the single-cell level and in clinical treatment contexts. Some\nstudies employ transfer learning techniques to predict drug responses in\nindividual cells and patients, but they require access to target-domain data\nduring training, which is often unavailable or only obtainable in future. In\nthis study, we propose a novel domain generalization framework, termed\npanCancerDR, to address this challenge. We conceptualize each cancer type as a\ndistinct source domain, with its cell lines serving as domain-specific samples.\nOur primary objective is to extract domain-invariant features from the\nexpression profiles of cell lines across diverse cancer types, thereby\ngeneralize the predictive capacity to out-of-distribution samples. To enhance\nrobustness, we introduce a latent independence projection (LIP) module that\nencourages the encoder to extract informative yet non-redundant features. Also,\nwe propose an asymmetric adaptive clustering constraint, which clusters\ndrug-sensitive samples into a compact group while drives resistant samples\ndispersed across separate clusters in the latent space. Our empirical\nexperiments demonstrate that panCancerDR effectively learns task-relevant\nfeatures from diverse source domains, and achieves accurate predictions of drug\nresponse for unseen cancer type during training. Furthermore, when evaluated on\nsingle-cell and patient-level prediction tasks, our model-trained solely on in\nvitro cell line data without access to target-domain information-consistently\noutperforms and matched current state-of-the-art methods. These findings\nhighlights the potential of our method for real-world clinical applications.",
      "tldr_zh": "这篇论文提出了一种名为 panCancerDR 的域泛化框架，用于解决药物反应预测的挑战，特别是针对单细胞和临床场景中缺乏目标域数据的问题。框架将每种癌症类型视为独立源域，从细胞系表达谱中提取域不变特征，以提升对分布外样本的泛化能力。核心方法包括 Latent Independent Projection (LIP) 模块，用于提取信息丰富且非冗余的特征，以及 Asymmetric Adaptive Clustering Constraint，将药物敏感样本聚类成紧凑组，同时将耐药样本分散到不同聚类。实验结果表明，panCancerDR 仅基于 in vitro 细胞系数据训练，就能准确预测未见癌症类型的药物反应，并在单细胞和患者级任务上优于现有最先进方法，展示了其在临床应用的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04034v1",
      "published_date": "2025-02-06 12:53:45 UTC",
      "updated_date": "2025-02-06 12:53:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:05:49.176541"
    },
    {
      "arxiv_id": "2502.04030v1",
      "title": "Fine, I'll Merge It Myself: A Multi-Fidelity Framework for Automated Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Guinan Su",
        "Jonas Geiping"
      ],
      "abstract": "Reasoning capabilities represent a critical frontier for large language\nmodels (LLMs), but developing them requires extensive proprietary datasets and\ncomputational resources. One way to efficiently supplement capabilities with is\nby model merging, which offers a promising alternative by combining multiple\nmodels without retraining. However, current merging approaches rely on\nmanually-designed strategies for merging hyperparameters, limiting the\nexploration of potential model combinations and requiring significant human\neffort. We propose an Automated Model Merging Framework that enables\nfine-grained exploration of merging strategies while reducing costs through\nmulti-fidelity approximations. We support both single and multi-objective\noptimization and introduce two novel search spaces: layerwise fusion (LFS) and\ndepth-wise integration (DIS). Evaluating across a number of benchmarks, we find\nthat the search autonomously finds 1) Merges that further boost\nsingle-objective performance, even on tasks the model has already been\nfinetuned on, and 2) Merges that optimize multi-objective frontiers across\ntasks. Effective merges are found with limited compute, e.g. within less than\n500 search steps.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 的推理能力开发问题，提出了一种 Automated Model Merging Framework，通过模型合并来高效结合多个模型，而无需重新训练，以减少对手动超参数策略的依赖。框架利用多保真度近似支持细粒度探索，并引入两个新搜索空间：layerwise fusion (LFS) 和 depth-wise integration (DIS)，实现单目标和多目标优化。实验结果显示，该框架能在少于500个搜索步骤内自动发现性能提升的合并策略，包括在已微调任务上的进一步改进，以及跨任务的多目标优化前沿。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04030v1",
      "published_date": "2025-02-06 12:47:25 UTC",
      "updated_date": "2025-02-06 12:47:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:06:01.189916"
    },
    {
      "arxiv_id": "2502.04413v1",
      "title": "MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot",
      "title_zh": "翻译失败",
      "authors": [
        "Xuejiao Zhao",
        "Siyan Liu",
        "Su-Yin Yang",
        "Chunyan Miao"
      ],
      "abstract": "Retrieval-augmented generation (RAG) is a well-suited technique for\nretrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a\nkey module of the healthcare copilot, helping reduce misdiagnosis for\nhealthcare practitioners and patients. However, the diagnostic accuracy and\nspecificity of existing heuristic-based RAG models used in the medical domain\nare inadequate, particularly for diseases with similar manifestations. This\npaper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited\nreasoning for the medical domain that retrieves diagnosis and treatment\nrecommendations based on manifestations. MedRAG systematically constructs a\ncomprehensive four-tier hierarchical diagnostic KG encompassing critical\ndiagnostic differences of various diseases. These differences are dynamically\nintegrated with similar EHRs retrieved from an EHR database, and reasoned\nwithin a large language model. This process enables more accurate and specific\ndecision support, while also proactively providing follow-up questions to\nenhance personalized medical decision-making. MedRAG is evaluated on both a\npublic dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD)\ncollected from Tan Tock Seng Hospital, and its performance is compared against\nvarious existing RAG methods. Experimental results show that, leveraging the\ninformation integration and relational abilities of the KG, our MedRAG provides\nmore specific diagnostic insights and outperforms state-of-the-art models in\nreducing misdiagnosis rates. Our code will be available at\nhttps://github.com/SNOWTEAM2023/MedRAG",
      "tldr_zh": "本论文提出 MedRAG，一种通过知识图谱(KG)引发的推理增强检索生成(RAG)模型，旨在解决医疗领域中现有 RAG 模型在诊断准确性和特异性上的不足，特别是针对症状相似的疾病。MedRAG 构建了一个全面的四层层次诊断 KG，并动态整合 KG 中的关键诊断差异与从 Electronic Health Records (EHR) 数据库检索到的类似记录，在大型语言模型中进行推理，从而提供更精确的诊断、治疗推荐和后续问题。实验在公共数据集 DDXPlus 和私人数据集 CPDD 上显示，MedRAG 利用 KG 的信息整合和关系能力，显著降低了误诊率，并优于现有 RAG 方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04413v1",
      "published_date": "2025-02-06 12:27:35 UTC",
      "updated_date": "2025-02-06 12:27:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:06:13.293102"
    },
    {
      "arxiv_id": "2502.04412v1",
      "title": "Decoder-Only LLMs are Better Controllers for Diffusion Models",
      "title_zh": "仅解码器 LLMs 是扩散模型更好的控制器",
      "authors": [
        "Ziyi Dong",
        "Yao Xiao",
        "Pengxu Wei",
        "Liang Lin"
      ],
      "abstract": "Groundbreaking advancements in text-to-image generation have recently been\nachieved with the emergence of diffusion models. These models exhibit a\nremarkable ability to generate highly artistic and intricately detailed images\nbased on textual prompts. However, obtaining desired generation outcomes often\nnecessitates repetitive trials of manipulating text prompts just like casting\nspells on a magic mirror, and the reason behind that is the limited capability\nof semantic understanding inherent in current image generation models.\nSpecifically, existing diffusion models encode the text prompt input with a\npre-trained encoder structure, which is usually trained on a limited number of\nimage-caption pairs. The state-of-the-art large language models (LLMs) based on\nthe decoder-only structure have shown a powerful semantic understanding\ncapability as their architectures are more suitable for training on very\nlarge-scale unlabeled data. In this work, we propose to enhance text-to-image\ndiffusion models by borrowing the strength of semantic understanding from large\nlanguage models, and devise a simple yet effective adapter to allow the\ndiffusion models to be compatible with the decoder-only structure. Meanwhile,\nwe also provide a supporting theoretical analysis with various architectures\n(e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive\nempirical evaluations to verify its effectiveness. The experimental results\nshow that the enhanced models with our adapter module are superior to the\nstat-of-the-art models in terms of text-to-image generation quality and\nreliability.",
      "tldr_zh": "本文研究发现，现有的文本到图像扩散模型（diffusion models）由于语义理解能力有限，常需反复调整文本提示才能获得理想结果，而这源于其依赖于预训练编码器（encoder）处理有限的数据。作者提出一种简单有效的适配器，将基于 decoder-only 结构的LLMs（大型语言模型）整合进扩散模型，作为更好的控制器，以借用LLMs的强大语义理解能力。实验结果显示，该增强模型在文本到图像生成的质量和可靠性上优于现有最先进模型，并通过理论分析验证了decoder-only架构的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04412v1",
      "published_date": "2025-02-06 12:17:35 UTC",
      "updated_date": "2025-02-06 12:17:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:06:25.360037"
    },
    {
      "arxiv_id": "2502.04008v1",
      "title": "Automating a Complete Software Test Process Using LLMs: An Automotive Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Shuai Wang",
        "Yinan Yu",
        "Robert Feldt",
        "Dhasarathy Parthasarathy"
      ],
      "abstract": "Vehicle API testing verifies whether the interactions between a vehicle's\ninternal systems and external applications meet expectations, ensuring that\nusers can access and control various vehicle functions and data. However, this\ntask is inherently complex, requiring the alignment and coordination of API\nsystems, communication protocols, and even vehicle simulation systems to\ndevelop valid test cases. In practical industrial scenarios, inconsistencies,\nambiguities, and interdependencies across various documents and system\nspecifications pose significant challenges. This paper presents a system\ndesigned for the automated testing of in-vehicle APIs. By clearly defining and\nsegmenting the testing process, we enable Large Language Models (LLMs) to focus\non specific tasks, ensuring a stable and controlled testing workflow.\nExperiments conducted on over 100 APIs demonstrate that our system effectively\nautomates vehicle API testing. The results also confirm that LLMs can\nefficiently handle mundane tasks requiring human judgment, making them suitable\nfor complete automation in similar industrial contexts.",
      "tldr_zh": "本研究针对车辆API测试的复杂性（如系统协调和文档不一致问题），提出了一种使用Large Language Models (LLMs)实现完整软件测试过程自动化的系统。该系统通过明确定义和细分测试流程，让LLMs专注于特定任务，从而确保测试工作流的稳定性和可控性。在一个汽车行业案例研究中，对超过100个API进行实验，证明该系统能有效自动化测试过程，并显示LLMs能够高效处理需要人类判断的常规任务，为类似工业场景的完全自动化提供可行性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by International Conference on Software Engineering (ICSE)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2502.04008v1",
      "published_date": "2025-02-06 12:10:01 UTC",
      "updated_date": "2025-02-06 12:10:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:06:35.079853"
    },
    {
      "arxiv_id": "2502.03998v1",
      "title": "Online Learning of Counter Categories and Ratings in PvP Games",
      "title_zh": "PvP 游戏中对抗类别和评级的在线学习",
      "authors": [
        "Chiu-Chou Lin",
        "I-Chen Wu"
      ],
      "abstract": "In competitive games, strength ratings like Elo are widely used to quantify\nplayer skill and support matchmaking by accounting for skill disparities better\nthan simple win rate statistics. However, scalar ratings cannot handle complex\nintransitive relationships, such as counter strategies seen in\nRock-Paper-Scissors. To address this, recent work introduced Neural Rating\nTable and Neural Counter Table, which combine scalar ratings with discrete\ncounter categories to model intransitivity. While effective, these methods rely\non neural network training and cannot perform real-time updates. In this paper,\nwe propose an online update algorithm that extends Elo principles to\nincorporate real-time learning of counter categories. Our method dynamically\nadjusts both ratings and counter relationships after each match, preserving the\nexplainability of scalar ratings while addressing intransitivity. Experiments\non zero-sum competitive games demonstrate its practicality, particularly in\nscenarios without complex team compositions.",
      "tldr_zh": "本研究针对 PvP 游戏中传统标量评分如 Elo 的局限性，提出了一种在线更新算法，以处理复杂的非传递关系(intransitivity)，如对抗策略。算法扩展 Elo 原则，通过实时学习 counter categories 和 ratings，在每场比赛后动态调整评分和对抗关系，同时保留标量评分的 explainability。实验在零和竞技游戏中验证了该方法的实用性，尤其适用于无复杂团队组成的场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03998v1",
      "published_date": "2025-02-06 11:57:18 UTC",
      "updated_date": "2025-02-06 11:57:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:06:47.953895"
    },
    {
      "arxiv_id": "2502.03992v1",
      "title": "Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Longquan Jiang",
        "Junbo Huang",
        "Cedric Möller",
        "Ricardo Usbeck"
      ],
      "abstract": "Most existing Knowledge Graph Question Answering (KGQA) approaches are\ndesigned for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the\nheterogeneity of the underlying graph schema, topology and assertions, most\nKGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without\nresource-intensive training data. We present OntoSCPrompt, a novel Large\nLanguage Model (LLM)-based KGQA approach with a two-stage architecture that\nseparates semantic parsing from KG-dependent interactions. OntoSCPrompt first\ngenerates a SPARQL query structure (including SPARQL keywords such as SELECT,\nASK, WHERE and placeholders for missing tokens) and then fills them with\nKG-specific information. To enhance the understanding of the underlying KG, we\npresent an ontology-guided, hybrid prompt learning strategy that integrates KG\nontology into the learning process of hybrid prompts (e.g., discrete and\ncontinuous vectors). We also present several task-specific decoding strategies\nto ensure the correctness and executability of generated SPARQL queries in both\nstages. Experimental results demonstrate that OntoSCPrompt performs as well as\nSOTA approaches without retraining on a number of KGQA datasets such as CWQ,\nWebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well\nto unseen domain-specific KGs like DBLP-QuAD and CoyPu KG Code:\n\\href{https://github.com/LongquanJiang/OntoSCPrompt}{https://github.com/LongquanJiang/OntoSCPrompt}",
      "tldr_zh": "本文提出 OntoSCPrompt，一种基于 Large Language Model (LLM) 的 Knowledge Graph Question Answering (KGQA) 方法，通过两阶段架构（先生成 SPARQL 查询结构，再填充 KG 特定信息）来实现对异质知识图谱的泛化，避免了资源密集型训练。方法引入 ontology-guided, hybrid prompt learning 策略，将 KG 本体整合到混合提示（包括 discrete 和 continuous vectors）的学习过程中，并采用任务特定解码策略确保 SPARQL 查询的正确性和可执行性。实验结果显示，OntoSCPrompt 在 CWQ、WebQSP 和 LC-QuAD 1.0 等数据集上与 SOTA 方法相当，且能高效泛化到未见领域的知识图谱如 DBLP-QuAD 和 CoyPu KG。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted By ICSC 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.03992v1",
      "published_date": "2025-02-06 11:47:58 UTC",
      "updated_date": "2025-02-06 11:47:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:01.330006"
    },
    {
      "arxiv_id": "2502.03984v1",
      "title": "PGB: One-Shot Pruning for BERT via Weight Grouping and Permutation",
      "title_zh": "翻译失败",
      "authors": [
        "Hyemin Lim",
        "Jaeyeon Lee",
        "Dong-Wan Choi"
      ],
      "abstract": "Large pretrained language models such as BERT suffer from slow inference and\nhigh memory usage, due to their huge size. Recent approaches to compressing\nBERT rely on iterative pruning and knowledge distillation, which, however, are\noften too complicated and computationally intensive. This paper proposes a\nnovel semi-structured one-shot pruning method for BERT, called\n$\\textit{Permutation and Grouping for BERT}$ (PGB), which achieves high\ncompression efficiency and sparsity while preserving accuracy. To this end, PGB\nidentifies important groups of individual weights by permutation and prunes all\nother weights as a structure in both multi-head attention and feed-forward\nlayers. Furthermore, if no important group is formed in a particular layer, PGB\ndrops the entire layer to produce an even more compact model. Our experimental\nresults on BERT$_{\\text{BASE}}$ demonstrate that PGB outperforms the\nstate-of-the-art structured pruning methods in terms of computational cost and\naccuracy preservation.",
      "tldr_zh": "该研究提出 PGB，一种针对 BERT 的单次修剪(One-Shot Pruning)方法，通过权重分组(Weight Grouping)和排列(Permutation)来高效压缩模型，解决其推理速度慢和高内存占用问题。PGB 在 multi-head attention 和 feed-forward layers 中识别重要权重组，并修剪其他权重；如果某个层没有重要组，则直接删除整个层，从而实现高稀疏性和紧凑性。实验结果显示，PGB 在 BERT_BASE 上在计算成本和准确性保留方面优于现有结构化修剪方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03984v1",
      "published_date": "2025-02-06 11:34:41 UTC",
      "updated_date": "2025-02-06 11:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:13.085024"
    },
    {
      "arxiv_id": "2502.04411v2",
      "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
      "title_zh": "Mediator: 内存高效的 LLM 合并，减少参数冲突并基于不确定性路由",
      "authors": [
        "Kunfeng Lai",
        "Zhenheng Tang",
        "Xinglin Pan",
        "Peijie Dong",
        "Xiang Liu",
        "Haolan Chen",
        "Li Shen",
        "Bo Li",
        "Xiaowen Chu"
      ],
      "abstract": "Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.",
      "tldr_zh": "该论文提出Mediator方法，用于高效合并Large Language Models (LLMs)，通过识别不同层级的parameter conflicts来优化模型融合：对于冲突较小的层进行平均融合，对于冲突较大的层采用新型任务级expert routing，并将微调专家解耦为一个dense expert和多个sparse experts，以减少存储成本。基于输入数据的任务uncertainty，方法动态选择和合并合适的专家。实验在LLaMA和Qwen模型上显示，Mediator在真实世界推理任务中显著提升性能，同时降低了系统开销。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T50"
      ],
      "primary_category": "cs.LG",
      "comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2502.04411v2",
      "published_date": "2025-02-06 11:26:30 UTC",
      "updated_date": "2025-02-11 12:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:24.120824"
    },
    {
      "arxiv_id": "2502.03979v2",
      "title": "Towards Unified Music Emotion Recognition across Dimensional and Categorical Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jaeyong Kang",
        "Dorien Herremans"
      ],
      "abstract": "One of the most significant challenges in Music Emotion Recognition (MER)\ncomes from the fact that emotion labels can be heterogeneous across datasets\nwith regard to the emotion representation, including categorical (e.g., happy,\nsad) versus dimensional labels (e.g., valence-arousal). In this paper, we\npresent a unified multitask learning framework that combines these two types of\nlabels and is thus able to be trained on multiple datasets. This framework uses\nan effective input representation that combines musical features (i.e., key and\nchords) and MERT embeddings. Moreover, knowledge distillation is employed to\ntransfer the knowledge of teacher models trained on individual datasets to a\nstudent model, enhancing its ability to generalize across multiple tasks. To\nvalidate our proposed framework, we conducted extensive experiments on a\nvariety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic.\nAccording to our experimental results, the inclusion of musical features,\nmultitask learning, and knowledge distillation significantly enhances\nperformance. In particular, our model outperforms the state-of-the-art models,\nincluding the best-performing model from the MediaEval 2021 competition on the\nMTG-Jamendo dataset. Our work makes a significant contribution to MER by\nallowing the combination of categorical and dimensional emotion labels in one\nunified framework, thus enabling training across datasets.",
      "tldr_zh": "本研究针对音乐情感识别（MER）中情感标签的异质性（如分类标签happy、sad和维度标签valence-arousal）提出一个统一的multitask learning框架，该框架能整合这两种标签，并在多个数据集上进行训练。方法包括使用结合音乐特征（如key and chords）和MERT embeddings的输入表示，以及通过knowledge distillation从单个数据集训练的教师模型向学生模型转移知识。实验在MTG-Jamendo、DEAM、PMEmo和EmoMusic等数据集上验证，结果显示该框架显著提升性能，并在MTG-Jamendo上超越了最先进模型，包括MediaEval 2021比赛的获胜者，从而实现了跨数据集的MER训练。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03979v2",
      "published_date": "2025-02-06 11:20:22 UTC",
      "updated_date": "2025-04-11 12:58:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:36.546269"
    },
    {
      "arxiv_id": "2503.05702v1",
      "title": "A Comprehensive Survey of Fuzzy Implication Functions",
      "title_zh": "模糊蕴涵函数的全面综述",
      "authors": [
        "Raquel Fernandez-Peralta"
      ],
      "abstract": "Fuzzy implication functions are a key area of study in fuzzy logic, extending\nthe classical logical conditional to handle truth degrees in the interval\n$[0,1]$. While existing literature often focuses on a limited number of\nfamilies, in the last ten years many new families have been introduced, each\ndefined by specific construction methods and having different key properties.\nThis survey aims to provide a comprehensive and structured overview of the\ndiverse families of fuzzy implication functions, emphasizing their motivations,\nproperties, and potential applications. By organizing the information\nschematically, this document serves as a valuable resource for both theoretical\nresearchers seeking to avoid redundancy and practitioners looking to select\nappropriate operators for specific applications.",
      "tldr_zh": "本调查对Fuzzy Implication Functions进行了全面回顾，这些函数是模糊逻辑中的核心研究领域，用于扩展经典逻辑条件以处理[0,1]区间内的真值度。论文系统整理了过去十年中新出现的多种函数家族，强调了它们的构建方法、关键属性以及潜在应用动机。凭借结构化的图表组织，该资源有助于理论研究者避免重复工作，并为从业者提供指导，以选择适合特定应用的运算符。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05702v1",
      "published_date": "2025-02-06 11:09:24 UTC",
      "updated_date": "2025-02-06 11:09:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:48.248290"
    },
    {
      "arxiv_id": "2502.03966v3",
      "title": "MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation",
      "title_zh": "MultiFloodSynth：多标注洪水合成数据集生成",
      "authors": [
        "YoonJe Kang",
        "Yonghoon Jung",
        "Wonseop Shin",
        "Bumsoo Kim",
        "Sanghyun Seo"
      ],
      "abstract": "In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.",
      "tldr_zh": "本研究提出MultiFloodSynth框架，用于生成多注释的洪水合成数据集，以支持洪水灾害检测系统。该框架将现实世界的属性融入虚拟环境，通过模拟洪水情景和利用图像到3D以及urban city synthesis生成模型，避免了手工制作导致的数据偏差，并确保了高保真度和效率。MultiFloodSynth数据集包含5个级别和丰富的注释类型，如normal map、segmentation以及3D bounding box，适用于多种下游任务。实验结果显示，该数据集显著提升了洪水灾害检测的性能，其真实性与真实数据集相当。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data",
      "pdf_url": "http://arxiv.org/pdf/2502.03966v3",
      "published_date": "2025-02-06 10:59:44 UTC",
      "updated_date": "2025-02-13 08:54:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:07:59.379975"
    },
    {
      "arxiv_id": "2502.07806v1",
      "title": "Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Rath Minati",
        "Date Hema"
      ],
      "abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape\nof financial risk analysis presents a promising avenue for innovation. This\nstudy introduces a framework for credit risk assessment in the banking sector,\ncombining quantum deep learning techniques with adaptive modeling for Row-Type\nDependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed\napproach tailors predictive models to different loan categories, aiming to\nenhance the accuracy and efficiency of credit risk evaluation. While this work\nexplores the potential of integrating quantum methods with classical deep\nlearning for risk assessment, it focuses on the feasibility and performance of\nthis hybrid framework rather than claiming transformative industry-wide\nimpacts. The findings offer insights into how quantum techniques can complement\ntraditional financial analysis, paving the way for further advancements in\npredictive modeling for credit risk.",
      "tldr_zh": "这篇论文提出了一种新型信用风险评估框架，使用混合量子-经典深度神经网络（hybrid Quantum-Classical Deep Neural Network）结合 Row-Type Dependent Predictive Analysis (RTDPA)，针对银行部门的贷款类别进行自适应预测建模，以提升评估的准确性和效率。RTDPA 方法允许模型针对不同贷款类型（如行类型依赖）定制化处理，从而更好地整合量子深度学习 (QDL) 技术与传统方法。研究结果显示，这种混合框架在可行性和性能上表现出色，为量子技术补充金融分析提供了新见解，并为未来信用风险预测建模铺平道路。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.CP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.07806v1",
      "published_date": "2025-02-06 10:57:18 UTC",
      "updated_date": "2025-02-06 10:57:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:08:12.443346"
    },
    {
      "arxiv_id": "2502.03962v2",
      "title": "Quantum Circuit Design using a Progressive Widening Enhanced Monte Carlo Tree Search",
      "title_zh": "翻译失败",
      "authors": [
        "Vincenzo Lipardi",
        "Domenica Dibenedetto",
        "Georgios Stamoulis",
        "Mark H. M. Winands"
      ],
      "abstract": "The performance of Variational Quantum Algorithms (VQAs) strongly depends on\nthe choice of the parameterized quantum circuit to optimize. One of the biggest\nchallenges in VQAs is designing quantum circuits tailored to the particular\nproblem. This article proposes a gradient-free Monte Carlo Tree Search (MCTS)\ntechnique to automate the process of quantum circuit design. Our proposed\ntechnique introduces a novel formulation of the action space based on a\nsampling scheme and a progressive widening technique to explore the space\ndynamically. When testing our MCTS approach on the domain of random quantum\ncircuits, MCTS approximates unstructured circuits under different values of\nstabilizer R\\'enyi entropy. It turns out that MCTS manages to approximate the\nbenchmark quantum states independently from their degree of nonstabilizerness.\nNext, our technique exhibits robustness across various application domains,\nincluding quantum chemistry and systems of linear equations. Compared to\nprevious MCTS research, our technique reduces the number of quantum circuit\nevaluations by a factor of 10 up to 100 while achieving equal or better\nresults. In addition, the resulting quantum circuits exhibit up to three times\nfewer CNOT gates, which is important for implementation on noisy quantum\nhardware.",
      "tldr_zh": "这篇论文提出了一种增强型 Monte Carlo Tree Search (MCTS) 方法，利用 progressive widening 技术和采样方案来自动化 Variational Quantum Algorithms (VQAs) 的量子电路设计，解决了电路设计对问题特定性的挑战。实验结果显示，该方法在随机量子电路领域能够近似不同 stabilizer Rényi entropy 值的非结构化电路，且在量子化学和线性方程系统等领域表现出鲁棒性。相比现有MCTS研究，该技术减少了10至100倍的量子电路评估次数，同时生成的电路使用最多三倍少的CNOT gates，提升了在嘈杂量子硬件上的实用性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03962v2",
      "published_date": "2025-02-06 10:52:11 UTC",
      "updated_date": "2025-04-28 12:38:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:08:24.667579"
    },
    {
      "arxiv_id": "2502.03957v1",
      "title": "Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples",
      "title_zh": "翻译失败",
      "authors": [
        "Konstantinos Tsigos",
        "Evlampios Apostolidis",
        "Vasileios Mezaris"
      ],
      "abstract": "In this paper, we introduce the idea of using adversarially-generated samples\nof the input images that were classified as deepfakes by a detector, to form\nperturbation masks for inferring the importance of different input features and\nproduce visual explanations. We generate these samples based on Natural\nEvolution Strategies, aiming to flip the original deepfake detector's decision\nand classify these samples as real. We apply this idea to four\nperturbation-based explanation methods (LIME, SHAP, SOBOL and RISE) and\nevaluate the performance of the resulting modified methods using a SOTA\ndeepfake detection model, a benchmarking dataset (FaceForensics++) and a\ncorresponding explanation evaluation framework. Our quantitative assessments\ndocument the mostly positive contribution of the proposed perturbation approach\nin the performance of explanation methods. Our qualitative analysis shows the\ncapacity of the modified explanation methods to demarcate the manipulated image\nregions more accurately, and thus to provide more useful explanations.",
      "tldr_zh": "本文提出一种改进深度伪造检测器（Deepfake Detectors）解释方法的新思路，通过使用基于 Natural Evolution Strategies 生成的对抗样本（Adversarially-Generated Samples），来形成扰动掩码并推断输入特征的重要性。作者将此方法应用于四个基于扰动的解释技术（LIME, SHAP, SOBOL 和 RISE），并在 SOTA 深度伪造检测模型和 FaceForensics++ 数据集上进行评估。结果显示，该方法显著提升了解释性能，定量评估证明其积极贡献，而定性分析表明它能更准确地标记被操纵图像区域，提供更有用的视觉解释。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication, AI4MFDD Workshop @ IEEE/CVF Winter\n  Conference on Applications of Computer Vision (WACV 2025), Tucson, AZ, USA,\n  Feb. 2025. This is the authors' \"accepted version\"",
      "pdf_url": "http://arxiv.org/pdf/2502.03957v1",
      "published_date": "2025-02-06 10:47:34 UTC",
      "updated_date": "2025-02-06 10:47:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:08:37.732541"
    },
    {
      "arxiv_id": "2502.03954v1",
      "title": "MAQInstruct: Instruction-based Unified Event Relation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Xu",
        "Mengshu Sun",
        "Zhiqiang Zhang",
        "Jun Zhou"
      ],
      "abstract": "Extracting event relations that deviate from known schemas has proven\nchallenging for previous methods based on multi-class classification, MASK\nprediction, or prototype matching. Recent advancements in large language models\nhave shown impressive performance through instruction tuning. Nevertheless, in\nthe task of event relation extraction, instruction-based methods face several\nchallenges: there are a vast number of inference samples, and the relations\nbetween events are non-sequential. To tackle these challenges, we present an\nimproved instruction-based event relation extraction framework named\nMAQInstruct. Firstly, we transform the task from extracting event relations\nusing given event-event instructions to selecting events using given\nevent-relation instructions, which reduces the number of samples required for\ninference. Then, by incorporating a bipartite matching loss, we reduce the\ndependency of the instruction-based method on the generation sequence. Our\nexperimental results demonstrate that MAQInstruct significantly improves the\nperformance of event relation extraction across multiple LLMs.",
      "tldr_zh": "论文提出 MAQInstruct 框架，这是一种基于指令的统一事件关系提取方法，旨在解决传统方法（如多类分类或 MASK 预测）在处理偏离已知模式的复杂事件关系时的挑战。框架通过将任务从事件-事件指令提取转变为使用事件-relation instructions 选择事件，从而减少推理样本数量，并引入 bipartite matching loss 来降低对生成序列的依赖。实验结果表明，MAQInstruct 显著提升了多个 LLMs 在事件关系提取任务上的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by WWW 2025 short",
      "pdf_url": "http://arxiv.org/pdf/2502.03954v1",
      "published_date": "2025-02-06 10:46:19 UTC",
      "updated_date": "2025-02-06 10:46:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:08:48.424199"
    },
    {
      "arxiv_id": "2502.03948v1",
      "title": "Enhancing Online Learning Efficiency Through Heterogeneous Resource Integration with a Multi-Agent RAG System",
      "title_zh": "翻译失败",
      "authors": [
        "Devansh Srivastav",
        "Hasan Md Tusfiqur Alam",
        "Afsaneh Asaei",
        "Mahmoud Fazeli",
        "Tanisha Sharma",
        "Daniel Sonntag"
      ],
      "abstract": "Efficient online learning requires seamless access to diverse resources such\nas videos, code repositories, documentation, and general web content. This\nposter paper introduces early-stage work on a Multi-Agent Retrieval-Augmented\nGeneration (RAG) System designed to enhance learning efficiency by integrating\nthese heterogeneous resources. Using specialized agents tailored for specific\nresource types (e.g., YouTube tutorials, GitHub repositories, documentation\nwebsites, and search engines), the system automates the retrieval and synthesis\nof relevant information. By streamlining the process of finding and combining\nknowledge, this approach reduces manual effort and enhances the learning\nexperience. A preliminary user study confirmed the system's strong usability\nand moderate-high utility, demonstrating its potential to improve the\nefficiency of knowledge acquisition.",
      "tldr_zh": "这篇论文提出了一种Multi-Agent RAG System，通过整合异构资源（如视频、代码库、文档和网络内容），以提升在线学习效率。系统使用专门的agents针对特定资源类型（如YouTube教程、GitHub仓库和搜索引擎）自动进行信息检索和合成，从而减少手动努力并优化学习体验。初步用户研究显示，该系统具备强可用性和中等到高的实用性，证明了其在提高知识获取效率方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03948v1",
      "published_date": "2025-02-06 10:36:17 UTC",
      "updated_date": "2025-02-06 10:36:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:09:00.073055"
    },
    {
      "arxiv_id": "2502.03930v2",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "abstract": "Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.",
      "tldr_zh": "该研究提出 DiTAR，一种基于 patch 的自回归框架，将语言模型与扩散 transformer 结合，用于高效生成连续语音表示，解决了现有方法的计算负荷大和效果不佳问题。DiTAR 采用 divide-and-conquer 策略，让语言模型处理聚合的 patch 嵌入，然后由扩散 transformer 生成下一个 patch，并在推理中通过定义温度作为噪声引入时间点来平衡多样性和确定性。实验结果显示，DiTAR 在零样本语音生成中表现出色，在鲁棒性、说话者相似性和自然性上达到最先进水平，并具有优秀的可扩展性。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "16 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.03930v2",
      "published_date": "2025-02-06 10:09:49 UTC",
      "updated_date": "2025-02-14 09:49:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:09:12.358942"
    },
    {
      "arxiv_id": "2502.04408v1",
      "title": "Transforming Multimodal Models into Action Models for Radiotherapy",
      "title_zh": "翻译失败",
      "authors": [
        "Matteo Ferrante",
        "Alessandra Carosi",
        "Rolando Maria D Angelillo",
        "Nicola Toschi"
      ],
      "abstract": "Radiotherapy is a crucial cancer treatment that demands precise planning to\nbalance tumor eradication and preservation of healthy tissue. Traditional\ntreatment planning (TP) is iterative, time-consuming, and reliant on human\nexpertise, which can potentially introduce variability and inefficiency. We\npropose a novel framework to transform a large multimodal foundation model\n(MLM) into an action model for TP using a few-shot reinforcement learning (RL)\napproach. Our method leverages the MLM's extensive pre-existing knowledge of\nphysics, radiation, and anatomy, enhancing it through a few-shot learning\nprocess. This allows the model to iteratively improve treatment plans using a\nMonte Carlo simulator. Our results demonstrate that this method outperforms\nconventional RL-based approaches in both quality and efficiency, achieving\nhigher reward scores and more optimal dose distributions in simulations on\nprostate cancer data. This proof-of-concept suggests a promising direction for\nintegrating advanced AI models into clinical workflows, potentially enhancing\nthe speed, quality, and standardization of radiotherapy treatment planning.",
      "tldr_zh": "该研究针对放射治疗(radiotherapy)的精确规划问题，提出一种新框架，将大型多模态基础模型(MLM)转化为行动模型，通过少样本强化学习(RL)方法利用其预有知识（如物理、辐射和解剖学），并结合Monte Carlo模拟器进行迭代优化治疗计划。相比传统RL方法，该框架在前列腺癌数据模拟中实现了更高的奖励分数和更优的剂量分布，提高了规划的质量和效率。该方法为将先进AI模型整合到临床工作流中提供了可行路径，有望提升放射治疗的标准化和速度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04408v1",
      "published_date": "2025-02-06 09:51:28 UTC",
      "updated_date": "2025-02-06 09:51:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:09:24.273251"
    },
    {
      "arxiv_id": "2502.03918v1",
      "title": "Adaptation of Task Goal States from Prior Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Andrei Costinescu",
        "Darius Burschka"
      ],
      "abstract": "This paper presents a framework to define a task with freedom and variability\nin its goal state. A robot could use this to observe the execution of a task\nand target a different goal from the observed one; a goal that is still\ncompatible with the task description but would be easier for the robot to\nexecute. We define the model of an environment state and an environment\nvariation, and present experiments on how to interactively create the variation\nfrom a single task demonstration and how to use this variation to create an\nexecution plan for bringing any environment into the goal state.",
      "tldr_zh": "本论文提出一个框架，用于定义任务目标状态（task goal states），允许目标具有自由度和可变性，从而让机器人从观察的任务执行中，选取一个与任务描述兼容但更易执行的目标。论文定义了环境状态（environment state）和环境变异（environment variation）的模型，并通过实验展示了如何从单个任务演示（task demonstration）中互动创建变异，以及利用该变异生成执行计划（execution plan），以将任何环境带入目标状态。该框架有助于机器人基于先验知识（prior knowledge）适应任务，提高执行效率和灵活性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03918v1",
      "published_date": "2025-02-06 09:51:04 UTC",
      "updated_date": "2025-02-06 09:51:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:09:37.917609"
    },
    {
      "arxiv_id": "2502.03916v1",
      "title": "Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software",
      "title_zh": "翻译失败",
      "authors": [
        "Andreas Baumann",
        "Peter Eberhard"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly helpful in text generation,\neven writing code in programming languages based on user prompts written in\nnatural language. They are even applied to generate simulation models for\nmultibody systems from natural language. Research results suggest that LLMs\nsurpass the mere replication of existing code examples, where some LLMs have\nbeen trained on an open-source multibody simulation code. However, for\nclosed-source simulation software, such results are not to be expected as their\nideas and concepts might differ from other publicly available ones. LLMs can\nhallucinate for knowledge-intensive tasks, such as model creation, which can\nlead to wrong responses. This is especially the case for the LLM unknown\nclosed-source simulation software. The same applies to other internal knowledge\nkept private to protect intellectual property or data privacy. The\nRetrieval-Augmented Generation (RAG) approach might yield a solution for these\nknowledge-intensive tasks. This paper explores the application of RAG to\nclosed-source simulation software and presents first experiments. After a brief\nintroduction to LLMs, the RAG approach, and the simulation method applied by\nthe close-source simulation software, several examples are provided to test\nLLMs' knowledge of the simulation software and the creation of simulation\nmodels using two RAG systems. The examples show promising results indicating\nthe benefits of applying RAG systems to closed-source simulation software,\nhelping to access their knowledge. Nevertheless, they also reveal gaps in the\napplied information and open questions for further research.",
      "tldr_zh": "本文研究了 Large Language Models (LLMs) 在闭源模拟软件中的应用问题，重点探讨 Retrieval-Augmented Generation (RAG) 如何缓解 LLMs 在知识密集任务中的幻觉问题，如生成多体系统模拟模型。作者通过实验测试了 LLMs 对闭源软件的知识水平，并使用两种 RAG 系统创建模拟模型，结果显示 RAG 系统能有效提升访问闭源知识的能力。实验揭示了 RAG 的潜在益处，但也暴露了信息缺口和进一步研究的需求。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.03916v1",
      "published_date": "2025-02-06 09:48:04 UTC",
      "updated_date": "2025-02-06 09:48:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:09:48.282599"
    },
    {
      "arxiv_id": "2502.04407v1",
      "title": "Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall Partitioning for Architectural Layout Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Reza Kakooee",
        "Benjamin Dillenburger"
      ],
      "abstract": "Space layout design (SLD), occurring in the early stages of the design\nprocess, nonetheless influences both the functionality and aesthetics of the\nultimate architectural outcome. The complexity of SLD necessitates innovative\napproaches to efficiently explore vast solution spaces. While image-based\ngenerative AI has emerged as a potential solution, they often rely on\npixel-based space composition methods that lack intuitive representation of\narchitectural processes. This paper leverages deep Reinforcement Learning (RL),\nas it offers a procedural approach that intuitively mimics the process of human\ndesigners. Effectively using RL for SLD requires an explorative space composing\nmethod to generate desirable design solutions. We introduce \"laser-wall\", a\nnovel space partitioning method that conceptualizes walls as emitters of\nimaginary light beams to partition spaces. This approach bridges vector-based\nand pixel-based partitioning methods, offering both flexibility and exploratory\npower in generating diverse layouts. We present two planning strategies:\none-shot planning, which generates entire layouts in a single pass, and dynamic\nplanning, which allows for adaptive refinement by continuously transforming\nlaser-walls. Additionally, we introduce on-light and off-light wall\ntransformations for smooth and fast layout refinement, as well as identity-less\nand identity-full walls for versatile room assignment. We developed\nSpaceLayoutGym, an open-source OpenAI Gym compatible simulator for generating\nand evaluating space layouts. The RL agent processes the input design scenarios\nand generates solutions following a reward function that balances geometrical\nand topological requirements. Our results demonstrate that the RL-based\nlaser-wall approach can generate diverse and functional space layouts that\nsatisfy both geometric constraints and topological requirements and is\narchitecturally intuitive.",
      "tldr_zh": "本文提出一种基于深度强化学习（Deep Reinforcement Learning）的建筑布局生成方法，结合新型激光墙分区（laser-wall）技术，以模拟人类设计师的程序化过程，解决传统图像生成AI在直观性和灵活性上的不足。激光墙方法将墙壁视为发射假想光束来分区空间，并引入one-shot planning和dynamic planning策略，以及on-light/off-light墙壁变换和identity-less/identity-full墙壁类型，以实现多样化的布局探索和优化。研究开发了开源模拟器SpaceLayoutGym，通过奖励函数平衡几何和拓扑要求，实验结果表明该方法能生成满足约束的功能性布局，并提供更直观的建筑设计流程。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04407v1",
      "published_date": "2025-02-06 09:35:24 UTC",
      "updated_date": "2025-02-06 09:35:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:10:01.253395"
    },
    {
      "arxiv_id": "2502.04406v1",
      "title": "Calibrated Physics-Informed Uncertainty Quantification",
      "title_zh": "校准的基于物理信息的不确定性量化",
      "authors": [
        "Vignesh Gopakumar",
        "Ander Gray",
        "Lorenzo Zanisi",
        "Timothy Nunn",
        "Stanislas Pamela",
        "Daniel Giles",
        "Matt J. Kusner",
        "Marc Peter Deisenroth"
      ],
      "abstract": "Neural PDEs offer efficient alternatives to computationally expensive\nnumerical PDE solvers for simulating complex physical systems. However, their\nlack of robust uncertainty quantification (UQ) limits deployment in critical\napplications. We introduce a model-agnostic, physics-informed conformal\nprediction (CP) framework that provides guaranteed uncertainty estimates\nwithout requiring labelled data. By utilising a physics-based approach, we are\nable to quantify and calibrate the model's inconsistencies with the PDE rather\nthan the uncertainty arising from the data. Our approach uses convolutional\nlayers as finite-difference stencils and leverages physics residual errors as\nnonconformity scores, enabling data-free UQ with marginal and joint coverage\nguarantees across prediction domains for a range of complex PDEs. We further\nvalidate the efficacy of our method on neural PDE models for plasma modelling\nand shot design in fusion reactors.",
      "tldr_zh": "该研究提出了一种模型无关的物理信息共形预测(CP)框架，用于在Neural PDEs中进行校准的不确定性量化(UQ)，以解决传统数值PDE求解器效率问题，同时提供可靠的物理不一致性估计，而无需标记数据。框架通过将卷积层用作有限差分模板，并利用物理残差错误作为非一致性分数，实现数据无关的UQ，并确保预测领域的边际和联合覆盖保证。该方法在等离子体建模和聚变反应堆射击设计等复杂PDE应用中得到验证，展示了其在关键领域的有效性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04406v1",
      "published_date": "2025-02-06 09:23:06 UTC",
      "updated_date": "2025-02-06 09:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:10:12.829903"
    },
    {
      "arxiv_id": "2502.06839v1",
      "title": "A Hybrid Model for Weakly-Supervised Speech Dereverberation",
      "title_zh": "翻译失败",
      "authors": [
        "Louis Bahrman",
        "Mathieu Fontaine",
        "Gael Richard"
      ],
      "abstract": "This paper introduces a new training strategy to improve speech\ndereverberation systems using minimal acoustic information and reverberant\n(wet) speech. Most existing algorithms rely on paired dry/wet data, which is\ndifficult to obtain, or on target metrics that may not adequately capture\nreverberation characteristics and can lead to poor results on non-target\nmetrics. Our approach uses limited acoustic information, like the reverberation\ntime (RT60), to train a dereverberation system. The system's output is\nresynthesized using a generated room impulse response and compared with the\noriginal reverberant speech, providing a novel reverberation matching loss\nreplacing the standard target metrics. During inference, only the trained\ndereverberation model is used. Experimental results demonstrate that our method\nachieves more consistent performance across various objective metrics used in\nspeech dereverberation than the state-of-the-art.",
      "tldr_zh": "本论文提出了一种混合模型，用于弱监督的语音去混响（speech dereverberation），旨在通过最小声学信息（如 RT60）来训练系统，以解决传统方法依赖难以获取的配对干/湿语音数据或目标指标不足的问题。创新点在于引入混响匹配损失（reverberation matching loss），通过生成房间脉冲响应（room impulse response）重新合成输出，并与原始混响语音比较，从而提升模型的鲁棒性。在推理阶段，仅需使用训练好的模型，实验结果显示该方法在各种客观指标上比现有最先进（state-of-the-art）方法表现出更一致的性能。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.06839v1",
      "published_date": "2025-02-06 09:21:22 UTC",
      "updated_date": "2025-02-06 09:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:10:24.401992"
    },
    {
      "arxiv_id": "2502.03897v4",
      "title": "UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Zhao",
        "Linfeng Feng",
        "Dongxu Ge",
        "Rujin Chen",
        "Fangqiu Yi",
        "Chi Zhang",
        "Xiao-Lei Zhang",
        "Xuelong Li"
      ],
      "abstract": "With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\naddress these limitations, we first propose UniForm, a unified multi-task\ndiffusion transformer that jointly generates audio and visual modalities in a\nshared latent space. A single diffusion process models both audio and video,\ncapturing the inherent correlations between sound and vision. Second, we\nintroduce task-specific noise schemes and task tokens, enabling a single model\nto support multiple tasks, including text-to-audio-video, audio-to-video, and\nvideo-to-audio generation. Furthermore, by leveraging large language models and\na large-scale text-audio-video combined dataset, UniForm achieves greater\ngenerative diversity than prior approaches. Extensive experiments show that\nUniForm achieves the state-of-the-art performance across audio-video generation\ntasks, producing content that is both well-aligned and close to real-world data\ndistributions. Our demos are available at https://uniform-t2av.github.io/.",
      "tldr_zh": "本论文提出 UniForm，一种统一的 multi-task diffusion transformer，用于在共享潜在空间中联合生成音频和视频模态，从而捕捉声音与视觉的内在相关性。UniForm 引入任务-specific noise schemes 和 task tokens，使单一模型支持多种任务，包括 text-to-audio-video、audio-to-video 和 video-to-audio 生成，并利用大型语言模型和大规模文本-音频-视频数据集提升生成多样性。实验结果显示，UniForm 在音频-视频生成任务中达到 state-of-the-art 性能，生成的內容与真实世界数据分布高度对齐。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "Our demos are available at https://uniform-t2av.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2502.03897v4",
      "published_date": "2025-02-06 09:18:30 UTC",
      "updated_date": "2025-04-15 06:53:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:10:36.894230"
    },
    {
      "arxiv_id": "2502.15735v1",
      "title": "DistrEE: Distributed Early Exit of Deep Neural Network Inference on Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Xian Peng",
        "Xin Wu",
        "Lianming Xu",
        "Li Wang",
        "Aiguo Fei"
      ],
      "abstract": "Distributed DNN inference is becoming increasingly important as the demand\nfor intelligent services at the network edge grows. By leveraging the power of\ndistributed computing, edge devices can perform complicated and resource-hungry\ninference tasks previously only possible on powerful servers, enabling new\napplications in areas such as autonomous vehicles, industrial automation, and\nsmart homes. However, it is challenging to achieve accurate and efficient\ndistributed edge inference due to the fluctuating nature of the actual\nresources of the devices and the processing difficulty of the input data. In\nthis work, we propose DistrEE, a distributed DNN inference framework that can\nexit model inference early to meet specific quality of service requirements. In\nparticular, the framework firstly integrates model early exit and distributed\ninference for multi-node collaborative inferencing scenarios. Furthermore, it\ndesigns an early exit policy to control when the model inference terminates.\nExtensive simulation results demonstrate that DistrEE can efficiently realize\nefficient collaborative inference, achieving an effective trade-off between\ninference latency and accuracy.",
      "tldr_zh": "本论文提出 DistrEE 框架，用于在边缘设备上实现分布式 DNN 推理，通过整合模型 early exit 和分布式推理，支持多节点协作场景。DistrEE 设计了早退出策略来动态控制推理终止时机，以应对设备资源波动和输入数据处理挑战。实验结果显示，该框架在模拟环境中实现了推理延迟和准确性之间的有效权衡，提升了整体效率。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.15735v1",
      "published_date": "2025-02-06 09:16:54 UTC",
      "updated_date": "2025-02-06 09:16:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:10:47.457033"
    },
    {
      "arxiv_id": "2502.04405v2",
      "title": "FAS: Fast ANN-SNN Conversion for Spiking Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Long Chen",
        "Xiaotian Song",
        "Andy Song",
        "BaDong Chen",
        "Jiancheng Lv",
        "Yanan Sun"
      ],
      "abstract": "Spiking Large Language Models have been shown as a good alternative to LLMs\nin various scenarios. Existing methods for creating Spiking LLMs, i.e., direct\ntraining and ANN-SNN conversion, often suffer from performance degradation and\nrelatively high computational costs. To address these issues, we propose a\nnovel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking\nLLMs in two stages. The first stage employs a full-parameter fine-tuning of\npre-trained models, so it does not need any direct training from scratch. The\nsecond stage introduces a coarse-to-fine calibration method to reduce\nconversion errors and improve accuracy. Experiments on both language and\nvision-language tasks across four different scales of LLMs demonstrate that FAS\ncan achieve state-of-the-art performance yet with significantly reduced\ninference latency and computational costs. Notably, FAS only takes eight\ntimesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model,\nwhile reducing energy consumption by 96.63\\%. The source code is available at\nhttps://github.com/lc783/FAS",
      "tldr_zh": "该研究提出了一种快速ANN-SNN转换策略FAS，用于将大型语言模型（LLMs）转换为Spiking Large Language Models，以解决现有方法的性能下降和高计算成本问题。FAS分为两个阶段：首先对预训练模型进行全参数微调，避免从零开始训练；其次采用粗到细的校准方法来减少转换错误并提高准确性。在语言和视觉语言任务上进行的实验显示，FAS在不同规模的LLMs中实现了最先进的性能，仅需八个时间步就比OPT-7B模型准确率高3%，并减少了96.63%的能源消耗。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04405v2",
      "published_date": "2025-02-06 09:08:12 UTC",
      "updated_date": "2025-05-14 05:23:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:11:01.217502"
    },
    {
      "arxiv_id": "2502.03884v1",
      "title": "Rank Also Matters: Hierarchical Configuration for Mixture of Adapter Experts in LLM Fine-Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Peizhuang Cong",
        "Wenpu Liu",
        "Wenhan Yu",
        "Haochen Zhao",
        "Tong Yang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable success across\nvarious tasks, accompanied by a continuous increase in their parameter size.\nParameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation\n(LoRA), address the challenges of fine-tuning LLMs by significantly reducing\nthe number of trainable parameters. Recent studies have integrated LoRA with\nMixture of Experts (MoE) architectures, leveraging multiple adapter experts and\ngating mechanisms to further improve fine-tuning performance. However, existing\napproaches primarily focus on adjusting the allocations of adapter experts per\nlayer to optimize the introduced trainable parameter size, while neglecting a\ncritical factor of adapters' rank. To this end, we propose a hierarchical\nscheme for expert allocation and rank configuration, HILO, which dynamically\nadjusts the number and rank of adapter experts across layers, matching the\nvarying representational complexity of model layers in adapter-granularity.\nExtensive experiments on multiple benchmark tasks demonstrate that HILO\noutperforms existing methods in accuracy while introducing fewer trainable\nparameters, providing an efficient and practical solution for fine-tuning LLMs.",
      "tldr_zh": "本研究关注大型语言模型 (LLMs) 的参数高效微调 (PEFT)，指出现有方法如 Low-Rank Adaptation (LoRA) 与 Mixture of Experts (MoE) 架构虽优化了适配器专家分配，但忽略了适配器排名的关键因素。论文提出 HILO 方案，一种分层配置机制，能够动态调整各层适配器专家的数量和排名，以适应不同层级的表示复杂性。实验在多个基准任务上证明，HILO 比现有方法实现了更高的准确率，同时显著减少了可训练参数，提供了一种高效的 LLM 微调解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03884v1",
      "published_date": "2025-02-06 08:58:03 UTC",
      "updated_date": "2025-02-06 08:58:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:11:12.781542"
    },
    {
      "arxiv_id": "2502.04404v1",
      "title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao-Wen Yang",
        "Xuan-Yi Zhu",
        "Wen-Da Wei",
        "Ding-Chu Zhang",
        "Jie-Jing Shao",
        "Zhi Zhou",
        "Lan-Zhe Guo",
        "Yu-Feng Li"
      ],
      "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs)\noffers a promising way toward achieving Level 2 AGI Reasoners, as exemplified\nby systems like OpenAI's o1. However, several significant challenges remain,\nincluding inefficient overthinking and an overreliance on auxiliary reward\nmodels. We point out that these limitations stem from LLMs' inability to\ninternalize the search process, a key component of effective reasoning. A\ncritical step toward addressing this issue is enabling LLMs to autonomously\ndetermine when and where to backtrack, a fundamental operation in traditional\nsearch algorithms. To this end, we propose a self-backtracking mechanism that\nequips LLMs with the ability to backtrack during both training and inference.\nThis mechanism not only enhances reasoning ability but also efficiency by\ntransforming slow-thinking processes into fast-thinking through\nself-improvement. Empirical evaluations demonstrate that our proposal\nsignificantly enhances the reasoning capabilities of LLMs, achieving a\nperformance gain of over 40 percent compared to the optimal-path supervised\nfine-tuning method. We believe this study introduces a novel and promising\npathway for developing more advanced and robust Reasoners.",
      "tldr_zh": "该论文探讨了大型语言模型 (LLMs) 在实现 Level 2 AGI Reasoners 时面临的挑战，如低效的过度思考和对辅助奖励模型的过度依赖，这些问题源于 LLMs 无法内化搜索过程。研究提出 self-backtracking 机制，让 LLMs 在训练和推理过程中自主决定何时何地回溯，从而提升推理能力和效率，并通过自我改进将慢思考转化为快思考。实验结果显示，该机制相较于最佳路径监督微调方法，性能提升超过 40%。这项研究为开发更高级、更鲁棒的 Reasoners 提供了创新路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "This is a preprint under review, 15 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.04404v1",
      "published_date": "2025-02-06 08:52:43 UTC",
      "updated_date": "2025-02-06 08:52:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:11:24.445228"
    },
    {
      "arxiv_id": "2502.05228v1",
      "title": "Multi-Objective Mobile Damped Wave Algorithm (MOMDWA): A Novel Approach For Quantum System Control",
      "title_zh": "翻译失败",
      "authors": [
        "Juntao Yu",
        "Jiaquan Yu",
        "Dedai Wei",
        "Xinye Sha",
        "Shengwei Fu",
        "Miuyu Qiu",
        "Yurun Jin",
        "Kaichen Ouyang"
      ],
      "abstract": "In this paper, we introduce a novel multi-objective optimization algorithm,\nthe Multi-Objective Mobile Damped Wave Algorithm (MOMDWA), specifically\ndesigned to address complex quantum control problems. Our approach extends the\ncapabilities of the original Mobile Damped Wave Algorithm (MDWA) by\nincorporating multiple objectives, enabling a more comprehensive optimization\nprocess. We applied MOMDWA to three quantum control scenarios, focusing on\noptimizing the balance between control fidelity, energy consumption, and\ncontrol smoothness. The results demonstrate that MOMDWA significantly enhances\nquantum control efficiency and robustness, achieving high fidelity while\nminimizing energy use and ensuring smooth control pulses. This advancement\noffers a valuable tool for quantum computing and other domains requiring\nprecise, multi-objective control.",
      "tldr_zh": "本研究提出了一种新型多目标优化算法，Multi-Objective Mobile Damped Wave Algorithm (MOMDWA)，它是基于原Mobile Damped Wave Algorithm (MDWA)的扩展，旨在处理复杂的量子系统控制问题。MOMDWA 通过同时优化控制保真度(fidelity)、能量消耗和控制平滑度(control smoothness)，应用于三个量子控制场景，实现了目标间的平衡优化。实验结果显示，该算法显著提升了量子控制的效率和鲁棒性，实现了高保真度同时最小化能量使用和确保平滑控制脉冲，为量子计算及其他需要精确多目标控制的领域提供了宝贵工具。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.05228v1",
      "published_date": "2025-02-06 08:43:21 UTC",
      "updated_date": "2025-02-06 08:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:11:36.222280"
    },
    {
      "arxiv_id": "2502.04403v1",
      "title": "Agency Is Frame-Dependent",
      "title_zh": "翻译失败",
      "authors": [
        "David Abel",
        "André Barreto",
        "Michael Bowling",
        "Will Dabney",
        "Shi Dong",
        "Steven Hansen",
        "Anna Harutyunyan",
        "Khimya Khetarpal",
        "Clare Lyle",
        "Razvan Pascanu",
        "Georgios Piliouras",
        "Doina Precup",
        "Jonathan Richens",
        "Mark Rowland",
        "Tom Schaul",
        "Satinder Singh"
      ],
      "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
      "tldr_zh": "论文认为，“agency”（代理性）是系统引导结果朝向目标的能力，这一概念在生物学、哲学、认知科学和人工智能领域备受关注，但判断系统是否具有代理性极具挑战。作者从“reinforcement learning”（强化学习）的视角出发，论证代理性本质上是“frame-dependent”（框架依赖的），即任何代理性评估必须相对参考框架进行。通过分析Barandiaran et al. (2009)和Moreno (2018)提出的代理性关键属性，论文证明这些属性本身也依赖框架。最终，论文强调，代理性的基础科学需纳入框架依赖性，并讨论了其对强化学习的影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04403v1",
      "published_date": "2025-02-06 08:34:57 UTC",
      "updated_date": "2025-02-06 08:34:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:11:48.841876"
    },
    {
      "arxiv_id": "2502.03852v1",
      "title": "Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount",
      "title_zh": "通过类别信息量追求长尾物体检测的更好决策边界",
      "authors": [
        "Yanbiao Ma",
        "Wei Dai",
        "Jiayi Chen"
      ],
      "abstract": "In object detection, the instance count is typically used to define whether a\ndataset exhibits a long-tail distribution, implicitly assuming that models will\nunderperform on categories with fewer instances. This assumption has led to\nextensive research on category bias in datasets with imbalanced instance\ncounts. However, models still exhibit category bias even in datasets where\ninstance counts are relatively balanced, clearly indicating that instance count\nalone cannot explain this phenomenon. In this work, we first introduce the\nconcept and measurement of category information amount. We observe a\nsignificant negative correlation between category information amount and\naccuracy, suggesting that category information amount more accurately reflects\nthe learning difficulty of a category. Based on this observation, we propose\nInformation Amount-Guided Angular Margin (IGAM) Loss. The core idea of IGAM is\nto dynamically adjust the decision space of each category based on its\ninformation amount, thereby reducing category bias in long-tail datasets. IGAM\nLoss not only performs well on long-tailed benchmark datasets such as LVIS v1.0\nand COCO-LT but also shows significant improvement for underrepresented\ncategories in the non-long-tailed dataset Pascal VOC. Comprehensive experiments\ndemonstrate the potential of category information amount as a tool and the\ngenerality of our proposed method.",
      "tldr_zh": "本文研究发现，在长尾对象检测中，类别偏差不仅仅取决于实例数量，还与 category information amount 密切相关，后者更能反映类别的学习难度。作者引入了 category information amount 的概念和测量方法，并提出 Information Amount-Guided Angular Margin (IGAM) Loss，通过动态调整每个类别的决策空间来减少偏差。该方法在长尾基准数据集如 LVIS v1.0 和 COCO-LT 上表现出色，并在非长尾数据集 Pascal VOC 的 underrepresented 类别上实现了显著改进，证明了其通用性和潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.03852v1",
      "published_date": "2025-02-06 08:08:18 UTC",
      "updated_date": "2025-02-06 08:08:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:12:00.371169"
    },
    {
      "arxiv_id": "2502.04402v1",
      "title": "Beyond Interpolation: Extrapolative Reasoning with Reinforcement Learning and Graph Neural Networks",
      "title_zh": "超越插值：利用强化学习和图神经网络的外推推理",
      "authors": [
        "Niccolò Grillo",
        "Andrea Toccaceli",
        "Joël Mathys",
        "Benjamin Estermann",
        "Stefania Fresca",
        "Roger Wattenhofer"
      ],
      "abstract": "Despite incredible progress, many neural architectures fail to properly\ngeneralize beyond their training distribution. As such, learning to reason in a\ncorrect and generalizable way is one of the current fundamental challenges in\nmachine learning. In this respect, logic puzzles provide a great testbed, as we\ncan fully understand and control the learning environment. Thus, they allow to\nevaluate performance on previously unseen, larger and more difficult puzzles\nthat follow the same underlying rules. Since traditional approaches often\nstruggle to represent such scalable logical structures, we propose to model\nthese puzzles using a graph-based approach. Then, we investigate the key\nfactors enabling the proposed models to learn generalizable solutions in a\nreinforcement learning setting. Our study focuses on the impact of the\ninductive bias of the architecture, different reward systems and the role of\nrecurrent modeling in enabling sequential reasoning. Through extensive\nexperiments, we demonstrate how these elements contribute to successful\nextrapolation on increasingly complex puzzles.These insights and frameworks\noffer a systematic way to design learning-based systems capable of\ngeneralizable reasoning beyond interpolation.",
      "tldr_zh": "该研究探讨了神经模型在训练分布外泛化失败的问题，提出使用强化学习(Reinforcement Learning)和图神经网络(Graph Neural Networks)实现外推式推理(Extrapolative Reasoning)。通过将逻辑谜题建模为图结构，作者调查了架构的归纳偏差、不同奖励系统以及循环建模在顺序推理中的作用，以提升模型的可泛化能力。实验结果显示，这些因素显著提高了模型在更大、更复杂谜题上的性能，并为设计可泛化推理系统提供了系统框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The first two authors contributed equally to this work. Accepted as\n  workshop paper at NEURMAD@AAAI25",
      "pdf_url": "http://arxiv.org/pdf/2502.04402v1",
      "published_date": "2025-02-06 08:07:35 UTC",
      "updated_date": "2025-02-06 08:07:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:12:12.681885"
    },
    {
      "arxiv_id": "2502.03843v1",
      "title": "Improving Natural Language Understanding for LLMs via Large-Scale Instruction Synthesis",
      "title_zh": "通过大规模指令合成改进 LLMs 的自然语言理解",
      "authors": [
        "Lin Yuan",
        "Jun Xu",
        "Honghao Gui",
        "Mengshu Sun",
        "Zhiqiang Zhang",
        "Lei Liang",
        "Jun Zhou"
      ],
      "abstract": "High-quality, large-scale instructions are crucial for aligning large\nlanguage models (LLMs), however, there is a severe shortage of instruction in\nthe field of natural language understanding (NLU). Previous works on\nconstructing NLU instructions mainly focus on information extraction (IE),\nneglecting tasks such as machine reading comprehension, question answering, and\ntext classification. Furthermore, the lack of diversity in the data has led to\na decreased generalization ability of trained LLMs in other NLU tasks and a\nnoticeable decline in the fundamental model's general capabilities. To address\nthis issue, we propose Hum, a large-scale, high-quality synthetic instruction\ncorpus for NLU tasks, designed to enhance the NLU capabilities of LLMs.\nSpecifically, Hum includes IE (either close IE or open IE), machine reading\ncomprehension, text classification, and instruction generalist tasks, thereby\nenriching task diversity. Additionally, we introduce a human-LLMs collaborative\nmechanism to synthesize instructions, which enriches instruction diversity by\nincorporating guidelines, preference rules, and format variants. We conduct\nextensive experiments on 5 NLU tasks and 28 general capability evaluation\ndatasets for LLMs. Experimental results show that Hum enhances the NLU\ncapabilities of six LLMs by an average of 3.1\\%, with no significant decline\nobserved in other general capabilities.",
      "tldr_zh": "本论文针对自然语言理解（NLU）领域指令短缺的问题，提出一种大规模指令合成方法，以提升大语言模型（LLMs）的 NLU 能力。具体地，研究者构建了 Hum 语料库，该库涵盖信息提取（IE）、机器阅读理解、文本分类和通用任务，并采用人类-LLMs 协作机制来生成多样化的指令，包括指导方针、偏好规则和格式变体。实验结果显示，在 5 个 NLU 任务和 28 个通用能力评估数据集上，Hum 使六种 LLMs 的 NLU 性能平均提升 3.1%，同时未观察到其他通用能力的显著下降。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.03843v1",
      "published_date": "2025-02-06 07:53:40 UTC",
      "updated_date": "2025-02-06 07:53:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:12:24.472036"
    },
    {
      "arxiv_id": "2502.04400v1",
      "title": "Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Keke Gai",
        "Mohan Wang",
        "Jing Yu",
        "Dongjue Wang",
        "Qi Wu"
      ],
      "abstract": "Multimodal Federated Learning (MFL) enables multiple clients to\ncollaboratively train models on multimodal data while ensuring clients'\nprivacy. However, modality and task heterogeneity hinder clients from learning\na unified representation, weakening local model generalization, especially in\nMFL with mixed modalities where only some clients have multimodal data. In this\nwork, we propose an Adaptive prototype-based Multimodal Federated Learning\n(AproMFL) framework for mixed modalities and heterogeneous tasks to address the\naforementioned issues. Our AproMFL transfers knowledge through\nadaptively-constructed prototypes without a prior public dataset. Clients\nadaptively select prototype construction methods in line with tasks; server\nconverts client prototypes into unified multimodal prototypes and aggregates\nthem to form global prototypes, avoid clients keeping unified labels. We divide\nthe model into various modules and only aggregate mapping modules to reduce\ncommunication and computation overhead. To address aggregation issues in\nheterogeneity, we develop a client relationship graph-based scheme to\ndynamically adjust aggregation weights. Extensive experiments on representative\ndatasets evidence effectiveness of AproMFL.",
      "tldr_zh": "本文提出了一种自适应原型知识转移框架 AproMFL，用于解决 Multimodal Federated Learning (MFL) 中混合模态和异质任务带来的统一表示和模型泛化问题。该框架通过自适应构建的原型进行知识转移，无需先验公共数据集，客户端根据任务选择原型构建方法，而服务器则将客户端原型转换为统一的 multimodal 原型并进行聚合，以避免客户端保持统一标签。同时，AproMFL 将模型模块化，仅聚合映射模块以降低通信和计算开销，并采用基于客户端关系图的方案动态调整聚合权重。实验在代表性数据集上证明了 AproMFL 的有效性，显著提升了模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04400v1",
      "published_date": "2025-02-06 07:28:05 UTC",
      "updated_date": "2025-02-06 07:28:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:12:36.927118"
    },
    {
      "arxiv_id": "2502.03827v1",
      "title": "A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiqiang Shi",
        "Ruchit Agrawal"
      ],
      "abstract": "Sentiment Analysis, a popular subtask of Natural Language Processing, employs\ncomputational methods to extract sentiment, opinions, and other subjective\naspects from linguistic data. Given its crucial role in understanding human\nsentiment, research in sentiment analysis has witnessed significant growth in\nthe recent years. However, the majority of approaches are aimed at the English\nlanguage, and research towards Arabic sentiment analysis remains relatively\nunexplored. This paper presents a comprehensive and contemporary survey of\nArabic Sentiment Analysis, identifies the challenges and limitations of\nexisting literature in this field and presents avenues for future research. We\npresent a systematic review of Arabic sentiment analysis methods, focusing\nspecifically on research utilizing deep learning. We then situate Arabic\nSentiment Analysis within the broader context, highlighting research gaps in\nArabic sentiment analysis as compared to general sentiment analysis. Finally,\nwe outline the main challenges and promising future directions for research in\nArabic sentiment analysis.",
      "tldr_zh": "这篇论文对当代阿拉伯语情感分析(Sentiment Analysis)进行了全面调查，强调其作为自然语言处理(Natural Language Processing)子任务的重要性，并指出现有研究主要针对英语，而阿拉伯语领域仍相对未被充分探索。论文系统回顾了阿拉伯语情感分析的方法，特别是利用深度学习(Deep Learning)的研究，并将之与一般情感分析进行比较，突出了研究差距和现有文献的挑战与限制。最后，论文概述了主要挑战和未来研究方向，以推动该领域的进展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted to NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.03827v1",
      "published_date": "2025-02-06 07:23:51 UTC",
      "updated_date": "2025-02-06 07:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:12:47.805728"
    },
    {
      "arxiv_id": "2502.04399v1",
      "title": "Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning",
      "title_zh": "针对 AI-Defined 车辆的在线位置规划：优化订单服务与时空异构模型微",
      "authors": [
        "Bokeng Zheng",
        "Bo Rao",
        "Tianxiang Zhu",
        "Chee Wei Tan",
        "Jingpu Duan",
        "Zhi Zhou",
        "Xu Chen",
        "Xiaoxi Zhang"
      ],
      "abstract": "Advances in artificial intelligence (AI) including foundation models (FMs),\nare increasingly transforming human society, with smart city driving the\nevolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as\na key enabler, leveraging vehicles' mobility and sensor-equipped capabilities.\nIn particular, ride-hailing vehicles can effectively facilitate flexible data\ncollection and contribute towards urban intelligence, despite resource\nlimitations. Therefore, this work explores a promising scenario, where\nedge-assisted vehicles perform joint tasks of order serving and the emerging\nfoundation model fine-tuning using various urban data. However, integrating the\nVCS AI task with the conventional order serving task is challenging, due to\ntheir inconsistent spatio-temporal characteristics: (i) The distributions of\nride orders and data point-of-interests (PoIs) may not coincide in geography,\nboth following a priori unknown patterns; (ii) they have distinct forms of\ntemporal effects, i.e., prolonged waiting makes orders become instantly invalid\nwhile data with increased staleness gradually reduces its utility for model\nfine-tuning.To overcome these obstacles, we propose an online framework based\non multi-agent reinforcement learning (MARL) with careful augmentation. A new\nquality-of-service (QoS) metric is designed to characterize and balance the\nutility of the two joint tasks, under the effects of varying data volumes and\nstaleness. We also integrate graph neural networks (GNNs) with MARL to enhance\nstate representations, capturing graph-structured, time-varying dependencies\namong vehicles and across locations. Extensive experiments on our testbed\nsimulator, utilizing various real-world foundation model fine-tuning tasks and\nthe New York City Taxi ride order dataset, demonstrate the advantage of our\nproposed method.",
      "tldr_zh": "这篇论文探讨了AI定义车辆在执行订单服务和时空异构基础模型微调联合任务时的在线位置规划问题，针对两者在地理分布和时间效应上的不一致挑战。作者提出了一种基于多智能体强化学习 (MARL) 的在线框架，结合图神经网络 (GNNs) 增强状态表示，并设计新的质量服务 (QoS) 指标来平衡任务效用。实验结果显示，该方法在真实数据集上显著优于基线，证明了其在车辆众包 (VCS) 场景中的优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04399v1",
      "published_date": "2025-02-06 07:23:40 UTC",
      "updated_date": "2025-02-06 07:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:13:00.414456"
    },
    {
      "arxiv_id": "2502.03824v3",
      "title": "Syntriever: How to Train Your Retriever with Synthetic Data from LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Minsang Kim",
        "Seungjun Baek"
      ],
      "abstract": "LLMs have boosted progress in many AI applications. Recently, there were\nattempts to distill the vast knowledge of LLMs into information retrieval\nsystems. Those distillation methods mostly use output probabilities of LLMs\nwhich are unavailable in the latest black-box LLMs. We propose Syntriever, a\ntraining framework for retrievers using synthetic data from black-box LLMs.\nSyntriever consists of two stages. Firstly in the distillation stage, we\nsynthesize relevant and plausibly irrelevant passages and augmented queries\nusing chain-of-thoughts for the given queries. LLM is asked to self-verify the\nsynthetic data for possible hallucinations, after which retrievers are trained\nwith a loss designed to cluster the embeddings of relevant passages. Secondly\nin the alignment stage, we align the retriever with the preferences of LLMs. We\npropose a preference modeling called partial Plackett-Luce ranking to learn LLM\npreferences with regularization which prevents the model from deviating\nexcessively from that trained in the distillation stage. Experiments show that\nSyntriever achieves state-of-the-art performances on benchmark datasets from\nvarious domains in nDCG@$K$. The code is available at\n\\href{https://github.com/kmswin1/Syntriever}{https://github.com/kmswin1/Syntriever}.",
      "tldr_zh": "该研究提出了 Syntriever 框架，利用黑箱 LLMs 生成的合成数据来训练检索器，解决传统蒸馏方法依赖输出概率的局限性。框架分为两个阶段：首先在蒸馏阶段，使用 chain-of-thoughts 合成相关和不相关段落及增强查询，并通过 LLM 自验证避免幻觉，然后以聚类嵌入的损失函数训练检索器；其次在对齐阶段，通过部分 Plackett-Luce 排名建模偏好，并添加正则化防止模型过度偏离。实验结果表明，Syntriever 在各种领域的基准数据集上实现了最先进的 nDCG@K 性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "the Nations of the Americas Chapter of the Association for\n  Computational Linguistics (NAACL), Findings, Accepted",
      "pdf_url": "http://arxiv.org/pdf/2502.03824v3",
      "published_date": "2025-02-06 07:19:59 UTC",
      "updated_date": "2025-02-14 01:05:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:13:12.984199"
    },
    {
      "arxiv_id": "2502.04397v2",
      "title": "Multimodal Medical Code Tokenizer",
      "title_zh": "多模态医疗代码标记器",
      "authors": [
        "Xiaorui Su",
        "Shvat Messica",
        "Yepeng Huang",
        "Ruth Johnson",
        "Lukas Fesser",
        "Shanghua Gao",
        "Faryad Sahneh",
        "Marinka Zitnik"
      ],
      "abstract": "Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.",
      "tldr_zh": "该论文提出MedTok，一种多模态医疗代码标记器，它整合了医疗代码的文本描述、本体层次结构以及与其他代码的关系（如疾病共现和药物治疗关联），使用语言模型编码器处理文本并以图编码器编码关系结构，然后将两者量化到统一的标记空间。相比传统标记器，MedTok集成到五个EHR模型中后，在多种临床任务（如结果预测、诊断分类、药物推荐和风险分层）上显著提升性能，AUPRC在MIMIC-III上提高4.10%、MIMIC-IV上提高4.78%、EHRShot上提高11.30%，尤其在药物推荐任务中表现突出。此外，MedTok还适用于医疗QA系统，展示了其作为统一标记器的潜力，提高了医疗基础模型的整体效能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "conference",
      "pdf_url": "http://arxiv.org/pdf/2502.04397v2",
      "published_date": "2025-02-06 06:58:09 UTC",
      "updated_date": "2025-02-12 22:26:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:13:25.411886"
    },
    {
      "arxiv_id": "2502.03814v3",
      "title": "Large Language Models for Multi-Robot Systems: A Survey",
      "title_zh": "大语言模型用于多机器人系统：一个综述",
      "authors": [
        "Peihan Li",
        "Zijian An",
        "Shams Abrar",
        "Lifeng Zhou"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new\npossibilities in Multi-Robot Systems (MRS), enabling enhanced communication,\ntask planning, and human-robot interaction. Unlike traditional single-robot and\nmulti-agent systems, MRS poses unique challenges, including coordination,\nscalability, and real-world adaptability. This survey provides the first\ncomprehensive exploration of LLM integration into MRS. It systematically\ncategorizes their applications across high-level task allocation, mid-level\nmotion planning, low-level action generation, and human intervention. We\nhighlight key applications in diverse domains, such as household robotics,\nconstruction, formation control, target tracking, and robot games, showcasing\nthe versatility and transformative potential of LLMs in MRS. Furthermore, we\nexamine the challenges that limit adapting LLMs in MRS, including mathematical\nreasoning limitations, hallucination, latency issues, and the need for robust\nbenchmarking systems. Finally, we outline opportunities for future research,\nemphasizing advancements in fine-tuning, reasoning techniques, and\ntask-specific models. This survey aims to guide researchers in the intelligence\nand real-world deployment of MRS powered by LLMs. Based on the fast-evolving\nnature of research in the field, we keep updating the papers in the open-source\nGithub repository.",
      "tldr_zh": "这篇调查论文探讨了大型语言模型（LLMs）在多机器人系统（MRS）中的应用，系统分类了其在高层任务分配、中层运动规划、低层动作生成以及人类干预方面的作用，并展示了在家庭机器人、建筑、编队控制、目标跟踪和机器人游戏等领域的实际潜力。\n\n论文强调了 LLMs 如何提升 MRS 的通信、任务规划和人机交互，但也指出了关键挑战，包括数学推理限制、幻觉问题、延迟问题以及缺乏鲁棒基准系统。\n\n未来研究机会包括模型微调、改进推理技术和发展任务特定模型，以推动 LLMs 在 MRS 中的智能部署和实际应用。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03814v3",
      "published_date": "2025-02-06 06:52:14 UTC",
      "updated_date": "2025-02-12 23:25:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:13:36.930265"
    },
    {
      "arxiv_id": "2502.03804v2",
      "title": "Understanding and Supporting Formal Email Exchange by Answering AI-Generated Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Yusuke Miura",
        "Chi-Lan Yang",
        "Masaki Kuribayashi",
        "Keigo Matsumoto",
        "Hideaki Kuzuoka",
        "Shigeo Morishima"
      ],
      "abstract": "Replying to formal emails is time-consuming and cognitively demanding, as it\nrequires crafting polite phrasing and providing an adequate response to the\nsender's demands. Although systems with Large Language Models (LLMs) were\ndesigned to simplify the email replying process, users still need to provide\ndetailed prompts to obtain the expected output. Therefore, we proposed and\nevaluated an LLM-powered question-and-answer (QA)-based approach for users to\nreply to emails by answering a set of simple and short questions generated from\nthe incoming email. We developed a prototype system, ResQ, and conducted\ncontrolled and field experiments with 12 and 8 participants. Our results\ndemonstrated that the QA-based approach improves the efficiency of replying to\nemails and reduces workload while maintaining email quality, compared to a\nconventional prompt-based approach that requires users to craft appropriate\nprompts to obtain email drafts. We discuss how the QA-based approach influences\nthe email reply process and interpersonal relationship dynamics, as well as the\nopportunities and challenges associated with using a QA-based approach in\nAI-mediated communication.",
      "tldr_zh": "本研究针对回复正式邮件的耗时和认知负担问题，提出了一种基于Large Language Models (LLMs)的问答(QA-based)方法，用户只需回答从邮件中生成的简单问题，即可生成回复草案。研究团队开发了原型系统ResQ，并通过控制实验和现场实验（涉及12和8名参与者）进行评估，结果显示QA-based方法相较于传统prompt-based方法，提高了回复效率、降低了工作量，同时保持了邮件质量。论文讨论了这一方法对邮件回复过程和人际关系动态的影响，以及在AI-mediated communication中潜在的机遇和挑战。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03804v2",
      "published_date": "2025-02-06 06:27:09 UTC",
      "updated_date": "2025-02-07 02:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:13:48.409849"
    },
    {
      "arxiv_id": "2502.18480v1",
      "title": "QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration",
      "title_zh": "QExplorer：基于大型语言模型的查询提取，用于有毒内容探索",
      "authors": [
        "Shaola Ren",
        "Li Ke",
        "Longtao Huang",
        "Dehong Gao",
        "Hui Xue"
      ],
      "abstract": "Automatically extracting effective queries is challenging in information\nretrieval, especially in toxic content exploration, as such content is likely\nto be disguised. With the recent achievements in generative Large Language\nModel (LLM), we are able to leverage the capabilities of LLMs to extract\neffective queries for similar content exploration directly. This study proposes\nQExplorer, an approach of large language model based Query Extraction for toxic\ncontent Exploration. The QExplorer approach involves a 2-stage training\nprocess: instruction Supervised FineTuning (SFT) and preference alignment using\nDirect Preference Optimization (DPO), as well as the datasets construction with\nfeedback of search system. To verify the effectiveness of QExplorer, a series\nof offline and online experiments are conducted on our real-world system. The\noffline empirical results demonstrate that the performance of our automatic\nquery extraction outperforms that of several LLMs and humans. The online\ndeployment shows a significant increase in the detection of toxic items.",
      "tldr_zh": "该研究提出 QExplorer，一种基于 Large Language Model (LLM) 的查询提取方法，用于毒性内容探索，以应对信息检索中内容伪装的挑战。QExplorer 采用 2 阶段训练过程，包括 Instruction Supervised FineTuning (SFT) 和 Direct Preference Optimization (DPO)，并通过搜索系统反馈构建数据集。实验结果显示，在离线测试中，QExplorer 的性能优于其他 LLM 和人类；在在线部署中，它显著提高了毒性项目的检测率。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.18480v1",
      "published_date": "2025-02-06 06:11:58 UTC",
      "updated_date": "2025-02-06 06:11:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:14:01.180917"
    },
    {
      "arxiv_id": "2502.03801v1",
      "title": "SoK: Benchmarking Poisoning Attacks and Defenses in Federated Learning",
      "title_zh": "SoK：联邦学习中投毒攻击和防御的基准测试",
      "authors": [
        "Heyi Zhang",
        "Yule Liu",
        "Xinlei He",
        "Jun Wu",
        "Tianshuo Cong",
        "Xinyi Huang"
      ],
      "abstract": "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, but its decentralized nature exposes it to client-side data\npoisoning attacks (DPAs) and model poisoning attacks (MPAs) that degrade global\nmodel performance. While numerous proposed defenses claim substantial\neffectiveness, their evaluation is typically done in isolation with limited\nattack strategies, raising concerns about their validity. Additionally,\nexisting studies overlook the mutual effectiveness of defenses against both\nDPAs and MPAs, causing fragmentation in this field. This paper aims to provide\na unified benchmark and analysis of defenses against DPAs and MPAs, clarifying\nthe distinction between these two similar but slightly distinct domains. We\npresent a systematic taxonomy of poisoning attacks and defense strategies,\noutlining their design, strengths, and limitations. Then, a unified comparative\nevaluation across FL algorithms and data heterogeneity is conducted to validate\ntheir individual and mutual effectiveness and derive key insights for design\nprinciples and future research. Along with the analysis, we frame our work to a\nunified benchmark, FLPoison, with high modularity and scalability to evaluate\n15 representative poisoning attacks and 17 defense strategies, facilitating\nfuture research in this domain. Code is available at\nhttps://github.com/vio1etus/FLPoison.",
      "tldr_zh": "这篇论文（SoK）对Federated Learning (FL) 中的数据中毒攻击 (DPAs) 和模型中毒攻击 (MPAs) 进行了系统基准测试，强调了现有防御策略的评估不足，如孤立评估和忽略相互有效性。作者构建了一个统一框架，包括攻击和防御策略的分类，以及跨FL算法和数据异质性的比较评估，以验证防御的有效性和揭示设计原则。最终，论文推出了FLPoison基准工具，支持15种代表性攻击和17种防御策略，并提供开源代码（https://github.com/vio1etus/FLPoison），为未来研究提供模块化、可扩展的平台。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03801v1",
      "published_date": "2025-02-06 06:05:00 UTC",
      "updated_date": "2025-02-06 06:05:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:14:12.839716"
    },
    {
      "arxiv_id": "2502.05227v1",
      "title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Gonzalo Gonzalez-Pumariega",
        "Leong Su Yean",
        "Neha Sunkara",
        "Sanjiban Choudhury"
      ],
      "abstract": "Effective asynchronous planning, or the ability to efficiently reason and\nplan over states and actions that must happen in parallel or sequentially, is\nessential for agents that must account for time delays, reason over diverse\nlong-horizon tasks, and collaborate with other agents. While large language\nmodel (LLM) agents show promise in high-level task planning, current benchmarks\nfocus primarily on short-horizon tasks and do not evaluate such asynchronous\nplanning capabilities. We introduce Robotouille, a challenging benchmark\nenvironment designed to test LLM agents' ability to handle long-horizon\nasynchronous scenarios. Our synchronous and asynchronous datasets capture\nincreasingly complex planning challenges that go beyond existing benchmarks,\nrequiring agents to manage overlapping tasks and interruptions. Our results\nshow that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on\nasynchronous tasks, highlighting significant room for improvement. We further\nanalyze failure modes, demonstrating the need for LLM agents to better\nincorporate long-horizon feedback and self-audit their reasoning during task\nexecution. Code is available at https://github.com/portal-cornell/robotouille.",
      "tldr_zh": "这篇论文引入了 Robotouille，一个针对 LLM Agents 的异步规划基准环境，用于评估代理在处理并行或顺序状态、时间延迟以及长时域任务时的能力。Robotouille 通过同步和异步数据集设计了更复杂的规划挑战，包括管理重叠任务和中断，超越了现有基准的局限。实验结果显示，ReAct (gpt4-o) 在同步任务上达到47%的成功率，但异步任务仅为11%，突显了 LLM Agents 在长时域反馈和自我审计推理方面的不足。该基准为提升代理的异步规划性能提供了宝贵工具。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages (not including references or appendix); 41 figures (7 main\n  paper, 34 appendix); (v1) preprint",
      "pdf_url": "http://arxiv.org/pdf/2502.05227v1",
      "published_date": "2025-02-06 05:50:37 UTC",
      "updated_date": "2025-02-06 05:50:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:14:24.353938"
    },
    {
      "arxiv_id": "2502.03793v2",
      "title": "It's All in The [MASK]: Simple Instruction-Tuning Enables BERT-like Masked Language Models As Generative Classifiers",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Clavié",
        "Nathan Cooper",
        "Benjamin Warner"
      ],
      "abstract": "While encoder-only models such as BERT and ModernBERT are ubiquitous in\nreal-world NLP applications, their conventional reliance on task-specific\nclassification heads can limit their applicability compared to decoder-based\nlarge language models (LLMs). In this work, we introduce\nModernBERT-Large-Instruct, a 0.4B-parameter encoder model that leverages its\nmasked language modelling (MLM) head for generative classification. Our\napproach employs an intentionally simple training loop and inference mechanism\nthat requires no heavy pre-processing, heavily engineered prompting, or\narchitectural modifications. ModernBERT-Large-Instruct exhibits strong\nzero-shot performance on both classification and knowledge-based tasks,\noutperforming similarly sized LLMs on MMLU and achieving 93% of Llama3-1B's\nMMLU performance with 60% less parameters. We also demonstrate that, when\nfine-tuned, the generative approach using the MLM head matches or even\nsurpasses traditional classification-head methods across diverse NLU tasks.This\ncapability emerges specifically in models trained on contemporary, diverse data\nmixes, with models trained on lower volume, less-diverse data yielding\nconsiderably weaker performance. Although preliminary, these results\ndemonstrate the potential of using the original generative masked language\nmodelling head over traditional task-specific heads for downstream tasks. Our\nwork suggests that further exploration into this area is warranted,\nhighlighting many avenues for future improvements.",
      "tldr_zh": "本文提出 ModernBERT-Large-Instruct，一种0.4B参数的encoder-only模型，通过简单指令微调(instruction-tuning)，利用masked language modelling (MLM)头实现生成式分类，而非依赖传统任务特定分类头。实验显示，该模型在zero-shot任务上表现出色，在MMLU基准上超越类似大小的LLMs，并达到Llama3-1B的93%性能，同时参数减少60%。当微调后，使用MLM头的生成式方法在各种NLU任务上匹配或超过传统方法，这主要依赖于模型训练于当代多样化数据；研究建议此方法有潜力应用于下游任务，并呼吁进一步探索。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03793v2",
      "published_date": "2025-02-06 05:47:37 UTC",
      "updated_date": "2025-02-10 14:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:14:37.214013"
    },
    {
      "arxiv_id": "2502.08474v1",
      "title": "Training-Free Restoration of Pruned Neural Networks",
      "title_zh": "无需训练的修剪神经网络恢复",
      "authors": [
        "Keonho Lee",
        "Minsoo Kim",
        "Dong-Wan Choi"
      ],
      "abstract": "Although network pruning has been highly popularized to compress deep neural\nnetworks, its resulting accuracy heavily depends on a fine-tuning process that\nis often computationally expensive and requires the original data. However,\nthis may not be the case in real-world scenarios, and hence a few recent works\nattempt to restore pruned networks without any expensive retraining process.\nTheir strong assumption is that every neuron being pruned can be replaced with\nanother one quite similar to it, but unfortunately this does not hold in many\nneural networks, where the similarity between neurons is extremely low in some\nlayers. In this article, we propose a more rigorous and robust method of\nrestoring pruned networks in a fine-tuning free and data-free manner, called\nLBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned\nassumption in a way that each pruned neuron leaves its pieces of information to\nas many preserved neurons as possible and thereby multiple neurons together\nobtain a more robust approximation to the original output of the neuron who\njust left. Our method is based on a theoretical analysis on how to formulate\nthe reconstruction error between the original network and its approximation,\nwhich nicely leads to a closed form solution for our derived loss function.\nThrough the extensive experiments, LBYL is confirmed to be indeed more\neffective to approximate the original network and consequently able to achieve\nhigher accuracy for restored networks, compared to the recent approaches\nexploiting the similarity between two neurons. The very first version of this\nwork, which contains major technical and theoretical components, was submitted\nto NeurIPS 2021 and ICML 2022.",
      "tldr_zh": "本文提出了一种无需微调和原始数据的神经网络恢复方法，名为 LBYL (Leave Before You Leave)，以解决网络剪枝后准确率依赖昂贵重训的问题。LBYL 放宽了现有方法的神经元相似性假设，让每个被剪枝的神经元将信息分发给多个保留神经元，从而实现更鲁棒的输出近似。该方法基于对重建误差的理论分析，导出了一个闭式解的损失函数。实验结果表明，LBYL 在恢复剪枝 neural networks 时比基于神经元相似性的方法更有效，显著提高了准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Under Review in TNNLS since May 2022",
      "pdf_url": "http://arxiv.org/pdf/2502.08474v1",
      "published_date": "2025-02-06 05:30:48 UTC",
      "updated_date": "2025-02-06 05:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:14:48.449852"
    },
    {
      "arxiv_id": "2502.05225v1",
      "title": "BitAbuse: A Dataset of Visually Perturbed Texts for Defending Phishing Attacks",
      "title_zh": "翻译失败",
      "authors": [
        "Hanyong Lee",
        "Chaelyn Lee",
        "Yongjae Lee",
        "Jaesung Lee"
      ],
      "abstract": "Phishing often targets victims through visually perturbed texts to bypass\nsecurity systems. The noise contained in these texts functions as an\nadversarial attack, designed to deceive language models and hinder their\nability to accurately interpret the content. However, since it is difficult to\nobtain sufficient phishing cases, previous studies have used synthetic datasets\nthat do not contain real-world cases. In this study, we propose the BitAbuse\ndataset, which includes real-world phishing cases, to address the limitations\nof previous research. Our dataset comprises a total of 325,580 visually\nperturbed texts. The dataset inputs are drawn from the raw corpus, consisting\nof visually perturbed sentences and sentences generated through an artificial\nperturbation process. Each input sentence is labeled with its corresponding\nground truth, representing the restored, non-perturbed version. Language models\ntrained on our proposed dataset demonstrated significantly better performance\ncompared to previous methods, achieving an accuracy of approximately 96%. Our\nanalysis revealed a significant gap between real-world and synthetic examples,\nunderscoring the value of our dataset for building reliable pre-trained models\nfor restoration tasks. We release the BitAbuse dataset, which includes\nreal-world phishing cases annotated with visual perturbations, to support\nfuture research in adversarial attack defense.",
      "tldr_zh": "本文提出 BitAbuse dataset，这是一个包含 325,580 个真实世界 phishing attacks 案例的视觉扰动文本数据集，用于帮助语言模型防御 adversarial attacks。数据集包括原始语料中的视觉扰动句子和人工生成的扰动句子，每个输入均标注了对应的非扰动 ground truth 版本。相比先前使用合成数据集的方法，在 BitAbuse 上训练的语言模型准确率达到约 96%，显著提升了文本恢复性能。研究还揭示了真实案例与合成案例的显著差距，并发布该数据集以支持未来对抗攻击防御研究。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "18 pages, To appear in the Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.05225v1",
      "published_date": "2025-02-06 05:04:04 UTC",
      "updated_date": "2025-02-06 05:04:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:15:00.840781"
    },
    {
      "arxiv_id": "2502.05224v1",
      "title": "A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations",
      "title_zh": "翻译失败",
      "authors": [
        "Yihe Zhou",
        "Tao Ni",
        "Wei-Bin Lee",
        "Qingchuan Zhao"
      ],
      "abstract": "Large Language Models (LLMs) have achieved significantly advanced\ncapabilities in understanding and generating human language text, which have\ngained increasing popularity over recent years. Apart from their\nstate-of-the-art natural language processing (NLP) performance, considering\ntheir widespread usage in many industries, including medicine, finance,\neducation, etc., security concerns over their usage grow simultaneously. In\nrecent years, the evolution of backdoor attacks has progressed with the\nadvancement of defense mechanisms against them and more well-developed features\nin the LLMs. In this paper, we adapt the general taxonomy for classifying\nmachine learning attacks on one of the subdivisions - training-time white-box\nbackdoor attacks. Besides systematically classifying attack methods, we also\nconsider the corresponding defense methods against backdoor attacks. By\nproviding an extensive summary of existing works, we hope this survey can serve\nas a guideline for inspiring future research that further extends the attack\nscenarios and creates a stronger defense against them for more robust LLMs.",
      "tldr_zh": "这篇调查论文探讨了 Large Language Models (LLMs) 中的后门威胁，包括攻击、防御和评估方法，随着这些模型在医疗、金融、教育等行业的广泛应用，其安全问题日益突出。论文对机器学习攻击的分类体系进行了适应，重点系统化了训练时白盒后门攻击，并总结了相应的防御机制。最终，该工作旨在为现有研究的全面概述提供指导，推动未来对攻击场景的扩展和更强防御策略的开发，以提升 LLMs 的鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.05224v1",
      "published_date": "2025-02-06 04:43:05 UTC",
      "updated_date": "2025-02-06 04:43:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:15:12.373954"
    },
    {
      "arxiv_id": "2503.04747v1",
      "title": "E-LENS: User Requirements-Oriented AI Ethics Assurance",
      "title_zh": "E-LENS：用户需求导向的 AI 伦理保障",
      "authors": [
        "Jianlong Zhou",
        "Fang Chen"
      ],
      "abstract": "Despite the much proliferation of AI ethical principles in recent years,\nthere is a challenge of assuring AI ethics with current AI ethics frameworks in\nreal-world applications. While system safety has emerged as a distinct\ndiscipline for a long time, originated from safety concerns in early aircraft\nmanufacturing. The safety assurance is now an indispensable component in safety\ncritical domains. Motivated by the assurance approaches for safety-critical\nsystems such as aviation, this paper introduces the concept of AI ethics\nassurance cases into the AI ethics assurance. Three pillars of user\nrequirements, evidence, and validation are proposed as key components and\nintegrated into AI ethics assurance cases for a new approach of user\nrequirements-oriented AI ethics assurance. The user requirements-oriented AI\nethics assurance case is set up based on three pillars and hazard analysis\nmethods used in the safety assurance of safety-critical systems. This paper\nalso proposes a platform named Ethical-Lens (E-LENS) to implement the user\nrequirements-oriented AI ethics assurance approach. The proposed user\nrequirements-based E-LENS platform is then applied to assure AI ethics of an\nAI-driven human resource shortlisting system as a case study to show the\neffectiveness of the proposed approach.",
      "tldr_zh": "该论文针对AI伦理原则在实际应用中的保障挑战，借鉴安全关键系统（如航空领域）的安全保障方法，引入了AI伦理保障案例（AI ethics assurance cases）概念。论文提出以用户需求（user requirements）、证据（evidence）和验证（validation）作为三大支柱，构建用户需求导向的AI伦理保障框架，并结合hazard analysis方法进行实施。此外，论文开发了名为E-LENS的平台，并通过一个AI驱动的人力资源筛选系统案例研究，证明了该方法在提升AI伦理保障有效性方面的实际价值。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "29 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.04747v1",
      "published_date": "2025-02-06 04:37:55 UTC",
      "updated_date": "2025-02-06 04:37:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:15:23.909798"
    },
    {
      "arxiv_id": "2502.03773v1",
      "title": "ExpProof : Operationalizing Explanations for Confidential Models with ZKPs",
      "title_zh": "翻译失败",
      "authors": [
        "Chhavi Yadav",
        "Evan Monroe Laufer",
        "Dan Boneh",
        "Kamalika Chaudhuri"
      ],
      "abstract": "In principle, explanations are intended as a way to increase trust in machine\nlearning models and are often obligated by regulations. However, many\ncircumstances where these are demanded are adversarial in nature, meaning the\ninvolved parties have misaligned interests and are incentivized to manipulate\nexplanations for their purpose. As a result, explainability methods fail to be\noperational in such settings despite the demand \\cite{bordt2022post}. In this\npaper, we take a step towards operationalizing explanations in adversarial\nscenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.\nSpecifically we explore ZKP-amenable versions of the popular explainability\nalgorithm LIME and evaluate their performance on Neural Networks and Random\nForests.",
      "tldr_zh": "该论文探讨了在对抗性环境中，机器学习模型的解释（explanations）容易被操纵的问题，导致其无法有效操作化。研究提出 ExpProof 框架，利用 Zero-Knowledge Proofs (ZKPs) 这一加密原语来实现解释的可操作性，并开发了 ZKP 兼容版本的 LIME 算法。实验结果显示，该方法在 Neural Networks 和 Random Forests 上表现出色，为对抗场景下的可信解释提供了一种新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03773v1",
      "published_date": "2025-02-06 04:24:29 UTC",
      "updated_date": "2025-02-06 04:24:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:15:35.760035"
    },
    {
      "arxiv_id": "2502.03772v2",
      "title": "A Retrospective Systematic Study on Hierarchical Sparse Query Transformer-assisted Ultrasound Screening for Early Hepatocellular Carcinoma",
      "title_zh": "翻译失败",
      "authors": [
        "Chaoyin She",
        "Ruifang Lu",
        "Danni He",
        "Jiayi Lv",
        "Yadan Lin",
        "Meiqing Cheng",
        "Hui Huang",
        "Fengyu Ye",
        "Lida Chen",
        "Wei Wang",
        "Qinghua Huang"
      ],
      "abstract": "Hepatocellular carcinoma (HCC), ranking as the third leading cause of\ncancer-related mortality worldwide, demands urgent improvements in early\ndetection to enhance patient survival. While ultrasound remains the preferred\nscreening modality due to its cost-effectiveness and real-time capabilities,\nits sensitivity (59%-78%) heavily relies on radiologists' expertise, leading to\ninconsistent diagnostic outcomes and operational inefficiencies. Recent\nadvancements in AI technology offer promising solutions to bridge this gap.\nThis study introduces the Hierarchical Sparse Query Transformer (HSQformer), a\nnovel hybrid architecture that synergizes CNNs' local feature extraction with\nVision Transformers' global contextual awareness through latent space\nrepresentation and sparse learning. By dynamically activating task-specific\nexperts via a Mixture-of-Experts (MoE) framework, HSQformer achieves\nhierarchical feature integration without structural redundancy. Evaluated\nacross three clinical scenarios: single-center, multi-center, and high-risk\npatient cohorts, HSQformer outperforms state-of-the-art models (e.g., 95.38%\nAUC in multi-center testing) and matches senior radiologists' diagnostic\naccuracy while significantly surpassing junior counterparts. These results\nhighlight the potential of AI-assisted tools to standardize HCC screening,\nreduce dependency on human expertise, and improve early diagnosis rates. The\nfull code is available at https://github.com/Asunatan/HSQformer.",
      "tldr_zh": "本研究针对肝细胞癌 (HCC) 早期检测的挑战，提出 Hierarchical Sparse Query Transformer (HSQformer)，这是一种结合 CNN 的局部特征提取和 Vision Transformers 的全局上下文感知的混合架构，通过 Mixture-of-Experts (MoE) 框架动态激活特定专家，实现高效的层次特征整合并减少结构冗余。HSQformer 在单中心、多中心和高风险患者队列的临床评估中，超越了现有模型（如多中心测试中 AUC 达 95.38%），并与资深放射科医生的诊断准确性相当，同时显著优于初级医生。总体结果表明，该 AI 辅助工具可标准化 HCC 超声筛查，降低对人类专家的依赖，并提升早期诊断率，代码已在 GitHub 开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03772v2",
      "published_date": "2025-02-06 04:17:02 UTC",
      "updated_date": "2025-03-20 06:38:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:15:49.247163"
    },
    {
      "arxiv_id": "2502.04394v1",
      "title": "DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease",
      "title_zh": "翻译失败",
      "authors": [
        "Tingyu Mo",
        "Jacqueline C. K. Lam",
        "Victor O. K. Li",
        "Lawrence Y. L. Cheung"
      ],
      "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\naffecting 50 million people worldwide. Low-cost, accurate identification of key\nmarkers of AD is crucial for timely diagnosis and intervention. Language\nimpairment is one of the earliest signs of cognitive decline, which can be used\nto discriminate AD patients from normal control individuals.\nPatient-interviewer dialogues may be used to detect such impairments, but they\nare often mixed with ambiguous, noisy, and irrelevant information, making the\nAD detection task difficult. Moreover, the limited availability of AD speech\nsamples and variability in their speech styles pose significant challenges in\ndeveloping robust speech-based AD detection models. To address these\nchallenges, we propose DECT, a novel speech-based domain-specific approach\nleveraging large language models (LLMs) for fine-grained linguistic analysis\nand label-switched label-preserved data generation. Our study presents four\nnovelties: We harness the summarizing capabilities of LLMs to identify and\ndistill key Cognitive-Linguistic information from noisy speech transcripts,\neffectively filtering irrelevant information. We leverage the inherent\nlinguistic knowledge of LLMs to extract linguistic markers from unstructured\nand heterogeneous audio transcripts. We exploit the compositional ability of\nLLMs to generate AD speech transcripts consisting of diverse linguistic\npatterns to overcome the speech data scarcity challenge and enhance the\nrobustness of AD detection models. We use the augmented AD textual speech\ntranscript dataset and a more fine-grained representation of AD textual speech\ntranscript data to fine-tune the AD detection model. The results have shown\nthat DECT demonstrates superior model performance with an 11% improvement in AD\ndetection accuracy on the datasets from DementiaBank compared to the baselines.",
      "tldr_zh": "本文提出 DECT 方法，利用大型语言模型 (LLMs) 辅助细粒度语言知识提取和标签切换/保留数据生成，旨在解决阿尔茨海默病 (AD) 诊断中的噪声信息和数据稀缺挑战。DECT 的四个创新包括：使用 LLMs 总结并过滤语音转录中的关键认知-语言信息、提取语言标记、生成多样化 AD 语音数据以增强模型鲁棒性，以及基于增强数据集进行细粒度微调。实验结果显示，在 DementiaBank 数据集上，DECT 比基线模型的 AD 检测准确率提高了 11%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04394v1",
      "published_date": "2025-02-06 04:00:25 UTC",
      "updated_date": "2025-02-06 04:00:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:16:01.741143"
    },
    {
      "arxiv_id": "2502.03752v2",
      "title": "PRISM: A Robust Framework for Skill-based Meta-Reinforcement Learning with Noisy Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Sanghyeon Lee",
        "Sangjun Bae",
        "Yisak Park",
        "Seungyul Han"
      ],
      "abstract": "Meta-reinforcement learning (Meta-RL) facilitates rapid adaptation to unseen\ntasks but faces challenges in long-horizon environments. Skill-based approaches\ntackle this by decomposing state-action sequences into reusable skills and\nemploying hierarchical decision-making. However, these methods are highly\nsusceptible to noisy offline demonstrations, resulting in unstable skill\nlearning and degraded performance. To overcome this, we propose Prioritized\nRefinement for Skill-Based Meta-RL (PRISM), a robust framework that integrates\nexploration near noisy data to generate online trajectories and combines them\nwith offline data. Through prioritization, PRISM extracts high-quality data to\nlearn task-relevant skills effectively. By addressing the impact of noise, our\nmethod ensures stable skill learning and achieves superior performance in\nlong-horizon tasks, even with noisy and sub-optimal data.",
      "tldr_zh": "该论文针对元强化学习(Meta-RL)在大周期环境中面临的挑战，提出PRISM框架，这是一种基于技能的鲁棒方法，能够处理噪声离线演示导致的不稳定问题。PRISM通过在噪声数据附近进行探索生成在线轨迹，并将其与离线数据结合，通过优先化机制提取高质量数据，从而有效学习任务相关技能。该框架显著提升了技能学习的稳定性和性能，在长周期任务中即使面对噪声和次优数据时，也表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages main, 19 pages appendix with reference. Submitted to ICML\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2502.03752v2",
      "published_date": "2025-02-06 03:28:45 UTC",
      "updated_date": "2025-02-14 11:02:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:16:12.707834"
    },
    {
      "arxiv_id": "2502.03750v1",
      "title": "Principal Curvatures Estimation with Applications to Single Cell Data",
      "title_zh": "主曲率估计及其在单细胞数据中的应用",
      "authors": [
        "Yanlei Zhang",
        "Lydia Mezrag",
        "Xingzhi Sun",
        "Charles Xu",
        "Kincaid Macdonald",
        "Dhananjay Bhaskar",
        "Smita Krishnaswamy",
        "Guy Wolf",
        "Bastian Rieck"
      ],
      "abstract": "The rapidly growing field of single-cell transcriptomic sequencing (scRNAseq)\npresents challenges for data analysis due to its massive datasets. A common\nmethod in manifold learning consists in hypothesizing that datasets lie on a\nlower dimensional manifold. This allows to study the geometry of point clouds\nby extracting meaningful descriptors like curvature. In this work, we will\npresent Adaptive Local PCA (AdaL-PCA), a data-driven method for accurately\nestimating various notions of intrinsic curvature on data manifolds, in\nparticular principal curvatures for surfaces. The model relies on local PCA to\nestimate the tangent spaces. The evaluation of AdaL-PCA on sampled surfaces\nshows state-of-the-art results. Combined with a PHATE embedding, the model\napplied to single-cell RNA sequencing data allows us to identify key variations\nin the cellular differentiation.",
      "tldr_zh": "这篇论文提出 Adaptive Local PCA (AdaL-PCA)，一种数据驱动方法，用于精确估计数据流形上的 principal curvatures，特别是针对单细胞转录组测序 (scRNAseq) 的大规模数据集。方法通过局部 PCA 估计切平面，帮助分析点云几何。实验结果显示，AdaL-PCA 在采样表面上达到了最先进水平，并结合 PHATE 嵌入应用于 scRNAseq 数据，成功识别细胞分化中的关键变化。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in ICASSP 2025-2025 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)",
      "pdf_url": "http://arxiv.org/pdf/2502.03750v1",
      "published_date": "2025-02-06 03:23:31 UTC",
      "updated_date": "2025-02-06 03:23:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:16:24.946472"
    },
    {
      "arxiv_id": "2502.03740v1",
      "title": "Multiple Invertible and Partial-Equivariant Function for Latent Vector Transformation to Enhance Disentanglement in VAEs",
      "title_zh": "翻译失败",
      "authors": [
        "Hee-Jun Jung",
        "Jaehyoung Jeong",
        "Kangil Kim"
      ],
      "abstract": "Disentanglement learning is a core issue for understanding and re-using\ntrained information in Variational AutoEncoder (VAE), and effective inductive\nbias has been reported as a key factor. However, the actual implementation of\nsuch bias is still vague. In this paper, we propose a novel method, called\nMultiple Invertible and partial-equivariant transformation\n(MIPE-transformation), to inject inductive bias by 1) guaranteeing the\ninvertibility of latent-to-latent vector transformation while preserving a\ncertain portion of equivariance of input-to-latent vector transformation,\ncalled Invertible and partial-equivariant transformation (IPE-transformation),\n2) extending the form of prior and posterior in VAE frameworks to an\nunrestricted form through a learnable conversion to an approximated exponential\nfamily, called Exponential Family conversion (EF-conversion), and 3)\nintegrating multiple units of IPE-transformation and EF-conversion, and their\ntraining. In experiments on 3D Cars, 3D Shapes, and dSprites datasets,\nMIPE-transformation improves the disentanglement performance of\nstate-of-the-art VAEs.",
      "tldr_zh": "本论文针对 Variational AutoEncoder (VAE) 中的 disentanglement 学习问题，提出了一种新方法 Multiple Invertible and Partial-Equivariant transformation (MIPE-transformation)，旨在通过注入 inductive bias 来提升潜在向量的解耦效果。该方法包括 Invertible and Partial-Equivariant transformation (IPE-transformation) 来确保潜在向量转换的可逆性并保留部分输入到潜在向量的等变性，以及 Exponential Family conversion (EF-conversion) 来将 VAE 的先验和后验扩展到近似指数族的无限制形式，并整合多个单元进行训练。在 3D Cars、3D Shapes 和 dSprites 数据集的实验中，MIPE-transformation 显著提高了最先进 VAE 的 disentanglement 性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "24 pages, 21 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.03740v1",
      "published_date": "2025-02-06 03:08:12 UTC",
      "updated_date": "2025-02-06 03:08:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:16:37.465478"
    },
    {
      "arxiv_id": "2502.03729v2",
      "title": "Action-Free Reasoning for Policy Generalization",
      "title_zh": "翻译失败",
      "authors": [
        "Jaden Clark",
        "Suvir Mirchandani",
        "Dorsa Sadigh",
        "Suneel Belkhale"
      ],
      "abstract": "End-to-end imitation learning offers a promising approach for training robot\npolicies. However, generalizing to new settings remains a significant\nchallenge. Although large-scale robot demonstration datasets have shown\npotential for inducing generalization, they are resource-intensive to scale. In\ncontrast, human video data is abundant and diverse, presenting an attractive\nalternative. Yet, these human-video datasets lack action labels, complicating\ntheir use in imitation learning. Existing methods attempt to extract grounded\naction representations (e.g., hand poses), but resulting policies struggle to\nbridge the embodiment gap between human and robot actions. We propose an\nalternative approach: leveraging language-based reasoning from human\nvideos-essential for guiding robot actions-to train generalizable robot\npolicies. Building on recent advances in reasoning-based policy architectures,\nwe introduce Reasoning through Action-free Data (RAD). RAD learns from both\nrobot demonstration data (with reasoning and action labels) and action-free\nhuman video data (with only reasoning labels). The robot data teaches the model\nto map reasoning to low-level actions, while the action-free data enhances\nreasoning capabilities. Additionally, we will release a new dataset of 3,377\nhuman-hand demonstrations with reasoning annotations compatible with the Bridge\nV2 benchmark and aimed at facilitating future research on reasoning-driven\nrobot learning. Our experiments show that RAD enables effective transfer across\nthe embodiment gap, allowing robots to perform tasks seen only in action-free\ndata. Furthermore, scaling up action-free reasoning data significantly improves\npolicy performance and generalization to novel tasks. These results highlight\nthe promise of reasoning-driven learning from action-free datasets for\nadvancing generalizable robot control. Project page:\nhttps://rad-generalization.github.io",
      "tldr_zh": "该论文探讨了端到端 imitation learning 在机器人政策训练中的泛化挑战，提出了一种名为 RAD (Reasoning through Action-free Data) 的方法，利用语言-based reasoning 从丰富的人类视频数据中学习，而无需动作标签。RAD 通过机器人演示数据（包含 reasoning 和 action 标签）训练模型将 reasoning 映射到低级动作，并用行动-free 数据增强 reasoning 能力；此外，作者将发布一个包含 3,377 个人类手演示的 new dataset，与 Bridge V2 基准兼容。实验结果显示，RAD 有效桥接 embodiment gap，使机器人能执行仅在行动-free 数据中见过的任务，并通过扩展 reasoning 数据显著提升政策性能和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.03729v2",
      "published_date": "2025-02-06 02:43:23 UTC",
      "updated_date": "2025-02-11 04:51:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:16:50.430851"
    },
    {
      "arxiv_id": "2502.04392v1",
      "title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents",
      "title_zh": "Division-of-Thoughts：利用混合语言模型协同作用实现高效设备端代理",
      "authors": [
        "Chenyang Shao",
        "Xinyuan Hu",
        "Yutang Lin",
        "Fengli Xu"
      ],
      "abstract": "The rapid expansion of web content has made on-device AI assistants\nindispensable for helping users manage the increasing complexity of online\ntasks. The emergent reasoning ability in large language models offer a\npromising path for next-generation on-device AI agents. However, deploying\nfull-scale Large Language Models (LLMs) on resource-limited local devices is\nchallenging. In this paper, we propose Division-of-Thoughts (DoT), a\ncollaborative reasoning framework leveraging the synergy between locally\ndeployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT\nleverages a Task Decomposer to elicit the inherent planning abilities in\nlanguage models to decompose user queries into smaller sub-tasks, which allows\nhybrid language models to fully exploit their respective strengths. Besides,\nDoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks\nand create a dependency graph, facilitating parallel reasoning of sub-tasks and\nthe identification of key steps. To allocate the appropriate model based on the\ndifficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an\nadditional task head attached to the SLM that does not alter the SLM's\nparameters. To boost adapter's task allocation capability, we propose a\nself-reinforced training method that relies solely on task execution feedback.\nExtensive experiments on various benchmarks demonstrate that our DoT\nsignificantly reduces LLM costs while maintaining competitive reasoning\naccuracy. Specifically, DoT reduces the average reasoning time and API costs by\n66.12% and 83.57%, while achieving comparable reasoning accuracy with the best\nbaseline methods.",
      "tldr_zh": "本文提出 Division-of-Thoughts (DoT)，一个混合语言模型协同框架，结合本地 Smaller-scale Language Models (SLMs) 和云端 Large Language Models (LLMs)，以提升 on-device AI 代理的推理效率。DoT 通过 Task Decomposer 分解用户查询为子任务、Task Scheduler 分析依赖关系并支持并行处理，以及 Plug-and-Play Adapter 根据子任务难度动态分配模型，并采用自强化训练方法优化分配能力。实验结果显示，DoT 平均减少推理时间 66.12% 和 API 成本 83.57%，同时保持与最佳基线方法的推理准确性相当。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04392v1",
      "published_date": "2025-02-06 02:40:25 UTC",
      "updated_date": "2025-02-06 02:40:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:17:01.935281"
    },
    {
      "arxiv_id": "2502.06836v1",
      "title": "CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jaewan Lee",
        "Changyoung Park",
        "Hongjun Yang",
        "Sungbin Lim",
        "Sehui Han"
      ],
      "abstract": "Recent advancements in AI have revolutionized property prediction in\nmaterials science and accelerating material discovery. Graph neural networks\n(GNNs) stand out due to their ability to represent crystal structures as\ngraphs, effectively capturing local interactions and delivering superior\npredictions. However, these methods often lose critical global information,\nsuch as crystal systems and repetitive unit connectivity. To address this, we\npropose CAST, a cross-attention-based multimodal fusion model that integrates\ngraph and text modalities to preserve essential material information. CAST\ncombines node- and token-level features using cross-attention mechanisms,\nsurpassing previous approaches reliant on material-level embeddings like graph\nmean-pooling or [CLS] tokens. A masked node prediction pretraining strategy\nfurther enhances atomic-level information integration. Our method achieved up\nto 22.9\\% improvement in property prediction across four crystal properties\nincluding band gap compared to methods like CrysMMNet and MultiMat. Pretraining\nwas key to aligning node and text embeddings, with attention maps confirming\nits effectiveness in capturing relationships between nodes and tokens. This\nstudy highlights the potential of multimodal learning in materials science,\npaving the way for more robust predictive models that incorporate both local\nand global information.",
      "tldr_zh": "本研究提出CAST，一种基于跨注意力(cross-attention)机制的多模态融合模型，用于材料属性预测，通过整合图神经网络(GNNs)表示的晶体结构和文本信息，解决传统方法丢失全局信息（如晶体系统和重复单元连接）的问题。CAST通过节点级和标记级特征的跨注意力融合，优于依赖材料级嵌入（如图均池化或[CLS]标记）的先前方法，并采用掩码节点预测预训练策略来增强原子级信息整合。实验结果显示，CAST在包括带隙在内的四个晶体属性预测上，比CrysMMNet和MultiMat等方法提高了高达22.9%的性能，预训练有助于对齐节点和文本嵌入，并通过注意力图证实了节点与标记之间的关系。该方法突显了多模态学习在材料科学的潜力，提供更稳健的模型来结合局部和全局信息。",
      "categories": [
        "cs.LG",
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.06836v1",
      "published_date": "2025-02-06 02:29:39 UTC",
      "updated_date": "2025-02-06 02:29:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:17:13.609470"
    },
    {
      "arxiv_id": "2502.03724v1",
      "title": "MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream Fusion and Temporal Modeling",
      "title_zh": "翻译失败",
      "authors": [
        "Sharana Dharshikgan Suresh Dass",
        "Hrishav Bakul Barua",
        "Ganesh Krishnasamy",
        "Raveendran Paramesran",
        "Raphael C. -W. Phan"
      ],
      "abstract": "Action recognition in dark, low-light (under-exposed) or noisy videos is a\nchallenging task due to visibility degradation, which can hinder critical\nspatiotemporal details. This paper proposes MD-BERT, a novel multi-stream\napproach that integrates complementary pre-processing techniques such as gamma\ncorrection and histogram equalization alongside raw dark frames to address\nthese challenges. We introduce the Dynamic Feature Fusion (DFF) module,\nextending existing attentional fusion methods to a three-stream setting,\nthereby capturing fine-grained and global contextual information across\ndifferent brightness and contrast enhancements. The fused spatiotemporal\nfeatures are then processed by a BERT-based temporal model, which leverages its\nbidirectional self-attention to effectively capture long-range dependencies and\ncontextual relationships across frames. Extensive experiments on the ARID V1.0\nand ARID V1.5 dark video datasets show that MD-BERT outperforms existing\nmethods, establishing a new state-of-the-art performance. Ablation studies\nfurther highlight the individual contributions of each input stream and the\neffectiveness of the proposed DFF and BERT modules. The official website of\nthis work is available at: https://github.com/HrishavBakulBarua/DarkBERT",
      "tldr_zh": "这篇论文提出了MD-BERT，一种针对黑暗视频动作识别的多流方法，通过整合gamma校正、直方图均衡和原始黑暗帧作为输入，解决可见性降低带来的挑战。论文引入了Dynamic Feature Fusion (DFF)模块来融合三流特征，捕捉细粒度和全局上下文信息，并使用BERT-based temporal model通过双向自注意力机制处理时空特征，捕捉帧间的长程依赖。实验在ARID V1.0和ARID V1.5数据集上显示，MD-BERT超越现有方法，建立了新的最先进性能，并通过消融研究验证了各输入流和DFF模块的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.MM",
        "Artificial intelligence, Computer vision, Machine learning, Deep\n  learning, Human-computer Interaction",
        "I.2; I.2.9; I.2.10; I.3.3; I.4.5"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03724v1",
      "published_date": "2025-02-06 02:26:47 UTC",
      "updated_date": "2025-02-06 02:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:17:24.884971"
    },
    {
      "arxiv_id": "2502.10425v2",
      "title": "Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Wei Wu",
        "Can Liao",
        "Zizhen Deng",
        "Zhengrui Guo",
        "Jinzhuo Wang"
      ],
      "abstract": "The Platonic Representation Hypothesis suggests a universal,\nmodality-independent reality representation behind different data modalities.\nInspired by this, we view each neuron as a system and detect its multi-segment\nactivity data under various peripheral conditions. We assume there's a\ntime-invariant representation for the same neuron, reflecting its intrinsic\nproperties like molecular profiles, location, and morphology. The goal of\nobtaining these intrinsic neuronal representations has two criteria: (I)\nsegments from the same neuron should have more similar representations than\nthose from different neurons; (II) the representations must generalize well to\nout-of-domain data. To meet these, we propose the NeurPIR (Neuron Platonic\nIntrinsic Representation) framework. It uses contrastive learning, with\nsegments from the same neuron as positive pairs and those from different\nneurons as negative pairs. In implementation, we use VICReg, which focuses on\npositive pairs and separates dissimilar samples via regularization. We tested\nour method on Izhikevich model-simulated neuronal population dynamics data. The\nresults accurately identified neuron types based on preset hyperparameters. We\nalso applied it to two real-world neuron dynamics datasets with neuron type\nannotations from spatial transcriptomics and neuron locations. Our model's\nlearned representations accurately predicted neuron types and locations and\nwere robust on out-of-domain data (from unseen animals). This shows the\npotential of our approach for understanding neuronal systems and future\nneuroscience research.",
      "tldr_zh": "本研究基于 Platonic Representation Hypothesis，提出 NeurPIR 框架，通过对比学习（contrastive learning）从神经元动态数据中提取时间不变的内在表示，以反映神经元的分子特征、位置和形态。框架将同一神经元的活动段落作为正对样本、不同神经元的作为负对样本，并采用 VICReg 方法进行训练，确保表示的相似性和泛化能力。实验结果显示，在 Izhikevich 模型模拟数据和真实神经元动态数据集上，NeurPIR 准确预测了神经元类型、位置，并对 out-of-domain data 表现出鲁棒性，为神经科学研究提供新工具。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "q-bio.NC",
      "comment": "Accepted by ICLR'2025",
      "pdf_url": "http://arxiv.org/pdf/2502.10425v2",
      "published_date": "2025-02-06 02:22:23 UTC",
      "updated_date": "2025-02-18 11:07:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:17:36.867874"
    },
    {
      "arxiv_id": "2502.03717v2",
      "title": "Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jaden Clark",
        "Joey Hejna",
        "Dorsa Sadigh"
      ],
      "abstract": "Expressive robotic behavior is essential for the widespread acceptance of\nrobots in social environments. Recent advancements in learned legged locomotion\ncontrollers have enabled more dynamic and versatile robot behaviors. However,\ndetermining the optimal behavior for interactions with different users across\nvaried scenarios remains a challenge. Current methods either rely on natural\nlanguage input, which is efficient but low-resolution, or learn from human\npreferences, which, although high-resolution, is sample inefficient. This paper\nintroduces a novel approach that leverages priors generated by pre-trained LLMs\nalongside the precision of preference learning. Our method, termed\nLanguage-Guided Preference Learning (LGPL), uses LLMs to generate initial\nbehavior samples, which are then refined through preference-based feedback to\nlearn behaviors that closely align with human expectations. Our core insight is\nthat LLMs can guide the sampling process for preference learning, leading to a\nsubstantial improvement in sample efficiency. We demonstrate that LGPL can\nquickly learn accurate and expressive behaviors with as few as four queries,\noutperforming both purely language-parameterized models and traditional\npreference learning approaches. Website with videos:\nhttps://lgpl-gaits.github.io/",
      "tldr_zh": "本文提出Language-Guided Preference Learning (LGPL)方法，用于高效生成四足机器人的表达性行为，通过预训练的LLMs生成初始行为样本，并结合偏好反馈进行精炼，以适应不同用户和场景的需求。该方法的核心在于LLMs引导采样过程，大幅提升样本效率，仅需少量查询（如4个）即可实现高分辨率学习。实验结果表明，LGPL在动态行为生成上优于纯语言参数化模型和传统偏好学习方法，提供更准确和多样的机器人互动表现。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.03717v2",
      "published_date": "2025-02-06 02:07:18 UTC",
      "updated_date": "2025-03-31 23:24:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:17:48.809200"
    },
    {
      "arxiv_id": "2502.03715v1",
      "title": "Boosting Knowledge Graph-based Recommendations through Confidence-Aware Augmentation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Rui Cai",
        "Chao Wang",
        "Qianyi Cai",
        "Dazhong Shen",
        "Hui Xiong"
      ],
      "abstract": "Knowledge Graph-based recommendations have gained significant attention due\nto their ability to leverage rich semantic relationships. However, constructing\nand maintaining Knowledge Graphs (KGs) is resource-intensive, and the accuracy\nof KGs can suffer from noisy, outdated, or irrelevant triplets. Recent\nadvancements in Large Language Models (LLMs) offer a promising way to improve\nthe quality and relevance of KGs for recommendation tasks. Despite this,\nintegrating LLMs into KG-based systems presents challenges, such as efficiently\naugmenting KGs, addressing hallucinations, and developing effective joint\nlearning methods. In this paper, we propose the Confidence-aware KG-based\nRecommendation Framework with LLM Augmentation (CKG-LLMA), a novel framework\nthat combines KGs and LLMs for recommendation task. The framework includes: (1)\nan LLM-based subgraph augmenter for enriching KGs with high-quality\ninformation, (2) a confidence-aware message propagation mechanism to filter\nnoisy triplets, and (3) a dual-view contrastive learning method to integrate\nuser-item interactions and KG data. Additionally, we employ a confidence-aware\nexplanation generation process to guide LLMs in producing realistic\nexplanations for recommendations. Finally, extensive experiments demonstrate\nthe effectiveness of CKG-LLMA across multiple public datasets.",
      "tldr_zh": "本文提出 CKG-LLMA 框架，通过 Confidence-aware Augmentation 利用 Large Language Models (LLMs) 来提升基于 Knowledge Graphs (KGs) 的推荐系统性能，解决 KGs 中噪音、过时或无关三元组的问题。框架的关键组件包括 LLM-based subgraph augmenter 用于丰富 KGs 的高质量信息、confidence-aware message propagation 机制过滤噪音三元组，以及 dual-view contrastive learning 方法整合用户-物品互动和 KG 数据。此外，该框架还引入 confidence-aware explanation generation 来生成可靠的推荐解释。实验结果在多个公共数据集上证明，CKG-LLMA 显著提高了推荐系统的准确性和有效性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03715v1",
      "published_date": "2025-02-06 02:06:48 UTC",
      "updated_date": "2025-02-06 02:06:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:18:00.985308"
    },
    {
      "arxiv_id": "2502.03711v1",
      "title": "MultiQ&A: An Analysis in Measuring Robustness via Automated Crowdsourcing of Question Perturbations and Answers",
      "title_zh": "MultiQ&A：通过自动化众包问题扰动和答案来测量鲁棒性的分析",
      "authors": [
        "Nicole Cho",
        "William Watson"
      ],
      "abstract": "One critical challenge in the institutional adoption journey of Large\nLanguage Models (LLMs) stems from their propensity to hallucinate in generated\nresponses. To address this, we propose MultiQ&A, a systematic approach for\nevaluating the robustness and consistency of LLM-generated answers. We\ndemonstrate MultiQ&A's ability to crowdsource question perturbations and their\nrespective answers through independent LLM agents at scale. Our experiments\nculminated in the examination of 1.9 million question perturbations and 2.3\nmillion answers. Furthermore, MultiQ&A shows that ensembled LLMs, such as\ngpt-3.5-turbo, remain relatively robust and consistent under perturbations.\nMultiQ&A provides clarity in the response generation space, offering an\neffective method for inspecting disagreements and variability. Therefore, our\nsystem offers a potential framework for institutional LLM adoption with the\nability to measure confidence, consistency, and the quantification of\nhallucinations.",
      "tldr_zh": "该研究提出 MultiQ&A，一种系统方法，用于评估大型语言模型 (LLMs) 生成答案的鲁棒性和一致性，以应对 LLMs 容易产生 hallucination 的问题。通过独立 LLM 代理实现自动化众包，生成和分析 190 万个 question perturbations 和 230 万个答案。实验结果显示，集成 LLMs 如 gpt-3.5-turbo 在扰动下保持相对鲁棒和一致，并提供框架来量化置信度、一致性和 hallucination，从而为机构采用 LLMs 提供有效评估工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2025 Workshop on Preventing and Detecting LLM Misinformation\n  (PDLM) (Oral)",
      "pdf_url": "http://arxiv.org/pdf/2502.03711v1",
      "published_date": "2025-02-06 01:58:48 UTC",
      "updated_date": "2025-02-06 01:58:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:18:12.524760"
    },
    {
      "arxiv_id": "2502.03708v1",
      "title": "Aggregate and conquer: detecting and steering LLM concepts by combining nonlinear predictors over multiple layers",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Beaglehole",
        "Adityanarayanan Radhakrishnan",
        "Enric Boix-Adserà",
        "Mikhail Belkin"
      ],
      "abstract": "A trained Large Language Model (LLM) contains much of human knowledge. Yet,\nit is difficult to gauge the extent or accuracy of that knowledge, as LLMs do\nnot always ``know what they know'' and may even be actively misleading. In this\nwork, we give a general method for detecting semantic concepts in the internal\nactivations of LLMs. Furthermore, we show that our methodology can be easily\nadapted to steer LLMs toward desirable outputs. Our innovations are the\nfollowing: (1) we use a nonlinear feature learning method to identify important\nlinear directions for predicting concepts from each layer; (2) we aggregate\nfeatures across layers to build powerful concept detectors and steering\nmechanisms. We showcase the power of our approach by attaining state-of-the-art\nresults for detecting hallucinations, harmfulness, toxicity, and untruthful\ncontent on seven benchmarks. We highlight the generality of our approach by\nsteering LLMs towards new concepts that, to the best of our knowledge, have not\nbeen previously considered in the literature, including: semantic\ndisambiguation, human languages, programming languages, hallucinated responses,\nscience subjects, poetic/Shakespearean English, and even multiple concepts\nsimultaneously. Moreover, our method can steer concepts with numerical\nattributes such as product reviews. We provide our code (including a simple API\nfor our methods) at https://github.com/dmbeaglehole/neural_controllers .",
      "tldr_zh": "这篇论文提出了一种通用方法，通过在多个层上结合非线性预测器来检测和控制大型语言模型（LLM）的语义概念，解决了LLM可能存在知识不准确或误导的问题。创新点包括使用非线性特征学习识别每个层的重要线性方向，并跨层聚合这些特征，以构建高效的概念检测器和转向机制。实验结果显示，该方法在七个基准上达到了state-of-the-art水平，用于检测幻觉、危害性、毒性和不真实内容；此外，它还能泛化到新概念，如语义消歧、人文语言、编程语言，甚至同时处理多个概念或数值属性如产品评论。作者提供了开源代码和API以便应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.03708v1",
      "published_date": "2025-02-06 01:41:48 UTC",
      "updated_date": "2025-02-06 01:41:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:18:25.174882"
    },
    {
      "arxiv_id": "2502.03699v1",
      "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective",
      "title_zh": "LLM 对齐作为检索器优化：信息检索视角",
      "authors": [
        "Bowen Jin",
        "Jinsung Yoon",
        "Zhen Qin",
        "Ziqi Wang",
        "Wei Xiong",
        "Yu Meng",
        "Jiawei Han",
        "Sercan O. Arik"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.",
      "tldr_zh": "这篇论文从信息检索(IR)视角出发，将Large Language Models (LLMs)对齐问题视为检索器优化，提出了一种新方法LarPO，通过将LLM生成和奖励模型映射到IR的retriever-reranker范式，实现更简单的直接优化替代复杂Reinforcement Learning (RL)方法。LarPO框架桥接了LLM对齐和IR原理，旨在解决LLMs中的错误信息、幻觉、偏见和滥用问题。实验结果显示，该方法在AlpacaEval2和MixEval-Hard基准测试中分别平均提高了38.9%和13.7%的性能，为未来LLM对齐研究开辟了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.03699v1",
      "published_date": "2025-02-06 01:22:06 UTC",
      "updated_date": "2025-02-06 01:22:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:18:37.897516"
    },
    {
      "arxiv_id": "2502.04391v1",
      "title": "Towards Fair and Robust Face Parsing for Generative AI: A Multi-Objective Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Sophia J. Abraham",
        "Jonathan D. Hauenstein",
        "Walter J. Scheirer"
      ],
      "abstract": "Face parsing is a fundamental task in computer vision, enabling applications\nsuch as identity verification, facial editing, and controllable image\nsynthesis. However, existing face parsing models often lack fairness and\nrobustness, leading to biased segmentation across demographic groups and errors\nunder occlusions, noise, and domain shifts. These limitations affect downstream\nface synthesis, where segmentation biases can degrade generative model outputs.\nWe propose a multi-objective learning framework that optimizes accuracy,\nfairness, and robustness in face parsing. Our approach introduces a\nhomotopy-based loss function that dynamically adjusts the importance of these\nobjectives during training. To evaluate its impact, we compare multi-objective\nand single-objective U-Net models in a GAN-based face synthesis pipeline\n(Pix2PixHD). Our results show that fairness-aware and robust segmentation\nimproves photorealism and consistency in face generation. Additionally, we\nconduct preliminary experiments using ControlNet, a structured conditioning\nmodel for diffusion-based synthesis, to explore how segmentation quality\ninfluences guided image generation. Our findings demonstrate that\nmulti-objective face parsing improves demographic consistency and robustness,\nleading to higher-quality GAN-based synthesis.",
      "tldr_zh": "该论文针对面部解析（face parsing）模型在不同人口群体中的偏差以及在遮挡、噪声和领域偏移下的鲁棒性问题，提出一个多目标学习框架，以优化准确性、公平性和鲁棒性。框架引入基于 homotopy 的损失函数，在训练过程中动态调整这些目标的重要性。通过在 GAN-based 面部合成管道（如 Pix2PixHD）中与单目标 U-Net 模型比较，实验结果显示，该方法显著提高了生成图像的真实性和一致性。此外，初步实验在 ControlNet 中验证了其对人口统计一致性和整体合成质量的提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.04391v1",
      "published_date": "2025-02-06 00:41:35 UTC",
      "updated_date": "2025-02-06 00:41:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:18:49.689637"
    },
    {
      "arxiv_id": "2502.03688v2",
      "title": "A Comparison of DeepSeek and Other LLMs",
      "title_zh": "DeepSeek 与其他 LLMs 的比较",
      "authors": [
        "Tianchen Gao",
        "Jiashun Jin",
        "Zheng Tracy Ke",
        "Gabriel Moryoussef"
      ],
      "abstract": "Recently, DeepSeek has been the focus of attention in and beyond the AI\ncommunity. An interesting problem is how DeepSeek compares to other large\nlanguage models (LLMs). There are many tasks an LLM can do, and in this paper,\nwe use the task of predicting an outcome using a short text for comparison. We\nconsider two settings, an authorship classification setting and a citation\nclassification setting. In the first one, the goal is to determine whether a\nshort text is written by human or AI. In the second one, the goal is to\nclassify a citation to one of four types using the textual content. For each\nexperiment, we compare DeepSeek with $4$ popular LLMs: Claude, Gemini, GPT, and\nLlama.\n  We find that, in terms of classification accuracy, DeepSeek outperforms\nGemini, GPT, and Llama in most cases, but underperforms Claude. We also find\nthat DeepSeek is comparably slower than others but with a low cost to use,\nwhile Claude is much more expensive than all the others. Finally, we find that\nin terms of similarity, the output of DeepSeek is most similar to those of\nGemini and Claude (and among all $5$ LLMs, Claude and Gemini have the most\nsimilar outputs).\n  In this paper, we also present a fully-labeled dataset collected by\nourselves, and propose a recipe where we can use the LLMs and a recent data\nset, MADStat, to generate new data sets. The datasets in our paper can be used\nas benchmarks for future study on LLMs.",
      "tldr_zh": "这篇论文比较了 DeepSeek 与其他 LLMs（如 Claude、Gemini、GPT 和 Llama）在 authorship classification（判断文本是人类还是 AI 写的）和 citation classification（将引用分类为四种类型）任务上的性能。结果显示，DeepSeek 在准确率上优于 Gemini、GPT 和 Llama，但在大多数情况下不如 Claude；同时，DeepSeek 处理速度较慢但使用成本低，而 Claude 的成本最高。论文还引入了一个新数据集，并提出使用 LLMs 和 MADStat 生成新数据集的方法，作为未来 LLM 研究的基准。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 5 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.03688v2",
      "published_date": "2025-02-06 00:38:25 UTC",
      "updated_date": "2025-02-26 01:49:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:19:00.521559"
    },
    {
      "arxiv_id": "2502.03686v1",
      "title": "Variational Control for Guidance in Diffusion Models",
      "title_zh": "变分控制用于扩散模型中的引导",
      "authors": [
        "Kushagra Pandey",
        "Farrin Marouf Sofian",
        "Felix Draxler",
        "Theofanis Karaletsos",
        "Stephan Mandt"
      ],
      "abstract": "Diffusion models exhibit excellent sample quality, but existing guidance\nmethods often require additional model training or are limited to specific\ntasks. We revisit guidance in diffusion models from the perspective of\nvariational inference and control, introducing Diffusion Trajectory Matching\n(DTM) that enables guiding pretrained diffusion trajectories to satisfy a\nterminal cost. DTM unifies a broad class of guidance methods and enables novel\ninstantiations. We introduce a new method within this framework that achieves\nstate-of-the-art results on several linear and (blind) non-linear inverse\nproblems without requiring additional model training or modifications. For\ninstance, in ImageNet non-linear deblurring, our model achieves an FID score of\n34.31, significantly improving over the best pretrained-method baseline (FID\n78.07). We will make the code available in a future update.",
      "tldr_zh": "本论文从变分推理和控制的角度重新审视了Diffusion models的指导问题，引入了Diffusion Trajectory Matching (DTM)方法，用于引导预训练的diffusion trajectories以满足终端成本，从而统一了广泛的指导方法并启用了新实例。DTM无需额外模型训练或修改，就在多个线性及（盲）非线性逆问题上取得了state-of-the-art结果，例如在ImageNet非线性deblurring任务中，FID分数从78.07大幅提升至34.31。该框架为高效指导diffusion models提供了新途径，并展示了其在实际应用中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages in main text. Total of 20 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.03686v1",
      "published_date": "2025-02-06 00:24:39 UTC",
      "updated_date": "2025-02-06 00:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T08:19:13.271747"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 139,
  "processed_papers_count": 139,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T08:19:37.043475"
}
{
  "date": "2024-07-25",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-25 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 更新了 91 篇论文，主要聚焦于 AI 安全、联邦学习、多模态模型和图像处理等领域，其中 NeurIPS 和 ICML 接受的论文（如 Adversarially Robust Decision Transformer）令人印象深刻，展示了强化学习和联邦优化的创新进展，同时一些医疗和硬件应用论文（如 HDL-GPT）也值得关注，强调了 AI 在实际场景中的潜力。\n\n下面，我挑选了其中最具影响力和话题度的论文进行简要讨论，先从核心创新论文入手，再快速掠过其他相关或次要内容。每个条目列出论文标题（中文 + 英文），并突出主要贡献和发现。\n\n### 重点论文讨论\n\n**1. 逆向鲁棒决策变换器 / Adversarially Robust Decision Transformer**  \n   作者：Xiaohang Tang 等（NeurIPS 2024 接受）。  \n   这篇论文提出 ARDT 算法，通过 minimax expectile 回归学习逆向鲁棒策略，提升了强化学习在对抗环境下的性能，主要发现是 ARDT 在序列游戏和连续环境中显著提高 worst-case 返回值，超越传统决策变换器。\n\n**2. HDL-GPT：高质量 HDL 是你所需要的全部 / HDL-GPT: High-Quality HDL is All You Need**  \n   作者：Bhuvnesh Kumar 等（DAC 2024 Invited Paper）。  \n   论文引入 HDL-GPT 框架，利用开源 HDL 代码训练大代码模型，通过数据筛选和增强实现零样本泛化，主要贡献是提升电路解释、代码生成和调试任务的性能，较 SOTA 模型改善 50% 到 200%。\n\n**3. 联邦自适应异步优化 / FADAS: Towards Federated Adaptive Asynchronous Optimization**  \n   作者：Yujia Wang 等（ICML 2024 接受）。  \n   该工作开发 FADAS 方法，将异步更新融入联邦优化，并引入延迟自适应策略，主要发现是 FADAS 显著提高联邦学习的收敛率和鲁棒性，尤其在延迟场景下优于现有基线。\n\n**4. 使用类器官智能方法模拟神经对古典音乐的响应 / Simulation of Neural Responses to Classical Music Using Organoid Intelligence Methods**  \n   作者：Daniel Szelogowski。  \n   论文提出 PyOrganoid 库和 Pianoid 模型，利用双向 LSTM 预测 EEG 对音乐的响应，主要贡献是展示合成模型在音乐认知研究中的潜力，提供新工具用于神经科学和 AI 交叉应用。\n\n**5. PersonaGym：评估 Persona 代理和大型语言模型 / PersonaGym: Evaluating Persona Agents and LLMs**  \n   作者：Vinay Samuel 等。  \n   这篇论文构建 PersonaGym 框架和 PersonaScore 指标，用于评估 LLM 在角色扮演中的表现，主要发现是当前 SOTA 模型（如 Claude 3.5 Sonnet）在角色一致性上仍有改进空间，强调算法创新的需求。\n\n其他论文中，联邦学习相关如 \"SCALE: Self-regulated Clustered federAted LEarning\" 和 \"Robust Claim Verification Through Fact Detection\" 探讨了隐私保护和事实检测优化，但这些工作相对常规，仅快速提到它们在数据分布和鲁棒性上取得小幅进展。图像生成和医疗 AI 论文（如 \"GaussianSR\" 和 \"Mpox Detection Advanced\"）展示了实用性，但影响力有限，仅在合成数据和诊断精度上略有提升。剩余论文多为理论探索或小改进，这里不再详述，以保持篇幅简洁。\n\n总之，今天的更新突显 AI 模型在鲁棒性和实际应用上的潜力，感兴趣的读者可关注 NeurIPS 和 ICML 相关论文进行深入阅读！",
  "papers": [
    {
      "arxiv_id": "2407.18428v1",
      "title": "Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift",
      "title_zh": "加权风险不变性：在不变特征偏移下的领域泛化",
      "authors": [
        "Gina Wong",
        "Joshua Gleason",
        "Rama Chellappa",
        "Yoav Wald",
        "Anqi Liu"
      ],
      "abstract": "Learning models whose predictions are invariant under multiple environments\nis a promising approach for out-of-distribution generalization. Such models are\ntrained to extract features $X_{\\text{inv}}$ where the conditional distribution\n$Y \\mid X_{\\text{inv}}$ of the label given the extracted features does not\nchange across environments. Invariant models are also supposed to generalize to\nshifts in the marginal distribution $p(X_{\\text{inv}})$ of the extracted\nfeatures $X_{\\text{inv}}$, a type of shift we call an $\\textit{invariant\ncovariate shift}$. However, we show that proposed methods for learning\ninvariant models underperform under invariant covariate shift, either failing\nto learn invariant models$\\unicode{x2014}$even for data generated from simple\nand well-studied linear-Gaussian models$\\unicode{x2014}$or having poor\nfinite-sample performance. To alleviate these problems, we propose\n$\\textit{weighted risk invariance}$ (WRI). Our framework is based on imposing\ninvariance of the loss across environments subject to appropriate reweightings\nof the training examples. We show that WRI provably learns invariant models,\ni.e. discards spurious correlations, in linear-Gaussian settings. We propose a\npractical algorithm to implement WRI by learning the density\n$p(X_{\\text{inv}})$ and the model parameters simultaneously, and we demonstrate\nempirically that WRI outperforms previous invariant learning methods under\ninvariant covariate shift.",
      "tldr_zh": "该论文探讨了在不变特征偏移（invariant covariate shift）下实现领域泛化（domain generalization）的挑战，现有方法往往无法有效学习不变模型或在有限样本下表现不佳。作者提出 Weighted Risk Invariance (WRI) 框架，通过对训练样本施加适当加权以确保损失函数在不同环境中的不变性，从而去除虚假相关（spurious correlations）。在线性-Gaussian 设置中，WRI 被证明能可靠地学习不变模型，并在实验中显示出比传统 invariant learning 方法更优的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18428v1",
      "published_date": "2024-07-25 23:27:10 UTC",
      "updated_date": "2024-07-25 23:27:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:47:16.494782"
    },
    {
      "arxiv_id": "2407.21058v1",
      "title": "Understanding the Interplay of Scale, Data, and Bias in Language Models: A Case Study with BERT",
      "title_zh": "理解规模、数据和偏差在语言模型中的相互作用：以 BERT 为案例研究",
      "authors": [
        "Muhammad Ali",
        "Swetasudha Panda",
        "Qinlan Shen",
        "Michael Wick",
        "Ari Kobren"
      ],
      "abstract": "In the current landscape of language model research, larger models, larger\ndatasets and more compute seems to be the only way to advance towards\nintelligence. While there have been extensive studies of scaling laws and\nmodels' scaling behaviors, the effect of scale on a model's social biases and\nstereotyping tendencies has received less attention. In this study, we explore\nthe influence of model scale and pre-training data on its learnt social biases.\nWe focus on BERT -- an extremely popular language model -- and investigate\nbiases as they show up during language modeling (upstream), as well as during\nclassification applications after fine-tuning (downstream). Our experiments on\nfour architecture sizes of BERT demonstrate that pre-training data\nsubstantially influences how upstream biases evolve with model scale. With\nincreasing scale, models pre-trained on large internet scrapes like Common\nCrawl exhibit higher toxicity, whereas models pre-trained on moderated data\nsources like Wikipedia show greater gender stereotypes. However, downstream\nbiases generally decrease with increasing model scale, irrespective of the\npre-training data. Our results highlight the qualitative role of pre-training\ndata in the biased behavior of language models, an often overlooked aspect in\nthe study of scale. Through a detailed case study of BERT, we shed light on the\ncomplex interplay of data and model scale, and investigate how it translates to\nconcrete biases.",
      "tldr_zh": "这篇论文通过对 BERT 的案例研究，探讨了语言模型中模型规模、预训练数据和社会偏见之间的互动影响。实验使用四种不同规模的 BERT 模型，分析了预训练数据（如 Common Crawl 和 Wikipedia）对 upstream biases（预训练阶段偏见）和 downstream biases（微调后应用偏见）的效果。结果显示，预训练数据显著影响上游偏见：规模增大时，使用 Common Crawl 的模型毒性升高，而使用 Wikipedia 的模型性别刻板印象更明显；然而，下游偏见总体上随模型规模增加而减少，不依赖于预训练数据。该研究强调了预训练数据在偏见行为中的关键作用，揭示了规模与数据互动的复杂性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21058v1",
      "published_date": "2024-07-25 23:09:33 UTC",
      "updated_date": "2024-07-25 23:09:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:47:28.173813"
    },
    {
      "arxiv_id": "2407.18423v1",
      "title": "HDL-GPT: High-Quality HDL is All You Need",
      "title_zh": "HDL-GPT：高质量 HDL 就是你所需要的一切",
      "authors": [
        "Bhuvnesh Kumar",
        "Saurav Nanda",
        "Ganapathy Parthasarathy",
        "Pawan Patil",
        "Austin Tsai",
        "Parivesh Choudhary"
      ],
      "abstract": "This paper presents Hardware Description Language Generative Pre-trained\nTransformers (HDL-GPT), a novel approach that leverages the vast repository of\nopen-source High Definition Language (HDL) codes to train superior quality\nlarge code models. The core premise of this paper is the hypothesis that\nhigh-quality HDL is all you need to create models with exceptional performance\nand broad zero-shot generalization abilities. The paper elucidates the methods\nemployed for the curation and augmentation of large corpora from open-source\nHDL code, transforming highly variable quality data into high-quality data\nthrough careful prompting and context maintenance. We demonstrate that the\ncareful selection, filtering, and augmentation of data across HDLs can yield\npowerful models that surpass current state-of-the-art models. We also explore\nthe impact of different fine-tuning methods on the quality of results. We\ndescribe experimental results across a range of fine-tuned SOTA LLMs,\nsubstantiating our claims. We demonstrate improvements of 50% to 200% over SOTA\nHDL models on current benchmarks in tasks ranging from HDL circuit\nexplanations, code generation, formal and simulation testbench creation,\ntriaging bugs, and fixing them. HDL-GPT opens new avenues for the development\nof advanced model training techniques for circuit design tasks.",
      "tldr_zh": "本研究提出HDL-GPT，一种利用开源Hardware Description Language (HDL)代码训练高质量大型代码模型的方法，其核心假设是“High-Quality HDL is All You Need”，能实现卓越性能和广泛zero-shot generalization能力。研究通过数据curate、augmentation、过滤和prompting等技术，将开源HDL代码转化为高质量语料库，并探索不同fine-tuning方法来提升模型效果。实验结果显示，HDL-GPT在HDL电路解释、代码生成、测试bench创建、bug诊断和修复等任务上，比现有SOTA模型提高了50%到200%。这项工作为电路设计任务的先进模型训练技术开辟了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "DAC 2024 Invited Paper",
      "pdf_url": "http://arxiv.org/pdf/2407.18423v1",
      "published_date": "2024-07-25 22:48:08 UTC",
      "updated_date": "2024-07-25 22:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:47:36.775441"
    },
    {
      "arxiv_id": "2407.18422v3",
      "title": "A Black Swan Hypothesis: The Role of Human Irrationality in AI Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Hyunin Lee",
        "Chanwoo Park",
        "David Abel",
        "Ming Jin"
      ],
      "abstract": "Black swan events are statistically rare occurrences that carry extremely\nhigh risks. A typical view of defining black swan events is heavily assumed to\noriginate from an unpredictable time-varying environments; however, the\ncommunity lacks a comprehensive definition of black swan events. To this end,\nthis paper challenges that the standard view is incomplete and claims that\nhigh-risk, statistically rare events can also occur in unchanging environments\ndue to human misperception of their value and likelihood, which we call as\nspatial black swan event. We first carefully categorize black swan events,\nfocusing on spatial black swan events, and mathematically formalize the\ndefinition of black swan events. We hope these definitions can pave the way for\nthe development of algorithms to prevent such events by rationally correcting\nhuman perception.",
      "tldr_zh": "本文提出“Black Swan Hypothesis”，认为传统观点忽略了人类非理性在AI安全中的作用，即高风险的统计罕见事件（Black Swan events）可能在不变环境中因人类对价值和概率的误判而发生，称之为“spatial black swan event”。作者对Black Swan events进行了分类，并对其进行了数学形式化定义，以强调这类事件的机制。最终，该框架旨在为开发算法铺平道路，这些算法通过理性修正人类感知来预防此类事件。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "The title was changed and acknowledgment was included",
      "pdf_url": "http://arxiv.org/pdf/2407.18422v3",
      "published_date": "2024-07-25 22:44:39 UTC",
      "updated_date": "2025-03-20 19:18:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:47:49.977955"
    },
    {
      "arxiv_id": "2407.18416v3",
      "title": "PersonaGym: Evaluating Persona Agents and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Vinay Samuel",
        "Henry Peng Zou",
        "Yue Zhou",
        "Shreyas Chaudhari",
        "Ashwin Kalyan",
        "Tanmay Rajpurohit",
        "Ameet Deshpande",
        "Karthik Narasimhan",
        "Vishvak Murahari"
      ],
      "abstract": "Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.",
      "tldr_zh": "该研究引入了PersonaGym，这是第一个动态评估框架，用于评估基于LLMs的Persona agents在不同环境中的表现，以及PersonaScore，这是一个基于决策理论的自动指标，用于全面量化代理的角色一致性。研究通过一个包含200个personas和10,000个questions的基准，对6个开源和闭源LLMs进行了大规模评估。结果显示，即使是先进模型如Claude 3.5 Sonnet，与GPT 3.5相比仅在PersonaScore上实现了2.97%的相对提升，表明模型规模和复杂性并不必然提升Persona agent能力，强调了需要算法和架构创新来实现更可靠的代理。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18416v3",
      "published_date": "2024-07-25 22:24:45 UTC",
      "updated_date": "2024-12-18 14:25:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:48:01.747227"
    },
    {
      "arxiv_id": "2407.18414v2",
      "title": "Adversarially Robust Decision Transformer",
      "title_zh": "对抗鲁棒的 Decision Transformer",
      "authors": [
        "Xiaohang Tang",
        "Afonso Marques",
        "Parameswaran Kamalaruban",
        "Ilija Bogunovic"
      ],
      "abstract": "Decision Transformer (DT), as one of the representative Reinforcement\nLearning via Supervised Learning (RvS) methods, has achieved strong performance\nin offline learning tasks by leveraging the powerful Transformer architecture\nfor sequential decision-making. However, in adversarial environments, these\nmethods can be non-robust, since the return is dependent on the strategies of\nboth the decision-maker and adversary. Training a probabilistic model\nconditioned on observed return to predict action can fail to generalize, as the\ntrajectories that achieve a return in the dataset might have done so due to a\nsuboptimal behavior adversary. To address this, we propose a worst-case-aware\nRvS algorithm, the Adversarially Robust Decision Transformer (ARDT), which\nlearns and conditions the policy on in-sample minimax returns-to-go. ARDT\naligns the target return with the worst-case return learned through minimax\nexpectile regression, thereby enhancing robustness against powerful test-time\nadversaries. In experiments conducted on sequential games with full data\ncoverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution\nwith the largest adversarial robustness. In large-scale sequential games and\ncontinuous adversarial RL environments with partial data coverage, ARDT\ndemonstrates significantly superior robustness to powerful test-time\nadversaries and attains higher worst-case returns compared to contemporary DT\nmethods.",
      "tldr_zh": "本研究针对 Decision Transformer (DT) 在对抗环境中存在的鲁棒性问题，提出了一种 Adversarially Robust Decision Transformer (ARDT) 方法，以解决 Reinforcement Learning via Supervised Learning (RvS) 算法在处理对手策略时的泛化失败。ARDT 通过 worst-case-aware 算法学习并条件化策略于 in-sample minimax returns-to-go，利用 minimax expectile regression 来对齐目标回报，从而提升对强大测试时对手的鲁棒性。在实验中，ARDT 在顺序游戏中生成 maximin (Nash Equilibrium) 策略，并在大型顺序游戏及连续对抗 RL 环境中，显著超越传统 DT 方法，实现更高的 worst-case returns。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18414v2",
      "published_date": "2024-07-25 22:12:47 UTC",
      "updated_date": "2024-11-01 17:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:48:12.641920"
    },
    {
      "arxiv_id": "2407.18413v1",
      "title": "Simulation of Neural Responses to Classical Music Using Organoid Intelligence Methods",
      "title_zh": "使用类器官智能方法模拟古典音乐的神经响应",
      "authors": [
        "Daniel Szelogowski"
      ],
      "abstract": "Music is a complex auditory stimulus capable of eliciting significant changes\nin brain activity, influencing cognitive processes such as memory, attention,\nand emotional regulation. However, the underlying mechanisms of music-induced\ncognitive processes remain largely unknown. Organoid intelligence and deep\nlearning models show promise for simulating and analyzing these neural\nresponses to classical music, an area significantly unexplored in computational\nneuroscience. Hence, we present the PyOrganoid library, an innovative tool that\nfacilitates the simulation of organoid learning models, integrating\nsophisticated machine learning techniques with biologically inspired organoid\nsimulations. Our study features the development of the Pianoid model, a \"deep\norganoid learning\" model that utilizes a Bidirectional LSTM network to predict\nEEG responses based on audio features from classical music recordings. This\nmodel demonstrates the feasibility of using computational methods to replicate\ncomplex neural processes, providing valuable insights into music perception and\ncognition. Likewise, our findings emphasize the utility of synthetic models in\nneuroscience research and highlight the PyOrganoid library's potential as a\nversatile tool for advancing studies in neuroscience and artificial\nintelligence.",
      "tldr_zh": "本研究探讨了音乐对大脑活动的影响及其认知机制，使用 Organoid intelligence 方法模拟神经响应。研究者开发了 PyOrganoid 库，这是一个整合机器学习技术和生物启发模拟的工具，并基于此构建了 Pianoid 模型，该模型采用 Bidirectional LSTM 网络，通过分析古典音乐音频特征来预测 EEG 响应。结果显示，这种“deep organoid learning”方法可有效复制复杂神经过程，提供音乐感知和认知的宝贵见解，并强调了合成模型在神经科学和人工智能研究中的应用潜力。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS",
        "I.2; I.6; J.3; J.4; J.5"
      ],
      "primary_category": "cs.NE",
      "comment": "10 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18413v1",
      "published_date": "2024-07-25 22:11:30 UTC",
      "updated_date": "2024-07-25 22:11:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:48:24.710372"
    },
    {
      "arxiv_id": "2407.18387v1",
      "title": "SCALE: Self-regulated Clustered federAted LEarning in a Homogeneous Environment",
      "title_zh": "SCALE：自我调控的集群联邦学习在同质环境",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Md Jahangir Alam",
        "Sajedul Talukder",
        "Zahidur Talukder",
        "Syed Bahauddin"
      ],
      "abstract": "Federated Learning (FL) has emerged as a transformative approach for enabling\ndistributed machine learning while preserving user privacy, yet it faces\nchallenges like communication inefficiencies and reliance on centralized\ninfrastructures, leading to increased latency and costs. This paper presents a\nnovel FL methodology that overcomes these limitations by eliminating the\ndependency on edge servers, employing a server-assisted Proximity Evaluation\nfor dynamic cluster formation based on data similarity, performance indices,\nand geographical proximity. Our integrated approach enhances operational\nefficiency and scalability through a Hybrid Decentralized Aggregation Protocol,\nwhich merges local model training with peer-to-peer weight exchange and a\ncentralized final aggregation managed by a dynamically elected driver node,\nsignificantly curtailing global communication overhead. Additionally, the\nmethodology includes Decentralized Driver Selection, Check-pointing to reduce\nnetwork traffic, and a Health Status Verification Mechanism for system\nrobustness. Validated using the breast cancer dataset, our architecture not\nonly demonstrates a nearly tenfold reduction in communication overhead but also\nshows remarkable improvements in reducing training latency and energy\nconsumption while maintaining high learning performance, offering a scalable,\nefficient, and privacy-preserving solution for the future of federated learning\necosystems.",
      "tldr_zh": "本研究提出了一种名为SCALE的自调节集群联邦学习(Federated Learning)方法，旨在解决传统FL在同质环境中的通信效率低和对集中式基础设施的依赖问题。该方法通过server-assisted Proximity Evaluation动态形成集群（基于数据相似性、性能指标和地理位置），并采用Hybrid Decentralized Aggregation Protocol结合本地模型训练、点对点权重交换以及由动态选举的驱动节点管理的最终聚合，从而显著减少全局通信开销。此外，SCALE还包括Decentralized Driver Selection、Check-pointing机制和Health Status Verification Mechanism，以提升系统鲁棒性和效率。在乳腺癌数据集上的验证显示，该方法使通信开销减少近十倍，同时降低训练延迟和能源消耗，并保持高学习性能，提供了一个可扩展、高效且隐私保护的FL解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "This research article got accepted in COMPSAC conference and going to\n  be published to IEEE",
      "pdf_url": "http://arxiv.org/pdf/2407.18387v1",
      "published_date": "2024-07-25 20:42:16 UTC",
      "updated_date": "2024-07-25 20:42:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:48:47.664894"
    },
    {
      "arxiv_id": "2407.18367v1",
      "title": "Robust Claim Verification Through Fact Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Nazanin Jafari",
        "James Allan"
      ],
      "abstract": "Claim verification can be a challenging task. In this paper, we present a\nmethod to enhance the robustness and reasoning capabilities of automated claim\nverification through the extraction of short facts from evidence. Our novel\napproach, FactDetect, leverages Large Language Models (LLMs) to generate\nconcise factual statements from evidence and label these facts based on their\nsemantic relevance to the claim and evidence. The generated facts are then\ncombined with the claim and evidence. To train a lightweight supervised model,\nwe incorporate a fact-detection task into the claim verification process as a\nmultitasking approach to improve both performance and explainability. We also\nshow that augmenting FactDetect in the claim verification prompt enhances\nperformance in zero-shot claim verification using LLMs. Our method demonstrates\ncompetitive results in the supervised claim verification model by 15% on the F1\nscore when evaluated for challenging scientific claim verification datasets. We\nalso demonstrate that FactDetect can be augmented with claim and evidence for\nzero-shot prompting (AugFactDetect) in LLMs for verdict prediction. We show\nthat AugFactDetect outperforms the baseline with statistical significance on\nthree challenging scientific claim verification datasets with an average of\n17.3% performance gain compared to the best performing baselines.",
      "tldr_zh": "本研究提出了一种名为 FactDetect 的方法，以提升声明验证的鲁棒性和推理能力，通过使用 Large Language Models (LLMs) 从证据中提取简洁事实并根据语义相关性进行标记。FactDetect 将这些事实与声明和证据结合，采用多任务训练策略来训练轻量级监督模型，从而提高验证性能和可解释性。在实验中，该方法在监督模型上使 F1 score 提升 15%，而在零样本提示设置中，增强版 AugFactDetect 平均比基线模型提高 17.3%，在多个科学声明验证数据集上表现出显著优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18367v1",
      "published_date": "2024-07-25 20:03:43 UTC",
      "updated_date": "2024-07-25 20:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:48:49.031675"
    },
    {
      "arxiv_id": "2407.18365v1",
      "title": "FADAS: Towards Federated Adaptive Asynchronous Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yujia Wang",
        "Shiqiang Wang",
        "Songtao Lu",
        "Jinghui Chen"
      ],
      "abstract": "Federated learning (FL) has emerged as a widely adopted training paradigm for\nprivacy-preserving machine learning. While the SGD-based FL algorithms have\ndemonstrated considerable success in the past, there is a growing trend towards\nadopting adaptive federated optimization methods, particularly for training\nlarge-scale models. However, the conventional synchronous aggregation design\nposes a significant challenge to the practical deployment of those adaptive\nfederated optimization methods, particularly in the presence of straggler\nclients. To fill this research gap, this paper introduces federated adaptive\nasynchronous optimization, named FADAS, a novel method that incorporates\nasynchronous updates into adaptive federated optimization with provable\nguarantees. To further enhance the efficiency and resilience of our proposed\nmethod in scenarios with significant asynchronous delays, we also extend FADAS\nwith a delay-adaptive learning adjustment strategy. We rigorously establish the\nconvergence rate of the proposed algorithms and empirical results demonstrate\nthe superior performance of FADAS over other asynchronous FL baselines.",
      "tldr_zh": "本文提出 FADAS，一种面向 Federated Learning (FL) 的联邦自适应异步优化方法，旨在解决传统同步聚合在处理落后客户端（straggler clients）时的挑战，同时适用于训练大规模模型。FADAS 通过将异步更新整合到 adaptive federated optimization 中，提供可证明的收敛保证，并引入 delay-adaptive learning adjustment 策略来提升算法在延迟场景下的效率和弹性。实验结果显示，FADAS 在性能上优于其他异步 FL 基线，为隐私保护机器学习提供了更可靠的优化框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18365v1",
      "published_date": "2024-07-25 20:02:57 UTC",
      "updated_date": "2024-07-25 20:02:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:49:00.659816"
    },
    {
      "arxiv_id": "2407.18358v1",
      "title": "Generative AI like ChatGPT in Blockchain Federated Learning: use cases, opportunities and future",
      "title_zh": "翻译失败",
      "authors": [
        "Sai Puppala",
        "Ismail Hossain",
        "Md Jahangir Alam",
        "Sajedul Talukder",
        "Jannatul Ferdaus",
        "Mahedi Hasan",
        "Sameera Pisupati",
        "Shanmukh Mathukumilli"
      ],
      "abstract": "Federated learning has become a significant approach for training machine\nlearning models using decentralized data without necessitating the sharing of\nthis data. Recently, the incorporation of generative artificial intelligence\n(AI) methods has provided new possibilities for improving privacy, augmenting\ndata, and customizing models. This research explores potential integrations of\ngenerative AI in federated learning, revealing various opportunities to enhance\nprivacy, data efficiency, and model performance. It particularly emphasizes the\nimportance of generative models like generative adversarial networks (GANs) and\nvariational autoencoders (VAEs) in creating synthetic data that replicates the\ndistribution of real data. Generating synthetic data helps federated learning\naddress challenges related to limited data availability and supports robust\nmodel development. Additionally, we examine various applications of generative\nAI in federated learning that enable more personalized solutions.",
      "tldr_zh": "这篇论文探讨了生成式 AI（如 ChatGPT）在区块链联邦学习中的应用前景，强调其如何通过生成合成数据来提升隐私保护、数据效率和模型性能。论文特别关注生成模型如 GANs 和 VAEs，用于创建模拟真实数据分布的合成数据，以解决数据可用性不足的挑战，并支持更个性化的模型开发。同时，该研究突出了潜在机会，包括在各种应用场景中实现更安全和高效的联邦学习框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "We are going to submit this research article into a conference which\n  is best fit for this topic",
      "pdf_url": "http://arxiv.org/pdf/2407.18358v1",
      "published_date": "2024-07-25 19:43:49 UTC",
      "updated_date": "2024-07-25 19:43:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:49:54.110803"
    },
    {
      "arxiv_id": "2407.18343v2",
      "title": "Introducing δ-XAI: a novel sensitivity-based method for local AI explanations",
      "title_zh": "引入 δ-XAI：一种新颖的基于敏感性的局部 AI 解释方法",
      "authors": [
        "Alessandro De Carlo",
        "Enea Parimbelli",
        "Nicola Melillo",
        "Giovanna Nicora"
      ],
      "abstract": "Explainable Artificial Intelligence (XAI) is central to the debate on\nintegrating Artificial Intelligence (AI) and Machine Learning (ML) algorithms\ninto clinical practice. High-performing AI/ML models, such as ensemble learners\nand deep neural networks, often lack interpretability, hampering clinicians'\ntrust in their predictions. To address this, XAI techniques are being developed\nto describe AI/ML predictions in human-understandable terms. One promising\ndirection is the adaptation of sensitivity analysis (SA) and global sensitivity\nanalysis (GSA), which inherently rank model inputs by their impact on\npredictions. Here, we introduce a novel delta-XAI method that provides local\nexplanations of ML model predictions by extending the delta index, a GSA\nmetric. The delta-XAI index assesses the impact of each feature's value on the\npredicted output for individual instances in both regression and classification\nproblems. We formalize the delta-XAI index and provide code for its\nimplementation. The delta-XAI method was evaluated on simulated scenarios using\nlinear regression models, with Shapley values serving as a benchmark. Results\nshowed that the delta-XAI index is generally consistent with Shapley values,\nwith notable discrepancies in models with highly impactful or extreme feature\nvalues. The delta-XAI index demonstrated higher sensitivity in detecting\ndominant features and handling extreme feature values. Qualitatively, the\ndelta-XAI provides intuitive explanations by leveraging probability density\nfunctions, making feature rankings clearer and more explainable for\npractitioners. Overall, the delta-XAI method appears promising for robustly\nobtaining local explanations of ML model predictions. Further investigations in\nreal-world clinical settings will be conducted to evaluate its impact on\nAI-assisted clinical workflows.",
      "tldr_zh": "该研究引入了δ-XAI，一种基于敏感性分析(SA)和全局敏感性分析(GSA)的新方法，用于提供机器学习(ML)模型的局部解释，旨在解决高性能模型如集成学习器和深度神经网络的解释性问题。δ-XAI 通过扩展 delta index 来评估每个特征值对单个实例预测的影响，支持回归和分类任务，并利用概率密度函数生成直观的特征排名。实验结果显示，δ-XAI 与 Shapley values 基本一致，但在检测主导特征和处理极端值时表现出更高的敏感性，并有望提升AI在临床实践中的可解释性和可信度。未来计划在真实临床环境中进一步验证其对AI辅助工作流程的影响。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18343v2",
      "published_date": "2024-07-25 19:07:49 UTC",
      "updated_date": "2024-07-29 13:25:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:49:27.975782"
    },
    {
      "arxiv_id": "2408.05354v2",
      "title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust",
      "title_zh": "翻译失败",
      "authors": [
        "Ruoxi Shang",
        "Gary Hsieh",
        "Chirag Shah"
      ],
      "abstract": "Trust is not just a cognitive issue but also an emotional one, yet the\nresearch in human-AI interactions has primarily focused on the cognitive route\nof trust development. Recent work has highlighted the importance of studying\naffective trust towards AI, especially in the context of emerging human-like\nLLMs-powered conversational agents. However, there is a lack of validated and\ngeneralizable measures for the two-dimensional construct of trust in AI agents.\nTo address this gap, we developed and validated a set of 27-item semantic\ndifferential scales for affective and cognitive trust through a scenario-based\nsurvey study. We then further validated and applied the scale through an\nexperiment study. Our empirical findings showed how the emotional and cognitive\naspects of trust interact with each other and collectively shape a person's\noverall trust in AI agents. Our study methodology and findings also provide\ninsights into the capability of the state-of-art LLMs to foster trust through\ndifferent routes.",
      "tldr_zh": "该研究强调了AI代理信任的情感和认知双重维度，填补了现有测量工具的空白，通过开发并验证一套27项语义差异量表（semantic differential scales）来评估这两种信任。研究采用基于场景的调查和实验方法，探讨了情感信任和认知信任的相互作用及其对整体信任的影响。结果显示，这两个维度共同塑造了对AI代理的信任，并为大型语言模型（LLMs）通过不同路径培养信任提供了重要洞见。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05354v2",
      "published_date": "2024-07-25 18:55:33 UTC",
      "updated_date": "2024-11-07 20:34:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:49:50.771798"
    },
    {
      "arxiv_id": "2407.18335v1",
      "title": "Combining Cognitive and Generative AI for Self-explanation in Interactive AI Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Shalini Sushri",
        "Rahul Dass",
        "Rhea Basappa",
        "Hong Lu",
        "Ashok Goel"
      ],
      "abstract": "The Virtual Experimental Research Assistant (VERA) is an inquiry-based\nlearning environment that empowers a learner to build conceptual models of\ncomplex ecological systems and experiment with agent-based simulations of the\nmodels. This study investigates the convergence of cognitive AI and generative\nAI for self-explanation in interactive AI agents such as VERA. From a cognitive\nAI viewpoint, we endow VERA with a functional model of its own design,\nknowledge, and reasoning represented in the Task--Method--Knowledge (TMK)\nlanguage. From the perspective of generative AI, we use ChatGPT, LangChain, and\nChain-of-Thought to answer user questions based on the VERA TMK model. Thus, we\ncombine cognitive and generative AI to generate explanations about how VERA\nworks and produces its answers. The preliminary evaluation of the generation of\nexplanations in VERA on a bank of 66 questions derived from earlier work\nappears promising.",
      "tldr_zh": "本研究探讨了将认知 AI 和生成式 AI 结合，用于交互式 AI 代理（如 Virtual Experimental Research Assistant, VERA）的自我解释功能。研究从认知 AI 角度，使用 Task--Method--Knowledge (TMK) 语言为 VERA 构建其设计、知识和推理的功能模型；从生成式 AI 角度，则利用 ChatGPT、LangChain 和 Chain-of-Thought 来基于 TMK 模型回答用户问题并生成解释。通过这种整合方法，VERA 能够提供关于自身工作原理和答案生成的清晰解释。初步评估在 66 个问题上显示，该方法的效果令人乐观。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 2 figures, 2 tables, 1 appendix, HEXED Workshop @EDM July\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18335v1",
      "published_date": "2024-07-25 18:46:11 UTC",
      "updated_date": "2024-07-25 18:46:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:50:01.194954"
    },
    {
      "arxiv_id": "2408.05361v1",
      "title": "MindGPT: Advancing Human-AI Interaction with Non-Invasive fNIRS-Based Imagined Speech Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Suyi Zhang",
        "Ekram Alam",
        "Jack Baber",
        "Francesca Bianco",
        "Edward Turner",
        "Maysam Chamanzar",
        "Hamid Dehghani"
      ],
      "abstract": "In the coming decade, artificial intelligence systems are set to\nrevolutionise every industry and facet of human life. Building communication\nsystems that enable seamless and symbiotic communication between humans and AI\nagents is increasingly important. This research advances the field of human-AI\ninteraction by developing an innovative approach to decode imagined speech\nusing non-invasive high-density functional near-infrared spectroscopy (fNIRS).\nNotably, this study introduces MindGPT, the first thought-to-LLM (large\nlanguage model) system in the world.",
      "tldr_zh": "该研究旨在提升人类-AI 交互，通过非侵入性高密度功能近红外光谱(fNIRS)技术开发一种创新方法来解码想象中的言语。研究引入了MindGPT，这是世界上首个thought-to-LLM（thought-to-large language model）系统，能够将脑部活动直接转化为AI响应。MindGPT 的开发有望实现更无缝的共生式沟通，推动人工智能在日常生活中的应用。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05361v1",
      "published_date": "2024-07-25 18:18:52 UTC",
      "updated_date": "2024-07-25 18:18:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:50:04.905307"
    },
    {
      "arxiv_id": "2407.18316v1",
      "title": "Affectively Framework: Towards Human-like Affect-Based Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Barthet",
        "Roberto Gallotta",
        "Ahmed Khalifa",
        "Antonios Liapis",
        "Georgios N. Yannakakis"
      ],
      "abstract": "Game environments offer a unique opportunity for training virtual agents due\nto their interactive nature, which provides diverse play traces and affect\nlabels. Despite their potential, no reinforcement learning framework\nincorporates human affect models as part of their observation space or reward\nmechanism. To address this, we present the \\emph{Affectively Framework}, a set\nof Open-AI Gym environments that integrate affect as part of the observation\nspace. This paper introduces the framework and its three game environments and\nprovides baseline experiments to validate its effectiveness and potential.",
      "tldr_zh": "该论文提出 Affectively Framework，这是一个强化学习框架，旨在通过整合人类情感模型到观察空间中，创建更接近人类的虚拟代理。框架基于 Open-AI Gym 环境，提供了三个游戏场景，以利用互动游戏轨迹和情感标签进行训练。实验结果显示，该框架有效提升了代理的性能，并验证了其在情感建模方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 2 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.18316v1",
      "published_date": "2024-07-25 18:18:10 UTC",
      "updated_date": "2024-07-25 18:18:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:50:23.553768"
    },
    {
      "arxiv_id": "2407.18310v2",
      "title": "Revolutionizing Undergraduate Learning: CourseGPT and Its Generative AI Advancements",
      "title_zh": "革新本科生学习：CourseGPT 及其生成式 AI 进步",
      "authors": [
        "Ahmad M. Nazar",
        "Mohamed Y. Selim",
        "Ashraf Gaffar",
        "Shakil Ahmed"
      ],
      "abstract": "Integrating Generative AI (GenAI) into educational contexts presents a\ntransformative potential for enhancing learning experiences. This paper\nintroduces CourseGPT, a generative AI tool designed to support instructors and\nenhance the educational experiences of undergraduate students. Built on\nopen-source Large Language Models (LLMs) from Mistral AI, CourseGPT offers\ncontinuous instructor support and regular updates to course materials,\nenriching the learning environment. By utilizing course-specific content, such\nas slide decks and supplementary readings and references, CourseGPT provides\nprecise, dynamically generated responses to student inquiries. Unlike generic\nAI models, CourseGPT allows instructors to manage and control the responses,\nthus extending the course scope without overwhelming details. The paper\ndemonstrates the application of CourseGPT using the CPR E 431 - Basics of\nInformation System Security course as a pilot. This course, with its large\nenrollments and diverse curriculum, serves as an ideal testbed for CourseGPT.\nThe tool aims to enhance the learning experience, accelerate feedback\nprocesses, and streamline administrative tasks. The study evaluates CourseGPT's\nimpact on student outcomes, focusing on correctness scores, context recall, and\nfaithfulness of responses. Results indicate that the Mixtral-8x7b model, with a\nhigher parameter count, outperforms smaller models, achieving an 88.0%\ncorrectness score and a 66.6% faithfulness score. Additionally, feedback from\nformer students and teaching assistants on CourseGPT's accuracy, helpfulness,\nand overall performance was collected. The outcomes revealed that a significant\nmajority found CourseGPT to be highly accurate and beneficial in addressing\ntheir queries, with many praising its ability to provide timely and relevant\ninformation.",
      "tldr_zh": "这篇论文介绍了 CourseGPT，一种基于 Generative AI 的工具，旨在通过开源 Large Language Models (LLMs) 如 Mistral AI 的模型，支持教师并提升本科生学习体验。CourseGPT 利用课程特定内容（如幻灯片和阅读材料）生成精确动态响应，并允许教师管理控制以避免信息过载。论文以 CPR E 431 课程为试点，评估了其对学生成果的影响，结果显示 Mixtral-8x7b 模型取得了 88.0% 的正确性分数和 66.6% 的忠实度。总体反馈表明，CourseGPT 在准确性和帮助性方面获得高度认可，有助于加速反馈和简化行政任务。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.18310v2",
      "published_date": "2024-07-25 18:02:16 UTC",
      "updated_date": "2024-12-24 02:40:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:50:37.659875"
    },
    {
      "arxiv_id": "2407.18242v3",
      "title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengbo Wang",
        "Jian Liang",
        "Ran He",
        "Zilei Wang",
        "Tieniu Tan"
      ],
      "abstract": "Low-rank adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning of foundation models. Despite its\ncomputational efficiency, LoRA still yields inferior performance compared to\nfull fine-tuning. In this paper, we first uncover a fundamental connection\nbetween the optimization processes of LoRA and full fine-tuning: using LoRA for\noptimization is mathematically equivalent to full fine-tuning using a low-rank\ngradient for parameter updates. And this low-rank gradient can be expressed in\nterms of the gradients of the two low-rank matrices in LoRA. Leveraging this\ninsight, we introduce LoRA-Pro, a method that enhances LoRA's performance by\nstrategically adjusting the gradients of these low-rank matrices. This\nadjustment allows the low-rank gradient to more accurately approximate the full\nfine-tuning gradient, thereby narrowing the performance gap between LoRA and\nfull fine-tuning. Furthermore, we theoretically derive the optimal solutions\nfor adjusting the gradients of the low-rank matrices, applying them during\nfine-tuning in LoRA-Pro. We conduct extensive experiments across natural\nlanguage understanding, dialogue generation, mathematical reasoning, code\ngeneration, and image classification tasks, demonstrating that LoRA-Pro\nsubstantially improves LoRA's performance, effectively narrowing the gap with\nfull fine-tuning. Code is publicly available at\nhttps://github.com/mrflogs/LoRA-Pro.",
      "tldr_zh": "本文研究发现，Low-rank adaptation (LoRA) 作为高效微调基础模型的方法，虽然计算效率高，但性能不如全微调，因为其优化过程等价于使用低秩梯度更新参数。论文揭示了 LoRA 的两个低秩矩阵梯度间的数学联系，并提出 LoRA-Pro 方法，通过战略性调整这些梯度，使低秩梯度更准确地逼近全微调梯度。作者理论上推导了调整梯度的最优解，并在微调过程中应用。实验在自然语言理解、对话生成、数学推理、代码生成和图像分类等任务上显示，LoRA-Pro 显著提升了 LoRA 的性能，并有效缩小了与全微调的差距。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Camera-Ready Version for ICLR 2025; technical corrections to previous\n  version",
      "pdf_url": "http://arxiv.org/pdf/2407.18242v3",
      "published_date": "2024-07-25 17:57:12 UTC",
      "updated_date": "2025-03-22 09:29:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:50:48.288921"
    },
    {
      "arxiv_id": "2407.18219v2",
      "title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
      "title_zh": "递归内省：教导语言模型代理如何自我改进",
      "authors": [
        "Yuxiao Qu",
        "Tianjun Zhang",
        "Naman Garg",
        "Aviral Kumar"
      ],
      "abstract": "A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.",
      "tldr_zh": "这篇论文提出了 RISE（Recursive IntroSpEction）方法，用于训练语言模型代理（LLMs）实现自我审视和改进能力，以解决现有模型在错误纠正方面的不足。RISE 通过迭代微调过程，将单轮提示转化为多轮 Markov 决策过程（MDP），结合在线模仿学习和强化学习策略，让模型能够递归检测错误并在后续迭代中优化响应。实验结果显示，RISE 显著提升了 Llama2、Llama3 和 Mistral 模型在数学推理任务上的表现，优于单轮策略，且扩展性强，同时不影响模型的单轮能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18219v2",
      "published_date": "2024-07-25 17:35:59 UTC",
      "updated_date": "2024-07-26 17:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:51:00.832643"
    },
    {
      "arxiv_id": "2407.18213v4",
      "title": "Scaling Trends in Language Model Robustness",
      "title_zh": "翻译失败",
      "authors": [
        "Nikolaus Howe",
        "Ian McKenzie",
        "Oskar Hollinsworth",
        "Michał Zajac",
        "Tom Tseng",
        "Aaron Tucker",
        "Pierre-Luc Bacon",
        "Adam Gleave"
      ],
      "abstract": "Language models exhibit scaling laws, whereby increasing model and dataset\nsize predictably decrease negative log likelihood, unlocking a dazzling array\nof capabilities. At the same time, even the most capable systems are currently\nvulnerable to adversarial inputs such as jailbreaks and prompt injections,\ndespite concerted efforts to make them robust. As compute becomes more\naccessible to both attackers and defenders, which side will benefit more from\nscale? We attempt to answer this question with a detailed study of robustness\non language models spanning three orders of magnitude in parameter count. From\nthe defender's perspective, we find that in the absence of other interventions,\nincreasing model size alone does not consistently improve robustness. In\nadversarial training, we find that larger models are more sample-efficient and\nless compute-efficient than smaller models, and often better generalize their\ndefense to new threat models. From the attacker's perspective, we find that\nincreasing attack compute smoothly and reliably increases attack success rate\nagainst both finetuned and adversarially trained models. Finally, we show that\nacross model sizes studied, doubling compute on adversarial training only\nforces an attacker to less than double attack compute to maintain the same\nattack success rate. However, adversarial training becomes more and more\neffective on larger models, suggesting that defenders could eventually have the\nadvantage with increasing model size. These results underscore the value of\nadopting a scaling lens when discussing robustness of frontier models.",
      "tldr_zh": "这篇论文探讨了语言模型规模对鲁棒性的影响，发现增大模型和数据集大小虽能提升模型能力，但无法一致改善对攻击如jailbreaks和prompt injections的防御。在对抗训练中，大模型显示出更高的样本效率和泛化能力，但计算效率较低；同时，攻击计算的增加能平滑提升攻击成功率。研究结果表明，双倍对抗训练计算仅需攻击者少于双倍计算来维持成功率，而在大模型上防御策略更具优势，强调在讨论前沿模型鲁棒性时应采用scaling lens视角。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "58 pages; updated to include new results and analysis",
      "pdf_url": "http://arxiv.org/pdf/2407.18213v4",
      "published_date": "2024-07-25 17:26:41 UTC",
      "updated_date": "2025-02-19 22:32:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:51:13.635868"
    },
    {
      "arxiv_id": "2407.18202v1",
      "title": "Differentiable Quantum Architecture Search in Asynchronous Quantum Reinforcement Learning",
      "title_zh": "异步量子强化学习中的可微量子架构搜索",
      "authors": [
        "Samuel Yen-Chi Chen"
      ],
      "abstract": "The emergence of quantum reinforcement learning (QRL) is propelled by\nadvancements in quantum computing (QC) and machine learning (ML), particularly\nthrough quantum neural networks (QNN) built on variational quantum circuits\n(VQC). These advancements have proven successful in addressing sequential\ndecision-making tasks. However, constructing effective QRL models demands\nsignificant expertise due to challenges in designing quantum circuit\narchitectures, including data encoding and parameterized circuits, which\nprofoundly influence model performance. In this paper, we propose addressing\nthis challenge with differentiable quantum architecture search (DiffQAS),\nenabling trainable circuit parameters and structure weights using\ngradient-based optimization. Furthermore, we enhance training efficiency\nthrough asynchronous reinforcement learning (RL) methods facilitating parallel\ntraining. Through numerical simulations, we demonstrate that our proposed\nDiffQAS-QRL approach achieves performance comparable to manually-crafted\ncircuit architectures across considered environments, showcasing stability\nacross diverse scenarios. This methodology offers a pathway for designing QRL\nmodels without extensive quantum knowledge, ensuring robust performance and\nfostering broader application of QRL.",
      "tldr_zh": "本研究提出了一种可微量子架构搜索（DiffQAS）方法，旨在解决量子强化学习（QRL）中设计量子电路（如数据编码和参数化电路）所需的专业知识挑战，通过梯度-based 优化来训练电路参数和结构权重。结合异步强化学习（RL）技术，该方法支持并行训练，提高了训练效率。在数值模拟中，DiffQAS-QRL 框架在各种环境中展现出与手动设计的电路架构相当的性能，并显示出良好的稳定性。该方法为无需深入量子知识即可构建鲁棒的 QRL 模型提供了新途径，促进 QRL 在更广泛领域的应用。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.DC",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted by IEEE International Conference on Quantum Computing and\n  Engineering - QCE 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18202v1",
      "published_date": "2024-07-25 17:11:00 UTC",
      "updated_date": "2024-07-25 17:11:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:51:39.257598"
    },
    {
      "arxiv_id": "2407.18181v1",
      "title": "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Sindhura Kommu",
        "Yizhi Wang",
        "Yue Wang",
        "Xuan Wang"
      ],
      "abstract": "Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.",
      "tldr_zh": "本研究针对从单细胞RNA测序(scRNA-seq)数据推断基因调控网络(GRNs)的复杂挑战，提出了一种新型联合图学习方法。该方法利用预训练的单细胞BERT模型(scBERT)，结合图神经网络(GNNs)，将scBERT从大量无标签scRNA-seq数据中学得的上下文表示与GRNs的结构化生物知识整合，从而同时考虑基因表达水平约束和生物学知识。在BEELINE研究的人类细胞基准数据集上，实验结果显示该方法优于现有最先进基线，提供对细胞调控机制的更深入理解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted into the ICML 2024 AI for Science workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.18181v1",
      "published_date": "2024-07-25 16:42:08 UTC",
      "updated_date": "2024-07-25 16:42:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:51:50.506105"
    },
    {
      "arxiv_id": "2408.05362v1",
      "title": "MindSpeech: Continuous Imagined Speech Decoding using High-Density fNIRS and Prompt Tuning for Advanced Human-AI Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Suyi Zhang",
        "Ekram Alam",
        "Jack Baber",
        "Francesca Bianco",
        "Edward Turner",
        "Maysam Chamanzar",
        "Hamid Dehghani"
      ],
      "abstract": "In the coming decade, artificial intelligence systems will continue to\nimprove and revolutionise every industry and facet of human life. Designing\neffective, seamless and symbiotic communication paradigms between humans and AI\nagents is increasingly important. This paper reports a novel method for\nhuman-AI interaction by developing a direct brain-AI interface. We discuss a\nnovel AI model, called MindSpeech, which enables open-vocabulary, continuous\ndecoding for imagined speech. This study focuses on enhancing human-AI\ncommunication by utilising high-density functional near-infrared spectroscopy\n(fNIRS) data to develop an AI model capable of decoding imagined speech\nnon-invasively. We discuss a new word cloud paradigm for data collection,\nimproving the quality and variety of imagined sentences generated by\nparticipants and covering a broad semantic space. Utilising a prompt\ntuning-based approach, we employed the Llama2 large language model (LLM) for\ntext generation guided by brain signals. Our results show significant\nimprovements in key metrics, such as BLEU-1 and BERT P scores, for three out of\nfour participants, demonstrating the method's effectiveness. Additionally, we\ndemonstrate that combining data from multiple participants enhances the decoder\nperformance, with statistically significant improvements in BERT scores for two\nparticipants. Furthermore, we demonstrated significantly above-chance decoding\naccuracy for imagined speech versus resting conditions and the identified\nactivated brain regions during imagined speech tasks in our study are\nconsistent with the previous studies on brain regions involved in speech\nencoding. This study underscores the feasibility of continuous imagined speech\ndecoding. By integrating high-density fNIRS with advanced AI techniques, we\nhighlight the potential for non-invasive, accurate communication systems with\nAI in the near future.",
      "tldr_zh": "本文提出MindSpeech模型，利用高密度fNIRS数据实现非侵入式、开放词汇的连续想象语音解码，旨在提升人-AI交互的效率。方法包括引入新的词云数据收集范式来提升数据质量，并采用prompt tuning技术结合Llama2 LLM基于脑信号生成文本。实验结果显示，该模型在BLEU-1和BERT P分数上对三名参与者有显著改善，且整合多参与者数据进一步提升了BERT分数和解码准确率。整体研究证实了想象语音解码的可行性，并验证了激活脑区与既有研究一致，为未来非侵入式脑-AI通信系统提供了潜力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05362v1",
      "published_date": "2024-07-25 16:39:21 UTC",
      "updated_date": "2024-07-25 16:39:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:52:04.201047"
    },
    {
      "arxiv_id": "2407.18178v1",
      "title": "PianoMime: Learning a Generalist, Dexterous Piano Player from Internet Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Cheng Qian",
        "Julen Urain",
        "Kevin Zakka",
        "Jan Peters"
      ],
      "abstract": "In this work, we introduce PianoMime, a framework for training a\npiano-playing agent using internet demonstrations. The internet is a promising\nsource of large-scale demonstrations for training our robot agents. In\nparticular, for the case of piano-playing, Youtube is full of videos of\nprofessional pianists playing a wide myriad of songs. In our work, we leverage\nthese demonstrations to learn a generalist piano-playing agent capable of\nplaying any arbitrary song. Our framework is divided into three parts: a data\npreparation phase to extract the informative features from the Youtube videos,\na policy learning phase to train song-specific expert policies from the\ndemonstrations and a policy distillation phase to distil the policies into a\nsingle generalist agent. We explore different policy designs to represent the\nagent and evaluate the influence of the amount of training data on the\ngeneralization capability of the agent to novel songs not available in the\ndataset. We show that we are able to learn a policy with up to 56\\% F1 score on\nunseen songs.",
      "tldr_zh": "本研究引入了PianoMime框架，通过利用YouTube上的专业钢琴家演示视频，训练一个通用的、灵巧的钢琴演奏代理。框架分为三个阶段：数据准备阶段从视频中提取关键特征、policy learning阶段训练特定歌曲的专家策略，以及policy distillation阶段将这些策略提炼成单一的通用代理。实验结果显示，该代理在未见过歌曲上实现了高达56%的F1 score，证明了训练数据量对代理泛化能力的积极影响。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18178v1",
      "published_date": "2024-07-25 16:37:07 UTC",
      "updated_date": "2024-07-25 16:37:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:52:17.235220"
    },
    {
      "arxiv_id": "2407.18175v1",
      "title": "Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for Vision Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengang Li",
        "Alec Lu",
        "Yanyue Xie",
        "Zhenglun Kong",
        "Mengshu Sun",
        "Hao Tang",
        "Zhong Jia Xue",
        "Peiyan Dong",
        "Caiwen Ding",
        "Yanzhi Wang",
        "Xue Lin",
        "Zhenman Fang"
      ],
      "abstract": "Vision transformers (ViTs) have demonstrated their superior accuracy for\ncomputer vision tasks compared to convolutional neural networks (CNNs).\nHowever, ViT models are often computation-intensive for efficient deployment on\nresource-limited edge devices. This work proposes Quasar-ViT, a\nhardware-oriented quantization-aware architecture search framework for ViTs, to\ndesign efficient ViT models for hardware implementation while preserving the\naccuracy. First, Quasar-ViT trains a supernet using our row-wise flexible\nmixed-precision quantization scheme, mixed-precision weight entanglement, and\nsupernet layer scaling techniques. Then, it applies an efficient\nhardware-oriented search algorithm, integrated with hardware latency and\nresource modeling, to determine a series of optimal subnets from supernet under\ndifferent inference latency targets. Finally, we propose a series of\nmodel-adaptive designs on the FPGA platform to support the architecture search\nand mitigate the gap between the theoretical computation reduction and the\npractical inference speedup. Our searched models achieve 101.5, 159.6, and\n251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA\nwith 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet\ndataset, consistently outperforming prior works.",
      "tldr_zh": "该研究提出Quasar-ViT，一种硬件导向的量化感知架构搜索框架，旨在优化Vision Transformers (ViTs)模型，使其在资源有限的边缘设备上高效部署，同时保持高准确率。框架通过row-wise flexible mixed-precision quantization scheme、mixed-precision weight entanglement和supernet layer scaling技术训练supernet，并结合硬件延迟和资源建模的搜索算法，生成针对不同推理延迟目标的最优子网。在AMD/Xilinx ZCU102 FPGA平台上，搜索的模型在ImageNet数据集上实现了101.5、159.6和251.6 FPS的推理速度，并分别达到80.4%、78.6%和74.9%的top-1准确率，优于现有工作。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ICS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18175v1",
      "published_date": "2024-07-25 16:35:46 UTC",
      "updated_date": "2024-07-25 16:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:52:49.059873"
    },
    {
      "arxiv_id": "2408.05369v1",
      "title": "A Cost-Effective Eye-Tracker for Early Detection of Mild Cognitive Impairment",
      "title_zh": "翻译失败",
      "authors": [
        "Danilo Greco",
        "Francesco Masulli",
        "Stefano Rovetta",
        "Alberto Cabri",
        "Davide Daffonchio"
      ],
      "abstract": "This paper presents a low-cost eye-tracker aimed at carrying out tests based\non a Visual Paired Comparison protocol for the early detection of Mild\nCognitive Impairment. The proposed eye-tracking system is based on machine\nlearning algorithms, a standard webcam, and two personal computers that\nconstitute, respectively, the \"Measurement Sub-System\" performing the test on\nthe patients and the \"Test Management Sub-System\" used by medical staff for\nconfiguring the test protocol, recording the patient data, monitoring the test\nand storing the test results. The system also integrates an stress estimator\nbased on the measurement of heart rate variability obtained with\nphotoplethysmography.",
      "tldr_zh": "本论文提出了一种低成本的 Eye-Tracker 系统，旨在通过 Visual Paired Comparison 协议进行测试，以实现 Mild Cognitive Impairment 的早期检测。系统采用 Machine Learning 算法、标准 Webcam 以及两台个人电脑（Measurement Sub-System 用于患者测试，Test Management Sub-System 用于配置协议、记录数据、监控和存储结果）。此外，该系统还整合了基于 Heart Rate Variability 和 Photoplethysmography 的压力估计器，提升了测试的全面性。该设计为临床应用提供了经济有效的工具。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "68T07, 68T10, 68T45",
        "I.2; I.4; I.5"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05369v1",
      "published_date": "2024-07-25 16:00:02 UTC",
      "updated_date": "2024-07-25 16:00:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:52:42.226250"
    },
    {
      "arxiv_id": "2407.18145v2",
      "title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for Open-World Perception",
      "title_zh": "翻译失败",
      "authors": [
        "Julia Hindel",
        "Daniele Cattaneo",
        "Abhinav Valada"
      ],
      "abstract": "Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.",
      "tldr_zh": "该论文针对语义分割模型在开放世界感知中的局限性，提出 TOPICS（Taxonomy-Oriented Poincaré-regularized Incremental-Class Segmentation）方法，在双曲空间（hyperbolic spaces）中学习特征嵌入，并遵循显式分类学树结构，以实现类增量学习。TOPICS 通过提供旧类的可塑性、更新祖先类并整合新类，同时维护 Poincaré ball 中的隐式类关系约束，有效抵抗 catastrophic forgetting。实验在 Cityscapes 和 Mapillary Vistas 2.0 基准上展示了 state-of-the-art 性能，并建立了八个自动驾驶场景的增量学习协议。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18145v2",
      "published_date": "2024-07-25 15:49:26 UTC",
      "updated_date": "2024-11-04 17:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:53:00.037107"
    },
    {
      "arxiv_id": "2407.18143v1",
      "title": "Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Jean Seong Bjorn Choe",
        "Jong-Kook Kim"
      ],
      "abstract": "Entropy Regularisation is a widely adopted technique that enhances policy\noptimisation performance and stability. A notable form of entropy\nregularisation is augmenting the objective with an entropy term, thereby\nsimultaneously optimising the expected return and the entropy. This framework,\nknown as maximum entropy reinforcement learning (MaxEnt RL), has shown\ntheoretical and empirical successes. However, its practical application in\nstraightforward on-policy actor-critic settings remains surprisingly\nunderexplored. We hypothesise that this is due to the difficulty of managing\nthe entropy reward in practice. This paper proposes a simple method of\nseparating the entropy objective from the MaxEnt RL objective, which\nfacilitates the implementation of MaxEnt RL in on-policy settings. Our\nempirical evaluations demonstrate that extending Proximal Policy Optimisation\n(PPO) and Trust Region Policy Optimisation (TRPO) within the MaxEnt framework\nimproves policy optimisation performance in both MuJoCo and Procgen tasks.\nAdditionally, our results highlight MaxEnt RL's capacity to enhance\ngeneralisation.",
      "tldr_zh": "这篇论文探讨了熵正则化（Entropy Regularisation）在强化学习中的应用，提出了一种简单方法，通过熵优势估计（Entropy Advantage Estimation）将熵目标从最大熵强化学习（MaxEnt RL）目标中分离，从而便于在on-policy actor-critic设置中实现。方法解决了管理熵奖励的实际困难，并扩展了Proximal Policy Optimisation (PPO)和Trust Region Policy Optimisation (TRPO)框架。实验结果显示，这种MaxEnt RL方法在MuJoCo和Procgen任务中显著提高了策略优化性能，并增强了模型的泛化能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18143v1",
      "published_date": "2024-07-25 15:48:24 UTC",
      "updated_date": "2024-07-25 15:48:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:53:47.113276"
    },
    {
      "arxiv_id": "2407.18129v2",
      "title": "Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic",
      "title_zh": "翻译失败",
      "authors": [
        "Fakhraddin Alwajih",
        "Gagan Bhatia",
        "Muhammad Abdul-Mageed"
      ],
      "abstract": "Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.",
      "tldr_zh": "本研究介绍了Dallah，一种针对阿拉伯语的方言感知多模态大型语言模型（MLLMs），旨在解决非英语语言资源匮乏的问题。Dallah基于LLaMA-2先进语言模型，通过微调六种阿拉伯方言，实现对文本和视觉元素的复杂交互处理。实验结果显示，Dallah在阿拉伯MLLMs中达到最先进性能，在评估现代标准阿拉伯语（MSA）和方言响应的基准测试中表现出色。该模型为发展更多方言感知的阿拉伯MLLMs铺平了道路。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18129v2",
      "published_date": "2024-07-25 15:36:48 UTC",
      "updated_date": "2024-07-26 15:34:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:53:44.174999"
    },
    {
      "arxiv_id": "2407.18125v3",
      "title": "Self-supervised pre-training with diffusion model for few-shot landmark detection in x-ray images",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto Di Via",
        "Francesca Odone",
        "Vito Paolo Pastore"
      ],
      "abstract": "Deep neural networks have been extensively applied in the medical domain for\nvarious tasks, including image classification, segmentation, and landmark\ndetection. However, their application is often hindered by data scarcity, both\nin terms of available annotations and images. This study introduces a novel\napplication of denoising diffusion probabilistic models (DDPMs) to the landmark\ndetection task, specifically addressing the challenge of limited annotated data\nin x-ray imaging. Our key innovation lies in leveraging DDPMs for\nself-supervised pre-training in landmark detection, a previously unexplored\napproach in this domain. This method enables accurate landmark detection with\nminimal annotated training data (as few as 50 images), surpassing both ImageNet\nsupervised pre-training and traditional self-supervised techniques across three\npopular x-ray benchmark datasets. To our knowledge, this work represents the\nfirst application of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.",
      "tldr_zh": "本研究提出了一种利用去噪扩散概率模型（DDPMs）进行自监督预训练的方法，旨在解决X射线图像中地标检测任务的数据稀缺问题，特别是标注数据有限的场景。创新点在于首次将DDPMs应用于地标检测的自监督学习，仅需少量标注数据（如50张图像）即可实现高精度检测。实验结果显示，该方法在三个流行X射线基准数据集上优于ImageNet监督预训练和传统自监督技术，为few-shot landmark detection提供了一种有效的预训练策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.18125v3",
      "published_date": "2024-07-25 15:32:59 UTC",
      "updated_date": "2025-03-06 17:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:53:26.587977"
    },
    {
      "arxiv_id": "2407.18110v1",
      "title": "MapTune: Advancing ASIC Technology Mapping via Reinforcement Learning Guided Library Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Mingju Liu",
        "Daniel Robinson",
        "Yingjie Li",
        "Cunxi Yu"
      ],
      "abstract": "Technology mapping involves mapping logical circuits to a library of cells.\nTraditionally, the full technology library is used, leading to a large search\nspace and potential overhead. Motivated by randomly sampled technology mapping\ncase studies, we propose MapTune framework that addresses this challenge by\nutilizing reinforcement learning to make design-specific choices during cell\nselection. By learning from the environment, MapTune refines the cell selection\nprocess, resulting in a reduced search space and potentially improved mapping\nquality.\n  The effectiveness of MapTune is evaluated on a wide range of benchmarks,\ndifferent technology libraries and technology mappers. The experimental results\ndemonstrate that MapTune achieves higher mapping accuracy and reducing\ndelay/area across diverse circuit designs, technology libraries and mappers.\nThe paper also discusses the Pareto-Optimal exploration and confirms the\nperpetual delay-area trade-off. Conducted on benchmark suites ISCAS 85/89,\nITC/ISCAS 99, VTR8.0 and EPFL benchmarks, the post-technology mapping and\npost-sizing quality-of-results (QoR) have been significantly improved, with\naverage Area-Delay Product (ADP) improvement of 22.54\\% among all different\nexploration settings in MapTune. The improvements are consistently remained for\nfour different technologies (7nm, 45nm, 130nm, and 180 nm) and two different\nmappers.",
      "tldr_zh": "这篇论文提出了 MapTune 框架，通过 Reinforcement Learning 指导的库调优（Library Tuning）来优化 ASIC Technology Mapping，从而减少传统方法中的搜索空间和开销。MapTune 通过从环境中学习进行设计特定的单元选择，提升映射质量，并在各种基准测试（如 ISCAS 85/89、ITC/ISCAS 99、VTR8.0 和 EPFL 基准）中验证其有效性。实验结果显示，该框架平均改善 Area-Delay Product (ADP) 22.54%，并在不同技术节点（7nm、45nm、130nm 和 180nm）和映射器上实现了更高的映射准确性及延时/面积减少。论文还讨论了 Pareto-Optimal 探索，确认了延时-面积权衡的持续性。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD\n  '24), October 27--31, 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18110v1",
      "published_date": "2024-07-25 15:18:47 UTC",
      "updated_date": "2024-07-25 15:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:54:13.634270"
    },
    {
      "arxiv_id": "2407.18105v1",
      "title": "Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer Subtyping",
      "title_zh": "多分辨率组织病理学补丁图用于卵巢癌亚型分类",
      "authors": [
        "Jack Breen",
        "Katie Allen",
        "Kieran Zucker",
        "Nicolas M. Orsi",
        "Nishant Ravikumar"
      ],
      "abstract": "Computer vision models are increasingly capable of classifying ovarian\nepithelial cancer subtypes, but they differ from pathologists by processing\nsmall tissue patches at a single resolution. Multi-resolution graph models\nleverage the spatial relationships of patches at multiple magnifications,\nlearning the context for each patch. In this study, we conduct the most\nthorough validation of a graph model for ovarian cancer subtyping to date.\nSeven models were tuned and trained using five-fold cross-validation on a set\nof 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching\nHospitals NHS Trust. The cross-validation models were ensembled and evaluated\nusing a balanced hold-out test set of 100 WSIs from 30 patients, and an\nexternal validation set of 80 WSIs from 80 patients in the Transcanadian Study.\nThe best-performing model, a graph model using 10x+20x magnification data, gave\nbalanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing,\nand external validation, respectively. However, this only exceeded the\nperformance of attention-based multiple instance learning in external\nvalidation, with a 93% balanced accuracy. Graph models benefitted greatly from\nusing the UNI foundation model rather than an ImageNet-pretrained ResNet50 for\nfeature extraction, with this having a much greater effect on performance than\nchanging the subsequent classification approach. The accuracy of the combined\nfoundation model and multi-resolution graph network offers a step towards the\nclinical applicability of these models, with a new highest-reported performance\nfor this task, though further validations are still required to ensure the\nrobustness and usability of the models.",
      "tldr_zh": "本文提出多分辨率组织斑块图模型，用于卵巢上皮癌亚型的分类，通过整合多个放大倍数（如10x+20x）的空间关系，学习每个斑块的上下文，从而超越传统单一分辨率方法的局限。研究在1864张全滑图像（WSIs）上训练并验证7个模型，最终最佳模型在交叉验证、保留测试和外部验证中分别获得73%、88%和99%的平衡准确率，使用UNI基础模型进行特征提取比ImageNet-pretrained ResNet50更有效。相比注意力-based multiple instance learning，该方法在外部验证中表现出色，标志着向临床应用迈进，但仍需进一步验证以确保鲁棒性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Initially submitted version of a paper which has been accepted in the\n  GRAIL workshop at MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18105v1",
      "published_date": "2024-07-25 15:08:54 UTC",
      "updated_date": "2024-07-25 15:08:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:54:23.360613"
    },
    {
      "arxiv_id": "2407.18096v1",
      "title": "Privacy Threats and Countermeasures in Federated Learning for Internet of Things: A Systematic Review",
      "title_zh": "物联网联邦学习中的隐私威胁与应对措施：系统综述",
      "authors": [
        "Adel ElZemity",
        "Budi Arief"
      ],
      "abstract": "Federated Learning (FL) in the Internet of Things (IoT) environments can\nenhance machine learning by utilising decentralised data, but at the same time,\nit might introduce significant privacy and security concerns due to the\nconstrained nature of IoT devices. This represents a research challenge that we\naim to address in this paper. We systematically analysed recent literature to\nidentify privacy threats in FL within IoT environments, and evaluate the\ndefensive measures that can be employed to mitigate these threats. Using a\nSystematic Literature Review (SLR) approach, we searched five publication\ndatabases (Scopus, IEEE Xplore, Wiley, ACM, and Science Direct), collating\nrelevant papers published between 2017 and April 2024, a period which spans\nfrom the introduction of FL until now. Guided by the PRISMA protocol, we\nselected 49 papers to focus our systematic review on. We analysed these papers,\npaying special attention to the privacy threats and defensive measures --\nspecifically within the context of IoT -- using inclusion and exclusion\ncriteria tailored to highlight recent advances and critical insights. We\nidentified various privacy threats, including inference attacks, poisoning\nattacks, and eavesdropping, along with defensive measures such as Differential\nPrivacy and Secure Multi-Party Computation. These defences were evaluated for\ntheir effectiveness in protecting privacy without compromising the functional\nintegrity of FL in IoT settings. Our review underscores the necessity for\nrobust and efficient privacy-preserving strategies tailored for IoT\nenvironments. Notably, there is a need for strategies against replay, evasion,\nand model stealing attacks. Exploring lightweight defensive measures and\nemerging technologies such as blockchain may help improve the privacy of FL in\nIoT, leading to the creation of FL models that can operate under variable\nnetwork conditions.",
      "tldr_zh": "本论文通过系统文献综述（Systematic Literature Review, SLR）分析了Federated Learning (FL)在Internet of Things (IoT)环境中的隐私威胁和防御措施，共审阅了2017年至2024年4月的49篇相关文献。研究识别了主要隐私威胁，如inference attacks、poisoning attacks和eavesdropping，并评估了Differential Privacy和Secure Multi-Party Computation等防御策略的有效性，这些措施能保护隐私同时维持FL的功能完整性。该综述强调了针对IoT的轻量级防御需求，并建议探索blockchain等新兴技术来应对replay、evasion和model stealing attacks，从而提升FL在可变网络条件下的隐私保护。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18096v1",
      "published_date": "2024-07-25 15:01:56 UTC",
      "updated_date": "2024-07-25 15:01:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:54:37.725541"
    },
    {
      "arxiv_id": "2407.18078v1",
      "title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization",
      "title_zh": "翻译失败",
      "authors": [
        "Christopher Clarke",
        "Yuzhao Heng",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "abstract": "The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.",
      "tldr_zh": "大型语言模型 (LLMs) 如 Chat-GPT 虽然在语言理解方面表现出色，但其通用设计往往忽略了用户个体的多样性和个性化需求，导致无法有效处理用户特定偏好。  \n本文引入 PEFT-U Benchmark，这是一个新数据集，包含多样化的用户中心任务，用于评估和构建 NLP 模型的个性化能力。  \n通过 PEFT-U，我们探索了参数高效微调 (Parameter-Efficient Fine-Tuning) 方法，以高效地适应用户偏好，并在各种任务中提升模型的个性化性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18078v1",
      "published_date": "2024-07-25 14:36:18 UTC",
      "updated_date": "2024-07-25 14:36:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:55:55.412109"
    },
    {
      "arxiv_id": "2407.18061v1",
      "title": "Difficulty Estimation and Simplification of French Text Using LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Henri Jamet",
        "Yash Raj Shrestha",
        "Michalis Vlachos"
      ],
      "abstract": "We leverage generative large language models for language learning\napplications, focusing on estimating the difficulty of foreign language texts\nand simplifying them to lower difficulty levels. We frame both tasks as\nprediction problems and develop a difficulty classification model using labeled\nexamples, transfer learning, and large language models, demonstrating superior\naccuracy compared to previous approaches. For simplification, we evaluate the\ntrade-off between simplification quality and meaning preservation, comparing\nzero-shot and fine-tuned performances of large language models. We show that\nmeaningful text simplifications can be obtained with limited fine-tuning. Our\nexperiments are conducted on French texts, but our methods are\nlanguage-agnostic and directly applicable to other foreign languages.",
      "tldr_zh": "本研究利用大型语言模型（LLMs）来估计外语文本的难度并进行简化，将两者视为预测问题。研究开发了一个难度分类模型，通过标记示例、transfer learning和LLMs，实现了比以往方法更高的准确率。对于文本简化，评估了zero-shot和fine-tuned LLMs的表现，探讨了简化质量与含义保留的权衡，并证明有限fine-tuning即可获得有效的简化结果。该方法基于法语文本进行实验，但具有语言无关性，可直接应用于其他语言。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18061v1",
      "published_date": "2024-07-25 14:16:08 UTC",
      "updated_date": "2024-07-25 14:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:55:14.638770"
    },
    {
      "arxiv_id": "2407.18046v1",
      "title": "GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale Image Super-Resolution",
      "title_zh": "翻译失败",
      "authors": [
        "Jintong Hu",
        "Bin Xia",
        "Bin Chen",
        "Wenming Yang",
        "Lei Zhang"
      ],
      "abstract": "Implicit neural representations (INRs) have significantly advanced the field\nof arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based\nASSR networks first extract features from the given low-resolution image using\nan encoder, and then render the super-resolved result via a multi-layer\nperceptron decoder. Although these approaches have shown promising results,\ntheir performance is constrained by the limited representation ability of\ndiscrete latent codes in the encoded features. In this paper, we propose a\nnovel ASSR method named GaussianSR that overcomes this limitation through 2D\nGaussian Splatting (2DGS). Unlike traditional methods that treat pixels as\ndiscrete points, GaussianSR represents each pixel as a continuous Gaussian\nfield. The encoded features are simultaneously refined and upsampled by\nrendering the mutually stacked Gaussian fields. As a result, long-range\ndependencies are established to enhance representation ability. In addition, a\nclassifier is developed to dynamically assign Gaussian kernels to all pixels to\nfurther improve flexibility. All components of GaussianSR (i.e., encoder,\nclassifier, Gaussian kernels, and decoder) are jointly learned end-to-end.\nExperiments demonstrate that GaussianSR achieves superior ASSR performance with\nfewer parameters than existing methods while enjoying interpretable and\ncontent-aware feature aggregations.",
      "tldr_zh": "该论文提出 GaussianSR，一种基于 2D Gaussian Splatting (2DGS) 的任意尺度图像超分辨率 (ASSR) 方法，以克服隐式神经表示 (INRs) 中编码特征离散潜在码的局限性。GaussianSR 将像素表示为连续的高斯场，通过渲染相互堆叠的 Gaussian 字段来细化和上采样特征，建立长程依赖关系，并引入一个分类器动态分配 Gaussian 内核以提升灵活性。所有组件（编码器、分类器、Gaussian 内核和解码器）端到端联合学习，实验结果显示 GaussianSR 比现有方法在 ASSR 性能上更出色，使用更少参数，并实现可解释和内容感知的特征聚合。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18046v1",
      "published_date": "2024-07-25 13:53:48 UTC",
      "updated_date": "2024-07-25 13:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:55:19.531128"
    },
    {
      "arxiv_id": "2407.20276v2",
      "title": "Assessing AI Utility: The Random Guesser Test for Sequential Decision-Making Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Shun Ide",
        "Allison Blunt",
        "Djallel Bouneffouf"
      ],
      "abstract": "We propose a general approach to quantitatively assessing the risk and\nvulnerability of artificial intelligence (AI) systems to biased decisions. The\nguiding principle of the proposed approach is that any AI algorithm must\noutperform a random guesser. This may appear trivial, but empirical results\nfrom a simplistic sequential decision-making scenario involving roulette games\nshow that sophisticated AI-based approaches often underperform the random\nguesser by a significant margin. We highlight that modern recommender systems\nmay exhibit a similar tendency to favor overly low-risk options. We argue that\nthis \"random guesser test\" can serve as a useful tool for evaluating the\nutility of AI actions, and also points towards increasing exploration as a\npotential improvement to such systems.",
      "tldr_zh": "本论文提出“Random Guesser Test”，一种评估人工智能(AI)系统在顺序决策(Sequential Decision-Making Systems)中的风险和易受偏见影响的通用方法，该测试的核心原则是AI算法必须优于随机猜测者。实验通过一个简单的轮盘游戏场景显示，许多先进的AI方法表现不如随机猜测者，准确率显著落后。论文进一步指出，现代推荐系统可能存在类似问题，即过度偏好低风险选项，并建议通过增加探索来提升AI系统的效用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted into AIBS 2024: The First Workshop on AI Behavioral Science,\n  5 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.20276v2",
      "published_date": "2024-07-25 13:44:22 UTC",
      "updated_date": "2024-08-11 13:56:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:55:38.818876"
    },
    {
      "arxiv_id": "2407.18039v1",
      "title": "Peak-Controlled Logits Poisoning Attack in Federated Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Tang",
        "Aoxu Zhang",
        "Zhiyuan Wu",
        "Bo Gao",
        "Tian Wen",
        "Yuwei Wang",
        "Sheng Sun"
      ],
      "abstract": "Federated Distillation (FD) offers an innovative approach to distributed\nmachine learning, leveraging knowledge distillation for efficient and flexible\ncross-device knowledge transfer without necessitating the upload of extensive\nmodel parameters to a central server. While FD has gained popularity, its\nvulnerability to poisoning attacks remains underexplored. To address this gap,\nwe previously introduced FDLA (Federated Distillation Logits Attack), a method\nthat manipulates logits communication to mislead and degrade the performance of\nclient models. However, the impact of FDLA on participants with different\nidentities and the effects of malicious modifications at various stages of\nknowledge transfer remain unexplored. To this end, we present PCFDLA\n(Peak-Controlled Federated Distillation Logits Attack), an advanced and more\nstealthy logits poisoning attack method for FD. PCFDLA enhances the\neffectiveness of FDLA by carefully controlling the peak values of logits to\ncreate highly misleading yet inconspicuous modifications. Furthermore, we\nintroduce a novel metric for better evaluating attack efficacy, demonstrating\nthat PCFDLA maintains stealth while being significantly more disruptive to\nvictim models compared to its predecessors. Experimental results across various\ndatasets confirm the superior impact of PCFDLA on model accuracy, solidifying\nits potential threat in federated distillation systems.",
      "tldr_zh": "本研究探讨了Federated Distillation (FD) 在分布式机器学习中的应用及其对投毒攻击的脆弱性，特别针对先前提出的FDLA (Federated Distillation Logits Attack) 方法进行了改进。论文引入了PCFDLA (Peak-Controlled Federated Distillation Logits Attack)，一种更隐蔽的logits投毒攻击，通过精确控制logits的峰值来制造误导性修改，同时保持攻击的隐蔽性。实验结果显示，PCFDLA 在各种数据集上显著降低了受害者模型的准确性，比前驱方法更具破坏力，并通过新颖的评估指标证实了其潜在威胁。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2401.03685",
      "pdf_url": "http://arxiv.org/pdf/2407.18039v1",
      "published_date": "2024-07-25 13:36:42 UTC",
      "updated_date": "2024-07-25 13:36:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:56:07.089734"
    },
    {
      "arxiv_id": "2407.18035v1",
      "title": "RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models",
      "title_zh": "RestoreAgent：通过多模",
      "authors": [
        "Haoyu Chen",
        "Wenbo Li",
        "Jinjin Gu",
        "Jingjing Ren",
        "Sixiang Chen",
        "Tian Ye",
        "Renjing Pei",
        "Kaiwen Zhou",
        "Fenglong Song",
        "Lei Zhu"
      ],
      "abstract": "Natural images captured by mobile devices often suffer from multiple types of\ndegradation, such as noise, blur, and low light. Traditional image restoration\nmethods require manual selection of specific tasks, algorithms, and execution\nsequences, which is time-consuming and may yield suboptimal results. All-in-one\nmodels, though capable of handling multiple tasks, typically support only a\nlimited range and often produce overly smooth, low-fidelity outcomes due to\ntheir broad data distribution fitting. To address these challenges, we first\ndefine a new pipeline for restoring images with multiple degradations, and then\nintroduce RestoreAgent, an intelligent image restoration system leveraging\nmultimodal large language models. RestoreAgent autonomously assesses the type\nand extent of degradation in input images and performs restoration through (1)\ndetermining the appropriate restoration tasks, (2) optimizing the task\nsequence, (3) selecting the most suitable models, and (4) executing the\nrestoration. Experimental results demonstrate the superior performance of\nRestoreAgent in handling complex degradation, surpassing human experts.\nFurthermore, the system modular design facilitates the fast integration of new\ntasks and models, enhancing its flexibility and scalability for various\napplications.",
      "tldr_zh": "该论文提出RestoreAgent，一种基于Multimodal Large Language Models的自主图像恢复系统，旨在解决传统方法需手动选择任务和序列的低效问题，以及全能模型在处理多种退化（如噪声、模糊和低光）时的保真度不足。RestoreAgent通过评估图像退化类型和程度、优化任务序列、选择合适模型并执行恢复，实现自动化处理。实验结果显示，该系统在复杂退化场景中超越人类专家的表现，并通过模块化设计提升了灵活性和可扩展性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18035v1",
      "published_date": "2024-07-25 13:29:37 UTC",
      "updated_date": "2024-07-25 13:29:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:55:58.575870"
    },
    {
      "arxiv_id": "2407.18034v1",
      "title": "AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild",
      "title_zh": "AttentionHand：文本驱动的可控手部图像生成，用于野外 3D 手部重建",
      "authors": [
        "Junho Park",
        "Kyeongbo Kong",
        "Suk-Ju Kang"
      ],
      "abstract": "Recently, there has been a significant amount of research conducted on 3D\nhand reconstruction to use various forms of human-computer interaction.\nHowever, 3D hand reconstruction in the wild is challenging due to extreme lack\nof in-the-wild 3D hand datasets. Especially, when hands are in complex pose\nsuch as interacting hands, the problems like appearance similarity, self-handed\noccclusion and depth ambiguity make it more difficult. To overcome these\nissues, we propose AttentionHand, a novel method for text-driven controllable\nhand image generation. Since AttentionHand can generate various and numerous\nin-the-wild hand images well-aligned with 3D hand label, we can acquire a new\n3D hand dataset, and can relieve the domain gap between indoor and outdoor\nscenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand\nmesh image from 3D label, a bounding box, and a text prompt). These modalities\nare embedded into the latent space by the encoding phase. Then, through the\ntext attention stage, hand-related tokens from the given text prompt are\nattended to highlight hand-related regions of the latent embedding. After the\nhighlighted embedding is fed to the visual attention stage, hand-related\nregions in the embedding are attended by conditioning global and local hand\nmesh images with the diffusion-based pipeline. In the decoding phase, the final\nfeature is decoded to new hand images, which are well-aligned with the given\nhand mesh image and text prompt. As a result, AttentionHand achieved\nstate-of-the-art among text-to-hand image generation models, and the\nperformance of 3D hand mesh reconstruction was improved by additionally\ntraining with hand images generated by AttentionHand.",
      "tldr_zh": "该论文针对野外 3D hand reconstruction 的数据缺乏和复杂姿势（如互动手部）问题，提出了一种新型方法 AttentionHand，用于文本驱动的可控手部图像生成。该方法通过编码四个模态（RGB 图像、手部网格图像、边界框和文本提示）将输入嵌入潜在空间，然后利用文本注意力阶段突出手部相关区域，并结合视觉注意力阶段和 diffusion-based pipeline 生成与给定手部网格图像和文本提示高度对齐的新图像。结果显示，AttentionHand 在文本到手部图像生成中达到最先进水平，并通过使用生成的图像来辅助训练，显著提高了 3D hand mesh reconstruction 的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18034v1",
      "published_date": "2024-07-25 13:29:32 UTC",
      "updated_date": "2024-07-25 13:29:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:56:03.453887"
    },
    {
      "arxiv_id": "2407.18022v1",
      "title": "Learning mental states estimation through self-observation: a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind",
      "title_zh": "翻译失败",
      "authors": [
        "Francesca Bianco",
        "Silvia Rigato",
        "Maria Laura Filippetti",
        "Dimitri Ognibene"
      ],
      "abstract": "Theory of Mind (ToM), the ability to attribute beliefs, intentions, or mental\nstates to others, is a crucial feature of human social interaction. In complex\nenvironments, where the human sensory system reaches its limits, behaviour is\nstrongly driven by our beliefs about the state of the world around us.\nAccessing others' mental states, e.g., beliefs and intentions, allows for more\neffective social interactions in natural contexts. Yet, these variables are not\ndirectly observable, making understanding ToM a challenging quest of interest\nfor different fields, including psychology, machine learning and robotics. In\nthis paper, we contribute to this topic by showing a developmental synergy\nbetween learning to predict low-level mental states (e.g., intentions, goals)\nand attributing high-level ones (i.e., beliefs). Specifically, we assume that\nlearning beliefs attribution can occur by observing one's own decision\nprocesses involving beliefs, e.g., in a partially observable environment. Using\na simple feed-forward deep learning model, we show that, when learning to\npredict others' intentions and actions, more accurate predictions can be\nacquired earlier if beliefs attribution is learnt simultaneously. Furthermore,\nwe show that the learning performance improves even when observed actors have a\ndifferent embodiment than the observer and the gain is higher when observing\nbeliefs-driven chunks of behaviour. We propose that our computational approach\ncan inform the understanding of human social cognitive development and be\nrelevant for the design of future adaptive social robots able to autonomously\nunderstand, assist, and learn from human interaction partners in novel natural\nenvironments and tasks.",
      "tldr_zh": "这篇论文探讨了 Theory of Mind (ToM) 的发育协同效应，展示了通过自我观察学习心理状态估计的过程，即在深度学习模型中，意图和信念表示之间存在协同作用。研究使用一个简单的 feed-forward deep learning model，在部分可观察环境中，让模型通过观察自身决策过程来同时学习预测他人意图和归因信念。结果显示，这种联合学习方法能显著提高意图预测的准确性和效率，即使观察对象与模型有不同实体形式，且在信念驱动的行为中表现更佳。该方法为人类社会认知发展和设计自适应社会机器人提供了重要启发。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18022v1",
      "published_date": "2024-07-25 13:15:25 UTC",
      "updated_date": "2024-07-25 13:15:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:56:34.114118"
    },
    {
      "arxiv_id": "2407.18021v1",
      "title": "Quadratic Advantage with Quantum Randomized Smoothing Applied to Time-Series Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Nicola Franco",
        "Marie Kempkes",
        "Jakob Spiegelberg",
        "Jeanette Miriam Lorenz"
      ],
      "abstract": "As quantum machine learning continues to develop at a rapid pace, the\nimportance of ensuring the robustness and efficiency of quantum algorithms\ncannot be overstated. Our research presents an analysis of quantum randomized\nsmoothing, how data encoding and perturbation modeling approaches can be\nmatched to achieve meaningful robustness certificates. By utilizing an\ninnovative approach integrating Grover's algorithm, a quadratic sampling\nadvantage over classical randomized smoothing is achieved. This strategy\nnecessitates a basis state encoding, thus restricting the space of meaningful\nperturbations. We show how constrained $k$-distant Hamming weight perturbations\nare a suitable noise distribution here, and elucidate how they can be\nconstructed on a quantum computer. The efficacy of the proposed framework is\ndemonstrated on a time series classification task employing a Bag-of-Words\npre-processing solution. The advantage of quadratic sample reduction is\nrecovered especially in the regime with large number of samples. This may allow\nquantum computers to efficiently scale randomized smoothing to more complex\ntasks beyond the reach of classical methods.",
      "tldr_zh": "本研究分析了量子随机平滑（quantum randomized smoothing）的鲁棒性和效率，通过整合Grover's algorithm 实现了相对于经典方法的二次采样优势（quadratic sampling advantage）。该框架采用basis state encoding 来限制扰动空间，并引入受限的k-distant Hamming weight perturbations 作为合适的噪声分布，在量子计算机上进行构建。实验在采用Bag-of-Words预处理的时序分类任务中验证了其有效性，尤其在大样本数量下，量子方法显著减少了样本需求。总体而言，此方法为量子计算机扩展到更复杂任务提供了高效的可扩展性。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted at the IEEE International Conference on Quantum Computing\n  and Engineering (QCE)",
      "pdf_url": "http://arxiv.org/pdf/2407.18021v1",
      "published_date": "2024-07-25 13:15:16 UTC",
      "updated_date": "2024-07-25 13:15:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:56:43.230519"
    },
    {
      "arxiv_id": "2407.18017v1",
      "title": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Tom Eivind Glover",
        "Ruben Jahren",
        "Francesco Martinuzzi",
        "Pedro Gonçalves Lind",
        "Stefano Nichele"
      ],
      "abstract": "Elementary Cellular Automata (ECA) are a well-studied computational universe\nthat is, despite its simple configurations, capable of impressive computational\nvariety. Harvesting this computation in a useful way has historically shown\nitself to be difficult, but if combined with reservoir computing (RC), this\nbecomes much more feasible. Furthermore, RC and ECA enable energy-efficient AI,\nmaking the combination a promising concept for Edge AI. In this work, we\ncontrast ECA to substrates of Partially-Local CA (PLCA) and Homogeneous\nHomogeneous Random Boolean Networks (HHRBN). They are, in comparison, the\ntopological heterogeneous counterparts of ECA. This represents a step from ECA\ntowards more biological-plausible substrates. We analyse these substrates by\ntesting on an RC benchmark (5-bit memory), using Temporal Derrida plots to\nestimate the sensitivity and assess the defect collapse rate. We find that,\ncounterintuitively, disordered topology does not necessarily mean disordered\ncomputation. There are countering computational \"forces\" of topology\nimperfections leading to a higher collapse rate (order) and yet, if accounted\nfor, an increased sensitivity to the initial condition. These observations\ntogether suggest a shrinking critical range.",
      "tldr_zh": "这篇论文分析了Elementary Cellular Automata (ECA)与拓扑异构网络，包括Partially-Local CA (PLCA)和Homogeneous Homogeneous Random Boolean Networks (HHRBN)，探讨了这些基底在计算敏感性方面的差异，以更接近生物学上的可信度。研究采用reservoir computing (RC)基准测试（如5-bit memory）和Temporal Derrida plots来评估敏感性、缺陷崩溃率，并比较拓扑不完善的影响。结果显示，无序拓扑并不一定导致无序计算，反而可能提高初始条件的敏感性，同时导致更高的崩溃率（秩序）和临界范围的缩小。",
      "categories": [
        "nlin.CG",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "primary_category": "nlin.CG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18017v1",
      "published_date": "2024-07-25 13:08:24 UTC",
      "updated_date": "2024-07-25 13:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:56:56.227773"
    },
    {
      "arxiv_id": "2407.17980v1",
      "title": "Personalized and Context-aware Route Planning for Edge-assisted Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Dinesh Cyril Selvaraj",
        "Falko Dressler",
        "Carla Fabiana Chiasserini"
      ],
      "abstract": "Conventional route planning services typically offer the same routes to all\ndrivers, focusing primarily on a few standardized factors such as travel\ndistance or time, overlooking individual driver preferences. With the inception\nof autonomous vehicles expected in the coming years, where vehicles will rely\non routes decided by such planners, there arises a need to incorporate the\nspecific preferences of each driver, ensuring personalized navigation\nexperiences. In this work, we propose a novel approach based on graph neural\nnetworks (GNNs) and deep reinforcement learning (DRL), aimed at customizing\nroutes to suit individual preferences. By analyzing the historical trajectories\nof individual drivers, we classify their driving behavior and associate it with\nrelevant road attributes as indicators of driver preferences. The GNN is\ncapable of representing the road network as graph-structured data effectively,\nwhile DRL is capable of making decisions utilizing reward mechanisms to\noptimize route selection with factors such as travel costs, congestion level,\nand driver satisfaction. We evaluate our proposed GNN-based DRL framework using\na real-world road network and demonstrate its ability to accommodate driver\npreferences, offering a range of route options tailored to individual drivers.\nThe results indicate that our framework can select routes that accommodate\ndriver's preferences with up to a 17% improvement compared to a generic route\nplanner, and reduce the travel time by 33% (afternoon) and 46% (evening)\nrelatively to the shortest distance-based approach.",
      "tldr_zh": "本研究针对传统路线规划服务忽略驾驶者偏好的问题，提出了一种基于图神经网络 (GNNs) 和深度强化学习 (DRL) 的个性化上下文感知框架，适用于边缘辅助车辆。该框架通过分析驾驶者的历史轨迹来分类行为偏好，并利用 GNNs 表示路网结构，结合 DRL 的奖励机制优化路线选择，考虑因素如旅行成本、拥堵水平和驾驶满意度。实验在真实路网中评估表明，与通用规划器相比，该方法可提高17%的驾驶偏好满足率，并在下午减少33%的旅行时间，在晚上减少46%的旅行时间，从而为自主车辆提供更高效的导航体验。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17980v1",
      "published_date": "2024-07-25 12:14:12 UTC",
      "updated_date": "2024-07-25 12:14:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:57:08.139949"
    },
    {
      "arxiv_id": "2407.18990v2",
      "title": "Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM Tuning in Real-World Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Alon Halfon",
        "Shai Gretz",
        "Ofir Arviv",
        "Artem Spector",
        "Orith Toledo-Ronen",
        "Yoav Katz",
        "Liat Ein-Dor",
        "Michal Shmueli-Scheuer",
        "Noam Slonim"
      ],
      "abstract": "Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.",
      "tldr_zh": "本研究通过实证分析探讨了超参数 (HPs) 对大型语言模型 (LLMs) 微调的影响，旨在为实际应用提供更高效的 HP 配置推荐。作者引入了 Coverage-based Search (CBS) 方法，该方法基于离线网格搜索对 HP 配置进行排名，以覆盖多种数据集和领域，提供鲁棒的起始点。实验涉及 Llama-3-8B 和 Mistral-7B 模型，以及 full fine-tuning 和 LoRA 调优方式，共进行了超过 10,000 次实验。结果显示，Llama-3-8B 和 LoRA 应优先选择，且仅探索少数推荐 HP 配置即可实现优秀性能，从而为从业者节省时间和计算资源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18990v2",
      "published_date": "2024-07-25 12:07:55 UTC",
      "updated_date": "2024-08-07 07:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:57:19.279857"
    },
    {
      "arxiv_id": "2407.17963v1",
      "title": "Relating the Seemingly Unrelated: Principled Understanding of Generalization for Generative Models in Arithmetic Reasoning Tasks",
      "title_zh": "关联看似无关的事物：生成模型在算术推理任务中泛化的原则性理解",
      "authors": [
        "Xingcheng Xu",
        "Zibo Zhao",
        "Haipeng Zhang",
        "Yanqing Yang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated impressive versatility across\nnumerous tasks, yet their generalization capabilities remain poorly understood.\nTo investigate these behaviors, arithmetic tasks serve as important venues. In\nprevious studies, seemingly unrelated mysteries still exist -- (1) models with\nappropriate positional embeddings can correctly perform longer unseen\narithmetic operations such as addition, but their effectiveness varies in more\ncomplex tasks like multiplication; (2) models perform well for longer unseen\ncases in modular addition under specific moduli (e.g., modulo 100) but struggle\nunder very close moduli (e.g., modulo 101), regardless of the positional\nencoding used. We believe previous studies have been treating the symptoms\nrather than addressing the root cause -- they have paid excessive attention to\nimproving model components, while overlooking the differences in task\nproperties that may be the real drivers. This is confirmed by our unified\ntheoretical framework for different arithmetic scenarios. For example, unlike\nmultiplication, the digital addition task has the property of translation\ninvariance which naturally aligns with the relative positional encoding, and\nthis combination leads to successful generalization of addition to unseen\nlonger domains. The discrepancy in operations modulo 100 and 101 arises from\nthe base. Modulo 100, unlike 101, is compatible with the decimal system (base\n10), such that unseen information in digits beyond the units digit and the tens\ndigit is actually not needed for the task. Extensive experiments with GPT-like\nmodels validate our theoretical predictions. These findings deepen our\nunderstanding of the generalization mechanisms, and facilitate more\ndata-efficient model training and objective-oriented AI alignment.",
      "tldr_zh": "本论文探讨大型语言模型(LLMs)在算术推理任务中的泛化能力，解释了之前未解的谜题，例如模型在加法任务上能泛化到更长操作，但不适用于乘法，或在modular addition中模100表现良好而模101失败。研究提出一个统一的理论框架，强调任务属性的差异（如加法的平移不变性与相对positional embeddings的兼容性）是泛化成功的关键，而不是单纯优化模型组件。实验使用GPT-like模型验证了这些预测，揭示模100与十进制基数的兼容性减少了信息需求，从而为更数据高效的模型训练和AI目标导向提供新见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17963v1",
      "published_date": "2024-07-25 11:35:22 UTC",
      "updated_date": "2024-07-25 11:35:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:57:31.499274"
    },
    {
      "arxiv_id": "2407.17951v1",
      "title": "Pruning Boolean d-DNNF Circuits Through Tseitin-Awareness",
      "title_zh": "翻译失败",
      "authors": [
        "Vincent Derkinderen"
      ],
      "abstract": "Boolean circuits in d-DNNF form enable tractable probabilistic inference.\nHowever, as a key insight of this work, we show that commonly used d-DNNF\ncompilation approaches introduce irrelevant subcircuits. We call these\nsubcircuits Tseitin artifacts, as they are introduced due to the Tseitin\ntransformation step -- a well-established procedure to transform any circuit\ninto the CNF format required by several d-DNNF knowledge compilers. We discuss\nhow to detect and remove both Tseitin variables and Tseitin artifacts, leading\nto more succinct circuits. We empirically observe an average size reduction of\n77.5% when removing both Tseitin variables and artifacts. The additional\npruning of Tseitin artifacts reduces the size by 22.2% on average. This\nsignificantly improves downstream tasks that benefit from a more succinct\ncircuit, e.g., probabilistic inference tasks.",
      "tldr_zh": "这篇论文揭示了 d-DNNF 电路在编译过程中常因 Tseitin transformation 而引入无关子电路（Tseitin artifacts），导致电路冗余。作者提出一种检测并移除 Tseitin variables 和 artifacts 的修剪方法，以生成更简洁的电路。实验结果显示，这种方法平均将电路大小减少 77.5%，额外移除 artifacts 后进一步减少 22.2%，从而显著提升概率推理等下游任务的效率。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "submitted to ICTAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17951v1",
      "published_date": "2024-07-25 11:15:57 UTC",
      "updated_date": "2024-07-25 11:15:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:57:43.864904"
    },
    {
      "arxiv_id": "2407.17950v1",
      "title": "Real Time American Sign Language Detection Using Yolo-v9",
      "title_zh": "翻译失败",
      "authors": [
        "Amna Imran",
        "Meghana Shashishekhara Hulikal",
        "Hamza A. A. Gardi"
      ],
      "abstract": "This paper focuses on real-time American Sign Language Detection. YOLO is a\nconvolutional neural network (CNN) based model, which was first released in\n2015. In recent years, it gained popularity for its real-time detection\ncapabilities. Our study specifically targets YOLO-v9 model, released in 2024.\nAs the model is newly introduced, not much work has been done on it, especially\nnot in Sign Language Detection. Our paper provides deep insight on how YOLO- v9\nworks and better than previous model.",
      "tldr_zh": "这篇论文探讨了使用YOLO-v9模型进行实时美国手语检测，YOLO-v9是2024年发布的基于CNN的神经网络模型，以其高效的实时检测能力而备受关注。相比早期版本，YOLO-v9在手语检测领域几乎未被应用，本文提供了其工作原理的深入分析，并强调了其性能上的改进优势。尽管摘要未详述实验结果，但这为未来手语识别应用奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 13 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2407.17950v1",
      "published_date": "2024-07-25 11:11:05 UTC",
      "updated_date": "2024-07-25 11:11:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:58:03.692926"
    },
    {
      "arxiv_id": "2407.17940v3",
      "title": "Positive Text Reframing under Multi-strategy Optimization",
      "title_zh": "多策略优化下的正面文本重构",
      "authors": [
        "Shutong Jia",
        "Biwei Cao",
        "Qingqing Gao",
        "Jiuxin Cao",
        "Bo Liu"
      ],
      "abstract": "Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.",
      "tldr_zh": "本论文探讨了正向重构（Positive Reframing）任务，即将负面视角转化为正面表达，同时保留原意。该研究提出了一种多策略优化框架（MSOF），通过正向情感奖励和内容保留奖励来指导模型转换负面表达，确保语义完整性，并结合解码优化方法和多维度重新排序（从策略一致性、文本相似性和流畅性维度）提升生成文本的质量。在BART和T5等Seq2Seq预训练语言模型（PLMs）上进行的广泛实验表明，该框架在无约束和受控正向重构任务中取得了显著性能改善。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.17940v3",
      "published_date": "2024-07-25 10:58:42 UTC",
      "updated_date": "2024-12-16 12:57:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:59:53.012609"
    },
    {
      "arxiv_id": "2407.17930v1",
      "title": "Comparison of different Artificial Neural Networks for Bitcoin price forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Silas Baumann",
        "Karl A. Busch",
        "Hamza A. A. Gardi"
      ],
      "abstract": "This study investigates the impact of varying sequence lengths on the\naccuracy of predicting cryptocurrency returns using Artificial Neural Networks\n(ANNs). Utilizing the Mean Absolute Error (MAE) as a threshold criterion, we\naim to enhance prediction accuracy by excluding returns that are smaller than\nthis threshold, thus mitigating errors associated with minor returns. The\nsubsequent evaluation focuses on the accuracy of predicted returns that exceed\nthis threshold. We compare four sequence lengths 168 hours (7 days), 72 hours\n(3 days), 24 hours, and 12 hours each with a return prediction interval of 2\nhours. Our findings reveal the influence of sequence length on prediction\naccuracy and underscore the potential for optimized sequence configurations in\nfinancial forecasting models.",
      "tldr_zh": "本研究比较了不同序列长度对使用 Artificial Neural Networks (ANNs) 预测比特币价格回报的影响，旨在通过 Mean Absolute Error (MAE) 作为阈值标准排除小回报，从而提高预测准确性。实验设置包括四种序列长度：168 小时 (7 天)、72 小时 (3 天)、24 小时和 12 小时，每个序列的回报预测间隔为 2 小时。结果显示，序列长度显著影响预测准确性，并强调了优化序列配置在金融预测模型中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 8 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.17930v1",
      "published_date": "2024-07-25 10:39:50 UTC",
      "updated_date": "2024-07-25 10:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:58:27.017931"
    },
    {
      "arxiv_id": "2407.17927v2",
      "title": "Invariance of deep image quality metrics to affine transformations",
      "title_zh": "深度图像质量指标对仿射变换的不变性",
      "authors": [
        "Nuria Alabau-Bosque",
        "Paula Daudén-Oliver",
        "Jorge Vila-Tomás",
        "Valero Laparra",
        "Jesús Malo"
      ],
      "abstract": "Deep architectures are the current state-of-the-art in predicting subjective\nimage quality. Usually, these models are evaluated according to their ability\nto correlate with human opinion in databases with a range of distortions that\nmay appear in digital media. However, these oversee affine transformations\nwhich may represent better the changes in the images actually happening in\nnatural conditions. Humans can be particularly invariant to these natural\ntransformations, as opposed to the digital ones. In this work, we evaluate\nstate-of-the-art deep image quality metrics by assessing their invariance to\naffine transformations, specifically: rotation, translation, scaling, and\nchanges in spectral illumination. Here invariance of a metric refers to the\nfact that certain distances should be neglected (considered to be zero) if\ntheir values are below a threshold. This is what we call invisibility threshold\nof a metric. We propose a methodology to assign such invisibility thresholds\nfor any perceptual metric. This methodology involves transformations to a\ndistance space common to any metric, and psychophysical measurements of\nthresholds in this common space. By doing so, we allow the analyzed metrics to\nbe directly comparable with actual human thresholds. We find that none of the\nstate-of-the-art metrics shows human-like results under this strong test based\non invisibility thresholds. This means that tuning the models exclusively to\npredict the visibility of generic distortions may disregard other properties of\nhuman vision as for instance invariances or invisibility thresholds.",
      "tldr_zh": "这篇论文评估了深度图像质量指标（deep image quality metrics）对仿射变换（affine transformations）的不变性，包括旋转、平移、缩放和光谱照度变化。作者提出了一种方法，通过定义不可见阈值（invisibility threshold）和心理物理测量，将指标映射到公共距离空间，以直接与人类感知阈值比较。实验结果显示，现有的最先进指标未能达到人类般的性能，表明仅针对通用畸变的可视性优化模型可能忽略了人类视觉的不变性和不可见阈值等属性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "24 pages 40 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.17927v2",
      "published_date": "2024-07-25 10:24:54 UTC",
      "updated_date": "2024-07-29 11:55:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:00:21.038014"
    },
    {
      "arxiv_id": "2407.20274v1",
      "title": "Exploring the Plausibility of Hate and Counter Speech Detectors with Explainable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Adrian Jaques Böck",
        "Djordje Slijepčević",
        "Matthias Zeppelzauer"
      ],
      "abstract": "In this paper we investigate the explainability of transformer models and\ntheir plausibility for hate speech and counter speech detection. We compare\nrepresentatives of four different explainability approaches, i.e.,\ngradient-based, perturbation-based, attention-based, and prototype-based\napproaches, and analyze them quantitatively with an ablation study and\nqualitatively in a user study. Results show that perturbation-based\nexplainability performs best, followed by gradient-based and attention-based\nexplainability. Prototypebased experiments did not yield useful results.\nOverall, we observe that explainability strongly supports the users in better\nunderstanding the model predictions.",
      "tldr_zh": "本研究探讨了Transformer模型在仇恨言论和反仇恨言论检测中的可解释性及其合理性，通过比较四种解释方法：gradient-based、perturbation-based、attention-based和prototype-based方法。研究采用定量消融研究和定性用户研究进行分析，结果显示perturbation-based方法表现最佳，其次是gradient-based和attention-based方法，而prototype-based方法未产生有效结果。总体上，这些可解释性方法有助于用户更好地理解模型预测，从而提升仇恨言论检测的可信度。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "conference, CBMI2024, 6 pages,",
      "pdf_url": "http://arxiv.org/pdf/2407.20274v1",
      "published_date": "2024-07-25 10:17:04 UTC",
      "updated_date": "2024-07-25 10:17:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:00:29.173765"
    },
    {
      "arxiv_id": "2407.17915v4",
      "title": "The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models",
      "title_zh": "功能调用的阴暗面：绕过大语言模型的越狱途径",
      "authors": [
        "Zihui Wu",
        "Haichang Gao",
        "Jianping He",
        "Ping Wang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\ntheir power comes with significant security considerations. While extensive\nresearch has been conducted on the safety of LLMs in chat mode, the security\nimplications of their function calling feature have been largely overlooked.\nThis paper uncovers a critical vulnerability in the function calling process of\nLLMs, introducing a novel \"jailbreak function\" attack method that exploits\nalignment discrepancies, user coercion, and the absence of rigorous safety\nfilters. Our empirical study, conducted on six state-of-the-art LLMs including\nGPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro, reveals an alarming average\nsuccess rate of over 90\\% for this attack. We provide a comprehensive analysis\nof why function calls are susceptible to such attacks and propose defensive\nstrategies, including the use of defensive prompts. Our findings highlight the\nurgent need for enhanced security measures in the function calling capabilities\nof LLMs, contributing to the field of AI safety by identifying a previously\nunexplored risk, designing an effective attack method, and suggesting practical\ndefensive measures. Our code is available at\nhttps://github.com/wooozihui/jailbreakfunction.",
      "tldr_zh": "本研究揭示了大型语言模型（LLMs）的function calling功能存在的严重安全漏洞，提出了一种新型“jailbreak function”攻击方法，该方法利用对齐差异、用户胁迫和缺乏安全过滤来绕过模型防护。实验在六种先进LLMs（如GPT-4o、Claude-3.5-Sonnet和Gemini-1.5-pro）上进行，结果显示攻击成功率平均超过90%。论文分析了function calling易受攻击的原因，并建议了防御策略，包括使用defensive prompts，以提升AI安全。整体贡献突出了加强LLMs函数调用安全措施的紧迫性，并提供了相关代码开源。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17915v4",
      "published_date": "2024-07-25 10:09:21 UTC",
      "updated_date": "2024-12-24 09:35:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:00:42.003596"
    },
    {
      "arxiv_id": "2407.17911v1",
      "title": "ReCorD: Reasoning and Correcting Diffusion for HOI Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Jian-Yu Jiang-Lin",
        "Kang-Yang Huang",
        "Ling Lo",
        "Yi-Ning Huang",
        "Terence Lin",
        "Jhih-Ciang Wu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "abstract": "Diffusion models revolutionize image generation by leveraging natural\nlanguage to guide the creation of multimedia content. Despite significant\nadvancements in such generative models, challenges persist in depicting\ndetailed human-object interactions, especially regarding pose and object\nplacement accuracy. We introduce a training-free method named Reasoning and\nCorrecting Diffusion (ReCorD) to address these challenges. Our model couples\nLatent Diffusion Models with Visual Language Models to refine the generation\nprocess, ensuring precise depictions of HOIs. We propose an interaction-aware\nreasoning module to improve the interpretation of the interaction, along with\nan interaction correcting module to refine the output image for more precise\nHOI generation delicately. Through a meticulous process of pose selection and\nobject positioning, ReCorD achieves superior fidelity in generated images while\nefficiently reducing computational requirements. We conduct comprehensive\nexperiments on three benchmarks to demonstrate the significant progress in\nsolving text-to-image generation tasks, showcasing ReCorD's ability to render\ncomplex interactions accurately by outperforming existing methods in HOI\nclassification score, as well as FID and Verb CLIP-Score. Project website is\navailable at https://alberthkyhky.github.io/ReCorD/ .",
      "tldr_zh": "该论文提出了一种无需训练的方法ReCorD（Reasoning and Correcting Diffusion），旨在解决扩散模型在生成人类物体交互（HOI）图像时存在的姿势和物体放置准确性问题。ReCorD通过将Latent Diffusion Models与Visual Language Models结合，引入交互感知推理模块来提升交互解释，以及交互校正模块来精炼输出图像，从而实现更精确的HOI生成并降低计算需求。在三个基准测试中，ReCorD在HOI分类分数、FID和Verb CLIP-Score上均优于现有方法，展示了其在复杂交互渲染方面的显著进步。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by ACM MM 2024. Project website:\n  https://alberthkyhky.github.io/ReCorD/",
      "pdf_url": "http://arxiv.org/pdf/2407.17911v1",
      "published_date": "2024-07-25 10:06:26 UTC",
      "updated_date": "2024-07-25 10:06:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:00:52.835336"
    },
    {
      "arxiv_id": "2407.17910v1",
      "title": "Causal Deepsets for Off-policy Evaluation under Spatial or Spatio-temporal Interferences",
      "title_zh": "翻译失败",
      "authors": [
        "Runpeng Dai",
        "Jianing Wang",
        "Fan Zhou",
        "Shikai Luo",
        "Zhiwei Qin",
        "Chengchun Shi",
        "Hongtu Zhu"
      ],
      "abstract": "Off-policy evaluation (OPE) is widely applied in sectors such as\npharmaceuticals and e-commerce to evaluate the efficacy of novel products or\npolicies from offline datasets. This paper introduces a causal deepset\nframework that relaxes several key structural assumptions, primarily the\nmean-field assumption, prevalent in existing OPE methodologies that handle\nspatio-temporal interference. These traditional assumptions frequently prove\ninadequate in real-world settings, thereby restricting the capability of\ncurrent OPE methods to effectively address complex interference effects. In\nresponse, we advocate for the implementation of the permutation invariance (PI)\nassumption. This innovative approach enables the data-driven, adaptive learning\nof the mean-field function, offering a more flexible estimation method beyond\nconventional averaging. Furthermore, we present novel algorithms that\nincorporate the PI assumption into OPE and thoroughly examine their theoretical\nfoundations. Our numerical analyses demonstrate that this novel approach yields\nsignificantly more precise estimations than existing baseline algorithms,\nthereby substantially improving the practical applicability and effectiveness\nof OPE methodologies. A Python implementation of our proposed method is\navailable at https://github.com/BIG-S2/Causal-Deepsets.",
      "tldr_zh": "本论文提出了一种 causal deepset 框架，用于处理空间或时空干扰下的 Off-policy Evaluation (OPE)，通过放松传统方法的 mean-field assumption，实现更灵活的估计。该框架引入 permutation invariance (PI) 假设，支持数据驱动的学习 mean-field function，从而更好地适应复杂干扰效果。研究者开发了新的算法，并提供了理论分析；实验结果显示，该方法比现有基线算法更精确，提升了 OPE 的实用性和有效性。开源代码可在 GitHub 上获取。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17910v1",
      "published_date": "2024-07-25 10:02:11 UTC",
      "updated_date": "2024-07-25 10:02:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:01:05.758085"
    },
    {
      "arxiv_id": "2407.17896v2",
      "title": "SR-CurvANN: Advancing 3D Surface Reconstruction through Curvature-Aware Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Marina Hernández-Bautista",
        "Francisco J. Melero"
      ],
      "abstract": "Incomplete or missing data in three-dimensional (3D) models can lead to\nerroneous or flawed renderings, limiting their usefulness in applications such\nas visualization, geometric computation, and 3D printing. Conventional\nsurface-repair techniques often fail to infer complex geometric details in\nmissing areas. Neural networks successfully address hole-filling tasks in 2D\nimages using inpainting techniques. The combination of surface reconstruction\nalgorithms, guided by the model's curvature properties and the creativity of\nneural networks in the inpainting processes should provide realistic results in\nthe hole completion task. In this paper, we propose a novel method entitled\nSR-CurvANN (Surface Reconstruction Based on Curvature-Aware Neural Networks)\nthat incorporates neural network-based 2D inpainting to effectively reconstruct\n3D surfaces. We train the neural networks with images that represent planar\nrepresentations of the curvature at vertices of hundreds of 3D models. Once the\nmissing areas have been inferred, a coarse-to-fine surface deformation process\nensures that the surface fits the reconstructed curvature image. Our proposal\nmakes it possible to learn and generalize patterns from a wide variety of\ntraining 3D models, generating comprehensive inpainted curvature images and\nsurfaces. Experiments conducted on 959 models with several holes have\ndemonstrated that SR-CurvANN excels in the shape completion process, filling\nholes with a remarkable level of realism and precision.",
      "tldr_zh": "该论文提出SR-CurvANN，一种基于Curvature-Aware Neural Networks的3D表面重建方法，旨在解决3D模型中缺失数据导致的渲染错误问题。SR-CurvANN通过训练神经网络处理表示顶点曲率的2D图像，进行缺失区域的推断，并采用粗到细的表面变形过程，确保重建的表面符合曲率属性。该方法从大量训练模型中学习模式，并在959个带有孔洞的模型实验中表现出色，提供高度真实和精确的形状完成结果。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "Major changes in title, paper structure, text and figures. Improved\n  results. 23 pages, 14 figures. Decision about submission not taken yet",
      "pdf_url": "http://arxiv.org/pdf/2407.17896v2",
      "published_date": "2024-07-25 09:36:37 UTC",
      "updated_date": "2024-09-26 10:33:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:01:19.661274"
    },
    {
      "arxiv_id": "2407.17892v1",
      "title": "An Iterative Approach to Topic Modelling",
      "title_zh": "主题建模的迭代方法",
      "authors": [
        "Albert Wong",
        "Florence Wing Yau Cheng",
        "Ashley Keung",
        "Yamileth Hercules",
        "Mary Alexandra Garcia",
        "Yew-Wei Lim",
        "Lien Pham"
      ],
      "abstract": "Topic modelling has become increasingly popular for summarizing text data,\nsuch as social media posts and articles. However, topic modelling is usually\ncompleted in one shot. Assessing the quality of resulting topics is\nchallenging. No effective methods or measures have been developed for assessing\nthe results or for making further enhancements to the topics. In this research,\nwe propose we propose to use an iterative process to perform topic modelling\nthat gives rise to a sense of completeness of the resulting topics when the\nprocess is complete. Using the BERTopic package, a popular method in topic\nmodelling, we demonstrate how the modelling process can be applied iteratively\nto arrive at a set of topics that could not be further improved upon using one\nof the three selected measures for clustering comparison as the decision\ncriteria. This demonstration is conducted using a subset of the COVIDSenti-A\ndataset. The early success leads us to believe that further research using in\nusing this approach in conjunction with other topic modelling algorithms could\nbe viable.",
      "tldr_zh": "本研究针对传统主题建模的一次性过程及其质量评估难题，提出了一种迭代方法，以实现主题的完整性和优化。使用 BERTopic 包作为工具，该方法通过重复应用主题建模，并以三种聚类比较措施作为决策标准，在 COVIDSenti-A 数据集子集上进行演示，最终获得无法进一步改进的主题集。初步结果表明，此迭代方法有效，并为与其他主题建模算法结合的应用提供了可行性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17892v1",
      "published_date": "2024-07-25 09:26:07 UTC",
      "updated_date": "2024-07-25 09:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:01:29.702937"
    },
    {
      "arxiv_id": "2408.01455v1",
      "title": "Ontology of Belief Diversity: A Community-Based Epistemological Approach",
      "title_zh": "信念多样性的本体论：一种基于社区的认识论方法",
      "authors": [
        "Tyler Fischella",
        "Erin van Liemt",
        "Qiuyi",
        "Zhang"
      ],
      "abstract": "AI applications across classification, fairness, and human interaction often\nimplicitly require ontologies of social concepts. Constructing these well,\nespecially when there are many relevant categories, is a controversial task but\nis crucial for achieving meaningful inclusivity. Here, we focus on developing a\npragmatic ontology of belief systems, which is a complex and often\ncontroversial space. By iterating on our community-based design until mutual\nagreement is reached, we found that epistemological methods were best for\ncategorizing the fundamental ways beliefs differ, maximally respecting our\nprinciples of inclusivity and brevity. We demonstrate our methodology's utility\nand interpretability via user studies in term annotation and sentiment analysis\nexperiments for belief fairness in language models.",
      "tldr_zh": "本研究提出了一种基于社区的认识论（epistemological）方法，来构建信念系统的实用本体（ontology），以解决AI应用（如分类、公平性和人类互动）中社会概念包容性的挑战。方法通过迭代社区设计过程，直至达成共识，并分类信念差异的根本方式，同时遵循包容性和简洁性的原则。用户研究显示，该方法在术语标注和情感分析实验中表现出色，提升了语言模型中信念公平性的实用性和可解释性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "AIES 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01455v1",
      "published_date": "2024-07-25 09:02:50 UTC",
      "updated_date": "2024-07-25 09:02:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:01:40.419010"
    },
    {
      "arxiv_id": "2407.17881v1",
      "title": "Unraveling the Never-Ending Story of Lifecycles and Vitalizing Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Stephan A. Fahrenkrog-Petersen",
        "Saimir Bala",
        "Luise Pufahl",
        "Jan Mendling"
      ],
      "abstract": "Business process management (BPM) has been widely used to discover, model,\nanalyze, and optimize organizational processes. BPM looks at these processes\nwith analysis techniques that assume a clearly defined start and end. However,\nnot all processes adhere to this logic, with the consequence that their\nbehavior cannot be appropriately captured by BPM analysis techniques. This\npaper addresses this research problem at a conceptual level. More specifically,\nwe introduce the notion of vitalizing business processes that target the\nlifecycle process of one or more entities. We show the existence of lifecycle\nprocesses in many industries and that their appropriate conceptualizations pave\nthe way for the definition of suitable modeling and analysis techniques. This\npaper provides a set of requirements for their analysis, and a\nconceptualization of lifecycle and vitalizing processes.",
      "tldr_zh": "本论文探讨了商业过程管理(BPM)的局限性，即其分析技术假设过程有明确开始和结束，但许多过程（如实体生命周期过程）并非如此，导致行为无法被恰当捕捉。作者引入了vitalizing business processes的概念，针对一个或多个实体的生命周期过程，并展示了这些过程在多个行业的广泛存在。论文提供了分析这些过程的要求和概念化框架，为定义合适的建模和分析技术铺平道路。",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17881v1",
      "published_date": "2024-07-25 08:52:23 UTC",
      "updated_date": "2024-07-25 08:52:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:01:53.102733"
    },
    {
      "arxiv_id": "2407.17879v2",
      "title": "HG-PIPE: Vision Transformer Acceleration with Hybrid-Grained Pipeline",
      "title_zh": "HG-PIPE：基于混合粒度流水线的 Vision Transformer 加速",
      "authors": [
        "Qingyu Guo",
        "Jiayong Wan",
        "Songqiang Xu",
        "Meng Li",
        "Yuan Wang"
      ],
      "abstract": "Vision Transformer (ViT) acceleration with field programmable gate array\n(FPGA) is promising but challenging. Existing FPGA-based ViT accelerators\nmainly rely on temporal architectures, which process different operators by\nreusing the same hardware blocks and suffer from extensive memory access\noverhead. Pipelined architectures, either coarse-grained or fine-grained,\nunroll the ViT computation spatially for memory access efficiency. However,\nthey usually suffer from significant hardware resource constraints and pipeline\nbubbles induced by the global computation dependency of ViT. In this paper, we\nintroduce HG-PIPE, a pipelined FPGA accelerator for high-throughput and\nlow-latency ViT processing. HG-PIPE features a hybrid-grained pipeline\narchitecture to reduce on-chip buffer cost and couples the computation dataflow\nand parallelism design to eliminate the pipeline bubbles. HG-PIPE further\nintroduces careful approximations to implement both linear and non-linear\noperators with abundant Lookup Tables (LUTs), thus alleviating resource\nconstraints. On a ZCU102 FPGA, HG-PIPE achieves 2.78 times better throughput\nand 2.52 times better resource efficiency than the prior-art accelerators,\ne.g., AutoViTAcc. With a VCK190 FPGA, HG-PIPE realizes end-to-end ViT\nacceleration on a single device and achieves 7118 images/s, which is 2.81 times\nfaster than a V100 GPU.",
      "tldr_zh": "本研究提出 HG-PIPE，一种基于 FPGA 的 Vision Transformer (ViT) 加速器，旨在解决现有加速器在内存访问开销、硬件资源约束和 pipeline bubbles 方面的挑战。HG-PIPE 采用 hybrid-grained pipeline 架构，通过优化计算数据流和并行设计减少 on-chip buffer 成本，并利用 Lookup Tables (LUTs) 实现线性与非线性操作以缓解资源限制。在 ZCU102 FPGA 上，该加速器比现有方法如 AutoViTAcc 提高吞吐量 2.78 倍和资源效率 2.52 倍；在 VCK190 FPGA 上，实现端到端 ViT 加速，每秒处理 7118 张图像，比 V100 GPU 快 2.81 倍。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "68T07"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted by ICCAD 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17879v2",
      "published_date": "2024-07-25 08:47:40 UTC",
      "updated_date": "2024-08-01 08:18:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:02:04.800791"
    },
    {
      "arxiv_id": "2407.18989v4",
      "title": "Machine Learning for Fairness-Aware Load Shedding: A Real-Time Solution via Identifying Binding Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqi Zhou",
        "Joseph Severino",
        "Sanjana Vijayshankar",
        "Juliette Ugirumurera",
        "Jibo Sanyal"
      ],
      "abstract": "Timely and effective load shedding in power systems is critical for\nmaintaining supply-demand balance and preventing cascading blackouts. To\neliminate load shedding bias against specific regions in the system,\noptimization-based methods are uniquely positioned to help balance between\neconomic and fairness considerations. However, the resulting optimization\nproblem involves complex constraints, which can be time-consuming to solve and\nthus cannot meet the real-time requirements of load shedding. To tackle this\nchallenge, in this paper we present an efficient machine learning algorithm to\nenable millisecond-level computation for the optimization-based load shedding\nproblem. Numerical studies on both a 3-bus toy example and a realistic RTS-GMLC\nsystem have demonstrated the validity and efficiency of the proposed algorithm\nfor delivering fairness-aware and real-time load shedding decisions.",
      "tldr_zh": "该论文针对电力系统中负载削减的公平性问题，提出了一种基于机器学习的实时解决方案，以平衡经济和公平考虑。传统优化方法因复杂约束而计算耗时，无法满足实时需求，该算法通过识别 binding constraints 实现毫秒级计算。实验在 3-bus 玩具示例和 RTS-GMLC 系统上验证了该方法的有效性与效率，为公平意识的负载削减决策提供了可靠支持。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.18989v4",
      "published_date": "2024-07-25 08:47:11 UTC",
      "updated_date": "2025-02-27 05:23:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:02:35.284863"
    },
    {
      "arxiv_id": "2407.17874v1",
      "title": "Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions",
      "title_zh": "翻译失败",
      "authors": [
        "Jiwon Suh",
        "Injae Na",
        "Woohwan Jung"
      ],
      "abstract": "End-to-end automatic speech recognition (E2E ASR) systems have significantly\nimproved speech recognition through training on extensive datasets. Despite\nthese advancements, they still struggle to accurately recognize domain specific\nwords, such as proper nouns and technical terminologies. To address this\nproblem, we propose a method to utilize the state-of-the-art Whisper without\nmodifying its architecture, preserving its generalization performance while\nenabling it to leverage descriptions effectively. Moreover, we propose two\nadditional training techniques to improve the domain specific ASR: decoder\nfine-tuning, and context perturbation. We also propose a method to use a Large\nLanguage Model (LLM) to generate descriptions with simple metadata, when\ndescriptions are unavailable. Our experiments demonstrate that proposed methods\nnotably enhance domain-specific ASR accuracy on real-life datasets, with\nLLM-generated descriptions outperforming human-crafted ones in effectiveness.",
      "tldr_zh": "该论文针对端到端自动语音识别（E2E ASR）系统在识别领域特定词汇（如专有名词和技术术语）时的不足，提出了一种改进方法，利用 Large Language Model (LLM) 生成的上下文描述来提升性能，而无需修改 Whisper 模型的架构，从而保持其泛化能力。方法包括引入 decoder fine-tuning 和 context perturbation 等训练技巧，以及当描述不可用时，使用 LLM 和简单 metadata 生成描述。实验结果表明，该方法在真实数据集上显著提高了领域特定 ASR 的准确性，且 LLM 生成的描述比人工制作的更有效。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to INTERSPEECH 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17874v1",
      "published_date": "2024-07-25 08:44:04 UTC",
      "updated_date": "2024-07-25 08:44:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:02:41.020145"
    },
    {
      "arxiv_id": "2407.17866v3",
      "title": "Financial Statement Analysis with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Kim",
        "Maximilian Muhn",
        "Valeri Nikolaev"
      ],
      "abstract": "We investigate whether large language models (LLMs) can successfully perform\nfinancial statement analysis in a way similar to a professional human analyst.\nWe provide standardized and anonymous financial statements to GPT4 and instruct\nthe model to analyze them to determine the direction of firms' future earnings.\nEven without narrative or industry-specific information, the LLM outperforms\nfinancial analysts in its ability to predict earnings changes directionally.\nThe LLM exhibits a relative advantage over human analysts in situations when\nthe analysts tend to struggle. Furthermore, we find that the prediction\naccuracy of the LLM is on par with a narrowly trained state-of-the-art ML\nmodel. LLM prediction does not stem from its training memory. Instead, we find\nthat the LLM generates useful narrative insights about a company's future\nperformance. Lastly, our trading strategies based on GPT's predictions yield a\nhigher Sharpe ratio and alphas than strategies based on other models. Our\nresults suggest that LLMs may take a central role in analysis and\ndecision-making.",
      "tldr_zh": "本研究探讨大型语言模型 (LLMs) 如 GPT4 是否能像专业人类分析师一样分析财务报表，方法是通过提供标准化匿名财务报表来预测公司未来收益方向。结果显示，LLMs 在预测收益变化方向上优于人类分析师，尤其在分析师易出错的情形下，且其准确率与专门训练的机器学习模型相当。LLMs 的预测并非依赖训练记忆，而是通过生成有价值的叙述性洞见来实现。最后，基于 LLMs 预测的交易策略显示出更高的 Sharpe ratio 和 alphas，这表明 LLMs 可能在财务分析和决策中扮演核心角色。",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.CL",
        "q-fin.GN",
        "q-fin.PM"
      ],
      "primary_category": "q-fin.ST",
      "comment": "A co-author identified inconsistencies in the data and analyses while\n  attempting to replicate past analyses from the working paper. Accordingly, we\n  have temporarily withdrawn the working paper from circulation while we review\n  the research findings.",
      "pdf_url": "http://arxiv.org/pdf/2407.17866v3",
      "published_date": "2024-07-25 08:36:58 UTC",
      "updated_date": "2025-02-20 18:54:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:02:54.635474"
    },
    {
      "arxiv_id": "2407.17857v1",
      "title": "Mew: Multiplexed Immunofluorescence Image Analysis through an Efficient Multiplex Network",
      "title_zh": "翻译失败",
      "authors": [
        "Sukwon Yun",
        "Jie Peng",
        "Alexandro E. Trevino",
        "Chanyoung Park",
        "Tianlong Chen"
      ],
      "abstract": "Recent advancements in graph-based approaches for multiplexed\nimmunofluorescence (mIF) images have significantly propelled the field forward,\noffering deeper insights into patient-level phenotyping. However, current\ngraph-based methodologies encounter two primary challenges: (1) Cellular\nHeterogeneity, where existing approaches fail to adequately address the\ninductive biases inherent in graphs, particularly the homophily characteristic\nobserved in cellular connectivity and; (2) Scalability, where handling cellular\ngraphs from high-dimensional images faces difficulties in managing a high\nnumber of cells. To overcome these limitations, we introduce Mew, a novel\nframework designed to efficiently process mIF images through the lens of\nmultiplex network. Mew innovatively constructs a multiplex network comprising\ntwo distinct layers: a Voronoi network for geometric information and a\nCell-type network for capturing cell-wise homogeneity. This framework equips a\nscalable and efficient Graph Neural Network (GNN), capable of processing the\nentire graph during training. Furthermore, Mew integrates an interpretable\nattention module that autonomously identifies relevant layers for image\nclassification. Extensive experiments on a real-world patient dataset from\nvarious institutions highlight Mew's remarkable efficacy and efficiency,\nmarking a significant advancement in mIF image analysis. The source code of Mew\ncan be found here: \\url{https://github.com/UNITES-Lab/Mew}",
      "tldr_zh": "该论文提出 Mew 框架，用于解决 multiplexed immunofluorescence (mIF) 图像分析中的细胞异质性（Cellular Heterogeneity）和可扩展性（Scalability）挑战，这些问题源于现有图-based 方法对细胞连接同质性（homophily）的处理不足和高维图像的细胞数量管理困难。Mew 创新性地构建了一个多层网络，包括 Voronoi network 处理几何信息和 Cell-type network 捕获细胞同质性，并采用高效的 Graph Neural Network (GNN) 来处理整个图训练，同时整合可解释的 attention module 自动识别相关层进行图像分类。在真实患者数据集上的广泛实验证明，Mew 显著提高了分析的效能和效率，为 mIF 图像分析领域带来了重要进展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17857v1",
      "published_date": "2024-07-25 08:22:30 UTC",
      "updated_date": "2024-07-25 08:22:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:15.137660"
    },
    {
      "arxiv_id": "2407.17854v1",
      "title": "Shapley Value-based Contrastive Alignment for Multimodal Information Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Wen Luo",
        "Yu Xia",
        "Shen Tianshu",
        "Sujian Li"
      ],
      "abstract": "The rise of social media and the exponential growth of multimodal\ncommunication necessitates advanced techniques for Multimodal Information\nExtraction (MIE). However, existing methodologies primarily rely on direct\nImage-Text interactions, a paradigm that often faces significant challenges due\nto semantic and modality gaps between images and text. In this paper, we\nintroduce a new paradigm of Image-Context-Text interaction, where large\nmultimodal models (LMMs) are utilized to generate descriptive textual context\nto bridge these gaps. In line with this paradigm, we propose a novel Shapley\nValue-based Contrastive Alignment (Shap-CA) method, which aligns both\ncontext-text and context-image pairs. Shap-CA initially applies the Shapley\nvalue concept from cooperative game theory to assess the individual\ncontribution of each element in the set of contexts, texts and images towards\ntotal semantic and modality overlaps. Following this quantitative evaluation, a\ncontrastive learning strategy is employed to enhance the interactive\ncontribution within context-text/image pairs, while minimizing the influence\nacross these pairs. Furthermore, we design an adaptive fusion module for\nselective cross-modal fusion. Extensive experiments across four MIE datasets\ndemonstrate that our method significantly outperforms existing state-of-the-art\nmethods.",
      "tldr_zh": "该论文针对 Multimodal Information Extraction (MIE) 的挑战，提出一种新的 Image-Context-Text 互动范式，利用 large multimodal models (LMMs) 生成描述性文本上下文，以桥接图像和文本间的语义及模态差距。核心方法是 Shapley Value-based Contrastive Alignment (Shap-CA)，它借鉴合作博弈理论的 Shapley value 评估上下文、文本和图像元素的贡献，然后通过对比学习策略增强 context-text 和 context-image 配对的互动，同时最小化配对间的干扰，并引入 adaptive fusion module 进行选择性跨模态融合。实验结果显示，该方法在四个 MIE 数据集上显著优于现有最先进方法，证明了其有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ACM Multimedia 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17854v1",
      "published_date": "2024-07-25 08:15:43 UTC",
      "updated_date": "2024-07-25 08:15:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:16.711969"
    },
    {
      "arxiv_id": "2407.17844v4",
      "title": "Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease Classification: A Systematic Review",
      "title_zh": "翻译失败",
      "authors": [
        "Lisanne van Gelderen",
        "Cristian Tejedor-García"
      ],
      "abstract": "Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy concerns. The goal of this systematic review is to explore the current\nlandscape of speech-based DL approaches for PD classification, based on 33\nscientific works published between January 2020 and March 2024. We discuss\ntheir available resources, capabilities, and potential limitations, and issues\nrelated to bias, explainability, and privacy. Furthermore, this review provides\nan overview of publicly accessible speech-based datasets and open-source\nmaterial for PD. The DL approaches identified are categorized into end-to-end\n(E2E) learning, transfer learning (TL), and deep acoustic feature extraction\n(DAFE). Among E2E approaches, Convolutional Neural Networks (CNNs) are\nprevalent, though Transformers are increasingly popular. E2E approaches face\nchallenges such as limited data and computational resources, especially with\nTransformers. TL addresses these issues by providing more robust PD diagnosis\nand better generalizability across languages. DAFE aims to improve the\nexplainability and interpretability of results by examining the specific\neffects of deep features on both other DL approaches and more traditional\nmachine learning (ML) methods. However, it often underperforms compared to E2E\nand TL approaches.",
      "tldr_zh": "这篇系统综述探讨了基于语音数据的深度学习（DL）方法在帕金森病（PD）分类中的创新应用，回顾了2020年至2024年间33篇相关文献。研究将DL方法分为端到端（E2E）学习（如Convolutional Neural Networks (CNNs)和Transformers）、转移学习（TL）以及深度声学特征提取（DAFE），其中E2E方法常见但受限于数据和计算资源，而TL提升了诊断的稳健性和语言泛化，DAFE则提高了结果的可解释性但性能较差。综述还指出了偏见、隐私和数据集可用性的挑战，并提供了公开可访问的语音数据集和开源资源的概述，为未来PD诊断研究提供了宝贵参考。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "van Gelderen, L., & Tejedor-Garc\\'ia, C. (2024). Innovative\n  Speech-Based Deep Learning Approaches for Parkinson's Disease Classification:\n  A Systematic Review. Applied Sciences, 14(17). doi:10.3390/app14177873 This\n  research was funded by the NWO research programme NGF AiNed Fellowship Grants\n  under the project Responsible AI for Voice Diagnostics (RAIVD) - grant number\n  NGF.1607.22.013",
      "pdf_url": "http://arxiv.org/pdf/2407.17844v4",
      "published_date": "2024-07-25 07:58:19 UTC",
      "updated_date": "2024-09-24 06:29:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:23.443449"
    },
    {
      "arxiv_id": "2407.17843v2",
      "title": "DragText: Rethinking Text Embedding in Point-based Image Editing",
      "title_zh": "DragText：重新思考基于点的图像编辑中的文本嵌入",
      "authors": [
        "Gayoon Choi",
        "Taejin Jeong",
        "Sujung Hong",
        "Seong Jae Hwang"
      ],
      "abstract": "Point-based image editing enables accurate and flexible control through\ncontent dragging. However, the role of text embedding during the editing\nprocess has not been thoroughly investigated. A significant aspect that remains\nunexplored is the interaction between text and image embeddings. During the\nprogressive editing in a diffusion model, the text embedding remains constant.\nAs the image embedding increasingly diverges from its initial state, the\ndiscrepancy between the image and text embeddings presents a significant\nchallenge. In this study, we found that the text prompt significantly\ninfluences the dragging process, particularly in maintaining content integrity\nand achieving the desired manipulation. Upon these insights, we propose\nDragText, which optimizes text embedding in conjunction with the dragging\nprocess to pair with the modified image embedding. Simultaneously, we\nregularize the text optimization process to preserve the integrity of the\noriginal text prompt. Our approach can be seamlessly integrated with existing\ndiffusion-based drag methods, enhancing performance with only a few lines of\ncode.",
      "tldr_zh": "该论文重新审视了基于点的图像编辑（point-based image editing）中文本嵌入（text embedding）的作用，指出文本和图像嵌入（image embedding）之间的交互是关键挑战，尤其在扩散模型（diffusion model）中的渐进编辑过程中。研究发现，文本提示对拖拽过程有显著影响，能帮助维护内容完整性和实现预期操作。针对此，作者提出 DragText 方法，通过优化文本嵌入以匹配修改后的图像嵌入，同时通过正则化过程保留原始文本提示的完整性。该方法可无缝集成到现有基于扩散的拖拽技术中，仅需少量代码即可提升编辑性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at WACV 2025; Code is released at\n  https://github.com/MICV-yonsei/DragText",
      "pdf_url": "http://arxiv.org/pdf/2407.17843v2",
      "published_date": "2024-07-25 07:57:55 UTC",
      "updated_date": "2024-12-04 07:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:30.668670"
    },
    {
      "arxiv_id": "2407.17842v1",
      "title": "On the Opportunities of (Re)-Exploring Atmospheric Science by Foundation Models: A Case Study",
      "title_zh": "翻译失败",
      "authors": [
        "Lujia Zhang",
        "Hanzhe Cui",
        "Yurong Song",
        "Chenyue Li",
        "Binhang Yuan",
        "Mengqian Lu"
      ],
      "abstract": "Most state-of-the-art AI applications in atmospheric science are based on\nclassic deep learning approaches. However, such approaches cannot automatically\nintegrate multiple complicated procedures to construct an intelligent agent,\nsince each functionality is enabled by a separate model learned from\nindependent climate datasets. The emergence of foundation models, especially\nmultimodal foundation models, with their ability to process heterogeneous input\ndata and execute complex tasks, offers a substantial opportunity to overcome\nthis challenge. In this report, we want to explore a central question - how the\nstate-of-the-art foundation model, i.e., GPT-4o, performs various atmospheric\nscientific tasks. Toward this end, we conduct a case study by categorizing the\ntasks into four main classes, including climate data processing, physical\ndiagnosis, forecast and prediction, and adaptation and mitigation. For each\ntask, we comprehensively evaluate the GPT-4o's performance along with a\nconcrete discussion. We hope that this report may shed new light on future AI\napplications and research in atmospheric science.",
      "tldr_zh": "这篇论文探讨了基础模型(foundation models)在大气科学领域的应用潜力，特别是多模态基础模型如何克服传统深度学习方法无法整合多个复杂过程的局限性。研究者通过一个案例研究，评估了GPT-4o在四个主要任务类别中的表现，包括气候数据处理、物理诊断、预测和预报，以及适应和缓解。结果显示，GPT-4o能够处理异构输入数据并执行复杂任务，为未来大气科学中的AI应用和研究提供了新启发。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.17842v1",
      "published_date": "2024-07-25 07:57:34 UTC",
      "updated_date": "2024-07-25 07:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:45.450955"
    },
    {
      "arxiv_id": "2407.17839v1",
      "title": "Long-term Fairness in Ride-Hailing Platform",
      "title_zh": "长期公平性在叫车平台中的应用",
      "authors": [
        "Yufan Kang",
        "Jeffrey Chan",
        "Wei Shao",
        "Flora D. Salim",
        "Christopher Leckie"
      ],
      "abstract": "Matching in two-sided markets such as ride-hailing has recently received\nsignificant attention. However, existing studies on ride-hailing mainly focus\non optimising efficiency, and fairness issues in ride-hailing have been\nneglected. Fairness issues in ride-hailing, including significant earning\ndifferences between drivers and variance of passenger waiting times among\ndifferent locations, have potential impacts on economic and ethical aspects.\nThe recent studies that focus on fairness in ride-hailing exploit traditional\noptimisation methods and the Markov Decision Process to balance efficiency and\nfairness. However, there are several issues in these existing studies, such as\nmyopic short-term decision-making from traditional optimisation and instability\nof fairness in a comparably longer horizon from both traditional optimisation\nand Markov Decision Process-based methods. To address these issues, we propose\na dynamic Markov Decision Process model to alleviate fairness issues currently\nfaced by ride-hailing, and seek a balance between efficiency and fairness, with\ntwo distinct characteristics: (i) a prediction module to predict the number of\nrequests that will be raised in the future from different locations to allow\nthe proposed method to consider long-term fairness based on the whole timeline\ninstead of consider fairness only based on historical and current data\npatterns; (ii) a customised scalarisation function for multi-objective\nmulti-agent Q Learning that aims to balance efficiency and fairness. Extensive\nexperiments on a publicly available real-world dataset demonstrate that our\nproposed method outperforms existing state-of-the-art methods.",
      "tldr_zh": "该论文探讨了叫车平台（如Uber或滴滴）中的公平性问题，包括司机收入差异和不同地点乘客等待时间不均，这些问题可能影响经济和伦理方面。现有方法依赖传统优化或Markov Decision Process，但存在短期决策和长期不稳定等问题。为此，作者提出了一种动态Markov Decision Process模型，包含一个预测模块（用于预测未来请求数量以考虑长期公平）和一个自定义标量化函数（用于多目标多代理Q Learning，以平衡效率和公平）。实验在真实数据集上表明，该方法优于现有最先进方法，在长期公平性方面取得了显著改善。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by ECML PKDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17839v1",
      "published_date": "2024-07-25 07:54:07 UTC",
      "updated_date": "2024-07-25 07:54:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:03:55.071640"
    },
    {
      "arxiv_id": "2407.17838v1",
      "title": "UMono: Physical Model Informed Hybrid CNN-Transformer Framework for Underwater Monocular Depth Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Jian Wang",
        "Jing Wang",
        "Shenghui Rong",
        "Bo He"
      ],
      "abstract": "Underwater monocular depth estimation serves as the foundation for tasks such\nas 3D reconstruction of underwater scenes. However, due to the influence of\nlight and medium, the underwater environment undergoes a distinctive imaging\nprocess, which presents challenges in accurately estimating depth from a single\nimage. The existing methods fail to consider the unique characteristics of\nunderwater environments, leading to inadequate estimation results and limited\ngeneralization performance. Furthermore, underwater depth estimation requires\nextracting and fusing both local and global features, which is not fully\nexplored in existing methods. In this paper, an end-to-end learning framework\nfor underwater monocular depth estimation called UMono is presented, which\nincorporates underwater image formation model characteristics into network\narchitecture, and effectively utilize both local and global features of\nunderwater image. Experimental results demonstrate that the proposed method is\neffective for underwater monocular depth estimation and outperforms the\nexisting methods in both quantitative and qualitative analyses.",
      "tldr_zh": "这篇论文针对水下单目深度估计（underwater monocular depth estimation）的挑战，提出了一种名为 UMono 的端到端学习框架，以应对水下环境光线和介质影响导致的成像问题。UMono 整合了 underwater image formation model 的物理特性，并采用混合 CNN-Transformer 架构来有效提取和融合图像的局部和全局特征。实验结果表明，该方法在定量和定性分析中优于现有方法，为水下场景的 3D 重建提供了更准确的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17838v1",
      "published_date": "2024-07-25 07:52:11 UTC",
      "updated_date": "2024-07-25 07:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:04:05.557817"
    },
    {
      "arxiv_id": "2407.20272v1",
      "title": "An Efficient Inference Framework for Early-exit Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ruijie Miao",
        "Yihan Yan",
        "Xinshuo Yao",
        "Tong Yang"
      ],
      "abstract": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
      "tldr_zh": "该论文提出了一种高效的推理框架，针对early-exit large language models (LLMs)，以解决现有框架无法适配这类模型的问题。论文重点解决了两个挑战：(1) 在迭代级别实现批处理推理，通过处理批次直到所有序列超过early-exit置信度阈值；(2) 优化KV cache管理，在迭代终止前填充剩余层的缓存。实验结果显示，与原始vLLM相比，该框架实现了高达1.25倍的速度提升，为early-exit LLMs的实际部署提供了实用解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20272v1",
      "published_date": "2024-07-25 07:50:17 UTC",
      "updated_date": "2024-07-25 07:50:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:04:23.383043"
    },
    {
      "arxiv_id": "2407.17827v2",
      "title": "Unified Lexical Representation for Interpretable Visual-Language Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Li",
        "Yikai Wang",
        "Yanwei Fu",
        "Dongyu Ru",
        "Zheng Zhang",
        "Tong He"
      ],
      "abstract": "Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations are difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on the\nmodest multi-modal dataset and avoid intricate training configurations. On\ncross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal\ndataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M)\nand those trained from scratch on even bigger datasets (e.g., 1.1B data,\nincluding CC-12M). We conduct extensive experiments to analyze LexVLA. Codes\nare available at https://github.com/Clementine24/LexVLA.",
      "tldr_zh": "本研究针对Visual-Language Alignment (VLA)中现有方法（如CLIP）缺乏可解释性的问题，提出LexVLA框架，通过学习统一的lexical representation来实现视觉和语言模态的可解释对齐，避免了复杂的训练设计。LexVLA利用DINOv2作为视觉模型，其局部倾斜特征结合Llama 2的in-context lexical prediction能力，并引入overuse penalty机制来防止false discovery问题。实验结果显示，在CC-12M数据集上微调的LexVLA在cross-modal retrieval基准测试中超过了在更大数据集（如YFCC15M或1.1B数据）上训练的基线模型，证明了其高效性和可解释性优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17827v2",
      "published_date": "2024-07-25 07:35:27 UTC",
      "updated_date": "2024-11-11 13:46:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:04:32.856750"
    },
    {
      "arxiv_id": "2407.17816v1",
      "title": "NC-NCD: Novel Class Discovery for Node Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Hou",
        "Xueyuan Chen",
        "He Zhu",
        "Romei Liu",
        "Bowen Shi",
        "Jiaheng Liu",
        "Junran Wu",
        "Ke Xu"
      ],
      "abstract": "Novel Class Discovery (NCD) involves identifying new categories within\nunlabeled data by utilizing knowledge acquired from previously established\ncategories. However, existing NCD methods often struggle to maintain a balance\nbetween the performance of old and new categories. Discovering unlabeled new\ncategories in a class-incremental way is more practical but also more\nchallenging, as it is frequently hindered by either catastrophic forgetting of\nold categories or an inability to learn new ones. Furthermore, the\nimplementation of NCD on continuously scalable graph-structured data remains an\nunder-explored area. In response to these challenges, we introduce for the\nfirst time a more practical NCD scenario for node classification (i.e.,\nNC-NCD), and propose a novel self-training framework with prototype replay and\ndistillation called SWORD, adopted to our NC-NCD setting. Our approach enables\nthe model to cluster unlabeled new category nodes after learning labeled nodes\nwhile preserving performance on old categories without reliance on old category\nnodes. SWORD achieves this by employing a self-training strategy to learn new\ncategories and preventing the forgetting of old categories through the joint\nuse of feature prototypes and knowledge distillation. Extensive experiments on\nfour common benchmarks demonstrate the superiority of SWORD over other\nstate-of-the-art methods.",
      "tldr_zh": "本论文引入了针对节点分类的 Novel Class Discovery (NCD) 场景，即 NC-NCD，以解决现有方法在处理新类别时难以平衡旧类别性能的问题，同时避免灾难性遗忘。作者提出了一种名为 SWORD 的自训练框架，利用 prototype replay 和 knowledge distillation 策略，让模型在学习标记节点后能够聚类未标记的新类别节点，同时保持旧类别的性能，而无需依赖旧节点。实验结果显示，SWORD 在四个常见基准上优于其他最先进方法，证明了其在可扩展图结构数据上的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by CIKM'24",
      "pdf_url": "http://arxiv.org/pdf/2407.17816v1",
      "published_date": "2024-07-25 07:10:08 UTC",
      "updated_date": "2024-07-25 07:10:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:04:50.573023"
    },
    {
      "arxiv_id": "2407.20271v3",
      "title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models",
      "title_zh": "边学边忘：生成式语言模型的迭代遗忘框架",
      "authors": [
        "Haoyu Tang",
        "Ye Liu",
        "Xi Zhao",
        "Xukai Liu",
        "Yanghai Zhang",
        "Kai Zhang",
        "Xiaofang Zhou",
        "Enhong Chen"
      ],
      "abstract": "Recent advances in machine learning, particularly in Natural Language\nProcessing (NLP), have produced powerful models trained on vast datasets.\nHowever, these models risk leaking sensitive information, raising privacy\nconcerns. In response, regulatory measures such as the European Union's General\nData Protection Regulation (GDPR) have driven increasing interest in Machine\nUnlearning techniques, which enable models to selectively forget specific data\nentries. Early unlearning approaches primarily relied on pre-processing\nmethods, while more recent research has shifted towards training-based\nsolutions. Despite their effectiveness, a key limitation persists: most methods\nrequire access to original training data, which is often unavailable.\nAdditionally, directly applying unlearning techniques bears the cost of\nundermining the model's expressive capabilities. To address these challenges,\nwe introduce the Iterative Contrastive Unlearning (ICU) framework, which\nconsists of three core components: A Knowledge Unlearning Induction module\ndesigned to target specific knowledge for removal using an unlearning loss; A\nContrastive Learning Enhancement module to preserve the model's expressive\ncapabilities against the pure unlearning goal; And an Iterative Unlearning\nRefinement module that dynamically adjusts the unlearning process through\nongoing evaluation and updates. Experimental results demonstrate the efficacy\nof our ICU method in unlearning sensitive information while maintaining the\nmodel's overall performance, offering a promising solution for\nprivacy-conscious machine learning applications.",
      "tldr_zh": "该论文探讨了生成语言模型在训练数据中泄露敏感信息的问题，强调了如 GDPR 这样的法规推动 Machine Unlearning 技术的发展。现有方法依赖原始训练数据并可能削弱模型的表现，为此，作者提出 Iterative Contrastive Unlearning (ICU) 框架，包括 Knowledge Unlearning Induction 模块（使用 unlearning loss 移除特定知识）、Contrastive Learning Enhancement 模块（保护模型表达能力）以及 Iterative Unlearning Refinement 模块（通过动态评估和更新优化过程）。ICU 通过迭代对比学习平衡了隐私保护与性能维护，实验结果显示，该框架能有效删除敏感信息，同时保持模型整体性能，为隐私导向的机器学习应用提供了可行解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20271v3",
      "published_date": "2024-07-25 07:09:35 UTC",
      "updated_date": "2025-02-22 02:45:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:04:59.112848"
    },
    {
      "arxiv_id": "2407.17813v1",
      "title": "Enhancing Model Performance: Another Approach to Vision-Language Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Vedanshu",
        "MM Tripathi",
        "Bhavnesh Jaint"
      ],
      "abstract": "The integration of large language models (LLMs) with vision-language (VL)\ntasks has been a transformative development in the realm of artificial\nintelligence, highlighting the potential of LLMs as a versatile general-purpose\nchatbot. However, the current trend in this evolution focuses on the\nintegration of vision and language to create models that can operate in more\ndiverse and real-world contexts. We present a novel approach, termed Bottleneck\nAdapter, specifically crafted for enhancing the multimodal functionalities of\nthese complex models, enabling joint optimization of the entire multimodal LLM\nframework through a process known as Multimodal Model Tuning (MMT). Our\napproach utilizes lightweight adapters to connect the image encoder and LLM\nwithout the need for large, complex neural networks. Unlike the conventional\nmodular training schemes, our approach adopts an end-to-end optimization\nregime, which, when combined with the adapters, facilitates the joint\noptimization using a significantly smaller parameter set. Our method exhibits\nrobust performance with 90.12\\% accuracy, outperforming both human-level\nperformance (88.4\\%) and LaVIN-7B (89.41\\%).",
      "tldr_zh": "本研究提出了一种名为Bottleneck Adapter的新方法，用于提升视觉语言指令调整(Vision-Language Instruction Tuning)的模型性能，旨在通过Multimodal Model Tuning (MMT)实现大型语言模型(LLMs)和视觉语言(VL)任务的联合优化。不同于传统的模块化训练，该方法采用轻量级适配器连接图像编码器和LLM，进行端到端优化，从而以更少的参数集实现高效的多模态框架。实验结果显示，该方法在相关任务上达到90.12%的准确率，超过了人类水平(88.4%)和LaVIN-7B(89.41%)的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17813v1",
      "published_date": "2024-07-25 06:59:15 UTC",
      "updated_date": "2024-07-25 06:59:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:05:19.536089"
    },
    {
      "arxiv_id": "2407.17801v1",
      "title": "EEG-SSM: Leveraging State-Space Model for Dementia Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan-The Tran",
        "Linh Le",
        "Quoc Toan Nguyen",
        "Thomas Do",
        "Chin-Teng Lin"
      ],
      "abstract": "State-space models (SSMs) have garnered attention for effectively processing\nlong data sequences, reducing the need to segment time series into shorter\nintervals for model training and inference. Traditionally, SSMs capture only\nthe temporal dynamics of time series data, omitting the equally critical\nspectral features. This study introduces EEG-SSM, a novel state-space\nmodel-based approach for dementia classification using EEG data. Our model\nfeatures two primary innovations: EEG-SSM temporal and EEG-SSM spectral\ncomponents. The temporal component is designed to efficiently process EEG\nsequences of varying lengths, while the spectral component enhances the model\nby integrating frequency-domain information from EEG signals. The synergy of\nthese components allows EEG-SSM to adeptly manage the complexities of\nmultivariate EEG data, significantly improving accuracy and stability across\ndifferent temporal resolutions. Demonstrating a remarkable 91.0 percent\naccuracy in classifying Healthy Control (HC), Frontotemporal Dementia (FTD),\nand Alzheimer's Disease (AD) groups, EEG-SSM outperforms existing models on the\nsame dataset. The development of EEG-SSM represents an improvement in the use\nof state-space models for screening dementia, offering more precise and\ncost-effective tools for clinical neuroscience.",
      "tldr_zh": "本研究提出 EEG-SSM，一种基于 State-Space Model 的创新方法，用于利用 EEG 数据进行痴呆分类，解决了传统 SSMs 仅捕捉时序动态而忽略频谱特征的局限。模型包括 EEG-SSM temporal 组件处理不同长度的 EEG 序列，以及 EEG-SSM spectral 组件整合频域信息，从而提升了对多变量 EEG 数据的准确性和稳定性。在实验中，EEG-SSM 在分类健康对照 (HC)、额颞叶痴呆 (FTD) 和阿尔茨海默病 (AD) 时达到 91.0% 的准确率，优于现有模型。该方法为痴呆筛查提供了更精确且成本有效的临床工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17801v1",
      "published_date": "2024-07-25 06:20:03 UTC",
      "updated_date": "2024-07-25 06:20:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:05:30.510965"
    },
    {
      "arxiv_id": "2407.17797v1",
      "title": "A Unified Understanding of Adversarial Vulnerability Regarding Unimodal Models and Vision-Language Pre-training Models",
      "title_zh": "一种对单模态模型和视觉-语言预训练模型对抗性脆弱性的统一理解",
      "authors": [
        "Haonan Zheng",
        "Xinyang Deng",
        "Wen Jiang",
        "Wenrui Li"
      ],
      "abstract": "With Vision-Language Pre-training (VLP) models demonstrating powerful\nmultimodal interaction capabilities, the application scenarios of neural\nnetworks are no longer confined to unimodal domains but have expanded to more\ncomplex multimodal V+L downstream tasks. The security vulnerabilities of\nunimodal models have been extensively examined, whereas those of VLP models\nremain challenging. We note that in CV models, the understanding of images\ncomes from annotated information, while VLP models are designed to learn image\nrepresentations directly from raw text. Motivated by this discrepancy, we\ndeveloped the Feature Guidance Attack (FGA), a novel method that uses text\nrepresentations to direct the perturbation of clean images, resulting in the\ngeneration of adversarial images. FGA is orthogonal to many advanced attack\nstrategies in the unimodal domain, facilitating the direct application of rich\nresearch findings from the unimodal to the multimodal scenario. By\nappropriately introducing text attack into FGA, we construct Feature Guidance\nwith Text Attack (FGA-T). Through the interaction of attacking two modalities,\nFGA-T achieves superior attack effects against VLP models. Moreover,\nincorporating data augmentation and momentum mechanisms significantly improves\nthe black-box transferability of FGA-T. Our method demonstrates stable and\neffective attack capabilities across various datasets, downstream tasks, and\nboth black-box and white-box settings, offering a unified baseline for\nexploring the robustness of VLP models.",
      "tldr_zh": "该论文探讨了单模态模型和视觉语言预训练（VLP）模型的对抗漏洞差异，强调 VLP 模型通过从原始文本学习图像表示而非依赖标注信息。作者提出 Feature Guidance Attack (FGA)，一种利用文本表示指导图像扰动的方法，以生成对抗样本，并扩展为 FGA-T，通过同时攻击文本和图像模态来增强对 VLP 模型的攻击效果。实验结果表明，FGA-T 结合数据增强和动量机制后，在各种数据集、下游任务和黑白盒设置中实现了稳定的攻击性能，并为 VLP 模型的鲁棒性研究提供了统一基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 9 figures, published in ACMMM2024(oral)",
      "pdf_url": "http://arxiv.org/pdf/2407.17797v1",
      "published_date": "2024-07-25 06:10:33 UTC",
      "updated_date": "2024-07-25 06:10:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:05:40.817023"
    },
    {
      "arxiv_id": "2407.17791v2",
      "title": "Untrained neural networks can demonstrate memorization-independent abstract reasoning",
      "title_zh": "未训练的神经网络可以展示独立于记忆的抽象推理",
      "authors": [
        "Tomer Barak",
        "Yonatan Loewenstein"
      ],
      "abstract": "The nature of abstract reasoning is a matter of debate. Modern artificial\nneural network (ANN) models, like large language models, demonstrate impressive\nsuccess when tested on abstract reasoning problems. However, it has been argued\nthat their success reflects some form of memorization of similar problems (data\ncontamination) rather than a general-purpose abstract reasoning capability.\nThis concern is supported by evidence of brittleness, and the requirement of\nextensive training. In our study, we explored whether abstract reasoning can be\nachieved using the toolbox of ANNs, without prior training. Specifically, we\nstudied an ANN model in which the weights of a naive network are optimized\nduring the solution of the problem, using the problem data itself, rather than\nany prior knowledge. We tested this modeling approach on visual reasoning\nproblems and found that it performs relatively well. Crucially, this success\ndoes not rely on memorization of similar problems. We further suggest an\nexplanation of how it works. Finally, as problem solving is performed by\nchanging the ANN weights, we explored the connection between problem solving\nand the accumulation of knowledge in the ANNs.",
      "tldr_zh": "本研究探讨了神经网络(ANNs)是否能在没有先验训练的情况下实现独立于记忆(memorization)的抽象推理，以回应对现有模型依赖数据污染的质疑。研究方法涉及优化一个naive网络的权重，使用问题数据本身进行问题求解，而非依赖任何先前知识。在视觉推理问题上测试，该方法表现出相对良好的性能，且不依赖于类似问题的记忆。该框架不仅解释了其工作原理，还揭示了问题求解与ANNs中知识积累之间的联系，为开发更可靠的抽象推理能力提供了新见解。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17791v2",
      "published_date": "2024-07-25 05:58:58 UTC",
      "updated_date": "2024-11-08 13:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:05:45.477022"
    },
    {
      "arxiv_id": "2407.17789v2",
      "title": "Very Large-Scale Multi-Agent Simulation in AgentScope",
      "title_zh": "AgentScope 中的超大规模多智能体模拟",
      "authors": [
        "Xuchen Pan",
        "Dawei Gao",
        "Yuexiang Xie",
        "Yushuo Chen",
        "Zhewei Wei",
        "Yaliang Li",
        "Bolin Ding",
        "Ji-Rong Wen",
        "Jingren Zhou"
      ],
      "abstract": "Recent advances in large language models (LLMs) have opened new avenues for\napplying multi-agent systems in very large-scale simulations. However, there\nremain several challenges when conducting multi-agent simulations with existing\nplatforms, such as limited scalability and low efficiency, unsatisfied agent\ndiversity, and effort-intensive management processes. To address these\nchallenges, we develop several new features and components for AgentScope, a\nuser-friendly multi-agent platform, enhancing its convenience and flexibility\nfor supporting very large-scale multi-agent simulations. Specifically, we\npropose an actor-based distributed mechanism as the underlying technological\ninfrastructure towards great scalability and high efficiency, and provide\nflexible environment support for simulating various real-world scenarios, which\nenables parallel execution of multiple agents, automatic workflow conversion\nfor distributed deployment, and both inter-agent and agent-environment\ninteractions. Moreover, we integrate an easy-to-use configurable tool and an\nautomatic background generation pipeline in AgentScope, simplifying the process\nof creating agents with diverse yet detailed background settings. Last but not\nleast, we provide a web-based interface for conveniently monitoring and\nmanaging a large number of agents that might deploy across multiple devices. We\nconduct a comprehensive simulation to demonstrate the effectiveness of these\nproposed enhancements in AgentScope, and provide detailed observations and\ninsightful discussions to highlight the great potential of applying multi-agent\nsystems in large-scale simulations. The source code is released on GitHub at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation\nto inspire further research and development in large-scale multi-agent\nsimulations.",
      "tldr_zh": "该论文探讨了利用大型语言模型 (LLMs) 进行大规模多智能体模拟的挑战，包括有限的可扩展性、低效率、代理多样性不足和管理过程繁琐等问题，并针对 AgentScope 平台引入了新功能以提升其便利性和灵活性。具体方法包括采用基于 actor 的分布式机制实现高效可扩展模拟、提供灵活的环境支持（如并行执行和交互）、集成易用工具及自动背景生成管道简化代理创建，以及开发 web 接口便于监控和管理大量代理。通过全面模拟实验，证明了这些增强的有效性，并提供了观察和讨论，突显多智能体系统在大型模拟中的潜力。代码已在 GitHub 上开源，以推动进一步研究。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "We have released code on\n  https://github.com/modelscope/agentscope/tree/main/examples/paper_large_scale_simulation",
      "pdf_url": "http://arxiv.org/pdf/2407.17789v2",
      "published_date": "2024-07-25 05:50:46 UTC",
      "updated_date": "2024-10-28 13:19:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:06:06.692264"
    },
    {
      "arxiv_id": "2407.17787v1",
      "title": "HC-GST: Heterophily-aware Distribution Consistency based Graph Self-training",
      "title_zh": "翻译失败",
      "authors": [
        "Fali Wang",
        "Tianxiang Zhao",
        "Junjie Xu",
        "Suhang Wang"
      ],
      "abstract": "Graph self-training (GST), which selects and assigns pseudo-labels to\nunlabeled nodes, is popular for tackling label sparsity in graphs. However,\nrecent study on homophily graphs show that GST methods could introduce and\namplify distribution shift between training and test nodes as they tend to\nassign pseudo-labels to nodes they are good at. As GNNs typically perform\nbetter on homophilic nodes, there could be potential shifts towards homophilic\npseudo-nodes, which is underexplored. Our preliminary experiments on\nheterophilic graphs verify that these methods can cause shifts in homophily\nratio distributions, leading to \\textit{training bias} that improves\nperformance on homophilic nodes while degrading it on heterophilic ones.\nTherefore, we study a novel problem of reducing homophily ratio distribution\nshifts during self-training on heterophilic graphs. A key challenge is the\naccurate calculation of homophily ratios and their distributions without\nextensive labeled data. To tackle them, we propose a novel Heterophily-aware\nDistribution Consistency-based Graph Self-Training (HC-GST) framework, which\nestimates homophily ratios using soft labels and optimizes a selection vector\nto align pseudo-nodes with the global homophily ratio distribution. Extensive\nexperiments on both homophilic and heterophilic graphs show that HC-GST\neffectively reduces training bias and enhances self-training performance.",
      "tldr_zh": "该论文研究了图自训练（Graph Self-Training, GST）在异质图（heterophilic graphs）中的问题，即现有方法可能导致同质性比率（homophily ratio）分布偏移，形成训练偏差，提升同质节点（homophilic nodes）性能的同时降低异质节点性能。针对这一问题，作者提出了一种新型框架 HC-GST，通过使用软标签估计同质性比率，并优化选择向量来使伪节点与全局同质性比率分布一致，从而减少分布偏移。实验结果显示，HC-GST 在同质和异质图上均有效降低了训练偏差，并显著提升了自训练的整体性能。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17787v1",
      "published_date": "2024-07-25 05:38:06 UTC",
      "updated_date": "2024-07-25 05:38:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:06:14.246713"
    },
    {
      "arxiv_id": "2407.17783v1",
      "title": "How Lightweight Can A Vision Transformer Be",
      "title_zh": "翻译失败",
      "authors": [
        "Jen Hong Tan"
      ],
      "abstract": "In this paper, we explore a strategy that uses Mixture-of-Experts (MoE) to\nstreamline, rather than augment, vision transformers. Each expert in an MoE\nlayer is a SwiGLU feedforward network, where V and W2 are shared across the\nlayer. No complex attention or convolutional mechanisms are employed.\nDepth-wise scaling is applied to progressively reduce the size of the hidden\nlayer and the number of experts is increased in stages. Grouped query attention\nis used. We studied the proposed approach with and without pre-training on\nsmall datasets and investigated whether transfer learning works at this scale.\nWe found that the architecture is competitive even at a size of 0.67M\nparameters.",
      "tldr_zh": "这篇论文探讨了使用 Mixture-of-Experts (MoE) 来简化视觉变压器的策略，每个专家是 SwiGLU 前向网络，并共享 V 和 W2 参数，而不采用复杂的注意力或卷积机制。方法包括深度缩放逐步减少隐藏层尺寸、分阶段增加专家数量，以及应用 Grouped query attention，以实现轻量级设计。实验结果表明，该架构即使在 0.67M 参数规模下，也具有竞争力，并在小数据集上支持预训练和转移学习。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17783v1",
      "published_date": "2024-07-25 05:23:20 UTC",
      "updated_date": "2024-07-25 05:23:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:06:37.506569"
    },
    {
      "arxiv_id": "2407.17777v2",
      "title": "Babel: A Scalable Pre-trained Model for Multi-Modal Sensing via Expandable Modality Alignment",
      "title_zh": "Babel：一个可扩展的预训练模型，用于多模态感知通过可扩展的模态对齐",
      "authors": [
        "Shenghong Dai",
        "Shiqi Jiang",
        "Yifan Yang",
        "Ting Cao",
        "Mo Li",
        "Suman Banerjee",
        "Lili Qiu"
      ],
      "abstract": "This paper presents Babel, the expandable modality alignment model, specially\ndesigned for multi-modal sensing. While there has been considerable work on\nmulti-modality alignment, they all struggle to effectively incorporate multiple\nsensing modalities due to the data scarcity constraints. How to utilize\nmulti-modal data with partial pairings in sensing remains an unresolved\nchallenge. Babel tackles this challenge by introducing the concept of\nexpandable modality alignment. The key idea involves transforming the\nN-modality alignment into a series of binary-modality alignments. Novel\ntechniques are also proposed to further mitigate data scarcity issue and\nbalance the contribution of the newly incorporated modality with the previously\nestablished modality alignment during the expandable alignment process. We\nprovide the comprehensive implementation. In the pre-training phase, Babel\ncurrently aligns 6 sensing modalities, namely Wi-Fi, mmWave, IMU, LiDAR, video,\nand depth. For the deployment phase, as a foundation model, any single or\ncombination of aligned modalities could be selected from Babel and applied to\ndownstream tasks. Evaluation demonstrates Babel's outstanding performance on\neight human activity recognition datasets, compared to a broad range of\nbaselines e.g., the SOTA single-modal sensing networks, multi-modal sensing\nframework, and multi-modal large language models. Babel not only improves the\nperformance of individual modality sensing (12% averaged accuracy improvement),\nbut also effectively fuses multiple available modalities (up to 22% accuracy\nincrease). Case studies also highlight emerging application scenarios empowered\nby Babel, including cross-modality retrieval (i.e., sensing imaging), and\nbridging LLM for sensing comprehension.",
      "tldr_zh": "本论文提出Babel，一种可扩展的多模态对齐预训练模型，用于多模态感知，旨在解决现有方法在处理部分配对数据时的挑战。Babel的核心方法是将N-modality alignment转化为一系列binary-modality alignments，并引入新技巧来缓解数据稀缺问题，同时平衡新模态与现有模态的贡献；在预训练中，对齐了Wi-Fi, mmWave, IMU, LiDAR, video和depth等6种模态。实验结果显示，Babel在八个人类活动识别数据集上优于基线模型，平均提升单个模态的准确率12%，多模态融合最高提高22%，并支持新兴应用如跨模态检索和桥接LLM用于感知理解。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by SenSys'25",
      "pdf_url": "http://arxiv.org/pdf/2407.17777v2",
      "published_date": "2024-07-25 05:10:48 UTC",
      "updated_date": "2025-03-21 10:51:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:06:51.038991"
    },
    {
      "arxiv_id": "2408.00588v1",
      "title": "Closing the gap between open-source and commercial large language models for medical evidence summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Gongbo Zhang",
        "Qiao Jin",
        "Yiliang Zhou",
        "Song Wang",
        "Betina R. Idnay",
        "Yiming Luo",
        "Elizabeth Park",
        "Jordan G. Nestor",
        "Matthew E. Spotnitz",
        "Ali Soroush",
        "Thomas Campion",
        "Zhiyong Lu",
        "Chunhua Weng",
        "Yifan Peng"
      ],
      "abstract": "Large language models (LLMs) hold great promise in summarizing medical\nevidence. Most recent studies focus on the application of proprietary LLMs.\nUsing proprietary LLMs introduces multiple risk factors, including a lack of\ntransparency and vendor dependency. While open-source LLMs allow better\ntransparency and customization, their performance falls short compared to\nproprietary ones. In this study, we investigated to what extent fine-tuning\nopen-source LLMs can further improve their performance in summarizing medical\nevidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs\nof systematic reviews and summaries, we fine-tuned three broadly-used,\nopen-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned\nLLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval:\n8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and\n15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of\nfine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore,\nsmaller fine-tuned models sometimes even demonstrated superior performance\ncompared to larger zero-shot models. The above trends of improvement were also\nmanifested in both human and GPT4-simulated evaluations. Our results can be\napplied to guide model selection for tasks demanding particular domain\nknowledge, such as medical evidence summarization.",
      "tldr_zh": "本研究探讨了通过微调开源大型语言模型（LLMs）来缩小其与商业LLMs在医疗证据总结方面的性能差距，旨在解决开源模型的透明性和自定义优势与性能不足的矛盾。\n研究者使用包含8,161对系统评论和总结的MedReview数据集，对PRIMERA、LongT5和Llama-2等开源LLMs进行微调。\n结果显示，微调后模型的ROUGE-L分数提高了9.89（95%置信区间：8.94-10.81）、METEOR分数提高了13.21（95%置信区间：12.05-14.37），以及CHRF分数提高了15.82（95%置信区间：13.89-16.44）。\n微调后的LongT5性能接近零-shot的GPT-3.5，且某些较小微调模型甚至优于较大的零-shot模型，这些改进也在人工和GPT4模拟评估中得到验证。\n该研究为选择适用于需要特定领域知识的任务（如医疗证据总结）的模型提供了实用指导。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00588v1",
      "published_date": "2024-07-25 05:03:01 UTC",
      "updated_date": "2024-07-25 05:03:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:07:04.385118"
    },
    {
      "arxiv_id": "2407.17773v3",
      "title": "KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Eunice Yiu",
        "Maan Qraitem",
        "Anisa Noor Majhi",
        "Charlie Wong",
        "Yutong Bai",
        "Shiry Ginosar",
        "Alison Gopnik",
        "Kate Saenko"
      ],
      "abstract": "This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 4,300 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children (ages three to five) and to adults. We structure\nthe evaluation into three stages: identifying what changed (e.g., color,\nnumber, etc.), how it changed (e.g., added one object), and applying the rule\nto new scenarios. Our findings show that while GPT-o1, GPT-4V, LLaVA-1.5, and\nMANTIS identify the \"what\" effectively, they struggle with quantifying the\n\"how\" and extrapolating this rule to new objects. In contrast, children and\nadults exhibit much stronger analogical reasoning at all three stages.\nAdditionally, the strongest tested model, GPT-o1, performs better in tasks\ninvolving simple surface-level visual attributes like color and size,\ncorrelating with quicker human adult response times. Conversely, more complex\ntasks such as number, rotation, and reflection, which necessitate extensive\ncognitive processing and understanding of extrinsic spatial properties in the\nphysical world, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.",
      "tldr_zh": "本研究提出KiVA基准测试，受儿童视觉类比推理启发，用于评估大型多模态模型(LMMs)的视觉类比能力，并与人类成人和儿童(3-5岁)进行比较。该基准包括4300个日常物体视觉变换，分为三个阶段：识别变化内容(如颜色、数量)、变化方式(如添加对象)，以及将规则应用到新场景。结果显示，模型如GPT-o1、GPT-4V和LLaVA-1.5在识别“what”方面表现良好，但难以量化“how”并外推规则，尤其在复杂任务如旋转和反射上落后于人类。总体而言，该研究突显了LMMs在处理认知密集型任务时的局限性，强调训练数据主要依赖2D图像和文本的不足。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages. Project website: https://ey242.github.io/kiva.github.io/.\n  Benchmark and code: https://github.com/ey242/KiVA",
      "pdf_url": "http://arxiv.org/pdf/2407.17773v3",
      "published_date": "2024-07-25 05:02:39 UTC",
      "updated_date": "2025-03-05 03:07:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:07:16.669834"
    },
    {
      "arxiv_id": "2407.17762v1",
      "title": "Mpox Detection Advanced: Rapid Epidemic Response Through Synthetic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yudara Kularathne",
        "Prathapa Janitha",
        "Sithira Ambepitiya",
        "Prarththanan Sothyrajah",
        "Thanveer Ahamed",
        "Dinuka Wijesundara"
      ],
      "abstract": "Rapid development of disease detection models using computer vision is\ncrucial in responding to medical emergencies, such as epidemics or bioterrorism\nevents. Traditional data collection methods are often too slow in these\nscenarios, requiring innovative approaches for quick, reliable model generation\nfrom minimal data. Our study introduces a novel approach by constructing a\ncomprehensive computer vision model to detect Mpox lesions using only synthetic\ndata. Initially, these models generated a diverse set of synthetic images\nrepresenting Mpox lesions on various body parts (face, back, chest, leg, neck,\narm) across different skin tones as defined by the Fitzpatrick scale (fair,\nbrown, dark skin). Subsequently, we trained and tested a vision model with this\nsynthetic dataset to evaluate the diffusion models' efficacy in producing\nhigh-quality training data and its impact on the vision model's medical image\nrecognition performance. The results were promising; the vision model achieved\na 97% accuracy rate, with 96% precision and recall for Mpox cases, and\nsimilarly high metrics for normal and other skin disorder cases, demonstrating\nits ability to correctly identify true positives and minimize false positives.\nThe model achieved an F1-Score of 96% for Mpox cases and 98% for normal and\nother skin disorders, reflecting a balanced precision-recall relationship, thus\nensuring reliability and robustness in its predictions. Our proposed\nSynthVision methodology indicates the potential to develop accurate computer\nvision models with minimal data input for future medical emergencies.",
      "tldr_zh": "该研究提出了一种名为 SynthVision 的新方法，利用合成数据快速构建计算机视觉模型，以应对疫情等医疗紧急事件的快速检测需求。研究团队生成了一系列合成图像，涵盖 Mpox 病变在不同身体部位（面部、背部、胸部、腿部、颈部、手臂）和 Fitzpatrick scale 定义的各种皮肤色调（fair、brown、dark skin），从而避免传统数据收集的延误。实验结果显示，训练后的视觉模型在 Mpox 检测中达到 97% 准确率、96% 精确率和召回率、96% F1-Score，并在正常和其它皮肤病变上表现同样出色，证明了合成数据在生成高质量训练集方面的有效性。该方法为未来医疗紧急情况提供了一种可靠的快速模型开发途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2407.17762v1",
      "published_date": "2024-07-25 04:33:19 UTC",
      "updated_date": "2024-07-25 04:33:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:07:25.233001"
    },
    {
      "arxiv_id": "2407.17760v1",
      "title": "TwIPS: A Large Language Model Powered Texting Application to Simplify Conversational Nuances for Autistic Users",
      "title_zh": "翻译失败",
      "authors": [
        "Rukhshan Haroon",
        "Fahad Dogar"
      ],
      "abstract": "Autistic individuals often experience difficulties in conveying and\ninterpreting emotional tone and non-literal nuances. Many also mask their\ncommunication style to avoid being misconstrued by others, spending\nconsiderable time and mental effort in the process. To address these challenges\nin text-based communication, we present TwIPS, a prototype texting application\npowered by a large language model (LLM), which can assist users with: a)\ndeciphering tone and meaning of incoming messages, b) ensuring the emotional\ntone of their message is in line with their intent, and c) coming up with\nalternate phrasing for messages that could be misconstrued and received\nnegatively by others. We leverage an AI-based simulation and a conversational\nscript to evaluate TwIPS with 8 autistic participants in an in-lab setting. Our\nfindings show TwIPS enables a convenient way for participants to seek\nclarifications, provides a better alternative to tone indicators, and\nfacilitates constructive reflection on writing technique and style. We also\nexamine how autistic users utilize language for self-expression and\ninterpretation in instant messaging, and gather feedback for enhancing our\nprototype. We conclude with a discussion around balancing user-autonomy with\nAI-mediation, establishing appropriate trust levels in AI systems, and\ncustomization needs if autistic users in the context of AI-assisted\ncommunication",
      "tldr_zh": "该研究针对自闭症用户在文本通信中解读和表达情感语气及非字面含义的困难，提出TwIPS，一款由Large Language Model (LLM)驱动的原型文本应用。TwIPS的功能包括解读传入消息的语气、确保用户消息的情感意图一致，以及为易被误解的消息提供替代表述。通过AI模拟和对话脚本在实验室环境中评估8名自闭症参与者，发现TwIPS便于寻求澄清、优于传统语气指示，并促进对写作技巧的反思。论文还讨论了在AI辅助通信中平衡用户自治与AI干预、建立适当的AI信任水平以及个性化定制需求。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17760v1",
      "published_date": "2024-07-25 04:15:54 UTC",
      "updated_date": "2024-07-25 04:15:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:07:50.490680"
    },
    {
      "arxiv_id": "2407.17734v1",
      "title": "Cost-effective Instruction Learning for Pathology Vision and Language Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Kaitao Chen",
        "Mianxin Liu",
        "Fang Yan",
        "Lei Ma",
        "Xiaoming Shi",
        "Lilong Wang",
        "Xiaosong Wang",
        "Lifeng Zhu",
        "Zhe Wang",
        "Mu Zhou",
        "Shaoting Zhang"
      ],
      "abstract": "The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology.",
      "tldr_zh": "本研究提出了一种成本有效的指令学习框架CLOVER，用于病理学领域的视觉和语言分析，旨在通过减少训练数据和计算资源来推动AI在临床中的应用。CLOVER仅训练轻量级模块并使用instruction tuning，同时冻结大语言模型的参数，并通过GPT-3.5生成基于病理知识的提示指令，同时构建高质量的模板-based指令集。实验结果显示，CLOVER在两个基准数据集上表现出色，尤其在病理视觉问答任务中优于基线模型，后者拥有37倍的参数且依赖GPT-4生成指令；此外，CLOVER在外部临床数据集上展现出强大的few-shot学习鲁棒性，有望加速数字病理学中对话应用的采用。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17734v1",
      "published_date": "2024-07-25 03:12:57 UTC",
      "updated_date": "2024-07-25 03:12:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:08:06.143802"
    },
    {
      "arxiv_id": "2407.21057v1",
      "title": "Multi-group Uncertainty Quantification for Long-form Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Terrance Liu",
        "Zhiwei Steven Wu"
      ],
      "abstract": "While large language models are rapidly moving towards consumer-facing\napplications, they are often still prone to factual errors and hallucinations.\nIn order to reduce the potential harms that may come from these errors, it is\nimportant for users to know to what extent they can trust an LLM when it makes\na factual claim. To this end, we study the problem of uncertainty\nquantification of factual correctness in long-form natural language generation.\nGiven some output from a large language model, we study both uncertainty at the\nlevel of individual claims contained within the output (via calibration) and\nuncertainty across the entire output itself (via conformal prediction).\nMoreover, we invoke multicalibration and multivalid conformal prediction to\nensure that such uncertainty guarantees are valid both marginally and across\ndistinct groups of prompts. Using the task of biography generation, we\ndemonstrate empirically that having access to and making use of additional\ngroup attributes for each prompt improves both overall and group-wise\nperformance. As the problems of calibration, conformal prediction, and their\nmulti-group counterparts have not been extensively explored previously in the\ncontext of long-form text generation, we consider these empirical results to\nform a benchmark for this setting.",
      "tldr_zh": "这篇论文研究了在长文本生成中量化大型语言模型(LLMs)的 factual correctness 不确定性，以减少错误和幻觉带来的风险。作者提出使用 calibration（针对单个声明的不确定性）和 conformal prediction（针对整个输出不确定性）的方法，并引入 multicalibration 和 multivalid conformal prediction，确保不确定性保证在整体和不同提示组别上都有效。通过传记生成任务的实验，他们证明了使用额外组属性能显著提升整体和组别性能，为长文本生成中的不确定性量化问题建立了基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21057v1",
      "published_date": "2024-07-25 02:59:52 UTC",
      "updated_date": "2024-07-25 02:59:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:08:03.263649"
    },
    {
      "arxiv_id": "2407.17695v2",
      "title": "Enhancing Agent Learning through World Dynamics Modeling",
      "title_zh": "通过世界动态建模增强代理学习",
      "authors": [
        "Zhiyuan Sun",
        "Haochen Shi",
        "Marc-Alexandre Côté",
        "Glen Berseth",
        "Xingdi Yuan",
        "Bang Liu"
      ],
      "abstract": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.",
      "tldr_zh": "该论文提出DiVE框架，以提升大型语言模型(LLMs)在交互决策中的表现，通过发现、验证和演化世界动态来弥补LLMs对环境知识的潜在缺失。DiVE包括三个关键组件：从少量演示中发现世界动态、验证其准确性，以及根据当前情况演化高级动态。实验结果显示，使用DiVE引导的LLMs在Crafter环境中实现了与人类玩家相当的奖励，并在MiniHack环境中超越了需要任务特定训练的基准方法，从而使代理学习更具适应性和高效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17695v2",
      "published_date": "2024-07-25 01:32:41 UTC",
      "updated_date": "2024-10-15 15:48:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:09:05.824427"
    },
    {
      "arxiv_id": "2407.17688v2",
      "title": "Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification",
      "title_zh": "研究政治偏见对大型语言模型在立场分类中性能的影响",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Iain Cruickshank",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nexecuting tasks based on natural language queries. However, these models,\ntrained on curated datasets, inherently embody biases ranging from racial to\nnational and gender biases. It remains uncertain whether these biases impact\nthe performance of LLMs for certain tasks. In this study, we investigate the\npolitical biases of LLMs within the stance classification task, specifically\nexamining whether these models exhibit a tendency to more accurately classify\npolitically-charged stances. Utilizing three datasets, seven LLMs, and four\ndistinct prompting schemes, we analyze the performance of LLMs on politically\noriented statements and targets. Our findings reveal a statistically\nsignificant difference in the performance of LLMs across various politically\noriented stance classification tasks. Furthermore, we observe that this\ndifference primarily manifests at the dataset level, with models and prompting\nschemes showing statistically similar performances across different stance\nclassification datasets. Lastly, we observe that when there is greater\nambiguity in the target the statement is directed towards, LLMs have poorer\nstance classification accuracy.\n  Code & Dataset: http://doi.org/10.5281/zenodo.12938478",
      "tldr_zh": "本研究考察了Large Language Models (LLMs) 的政治偏见对立场分类任务性能的影响，探讨这些模型是否在处理政治导向语句时表现出偏见。研究采用三个数据集、七个LLMs 和四种提示方案，对政治相关语句和目标进行分析，结果显示LLMs 在不同政治导向任务中的性能存在统计显著差异，主要体现在数据集层面，而模型和提示方案在各数据集间的表现相似。最后，发现当语句指向的目标模糊时，LLMs 的立场分类准确率会降低。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICWSM 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.17688v2",
      "published_date": "2024-07-25 01:11:38 UTC",
      "updated_date": "2024-07-26 12:47:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:08:26.684941"
    },
    {
      "arxiv_id": "2407.17687v2",
      "title": "A Crowding Distance That Provably Solves the Difficulties of the NSGA-II in Many-Objective Optimization",
      "title_zh": "一种可证明解决 NSGA-II 在多目标优化中难题的拥挤距离",
      "authors": [
        "Weijie Zheng",
        "Yao Gao",
        "Benjamin Doerr"
      ],
      "abstract": "Recent theoretical works have shown that the NSGA-II can have enormous\ndifficulties to solve problems with more than two objectives. In contrast,\nalgorithms like the NSGA-III or SMS-EMOA, differing from the NSGA-II only in\nthe secondary selection criterion, provably perform well in these situations.\n  To remedy this shortcoming of the NSGA-II, but at the same time keep the\nadvantages of the widely accepted crowding distance, we use the insights of\nthese previous work to define a variant of the crowding distance, called\ntruthful crowding distance. Different from the classic crowding distance, it\nhas for any number of objectives the desirable property that a small crowding\ndistance value indicates that some other solution has a similar objective\nvector.\n  Building on this property, we conduct mathematical runtime analyses for the\nNSGA-II with truthful crowding distance. We show that this algorithm can solve\nthe many-objective versions of the OneMinMax, COCZ, LOTZ, and OJZJ$_k$ problems\nin the same (polynomial) asymptotic runtimes as the NSGA-III and the SMS-EMOA.\nThis contrasts the exponential lower bounds previously shown for the classic\nNSGA-II. For the bi-objective versions of these problems, our NSGA-II has a\nsimilar performance as the classic NSGA-II, gaining however from smaller\nadmissible population sizes. For the bi-objective OneMinMax problem, we also\nobserve a (minimally) better performance in approximating the Pareto front.\n  These results suggest that our truthful version of the NSGA-II has the same\ngood performance as the classic NSGA-II in two objectives, but can resolve the\ndrastic problems in more than two objectives.",
      "tldr_zh": "本研究针对NSGA-II算法在多目标优化（many-objective optimization）中的困难，提出了一种改进的变体：truthful crowding distance。该方法保留了原crowding distance的优势，同时确保在任意目标数量下，小crowding distance值能准确表示其他解具有相似的目标向量，从而提升算法在多目标场景下的性能。通过数学运行时分析，研究证明，使用truthful crowding distance的NSGA-II能以多项式渐近运行时解决OneMinMax、COCZ、LOTZ和OJZJ$_k$等问题的多目标版本，与NSGA-III和SMS-EMOA相当，相比原NSGA-II的指数时间下界有显著改善。在双目标问题上，该算法表现与原NSGA-II类似，甚至在逼近Pareto front方面略优。总的来说，此改进使NSGA-II在多目标优化中更具鲁棒性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17687v2",
      "published_date": "2024-07-25 01:09:58 UTC",
      "updated_date": "2024-08-18 09:35:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:09:29.958618"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 91,
  "processed_papers_count": 91,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T10:11:06.946277"
}
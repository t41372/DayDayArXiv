[
  {
    "arxiv_id": "2404.00195v2",
    "title": "Multiple-policy Evaluation via Density Estimation",
    "authors": [
      "Yilei Chen",
      "Aldo Pacchiano",
      "Ioannis Ch. Paschalidis"
    ],
    "abstract": "We study the multiple-policy evaluation problem where we are given a set of\n$K$ policies and the goal is to evaluate their performance (expected total\nreward over a fixed horizon) to an accuracy $\\epsilon$ with probability at\nleast $1-\\delta$. We propose an algorithm named $\\mathrm{CAESAR}$ for this\nproblem. Our approach is based on computing an approximate optimal offline\nsampling distribution and using the data sampled from it to perform the\nsimultaneous estimation of the policy values. $\\mathrm{CAESAR}$ has two phases.\nIn the first we produce coarse estimates of the visitation distributions of the\ntarget policies at a low order sample complexity rate that scales with\n$\\tilde{O}(\\frac{1}{\\epsilon})$. In the second phase, we approximate the\noptimal offline sampling distribution and compute the importance weighting\nratios for all target policies by minimizing a step-wise quadratic loss\nfunction inspired by the DualDICE \\cite{nachum2019dualdice} objective. Up to\nlow order and logarithmic terms $\\mathrm{CAESAR}$ achieves a sample complexity\n$\\tilde{O}\\left(\\frac{H^4}{\\epsilon^2}\\sum_{h=1}^H\\max_{k\\in[K]}\\sum_{s,a}\\frac{(d_h^{\\pi^k}(s,a))^2}{\\mu^*_h(s,a)}\\right)$,\nwhere $d^{\\pi}$ is the visitation distribution of policy $\\pi$, $\\mu^*$ is the\noptimal sampling distribution, and $H$ is the horizon.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00195v2",
    "published_date": "2024-03-29 23:55:25 UTC",
    "updated_date": "2024-05-27 23:57:02 UTC"
  },
  {
    "arxiv_id": "2404.00188v1",
    "title": "DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries",
    "authors": [
      "Manit Mishra",
      "Abderrahman Braham",
      "Charles Marsom",
      "Bryan Chung",
      "Gavin Griffin",
      "Dakshesh Sidnerlikar",
      "Chatanya Sarin",
      "Arjun Rajaram"
    ],
    "abstract": "Conventional processes for analyzing datasets and extracting meaningful\ninformation are often time-consuming and laborious. Previous work has\nidentified manual, repetitive coding and data collection as major obstacles\nthat hinder data scientists from undertaking more nuanced labor and high-level\nprojects. To combat this, we evaluated OpenAI's GPT-3.5 as a \"Language Data\nScientist\" (LDS) that can extrapolate key findings, including correlations and\nbasic information, from a given dataset. The model was tested on a diverse set\nof benchmark datasets to evaluate its performance across multiple standards,\nincluding data science code-generation based tasks involving libraries such as\nNumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in\ncorrectly answering a given data science query related to the benchmark\ndataset. The LDS used various novel prompt engineering techniques to\neffectively answer a given question, including Chain-of-Thought reinforcement\nand SayCan prompt engineering. Our findings demonstrate great potential for\nleveraging Large Language Models for low-level, zero-shot data analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, Submitted to International Conference on AI in Cybersecurity",
    "pdf_url": "http://arxiv.org/pdf/2404.00188v1",
    "published_date": "2024-03-29 22:59:34 UTC",
    "updated_date": "2024-03-29 22:59:34 UTC"
  },
  {
    "arxiv_id": "2404.00185v2",
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ],
    "abstract": "Current Deep Neural Networks are vulnerable to adversarial examples, which\nalter their predictions by adding carefully crafted noise. Since human eyes are\nrobust to such inputs, it is possible that the vulnerability stems from the\nstandard way of processing inputs in one shot by processing every pixel with\nthe same importance. In contrast, neuroscience suggests that the human vision\nsystem can differentiate salient features by (1) switching between multiple\nfixation points (saccades) and (2) processing the surrounding with a\nnon-uniform external resolution (foveation). In this work, we advocate that the\nintegration of such active vision mechanisms into current deep learning systems\ncan offer robustness benefits. Specifically, we empirically demonstrate the\ninherent robustness of two active vision methods - GFNet and FALcon - under a\nblack box threat model. By learning and inferencing based on downsampled\nglimpses obtained from multiple distinct fixation points within an input, we\nshow that these active methods achieve (2-3) times greater robustness compared\nto a standard passive convolutional network under state-of-the-art adversarial\nattacks. More importantly, we provide illustrative and interpretable\nvisualization analysis that demonstrates how performing inference from distinct\nfixation points makes active vision methods less vulnerable to malicious\ninputs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00185v2",
    "published_date": "2024-03-29 22:51:45 UTC",
    "updated_date": "2024-04-05 16:10:44 UTC"
  },
  {
    "arxiv_id": "2404.01332v3",
    "title": "Explaining Large Language Models Decisions Using Shapley Values",
    "authors": [
      "Behnam Mohammadi"
    ],
    "abstract": "The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel's output. Through two applications - a discrete choice experiment and an\ninvestigation of cognitive biases - we demonstrate how the Shapley value method\ncan uncover what we term \"token noise\" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for practitioners and researchers\nto strategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin survey settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01332v3",
    "published_date": "2024-03-29 22:49:43 UTC",
    "updated_date": "2024-11-12 01:06:22 UTC"
  },
  {
    "arxiv_id": "2406.15360v1",
    "title": "Generative AI Adoption in Classroom in Context of Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT)",
    "authors": [
      "Aashish Ghimire",
      "John Edwards"
    ],
    "abstract": "The burgeoning development of generative artificial intelligence (GenAI) and\nthe widespread adoption of large language models (LLMs) in educational settings\nhave sparked considerable debate regarding their efficacy and\nacceptability.Despite the potential benefits, the assimilation of these\ncutting-edge technologies among educators exhibits a broad spectrum of\nattitudes, from enthusiastic advocacy to profound skepticism.This study aims to\ndissect the underlying factors influencing educators' perceptions and\nacceptance of GenAI and LLMs.We conducted a survey among educators and analyzed\nthe data through the frameworks of the Technology Acceptance Model (TAM) and\nInnovation Diffusion Theory (IDT). Our investigation reveals a strong positive\ncorrelation between the perceived usefulness of GenAI tools and their\nacceptance, underscoring the importance of demonstrating tangible benefits to\neducators. Additionally, the perceived ease of use emerged as a significant\nfactor, though to a lesser extent, influencing acceptance. Our findings also\nshow that the knowledge and acceptance of these tools is not uniform,\nsuggesting that targeted strategies are required to address the specific needs\nand concerns of each adopter category to facilitate broader integration of AI\ntools.in education.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15360v1",
    "published_date": "2024-03-29 22:41:51 UTC",
    "updated_date": "2024-03-29 22:41:51 UTC"
  },
  {
    "arxiv_id": "2404.00178v1",
    "title": "Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues",
    "authors": [
      "Ali Hassanzadeh",
      "Mojtaba Hosseini",
      "John G. Turner"
    ],
    "abstract": "Problem definition: Professional sports leagues may be suspended due to\nvarious reasons such as the recent COVID-19 pandemic. A critical question the\nleague must address when re-opening is how to appropriately select a subset of\nthe remaining games to conclude the season in a shortened time frame.\nAcademic/practical relevance: Despite the rich literature on scheduling an\nentire season starting from a blank slate, concluding an existing season is\nquite different. Our approach attempts to achieve team rankings similar to that\nwhich would have resulted had the season been played out in full. Methodology:\nWe propose a data-driven model which exploits predictive and prescriptive\nanalytics to produce a schedule for the remainder of the season comprised of a\nsubset of originally-scheduled games. Our model introduces novel rankings-based\nobjectives within a stochastic optimization model, whose parameters are first\nestimated using a predictive model. We introduce a deterministic equivalent\nreformulation along with a tailored Frank-Wolfe algorithm to efficiently solve\nour problem, as well as a robust counterpart based on min-max regret. Results:\nWe present simulation-based numerical experiments from previous National\nBasketball Association (NBA) seasons 2004--2019, and show that our models are\ncomputationally efficient, outperform a greedy benchmark that approximates a\nnon-rankings-based scheduling policy, and produce interpretable results.\nManagerial implications: Our data-driven decision-making framework may be used\nto produce a shortened season with 25-50\\% fewer games while still producing an\nend-of-season ranking similar to that of the full season, had it been played.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "90B50 (Primary) 90C06, 90C11, 90C90 (Secondary)"
    ],
    "primary_category": "math.OC",
    "comment": "32 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00178v1",
    "published_date": "2024-03-29 22:23:35 UTC",
    "updated_date": "2024-03-29 22:23:35 UTC"
  },
  {
    "arxiv_id": "2404.00172v1",
    "title": "Universal Bovine Identification via Depth Data and Deep Metric Learning",
    "authors": [
      "Asheesh Sharma",
      "Lucy Randewich",
      "William Andrew",
      "Sion Hannuna",
      "Neill Campbell",
      "Siobhan Mullan",
      "Andrew W. Dowsey",
      "Melvyn Smith",
      "Mark Hansen",
      "Tilo Burghardt"
    ],
    "abstract": "This paper proposes and evaluates, for the first time, a top-down (dorsal\nview), depth-only deep learning system for accurately identifying individual\ncattle and provides associated code, datasets, and training weights for\nimmediate reproducibility. An increase in herd size skews the cow-to-human\nratio at the farm and makes the manual monitoring of individuals more\nchallenging. Therefore, real-time cattle identification is essential for the\nfarms and a crucial step towards precision livestock farming. Underpinned by\nour previous work, this paper introduces a deep-metric learning method for\ncattle identification using depth data from an off-the-shelf 3D camera. The\nmethod relies on CNN and MLP backbones that learn well-generalised embedding\nspaces from the body shape to differentiate individuals -- requiring neither\nspecies-specific coat patterns nor close-up muzzle prints for operation. The\nnetwork embeddings are clustered using a simple algorithm such as $k$-NN for\nhighly accurate identification, thus eliminating the need to retrain the\nnetwork for enrolling new individuals. We evaluate two backbone architectures,\nResNet, as previously used to identify Holstein Friesians using RGB images, and\nPointNet, which is specialised to operate on 3D point clouds. We also present\nCowDepth2023, a new dataset containing 21,490 synchronised colour-depth image\npairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet\narchitectures, which consume depth maps and point clouds, respectively, led to\nhigh accuracy that is on par with the coat pattern-based backbone.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "LaTeX, 38 pages, 14 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.00172v1",
    "published_date": "2024-03-29 22:03:53 UTC",
    "updated_date": "2024-03-29 22:03:53 UTC"
  },
  {
    "arxiv_id": "2404.00166v2",
    "title": "Uncovering Bias in Large Vision-Language Models with Counterfactuals",
    "authors": [
      "Phillip Howard",
      "Anahita Bhiwandiwalla",
      "Kathleen C. Fraser",
      "Svetlana Kiritchenko"
    ],
    "abstract": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images.\nSpecifically, we present LVLMs with identical open-ended text prompts while\nconditioning on images from different counterfactual sets, where each set\ncontains images which are largely identical in their depiction of a common\nsubject (e.g., a doctor), but vary only in terms of intersectional social\nattributes (e.g., race and gender). We comprehensively evaluate the text\nproduced by different LVLMs under this counterfactual generation setting and\nfind that social attributes such as race, gender, and physical characteristics\ndepicted in input images can significantly influence toxicity and the\ngeneration of competency-associated words.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to the CVPR 2024 Responsible Generative AI (ReGenAI)\n  Workshop",
    "pdf_url": "http://arxiv.org/pdf/2404.00166v2",
    "published_date": "2024-03-29 21:45:53 UTC",
    "updated_date": "2024-06-07 23:29:19 UTC"
  },
  {
    "arxiv_id": "2406.09422v1",
    "title": "LooPIN: A PinFi protocol for decentralized computing",
    "authors": [
      "Yunwei Mao",
      "Qi He",
      "Ju Li"
    ],
    "abstract": "Networked computing power is a critical utility in the era of artificial\nintelligence. This paper presents a novel Physical Infrastructure Finance\n(PinFi) protocol designed to facilitate the distribution of computing power\nwithin networks in a decentralized manner. Addressing the core challenges of\ncoordination, pricing, and liquidity in decentralized physical infrastructure\nnetworks (DePIN), the PinFi protocol introduces a distinctive dynamic pricing\nmechanism. It enables providers to allocate excess computing resources to a\n\"dissipative\" PinFi liquidity pool, distinct from traditional DeFi liquidity\npools, ensuring seamless access for clients at equitable, market-based prices.\nThis approach significantly reduces the costs of accessing computing power,\npotentially to as low as 1% compared to existing services, while simultaneously\nenhancing security and dependability. The PinFi protocol is poised to transform\nthe dynamics of supply and demand in computing power networks, setting a new\nstandard for efficiency and accessibility.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CE",
      "cs.CR"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.09422v1",
    "published_date": "2024-03-29 21:37:59 UTC",
    "updated_date": "2024-03-29 21:37:59 UTC"
  },
  {
    "arxiv_id": "2404.01331v2",
    "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model",
    "authors": [
      "Musashi Hinck",
      "Matthew L. Olson",
      "David Cobbley",
      "Shao-Yen Tseng",
      "Vasudev Lal"
    ],
    "abstract": "We train a suite of multimodal foundation models (MMFM) using the popular\nLLaVA framework with the recently released Gemma family of large language\nmodels (LLMs). Of particular interest is the 2B parameter Gemma model, which\nprovides opportunities to construct capable small-scale MMFMs. In line with\nfindings from other papers in this space, we test the effect of ablating three\ndesign features: pretraining the connector, utilizing a more powerful image\nbackbone, and increasing the size of the language backbone. The resulting\nmodels, which we call LLaVA-Gemma, exhibit moderate performance on an array of\nevaluations, but fail to improve past the current comparably sized SOTA models.\nCloser analysis of performance shows mixed effects; skipping pretraining tends\nto reduce performance, larger vision models sometimes improve performance, and\nincreasing language model size has inconsistent effects. We publicly release\ntraining recipes, code and weights for our models for the LLaVA-Gemma models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "CVPR 2024, MMFM workshop. Authors 1 and 2 contributed equally. Models\n  available at https://huggingface.co/intel/llava-gemma-2b/ and\n  https://huggingface.co/intel/llava-gemma-7b/ Training code at\n  https://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/LLaVA-Gemma",
    "pdf_url": "http://arxiv.org/pdf/2404.01331v2",
    "published_date": "2024-03-29 21:32:50 UTC",
    "updated_date": "2024-06-10 20:59:48 UTC"
  },
  {
    "arxiv_id": "2404.00143v1",
    "title": "Accelerating Search-Based Planning for Multi-Robot Manipulation by Leveraging Online-Generated Experiences",
    "authors": [
      "Yorai Shaoul",
      "Itamar Mishani",
      "Maxim Likhachev",
      "Jiaoyang Li"
    ],
    "abstract": "An exciting frontier in robotic manipulation is the use of multiple arms at\nonce. However, planning concurrent motions is a challenging task using current\nmethods. The high-dimensional composite state space renders many well-known\nmotion planning algorithms intractable. Recently, Multi-Agent Path-Finding\n(MAPF) algorithms have shown promise in discrete 2D domains, providing rigorous\nguarantees. However, widely used conflict-based methods in MAPF assume an\nefficient single-agent motion planner. This poses challenges in adapting them\nto manipulation cases where this assumption does not hold, due to the high\ndimensionality of configuration spaces and the computational bottlenecks\nassociated with collision checking. To this end, we propose an approach for\naccelerating conflict-based search algorithms by leveraging their repetitive\nand incremental nature -- making them tractable for use in complex scenarios\ninvolving multi-arm coordination in obstacle-laden environments. We show that\nour method preserves completeness and bounded sub-optimality guarantees, and\ndemonstrate its practical efficacy through a set of experiments with up to 10\nrobotic arms.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "The first two authors contributed equally. Accepted to ICAPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00143v1",
    "published_date": "2024-03-29 20:31:07 UTC",
    "updated_date": "2024-03-29 20:31:07 UTC"
  },
  {
    "arxiv_id": "2404.00140v1",
    "title": "Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks",
    "authors": [
      "Xiaolei Lu",
      "Jianghong Ma"
    ],
    "abstract": "Explainability algorithms aimed at interpreting decision-making AI systems\nusually consider balancing two critical dimensions: 1) \\textit{faithfulness},\nwhere explanations accurately reflect the model's inference process. 2)\n\\textit{plausibility}, where explanations are consistent with domain experts.\nHowever, the question arises: do faithfulness and plausibility inherently\nconflict? In this study, through a comprehensive quantitative comparison\nbetween the explanations from the selected explainability methods and\nexpert-level interpretations across three NLP tasks: sentiment analysis, intent\ndetection, and topic labeling, we demonstrate that traditional\nperturbation-based methods Shapley value and LIME could attain greater\nfaithfulness and plausibility. Our findings suggest that rather than optimizing\nfor one dimension at the expense of the other, we could seek to optimize\nexplainability algorithms with dual objectives to achieve high levels of\naccuracy and user accessibility in their explanations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00140v1",
    "published_date": "2024-03-29 20:28:42 UTC",
    "updated_date": "2024-03-29 20:28:42 UTC"
  },
  {
    "arxiv_id": "2404.00139v1",
    "title": "Security Risks Concerns of Generative AI in the IoT",
    "authors": [
      "Honghui Xu",
      "Yingshu Li",
      "Olusesi Balogun",
      "Shaoen Wu",
      "Yue Wang",
      "Zhipeng Cai"
    ],
    "abstract": "In an era where the Internet of Things (IoT) intersects increasingly with\ngenerative Artificial Intelligence (AI), this article scrutinizes the emergent\nsecurity risks inherent in this integration. We explore how generative AI\ndrives innovation in IoT and we analyze the potential for data breaches when\nusing generative AI and the misuse of generative AI technologies in IoT\necosystems. These risks not only threaten the privacy and efficiency of IoT\nsystems but also pose broader implications for trust and safety in AI-driven\nenvironments. The discussion in this article extends to strategic approaches\nfor mitigating these risks, including the development of robust security\nprotocols, the multi-layered security approaches, and the adoption of AI\ntechnological solutions. Through a comprehensive analysis, this article aims to\nshed light on the critical balance between embracing AI advancements and\nensuring stringent security in IoT, providing insights into the future\ndirection of these intertwined technologies.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.00139v1",
    "published_date": "2024-03-29 20:28:30 UTC",
    "updated_date": "2024-03-29 20:28:30 UTC"
  },
  {
    "arxiv_id": "2404.00137v1",
    "title": "Budget-aware Query Tuning: An AutoML Perspective",
    "authors": [
      "Wentao Wu",
      "Chi Wang"
    ],
    "abstract": "Modern database systems rely on cost-based query optimizers to come up with\ngood execution plans for input queries. Such query optimizers rely on cost\nmodels to estimate the costs of candidate query execution plans. A cost model\nrepresents a function from a set of cost units to query execution cost, where\neach cost unit specifies the unit cost of executing a certain type of query\nprocessing operation (such as table scan or join). These cost units are\ntraditionally viewed as constants, whose values only depend on the platform\nconfiguration where the database system runs on top of but are invariant for\nqueries processed by the database system. In this paper, we challenge this\nclassic view by thinking of these cost units as variables instead. We show\nthat, by varying the cost-unit values one can obtain query plans that\nsignificantly outperform the default query plans returned by the query\noptimizer when viewing the cost units as constants. We term this cost-unit\ntuning process \"query tuning\" (QT) and show that it is similar to the\nwell-known hyper-parameter optimization (HPO) problem in AutoML. As a result,\nany state-of-the-art HPO technologies can be applied to QT. We study the QT\nproblem in the context of anytime tuning, which is desirable in practice by\nconstraining the total time spent on QT within a given budget -- we call this\nproblem budget-aware query tuning. We further extend our study from tuning a\nsingle query to tuning a workload with multiple queries, and we call this\ngeneralized problem budget-aware workload tuning (WT), which aims for\nminimizing the execution time of the entire workload. WT is more challenging as\none needs to further prioritize individual query tuning within the given time\nbudget. We propose solutions to both QT and WT and experimental evaluation\nusing both benchmark and real workloads demonstrates the efficacy of our\nproposed solutions.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00137v1",
    "published_date": "2024-03-29 20:19:36 UTC",
    "updated_date": "2024-03-29 20:19:36 UTC"
  },
  {
    "arxiv_id": "2404.15307v1",
    "title": "DCAE-SR: Design of a Denoising Convolutional Autoencoder for reconstructing Electrocardiograms signals at Super Resolution",
    "authors": [
      "Ugo Lomoio",
      "Pierangelo Veltri",
      "Pietro Hiram Guzzi",
      "Pietro Lio'"
    ],
    "abstract": "Electrocardiogram (ECG) signals play a pivotal role in cardiovascular\ndiagnostics, providing essential information on the electrical activity of the\nheart. However, the inherent noise and limited resolution in ECG recordings can\nhinder accurate interpretation and diagnosis. In this paper, we propose a novel\nmodel for ECG super resolution (SR) that uses a DNAE to enhance temporal and\nfrequency information inside ECG signals. Our approach addresses the\nlimitations of traditional ECG signal processing techniques. Our model takes in\ninput 5-second length ECG windows sampled at 50 Hz (very low resolution) and it\nis able to reconstruct a denoised super-resolution signal with an x10\nupsampling rate (sampled at 500 Hz). We trained the proposed DCAE-SR on public\navailable myocardial infraction ECG signals. Our method demonstrates superior\nperformance in reconstructing high-resolution ECG signals from very\nlow-resolution signals with a sampling rate of 50 Hz. We compared our results\nwith the current deep-learning literature approaches for ECG super-resolution\nand some non-deep learning reproducible methods that can perform both\nsuper-resolution and denoising. We obtained current state-of-the-art\nperformances in super-resolution of very low resolution ECG signals frequently\ncorrupted by ECG artifacts. We were able to obtain a signal-to-noise ratio of\n12.20 dB (outperforms previous 4.68 dB), mean squared error of 0.0044\n(outperforms previous 0.0154) and root mean squared error of 4.86% (outperforms\nprevious 12.40%). In conclusion, our DCAE-SR model offers a robust (to artefact\npresence), versatile and explainable solution to enhance the quality of ECG\nsignals. This advancement holds promise in advancing the field of\ncardiovascular diagnostics, paving the way for improved patient care and\nhigh-quality clinical decisions",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.15307v1",
    "published_date": "2024-03-29 19:46:08 UTC",
    "updated_date": "2024-03-29 19:46:08 UTC"
  },
  {
    "arxiv_id": "2404.00107v1",
    "title": "Robust Ensemble Person Re-Identification via Orthogonal Fusion with Occlusion Handling",
    "authors": [
      "Syeda Nyma Ferdous",
      "Xin Li"
    ],
    "abstract": "Occlusion remains one of the major challenges in person reidentification\n(ReID) as a result of the diversity of poses and the variation of appearances.\nDeveloping novel architectures to improve the robustness of occlusion-aware\nperson Re-ID requires new insights, especially on low-resolution edge cameras.\nWe propose a deep ensemble model that harnesses both CNN and Transformer\narchitectures to generate robust feature representations. To achieve robust\nRe-ID without the need to manually label occluded regions, we propose to take\nan ensemble learning-based approach derived from the analogy between\narbitrarily shaped occluded regions and robust feature representation. Using\nthe orthogonality principle, our developed deep CNN model makes use of masked\nautoencoder (MAE) and global-local feature fusion for robust person\nidentification. Furthermore, we present a part occlusion-aware transformer\ncapable of learning feature space that is robust to occluded regions.\nExperimental results are reported on several Re-ID datasets to show the\neffectiveness of our developed ensemble model named orthogonal fusion with\nocclusion handling (OFOH). Compared to competing methods, the proposed OFOH\napproach has achieved competent rank-1 and mAP performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00107v1",
    "published_date": "2024-03-29 18:38:59 UTC",
    "updated_date": "2024-03-29 18:38:59 UTC"
  },
  {
    "arxiv_id": "2406.16873v1",
    "title": "A Survey of Machine Learning Techniques for Improving Global Navigation Satellite Systems",
    "authors": [
      "Adyasha Mohanty",
      "Grace Gao"
    ],
    "abstract": "Global Navigation Satellite Systems (GNSS)-based positioning plays a crucial\nrole in various applications, including navigation, transportation, logistics,\nmapping, and emergency services. Traditional GNSS positioning methods are\nmodel-based and they utilize satellite geometry and the known properties of\nsatellite signals. However, model-based methods have limitations in challenging\nenvironments and often lack adaptability to uncertain noise models. This paper\nhighlights recent advances in Machine Learning (ML) and its potential to\naddress these limitations. It covers a broad range of ML methods, including\nsupervised learning, unsupervised learning, deep learning, and hybrid\napproaches. The survey provides insights into positioning applications related\nto GNSS such as signal analysis, anomaly detection, multi-sensor integration,\nprediction, and accuracy enhancement using ML. It discusses the strengths,\nlimitations, and challenges of current ML-based approaches for GNSS\npositioning, providing a comprehensive overview of the field.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "eess.SP",
    "comment": "Under consideration for EURASIP Journal on Advances in Signal\n  Processing",
    "pdf_url": "http://arxiv.org/pdf/2406.16873v1",
    "published_date": "2024-03-29 18:31:50 UTC",
    "updated_date": "2024-03-29 18:31:50 UTC"
  },
  {
    "arxiv_id": "2404.00099v2",
    "title": "Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes",
    "authors": [
      "Andrew Bennett",
      "Nathan Kallus",
      "Miruna Oprescu",
      "Wen Sun",
      "Kaiwen Wang"
    ],
    "abstract": "We study the evaluation of a policy under best- and worst-case perturbations\nto a Markov decision process (MDP), using transition observations from the\noriginal MDP, whether they are generated under the same or a different policy.\nThis is an important problem when there is the possibility of a shift between\nhistorical and future environments, $\\textit{e.g.}$ due to unmeasured\nconfounding, distributional shift, or an adversarial environment. We propose a\nperturbation model that allows changes in the transition kernel densities up to\na given multiplicative factor or its reciprocal, extending the classic marginal\nsensitivity model (MSM) for single time-step decision-making to\ninfinite-horizon RL. We characterize the sharp bounds on policy value under\nthis model $\\unicode{x2013}$ $\\textit{i.e.}$, the tightest possible bounds\nbased on transition observations from the original MDP $\\unicode{x2013}$ and we\nstudy the estimation of these bounds from such transition observations. We\ndevelop an estimator with several important guarantees: it is\nsemiparametrically efficient, and remains so even when certain necessary\nnuisance functions, such as worst-case Q-functions, are estimated at slow,\nnonparametric rates. Our estimator is also asymptotically normal, enabling\nstraightforward statistical inference using Wald confidence intervals.\nMoreover, when certain nuisances are estimated inconsistently, the estimator\nstill provides valid, albeit possibly not sharp, bounds on the policy value. We\nvalidate these properties in numerical simulations. The combination of\naccounting for environment shifts from train to test (robustness), being\ninsensitive to nuisance-function estimation (orthogonality), and addressing the\nchallenge of learning from finite samples (inference) together leads to\ncredible and reliable policy evaluation.",
    "categories": [
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "39 pages, 2 figures, NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00099v2",
    "published_date": "2024-03-29 18:11:49 UTC",
    "updated_date": "2024-11-01 19:00:50 UTC"
  },
  {
    "arxiv_id": "2403.20331v3",
    "title": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models",
    "authors": [
      "Atsuyuki Miyai",
      "Jingkang Yang",
      "Jingyang Zhang",
      "Yifei Ming",
      "Qing Yu",
      "Go Irie",
      "Yixuan Li",
      "Hai Li",
      "Ziwei Liu",
      "Kiyoharu Aizawa"
    ],
    "abstract": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/AtsuMiyai/UPD. Update from v2: Correction to\n  Figure 1",
    "pdf_url": "http://arxiv.org/pdf/2403.20331v3",
    "published_date": "2024-03-29 17:59:53 UTC",
    "updated_date": "2025-04-27 19:47:13 UTC"
  },
  {
    "arxiv_id": "2403.20329v2",
    "title": "ReALM: Reference Resolution As Language Modeling",
    "authors": [
      "Joel Ruben Antony Moniz",
      "Soundarya Krishnan",
      "Melis Ozyildirim",
      "Prathamesh Saraf",
      "Halim Cagri Ates",
      "Yuan Zhang",
      "Hong Yu"
    ],
    "abstract": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at SIGDIAL 2024 (Oral presentation)",
    "pdf_url": "http://arxiv.org/pdf/2403.20329v2",
    "published_date": "2024-03-29 17:59:06 UTC",
    "updated_date": "2024-08-19 03:06:24 UTC"
  },
  {
    "arxiv_id": "2403.20327v1",
    "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
    "authors": [
      "Jinhyuk Lee",
      "Zhuyun Dai",
      "Xiaoqi Ren",
      "Blair Chen",
      "Daniel Cer",
      "Jeremy R. Cole",
      "Kai Hui",
      "Michael Boratko",
      "Rajvi Kapadia",
      "Wen Ding",
      "Yi Luan",
      "Sai Meher Karthik Duddu",
      "Gustavo Hernandez Abrego",
      "Weiqiang Shi",
      "Nithi Gupta",
      "Aditya Kusupati",
      "Prateek Jain",
      "Siddhartha Reddy Jonnalagadda",
      "Ming-Wei Chang",
      "Iftekhar Naim"
    ],
    "abstract": "We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.20327v1",
    "published_date": "2024-03-29 17:56:40 UTC",
    "updated_date": "2024-03-29 17:56:40 UTC"
  },
  {
    "arxiv_id": "2403.20320v1",
    "title": "MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning",
    "authors": [
      "Ahmed Agiza",
      "Marina Neseem",
      "Sherief Reda"
    ],
    "abstract": "Adapting models pre-trained on large-scale datasets to a variety of\ndownstream tasks is a common strategy in deep learning. Consequently,\nparameter-efficient fine-tuning methods have emerged as a promising way to\nadapt pre-trained models to different tasks while training only a minimal\nnumber of parameters. While most of these methods are designed for single-task\nadaptation, parameter-efficient training in Multi-Task Learning (MTL)\narchitectures is still unexplored. In this paper, we introduce MTLoRA, a novel\nframework for parameter-efficient training of MTL models. MTLoRA employs\nTask-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively\ndisentangle the parameter space in MTL fine-tuning, thereby enabling the model\nto adeptly handle both task specialization and interaction within MTL contexts.\nWe applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting\nthem to multiple downstream dense prediction tasks. Our extensive experiments\non the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream\ntasks compared to fully fine-tuning the MTL model while reducing the number of\ntrainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal\ntrade-off between the number of trainable parameters and the accuracy of the\ndownstream tasks, outperforming current state-of-the-art parameter-efficient\ntraining methods in both accuracy and efficiency. Our code is publicly\navailable.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.20320v1",
    "published_date": "2024-03-29 17:43:58 UTC",
    "updated_date": "2024-03-29 17:43:58 UTC"
  },
  {
    "arxiv_id": "2403.20318v1",
    "title": "SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular 3D Detection of Large Objects",
    "authors": [
      "Abhinav Kumar",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren",
      "Xiaoming Liu"
    ],
    "abstract": "Monocular 3D detectors achieve remarkable performance on cars and smaller\nobjects. However, their performance drops on larger objects, leading to fatal\naccidents. Some attribute the failures to training data scarcity or their\nreceptive field requirements of large objects. In this paper, we highlight this\nunderstudied problem of generalization to large objects. We find that modern\nfrontal detectors struggle to generalize to large objects even on nearly\nbalanced datasets. We argue that the cause of failure is the sensitivity of\ndepth regression losses to noise of larger objects. To bridge this gap, we\ncomprehensively investigate regression and dice losses, examining their\nrobustness under varying error levels and object sizes. We mathematically prove\nthat the dice loss leads to superior noise-robustness and model convergence for\nlarge objects compared to regression losses for a simplified case. Leveraging\nour theoretical insights, we propose SeaBird (Segmentation in Bird's View) as\nthe first step towards generalizing to large objects. SeaBird effectively\nintegrates BEV segmentation on foreground objects for 3D detection, with the\nsegmentation head trained with the dice loss. SeaBird achieves SoTA results on\nthe KITTI-360 leaderboard and improves existing detectors on the nuScenes\nleaderboard, particularly for large objects. Code and models at\nhttps://github.com/abhi1kumar/SeaBird",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.20318v1",
    "published_date": "2024-03-29 17:41:57 UTC",
    "updated_date": "2024-03-29 17:41:57 UTC"
  },
  {
    "arxiv_id": "2403.20308v1",
    "title": "ChainNet: Structured Metaphor and Metonymy in WordNet",
    "authors": [
      "Rowan Hall Maudslay",
      "Simone Teufel",
      "Francis Bond",
      "James Pustejovsky"
    ],
    "abstract": "The senses of a word exhibit rich internal structure. In a typical lexicon,\nthis structure is overlooked: a word's senses are encoded as a list without\ninter-sense relations. We present ChainNet, a lexical resource which for the\nfirst time explicitly identifies these structures. ChainNet expresses how\nsenses in the Open English Wordnet are derived from one another: every nominal\nsense of a word is either connected to another sense by metaphor or metonymy,\nor is disconnected in the case of homonymy. Because WordNet senses are linked\nto resources which capture information about their meaning, ChainNet represents\nthe first dataset of grounded metaphor and metonymy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20308v1",
    "published_date": "2024-03-29 17:22:53 UTC",
    "updated_date": "2024-03-29 17:22:53 UTC"
  },
  {
    "arxiv_id": "2403.20306v1",
    "title": "Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference",
    "authors": [
      "Jovan Stojkovic",
      "Esha Choukse",
      "Chaojie Zhang",
      "Inigo Goiri",
      "Josep Torrellas"
    ],
    "abstract": "With the ubiquitous use of modern large language models (LLMs) across\nindustries, the inference serving for these models is ever expanding. Given the\nhigh compute and memory requirements of modern LLMs, more and more\ntop-of-the-line GPUs are being deployed to serve these models. Energy\navailability has come to the forefront as the biggest challenge for data center\nexpansion to serve these models. In this paper, we present the trade-offs\nbrought up by making energy efficiency the primary goal of LLM serving under\nperformance SLOs. We show that depending on the inputs, the model, and the\nservice-level agreements, there are several knobs available to the LLM\ninference provider to use for being energy efficient. We characterize the\nimpact of these knobs on the latency, throughput, as well as the energy. By\nexploring these trade-offs, we offer valuable insights into optimizing energy\nusage without compromising on performance, thereby paving the way for\nsustainable and cost-effective LLM deployment in data center environments.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.DC",
      "C.0; I.2"
    ],
    "primary_category": "cs.AI",
    "comment": "6 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.20306v1",
    "published_date": "2024-03-29 17:22:48 UTC",
    "updated_date": "2024-03-29 17:22:48 UTC"
  },
  {
    "arxiv_id": "2403.20300v1",
    "title": "Improving Learnt Local MAPF Policies with Heuristic Search",
    "authors": [
      "Rishi Veerapaneni",
      "Qian Wang",
      "Kevin Ren",
      "Arthur Jakobsson",
      "Jiaoyang Li",
      "Maxim Likhachev"
    ],
    "abstract": "Multi-agent path finding (MAPF) is the problem of finding collision-free\npaths for a team of agents to reach their goal locations. State-of-the-art\nclassical MAPF solvers typically employ heuristic search to find solutions for\nhundreds of agents but are typically centralized and can struggle to scale when\nrun with short timeouts. Machine learning (ML) approaches that learn policies\nfor each agent are appealing as these could enable decentralized systems and\nscale well while maintaining good solution quality. Current ML approaches to\nMAPF have proposed methods that have started to scratch the surface of this\npotential. However, state-of-the-art ML approaches produce \"local\" policies\nthat only plan for a single timestep and have poor success rates and\nscalability. Our main idea is that we can improve a ML local policy by using\nheuristic search methods on the output probability distribution to resolve\ndeadlocks and enable full horizon planning. We show several model-agnostic ways\nto use heuristic search with learnt policies that significantly improve the\npolicies' success rates and scalability. To our best knowledge, we demonstrate\nthe first time ML-based MAPF approaches have scaled to high congestion\nscenarios (e.g. 20% agent density).",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "Accepted in ICAPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.20300v1",
    "published_date": "2024-03-29 17:16:20 UTC",
    "updated_date": "2024-03-29 17:16:20 UTC"
  },
  {
    "arxiv_id": "2403.20288v2",
    "title": "Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",
    "authors": [
      "Burcu Sayin",
      "Pasquale Minervini",
      "Jacopo Staiano",
      "Andrea Passerini"
    ],
    "abstract": "We explore the potential of Large Language Models (LLMs) to assist and\npotentially correct physicians in medical decision-making tasks. We evaluate\nseveral LLMs, including Meditron, Llama2, and Mistral, to analyze the ability\nof these models to interact effectively with physicians across different\nscenarios. We consider questions from PubMedQA and several tasks, ranging from\nbinary (yes/no) responses to long answer generation, where the answer of the\nmodel is produced after an interaction with a physician. Our findings suggest\nthat prompt design significantly influences the downstream accuracy of LLMs and\nthat LLMs can provide valuable feedback to physicians, challenging incorrect\ndiagnoses and contributing to more accurate decision-making. For example, when\nthe physician is accurate 38% of the time, Mistral can produce the correct\nanswer, improving accuracy up to 74% depending on the prompt being used, while\nLlama2 and Meditron models exhibit greater sensitivity to prompt choice. Our\nanalysis also uncovers the challenges of ensuring that LLM-generated\nsuggestions are pertinent and useful, emphasizing the need for further research\nin this area.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for oral presentation at NAACL 2024, The 6th Clinical\n  Natural Language Processing Workshop",
    "pdf_url": "http://arxiv.org/pdf/2403.20288v2",
    "published_date": "2024-03-29 16:59:13 UTC",
    "updated_date": "2024-05-06 14:13:51 UTC"
  },
  {
    "arxiv_id": "2403.20280v2",
    "title": "Sparsely Multimodal Data Fusion",
    "authors": [
      "Josiah Bjorgaard"
    ],
    "abstract": "Multimodal data fusion is essential for applications requiring the\nintegration of diverse data sources, especially in the presence of incomplete\nor sparsely available modalities. This paper presents a comparative study of\nthree multimodal embedding techniques, Modal Channel Attention (MCA), Zorro,\nand Everything at Once (EAO), to evaluate their performance on sparsely\nmultimodal data. MCA introduces fusion embeddings for all combinations of input\nmodalities and uses attention masking to create distinct attention channels,\nenabling flexible and efficient data fusion. Experiments on two datasets with\nfour modalities each, CMU-MOSEI and TCGA, demonstrate that MCA outperforms\nZorro across ranking, recall, regression, and classification tasks and\noutperforms EAO across regression and classification tasks. MCA achieves\nsuperior performance by maintaining robust uniformity across unimodal and\nfusion embeddings. While EAO performs best in ranking metrics due to its\napproach of forming fusion embeddings post-inference, it underperforms in\ndownstream tasks requiring multimodal interactions. These results highlight the\nimportance of contrasting all modality combinations in constructing embedding\nspaces and offers insights into the design of multimodal architectures for\nreal-world applications with incomplete data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20280v2",
    "published_date": "2024-03-29 16:49:40 UTC",
    "updated_date": "2025-01-02 18:31:25 UTC"
  },
  {
    "arxiv_id": "2403.20266v2",
    "title": "Latxa: An Open Language Model and Evaluation Suite for Basque",
    "authors": [
      "Julen Etxaniz",
      "Oscar Sainz",
      "Naiara Perez",
      "Itziar Aldabe",
      "German Rigau",
      "Eneko Agirre",
      "Aitor Ormazabal",
      "Mikel Artetxe",
      "Aitor Soroa"
    ],
    "abstract": "We introduce Latxa, a family of large language models for Basque ranging from\n7 to 70 billion parameters. Latxa is based on Llama 2, which we continue\npretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.\nAddressing the scarcity of high-quality benchmarks for Basque, we further\nintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising\n5,169 questions from official language proficiency exams; EusReading,\ncomprising 352 reading comprehension questions; EusTrivia, comprising 1,715\ntrivia questions from 5 knowledge areas; and EusExams, comprising 16,774\nquestions from public examinations. In our extensive evaluation, Latxa\noutperforms all previous open models we compare to by a large margin. In\naddition, it is competitive with GPT-4 Turbo in language proficiency and\nunderstanding, despite lagging behind in reading comprehension and\nknowledge-intensive tasks. Both the Latxa family of models, as well as our new\npretraining corpora and evaluation datasets, are publicly available under open\nlicenses. Our suite enables reproducible research on methods to build LLMs for\nlow-resource languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.20266v2",
    "published_date": "2024-03-29 16:16:48 UTC",
    "updated_date": "2024-09-20 11:52:52 UTC"
  },
  {
    "arxiv_id": "2403.20262v3",
    "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
    "authors": [
      "Thibaut Thonet",
      "Jos Rozen",
      "Laurent Besacier"
    ],
    "abstract": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending the models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, we propose a new\nbenchmark for long-context LLMs focused on a practical meeting assistant\nscenario in which the long contexts consist of transcripts obtained by\nautomatic speech recognition, presenting unique challenges for LLMs due to the\ninherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,\naugments the existing ELITR corpus by adding 271 manually crafted questions\nwith their ground-truth answers, as well as noisy versions of meeting\ntranscripts altered to target different Word Error Rate levels. Our experiments\nwith 12 long-context LLMs on ELITR-Bench confirm the progress made across\nsuccessive generations of both proprietary and open models, and point out their\ndiscrepancies in terms of robustness to transcript noise. We also provide a\nthorough analysis of our GPT-4-based evaluation, including insights from a\ncrowdsourcing study. Our findings indicate that while GPT-4's scores align with\nhuman judges, its ability to distinguish beyond three score levels may be\nlimited.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.20262v3",
    "published_date": "2024-03-29 16:13:31 UTC",
    "updated_date": "2025-01-17 09:32:54 UTC"
  },
  {
    "arxiv_id": "2403.20261v4",
    "title": "FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation",
    "authors": [
      "Kaiyuan Gao",
      "Qizhi Pei",
      "Gongbo Zhang",
      "Jinhua Zhu",
      "Kun He",
      "Lijun Wu"
    ],
    "abstract": "Molecular docking is a pivotal process in drug discovery. While traditional\ntechniques rely on extensive sampling and simulation governed by physical\nprinciples, these methods are often slow and costly. The advent of deep\nlearning-based approaches has shown significant promise, offering increases in\nboth accuracy and efficiency. Building upon the foundational work of FABind, a\nmodel designed with a focus on speed and accuracy, we present FABind+, an\nenhanced iteration that largely boosts the performance of its predecessor. We\nidentify pocket prediction as a critical bottleneck in molecular docking and\npropose a novel methodology that significantly refines pocket prediction,\nthereby streamlining the docking process. Furthermore, we introduce\nmodifications to the docking module to enhance its pose generation\ncapabilities. In an effort to bridge the gap with conventional\nsampling/generative methods, we incorporate a simple yet effective sampling\ntechnique coupled with a confidence model, requiring only minor adjustments to\nthe regression framework of FABind. Experimental results and analysis reveal\nthat FABind+ remarkably outperforms the original FABind, achieves competitive\nstate-of-the-art performance, and delivers insightful modeling strategies. This\ndemonstrates FABind+ represents a substantial step forward in molecular docking\nand drug discovery. Our code is in https://github.com/QizhiPei/FABind.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Accepted for presentation at KDD 2025",
    "pdf_url": "http://arxiv.org/pdf/2403.20261v4",
    "published_date": "2024-03-29 16:10:34 UTC",
    "updated_date": "2025-02-24 15:39:15 UTC"
  },
  {
    "arxiv_id": "2403.20252v1",
    "title": "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
    "authors": [
      "Keiichi Namikoshi",
      "Alex Filipowicz",
      "David A. Shamma",
      "Rumen Iliev",
      "Candice L. Hogan",
      "Nikos Arechiga"
    ],
    "abstract": "We consider the problem of aligning a large language model (LLM) to model the\npreferences of a human population. Modeling the beliefs, preferences, and\nbehaviors of a specific population can be useful for a variety of different\napplications, such as conducting simulated focus groups for new products,\nconducting virtual surveys, and testing behavioral interventions, especially\nfor interventions that are expensive, impractical, or unethical. Existing work\nhas had mixed success using LLMs to accurately model human behavior in\ndifferent contexts. We benchmark and evaluate two well-known fine-tuning\napproaches and evaluate the resulting populations on their ability to match the\npreferences of real human respondents on a survey of preferences for battery\nelectric vehicles (BEVs). We evaluate our models against their ability to match\npopulation-wide statistics as well as their ability to match individual\nresponses, and we investigate the role of temperature in controlling the\ntrade-offs between these two. Additionally, we propose and evaluate a novel\nloss term to improve model performance on responses that require a numeric\nresponse.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20252v1",
    "published_date": "2024-03-29 15:58:46 UTC",
    "updated_date": "2024-03-29 15:58:46 UTC"
  },
  {
    "arxiv_id": "2403.20250v1",
    "title": "Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures",
    "authors": [
      "Giovanni Cerulli"
    ],
    "abstract": "This paper deals with optimal policy learning (OPL) with observational data,\ni.e. data-driven optimal decision-making, in multi-action (or multi-arm)\nsettings, where a finite set of decision options is available. It is organized\nin three parts, where I discuss respectively: estimation, risk preference, and\npotential failures. The first part provides a brief review of the key\napproaches to estimating the reward (or value) function and optimal policy\nwithin this context of analysis. Here, I delineate the identification\nassumptions and statistical properties related to offline optimal policy\nlearning estimators. In the second part, I delve into the analysis of decision\nrisk. This analysis reveals that the optimal choice can be influenced by the\ndecision maker's attitude towards risks, specifically in terms of the trade-off\nbetween reward conditional mean and conditional variance. Here, I present an\napplication of the proposed model to real data, illustrating that the average\nregret of a policy with multi-valued treatment is contingent on the\ndecision-maker's attitude towards risk. The third part of the paper discusses\nthe limitations of optimal data-driven decision-making by highlighting\nconditions under which decision-making can falter. This aspect is linked to the\nfailure of the two fundamental assumptions essential for identifying the\noptimal choice: (i) overlapping, and (ii) unconfoundedness. Some conclusions\nend the paper.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20250v1",
    "published_date": "2024-03-29 15:55:06 UTC",
    "updated_date": "2024-03-29 15:55:06 UTC"
  },
  {
    "arxiv_id": "2403.20234v2",
    "title": "Artificial Neural Networks-based Real-time Classification of ENG Signals for Implanted Nerve Interfaces",
    "authors": [
      "Antonio Coviello",
      "Francesco Linsalata",
      "Umberto Spagnolini",
      "Maurizio Magarini"
    ],
    "abstract": "Neuropathies are gaining higher relevance in clinical settings, as they risk\npermanently jeopardizing a person's life. To support the recovery of patients,\nthe use of fully implanted devices is emerging as one of the most promising\nsolutions. However, these devices, even if becoming an integral part of a fully\ncomplex neural nanonetwork system, pose numerous challenges. In this article,\nwe address one of them, which consists of the classification of motor/sensory\nstimuli. The task is performed by exploring four different types of artificial\nneural networks (ANNs) to extract various sensory stimuli from the\nelectroneurographic (ENG) signal measured in the sciatic nerve of rats.\nDifferent sizes of the data sets are considered to analyze the feasibility of\nthe investigated ANNs for real-time classification through a comparison of\ntheir performance in terms of accuracy, F1-score, and prediction time. The\ndesign of the ANNs takes advantage of the modelling of the ENG signal as a\nmultiple-input multiple-output (MIMO) system to describe the measures taken by\nstate-of-the-art implanted nerve interfaces. These are based on the use of\nmulti-contact cuff electrodes to achieve nanoscale spatial discrimination of\nthe nerve activity. The MIMO ENG signal model is another contribution of this\npaper. Our results show that some ANNs are more suitable for real-time\napplications, being capable of achieving accuracies over $90\\%$ for signal\nwindows of $100$ and $200\\,$ms with a low enough processing time to be\neffective for pathology recovery.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20234v2",
    "published_date": "2024-03-29 15:23:30 UTC",
    "updated_date": "2024-04-02 09:26:43 UTC"
  },
  {
    "arxiv_id": "2403.20221v1",
    "title": "Graph Neural Aggregation-diffusion with Metastability",
    "authors": [
      "Kaiyuan Cui",
      "Xinyan Wang",
      "Zicheng Zhang",
      "Weichen Zhao"
    ],
    "abstract": "Continuous graph neural models based on differential equations have expanded\nthe architecture of graph neural networks (GNNs). Due to the connection between\ngraph diffusion and message passing, diffusion-based models have been widely\nstudied. However, diffusion naturally drives the system towards an equilibrium\nstate, leading to issues like over-smoothing. To this end, we propose GRADE\ninspired by graph aggregation-diffusion equations, which includes the delicate\nbalance between nonlinear diffusion and aggregation induced by interaction\npotentials. The node representations obtained through aggregation-diffusion\nequations exhibit metastability, indicating that features can aggregate into\nmultiple clusters. In addition, the dynamics within these clusters can persist\nfor long time periods, offering the potential to alleviate over-smoothing\neffects. This nonlinear diffusion in our model generalizes existing\ndiffusion-based models and establishes a connection with classical GNNs. We\nprove that GRADE achieves competitive performance across various benchmarks and\nalleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet\nenergy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.20221v1",
    "published_date": "2024-03-29 15:05:57 UTC",
    "updated_date": "2024-03-29 15:05:57 UTC"
  },
  {
    "arxiv_id": "2403.20216v4",
    "title": "Distributed agency in second language learning and teaching through generative AI",
    "authors": [
      "Robert Godwin-Jones"
    ],
    "abstract": "Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "26 pages. Published in Language Learning & Technology, volume 28,\n  issue 2, pp. 5-31: http://doi.org/10125/73570",
    "pdf_url": "http://arxiv.org/pdf/2403.20216v4",
    "published_date": "2024-03-29 14:55:40 UTC",
    "updated_date": "2024-05-31 14:17:17 UTC"
  },
  {
    "arxiv_id": "2403.20212v2",
    "title": "On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem",
    "authors": [
      "Yimeng Min",
      "Carla P. Gomes"
    ],
    "abstract": "We study the generalization capability of Unsupervised Learning in solving\nthe Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN)\ntrained with a surrogate loss function to generate an embedding for each node.\nWe use these embeddings to construct a heat map that indicates the likelihood\nof each edge being part of the optimal route. We then apply local search to\ngenerate our final predictions. Our investigation explores how different\ntraining instance sizes, embedding dimensions, and distributions influence the\noutcomes of Unsupervised Learning methods. Our results show that training with\nlarger instance sizes and increasing embedding dimensions can build a more\neffective representation, enhancing the model's ability to solve TSP.\nFurthermore, in evaluating generalization across different distributions, we\nfirst determine the hardness of various distributions and explore how different\nhardnesses affect the final results. Our findings suggest that models trained\non harder instances exhibit better generalization capabilities, highlighting\nthe importance of selecting appropriate training instances in solving TSP using\nUnsupervised Learning.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20212v2",
    "published_date": "2024-03-29 14:47:54 UTC",
    "updated_date": "2024-11-19 15:23:29 UTC"
  },
  {
    "arxiv_id": "2403.20208v7",
    "title": "Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science",
    "authors": [
      "Yazheng Yang",
      "Yuqi Wang",
      "Yaxuan Li",
      "Sankalok Sen",
      "Lei Li",
      "Qi Liu"
    ],
    "abstract": "In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.20208v7",
    "published_date": "2024-03-29 14:41:21 UTC",
    "updated_date": "2025-01-25 13:35:23 UTC"
  },
  {
    "arxiv_id": "2403.20204v1",
    "title": "The Future of Combating Rumors? Retrieval, Discrimination, and Generation",
    "authors": [
      "Junhao Xu",
      "Longdi Xian",
      "Zening Liu",
      "Mingliang Chen",
      "Qiuyang Yin",
      "Fenghua Song"
    ],
    "abstract": "Artificial Intelligence Generated Content (AIGC) technology development has\nfacilitated the creation of rumors with misinformation, impacting societal,\neconomic, and political ecosystems, challenging democracy. Current rumor\ndetection efforts fall short by merely labeling potentially misinformation\n(classification task), inadequately addressing the issue, and it is unrealistic\nto have authoritative institutions debunk every piece of information on social\nmedia. Our proposed comprehensive debunking process not only detects rumors but\nalso provides explanatory generated content to refute the authenticity of the\ninformation. The Expert-Citizen Collective Wisdom (ECCW) module we designed\naensures high-precision assessment of the credibility of information and the\nretrieval module is responsible for retrieving relevant knowledge from a\nReal-time updated debunking database based on information keywords. By using\nprompt engineering techniques, we feed results and knowledge into a LLM (Large\nLanguage Model), achieving satisfactory discrimination and explanatory effects\nwhile eliminating the need for fine-tuning, saving computational costs, and\ncontributing to debunking efforts.",
    "categories": [
      "cs.AI",
      "68T99"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.20204v1",
    "published_date": "2024-03-29 14:32:41 UTC",
    "updated_date": "2024-03-29 14:32:41 UTC"
  },
  {
    "arxiv_id": "2403.20199v2",
    "title": "NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks",
    "authors": [
      "Parth Patel",
      "Milena Radenkovic"
    ],
    "abstract": "Space Communication poses challenges such as severe delays, hard-to-predict\nroutes and communication disruptions. The Delay Tolerant Network architecture,\nhaving been specifically designed keeping such scenarios in mind, is suitable\nto address some challenges. The traditional DTN routing protocols fall short of\ndelivering optimal performance, due to the inherent complexities of space\ncommunication. Researchers have aimed at using recent advancements in AI to\nmitigate some routing challenges [9]. We propose utilising a feedforward neural\nnetwork to develop a novel protocol NeuraLunaDTNet, which enhances the\nefficiency of the PRoPHET routing protocol for lunar communication, by learning\ncontact plans in dynamically changing spatio-temporal graph.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20199v2",
    "published_date": "2024-03-29 14:24:15 UTC",
    "updated_date": "2024-04-07 08:40:45 UTC"
  },
  {
    "arxiv_id": "2403.20188v1",
    "title": "Distributed Swarm Learning for Edge Internet of Things",
    "authors": [
      "Yue Wang",
      "Zhi Tian",
      "FXin Fan",
      "Zhipeng Cai",
      "Cameron Nowzari",
      "Kai Zeng"
    ],
    "abstract": "The rapid growth of Internet of Things (IoT) has led to the widespread\ndeployment of smart IoT devices at wireless edge for collaborative machine\nlearning tasks, ushering in a new era of edge learning. With a huge number of\nhardware-constrained IoT devices operating in resource-limited wireless\nnetworks, edge learning encounters substantial challenges, including\ncommunication and computation bottlenecks, device and data heterogeneity,\nsecurity risks, privacy leakages, non-convex optimization, and complex wireless\nenvironments. To address these issues, this article explores a novel framework\nknown as distributed swarm learning (DSL), which combines artificial\nintelligence and biological swarm intelligence in a holistic manner. By\nharnessing advanced signal processing and communications, DSL provides\nefficient solutions and robust tools for large-scale IoT at the edge of\nwireless networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2210.16705",
    "pdf_url": "http://arxiv.org/pdf/2403.20188v1",
    "published_date": "2024-03-29 14:05:40 UTC",
    "updated_date": "2024-03-29 14:05:40 UTC"
  },
  {
    "arxiv_id": "2403.20183v3",
    "title": "HARMamba: Efficient and Lightweight Wearable Sensor Human Activity Recognition Based on Bidirectional Mamba",
    "authors": [
      "Shuangjian Li",
      "Tao Zhu",
      "Furong Duan",
      "Liming Chen",
      "Huansheng Ning",
      "Christopher Nugent",
      "Yaping Wan"
    ],
    "abstract": "Wearable sensor-based human activity recognition (HAR) is a critical research\ndomain in activity perception. However, achieving high efficiency and long\nsequence recognition remains a challenge. Despite the extensive investigation\nof temporal deep learning models, such as CNNs, RNNs, and transformers, their\nextensive parameters often pose significant computational and memory\nconstraints, rendering them less suitable for resource-constrained mobile\nhealth applications. This study introduces HARMamba, an innovative light-weight\nand versatile HAR architecture that combines selective bidirectional State\nSpaces Model and hardware-aware design. To optimize real-time resource\nconsumption in practical scenarios, HARMamba employs linear recursive\nmechanisms and parameter discretization, allowing it to selectively focus on\nrelevant input sequences while efficiently fusing scan and recompute\noperations. The model employs independent channels to process sensor data\nstreams, dividing each channel into patches and appending classification tokens\nto the end of the sequence. It utilizes position embedding to represent the\nsequence order. The patch sequence is subsequently processed by HARMamba Block,\nand the classification head finally outputs the activity category. The HARMamba\nBlock serves as the fundamental component of the HARMamba architecture,\nenabling the effective capture of more discriminative activity sequence\nfeatures. HARMamba outperforms contemporary state-of-the-art frameworks,\ndelivering comparable or better accuracy with significantly reducing\ncomputational and memory demands. It's effectiveness has been extensively\nvalidated on 4 publically available datasets namely PAMAP2, WISDM, UNIMIB SHAR\nand UCI. The F1 scores of HARMamba on the four datasets are 99.74%, 99.20%,\n88.23% and 97.01%, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20183v3",
    "published_date": "2024-03-29 13:57:46 UTC",
    "updated_date": "2024-08-08 09:40:10 UTC"
  },
  {
    "arxiv_id": "2403.20177v3",
    "title": "Preliminaries to artificial consciousness: a multidimensional heuristic approach",
    "authors": [
      "K. Evers",
      "M. Farisco",
      "R. Chatila",
      "B. D. Earp",
      "I. T. Freire",
      "F. Hamker",
      "E. Nemeth",
      "P. F. M. J. Verschure",
      "M. Khamassi"
    ],
    "abstract": "The pursuit of artificial consciousness requires conceptual clarity to\nnavigate its theoretical and empirical challenges. This paper introduces a\ncomposite, multilevel, and multidimensional model of consciousness as a\nheuristic framework to guide research in this field. Consciousness is treated\nas a complex phenomenon, with distinct constituents and dimensions that can be\noperationalized for study and for evaluating their replication. We argue that\nthis model provides a balanced approach to artificial consciousness research by\navoiding binary thinking (e.g., conscious vs. non-conscious) and offering a\nstructured basis for testable hypotheses. To illustrate its utility, we focus\non \"awareness\" as a case study, demonstrating how specific dimensions of\nconsciousness can be pragmatically analyzed and targeted for potential\nartificial instantiation. By breaking down the conceptual intricacies of\nconsciousness and aligning them with practical research goals, this paper lays\nthe groundwork for a robust strategy to advance the scientific and technical\nunderstanding of artificial consciousness.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20177v3",
    "published_date": "2024-03-29 13:47:47 UTC",
    "updated_date": "2025-01-02 10:09:12 UTC"
  },
  {
    "arxiv_id": "2404.08664v1",
    "title": "Identifying Banking Transaction Descriptions via Support Vector Machine Short-Text Classification Based on a Specialized Labelled Corpus",
    "authors": [
      "Silvia Garca-Mndez",
      "Milagros Fernndez-Gavilanes",
      "Jonathan Juncal-Martnez",
      "Francisco J. Gonzlez-Castao",
      "Oscar Barba Seara"
    ],
    "abstract": "Short texts are omnipresent in real-time news, social network commentaries,\netc. Traditional text representation methods have been successfully applied to\nself-contained documents of medium size. However, information in short texts is\noften insufficient, due, for example, to the use of mnemonics, which makes them\nhard to classify. Therefore, the particularities of specific domains must be\nexploited. In this article we describe a novel system that combines Natural\nLanguage Processing techniques with Machine Learning algorithms to classify\nbanking transaction descriptions for personal finance management, a problem\nthat was not previously considered in the literature. We trained and tested\nthat system on a labelled dataset with real customer transactions that will be\navailable to other researchers on request. Motivated by existing solutions in\nspam detection, we also propose a short text similarity detector to reduce\ntraining set size based on the Jaccard distance. Experimental results with a\ntwo-stage classifier combining this detector with a SVM indicate a high\naccuracy in comparison with alternative approaches, taking into account\ncomplexity and computing time. Finally, we present a use case with a personal\nfinance application, CoinScrap, which is available at Google Play and App\nStore.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.08664v1",
    "published_date": "2024-03-29 13:15:46 UTC",
    "updated_date": "2024-03-29 13:15:46 UTC"
  },
  {
    "arxiv_id": "2403.20158v1",
    "title": "ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models",
    "authors": [
      "Zehao Wen",
      "Rabih Younes"
    ],
    "abstract": "In our rapidly evolving digital sphere, the ability to discern media bias\nbecomes crucial as it can shape public sentiment and influence pivotal\ndecisions. The advent of large language models (LLMs), such as ChatGPT, noted\nfor their broad utility in various natural language processing (NLP) tasks,\ninvites exploration of their efficacy in media bias detection. Can ChatGPT\ndetect media bias? This study seeks to answer this question by leveraging the\nMedia Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in\ndistinguishing six categories of media bias, juxtaposed against fine-tuned\nmodels such as BART, ConvBERT, and GPT-2. The findings present a dichotomy:\nChatGPT performs at par with fine-tuned models in detecting hate speech and\ntext-level context bias, yet faces difficulties with subtler elements of other\nbias detections, namely, fake news, racial, gender, and cognitive biases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figure, published on Applied and Computational Engineering",
    "pdf_url": "http://arxiv.org/pdf/2403.20158v1",
    "published_date": "2024-03-29 13:12:09 UTC",
    "updated_date": "2024-03-29 13:12:09 UTC"
  },
  {
    "arxiv_id": "2403.20156v2",
    "title": "CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening",
    "authors": [
      "Hei Yi Mak",
      "Flint Xiaofeng Fan",
      "Luca A. Lanzendrfer",
      "Cheston Tan",
      "Wei Tsang Ooi",
      "Roger Wattenhofer"
    ],
    "abstract": "In this study, we delve into Federated Reinforcement Learning (FedRL) in the\ncontext of value-based agents operating across diverse Markov Decision\nProcesses (MDPs). Existing FedRL methods typically aggregate agents' learning\nby averaging the value functions across them to improve their performance.\nHowever, this aggregation strategy is suboptimal in heterogeneous environments\nwhere agents converge to diverse optimal value functions. To address this\nproblem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR)\naggregation scheme designed to enhance the learning of individual agents across\nvaried MDPs. CAESAR is an aggregation strategy used by the server that combines\nconvergence-aware sampling with a screening mechanism. By exploiting the fact\nthat agents learning in identical MDPs are converging to the same optimal value\nfunction, CAESAR enables the selective assimilation of knowledge from more\nproficient counterparts, thereby significantly enhancing the overall learning\nefficiency. We empirically validate our hypothesis and demonstrate the\neffectiveness of CAESAR in enhancing the learning efficiency of agents, using\nboth a custom-built GridWorld environment and the classical FrozenLake-v1 task,\neach presenting varying levels of environmental heterogeneity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20156v2",
    "published_date": "2024-03-29 13:05:59 UTC",
    "updated_date": "2024-04-16 20:31:16 UTC"
  },
  {
    "arxiv_id": "2403.20151v2",
    "title": "A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles",
    "authors": [
      "Jiani Fan",
      "Minrui Xu",
      "Ziyao Liu",
      "Huanyi Ye",
      "Chaojie Gu",
      "Dusit Niyato",
      "Kwok-Yan Lam"
    ],
    "abstract": "Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of\nautomated content generation utilizing AI models. Mobile AIGC services in the\nInternet of Vehicles (IoV) network have numerous advantages over traditional\ncloud-based AIGC services, including enhanced network efficiency, better\nreconfigurability, and stronger data security and privacy. Nonetheless, AIGC\nservice provisioning frequently demands significant resources. Consequently,\nresource-constrained roadside units (RSUs) face challenges in maintaining a\nheterogeneous pool of AIGC services and addressing all user service requests\nwithout degrading overall performance. Therefore, in this paper, we propose a\ndecentralized incentive mechanism for mobile AIGC service allocation, employing\nmulti-agent deep reinforcement learning to find the balance between the supply\nof AIGC services on RSUs and user demand for services within the IoV context,\noptimizing user experience and minimizing transmission latency. Experimental\nresults demonstrate that our approach achieves superior performance compared to\nother baseline models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)",
    "pdf_url": "http://arxiv.org/pdf/2403.20151v2",
    "published_date": "2024-03-29 12:46:07 UTC",
    "updated_date": "2024-05-09 08:49:43 UTC"
  },
  {
    "arxiv_id": "2403.20150v3",
    "title": "TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods",
    "authors": [
      "Xiangfei Qiu",
      "Jilin Hu",
      "Lekui Zhou",
      "Xingjian Wu",
      "Junyang Du",
      "Buang Zhang",
      "Chenjuan Guo",
      "Aoying Zhou",
      "Christian S. Jensen",
      "Zhenli Sheng",
      "Bin Yang"
    ],
    "abstract": "Time series are generated in diverse domains such as economic, traffic,\nhealth, and energy, where forecasting of future values has numerous important\napplications. Not surprisingly, many forecasting methods are being proposed. To\nensure progress, it is essential to be able to study and compare such methods\nempirically in a comprehensive and reliable manner. To achieve this, we propose\nTFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB\nadvances the state-of-the-art by addressing shortcomings related to datasets,\ncomparison methods, and evaluation pipelines: 1) insufficient coverage of data\ndomains, 2) stereotype bias against traditional methods, and 3) inconsistent\nand inflexible pipelines. To achieve better domain coverage, we include\ndatasets from 10 different domains: traffic, electricity, energy, the\nenvironment, nature, economic, stock markets, banking, health, and the web. We\nalso provide a time series characterization to ensure that the selected\ndatasets are comprehensive. To remove biases against some methods, we include a\ndiverse range of methods, including statistical learning, machine learning, and\ndeep learning methods, and we also support a variety of evaluation strategies\nand metrics to ensure a more comprehensive evaluations of different methods. To\nsupport the integration of different methods into the benchmark and enable fair\ncomparisons, TFB features a flexible and scalable pipeline that eliminates\nbiases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate\nTime Series Forecasting (UTSF) methods on 8,068 univariate time series and 14\nMultivariate Time Series Forecasting (MTSF) methods on 25 datasets. The\nbenchmark code and data are available at\nhttps://github.com/decisionintelligence/TFB.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "Directly accepted by PVLDB 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.20150v3",
    "published_date": "2024-03-29 12:37:57 UTC",
    "updated_date": "2024-06-19 03:29:46 UTC"
  },
  {
    "arxiv_id": "2403.20137v1",
    "title": "Accurate Block Quantization in LLMs with Outliers",
    "authors": [
      "Nikita Trukhanov",
      "Ilya Soloveychik"
    ],
    "abstract": "The demand for inference on extremely large scale LLMs has seen enormous\ngrowth in the recent months. It made evident the colossal shortage of dedicated\nhardware capable of efficient and fast processing of the involved compute and\nmemory movement. The problem is aggravated by the exploding raise in the\nlengths of the sequences being processed, since those require efficient on-chip\nstorage of the KV-cache of size proportional to the sequence length. To make\nthe required compute feasible and fit the involved data into available memory,\nnumerous quantization techniques have been proposed that allow accurate\nquantization for both weights and activations. One of the main recent\nbreakthroughs in this direction was introduction of the family of Block\nFloating Point (BFP) formats characterized by a block of mantissas with a\nshared scale factor. These enable memory- power-, and compute- efficient\nhardware support of the tensor operations and provide extremely good\nquantization accuracy. The main issues preventing widespread application of\nblock formats is caused by the presence of outliers in weights and activations\nsince those affect the accuracy of the other values in the same block. In this\npaper, we focus on the most critical problem of limited KV-cache storage. We\npropose a novel approach enabling usage of low precision BFP formats without\ncompromising the resulting model accuracy. We exploit the common channel-wise\npatterns exhibited by the outliers to rearrange them in such a way, that their\nquantization quality is significantly improved. The methodology yields 2x\nsavings in the memory footprint without significant degradation of the model's\naccuracy. Importantly, the rearrangement of channels happens at the compile\ntime and thus has no impact on the inference latency.",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20137v1",
    "published_date": "2024-03-29 12:15:06 UTC",
    "updated_date": "2024-03-29 12:15:06 UTC"
  },
  {
    "arxiv_id": "2404.01327v1",
    "title": "Entertainment chatbot for the digital inclusion of elderly people without abstraction capabilities",
    "authors": [
      "Silvia Garca-Mndez",
      "Francisco de Arriba-Prez",
      "Francisco J. Gonzlez-Castao",
      "Jos A. Regueiro-Janeiro",
      "Felipe Gil-Castieira"
    ],
    "abstract": "Current language processing technologies allow the creation of conversational\nchatbot platforms. Even though artificial intelligence is still too immature to\nsupport satisfactory user experience in many mass market domains,\nconversational interfaces have found their way into ad hoc applications such as\ncall centres and online shopping assistants. However, they have not been\napplied so far to social inclusion of elderly people, who are particularly\nvulnerable to the digital divide. Many of them relieve their loneliness with\ntraditional media such as TV and radio, which are known to create a feeling of\ncompanionship. In this paper we present the EBER chatbot, designed to reduce\nthe digital gap for the elderly. EBER reads news in the background and adapts\nits responses to the user's mood. Its novelty lies in the concept of\n\"intelligent radio\", according to which, instead of simplifying a digital\ninformation system to make it accessible to the elderly, a traditional channel\nthey find familiar -- background news -- is augmented with interactions via\nvoice dialogues. We make it possible by combining Artificial Intelligence\nModelling Language, automatic Natural Language Generation and Sentiment\nAnalysis. The system allows accessing digital content of interest by combining\nwords extracted from user answers to chatbot questions with keywords extracted\nfrom the news items. This approach permits defining metrics of the abstraction\ncapabilities of the users depending on a spatial representation of the word\nspace. To prove the suitability of the proposed solution we present results of\nreal experiments conducted with elderly people that provided valuable insights.\nOur approach was considered satisfactory during the tests and improved the\ninformation search capabilities of the participants.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.01327v1",
    "published_date": "2024-03-29 12:10:21 UTC",
    "updated_date": "2024-03-29 12:10:21 UTC"
  },
  {
    "arxiv_id": "2403.20127v1",
    "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
    "authors": [
      "Kaito Taguchi",
      "Yujie Gu",
      "Kouichi Sakurai"
    ],
    "abstract": "In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20127v1",
    "published_date": "2024-03-29 11:33:34 UTC",
    "updated_date": "2024-03-29 11:33:34 UTC"
  },
  {
    "arxiv_id": "2403.20109v1",
    "title": "Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation",
    "authors": [
      "Jinyeong Park",
      "Jaegyoon Ahn",
      "Jonghwan Choi",
      "Jibum Kim"
    ],
    "abstract": "Optimizing techniques for discovering molecular structures with desired\nproperties is crucial in artificial intelligence(AI)-based drug discovery.\nCombining deep generative models with reinforcement learning has emerged as an\neffective strategy for generating molecules with specific properties. Despite\nits potential, this approach is ineffective in exploring the vast chemical\nspace and optimizing particular chemical properties. To overcome these\nlimitations, we present Mol-AIR, a reinforcement learning-based framework using\nadaptive intrinsic rewards for effective goal-directed molecular generation.\nMol-AIR leverages the strengths of both history-based and learning-based\nintrinsic rewards by exploiting random distillation network and counting-based\nstrategies. In benchmark tests, Mol-AIR demonstrates superior performance over\nexisting approaches in generating molecules with desired properties without any\nprior knowledge, including penalized LogP, QED, and celecoxib similarity. We\nbelieve that Mol-AIR represents a significant advancement in drug discovery,\noffering a more efficient path to discovering novel therapeutics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20109v1",
    "published_date": "2024-03-29 10:44:51 UTC",
    "updated_date": "2024-03-29 10:44:51 UTC"
  },
  {
    "arxiv_id": "2403.20097v2",
    "title": "ITCMA: A Generative Agent Based on a Computational Consciousness Structure",
    "authors": [
      "Hanzhong Zhang",
      "Jibin Yin",
      "Haoyang Wang",
      "Ziwei Xiang"
    ],
    "abstract": "Large Language Models (LLMs) still face challenges in tasks requiring\nunderstanding implicit instructions and applying common-sense knowledge. In\nsuch scenarios, LLMs may require multiple attempts to achieve human-level\nperformance, potentially leading to inaccurate responses or inferences in\npractical environments, affecting their long-term consistency and behavior.\nThis paper introduces the Internal Time-Consciousness Machine (ITCM), a\ncomputational consciousness structure to simulate the process of human\nconsciousness. We further propose the ITCM-based Agent (ITCMA), which supports\naction generation and reasoning in open-world settings, and can independently\ncomplete tasks. ITCMA enhances LLMs' ability to understand implicit\ninstructions and apply common-sense knowledge by considering agents'\ninteraction and reasoning with the environment. Evaluations in the Alfworld\nenvironment show that trained ITCMA outperforms the state-of-the-art (SOTA) by\n9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on\nthe seen set, 5% higher than SOTA, indicating its superiority over traditional\nintelligent agents in utility and generalization. In real-world tasks with\nquadruped robots, the untrained ITCMA achieves an 85% task completion rate,\nwhich is close to its performance in the unseen set, demonstrating its\ncomparable utility and universality in real-world settings.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "q-bio.NC",
      "I.2; J.4"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.20097v2",
    "published_date": "2024-03-29 10:23:18 UTC",
    "updated_date": "2024-06-08 13:04:40 UTC"
  },
  {
    "arxiv_id": "2403.20089v2",
    "title": "Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness",
    "authors": [
      "Luca Deck",
      "Jan-Laurin Mller",
      "Conradin Braun",
      "Domenique Zipperling",
      "Niklas Khl"
    ],
    "abstract": "The topic of fairness in AI, as debated in the FATE (Fairness,\nAccountability, Transparency, and Ethics in AI) communities, has sparked\nmeaningful discussions in the past years. However, from a legal perspective,\nparticularly from the perspective of European Union law, many open questions\nremain. Whereas algorithmic fairness aims to mitigate structural inequalities\nat design-level, European non-discrimination law is tailored to individual\ncases of discrimination after an AI model has been deployed. The AI Act might\npresent a tremendous step towards bridging these two approaches by shifting\nnon-discrimination responsibilities into the design stage of AI models. Based\non an integrative reading of the AI Act, we comment on legal as well as\ntechnical enforcement problems and propose practical implications on bias\ndetection and bias correction in order to specify and comply with specific\ntechnical requirements.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.20089v2",
    "published_date": "2024-03-29 09:54:09 UTC",
    "updated_date": "2024-06-26 07:35:30 UTC"
  },
  {
    "arxiv_id": "2404.00081v1",
    "title": "Molecular Generative Adversarial Network with Multi-Property Optimization",
    "authors": [
      "Huidong Tang",
      "Chen Li",
      "Sayaka Kamei",
      "Yoshihiro Yamanishi",
      "Yasuhiko Morimoto"
    ],
    "abstract": "Deep generative models, such as generative adversarial networks (GANs), have\nbeen employed for $de~novo$ molecular generation in drug discovery. Most prior\nstudies have utilized reinforcement learning (RL) algorithms, particularly\nMonte Carlo tree search (MCTS), to handle the discrete nature of molecular\nrepresentations in GANs. However, due to the inherent instability in training\nGANs and RL models, along with the high computational cost associated with MCTS\nsampling, MCTS RL-based GANs struggle to scale to large chemical databases. To\ntackle these challenges, this study introduces a novel GAN based on\nactor-critic RL with instant and global rewards, called InstGAN, to generate\nmolecules at the token-level with multi-property optimization. Furthermore,\nmaximized information entropy is leveraged to alleviate the mode collapse. The\nexperimental results demonstrate that InstGAN outperforms other baselines,\nachieves comparable performance to state-of-the-art models, and efficiently\ngenerates molecules with multi-property optimization. The source code will be\nreleased upon acceptance of the paper.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00081v1",
    "published_date": "2024-03-29 08:55:39 UTC",
    "updated_date": "2024-03-29 08:55:39 UTC"
  },
  {
    "arxiv_id": "2403.20058v3",
    "title": "Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks",
    "authors": [
      "Luoyu Wang",
      "Yitian Tao",
      "Qing Yang",
      "Yan Liang",
      "Siwei Liu",
      "Hongcheng Shi",
      "Dinggang Shen",
      "Han Zhang"
    ],
    "abstract": "Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal\nneuroimaging technique. It provides an unprecedented opportunity for\nconcurrently monitoring and integrating multifaceted brain networks built by\nspatiotemporally covaried metabolic activity, neural activity, and cerebral\nblood flow (perfusion). Albeit high scientific/clinical values, short in\nhardware accessibility of PET/MR hinders its applications, let alone modern\nAI-based PET/MR fusion models. Our objective is to develop a clinically\nfeasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR\ndata with the power of, during inferencing, allowing single modality input\n(e.g., PET only) as well as enforcing multimodal-based accuracy. To this end,\nwe propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction\nModel. It is modality detachable and exchangeable, allocating different\nmulti-layer perceptrons dynamically (\"mixture of experts\") through learnable\nweights to learn respective representations from different modalities. Such\ndesign will not sacrifice model performance in uni-modal situation. To fully\nexploit the inherent complex and nonlinear relation among modalities while\nproducing fine-grained representations for uni-modal inference, we subsequently\nadd a modal alignment module to line up a dominant modality (e.g., PET) with\nrepresentations of auxiliary modalities (MR). We further adopt multimodal\nreconstruction to promote the quality of learned features. Experiments on\nprecious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis\nshowcase the efficacy of our model toward clinically feasible precision\nmedicine.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2403.20058v3",
    "published_date": "2024-03-29 08:47:49 UTC",
    "updated_date": "2024-09-20 12:07:03 UTC"
  },
  {
    "arxiv_id": "2403.20015v1",
    "title": "Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion",
    "authors": [
      "Juhwan Choi",
      "YoungBin Kim"
    ],
    "abstract": "In the field of text data augmentation, rule-based methods are widely adopted\nfor real-world applications owing to their cost-efficiency. However,\nconventional rule-based approaches suffer from the possibility of losing the\noriginal semantics of the given text. We propose a novel text data augmentation\nstrategy that avoids such phenomena through a straightforward deletion of\nadverbs, which play a subsidiary role in the sentence. Our comprehensive\nexperiments demonstrate the efficiency and effectiveness of our proposed\napproach for not just single text classification, but also natural language\ninference that requires semantic preservation. We publicly released our source\ncode for reproducibility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2024 Tiny Papers",
    "pdf_url": "http://arxiv.org/pdf/2403.20015v1",
    "published_date": "2024-03-29 07:01:39 UTC",
    "updated_date": "2024-03-29 07:01:39 UTC"
  },
  {
    "arxiv_id": "2403.20014v1",
    "title": "PURPLE: Making a Large Language Model a Better SQL Writer",
    "authors": [
      "Tonghui Ren",
      "Yuankai Fan",
      "Zhenying He",
      "Ren Huang",
      "Jiaqi Dai",
      "Can Huang",
      "Yinan Jing",
      "Kai Zhang",
      "Yifan Yang",
      "X. Sean Wang"
    ],
    "abstract": "Large Language Model (LLM) techniques play an increasingly important role in\nNatural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora\nhave strong natural language understanding and basic SQL generation abilities\nwithout additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL\napproaches try to improve the translation by enhancing the LLMs with an\nemphasis on user intention understanding. However, LLMs sometimes fail to\ngenerate appropriate SQL due to their lack of knowledge in organizing complex\nlogical operator composition. A promising method is to input the LLMs with\ndemonstrations, which include known NL2SQL translations from various databases.\nLLMs can learn to organize operator compositions from the input demonstrations\nfor the given task. In this paper, we propose PURPLE (Pre-trained models\nUtilized to Retrieve Prompts for Logical Enhancement), which improves accuracy\nby retrieving demonstrations containing the requisite logical operator\ncomposition for the NL2SQL task on hand, thereby guiding LLMs to produce better\nSQL translation. PURPLE achieves a new state-of-the-art performance of 80.5%\nexact-set match accuracy and 87.8% execution match accuracy on the validation\nset of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy\nacross diverse benchmarks, budgetary constraints, and various LLMs, showing\nrobustness and cost-effectiveness.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "12 pages, accepted by ICDE 2024 (40th IEEE International Conference\n  on Data Engineering)",
    "pdf_url": "http://arxiv.org/pdf/2403.20014v1",
    "published_date": "2024-03-29 07:01:29 UTC",
    "updated_date": "2024-03-29 07:01:29 UTC"
  },
  {
    "arxiv_id": "2403.20012v1",
    "title": "Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning",
    "authors": [
      "Juhwan Choi",
      "YoungBin Kim"
    ],
    "abstract": "Data augmentation is one of the regularization strategies for the training of\ndeep learning models, which enhances generalizability and prevents overfitting,\nleading to performance improvement. Although researchers have proposed various\ndata augmentation techniques, they often lack consideration for the difficulty\nof augmented data. Recently, another line of research suggests incorporating\nthe concept of curriculum learning with data augmentation in the field of\nnatural language processing. In this study, we adopt curriculum data\naugmentation for image data augmentation and propose colorful cutout, which\ngradually increases the noise and difficulty introduced in the augmented image.\nOur experimental results highlight the possibility of curriculum data\naugmentation for image data. We publicly released our source code to improve\nthe reproducibility of our study.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2024 Tiny Papers",
    "pdf_url": "http://arxiv.org/pdf/2403.20012v1",
    "published_date": "2024-03-29 06:53:52 UTC",
    "updated_date": "2024-03-29 06:53:52 UTC"
  },
  {
    "arxiv_id": "2403.19995v2",
    "title": "Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots",
    "authors": [
      "Prasanna Vijayaraghavan",
      "Jeffrey Frederic Queisser",
      "Sergio Verduzco Flores",
      "Jun Tani"
    ],
    "abstract": "Humans excel at applying learned behavior to unlearned situations. A crucial\ncomponent of this generalization behavior is our ability to compose/decompose a\nwhole into reusable parts, an attribute known as compositionality. One of the\nfundamental questions in robotics concerns this characteristic. \"How can\nlinguistic compositionality be developed concomitantly with sensorimotor skills\nthrough associative learning, particularly when individuals only learn partial\nlinguistic compositions and their corresponding sensorimotor patterns?\" To\naddress this question, we propose a brain-inspired neural network model that\nintegrates vision, proprioception, and language into a framework of predictive\ncoding and active inference, based on the free-energy principle. The\neffectiveness and capabilities of this model were assessed through various\nsimulation experiments conducted with a robot arm. Our results show that\ngeneralization in learning to unlearned verb-noun compositions, is\nsignificantly enhanced when training variations of task composition are\nincreased. We attribute this to self-organized compositional structures in\nlinguistic latent state space being influenced significantly by sensorimotor\nlearning. Ablation studies show that visual attention and working memory are\nessential to accurately generate visuo-motor sequences to achieve\nlinguistically represented goals. These insights advance our understanding of\nmechanisms underlying development of compositionality through interactions of\nlinguistic and sensorimotor experience.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.RO",
      "68T35, 68T40",
      "I.2.9"
    ],
    "primary_category": "cs.AI",
    "comment": "64 pages, 6 figures, 10 supplementary figures",
    "pdf_url": "http://arxiv.org/pdf/2403.19995v2",
    "published_date": "2024-03-29 06:22:37 UTC",
    "updated_date": "2024-07-23 05:21:44 UTC"
  },
  {
    "arxiv_id": "2403.19992v2",
    "title": "MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System",
    "authors": [
      "Maha Nawaz",
      "Abdul Basit",
      "Muhammad Shafique"
    ],
    "abstract": "Currently, individuals with arm mobility impairments (referred to as\n\"patients\") face limited technological solutions due to two key challenges: (1)\nnon-invasive prosthetic devices are often prohibitively expensive and costly to\nmaintain, and (2) invasive solutions require high-risk, costly brain surgery,\nwhich can pose a health risk. Therefore, current technological solutions are\nnot accessible for all patients with different financial backgrounds. Toward\nthis, we propose a low-cost technological solution called MindArm, an\naffordable, non-invasive neuro-driven prosthetic arm system. MindArm employs a\ndeep neural network (DNN) to translate brain signals, captured by low-cost\nsurface electroencephalogram (EEG) electrodes, into prosthetic arm movements.\nUtilizing an Open Brain Computer Interface and UDP networking for signal\nprocessing, the system seamlessly controls arm motion. In the compute module,\nwe run a trained DNN model to interpret filtered micro-voltage brain signals,\nand then translate them into a prosthetic arm action via serial communication\nseamlessly. Experimental results from a fully functional prototype show high\naccuracy across three actions, with 91% for idle/stationary, 85% for handshake,\nand 84% for cup pickup. The system costs approximately $500-550, including $400\nfor the EEG headset and $100-150 for motors, 3D printing, and assembly,\noffering an affordable alternative for mind-controlled prosthetic devices.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO",
      "I.2.9"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 22 figures, Paper accepted at ICARCV 2024, funded by CAIR",
    "pdf_url": "http://arxiv.org/pdf/2403.19992v2",
    "published_date": "2024-03-29 06:09:24 UTC",
    "updated_date": "2024-10-19 18:23:46 UTC"
  },
  {
    "arxiv_id": "2403.19979v1",
    "title": "Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer",
    "authors": [
      "Yuwen Tan",
      "Qinhao Zhou",
      "Xiang Xiang",
      "Ke Wang",
      "Yuchuan Wu",
      "Yongbin Li"
    ],
    "abstract": "Class-incremental learning (CIL) aims to enable models to continuously learn\nnew classes while overcoming catastrophic forgetting. The introduction of\npre-trained models has brought new tuning paradigms to CIL. In this paper, we\nrevisit different parameter-efficient tuning (PET) methods within the context\nof continual learning. We observe that adapter tuning demonstrates superiority\nover prompt-based methods, even without parameter expansion in each learning\nsession. Motivated by this, we propose incrementally tuning the shared adapter\nwithout imposing parameter update constraints, enhancing the learning capacity\nof the backbone. Additionally, we employ feature sampling from stored\nprototypes to retrain a unified classifier, further improving its performance.\nWe estimate the semantic shift of old prototypes without access to past samples\nand update stored prototypes session by session. Our proposed method eliminates\nmodel expansion and avoids retaining any image samples. It surpasses previous\npre-trained model-based CIL methods and demonstrates remarkable continual\nlearning capabilities. Experimental results on five CIL benchmarks validate the\neffectiveness of our approach, achieving state-of-the-art (SOTA) performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear at CVPR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19979v1",
    "published_date": "2024-03-29 05:23:12 UTC",
    "updated_date": "2024-03-29 05:23:12 UTC"
  },
  {
    "arxiv_id": "2403.19962v1",
    "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
    "authors": [
      "Qinhao Zhou",
      "Zihan Zhang",
      "Xiang Xiang",
      "Ke Wang",
      "Yuchuan Wu",
      "Yongbin Li"
    ],
    "abstract": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language\nunderstanding and generation capabilities, making them highly successful in a\nvariety of tasks. However, when used as agents for dealing with complex\nproblems in the real world, their performance is far inferior to large\ncommercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need\nto have the capabilities of task planning, long-term memory, and the ability to\nleverage external tools to achieve satisfactory performance. Various methods\nhave been proposed to enhance the agent capabilities of LLMs. On the one hand,\nmethods involve constructing agent-specific data and fine-tuning the models. On\nthe other hand, some methods focus on designing prompts that effectively\nactivate the reasoning abilities of the LLMs. We explore both strategies on the\n7B and 13B models. We propose a comprehensive method for constructing\nagent-specific data using GPT-4. Through supervised fine-tuning with\nconstructed data, we find that for these models with a relatively small number\nof parameters, supervised fine-tuning can significantly reduce hallucination\noutputs and formatting errors in agent tasks. Furthermore, techniques such as\nmulti-path reasoning and task decomposition can effectively decrease problem\ncomplexity and enhance the performance of LLMs as agents. We evaluate our\nmethod on five agent tasks of AgentBench and achieve satisfactory results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at NAACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19962v1",
    "published_date": "2024-03-29 03:48:12 UTC",
    "updated_date": "2024-03-29 03:48:12 UTC"
  },
  {
    "arxiv_id": "2403.19946v1",
    "title": "A Peg-in-hole Task Strategy for Holes in Concrete",
    "authors": [
      "Andr Yuji Yasutomi",
      "Hiroki Mori",
      "Tetsuya Ogata"
    ],
    "abstract": "A method that enables an industrial robot to accomplish the peg-in-hole task\nfor holes in concrete is proposed. The proposed method involves slightly\ndetaching the peg from the wall, when moving between search positions, to avoid\nthe negative influence of the concrete's high friction coefficient. It uses a\ndeep neural network (DNN), trained via reinforcement learning, to effectively\nfind holes with variable shape and surface finish (due to the brittle nature of\nconcrete) without analytical modeling or control parameter tuning. The method\nuses displacement of the peg toward the wall surface, in addition to force and\ntorque, as one of the inputs of the DNN. Since the displacement increases as\nthe peg gets closer to the hole (due to the chamfered shape of holes in\nconcrete), it is a useful parameter for inputting in the DNN. The proposed\nmethod was evaluated by training the DNN on a hole 500 times and attempting to\nfind 12 unknown holes. The results of the evaluation show the DNN enabled a\nrobot to find the unknown holes with average success rate of 96.1% and average\nexecution time of 12.5 seconds. Additional evaluations with random initial\npositions and a different type of peg demonstrate the trained DNN can\ngeneralize well to different conditions. Analyses of the influence of the peg\ndisplacement input showed the success rate of the DNN is increased by utilizing\nthis parameter. These results validate the proposed method in terms of its\neffectiveness and applicability to the construction industry.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Published in 2021 IEEE International Conference on Robotics and\n  Automation (ICRA) on 30 May 2021",
    "pdf_url": "http://arxiv.org/pdf/2403.19946v1",
    "published_date": "2024-03-29 03:00:54 UTC",
    "updated_date": "2024-03-29 03:00:54 UTC"
  },
  {
    "arxiv_id": "2403.19943v1",
    "title": "TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis",
    "authors": [
      "Zhongzhi Li",
      "Rong Fan",
      "Jingqi Tu",
      "Jinyi Ma",
      "Jianliang Ai",
      "Yiqun Dong"
    ],
    "abstract": "Fault diagnosis plays a crucial role in maintaining the operational integrity\nof mechanical systems, preventing significant losses due to unexpected\nfailures. As intelligent manufacturing and data-driven approaches evolve, Deep\nLearning (DL) has emerged as a pivotal technique in fault diagnosis research,\nrecognized for its ability to autonomously extract complex features. However,\nthe practical application of current fault diagnosis methods is challenged by\nthe complexity of industrial environments. This paper proposed the Temporal\nDenoise Convolutional Neural Network With Attention (TDANet), designed to\nimprove fault diagnosis performance in noise environments. This model\ntransforms one-dimensional signals into two-dimensional tensors based on their\nperiodic properties, employing multi-scale 2D convolution kernels to extract\nsignal information both within and across periods. This method enables\neffective identification of signal characteristics that vary over multiple time\nscales. The TDANet incorporates a Temporal Variable Denoise (TVD) module with\nresidual connections and a Multi-head Attention Fusion (MAF) module, enhancing\nthe saliency of information within noisy data and maintaining effective fault\ndiagnosis performance. Evaluation on two datasets, CWRU (single sensor) and\nReal aircraft sensor fault (multiple sensors), demonstrates that the TDANet\nmodel significantly outperforms existing deep learning approaches in terms of\ndiagnostic accuracy under noisy environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19943v1",
    "published_date": "2024-03-29 02:54:41 UTC",
    "updated_date": "2024-03-29 02:54:41 UTC"
  },
  {
    "arxiv_id": "2403.19941v1",
    "title": "Diverse Feature Learning by Self-distillation and Reset",
    "authors": [
      "Sejik Park"
    ],
    "abstract": "Our paper addresses the problem of models struggling to learn diverse\nfeatures, due to either forgetting previously learned features or failing to\nlearn new ones. To overcome this problem, we introduce Diverse Feature Learning\n(DFL), a method that combines an important feature preservation algorithm with\na new feature learning algorithm. Specifically, for preserving important\nfeatures, we utilize self-distillation in ensemble models by selecting the\nmeaningful model weights observed during training. For learning new features,\nwe employ reset that involves periodically re-initializing part of the model.\nAs a result, through experiments with various models on the image\nclassification, we have identified the potential for synergistic effects\nbetween self-distillation and reset.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 6 Figures",
    "pdf_url": "http://arxiv.org/pdf/2403.19941v1",
    "published_date": "2024-03-29 02:49:15 UTC",
    "updated_date": "2024-03-29 02:49:15 UTC"
  },
  {
    "arxiv_id": "2403.19925v1",
    "title": "Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces",
    "authors": [
      "Toshihiro Ota"
    ],
    "abstract": "Decision Transformer, a promising approach that applies Transformer\narchitectures to reinforcement learning, relies on causal self-attention to\nmodel sequences of states, actions, and rewards. While this method has shown\ncompetitive results, this paper investigates the integration of the Mamba\nframework, known for its advanced capabilities in efficient and effective\nsequence modeling, into the Decision Transformer architecture, focusing on the\npotential performance enhancements in sequential decision-making tasks. Our\nstudy systematically evaluates this integration by conducting a series of\nexperiments across various decision-making environments, comparing the modified\nDecision Transformer, Decision Mamba, with its traditional counterpart. This\nwork contributes to the advancement of sequential decision-making models,\nsuggesting that the architecture and training methodology of neural networks\ncan significantly impact their performance in complex tasks, and highlighting\nthe potential of Mamba as a valuable tool for improving the efficacy of\nTransformer-based models in reinforcement learning scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.19925v1",
    "published_date": "2024-03-29 02:25:55 UTC",
    "updated_date": "2024-03-29 02:25:55 UTC"
  },
  {
    "arxiv_id": "2403.19918v3",
    "title": "CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning",
    "authors": [
      "Luke Rowe",
      "Roger Girgis",
      "Anthony Gosselin",
      "Bruno Carrez",
      "Florian Golemo",
      "Felix Heide",
      "Liam Paull",
      "Christopher Pal"
    ],
    "abstract": "Evaluating autonomous vehicle stacks (AVs) in simulation typically involves\nreplaying driving logs from real-world recorded traffic. However, agents\nreplayed from offline data are not reactive and hard to intuitively control.\nExisting approaches address these challenges by proposing methods that rely on\nheuristics or generative models of real-world data but these approaches either\nlack realism or necessitate costly iterative sampling procedures to control the\ngenerated behaviours. In this work, we take an alternative approach and propose\nCtRL-Sim, a method that leverages return-conditioned offline reinforcement\nlearning (RL) to efficiently generate reactive and controllable traffic agents.\nSpecifically, we process real-world driving data through a physics-enhanced\nNocturne simulator to generate a diverse offline RL dataset, annotated with\nvarious rewards. With this dataset, we train a return-conditioned multi-agent\nbehaviour model that allows for fine-grained manipulation of agent behaviours\nby modifying the desired returns for the various reward components. This\ncapability enables the generation of a wide range of driving behaviours beyond\nthe scope of the initial dataset, including adversarial behaviours. We show\nthat CtRL-Sim can generate realistic safety-critical scenarios while providing\nfine-grained control over agent behaviours.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.19918v3",
    "published_date": "2024-03-29 02:10:19 UTC",
    "updated_date": "2024-10-14 18:13:36 UTC"
  },
  {
    "arxiv_id": "2403.19913v2",
    "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
    "authors": [
      "Peng Ding",
      "Jiading Fang",
      "Peng Li",
      "Kangrui Wang",
      "Xiaochen Zhou",
      "Mo Yu",
      "Jing Li",
      "Matthew R. Walter",
      "Hongyuan Mei"
    ],
    "abstract": "Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "comment": "COLM 2024 camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2403.19913v2",
    "published_date": "2024-03-29 01:53:24 UTC",
    "updated_date": "2024-08-08 06:38:31 UTC"
  },
  {
    "arxiv_id": "2403.19907v1",
    "title": "Beyond the Known: Novel Class Discovery for Open-world Graph Learning",
    "authors": [
      "Yucheng Jin",
      "Yun Xiong",
      "Juncheng Fang",
      "Xixi Wu",
      "Dongxiao He",
      "Xing Jia",
      "Bingchen Zhao",
      "Philip Yu"
    ],
    "abstract": "Node classification on graphs is of great importance in many applications.\nDue to the limited labeling capability and evolution in real-world open\nscenarios, novel classes can emerge on unlabeled testing nodes. However, little\nattention has been paid to novel class discovery on graphs. Discovering novel\nclasses is challenging as novel and known class nodes are correlated by edges,\nwhich makes their representations indistinguishable when applying message\npassing GNNs. Furthermore, the novel classes lack labeling information to guide\nthe learning process. In this paper, we propose a novel method Open-world gRAph\nneuraL network (ORAL) to tackle these challenges. ORAL first detects\ncorrelations between classes through semi-supervised prototypical learning.\nInter-class correlations are subsequently eliminated by the prototypical\nattention network, leading to distinctive representations for different\nclasses. Furthermore, to fully explore multi-scale graph features for\nalleviating label deficiencies, ORAL generates pseudo-labels by aligning and\nensembling label estimations from multiple stacked prototypical attention\nnetworks. Extensive experiments on several benchmark datasets show the\neffectiveness of our proposed method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19907v1",
    "published_date": "2024-03-29 01:25:05 UTC",
    "updated_date": "2024-03-29 01:25:05 UTC"
  },
  {
    "arxiv_id": "2403.19905v1",
    "title": "Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models",
    "authors": [
      "Inas Al-Kamachy",
      "Reza Hassanpour",
      "Roya Choupani"
    ],
    "abstract": "Diabetic Retinopathy (DR) stands as the leading cause of blindness globally,\nparticularly affecting individuals between the ages of 20 and 70. This paper\npresents a Computer-Aided Diagnosis (CAD) system designed for the automatic\nclassification of retinal images into five distinct classes: Normal, Mild,\nModerate, Severe, and Proliferative Diabetic Retinopathy (PDR). The proposed\nsystem leverages Convolutional Neural Networks (CNNs) employing pre-trained\ndeep learning models. Through the application of fine-tuning techniques, our\nmodel is trained on fundus images of diabetic retinopathy with resolutions of\n350x350x3 and 224x224x3. Experimental results obtained on the Kaggle platform,\nutilizing resources comprising 4 CPUs, 17 GB RAM, and 1 GB Disk, demonstrate\nthe efficacy of our approach. The achieved Area Under the Curve (AUC) values\nfor CNN, MobileNet, VGG-16, InceptionV3, and InceptionResNetV2 models are 0.50,\n0.70, 0.53, 0.63, and 0.69, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07"
    ],
    "primary_category": "cs.CV",
    "comment": "3 pages, 1 figure, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2403.19905v1",
    "published_date": "2024-03-29 01:11:56 UTC",
    "updated_date": "2024-03-29 01:11:56 UTC"
  },
  {
    "arxiv_id": "2403.19889v1",
    "title": "Towards a Robust Retrieval-Based Summarization System",
    "authors": [
      "Shengjie Liu",
      "Jing Wu",
      "Jingyuan Bao",
      "Wenyi Wang",
      "Naira Hovakimyan",
      "Christopher G Healey"
    ],
    "abstract": "This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19889v1",
    "published_date": "2024-03-29 00:14:46 UTC",
    "updated_date": "2024-03-29 00:14:46 UTC"
  },
  {
    "arxiv_id": "2404.00076v2",
    "title": "A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks",
    "authors": [
      "Orson Mengara"
    ],
    "abstract": "Audio-based machine learning systems frequently use public or third-party\ndata, which might be inaccurate. This exposes deep neural network (DNN) models\ntrained on such data to potential data poisoning attacks. In this type of\nassault, attackers can train the DNN model using poisoned data, potentially\ndegrading its performance. Another type of data poisoning attack that is\nextremely relevant to our investigation is label flipping, in which the\nattacker manipulates the labels for a subset of data. It has been demonstrated\nthat these assaults may drastically reduce system performance, even for\nattackers with minimal abilities. In this study, we propose a backdoor attack\nnamed 'DirtyFlipping', which uses dirty label techniques, \"label-on-label\", to\ninput triggers (clapping) in the selected data patterns associated with the\ntarget class, thereby enabling a stealthy backdoor.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.CR",
    "comment": "Accept by \"IEEE Access\" let's take a look at our global approach to\n  the DNN(s) model(s) deployment chain in production: Danger NLP-Speech\n  (Trigger universal approach)",
    "pdf_url": "http://arxiv.org/pdf/2404.00076v2",
    "published_date": "2024-03-29 00:09:48 UTC",
    "updated_date": "2024-04-07 04:38:37 UTC"
  },
  {
    "arxiv_id": "2403.19888v4",
    "title": "MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection",
    "authors": [
      "Ali Behrouz",
      "Michele Santacatterina",
      "Ramin Zabih"
    ],
    "abstract": "Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.19888v4",
    "published_date": "2024-03-29 00:05:13 UTC",
    "updated_date": "2024-07-23 21:33:06 UTC"
  }
]
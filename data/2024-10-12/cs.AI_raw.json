[
  {
    "arxiv_id": "2410.09671v1",
    "title": "OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models",
    "authors": [
      "Jun Wang",
      "Meng Fang",
      "Ziyu Wan",
      "Muning Wen",
      "Jiachen Zhu",
      "Anjie Liu",
      "Ziqin Gong",
      "Yan Song",
      "Lei Chen",
      "Lionel M. Ni",
      "Linyi Yang",
      "Ying Wen",
      "Weinan Zhang"
    ],
    "abstract": "In this technical report, we introduce OpenR, an open-source framework\ndesigned to integrate key components for enhancing the reasoning capabilities\nof large language models (LLMs). OpenR unifies data acquisition, reinforcement\nlearning training (both online and offline), and non-autoregressive decoding\ninto a cohesive software platform. Our goal is to establish an open-source\nplatform and community to accelerate the development of LLM reasoning. Inspired\nby the success of OpenAI's o1 model, which demonstrated improved reasoning\nabilities through step-by-step reasoning and reinforcement learning, OpenR\nintegrates test-time compute, reinforcement learning, and process supervision\nto improve reasoning in LLMs. Our work is the first to provide an open-source\nframework that explores the core techniques of OpenAI's o1 model with\nreinforcement learning, achieving advanced reasoning capabilities beyond\ntraditional autoregressive methods. We demonstrate the efficacy of OpenR by\nevaluating it on the MATH dataset, utilising publicly available data and search\nmethods. Our initial experiments confirm substantial gains, with relative\nimprovements in reasoning and performance driven by test-time computation and\nreinforcement learning through process reward models. The OpenR framework,\nincluding code, models, and datasets, is accessible at\nhttps://openreasoner.github.io.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09671v1",
    "published_date": "2024-10-12 23:42:16 UTC",
    "updated_date": "2024-10-12 23:42:16 UTC"
  },
  {
    "arxiv_id": "2410.09656v1",
    "title": "LSTM-Based Proactive Congestion Management for Internet of Vehicle Networks",
    "authors": [
      "Aly Sabri Abdalla",
      "Ahmad Al-Kabbany",
      "Ehab F. Badran",
      "Vuk Marojevic"
    ],
    "abstract": "Vehicle-to-everything (V2X) networks support a variety of safety,\nentertainment, and commercial applications. This is realized by applying the\nprinciples of the Internet of Vehicles (IoV) to facilitate connectivity among\nvehicles and between vehicles and roadside units (RSUs). Network congestion\nmanagement is essential for IoVs and it represents a significant concern due to\nits impact on improving the efficiency of transportation systems and providing\nreliable communication among vehicles for the timely delivery of\nsafety-critical packets. This paper introduces a framework for proactive\ncongestion management for IoV networks. We generate congestion scenarios and a\ndata set to predict the congestion using LSTM. We present the framework and the\npacket congestion dataset. Simulation results using SUMO with NS3 demonstrate\nthe effectiveness of the framework for forecasting IoV network congestion and\nclustering/prioritizing packets employing recurrent neural networks.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "This article has been accepted for publication in the IEEE VTC Fall\n  2024",
    "pdf_url": "http://arxiv.org/pdf/2410.09656v1",
    "published_date": "2024-10-12 21:21:42 UTC",
    "updated_date": "2024-10-12 21:21:42 UTC"
  },
  {
    "arxiv_id": "2410.09652v1",
    "title": "Survival of the Safest: Towards Secure Prompt Optimization through Interleaved Multi-Objective Evolution",
    "authors": [
      "Ankita Sinha",
      "Wendi Cui",
      "Kamalika Das",
      "Jiaxin Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities;\nhowever, the optimization of their prompts has historically prioritized\nperformance metrics at the expense of crucial safety and security\nconsiderations. To overcome this shortcoming, we introduce \"Survival of the\nSafest\" (SoS), an innovative multi-objective prompt optimization framework that\nenhances both performance and security in LLMs simultaneously. SoS utilizes an\ninterleaved multi-objective evolution strategy, integrating semantic, feedback,\nand crossover mutations to effectively traverse the prompt landscape. Differing\nfrom the computationally demanding Pareto front methods, SoS provides a\nscalable solution that expedites optimization in complex, high-dimensional\ndiscrete search spaces while keeping computational demands low. Our approach\naccommodates flexible weighting of objectives and generates a pool of optimized\ncandidates, empowering users to select prompts that optimally meet their\nspecific performance and security needs. Experimental evaluations across\ndiverse benchmark datasets affirm SoS's efficacy in delivering high performance\nand notably enhancing safety and security compared to single-objective methods.\nThis advancement marks a significant stride towards the deployment of LLM\nsystems that are both high-performing and secure across varied industrial\napplications",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.CR",
    "comment": "EMNLP 2024 Industry Track",
    "pdf_url": "http://arxiv.org/pdf/2410.09652v1",
    "published_date": "2024-10-12 21:16:29 UTC",
    "updated_date": "2024-10-12 21:16:29 UTC"
  },
  {
    "arxiv_id": "2410.09643v1",
    "title": "Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions",
    "authors": [
      "Abdullah Mamun",
      "Krista S. Leonard",
      "Megan E. Petrov",
      "Matthew P. Buman",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Objective: This research aims to develop a lifestyle intervention system,\ncalled MoveSense, that forecasts a patient's activity behavior to allow for\nearly and personalized interventions in real-world clinical environments.\nMethods: We conducted two clinical studies involving 58 prediabetic veterans\nand 60 patients with obstructive sleep apnea to gather multimodal behavioral\ndata using wearable devices. We develop multimodal long short-term memory\n(LSTM) network models, which are capable of forecasting the number of step\ncounts of a patient up to 24 hours in advance by examining data from activity\nand engagement modalities. Furthermore, we design goal-based forecasting models\nto predict whether a person's next-day steps will be over a certain threshold.\nResults: Multimodal LSTM with early fusion achieves 33% and 37% lower mean\nabsolute errors than linear regression and ARIMA respectively on the\nprediabetes dataset. LSTM also outperforms linear regression and ARIMA with a\nmargin of 13% and 32% on the sleep dataset. Multimodal forecasting models also\nperform with 72% and 79% accuracy on the prediabetes dataset and sleep dataset\nrespectively on goal-based forecasting. Conclusion: Our experiments conclude\nthat multimodal LSTM models with early fusion are better than multimodal LSTM\nwith late fusion and unimodal LSTM models and also than ARIMA and linear\nregression models. Significance: We address an important and challenging task\nof time-series forecasting in uncontrolled environments. Effective forecasting\nof a person's physical activity can aid in designing adaptive behavioral\ninterventions to keep the user engaged and adherent to a prescribed routine.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09643v1",
    "published_date": "2024-10-12 20:44:00 UTC",
    "updated_date": "2024-10-12 20:44:00 UTC"
  },
  {
    "arxiv_id": "2410.10899v2",
    "title": "GPTON: Generative Pre-trained Transformers enhanced with Ontology Narration for accurate annotation of biological data",
    "authors": [
      "Rongbin Li",
      "Wenbo Chen",
      "Jinbo Li",
      "Hanwen Xing",
      "Hua Xu",
      "Zhao Li",
      "W. Jim Zheng"
    ],
    "abstract": "By leveraging GPT-4 for ontology narration, we developed GPTON to infuse\nstructured knowledge into LLMs through verbalized ontology terms, achieving\naccurate text and ontology annotations for over 68% of gene sets in the top\nfive predictions. Manual evaluations confirm GPTON's robustness, highlighting\nits potential to harness LLMs and structured knowledge to significantly advance\nbiomedical research beyond gene set annotation.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "J.3; I.2.7"
    ],
    "primary_category": "q-bio.QM",
    "comment": "25 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.10899v2",
    "published_date": "2024-10-12 20:27:05 UTC",
    "updated_date": "2024-10-17 04:08:57 UTC"
  },
  {
    "arxiv_id": "2410.09638v1",
    "title": "On Goodhart's law, with an application to value alignment",
    "authors": [
      "El-Mahdi El-Mhamdi",
      "Lê-Nguyên Hoang"
    ],
    "abstract": "``When a measure becomes a target, it ceases to be a good measure'', this\nadage is known as {\\it Goodhart's law}. In this paper, we investigate formally\nthis law and prove that it critically depends on the tail distribution of the\ndiscrepancy between the true goal and the measure that is optimized.\nDiscrepancies with long-tail distributions favor a Goodhart's law, that is, the\noptimization of the measure can have a counter-productive effect on the goal.\n  We provide a formal setting to assess Goodhart's law by studying the\nasymptotic behavior of the correlation between the goal and the measure, as the\nmeasure is optimized. Moreover, we introduce a distinction between a {\\it weak}\nGoodhart's law, when over-optimizing the metric is useless for the true goal,\nand a {\\it strong} Goodhart's law, when over-optimizing the metric is harmful\nfor the true goal. A distinction which we prove to depend on the tail\ndistribution.\n  We stress the implications of this result to large-scale decision making and\npolicies that are (and have to be) based on metrics, and propose numerous\nresearch directions to better assess the safety of such policies in general,\nand to the particularly concerning case where these policies are automated with\nalgorithms.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "47 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09638v1",
    "published_date": "2024-10-12 20:26:08 UTC",
    "updated_date": "2024-10-12 20:26:08 UTC"
  },
  {
    "arxiv_id": "2410.09637v3",
    "title": "ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models",
    "authors": [
      "Nandan Kumar Jha",
      "Brandon Reagen"
    ],
    "abstract": "LayerNorm is a critical component in modern large language models (LLMs) for\nstabilizing training and ensuring smooth optimization. However, it introduces\nsignificant challenges in mechanistic interpretability, outlier feature\nsuppression, faithful signal propagation, and computational and communication\ncomplexity of private inference. This work explores desirable activation\nfunctions in normalization-free decoder-only LLMs. Contrary to the conventional\npreference for the GELU in transformer-based models, our empirical findings\ndemonstrate an {\\em opposite trend} -- ReLU significantly outperforms GELU in\nLayerNorm-free models, leading to an {\\bf 8.2\\%} perplexity improvement. We\ndiscover a key issue with GELU, where early layers experience entropic\noverload, leading to the under-utilization of the representational capacity of\nattention heads. This highlights that smoother activations like GELU are {\\em\nill-suited} for LayerNorm-free architectures, whereas ReLU's geometrical\nproperties -- specialization in input space and intra-class selectivity -- lead\nto improved learning dynamics and better information retention in the absence\nof LayerNorm. This study offers key insights for optimizing transformer\narchitectures where LayerNorm introduces significant challenges. The code and\nimplementation are available at\nhttps://github.com/Nandan91/relu-revival-normfree",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024 Workshop on Attributing Model Behavior at\n  Scale (Camera-ready version)",
    "pdf_url": "http://arxiv.org/pdf/2410.09637v3",
    "published_date": "2024-10-12 20:26:01 UTC",
    "updated_date": "2024-11-16 17:59:35 UTC"
  },
  {
    "arxiv_id": "2410.09636v1",
    "title": "Can We Estimate Purchase Intention Based on Zero-shot Speech Emotion Recognition?",
    "authors": [
      "Ryotaro Nagase",
      "Takashi Sumiyoshi",
      "Natsuo Yamashita",
      "Kota Dohi",
      "Yohei Kawaguchi"
    ],
    "abstract": "This paper proposes a zero-shot speech emotion recognition (SER) method that\nestimates emotions not previously defined in the SER model training.\nConventional methods are limited to recognizing emotions defined by a single\nword. Moreover, we have the motivation to recognize unknown bipolar emotions\nsuch as ``I want to buy - I do not want to buy.'' In order to allow the model\nto define classes using sentences freely and to estimate unknown bipolar\nemotions, our proposed method expands upon the contrastive language-audio\npre-training (CLAP) framework by introducing multi-class and multi-task\nsettings. We also focus on purchase intention as a bipolar emotion and\ninvestigate the model's performance to zero-shot estimate it. This study is the\nfirst attempt to estimate purchase intention from speech directly. Experiments\nconfirm that the results of zero-shot estimation by the proposed method are at\nthe same level as those of the model trained by supervised learning.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 3 figures, accepted for APSIPA 2024 ASC",
    "pdf_url": "http://arxiv.org/pdf/2410.09636v1",
    "published_date": "2024-10-12 20:25:16 UTC",
    "updated_date": "2024-10-12 20:25:16 UTC"
  },
  {
    "arxiv_id": "2410.09635v1",
    "title": "Use of What-if Scenarios to Help Explain Artificial Intelligence Models for Neonatal Health",
    "authors": [
      "Abdullah Mamun",
      "Lawrence D. Devoe",
      "Mark I. Evans",
      "David W. Britt",
      "Judith Klein-Seetharaman",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Early detection of intrapartum risk enables interventions to potentially\nprevent or mitigate adverse labor outcomes such as cerebral palsy. Currently,\nthere is no accurate automated system to predict such events to assist with\nclinical decision-making. To fill this gap, we propose \"Artificial Intelligence\n(AI) for Modeling and Explaining Neonatal Health\" (AIMEN), a deep learning\nframework that not only predicts adverse labor outcomes from maternal, fetal,\nobstetrical, and intrapartum risk factors but also provides the model's\nreasoning behind the predictions made. The latter can provide insights into\nwhat modifications in the input variables of the model could have changed the\npredicted outcome. We address the challenges of imbalance and small datasets by\nsynthesizing additional training data using Adaptive Synthetic Sampling\n(ADASYN) and Conditional Tabular Generative Adversarial Networks (CTGAN). AIMEN\nuses an ensemble of fully-connected neural networks as the backbone for its\nclassification with the data augmentation supported by either ADASYN or CTGAN.\nAIMEN, supported by CTGAN, outperforms AIMEN supported by ADASYN in\nclassification. AIMEN can predict a high risk for adverse labor outcomes with\nan average F1 score of 0.784. It also provides counterfactual explanations that\ncan be achieved by changing 2 to 3 attributes on average. Resources available:\nhttps://github.com/ab9mamun/AIMEN.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09635v1",
    "published_date": "2024-10-12 20:21:00 UTC",
    "updated_date": "2024-10-12 20:21:00 UTC"
  },
  {
    "arxiv_id": "2410.09629v1",
    "title": "Synthetic Knowledge Ingestion: Towards Knowledge Refinement and Injection for Enhancing Large Language Models",
    "authors": [
      "Jiaxin Zhang",
      "Wendi Cui",
      "Yiran Huang",
      "Kamalika Das",
      "Sricharan Kumar"
    ],
    "abstract": "Large language models (LLMs) are proficient in capturing factual knowledge\nacross various domains. However, refining their capabilities on previously seen\nknowledge or integrating new knowledge from external sources remains a\nsignificant challenge. In this work, we propose a novel synthetic knowledge\ningestion method called Ski, which leverages fine-grained synthesis,\ninterleaved generation, and assemble augmentation strategies to construct\nhigh-quality data representations from raw knowledge sources. We then integrate\nSki and its variations with three knowledge injection techniques: Retrieval\nAugmented Generation (RAG), Supervised Fine-tuning (SFT), and Continual\nPre-training (CPT) to inject and refine knowledge in language models. Extensive\nempirical experiments are conducted on various question-answering tasks\nspanning finance, biomedicine, and open-generation domains to demonstrate that\nSki significantly outperforms baseline methods by facilitating effective\nknowledge injection. We believe that our work is an important step towards\nenhancing the factual accuracy of LLM outputs by refining knowledge\nrepresentation and injection capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 main conference long paper",
    "pdf_url": "http://arxiv.org/pdf/2410.09629v1",
    "published_date": "2024-10-12 19:38:09 UTC",
    "updated_date": "2024-10-12 19:38:09 UTC"
  },
  {
    "arxiv_id": "2410.12861v2",
    "title": "Scaled and Inter-token Relation Enhanced Transformer for Sample-restricted Residential NILM",
    "authors": [
      "Minhajur Rahman",
      "Yasir Arafat"
    ],
    "abstract": "Transformers have demonstrated exceptional performance across various domains\ndue to their self-attention mechanism, which captures complex relationships in\ndata. However, training on smaller datasets poses challenges, as standard\nattention mechanisms can over-smooth attention scores and overly prioritize\nintra-token relationships, reducing the capture of meaningful inter-token\ndependencies critical for tasks like Non-Intrusive Load Monitoring (NILM). To\naddress this, we propose a novel transformer architecture with two key\ninnovations: inter-token relation enhancement and dynamic temperature tuning.\nThe inter-token relation enhancement mechanism removes diagonal entries in the\nsimilarity matrix to improve attention focus on inter-token relations. The\ndynamic temperature tuning mechanism, a learnable parameter, adapts attention\nsharpness during training, preventing over-smoothing and enhancing sensitivity\nto token relationships. We validate our method on the REDD dataset and show\nthat it outperforms the original transformer and state-of-the-art models by\n10-15\\% in F1 score across various appliance types, demonstrating its efficacy\nfor training on smaller datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to 27th IEEE-ICCIT",
    "pdf_url": "http://arxiv.org/pdf/2410.12861v2",
    "published_date": "2024-10-12 18:58:45 UTC",
    "updated_date": "2024-12-06 19:24:54 UTC"
  },
  {
    "arxiv_id": "2410.09615v2",
    "title": "SLiM: One-shot Quantization and Sparsity with Low-rank Approximation for LLM Weight Compression",
    "authors": [
      "Mohammad Mozaffari",
      "Amir Yazdanbakhsh",
      "Maryam Mehri Dehnavi"
    ],
    "abstract": "Conventional model compression techniques for LLMs address high memory\nconsumption and slow inference challenges but typically require computationally\nexpensive retraining to preserve accuracy. In contrast, one-shot compression\nmethods eliminate retraining cost, but struggle to achieve accuracy comparable\nto dense models. This paper presents SLIM, a new one-shot compression framework\nthat holistically integrates hardware-friendly quantization, sparsity, and\nlow-rank approximation into a unified process. First, we formulate the\nquantization process using a probabilistic approach (SLIM-Quant) that enables\nus to apply uniform quantization. Then, we use an existing one-shot pruning\nmethod to apply semi-structured sparsity on top of the quantized weights.\nFinally, to compensate for the introduced aggregated quantization and sparsity\nerror, we use a novel saliency function with unique invertible and additive\nfeatures that enables us to mathematically compute the value of low-rank\nadapters. SLIM improves model accuracy by up to 5.66% (LLaMA-2-7B) for 2:4\nsparsity with 4-bit weight quantization, outperforming prior methods. Models\ncompressed with SLIM achieve up to 3.78x and 3.75x layer-wise speedup on Nvidia\nRTX3060 and A100 GPUs, respectively. We also propose an optional PEFT recipe\nthat further improves accuracy by up to 1.66% (LLaMA-2-13B) compared to SLIM\nwithout fine-tuning",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09615v2",
    "published_date": "2024-10-12 18:36:07 UTC",
    "updated_date": "2025-02-04 01:30:52 UTC"
  },
  {
    "arxiv_id": "2410.09613v1",
    "title": "Transformer-based Language Models for Reasoning in the Description Logic ALCQ",
    "authors": [
      "Angelos Poulis",
      "Eleni Tsalapati",
      "Manolis Koubarakis"
    ],
    "abstract": "Recent advancements in transformer-based language models have sparked\nresearch into their logical reasoning capabilities. Most of the benchmarks used\nto evaluate these models are simple: generated from short (fragments of)\nfirst-order logic sentences with only a few logical operators and quantifiers.\nWe construct the natural language dataset, DELTA$_D$, using the expressive\ndescription logic language $\\mathcal{ALCQ}$. DELTA$_D$ comprises 384K examples\nand increases in two dimensions: i) reasoning depth, and ii) linguistic\ncomplexity. In this way, we systematically investigate the logical reasoning\ncapabilities of a supervised fine-tuned DeBERTa-based model and two large\nlanguage models (GPT-3.5, GPT-4) with few-shot prompting. We show that the\nDeBERTa-based model fine-tuned on our dataset can master the entailment\nchecking task. Moreover, the performance of GPTs can improve significantly even\nwhen a small number of samples is provided (9 shots). We open-source our code\nand datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
    "pdf_url": "http://arxiv.org/pdf/2410.09613v1",
    "published_date": "2024-10-12 18:25:34 UTC",
    "updated_date": "2024-10-12 18:25:34 UTC"
  },
  {
    "arxiv_id": "2410.09609v1",
    "title": "Traversing Emotional Landscapes and Linguistic Patterns in Bernard-Marie Koltès' Plays: An NLP Perspective",
    "authors": [
      "Arezou Zahiri Pourzarandi",
      "Farshad Jafari"
    ],
    "abstract": "This study employs Natural Language Processing (NLP) to analyze the intricate\nlinguistic and emotional dimensions within the plays of Bernard-Marie Kolt\\`es,\na central figure in contemporary French theatre. By integrating advanced\ncomputational techniques, we dissect Kolt\\`es' narrative style, revealing the\nsubtle interplay between language and emotion across his dramatic oeuvre. Our\nfindings highlight how Kolt\\`es crafts his narratives, enriching our\nunderstanding of his thematic explorations and contributing to the broader\nfield of digital humanities in literary analysis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09609v1",
    "published_date": "2024-10-12 18:13:47 UTC",
    "updated_date": "2024-10-12 18:13:47 UTC"
  },
  {
    "arxiv_id": "2410.09604v1",
    "title": "EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment",
    "authors": [
      "Chen Gao",
      "Baining Zhao",
      "Weichen Zhang",
      "Jinzhu Mao",
      "Jun Zhang",
      "Zhiheng Zheng",
      "Fanhang Man",
      "Jianjie Fang",
      "Zile Zhou",
      "Jinqiang Cui",
      "Xinlei Chen",
      "Yong Li"
    ],
    "abstract": "Embodied artificial intelligence emphasizes the role of an agent's body in\ngenerating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of\nattention to building up machine learning models to possess perceiving,\nplanning, and acting abilities, thereby enabling real-time interaction with the\nworld. However, most works focus on bounded indoor environments, such as\nnavigation in a room or manipulating a device, with limited exploration of\nembodying the agents in open-world scenarios. That is, embodied intelligence in\nthe open and outdoor environment is less explored, for which one potential\nreason is the lack of high-quality simulators, benchmarks, and datasets. To\naddress it, in this paper, we construct a benchmark platform for embodied\nintelligence evaluation in real-world city environments. Specifically, we first\nconstruct a highly realistic 3D simulation environment based on the real\nbuildings, roads, and other elements in a real city. In this environment, we\ncombine historically collected data and simulation algorithms to conduct\nsimulations of pedestrian and vehicle flows with high fidelity. Further, we\ndesigned a set of evaluation tasks covering different EmbodiedAI abilities.\nMoreover, we provide a complete set of input and output interfaces for access,\nenabling embodied agents to easily take task requirements and current\nenvironmental observations as input and then make decisions and obtain\nperformance evaluations. On the one hand, it expands the capability of existing\nembodied intelligence to higher levels. On the other hand, it has a higher\npractical value in the real world and can support more potential applications\nfor artificial general intelligence. Based on this platform, we evaluate some\npopular large language models for embodied intelligence capabilities of\ndifferent dimensions and difficulties.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "All of the software, Python library, codes, datasets, tutorials, and\n  real-time online service are available on this website:\n  https://embodied-city.fiblab.net",
    "pdf_url": "http://arxiv.org/pdf/2410.09604v1",
    "published_date": "2024-10-12 17:49:26 UTC",
    "updated_date": "2024-10-12 17:49:26 UTC"
  },
  {
    "arxiv_id": "2410.09597v2",
    "title": "A Complete Characterization of Learnability for Stochastic Noisy Bandits",
    "authors": [
      "Steve Hanneke",
      "Kun Wang"
    ],
    "abstract": "We study the stochastic noisy bandit problem with an unknown reward function\n$f^*$ in a known function class $\\mathcal{F}$. Formally, a model $M$ maps arms\n$\\pi$ to a probability distribution $M(\\pi)$ of reward. A model class\n$\\mathcal{M}$ is a collection of models. For each model $M$, define its mean\nreward function $f^M(\\pi)=\\mathbb{E}_{r \\sim M(\\pi)}[r]$. In the bandit\nlearning problem, we proceed in rounds, pulling one arm $\\pi$ each round and\nobserving a reward sampled from $M(\\pi)$. With knowledge of $\\mathcal{M}$,\nsupposing that the true model $M\\in \\mathcal{M}$, the objective is to identify\nan arm $\\hat{\\pi}$ of near-maximal mean reward $f^M(\\hat{\\pi})$ with high\nprobability in a bounded number of rounds. If this is possible, then the model\nclass is said to be learnable.\n  Importantly, a result of \\cite{hanneke2023bandit} shows there exist model\nclasses for which learnability is undecidable. However, the model class they\nconsider features deterministic rewards, and they raise the question of whether\nlearnability is decidable for classes containing sufficiently noisy models. For\nthe first time, we answer this question in the positive by giving a complete\ncharacterization of learnability for model classes with arbitrary noise. In\naddition to that, we also describe the full spectrum of possible optimal query\ncomplexities. Further, we prove adaptivity is sometimes necessary to achieve\nthe optimal query complexity. Last, we revisit an important complexity measure\nfor interactive decision making, the Decision-Estimation-Coefficient\n\\citep{foster2021statistical,foster2023tight}, and propose a new variant of the\nDEC which also characterizes learnability in this setting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09597v2",
    "published_date": "2024-10-12 17:23:34 UTC",
    "updated_date": "2025-01-17 00:25:18 UTC"
  },
  {
    "arxiv_id": "2410.09592v1",
    "title": "ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model",
    "authors": [
      "Hongbin Xu",
      "Weitao Chen",
      "Zhipeng Zhou",
      "Feng Xiao",
      "Baigui Sun",
      "Mike Zheng Shou",
      "Wenxiong Kang"
    ],
    "abstract": "Despite recent advancements in 3D generation methods, achieving\ncontrollability still remains a challenging issue. Current approaches utilizing\nscore-distillation sampling are hindered by laborious procedures that consume a\nsignificant amount of time. Furthermore, the process of first generating 2D\nrepresentations and then mapping them to 3D lacks internal alignment between\nthe two forms of representation. To address these challenges, we introduce\nControLRM, an end-to-end feed-forward model designed for rapid and controllable\n3D generation using a large reconstruction model (LRM). ControLRM comprises a\n2D condition generator, a condition encoding transformer, and a triplane\ndecoder transformer. Instead of training our model from scratch, we advocate\nfor a joint training framework. In the condition training branch, we lock the\ntriplane decoder and reuses the deep and robust encoding layers pretrained with\nmillions of 3D data in LRM. In the image training branch, we unlock the\ntriplane decoder to establish an implicit alignment between the 2D and 3D\nrepresentations. To ensure unbiased evaluation, we curate evaluation samples\nfrom three distinct datasets (G-OBJ, GSO, ABO) rather than relying on\ncherry-picking manual generation. The comprehensive experiments conducted on\nquantitative and qualitative comparisons of 3D controllability and generation\nquality demonstrate the strong generalization capacity of our proposed\napproach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Draft version. This paper is still in submission. For access to our\n  project page and code, please visit:\n  https://toughstonex.github.io/controlrm.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.09592v1",
    "published_date": "2024-10-12 16:47:20 UTC",
    "updated_date": "2024-10-12 16:47:20 UTC"
  },
  {
    "arxiv_id": "2410.09584v1",
    "title": "Toward General Instruction-Following Alignment for Retrieval-Augmented Generation",
    "authors": [
      "Guanting Dong",
      "Xiaoshuai Song",
      "Yutao Zhu",
      "Runqi Qiao",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Following natural instructions is crucial for the effective application of\nRetrieval-Augmented Generation (RAG) systems. Despite recent advancements in\nLarge Language Models (LLMs), research on assessing and improving\ninstruction-following (IF) alignment within the RAG domain remains limited. To\naddress this issue, we propose VIF-RAG, the first automated, scalable, and\nverifiable synthetic pipeline for instruction-following alignment in RAG\nsystems. We start by manually crafting a minimal set of atomic instructions\n(<100) and developing combination rules to synthesize and verify complex\ninstructions for a seed set. We then use supervised models for instruction\nrewriting while simultaneously generating code to automate the verification of\ninstruction quality via a Python executor. Finally, we integrate these\ninstructions with extensive RAG and general data samples, scaling up to a\nhigh-quality VIF-RAG-QA dataset (>100k) through automated processes. To further\nbridge the gap in instruction-following auto-evaluation for RAG systems, we\nintroduce FollowRAG Benchmark, which includes approximately 3K test samples,\ncovering 22 categories of general instruction constraints and four\nknowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG\ncan seamlessly integrate with different RAG benchmarks. Using FollowRAG and\neight widely-used IF and foundational abilities benchmarks for LLMs, we\ndemonstrate that VIF-RAG markedly enhances LLM performance across a broad range\nof general instruction constraints while effectively leveraging its\ncapabilities in RAG scenarios. Further analysis offers practical insights for\nachieving IF alignment in RAG systems. Our code and datasets are released at\nhttps://FollowRAG.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Working in progress",
    "pdf_url": "http://arxiv.org/pdf/2410.09584v1",
    "published_date": "2024-10-12 16:30:51 UTC",
    "updated_date": "2024-10-12 16:30:51 UTC"
  },
  {
    "arxiv_id": "2410.09582v1",
    "title": "Improving 3D Finger Traits Recognition via Generalizable Neural Rendering",
    "authors": [
      "Hongbin Xu",
      "Junduan Huang",
      "Yuer Ma",
      "Zifeng Li",
      "Wenxiong Kang"
    ],
    "abstract": "3D biometric techniques on finger traits have become a new trend and have\ndemonstrated a powerful ability for recognition and anti-counterfeiting.\nExisting methods follow an explicit 3D pipeline that reconstructs the models\nfirst and then extracts features from 3D models. However, these explicit 3D\nmethods suffer from the following problems: 1) Inevitable information dropping\nduring 3D reconstruction; 2) Tight coupling between specific hardware and\nalgorithm for 3D reconstruction. It leads us to a question: Is it indispensable\nto reconstruct 3D information explicitly in recognition tasks? Hence, we\nconsider this problem in an implicit manner, leaving the nerve-wracking 3D\nreconstruction problem for learnable neural networks with the help of neural\nradiance fields (NeRFs). We propose FingerNeRF, a novel generalizable NeRF for\n3D finger biometrics. To handle the shape-radiance ambiguity problem that may\nresult in incorrect 3D geometry, we aim to involve extra geometric priors based\non the correspondence of binary finger traits like fingerprints or finger\nveins. First, we propose a novel Trait Guided Transformer (TGT) module to\nenhance the feature correspondence with the guidance of finger traits. Second,\nwe involve extra geometric constraints on the volume rendering loss with the\nproposed Depth Distillation Loss and Trait Guided Rendering Loss. To evaluate\nthe performance of the proposed method on different modalities, we collect two\nnew datasets: SCUT-Finger-3D with finger images and SCUT-FingerVein-3D with\nfinger vein images. Moreover, we also utilize the UNSW-3D dataset with\nfingerprint images for evaluation. In experiments, our FingerNeRF can achieve\n4.37% EER on SCUT-Finger-3D dataset, 8.12% EER on SCUT-FingerVein-3D dataset,\nand 2.90% EER on UNSW-3D dataset, showing the superiority of the proposed\nimplicit method in 3D finger biometrics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted in IJCV. For further information and access to\n  the code, please visit our project page:\n  https://scut-bip-lab.github.io/fingernerf/",
    "pdf_url": "http://arxiv.org/pdf/2410.09582v1",
    "published_date": "2024-10-12 16:27:21 UTC",
    "updated_date": "2024-10-12 16:27:21 UTC"
  },
  {
    "arxiv_id": "2410.19764v2",
    "title": "Unraveling Movie Genres through Cross-Attention Fusion of Bi-Modal Synergy of Poster",
    "authors": [
      "Utsav Kumar Nareti",
      "Chandranath Adak",
      "Soumi Chattopadhyay",
      "Pichao Wang"
    ],
    "abstract": "Movie posters are not just decorative; they are meticulously designed to\ncapture the essence of a movie, such as its genre, storyline, and tone/vibe.\nFor decades, movie posters have graced cinema walls, billboards, and now our\ndigital screens as a form of digital posters. Movie genre classification plays\na pivotal role in film marketing, audience engagement, and recommendation\nsystems. Previous explorations into movie genre classification have been mostly\nexamined in plot summaries, subtitles, trailers and movie scenes. Movie posters\nprovide a pre-release tantalizing glimpse into a film's key aspects, which can\nignite public interest. In this paper, we presented the framework that exploits\nmovie posters from a visual and textual perspective to address the multilabel\nmovie genre classification problem. Firstly, we extracted text from movie\nposters using an OCR and retrieved the relevant embedding. Next, we introduce a\ncross-attention-based fusion module to allocate attention weights to visual and\ntextual embedding. In validating our framework, we utilized 13882 posters\nsourced from the Internet Movie Database (IMDb). The outcomes of the\nexperiments indicate that our model exhibited promising performance and\noutperformed even some prominent contemporary architectures.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19764v2",
    "published_date": "2024-10-12 16:14:18 UTC",
    "updated_date": "2024-11-30 07:06:57 UTC"
  },
  {
    "arxiv_id": "2410.09579v1",
    "title": "Structure of Artificial Neural Networks -- Empirical Investigations",
    "authors": [
      "Julian Stier"
    ],
    "abstract": "Within one decade, Deep Learning overtook the dominating solution methods of\ncountless problems of artificial intelligence. ``Deep'' refers to the deep\narchitectures with operations in manifolds of which there are no immediate\nobservations. For these deep architectures some kind of structure is\npre-defined -- but what is this structure? With a formal definition for\nstructures of neural networks, neural architecture search problems and solution\nmethods can be formulated under a common framework. Both practical and\ntheoretical questions arise from closing the gap between applied neural\narchitecture search and learning theory. Does structure make a difference or\ncan it be chosen arbitrarily?\n  This work is concerned with deep structures of artificial neural networks and\nexamines automatic construction methods under empirical principles to shed\nlight on to the so called ``black-box models''.\n  Our contributions include a formulation of graph-induced neural networks that\nis used to pose optimisation problems for neural architecture. We analyse\nstructural properties for different neural network objectives such as\ncorrectness, robustness or energy consumption and discuss how structure affects\nthem. Selected automation methods for neural architecture optimisation problems\nare discussed and empirically analysed. With the insights gained from\nformalising graph-induced neural networks, analysing structural properties and\ncomparing the applicability of neural architecture search methods qualitatively\nand quantitatively we advance these methods in two ways. First, new predictive\nmodels are presented for replacing computationally expensive evaluation\nschemes, and second, new generative models for informed sampling during neural\narchitecture search are analysed and discussed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2410.09579v1",
    "published_date": "2024-10-12 16:13:28 UTC",
    "updated_date": "2024-10-12 16:13:28 UTC"
  },
  {
    "arxiv_id": "2410.09576v1",
    "title": "The Future of Learning in the Age of Generative AI: Automated Question Generation and Assessment with Large Language Models",
    "authors": [
      "Subhankar Maity",
      "Aniket Deroy"
    ],
    "abstract": "In recent years, large language models (LLMs) and generative AI have\nrevolutionized natural language processing (NLP), offering unprecedented\ncapabilities in education. This chapter explores the transformative potential\nof LLMs in automated question generation and answer assessment. It begins by\nexamining the mechanisms behind LLMs, emphasizing their ability to comprehend\nand generate human-like text. The chapter then discusses methodologies for\ncreating diverse, contextually relevant questions, enhancing learning through\ntailored, adaptive strategies. Key prompting techniques, such as zero-shot and\nchain-of-thought prompting, are evaluated for their effectiveness in generating\nhigh-quality questions, including open-ended and multiple-choice formats in\nvarious languages. Advanced NLP methods like fine-tuning and prompt-tuning are\nexplored for their role in generating task-specific questions, despite\nassociated costs. The chapter also covers the human evaluation of generated\nquestions, highlighting quality variations across different methods and areas\nfor improvement. Furthermore, it delves into automated answer assessment,\ndemonstrating how LLMs can accurately evaluate responses, provide constructive\nfeedback, and identify nuanced understanding or misconceptions. Examples\nillustrate both successful assessments and areas needing improvement. The\ndiscussion underscores the potential of LLMs to replace costly, time-consuming\nhuman assessments when appropriately guided, showcasing their advanced\nunderstanding and reasoning capabilities in streamlining educational processes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Book Chapter (Under Review)",
    "pdf_url": "http://arxiv.org/pdf/2410.09576v1",
    "published_date": "2024-10-12 15:54:53 UTC",
    "updated_date": "2024-10-12 15:54:53 UTC"
  },
  {
    "arxiv_id": "2410.09575v2",
    "title": "Reconstructive Visual Instruction Tuning",
    "authors": [
      "Haochen Wang",
      "Anlin Zheng",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Zhaoxiang Zhang"
    ],
    "abstract": "This paper introduces reconstructive visual instruction tuning (ROSS), a\nfamily of Large Multimodal Models (LMMs) that exploit vision-centric\nsupervision signals. In contrast to conventional visual instruction tuning\napproaches that exclusively supervise text outputs, ROSS prompts LMMs to\nsupervise visual outputs via reconstructing input images. By doing so, it\ncapitalizes on the inherent richness and detail present within input images\nthemselves, which are often lost in pure text supervision. However, producing\nmeaningful feedback from natural images is challenging due to the heavy spatial\nredundancy of visual signals. To address this issue, ROSS employs a denoising\nobjective to reconstruct latent representations of input images, avoiding\ndirectly regressing exact raw RGB values. This intrinsic activation design\ninherently encourages LMMs to maintain image detail, thereby enhancing their\nfine-grained comprehension capabilities and reducing hallucinations.\nEmpirically, ROSS consistently brings significant improvements across different\nvisual encoders and language models. In comparison with extrinsic assistance\nstate-of-the-art alternatives that aggregate multiple visual experts, ROSS\ndelivers competitive performance with a single SigLIP visual encoder,\ndemonstrating the efficacy of our vision-centric supervision tailored for\nvisual outputs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09575v2",
    "published_date": "2024-10-12 15:54:29 UTC",
    "updated_date": "2024-12-31 02:38:30 UTC"
  },
  {
    "arxiv_id": "2410.09569v2",
    "title": "Are You Human? An Adversarial Benchmark to Expose LLMs",
    "authors": [
      "Gilad Gressel",
      "Rahul Pankajakshan",
      "Yisroel Mirsky"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated an alarming ability to\nimpersonate humans in conversation, raising concerns about their potential\nmisuse in scams and deception. Humans have a right to know if they are\nconversing to an LLM. We evaluate text-based prompts designed as challenges to\nexpose LLM imposters in real-time. To this end we compile and release an\nopen-source benchmark dataset that includes 'implicit challenges' that exploit\nan LLM's instruction-following mechanism to cause role deviation, and 'exlicit\nchallenges' that test an LLM's ability to perform simple tasks typically easy\nfor humans but difficult for LLMs. Our evaluation of 9 leading models from the\nLMSYS leaderboard revealed that explicit challenges successfully detected LLMs\nin 78.4% of cases, while implicit challenges were effective in 22.9% of\ninstances. User studies validate the real-world applicability of our methods,\nwith humans outperforming LLMs on explicit challenges (78% vs 22% success\nrate). Our framework unexpectedly revealed that many study participants were\nusing LLMs to complete tasks, demonstrating its effectiveness in detecting both\nAI impostors and human misuse of AI tools. This work addresses the critical\nneed for reliable, real-time LLM detection methods in high-stakes\nconversations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09569v2",
    "published_date": "2024-10-12 15:33:50 UTC",
    "updated_date": "2024-12-20 12:25:22 UTC"
  },
  {
    "arxiv_id": "2410.09564v1",
    "title": "Extended Japanese Commonsense Morality Dataset with Masked Token and Label Enhancement",
    "authors": [
      "Takumi Ohashi",
      "Tsubasa Nakagawa",
      "Hitoshi Iyatomi"
    ],
    "abstract": "Rapid advancements in artificial intelligence (AI) have made it crucial to\nintegrate moral reasoning into AI systems. However, existing models and\ndatasets often overlook regional and cultural differences. To address this\nshortcoming, we have expanded the JCommonsenseMorality (JCM) dataset, the only\npublicly available dataset focused on Japanese morality. The Extended JCM\n(eJCM) has grown from the original 13,975 sentences to 31,184 sentences using\nour proposed sentence expansion method called Masked Token and Label\nEnhancement (MTLE). MTLE selectively masks important parts of sentences related\nto moral judgment and replaces them with alternative expressions generated by a\nlarge language model (LLM), while re-assigning appropriate labels. The model\ntrained using our eJCM achieved an F1 score of 0.857, higher than the scores\nfor the original JCM (0.837), ChatGPT one-shot classification (0.841), and data\naugmented using AugGPT, a state-of-the-art augmentation method (0.850).\nSpecifically, in complex moral reasoning tasks unique to Japanese culture, the\nmodel trained with eJCM showed a significant improvement in performance\n(increasing from 0.681 to 0.756) and achieved a performance close to that of\nGPT-4 Turbo (0.787). These results demonstrate the validity of the eJCM dataset\nand the importance of developing models and datasets that consider the cultural\ncontext.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09564v1",
    "published_date": "2024-10-12 15:21:40 UTC",
    "updated_date": "2024-10-12 15:21:40 UTC"
  },
  {
    "arxiv_id": "2410.09543v1",
    "title": "Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions",
    "authors": [
      "Xiaoran Jiao",
      "Weian Mao",
      "Wengong Jin",
      "Peiyuan Yang",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "abstract": "Predicting the change in binding free energy ($\\Delta \\Delta G$) is crucial\nfor understanding and modulating protein-protein interactions, which are\ncritical in drug design. Due to the scarcity of experimental $\\Delta \\Delta G$\ndata, existing methods focus on pre-training, while neglecting the importance\nof alignment. In this work, we propose the Boltzmann Alignment technique to\ntransfer knowledge from pre-trained inverse folding models to $\\Delta \\Delta G$\nprediction. We begin by analyzing the thermodynamic definition of $\\Delta\n\\Delta G$ and introducing the Boltzmann distribution to connect energy with\nprotein conformational distribution. However, the protein conformational\ndistribution is intractable; therefore, we employ Bayes' theorem to circumvent\ndirect estimation and instead utilize the log-likelihood provided by protein\ninverse folding models for $\\Delta \\Delta G$ estimation. Compared to previous\ninverse folding-based methods, our method explicitly accounts for the unbound\nstate of protein complex in the $\\Delta \\Delta G$ thermodynamic cycle,\nintroducing a physical inductive bias and achieving both supervised and\nunsupervised state-of-the-art (SoTA) performance. Experimental results on\nSKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201\n(unsupervised) and 0.5134 (supervised), significantly surpassing the previously\nreported SoTA values of 0.2632 and 0.4324, respectively. Futhermore, we\ndemonstrate the capability of our method on binding energy prediction,\nprotein-protein docking and antibody optimization tasks.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09543v1",
    "published_date": "2024-10-12 14:13:42 UTC",
    "updated_date": "2024-10-12 14:13:42 UTC"
  },
  {
    "arxiv_id": "2410.09542v2",
    "title": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models",
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Zhuoran Jin",
      "Yubo Chen",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Inductive reasoning is an essential capability for large language models\n(LLMs) to achieve higher intelligence, which requires the model to generalize\nrules from observed facts and then apply them to unseen examples. We present\nMIRAGE, a synthetic dataset that addresses the limitations of previous work,\nspecifically the lack of comprehensive evaluation and flexible test data. In\nit, we evaluate LLMs' capabilities in both the inductive and deductive stages,\nallowing for flexible variation in input distribution, task scenario, and task\ndifficulty to analyze the factors influencing LLMs' inductive reasoning. Based\non these multi-faceted evaluations, we demonstrate that the LLM is a poor\nrule-based reasoner. In many cases, when conducting inductive reasoning, they\ndo not rely on a correct rule to answer the unseen case. From the perspectives\nof different prompting methods, observation numbers, and task forms, models\ntend to consistently conduct correct deduction without correct inductive rules.\nBesides, we find that LLMs are good neighbor-based reasoners. In the inductive\nreasoning process, the model tends to focus on observed facts that are close to\nthe current test example in feature space. By leveraging these similar\nexamples, the model maintains strong inductive capabilities within a localized\nregion, significantly improving its deductive performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as ICLR 2025 conference paper (26 pages, 16 tables, 9\n  figures)",
    "pdf_url": "http://arxiv.org/pdf/2410.09542v2",
    "published_date": "2024-10-12 14:12:36 UTC",
    "updated_date": "2025-02-28 08:01:32 UTC"
  },
  {
    "arxiv_id": "2410.09541v1",
    "title": "LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Chenhao Wang",
      "Zhuoran Jin",
      "Yubo Chen",
      "Kang Liu",
      "Xiaojian Jiang",
      "Jiexin Xu",
      "Jun Zhao"
    ],
    "abstract": "Large language models (LLMs) sometimes demonstrate poor performance on\nknowledge-intensive tasks, commonsense reasoning is one of them. Researchers\ntypically address these issues by retrieving related knowledge from knowledge\ngraphs or employing self-enhancement methods to elicit knowledge in LLMs.\nHowever, noisy knowledge and invalid reasoning issues hamper their ability to\nanswer questions accurately. To this end, we propose a novel method named\neliciting, filtering and integrating knowledge in large language model\n(LINKED). In it, we design a reward model to filter out the noisy knowledge and\ntake the marginal consistent reasoning module to reduce invalid reasoning. With\nour comprehensive experiments on two complex commonsense reasoning benchmarks,\nour method outperforms SOTA baselines (up to 9.0% improvement of accuracy).\nBesides, to measure the positive and negative impact of the injected knowledge,\nwe propose a new metric called effectiveness-preservation score for the\nknowledge enhancement works. Finally, through extensive experiments, we conduct\nan in-depth analysis and find many meaningful conclusions about LLMs in\ncommonsense reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.09541v1",
    "published_date": "2024-10-12 14:12:22 UTC",
    "updated_date": "2024-10-12 14:12:22 UTC"
  },
  {
    "arxiv_id": "2410.09531v1",
    "title": "PrivQuant: Communication-Efficient Private Inference with Quantized Network/Protocol Co-Optimization",
    "authors": [
      "Tianshi Xu",
      "Shuzhang Zhong",
      "Wenxuan Zeng",
      "Runsheng Wang",
      "Meng Li"
    ],
    "abstract": "Private deep neural network (DNN) inference based on secure two-party\ncomputation (2PC) enables secure privacy protection for both the server and the\nclient. However, existing secure 2PC frameworks suffer from a high inference\nlatency due to enormous communication. As the communication of both linear and\nnon-linear DNN layers reduces with the bit widths of weight and activation, in\nthis paper, we propose PrivQuant, a framework that jointly optimizes the\n2PC-based quantized inference protocols and the network quantization algorithm,\nenabling communication-efficient private inference. PrivQuant proposes DNN\narchitecture-aware optimizations for the 2PC protocols for\ncommunication-intensive quantized operators and conducts graph-level operator\nfusion for communication reduction. Moreover, PrivQuant also develops a\ncommunication-aware mixed precision quantization algorithm to improve inference\nefficiency while maintaining high accuracy. The network/protocol\nco-optimization enables PrivQuant to outperform prior-art 2PC frameworks. With\nextensive experiments, we demonstrate PrivQuant reduces communication by\n$11\\times, 2.5\\times \\mathrm{and}~ 2.8\\times$, which results in $8.7\\times,\n1.8\\times ~ \\mathrm{and}~ 2.4\\times$ latency reduction compared with SiRNN,\nCOINN, and CoPriv, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "ICCAD 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.09531v1",
    "published_date": "2024-10-12 13:28:42 UTC",
    "updated_date": "2024-10-12 13:28:42 UTC"
  },
  {
    "arxiv_id": "2410.09529v1",
    "title": "Preserving Old Memories in Vivid Detail: Human-Interactive Photo Restoration Framework",
    "authors": [
      "Seung-Yeon Back",
      "Geonho Son",
      "Dahye Jeong",
      "Eunil Park",
      "Simon S. Woo"
    ],
    "abstract": "Photo restoration technology enables preserving visual memories in\nphotographs. However, physical prints are vulnerable to various forms of\ndeterioration, ranging from physical damage to loss of image quality, etc.\nWhile restoration by human experts can improve the quality of outcomes, it\noften comes at a high price in terms of cost and time for restoration. In this\nwork, we present the AI-based photo restoration framework composed of multiple\nstages, where each stage is tailored to enhance and restore specific types of\nphoto damage, accelerating and automating the photo restoration process. By\nintegrating these techniques into a unified architecture, our framework aims to\noffer a one-stop solution for restoring old and deteriorated photographs.\nFurthermore, we present a novel old photo restoration dataset because we lack a\npublicly available dataset for our evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09529v1",
    "published_date": "2024-10-12 13:23:08 UTC",
    "updated_date": "2024-10-12 13:23:08 UTC"
  },
  {
    "arxiv_id": "2410.09528v2",
    "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
    "authors": [
      "Jialian Li",
      "Yipin Zhang",
      "Wei Shen",
      "Yuzi Yan",
      "Jian Xie",
      "Dong Yan"
    ],
    "abstract": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09528v2",
    "published_date": "2024-10-12 13:19:11 UTC",
    "updated_date": "2024-10-24 09:36:53 UTC"
  },
  {
    "arxiv_id": "2410.10896v2",
    "title": "AT-MoE: Adaptive Task-planning Mixture of Experts via LoRA Approach",
    "authors": [
      "Xurui Li",
      "Juanjuan Yao"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has ushered in a new era of\nartificial intelligence, with the potential to transform various sectors\nthrough automation and insightful analysis. The Mixture of Experts (MoE)\narchitecture has been proposed as a solution to enhance model performance in\ncomplex tasks. Yet, existing MoE models struggle with task-specific learning\nand interpretability, especially in fields like medicine where precision is\ncritical. This paper introduces the Adaptive Task-planing Mixture of\nExperts(AT-MoE), an innovative architecture designed to address these\nlimitations. We first train task-specific experts via LoRA approach to enhance\nproblem-solving capabilities and interpretability in specialized areas.\nSubsequently, we introduce a layer-wise adaptive grouped routing module that\noptimizes module fusion based on complex task instructions, ensuring optimal\ntask resolution. The grouped routing module first perform overall weight\nallocation from the dimension of the expert group, and then conduct local\nweight normalization adjustments within the group. This design maintains\nmulti-dimensional balance, controllability, and interpretability, while\nfacilitating task-specific fusion in response to complex instructions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.10896v2",
    "published_date": "2024-10-12 13:03:15 UTC",
    "updated_date": "2024-10-19 02:24:07 UTC"
  },
  {
    "arxiv_id": "2410.09519v1",
    "title": "Pic@Point: Cross-Modal Learning by Local and Global Point-Picture Correspondence",
    "authors": [
      "Vencia Herzog",
      "Stefan Suwelack"
    ],
    "abstract": "Self-supervised pre-training has achieved remarkable success in NLP and 2D\nvision. However, these advances have yet to translate to 3D data. Techniques\nlike masked reconstruction face inherent challenges on unstructured point\nclouds, while many contrastive learning tasks lack in complexity and\ninformative value. In this paper, we present Pic@Point, an effective\ncontrastive learning method based on structural 2D-3D correspondences. We\nleverage image cues rich in semantic and contextual knowledge to provide a\nguiding signal for point cloud representations at various abstraction levels.\nOur lightweight approach outperforms state-of-the-art pre-training methods on\nseveral 3D benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ACML 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.09519v1",
    "published_date": "2024-10-12 12:43:41 UTC",
    "updated_date": "2024-10-12 12:43:41 UTC"
  },
  {
    "arxiv_id": "2410.09514v1",
    "title": "Eco-Aware Graph Neural Networks for Sustainable Recommendations",
    "authors": [
      "Antonio Purificato",
      "Fabrizio Silvestri"
    ],
    "abstract": "Recommender systems play a crucial role in alleviating information overload\nby providing personalized recommendations tailored to users' preferences and\ninterests. Recently, Graph Neural Networks (GNNs) have emerged as a promising\napproach for recommender systems, leveraging their ability to effectively\ncapture complex relationships and dependencies between users and items by\nrepresenting them as nodes in a graph structure. In this study, we investigate\nthe environmental impact of GNN-based recommender systems, an aspect that has\nbeen largely overlooked in the literature. Specifically, we conduct a\ncomprehensive analysis of the carbon emissions associated with training and\ndeploying GNN models for recommendation tasks. We evaluate the energy\nconsumption and carbon footprint of different GNN architectures and\nconfigurations, considering factors such as model complexity, training\nduration, hardware specifications and embedding size. By addressing the\nenvironmental impact of resource-intensive algorithms in recommender systems,\nthis study contributes to the ongoing efforts towards sustainable and\nresponsible artificial intelligence, promoting the development of eco-friendly\nrecommendation technologies that balance performance and environmental\nconsiderations. Code is available at:\nhttps://github.com/antoniopurificato/gnn_recommendation_and_environment.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 2 tables, 3 figures, RecSoGood Workshop",
    "pdf_url": "http://arxiv.org/pdf/2410.09514v1",
    "published_date": "2024-10-12 12:26:04 UTC",
    "updated_date": "2024-10-12 12:26:04 UTC"
  },
  {
    "arxiv_id": "2410.09506v1",
    "title": "Distribution-Aware Mean Estimation under User-level Local Differential Privacy",
    "authors": [
      "Corentin Pla",
      "Hugo Richard",
      "Maxime Vono"
    ],
    "abstract": "We consider the problem of mean estimation under user-level local\ndifferential privacy, where $n$ users are contributing through their local pool\nof data samples. Previous work assume that the number of data samples is the\nsame across users. In contrast, we consider a more general and realistic\nscenario where each user $u \\in [n]$ owns $m_u$ data samples drawn from some\ngenerative distribution $\\mu$; $m_u$ being unknown to the statistician but\ndrawn from a known distribution $M$ over $\\mathbb{N}^\\star$. Based on a\ndistribution-aware mean estimation algorithm, we establish an $M$-dependent\nupper bounds on the worst-case risk over $\\mu$ for the task of mean estimation.\nWe then derive a lower bound. The two bounds are asymptotically matching up to\nlogarithmic factors and reduce to known bounds when $m_u = m$ for any user $u$.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME",
    "comment": "25 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2410.09506v1",
    "published_date": "2024-10-12 11:57:52 UTC",
    "updated_date": "2024-10-12 11:57:52 UTC"
  },
  {
    "arxiv_id": "2410.09491v1",
    "title": "Dying Clusters Is All You Need -- Deep Clustering With an Unknown Number of Clusters",
    "authors": [
      "Collin Leiber",
      "Niklas Strauß",
      "Matthias Schubert",
      "Thomas Seidl"
    ],
    "abstract": "Finding meaningful groups, i.e., clusters, in high-dimensional data such as\nimages or texts without labeled data at hand is an important challenge in data\nmining. In recent years, deep clustering methods have achieved remarkable\nresults in these tasks. However, most of these methods require the user to\nspecify the number of clusters in advance. This is a major limitation since the\nnumber of clusters is typically unknown if labeled data is unavailable. Thus,\nan area of research has emerged that addresses this problem. Most of these\napproaches estimate the number of clusters separated from the clustering\nprocess. This results in a strong dependency of the clustering result on the\nquality of the initial embedding. Other approaches are tailored to specific\nclustering processes, making them hard to adapt to other scenarios. In this\npaper, we propose UNSEEN, a general framework that, starting from a given upper\nbound, is able to estimate the number of clusters. To the best of our\nknowledge, it is the first method that can be easily combined with various deep\nclustering algorithms. We demonstrate the applicability of our approach by\ncombining UNSEEN with the popular deep clustering algorithms DCN, DEC, and DKM\nand verify its effectiveness through an extensive experimental evaluation on\nseveral image and tabular datasets. Moreover, we perform numerous ablations to\nanalyze our approach and show the importance of its components. The code is\navailable at: https://github.com/collinleiber/UNSEEN",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Acceppted at the Sixth ICDM Workshop on Deep Learning and Clustering",
    "pdf_url": "http://arxiv.org/pdf/2410.09491v1",
    "published_date": "2024-10-12 11:04:10 UTC",
    "updated_date": "2024-10-12 11:04:10 UTC"
  },
  {
    "arxiv_id": "2410.09474v3",
    "title": "Distilling Invariant Representations with Dual Augmentation",
    "authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "abstract": "Knowledge distillation (KD) has been widely used to transfer knowledge from\nlarge, accurate models (teachers) to smaller, efficient ones (students). Recent\nmethods have explored enforcing consistency by incorporating causal\ninterpretations to distill invariant representations. In this work, we extend\nthis line of research by introducing a dual augmentation strategy to promote\ninvariant feature learning in both teacher and student models. Our approach\nleverages different augmentations applied to both models during distillation,\npushing the student to capture robust, transferable features. This dual\naugmentation strategy complements invariant causal distillation by ensuring\nthat the learned representations remain stable across a wider range of data\nvariations and transformations. Extensive experiments on CIFAR-100 demonstrate\nthe effectiveness of this approach, achieving competitive results in\nsame-architecture KD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T07",
      "I.4; I.2"
    ],
    "primary_category": "cs.CV",
    "comment": "Not completed work",
    "pdf_url": "http://arxiv.org/pdf/2410.09474v3",
    "published_date": "2024-10-12 10:27:23 UTC",
    "updated_date": "2024-12-20 22:10:44 UTC"
  },
  {
    "arxiv_id": "2410.09472v2",
    "title": "DRCap: Decoding CLAP Latents with Retrieval-Augmented Generation for Zero-shot Audio Captioning",
    "authors": [
      "Xiquan Li",
      "Wenxi Chen",
      "Ziyang Ma",
      "Xuenan Xu",
      "Yuzhe Liang",
      "Zhisheng Zheng",
      "Qiuqiang Kong",
      "Xie Chen"
    ],
    "abstract": "While automated audio captioning (AAC) has made notable progress, traditional\nfully supervised AAC models still face two critical challenges: the need for\nexpensive audio-text pair data for training and performance degradation when\ntransferring across domains. To overcome these limitations, we present DRCap, a\ndata-efficient and flexible zero-shot audio captioning system that requires\ntext-only data for training and can quickly adapt to new domains without\nadditional fine-tuning. DRCap integrates a contrastive language-audio\npre-training (CLAP) model and a large-language model (LLM) as its backbone.\nDuring training, the model predicts the ground-truth caption with a fixed text\nencoder from CLAP, whereas, during inference, the text encoder is replaced with\nthe audio encoder to generate captions for audio clips in a zero-shot manner.\nTo mitigate the modality gap of the CLAP model, we use both the projection\nstrategy from the encoder side and the retrieval-augmented generation strategy\nfrom the decoder side. Specifically, audio embeddings are first projected onto\na text embedding support to absorb extensive semantic information within the\njoint multi-modal space of CLAP. At the same time, similar captions retrieved\nfrom a datastore are fed as prompts to instruct the LLM, incorporating external\nknowledge to take full advantage of its strong generative capability.\nConditioned on both the projected CLAP embedding and the retrieved similar\ncaptions, the model is able to produce a more accurate and semantically rich\ntextual description. By tailoring the text embedding support and the caption\ndatastore to the target domain, DRCap acquires a robust ability to adapt to new\ndomains in a training-free manner. Experimental results demonstrate that DRCap\noutperforms all other zero-shot models in in-domain scenarios and achieves\nstate-of-the-art performance in cross-domain scenarios.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09472v2",
    "published_date": "2024-10-12 10:21:00 UTC",
    "updated_date": "2025-01-06 10:52:02 UTC"
  },
  {
    "arxiv_id": "2410.09463v2",
    "title": "From Theory to Practice: Implementing and Evaluating e-Fold Cross-Validation",
    "authors": [
      "Christopher Mahlich",
      "Tobias Vente",
      "Joeran Beel"
    ],
    "abstract": "This paper introduces e-fold cross-validation, an energy-efficient\nalternative to k-fold cross-validation. It dynamically adjusts the number of\nfolds based on a stopping criterion. The criterion checks after each fold\nwhether the standard deviation of the evaluated folds has consistently\ndecreased or remained stable. Once met, the process stops early. We tested\ne-fold cross-validation on 15 datasets and 10 machine-learning algorithms. On\naverage, it required 4 fewer folds than 10-fold cross-validation, reducing\nevaluation time, computational resources, and energy use by about 40%.\nPerformance differences between e-fold and 10-fold cross-validation were less\nthan 2% for larger datasets. More complex models showed even smaller\ndiscrepancies. In 96% of iterations, the results were within the confidence\ninterval, confirming statistical significance. E-fold cross-validation offers a\nreliable and efficient alternative to k-fold, reducing computational costs\nwhile maintaining comparable accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09463v2",
    "published_date": "2024-10-12 09:56:28 UTC",
    "updated_date": "2024-10-26 18:25:35 UTC"
  },
  {
    "arxiv_id": "2410.09455v1",
    "title": "VERITAS-NLI : Validation and Extraction of Reliable Information Through Automated Scraping and Natural Language Inference",
    "authors": [
      "Arjun Shah",
      "Hetansh Shah",
      "Vedica Bafna",
      "Charmi Khandor",
      "Sindhu Nair"
    ],
    "abstract": "In today's day and age where information is rapidly spread through online\nplatforms, the rise of fake news poses an alarming threat to the integrity of\npublic discourse, societal trust, and reputed news sources. Classical machine\nlearning and Transformer-based models have been extensively studied for the\ntask of fake news detection, however they are hampered by their reliance on\ntraining data and are unable to generalize on unseen headlines. To address\nthese challenges, we propose our novel solution, leveraging web-scraping\ntechniques and Natural Language Inference (NLI) models to retrieve external\nknowledge necessary for verifying the accuracy of a headline. Our system is\nevaluated on a diverse self-curated evaluation dataset spanning over multiple\nnews channels and broad domains. Our best performing pipeline achieves an\naccuracy of 84.3% surpassing the best classical Machine Learning model by 33.3%\nand Bidirectional Encoder Representations from Transformers (BERT) by 31.0% .\nThis highlights the efficacy of combining dynamic web-scraping with Natural\nLanguage Inference to find support for a claimed headline in the corresponding\nexternally retrieved knowledge for the task of fake news detection.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.1; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint, 15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09455v1",
    "published_date": "2024-10-12 09:25:12 UTC",
    "updated_date": "2024-10-12 09:25:12 UTC"
  },
  {
    "arxiv_id": "2410.09453v3",
    "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
    "authors": [
      "Xi Jiang",
      "Jian Li",
      "Hanqiu Deng",
      "Yong Liu",
      "Bin-Bin Gao",
      "Yifeng Zhou",
      "Jialin Li",
      "Chengjie Wang",
      "Feng Zheng"
    ],
    "abstract": "In the field of industrial inspection, Multimodal Large Language Models\n(MLLMs) have a high potential to renew the paradigms in practical applications\ndue to their robust language capabilities and generalization abilities.\nHowever, despite their impressive problem-solving skills in many domains,\nMLLMs' ability in industrial anomaly detection has not been systematically\nstudied. To bridge this gap, we present MMAD, the first-ever full-spectrum\nMLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks\nof MLLMs in industrial inspection and designed a novel pipeline to generate the\nMMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we\nhave conducted a comprehensive, quantitative evaluation of various\nstate-of-the-art MLLMs. The commercial models performed the best, with the\naverage accuracy of GPT-4o models reaching 74.9%. However, this result falls\nfar short of industrial requirements. Our analysis reveals that current MLLMs\nstill have significant room for improvement in answering questions related to\nindustrial anomalies and defects. We further explore two training-free\nperformance enhancement strategies to help models improve in industrial\nscenarios, highlighting their promising potential for future research.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ICLR 2025. The code and data are available at\n  https://github.com/jam-cc/MMAD",
    "pdf_url": "http://arxiv.org/pdf/2410.09453v3",
    "published_date": "2024-10-12 09:16:09 UTC",
    "updated_date": "2025-02-21 04:50:45 UTC"
  },
  {
    "arxiv_id": "2410.09437v3",
    "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
    "authors": [
      "Yaming Yang",
      "Dilxat Muhtar",
      "Yelong Shen",
      "Yuefeng Zhan",
      "Jianfeng Liu",
      "Yujing Wang",
      "Hao Sun",
      "Denvy Deng",
      "Feng Sun",
      "Qi Zhang",
      "Weizhu Chen",
      "Yunhai Tong"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain\nadaptation, with LoRA being one of the most prominent methods due to its\nsimplicity and effectiveness. However, in multi-task learning (MTL) scenarios,\nLoRA tends to obscure the distinction between tasks by projecting sparse\nhigh-dimensional features from different tasks into the same dense\nlow-dimensional intrinsic space. This leads to task interference and suboptimal\nperformance for LoRA and its variants. To tackle this challenge, we propose\nMTL-LoRA, which retains the advantages of low-rank adaptation while\nsignificantly enhancing MTL capabilities. MTL-LoRA augments LoRA by\nincorporating additional task-adaptive parameters that differentiate\ntask-specific information and capture shared knowledge across various tasks\nwithin low-dimensional spaces. This approach enables pre-trained models to\njointly adapt to different target domains with a limited number of trainable\nparameters. Comprehensive experimental results, including evaluations on public\nacademic benchmarks for natural language understanding, commonsense reasoning,\nand image-text understanding, as well as real-world industrial text Ads\nrelevance datasets, demonstrate that MTL-LoRA outperforms LoRA and its various\nvariants with comparable or even fewer learnable parameters in MTL setting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "12 Pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2410.09437v3",
    "published_date": "2024-10-12 08:32:26 UTC",
    "updated_date": "2025-04-01 10:18:48 UTC"
  },
  {
    "arxiv_id": "2410.09428v1",
    "title": "Declarative Knowledge Distillation from Large Language Models for Visual Question Answering Datasets",
    "authors": [
      "Thomas Eiter",
      "Jan Hadl",
      "Nelson Higuera",
      "Johannes Oetsch"
    ],
    "abstract": "Visual Question Answering (VQA) is the task of answering a question about an\nimage and requires processing multimodal input and reasoning to obtain the\nanswer. Modular solutions that use declarative representations within the\nreasoning component have a clear advantage over end-to-end trained systems\nregarding interpretability. The downside is that crafting the rules for such a\ncomponent can be an additional burden on the developer. We address this\nchallenge by presenting an approach for declarative knowledge distillation from\nLarge Language Models (LLMs). Our method is to prompt an LLM to extend an\ninitial theory on VQA reasoning, given as an answer-set program, to meet the\nrequirements of the VQA task. Examples from the VQA dataset are used to guide\nthe LLM, validate the results, and mend rules if they are not correct by using\nfeedback from the ASP solver. We demonstrate that our approach works on the\nprominent CLEVR and GQA datasets. Our results confirm that distilling knowledge\nfrom LLMs is in fact a promising direction besides data-driven rule learning\napproaches.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
    "pdf_url": "http://arxiv.org/pdf/2410.09428v1",
    "published_date": "2024-10-12 08:17:03 UTC",
    "updated_date": "2024-10-12 08:17:03 UTC"
  },
  {
    "arxiv_id": "2410.09416v1",
    "title": "Can Vision-Language Models Replace Human Annotators: A Case Study with CelebA Dataset",
    "authors": [
      "Haoming Lu",
      "Feifei Zhong"
    ],
    "abstract": "This study evaluates the capability of Vision-Language Models (VLMs) in image\ndata annotation by comparing their performance on the CelebA dataset in terms\nof quality and cost-effectiveness against manual annotation. Annotations from\nthe state-of-the-art LLaVA-NeXT model on 1000 CelebA images are in 79.5%\nagreement with the original human annotations. Incorporating re-annotations of\ndisagreed cases into a majority vote boosts AI annotation consistency to 89.1%\nand even higher for more objective labels. Cost assessments demonstrate that AI\nannotation significantly reduces expenditures compared to traditional manual\nmethods -- representing less than 1% of the costs for manual annotation in the\nCelebA dataset. These findings support the potential of VLMs as a viable,\ncost-effective alternative for specific annotation tasks, reducing both\nfinancial burden and ethical concerns associated with large-scale manual data\nannotation. The AI annotations and re-annotations utilized in this study are\navailable on https://github.com/evev2024/EVEV2024_CelebA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024 Workshop (EvalEval 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.09416v1",
    "published_date": "2024-10-12 07:49:08 UTC",
    "updated_date": "2024-10-12 07:49:08 UTC"
  },
  {
    "arxiv_id": "2410.09412v2",
    "title": "FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback",
    "authors": [
      "Youquan Li",
      "Miao Zheng",
      "Fan Yang",
      "Guosheng Dong",
      "Bin Cui",
      "Weipeng Chen",
      "Zenan Zhou",
      "Wentao Zhang"
    ],
    "abstract": "Human feedback is crucial in the interactions between humans and Large\nLanguage Models (LLMs). However, existing research primarily focuses on\nbenchmarking LLMs in single-turn dialogues. Even in benchmarks designed for\nmulti-turn dialogues, the user inputs are often independent, neglecting the\nnuanced and complex nature of human feedback within real-world usage scenarios.\nTo fill this research gap, we introduce FB-Bench, a fine-grained, multi-task\nbenchmark designed to evaluate LLMs' responsiveness to human feedback under\nreal-world usage scenarios in Chinese. Drawing from the two main interaction\nscenarios, FB-Bench comprises 591 meticulously curated samples, encompassing\neight task types, five deficiency types of response, and nine feedback types.\nWe extensively evaluate a broad array of popular LLMs, revealing significant\nvariations in their performance across different interaction scenarios. Further\nanalysis indicates that task, human feedback, and deficiencies of previous\nresponses can also significantly impact LLMs' responsiveness. Our findings\nunderscore both the strengths and limitations of current models, providing\nvaluable insights and directions for future research. Code and datasets are\navailable at https://github.com/PKU-Baichuan-MLSystemLab/FB-Bench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09412v2",
    "published_date": "2024-10-12 07:40:01 UTC",
    "updated_date": "2025-02-17 03:45:48 UTC"
  },
  {
    "arxiv_id": "2410.09407v1",
    "title": "CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device",
    "authors": [
      "Yicheng Fu",
      "Raviteja Anantha",
      "Jianpeng Cheng"
    ],
    "abstract": "While server-side Large Language Models (LLMs) demonstrate proficiency in\nfunction calling and complex reasoning, deploying Small Language Models (SLMs)\ndirectly on devices brings opportunities to improve latency and privacy but\nalso introduces unique challenges for accuracy and memory. We introduce\nCAMPHOR, an innovative on-device SLM multi-agent framework designed to handle\nmultiple user inputs and reason over personal context locally, ensuring privacy\nis maintained. CAMPHOR employs a hierarchical architecture where a high-order\nreasoning agent decomposes complex tasks and coordinates expert agents\nresponsible for personal context retrieval, tool interaction, and dynamic plan\ngeneration. By implementing parameter sharing across agents and leveraging\nprompt compression, we significantly reduce model size, latency, and memory\nusage. To validate our approach, we present a novel dataset capturing\nmulti-agent task trajectories centered on personalized mobile assistant\nuse-cases. Our experiments reveal that fine-tuned SLM agents not only surpass\nclosed-source LLMs in task completion F1 by~35\\% but also eliminate the need\nfor server-device communication, all while enhancing privacy.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09407v1",
    "published_date": "2024-10-12 07:28:10 UTC",
    "updated_date": "2024-10-12 07:28:10 UTC"
  },
  {
    "arxiv_id": "2410.09403v2",
    "title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System",
    "authors": [
      "Haoyang Su",
      "Renqi Chen",
      "Shixiang Tang",
      "Zhenfei Yin",
      "Xinzhe Zheng",
      "Jinzhe Li",
      "Biqing Qi",
      "Qi Wu",
      "Hui Li",
      "Wanli Ouyang",
      "Philip Torr",
      "Bowen Zhou",
      "Nanqing Dong"
    ],
    "abstract": "The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09403v2",
    "published_date": "2024-10-12 07:16:22 UTC",
    "updated_date": "2025-02-19 06:07:47 UTC"
  },
  {
    "arxiv_id": "2410.09401v1",
    "title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
    "authors": [
      "Lixia Zhang",
      "Tianxu Liu",
      "Kaihui Shen",
      "Cheng Chen"
    ],
    "abstract": "With the rapid advancement of Internet technology, the threat of malware to\ncomputer systems and network security has intensified. Malware affects\nindividual privacy and security and poses risks to critical infrastructures of\nenterprises and nations. The increasing quantity and complexity of malware,\nalong with its concealment and diversity, challenge traditional detection\ntechniques. Static detection methods struggle against variants and packed\nmalware, while dynamic methods face high costs and risks that limit their\napplication. Consequently, there is an urgent need for novel and efficient\nmalware detection techniques to improve accuracy and robustness.\n  This study first employs the minhash algorithm to convert binary files of\nmalware into grayscale images, followed by the extraction of global and local\ntexture features using GIST and LBP algorithms. Additionally, the study\nutilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and\ntf-idf algorithms for feature vectorization. The fusion of these features\nenables the model to comprehensively capture the behavioral characteristics of\nmalware.\n  In terms of model construction, a CNN-BiLSTM fusion model is designed to\nsimultaneously process image features and opcode sequences, enhancing\nclassification performance. Experimental validation on multiple public datasets\ndemonstrates that the proposed method significantly outperforms traditional\ndetection techniques in terms of accuracy, recall, and F1 score, particularly\nin detecting variants and obfuscated malware with greater stability.\n  The research presented in this paper offers new insights into the development\nof malware detection technologies, validating the effectiveness of feature and\nmodel fusion, and holds promising application prospects.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09401v1",
    "published_date": "2024-10-12 07:10:44 UTC",
    "updated_date": "2024-10-12 07:10:44 UTC"
  },
  {
    "arxiv_id": "2410.09397v1",
    "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes",
    "authors": [
      "Xiaoyu Li",
      "Yingyu Liang",
      "Zhenmei Shi",
      "Zhao Song",
      "Yufa Zhou"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09397v1",
    "published_date": "2024-10-12 07:01:30 UTC",
    "updated_date": "2024-10-12 07:01:30 UTC"
  },
  {
    "arxiv_id": "2410.09388v3",
    "title": "3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical Information",
    "authors": [
      "Peifan Jiang",
      "Xuben Wang",
      "Shuang Wang",
      "Fei Deng",
      "Kunpeng Wang",
      "Bin Wang",
      "Yuhan Yang"
    ],
    "abstract": "Magnetotelluric deep learning (DL) inversion methods based on joint\ndata-driven and physics-driven have become a hot topic in recent years. When\nmapping observation data (or forward modeling data) to the resistivity model\nusing neural networks (NNs), incorporating the error (loss) term of the\ninversion resistivity's forward modeling response--which introduces physical\ninformation about electromagnetic field propagation--can significantly enhance\nthe inversion accuracy. To efficiently achieve data-physical dual-driven MT\ndeep learning inversion for large-scale 3-D MT data, we propose using DL\nforward modeling networks to compute this portion of the loss. This approach\nintroduces pseudo-physical information through the forward modeling of NN\nsimulation, further guiding the inversion network fitting. Specifically, we\nfirst pre-train the forward modeling networks as fixed forward modeling\noperators, then transfer and integrate them into the inversion network\ntraining, and finally optimize the inversion network by minimizing the\nmultinomial loss. Theoretical experimental results indicate that despite some\nsimulation errors in DL forward modeling, the introduced pseudo-physical\ninformation still enhances inversion accuracy and significantly mitigates the\noverfitting problem during training. Additionally, we propose a new input mode\nthat involves masking and adding noise to the data, simulating the field data\nenvironment of 3-D MT inversion, thereby making the method more flexible and\neffective for practical applications.",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.geo-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09388v3",
    "published_date": "2024-10-12 06:39:31 UTC",
    "updated_date": "2025-05-15 19:17:11 UTC"
  },
  {
    "arxiv_id": "2410.09385v1",
    "title": "Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models",
    "authors": [
      "Sathya Kamesh Bhethanabhotla",
      "Omar Swelam",
      "Julien Siems",
      "David Salinas",
      "Frank Hutter"
    ],
    "abstract": "This paper introduces Mamba4Cast, a zero-shot foundation model for time\nseries forecasting. Based on the Mamba architecture and inspired by Prior-data\nFitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time\nseries tasks without the need for dataset specific fine-tuning. Mamba4Cast's\nkey innovation lies in its ability to achieve strong zero-shot performance on\nreal-world datasets while having much lower inference times than time series\nfoundation models based on the transformer architecture. Trained solely on\nsynthetic data, the model generates forecasts for entire horizons in a single\npass, outpacing traditional auto-regressive approaches. Our experiments show\nthat Mamba4Cast performs competitively against other state-of-the-art\nfoundation models in various data sets while scaling significantly better with\nthe prediction length. The source code can be accessed at\nhttps://github.com/automl/Mamba4Cast.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09385v1",
    "published_date": "2024-10-12 06:35:18 UTC",
    "updated_date": "2024-10-12 06:35:18 UTC"
  },
  {
    "arxiv_id": "2410.09380v1",
    "title": "Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering",
    "authors": [
      "Ting Yu",
      "Kunhao Fu",
      "Shuhui Wang",
      "Qingming Huang",
      "Jun Yu"
    ],
    "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between\nvideo understanding and language processing, requiring both discriminative\nunimodal comprehension and sophisticated cross-modal interaction for accurate\ninference. Despite advancements in multi-modal pre-trained models and\nvideo-language foundation models, these systems often struggle with\ndomain-specific VideoQA due to their generalized pre-training objectives.\nAddressing this gap necessitates bridging the divide between broad cross-modal\nknowledge and the specific inference demands of VideoQA tasks. To this end, we\nintroduce HeurVidQA, a framework that leverages domain-specific entity-action\nheuristics to refine pre-trained video-language foundation models. Our approach\ntreats these models as implicit knowledge engines, employing domain-specific\nentity-action prompters to direct the model's focus toward precise cues that\nenhance reasoning. By delivering fine-grained heuristics, we improve the\nmodel's ability to identify and interpret key entities and actions, thereby\nenhancing its reasoning capabilities. Extensive evaluations across multiple\nVideoQA datasets demonstrate that our method significantly outperforms existing\nmodels, underscoring the importance of integrating domain-specific knowledge\ninto video-language models for more accurate and context-aware VideoQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE Transactions on Circuits and Systems for Video Technology",
    "pdf_url": "http://arxiv.org/pdf/2410.09380v1",
    "published_date": "2024-10-12 06:22:23 UTC",
    "updated_date": "2024-10-12 06:22:23 UTC"
  },
  {
    "arxiv_id": "2410.09379v1",
    "title": "Multi-granularity Contrastive Cross-modal Collaborative Generation for End-to-End Long-term Video Question Answering",
    "authors": [
      "Ting Yu",
      "Kunhao Fu",
      "Jian Zhang",
      "Qingming Huang",
      "Jun Yu"
    ],
    "abstract": "Long-term Video Question Answering (VideoQA) is a challenging\nvision-and-language bridging task focusing on semantic understanding of\nuntrimmed long-term videos and diverse free-form questions, simultaneously\nemphasizing comprehensive cross-modal reasoning to yield precise answers. The\ncanonical approaches often rely on off-the-shelf feature extractors to detour\nthe expensive computation overhead, but often result in domain-independent\nmodality-unrelated representations. Furthermore, the inherent gradient blocking\nbetween unimodal comprehension and cross-modal interaction hinders reliable\nanswer generation. In contrast, recent emerging successful video-language\npre-training models enable cost-effective end-to-end modeling but fall short in\ndomain-specific ratiocination and exhibit disparities in task formulation.\nToward this end, we present an entirely end-to-end solution for long-term\nVideoQA: Multi-granularity Contrastive cross-modal collaborative Generation\n(MCG) model. To derive discriminative representations possessing high visual\nconcepts, we introduce Joint Unimodal Modeling (JUM) on a clip-bone\narchitecture and leverage Multi-granularity Contrastive Learning (MCL) to\nharness the intrinsically or explicitly exhibited semantic correspondences. To\nalleviate the task formulation discrepancy problem, we propose a Cross-modal\nCollaborative Generation (CCG) module to reformulate VideoQA as a generative\ntask instead of the conventional classification scheme, empowering the model\nwith the capability for cross-modal high-semantic fusion and generation so as\nto rationalize and answer. Extensive experiments conducted on six publicly\navailable VideoQA datasets underscore the superiority of our proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Transactions on Image Processing",
    "pdf_url": "http://arxiv.org/pdf/2410.09379v1",
    "published_date": "2024-10-12 06:21:58 UTC",
    "updated_date": "2024-10-12 06:21:58 UTC"
  },
  {
    "arxiv_id": "2410.09375v2",
    "title": "Looped ReLU MLPs May Be All You Need as Practical Programmable Computers",
    "authors": [
      "Yingyu Liang",
      "Zhizhou Sha",
      "Zhenmei Shi",
      "Zhao Song",
      "Yufa Zhou"
    ],
    "abstract": "Previous work has demonstrated that attention mechanisms are Turing complete.\nMore recently, it has been shown that a looped 9-layer Transformer can function\nas a universal programmable computer. In contrast, the multi-layer perceptrons\nwith $\\mathsf{ReLU}$ activation ($\\mathsf{ReLU}$-$\\mathsf{MLP}$), one of the\nmost fundamental components of neural networks, is known to be expressive;\nspecifically, a two-layer neural network is a universal approximator given an\nexponentially large number of hidden neurons. However, it remains unclear\nwhether a $\\mathsf{ReLU}$-$\\mathsf{MLP}$ can be made into a universal\nprogrammable computer using a practical number of weights. In this work, we\nprovide an affirmative answer that a looped 23-layer\n$\\mathsf{ReLU}$-$\\mathsf{MLP}$ is capable of performing the basic necessary\noperations, more efficiently and effectively functioning as a programmable\ncomputer than a looped Transformer. This indicates simple modules have stronger\nexpressive power than previously expected and have not been fully explored. Our\nwork provides insights into the mechanisms of neural networks and demonstrates\nthat complex tasks, such as functioning as a programmable computer, do not\nnecessarily require advanced architectures like Transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.LG",
    "comment": "AIStats 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.09375v2",
    "published_date": "2024-10-12 05:54:17 UTC",
    "updated_date": "2025-02-20 10:24:53 UTC"
  },
  {
    "arxiv_id": "2410.09368v1",
    "title": "Towards a Domain-Specific Modelling Environment for Reinforcement Learning",
    "authors": [
      "Natalie Sinani",
      "Sahil Salma",
      "Paul Boutot",
      "Sadaf Mustafiz"
    ],
    "abstract": "In recent years, machine learning technologies have gained immense popularity\nand are being used in a wide range of domains. However, due to the complexity\nassociated with machine learning algorithms, it is a challenge to make it\nuser-friendly, easy to understand and apply. Machine learning applications are\nespecially challenging for users who do not have proficiency in this area.\n  In this paper, we use model-driven engineering (MDE) methods and tools for\ndeveloping a domain-specific modelling environment to contribute towards\nproviding a solution for this problem. We targeted reinforcement learning from\nthe machine learning domain, and evaluated the proposed language, reinforcement\nlearning modelling language (RLML), with multiple applications. The tool\nsupports syntax-directed editing, constraint checking, and automatic generation\nof code from RLML models. The environment also provides support for comparing\nresults generated with multiple RL algorithms. With our proposed MDE approach,\nwe were able to help in abstracting reinforcement learning technologies and\nimprove the learning curve for RL users.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.09368v1",
    "published_date": "2024-10-12 04:56:01 UTC",
    "updated_date": "2024-10-12 04:56:01 UTC"
  },
  {
    "arxiv_id": "2410.09362v1",
    "title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins",
    "authors": [
      "Jongwoo Ko",
      "Saket Dingliwal",
      "Bhavana Ganesh",
      "Sailik Sengupta",
      "Sravan Bodapati",
      "Aram Galstyan"
    ],
    "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization\n(DPO), have become popular alternatives for Reinforcement Learning from Human\nFeedback (RLHF) due to their simplicity, efficiency, and stability. However,\nthe preferences used in DAAs are usually collected before the alignment\ntraining begins and remain unchanged (off-policy). This can lead to two\nproblems where the policy model (1) picks up on spurious correlations in the\ndataset (as opposed to learning the intended alignment expressed in the human\npreference labels), and (2) overfits to feedback on off-policy trajectories\nthat have less likelihood of being generated by an updated policy model. To\naddress these issues, we introduce Self-Reviewing and Alignment (SeRA), a\ncost-efficient and effective method that can be readily combined with existing\nDAAs. SeRA comprises of two components: (1) sample selection using implicit\nreward margins, which helps alleviate over-fitting to some undesired features,\nand (2) preference bootstrapping using implicit rewards to augment preference\ndata with updated policy models in a cost-efficient manner. Extensive\nexperimentation, including some on instruction-following tasks, demonstrate the\neffectiveness and generality of SeRA in training LLMs on offline preference\ndatasets with DAAs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09362v1",
    "published_date": "2024-10-12 04:17:28 UTC",
    "updated_date": "2024-10-12 04:17:28 UTC"
  },
  {
    "arxiv_id": "2410.09359v2",
    "title": "Green Recommender Systems: Optimizing Dataset Size for Energy-Efficient Algorithm Performance",
    "authors": [
      "Ardalan Arabzadeh",
      "Tobias Vente",
      "Joeran Beel"
    ],
    "abstract": "As recommender systems become increasingly prevalent, the environmental\nimpact and energy efficiency of training large-scale models have come under\nscrutiny. This paper investigates the potential for energy-efficient algorithm\nperformance by optimizing dataset sizes through downsampling techniques in the\ncontext of Green Recommender Systems. We conducted experiments on the MovieLens\n100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of\nvarious recommender algorithms under different portions of dataset size. Our\nresults indicate that while more training data generally leads to higher\nalgorithm performance, certain algorithms, such as FunkSVD and BiasedMF,\nparticularly with unbalanced and sparse datasets like Amazon Toys and Games,\nmaintain high-quality recommendations with up to a 50% reduction in training\ndata, achieving nDCG@10 scores within approximately 13% of full dataset\nperformance. These findings suggest that strategic dataset reduction can\ndecrease computational and environmental costs without substantially\ncompromising recommendation quality. This study advances sustainable and green\nrecommender systems by providing insights for reducing energy consumption while\nmaintaining effectiveness.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09359v2",
    "published_date": "2024-10-12 04:00:55 UTC",
    "updated_date": "2024-11-05 03:45:24 UTC"
  },
  {
    "arxiv_id": "2410.09350v1",
    "title": "Generative Subgraph Retrieval for Knowledge Graph-Grounded Dialog Generation",
    "authors": [
      "Jinyoung Park",
      "Minseok Joo",
      "Joo-Kyung Kim",
      "Hyunwoo J. Kim"
    ],
    "abstract": "Knowledge graph-grounded dialog generation requires retrieving a\ndialog-relevant subgraph from the given knowledge base graph and integrating it\nwith the dialog history. Previous works typically represent the graph using an\nexternal encoder, such as graph neural networks, and retrieve relevant triplets\nbased on the similarity between single-vector representations of triplets and\nthe dialog history. However, these external encoders fail to leverage the rich\nknowledge of pretrained language models, and the retrieval process is also\nsuboptimal due to the information bottleneck caused by the single-vector\nabstraction of the dialog history. In this work, we propose Dialog generation\nwith Generative Subgraph Retrieval (DialogGSR), which retrieves relevant\nknowledge subgraphs by directly generating their token sequences on top of\nlanguage models. For effective generative subgraph retrieval, we introduce two\nkey methods: (i) structure-aware knowledge graph linearization with\nself-supervised graph-specific tokens and (ii) graph-constrained decoding\nutilizing graph structural proximity-based entity informativeness scores for\nvalid and relevant generative retrieval. DialogGSR achieves state-of-the-art\nperformance in knowledge graph-grounded dialog generation, as demonstrated on\nOpenDialKG and KOMODIS datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP (main)",
    "pdf_url": "http://arxiv.org/pdf/2410.09350v1",
    "published_date": "2024-10-12 03:33:42 UTC",
    "updated_date": "2024-10-12 03:33:42 UTC"
  },
  {
    "arxiv_id": "2410.09349v2",
    "title": "Inference and Verbalization Functions During In-Context Learning",
    "authors": [
      "Junyi Tao",
      "Xiaoyin Chen",
      "Nelson F. Liu"
    ],
    "abstract": "Large language models (LMs) are capable of in-context learning from a few\ndemonstrations (example-label pairs) to solve new tasks during inference.\nDespite the intuitive importance of high-quality demonstrations, previous work\nhas observed that, in some settings, ICL performance is minimally affected by\nirrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with\nirrelevant labels via two sequential processes: an inference function that\nsolves the task, followed by a verbalization function that maps the inferred\nanswer to the label space. Importantly, we hypothesize that the inference\nfunction is invariant to remappings of the label space (e.g., \"true\"/\"false\" to\n\"cat\"/\"dog\"), enabling LMs to share the same inference function across settings\nwith different label words. We empirically validate this hypothesis with\ncontrolled layer-wise interchange intervention experiments. Our findings\nconfirm the hypotheses on multiple datasets and tasks (natural language\ninference, sentiment analysis, and topic classification) and further suggest\nthat the two functions can be localized in specific layers across various\nopen-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and\nLLAMA-3.1-70B.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "EMNLP 2024 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.09349v2",
    "published_date": "2024-10-12 03:31:37 UTC",
    "updated_date": "2025-05-17 05:41:08 UTC"
  },
  {
    "arxiv_id": "2410.09345v1",
    "title": "Contrastive Learning for Implicit Social Factors in Social Media Popularity Prediction",
    "authors": [
      "Zhizhen Zhang",
      "Ruihong Qiu",
      "Xiaohui Xie"
    ],
    "abstract": "On social media sharing platforms, some posts are inherently destined for\npopularity. Therefore, understanding the reasons behind this phenomenon and\npredicting popularity before post publication holds significant practical\nvalue. The previous work predominantly focuses on enhancing post content\nextraction for better prediction results. However, certain factors introduced\nby social platforms also impact post popularity, which has not been extensively\nstudied. For instance, users are more likely to engage with posts from\nindividuals they follow, potentially influencing the popularity of these posts.\nWe term these factors, unrelated to the explicit attractiveness of content, as\nimplicit social factors. Through the analysis of users' post browsing behavior\n(also validated in public datasets), we propose three implicit social factors\nrelated to popularity, including content relevance, user influence similarity,\nand user identity. To model the proposed social factors, we introduce three\nsupervised contrastive learning tasks. For different task objectives and data\ntypes, we assign them to different encoders and control their gradient flows to\nachieve joint optimization. We also design corresponding sampling and\naugmentation algorithms to improve the effectiveness of contrastive learning.\nExtensive experiments on the Social Media Popularity Dataset validate the\nsuperiority of our proposed method and also confirm the important role of\nimplicit social factors in popularity prediction. We open source the code at\nhttps://github.com/Daisy-zzz/PPCL.git.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09345v1",
    "published_date": "2024-10-12 03:25:11 UTC",
    "updated_date": "2024-10-12 03:25:11 UTC"
  },
  {
    "arxiv_id": "2410.09344v2",
    "title": "DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
    "authors": [
      "Wenlong Deng",
      "Yize Zhao",
      "Vala Vakilian",
      "Minghui Chen",
      "Xiaoxiao Li",
      "Christos Thrampoulidis"
    ],
    "abstract": "Storing open-source fine-tuned models separately introduces redundancy and\nincreases response times in applications utilizing multiple models.\nDelta-parameter pruning (DPP), particularly the random drop and rescale (DARE)\nmethod proposed by Yu et al., addresses this by pruning the majority of delta\nparameters--the differences between fine-tuned and pre-trained model\nweights--while typically maintaining minimal performance loss. However, DARE\nfails when either the pruning rate or the magnitude of the delta parameters is\nlarge. We highlight two key reasons for this failure: (1) an excessively large\nrescaling factor as pruning rates increase, and (2) high mean and variance in\nthe delta parameters. To push DARE's limits, we introduce DAREx (DARE the\neXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling\nfactor modification that significantly boosts performance at high pruning rates\n(e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in\ndecoder models), and (2) DAREx-L2, which combines DARE with AdamR, an\nin-training method that applies appropriate delta regularization before DPP. We\nalso demonstrate that DAREx-q can be seamlessly combined with vanilla\nparameter-efficient fine-tuning techniques like LoRA and can facilitate\nstructural DPP. Additionally, we revisit the application of importance-based\npruning techniques within DPP, demonstrating that they outperform random-based\nmethods when delta parameters are large. Through this comprehensive study, we\ndevelop a pipeline for selecting the most appropriate DPP method under various\npractical scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09344v2",
    "published_date": "2024-10-12 03:21:58 UTC",
    "updated_date": "2025-04-20 20:53:39 UTC"
  },
  {
    "arxiv_id": "2410.09339v1",
    "title": "Advanced Gesture Recognition in Autism: Integrating YOLOv7, Video Augmentation and VideoMAE for Video Analysis",
    "authors": [
      "Amit Kumar Singh",
      "Trapti Shrivastava",
      "Vrijendra Singh"
    ],
    "abstract": "Deep learning and advancements in contactless sensors have significantly\nenhanced our ability to understand complex human activities in healthcare\nsettings. In particular, deep learning models utilizing computer vision have\nbeen developed to enable detailed analysis of human gesture recognition,\nespecially repetitive gestures which are commonly observed behaviors in\nchildren with autism. This research work aims to identify repetitive behaviors\nindicative of autism by analyzing videos captured in natural settings as\nchildren engage in daily activities. The focus is on accurately categorizing\nreal-time repetitive gestures such as spinning, head banging, and arm flapping.\nTo this end, we utilize the publicly accessible Self-Stimulatory Behavior\nDataset (SSBD) to classify these stereotypical movements. A key component of\nthe proposed methodology is the use of \\textbf{VideoMAE}, a model designed to\nimprove both spatial and temporal analysis of video data through a masking and\nreconstruction mechanism. This model significantly outperformed traditional\nmethods, achieving an accuracy of 97.7\\%, a 14.7\\% improvement over the\nprevious state-of-the-art.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09339v1",
    "published_date": "2024-10-12 02:55:37 UTC",
    "updated_date": "2024-10-12 02:55:37 UTC"
  },
  {
    "arxiv_id": "2410.09335v2",
    "title": "Rethinking Data Selection at Scale: Random Selection is Almost All You Need",
    "authors": [
      "Tingyu Xia",
      "Bowen Yu",
      "Kai Dang",
      "An Yang",
      "Yuan Wu",
      "Yuan Tian",
      "Yi Chang",
      "Junyang Lin"
    ],
    "abstract": "Supervised fine-tuning (SFT) is crucial for aligning Large Language Models\n(LLMs) with human instructions. The primary goal during SFT is to select a\nsmall yet representative subset of training data from the larger pool, such\nthat fine-tuning with this subset achieves results comparable to or even\nexceeding those obtained using the entire dataset. However, most existing data\nselection techniques are designed for small-scale data pools, which fail to\nmeet the demands of real-world SFT scenarios. In this paper, we replicated\nseveral self-scoring methods those that do not rely on external model\nassistance on two million scale datasets, and found that nearly all methods\nstruggled to significantly outperform random selection when dealing with such\nlarge-scale data pools. Moreover, our comparisons suggest that, during SFT,\ndiversity in data selection is more critical than simply focusing on high\nquality data. We also analyzed the limitations of several current approaches,\nexplaining why they perform poorly on large-scale datasets and why they are\nunsuitable for such contexts. Finally, we found that filtering data by token\nlength offers a stable and efficient method for improving results. This\napproach, particularly when training on long text data, proves highly\nbeneficial for relatively weaker base models, such as Llama3.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09335v2",
    "published_date": "2024-10-12 02:48:34 UTC",
    "updated_date": "2024-12-09 09:31:39 UTC"
  },
  {
    "arxiv_id": "2410.19762v1",
    "title": "Reliable, Routable, and Reproducible: Collection of Pedestrian Pathways at Statewide Scale",
    "authors": [
      "Yuxiang Zhang",
      "Bill Howe",
      "Anat Caspi"
    ],
    "abstract": "While advances in mobility technology including autonomous vehicles and\nmulti-modal navigation systems can improve mobility equity for people with\ndisabilities, these technologies depend crucially on accurate, standardized,\nand complete pedestrian path networks. Ad hoc collection efforts lead to a data\nrecord that is sparse, unreliable, and non-interoperable.\n  This paper presents a sociotechnical methodology to collect, manage, serve,\nand maintain pedestrian path data at a statewide scale. Combining the\nautomation afforded by computer-vision approaches applied to aerial imagery and\nexisting road network data with the quality control afforded by interactive\ntools, we aim to produce routable pedestrian pathways for the entire State of\nWashington within approximately two years. We extract paths, crossings, and\ncurb ramps at scale from aerial imagery, integrating multi-input segmentation\nmethods with road topology data to ensure connected, routable networks. We then\norganize the predictions into project regions selected for their value to the\npublic interest, where each project region is divided into intersection-scale\ntasks. These tasks are assigned and tracked through an interactive tool that\nmanages concurrency, progress, feedback, and data management.\n  We demonstrate that our automated systems outperform state-of-the-art methods\nin producing routable pathway networks, which then significantly reduces the\ntime required for human vetting. Our results demonstrate the feasibility of\nyielding accurate, robust pedestrian pathway networks at the scale of an entire\nstate.\n  This paper intends to inform procedures for national-scale ADA compliance by\nproviding pedestrian equity, safety, and accessibility, and improving urban\nenvironments for all users.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2303.02323",
    "pdf_url": "http://arxiv.org/pdf/2410.19762v1",
    "published_date": "2024-10-12 02:31:57 UTC",
    "updated_date": "2024-10-12 02:31:57 UTC"
  },
  {
    "arxiv_id": "2410.09329v1",
    "title": "Zero-shot Commonsense Reasoning over Machine Imagination",
    "authors": [
      "Hyuntae Park",
      "Yeachan Kim",
      "Jun-Hyung Park",
      "SangKeun Lee"
    ],
    "abstract": "Recent approaches to zero-shot commonsense reasoning have enabled Pre-trained\nLanguage Models (PLMs) to learn a broad range of commonsense knowledge without\nbeing tailored to specific situations. However, they often suffer from human\nreporting bias inherent in textual commonsense knowledge, leading to\ndiscrepancies in understanding between PLMs and humans. In this work, we aim to\nbridge this gap by introducing an additional information channel to PLMs. We\npropose Imagine (Machine Imagination-based Reasoning), a novel zero-shot\ncommonsense reasoning framework designed to complement textual inputs with\nvisual signals derived from machine-generated images. To achieve this, we\nenhance PLMs with imagination capabilities by incorporating an image generator\ninto the reasoning process. To guide PLMs in effectively leveraging machine\nimagination, we create a synthetic pre-training dataset that simulates visual\nquestion-answering. Our extensive experiments on diverse reasoning benchmarks\nand analysis show that Imagine outperforms existing methods by a large margin,\nhighlighting the strength of machine imagination in mitigating reporting bias\nand enhancing generalization capabilities.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "21 pages, 9 figures, EMNLP 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2410.09329v1",
    "published_date": "2024-10-12 02:15:11 UTC",
    "updated_date": "2024-10-12 02:15:11 UTC"
  },
  {
    "arxiv_id": "2410.09324v1",
    "title": "Token Pruning using a Lightweight Background Aware Vision Transformer",
    "authors": [
      "Sudhakar Sah",
      "Ravish Kumar",
      "Honnesh Rohmetra",
      "Ehsan Saboori"
    ],
    "abstract": "High runtime memory and high latency puts significant constraint on Vision\nTransformer training and inference, especially on edge devices. Token pruning\nreduces the number of input tokens to the ViT based on importance criteria of\neach token. We present a Background Aware Vision Transformer (BAViT) model, a\npre-processing block to object detection models like DETR/YOLOS aimed to reduce\nruntime memory and increase throughput by using a novel approach to identify\nbackground tokens in the image. The background tokens can be pruned completely\nor partially before feeding to a ViT based object detector. We use the semantic\ninformation provided by segmentation map and/or bounding box annotation to\ntrain a few layers of ViT to classify tokens to either foreground or\nbackground. Using 2 layers and 10 layers of BAViT, background and foreground\ntokens can be separated with 75% and 88% accuracy on VOC dataset and 71% and\n80% accuracy on COCO dataset respectively. We show a 2 layer BAViT-small model\nas pre-processor to YOLOS can increase the throughput by 30% - 40% with a mAP\ndrop of 3% without any sparse fine-tuning and 2% with sparse fine-tuning. Our\napproach is specifically targeted for Edge AI use cases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 2 tables, 4 figures, FITML workshop@NeuRIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.09324v1",
    "published_date": "2024-10-12 01:44:54 UTC",
    "updated_date": "2024-10-12 01:44:54 UTC"
  },
  {
    "arxiv_id": "2410.09319v1",
    "title": "Hey AI Can You Grade My Essay?: Automatic Essay Grading",
    "authors": [
      "Maisha Maliha",
      "Vishal Pramanik"
    ],
    "abstract": "Automatic essay grading (AEG) has attracted the the attention of the NLP\ncommunity because of its applications to several educational applications, such\nas scoring essays, short answers, etc. AEG systems can save significant time\nand money when grading essays. In the existing works, the essays are graded\nwhere a single network is responsible for the whole process, which may be\nineffective because a single network may not be able to learn all the features\nof a human-written essay. In this work, we have introduced a new model that\noutperforms the state-of-the-art models in the field of AEG. We have used the\nconcept of collaborative and transfer learning, where one network will be\nresponsible for checking the grammatical and structural features of the\nsentences of an essay while another network is responsible for scoring the\noverall idea present in the essay. These learnings are transferred to another\nnetwork to score the essay. We also compared the performances of the different\nmodels mentioned in our work, and our proposed model has shown the highest\naccuracy of 85.50%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in ICAAAIML (4th International Conference on Advances and\n  Applications of Artificial Intelligence and Machine Learning) 2023",
    "pdf_url": "http://arxiv.org/pdf/2410.09319v1",
    "published_date": "2024-10-12 01:17:55 UTC",
    "updated_date": "2024-10-12 01:17:55 UTC"
  },
  {
    "arxiv_id": "2410.09314v1",
    "title": "\\llinstruct: An Instruction-tuned model for English Language Proficiency Assessments",
    "authors": [
      "Debanjan Ghosh",
      "Sophia Chan"
    ],
    "abstract": "We present \\llinstruct: An 8B instruction-tuned model that is designed to\ngenerate content for English Language Proficiency Assessments (ELPA) and\nrelated applications. Our work involves creating a new dataset of 70K\ninstructions and explanations in the ELPA domain and using these to fine-tune\nLlama-3 8B models (SFT) of different sizes (e.g., SFT-17K, SFT-50K and\nSFT-70K). Human evaluations are conducted over unseen instructions to compare\nthese SFT models against SOTA models (e.g., Dolly-2, Mistral, Llama-3 base\nversion, and GPT-3.5). The findings show although all three SFT models perform\ncomparably, the model trained on largest instruction dataset -- SFT-70K - leads\nto the most valid outputs ready for assessments. However, although the SFT\nmodels perform better than larger model, e.g., GPT 3.5 on the aspect of\nexplanations of outputs, many outputs still need human interventions to make\nthem actual ready for real world assessments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09314v1",
    "published_date": "2024-10-12 00:47:45 UTC",
    "updated_date": "2024-10-12 00:47:45 UTC"
  },
  {
    "arxiv_id": "2410.09312v2",
    "title": "Towards Multi-Modal Animal Pose Estimation: A Survey and In-Depth Analysis",
    "authors": [
      "Qianyi Deng",
      "Oishi Deb",
      "Amir Patel",
      "Christian Rupprecht",
      "Philip Torr",
      "Niki Trigoni",
      "Andrew Markham"
    ],
    "abstract": "Animal pose estimation (APE) aims to locate the animal body parts using a\ndiverse array of sensor and modality inputs (e.g. RGB cameras, LiDAR, infrared,\nIMU, acoustic and language cues), which is crucial for research across\nneuroscience, biomechanics, and veterinary medicine. By evaluating 176 papers\nsince 2011, APE methods are categorised by their input sensor and modality\ntypes, output forms, learning paradigms, experimental setup, and application\ndomains, presenting detailed analyses of current trends, challenges, and future\ndirections in single- and multi-modality APE systems. The analysis also\nhighlights the transition between human and animal pose estimation, and how\ninnovations in APE can reciprocally enrich human pose estimation and the\nbroader machine learning paradigm. Additionally, 2D and 3D APE datasets and\nevaluation metrics based on different sensors and modalities are provided. A\nregularly updated project page is provided here:\nhttps://github.com/ChennyDeng/MM-APE.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "A.1"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages, 5 figures, 8 tables. Qianyi Deng and Oishi Deb are Joint\n  Major Contributors to this work",
    "pdf_url": "http://arxiv.org/pdf/2410.09312v2",
    "published_date": "2024-10-12 00:37:07 UTC",
    "updated_date": "2025-01-04 20:01:22 UTC"
  },
  {
    "arxiv_id": "2410.09307v1",
    "title": "Graph Neural Alchemist: An innovative fully modular architecture for time series-to-graph classification",
    "authors": [
      "Paulo Coelho",
      "Raul Araju",
      "Luís Ramos",
      "Samir Saliba",
      "Renato Vimieiro"
    ],
    "abstract": "This paper introduces a novel Graph Neural Network (GNN) architecture for\ntime series classification, based on visibility graph representations.\nTraditional time series classification methods often struggle with high\ncomputational complexity and inadequate capture of spatio-temporal dynamics. By\nrepresenting time series as visibility graphs, it is possible to encode both\nspatial and temporal dependencies inherent to time series data, while being\ncomputationally efficient. Our architecture is fully modular, enabling flexible\nexperimentation with different models and representations. We employ directed\nvisibility graphs encoded with in-degree and PageRank features to improve the\nrepresentation of time series, ensuring efficient computation while enhancing\nthe model's ability to capture long-range dependencies in the data. We show the\nrobustness and generalization capability of the proposed architecture across a\ndiverse set of classification tasks and against a traditional model. Our work\nrepresents a significant advancement in the application of GNNs for time series\nanalysis, offering a powerful and flexible framework for future research and\npractical implementations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.09307v1",
    "published_date": "2024-10-12 00:03:40 UTC",
    "updated_date": "2024-10-12 00:03:40 UTC"
  }
]
[
  {
    "arxiv_id": "2412.12417v1",
    "title": "Bridging the Gap: Enhancing LLM Performance for Low-Resource African Languages with New Benchmarks, Fine-Tuning, and Cultural Adjustments",
    "authors": [
      "Tuka Alhanai",
      "Adam Kasumovic",
      "Mohammad Ghassemi",
      "Aven Zitzelberger",
      "Jessica Lundin",
      "Guillaume Chabot-Couture"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable performance across various\ntasks, yet significant disparities remain for non-English languages, and\nespecially native African languages. This paper addresses these disparities by\ncreating approximately 1 million human-translated words of new benchmark data\nin 8 low-resource African languages, covering a population of over 160 million\nspeakers of: Amharic, Bambara, Igbo, Sepedi (Northern Sotho), Shona, Sesotho\n(Southern Sotho), Setswana, and Tsonga. Our benchmarks are translations of\nWinogrande and three sections of MMLU: college medicine, clinical knowledge,\nand virology. Using the translated benchmarks, we report previously unknown\nperformance gaps between state-of-the-art (SOTA) LLMs in English and African\nlanguages. Finally, using results from over 400 fine-tuned models, we explore\nseveral methods to reduce the LLM performance gap, including high-quality\ndataset fine-tuning (using an LLM-as-an-Annotator), cross-lingual transfer, and\ncultural appropriateness adjustments. Key findings include average mono-lingual\nimprovements of 5.6% with fine-tuning (with 5.4% average mono-lingual\nimprovements when using high-quality data over low-quality data), 2.9% average\ngains from cross-lingual transfer, and a 3.0% out-of-the-box performance boost\non culturally appropriate questions. The publicly available benchmarks,\ntranslations, and code from this study support further research and development\naimed at creating more inclusive and effective language technologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI 2025. Main content is 9 pages, 3 figures. Includes\n  supplementary materials",
    "pdf_url": "http://arxiv.org/pdf/2412.12417v1",
    "published_date": "2024-12-16 23:50:21 UTC",
    "updated_date": "2024-12-16 23:50:21 UTC"
  },
  {
    "arxiv_id": "2412.12416v1",
    "title": "DeepSN: A Sheaf Neural Framework for Influence Maximization",
    "authors": [
      "Asela Hevapathige",
      "Qing Wang",
      "Ahad N. Zehmakan"
    ],
    "abstract": "Influence maximization is key topic in data mining, with broad applications\nin social network analysis and viral marketing. In recent years, researchers\nhave increasingly turned to machine learning techniques to address this\nproblem. They have developed methods to learn the underlying diffusion\nprocesses in a data-driven manner, which enhances the generalizability of the\nsolution, and have designed optimization objectives to identify the optimal\nseed set. Nonetheless, two fundamental gaps remain unsolved: (1) Graph Neural\nNetworks (GNNs) are increasingly used to learn diffusion models, but in their\ntraditional form, they often fail to capture the complex dynamics of influence\ndiffusion, (2) Designing optimization objectives is challenging due to\ncombinatorial explosion when solving this problem. To address these challenges,\nwe propose a novel framework, DeepSN. Our framework employs sheaf neural\ndiffusion to learn diverse influence patterns in a data-driven, end-to-end\nmanner, providing enhanced separability in capturing diffusion characteristics.\nWe also propose an optimization technique that accounts for overlapping\ninfluence between vertices, which helps to reduce the search space and identify\nthe optimal seed set effectively and efficiently. Finally, we conduct extensive\nexperiments on both synthetic and real-world datasets to demonstrate the\neffectiveness of our framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12416v1",
    "published_date": "2024-12-16 23:49:51 UTC",
    "updated_date": "2024-12-16 23:49:51 UTC"
  },
  {
    "arxiv_id": "2412.12409v1",
    "title": "Improving Cooperation in Language Games with Bayesian Inference and the Cognitive Hierarchy",
    "authors": [
      "Joseph Bills",
      "Christopher Archibald",
      "Diego Blaylock"
    ],
    "abstract": "In two-player cooperative games, agents can play together effectively when\nthey have accurate assumptions about how their teammate will behave, but may\nperform poorly when these assumptions are inaccurate. In language games,\nfailure may be due to disagreement in the understanding of either the semantics\nor pragmatics of an utterance. We model coarse uncertainty in semantics using a\nprior distribution of language models and uncertainty in pragmatics using the\ncognitive hierarchy, combining the two aspects into a single prior distribution\nover possible partner types. Fine-grained uncertainty in semantics is modeled\nusing noise that is added to the embeddings of words in the language. To handle\nall forms of uncertainty we construct agents that learn the behavior of their\npartner using Bayesian inference and use this information to maximize the\nexpected value of a heuristic function. We test this approach by constructing\nBayesian agents for the game of Codenames, and show that they perform better in\nexperiments where semantics is uncertain",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "Full version of AAAI-25 paper",
    "pdf_url": "http://arxiv.org/pdf/2412.12409v1",
    "published_date": "2024-12-16 23:24:12 UTC",
    "updated_date": "2024-12-16 23:24:12 UTC"
  },
  {
    "arxiv_id": "2412.12408v1",
    "title": "Automated Generation of Massive Reasonable Empirical Theorems by Forward Reasoning Based on Strong Relevant Logics -- A Solution to the Problem of LLM Pre-training Data Exhaustion",
    "authors": [
      "Jingde Cheng"
    ],
    "abstract": "Recently, it is often said that the data used for the pre-training of large\nlanguage models (LLMs) have been exhausted. This paper proposes a solution to\nthe problem: Automated generation of massive reasonable empirical theorems by\nforward reasoning based on strong relevant logics. In fact, this can be\nregarded as a part of our approach to the problems of ATF (Automated Theorem\nFinding) and AKA (Automated Knowledge Appreciation).",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12408v1",
    "published_date": "2024-12-16 23:18:17 UTC",
    "updated_date": "2024-12-16 23:18:17 UTC"
  },
  {
    "arxiv_id": "2412.12395v1",
    "title": "Sound Classification of Four Insect Classes",
    "authors": [
      "Yinxuan Wang",
      "Sudip Vhaduri"
    ],
    "abstract": "The goal of this project is to classify four different insect sounds: cicada,\nbeetle, termite, and cricket. One application of this project is for pest\ncontrol to monitor and protect our ecosystem. Our project leverages data\naugmentation, including pitch shifting and speed changing, to improve model\ngeneralization. This project will test the performance of Decision Tree, Random\nForest, SVM RBF, XGBoost, and k-NN models, combined with MFCC feature. A\npotential novelty of this project is that various data augmentation techniques\nare used and created 6 data along with the original sound. The dataset consists\nof the sound recordings of these four insects. This project aims to achieve a\nhigh classification accuracy and to reduce the over-fitting problem.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "The manuscript is in submission",
    "pdf_url": "http://arxiv.org/pdf/2412.12395v1",
    "published_date": "2024-12-16 23:03:28 UTC",
    "updated_date": "2024-12-16 23:03:28 UTC"
  },
  {
    "arxiv_id": "2412.12385v1",
    "title": "Enhancing Temporal Link Prediction with HierTKG: A Hierarchical Temporal Knowledge Graph Framework",
    "authors": [
      "Mariam Almutairi",
      "Melike Yildiz Aktas",
      "Nawar Wali",
      "Shutonu Mitra",
      "Dawei Zhou"
    ],
    "abstract": "The rapid spread of misinformation on social media, especially during crises,\nchallenges public decision-making. To address this, we propose HierTKG, a\nframework combining Temporal Graph Networks (TGN) and hierarchical pooling\n(DiffPool) to model rumor dynamics across temporal and structural scales.\nHierTKG captures key propagation phases, enabling improved temporal link\nprediction and actionable insights for misinformation control. Experiments\ndemonstrate its effectiveness, achieving an MRR of 0.9845 on ICEWS14 and 0.9312\non WikiData, with competitive performance on noisy datasets like PHEME (MRR:\n0.8802). By modeling structured event sequences and dynamic social\ninteractions, HierTKG adapts to diverse propagation patterns, offering a\nscalable and robust solution for real-time analysis and prediction of rumor\nspread, aiding proactive intervention strategies.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2412.12385v1",
    "published_date": "2024-12-16 22:43:41 UTC",
    "updated_date": "2024-12-16 22:43:41 UTC"
  },
  {
    "arxiv_id": "2412.14205v1",
    "title": "Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat",
    "authors": [
      "Louis Rosenberg",
      "Hans Schumann",
      "Christopher Dishop",
      "Gregg Willcox",
      "Anita Woolley",
      "Ganesh Mani"
    ],
    "abstract": "Conversational Swarm Intelligence (CSI) is an AI-facilitated method for\nenabling real-time conversational deliberations and prioritizations among\nnetworked human groups of potentially unlimited size. Based on the biological\nprinciple of Swarm Intelligence and modelled on the decision-making dynamics of\nfish schools, CSI has been shown in prior studies to amplify group\nintelligence, increase group participation, and facilitate productive\ncollaboration among hundreds of participants at once. It works by dividing a\nlarge population into a set of small subgroups that are woven together by\nreal-time AI agents called Conversational Surrogates. The present study focuses\non the use of a CSI platform called Thinkscape to enable real-time\nbrainstorming and prioritization among groups of 75 networked users. The study\nemployed a variant of a common brainstorming intervention called an Alternative\nUse Task (AUT) and was designed to compare through subjective feedback, the\nexperience of participants brainstorming using a CSI structure vs brainstorming\nin a single large chat room. This comparison revealed that participants\nsignificantly preferred brainstorming with the CSI structure and reported that\nit felt (i) more collaborative, (ii) more productive, and (iii) was better at\nsurfacing quality answers. In addition, participants using the CSI structure\nreported (iv) feeling more ownership and more buy-in in the final answers the\ngroup converged on and (v) reported feeling more heard as compared to\nbrainstorming in a traditional text chat environment. Overall, the results\nsuggest that CSI is a very promising AI-facilitated method for brainstorming\nand prioritization among large-scale, networked human groups.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI",
      "I.2.11"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14205v1",
    "published_date": "2024-12-16 22:11:25 UTC",
    "updated_date": "2024-12-16 22:11:25 UTC"
  },
  {
    "arxiv_id": "2412.12370v5",
    "title": "Scam Detection for Ethereum Smart Contracts: Leveraging Graph Representation Learning for Secure Blockchain",
    "authors": [
      "Yihong Jin",
      "Ze Yang",
      "Xinhe Xu"
    ],
    "abstract": "As more and more attacks have been detected on Ethereum smart contracts, it\nhas seriously affected finance and credibility. Current anti-fraud detection\ntechniques, including code parsing or manual feature extraction, still have\nsome shortcomings, although some generalization or adaptability can be\nobtained. In the face of this situation, this paper proposes to use graphical\nrepresentation learning technology to find transaction patterns and distinguish\nmalicious transaction contracts, that is, to represent Ethereum transaction\ndata as graphs, and then use advanced ML technology to obtain reliable and\naccurate results. Taking into account the sample imbalance, we treated with\nSMOTE-ENN and tested several models, in which MLP performed better than GCN,\nbut the exact effect depends on its field trials. Our research opens up more\npossibilities for trust and security in the Ethereum ecosystem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC",
      "cs.SI",
      "I.2.1"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ISCAIT 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12370v5",
    "published_date": "2024-12-16 21:56:01 UTC",
    "updated_date": "2025-03-19 05:38:58 UTC"
  },
  {
    "arxiv_id": "2412.12364v1",
    "title": "LogBabylon: A Unified Framework for Cross-Log File Integration and Analysis",
    "authors": [
      "Rabimba Karanjai",
      "Yang Lu",
      "Dana Alsagheer",
      "Keshav Kasichainula",
      "Lei Xu",
      "Weidong Shi",
      "Shou-Hsuan Stephen Huang"
    ],
    "abstract": "Logs are critical resources that record events, activities, or messages\nproduced by software applications, operating systems, servers, and network\ndevices. However, consolidating the heterogeneous logs and cross-referencing\nthem is challenging and complicated. Manually analyzing the log data is\ntime-consuming and prone to errors. LogBabylon is a centralized log data\nconsolidating solution that leverages Large Language Models (LLMs) integrated\nwith Retrieval-Augmented Generation (RAG) technology. LogBabylon interprets the\nlog data in a human-readable way and adds insight analysis of the system\nperformance and anomaly alerts. It provides a paramount view of the system\nlandscape, enabling proactive management and rapid incident response.\nLogBabylon consolidates diverse log sources and enhances the extracted\ninformation's accuracy and relevancy. This facilitates a deeper understanding\nof log data, supporting more effective decision-making and operational\nefficiency. Furthermore, LogBabylon streamlines the log analysis process,\nsignificantly reducing the time and effort required to interpret complex\ndatasets. Its capabilities extend to generating context-aware insights,\noffering an invaluable tool for continuous monitoring, performance\noptimization, and security assurance in dynamic computing environments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12364v1",
    "published_date": "2024-12-16 21:36:03 UTC",
    "updated_date": "2024-12-16 21:36:03 UTC"
  },
  {
    "arxiv_id": "2412.12362v1",
    "title": "How Different AI Chatbots Behave? Benchmarking Large Language Models in Behavioral Economics Games",
    "authors": [
      "Yutong Xie",
      "Yiyao Liu",
      "Zhuang Ma",
      "Lin Shi",
      "Xiyuan Wang",
      "Walter Yuan",
      "Matthew O. Jackson",
      "Qiaozhu Mei"
    ],
    "abstract": "The deployment of large language models (LLMs) in diverse applications\nrequires a thorough understanding of their decision-making strategies and\nbehavioral patterns. As a supplement to a recent study on the behavioral Turing\ntest, this paper presents a comprehensive analysis of five leading LLM-based\nchatbot families as they navigate a series of behavioral economics games. By\nbenchmarking these AI chatbots, we aim to uncover and document both common and\ndistinct behavioral patterns across a range of scenarios. The findings provide\nvaluable insights into the strategic preferences of each LLM, highlighting\npotential implications for their deployment in critical decision-making roles.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at The First Workshop on AI Behavioral Science (AIBS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.12362v1",
    "published_date": "2024-12-16 21:25:45 UTC",
    "updated_date": "2024-12-16 21:25:45 UTC"
  },
  {
    "arxiv_id": "2412.12361v2",
    "title": "The Ramanujan Library -- Automated Discovery on the Hypergraph of Integer Relations",
    "authors": [
      "Itay Beit-Halachmi",
      "Ido Kaminer"
    ],
    "abstract": "Fundamental mathematical constants appear in nearly every field of science,\nfrom physics to biology. Formulas that connect different constants often bring\ngreat insight by hinting at connections between previously disparate fields.\nDiscoveries of such relations, however, have remained scarce events, relying on\nsporadic strokes of creativity by human mathematicians. Recent developments of\nalgorithms for automated conjecture generation have accelerated the discovery\nof formulas for specific constants. Yet, the discovery of connections between\nconstants has not been addressed. In this paper, we present the first library\ndedicated to mathematical constants and their interrelations. This library can\nserve as a central repository of knowledge for scientists from different areas,\nand as a collaborative platform for development of new algorithms. The library\nis based on a new representation that we propose for organizing the formulas of\nmathematical constants: a hypergraph, with each node representing a constant\nand each edge representing a formula. Using this representation, we propose and\ndemonstrate a systematic approach for automatically enriching this library\nusing PSLQ, an integer relation algorithm based on QR decomposition and lattice\nconstruction. During its development and testing, our strategy led to the\ndiscovery of 75 previously unknown connections between constants, including a\nnew formula for the `first continued fraction' constant $C_1$, novel formulas\nfor natural logarithms, and new formulas connecting $\\pi$ and $e$. The latter\nformulas generalize a century-old relation between $\\pi$ and $e$ by Ramanujan,\nwhich until now was considered a singular formula and is now found to be part\nof a broader mathematical structure. The code supporting this library is a\npublic, open-source API that can serve researchers in experimental mathematics\nand other fields of science.",
    "categories": [
      "cs.AI",
      "cs.MS",
      "math.NT"
    ],
    "primary_category": "cs.AI",
    "comment": "20 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12361v2",
    "published_date": "2024-12-16 21:18:44 UTC",
    "updated_date": "2025-01-19 10:51:22 UTC"
  },
  {
    "arxiv_id": "2412.12358v1",
    "title": "BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A",
    "authors": [
      "Samy Ateia",
      "Udo Kruschwitz"
    ],
    "abstract": "We present BioRAGent, an interactive web-based retrieval-augmented generation\n(RAG) system for biomedical question answering. The system uses large language\nmodels (LLMs) for query expansion, snippet extraction, and answer generation\nwhile maintaining transparency through citation links to the source documents\nand displaying generated queries for further editing. Building on our\nsuccessful participation in the BioASQ 2024 challenge, we demonstrate how\nfew-shot learning with LLMs can be effectively applied for a professional\nsearch setting. The system supports both direct short paragraph style responses\nand responses with inline citations. Our demo is available online, and the\nsource code is publicly accessible through GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Version as accepted at the Demo Track at ECIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.12358v1",
    "published_date": "2024-12-16 21:09:28 UTC",
    "updated_date": "2024-12-16 21:09:28 UTC"
  },
  {
    "arxiv_id": "2501.01432v1",
    "title": "Survey on safe robot control via learning",
    "authors": [
      "Bassel El Mabsout"
    ],
    "abstract": "Control systems are critical to modern technological infrastructure, spanning\nindustries from aerospace to healthcare. This survey explores the landscape of\nsafe robot learning, investigating methods that balance high-performance\ncontrol with rigorous safety constraints. By examining classical control\ntechniques, learning-based approaches, and embedded system design, the research\nseeks to understand how robotic systems can be developed to prevent hazardous\nstates while maintaining optimal performance across complex operational\nenvironments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.01432v1",
    "published_date": "2024-12-16 21:04:09 UTC",
    "updated_date": "2024-12-16 21:04:09 UTC"
  },
  {
    "arxiv_id": "2412.15262v1",
    "title": "Advanced ingestion process powered by LLM parsing for RAG system",
    "authors": [
      "Arnau Perez",
      "Xavier Vizcaino"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) systems struggle with processing\nmultimodal documents of varying structural complexity. This paper introduces a\nnovel multi-strategy parsing approach using LLM-powered OCR to extract content\nfrom diverse document types, including presentations and high text density\nfiles both scanned or not. The methodology employs a node-based extraction\ntechnique that creates relationships between different information types and\ngenerates context-aware metadata. By implementing a Multimodal Assembler Agent\nand a flexible embedding strategy, the system enhances document comprehension\nand retrieval capabilities. Experimental evaluations across multiple knowledge\nbases demonstrate the approach's effectiveness, showing improvements in answer\nrelevancy and information faithfulness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.15262v1",
    "published_date": "2024-12-16 20:33:33 UTC",
    "updated_date": "2024-12-16 20:33:33 UTC"
  },
  {
    "arxiv_id": "2412.16196v1",
    "title": "AgroXAI: Explainable AI-Driven Crop Recommendation System for Agriculture 4.0",
    "authors": [
      "Ozlem Turgut",
      "Ibrahim Kok",
      "Suat Ozdemir"
    ],
    "abstract": "Today, crop diversification in agriculture is a critical issue to meet the\nincreasing demand for food and improve food safety and quality. This issue is\nconsidered to be the most important challenge for the next generation of\nagriculture due to the diminishing natural resources, the limited arable land,\nand unpredictable climatic conditions caused by climate change. In this paper,\nwe employ emerging technologies such as the Internet of Things (IoT), machine\nlearning (ML), and explainable artificial intelligence (XAI) to improve\noperational efficiency and productivity in the agricultural sector.\nSpecifically, we propose an edge computing-based explainable crop\nrecommendation system, AgroXAI, which suggests suitable crops for a region\nbased on weather and soil conditions. In this system, we provide local and\nglobal explanations of ML model decisions with methods such as ELI5, LIME,\nSHAP, which we integrate into ML models. More importantly, we provide regional\nalternative crop recommendations with the counterfactual explainability method.\nIn this way, we envision that our proposed AgroXAI system will be a platform\nthat provides regional crop diversity in the next generation agriculture.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData), 10 pages, 9 Figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2412.16196v1",
    "published_date": "2024-12-16 20:18:10 UTC",
    "updated_date": "2024-12-16 20:18:10 UTC"
  },
  {
    "arxiv_id": "2412.19823v1",
    "title": "A Survey on Large Language Models for Communication, Network, and Service Management: Application Insights, Challenges, and Future Directions",
    "authors": [
      "Gordon Owusu Boateng",
      "Hani Sami",
      "Ahmed Alagha",
      "Hanae Elmekki",
      "Ahmad Hammoud",
      "Rabeb Mizouni",
      "Azzam Mourad",
      "Hadi Otrok",
      "Jamal Bentahar",
      "Sami Muhaidat",
      "Chamseddine Talhi",
      "Zbigniew Dziong",
      "Mohsen Guizani"
    ],
    "abstract": "The rapid evolution of communication networks in recent decades has\nintensified the need for advanced Network and Service Management (NSM)\nstrategies to address the growing demands for efficiency, scalability, enhanced\nperformance, and reliability of these networks. Large Language Models (LLMs)\nhave received tremendous attention due to their unparalleled capabilities in\nvarious Natural Language Processing (NLP) tasks and generating context-aware\ninsights, offering transformative potential for automating diverse\ncommunication NSM tasks. Contrasting existing surveys that consider a single\nnetwork domain, this survey investigates the integration of LLMs across\ndifferent communication network domains, including mobile networks and related\ntechnologies, vehicular networks, cloud-based networks, and fog/edge-based\nnetworks. First, the survey provides foundational knowledge of LLMs, explicitly\ndetailing the generic transformer architecture, general-purpose and\ndomain-specific LLMs, LLM model pre-training and fine-tuning, and their\nrelation to communication NSM. Under a novel taxonomy of network monitoring and\nreporting, AI-powered network planning, network deployment and distribution,\nand continuous network support, we extensively categorize LLM applications for\nNSM tasks in each of the different network domains, exploring existing\nliterature and their contributions thus far. Then, we identify existing\nchallenges and open issues, as well as future research directions for\nLLM-driven communication NSM, emphasizing the need for scalable, adaptable, and\nresource-efficient solutions that align with the dynamic landscape of\ncommunication networks. We envision that this survey serves as a holistic\nroadmap, providing critical insights for leveraging LLMs to enhance NSM.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.19823v1",
    "published_date": "2024-12-16 20:01:36 UTC",
    "updated_date": "2024-12-16 20:01:36 UTC"
  },
  {
    "arxiv_id": "2412.12326v1",
    "title": "Achieving Collective Welfare in Multi-Agent Reinforcement Learning via Suggestion Sharing",
    "authors": [
      "Yue Jin",
      "Shuangqing Wei",
      "Giovanni Montana"
    ],
    "abstract": "In human society, the conflict between self-interest and collective\nwell-being often obstructs efforts to achieve shared welfare. Related concepts\nlike the Tragedy of the Commons and Social Dilemmas frequently manifest in our\ndaily lives. As artificial agents increasingly serve as autonomous proxies for\nhumans, we propose using multi-agent reinforcement learning (MARL) to address\nthis issue - learning policies to maximise collective returns even when\nindividual agents' interests conflict with the collective one. Traditional MARL\nsolutions involve sharing rewards, values, and policies or designing intrinsic\nrewards to encourage agents to learn collectively optimal policies. We\nintroduce a novel MARL approach based on Suggestion Sharing (SS), where agents\nexchange only action suggestions. This method enables effective cooperation\nwithout the need to design intrinsic rewards, achieving strong performance\nwhile revealing less private information compared to sharing rewards, values,\nor policies. Our theoretical analysis establishes a bound on the discrepancy\nbetween collective and individual objectives, demonstrating how sharing\nsuggestions can align agents' behaviours with the collective objective.\nExperimental results demonstrate that SS performs competitively with baselines\nthat rely on value or policy sharing or intrinsic rewards.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12326v1",
    "published_date": "2024-12-16 19:44:44 UTC",
    "updated_date": "2024-12-16 19:44:44 UTC"
  },
  {
    "arxiv_id": "2412.12322v1",
    "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems",
    "authors": [
      "Ioannis Papadimitriou",
      "Ilias Gialampoukidis",
      "Stefanos Vrochidis",
      "Ioannis",
      "Kompatsiaris"
    ],
    "abstract": "We present RAG Playground, an open-source framework for systematic evaluation\nof Retrieval-Augmented Generation (RAG) systems. The framework implements and\ncompares three retrieval approaches: naive vector search, reranking, and hybrid\nvector-keyword search, combined with ReAct agents using different prompting\nstrategies. We introduce a comprehensive evaluation framework with novel\nmetrics and provide empirical results comparing different language models\n(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our\nexperiments demonstrate significant performance improvements through hybrid\nsearch methods and structured self-evaluation prompting, achieving up to 72.7%\npass rate on our multi-metric evaluation framework. The results also highlight\nthe importance of prompt engineering in RAG systems, with our custom-prompted\nagents showing consistent improvements in retrieval accuracy and response\nquality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "Work In Progress",
    "pdf_url": "http://arxiv.org/pdf/2412.12322v1",
    "published_date": "2024-12-16 19:40:26 UTC",
    "updated_date": "2024-12-16 19:40:26 UTC"
  },
  {
    "arxiv_id": "2412.12276v2",
    "title": "Emergence of Abstractions: Concept Encoding and Decoding Mechanism for In-Context Learning in Transformers",
    "authors": [
      "Seungwook Han",
      "Jinyeop Song",
      "Jeff Gore",
      "Pulkit Agrawal"
    ],
    "abstract": "Humans distill complex experiences into fundamental abstractions that enable\nrapid learning and adaptation. Similarly, autoregressive transformers exhibit\nadaptive learning through in-context learning (ICL), which begs the question of\nhow. In this paper, we propose concept encoding-decoding mechanism to explain\nICL by studying how transformers form and use internal abstractions in their\nrepresentations. On synthetic ICL tasks, we analyze the training dynamics of a\nsmall transformer and report the coupled emergence of concept encoding and\ndecoding. As the model learns to encode different latent concepts (e.g.,\n``Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concureently builds conditional decoding algorithms and\nimprove its ICL performance. We validate the existence of this mechanism across\npretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B).\nFurther, through mechanistic interventions and controlled finetuning, we\ndemonstrate that the quality of concept encoding is causally related and\npredictive of ICL performance. Our empirical insights shed light into better\nunderstanding the success and failure modes of large language models via their\nrepresentations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12276v2",
    "published_date": "2024-12-16 19:00:18 UTC",
    "updated_date": "2024-12-18 06:02:03 UTC"
  },
  {
    "arxiv_id": "2412.12098v1",
    "title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
    "authors": [
      "Bhavya Sukhija",
      "Stelian Coros",
      "Andreas Krause",
      "Pieter Abbeel",
      "Carmelo Sferrazza"
    ],
    "abstract": "Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12098v1",
    "published_date": "2024-12-16 18:59:53 UTC",
    "updated_date": "2024-12-16 18:59:53 UTC"
  },
  {
    "arxiv_id": "2412.12242v1",
    "title": "OmniPrism: Learning Disentangled Visual Concept for Image Generation",
    "authors": [
      "Yangyang Li",
      "Daqing Liu",
      "Wu Liu",
      "Allen He",
      "Xinchen Liu",
      "Yongdong Zhang",
      "Guoqing Jin"
    ],
    "abstract": "Creative visual concept generation often draws inspiration from specific\nconcepts in a reference image to produce relevant outcomes. However, existing\nmethods are typically constrained to single-aspect concept generation or are\neasily disrupted by irrelevant concepts in multi-aspect concept scenarios,\nleading to concept confusion and hindering creative generation. To address\nthis, we propose OmniPrism, a visual concept disentangling approach for\ncreative image generation. Our method learns disentangled concept\nrepresentations guided by natural language and trains a diffusion model to\nincorporate these concepts. We utilize the rich semantic space of a multimodal\nextractor to achieve concept disentanglement from given images and concept\nguidance. To disentangle concepts with different semantics, we construct a\npaired concept disentangled dataset (PCD-200K), where each pair shares the same\nconcept such as content, style, and composition. We learn disentangled concept\nrepresentations through our contrastive orthogonal disentangled (COD) training\npipeline, which are then injected into additional diffusion cross-attention\nlayers for generation. A set of block embeddings is designed to adapt each\nblock's concept domain in the diffusion models. Extensive experiments\ndemonstrate that our method can generate high-quality, concept-disentangled\nresults with high fidelity to text prompts and desired concepts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "WebPage available at https://tale17.github.io/omni/",
    "pdf_url": "http://arxiv.org/pdf/2412.12242v1",
    "published_date": "2024-12-16 18:59:52 UTC",
    "updated_date": "2024-12-16 18:59:52 UTC"
  },
  {
    "arxiv_id": "2412.12094v5",
    "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator",
    "authors": [
      "Guoxuan Chen",
      "Han Shi",
      "Jiawei Li",
      "Yihang Gao",
      "Xiaozhe Ren",
      "Yimeng Chen",
      "Xin Jiang",
      "Zhenguo Li",
      "Weiyang Liu",
      "Chao Huang"
    ],
    "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless separator tokens (i.e.,\npunctuations) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on GitHub\n  ^_^ Thank you very much!",
    "pdf_url": "http://arxiv.org/pdf/2412.12094v5",
    "published_date": "2024-12-16 18:58:57 UTC",
    "updated_date": "2025-02-24 15:42:59 UTC"
  },
  {
    "arxiv_id": "2412.12089v2",
    "title": "Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation",
    "authors": [
      "Eliot Xing",
      "Vernon Luk",
      "Jean Oh"
    ],
    "abstract": "Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables. Additional details at https://rewarped.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages, 13 figures, 18 tables. Accepted to ICLR 2025 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2412.12089v2",
    "published_date": "2024-12-16 18:56:24 UTC",
    "updated_date": "2025-02-27 19:05:47 UTC"
  },
  {
    "arxiv_id": "2412.12063v1",
    "title": "Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives",
    "authors": [
      "Marius Belly",
      "Nathanaël Fijalkow",
      "Hugo Gimbert",
      "Florian Horn",
      "Guillermo A. Pérez",
      "Pierre Vandenhove"
    ],
    "abstract": "Partially observable Markov decision processes (POMDPs) form a prominent\nmodel for uncertainty in sequential decision making. We are interested in\nconstructing algorithms with theoretical guarantees to determine whether the\nagent has a strategy ensuring a given specification with probability 1. This\nwell-studied problem is known to be undecidable already for very simple\nomega-regular objectives, because of the difficulty of reasoning on uncertain\nevents. We introduce a revelation mechanism which restricts information loss by\nrequiring that almost surely the agent has eventually full information of the\ncurrent state. Our main technical results are to construct exact algorithms for\ntwo classes of POMDPs called weakly and strongly revealing. Importantly, the\ndecidable cases reduce to the analysis of a finite belief-support Markov\ndecision process. This yields a conceptually simple and exact algorithm for a\nlarge class of POMDPs.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of paper accepted to AAAI 2025. 26 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12063v1",
    "published_date": "2024-12-16 18:37:06 UTC",
    "updated_date": "2024-12-16 18:37:06 UTC"
  },
  {
    "arxiv_id": "2412.12046v1",
    "title": "Artificial Intelligence in Traffic Systems",
    "authors": [
      "Ritwik Raj Saxena"
    ],
    "abstract": "Existing research on AI-based traffic management systems, utilizing\ntechniques such as fuzzy logic, reinforcement learning, deep neural networks,\nand evolutionary algorithms, demonstrates the potential of AI to transform the\ntraffic landscape. This article endeavors to review the topics where AI and\ntraffic management intersect. It comprises areas like AI-powered traffic signal\ncontrol systems, automatic distance and velocity recognition (for instance, in\nautonomous vehicles, hereafter AVs), smart parking systems, and Intelligent\nTraffic Management Systems (ITMS), which use data captured in real-time to keep\ntrack of traffic conditions, and traffic-related law enforcement and\nsurveillance using AI. AI applications in traffic management cover a wide range\nof spheres. The spheres comprise, inter alia, streamlining traffic signal\ntimings, predicting traffic bottlenecks in specific areas, detecting potential\naccidents and road hazards, managing incidents accurately, advancing public\ntransportation systems, development of innovative driver assistance systems,\nand minimizing environmental impact through simplified routes and reduced\nemissions. The benefits of AI in traffic management are also diverse. They\ncomprise improved management of traffic data, sounder route decision\nautomation, easier and speedier identification and resolution of vehicular\nissues through monitoring the condition of individual vehicles, decreased\ntraffic snarls and mishaps, superior resource utilization, alleviated stress of\ntraffic management manpower, greater on-road safety, and better emergency\nresponse time.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages, 17343 words, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.12046v1",
    "published_date": "2024-12-16 18:15:49 UTC",
    "updated_date": "2024-12-16 18:15:49 UTC"
  },
  {
    "arxiv_id": "2412.12042v1",
    "title": "The Impact of AI Assistance on Radiology Reporting: A Pilot Study Using Simulated AI Draft Reports",
    "authors": [
      "Julián N. Acosta",
      "Siddhant Dogra",
      "Subathra Adithan",
      "Kay Wu",
      "Michael Moritz",
      "Stephen Kwak",
      "Pranav Rajpurkar"
    ],
    "abstract": "Radiologists face increasing workload pressures amid growing imaging volumes,\ncreating risks of burnout and delayed reporting times. While artificial\nintelligence (AI) based automated radiology report generation shows promise for\nreporting workflow optimization, evidence of its real-world impact on clinical\naccuracy and efficiency remains limited. This study evaluated the effect of\ndraft reports on radiology reporting workflows by conducting a three reader\nmulti-case study comparing standard versus AI-assisted reporting workflows. In\nboth workflows, radiologists reviewed the cases and modified either a standard\ntemplate (standard workflow) or an AI-generated draft report (AI-assisted\nworkflow) to create the final report. For controlled evaluation, we used GPT-4\nto generate simulated AI drafts and deliberately introduced 1-3 errors in half\nthe cases to mimic real AI system performance. The AI-assisted workflow\nsignificantly reduced average reporting time from 573 to 435 seconds (p=0.003),\nwithout a statistically significant difference in clinically significant errors\nbetween workflows. These findings suggest that AI-generated drafts can\nmeaningfully accelerate radiology reporting while maintaining diagnostic\naccuracy, offering a practical solution to address mounting workload challenges\nin clinical practice.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12042v1",
    "published_date": "2024-12-16 18:10:49 UTC",
    "updated_date": "2024-12-16 18:10:49 UTC"
  },
  {
    "arxiv_id": "2412.12039v2",
    "title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection",
    "authors": [
      "Ira Ceka",
      "Feitong Qiao",
      "Anik Dey",
      "Aastha Valecha",
      "Gail Kaiser",
      "Baishakhi Ray"
    ],
    "abstract": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12039v2",
    "published_date": "2024-12-16 18:08:14 UTC",
    "updated_date": "2025-01-18 01:19:05 UTC"
  },
  {
    "arxiv_id": "2412.12032v3",
    "title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning",
    "authors": [
      "Gaojian Wang",
      "Feng Lin",
      "Tong Wu",
      "Zhenguang Liu",
      "Zhongjie Ba",
      "Kui Ren"
    ],
    "abstract": "This work asks: with abundant, unlabeled real faces, how to learn a robust\nand transferable facial representation that boosts various face security tasks\nwith respect to generalization performance? We make the first attempt and\npropose a self-supervised pretraining framework to learn fundamental\nrepresentations of real face images, FSFM, that leverages the synergy between\nmasked image modeling (MIM) and instance discrimination (ID). We explore\nvarious facial masking strategies for MIM and present a simple yet powerful\nCRFR-P masking, which explicitly forces the model to capture meaningful\nintra-region consistency and challenging inter-region coherency. Furthermore,\nwe devise the ID network that naturally couples with MIM to establish\nunderlying local-to-global correspondence via tailored self-distillation. These\nthree learning objectives, namely 3C, empower encoding both local features and\nglobal semantics of real faces. After pretraining, a vanilla ViT serves as a\nuniversal vision foundation model for downstream face security tasks:\ncross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen\ndiffusion facial forgery detection. Extensive experiments on 10 public datasets\ndemonstrate that our model transfers better than supervised pretraining, visual\nand facial self-supervised learning arts, and even outperforms task-specialized\nSOTA methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 11 figures, project page: https://fsfm-3c.github.io",
    "pdf_url": "http://arxiv.org/pdf/2412.12032v3",
    "published_date": "2024-12-16 17:58:45 UTC",
    "updated_date": "2025-04-06 14:07:12 UTC"
  },
  {
    "arxiv_id": "2412.12237v1",
    "title": "Equivariant Action Sampling for Reinforcement Learning and Planning",
    "authors": [
      "Linfeng Zhao",
      "Owen Howell",
      "Xupeng Zhu",
      "Jung Yeon Park",
      "Zhewen Zhang",
      "Robin Walters",
      "Lawson L. S. Wong"
    ],
    "abstract": "Reinforcement learning (RL) algorithms for continuous control tasks require\naccurate sampling-based action selection. Many tasks, such as robotic\nmanipulation, contain inherent problem symmetries. However, correctly\nincorporating symmetry into sampling-based approaches remains a challenge. This\nwork addresses the challenge of preserving symmetry in sampling-based planning\nand control, a key component for enhancing decision-making efficiency in RL. We\nintroduce an action sampling approach that enforces the desired symmetry. We\napply our proposed method to a coordinate regression problem and show that the\nsymmetry aware sampling method drastically outperforms the naive sampling\napproach. We furthermore develop a general framework for sampling-based\nmodel-based planning with Model Predictive Path Integral (MPPI). We compare our\nMPPI approach with standard sampling methods on several continuous control\ntasks. Empirical demonstrations across multiple continuous control environments\nvalidate the effectiveness of our approach, showcasing the importance of\nsymmetry preservation in sampling-based action selection.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Published at International Workshop on the Algorithmic Foundations of\n  Robotics (WAFR) 2024. Website: http://lfzhao.com/EquivSampling",
    "pdf_url": "http://arxiv.org/pdf/2412.12237v1",
    "published_date": "2024-12-16 17:51:14 UTC",
    "updated_date": "2024-12-16 17:51:14 UTC"
  },
  {
    "arxiv_id": "2412.12024v1",
    "title": "Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down Maps",
    "authors": [
      "Linfeng Zhao",
      "Lawson L. S. Wong"
    ],
    "abstract": "Learning navigation capabilities in different environments has long been one\nof the major challenges in decision-making. In this work, we focus on zero-shot\nnavigation ability using given abstract $2$-D top-down maps. Like human\nnavigation by reading a paper map, the agent reads the map as an image when\nnavigating in a novel layout, after learning to navigate on a set of training\nmaps. We propose a model-based reinforcement learning approach for this\nmulti-task learning problem, where it jointly learns a hypermodel that takes\ntop-down maps as input and predicts the weights of the transition network. We\nuse the DeepMind Lab environment and customize layouts using generated maps.\nOur method can adapt better to novel environments in zero-shot and is more\nrobust to noise.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Published at Reinforcement Learning Conference (RLC) 2024. Website:\n  http://lfzhao.com/map-nav/",
    "pdf_url": "http://arxiv.org/pdf/2412.12024v1",
    "published_date": "2024-12-16 17:51:09 UTC",
    "updated_date": "2024-12-16 17:51:09 UTC"
  },
  {
    "arxiv_id": "2412.16195v2",
    "title": "Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication",
    "authors": [
      "Shekhar Madhav Khairnar",
      "Huu Phong Nguyen",
      "Alexis Desir",
      "Carla Holcomb",
      "Daniel J. Scott",
      "Ganesh Sankaranarayanan"
    ],
    "abstract": "Automated assessment of surgical skills using artificial intelligence (AI)\nprovides trainees with instantaneous feedback. After bimanual tool motions are\ncaptured, derived kinematic metrics are reliable predictors of performance in\nlaparoscopic tasks. Implementing automated tool tracking requires\ntime-intensive human annotation. We developed AI-based tool tracking using the\nSegment Anything Model (SAM) to eliminate the need for human annotators. Here,\nwe describe a study evaluating the usefulness of our tool tracking model in\nautomated assessment during a laparoscopic suturing task in the fundoplication\nprocedure. An automated tool tracking model was applied to recorded videos of\nNissen fundoplication on porcine bowel. Surgeons were grouped as novices\n(PGY1-2) and experts (PGY3-5, attendings). The beginning and end of each\nsuturing step were segmented, and motions of the left and right tools were\nextracted. A low-pass filter with a 24 Hz cut-off frequency removed noise.\nPerformance was assessed using supervised and unsupervised models, and an\nablation study compared results. Kinematic features--RMS velocity, RMS\nacceleration, RMS jerk, total path length, and Bimanual Dexterity--were\nextracted and analyzed using Logistic Regression, Random Forest, Support Vector\nClassifier, and XGBoost. PCA was performed for feature reduction. For\nunsupervised learning, a Denoising Autoencoder (DAE) model with classifiers,\nsuch as a 1-D CNN and traditional models, was trained. Data were extracted for\n28 participants (9 novices, 19 experts). Supervised learning with PCA and\nRandom Forest achieved an accuracy of 0.795 and an F1 score of 0.778. The\nunsupervised 1-D CNN achieved superior results with an accuracy of 0.817 and an\nF1 score of 0.806, eliminating the need for kinematic feature computation. We\ndemonstrated an AI model capable of automated performance classification,\nindependent of human annotation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.16195v2",
    "published_date": "2024-12-16 17:44:44 UTC",
    "updated_date": "2025-04-24 05:38:26 UTC"
  },
  {
    "arxiv_id": "2412.12009v2",
    "title": "SpeechPrune: Context-aware Token Pruning for Speech Information Retrieval",
    "authors": [
      "Yueqian Lin",
      "Yuzhe Fu",
      "Jingyang Zhang",
      "Yudong Liu",
      "Jianyi Zhang",
      "Jingwei Sun",
      "Hai \"Helen\" Li",
      "Yiran Chen"
    ],
    "abstract": "We introduce Speech Information Retrieval (SIR), a new long-context task for\nSpeech Large Language Models (Speech LLMs), and present SPIRAL, a 1,012-sample\nbenchmark testing models' ability to extract critical details from\napproximately 90-second spoken inputs. While current Speech LLMs excel at\nshort-form tasks, they struggle with the computational and representational\ndemands of longer audio sequences. To address this limitation, we propose\nSpeechPrune, a training-free token pruning strategy that uses speech-text\nsimilarity and approximated attention scores to efficiently discard irrelevant\ntokens. In SPIRAL, SpeechPrune achieves accuracy improvements of 29% and up to\n47% over the original model and the random pruning model at a pruning rate of\n20%, respectively. SpeechPrune can maintain network performance even at a\npruning level of 80%. This approach highlights the potential of token-level\npruning for efficient and scalable long-form speech understanding.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted at IEEE ICME 2025. Project page:\n  https://speechprune.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2412.12009v2",
    "published_date": "2024-12-16 17:36:02 UTC",
    "updated_date": "2025-03-30 02:39:14 UTC"
  },
  {
    "arxiv_id": "2412.12006v2",
    "title": "Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A Novel Weighted Retrieval-Augmented Generation Paradigm",
    "authors": [
      "Rajat Khanda"
    ],
    "abstract": "Technical troubleshooting in enterprise environments often involves\nnavigating diverse, heterogeneous data sources to resolve complex issues\neffectively. This paper presents a novel agentic AI solution built on a\nWeighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise\ntechnical troubleshooting. By dynamically weighting retrieval sources such as\nproduct manuals, internal knowledge bases, FAQs, and troubleshooting guides\nbased on query context, the framework prioritizes the most relevant data. For\ninstance, it gives precedence to product manuals for SKU-specific queries while\nincorporating general FAQs for broader issues. The system employs FAISS for\nefficient dense vector search, coupled with a dynamic aggregation mechanism to\nseamlessly integrate results from multiple sources. A Llama-based\nself-evaluator ensures the contextual accuracy and confidence of the generated\nresponses before delivering them. This iterative cycle of retrieval and\nvalidation enhances precision, diversity, and reliability in response\ngeneration. Preliminary evaluations on large enterprise datasets demonstrate\nthe framework's efficacy in improving troubleshooting accuracy, reducing\nresolution times, and adapting to varied technical challenges. Future research\naims to enhance the framework by integrating advanced conversational AI\ncapabilities, enabling more interactive and intuitive troubleshooting\nexperiences. Efforts will also focus on refining the dynamic weighting\nmechanism through reinforcement learning to further optimize the relevance and\nprecision of retrieved information. By incorporating these advancements, the\nproposed framework is poised to evolve into a comprehensive, autonomous AI\nsolution, redefining technical service workflows across enterprise settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12006v2",
    "published_date": "2024-12-16 17:32:38 UTC",
    "updated_date": "2024-12-17 17:02:27 UTC"
  },
  {
    "arxiv_id": "2412.12000v1",
    "title": "CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception",
    "authors": [
      "Senkang Hu",
      "Yihang Tao",
      "Guowen Xu",
      "Yiqin Deng",
      "Xianhao Chen",
      "Yuguang Fang",
      "Sam Kwong"
    ],
    "abstract": "Collaborative Perception (CP) has shown a promising technique for autonomous\ndriving, where multiple connected and autonomous vehicles (CAVs) share their\nperception information to enhance the overall perception performance and expand\nthe perception range. However, in CP, ego CAV needs to receive messages from\nits collaborators, which makes it easy to be attacked by malicious agents. For\nexample, a malicious agent can send harmful information to the ego CAV to\nmislead it. To address this critical issue, we propose a novel method,\n\\textbf{CP-Guard}, a tailored defense mechanism for CP that can be deployed by\neach agent to accurately detect and eliminate malicious agents in its\ncollaboration network. Our key idea is to enable CP to reach a consensus rather\nthan a conflict against the ego CAV's perception results. Based on this idea,\nwe first develop a probability-agnostic sample consensus (PASAC) method to\neffectively sample a subset of the collaborators and verify the consensus\nwithout prior probabilities of malicious agents. Furthermore, we define a\ncollaborative consistency loss (CCLoss) to capture the discrepancy between the\nego CAV and its collaborators, which is used as a verification criterion for\nconsensus. Finally, we conduct extensive experiments in collaborative bird's\neye view (BEV) tasks and our results demonstrate the effectiveness of our\nCP-Guard.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI'25",
    "pdf_url": "http://arxiv.org/pdf/2412.12000v1",
    "published_date": "2024-12-16 17:28:25 UTC",
    "updated_date": "2024-12-16 17:28:25 UTC"
  },
  {
    "arxiv_id": "2412.11995v1",
    "title": "Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support",
    "authors": [
      "Devika Venugopalan",
      "Ziwen Yan",
      "Conrad Borchers",
      "Jionghao Lin",
      "Vincent Aleven"
    ],
    "abstract": "Caregivers (i.e., parents and members of a child's caring community) are\nunderappreciated stakeholders in learning analytics. Although caregiver\ninvolvement can enhance student academic outcomes, many obstacles hinder\ninvolvement, most notably knowledge gaps with respect to modern school\ncurricula. An emerging topic of interest in learning analytics is hybrid\ntutoring, which includes instructional and motivational support. Caregivers\nassert similar roles in homework, yet it is unknown how learning analytics can\nsupport them. Our past work with caregivers suggested that conversational\nsupport is a promising method of providing caregivers with the guidance needed\nto effectively support student learning. We developed a system that provides\ninstructional support to caregivers through conversational recommendations\ngenerated by a Large Language Model (LLM). Addressing known instructional\nlimitations of LLMs, we use instructional intelligence from tutoring systems\nwhile conducting prompt engineering experiments with the open-source Llama 3\nLLM. This LLM generated message recommendations for caregivers supporting their\nchild's math practice via chat. Few-shot prompting and combining real-time\nproblem-solving context from tutoring systems with examples of tutoring\npractices yielded desirable message recommendations. These recommendations were\nevaluated with ten middle school caregivers, who valued recommendations\nfacilitating content-level support and student metacognition through\nself-explanation. We contribute insights into how tutoring systems can best be\nmerged with LLMs to support hybrid tutoring settings through conversational\nassistance, facilitating effective caregiver involvement in tutoring systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.11995v1",
    "published_date": "2024-12-16 17:22:40 UTC",
    "updated_date": "2024-12-16 17:22:40 UTC"
  },
  {
    "arxiv_id": "2412.11994v1",
    "title": "Fairness Shields: Safeguarding against Biased Decision Makers",
    "authors": [
      "Filip Cano",
      "Thomas A. Henzinger",
      "Bettina Könighofer",
      "Konstantin Kueffner",
      "Kaushik Mallik"
    ],
    "abstract": "As AI-based decision-makers increasingly influence human lives, it is a\ngrowing concern that their decisions are often unfair or biased with respect to\npeople's sensitive attributes, such as gender and race. Most existing bias\nprevention measures provide probabilistic fairness guarantees in the long run,\nand it is possible that the decisions are biased on specific instances of short\ndecision sequences. We introduce fairness shielding, where a symbolic\ndecision-maker -- the fairness shield -- continuously monitors the sequence of\ndecisions of another deployed black-box decision-maker, and makes interventions\nso that a given fairness criterion is met while the total intervention costs\nare minimized. We present four different algorithms for computing fairness\nshields, among which one guarantees fairness over fixed horizons, and three\nguarantee fairness periodically after fixed intervals. Given a distribution\nover future decisions and their intervention costs, our algorithms solve\ndifferent instances of bounded-horizon optimal control problems with different\nlevels of computational costs and optimality guarantees. Our empirical\nevaluation demonstrates the effectiveness of these shields in ensuring fairness\nwhile maintaining cost efficiency across various scenarios.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11994v1",
    "published_date": "2024-12-16 17:21:12 UTC",
    "updated_date": "2024-12-16 17:21:12 UTC"
  },
  {
    "arxiv_id": "2412.11983v3",
    "title": "Leveraging Large Language Models for Effective Label-free Node Classification in Text-Attributed Graphs",
    "authors": [
      "Taiyan Zhang",
      "Renchi Yang",
      "Yurui Lai",
      "Mingyu Yan",
      "Xiaochun Ye",
      "Dongrui Fan"
    ],
    "abstract": "Graph neural networks (GNNs) have become the preferred models for node\nclassification in graph data due to their robust capabilities in integrating\ngraph structures and attributes. However, these models heavily depend on a\nsubstantial amount of high-quality labeled data for training, which is often\ncostly to obtain. With the rise of large language models (LLMs), a promising\napproach is to utilize their exceptional zero-shot capabilities and extensive\nknowledge for node labeling. Despite encouraging results, this approach either\nrequires numerous queries to LLMs or suffers from reduced performance due to\nnoisy labels generated by LLMs. To address these challenges, we introduce\nLocle, an active self-training framework that does Label-free node\nClassification with LLMs cost-Effectively. Locle iteratively identifies small\nsets of \"critical\" samples using GNNs and extracts informative pseudo-labels\nfor them with both LLMs and GNNs, serving as additional supervision signals to\nenhance model training. Specifically, Locle comprises three key components: (i)\nan effective active node selection strategy for initial annotations; (ii) a\ncareful sample selection scheme to identify \"critical\" nodes based on label\ndisharmonicity and entropy; and (iii) a label refinement module that combines\nLLMs and GNNs with a rewired topology. Extensive experiments on five benchmark\ntext-attributed graph datasets demonstrate that Locle significantly outperforms\nstate-of-the-art methods under the same query budget to LLMs in terms of\nlabel-free node classification. Notably, on the DBLP dataset with 14.3k nodes,\nLocle achieves an 8.08% improvement in accuracy over the state-of-the-art at a\ncost of less than one cent. Our code is available at\nhttps://github.com/HKBU-LAGAS/Locle.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by SIGIR2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11983v3",
    "published_date": "2024-12-16 17:04:40 UTC",
    "updated_date": "2025-05-16 06:58:58 UTC"
  },
  {
    "arxiv_id": "2412.11974v2",
    "title": "Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning",
    "authors": [
      "Qi Sun",
      "Pengfei Hong",
      "Tej Deep Pala",
      "Vernon Toh",
      "U-Xuan Tan",
      "Deepanway Ghosal",
      "Soujanya Poria"
    ],
    "abstract": "Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X",
    "pdf_url": "http://arxiv.org/pdf/2412.11974v2",
    "published_date": "2024-12-16 16:58:28 UTC",
    "updated_date": "2024-12-17 14:12:56 UTC"
  },
  {
    "arxiv_id": "2412.11959v2",
    "title": "Gramian Multimodal Representation Learning and Alignment",
    "authors": [
      "Giordano Cicchetti",
      "Eleonora Grassucci",
      "Luigi Sigillo",
      "Danilo Comminiello"
    ],
    "abstract": "Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11959v2",
    "published_date": "2024-12-16 16:41:51 UTC",
    "updated_date": "2025-02-12 13:25:10 UTC"
  },
  {
    "arxiv_id": "2412.11952v1",
    "title": "Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning",
    "authors": [
      "Yuti Liu",
      "Shice Liu",
      "Junyuan Gao",
      "Pengtao Jiang",
      "Hao Zhang",
      "Jinwei Chen",
      "Bo Li"
    ],
    "abstract": "Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11952v1",
    "published_date": "2024-12-16 16:35:35 UTC",
    "updated_date": "2024-12-16 16:35:35 UTC"
  },
  {
    "arxiv_id": "2412.11951v1",
    "title": "The Impact of Generalization Techniques on the Interplay Among Privacy, Utility, and Fairness in Image Classification",
    "authors": [
      "Ahmad Hassanpour",
      "Amir Zarei",
      "Khawla Mallat",
      "Anderson Santana de Oliveira",
      "Bian Yang"
    ],
    "abstract": "This study investigates the trade-offs between fairness, privacy, and utility\nin image classification using machine learning (ML). Recent research suggests\nthat generalization techniques can improve the balance between privacy and\nutility. One focus of this work is sharpness-aware training (SAT) and its\nintegration with differential privacy (DP-SAT) to further improve this balance.\nAdditionally, we examine fairness in both private and non-private learning\nmodels trained on datasets with synthetic and real-world biases. We also\nmeasure the privacy risks involved in these scenarios by performing membership\ninference attacks (MIAs) and explore the consequences of eliminating\nhigh-privacy risk samples, termed outliers. Moreover, we introduce a new\nmetric, named \\emph{harmonic score}, which combines accuracy, privacy, and\nfairness into a single measure.\n  Through empirical analysis using generalization techniques, we achieve an\naccuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\%\nreported by De et al. (2022). Moreover, our experiments show that memorization\nof training samples can begin before the overfitting point, and generalization\ntechniques do not guarantee the prevention of this memorization. Our analysis\nof synthetic biases shows that generalization techniques can amplify model bias\nin both private and non-private models. Additionally, our results indicate that\nincreased bias in training data leads to reduced accuracy, greater\nvulnerability to privacy attacks, and higher model bias. We validate these\nfindings with the CelebA dataset, demonstrating that similar trends persist\nwith real-world attribute imbalances. Finally, our experiments show that\nremoving outlier data decreases accuracy and further amplifies model bias.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at the 25th Privacy Enhancing\n  Technologies Symposium (PETS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.11951v1",
    "published_date": "2024-12-16 16:35:31 UTC",
    "updated_date": "2024-12-16 16:35:31 UTC"
  },
  {
    "arxiv_id": "2412.11948v3",
    "title": "OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews",
    "authors": [
      "Maximilian Idahl",
      "Zahra Ahmadi"
    ],
    "abstract": "We present OpenReviewer, an open-source system for generating high-quality\npeer reviews of machine learning and AI conference papers. At its core is\nLlama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned\non 79,000 expert reviews from top conferences. Given a PDF paper submission and\nreview template as input, OpenReviewer extracts the full text, including\ntechnical content like equations and tables, and generates a structured review\nfollowing conference-specific guidelines. Our evaluation on 400 test papers\nshows that OpenReviewer produces considerably more critical and realistic\nreviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other\nLLMs tend toward overly positive assessments, OpenReviewer's recommendations\nclosely match the distribution of human reviewer ratings. The system provides\nauthors with rapid, constructive feedback to improve their manuscripts before\nsubmission, though it is not intended to replace human peer review.\nOpenReviewer is available as an online demo and open-source tool.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "NAACL 2025 System Demonstrations Track (Camera-ready version) Demo:\n  https://huggingface.co/spaces/maxidl/openreviewer Model:\n  https://huggingface.co/maxidl/Llama-OpenReviewer-8B",
    "pdf_url": "http://arxiv.org/pdf/2412.11948v3",
    "published_date": "2024-12-16 16:31:00 UTC",
    "updated_date": "2025-03-18 08:37:47 UTC"
  },
  {
    "arxiv_id": "2412.11943v2",
    "title": "autrainer: A Modular and Extensible Deep Learning Toolkit for Computer Audition Tasks",
    "authors": [
      "Simon Rampp",
      "Andreas Triantafyllopoulos",
      "Manuel Milling",
      "Björn W. Schuller"
    ],
    "abstract": "This work introduces the key operating principles for autrainer, our new deep\nlearning training framework for computer audition tasks. autrainer is a\nPyTorch-based toolkit that allows for rapid, reproducible, and easily\nextensible training on a variety of different computer audition tasks.\nConcretely, autrainer offers low-code training and supports a wide range of\nneural networks as well as preprocessing routines. In this work, we present an\noverview of its inner workings and key capabilities.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11943v2",
    "published_date": "2024-12-16 16:25:58 UTC",
    "updated_date": "2025-04-10 13:51:44 UTC"
  },
  {
    "arxiv_id": "2412.11939v1",
    "title": "SEAGraph: Unveiling the Whole Story of Paper Review Comments",
    "authors": [
      "Jianxiang Yu",
      "Jiaqi Tan",
      "Zichen Ding",
      "Jiapeng Zhu",
      "Jiahao Li",
      "Yao Cheng",
      "Qier Cui",
      "Yunshi Lan",
      "Xiang Li"
    ],
    "abstract": "Peer review, as a cornerstone of scientific research, ensures the integrity\nand quality of scholarly work by providing authors with objective feedback for\nrefinement. However, in the traditional peer review process, authors often\nreceive vague or insufficiently detailed feedback, which provides limited\nassistance and leads to a more time-consuming review cycle. If authors can\nidentify some specific weaknesses in their paper, they can not only address the\nreviewer's concerns but also improve their work. This raises the critical\nquestion of how to enhance authors' comprehension of review comments. In this\npaper, we present SEAGraph, a novel framework developed to clarify review\ncomments by uncovering the underlying intentions behind them. We construct two\ntypes of graphs for each paper: the semantic mind graph, which captures the\nauthor's thought process, and the hierarchical background graph, which\ndelineates the research domains related to the paper. A retrieval method is\nthen designed to extract relevant content from both graphs, facilitating\ncoherent explanations for the review comments. Extensive experiments show that\nSEAGraph excels in review comment understanding tasks, offering significant\nbenefits to authors.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11939v1",
    "published_date": "2024-12-16 16:24:36 UTC",
    "updated_date": "2024-12-16 16:24:36 UTC"
  },
  {
    "arxiv_id": "2412.11934v3",
    "title": "Stepwise Reasoning Error Disruption Attack of LLMs",
    "authors": [
      "Jingyu Peng",
      "Maolin Wang",
      "Xiangyu Zhao",
      "Kai Zhang",
      "Wanyu Wang",
      "Pengyue Jia",
      "Qidong Liu",
      "Ruocheng Guo",
      "Qi Liu"
    ],
    "abstract": "Large language models (LLMs) have made remarkable strides in complex\nreasoning tasks, but their safety and robustness in reasoning processes remain\nunderexplored. Existing attacks on LLM reasoning are constrained by specific\nsettings or lack of imperceptibility, limiting their feasibility and\ngeneralizability. To address these challenges, we propose the Stepwise\nrEasoning Error Disruption (SEED) attack, which subtly injects errors into\nprior reasoning steps to mislead the model into producing incorrect subsequent\nreasoning and final answers. Unlike previous methods, SEED is compatible with\nzero-shot and few-shot settings, maintains the natural reasoning flow, and\nensures covert execution without modifying the instruction. Extensive\nexperiments on four datasets across four different models demonstrate SEED's\neffectiveness, revealing the vulnerabilities of LLMs to disruptions in\nreasoning processes. These findings underscore the need for greater attention\nto the robustness of LLM reasoning to ensure safety in practical applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11934v3",
    "published_date": "2024-12-16 16:20:41 UTC",
    "updated_date": "2025-03-10 06:22:15 UTC"
  },
  {
    "arxiv_id": "2412.11930v1",
    "title": "Hierarchical Meta-Reinforcement Learning via Automated Macro-Action Discovery",
    "authors": [
      "Minjae Cho",
      "Chuangchuang Sun"
    ],
    "abstract": "Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing\ntasks. Despite recent advancements, it is still challenging to learn performant\npolicies across multiple complex and high-dimensional tasks. To address this,\nwe propose a novel architecture with three hierarchical levels for 1) learning\ntask representations, 2) discovering task-agnostic macro-actions in an\nautomated manner, and 3) learning primitive actions. The macro-action can guide\nthe low-level primitive policy learning to more efficiently transition to goal\nstates. This can address the issue that the policy may forget previously\nlearned behavior while learning new, conflicting tasks. Moreover, the\ntask-agnostic nature of the macro-actions is enabled by removing task-specific\ncomponents from the state space. Hence, this makes them amenable to\nre-composition across different tasks and leads to promising fast adaptation to\nnew tasks. Also, the prospective instability from the tri-level hierarchies is\neffectively mitigated by our innovative, independently tailored training\nschemes. Experiments in the MetaWorld framework demonstrate the improved sample\nefficiency and success rate of our approach compared to previous\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11930v1",
    "published_date": "2024-12-16 16:15:36 UTC",
    "updated_date": "2024-12-16 16:15:36 UTC"
  },
  {
    "arxiv_id": "2412.11927v1",
    "title": "Explainable Procedural Mistake Detection",
    "authors": [
      "Shane Storks",
      "Itamar Bar-Yossef",
      "Yayuan Li",
      "Zheyuan Zhang",
      "Jason J. Corso",
      "Joyce Chai"
    ],
    "abstract": "Automated task guidance has recently attracted attention from the AI research\ncommunity. Procedural mistake detection (PMD) is a challenging sub-problem of\nclassifying whether a human user (observed through egocentric video) has\nsuccessfully executed the task at hand (specified by a procedural text).\nDespite significant efforts in building resources and models for PMD, machine\nperformance remains nonviable, and the reasoning processes underlying this\nperformance are opaque. As such, we recast PMD to an explanatory self-dialog of\nquestions and answers, which serve as evidence for a decision. As this\nreformulation enables an unprecedented transparency, we leverage a fine-tuned\nnatural language inference (NLI) model to formulate two automated coherence\nmetrics for generated explanations. Our results show that while open-source\nVLMs struggle with this task off-the-shelf, their accuracy, coherence, and\ndialog efficiency can be vastly improved by incorporating these coherence\nmetrics into common inference and fine-tuning methods. Furthermore, our\nmulti-faceted metrics can visualize common outcomes at a glance, highlighting\nareas for improvement.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11927v1",
    "published_date": "2024-12-16 16:13:55 UTC",
    "updated_date": "2024-12-16 16:13:55 UTC"
  },
  {
    "arxiv_id": "2412.11923v2",
    "title": "PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection",
    "authors": [
      "Sepideh Mamooler",
      "Syrielle Montariol",
      "Alexander Mathis",
      "Antoine Bosselut"
    ],
    "abstract": "In-context learning (ICL) enables Large Language Models (LLMs) to perform\ntasks using few demonstrations, facilitating task adaptation when labeled\nexamples are hard to obtain. However, ICL is sensitive to the choice of\ndemonstrations, and it remains unclear which demonstration attributes enable\nin-context generalization. In this work, we conduct a perturbation study of\nin-context demonstrations for low-resource Named Entity Detection (NED). Our\nsurprising finding is that in-context demonstrations with partially correct\nannotated entity mentions can be as effective for task transfer as fully\ncorrect demonstrations. Based off our findings, we propose Pseudo-annotated\nIn-Context Learning (PICLe), a framework for in-context learning with noisy,\npseudo-annotated demonstrations. PICLe leverages LLMs to annotate many\ndemonstrations in a zero-shot first pass. We then cluster these synthetic\ndemonstrations, sample specific sets of in-context demonstrations from each\ncluster, and predict entity mentions using each set independently. Finally, we\nuse self-verification to select the final set of entity mentions. We evaluate\nPICLe on five biomedical NED datasets and show that, with zero human\nannotation, PICLe outperforms ICL in low-resource settings where limited gold\nexamples can be used as in-context demonstrations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In Proceedings of NAACL2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11923v2",
    "published_date": "2024-12-16 16:09:35 UTC",
    "updated_date": "2025-04-01 12:45:58 UTC"
  },
  {
    "arxiv_id": "2412.11919v1",
    "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
    "authors": [
      "Xiaoxi Li",
      "Jiajie Jin",
      "Yujia Zhou",
      "Yongkang Wu",
      "Zhonghua Li",
      "Qi Ye",
      "Zhicheng Dou"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable generative capabilities but\noften suffer from hallucinations. Retrieval-augmented generation (RAG) offers\nan effective solution by incorporating external knowledge, but existing methods\nstill face several limitations: additional deployment costs of separate\nretrievers, redundant input tokens from retrieved text chunks, and the lack of\njoint optimization of retrieval and generation. To address these issues, we\npropose \\textbf{RetroLLM}, a unified framework that integrates retrieval and\ngeneration into a single, cohesive process, enabling LLMs to directly generate\nfine-grained evidence from the corpus with constrained decoding. Moreover, to\nmitigate false pruning in the process of constrained evidence generation, we\nintroduce (1) hierarchical FM-Index constraints, which generate\ncorpus-constrained clues to identify a subset of relevant documents before\nevidence generation, reducing irrelevant decoding space; and (2) a\nforward-looking constrained decoding strategy, which considers the relevance of\nfuture sequences to improve evidence accuracy. Extensive experiments on five\nopen-domain QA datasets demonstrate RetroLLM's superior performance across both\nin-domain and out-of-domain tasks. The code is available at\n\\url{https://github.com/sunnynexus/RetroLLM}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11919v1",
    "published_date": "2024-12-16 16:03:25 UTC",
    "updated_date": "2024-12-16 16:03:25 UTC"
  },
  {
    "arxiv_id": "2412.11906v1",
    "title": "PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension",
    "authors": [
      "Kun Ouyang",
      "Yuanxin Liu",
      "Shicheng Li",
      "Yi Liu",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ],
    "abstract": "Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11906v1",
    "published_date": "2024-12-16 15:52:59 UTC",
    "updated_date": "2024-12-16 15:52:59 UTC"
  },
  {
    "arxiv_id": "2412.11888v1",
    "title": "GNN Applied to Ego-nets for Friend Suggestions",
    "authors": [
      "Evgeny Zamyatin"
    ],
    "abstract": "A major problem of making friend suggestions in social networks is the large\nsize of social graphs, which can have hundreds of millions of people and tens\nof billions of connections. Classic methods based on heuristics or\nfactorizations are often used to address the difficulties of scaling more\ncomplex models. However, the unsupervised nature of these methods can lead to\nsuboptimal results. In this work, we introduce the Generalized Ego-network\nFriendship Score framework, which makes it possible to use complex supervised\nmodels without sacrificing scalability. The main principle of the framework is\nto reduce the problem of link prediction on a full graph to a series of\nlow-scale tasks on ego-nets with subsequent aggregation of their results. Here,\nthe underlying model takes an ego-net as input and produces a pairwise\nrelevance matrix for its nodes. In addition, we develop the WalkGNN model which\nis capable of working effectively in the social network domain, where these\ngraph-level link prediction tasks are heterogeneous, dynamic and featureless.\nTo measure the accuracy of this model, we introduce the Ego-VK dataset that\nserves as an exact representation of the real-world problem that we are\naddressing. Offline experiments on the dataset show that our model outperforms\nall baseline methods, and a live A/B test demonstrates the growth of business\nmetrics as a result of utilizing our approach.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11888v1",
    "published_date": "2024-12-16 15:37:17 UTC",
    "updated_date": "2024-12-16 15:37:17 UTC"
  },
  {
    "arxiv_id": "2412.11868v2",
    "title": "A Variable Occurrence-Centric Framework for Inconsistency Handling (Extended Version)",
    "authors": [
      "Yakoub Salhi"
    ],
    "abstract": "In this paper, we introduce a syntactic framework for analyzing and handling\ninconsistencies in propositional bases. Our approach focuses on examining the\nrelationships between variable occurrences within conflicts. We propose two\ndual concepts: Minimal Inconsistency Relation (MIR) and Maximal Consistency\nRelation (MCR). Each MIR is a minimal equivalence relation on variable\noccurrences that results in inconsistency, while each MCR is a maximal\nequivalence relation designed to prevent inconsistency. Notably, MIRs capture\nconflicts overlooked by minimal inconsistent subsets. Using MCRs, we develop a\nseries of non-explosive inference relations. The main strategy involves\nrestoring consistency by modifying the propositional base according to each\nMCR, followed by employing the classical inference relation to derive\nconclusions. Additionally, we propose an unusual semantics that assigns truth\nvalues to variable occurrences instead of the variables themselves. The\nassociated inference relations are established through Boolean interpretations\ncompatible with the occurrence-based models.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11868v2",
    "published_date": "2024-12-16 15:22:10 UTC",
    "updated_date": "2024-12-17 08:17:54 UTC"
  },
  {
    "arxiv_id": "2412.11867v2",
    "title": "Transformers Use Causal World Models in Maze-Solving Tasks",
    "authors": [
      "Alex F. Spies",
      "William Edwards",
      "Michael I. Ivanitskiy",
      "Adrians Skapars",
      "Tilman Räuker",
      "Katsumi Inoue",
      "Alessandra Russo",
      "Murray Shanahan"
    ],
    "abstract": "Recent studies in interpretability have explored the inner workings of\ntransformer models trained on tasks across various domains, often discovering\nthat these networks naturally develop highly structured representations. When\nsuch representations comprehensively reflect the task domain's structure, they\nare commonly referred to as \"World Models\" (WMs). In this work, we identify WMs\nin transformers trained on maze-solving tasks. By using Sparse Autoencoders\n(SAEs) and analyzing attention patterns, we examine the construction of WMs and\ndemonstrate consistency between SAE feature-based and circuit-based analyses.\nBy subsequently intervening on isolated features to confirm their causal role,\nwe find that it is easier to activate features than to suppress them.\nFurthermore, we find that models can reason about mazes involving more\nsimultaneously active features than they encountered during training; however,\nwhen these same mazes (with greater numbers of connections) are provided to\nmodels via input tokens instead, the models fail. Finally, we demonstrate that\npositional encoding schemes appear to influence how World Models are structured\nwithin the model's residual stream.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "Main paper: 9 pages, 9 figures. Supplementary material: 10 pages, 17\n  additional figures. Code and data will be available upon publication.\n  Corresponding author: A. F. Spies (afspies@imperial.ac.uk)",
    "pdf_url": "http://arxiv.org/pdf/2412.11867v2",
    "published_date": "2024-12-16 15:21:04 UTC",
    "updated_date": "2025-03-05 23:16:16 UTC"
  },
  {
    "arxiv_id": "2412.11864v1",
    "title": "Investigating Mixture of Experts in Dense Retrieval",
    "authors": [
      "Effrosyni Sokli",
      "Pranav Kasela",
      "Georgios Peikos",
      "Gabriella Pasi"
    ],
    "abstract": "While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR),\none limitation of these neural models is their narrow generalizability and\nrobustness. To cope with this issue, one can leverage the Mixture-of-Experts\n(MoE) architecture. While previous IR studies have incorporated MoE\narchitectures within the Transformer layers of DRMs, our work investigates an\narchitecture that integrates a single MoE block (SB-MoE) after the output of\nthe final Transformer layer. Our empirical evaluation investigates how SB-MoE\ncompares, in terms of retrieval effectiveness, to standard fine-tuning. In\ndetail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four\nbenchmark collections with and without adding the MoE block. Moreover, since\nMoE showcases performance variations with respect to its parameters (i.e., the\nnumber of experts), we conduct additional experiments to investigate this\naspect further. The findings show the effectiveness of SB-MoE especially for\nDRMs with a low number of parameters (i.e., TinyBERT), as it consistently\noutperforms the fine-tuned underlying model on all four benchmarks. For DRMs\nwith a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires\nlarger numbers of training samples to yield better retrieval performance.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11864v1",
    "published_date": "2024-12-16 15:20:13 UTC",
    "updated_date": "2024-12-16 15:20:13 UTC"
  },
  {
    "arxiv_id": "2412.11855v2",
    "title": "A Theory of Formalisms for Representing Knowledge",
    "authors": [
      "Heng Zhang",
      "Guifei Jiang",
      "Donghui Quan"
    ],
    "abstract": "There has been a longstanding dispute over which formalism is the best for\nrepresenting knowledge in AI. The well-known \"declarative vs. procedural\ncontroversy\" is concerned with the choice of utilizing declarations or\nprocedures as the primary mode of knowledge representation. The ongoing debate\nbetween symbolic AI and connectionist AI also revolves around the question of\nwhether knowledge should be represented implicitly (e.g., as parametric\nknowledge in deep learning and large language models) or explicitly (e.g., as\nlogical theories in traditional knowledge representation and reasoning). To\naddress these issues, we propose a general framework to capture various\nknowledge representation formalisms in which we are interested. Within the\nframework, we find a family of universal knowledge representation formalisms,\nand prove that all universal formalisms are recursively isomorphic. Moreover,\nwe show that all pairwise intertranslatable formalisms that admit the padding\nproperty are also recursively isomorphic. These imply that, up to an offline\ncompilation, all universal (or natural and equally expressive) representation\nformalisms are in fact the same, which thus provides a partial answer to the\naforementioned dispute.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of a paper to appear in AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2412.11855v2",
    "published_date": "2024-12-16 15:13:30 UTC",
    "updated_date": "2024-12-29 13:14:29 UTC"
  },
  {
    "arxiv_id": "2412.11834v3",
    "title": "Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture",
    "authors": [
      "Jingze Shi",
      "Bingheng Wu"
    ],
    "abstract": "In order to make the foundation model more efficient and effective, our idea\nis combining sequence transformation and state transformation. First, we prove\nthe availability of rotary position embedding in the state space duality\nalgorithm, which reduces the perplexity of the hybrid quadratic causal\nself-attention and state space duality by more than 4%, to ensure that the\ncombining sequence transformation unifies position encoding. Second, we propose\ndynamic mask attention, which maintains 100% accuracy in the more challenging\nmulti-query associative recall task, improving by more than 150% compared to\nquadratic causal self-attention and state space duality, to ensure that the\ncombining sequence transformation selectively filters relevant information.\nThird, we design cross domain mixture of experts, which makes the computational\nspeed of expert retrieval with more than 1024 experts 8 to 10 times faster than\nthe mixture of experts, to ensure that the combining state transformation\nquickly retrieval mixture. Finally, we summarize these matrix algorithms that\ncan form the foundation model: Wonderful Matrices, which can be a competitor to\npopular model architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "The code is open-sourced at\n  https://github.com/LoserCheems/WonderfulMatrices",
    "pdf_url": "http://arxiv.org/pdf/2412.11834v3",
    "published_date": "2024-12-16 14:56:28 UTC",
    "updated_date": "2024-12-20 11:43:13 UTC"
  },
  {
    "arxiv_id": "2412.12232v1",
    "title": "You Only Submit One Image to Find the Most Suitable Generative Model",
    "authors": [
      "Zhi Zhou",
      "Lan-Zhe Guo",
      "Peng-Xiao Song",
      "Yu-Feng Li"
    ],
    "abstract": "Deep generative models have achieved promising results in image generation,\nand various generative model hubs, e.g., Hugging Face and Civitai, have been\ndeveloped that enable model developers to upload models and users to download\nmodels. However, these model hubs lack advanced model management and\nidentification mechanisms, resulting in users only searching for models through\ntext matching, download sorting, etc., making it difficult to efficiently find\nthe model that best meets user requirements. In this paper, we propose a novel\nsetting called Generative Model Identification (GMI), which aims to enable the\nuser to identify the most appropriate generative model(s) for the user's\nrequirements from a large number of candidate models efficiently. To our best\nknowledge, it has not been studied yet. In this paper, we introduce a\ncomprehensive solution consisting of three pivotal modules: a weighted Reduced\nKernel Mean Embedding (RKME) framework for capturing the generated image\ndistribution and the relationship between images and prompts, a pre-trained\nvision-language model aimed at addressing dimensionality challenges, and an\nimage interrogator designed to tackle cross-modality issues. Extensive\nempirical results demonstrate the proposal is both efficient and effective. For\nexample, users only need to submit a single example image to describe their\nrequirements, and the model platform can achieve an average top-4\nidentification accuracy of more than 80%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2023 Workshop on Diffusion Models",
    "pdf_url": "http://arxiv.org/pdf/2412.12232v1",
    "published_date": "2024-12-16 14:46:57 UTC",
    "updated_date": "2024-12-16 14:46:57 UTC"
  },
  {
    "arxiv_id": "2412.14203v1",
    "title": "BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement",
    "authors": [
      "Yuhao Du",
      "Shunian Chen",
      "Wenbo Zan",
      "Peizhao Li",
      "Mingxuan Wang",
      "Dingjie Song",
      "Bo Li",
      "Yan Hu",
      "Benyou Wang"
    ],
    "abstract": "The application of Large Language Models (LLMs) in Computer-Aided Design\n(CAD) remains an underexplored area, despite their remarkable advancements in\nother domains. In this paper, we present BlenderLLM, a novel framework for\ntraining LLMs specifically for CAD tasks leveraging a self-improvement\nmethodology. To support this, we developed a bespoke training dataset,\nBlendNet, and introduced a comprehensive evaluation suite, CADBench. Our\nresults reveal that existing models demonstrate significant limitations in\ngenerating accurate CAD scripts. However, through minimal instruction-based\nfine-tuning and iterative self-improvement, BlenderLLM significantly surpasses\nthese models in both functionality and accuracy of CAD script generation. This\nresearch establishes a strong foundation for the application of LLMs in CAD\nwhile demonstrating the transformative potential of self-improving models in\nadvancing CAD automation. We encourage further exploration and adoption of\nthese methodologies to drive innovation in the field. The dataset, model,\nbenchmark, and source code are publicly available at\nhttps://github.com/FreedomIntelligence/BlenderLLM",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.14203v1",
    "published_date": "2024-12-16 14:34:02 UTC",
    "updated_date": "2024-12-16 14:34:02 UTC"
  },
  {
    "arxiv_id": "2412.11807v2",
    "title": "PhysAug: A Physical-guided and Frequency-based Data Augmentation for Single-Domain Generalized Object Detection",
    "authors": [
      "Xiaoran Xu",
      "Jiangang Yang",
      "Wenhui Shi",
      "Siyuan Ding",
      "Luqing Luo",
      "Jian Liu"
    ],
    "abstract": "Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single\nsource domain for robust performance across a variety of unseen target domains\nby taking advantage of an object detector. Existing S-DGOD approaches often\nrely on data augmentation strategies, including a composition of visual\ntransformations, to enhance the detector's generalization ability. However, the\nabsence of real-world prior knowledge hinders data augmentation from\ncontributing to the diversity of training data distributions. To address this\nissue, we propose PhysAug, a novel physical model-based non-ideal imaging\ncondition data augmentation method, to enhance the adaptability of the S-DGOD\ntasks. Drawing upon the principles of atmospheric optics, we develop a\nuniversal perturbation model that serves as the foundation for our proposed\nPhysAug. Given that visual perturbations typically arise from the interaction\nof light with atmospheric particles, the image frequency spectrum is harnessed\nto simulate real-world variations during training. This approach fosters the\ndetector to learn domain-invariant representations, thereby enhancing its\nability to generalize across various settings. Without altering the network\narchitecture or loss function, our approach significantly outperforms the\nstate-of-the-art across various S-DGOD datasets. In particular, it achieves a\nsubstantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and\nCityscape-C, highlighting its enhanced generalizability in real-world settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to AAAI,2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11807v2",
    "published_date": "2024-12-16 14:18:01 UTC",
    "updated_date": "2025-02-21 03:06:07 UTC"
  },
  {
    "arxiv_id": "2412.11802v1",
    "title": "AMI-Net: Adaptive Mask Inpainting Network for Industrial Anomaly Detection and Localization",
    "authors": [
      "Wei Luo",
      "Haiming Yao",
      "Wenyong Yu",
      "Zhengyong Li"
    ],
    "abstract": "Unsupervised visual anomaly detection is crucial for enhancing industrial\nproduction quality and efficiency. Among unsupervised methods, reconstruction\napproaches are popular due to their simplicity and effectiveness. The key\naspect of reconstruction methods lies in the restoration of anomalous regions,\nwhich current methods have not satisfactorily achieved. To tackle this issue,\nwe introduce a novel \\uline{A}daptive \\uline{M}ask \\uline{I}npainting\n\\uline{Net}work (AMI-Net) from the perspective of adaptive mask-inpainting. In\ncontrast to traditional reconstruction methods that treat non-semantic image\npixels as targets, our method uses a pre-trained network to extract multi-scale\nsemantic features as reconstruction targets. Given the multiscale nature of\nindustrial defects, we incorporate a training strategy involving random\npositional and quantitative masking. Moreover, we propose an innovative\nadaptive mask generator capable of generating adaptive masks that effectively\nmask anomalous regions while preserving normal regions. In this manner, the\nmodel can leverage the visible normal global contextual information to restore\nthe masked anomalous regions, thereby effectively suppressing the\nreconstruction of defects. Extensive experimental results on the MVTec AD and\nBTAD industrial datasets validate the effectiveness of the proposed method.\nAdditionally, AMI-Net exhibits exceptional real-time performance, striking a\nfavorable balance between detection accuracy and speed, rendering it highly\nsuitable for industrial applications. Code is available at:\nhttps://github.com/luow23/AMI-Net",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Automation Science and\n  Engineering.Code is available at: https://github.com/luow23/AMI-Net",
    "pdf_url": "http://arxiv.org/pdf/2412.11802v1",
    "published_date": "2024-12-16 14:12:06 UTC",
    "updated_date": "2024-12-16 14:12:06 UTC"
  },
  {
    "arxiv_id": "2412.11787v1",
    "title": "A Method for Detecting Legal Article Competition for Korean Criminal Law Using a Case-augmented Mention Graph",
    "authors": [
      "Seonho An",
      "Young Yik Rhim",
      "Min-Soo Kim"
    ],
    "abstract": "As social systems become increasingly complex, legal articles are also\ngrowing more intricate, making it progressively harder for humans to identify\nany potential competitions among them, particularly when drafting new laws or\napplying existing laws. Despite this challenge, no method for detecting such\ncompetitions has been proposed so far. In this paper, we propose a new legal AI\ntask called Legal Article Competition Detection (LACD), which aims to identify\ncompeting articles within a given law. Our novel retrieval method, CAM-Re2,\noutperforms existing relevant methods, reducing false positives by 20.8% and\nfalse negatives by 8.3%, while achieving a 98.2% improvement in precision@5,\nfor the LACD task. We release our codes at\nhttps://github.com/asmath472/LACD-public.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "under review",
    "pdf_url": "http://arxiv.org/pdf/2412.11787v1",
    "published_date": "2024-12-16 13:59:10 UTC",
    "updated_date": "2024-12-16 13:59:10 UTC"
  },
  {
    "arxiv_id": "2501.10377v1",
    "title": "The Three Social Dimensions of Chatbot Technology",
    "authors": [
      "Mauricio Figueroa-Torres"
    ],
    "abstract": "The development and deployment of chatbot technology, while spanning decades\nand employing different techniques, require innovative frameworks to understand\nand interrogate their functionality and implications. A mere technocentric\naccount of the evolution of chatbot technology does not fully illuminate how\nconversational systems are embedded in societal dynamics. This study presents a\nstructured examination of chatbots across three societal dimensions,\nhighlighting their roles as objects of scientific research, commercial\ninstruments, and agents of intimate interaction. Through furnishing a\ndimensional framework for the evolution of conversational systems, from\nlaboratories to marketplaces to private lives, this article contributes to the\nwider scholarly inquiry of chatbot technology and its impact in lived human\nexperiences and dynamics.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "I.2; J.4; K.4"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.10377v1",
    "published_date": "2024-12-16 13:45:53 UTC",
    "updated_date": "2024-12-16 13:45:53 UTC"
  },
  {
    "arxiv_id": "2412.11769v1",
    "title": "Does it Chug? Towards a Data-Driven Understanding of Guitar Tone Description",
    "authors": [
      "Pratik Sutar",
      "Jason Naradowsky",
      "Yusuke Miyao"
    ],
    "abstract": "Natural language is commonly used to describe instrument timbre, such as a\n\"warm\" or \"heavy\" sound. As these descriptors are based on human perception,\nthere can be disagreement over which acoustic features correspond to a given\nadjective. In this work, we pursue a data-driven approach to further our\nunderstanding of such adjectives in the context of guitar tone. Our main\ncontribution is a dataset of timbre adjectives, constructed by processing\nsingle clips of instrument audio to produce varied timbres through adjustments\nin EQ and effects such as distortion. Adjective annotations are obtained for\neach clip by crowdsourcing experts to complete a pairwise comparison and a\nlabeling task. We examine the dataset and reveal correlations between adjective\nratings and highlight instances where the data contradicts prevailing theories\non spectral features and timbral adjectives, suggesting a need for a more\nnuanced, data-driven understanding of timbre.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication at the 3rd Workshop on NLP for Music and\n  Audio (NLP4MusA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.11769v1",
    "published_date": "2024-12-16 13:44:19 UTC",
    "updated_date": "2024-12-16 13:44:19 UTC"
  },
  {
    "arxiv_id": "2412.11768v2",
    "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
    "authors": [
      "Minghao Xu",
      "Lichuan Xiang",
      "Xu Cai",
      "Hongkai Wen"
    ],
    "abstract": "In this work, we question the necessity of adaptive gradient methods for\ntraining deep neural networks. SGD-SaI is a simple yet effective enhancement to\nstochastic gradient descent with momentum (SGDM). SGD-SaI performs learning\nrate Scaling at Initialization (SaI) to distinct parameter groups, guided by\ntheir respective gradient signal-to-noise ratios (g-SNR). By adjusting learning\nrates without relying on adaptive second-order momentum, SGD-SaI helps prevent\ntraining imbalances from the very first iteration and cuts the optimizer's\nmemory usage by half compared to AdamW. Despite its simplicity and efficiency,\nSGD-SaI consistently matches or outperforms AdamW in training a variety of\nTransformer-based tasks, effectively overcoming a long-standing challenge of\nusing SGD for training Transformers. SGD-SaI excels in ImageNet-1K\nclassification with Vision Transformers(ViT) and GPT-2 pretraining for large\nlanguage models (LLMs, transformer decoder-only), demonstrating robustness to\nhyperparameter variations and practicality for diverse applications. We further\ntested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion\nmodels, where it consistently outperforms state-of-the-art optimizers. From a\nmemory efficiency perspective, SGD-SaI achieves substantial memory savings for\noptimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters)\nand 25.15 GB for Llama2-7B compared to AdamW in full-precision training\nsettings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.11768v2",
    "published_date": "2024-12-16 13:41:37 UTC",
    "updated_date": "2024-12-17 09:30:44 UTC"
  },
  {
    "arxiv_id": "2412.11761v2",
    "title": "Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control",
    "authors": [
      "Timothée Anne",
      "Noah Syrkis",
      "Meriem Elhosni",
      "Florian Turati",
      "Franck Legendre",
      "Alain Jaquier",
      "Sebastian Risi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. Their potential to facilitate human coordination with many\nagents is a promising but largely under-explored area. Such capabilities would\nbe helpful in disaster response, urban planning, and real-time strategy\nscenarios. In this work, we introduce (1) a real-time strategy game benchmark\ndesigned to evaluate these abilities and (2) a novel framework we term HIVE.\nHIVE empowers a single human to coordinate swarms of up to 2,000 agents through\na natural language dialog with an LLM. We present promising results on this\nmulti-agent benchmark, with our hybrid approach solving tasks such as\ncoordinating agent movements, exploiting unit weaknesses, leveraging human\nannotations, and understanding terrain and strategic points. Our findings also\nhighlight critical limitations of current models, including difficulties in\nprocessing spatial visual information and challenges in formulating long-term\nstrategic plans. This work sheds light on the potential and limitations of LLMs\nin human-swarm coordination, paving the way for future research in this area.\nThe HIVE project page, hive.syrkis.com, includes videos of the system in\naction.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11761v2",
    "published_date": "2024-12-16 13:25:42 UTC",
    "updated_date": "2025-04-22 11:24:23 UTC"
  },
  {
    "arxiv_id": "2412.11753v1",
    "title": "DriveGazen: Event-Based Driving Status Recognition using Conventional Camera",
    "authors": [
      "Xiaoyin Yang"
    ],
    "abstract": "We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 4 figures, (AAAI25)The 39th Annual AAAI Conference on\n  Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2412.11753v1",
    "published_date": "2024-12-16 13:12:11 UTC",
    "updated_date": "2024-12-16 13:12:11 UTC"
  },
  {
    "arxiv_id": "2412.11735v2",
    "title": "Transferable Adversarial Face Attack with Text Controlled Attribute",
    "authors": [
      "Wenyun Li",
      "Zheng Zhang",
      "Xiangyuan Lan",
      "Dongmei Jiang"
    ],
    "abstract": "Traditional adversarial attacks typically produce adversarial examples under\nnorm-constrained conditions, whereas unrestricted adversarial examples are\nfree-form with semantically meaningful perturbations. Current unrestricted\nadversarial impersonation attacks exhibit limited control over adversarial face\nattributes and often suffer from low transferability. In this paper, we propose\na novel Text Controlled Attribute Attack (TCA$^2$) to generate photorealistic\nadversarial impersonation faces guided by natural language. Specifically, the\ncategory-level personal softmax vector is employed to precisely guide the\nimpersonation attacks. Additionally, we propose both data and model\naugmentation strategies to achieve transferable attacks on unknown target\nmodels. Finally, a generative model, \\textit{i.e}, Style-GAN, is utilized to\nsynthesize impersonated faces with desired attributes. Extensive experiments on\ntwo high-resolution face recognition datasets validate that our TCA$^2$ method\ncan generate natural text-guided adversarial impersonation faces with high\ntransferability. We also evaluate our method on real-world face recognition\nsystems, \\textit{i.e}, Face++ and Aliyun, further demonstrating the practical\npotential of our approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11735v2",
    "published_date": "2024-12-16 12:56:57 UTC",
    "updated_date": "2025-02-02 06:47:16 UTC"
  },
  {
    "arxiv_id": "2502.17439v2",
    "title": "Large Language Models as Realistic Microservice Trace Generators",
    "authors": [
      "Donghyun Kim",
      "Sriram Ravula",
      "Taemin Ha",
      "Alexandros G. Dimakis",
      "Daehyeok Kim",
      "Aditya Akella"
    ],
    "abstract": "Workload traces are essential to understand complex computer systems'\nbehavior and manage processing and memory resources. Since real-world traces\nare hard to obtain, synthetic trace generation is a promising alternative. This\npaper proposes a first-of-a-kind approach that relies on training a large\nlanguage model (LLM) to generate synthetic workload traces, specifically\nmicroservice call graphs. To capture complex and arbitrary hierarchical\nstructures and implicit constraints in such traces, we show how to fine-tune\nLLMs to generate recursively, making call graph generation a sequence of easier\nsteps. To further enforce learning constraints in traces and generate uncommon\nsituations, we argue for applying additional instruction tuning steps to align\nour model with the desired trace features. Our evaluation results show that we\ncan generate diverse realistic traces under various conditions and outperform\nexisting methods in accuracy and validity. We demonstrate that our\nsynthetically generated traces can effectively replace real data to optimize\nimportant microservice management tasks. Additionally, our model adapts to\ndownstream trace-related tasks, such as predicting key trace features and\ninfilling missing data.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.OS"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.17439v2",
    "published_date": "2024-12-16 12:48:04 UTC",
    "updated_date": "2025-02-26 03:02:29 UTC"
  },
  {
    "arxiv_id": "2412.11716v1",
    "title": "LLMs Can Simulate Standardized Patients via Agent Coevolution",
    "authors": [
      "Zhuoyun Du",
      "Lujie Zheng",
      "Renjun Hu",
      "Yuyang Xu",
      "Xiawei Li",
      "Ying Sun",
      "Wei Chen",
      "Jian Wu",
      "Haolei Cai",
      "Haohao Ying"
    ],
    "abstract": "Training medical personnel using standardized patients (SPs) remains a\ncomplex challenge, requiring extensive domain expertise and role-specific\npractice. Most research on Large Language Model (LLM)-based simulated patients\nfocuses on improving data retrieval accuracy or adjusting prompts through human\nfeedback. However, this focus has overlooked the critical need for patient\nagents to learn a standardized presentation pattern that transforms data into\nhuman-like patient responses through unsupervised simulations. To address this\ngap, we propose EvoPatient, a novel simulated patient framework in which a\npatient agent and doctor agents simulate the diagnostic process through\nmulti-turn dialogues, simultaneously gathering experience to improve the\nquality of both questions and answers, ultimately enabling human doctor\ntraining. Extensive experiments on various cases demonstrate that, by providing\nonly overall SP requirements, our framework improves over existing reasoning\nmethods by more than 10% in requirement alignment and better human preference,\nwhile achieving an optimal balance of resource consumption after evolving over\n200 cases for 10 hours, with excellent generalizability. The code will be\navailable at https://github.com/ZJUMAI/EvoPatient.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2412.11716v1",
    "published_date": "2024-12-16 12:36:47 UTC",
    "updated_date": "2024-12-16 12:36:47 UTC"
  },
  {
    "arxiv_id": "2412.11710v1",
    "title": "Re-Attentional Controllable Video Diffusion Editing",
    "authors": [
      "Yuanzhi Wang",
      "Yong Li",
      "Mengyi Liu",
      "Xiaoya Zhang",
      "Xin Liu",
      "Zhen Cui",
      "Antoni B. Chan"
    ],
    "abstract": "Editing videos with textual guidance has garnered popularity due to its\nstreamlined process which mandates users to solely edit the text prompt\ncorresponding to the source video. Recent studies have explored and exploited\nlarge-scale text-to-image diffusion models for text-guided video editing,\nresulting in remarkable video editing capabilities. However, they may still\nsuffer from some limitations such as mislocated objects, incorrect number of\nobjects. Therefore, the controllability of video editing remains a formidable\nchallenge. In this paper, we aim to challenge the above limitations by\nproposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo)\nmethod. Specially, to align the spatial placement of the target objects with\nthe edited text prompt in a training-free manner, we propose a Re-Attentional\nDiffusion (RAD) to refocus the cross-attention activation responses between the\nedited text prompt and the target video during the denoising stage, resulting\nin a spatially location-aligned and semantically high-fidelity manipulated\nvideo. In particular, to faithfully preserve the invariant region content with\nless border artifacts, we propose an Invariant Region-guided Joint Sampling\n(IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant\nregions at each denoising timestep and constrain the generated content to be\nharmonized with the invariant region content. Experimental results verify that\nReAtCo consistently improves the controllability of video diffusion editing and\nachieves superior video editing performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2025. Codes are released at:\n  https://github.com/mdswyz/ReAtCo",
    "pdf_url": "http://arxiv.org/pdf/2412.11710v1",
    "published_date": "2024-12-16 12:32:21 UTC",
    "updated_date": "2024-12-16 12:32:21 UTC"
  },
  {
    "arxiv_id": "2412.11704v3",
    "title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data",
    "authors": [
      "Atsuki Yamaguchi",
      "Terufumi Morishita",
      "Aline Villavicencio",
      "Nikolaos Aletras"
    ],
    "abstract": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11704v3",
    "published_date": "2024-12-16 12:26:28 UTC",
    "updated_date": "2025-04-25 16:08:57 UTC"
  },
  {
    "arxiv_id": "2412.11698v2",
    "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?",
    "authors": [
      "Matteo Esposito",
      "Francesco Palagiano",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "abstract": "Context. The security of critical infrastructure has been a pressing concern\nsince the advent of computers and has become even more critical in today's era\nof cyber warfare. Protecting mission-critical systems (MCSs), essential for\nnational security, requires swift and robust governance, yet recent events\nreveal the increasing difficulty of meeting these challenges. Aim. Building on\nprior research showcasing the potential of Generative AI (GAI), such as Large\nLanguage Models, in enhancing risk analysis, we aim to explore practitioners'\nviews on integrating GAI into the governance of IT MCSs. Our goal is to provide\nactionable insights and recommendations for stakeholders, including\nresearchers, practitioners, and policymakers. Method. We designed a survey to\ncollect practical experiences, concerns, and expectations of practitioners who\ndevelop and implement security solutions in the context of MCSs. Conclusions\nand Future Works. Our findings highlight that the safe use of LLMs in MCS\ngovernance requires interdisciplinary collaboration. Researchers should focus\non designing regulation-oriented models and focus on accountability;\npractitioners emphasize data protection and transparency, while policymakers\nmust establish a unified AI framework with global benchmarks to ensure ethical\nand secure LLMs-based MCS governance.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11698v2",
    "published_date": "2024-12-16 12:21:05 UTC",
    "updated_date": "2025-01-10 13:35:37 UTC"
  },
  {
    "arxiv_id": "2412.12228v1",
    "title": "Linear Equations with Min and Max Operators: Computational Complexity",
    "authors": [
      "Krishnendu Chatterjee",
      "Ruichen Luo",
      "Raimundo Saona",
      "Jakub Svoboda"
    ],
    "abstract": "We consider a class of optimization problems defined by a system of linear\nequations with min and max operators. This class of optimization problems has\nbeen studied under restrictive conditions, such as, (C1) the halting or\nstability condition; (C2) the non-negative coefficients condition; (C3) the sum\nup to 1 condition; and (C4) the only min or only max oerator condition. Several\nseminal results in the literature focus on special cases. For example,\nturn-based stochastic games correspond to conditions C2 and C3; and Markov\ndecision process to conditions C2, C3, and C4. However, the systematic\ncomputational complexity study of all the cases has not been explored, which we\naddress in this work. Some highlights of our results are: with conditions C2\nand C4, and with conditions C3 and C4, the problem is NP-complete, whereas with\ncondition C1 only, the problem is in UP intersects coUP. Finally, we establish\nthe computational complexity of the decision problem of checking the respective\nconditions.",
    "categories": [
      "cs.CC",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.CC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12228v1",
    "published_date": "2024-12-16 12:18:36 UTC",
    "updated_date": "2024-12-16 12:18:36 UTC"
  },
  {
    "arxiv_id": "2412.11694v3",
    "title": "From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities",
    "authors": [
      "Shixin Jiang",
      "Jiafeng Liang",
      "Jiyuan Wang",
      "Xuan Dong",
      "Heng Chang",
      "Weijiang Yu",
      "Jinhua Du",
      "Ming Liu",
      "Bing Qin"
    ],
    "abstract": "To tackle complex tasks in real-world scenarios, more researchers are\nfocusing on Omni-MLLMs, which aim to achieve omni-modal understanding and\ngeneration. Beyond the constraints of any specific non-linguistic modality,\nOmni-MLLMs map various non-linguistic modalities into the embedding space of\nLLMs and enable the interaction and understanding of arbitrary combinations of\nmodalities within a single model. In this paper, we systematically investigate\nrelevant research and provide a comprehensive survey of Omni-MLLMs.\nSpecifically, we first explain the four core components of Omni-MLLMs for\nunified multi-modal modeling with a meticulous taxonomy that offers novel\nperspectives. Then, we introduce the effective integration achieved through\ntwo-stage training and discuss the corresponding datasets as well as\nevaluation. Furthermore, we summarize the main challenges of current Omni-MLLMs\nand outline future directions. We hope this paper serves as an introduction for\nbeginners and promotes the advancement of related research. Resources have been\nmade publicly available at https://github.com/threegold116/Awesome-Omni-MLLMs.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "35 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.11694v3",
    "published_date": "2024-12-16 12:12:45 UTC",
    "updated_date": "2025-03-04 01:47:20 UTC"
  },
  {
    "arxiv_id": "2412.11691v1",
    "title": "Multilingual and Explainable Text Detoxification with Parallel Corpora",
    "authors": [
      "Daryna Dementieva",
      "Nikolay Babakov",
      "Amit Ronen",
      "Abinew Ali Ayele",
      "Naquee Rizwan",
      "Florian Schneider",
      "Xintong Wang",
      "Seid Muhie Yimam",
      "Daniil Moskovskiy",
      "Elisei Stakovskii",
      "Eran Kaufman",
      "Ashraf Elnagar",
      "Animesh Mukherjee",
      "Alexander Panchenko"
    ],
    "abstract": "Even with various regulations in place across countries and social media\nplatforms (Government of India, 2021; European Parliament and Council of the\nEuropean Union, 2022, digital abusive speech remains a significant issue. One\npotential approach to address this challenge is automatic text detoxification,\na text style transfer (TST) approach that transforms toxic language into a more\nneutral or non-toxic form. To date, the availability of parallel corpora for\nthe text detoxification task (Logachevavet al., 2022; Atwell et al., 2022;\nDementievavet al., 2024a) has proven to be crucial for state-of-the-art\napproaches. With this work, we extend parallel text detoxification corpus to\nnew languages -- German, Chinese, Arabic, Hindi, and Amharic -- testing in the\nextensive multilingual setup TST baselines. Next, we conduct the first of its\nkind an automated, explainable analysis of the descriptive features of both\ntoxic and non-toxic sentences, diving deeply into the nuances, similarities,\nand differences of toxicity and detoxification across 9 languages. Finally,\nbased on the obtained insights, we experiment with a novel text detoxification\nmethod inspired by the Chain-of-Thoughts reasoning approach, enhancing the\nprompting process through clustering on relevant descriptive attributes.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "COLING 2025, main conference, long",
    "pdf_url": "http://arxiv.org/pdf/2412.11691v1",
    "published_date": "2024-12-16 12:08:59 UTC",
    "updated_date": "2024-12-16 12:08:59 UTC"
  },
  {
    "arxiv_id": "2412.11682v1",
    "title": "NEST: A Neuromodulated Small-world Hypergraph Trajectory Prediction Model for Autonomous Driving",
    "authors": [
      "Chengyue Wang",
      "Haicheng Liao",
      "Bonan Wang",
      "Yanchen Guan",
      "Bin Rao",
      "Ziyuan Pu",
      "Zhiyong Cui",
      "Chengzhong Xu",
      "Zhenning Li"
    ],
    "abstract": "Accurate trajectory prediction is essential for the safety and efficiency of\nautonomous driving. Traditional models often struggle with real-time\nprocessing, capturing non-linearity and uncertainty in traffic environments,\nefficiency in dense traffic, and modeling temporal dynamics of interactions. We\nintroduce NEST (Neuromodulated Small-world Hypergraph Trajectory Prediction), a\nnovel framework that integrates Small-world Networks and hypergraphs for\nsuperior interaction modeling and prediction accuracy. This integration enables\nthe capture of both local and extended vehicle interactions, while the\nNeuromodulator component adapts dynamically to changing traffic conditions. We\nvalidate the NEST model on several real-world datasets, including nuScenes,\nMoCAD, and HighD. The results consistently demonstrate that NEST outperforms\nexisting methods in various traffic scenarios, showcasing its exceptional\ngeneralization capability, efficiency, and temporal foresight. Our\ncomprehensive evaluation illustrates that NEST significantly improves the\nreliability and operational efficiency of autonomous driving systems, making it\na robust solution for trajectory prediction in complex traffic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by AAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2412.11682v1",
    "published_date": "2024-12-16 11:49:12 UTC",
    "updated_date": "2024-12-16 11:49:12 UTC"
  },
  {
    "arxiv_id": "2412.11681v1",
    "title": "Fast-staged CNN Model for Accurate pulmonary diseases and Lung cancer detection",
    "authors": [
      "Abdelbaki Souid",
      "Mohamed Hamroun",
      "Soufiene Ben Othman",
      "Hedi Sakli",
      "Naceur Abdelkarim"
    ],
    "abstract": "Pulmonary pathologies are a significant global health concern, often leading\nto fatal outcomes if not diagnosed and treated promptly. Chest radiography\nserves as a primary diagnostic tool, but the availability of experienced\nradiologists remains limited. Advances in Artificial Intelligence (AI) and\nmachine learning, particularly in computer vision, offer promising solutions to\naddress this challenge.\n  This research evaluates a deep learning model designed to detect lung cancer,\nspecifically pulmonary nodules, along with eight other lung pathologies, using\nchest radiographs. The study leverages diverse datasets comprising over 135,120\nfrontal chest radiographs to train a Convolutional Neural Network (CNN). A\ntwo-stage classification system, utilizing ensemble methods and transfer\nlearning, is employed to first triage images into Normal or Abnormal categories\nand then identify specific pathologies, including lung nodules.\n  The deep learning model achieves notable results in nodule classification,\nwith a top-performing accuracy of 77%, a sensitivity of 0.713, a specificity of\n0.776 during external validation, and an AUC score of 0.888. Despite these\nsuccesses, some misclassifications were observed, primarily false negatives.\n  In conclusion, the model demonstrates robust potential for generalization\nacross diverse patient populations, attributed to the geographic diversity of\nthe training dataset. Future work could focus on integrating ETL data\ndistribution strategies and expanding the dataset with additional nodule-type\nsamples to further enhance diagnostic accuracy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "IEEE International Workshop on Mechatronic Systems Supervision 2023",
    "pdf_url": "http://arxiv.org/pdf/2412.11681v1",
    "published_date": "2024-12-16 11:47:07 UTC",
    "updated_date": "2024-12-16 11:47:07 UTC"
  },
  {
    "arxiv_id": "2412.11679v1",
    "title": "Bias Vector: Mitigating Biases in Language Models with Task Arithmetic Approach",
    "authors": [
      "Daiki Shirafuji",
      "Makoto Takenaka",
      "Shinya Taguchi"
    ],
    "abstract": "The use of language models (LMs) has increased considerably in recent years,\nand the biases and stereotypes in training data that are reflected in the LM\noutputs are causing social problems. In this paper, inspired by the task\narithmetic, we propose the ``Bias Vector'' method for the mitigation of these\nLM biases. The Bias Vector method does not require manually created debiasing\ndata. The three main steps of our approach involve: (1) continual training the\npre-trained LMs on biased data using masked language modeling; (2) constructing\nthe Bias Vector as the difference between the weights of the biased LMs and\nthose of pre-trained LMs; and (3) subtracting the Bias Vector from the weights\nof the pre-trained LMs for debiasing. We evaluated the Bias Vector method on\nthe SEAT across three LMs and confirmed an average improvement of 0.177 points.\nWe demonstrated that the Bias Vector method does not degrade the LM performance\non downstream tasks in the GLUE benchmark. In addition, we examined the impact\nof scaling factors, which control the magnitudes of Bias Vectors, with effect\nsizes on the SEAT and conducted a comprehensive evaluation of our debiased LMs\nacross both the SEAT and GLUE benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to COLING2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11679v1",
    "published_date": "2024-12-16 11:38:23 UTC",
    "updated_date": "2024-12-16 11:38:23 UTC"
  },
  {
    "arxiv_id": "2412.11678v2",
    "title": "Loosely Synchronized Rule-Based Planning for Multi-Agent Path Finding with Asynchronous Actions",
    "authors": [
      "Shuai Zhou",
      "Shizhe Zhao",
      "Zhongqiang Ren"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) seeks collision-free paths for multiple\nagents from their respective starting locations to their respective goal\nlocations while minimizing path costs. Although many MAPF algorithms were\ndeveloped and can handle up to thousands of agents, they usually rely on the\nassumption that each action of the agent takes a time unit, and the actions of\nall agents are synchronized in a sense that the actions of agents start at the\nsame discrete time step, which may limit their use in practice. Only a few\nalgorithms were developed to address asynchronous actions, and they all lie on\none end of the spectrum, focusing on finding optimal solutions with limited\nscalability. This paper develops new planners that lie on the other end of the\nspectrum, trading off solution quality for scalability, by finding an unbounded\nsub-optimal solution for many agents. Our method leverages both search methods\n(LSS) in handling asynchronous actions and rule-based planning methods (PIBT)\nfor MAPF. We analyze the properties of our method and test it against several\nbaselines with up to 1000 agents in various maps. Given a runtime limit, our\nmethod can handle an order of magnitude more agents than the baselines with\nabout 25% longer makespan.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11678v2",
    "published_date": "2024-12-16 11:36:24 UTC",
    "updated_date": "2024-12-21 08:38:09 UTC"
  },
  {
    "arxiv_id": "2412.11674v1",
    "title": "UA-PDFL: A Personalized Approach for Decentralized Federated Learning",
    "authors": [
      "Hangyu Zhu",
      "Yuxiang Fan",
      "Zhenping Xie"
    ],
    "abstract": "Federated learning (FL) is a privacy preserving machine learning paradigm\ndesigned to collaboratively learn a global model without data leakage.\nSpecifically, in a typical FL system, the central server solely functions as an\ncoordinator to iteratively aggregate the collected local models trained by each\nclient, potentially introducing single-point transmission bottleneck and\nsecurity threats. To mitigate this issue, decentralized federated learning\n(DFL) has been proposed, where all participating clients engage in peer-to-peer\ncommunication without a central server. Nonetheless, DFL still suffers from\ntraining degradation as FL does due to the non-independent and identically\ndistributed (non-IID) nature of client data. And incorporating personalization\nlayers into DFL may be the most effective solutions to alleviate the side\neffects caused by non-IID data. Therefore, in this paper, we propose a novel\nunit representation aided personalized decentralized federated learning\nframework, named UA-PDFL, to deal with the non-IID challenge in DFL. By\nadaptively adjusting the level of personalization layers through the guidance\nof the unit representation, UA-PDFL is able to address the varying degrees of\ndata skew. Based on this scheme, client-wise dropout and layer-wise\npersonalization are proposed to further enhance the learning performance of\nDFL. Extensive experiments empirically prove the effectiveness of our proposed\nmethod.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11674v1",
    "published_date": "2024-12-16 11:27:35 UTC",
    "updated_date": "2024-12-16 11:27:35 UTC"
  },
  {
    "arxiv_id": "2412.11672v1",
    "title": "LLM-DaaS: LLM-driven Drone-as-a-Service Operations from Text User Requests",
    "authors": [
      "Lillian Wassim",
      "Kamal Mohamed",
      "Ali Hamdi"
    ],
    "abstract": "We propose LLM-DaaS, a novel Drone-as-a-Service (DaaS) framework that\nleverages Large Language Models (LLMs) to transform free-text user requests\ninto structured, actionable DaaS operation tasks. Our approach addresses the\nkey challenge of interpreting and structuring natural language input to\nautomate drone service operations under uncertain conditions. The system is\ncomposed of three main components: free-text request processing, structured\nrequest generation, and dynamic DaaS selection and composition. First, we\nfine-tune different LLM models such as Phi-3.5, LLaMA-3.2 7b and Gemma 2b on a\ndataset of text user requests mapped to structured DaaS requests. Users\ninteract with our model in a free conversational style, discussing package\ndelivery requests, while the fine-tuned LLM extracts DaaS metadata such as\ndelivery time, source and destination locations, and package weight. The DaaS\nservice selection model is designed to select the best available drone capable\nof delivering the requested package from the delivery point to the nearest\noptimal destination. Additionally, the DaaS composition model composes a\nservice from a set of the best available drones to deliver the package from the\nsource to the final destination. Second, the system integrates real-time\nweather data to optimize drone route planning and scheduling, ensuring safe and\nefficient operations. Simulations demonstrate the system's ability to\nsignificantly improve task accuracy, operational efficiency, and establish\nLLM-DaaS as a robust solution for DaaS operations in uncertain environments.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11672v1",
    "published_date": "2024-12-16 11:25:56 UTC",
    "updated_date": "2024-12-16 11:25:56 UTC"
  },
  {
    "arxiv_id": "2412.11671v1",
    "title": "BioBridge: Unified Bio-Embedding with Bridging Modality in Code-Switched EMR",
    "authors": [
      "Jangyeong Jeon",
      "Sangyeon Cho",
      "Dongjoon Lee",
      "Changhee Lee",
      "Junyeong Kim"
    ],
    "abstract": "Pediatric Emergency Department (PED) overcrowding presents a significant\nglobal challenge, prompting the need for efficient solutions. This paper\nintroduces the BioBridge framework, a novel approach that applies Natural\nLanguage Processing (NLP) to Electronic Medical Records (EMRs) in written\nfree-text form to enhance decision-making in PED. In non-English speaking\ncountries, such as South Korea, EMR data is often written in a Code-Switching\n(CS) format that mixes the native language with English, with most\ncode-switched English words having clinical significance. The BioBridge\nframework consists of two core modules: \"bridging modality in context\" and\n\"unified bio-embedding.\" The \"bridging modality in context\" module improves the\ncontextual understanding of bilingual and code-switched EMRs. In the \"unified\nbio-embedding\" module, the knowledge of the model trained in the medical domain\nis injected into the encoder-based model to bridge the gap between the medical\nand general domains. Experimental results demonstrate that the proposed\nBioBridge significantly performance traditional machine learning and\npre-trained encoder-based models on several metrics, including F1 score, area\nunder the receiver operating characteristic curve (AUROC), area under the\nprecision-recall curve (AUPRC), and Brier score. Specifically, BioBridge-XLM\nachieved enhancements of 0.85% in F1 score, 0.75% in AUROC, and 0.76% in AUPRC,\nalong with a notable 3.04% decrease in the Brier score, demonstrating marked\nimprovements in accuracy, reliability, and prediction calibration over the\nbaseline XLM model. The source code will be made publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at IEEE Access 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.11671v1",
    "published_date": "2024-12-16 11:24:54 UTC",
    "updated_date": "2024-12-16 11:24:54 UTC"
  },
  {
    "arxiv_id": "2412.12227v1",
    "title": "EDformer: Embedded Decomposition Transformer for Interpretable Multivariate Time Series Predictions",
    "authors": [
      "Sanjay Chakraborty",
      "Ibrahim Delibasoglu",
      "Fredrik Heintz"
    ],
    "abstract": "Time series forecasting is a crucial challenge with significant applications\nin areas such as weather prediction, stock market analysis, and scientific\nsimulations. This paper introduces an embedded decomposed transformer,\n'EDformer', for multivariate time series forecasting tasks. Without altering\nthe fundamental elements, we reuse the Transformer architecture and consider\nthe capable functions of its constituent parts in this work. Edformer first\ndecomposes the input multivariate signal into seasonal and trend components.\nNext, the prominent multivariate seasonal component is reconstructed across the\nreverse dimensions, followed by applying the attention mechanism and\nfeed-forward network in the encoder stage. In particular, the feed-forward\nnetwork is used for each variable frame to learn nonlinear representations,\nwhile the attention mechanism uses the time points of individual seasonal\nseries embedded within variate frames to capture multivariate correlations.\nTherefore, the trend signal is added with projection and performs the final\nforecasting. The EDformer model obtains state-of-the-art predicting results in\nterms of accuracy and efficiency on complex real-world time series datasets.\nThis paper also addresses model explainability techniques to provide insights\ninto how the model makes its predictions and why specific features or time\nsteps are important, enhancing the interpretability and trustworthiness of the\nforecasting results.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12227v1",
    "published_date": "2024-12-16 11:13:57 UTC",
    "updated_date": "2024-12-16 11:13:57 UTC"
  },
  {
    "arxiv_id": "2412.12226v1",
    "title": "Apollo-Forecast: Overcoming Aliasing and Inference Speed Challenges in Language Models for Time Series Forecasting",
    "authors": [
      "Tianyi Yin",
      "Jingwei Wang",
      "Yunlong Ma",
      "Han Wang",
      "Chenze Wang",
      "Yukai Zhao",
      "Min Liu",
      "Weiming Shen",
      "Yufeng Chen"
    ],
    "abstract": "Encoding time series into tokens and using language models for processing has\nbeen shown to substantially augment the models' ability to generalize to unseen\ntasks. However, existing language models for time series forecasting encounter\nseveral obstacles, including aliasing distortion and prolonged inference times,\nprimarily due to the limitations of quantization processes and the\ncomputational demands of large models. This paper introduces Apollo-Forecast, a\nnovel framework that tackles these challenges with two key innovations: the\nAnti-Aliasing Quantization Module (AAQM) and the Race Decoding (RD) technique.\nAAQM adeptly encodes sequences into tokens while mitigating high-frequency\nnoise in the original signals, thus enhancing both signal fidelity and overall\nquantization efficiency. RD employs a draft model to enable parallel processing\nand results integration, which markedly accelerates the inference speed for\nlong-term predictions, particularly in large-scale models. Extensive\nexperiments on various real-world datasets show that Apollo-Forecast\noutperforms state-of-the-art methods by 35.41\\% and 18.99\\% in WQL and MASE\nmetrics, respectively, in zero-shot scenarios. Furthermore, our method achieves\na 1.9X-2.7X acceleration in inference speed over baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12226v1",
    "published_date": "2024-12-16 11:01:20 UTC",
    "updated_date": "2024-12-16 11:01:20 UTC"
  },
  {
    "arxiv_id": "2412.11654v3",
    "title": "Smoothness Really Matters: A Simple Yet Effective Approach for Unsupervised Graph Domain Adaptation",
    "authors": [
      "Wei Chen",
      "Guo Ye",
      "Yakun Wang",
      "Zhao Zhang",
      "Libang Zhang",
      "Daixin Wang",
      "Zhiqiang Zhang",
      "Fuzhen Zhuang"
    ],
    "abstract": "Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, Accpected by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11654v3",
    "published_date": "2024-12-16 10:56:58 UTC",
    "updated_date": "2025-01-16 03:04:10 UTC"
  },
  {
    "arxiv_id": "2412.11652v1",
    "title": "SE-GCL: An Event-Based Simple and Effective Graph Contrastive Learning for Text Representation",
    "authors": [
      "Tao Meng",
      "Wei Ai",
      "Jianbin Li",
      "Ze Wang",
      "Yuntao Shou",
      "Keqin Li"
    ],
    "abstract": "Text representation learning is significant as the cornerstone of natural\nlanguage processing. In recent years, graph contrastive learning (GCL) has been\nwidely used in text representation learning due to its ability to represent and\ncapture complex text information in a self-supervised setting. However, current\nmainstream graph contrastive learning methods often require the incorporation\nof domain knowledge or cumbersome computations to guide the data augmentation\nprocess, which significantly limits the application efficiency and scope of\nGCL. Additionally, many methods learn text representations only by constructing\nword-document relationships, which overlooks the rich contextual semantic\ninformation in the text. To address these issues and exploit representative\ntextual semantics, we present an event-based, simple, and effective graph\ncontrastive learning (SE-GCL) for text representation. Precisely, we extract\nevent blocks from text and construct internal relation graphs to represent\ninter-semantic interconnections, which can ensure that the most critical\nsemantic information is preserved. Then, we devise a streamlined, unsupervised\ngraph contrastive learning framework to leverage the complementary nature of\nthe event semantic and structural information for intricate feature data\ncapture. In particular, we introduce the concept of an event skeleton for core\nrepresentation semantics and simplify the typically complex data augmentation\ntechniques found in existing graph contrastive learning to boost algorithmic\nefficiency. We employ multiple loss functions to prompt diverse embeddings to\nconverge or diverge within a confined distance in the vector space, ultimately\nachieving a harmonious equilibrium. We conducted experiments on the proposed\nSE-GCL on four standard data sets (AG News, 20NG, SougouNews, and THUCNews) to\nverify its effectiveness in text representation learning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.11652v1",
    "published_date": "2024-12-16 10:53:24 UTC",
    "updated_date": "2024-12-16 10:53:24 UTC"
  },
  {
    "arxiv_id": "2412.11643v1",
    "title": "A comprehensive GeoAI review: Progress, Challenges and Outlooks",
    "authors": [
      "Anasse Boutayeb",
      "Iyad Lahsen-cherif",
      "Ahmed El Khadimi"
    ],
    "abstract": "In recent years, Geospatial Artificial Intelligence (GeoAI) has gained\ntraction in the most relevant research works and industrial applications, while\nalso becoming involved in various fields of use. This paper offers a\ncomprehensive review of GeoAI as a synergistic concept applying Artificial\nIntelligence (AI) methods and models to geospatial data. A preliminary study is\ncarried out, identifying the methodology of the work, the research motivations,\nthe issues and the directions to be tracked, followed by exploring how GeoAI\ncan be used in various interesting fields of application, such as precision\nagriculture, environmental monitoring, disaster management and urban planning.\nNext, a statistical and semantic analysis is carried out, followed by a clear\nand precise presentation of the challenges facing GeoAI. Then, a concrete\nexploration of the future prospects is provided, based on several informations\ngathered during the census. To sum up, this paper provides a complete overview\nof the correlation between AI and the geospatial domain, while mentioning the\nresearches conducted in this context, and emphasizing the close relationship\nlinking GeoAI with other advanced concepts such as geographic information\nsystems (GIS) and large-scale geospatial data, known as big geodata. This will\nenable researchers and scientific community to assess the state of progress in\nthis promising field, and will help other interested parties to gain a better\nunderstanding of the issues involved.",
    "categories": [
      "cs.AI",
      "physics.geo-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "A comprehensive GeoAI review with 50 pages, 52 figures and 13 tables.\n  This paper explores the synergy between the most advanced artificial\n  intelligence techniques and geospatial data, while highlighting the close\n  relationship between this concept and the notions of GIS and big geodata",
    "pdf_url": "http://arxiv.org/pdf/2412.11643v1",
    "published_date": "2024-12-16 10:41:02 UTC",
    "updated_date": "2024-12-16 10:41:02 UTC"
  },
  {
    "arxiv_id": "2412.11642v1",
    "title": "Introduction to AI Planning",
    "authors": [
      "Marco Aiello",
      "Ilche Georgievski"
    ],
    "abstract": "These are notes for lectures presented at the University of Stuttgart that\nprovide an introduction to key concepts and techniques in AI Planning.\nArtificial Intelligence Planning, also known as Automated Planning, emerged\nsomewhere in 1966 from the need to give autonomy to a wheeled robot. Since\nthen, it has evolved into a flourishing research and development discipline,\noften associated with scheduling. Over the decades, various approaches to\nplanning have been developed with characteristics that make them appropriate\nfor specific tasks and applications. Most approaches represent the world as a\nstate within a state transition system; then the planning problem becomes that\nof searching a path in the state space from the current state to one which\nsatisfies the goals of the user. The notes begin by introducing the state model\nand move on to exploring classical planning, the foundational form of planning,\nand present fundamental algorithms for solving such problems. Subsequently, we\nexamine planning as a constraint satisfaction problem, outlining the mapping\nprocess and describing an approach to solve such problems. The most extensive\nsection is dedicated to Hierarchical Task Network (HTN) planning, one of the\nmost widely used and powerful planning techniques in the field. The lecture\nnotes end with a bonus chapter on the Planning Domain Definition (PDDL)\nLanguage, the de facto standard syntax for representing non-hierarchical\nplanning problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11642v1",
    "published_date": "2024-12-16 10:38:04 UTC",
    "updated_date": "2024-12-16 10:38:04 UTC"
  },
  {
    "arxiv_id": "2412.11632v1",
    "title": "Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in Human-Robot Collaboration",
    "authors": [
      "Juncheng Zou"
    ],
    "abstract": "Accurate human motion prediction is crucial for safe human-robot\ncollaboration but remains challenging due to the complexity of modeling\nintricate and variable human movements. This paper presents Parallel\nMulti-scale Incremental Prediction (PMS), a novel framework that explicitly\nmodels incremental motion across multiple spatio-temporal scales to capture\nsubtle joint evolutions and global trajectory shifts. PMS encodes these\nmulti-scale increments using parallel sequence branches, enabling iterative\nrefinement of predictions. A multi-stage training procedure with a\nfull-timeline loss integrates temporal context. Extensive experiments on four\ndatasets demonstrate substantial improvements in continuity, biomechanical\nconsistency, and long-term forecast stability by modeling inter-frame\nincrements. PMS achieves state-of-the-art performance, increasing prediction\naccuracy by 16.3%-64.2% over previous methods. The proposed multi-scale\nincremental approach provides a powerful technique for advancing human motion\nprediction capabilities critical for seamless human-robot interaction.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11632v1",
    "published_date": "2024-12-16 10:20:46 UTC",
    "updated_date": "2024-12-16 10:20:46 UTC"
  },
  {
    "arxiv_id": "2412.11620v3",
    "title": "Combating Semantic Contamination in Learning with Label Noise",
    "authors": [
      "Wenxiao Fan",
      "Kan Li"
    ],
    "abstract": "Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11620v3",
    "published_date": "2024-12-16 10:07:15 UTC",
    "updated_date": "2025-03-28 09:47:41 UTC"
  },
  {
    "arxiv_id": "2412.12225v3",
    "title": "DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis",
    "authors": [
      "Pan Wang",
      "Qiang Zhou",
      "Yawen Wu",
      "Tianlong Chen",
      "Jingtong Hu"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) leverages heterogeneous modalities, such\nas language, vision, and audio, to enhance the understanding of human\nsentiment. While existing models often focus on extracting shared information\nacross modalities or directly fusing heterogeneous modalities, such approaches\ncan introduce redundancy and conflicts due to equal treatment of all modalities\nand the mutual transfer of information between modality pairs. To address these\nissues, we propose a Disentangled-Language-Focused (DLF) multimodal\nrepresentation learning framework, which incorporates a feature disentanglement\nmodule to separate modality-shared and modality-specific information. To\nfurther reduce redundancy and enhance language-targeted features, four\ngeometric measures are introduced to refine the disentanglement process. A\nLanguage-Focused Attractor (LFA) is further developed to strengthen language\nrepresentation by leveraging complementary modality-specific information\nthrough a language-guided cross-attention mechanism. The framework also employs\nhierarchical predictions to improve overall accuracy. Extensive experiments on\ntwo popular MSA datasets, CMU-MOSI and CMU-MOSEI, demonstrate the significant\nperformance gains achieved by the proposed DLF framework. Comprehensive\nablation studies further validate the effectiveness of the feature\ndisentanglement module, language-focused attractor, and hierarchical\npredictions. Our code is available at https://github.com/pwang322/DLF.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025 accepted",
    "pdf_url": "http://arxiv.org/pdf/2412.12225v3",
    "published_date": "2024-12-16 10:03:44 UTC",
    "updated_date": "2025-04-09 00:52:30 UTC"
  },
  {
    "arxiv_id": "2412.11618v1",
    "title": "EvoLlama: Enhancing LLMs' Understanding of Proteins via Multimodal Structure and Sequence Representations",
    "authors": [
      "Nuowei Liu",
      "Changzhi Sun",
      "Tao Ji",
      "Junfeng Tian",
      "Jianxin Tang",
      "Yuanbin Wu",
      "Man Lan"
    ],
    "abstract": "Current Large Language Models (LLMs) for understanding proteins primarily\ntreats amino acid sequences as a text modality. Meanwhile, Protein Language\nModels (PLMs), such as ESM-2, have learned massive sequential evolutionary\nknowledge from the universe of natural protein sequences. Furthermore,\nstructure-based encoders like ProteinMPNN learn the structural information of\nproteins through Graph Neural Networks. However, whether the incorporation of\nprotein encoders can enhance the protein understanding of LLMs has not been\nexplored. To bridge this gap, we propose EvoLlama, a multimodal framework that\nconnects a structure-based encoder, a sequence-based protein encoder and an LLM\nfor protein understanding. EvoLlama consists of a ProteinMPNN structure\nencoder, an ESM-2 protein sequence encoder, a multimodal projector to align\nprotein and text representations and a Llama-3 text decoder. To train EvoLlama,\nwe fine-tune it on protein-oriented instructions and protein property\nprediction datasets verbalized via natural language instruction templates. Our\nexperiments show that EvoLlama's protein understanding capabilities have been\nsignificantly enhanced, outperforming other fine-tuned protein-oriented LLMs in\nzero-shot settings by an average of 1%-8% and surpassing the state-of-the-art\nbaseline with supervised fine-tuning by an average of 6%. On protein property\nprediction datasets, our approach achieves promising results that are\ncompetitive with state-of-the-art task-specific baselines. We will release our\ncode in a future version.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11618v1",
    "published_date": "2024-12-16 10:01:33 UTC",
    "updated_date": "2024-12-16 10:01:33 UTC"
  },
  {
    "arxiv_id": "2412.11605v2",
    "title": "SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models",
    "authors": [
      "Jiale Cheng",
      "Xiao Liu",
      "Cunxiang Wang",
      "Xiaotao Gu",
      "Yida Lu",
      "Dan Zhang",
      "Yuxiao Dong",
      "Jie Tang",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability, greatly enhancing models\nlike GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree\nsearch would impact model performance. Our code and data are publicly available\nat https://github.com/thu-coai/SPaR.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11605v2",
    "published_date": "2024-12-16 09:47:43 UTC",
    "updated_date": "2025-03-16 09:43:15 UTC"
  },
  {
    "arxiv_id": "2412.12223v2",
    "title": "Can video generation replace cinematographers? Research on the cinematic language of generated video",
    "authors": [
      "Xiaozhe Li",
      "Kai WU",
      "Siyi Yang",
      "YiZhan Qu",
      "Guohua. Zhang",
      "Zhiyu Chen",
      "Jiayao Li",
      "Jiangchuan Mu",
      "Xiaobin Hu",
      "Wen Fang",
      "Mingliang Xiong",
      "Hao Deng",
      "Qingwen Liu",
      "Gang Li",
      "Bin He"
    ],
    "abstract": "Recent advancements in text-to-video (T2V) generation have leveraged\ndiffusion models to enhance visual coherence in videos synthesized from textual\ndescriptions. However, existing research primarily focuses on object motion,\noften overlooking cinematic language, which is crucial for conveying emotion\nand narrative pacing in cinematography. To address this, we propose a threefold\napproach to improve cinematic control in T2V models. First, we introduce a\nmeticulously annotated cinematic language dataset with twenty subcategories,\ncovering shot framing, shot angles, and camera movements, enabling models to\nlearn diverse cinematic styles. Second, we present CameraDiff, which employs\nLoRA for precise and stable cinematic control, ensuring flexible shot\ngeneration. Third, we propose CameraCLIP, designed to evaluate cinematic\nalignment and guide multi-shot composition. Building on CameraCLIP, we\nintroduce CLIPLoRA, a CLIP-guided dynamic LoRA composition method that\nadaptively fuses multiple pre-trained cinematic LoRAs, enabling smooth\ntransitions and seamless style blending. Experimental results demonstrate that\nCameraDiff ensures stable and precise cinematic control, CameraCLIP achieves an\nR@1 score of 0.83, and CLIPLoRA significantly enhances multi-shot composition\nwithin a single video, bridging the gap between automated video generation and\nprofessional cinematography.\\textsuperscript{1}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.12223v2",
    "published_date": "2024-12-16 09:02:24 UTC",
    "updated_date": "2025-03-28 03:50:25 UTC"
  },
  {
    "arxiv_id": "2412.11556v1",
    "title": "Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs",
    "authors": [
      "Yuchen Fu",
      "Zifeng Cheng",
      "Zhiwei Jiang",
      "Zhonghui Wang",
      "Yafeng Yin",
      "Zhengliang Li",
      "Qing Gu"
    ],
    "abstract": "Extracting sentence embeddings from large language models (LLMs) is a\npromising direction, as LLMs have demonstrated stronger semantic understanding\ncapabilities. Previous studies typically focus on prompt engineering to elicit\nsentence embeddings from LLMs by prompting the model to encode sentence\ninformation into the embedding of the last token. However, LLMs are mostly\ndecoder-only models with causal attention and the earlier tokens in the\nsentence cannot attend to the latter tokens, resulting in biased encoding of\nsentence information and cascading effects on the final decoded token. To this\nend, we propose a novel Token Prepending (TP) technique that prepends each\nlayer's decoded sentence embedding to the beginning of the sentence in the next\nlayer's input, allowing earlier tokens to attend to the complete sentence\ninformation under the causal attention mechanism. The proposed TP technique is\na plug-and-play and training-free technique, which means it can be seamlessly\nintegrated with various prompt-based sentence embedding methods and\nautoregressive LLMs. Extensive experiments on various Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nproposed TP technique can significantly improve the performance of existing\nprompt-based sentence embedding methods across different LLMs, while incurring\nnegligible additional inference cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.11556v1",
    "published_date": "2024-12-16 08:42:00 UTC",
    "updated_date": "2024-12-16 08:42:00 UTC"
  },
  {
    "arxiv_id": "2412.11555v1",
    "title": "TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for Wildfire Detection and Prediction",
    "authors": [
      "Yu Zhao",
      "Sebastian Gerard",
      "Yifang Ban"
    ],
    "abstract": "Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11555v1",
    "published_date": "2024-12-16 08:40:12 UTC",
    "updated_date": "2024-12-16 08:40:12 UTC"
  },
  {
    "arxiv_id": "2412.11551v1",
    "title": "Region-Based Optimization in Continual Learning for Audio Deepfake Detection",
    "authors": [
      "Yujie Chen",
      "Jiangyan Yi",
      "Cunhang Fan",
      "Jianhua Tao",
      "Yong Ren",
      "Siding Zeng",
      "Chu Yuan Zhang",
      "Xinrui Yan",
      "Hao Gu",
      "Jun Xue",
      "Chenglong Wang",
      "Zhao Lv",
      "Xiaohui Zhang"
    ],
    "abstract": "Rapid advancements in speech synthesis and voice conversion bring convenience\nbut also new security risks, creating an urgent need for effective audio\ndeepfake detection. Although current models perform well, their effectiveness\ndiminishes when confronted with the diverse and evolving nature of real-world\ndeepfakes. To address this issue, we propose a continual learning method named\nRegion-Based Optimization (RegO) for audio deepfake detection. Specifically, we\nuse the Fisher information matrix to measure important neuron regions for real\nand fake audio detection, dividing them into four regions. First, we directly\nfine-tune the less important regions to quickly adapt to new tasks. Next, we\napply gradient optimization in parallel for regions important only to real\naudio detection, and in orthogonal directions for regions important only to\nfake audio detection. For regions that are important to both, we use sample\nproportion-based adaptive gradient optimization. This region-adaptive\noptimization ensures an appropriate trade-off between memory stability and\nlearning plasticity. Additionally, to address the increase of redundant neurons\nfrom old tasks, we further introduce the Ebbinghaus forgetting mechanism to\nrelease them, thereby promoting the capability of the model to learn more\ngeneralized discriminative features. Experimental results show our method\nachieves a 21.3% improvement in EER over the state-of-the-art continual\nlearning approach RWM for audio deepfake detection. Moreover, the effectiveness\nof RegO extends beyond the audio deepfake detection domain, showing potential\nsignificance in other tasks, such as image recognition. The code is available\nat https://github.com/cyjie429/RegO",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11551v1",
    "published_date": "2024-12-16 08:34:09 UTC",
    "updated_date": "2024-12-16 08:34:09 UTC"
  },
  {
    "arxiv_id": "2412.11543v2",
    "title": "Error Diversity Matters: An Error-Resistant Ensemble Method for Unsupervised Dependency Parsing",
    "authors": [
      "Behzad Shayegh",
      "Hobie H. -B. Lee",
      "Xiaodan Zhu",
      "Jackie Chi Kit Cheung",
      "Lili Mou"
    ],
    "abstract": "We address unsupervised dependency parsing by building an ensemble of diverse\nexisting models through post hoc aggregation of their output dependency parse\nstructures. We observe that these ensembles often suffer from low robustness\nagainst weak ensemble components due to error accumulation. To tackle this\nproblem, we propose an efficient ensemble-selection approach that considers\nerror diversity and avoids error accumulation. Results demonstrate that our\napproach outperforms each individual model as well as previous ensemble\ntechniques. Additionally, our experiments show that the proposed\nensemble-selection method significantly enhances the performance and robustness\nof our ensemble, surpassing previously proposed strategies, which have not\naccounted for error diversity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by the AAAI Conference on Artificial Intelligence (AAAI)\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11543v2",
    "published_date": "2024-12-16 08:23:50 UTC",
    "updated_date": "2025-02-06 21:00:42 UTC"
  },
  {
    "arxiv_id": "2412.11540v1",
    "title": "SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer",
    "authors": [
      "Jiaxu Wan",
      "Hong Zhang",
      "Ziqi He",
      "Qishu Wang",
      "Ding Yuan",
      "Yifan Yang"
    ],
    "abstract": "In 3D understanding, point transformers have yielded significant advances in\nbroadening the receptive field. However, further enhancement of the receptive\nfield is hindered by the constraints of grouping attention. The proxy-based\nmodel, as a hot topic in image and language feature extraction, uses global or\nlocal proxies to expand the model's receptive field. But global proxy-based\nmethods fail to precisely determine proxy positions and are not suited for\ntasks like segmentation and detection in the point cloud, and exist local\nproxy-based methods for image face difficulties in global-local balance, proxy\nsampling in various point clouds, and parallel cross-attention computation for\nsparse association. In this paper, we present SP$^2$T, a local proxy-based dual\nstream point transformer, which promotes global receptive field while\nmaintaining a balance between local and global information. To tackle robust 3D\nproxy sampling, we propose a spatial-wise proxy sampling with vertex-based\npoint proxy associations, ensuring robust point-cloud sampling in many scales\nof point cloud. To resolve economical association computation, we introduce\nsparse proxy attention combined with table-based relative bias, which enables\nlow-cost and precise interactions between proxy and point features.\nComprehensive experiments across multiple datasets reveal that our model\nachieves SOTA performance in downstream tasks. The code has been released in\nhttps://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 14 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.11540v1",
    "published_date": "2024-12-16 08:21:09 UTC",
    "updated_date": "2024-12-16 08:21:09 UTC"
  },
  {
    "arxiv_id": "2412.11538v2",
    "title": "MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond",
    "authors": [
      "Muhammad Huzaifah",
      "Geyu Lin",
      "Tianchi Liu",
      "Hardik B. Sailor",
      "Kye Min Tan",
      "Tarun K. Vangani",
      "Qiongqiong Wang",
      "Jeremy H. M. Wong",
      "Nancy F. Chen",
      "Ai Ti Aw"
    ],
    "abstract": "This technical report describes the MERaLiON-SpeechEncoder, a foundation\nmodel designed to support a wide range of downstream speech applications.\nDeveloped as part of Singapore's National Multimodal Large Language Model\nProgramme, the MERaLiON-SpeechEncoder is tailored to address the speech\nprocessing needs in Singapore and the surrounding Southeast Asian region. The\nmodel currently supports mainly English, including the variety spoken in\nSingapore. We are actively expanding our datasets to gradually cover other\nlanguages in subsequent releases. The MERaLiON-SpeechEncoder was pre-trained\nfrom scratch on 200,000 hours of unlabelled speech data using a self-supervised\nlearning approach based on masked language modelling. We describe our training\nprocedure and hyperparameter tuning experiments in detail below. Our evaluation\ndemonstrates improvements to spontaneous and Singapore speech benchmarks for\nspeech recognition, while remaining competitive to other state-of-the-art\nspeech encoders across ten other speech tasks. We commit to releasing our\nmodel, supporting broader research endeavours, both in Singapore and beyond.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11538v2",
    "published_date": "2024-12-16 08:15:19 UTC",
    "updated_date": "2024-12-20 09:12:47 UTC"
  },
  {
    "arxiv_id": "2412.11520v2",
    "title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting",
    "authors": [
      "Dong In Lee",
      "Hyeongcheol Park",
      "Jiyoung Seo",
      "Eunbyung Park",
      "Hyunje Park",
      "Ha Dam Baek",
      "Sangheon Shin",
      "Sangmin Kim",
      "Sangpil Kim"
    ],
    "abstract": "Recent advancements in 3D editing have highlighted the potential of\ntext-driven methods in real-time, user-friendly AR/VR applications. However,\ncurrent methods rely on 2D diffusion models without adequately considering\nmulti-view information, resulting in multi-view inconsistency. While 3D\nGaussian Splatting (3DGS) significantly improves rendering quality and speed,\nits 3D editing process encounters difficulties with inefficient optimization,\nas pre-trained Gaussians retain excessive source information, hindering\noptimization. To address these limitations, we propose EditSplat, a novel\ntext-driven 3D scene editing framework that integrates Multi-view Fusion\nGuidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view\nconsistency by incorporating essential multi-view information into the\ndiffusion process, leveraging classifier-free guidance from the text-to-image\ndiffusion model and the geometric structure inherent to 3DGS. Additionally, our\nAGT utilizes the explicit representation of 3DGS to selectively prune and\noptimize 3D Gaussians, enhancing optimization efficiency and enabling precise,\nsemantically rich local editing. Through extensive qualitative and quantitative\nevaluations, EditSplat achieves state-of-the-art performance, establishing a\nnew benchmark for text-driven 3D scene editing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11520v2",
    "published_date": "2024-12-16 07:56:04 UTC",
    "updated_date": "2025-04-17 20:10:32 UTC"
  },
  {
    "arxiv_id": "2412.11517v2",
    "title": "DART: An AIGT Detector using AMR of Rephrased Text",
    "authors": [
      "Hyeonchu Park",
      "Byungjun Kim",
      "Bugeun Kim"
    ],
    "abstract": "As large language models (LLMs) generate more human-like texts, concerns\nabout the side effects of AI-generated texts (AIGT) have grown. So, researchers\nhave developed methods for detecting AIGT. However, two challenges remain.\nFirst, the performance of detecting black-box LLMs is low because existing\nmodels focus on probabilistic features. Second, most AIGT detectors have been\ntested on a single-candidate setting, which assumes that we know the origin of\nan AIGT and which may deviate from the real-world scenario. To resolve these\nchallenges, we propose DART, which consists of four steps: rephrasing, semantic\nparsing, scoring, and multiclass classification. We conducted three experiments\nto test the performance of DART. The experimental result shows that DART can\ndiscriminate multiple black-box LLMs without probabilistic features and the\norigin of AIGT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Presented in NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11517v2",
    "published_date": "2024-12-16 07:51:09 UTC",
    "updated_date": "2025-02-04 10:52:02 UTC"
  },
  {
    "arxiv_id": "2412.11506v2",
    "title": "Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection",
    "authors": [
      "Guangsheng Bao",
      "Yanbin Zhao",
      "Juncai He",
      "Yue Zhang"
    ],
    "abstract": "Advanced large language models (LLMs) can generate text almost\nindistinguishable from human-written text, highlighting the importance of\nLLM-generated text detection. However, current zero-shot techniques face\nchallenges as white-box methods are restricted to use weaker open-source LLMs,\nand black-box methods are limited by partial observation from stronger\nproprietary LLMs. It seems impossible to enable white-box methods to use\nproprietary models because API-level access to the models neither provides full\npredictive distributions nor inner embeddings. To traverse the divide, we\npropose **Glimpse**, a probability distribution estimation approach, predicting\nthe full distributions from partial observations. Despite the simplicity of\nGlimpse, we successfully extend white-box methods like Entropy, Rank, Log-Rank,\nand Fast-DetectGPT to latest proprietary models. Experiments show that Glimpse\nwith Fast-DetectGPT and GPT-3.5 achieves an average AUROC of about 0.95 in five\nlatest source models, improving the score by 51% relative to the remaining\nspace of the open source baseline. It demonstrates that the latest LLMs can\neffectively detect their own outputs, suggesting that advanced LLMs may be the\nbest shield against themselves. We release our code and data at\nhttps://github.com/baoguangsheng/glimpse.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 camera version (10 pages, 9 figures, 9 tables)",
    "pdf_url": "http://arxiv.org/pdf/2412.11506v2",
    "published_date": "2024-12-16 07:28:36 UTC",
    "updated_date": "2025-02-19 07:37:55 UTC"
  },
  {
    "arxiv_id": "2412.11500v2",
    "title": "Intention Knowledge Graph Construction for User Intention Relation Modeling",
    "authors": [
      "Jiaxin Bai",
      "Zhaobo Wang",
      "Junfei Cheng",
      "Dan Yu",
      "Zerui Huang",
      "Weiqi Wang",
      "Xin Liu",
      "Chen Luo",
      "Yanming Zhu",
      "Bo Li",
      "Yangqiu Song"
    ],
    "abstract": "Understanding user intentions is challenging for online platforms. Recent\nwork on intention knowledge graphs addresses this but often lacks focus on\nconnecting intentions, which is crucial for modeling user behavior and\npredicting future actions. This paper introduces a framework to automatically\ngenerate an intention knowledge graph, capturing connections between user\nintentions. Using the Amazon m2 dataset, we construct an intention graph with\n351 million edges, demonstrating high plausibility and acceptance. Our model\neffectively predicts new session intentions and enhances product\nrecommendations, outperforming previous state-of-the-art methods and showcasing\nthe approach's practical utility.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11500v2",
    "published_date": "2024-12-16 07:18:40 UTC",
    "updated_date": "2025-05-17 05:38:32 UTC"
  },
  {
    "arxiv_id": "2412.11499v1",
    "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
    "authors": [
      "Wonje Choi",
      "Woo Kyung Kim",
      "Minjong Yoo",
      "Honguk Woo"
    ],
    "abstract": "We address the challenge of utilizing large language models (LLMs) for\ncomplex embodied tasks, in the environment where decision-making systems\noperate timely on capacity-limited, off-the-shelf devices. We present DeDer, a\nframework for decomposing and distilling the embodied reasoning capabilities\nfrom LLMs to efficient, small language model (sLM)-based policies. In DeDer,\nthe decision-making process of LLM-based strategies is restructured into a\nhierarchy with a reasoning-policy and planning-policy. The reasoning-policy is\ndistilled from the data that is generated through the embodied in-context\nlearning and self-verification of an LLM, so it can produce effective\nrationales. The planning-policy, guided by the rationales, can render optimized\nplans efficiently. In turn, DeDer allows for adopting sLMs for both policies,\ndeployed on off-the-shelf devices. Furthermore, to enhance the quality of\nintermediate rationales, specific to embodied tasks, we devise the embodied\nknowledge graph, and to generate multiple rationales timely through a single\ninference, we also use the contrastively prompted attention model. Our\nexperiments with the ALFRED benchmark demonstrate that DeDer surpasses leading\nlanguage planning and distillation approaches, indicating the applicability and\nefficiency of sLM-based embodied policies derived through DeDer.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.11499v1",
    "published_date": "2024-12-16 07:18:02 UTC",
    "updated_date": "2024-12-16 07:18:02 UTC"
  },
  {
    "arxiv_id": "2412.11489v1",
    "title": "HGSFusion: Radar-Camera Fusion with Hybrid Generation and Synchronization for 3D Object Detection",
    "authors": [
      "Zijian Gu",
      "Jianwei Ma",
      "Yan Huang",
      "Honghao Wei",
      "Zhanye Chen",
      "Hui Zhang",
      "Wei Hong"
    ],
    "abstract": "Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2412.11489v1",
    "published_date": "2024-12-16 07:06:17 UTC",
    "updated_date": "2024-12-16 07:06:17 UTC"
  },
  {
    "arxiv_id": "2412.11484v1",
    "title": "Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents",
    "authors": [
      "Wonje Choi",
      "Woo Kyung Kim",
      "SeungHyun Kim",
      "Honguk Woo"
    ],
    "abstract": "For embodied reinforcement learning (RL) agents interacting with the\nenvironment, it is desirable to have rapid policy adaptation to unseen visual\nobservations, but achieving zero-shot adaptation capability is considered as a\nchallenging problem in the RL context. To address the problem, we present a\nnovel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained\nvision-language model and a set of visual prompts, thus enabling efficient\npolicy learning and adaptation upon a wide range of environmental and physical\nchanges encountered by embodied agents. Specifically, we devise a\nguided-attention-based ensemble approach with multiple visual prompts on the\nvision-language model to construct robust state representations. Each prompt is\ncontrastively learned in terms of an individual domain factor that\nsignificantly affects the agent's egocentric perception and observation. For a\ngiven task, the attention-based ensemble and policy are jointly learned so that\nthe resulting state representations not only generalize to various domains but\nare also optimized for learning the task. Through experiments, we show that\nConPE outperforms other state-of-the-art algorithms for several embodied agent\ntasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,\nand autonomous driving in CARLA, while also improving the sample efficiency of\npolicy learning and adaptation.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at NeurIPS 2023",
    "pdf_url": "http://arxiv.org/pdf/2412.11484v1",
    "published_date": "2024-12-16 06:53:00 UTC",
    "updated_date": "2024-12-16 06:53:00 UTC"
  },
  {
    "arxiv_id": "2412.11472v1",
    "title": "Leveraging Foundation Language Models (FLMs) for Automated Cohort Extraction from Large EHR Databases",
    "authors": [
      "Purity Mugambi",
      "Alexandra Meliou",
      "Madalina Fiterau"
    ],
    "abstract": "A crucial step in cohort studies is to extract the required cohort from one\nor more study datasets. This step is time-consuming, especially when a\nresearcher is presented with a dataset that they have not previously worked\nwith. When the cohort has to be extracted from multiple datasets, cohort\nextraction can be extremely laborious. In this study, we present an approach\nfor partially automating cohort extraction from multiple electronic health\nrecord (EHR) databases. We formulate the guided multi-dataset cohort extraction\nproblem in which selection criteria are first converted into queries,\ntranslating them from natural language text to language that maps to database\nentities. Then, using FLMs, columns of interest identified from the queries are\nautomatically matched between the study databases. Finally, the generated\nqueries are run across all databases to extract the study cohort. We propose\nand evaluate an algorithm for automating column matching on two large, popular\nand publicly-accessible EHR databases -- MIMIC-III and eICU. Our approach\nachieves a high top-three accuracy of $92\\%$, correctly matching $12$ out of\nthe $13$ columns of interest, when using a small, pre-trained general purpose\nlanguage model. Furthermore, this accuracy is maintained even as the search\nspace (i.e., size of the database) increases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11472v1",
    "published_date": "2024-12-16 06:19:35 UTC",
    "updated_date": "2024-12-16 06:19:35 UTC"
  },
  {
    "arxiv_id": "2412.11471v1",
    "title": "Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning",
    "authors": [
      "Siyuan Liang",
      "Jiajun Gong",
      "Tianmeng Fang",
      "Aishan Liu",
      "Tao Wang",
      "Xianglong Liu",
      "Xiaochun Cao",
      "Dacheng Tao",
      "Chang Ee-Chien"
    ],
    "abstract": "Website fingerprint (WF) attacks, which covertly monitor user communications\nto identify the web pages they visit, pose a serious threat to user privacy.\nExisting WF defenses attempt to reduce the attacker's accuracy by disrupting\nunique traffic patterns; however, they often suffer from the trade-off between\noverhead and effectiveness, resulting in less usefulness in practice. To\novercome this limitation, we introduce Controllable Website Fingerprint Defense\n(CWFD), a novel defense perspective based on backdoor learning. CWFD exploits\nbackdoor vulnerabilities in neural networks to directly control the attacker's\nmodel by designing trigger patterns based on network traffic. Specifically,\nCWFD injects only incoming packets on the server side into the target web\npage's traffic, keeping overhead low while effectively poisoning the attacker's\nmodel during training. During inference, the defender can influence the\nattacker's model through a 'red pill, blue pill' choice: traces with the\ntrigger (red pill) lead to misclassification as the target web page, while\nnormal traces (blue pill) are classified correctly, achieving directed control\nover the defense outcome. We use the Fast Levenshtein-like distance as the\noptimization objective to compute trigger patterns that can be effectively\nassociated with our target page. Experiments show that CWFD significantly\nreduces RF's accuracy from 99% to 6% with 74% data overhead. In comparison,\nFRONT reduces accuracy to only 97% at similar overhead, while Palette achieves\n32% accuracy with 48% more overhead. We further validate the practicality of\nour method in a real Tor network environment.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "68M10",
      "C.2.0"
    ],
    "primary_category": "cs.CR",
    "comment": "18 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.11471v1",
    "published_date": "2024-12-16 06:12:56 UTC",
    "updated_date": "2024-12-16 06:12:56 UTC"
  },
  {
    "arxiv_id": "2412.11463v1",
    "title": "FedCAR: Cross-client Adaptive Re-weighting for Generative Models in Federated Learning",
    "authors": [
      "Minjun Kim",
      "Minjee Kim",
      "Jinhoon Jeong"
    ],
    "abstract": "Generative models trained on multi-institutional datasets can provide an\nenriched understanding through diverse data distributions. However, training\nthe models on medical images is often challenging due to hospitals' reluctance\nto share data for privacy reasons. Federated learning(FL) has emerged as a\nprivacy-preserving solution for training distributed datasets across data\ncenters by aggregating model weights from multiple clients instead of sharing\nraw data. Previous research has explored the adaptation of FL to generative\nmodels, yet effective aggregation algorithms specifically tailored for\ngenerative models remain unexplored. We hereby propose a novel algorithm aimed\nat improving the performance of generative models within FL. Our approach\nadaptively re-weights the contribution of each client, resulting in\nwell-trained shared parameters. In each round, the server side measures the\ndistribution distance between fake images generated by clients instead of\ndirectly comparing the Fr\\'echet Inception Distance per client, thereby\nenhancing efficiency of the learning. Experimental results on three public\nchest X-ray datasets show superior performance in medical image generation,\noutperforming both centralized learning and conventional FL algorithms. Our\ncode is available at https://github.com/danny0628/FedCAR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11463v1",
    "published_date": "2024-12-16 05:43:14 UTC",
    "updated_date": "2024-12-16 05:43:14 UTC"
  },
  {
    "arxiv_id": "2412.12221v1",
    "title": "Parallel Greedy Best-First Search with a Bound on the Number of Expansions Relative to Sequential Search",
    "authors": [
      "Takumi Shimoda",
      "Alex Fukunaga"
    ],
    "abstract": "Parallelization of non-admissible search algorithms such as GBFS poses a\nchallenge because straightforward parallelization can result in search behavior\nwhich significantly deviates from sequential search. Previous work proposed\nPUHF, a parallel search algorithm which is constrained to only expand states\nthat can be expanded by some tie-breaking strategy for GBFS. We show that\ndespite this constraint, the number of states expanded by PUHF is not bounded\nby a constant multiple of the number of states expanded by sequential GBFS with\nthe worst-case tie-breaking strategy. We propose and experimentally evaluate\nOne Bench At a Time (OBAT), a parallel greedy search which guarantees that the\nnumber of states expanded is within a constant factor of the number of states\nexpanded by sequential GBFS with some tie-breaking policy.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12221v1",
    "published_date": "2024-12-16 05:39:59 UTC",
    "updated_date": "2024-12-16 05:39:59 UTC"
  },
  {
    "arxiv_id": "2412.11461v1",
    "title": "Unsupervised Anomaly Detection for Tabular Data Using Noise Evaluation",
    "authors": [
      "Wei Dai",
      "Kai Hwang",
      "Jicong Fan"
    ],
    "abstract": "Unsupervised anomaly detection (UAD) plays an important role in modern data\nanalytics and it is crucial to provide simple yet effective and guaranteed UAD\nalgorithms for real applications. In this paper, we present a novel UAD method\nfor tabular data by evaluating how much noise is in the data. Specifically, we\npropose to learn a deep neural network from the clean (normal) training dataset\nand a noisy dataset, where the latter is generated by adding highly diverse\nnoises to the clean data. The neural network can learn a reliable decision\nboundary between normal data and anomalous data when the diversity of the\ngenerated noisy data is sufficiently high so that the hard abnormal samples lie\nin the noisy region. Importantly, we provide theoretical guarantees, proving\nthat the proposed method can detect anomalous data successfully, although the\nmethod does not utilize any real anomalous data in the training stage.\nExtensive experiments through more than 60 benchmark datasets demonstrate the\neffectiveness of the proposed method in comparison to 12 baselines of UAD. Our\nmethod obtains a 92.27\\% AUC score and a 1.68 ranking score on average.\nMoreover, compared to the state-of-the-art UAD methods, our method is easier to\nimplement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The paper was accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11461v1",
    "published_date": "2024-12-16 05:35:58 UTC",
    "updated_date": "2024-12-16 05:35:58 UTC"
  },
  {
    "arxiv_id": "2412.11455v1",
    "title": "Towards Better Multi-task Learning: A Framework for Optimizing Dataset Combinations in Large Language Models",
    "authors": [
      "Zaifu Zhan",
      "Rui Zhang"
    ],
    "abstract": "To efficiently select optimal dataset combinations for enhancing multi-task\nlearning (MTL) performance in large language models, we proposed a novel\nframework that leverages a neural network to predict the best dataset\ncombinations. The framework iteratively refines the selection, greatly\nimproving efficiency, while being model-, dataset-, and domain-independent.\nThrough experiments on 12 biomedical datasets across four tasks - named entity\nrecognition, relation extraction, event extraction, and text classification-we\ndemonstrate that our approach effectively identifies better combinations, even\nfor tasks that may seem unpromising from a human perspective. This verifies\nthat our framework provides a promising solution for maximizing MTL potential.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.11455v1",
    "published_date": "2024-12-16 05:20:18 UTC",
    "updated_date": "2024-12-16 05:20:18 UTC"
  },
  {
    "arxiv_id": "2412.11453v1",
    "title": "ACE-$M^3$: Automatic Capability Evaluator for Multimodal Medical Models",
    "authors": [
      "Xiechi Zhang",
      "Shunfan Zheng",
      "Linlin Wang",
      "Gerard de Melo",
      "Zhu Cao",
      "Xiaoling Wang",
      "Liang He"
    ],
    "abstract": "As multimodal large language models (MLLMs) gain prominence in the medical\nfield, the need for precise evaluation methods to assess their effectiveness\nhas become critical. While benchmarks provide a reliable means to evaluate the\ncapabilities of MLLMs, traditional metrics like ROUGE and BLEU employed for\nopen domain evaluation only focus on token overlap and may not align with human\njudgment. Although human evaluation is more reliable, it is labor-intensive,\ncostly, and not scalable. LLM-based evaluation methods have proven promising,\nbut to date, there is still an urgent need for open-source multimodal LLM-based\nevaluators in the medical field. To address this issue, we introduce ACE-$M^3$,\nan open-sourced \\textbf{A}utomatic \\textbf{C}apability \\textbf{E}valuator for\n\\textbf{M}ultimodal \\textbf{M}edical \\textbf{M}odels specifically designed to\nassess the question answering abilities of medical MLLMs. It first utilizes a\nbranch-merge architecture to provide both detailed analysis and a concise final\nscore based on standard medical evaluation criteria. Subsequently, a reward\ntoken-based direct preference optimization (RTDPO) strategy is incorporated to\nsave training time without compromising performance of our model. Extensive\nexperiments have demonstrated the effectiveness of our ACE-$M^3$\nmodel\\footnote{\\url{https://huggingface.co/collections/AIUSRTMP/ace-m3-67593297ff391b93e3e5d068}}\nin evaluating the capabilities of medical MLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11453v1",
    "published_date": "2024-12-16 05:15:43 UTC",
    "updated_date": "2024-12-16 05:15:43 UTC"
  },
  {
    "arxiv_id": "2412.11449v1",
    "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "authors": [
      "Prateek Verma"
    ],
    "abstract": "We propose WHISPER-GPT: A generative large language model (LLM) for speech\nand music that allows us to work with continuous audio representations and\ndiscrete tokens simultaneously as part of a single architecture. There has been\na huge surge in generative audio, speech, and music models that utilize\ndiscrete audio tokens derived from neural compression algorithms, e.g. ENCODEC.\nHowever, one of the major drawbacks of this approach is handling the context\nlength. It blows up for high-fidelity generative architecture if one has to\naccount for all the audio contents at various frequencies for the next token\nprediction. By combining continuous audio representation like the spectrogram\nand discrete acoustic tokens, we retain the best of both worlds: Have all the\ninformation needed from the audio at a specific time instance in a single\ntoken, yet allow LLM to predict the future token to allow for sampling and\nother benefits discrete space provides. We show how our architecture improves\nthe perplexity and negative log-likelihood scores for the next token prediction\ncompared to a token-based LLM for speech and music.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "6 pages, 3 figures. 50th International Conference on Acoustics,\n  Speech and Signal Processing, Hyderabad, India",
    "pdf_url": "http://arxiv.org/pdf/2412.11449v1",
    "published_date": "2024-12-16 05:03:48 UTC",
    "updated_date": "2024-12-16 05:03:48 UTC"
  },
  {
    "arxiv_id": "2412.11448v3",
    "title": "TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated Learning",
    "authors": [
      "Gangqiang Hu",
      "Jianfeng Lu",
      "Jianmin Han",
      "Shuqin Cao",
      "Jing Liu",
      "Hao Fu"
    ],
    "abstract": "Due to the sensitivity of data, Federated Learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized FL, clients' communication and training states are dynamic.\nThis variability arises from local training fluctuations, heterogeneous data\ndistributions, and intermittent client participation. Most existing studies\nprimarily focus on stable client states, neglecting the dynamic challenges\ninherent in real-world scenarios. To tackle this issue, we propose a\nTRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client\nstates and contributions, enhancing model training efficiency through selective\nclient participation. We focus on a semi-decentralized FL framework where edge\nservers and clients train a shared global model using unreliable intra-cluster\nmodel aggregation and inter-cluster model consensus. First, we propose an\nadaptive hidden semi-Markov model to estimate clients' communication states and\ncontributions. Next, we address a client-server association optimization\nproblem to minimize global training loss. Using convergence analysis, we\npropose a greedy client scheduling algorithm. Finally, our experiments\nconducted on real-world datasets demonstrate that TRAIL outperforms\nstate-of-the-art baselines, achieving an improvement of 8.7% in test accuracy\nand a reduction of 15.3% in training loss.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11448v3",
    "published_date": "2024-12-16 05:02:50 UTC",
    "updated_date": "2024-12-19 12:46:27 UTC"
  },
  {
    "arxiv_id": "2412.18619v2",
    "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey",
    "authors": [
      "Liang Chen",
      "Zekun Wang",
      "Shuhuai Ren",
      "Lei Li",
      "Haozhe Zhao",
      "Yunshui Li",
      "Zefan Cai",
      "Hongcheng Guo",
      "Lei Zhang",
      "Yizhe Xiong",
      "Yichi Zhang",
      "Ruoyu Wu",
      "Qingxiu Dong",
      "Ge Zhang",
      "Jian Yang",
      "Lingwei Meng",
      "Shujie Hu",
      "Yulong Chen",
      "Junyang Lin",
      "Shuai Bai",
      "Andreas Vlachos",
      "Xu Tan",
      "Minjia Zhang",
      "Wen Xiao",
      "Aaron Yee",
      "Tianyu Liu",
      "Baobao Chang"
    ],
    "abstract": "Building on the foundations of language modeling in natural language\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\nobjective for machine learning tasks across various modalities, achieving\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\nunderstanding and generation tasks within the textual modality, recent research\nhas shown that tasks from different modalities can also be effectively\nencapsulated within the NTP framework, transforming the multimodal information\ninto tokens and predict the next one given the context. This survey introduces\na comprehensive taxonomy that unifies both understanding and generation within\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\nrepresentation, datasets \\& evaluation, and open challenges. This new taxonomy\naims to aid researchers in their exploration of multimodal intelligence. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "69 papes, 18 figures, repo at\n  https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
    "pdf_url": "http://arxiv.org/pdf/2412.18619v2",
    "published_date": "2024-12-16 05:02:25 UTC",
    "updated_date": "2024-12-30 03:00:30 UTC"
  },
  {
    "arxiv_id": "2412.11446v1",
    "title": "Theoretical Analysis of Quality Diversity Algorithms for a Classical Path Planning Problem",
    "authors": [
      "Duc-Cuong Dang",
      "Aneta Neumann",
      "Frank Neumann",
      "Andre Opris",
      "Dirk Sudholt"
    ],
    "abstract": "Quality diversity (QD) algorithms have shown to provide sets of high quality\nsolutions for challenging problems in robotics, games, and combinatorial\noptimisation. So far, theoretical foundational explaining their good behaviour\nin practice lack far behind their practical success. We contribute to the\ntheoretical understanding of these algorithms and study the behaviour of QD\nalgorithms for a classical planning problem seeking several solutions. We study\nthe all-pairs-shortest-paths (APSP) problem which gives a natural formulation\nof the behavioural space based on all pairs of nodes of the given input graph\nthat can be used by Map-Elites QD algorithms. Our results show that Map-Elites\nQD algorithms are able to compute a shortest path for each pair of nodes\nefficiently in parallel. Furthermore, we examine parent selection techniques\nfor crossover that exhibit significant speed ups compared to the standard QD\napproach.",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11446v1",
    "published_date": "2024-12-16 04:58:32 UTC",
    "updated_date": "2024-12-16 04:58:32 UTC"
  },
  {
    "arxiv_id": "2412.11443v1",
    "title": "Universal Domain Adaptive Object Detection via Dual Probabilistic Alignment",
    "authors": [
      "Yuanfan Zheng",
      "Jinlin Wu",
      "Wuyang Li",
      "Zhen Chen"
    ],
    "abstract": "Domain Adaptive Object Detection (DAOD) transfers knowledge from a labeled\nsource domain to an unannotated target domain under closed-set assumption.\nUniversal DAOD (UniDAOD) extends DAOD to handle open-set, partial-set, and\nclosed-set domain adaptation. In this paper, we first unveil two issues:\ndomain-private category alignment is crucial for global-level features, and the\ndomain probability heterogeneity of features across different levels. To\naddress these issues, we propose a novel Dual Probabilistic Alignment (DPA)\nframework to model domain probability as Gaussian distribution, enabling the\nheterogeneity domain distribution sampling and measurement. The DPA consists of\nthree tailored modules: the Global-level Domain Private Alignment (GDPA), the\nInstance-level Domain Shared Alignment (IDSA), and the Private Class Constraint\n(PCC). GDPA utilizes the global-level sampling to mine domain-private category\nsamples and calculate alignment weight through a cumulative distribution\nfunction to address the global-level private category alignment. IDSA utilizes\ninstance-level sampling to mine domain-shared category samples and calculates\nalignment weight through Gaussian distribution to conduct the domain-shared\ncategory domain alignment to address the feature heterogeneity. The PCC\naggregates domain-private category centroids between feature and probability\nspaces to mitigate negative transfer. Extensive experiments demonstrate that\nour DPA outperforms state-of-the-art UniDAOD and DAOD methods across various\ndatasets and scenarios, including open, partial, and closed sets. Codes are\navailable at \\url{https://github.com/zyfone/DPA}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "This work is accepted by AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11443v1",
    "published_date": "2024-12-16 04:55:13 UTC",
    "updated_date": "2024-12-16 04:55:13 UTC"
  },
  {
    "arxiv_id": "2412.11439v4",
    "title": "Bayesian Flow Is All You Need to Sample Out-of-Distribution Chemical Spaces",
    "authors": [
      "Nianze Tao"
    ],
    "abstract": "Generating novel molecules with higher properties than the training space,\nnamely the out-of-distribution generation, is important for ${de~novo}$ drug\ndesign. However, it is not easy for distribution learning-based models, for\nexample diffusion models, to solve this challenge as these methods are designed\nto fit the distribution of training data as close as possible. In this paper,\nwe show that Bayesian flow network is capable of effortlessly generating high\nquality out-of-distribution samples that meet several scenarios. We introduce a\nsemi-autoregressive training/sampling method that helps to enhance the model\nperformance and surpass the state-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 10 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.11439v4",
    "published_date": "2024-12-16 04:43:54 UTC",
    "updated_date": "2025-02-15 01:29:33 UTC"
  },
  {
    "arxiv_id": "2412.11434v3",
    "title": "Auto-bidding in real-time auctions via Oracle Imitation Learning (OIL)",
    "authors": [
      "Alberto Silvio Chiappa",
      "Briti Gangopadhyay",
      "Zhao Wang",
      "Shingo Takamatsu"
    ],
    "abstract": "Online advertising has become one of the most successful business models of\nthe internet era. Impression opportunities are typically allocated through\nreal-time auctions, where advertisers bid to secure advertisement slots.\nDeciding the best bid for an impression opportunity is challenging, due to the\nstochastic nature of user behavior and the variability of advertisement traffic\nover time. In this work, we propose a framework for training auto-bidding\nagents in multi-slot second-price auctions to maximize acquisitions (e.g.,\nclicks, conversions) while adhering to budget and cost-per-acquisition (CPA)\nconstraints. We exploit the insight that, after an advertisement campaign\nconcludes, determining the optimal bids for each impression opportunity can be\nframed as a multiple-choice knapsack problem (MCKP) with a nonlinear objective.\nWe propose an \"oracle\" algorithm that identifies a near-optimal combination of\nimpression opportunities and advertisement slots, considering both past and\nfuture advertisement traffic data. This oracle solution serves as a training\ntarget for a student network which bids having access only to real-time\ninformation, a method we term Oracle Imitation Learning (OIL). Through\nnumerical experiments, we demonstrate that OIL achieves superior performance\ncompared to both online and offline reinforcement learning algorithms, offering\nimproved sample efficiency. Notably, OIL shifts the complexity of training\nauto-bidding agents from crafting sophisticated learning algorithms to solving\na nonlinear constrained optimization problem efficiently.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11434v3",
    "published_date": "2024-12-16 04:21:35 UTC",
    "updated_date": "2025-05-16 21:21:13 UTC"
  },
  {
    "arxiv_id": "2412.12220v1",
    "title": "Relieving Universal Label Noise for Unsupervised Visible-Infrared Person Re-Identification by Inferring from Neighbors",
    "authors": [
      "Xiao Teng",
      "Long Lan",
      "Dingyao Chen",
      "Kele Xu",
      "Nan Yin"
    ],
    "abstract": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of\ngreat research and practical significance yet remains challenging due to the\nabsence of annotations. Existing approaches aim to learn modality-invariant\nrepresentations in an unsupervised setting. However, these methods often\nencounter label noise within and across modalities due to suboptimal clustering\nresults and considerable modality discrepancies, which impedes effective\ntraining. To address these challenges, we propose a straightforward yet\neffective solution for USL-VI-ReID by mitigating universal label noise using\nneighbor information. Specifically, we introduce the Neighbor-guided Universal\nLabel Calibration (N-ULC) module, which replaces explicit hard pseudo labels in\nboth homogeneous and heterogeneous spaces with soft labels derived from\nneighboring samples to reduce label noise. Additionally, we present the\nNeighbor-guided Dynamic Weighting (N-DW) module to enhance training stability\nby minimizing the influence of unreliable samples. Extensive experiments on the\nRegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing\nUSL-VI-ReID approaches, despite its simplicity. The source code is available\nat: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12220v1",
    "published_date": "2024-12-16 04:04:41 UTC",
    "updated_date": "2024-12-16 04:04:41 UTC"
  },
  {
    "arxiv_id": "2412.11427v2",
    "title": "Towards Scientific Discovery with Generative AI: Progress, Opportunities, and Challenges",
    "authors": [
      "Chandan K Reddy",
      "Parshin Shojaee"
    ],
    "abstract": "Scientific discovery is a complex cognitive process that has driven human\nknowledge and technological progress for centuries. While artificial\nintelligence (AI) has made significant advances in automating aspects of\nscientific reasoning, simulation, and experimentation, we still lack integrated\nAI systems capable of performing autonomous long-term scientific research and\ndiscovery. This paper examines the current state of AI for scientific\ndiscovery, highlighting recent progress in large language models and other AI\ntechniques applied to scientific tasks. We then outline key challenges and\npromising research directions toward developing more comprehensive AI systems\nfor scientific discovery, including the need for science-focused AI agents,\nimproved benchmarks and evaluation metrics, multimodal scientific\nrepresentations, and unified frameworks combining reasoning, theorem proving,\nand data-driven modeling. Addressing these challenges could lead to\ntransformative AI tools to accelerate progress across disciplines towards\nscientific discovery.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11427v2",
    "published_date": "2024-12-16 03:52:20 UTC",
    "updated_date": "2024-12-21 19:22:53 UTC"
  },
  {
    "arxiv_id": "2412.11417v2",
    "title": "RL-LLM-DT: An Automatic Decision Tree Generation Method Based on RL Evaluation and LLM Enhancement",
    "authors": [
      "Junjie Lin",
      "Jian Zhao",
      "Lin Liu",
      "Yue Deng",
      "Youpeng Zhao",
      "Lanxiao Huang",
      "Xia Lin",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "abstract": "Traditionally, AI development for two-player zero-sum games has relied on two\nprimary techniques: decision trees and reinforcement learning (RL). A common\napproach involves using a fixed decision tree as one player's strategy while\ntraining an RL agent as the opponent to identify vulnerabilities in the\ndecision tree, thereby improving its strategic strength iteratively. However,\nthis process often requires significant human intervention to refine the\ndecision tree after identifying its weaknesses, resulting in inefficiencies and\nhindering full automation of the strategy enhancement process. Fortunately, the\nadvent of Large Language Models (LLMs) offers a transformative opportunity to\nautomate the process. We propose RL-LLM-DT, an automatic decision tree\ngeneration method based on RL Evaluation and LLM Enhancement. Given an initial\ndecision tree, the method involves two important iterative steps. Response\nPolicy Search: RL is used to discover counter-strategies targeting the decision\ntree. Policy Improvement: LLMs analyze failure scenarios and generate improved\ndecision tree code. In our method, RL focuses on finding the decision tree's\nflaws while LLM is prompted to generate an improved version of the decision\ntree. The iterative refinement process terminates when RL can't find any flaw\nof the tree or LLM fails to improve the tree. To evaluate the effectiveness of\nthis integrated approach, we conducted experiments in a curling game. After\niterative refinements, our curling AI based on the decision tree ranks first on\nthe Jidi platform among 34 curling AIs in total, which demonstrates that LLMs\ncan significantly enhance the robustness and adaptability of decision trees,\nrepresenting a substantial advancement in the field of Game AI. Our code is\navailable at https://github.com/Linjunjie99/RL-LLM-DT.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T05",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.AI",
    "comment": "Length:10 pages. Figures:10 figures. Additional Notes:In this paper,\n  we have introduced a novel hybrid approach which leverages the strengths of\n  both RL and LLMs to itera- tively refine decision tree tactics, enhancing\n  their performance and adaptability",
    "pdf_url": "http://arxiv.org/pdf/2412.11417v2",
    "published_date": "2024-12-16 03:33:49 UTC",
    "updated_date": "2024-12-17 04:04:12 UTC"
  },
  {
    "arxiv_id": "2412.11409v3",
    "title": "Multi-modal and Multi-scale Spatial Environment Understanding for Immersive Visual Text-to-Speech",
    "authors": [
      "Rui Liu",
      "Shuwei He",
      "Yifan Hu",
      "Haizhou Li"
    ],
    "abstract": "Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages,2 figures, Accepted by AAAI'2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11409v3",
    "published_date": "2024-12-16 03:25:23 UTC",
    "updated_date": "2025-01-15 01:59:02 UTC"
  },
  {
    "arxiv_id": "2412.11408v2",
    "title": "Federated Domain Generalization with Label Smoothing and Balanced Decentralized Training",
    "authors": [
      "Milad Soltany",
      "Farhad Pourpanah",
      "Mahdiyar Molahasani",
      "Michael Greenspan",
      "Ali Etemad"
    ],
    "abstract": "In this paper, we propose a novel approach, Federated Domain Generalization\nwith Label Smoothing and Balanced Decentralized Training (FedSB), to address\nthe challenges of data heterogeneity within a federated learning framework.\nFedSB utilizes label smoothing at the client level to prevent overfitting to\ndomain-specific features, thereby enhancing generalization capabilities across\ndiverse domains when aggregating local models into a global model.\nAdditionally, FedSB incorporates a decentralized budgeting mechanism which\nbalances training among clients, which is shown to improve the performance of\nthe aggregated global model. Extensive experiments on four commonly used\nmulti-domain datasets, PACS, VLCS, OfficeHome, and TerraInc, demonstrate that\nFedSB outperforms competing methods, achieving state-of-the-art results on\nthree out of four datasets, indicating the effectiveness of FedSB in addressing\ndata heterogeneity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.11408v2",
    "published_date": "2024-12-16 03:25:19 UTC",
    "updated_date": "2025-02-08 15:41:08 UTC"
  },
  {
    "arxiv_id": "2412.11404v1",
    "title": "Attention with Dependency Parsing Augmentation for Fine-Grained Attribution",
    "authors": [
      "Qiang Ding",
      "Lvzhou Luo",
      "Yixuan Cao",
      "Ping Luo"
    ],
    "abstract": "To assist humans in efficiently validating RAG-generated content, developing\na fine-grained attribution mechanism that provides supporting evidence from\nretrieved documents for every answer span is essential. Existing fine-grained\nattribution methods rely on model-internal similarity metrics between responses\nand documents, such as saliency scores and hidden state similarity. However,\nthese approaches suffer from either high computational complexity or\ncoarse-grained representations. Additionally, a common problem shared by the\nprevious works is their reliance on decoder-only Transformers, limiting their\nability to incorporate contextual information after the target span. To address\nthe above problems, we propose two techniques applicable to all\nmodel-internals-based methods. First, we aggregate token-wise evidence through\nset union operations, preserving the granularity of representations. Second, we\nenhance the attributor by integrating dependency parsing to enrich the semantic\ncompleteness of target spans. For practical implementation, our approach\nemploys attention weights as the similarity metric. Experimental results\ndemonstrate that the proposed method consistently outperforms all prior works.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 7 figures, submitted to ACL ARR 2024 October",
    "pdf_url": "http://arxiv.org/pdf/2412.11404v1",
    "published_date": "2024-12-16 03:12:13 UTC",
    "updated_date": "2024-12-16 03:12:13 UTC"
  },
  {
    "arxiv_id": "2412.15256v1",
    "title": "Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search",
    "authors": [
      "Edward Kim",
      "Manil Shrestha",
      "Richard Foty",
      "Tom DeLay",
      "Vicki Seyfert-Margolis"
    ],
    "abstract": "Creation and curation of knowledge graphs can accelerate disease discovery\nand analysis in real-world data. While disease ontologies aid in biological\ndata annotation, codified categories (SNOMED-CT, ICD10, CPT) may not capture\npatient condition nuances or rare diseases. Multiple disease definitions across\ndata sources complicate ontology mapping and disease clustering. We propose\ncreating patient knowledge graphs using large language model extraction\ntechniques, allowing data extraction via natural language rather than rigid\nontological hierarchies. Our method maps to existing ontologies (MeSH,\nSNOMED-CT, RxNORM, HPO) to ground extracted entities.\n  Using a large ambulatory care EHR database with 33.6M patients, we\ndemonstrate our method through the patient search for Dravet syndrome, which\nreceived ICD10 recognition in October 2020. We describe our construction of\npatient-specific knowledge graphs and symptom-based patient searches. Using\nconfirmed Dravet syndrome ICD10 codes as ground truth, we employ LLM-based\nentity extraction to characterize patients in grounded ontologies. We then\napply this method to identify Beta-propeller protein-associated\nneurodegeneration (BPAN) patients, demonstrating real-world discovery where no\nground truth exists.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 3 figures, Work published in 4th Workshop on Knowledge\n  Graphs and Big Data (In Conjunction with IEEE Big Data 2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.15256v1",
    "published_date": "2024-12-16 02:57:00 UTC",
    "updated_date": "2024-12-16 02:57:00 UTC"
  },
  {
    "arxiv_id": "2412.12219v1",
    "title": "Are Large Language Models Useful for Time Series Data Analysis?",
    "authors": [
      "Francis Tang",
      "Ying Ding"
    ],
    "abstract": "Time series data plays a critical role across diverse domains such as\nhealthcare, energy, and finance, where tasks like classification, anomaly\ndetection, and forecasting are essential for informed decision-making.\nRecently, large language models (LLMs) have gained prominence for their ability\nto handle complex data and extract meaningful insights. This study investigates\nwhether LLMs are effective for time series data analysis by comparing their\nperformance with non-LLM-based approaches across three tasks: classification,\nanomaly detection, and forecasting.\n  Through a series of experiments using GPT4TS and autoregressive models, we\nevaluate their performance on benchmark datasets and assess their accuracy,\nprecision, and ability to generalize. Our findings indicate that while\nLLM-based methods excel in specific tasks like anomaly detection, their\nbenefits are less pronounced in others, such as forecasting, where simpler\nmodels sometimes perform comparably or better. This research highlights the\nrole of LLMs in time series analysis and lays the groundwork for future studies\nto systematically explore their applications and limitations in handling\ntemporal data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.12219v1",
    "published_date": "2024-12-16 02:47:44 UTC",
    "updated_date": "2024-12-16 02:47:44 UTC"
  },
  {
    "arxiv_id": "2412.11387v1",
    "title": "How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach",
    "authors": [
      "Abdulrahman Althobaiti",
      "Angel Ayala",
      "JingYing Gao",
      "Ali Almutairi",
      "Mohammad Deghat",
      "Imran Razzak",
      "Francisco Cruz"
    ],
    "abstract": "Large Language Models (LLMs) are transforming the robotics domain by enabling\nrobots to comprehend and execute natural language instructions. The cornerstone\nbenefits of LLM include processing textual data from technical manuals,\ninstructions, academic papers, and user queries based on the knowledge\nprovided. However, deploying LLM-generated code in robotic systems without\nsafety verification poses significant risks. This paper outlines a safety layer\nthat verifies the code generated by ChatGPT before executing it to control a\ndrone in a simulated environment. The safety layer consists of a fine-tuned\nGPT-4o model using Few-Shot learning, supported by knowledge graph prompting\n(KGP). Our approach improves the safety and compliance of robotic actions,\nensuring that they adhere to the regulations of drone operations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11387v1",
    "published_date": "2024-12-16 02:28:34 UTC",
    "updated_date": "2024-12-16 02:28:34 UTC"
  },
  {
    "arxiv_id": "2412.11385v1",
    "title": "Why Does ChatGPT \"Delve\" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models",
    "authors": [
      "Tom S. Juzek",
      "Zina B. Ward"
    ],
    "abstract": "Scientific English is currently undergoing rapid change, with words like\n\"delve,\" \"intricate,\" and \"underscore\" appearing far more frequently than just\na few years ago. It is widely assumed that scientists' use of large language\nmodels (LLMs) is responsible for such trends. We develop a formal, transferable\nmethod to characterize these linguistic changes. Application of our method\nyields 21 focal words whose increased occurrence in scientific abstracts is\nlikely the result of LLM usage. We then pose \"the puzzle of lexical\noverrepresentation\": WHY are such words overused by LLMs? We fail to find\nevidence that lexical overrepresentation is caused by model architecture,\nalgorithm choices, or training data. To assess whether reinforcement learning\nfrom human feedback (RLHF) contributes to the overuse of focal words, we\nundertake comparative model testing and conduct an exploratory online study.\nWhile the model testing is consistent with RLHF playing a role, our\nexperimental results suggest that participants may be reacting differently to\n\"delve\" than to other focal words. With LLMs quickly becoming a driver of\nglobal language change, investigating these potential sources of lexical\noverrepresentation is important. We note that while insights into the workings\nof LLMs are within reach, a lack of transparency surrounding model development\nremains an obstacle to such research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 8 figures, The 31st International Conference on\n  Computational Linguistics (COLING 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.11385v1",
    "published_date": "2024-12-16 02:27:59 UTC",
    "updated_date": "2024-12-16 02:27:59 UTC"
  },
  {
    "arxiv_id": "2412.11381v1",
    "title": "Adapting Segment Anything Model (SAM) to Experimental Datasets via Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing",
    "authors": [
      "Anika Tabassum",
      "Amirkoushyar Ziabari"
    ],
    "abstract": "Industrial X-ray computed tomography (XCT) is a powerful tool for\nnon-destructive characterization of materials and manufactured components. XCT\ncommonly accompanied by advanced image analysis and computer vision algorithms\nto extract relevant information from the images. Traditional computer vision\nmodels often struggle due to noise, resolution variability, and complex\ninternal structures, particularly in scientific imaging applications.\nState-of-the-art foundational models, like the Segment Anything Model\n(SAM)-designed for general-purpose image segmentation-have revolutionized image\nsegmentation across various domains, yet their application in specialized\nfields like materials science remains under-explored. In this work, we explore\nthe application and limitations of SAM for industrial X-ray CT inspection of\nadditive manufacturing components. We demonstrate that while SAM shows promise,\nit struggles with out-of-distribution data, multiclass segmentation, and\ncomputational efficiency during fine-tuning. To address these issues, we\npropose a fine-tuning strategy utilizing parameter-efficient techniques,\nspecifically Conv-LoRa, to adapt SAM for material-specific datasets.\nAdditionally, we leverage generative adversarial network (GAN)-generated data\nto enhance the training process and improve the model's segmentation\nperformance on complex X-ray CT data. Our experimental results highlight the\nimportance of tailored segmentation models for accurate inspection, showing\nthat fine-tuning SAM on domain-specific scientific imaging data significantly\nimproves performance. However, despite improvements, the model's ability to\ngeneralize across diverse datasets remains limited, highlighting the need for\nfurther research into robust, scalable solutions for domain-specific\nsegmentation tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11381v1",
    "published_date": "2024-12-16 02:11:19 UTC",
    "updated_date": "2024-12-16 02:11:19 UTC"
  },
  {
    "arxiv_id": "2412.11377v1",
    "title": "Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function",
    "authors": [
      "Shijia Zhou",
      "Euijoon Ahn",
      "Hao Wang",
      "Ann Quinton",
      "Narelle Kennedy",
      "Pradeeba Sridar",
      "Ralph Nanan",
      "Jinman Kim"
    ],
    "abstract": "The measurement of fetal thalamus diameter (FTD) and fetal head circumference\n(FHC) are crucial in identifying abnormal fetal thalamus development as it may\nlead to certain neuropsychiatric disorders in later life. However, manual\nmeasurements from 2D-US images are laborious, prone to high inter-observer\nvariability, and complicated by the high signal-to-noise ratio nature of the\nimages. Deep learning-based landmark detection approaches have shown promise in\nmeasuring biometrics from US images, but the current state-of-the-art (SOTA)\nalgorithm, BiometryNet, is inadequate for FTD and FHC measurement due to its\ninability to account for the fuzzy edges of these structures and the complex\nshape of the FTD structure. To address these inadequacies, we propose a novel\nSwoosh Activation Function (SAF) designed to enhance the regularization of\nheatmaps produced by landmark detection algorithms. Our SAF serves as a\nregularization term to enforce an optimum mean squared error (MSE) level\nbetween predicted heatmaps, reducing the dispersiveness of hotspots in\npredicted heatmaps. Our experimental results demonstrate that SAF significantly\nimproves the measurement performances of FTD and FHC with higher intraclass\ncorrelation coefficient scores in FTD and lower mean difference scores in FHC\nmeasurement than those of the current SOTA algorithm BiometryNet. Moreover, our\nproposed SAF is highly generalizable and architecture-agnostic. The SAF's\ncoefficients can be configured for different tasks, making it highly\ncustomizable. Our study demonstrates that the SAF activation function is a\nnovel method that can improve measurement accuracy in fetal biometry landmark\ndetection. This improvement has the potential to contribute to better fetal\nmonitoring and improved neonatal outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11377v1",
    "published_date": "2024-12-16 02:05:15 UTC",
    "updated_date": "2024-12-16 02:05:15 UTC"
  },
  {
    "arxiv_id": "2412.11373v2",
    "title": "Codenames as a Benchmark for Large Language Models",
    "authors": [
      "Matthew Stephenson",
      "Matthew Sidji",
      "Benoît Ronval"
    ],
    "abstract": "In this paper, we propose the use of the popular word-based board game\nCodenames as a suitable benchmark for evaluating the reasoning capabilities of\nLarge Language Models (LLMs). Codenames presents a highly interesting challenge\nfor achieving successful AI performance, requiring both a sophisticated\nunderstanding of language, theory of mind, and epistemic reasoning\ncapabilities. Prior attempts to develop agents for Codenames have largely\nrelied on word embedding techniques, which have a limited vocabulary range and\nperform poorly when paired with differing approaches. LLMs have demonstrated\nenhanced reasoning and comprehension capabilities for language-based tasks, but\ncan still suffer in lateral thinking challenges. We evaluate the capabilities\nof several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5\nSonnet, and Llama 3.1, across a variety of board setups. Our results indicate\nthat while certain LLMs perform better than others overall, different models\nexhibit varying emergent behaviours during gameplay and excel at specific\nroles. We also evaluate the performance of different combinations of LLMs when\nplaying cooperatively together, demonstrating that LLM agents are more\ngeneralisable to a wider range of teammates than prior techniques.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 2 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.11373v2",
    "published_date": "2024-12-16 01:59:03 UTC",
    "updated_date": "2025-04-21 22:53:07 UTC"
  },
  {
    "arxiv_id": "2412.11364v1",
    "title": "Individual Bus Trip Chain Prediction and Pattern Identification Considering Similarities",
    "authors": [
      "Xiannan Huang",
      "Yixin Chen",
      "Quan Yuan",
      "Chao Yang"
    ],
    "abstract": "Predicting future bus trip chains for an existing user is of great\nsignificance for operators of public transit systems. Existing methods always\ntreat this task as a time-series prediction problem, but the 1-dimensional time\nseries structure cannot express the complex relationship between trips. To\nbetter capture the inherent patterns in bus travel behavior, this paper\nproposes a novel approach that synthesizes future bus trip chains based on\nthose from similar days. Key similarity patterns are defined and tested using\nreal-world data, and a similarity function is then developed to capture these\npatterns. Afterwards, a graph is constructed where each day is represented as a\nnode and edge weight reflects the similarity between days. Besides, the trips\non a given day can be regarded as labels for each node, transferring the bus\ntrip chain prediction problem to a semi-supervised classification problem on a\ngraph. To address this, we propose several methods and validate them on a\nreal-world dataset of 10000 bus users, achieving state-of-the-art prediction\nresults. Analyzing the parameters of similarity function reveals some\ninteresting bus usage patterns, allowing us can to cluster bus users into three\ntypes: repeat-dominated, evolve-dominate and repeat-evolve balanced. In\nsummary, our work demonstrates the effectiveness of similarity-based prediction\nfor bus trip chains and provides a new perspective for analyzing individual bus\ntravel patterns. The code for our prediction model is publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11364v1",
    "published_date": "2024-12-16 01:32:26 UTC",
    "updated_date": "2024-12-16 01:32:26 UTC"
  },
  {
    "arxiv_id": "2412.11360v1",
    "title": "Visual IRL for Human-Like Robotic Manipulation",
    "authors": [
      "Ehsan Asali",
      "Prashant Doshi"
    ],
    "abstract": "We present a novel method for collaborative robots (cobots) to learn\nmanipulation tasks and perform them in a human-like manner. Our method falls\nunder the learn-from-observation (LfO) paradigm, where robots learn to perform\ntasks by observing human actions, which facilitates quicker integration into\nindustrial settings compared to programming from scratch. We introduce Visual\nIRL that uses the RGB-D keypoints in each frame of the observed human task\nperformance directly as state features, which are input to inverse\nreinforcement learning (IRL). The inversely learned reward function, which maps\nkeypoints to reward values, is transferred from the human to the cobot using a\nnovel neuro-symbolic dynamics model, which maps human kinematics to the cobot\narm. This model allows similar end-effector positioning while minimizing joint\nadjustments, aiming to preserve the natural dynamics of human motion in robotic\nmanipulation. In contrast with previous techniques that focus on end-effector\nplacement only, our method maps multiple joint angles of the human arm to the\ncorresponding cobot joints. Moreover, it uses an inverse kinematics model to\nthen minimally adjust the joint angles, for accurate end-effector positioning.\nWe evaluate the performance of this approach on two different realistic\nmanipulation tasks. The first task is produce processing, which involves\npicking, inspecting, and placing onions based on whether they are blemished.\nThe second task is liquid pouring, where the robot picks up bottles, pours the\ncontents into designated containers, and disposes of the empty bottles. Our\nresults demonstrate advances in human-like robotic manipulation, leading to\nmore human-robot compatibility in manufacturing applications.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11360v1",
    "published_date": "2024-12-16 01:23:13 UTC",
    "updated_date": "2024-12-16 01:23:13 UTC"
  },
  {
    "arxiv_id": "2412.11356v1",
    "title": "The Stabilizer Bootstrap of Quantum Machine Learning with up to 10000 qubits",
    "authors": [
      "Yuqing Li",
      "Jinglei Cheng",
      "Xulong Tang",
      "Youtao Zhang",
      "Frederic T. Chong",
      "Junyu Liu"
    ],
    "abstract": "Quantum machine learning is considered one of the flagship applications of\nquantum computers, where variational quantum circuits could be the leading\nparadigm both in the near-term quantum devices and the early fault-tolerant\nquantum computers. However, it is not clear how to identify the regime of\nquantum advantages from these circuits, and there is no explicit theory to\nguide the practical design of variational ansatze to achieve better\nperformance. We address these challenges with the stabilizer bootstrap, a\nmethod that uses stabilizer-based techniques to optimize quantum neural\nnetworks before their quantum execution, together with theoretical proofs and\nhigh-performance computing with 10000 qubits or random datasets up to 1000\ndata. We find that, in a general setup of variational ansatze, the possibility\nof improvements from the stabilizer bootstrap depends on the structure of the\nobservables and the size of the datasets. The results reveal that\nconfigurations exhibit two distinct behaviors: some maintain a constant\nprobability of circuit improvement, while others show an exponential decay in\nimprovement probability as qubit numbers increase. These patterns are termed\nstrong stabilizer enhancement and weak stabilizer enhancement, respectively,\nwith most situations falling in between. Our work seamlessly bridges techniques\nfrom fault-tolerant quantum computing with applications of variational quantum\nalgorithms. Not only does it offer practical insights for designing variational\ncircuits tailored to large-scale machine learning challenges, but it also maps\nout a clear trajectory for defining the boundaries of feasible and practical\nquantum advantages.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "15 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.11356v1",
    "published_date": "2024-12-16 01:12:00 UTC",
    "updated_date": "2024-12-16 01:12:00 UTC"
  },
  {
    "arxiv_id": "2412.11344v1",
    "title": "Can AI Extract Antecedent Factors of Human Trust in AI? An Application of Information Extraction for Scientific Literature in Behavioural and Computer Sciences",
    "authors": [
      "Melanie McGrath",
      "Harrison Bailey",
      "Necva Bölücü",
      "Xiang Dai",
      "Sarvnaz Karimi",
      "Cecile Paris"
    ],
    "abstract": "Information extraction from the scientific literature is one of the main\ntechniques to transform unstructured knowledge hidden in the text into\nstructured data which can then be used for decision-making in down-stream\ntasks. One such area is Trust in AI, where factors contributing to human trust\nin artificial intelligence applications are studied. The relationships of these\nfactors with human trust in such applications are complex. We hence explore\nthis space from the lens of information extraction where, with the input of\ndomain experts, we carefully design annotation guidelines, create the first\nannotated English dataset in this domain, investigate an LLM-guided annotation,\nand benchmark it with state-of-the-art methods using large language models in\nnamed entity and relation extraction. Our results indicate that this problem\nrequires supervised learning which may not be currently feasible with\nprompt-based LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.11344v1",
    "published_date": "2024-12-16 00:02:38 UTC",
    "updated_date": "2024-12-16 00:02:38 UTC"
  }
]
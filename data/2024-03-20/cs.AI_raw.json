[
  {
    "arxiv_id": "2403.14037v1",
    "title": "Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection",
    "authors": [
      "Sheetal Harris",
      "Jinshuo Liu",
      "Hassan Jalil Hadi",
      "Yue Cao"
    ],
    "abstract": "Misinformation can seriously impact society, affecting anything from public\nopinion to institutional confidence and the political horizon of a state. Fake\nNews (FN) proliferation on online websites and Online Social Networks (OSNs)\nhas increased profusely. Various fact-checking websites include news in English\nand barely provide information about FN in regional languages. Thus the Urdu FN\npurveyors cannot be discerned using factchecking portals. SOTA approaches for\nFake News Detection (FND) count upon appropriately labelled and large datasets.\nFND in regional and resource-constrained languages lags due to the lack of\nlimited-sized datasets and legitimate lexical resources. The previous datasets\nfor Urdu FND are limited-sized, domain-restricted, publicly unavailable and not\nmanually verified where the news is translated from English into Urdu. In this\npaper, we curate and contribute the first largest publicly available dataset\nfor Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations\nof existing Urdu datasets in the literature. It constitutes 10,083 fake and\nreal news on fifteen domains collected from leading and authentic Urdu\nnewspapers and news channel websites in Pakistan and India. FN for the\nAx-to-Grind dataset is collected from websites and crowdsourcing. The dataset\ncontains news items in Urdu from the year 2017 to the year 2023. Expert\njournalists annotated the dataset. We benchmark the dataset with an ensemble\nmodel of mBERT,XLNet, and XLM RoBERTa. The selected models are originally\ntrained on multilingual large corpora. The results of the proposed model are\nbased on performance metrics, F1-score, accuracy, precision, recall and MCC\nvalue.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.14037v1",
    "published_date": "2024-03-20 23:21:35 UTC",
    "updated_date": "2024-03-20 23:21:35 UTC"
  },
  {
    "arxiv_id": "2403.14019v1",
    "title": "Searching Search Spaces: Meta-evolving a Geometric Encoding for Neural Networks",
    "authors": [
      "Tarek Kunze",
      "Paul Templier",
      "Dennis G Wilson"
    ],
    "abstract": "In evolutionary policy search, neural networks are usually represented using\na direct mapping: each gene encodes one network weight. Indirect encoding\nmethods, where each gene can encode for multiple weights, shorten the genome to\nreduce the dimensions of the search space and better exploit permutations and\nsymmetries. The Geometric Encoding for Neural network Evolution (GENE)\nintroduced an indirect encoding where the weight of a connection is computed as\nthe (pseudo-)distance between the two linked neurons, leading to a genome size\ngrowing linearly with the number of genes instead of quadratically in direct\nencoding. However GENE still relies on hand-crafted distance functions with no\nprior optimization. Here we show that better performing distance functions can\nbe found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution\napproach, hence optimizing the encoding to create a search space that is easier\nto exploit. We show that GENE with a learned function can outperform both\ndirect encoding and the hand-crafted distances, generalizing on unseen\nproblems, and we study how the encoding impacts neural network properties.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.NE",
    "comment": "9 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.14019v1",
    "published_date": "2024-03-20 22:40:53 UTC",
    "updated_date": "2024-03-20 22:40:53 UTC"
  },
  {
    "arxiv_id": "2403.14006v1",
    "title": "On Prompt Sensitivity of ChatGPT in Affective Computing",
    "authors": [
      "Mostafa M. Amin",
      "Bj√∂rn W. Schuller"
    ],
    "abstract": "Recent studies have demonstrated the emerging capabilities of foundation\nmodels like ChatGPT in several fields, including affective computing. However,\naccessing these emerging capabilities is facilitated through prompt\nengineering. Despite the existence of some prompting techniques, the field is\nstill rapidly evolving and many prompting ideas still require investigation. In\nthis work, we introduce a method to evaluate and investigate the sensitivity of\nthe performance of foundation models based on different prompts or generation\nparameters. We perform our evaluation on ChatGPT within the scope of affective\ncomputing on three major problems, namely sentiment analysis, toxicity\ndetection, and sarcasm detection. First, we carry out a sensitivity analysis on\npivotal parameters in auto-regressive text generation, specifically the\ntemperature parameter $T$ and the top-$p$ parameter in Nucleus sampling,\ndictating how conservative or creative the model should be during generation.\nFurthermore, we explore the efficacy of several prompting ideas, where we\nexplore how giving different incentives or structures affect the performance.\nOur evaluation takes into consideration performance measures on the affective\ncomputing tasks, and the effectiveness of the model to follow the stated\ninstructions, hence generating easy-to-parse responses to be smoothly used in\ndownstream applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "2 Tables, 1 Figure, preprint submission to ACII 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.14006v1",
    "published_date": "2024-03-20 22:11:01 UTC",
    "updated_date": "2024-03-20 22:11:01 UTC"
  },
  {
    "arxiv_id": "2404.00027v5",
    "title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning",
    "authors": [
      "Azmine Toushik Wasi",
      "Mst Rafia Islam",
      "Raima Islam"
    ],
    "abstract": "Sense of ownership in writing confines our investment of thoughts, time, and\ncontribution, leading to attachment to the output. However, using writing\nassistants introduces a mental dilemma, as some content isn't directly our\ncreation. For instance, we tend to credit Large Language Models (LLMs) more in\ncreative tasks, even though all tasks are equal for them. Additionally, while\nwe may not claim complete ownership of LLM-generated content, we freely claim\nauthorship. We conduct a short survey to examine these issues and understand\nunderlying cognitive processes in order to gain a better knowledge of\nhuman-computer interaction in writing and improve writing aid systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "5 Pages, 3 Figures. Accepted in The Third Workshop on Intelligent and\n  Interactive Writing Assistants at CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00027v5",
    "published_date": "2024-03-20 21:06:42 UTC",
    "updated_date": "2024-10-02 20:45:35 UTC"
  },
  {
    "arxiv_id": "2404.00026v5",
    "title": "Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs",
    "authors": [
      "Azmine Toushik Wasi",
      "Raima Islam",
      "Mst Rafia Islam"
    ],
    "abstract": "Individuality and personalization comprise the distinctive characteristics\nthat make each writer unique and influence their words in order to effectively\nengage readers while conveying authenticity. However, our growing reliance on\nLLM-based writing assistants risks compromising our creativity and\nindividuality over time. We often overlook the negative impacts of this trend\non our creativity and uniqueness, despite the possible consequences. This study\ninvestigates these concerns by performing a brief survey to explore different\nperspectives and concepts, as well as trying to understand people's viewpoints,\nin conjunction with past studies in the area. Addressing these issues is\nessential for improving human-computer interaction systems and enhancing\nwriting assistants for personalization and individuality.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "5 Pages, 4 Figures. Accepted in The Third Workshop on Intelligent and\n  Interactive Writing Assistants at CHI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.00026v5",
    "published_date": "2024-03-20 21:02:16 UTC",
    "updated_date": "2024-10-02 20:45:53 UTC"
  },
  {
    "arxiv_id": "2403.13969v2",
    "title": "\"This is not a data problem\": Algorithms and Power in Public Higher Education in Canada",
    "authors": [
      "Kelly McConvey",
      "Shion Guha"
    ],
    "abstract": "Algorithmic decision-making is increasingly being adopted across public\nhigher education. The expansion of data-driven practices by post-secondary\ninstitutions has occurred in parallel with the adoption of New Public\nManagement approaches by neoliberal administrations. In this study, we conduct\na qualitative analysis of an in-depth ethnographic case study of data and\nalgorithms in use at a public college in Ontario, Canada. We identify the data,\nalgorithms, and outcomes in use at the college. We assess how the college's\nprocesses and relationships support those outcomes and the different\nstakeholders' perceptions of the college's data-driven systems. In addition, we\nfind that the growing reliance on algorithmic decisions leads to increased\nstudent surveillance, exacerbation of existing inequities, and the automation\nof the faculty-student relationship. Finally, we identify a cycle of increased\ninstitutional power perpetuated by algorithmic decision-making, and driven by a\npush towards financial sustainability.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "In CHI '24 Proceedings of the CHI Conference on Human Factors in\n  Computing Systems Honolulu, HI, USA",
    "pdf_url": "http://arxiv.org/pdf/2403.13969v2",
    "published_date": "2024-03-20 20:46:41 UTC",
    "updated_date": "2024-03-22 15:57:20 UTC"
  },
  {
    "arxiv_id": "2403.13960v1",
    "title": "Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot",
    "authors": [
      "Antonio Bono",
      "Kenji Brameld",
      "Luigi D'Alfonso",
      "Giuseppe Fedele"
    ],
    "abstract": "This paper presents a new software framework for HRI experimentation with the\nsixth version of the common NAO robot produced by the United Robotics Group.\nEmbracing the common demand of researchers for better performance and new\nfeatures for NAO, the authors took advantage of the ability to run ROS2 onboard\non the NAO to develop a framework independent of the APIs provided by the\nmanufacturer. Such a system provides NAO with not only the basic skills of a\nhumanoid robot such as walking and reproducing movements of interest but also\nfeatures often used in HRI such as: speech recognition/synthesis, face and\nobject detention, and the use of Generative Pre-trained Transformer (GPT)\nmodels for conversation. The developed code is therefore configured as a\nready-to-use but also highly expandable and improvable tool thanks to the\npossibilities provided by the ROS community.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13960v1",
    "published_date": "2024-03-20 20:13:39 UTC",
    "updated_date": "2024-03-20 20:13:39 UTC"
  },
  {
    "arxiv_id": "2403.13951v1",
    "title": "ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On",
    "authors": [
      "Jeffrey Zhang",
      "Kedan Li",
      "Shao-Yu Chang",
      "David Forsyth"
    ],
    "abstract": "Virtual Try-on (VTON) involves generating images of a person wearing selected\ngarments. Diffusion-based methods, in particular, can create high-quality\nimages, but they struggle to maintain the identities of the input garments. We\nidentified this problem stems from the specifics in the training formulation\nfor diffusion. To address this, we propose a unique training scheme that limits\nthe scope in which diffusion is trained. We use a control image that perfectly\naligns with the target image during training. In turn, this accurately\npreserves garment details during inference. We demonstrate our method not only\neffectively conserves garment details but also allows for layering, styling,\nand shoe try-on. Our method runs multi-garment try-on in a single inference\ncycle and can support high-quality zoomed-in generations without training in\nhigher resolutions. Finally, we show our method surpasses prior methods in\naccuracy and quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13951v1",
    "published_date": "2024-03-20 19:45:06 UTC",
    "updated_date": "2024-03-20 19:45:06 UTC"
  },
  {
    "arxiv_id": "2403.13950v1",
    "title": "Evo* 2023 -- Late-Breaking Abstracts Volume",
    "authors": [
      "A. M. Mora",
      "A. I. Esparcia-Alc√°zar"
    ],
    "abstract": "Volume with the Late-Breaking Abstracts submitted to the Evo* 2023\nConference, held in Brno (Czech Republic), from 12 to 14 of April. These papers\npresent ongoing research and preliminary results investigating on the\napplication of different approaches of Bioinspired Methods (mainly Evolutionary\nComputation) to different problems, most of them real world ones.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "68T05, 68W20",
      "I.0; I.2; K.4"
    ],
    "primary_category": "cs.NE",
    "comment": "LBAs accepted in Evo* 2023. Part of the Conference Proceedings",
    "pdf_url": "http://arxiv.org/pdf/2403.13950v1",
    "published_date": "2024-03-20 19:42:11 UTC",
    "updated_date": "2024-03-20 19:42:11 UTC"
  },
  {
    "arxiv_id": "2403.13947v2",
    "title": "BlendScape: Enabling End-User Customization of Video-Conferencing Environments through Generative AI",
    "authors": [
      "Shwetha Rajaram",
      "Nels Numan",
      "Balasaravanan Thoravi Kumaravel",
      "Nicolai Marquardt",
      "Andrew D. Wilson"
    ],
    "abstract": "Today's video-conferencing tools support a rich range of professional and\nsocial activities, but their generic meeting environments cannot be dynamically\nadapted to align with distributed collaborators' needs. To enable end-user\ncustomization, we developed BlendScape, a rendering and composition system for\nvideo-conferencing participants to tailor environments to their meeting context\nby leveraging AI image generation techniques. BlendScape supports flexible\nrepresentations of task spaces by blending users' physical or digital\nbackgrounds into unified environments and implements multimodal interaction\ntechniques to steer the generation. Through an exploratory study with 15\nend-users, we investigated whether and how they would find value in using\ngenerative AI to customize video-conferencing environments. Participants\nenvisioned using a system like BlendScape to facilitate collaborative\nactivities in the future, but required further controls to mitigate distracting\nor unrealistic visual elements. We implemented scenarios to demonstrate\nBlendScape's expressiveness for supporting environment design strategies from\nprior work and propose composition techniques to improve the quality of\nenvironments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "ACM UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13947v2",
    "published_date": "2024-03-20 19:41:05 UTC",
    "updated_date": "2024-10-01 12:07:57 UTC"
  },
  {
    "arxiv_id": "2403.13940v2",
    "title": "A multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers",
    "authors": [
      "Ignacy Stƒôpka",
      "Mateusz Lango",
      "Jerzy Stefanowski"
    ],
    "abstract": "Counterfactuals are widely used to explain ML model predictions by providing\nalternative scenarios for obtaining the more desired predictions. They can be\ngenerated by a variety of methods that optimize different, sometimes\nconflicting, quality measures and produce quite different solutions. However,\nchoosing the most appropriate explanation method and one of the generated\ncounterfactuals is not an easy task. Instead of forcing the user to test many\ndifferent explanation methods and analysing conflicting solutions, in this\npaper, we propose to use a multi-stage ensemble approach that will select\nsingle counterfactual based on the multiple-criteria analysis. It offers a\ncompromise solution that scores well on several popular quality measures. This\napproach exploits the dominance relation and the ideal point decision aid\nmethod, which selects one counterfactual from the Pareto front. The conducted\nexperiments demonstrated that the proposed approach generates fully actionable\ncounterfactuals with attractive compromise values of the considered quality\nmeasures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13940v2",
    "published_date": "2024-03-20 19:25:11 UTC",
    "updated_date": "2024-08-02 15:54:21 UTC"
  },
  {
    "arxiv_id": "2403.13925v1",
    "title": "Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification",
    "authors": [
      "Devam Mondal",
      "Carlo Lipizzi"
    ],
    "abstract": "Despite the growing capabilities of large language models, there exists\nconcerns about the biases they develop. In this paper, we propose a novel,\nautomated mechanism for debiasing through specified dataset augmentation in the\nlens of bias producers and in the context of 'restricted industries' with\nlimited data. We additionally create two new additional metrics, the mb-index\nand db-index, to quantify bias, considering the idea that bias occurs due to\nboth intrinsic model architecture and dataset.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13925v1",
    "published_date": "2024-03-20 18:59:18 UTC",
    "updated_date": "2024-03-20 18:59:18 UTC"
  },
  {
    "arxiv_id": "2403.13890v3",
    "title": "Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models",
    "authors": [
      "Richard Osuala",
      "Daniel M. Lang",
      "Preeti Verma",
      "Smriti Joshi",
      "Apostolia Tsirikoglou",
      "Grzegorz Skorupko",
      "Kaisar Kushibar",
      "Lidia Garrucho",
      "Walter H. L. Pinaya",
      "Oliver Diaz",
      "Julia A. Schnabel",
      "Karim Lekadir"
    ],
    "abstract": "Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow\nto localize tumors and observe their contrast kinetics, which is essential for\ncancer characterization and respective treatment decision-making. However,\ncontrast agent administration is not only associated with adverse health risks,\nbut also restricted for patients during pregnancy, and for those with kidney\nmalfunction, or other adverse reactions. With contrast uptake as key biomarker\nfor lesion malignancy, cancer recurrence risk, and treatment response, it\nbecomes pivotal to reduce the dependency on intravenous contrast agent\nadministration. To this end, we propose a multi-conditional latent diffusion\nmodel capable of acquisition time-conditioned image synthesis of DCE-MRI\ntemporal sequences. To evaluate medical image synthesis, we additionally\npropose and validate the Fr\\'echet radiomics distance as an image quality\nmeasure based on biomarker variability between synthetic and real imaging data.\nOur results demonstrate our method's ability to generate realistic\nmulti-sequence fat-saturated breast DCE-MRI and uncover the emerging potential\nof deep learning based contrast kinetics simulation. We publicly share our\naccessible codebase at https://github.com/RichardObi/ccnet and provide a\nuser-friendly library for Fr\\'echet radiomics distance calculation at\nhttps://pypi.org/project/frd-score.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Early Accept at MICCAI2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13890v3",
    "published_date": "2024-03-20 18:01:57 UTC",
    "updated_date": "2024-07-17 16:04:45 UTC"
  },
  {
    "arxiv_id": "2403.13808v3",
    "title": "On Pretraining Data Diversity for Self-Supervised Learning",
    "authors": [
      "Hasan Abed Al Kader Hammoud",
      "Tuhin Das",
      "Fabio Pizzati",
      "Philip Torr",
      "Adel Bibi",
      "Bernard Ghanem"
    ],
    "abstract": "We explore the impact of training with more diverse datasets, characterized\nby the number of unique samples, on the performance of self-supervised learning\n(SSL) under a fixed computational budget. Our findings consistently demonstrate\nthat increasing pretraining data diversity enhances SSL performance, albeit\nonly when the distribution distance to the downstream data is minimal. Notably,\neven with an exceptionally large pretraining data diversity achieved through\nmethods like web crawling or diffusion-generated data, among other ways, the\ndistribution shift remains a challenge. Our experiments are comprehensive with\nseven SSL methods using large-scale datasets such as ImageNet and YFCC100M\namounting to over 200 GPU days. Code and trained models are available at\nhttps://github.com/hammoudhasan/DiversitySSL",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13808v3",
    "published_date": "2024-03-20 17:59:58 UTC",
    "updated_date": "2024-07-18 09:15:00 UTC"
  },
  {
    "arxiv_id": "2403.13807v1",
    "title": "Editing Massive Concepts in Text-to-Image Diffusion Models",
    "authors": [
      "Tianwei Xiong",
      "Yue Wu",
      "Enze Xie",
      "Yue Wu",
      "Zhenguo Li",
      "Xihui Liu"
    ],
    "abstract": "Text-to-image diffusion models suffer from the risk of generating outdated,\ncopyrighted, incorrect, and biased content. While previous methods have\nmitigated the issues on a small scale, it is essential to handle them\nsimultaneously in larger-scale real-world scenarios. We propose a two-stage\nmethod, Editing Massive Concepts In Diffusion Models (EMCID). The first stage\nperforms memory optimization for each individual concept with dual\nself-distillation from text alignment loss and diffusion noise prediction loss.\nThe second stage conducts massive concept editing with multi-layer, closed form\nmodel editing. We further propose a comprehensive benchmark, named ImageNet\nConcept Editing Benchmark (ICEB), for evaluating massive concept editing for\nT2I models with two subtasks, free-form prompts, massive concept categories,\nand extensive evaluation metrics. Extensive experiments conducted on our\nproposed benchmark and previous benchmarks demonstrate the superior scalability\nof EMCID for editing up to 1,000 concepts, providing a practical approach for\nfast adjustment and re-deployment of T2I diffusion models in real-world\napplications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://silentview.github.io/EMCID/ . Code:\n  https://github.com/SilentView/EMCID",
    "pdf_url": "http://arxiv.org/pdf/2403.13807v1",
    "published_date": "2024-03-20 17:59:57 UTC",
    "updated_date": "2024-03-20 17:59:57 UTC"
  },
  {
    "arxiv_id": "2403.13805v1",
    "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
    "authors": [
      "Ziyu Liu",
      "Zeyi Sun",
      "Yuhang Zang",
      "Wei Li",
      "Pan Zhang",
      "Xiaoyi Dong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "abstract": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project: https://github.com/Liuziyu77/RAR",
    "pdf_url": "http://arxiv.org/pdf/2403.13805v1",
    "published_date": "2024-03-20 17:59:55 UTC",
    "updated_date": "2024-03-20 17:59:55 UTC"
  },
  {
    "arxiv_id": "2403.13802v3",
    "title": "ZigMa: A DiT-style Zigzag Mamba Diffusion Model",
    "authors": [
      "Vincent Tao Hu",
      "Stefan Andreas Baumann",
      "Ming Gui",
      "Olga Grebenkova",
      "Pingchuan Ma",
      "Johannes Schusterbauer",
      "Bj√∂rn Ommer"
    ],
    "abstract": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$ . Code will be released at https://taohu.me/zigma/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024 Project Page: https://taohu.me/zigma/",
    "pdf_url": "http://arxiv.org/pdf/2403.13802v3",
    "published_date": "2024-03-20 17:59:14 UTC",
    "updated_date": "2024-11-24 14:25:05 UTC"
  },
  {
    "arxiv_id": "2403.13801v2",
    "title": "Natural Language as Policies: Reasoning for Coordinate-Level Embodied Control with LLMs",
    "authors": [
      "Yusuke Mikami",
      "Andrew Melnik",
      "Jun Miura",
      "Ville Hautam√§ki"
    ],
    "abstract": "We demonstrate experimental results with LLMs that address robotics task\nplanning problems. Recently, LLMs have been applied in robotics task planning,\nparticularly using a code generation approach that converts complex high-level\ninstructions into mid-level policy codes. In contrast, our approach acquires\ntext descriptions of the task and scene objects, then formulates task planning\nthrough natural language reasoning, and outputs coordinate level control\ncommands, thus reducing the necessity for intermediate representation code as\npolicies with pre-defined APIs. Our approach is evaluated on a multi-modal\nprompt simulation benchmark, demonstrating that our prompt engineering\nexperiments with natural language reasoning significantly enhance success rates\ncompared to its absence. Furthermore, our approach illustrates the potential\nfor natural language descriptions to transfer robotics skills from known tasks\nto previously unseen tasks. The project website:\nhttps://natural-language-as-policies.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "I.2.9; I.2.7"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13801v2",
    "published_date": "2024-03-20 17:58:12 UTC",
    "updated_date": "2024-04-06 04:12:47 UTC"
  },
  {
    "arxiv_id": "2403.13799v3",
    "title": "Reverse Training to Nurse the Reversal Curse",
    "authors": [
      "Olga Golovneva",
      "Zeyuan Allen-Zhu",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "abstract": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13799v3",
    "published_date": "2024-03-20 17:55:35 UTC",
    "updated_date": "2024-05-07 20:35:15 UTC"
  },
  {
    "arxiv_id": "2403.13798v2",
    "title": "Hierarchical NeuroSymbolic Approach for Comprehensive and Explainable Action Quality Assessment",
    "authors": [
      "Lauren Okamoto",
      "Paritosh Parmar"
    ],
    "abstract": "Action quality assessment (AQA) applies computer vision to quantitatively\nassess the performance or execution of a human action. Current AQA approaches\nare end-to-end neural models, which lack transparency and tend to be biased\nbecause they are trained on subjective human judgements as ground-truth. To\naddress these issues, we introduce a neuro-symbolic paradigm for AQA, which\nuses neural networks to abstract interpretable symbols from video data and\nmakes quality assessments by applying rules to those symbols. We take diving as\nthe case study. We found that domain experts prefer our system and find it more\ninformative than purely neural approaches to AQA in diving. Our system also\nachieves state-of-the-art action recognition and temporal segmentation, and\nautomatically generates a detailed report that breaks the dive down into its\nelements and provides objective scoring with visual evidence. As verified by a\ngroup of domain experts, this report may be used to assist judges in scoring,\nhelp train judges, and provide feedback to divers. Annotated training data and\ncode: https://github.com/laurenok24/NSAQA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2024 CVSports (Oral Presentation; 3/3 Strong Accepts) + Selected\n  for CVPR 2024 Demos",
    "pdf_url": "http://arxiv.org/pdf/2403.13798v2",
    "published_date": "2024-03-20 17:55:21 UTC",
    "updated_date": "2024-05-24 17:44:11 UTC"
  },
  {
    "arxiv_id": "2403.13784v6",
    "title": "The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence",
    "authors": [
      "Matt White",
      "Ibrahim Haddad",
      "Cailean Osborne",
      "Xiao-Yang Yanglet Liu",
      "Ahmed Abdelmonsef",
      "Sachin Varghese",
      "Arnaud Le Hors"
    ],
    "abstract": "Generative artificial intelligence (AI) offers numerous opportunities for\nresearch and innovation, but its commercialization has raised concerns about\nthe transparency and safety of frontier AI models. Most models lack the\nnecessary components for full understanding, auditing, and reproducibility, and\nsome model producers use restrictive licenses whilst claiming that their models\nare \"open source\". To address these concerns, we introduce the Model Openness\nFramework (MOF), a three-tiered ranked classification system that rates machine\nlearning models based on their completeness and openness, following open\nscience principles. For each MOF class, we specify code, data, and\ndocumentation components of the model development lifecycle that must be\nreleased and under which open licenses. In addition, the Model Openness Tool\n(MOT) provides a user-friendly reference implementation to evaluate the\nopenness and completeness of models against the MOF classification system.\nTogether, the MOF and MOT provide timely practical guidance for (i) model\nproducers to enhance the openness and completeness of their publicly-released\nmodels, and (ii) model consumers to identify open models and their constituent\ncomponents that can be permissively used, studied, modified, and redistributed.\nThrough the MOF, we seek to establish completeness and openness as core tenets\nof responsible AI research and development, and to promote best practices in\nthe burgeoning open AI ecosystem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.13784v6",
    "published_date": "2024-03-20 17:47:08 UTC",
    "updated_date": "2024-10-18 08:20:22 UTC"
  },
  {
    "arxiv_id": "2403.13780v2",
    "title": "Information-Theoretic Distillation for Reference-less Summarization",
    "authors": [
      "Jaehun Jung",
      "Ximing Lu",
      "Liwei Jiang",
      "Faeze Brahman",
      "Peter West",
      "Pang Wei Koh",
      "Yejin Choi"
    ],
    "abstract": "The current winning recipe for automatic summarization is using proprietary\nlarge-scale language models (LLMs) such as ChatGPT as is, or imitation learning\nfrom them as teacher models. While increasingly ubiquitous dependence on such\nlarge-scale language models is convenient, there remains an important question\nof whether small-scale models could have achieved competitive results, if we\nwere to seek an alternative learning method -- that allows for a more\ncost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a\nnovel framework to distill a powerful summarizer based on the\ninformation-theoretic objective for summarization, without relying on either\nthe LLM's capability or human-written references. To achieve this, we first\npropose a novel formulation of the desiderata of summarization (saliency,\nfaithfulness and brevity) through the lens of mutual information between the\noriginal document and the summary. Based on this formulation, we start off from\nPythia-2.8B as the teacher model, which is not yet capable of summarization,\nthen self-train the model to optimize for the information-centric measures of\nideal summaries. Distilling from the improved teacher, we arrive at a compact\nbut powerful summarizer with only 568M parameters that performs competitively\nagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensive\nanalysis demonstrates that our approach outperforms in-domain supervised models\nin human evaluation, let alone state-of-the-art unsupervised methods, and wins\nover ChatGPT in controllable summarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13780v2",
    "published_date": "2024-03-20 17:42:08 UTC",
    "updated_date": "2024-08-19 22:38:14 UTC"
  },
  {
    "arxiv_id": "2403.15476v2",
    "title": "Learning to Infer Generative Template Programs for Visual Concepts",
    "authors": [
      "R. Kenny Jones",
      "Siddhartha Chaudhuri",
      "Daniel Ritchie"
    ],
    "abstract": "People grasp flexible visual concepts from a few examples. We explore a\nneurosymbolic system that learns how to infer programs that capture visual\nconcepts in a domain-general fashion. We introduce Template Programs:\nprogrammatic expressions from a domain-specific language that specify\nstructural and parametric patterns common to an input concept. Our framework\nsupports multiple concept-related tasks, including few-shot generation and\nco-segmentation through parsing. We develop a learning paradigm that allows us\nto train networks that infer Template Programs directly from visual datasets\nthat contain concept groupings. We run experiments across multiple visual\ndomains: 2D layouts, Omniglot characters, and 3D shapes. We find that our\nmethod outperforms task-specific alternatives, and performs competitively\nagainst domain-specific approaches for the limited domains where they exist.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ICML 2024; Project page: https://rkjones4.github.io/template.html",
    "pdf_url": "http://arxiv.org/pdf/2403.15476v2",
    "published_date": "2024-03-20 17:29:58 UTC",
    "updated_date": "2024-06-09 21:54:18 UTC"
  },
  {
    "arxiv_id": "2403.13765v1",
    "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
    "authors": [
      "Dipendra Misra",
      "Akanksha Saran",
      "Tengyang Xie",
      "Alex Lamb",
      "John Langford"
    ],
    "abstract": "We study pre-training representations for decision-making using video data,\nwhich is abundantly available for tasks such as game agents and software\ntesting. Even though significant empirical advances have been made on this\nproblem, a theoretical understanding remains absent. We initiate the\ntheoretical investigation into principled approaches for representation\nlearning and focus on learning the latent state representations of the\nunderlying MDP using video data. We study two types of settings: one where\nthere is iid noise in the observation, and a more challenging setting where\nthere is also the presence of exogenous noise, which is non-iid noise that is\ntemporally correlated, such as the motion of people or cars in the background.\nWe study three commonly used approaches: autoencoding, temporal contrastive\nlearning, and forward modeling. We prove upper bounds for temporal contrastive\nlearning and forward modeling in the presence of only iid noise. We show that\nthese approaches can learn the latent state and use it to do efficient\ndownstream RL with polynomial sample complexity. When exogenous noise is also\npresent, we establish a lower bound result showing that the sample complexity\nof learning from video data can be exponentially worse than learning from\naction-labeled trajectory data. This partially explains why reinforcement\nlearning with video pre-training is hard. We evaluate these representational\nlearning methods in two visual domains, yielding results that are consistent\nwith our theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024 Spotlight Conference Paper",
    "pdf_url": "http://arxiv.org/pdf/2403.13765v1",
    "published_date": "2024-03-20 17:28:17 UTC",
    "updated_date": "2024-03-20 17:28:17 UTC"
  },
  {
    "arxiv_id": "2403.13741v1",
    "title": "Hyper Strategy Logic",
    "authors": [
      "Raven Beutner",
      "Bernd Finkbeiner"
    ],
    "abstract": "Strategy logic (SL) is a powerful temporal logic that enables strategic\nreasoning in multi-agent systems. SL supports explicit (first-order)\nquantification over strategies and provides a logical framework to express many\nimportant properties such as Nash equilibria, dominant strategies, etc. While\nin SL the same strategy can be used in multiple strategy profiles, each such\nprofile is evaluated w.r.t. a path-property, i.e., a property that considers\nthe single path resulting from a particular strategic interaction. In this\npaper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the\noutcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,\ni.e., a property that relates multiple paths. We show that HyperSL can capture\nimportant properties that cannot be expressed in SL, including\nnon-interference, quantitative Nash equilibria, optimal adversarial planning,\nand reasoning under imperfect information. On the algorithmic side, we identify\nan expressive fragment of HyperSL with decidable model checking and present a\nmodel-checking algorithm. We contribute a prototype implementation of our\nalgorithm and report on encouraging experimental results.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.MA",
    "comment": "AAMAS 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13741v1",
    "published_date": "2024-03-20 16:47:53 UTC",
    "updated_date": "2024-03-20 16:47:53 UTC"
  },
  {
    "arxiv_id": "2403.13729v1",
    "title": "Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study",
    "authors": [
      "Luca Giamattei",
      "Matteo Biagiola",
      "Roberto Pietrantuono",
      "Stefano Russo",
      "Paolo Tonella"
    ],
    "abstract": "In a recent study, Reinforcement Learning (RL) used in combination with\nmany-objective search, has been shown to outperform alternative techniques\n(random search and many-objective search) for online testing of Deep Neural\nNetwork-enabled systems. The empirical evaluation of these techniques was\nconducted on a state-of-the-art Autonomous Driving System (ADS). This work is a\nreplication and extension of that empirical study. Our replication shows that\nRL does not outperform pure random test generation in a comparison conducted\nunder the same settings of the original study, but with no confounding factor\ncoming from the way collisions are measured. Our extension aims at eliminating\nsome of the possible reasons for the poor performance of RL observed in our\nreplication: (1) the presence of reward components providing contrasting or\nuseless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)\nwhich requires discretization of an intrinsically continuous state space.\nResults show that our new RL agent is able to converge to an effective policy\nthat outperforms random testing. Results also highlight other possible\nimprovements, which open to further investigations on how to best leverage RL\nfor online ADS testing.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13729v1",
    "published_date": "2024-03-20 16:39:17 UTC",
    "updated_date": "2024-03-20 16:39:17 UTC"
  },
  {
    "arxiv_id": "2403.13728v3",
    "title": "M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling",
    "authors": [
      "Xudong Sun",
      "Nutan Chen",
      "Alexej Gossmann",
      "Matteo Wohlrapp",
      "Yu Xing",
      "Carla Feistner",
      "Emilio Dorigatt",
      "Felix Drost",
      "Daniele Scarcella",
      "Lisa Beer",
      "Carsten Marr"
    ],
    "abstract": "A probabilistic graphical model is proposed, modeling the joint model\nparameter and multiplier evolution, with a hypervolume based likelihood,\npromoting multi-objective descent in structural risk minimization. We address\nmulti-objective model parameter optimization via a surrogate single objective\npenalty loss with time-varying multipliers, equivalent to online scheduling of\nloss landscape. The multi-objective descent goal is dispatched hierarchically\ninto a series of constraint optimization sub-problems with shrinking bounds\naccording to Pareto dominance. The bound serves as setpoint for the low-level\nmultiplier controller to schedule loss landscapes via output feedback of each\nloss term. Our method forms closed loop of model parameter dynamic, circumvents\nexcessive memory requirements and extra computational burden of existing\nmulti-objective deep learning methods, and is robust against controller\nhyperparameter variation, demonstrated on domain generalization tasks with\nmulti-dimensional regularization losses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13728v3",
    "published_date": "2024-03-20 16:38:26 UTC",
    "updated_date": "2025-03-11 14:02:30 UTC"
  },
  {
    "arxiv_id": "2403.13721v1",
    "title": "Large Language Models meet Network Slicing Management and Orchestration",
    "authors": [
      "Abdulhalim Dandoush",
      "Viswanath Kumarskandpriya",
      "Mueen Uddin",
      "Usman Khalil"
    ],
    "abstract": "Network slicing, a cornerstone technology for future networks, enables the\ncreation of customized virtual networks on a shared physical infrastructure.\nThis fosters innovation and agility by providing dedicated resources tailored\nto specific applications. However, current orchestration and management\napproaches face limitations in handling the complexity of new service demands\nwithin multi-administrative domain environments. This paper proposes a future\nvision for network slicing powered by Large Language Models (LLMs) and\nmulti-agent systems, offering a framework that can be integrated with existing\nManagement and Orchestration (MANO) frameworks. This framework leverages LLMs\nto translate user intent into technical requirements, map network functions to\ninfrastructure, and manage the entire slice lifecycle, while multi-agent\nsystems facilitate collaboration across different administrative domains. We\nalso discuss the challenges associated with implementing this framework and\npotential solutions to mitigate them.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13721v1",
    "published_date": "2024-03-20 16:29:52 UTC",
    "updated_date": "2024-03-20 16:29:52 UTC"
  },
  {
    "arxiv_id": "2403.15474v2",
    "title": "EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union",
    "authors": [
      "Brian Hsuan-Cheng Liao",
      "Chih-Hong Cheng",
      "Hasan Esen",
      "Alois Knoll"
    ],
    "abstract": "This paper presents Ego-Centric Intersection-over-Union (EC-IoU), addressing\nthe limitation of the standard IoU measure in characterizing safety-related\nperformance for object detectors in navigating contexts. Concretely, we propose\na weighting mechanism to refine IoU, allowing it to assign a higher score to a\nprediction that covers closer points of a ground-truth object from the ego\nagent's perspective. The proposed EC-IoU measure can be used in typical\nevaluation processes to select object detectors with better safety-related\nperformance for downstream tasks. It can also be integrated into common loss\nfunctions for model fine-tuning. While geared towards safety, our experiment\nwith the KITTI dataset demonstrates the performance of a model trained on\nEC-IoU can be better than that of a variant trained on IoU in terms of mean\nAverage Precision as well.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages (IEEE double column format), 7 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.15474v2",
    "published_date": "2024-03-20 16:25:49 UTC",
    "updated_date": "2025-01-02 11:28:39 UTC"
  },
  {
    "arxiv_id": "2403.13705v1",
    "title": "Research Re: search & Re-search",
    "authors": [
      "Aske Plaat"
    ],
    "abstract": "Search algorithms are often categorized by their node expansion strategy. One\noption is the depth-first strategy, a simple backtracking strategy that\ntraverses the search space in the order in which successor nodes are generated.\nAn alternative is the best-first strategy, which was designed to make it\npossible to use domain-specific heuristic information. By exploring promising\nparts of the search space first, best-first algorithms are usually more\nefficient than depth-first algorithms.\n  In programs that play minimax games such as chess and checkers, the\nefficiency of the search is of crucial importance. Given the success of\nbest-first algorithms in other domains, one would expect them to be used for\nminimax games too. However, all high-performance game-playing programs are\nbased on a depth-first algorithm.\n  This study takes a closer look at a depth-first algorithm, AB, and a\nbest-first algorithm, SSS. The prevailing opinion on these algorithms is that\nSSS offers the potential for a more efficient search, but that its complicated\nformulation and exponential memory requirements render it impractical. The\ntheoretical part of this work shows that there is a surprisingly\nstraightforward link between the two algorithms -- for all practical purposes,\nSSS is a special case of AB. Subsequent empirical evidence proves the\nprevailing opinion on SSS to be wrong: it is not a complicated algorithm, it\ndoes not need too much memory, and it is also not more efficient than\ndepth-first search.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "PhD thesis Aske Plaat 20 June 1996. AlphaBeta, SSS*, MTD(f)",
    "pdf_url": "http://arxiv.org/pdf/2403.13705v1",
    "published_date": "2024-03-20 16:08:57 UTC",
    "updated_date": "2024-03-20 16:08:57 UTC"
  },
  {
    "arxiv_id": "2403.13703v1",
    "title": "Fostc3net:A Lightweight YOLOv5 Based On the Network Structure Optimization",
    "authors": [
      "Danqing Ma",
      "Shaojie Li",
      "Bo Dang",
      "Hengyi Zang",
      "Xinqi Dong"
    ],
    "abstract": "Transmission line detection technology is crucial for automatic monitoring\nand ensuring the safety of electrical facilities. The YOLOv5 series is\ncurrently one of the most advanced and widely used methods for object\ndetection. However, it faces inherent challenges, such as high computational\nload on devices and insufficient detection accuracy. To address these concerns,\nthis paper presents an enhanced lightweight YOLOv5 technique customized for\nmobile devices, specifically intended for identifying objects associated with\ntransmission lines. The C3Ghost module is integrated into the convolutional\nnetwork of YOLOv5 to reduce floating point operations per second (FLOPs) in the\nfeature channel fusion process and improve feature expression performance. In\naddition, a FasterNet module is introduced to replace the c3 module in the\nYOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only\na portion of the input channels, improving feature extraction efficiency and\nreducing computational overhead. To address the imbalance between simple and\nchallenging samples in the dataset and the diversity of aspect ratios of\nbounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate\nthe performance of the proposed approach, Experiments are conducted on a custom\ndataset of transmission line poles. The results show that the proposed model\nachieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a\n26% decrease in model parameters compared to the existing YOLOv5.In the\nablation experiment, it was also discovered that while the Fastnet module and\nthe CSghost module improved the precision of the original YOLOv5 baseline\nmodel, they caused a decrease in the mAP@.5-.95 metric. However, the\nimprovement of the wIoUv3 loss function significantly mitigated the decline of\nthe mAP@.5-.95 metric.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13703v1",
    "published_date": "2024-03-20 16:07:04 UTC",
    "updated_date": "2024-03-20 16:07:04 UTC"
  },
  {
    "arxiv_id": "2403.15472v3",
    "title": "Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course",
    "authors": [
      "Boxaun Ma",
      "Li Chen",
      "Shin'ichi Konomi"
    ],
    "abstract": "The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.15472v3",
    "published_date": "2024-03-20 15:47:28 UTC",
    "updated_date": "2024-04-05 11:32:24 UTC"
  },
  {
    "arxiv_id": "2403.13684v3",
    "title": "SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning",
    "authors": [
      "Hongjun Wang",
      "Sagar Vaze",
      "Kai Han"
    ],
    "abstract": "Generalized Category Discovery (GCD) aims to classify unlabelled images from\nboth `seen' and `unseen' classes by transferring knowledge from a set of\nlabelled `seen' class images. A key theme in existing GCD approaches is\nadapting large-scale pre-trained models for the GCD task. An alternate\nperspective, however, is to adapt the data representation itself for better\nalignment with the pre-trained model. As such, in this paper, we introduce a\ntwo-stage adaptation approach termed SPTNet, which iteratively optimizes model\nparameters (i.e., model-finetuning) and data parameters (i.e., prompt\nlearning). Furthermore, we propose a novel spatial prompt tuning method (SPT)\nwhich considers the spatial property of image data, enabling the method to\nbetter focus on object parts, which can transfer between seen and unseen\nclasses. We thoroughly evaluate our SPTNet on standard benchmarks and\ndemonstrate that our method outperforms existing GCD methods. Notably, we find\nour method achieves an average accuracy of 61.4% on the SSB, surpassing prior\nstate-of-the-art methods by approximately 10%. The improvement is particularly\nremarkable as our method yields extra parameters amounting to only 0.117% of\nthose in the backbone architecture. Project page:\nhttps://visual-ai.github.io/sptnet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "v3: Fix bold typos in table 2 and 3; v2: Update DINOv2 results;\n  Accepted as a conference paper at ICLR 2024; Project page:\n  https://visual-ai.github.io/sptnet",
    "pdf_url": "http://arxiv.org/pdf/2403.13684v3",
    "published_date": "2024-03-20 15:41:39 UTC",
    "updated_date": "2025-03-12 08:14:35 UTC"
  },
  {
    "arxiv_id": "2403.13682v5",
    "title": "Threats, Attacks, and Defenses in Machine Unlearning: A Survey",
    "authors": [
      "Ziyao Liu",
      "Huanyi Ye",
      "Chen Chen",
      "Yongsen Zheng",
      "Kwok-Yan Lam"
    ],
    "abstract": "Machine Unlearning (MU) has recently gained considerable attention due to its\npotential to achieve Safe AI by removing the influence of specific data from\ntrained Machine Learning (ML) models. This process, known as knowledge removal,\naddresses AI governance concerns of training data such as quality, sensitivity,\ncopyright restrictions, and obsolescence. This capability is also crucial for\nensuring compliance with privacy regulations such as the Right To Be Forgotten\n(RTBF). Furthermore, effective knowledge removal mitigates the risk of harmful\noutcomes, safeguarding against biases, misinformation, and unauthorized data\nexploitation, thereby enhancing the safe and responsible use of AI systems.\nEfforts have been made to design efficient unlearning approaches, with MU\nservices being examined for integration with existing machine learning as a\nservice (MLaaS), allowing users to submit requests to remove specific data from\nthe training corpus. However, recent research highlights vulnerabilities in\nmachine unlearning systems, such as information leakage and malicious\nunlearning, that can lead to significant security and privacy concerns.\nMoreover, extensive research indicates that unlearning methods and prevalent\nattacks fulfill diverse roles within MU systems. This underscores the intricate\nrelationship and complex interplay among these mechanisms in maintaining system\nfunctionality and safety. This survey aims to fill the gap between the\nextensive number of studies on threats, attacks, and defenses in machine\nunlearning and the absence of a comprehensive review that categorizes their\ntaxonomy, methods, and solutions, thus offering valuable insights for future\nresearch directions and practical implementations.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by IEEE Open Journal of the Computer Society",
    "pdf_url": "http://arxiv.org/pdf/2403.13682v5",
    "published_date": "2024-03-20 15:40:18 UTC",
    "updated_date": "2025-02-17 05:57:26 UTC"
  },
  {
    "arxiv_id": "2403.13681v2",
    "title": "PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for Legal Domain Adaptation?",
    "authors": [
      "Mitodru Niyogi",
      "Arnab Bhattacharya"
    ],
    "abstract": "In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13681v2",
    "published_date": "2024-03-20 15:39:54 UTC",
    "updated_date": "2024-10-03 16:01:01 UTC"
  },
  {
    "arxiv_id": "2403.13653v2",
    "title": "Learning User Embeddings from Human Gaze for Personalised Saliency Prediction",
    "authors": [
      "Florian Strohm",
      "Mihai B√¢ce",
      "Andreas Bulling"
    ],
    "abstract": "Reusable embeddings of user behaviour have shown significant performance\nimprovements for the personalised saliency prediction task. However, prior\nworks require explicit user characteristics and preferences as input, which are\noften difficult to obtain. We present a novel method to extract user embeddings\nfrom pairs of natural images and corresponding saliency maps generated from a\nsmall amount of user-specific eye tracking data. At the core of our method is a\nSiamese convolutional neural encoder that learns the user embeddings by\ncontrasting the image and personal saliency map pairs of different users.\nEvaluations on two public saliency datasets show that the generated embeddings\nhave high discriminative power, are effective at refining universal saliency\nmaps to the individual users, and generalise well across users and images.\nFinally, based on our model's ability to encode individual user\ncharacteristics, our work points towards other applications that can benefit\nfrom reusable embeddings of gaze behaviour.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13653v2",
    "published_date": "2024-03-20 14:58:40 UTC",
    "updated_date": "2024-03-26 08:45:09 UTC"
  },
  {
    "arxiv_id": "2404.10177v2",
    "title": "Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data",
    "authors": [
      "Giannis Daras",
      "Alexandros G. Dimakis",
      "Constantinos Daskalakis"
    ],
    "abstract": "Ambient diffusion is a recently proposed framework for training diffusion\nmodels using corrupted data. Both Ambient Diffusion and alternative SURE-based\napproaches for learning diffusion models from corrupted data resort to\napproximations which deteriorate performance. We present the first framework\nfor training diffusion models that provably sample from the uncorrupted\ndistribution given only noisy training data, solving an open problem in this\nspace. Our key technical contribution is a method that uses a double\napplication of Tweedie's formula and a consistency loss function that allows us\nto extend sampling at noise levels below the observed data noise. We also\nprovide further evidence that diffusion models memorize from their training\nsets by identifying extremely corrupted images that are almost perfectly\nreconstructed, raising copyright and privacy concerns. Our method for training\nusing corrupted samples can be used to mitigate this problem. We demonstrate\nthis by fine-tuning Stable Diffusion XL to generate samples from a distribution\nusing only noisy samples. Our framework reduces the amount of memorization of\nthe fine-tuning dataset, while maintaining competitive performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.10177v2",
    "published_date": "2024-03-20 14:22:12 UTC",
    "updated_date": "2024-07-22 11:31:08 UTC"
  },
  {
    "arxiv_id": "2404.00022v1",
    "title": "Analysing and Organising Human Communications for AI Fairness-Related Decisions: Use Cases from the Public Sector",
    "authors": [
      "Mirthe Dankloff",
      "Vanja Skoric",
      "Giovanni Sileno",
      "Sennay Ghebreab",
      "Jacco Van Ossenbruggen",
      "Emma Beauxis-Aussalet"
    ],
    "abstract": "AI algorithms used in the public sector, e.g., for allocating social benefits\nor predicting fraud, often involve multiple public and private stakeholders at\nvarious phases of the algorithm's life-cycle. Communication issues between\nthese diverse stakeholders can lead to misinterpretation and misuse of\nalgorithms. We investigate the communication processes for AI fairness-related\ndecisions by conducting interviews with practitioners working on algorithmic\nsystems in the public sector. By applying qualitative coding analysis, we\nidentify key elements of communication processes that underlie fairness-related\nhuman decisions. We analyze the division of roles, tasks, skills, and\nchallenges perceived by stakeholders. We formalize the underlying communication\nissues within a conceptual framework that i. represents the communication\npatterns ii. outlines missing elements, such as actors who miss skills for\ntheir tasks. The framework is used for describing and analyzing key\norganizational issues for fairness-related decisions. Three general patterns\nemerge from the analysis: 1. Policy-makers, civil servants, and domain experts\nare less involved compared to developers throughout a system's life-cycle. This\nleads to developers taking on extra roles such as advisor, while they\npotentially miss the required skills and guidance from domain experts. 2.\nEnd-users and policy-makers often lack the technical skills to interpret a\nsystem's limitations, and rely on developer roles for making decisions\nconcerning fairness issues. 3. Citizens are structurally absent throughout a\nsystem's life-cycle, which may lead to decisions that do not include relevant\nconsiderations from impacted stakeholders.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.00022v1",
    "published_date": "2024-03-20 14:20:42 UTC",
    "updated_date": "2024-03-20 14:20:42 UTC"
  },
  {
    "arxiv_id": "2403.13619v1",
    "title": "Dynamic Resource Allocation for Virtual Machine Migration Optimization using Machine Learning",
    "authors": [
      "Yulu Gong",
      "Jiaxin Huang",
      "Bo Liu",
      "Jingyu Xu",
      "Binbin Wu",
      "Yifan Zhang"
    ],
    "abstract": "The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13619v1",
    "published_date": "2024-03-20 14:13:44 UTC",
    "updated_date": "2024-03-20 14:13:44 UTC"
  },
  {
    "arxiv_id": "2403.13869v3",
    "title": "Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems",
    "authors": [
      "Ruoxuan Bai",
      "Jingxuan Yang",
      "Weiduo Gong",
      "Yi Zhang",
      "Qiujing Lu",
      "Shuo Feng"
    ],
    "abstract": "Intelligent systems are increasingly integral to our daily lives, yet rare\nsafety-critical events present significant latent threats to their practical\ndeployment. Addressing this challenge hinges on accurately predicting the\nprobability of safety-critical events occurring within a given time step from\nthe current state, a metric we define as 'criticality'. The complexity of\npredicting criticality arises from the extreme data imbalance caused by rare\nevents in high dimensional variables associated with the rare events, a\nchallenge we refer to as the curse of rarity. Existing methods tend to be\neither overly conservative or prone to overlooking safety-critical events, thus\nstruggling to achieve both high precision and recall rates, which severely\nlimits their applicability. This study endeavors to develop a criticality\nprediction model that excels in both precision and recall rates for evaluating\nthe criticality of safety-critical autonomous systems. We propose a multi-stage\nlearning framework designed to progressively densify the dataset, mitigating\nthe curse of rarity across stages. To validate our approach, we evaluate it in\ntwo cases: lunar lander and bipedal walker scenarios. The results demonstrate\nthat our method surpasses traditional approaches, providing a more accurate and\ndependable assessment of criticality in intelligent systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13869v3",
    "published_date": "2024-03-20 14:00:29 UTC",
    "updated_date": "2024-04-05 15:48:03 UTC"
  },
  {
    "arxiv_id": "2403.13597v2",
    "title": "No more optimization rules: LLM-enabled policy-based multi-modal query optimizer",
    "authors": [
      "Yifan Wang",
      "Haodi Ma",
      "Daisy Zhe Wang"
    ],
    "abstract": "Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "Yifan and Haodi contribute equally to the work",
    "pdf_url": "http://arxiv.org/pdf/2403.13597v2",
    "published_date": "2024-03-20 13:44:30 UTC",
    "updated_date": "2024-03-23 17:05:15 UTC"
  },
  {
    "arxiv_id": "2403.13574v1",
    "title": "A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation",
    "authors": [
      "Bowen Zheng",
      "Zihan Lin",
      "Enze Liu",
      "Chen Yang",
      "Enyang Bai",
      "Cheng Ling",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "abstract": "In online video platforms, reading or writing comments on interesting videos\nhas become an essential part of the video watching experience. However,\nexisting video recommender systems mainly model users' interaction behaviors\nwith videos, lacking consideration of comments in user behavior modeling. In\nthis paper, we propose a novel recommendation approach called LSVCR by\nleveraging user interaction histories with both videos and comments, so as to\njointly conduct personalized video and comment recommendation. Specifically,\nour approach consists of two key components, namely sequential recommendation\n(SR) model and supplemental large language model (LLM) recommender. The SR\nmodel serves as the primary recommendation backbone (retained in deployment) of\nour approach, allowing for efficient user preference modeling. Meanwhile, we\nleverage the LLM recommender as a supplemental component (discarded in\ndeployment) to better capture underlying user preferences from heterogeneous\ninteraction behaviors. In order to integrate the merits of the SR model and the\nsupplemental LLM recommender, we design a twostage training paradigm. The first\nstage is personalized preference alignment, which aims to align the preference\nrepresentations from both components, thereby enhancing the semantics of the SR\nmodel. The second stage is recommendation-oriented fine-tuning, in which the\nalignment-enhanced SR model is fine-tuned according to specific objectives.\nExtensive experiments in both video and comment recommendation tasks\ndemonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the\nKuaiShou platform verifies the actual benefits brought by our approach. In\nparticular, we achieve a significant overall gain of 4.13% in comment watch\ntime.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13574v1",
    "published_date": "2024-03-20 13:14:29 UTC",
    "updated_date": "2024-03-20 13:14:29 UTC"
  },
  {
    "arxiv_id": "2403.13556v2",
    "title": "Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments",
    "authors": [
      "Djamahl Etchegaray",
      "Zi Huang",
      "Tatsuya Harada",
      "Yadan Luo"
    ],
    "abstract": "In this work, we tackle the limitations of current LiDAR-based 3D object\ndetection systems, which are hindered by a restricted class vocabulary and the\nhigh costs associated with annotating new object classes. Our exploration of\nopen-vocabulary (OV) learning in urban environments aims to capture novel\ninstances using pre-trained vision-language models (VLMs) with multi-sensor\ndata. We design and benchmark a set of four potential solutions as baselines,\ncategorizing them into either top-down or bottom-up approaches based on their\ninput data strategies. While effective, these methods exhibit certain\nlimitations, such as missing novel objects in 3D box estimation or applying\nrigorous priors, leading to biases towards objects near the camera or of\nrectangular geometries. To overcome these limitations, we introduce a universal\n\\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the\nrecall of novel objects and propagating this detection capability to more\ndistant areas thereby progressively capturing more. In particular, we utilize a\ngreedy box seeker to search against 3D novel boxes of varying orientations and\ndepth in each generated frustum and ensure the reliability of newly identified\nboxes by cross alignment and density ranker. Additionally, the inherent bias\ntowards camera-proximal objects is alleviated by the proposed remote simulator,\nwhich randomly diversifies pseudo-labeled novel instances in the self-training\nprocess, combined with the fusion of base samples in the memory bank. Extensive\nexperiments demonstrate a 53% improvement in novel recall across diverse OV\nsettings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold\nincrease in Average Precision (AP) for novel object classes. The source code is\nmade available at https://github.com/djamahl99/findnpropagate.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in ECCV 2024. Source code:\n  https://github.com/djamahl99/findnpropagate",
    "pdf_url": "http://arxiv.org/pdf/2403.13556v2",
    "published_date": "2024-03-20 12:51:30 UTC",
    "updated_date": "2024-07-12 10:42:30 UTC"
  },
  {
    "arxiv_id": "2403.13553v1",
    "title": "VCounselor: A Psychological Intervention Chat Agent Based on a Knowledge-Enhanced Large Language Model",
    "authors": [
      "H. Zhang",
      "Z. Qiao",
      "H. Wang",
      "B. Duan",
      "J. Yin"
    ],
    "abstract": "Conversational artificial intelligence can already independently engage in\nbrief conversations with clients with psychological problems and provide\nevidence-based psychological interventions. The main objective of this study is\nto improve the effectiveness and credibility of the large language model in\npsychological intervention by creating a specialized agent, the VCounselor, to\naddress the limitations observed in popular large language models such as\nChatGPT in domain applications. We achieved this goal by proposing a new\naffective interaction structure and knowledge-enhancement structure. In order\nto evaluate VCounselor, this study compared the general large language model,\nthe fine-tuned large language model, and VCounselor's knowledge-enhanced large\nlanguage model. At the same time, the general large language model and the\nfine-tuned large language model will also be provided with an avatar to compare\nthem as an agent with VCounselor. The comparison results indicated that the\naffective interaction structure and knowledge-enhancement structure of\nVCounselor significantly improved the effectiveness and credibility of the\npsychological intervention, and VCounselor significantly provided positive\ntendencies for clients' emotions. The conclusion of this study strongly\nsupports that VConselor has a significant advantage in providing psychological\nsupport to clients by being able to analyze the patient's problems with\nrelative accuracy and provide professional-level advice that enhances support\nfor clients.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "J.4"
    ],
    "primary_category": "cs.HC",
    "comment": "24 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13553v1",
    "published_date": "2024-03-20 12:46:02 UTC",
    "updated_date": "2024-03-20 12:46:02 UTC"
  },
  {
    "arxiv_id": "2405.09550v3",
    "title": "Mask-based Invisible Backdoor Attacks on Object Detection",
    "authors": [
      "Jeongjin Shin"
    ],
    "abstract": "Deep learning models have achieved unprecedented performance in the domain of\nobject detection, resulting in breakthroughs in areas such as autonomous\ndriving and security. However, deep learning models are vulnerable to backdoor\nattacks. These attacks prompt models to behave similarly to standard models\nwithout a trigger; however, they act maliciously upon detecting a predefined\ntrigger. Despite extensive research on backdoor attacks in image\nclassification, their application to object detection remains relatively\nunderexplored. Given the widespread application of object detection in critical\nreal-world scenarios, the sensitivity and potential impact of these\nvulnerabilities cannot be overstated. In this study, we propose an effective\ninvisible backdoor attack on object detection utilizing a mask-based approach.\nThree distinct attack scenarios were explored for object detection: object\ndisappearance, object misclassification, and object generation attack. Through\nextensive experiments, we comprehensively examined the effectiveness of these\nattacks and tested certain defense methods to determine effective\ncountermeasures. Code will be available at\nhttps://github.com/jeongjin0/invisible-backdoor-object-detection",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "I.4.8"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.09550v3",
    "published_date": "2024-03-20 12:27:30 UTC",
    "updated_date": "2024-06-04 11:28:42 UTC"
  },
  {
    "arxiv_id": "2406.19397v1",
    "title": "How scanning probe microscopy can be supported by Artificial Intelligence and quantum computing",
    "authors": [
      "Agnieszka Pregowska",
      "Agata Roszkiewicz",
      "Magdalena Osial",
      "Michael Giersig"
    ],
    "abstract": "We focus on the potential possibilities for supporting Scanning Probe\nMicroscopy measurements, emphasizing the application of Artificial\nIntelligence, especially Machine Learning as well as quantum computing. It\nturned out that Artificial Intelligence can be helpful in the experimental\nprocesses automation in routine operations, the algorithmic search for good\nsample regions, and shed light on the structure property relationships. Thus,\nit contributes to increasing the efficiency and accuracy of optical nanoscopy\nscanning probes. Moreover, the combination of Artificial Intelligence based\nalgorithms and quantum computing may have a huge potential to increase the\npractical application of Scanning Probe Microscopy. The limitations were also\ndiscussed. Finally, we outline a research path for the improvement of the\nproposed approach.",
    "categories": [
      "q-bio.NC",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "q-bio.NC",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.19397v1",
    "published_date": "2024-03-20 12:22:02 UTC",
    "updated_date": "2024-03-20 12:22:02 UTC"
  },
  {
    "arxiv_id": "2403.13537v1",
    "title": "What explains the success of cross-modal fine-tuning with ORCA?",
    "authors": [
      "Paloma Garc√≠a-de-Herreros",
      "Vagrant Gautam",
      "Philipp Slusallek",
      "Dietrich Klakow",
      "Marius Mosbach"
    ],
    "abstract": "ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,\ni.e., applying pre-trained transformer models to modalities beyond their\ntraining data. The technique consists primarily of training an embedder and\nfine-tuning the embedder and model. Despite its high performance on a variety\nof downstream tasks, we do not understand precisely how each of these\ncomponents contribute to ORCA's success. Therefore, we run a series of\nablations and find that embedder training does not help 2D tasks at all,\ncontrary to what the original paper posits. In 1D tasks, some amount of\nembedder training is necessary but more is not better. In 4 out of 6 datasets\nwe experiment with, it is model fine-tuning that makes the biggest difference.\nThrough our ablations and baselines, we contribute a better understanding of\nthe individual components of ORCA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13537v1",
    "published_date": "2024-03-20 12:14:54 UTC",
    "updated_date": "2024-03-20 12:14:54 UTC"
  },
  {
    "arxiv_id": "2403.13524v1",
    "title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single Image",
    "authors": [
      "Bowen Zhang",
      "Tianyu Yang",
      "Yu Li",
      "Lei Zhang",
      "Xi Zhao"
    ],
    "abstract": "3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13524v1",
    "published_date": "2024-03-20 11:51:04 UTC",
    "updated_date": "2024-03-20 11:51:04 UTC"
  },
  {
    "arxiv_id": "2403.13523v1",
    "title": "Have You Poisoned My Data? Defending Neural Networks against Data Poisoning",
    "authors": [
      "Fabio De Gaspari",
      "Dorjan Hitaj",
      "Luigi V. Mancini"
    ],
    "abstract": "The unprecedented availability of training data fueled the rapid development\nof powerful neural networks in recent years. However, the need for such large\namounts of data leads to potential threats such as poisoning attacks:\nadversarial manipulations of the training data aimed at compromising the\nlearned model to achieve a given adversarial goal.\n  This paper investigates defenses against clean-label poisoning attacks and\nproposes a novel approach to detect and filter poisoned datapoints in the\ntransfer learning setting. We define a new characteristic vector representation\nof datapoints and show that it effectively captures the intrinsic properties of\nthe data distribution. Through experimental analysis, we demonstrate that\neffective poisons can be successfully differentiated from clean points in the\ncharacteristic vector space. We thoroughly evaluate our proposed approach and\ncompare it to existing state-of-the-art defenses using multiple architectures,\ndatasets, and poison budgets. Our evaluation shows that our proposal\noutperforms existing approaches in defense rate and final trained model\nperformance across all experimental settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted for publication at European Symposium on Research in\n  Computer Security (ESORICS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13523v1",
    "published_date": "2024-03-20 11:50:16 UTC",
    "updated_date": "2024-03-20 11:50:16 UTC"
  },
  {
    "arxiv_id": "2403.13866v1",
    "title": "The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training",
    "authors": [
      "Joo Yong Shim",
      "Jean Seong Bjorn Choe",
      "Jong-Kook Kim"
    ],
    "abstract": "This article proposes auction-inspired multi-player generative adversarial\nnetworks training, which mitigates the mode collapse problem of GANs. Mode\ncollapse occurs when an over-fitted generator generates a limited range of\nsamples, often concentrating on a small subset of the data distribution.\nDespite the restricted diversity of generated samples, the discriminator can\nstill be deceived into distinguishing these samples as real samples from the\nactual distribution. In the absence of external standards, a model cannot\nrecognize its failure during the training phase. We extend the two-player game\nof generative adversarial networks to the multi-player game. During the\ntraining, the values of each model are determined by the bids submitted by\nother players in an auction-like process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13866v1",
    "published_date": "2024-03-20 11:47:42 UTC",
    "updated_date": "2024-03-20 11:47:42 UTC"
  },
  {
    "arxiv_id": "2403.13518v2",
    "title": "Motion Generation from Fine-grained Textual Descriptions",
    "authors": [
      "Kunhang Li",
      "Yansong Feng"
    ],
    "abstract": "The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13518v2",
    "published_date": "2024-03-20 11:38:30 UTC",
    "updated_date": "2024-03-26 11:16:47 UTC"
  },
  {
    "arxiv_id": "2403.13513v2",
    "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
    "authors": [
      "Junho Kim",
      "Yeon Ju Kim",
      "Yong Man Ro"
    ],
    "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal\nModels (LMMs) in addressing hallucination, where the models generate\ncross-modal inconsistent responses. Without additional training, we propose\nCounterfactual Inception, a novel method that implants counterfactual thinking\ninto LMMs using self-generated counterfactual keywords. Our method is grounded\nin the concept of counterfactual thinking, a cognitive process where human\nconsiders alternative realities, enabling more extensive context exploration.\nBridging the human cognition mechanism into LMMs, we aim for the models to\nengage with and generate responses that span a wider contextual scene\nunderstanding, mitigating hallucinatory outputs. We further introduce\nPlausibility Verification Process (PVP), a simple yet robust keyword constraint\nthat effectively filters out sub-optimal keywords to enable the consistent\ntriggering of counterfactual thinking in the model responses. Comprehensive\nanalyses across various LMMs, including both open-source and proprietary\nmodels, corroborate that counterfactual thinking significantly reduces\nhallucination and helps to broaden contextual understanding based on true\nvisual clues.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://ivy-lvlm.github.io/Counterfactual-Inception/",
    "pdf_url": "http://arxiv.org/pdf/2403.13513v2",
    "published_date": "2024-03-20 11:27:20 UTC",
    "updated_date": "2024-06-21 06:11:25 UTC"
  },
  {
    "arxiv_id": "2403.13512v1",
    "title": "Scale Decoupled Distillation",
    "authors": [
      "Shicai Wei Chunbo Luo Yang Luo"
    ],
    "abstract": "Logit knowledge distillation attracts increasing attention due to its\npracticality in recent studies. However, it often suffers inferior performance\ncompared to the feature knowledge distillation. In this paper, we argue that\nexisting logit-based methods may be sub-optimal since they only leverage the\nglobal logit output that couples multiple semantic knowledge. This may transfer\nambiguous knowledge to the student and mislead its learning. To this end, we\npropose a simple but effective method, i.e., Scale Decoupled Distillation\n(SDD), for logit knowledge distillation. SDD decouples the global logit output\ninto multiple local logit outputs and establishes distillation pipelines for\nthem. This helps the student to mine and inherit fine-grained and unambiguous\nlogit knowledge. Moreover, the decoupled knowledge can be further divided into\nconsistent and complementary logit knowledge that transfers the semantic\ninformation and sample ambiguity, respectively. By increasing the weight of\ncomplementary parts, SDD can guide the student to focus more on ambiguous\nsamples, improving its discrimination ability. Extensive experiments on several\nbenchmark datasets demonstrate the effectiveness of SDD for wide\nteacher-student pairs, especially in the fine-grained classification task. Code\nis available at: https://github.com/shicaiwei123/SDD-CVPR2024",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR2024 10 pages 6figure",
    "pdf_url": "http://arxiv.org/pdf/2403.13512v1",
    "published_date": "2024-03-20 11:21:22 UTC",
    "updated_date": "2024-03-20 11:21:22 UTC"
  },
  {
    "arxiv_id": "2403.13501v2",
    "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
    "authors": [
      "Yumeng Li",
      "William Beluch",
      "Margret Keuper",
      "Dan Zhang",
      "Anna Khoreva"
    ],
    "abstract": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR",
    "pdf_url": "http://arxiv.org/pdf/2403.13501v2",
    "published_date": "2024-03-20 10:58:58 UTC",
    "updated_date": "2025-03-18 13:55:22 UTC"
  },
  {
    "arxiv_id": "2403.13479v1",
    "title": "Deepfake Detection without Deepfakes: Generalization via Synthetic Frequency Patterns Injection",
    "authors": [
      "Davide Alessandro Coccomini",
      "Roberto Caldelli",
      "Claudio Gennaro",
      "Giuseppe Fiameni",
      "Giuseppe Amato",
      "Fabrizio Falchi"
    ],
    "abstract": "Deepfake detectors are typically trained on large sets of pristine and\ngenerated images, resulting in limited generalization capacity; they excel at\nidentifying deepfakes created through methods encountered during training but\nstruggle with those generated by unknown techniques. This paper introduces a\nlearning approach aimed at significantly enhancing the generalization\ncapabilities of deepfake detectors. Our method takes inspiration from the\nunique \"fingerprints\" that image generation processes consistently introduce\ninto the frequency domain. These fingerprints manifest as structured and\ndistinctly recognizable frequency patterns. We propose to train detectors using\nonly pristine images injecting in part of them crafted frequency patterns,\nsimulating the effects of various deepfake generation techniques without being\nspecific to any. These synthetic patterns are based on generic shapes, grids,\nor auras. We evaluated our approach using diverse architectures across 25\ndifferent generation methods. The models trained with our approach were able to\nperform state-of-the-art deepfake detection, demonstrating also superior\ngeneralization capabilities in comparison with previous methods. Indeed, they\nare untied to any specific generation technique and can effectively identify\ndeepfakes regardless of how they were made.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13479v1",
    "published_date": "2024-03-20 10:33:10 UTC",
    "updated_date": "2024-03-20 10:33:10 UTC"
  },
  {
    "arxiv_id": "2403.13447v1",
    "title": "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models",
    "authors": [
      "Wenqiao Zhang",
      "Tianwei Lin",
      "Jiang Liu",
      "Fangxun Shu",
      "Haoyuan Li",
      "Lei Zhang",
      "He Wanggui",
      "Hao Zhou",
      "Zheqi Lv",
      "Hao Jiang",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "abstract": "Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13447v1",
    "published_date": "2024-03-20 09:42:43 UTC",
    "updated_date": "2024-03-20 09:42:43 UTC"
  },
  {
    "arxiv_id": "2403.13441v1",
    "title": "Robustness Verifcation in Neural Networks",
    "authors": [
      "Adrian Wurm"
    ],
    "abstract": "In this paper we investigate formal verification problems for Neural Network\ncomputations. Of central importance will be various robustness and minimization\nproblems such as: Given symbolic specifications of allowed inputs and outputs\nin form of Linear Programming instances, one question is whether there do exist\nvalid inputs such that the network computes a valid output? And does this\nproperty hold for all valid inputs? Do two given networks compute the same\nfunction? Is there a smaller network computing the same function?\n  The complexity of these questions have been investigated recently from a\npractical point of view and approximated by heuristic algorithms. We complement\nthese achievements by giving a theoretical framework that enables us to\ninterchange security and efficiency questions in neural networks and analyze\ntheir computational complexities. We show that the problems are conquerable in\na semi-linear setting, meaning that for piecewise linear activation functions\nand when the sum- or maximum metric is used, most of them are in P or in NP at\nmost.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2403.13441v1",
    "published_date": "2024-03-20 09:34:38 UTC",
    "updated_date": "2024-03-20 09:34:38 UTC"
  },
  {
    "arxiv_id": "2403.13433v2",
    "title": "AgentGroupChat: An Interactive Group Chat Simulacra For Better Eliciting Emergent Behavior",
    "authors": [
      "Zhouhong Gu",
      "Xiaoxuan Zhu",
      "Haoran Guo",
      "Lin Zhang",
      "Yin Cai",
      "Hao Shen",
      "Jiangjie Chen",
      "Zheyu Ye",
      "Yifei Dai",
      "Yan Gao",
      "Yao Hu",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "abstract": "Language significantly influences the formation and evolution of Human\nemergent behavior, which is crucial in understanding collective intelligence\nwithin human societies. Considering that the study of how language affects\nhuman behavior needs to put it into the dynamic scenarios in which it is used,\nwe introduce AgentGroupChat in this paper, a simulation that delves into the\ncomplex role of language in shaping collective behavior through interactive\ndebate scenarios. Central to this simulation are characters engaging in dynamic\nconversation interactions. To enable simulation, we introduce the Verbal\nStrategist Agent, utilizing large language models to enhance interaction\nstrategies by incorporating elements of persona and action. We set four\nnarrative scenarios based on AgentGroupChat to demonstrate the simulation's\ncapacity to mimic complex language use in group dynamics. Evaluations focus on\naligning agent behaviors with human expectations and the emergence of\ncollective behaviors within the simulation. Results reveal that emergent\nbehaviors materialize from a confluence of factors: a conducive environment for\nextensive information exchange, characters with diverse traits, high linguistic\ncomprehension, and strategic adaptability. During discussions on ``the impact\nof AI on humanity'' in AgentGroupChat simulation, philosophers commonly agreed\nthat ``AI could enhance societal welfare with judicious limitations'' and even\ncome to a conclusion that ``the essence of true intelligence encompasses\nunderstanding the necessity to constrain self abilities''. Additionally, in the\ncompetitive domain of casting for primary roles in films in AgentGroupChat,\ncertain actors were ready to reduce their remuneration or accept lesser roles,\nmotivated by their deep-seated desire to contribute to the project.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13433v2",
    "published_date": "2024-03-20 09:21:32 UTC",
    "updated_date": "2024-04-04 07:40:31 UTC"
  },
  {
    "arxiv_id": "2403.13421v3",
    "title": "Caching-Augmented Lifelong Multi-Agent Path Finding",
    "authors": [
      "Yimin Tang",
      "Zhenghong Yu",
      "Yi Zheng",
      "T. K. Satish Kumar",
      "Jiaoyang Li",
      "Sven Koenig"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF), which involves finding collision-free paths\nfor multiple robots, is crucial in various applications. Lifelong MAPF, where\ntargets are reassigned to agents as soon as they complete their initial\ntargets, offers a more accurate approximation of real-world warehouse planning.\nIn this paper, we present a novel mechanism named Caching-Augmented Lifelong\nMAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have\ndeveloped a new type of map grid called cache for temporary item storage and\nreplacement, and created a locking mechanism to improve the planning solution's\nstability. A task assigner (TA) is designed for CAL-MAPF to allocate target\nlocations to agents and control agent status in different situations. CAL-MAPF\nhas been evaluated using various cache replacement policies and input task\ndistributions. We have identified three main factors significantly impacting\nCAL-MAPF performance through experimentation: suitable input task distribution,\nhigh cache hit rate, and smooth traffic. In general, CAL-MAPF has demonstrated\npotential for performance improvements in certain task distributions, map and\nagent configurations.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13421v3",
    "published_date": "2024-03-20 09:07:23 UTC",
    "updated_date": "2024-04-05 18:23:38 UTC"
  },
  {
    "arxiv_id": "2403.13408v2",
    "title": "S2DM: Sector-Shaped Diffusion Models for Video Generation",
    "authors": [
      "Haoran Lang",
      "Yuxuan Ge",
      "Zheng Tian"
    ],
    "abstract": "Diffusion models have achieved great success in image generation. However,\nwhen leveraging this idea for video generation, we face significant challenges\nin maintaining the consistency and continuity across video frames. This is\nmainly caused by the lack of an effective framework to align frames of videos\nwith desired temporal features while preserving consistent semantic and\nstochastic features. In this work, we propose a novel Sector-Shaped Diffusion\nModel (S2DM) whose sector-shaped diffusion region is formed by a set of\nray-shaped reverse diffusion processes starting at the same noise point. S2DM\ncan generate a group of intrinsically related data sharing the same semantic\nand stochastic features while varying on temporal features with appropriate\nguided conditions. We apply S2DM to video generation tasks, and explore the use\nof optical flow as temporal conditions. Our experimental results show that S2DM\noutperforms many existing methods in the task of video generation without any\ntemporal-feature modelling modules. For text-to-video generation tasks where\ntemporal conditions are not explicitly given, we propose a two-stage generation\nstrategy which can decouple the generation of temporal features from\nsemantic-content features. We show that, without additional training, our model\nintegrated with another temporal conditions generative model can still achieve\ncomparable performance with existing works. Our results can be viewd at\nhttps://s2dm.github.io/S2DM/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13408v2",
    "published_date": "2024-03-20 08:50:15 UTC",
    "updated_date": "2024-03-22 11:41:38 UTC"
  },
  {
    "arxiv_id": "2403.13405v1",
    "title": "DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation",
    "authors": [
      "Yamin Mao",
      "Zhihua Liu",
      "Weiming Li",
      "SoonYong Cho",
      "Qiang Wang",
      "Xiaoshuai Hao"
    ],
    "abstract": "Depth-based 3D hand pose estimation is an important but challenging research\ntask in human-machine interaction community. Recently, dense regression methods\nhave attracted increasing attention in 3D hand pose estimation task, which\nprovide a low computational burden and high accuracy regression way by densely\nregressing hand joint offset maps. However, large-scale regression offset\nvalues are often affected by noise and outliers, leading to a significant drop\nin accuracy. To tackle this, we re-formulate 3D hand pose estimation as a dense\nordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose\nNetwork (DOR3D-Net). Specifically, we first decompose offset value regression\ninto sub-tasks of binary classifications with ordinal constraints. Then, each\nbinary classifier can predict the probability of a binary spatial relationship\nrelative to joint, which is easier to train and yield much lower level of\nnoise. The estimated hand joint positions are inferred by aggregating the\nordinal regression results at local positions with a weighted sum. Furthermore,\nboth joint regression loss and ordinal regression loss are used to train our\nDOR3D-Net in an end-to-end manner. Extensive experiments on public datasets\n(ICVL, MSRA, NYU and HANDS2017) show that our design provides significant\nimprovements over SOTA methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13405v1",
    "published_date": "2024-03-20 08:47:51 UTC",
    "updated_date": "2024-03-20 08:47:51 UTC"
  },
  {
    "arxiv_id": "2403.13863v1",
    "title": "DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model",
    "authors": [
      "Yizhu Wen",
      "Kai Yi",
      "Jing Ke",
      "Yiqing Shen"
    ],
    "abstract": "Tabular data plays a crucial role in various domains but often suffers from\nmissing values, thereby curtailing its potential utility. Traditional\nimputation techniques frequently yield suboptimal results and impose\nsubstantial computational burdens, leading to inaccuracies in subsequent\nmodeling tasks. To address these challenges, we propose DiffImpute, a novel\nDenoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is\ntrained on complete tabular datasets, ensuring that it can produce credible\nimputations for missing entries without undermining the authenticity of the\nexisting data. Innovatively, it can be applied to various settings of Missing\nCompletely At Random (MCAR) and Missing At Random (MAR). To effectively handle\nthe tabular features in DDPM, we tailor four tabular denoising networks,\nspanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to\nenhance coherence between observed and imputed data by infusing the data back\nand denoising them multiple times during the sampling stage. To enable\nefficient inference while maintaining imputation performance, we propose a\nrefined non-Markovian sampling process that works along with Harmonization.\nEmpirical evaluations on seven diverse datasets underscore the prowess of\nDiffImpute. Specifically, when paired with the Transformer as the denoising\nnetwork, it consistently outperforms its competitors, boasting an average\nranking of 1.7 and the most minimal standard deviation. In contrast, the next\nbest method lags with a ranking of 2.8 and a standard deviation of 0.9. The\ncode is available at https://github.com/Dendiiiii/DiffImpute.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13863v1",
    "published_date": "2024-03-20 08:45:31 UTC",
    "updated_date": "2024-03-20 08:45:31 UTC"
  },
  {
    "arxiv_id": "2406.17590v1",
    "title": "Multimodal Chaptering for Long-Form TV Newscast Video",
    "authors": [
      "Khalil Guetari",
      "Yannis Tevissen",
      "Fr√©d√©ric Petitpont"
    ],
    "abstract": "We propose a novel approach for automatic chaptering of TV newscast videos,\naddressing the challenge of structuring and organizing large collections of\nunsegmented broadcast content. Our method integrates both audio and visual cues\nthrough a two-stage process involving frozen neural networks and a trained LSTM\nnetwork. The first stage extracts essential features from separate modalities,\nwhile the LSTM effectively fuses these features to generate accurate segment\nboundaries. Our proposed model has been evaluated on a diverse dataset\ncomprising over 500 TV newscast videos of an average of 41 minutes gathered\nfrom TF1, a French TV channel, with varying lengths and topics. Experimental\nresults demonstrate that this innovative fusion strategy achieves state of the\nart performance, yielding a high precision rate of 82% at IoU of 90%.\nConsequently, this approach significantly enhances analysis, indexing and\nstorage capabilities for TV newscast archives, paving the way towards efficient\nmanagement and utilization of vast audiovisual resources.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17590v1",
    "published_date": "2024-03-20 08:39:41 UTC",
    "updated_date": "2024-03-20 08:39:41 UTC"
  },
  {
    "arxiv_id": "2405.02305v1",
    "title": "Inserting Faces inside Captions: Image Captioning with Attention Guided Merging",
    "authors": [
      "Yannis Tevissen",
      "Khalil Guetari",
      "Marine Tassel",
      "Erwan Kerleroux",
      "Fr√©d√©ric Petitpont"
    ],
    "abstract": "Image captioning models are widely used to describe recent and archived\npictures with the objective of improving their accessibility and retrieval.\nYet, these approaches tend to be inefficient and biased at retrieving people's\nnames. In this work we introduce AstroCaptions, a dataset for the image\ncaptioning task. This dataset specifically contains thousands of public\nfig-ures that are complex to identify for a traditional model. We also propose\na novel post-processing method to insert identified people's names inside the\ncaption using explainable AI tools and the grounding capabilities of\nvi-sion-language models. The results obtained with this method show\nsignifi-cant improvements of captions quality and a potential of reducing\nhalluci-nations. Up to 93.2% of the persons detected can be inserted in the\nimage captions leading to improvements in the BLEU, ROUGE, CIDEr and METEOR\nscores of each captioning model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02305v1",
    "published_date": "2024-03-20 08:38:25 UTC",
    "updated_date": "2024-03-20 08:38:25 UTC"
  },
  {
    "arxiv_id": "2403.13374v3",
    "title": "Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity",
    "authors": [
      "Shiyuan Zuo",
      "Xingrun Yan",
      "Rongfei Fan",
      "Han Hu",
      "Hangguan Shan",
      "Tony Q. S. Quek"
    ],
    "abstract": "This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13374v3",
    "published_date": "2024-03-20 08:15:08 UTC",
    "updated_date": "2024-03-27 14:57:54 UTC"
  },
  {
    "arxiv_id": "2405.15776v1",
    "title": "CalliRewrite: Recovering Handwriting Behaviors from Calligraphy Images without Supervision",
    "authors": [
      "Yuxuan Luo",
      "Zekun Wu",
      "Zhouhui Lian"
    ],
    "abstract": "Human-like planning skills and dexterous manipulation have long posed\nchallenges in the fields of robotics and artificial intelligence (AI). The task\nof reinterpreting calligraphy presents a formidable challenge, as it involves\nthe decomposition of strokes and dexterous utensil control. Previous efforts\nhave primarily focused on supervised learning of a single instrument, limiting\nthe performance of robots in the realm of cross-domain text replication. To\naddress these challenges, we propose CalliRewrite: a coarse-to-fine approach\nfor robot arms to discover and recover plausible writing orders from diverse\ncalligraphy images without requiring labeled demonstrations. Our model achieves\nfine-grained control of various writing utensils. Specifically, an unsupervised\nimage-to-sequence model decomposes a given calligraphy glyph to obtain a coarse\nstroke sequence. Using an RL algorithm, a simulated brush is fine-tuned to\ngenerate stylized trajectories for robotic arm control. Evaluation in\nsimulation and physical robot scenarios reveals that our method successfully\nreplicates unseen fonts and styles while achieving integrity in unknown\ncharacters.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, accepted as ICRA 2024 contributed paper",
    "pdf_url": "http://arxiv.org/pdf/2405.15776v1",
    "published_date": "2024-03-20 08:12:02 UTC",
    "updated_date": "2024-03-20 08:12:02 UTC"
  },
  {
    "arxiv_id": "2403.13372v4",
    "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
    "authors": [
      "Yaowei Zheng",
      "Richong Zhang",
      "Junhao Zhang",
      "Yanhan Ye",
      "Zheyan Luo",
      "Zhangchi Feng",
      "Yongqiang Ma"
    ],
    "abstract": "Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and\n3,000 forks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, accepted to ACL 2024 System Demonstration Track",
    "pdf_url": "http://arxiv.org/pdf/2403.13372v4",
    "published_date": "2024-03-20 08:08:54 UTC",
    "updated_date": "2024-06-27 22:44:48 UTC"
  },
  {
    "arxiv_id": "2403.13369v2",
    "title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting",
    "authors": [
      "Phillip Richter-Pechanski",
      "Philipp Wiesenbach",
      "Dominic M. Schwab",
      "Christina Kiriakou",
      "Nicolas Geis",
      "Christoph Dieterich",
      "Anette Frank"
    ],
    "abstract": "Automatic extraction of medical information from clinical documents poses\nseveral challenges: high costs of required clinical expertise, limited\ninterpretability of model predictions, restricted computational resources and\nprivacy regulations. Recent advances in domain-adaptation and prompting methods\nshowed promising results with minimal training data using lightweight masked\nlanguage models, which are suited for well-established interpretability\nmethods. We are first to present a systematic evaluation of these methods in a\nlow-resource setting, by performing multi-class section classification on\nGerman doctor's letters. We conduct extensive class-wise evaluations supported\nby Shapley values, to validate the quality of our small training data set and\nto ensure the interpretability of model predictions. We demonstrate that a\nlightweight, domain-adapted pretrained model, prompted with just 20 shots,\noutperforms a traditional classification model by 30.5% accuracy. Our results\nserve as a process-oriented guideline for clinical information extraction\nprojects working with low-resource.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted for publication in the journal: Natural Language\n  Engineering (Cambridge Core)",
    "pdf_url": "http://arxiv.org/pdf/2403.13369v2",
    "published_date": "2024-03-20 08:01:33 UTC",
    "updated_date": "2024-08-13 07:35:31 UTC"
  },
  {
    "arxiv_id": "2403.13368v1",
    "title": "Computational Models to Study Language Processing in the Human Brain: A Survey",
    "authors": [
      "Shaonan Wang",
      "Jingyuan Sun",
      "Yunhao Zhang",
      "Nan Lin",
      "Marie-Francine Moens",
      "Chengqing Zong"
    ],
    "abstract": "Despite differing from the human language processing mechanism in\nimplementation and algorithms, current language models demonstrate remarkable\nhuman-like or surpassing language capabilities. Should computational language\nmodels be employed in studying the brain, and if so, when and how? To delve\ninto this topic, this paper reviews efforts in using computational models for\nbrain research, highlighting emerging trends. To ensure a fair comparison, the\npaper evaluates various computational models using consistent metrics on the\nsame dataset. Our analysis reveals that no single model outperforms others on\nall datasets, underscoring the need for rich testing datasets and rigid\nexperimental control to draw robust conclusions in studies involving\ncomputational models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13368v1",
    "published_date": "2024-03-20 08:01:22 UTC",
    "updated_date": "2024-03-20 08:01:22 UTC"
  },
  {
    "arxiv_id": "2403.13362v3",
    "title": "Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts",
    "authors": [
      "Hadi Askari",
      "Anshuman Chhabra",
      "Bernhard Clemm von Hohenberg",
      "Michael Heseltine",
      "Magdalena Wojcieszak"
    ],
    "abstract": "Polarization, declining trust, and wavering support for democratic norms are\npressing threats to U.S. democracy. Exposure to verified and quality news may\nlower individual susceptibility to these threats and make citizens more\nresilient to misinformation, populism, and hyperpartisan rhetoric. This project\nexamines how to enhance users' exposure to and engagement with verified and\nideologically balanced news in an ecologically valid setting. We rely on a\nlarge-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on\n28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users\ntweeting about sports, entertainment, or lifestyle with a contextual reply\ncontaining two hardcoded elements: a URL to the topic-relevant section of\nquality news organization and an encouragement to follow its Twitter account.\nTo further test differential effects by gender of the bots, treated users were\nrandomly assigned to receive responses by bots presented as female or male. We\nexamine whether our over-time intervention enhances the following of news media\norganization, the sharing and the liking of news content and the tweeting about\npolitics and the liking of political content. We find that the treated users\nfollowed more news accounts and the users in the female bot treatment were more\nlikely to like news content than the control. Most of these results, however,\nwere small in magnitude and confined to the already politically interested\nTwitter users, as indicated by their pre-treatment tweeting about politics.\nThese findings have implications for social media and news organizations, and\nalso offer direction for future work on how Large Language Models and other\ncomputational interventions can effectively enhance individual on-platform\nengagement with quality news and public affairs.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13362v3",
    "published_date": "2024-03-20 07:44:06 UTC",
    "updated_date": "2024-03-30 03:10:48 UTC"
  },
  {
    "arxiv_id": "2403.13355v1",
    "title": "BadEdit: Backdooring large language models by model editing",
    "authors": [
      "Yanzhou Li",
      "Tianlin Li",
      "Kangjie Chen",
      "Jian Zhang",
      "Shangqing Liu",
      "Wenhan Wang",
      "Tianwei Zhang",
      "Yang Liu"
    ],
    "abstract": "Mainstream backdoor attack methods typically demand substantial tuning data\nfor poisoning, limiting their practicality and potentially degrading the\noverall performance when applied to Large Language Models (LLMs). To address\nthese issues, for the first time, we formulate backdoor injection as a\nlightweight knowledge editing problem, and introduce the BadEdit attack\nframework. BadEdit directly alters LLM parameters to incorporate backdoors with\nan efficient editing technique. It boasts superiority over existing backdoor\ninjection techniques in several areas: (1) Practicality: BadEdit necessitates\nonly a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only\nadjusts a subset of parameters, leading to a dramatic reduction in time\nconsumption. (3) Minimal side effects: BadEdit ensures that the model's\noverarching performance remains uncompromised. (4) Robustness: the backdoor\nremains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently\nattack pre-trained LLMs with up to 100\\% success rate while maintaining the\nmodel's performance on benign inputs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13355v1",
    "published_date": "2024-03-20 07:34:18 UTC",
    "updated_date": "2024-03-20 07:34:18 UTC"
  },
  {
    "arxiv_id": "2403.13344v1",
    "title": "USE: Dynamic User Modeling with Stateful Sequence Models",
    "authors": [
      "Zhihan Zhou",
      "Qixiang Fang",
      "Leonardo Neves",
      "Francesco Barbieri",
      "Yozen Liu",
      "Han Liu",
      "Maarten W. Bos",
      "Ron Dotsch"
    ],
    "abstract": "User embeddings play a crucial role in user engagement forecasting and\npersonalized services. Recent advances in sequence modeling have sparked\ninterest in learning user embeddings from behavioral data. Yet behavior-based\nuser embedding learning faces the unique challenge of dynamic user modeling. As\nusers continuously interact with the apps, user embeddings should be\nperiodically updated to account for users' recent and long-term behavior\npatterns. Existing methods highly rely on stateless sequence models that lack\nmemory of historical behavior. They have to either discard historical data and\nuse only the most recent data or reprocess the old and new data jointly. Both\ncases incur substantial computational overhead. To address this limitation, we\nintroduce User Stateful Embedding (USE). USE generates user embeddings and\nreflects users' evolving behaviors without the need for exhaustive reprocessing\nby storing previous model states and revisiting them in the future.\nFurthermore, we introduce a novel training objective named future W-behavior\nprediction to transcend the limitations of next-token prediction by forecasting\na broader horizon of upcoming user behaviors. By combining it with the Same\nUser Prediction, a contrastive learning-based objective that predicts whether\ndifferent segments of behavior sequences belong to the same user, we further\nimprove the embeddings' distinctiveness and representativeness. We conducted\nexperiments on 8 downstream tasks using Snapchat users' behavioral logs in both\nstatic (i.e., fixed user behavior sequences) and dynamic (i.e., periodically\nupdated user behavior sequences) settings. We demonstrate USE's superior\nperformance over established baselines. The results underscore USE's\neffectiveness and efficiency in integrating historical and recent user behavior\nsequences into user embeddings in dynamic user modeling.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13344v1",
    "published_date": "2024-03-20 07:05:19 UTC",
    "updated_date": "2024-03-20 07:05:19 UTC"
  },
  {
    "arxiv_id": "2403.13341v2",
    "title": "FissionFusion: Fast Geometric Generation and Hierarchical Souping for Medical Image Analysis",
    "authors": [
      "Santosh Sanjeev",
      "Nuren Zhaksylyk",
      "Ibrahim Almakky",
      "Anees Ur Rehman Hashmi",
      "Mohammad Areeb Qazi",
      "Mohammad Yaqub"
    ],
    "abstract": "The scarcity of well-annotated medical datasets requires leveraging transfer\nlearning from broader datasets like ImageNet or pre-trained models like CLIP.\nModel soups averages multiple fine-tuned models aiming to improve performance\non In-Domain (ID) tasks and enhance robustness against Out-of-Distribution\n(OOD) datasets. However, applying these methods to the medical imaging domain\nfaces challenges and results in suboptimal performance. This is primarily due\nto differences in error surface characteristics that stem from data\ncomplexities such as heterogeneity, domain shift, class imbalance, and\ndistributional shifts between training and testing phases. To address this\nissue, we propose a hierarchical merging approach that involves local and\nglobal aggregation of models at various levels based on models' hyperparameter\nconfigurations. Furthermore, to alleviate the need for training a large number\nof models in the hyperparameter search, we introduce a computationally\nefficient method using a cyclical learning rate scheduler to produce multiple\nmodels for aggregation in the weight space. Our method demonstrates significant\nimprovements over the model souping approach across multiple datasets (around\n6% gain in HAM10000 and CheXpert datasets) while maintaining low computational\ncosts for model generation and selection. Moreover, we achieve better results\non OOD datasets than model soups. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/FissionFusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13341v2",
    "published_date": "2024-03-20 06:48:48 UTC",
    "updated_date": "2024-06-03 12:11:52 UTC"
  },
  {
    "arxiv_id": "2403.13337v1",
    "title": "Learning Novel View Synthesis from Heterogeneous Low-light Captures",
    "authors": [
      "Quan Zheng",
      "Hao Sun",
      "Huiyao Xu",
      "Fanjiang Xu"
    ],
    "abstract": "Neural radiance field has achieved fundamental success in novel view\nsynthesis from input views with the same brightness level captured under fixed\nnormal lighting. Unfortunately, synthesizing novel views remains to be a\nchallenge for input views with heterogeneous brightness level captured under\nlow-light condition. The condition is pretty common in the real world. It\ncauses low-contrast images where details are concealed in the darkness and\ncamera sensor noise significantly degrades the image quality. To tackle this\nproblem, we propose to learn to decompose illumination, reflectance, and noise\nfrom input views according to that reflectance remains invariant across\nheterogeneous views. To cope with heterogeneous brightness and noise levels\nacross multi-views, we learn an illumination embedding and optimize a noise map\nindividually for each view. To allow intuitive editing of the illumination, we\ndesign an illumination adjustment module to enable either brightening or\ndarkening of the illumination component. Comprehensive experiments demonstrate\nthat this approach enables effective intrinsic decomposition for low-light\nmulti-view noisy images and achieves superior visual quality and numerical\nperformance for synthesizing novel views compared to state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13337v1",
    "published_date": "2024-03-20 06:44:26 UTC",
    "updated_date": "2024-03-20 06:44:26 UTC"
  },
  {
    "arxiv_id": "2403.13335v1",
    "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
    "authors": [
      "Zhixin Lai",
      "Xuesheng Zhang",
      "Suiyao Chen"
    ],
    "abstract": "Large language models (LLMs) have reached human-like proficiency in\ngenerating diverse textual content, underscoring the necessity for effective\nfake text detection to avoid potential risks such as fake news in social media.\nPrevious research has mostly tested single models on in-distribution datasets,\nlimiting our understanding of how these models perform on different types of\ndata for LLM-generated text detection task. We researched this by testing five\nspecialized transformer-based models on both in-distribution and\nout-of-distribution datasets to better assess their performance and\ngeneralizability. Our results revealed that single transformer-based\nclassifiers achieved decent performance on in-distribution dataset but limited\ngeneralization ability on out-of-distribution dataset. To improve it, we\ncombined the individual classifiers models using adaptive ensemble algorithms,\nwhich improved the average accuracy significantly from 91.8% to 99.2% on an\nin-distribution test set and from 62.9% to 72.5% on an out-of-distribution test\nset. The results indicate the effectiveness, good generalization ability, and\ngreat potential of adaptive ensemble algorithms in LLM-generated text\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13335v1",
    "published_date": "2024-03-20 06:38:13 UTC",
    "updated_date": "2024-03-20 06:38:13 UTC"
  },
  {
    "arxiv_id": "2403.13334v2",
    "title": "Hyacinth6B: A large language model for Traditional Chinese",
    "authors": [
      "Chih-Wei Song",
      "Yin-Te Tsai"
    ],
    "abstract": "This research's primary motivation of this study is to address the high\nhardware and computational demands typically associated with LLMs.Therefore,our\ngoal is to find a balance between model lightness and performance,striving to\nmaximize performance while using a comparatively lightweight model. Hyacinth6B\nwas developed with this objective in mind,aiming to fully leverage the core\ncapabilities of LLMs without incurring substantial resource costs, effectively\npushing the boundaries of smaller model's performance. The training approach\ninvolves parameter efficient finetuning using the LoRA method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "14pages",
    "pdf_url": "http://arxiv.org/pdf/2403.13334v2",
    "published_date": "2024-03-20 06:37:59 UTC",
    "updated_date": "2024-03-26 12:24:46 UTC"
  },
  {
    "arxiv_id": "2403.13313v1",
    "title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare",
    "authors": [
      "Subhabrata Mukherjee",
      "Paul Gamble",
      "Markel Sanz Ausin",
      "Neel Kant",
      "Kriti Aggarwal",
      "Neha Manjunath",
      "Debajyoti Datta",
      "Zhengliang Liu",
      "Jiayuan Ding",
      "Sophia Busacca",
      "Cezanne Bianco",
      "Swapnil Sharma",
      "Rae Lasko",
      "Michelle Voisard",
      "Sanchay Harneja",
      "Darya Filippova",
      "Gerry Meixiong",
      "Kevin Cha",
      "Amir Youssefi",
      "Meyhaa Buvanesh",
      "Howard Weingram",
      "Sebastian Bierman-Lytle",
      "Harpreet Singh Mangat",
      "Kim Parikh",
      "Saad Godil",
      "Alex Miller"
    ],
    "abstract": "We develop Polaris, the first safety-focused LLM constellation for real-time\npatient-AI healthcare conversations. Unlike prior LLM works in healthcare\nfocusing on tasks like question answering, our work specifically focuses on\nlong multi-turn voice conversations. Our one-trillion parameter constellation\nsystem is composed of several multibillion parameter LLMs as co-operative\nagents: a stateful primary agent that focuses on driving an engaging\nconversation and several specialist support agents focused on healthcare tasks\nperformed by nurses to increase safety and reduce hallucinations. We develop a\nsophisticated training protocol for iterative co-training of the agents that\noptimize for diverse objectives. We train our models on proprietary data,\nclinical care plans, healthcare regulatory documents, medical manuals, and\nother medical reasoning documents. We align our models to speak like medical\nprofessionals, using organic healthcare conversations and simulated ones\nbetween patient actors and experienced nurses. This allows our system to\nexpress unique capabilities such as rapport building, trust building, empathy\nand bedside manner. Finally, we present the first comprehensive clinician\nevaluation of an LLM system for healthcare. We recruited over 1100 U.S.\nlicensed nurses and over 130 U.S. licensed physicians to perform end-to-end\nconversational evaluations of our system by posing as patients and rating the\nsystem on several measures. We demonstrate Polaris performs on par with human\nnurses on aggregate across dimensions such as medical safety, clinical\nreadiness, conversational quality, and bedside manner. Additionally, we conduct\na challenging task-based evaluation of the individual specialist support\nagents, where we demonstrate our LLM agents significantly outperform a much\nlarger general-purpose LLM (GPT-4) as well as from its own medium-size class\n(LLaMA-2 70B).",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13313v1",
    "published_date": "2024-03-20 05:34:03 UTC",
    "updated_date": "2024-03-20 05:34:03 UTC"
  },
  {
    "arxiv_id": "2403.13311v3",
    "title": "Multi-Robot Connected Fermat Spiral Coverage",
    "authors": [
      "Jingtao Tang",
      "Hang Ma"
    ],
    "abstract": "We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel\nalgorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts\nConnected Fermat Spiral (CFS) from the computer graphics community to\nmulti-robot coordination for the first time. MCFS uniquely enables the\norchestration of multiple robots to generate coverage paths that contour around\narbitrarily shaped obstacles, a feature that is notably lacking in traditional\nmethods. Our framework not only enhances area coverage and optimizes task\nperformance, particularly in terms of makespan, for workspaces rich in\nirregular obstacles but also addresses the challenges of path continuity and\ncurvature critical for non-holonomic robots by generating smooth paths without\ndecomposing the workspace. MCFS solves MCPP by constructing a graph of isolines\nand transforming MCPP into a combinatorial optimization problem, aiming to\nminimize the makespan while covering all vertices. Our contributions include\ndeveloping a unified CFS version for scalable and adaptable MCPP, extending it\nto MCPP with novel optimization techniques for cost reduction and path\ncontinuity and smoothness, and demonstrating through extensive experiments that\nMCFS outperforms existing MCPP methods in makespan, path curvature, coverage\nratio, and overlapping ratio. Our research marks a significant step in MCPP,\nshowcasing the fusion of computer graphics and automated planning principles to\nadvance the capabilities of multi-robot systems in complex environments. Our\ncode is available at https://github.com/reso1/MCFS.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "accepted to ICAPS24",
    "pdf_url": "http://arxiv.org/pdf/2403.13311v3",
    "published_date": "2024-03-20 05:23:24 UTC",
    "updated_date": "2024-04-16 15:35:50 UTC"
  },
  {
    "arxiv_id": "2405.07994v1",
    "title": "BubbleID: A Deep Learning Framework for Bubble Interface Dynamics Analysis",
    "authors": [
      "Christy Dunlap",
      "Changgen Li",
      "Hari Pandey",
      "Ngan Le",
      "Han Hu"
    ],
    "abstract": "This paper presents BubbleID, a sophisticated deep learning architecture\ndesigned to comprehensively identify both static and dynamic attributes of\nbubbles within sequences of boiling images. By amalgamating segmentation\npowered by Mask R-CNN with SORT-based tracking techniques, the framework is\ncapable of analyzing each bubble's location, dimensions, interface shape, and\nvelocity over its lifetime, and capturing dynamic events such as bubble\ndeparture. BubbleID is trained and tested on boiling images across diverse\nheater surfaces and operational settings. This paper also offers a comparative\nanalysis of bubble interface dynamics prior to and post-critical heat flux\n(CHF) conditions.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "16 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.07994v1",
    "published_date": "2024-03-20 05:17:43 UTC",
    "updated_date": "2024-03-20 05:17:43 UTC"
  },
  {
    "arxiv_id": "2403.13309v1",
    "title": "Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal",
    "authors": [
      "Rahul Pankajakshan",
      "Sumitra Biswal",
      "Yuvaraj Govindarajulu",
      "Gilad Gressel"
    ],
    "abstract": "The rapid integration of Large Language Models (LLMs) across diverse sectors\nhas marked a transformative era, showcasing remarkable capabilities in text\ngeneration and problem-solving tasks. However, this technological advancement\nis accompanied by significant risks and vulnerabilities. Despite ongoing\nsecurity enhancements, attackers persistently exploit these weaknesses, casting\ndoubts on the overall trustworthiness of LLMs. Compounding the issue,\norganisations are deploying LLM-integrated systems without understanding the\nseverity of potential consequences. Existing studies by OWASP and MITRE offer a\ngeneral overview of threats and vulnerabilities but lack a method for directly\nand succinctly analysing the risks for security practitioners, developers, and\nkey decision-makers who are working with this novel technology. To address this\ngap, we propose a risk assessment process using tools like the OWASP risk\nrating methodology which is used for traditional systems. We conduct scenario\nanalysis to identify potential threat agents and map the dependent system\ncomponents against vulnerability factors. Through this analysis, we assess the\nlikelihood of a cyberattack. Subsequently, we conduct a thorough impact\nanalysis to derive a comprehensive threat matrix. We also map threats against\nthree key stakeholder groups: developers engaged in model fine-tuning,\napplication developers utilizing third-party APIs, and end users. The proposed\nthreat matrix provides a holistic evaluation of LLM-related risks, enabling\nstakeholders to make informed decisions for effective mitigation strategies.\nOur outlined process serves as an actionable and comprehensive tool for\nsecurity practitioners, offering insights for resource management and enhancing\nthe overall system security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, 1 figure, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2403.13309v1",
    "published_date": "2024-03-20 05:17:22 UTC",
    "updated_date": "2024-03-20 05:17:22 UTC"
  },
  {
    "arxiv_id": "2403.13293v1",
    "title": "Building Optimal Neural Architectures using Interpretable Knowledge",
    "authors": [
      "Keith G. Mills",
      "Fred X. Han",
      "Mohammad Salameh",
      "Shengyao Lu",
      "Chunhua Zhou",
      "Jiao He",
      "Fengyu Sun",
      "Di Niu"
    ],
    "abstract": "Neural Architecture Search is a costly practice. The fact that a search space\ncan span a vast number of design choices with each architecture evaluation\ntaking nontrivial overhead makes it hard for an algorithm to sufficiently\nexplore candidate networks. In this paper, we propose AutoBuild, a scheme which\nlearns to align the latent embeddings of operations and architecture modules\nwith the ground-truth performance of the architectures they appear in. By doing\nso, AutoBuild is capable of assigning interpretable importance scores to\narchitecture modules, such as individual operation features and larger macro\noperation sequences such that high-performance neural networks can be\nconstructed without any need for search. Through experiments performed on\nstate-of-the-art image classification, segmentation, and Stable Diffusion\nmodels, we show that by mining a relatively small set of evaluated\narchitectures, AutoBuild can learn to build high-quality architectures directly\nor help to reduce search space to focus on relevant areas, finding better\narchitectures that outperform both the original labeled ones and ones found by\nsearch baselines. Code available at\nhttps://github.com/Ascend-Research/AutoBuild",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR'24; 18 Pages, 18 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2403.13293v1",
    "published_date": "2024-03-20 04:18:38 UTC",
    "updated_date": "2024-03-20 04:18:38 UTC"
  },
  {
    "arxiv_id": "2403.13269v3",
    "title": "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models",
    "authors": [
      "Zeyu Liu",
      "Souvik Kundu",
      "Anni Li",
      "Junrui Wan",
      "Lianghao Jiang",
      "Peter Anthony Beerel"
    ],
    "abstract": "We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as\nAdaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each\npre-trained frozen weight tensor, we add a parallel path of trainable low-rank\nmatrices, namely a down-projection and an up-projection matrix, each of which\nis followed by a feature transformation vector. Based on a novel freezing\nscore, we the incrementally freeze these projection matrices during fine-tuning\nto reduce the computation and alleviate over-fitting. Our experimental results\ndemonstrate that we can achieve state-of-the-art performance with an average\nimprovement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up\nto $9.5\\times$ fewer average trainable parameters. While compared in terms of\nruntime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar\nPEFT alternatives. Besides the practical utility of our approach, we provide\ninsights on the trainability requirements of LoRA paths at different modules\nand the freezing schedule for the different projection matrices. Code will be\nreleased.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13269v3",
    "published_date": "2024-03-20 03:07:50 UTC",
    "updated_date": "2024-04-16 17:37:12 UTC"
  },
  {
    "arxiv_id": "2403.13257v3",
    "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models",
    "authors": [
      "Charles Goddard",
      "Shamane Siriwardhana",
      "Malikeh Ehghaghi",
      "Luke Meyers",
      "Vlad Karpukhin",
      "Brian Benedict",
      "Mark McQuade",
      "Jacob Solawetz"
    ],
    "abstract": "The rapid expansion of the open-source language model landscape presents an\nopportunity to merge the competencies of these model checkpoints by combining\ntheir parameters. Advances in transfer learning, the process of fine-tuning\npretrained models for specific tasks, has resulted in the development of vast\namounts of task-specific models, typically specialized in individual tasks and\nunable to utilize each other's strengths. Model merging facilitates the\ncreation of multitask models without the need for additional training, offering\na promising avenue for enhancing model performance and versatility. By\npreserving the intrinsic capabilities of the original models, model merging\naddresses complex challenges in AI - including the difficulties of catastrophic\nforgetting and multitask learning. To support this expanding area of research,\nwe introduce MergeKit, a comprehensive, open-source library designed to\nfacilitate the application of model merging strategies. MergeKit offers an\nextensible framework to efficiently merge models on any hardware, providing\nutility to researchers and practitioners. To date, thousands of models have\nbeen merged by the open-source community, leading to the creation of some of\nthe worlds most powerful open-source model checkpoints, as assessed by the Open\nLLM Leaderboard. The library is accessible at\nhttps://github.com/arcee-ai/MergeKit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13257v3",
    "published_date": "2024-03-20 02:38:01 UTC",
    "updated_date": "2025-01-09 22:21:56 UTC"
  },
  {
    "arxiv_id": "2403.13249v1",
    "title": "A Unified and General Framework for Continual Learning",
    "authors": [
      "Zhenyi Wang",
      "Yan Li",
      "Li Shen",
      "Heng Huang"
    ],
    "abstract": "Continual Learning (CL) focuses on learning from dynamic and changing data\ndistributions while retaining previously acquired knowledge. Various methods\nhave been developed to address the challenge of catastrophic forgetting,\nincluding regularization-based, Bayesian-based, and memory-replay-based\ntechniques. However, these methods lack a unified framework and common\nterminology for describing their approaches. This research aims to bridge this\ngap by introducing a comprehensive and overarching framework that encompasses\nand reconciles these existing methodologies. Notably, this new framework is\ncapable of encompassing established CL approaches as special instances within a\nunified and general optimization objective. An intriguing finding is that\ndespite their diverse origins, these methods share common mathematical\nstructures. This observation highlights the compatibility of these seemingly\ndistinct techniques, revealing their interconnectedness through a shared\nunderlying optimization objective. Moreover, the proposed general framework\nintroduces an innovative concept called refresh learning, specifically designed\nto enhance the CL performance. This novel approach draws inspiration from\nneuroscience, where the human brain often sheds outdated information to improve\nthe retention of crucial knowledge and facilitate the acquisition of new\ninformation. In essence, refresh learning operates by initially unlearning\ncurrent data and subsequently relearning it. It serves as a versatile plug-in\nthat seamlessly integrates with existing CL methods, offering an adaptable and\neffective enhancement to the learning process. Extensive experiments on CL\nbenchmarks and theoretical analysis demonstrate the effectiveness of the\nproposed refresh learning. Code is available at\n\\url{https://github.com/joey-wang123/CL-refresh-learning}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2403.13249v1",
    "published_date": "2024-03-20 02:21:44 UTC",
    "updated_date": "2024-03-20 02:21:44 UTC"
  },
  {
    "arxiv_id": "2403.13245v2",
    "title": "Federated reinforcement learning for robot motion planning with zero-shot generalization",
    "authors": [
      "Zhenyuan Yuan",
      "Siyuan Xu",
      "Minghui Zhu"
    ],
    "abstract": "This paper considers the problem of learning a control policy for robot\nmotion planning with zero-shot generalization, i.e., no data collection and\npolicy adaptation is needed when the learned policy is deployed in new\nenvironments. We develop a federated reinforcement learning framework that\nenables collaborative learning of multiple learners and a central server, i.e.,\nthe Cloud, without sharing their raw data. In each iteration, each learner\nuploads its local control policy and the corresponding estimated normalized\narrival time to the Cloud, which then computes the global optimum among the\nlearners and broadcasts the optimal policy to the learners. Each learner then\nselects between its local control policy and that from the Cloud for next\niteration. The proposed framework leverages on the derived zero-shot\ngeneralization guarantees on arrival time and safety. Theoretical guarantees on\nalmost-sure convergence, almost consensus, Pareto improvement and optimality\ngap are also provided. Monte Carlo simulation is conducted to evaluate the\nproposed framework.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13245v2",
    "published_date": "2024-03-20 02:16:54 UTC",
    "updated_date": "2024-04-07 19:25:47 UTC"
  },
  {
    "arxiv_id": "2403.13244v4",
    "title": "Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model",
    "authors": [
      "Peng Zhou",
      "Jianmin Wang",
      "Chunyan Li",
      "Zixu Wang",
      "Yiping Liu",
      "Siqi Sun",
      "Jianxin Lin",
      "Leyi Wei",
      "Xibao Cai",
      "Houtim Lai",
      "Wei Liu",
      "Longyue Wang",
      "Yuansheng Liu",
      "Xiangxiang Zeng"
    ],
    "abstract": "While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 82.58%, 68.03%, and\n67.48%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2403.13244v4",
    "published_date": "2024-03-20 02:15:55 UTC",
    "updated_date": "2024-10-10 04:20:55 UTC"
  },
  {
    "arxiv_id": "2403.13236v1",
    "title": "Safety-Aware Reinforcement Learning for Electric Vehicle Charging Station Management in Distribution Network",
    "authors": [
      "Jiarong Fan",
      "Ariel Liebman",
      "Hao Wang"
    ],
    "abstract": "The increasing integration of electric vehicles (EVs) into the grid can pose\na significant risk to the distribution system operation in the absence of\ncoordination. In response to the need for effective coordination of EVs within\nthe distribution network, this paper presents a safety-aware reinforcement\nlearning (RL) algorithm designed to manage EV charging stations while ensuring\nthe satisfaction of system constraints. Unlike existing methods, our proposed\nalgorithm does not rely on explicit penalties for constraint violations,\neliminating the need for penalty coefficient tuning. Furthermore, managing EV\ncharging stations is further complicated by multiple uncertainties, notably the\nvariability in solar energy generation and energy prices. To address this\nchallenge, we develop an off-policy RL algorithm to efficiently utilize data to\nlearn patterns in such uncertain environments. Our algorithm also incorporates\na maximum entropy framework to enhance the RL algorithm's exploratory process,\npreventing convergence to local optimal solutions. Simulation results\ndemonstrate that our algorithm outperforms traditional RL algorithms in\nmanaging EV charging in the distribution network.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "2024 IEEE Power & Energy Society General Meeting (PESGM)",
    "pdf_url": "http://arxiv.org/pdf/2403.13236v1",
    "published_date": "2024-03-20 01:57:38 UTC",
    "updated_date": "2024-03-20 01:57:38 UTC"
  },
  {
    "arxiv_id": "2403.13218v1",
    "title": "Self-Attention Based Semantic Decomposition in Vector Symbolic Architectures",
    "authors": [
      "Calvin Yeung",
      "Prathyush Poduval",
      "Mohsen Imani"
    ],
    "abstract": "Vector Symbolic Architectures (VSAs) have emerged as a novel framework for\nenabling interpretable machine learning algorithms equipped with the ability to\nreason and explain their decision processes. The basic idea is to represent\ndiscrete information through high dimensional random vectors. Complex data\nstructures can be built up with operations over vectors such as the \"binding\"\noperation involving element-wise vector multiplication, which associates data\ntogether. The reverse task of decomposing the associated elements is a\ncombinatorially hard task, with an exponentially large search space. The main\nalgorithm for performing this search is the resonator network, inspired by\nHopfield network-based memory search operations.\n  In this work, we introduce a new variant of the resonator network, based on\nself-attention based update rules in the iterative search problem. This update\nrule, based on the Hopfield network with log-sum-exp energy function and\nnorm-bounded states, is shown to substantially improve the performance and rate\nof convergence. As a result, our algorithm enables a larger capacity for\nassociative memory, enabling applications in many tasks like perception based\npattern recognition, scene decomposition, and object reasoning. We substantiate\nour algorithm with a thorough evaluation and comparisons to baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2403.13218v1",
    "published_date": "2024-03-20 00:37:19 UTC",
    "updated_date": "2024-03-20 00:37:19 UTC"
  },
  {
    "arxiv_id": "2403.13214v2",
    "title": "Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy",
    "authors": [
      "Austin E. Y. T. Lefebvre",
      "Gabriel Sturm",
      "Ting-Yu Lin",
      "Emily Stoops",
      "Magdalena Preciado Lopez",
      "Benjamin Kaufmann-Malaga",
      "Kayley Hake"
    ],
    "abstract": "The analysis of dynamic organelles remains a formidable challenge, though key\nto understanding biological processes. We introduce Nellie, an automated and\nunbiased user-friendly pipeline for segmentation, tracking, and feature\nextraction of diverse intracellular structures. Nellie adapts to image\nmetadata, eliminating user input. Nellie's preprocessing pipeline enhances\nstructural contrast on multiple intracellular scales allowing for robust\nhierarchical segmentation of sub-organellar regions. Internal motion capture\nmarkers are generated and tracked via a radius-adaptive pattern matching\nscheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a\nplethora of features at multiple hierarchical levels for deep and customizable\nanalysis. Nellie features a point-and-click Napari-based GUI that allows for\ncode-free operation and visualization, while its modular open-source codebase\ninvites extension by experienced users. We demonstrate Nellie's wide variety of\nuse cases with three examples: unmixing multiple organelles from a single\nchannel using feature-based classification, training an unsupervised graph\nautoencoder on mitochondrial multi-mesh graphs to quantify latent space\nembedding changes following ionomycin treatment, and performing in-depth\ncharacterization and comparison of endoplasmic reticulum networks across\ndifferent cell types and temporal frames.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV",
    "comment": "134 pages, 6 main figures, 6 extended figures, 8 supplementary\n  figures; for associated code, see https://github.com/aelefebv/nellie",
    "pdf_url": "http://arxiv.org/pdf/2403.13214v2",
    "published_date": "2024-03-20 00:23:42 UTC",
    "updated_date": "2024-10-15 00:12:48 UTC"
  }
]
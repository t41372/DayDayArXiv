{
  "date": "2025-03-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-04 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 142 篇论文，主要聚焦 AI 模型优化（如 LLM 的偏见和鲁棒性）、强化学习在机器人和游戏中的应用、医疗图像处理与诊断，以及计算机视觉技术创新；重点包括 LLM 在数学推理和多模态任务中的进展，以及知名学者如 Philip S. Yu 的贡献；令人印象深刻的文章有那些探索 LLM 代理和高效生成框架的，如 \"Scaling Laws for Many-Shot In-Context Learning\"。\n\n下面，我将逐一简要概述部分关键论文，先优先选取重要、创新性和话题度高的文章（如涉及 LLM、强化学习和医疗 AI 的），并将相关主题归类讨论。对于其他较常规或技术细节较多的论文（如一些纯优化算法或特定数据集），我将快速掠过，只提核心贡献。每个论文标题以“中文标题 + 英文标题”形式呈现，保留核心学术术语。\n\n### LLM 和 AI 模型优化\n这些论文探讨了 LLM 的偏见、鲁棒性和应用，相关主题紧密，是本日亮点。\n- **缩放定律：多样本上下文学习中的自生成标注** + Scaling Laws for Many-Shot In-Context Learning：作者包括 Philip S. Yu，该论文提出了一种基于自生成标注的框架，改进了零样本和多样本设置下的 ICL 性能，发现使用超过 1000 个演示样本可实现最佳效果，并在分类任务中提升 6.8%。\n- **SAGE：状态-动作增强用于对话生成引导** + SAGE: Steering and Refining Dialog Generation with State-Action Augmentation：Yizhe Zhang 和 Navdeep Jaitly 合作，引入潜在变量控制对话行为，通过自提升管道优化情感智能，提升 LLM 在对话任务中的性能。\n- **LLM 失调：通过对抗 RLHF 平台** + LLM Misalignment via Adversarial RLHF Platforms：该研究揭示 RLHF 平台的漏洞，通过操纵偏好数据集导致 LLM 行为偏移，强调了安全性和可信性的需求。\n- **PromptCoT：用于 LLM 数学推理的奥林匹克级问题合成** + PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning：提出自动生成高难度数学问题的框架，使用结构化 CoT 训练，提升 LLM 在复杂推理任务中的表现。\n- **IterPref：通过迭代调试的焦点偏好学习提升 LLM 数学推理** + IterPref: Focal Preference Learning for Code Generation via Iterative Debugging：贡献在于使用迭代调试生成高质量偏好数据，显著提高 LLM 在代码生成中的准确性。\n- **Prompting Science Report 1：提示工程的复杂性和偶然性** + Prompting Science Report 1: Prompt Engineering is Complicated and Contingent：作者包括 Ethan Mollick，分析 LLM 基准测试的变异性，发现礼貌提示有时提升性能，有时降低，强调提示策略的非通用性。\n- **AlignDistil：通过自适应策略蒸馏的令牌级 LLM 对齐** + AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation：提出令牌级 DPO 算法，提升 LLM 对齐效率，减少全局优化错误。\n- **Implicit Bias in LLMs：一个调查** + Implicit Bias in LLMs: A Survey：系统回顾 LLM 中的隐式偏见，分类检测方法并讨论缓解策略。\n- 其他如 **Effectively Steer LLM To Follow Preference** + Effectively Steer LLM To Follow Preference 和 **Self-Evolved Preference Optimization** + Self-Evolved Preference Optimization 等，快速提炼：这些工作通过偏好优化和置信度建模，提升 LLM 在特定任务（如数学和对话）的鲁棒性，但整体进展仍需更多实验验证。\n\n### 强化学习和机器人\n这些论文强调强化学习在实际应用中的创新，相关性强。\n- **ArticuBot：通过大规模模拟学习通用铰接物体操作策略** + ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation：作者包括 David Held，提出零样本 sim2real 转移框架，使用分层策略在真实机器人上操作多样铰接物体，提升泛化能力。\n- **RAILGUN：用于多代理路径寻找的统一卷积策略** + RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding：Yimin Tang 和 Sven Koenig 合作，开发首个中心化学习策略，适用于不同环境和任务，提高路径规划效率。\n- **Reactive Diffusion Policy：慢快视觉-触觉策略学习用于接触丰富操作** + Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation：贡献在于分层策略融合视觉和触觉，提升机器人对外部变化的响应速度。\n- 其他如 **Multi-Agent System for AI-Assisted Extraction** + Multi-Agent System for AI-Assisted Extraction 等，快速掠过：这些探索多代理协调，但细节较琐碎。\n\n### 医疗和生物图像处理\n医疗 AI 论文有实际影响，值得关注。\n- **Passive Heart Rate Monitoring During Smartphone Use** + Passive Heart Rate Monitoring During Smartphone Use：作者包括 Shwetak Patel，开发 PHRM 系统，使用面部视频光电容积描记法实现无穿戴设备的心率监测，准确率低于 10% 的 MAPE。\n- **MoSE：分层自蒸馏增强早期层嵌入** + MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings：提出自蒸馏机制，提升代码检索性能，支持灵活部署。\n- **Multimodal Deep Learning for Subtype Classification** + Multimodal Deep Learning for Subtype Classification：使用 ResNet-50 和注意机制融合组织图像和基因数据，提高乳腺癌亚型分类准确性。\n- 其他如 **Teaching AI to Handle Exceptions** + Teaching AI to Handle Exceptions 等，快速提炼：这些工作改善 AI 在医疗决策中的鲁棒性，但实验规模有限。\n\n### 计算机视觉和图像生成\n这些论文创新性高，但部分较技术化，快速概述。\n- **InfiniSST：无界语音的同步翻译** + InfiniSST: Simultaneous Translation of Unbounded Speech：提出多轮对话式框架，提升语音翻译效率，减少延迟。\n- **KodCode：用于编码的合成数据集** + KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset：构建可验证数据集，提升 LLM 在代码生成的性能。\n- **LiteWebAgent：基于 VLM 的网络代理套件** + LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent：提供开源框架，支持网络代理的规划和记忆。\n- 其他如 **Diverse Controllable Diffusion Policy** + Diverse Controllable Diffusion Policy 等，快速掠过：这些优化生成模型，但贡献较局部。\n\n其他论文，如纯理论优化（如 **Accelerating Focal Search** + Accelerating Focal Search）或特定数据集（如 **Deepfake-Eval-2024** + Deepfake-Eval-2024），虽有贡献（如改进搜索算法或基准测试），但不为核心亮点，故仅简要提及：它们提升了特定领域的效率和鲁棒性，但实际影响需进一步验证。\n\n总之，今天的论文突显 AI 领域的快速迭代，LLM 和强化学习的创新尤为突出，期待这些进展推动更可靠的 AI 应用！如果您对特定主题感兴趣，建议查看相关论文的摘要。",
  "papers": [
    {
      "arxiv_id": "2503.03062v2",
      "title": "Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyao Gu",
        "Henry Peng Zou",
        "Yankai Chen",
        "Aiwei Liu",
        "Weizhi Zhang",
        "Philip S. Yu"
      ],
      "abstract": "The high cost of obtaining high-quality annotated data for in-context\nlearning (ICL) has motivated the development of methods that use self-generated\nannotations in place of ground-truth labels. While these approaches have shown\npromising results in few-shot settings, they generally do not scale to\nmany-shot scenarios. In this work, we study ICL with self-generated examples\nusing a framework analogous to traditional semi-supervised learning, consisting\nof annotation generation, demonstration selection, and in-context inference.\nWithin this framework, we propose a simple baseline that outperforms\nground-truth ICL in zero-shot, few-shot, and many-shot settings. Notably, we\nobserve a scaling law with this baseline, where optimal performance is achieved\nwith more than 1,000 demonstrations. To fully exploit the many-shot\ncapabilities of semi-supervised ICL, we introduce IterPSD, an iterative\nannotation approach that integrates iterative refinement and curriculum\npseudo-labeling techniques from semi-supervised learning, yielding up to 6.8%\nadditional gains on classification tasks.",
      "tldr_zh": "本研究探讨了使用自生成标注(Self-Generated Annotations)进行多样本In-Context Learning (ICL)的扩展规律，针对传统标注数据成本高的问题，提出一个类似于半监督学习的框架，包括标注生成、演示选择和推理过程。研究中，一个简单基线方法在zero-shot、few-shot和many-shot设置下优于基于真实标注的ICL，并观察到scaling law，即性能最佳时需要超过1,000个演示。进一步，作者引入IterPSD迭代标注方法，结合迭代精炼和课程伪标签技术，在分类任务上实现高达6.8%的额外提升。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03062v2",
      "published_date": "2025-03-04 23:52:49 UTC",
      "updated_date": "2025-05-20 22:36:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:37:12.872169"
    },
    {
      "arxiv_id": "2503.03783v3",
      "title": "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
      "title_zh": "在日常生活中使用智能手机时的被动心率监测",
      "authors": [
        "Shun Liao",
        "Paolo Di Achille",
        "Jiang Wu",
        "Silviu Borac",
        "Jonathan Wang",
        "Xin Liu",
        "Eric Teasley",
        "Lawrence Cai",
        "Yuzhe Yang",
        "Yun Liu",
        "Daniel McDuff",
        "Hao-Wei Su",
        "Brent Winslow",
        "Anupam Pathak",
        "Shwetak Patel",
        "James A. Taylor",
        "Jameson K. Rogers",
        "Ming-Zher Poh"
      ],
      "abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
      "tldr_zh": "本文提出 PHRM 系统，一种基于深度学习的框架，利用 facial video-based photoplethysmography 从日常智能手机视频中实现被动心率 (HR) 和静息心率 (RHR) 监测，无需可穿戴设备。系统使用 225,773 个视频从 495 名参与者开发，并通过 185,970 个视频从 205 名参与者验证，在实验室和自由生活条件下，与 electrocardiogram 比较，HR 测量的均绝对百分比误差 (MAPE) < 10%，且在浅、中、深皮肤色调组间表现非劣效。结果显示，PHRM 测量的每日 RHR 平均绝对误差 < 5 bpm，与可穿戴设备一致，并与已知风险因素相关联。这些发现突显了智能手机在被动和公平心健康监测方面的潜力。",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "q-bio.TO",
      "comment": "Updated author list",
      "pdf_url": "http://arxiv.org/pdf/2503.03783v3",
      "published_date": "2025-03-04 23:28:10 UTC",
      "updated_date": "2025-03-21 20:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:37:26.939165"
    },
    {
      "arxiv_id": "2503.03045v2",
      "title": "ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation",
      "title_zh": "ArticuBot：通过大规模模拟学习通用铰接物体操作策略",
      "authors": [
        "Yufei Wang",
        "Ziyu Wang",
        "Mino Nakura",
        "Pratik Bhowal",
        "Chia-Liang Kuo",
        "Yi-Ting Chen",
        "Zackory Erickson",
        "David Held"
      ],
      "abstract": "This paper presents ArticuBot, in which a single learned policy enables a\nrobotics system to open diverse categories of unseen articulated objects in the\nreal world. This task has long been challenging for robotics due to the large\nvariations in the geometry, size, and articulation types of such objects. Our\nsystem, Articubot, consists of three parts: generating a large number of\ndemonstrations in physics-based simulation, distilling all generated\ndemonstrations into a point cloud-based neural policy via imitation learning,\nand performing zero-shot sim2real transfer to real robotics systems. Utilizing\nsampling-based grasping and motion planning, our demonstration generalization\npipeline is fast and effective, generating a total of 42.3k demonstrations over\n322 training articulated objects. For policy learning, we propose a novel\nhierarchical policy representation, in which the high-level policy learns the\nsub-goal for the end-effector, and the low-level policy learns how to move the\nend-effector conditioned on the predicted goal. We demonstrate that this\nhierarchical approach achieves much better object-level generalization compared\nto the non-hierarchical version. We further propose a novel weighted\ndisplacement model for the high-level policy that grounds the prediction into\nthe existing 3D structure of the scene, outperforming alternative policy\nrepresentations. We show that our learned policy can zero-shot transfer to\nthree different real robot settings: a fixed table-top Franka arm across two\ndifferent labs, and an X-Arm on a mobile base, opening multiple unseen\narticulated objects across two labs, real lounges, and kitchens. Videos and\ncode can be found on our project website: https://articubot.github.io/.",
      "tldr_zh": "这篇论文介绍了 ArticuBot 系统，通过大规模模拟学习一个通用的铰接物体操作策略，旨在让机器人零样本处理各种几何、尺寸和铰接类型不同的 unseen articulated objects。系统包括三个部分：基于物理模拟生成42.3k个演示、使用模仿学习提炼基于点云的分层神经策略，以及实现 sim2real transfer。分层策略采用高层政策预测端执行器子目标（如加权位移模型）和低层政策执行运动，这比非分层方法显著提高了物体级泛化能力。实验验证了该策略在三个真实机器人设置中（如Franka臂和X-Arm移动基座）成功打开多种未见过的 articulated objects，为机器人操作提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted at RSS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.03045v2",
      "published_date": "2025-03-04 22:51:50 UTC",
      "updated_date": "2025-05-01 21:26:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:37:38.414138"
    },
    {
      "arxiv_id": "2503.03040v1",
      "title": "SAGE: Steering and Refining Dialog Generation with State-Action Augmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Yizhe Zhang",
        "Navdeep Jaitly"
      ],
      "abstract": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel.",
      "tldr_zh": "本研究提出SAGE方法，利用潜在变量控制对话生成的长期行为，以构建情感智能聊天机器人。核心是State-Action Chain (SAC)，通过引入封装情感状态和对话策略的潜在变量，在推理过程中生成这些变量，实现对对话进程的粗粒度控制，同时保持自然互动。研究还引入自提升管道，包括对话树搜索、LLM-based奖励建模和针对性微调。实验结果表明，SAGE在情感智能指标上表现出色，同时维持LLM基准性能，并为对话系统的强化学习应用奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03040v1",
      "published_date": "2025-03-04 22:45:24 UTC",
      "updated_date": "2025-03-04 22:45:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:37:48.937985"
    },
    {
      "arxiv_id": "2503.03039v1",
      "title": "LLM Misalignment via Adversarial RLHF Platforms",
      "title_zh": "翻译失败",
      "authors": [
        "Erfan Entezami",
        "Ali Naseh"
      ],
      "abstract": "Reinforcement learning has shown remarkable performance in aligning language\nmodels with human preferences, leading to the rise of attention towards\ndeveloping RLHF platforms. These platforms enable users to fine-tune models\nwithout requiring any expertise in developing complex machine learning\nalgorithms. While these platforms offer useful features such as reward modeling\nand RLHF fine-tuning, their security and reliability remain largely unexplored.\nGiven the growing adoption of RLHF and open-source RLHF frameworks, we\ninvestigate the trustworthiness of these systems and their potential impact on\nbehavior of LLMs. In this paper, we present an attack targeting publicly\navailable RLHF tools. In our proposed attack, an adversarial RLHF platform\ncorrupts the LLM alignment process by selectively manipulating data samples in\nthe preference dataset. In this scenario, when a user's task aligns with the\nattacker's objective, the platform manipulates a subset of the preference\ndataset that contains samples related to the attacker's target. This\nmanipulation results in a corrupted reward model, which ultimately leads to the\nmisalignment of the language model. Our results demonstrate that such an attack\ncan effectively steer LLMs toward undesirable behaviors within the targeted\ndomains. Our work highlights the critical need to explore the vulnerabilities\nof RLHF platforms and their potential to cause misalignment in LLMs during the\nRLHF fine-tuning process.",
      "tldr_zh": "本文研究了强化学习（Reinforcement Learning）在对齐语言模型（LLMs）时所依赖的 RLHF 平台的安全性问题，提出了一种针对公开 RLHF 工具的攻击方法。攻击通过敌对 RLHF 平台选择性地操纵偏好数据集中的样本，当用户任务与攻击者目标一致时，破坏奖励模型（Reward Model），最终导致 LLMs 失调。实验结果表明，这种攻击能有效引导 LLMs 在目标领域表现出不期望的行为。该工作强调了 RLHF 平台的潜在漏洞，并呼吁进一步探索以提升其可信度。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03039v1",
      "published_date": "2025-03-04 22:38:54 UTC",
      "updated_date": "2025-03-04 22:38:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:38:00.906999"
    },
    {
      "arxiv_id": "2503.04819v1",
      "title": "Technique Inference Engine: A Recommender Model to Support Cyber Threat Hunting",
      "title_zh": "技术推理引擎：一种支持网络威胁狩猎的推荐模型",
      "authors": [
        "Matthew J. Turner",
        "Mike Carenzo",
        "Jackie Lasky",
        "James Morris-King",
        "James Ross"
      ],
      "abstract": "Cyber threat hunting is the practice of proactively searching for latent\nthreats in a network. Engaging in threat hunting can be difficult due to the\nvolume of network traffic, variety of adversary techniques, and constantly\nevolving vulnerabilities. To aid analysts in identifying techniques which may\nbe co-occurring as part of a campaign, we present the Technique Inference\nEngine, a tool to infer tactics, techniques, and procedures (TTPs) which may be\nrelated to existing observations of adversarial behavior. We compile the\nlargest (to our knowledge) available dataset of cyber threat intelligence (CTI)\nreports labeled with relevant TTPs. With the knowledge that techniques are\nchronically under-reported in CTI, we apply several implicit feedback\nrecommender models to the data in order to predict additional techniques which\nmay be part of a given campaign. We evaluate the results in the context of the\ncyber analyst's use case and apply t-SNE to visualize the model embeddings. We\nprovide our code and a web interface.",
      "tldr_zh": "该研究引入了Technique Inference Engine，一种推荐模型，用于辅助网络威胁狩猎，通过推断与现有对手行为相关的战术、技术和程序（TTPs），帮助分析师识别潜在的关联威胁。该模型基于一个最大的（据称）标记有相关TTPs的网络威胁情报（CTI）报告数据集，并应用隐式反馈推荐模型来预测可能未报告的技术，从而处理网络流量大和技术多样性的挑战。实验评估显示模型在分析师用例中有效，并通过t-SNE可视化嵌入结果；研究还提供了开源代码和网络界面，以支持实际应用。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04819v1",
      "published_date": "2025-03-04 22:31:43 UTC",
      "updated_date": "2025-03-04 22:31:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:38:12.681352"
    },
    {
      "arxiv_id": "2503.04818v1",
      "title": "Prompting Science Report 1: Prompt Engineering is Complicated and Contingent",
      "title_zh": "翻译失败",
      "authors": [
        "Lennart Meincke",
        "Ethan Mollick",
        "Lilach Mollick",
        "Dan Shapiro"
      ],
      "abstract": "This is the first of a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we demonstrate two things:\n  - There is no single standard for measuring whether a Large Language Model\n(LLM) passes a benchmark, and that choosing a standard has a big impact on how\nwell the LLM does on that benchmark. The standard you choose will depend on\nyour goals for using an LLM in a particular case.\n  - It is hard to know in advance whether a particular prompting approach will\nhelp or harm the LLM's ability to answer any particular question. Specifically,\nwe find that sometimes being polite to the LLM helps performance, and sometimes\nit lowers performance. We also find that constraining the AI's answers helps\nperformance in some cases, though it may lower performance in other cases.\n  Taken together, this suggests that benchmarking AI performance is not\none-size-fits-all, and also that particular prompting formulas or approaches,\nlike being polite to the AI, are not universally valuable.",
      "tldr_zh": "本报告是“Prompting Science Report”系列的第一篇，旨在通过严格测试帮助商业、教育和政策领导者理解 AI 技术的细节，强调 Prompt Engineering 的复杂性和条件性。主要发现是，没有单一标准来衡量 Large Language Model (LLM) 在基准测试中的表现，选择标准会极大影响结果，从而取决于具体应用目标。其次，特定提示方法（如礼貌表达或约束答案）的影响不确定：有时有助于提升 LLM 的回答性能，有时则会降低。总体而言，这表明 AI 性能基准并非一刀切，提示策略需要根据情境量身定制。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04818v1",
      "published_date": "2025-03-04 21:09:12 UTC",
      "updated_date": "2025-03-04 21:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:38:24.465763"
    },
    {
      "arxiv_id": "2503.03008v2",
      "title": "MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings",
      "title_zh": "MoSE：分层自蒸馏增强早期层嵌入",
      "authors": [
        "Andrea Gurioli",
        "Federico Pennino",
        "João Monteiro",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Deploying language models often requires navigating accuracy vs. performance\ntrade-offs to meet latency constraints while preserving utility. Traditional\nmodel distillation reduces size but incurs substantial costs through training\nseparate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter\nmulti-exit encoder for code retrieval and classification that employs a novel\nSelf-Distillation mechanism. This approach significantly enhances lower-layer\nrepresentations, enabling flexible deployment of different model portions with\nfavorable performance trade-offs. Our architecture improves text-to-code and\ncode-to-code search by targeting specific encoder layers as exit heads, where\nhigher layers guide earlier ones during training-improving intermediate\nrepresentations at minimal additional cost. We further enhance MoSE with a\nrepository-level contextual loss that maximizes training context window\nutilization. Additionally, we release a new dataset created through code\ntranslation that extends text-to-code benchmarks with cross-language\ncode-to-code pairs. Evaluations demonstrate the effectiveness of\nSelf-Distillation as a principled approach to trading inference cost for\naccuracy across various code understanding tasks.",
      "tldr_zh": "该论文提出 MoSE，一种基于 Hierarchical Self-Distillation 的 1 亿参数多出口编码器，用于代码检索和分类任务，通过让高层嵌入指导早期层嵌入来显著提升中间表示的性能，从而实现准确率与推理成本的灵活权衡。MoSE 还引入了 repository-level contextual loss 来最大化训练上下文窗口利用，并发布了一个新数据集，扩展了文本到代码和跨语言代码对的基准。实验结果证明，这种 Self-Distillation 方法在各种代码理解任务中有效降低了部署成本，同时保持了较高的准确性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03008v2",
      "published_date": "2025-03-04 21:08:17 UTC",
      "updated_date": "2025-05-19 13:39:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:38:36.817846"
    },
    {
      "arxiv_id": "2503.03779v1",
      "title": "Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds",
      "title_zh": "通过更紧的下界",
      "authors": [
        "Yimin Tang",
        "Zhenghong Yu",
        "Jiaoyang Li",
        "Sven Koenig"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) involves finding collision-free paths for\nmultiple agents while minimizing a cost function--an NP-hard problem. Bounded\nsuboptimal methods like Enhanced Conflict-Based Search (ECBS) and Explicit\nEstimation CBS (EECBS) balance solution quality with computational efficiency\nusing focal search mechanisms. While effective, traditional focal search faces\na limitation: the lower bound (LB) value determining which nodes enter the\nFOCAL list often increases slowly in early search stages, resulting in a\nconstrained search space that delays finding valid solutions. In this paper, we\npropose a novel bounded suboptimal algorithm, double-ECBS (DECBS), to address\nthis issue by first determining the maximum LB value and then employing a\nbest-first search guided by this LB to find a collision-free path. Experimental\nresults demonstrate that DECBS outperforms ECBS in most test cases and is\ncompatible with existing optimization techniques. DECBS can reduce nearly 30%\nhigh-level CT nodes and 50% low-level focal search nodes. When agent density is\nmoderate to high, DECBS achieves a 23.5% average runtime improvement over ECBS\nwith identical suboptimality bounds and optimizations.",
      "tldr_zh": "本研究针对多智能体路径寻找（Multi-Agent Path Finding, MAPF）问题中的focal search机制，解决了传统方法如ECBS和EECBS中lower bound (LB)值在早期阶段增加缓慢导致的搜索效率低下问题。提出了一种新算法double-ECBS (DECBS)，它先计算最大LB值，然后采用基于此的最佳优先搜索来快速找到无碰撞路径。实验结果显示，DECBS在大多数测试用例中优于ECBS，能减少近30%的高层CT节点和50%的底层focal搜索节点，并在代理密度中等偏高时，实现23.5%的平均运行时间提升，同时兼容现有优化技术。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.03779v1",
      "published_date": "2025-03-04 20:39:00 UTC",
      "updated_date": "2025-03-04 20:39:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:38:48.776460"
    },
    {
      "arxiv_id": "2503.02992v1",
      "title": "RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Yimin Tang",
        "Xiao Xiong",
        "Jingyi Xi",
        "Jiaoyang Li",
        "Erdem Bıyık",
        "Sven Koenig"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial for applications ranging from aerial\nswarms to warehouse automation. Solving MAPF is NP-hard so learning-based\napproaches for MAPF have gained attention, particularly those leveraging deep\nneural networks. Nonetheless, despite the community's continued efforts, all\nlearning-based MAPF planners still rely on decentralized planning due to\nvariability in the number of agents and map sizes. We have developed the first\ncentralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is\nnot an agent-based policy but a map-based policy. By leveraging a CNN-based\narchitecture, RAILGUN can generalize across different maps and handle any\nnumber of agents. We collect trajectories from rule-based methods to train our\nmodel in a supervised way. In experiments, RAILGUN outperforms most baseline\nmethods and demonstrates great zero-shot generalization capabilities on various\ntasks, maps and agent numbers that were not seen in the training dataset.",
      "tldr_zh": "这篇论文提出了 RAILGUN，一种统一的卷积神经网络 (CNN) 策略，用于 Multi-Agent Path Finding (MAPF)，能够处理不同环境和任务中的多机器人无碰撞路径规划。不同于现有的去中心化学习方法，RAILGUN 是首个中心化策略，通过基于地图的架构实现对任意代理数量和地图的泛化，并采用监督学习从规则-based 方法收集的轨迹进行训练。实验结果显示，RAILGUN 优于大多数基线方法，并在未见数据集上的各种任务、地图和代理数量中展现出优秀的零样本泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02992v1",
      "published_date": "2025-03-04 20:35:20 UTC",
      "updated_date": "2025-03-04 20:35:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:39:01.606804"
    },
    {
      "arxiv_id": "2503.02989v1",
      "title": "Effectively Steer LLM To Follow Preference via Building Confident Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Bingqing Song",
        "Boran Han",
        "Shuai Zhang",
        "Hao Wang",
        "Haoyang Fang",
        "Bonan Min",
        "Yuyang Wang",
        "Mingyi Hong"
      ],
      "abstract": "Having an LLM that aligns with human preferences is essential for\naccommodating individual needs, such as maintaining writing style or generating\nspecific topics of interest. The majority of current alignment methods rely on\nfine-tuning or prompting, which can be either costly or difficult to control.\nModel steering algorithms, which modify the model output by constructing\nspecific steering directions, are typically easy to implement and\noptimization-free. However, their capabilities are typically limited to\nsteering the model into one of the two directions (i.e., bidirectional\nsteering), and there has been no theoretical understanding to guarantee their\nperformance. In this work, we propose a theoretical framework to understand and\nquantify the model steering methods. Inspired by the framework, we propose a\nconfident direction steering method (CONFST) that steers LLMs via modifying\ntheir activations at inference time. More specifically, CONFST builds a\nconfident direction that is closely aligned with users' preferences, and this\ndirection is then added to the activations of the LLMs to effectively steer the\nmodel output. Our approach offers three key advantages over popular\nbidirectional model steering methods: 1) It is more powerful, since multiple\n(i.e. more than two) users' preferences can be aligned simultaneously; 2) It is\nsimple to implement, since there is no need to determine which layer to add the\nsteering vector to; 3) No explicit user instruction is required. We validate\nour method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks\nthat require shifting the output of LLMs across various topics and styles,\nachieving superior performance over competing methods.",
      "tldr_zh": "本文提出一个理论框架来理解和量化模型转向方法，以解决大型语言模型（LLM）在遵循用户偏好时的局限性。作者引入了 CONFST 方法，通过构建置信方向并修改模型激活来实现高效转向，该方法支持同时处理多个偏好、实现简单且无需显式用户指令。实验在 GPT-2 XL、Mistral 和 Gemma-it 模型上验证，CONFST 在改变输出主题和风格的任务中，表现出色地超越了双向转向（bidirectional steering）竞争方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02989v1",
      "published_date": "2025-03-04 20:32:27 UTC",
      "updated_date": "2025-03-04 20:32:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:39:14.209307"
    },
    {
      "arxiv_id": "2503.04817v1",
      "title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series",
      "title_zh": "AI辅助提取电视连续剧叙事弧线多智能体系统",
      "authors": [
        "Roberto Balestri",
        "Guglielmo Pescatore"
      ],
      "abstract": "Serialized TV shows are built on complex storylines that can be hard to track\nand evolve in ways that defy straightforward analysis. This paper introduces a\nmulti-agent system designed to extract and analyze these narrative arcs. Tested\non the first season of Grey's Anatomy (ABC 2005-), the system identifies three\ntypes of arcs: Anthology (self-contained), Soap (relationship-focused), and\nGenre-Specific (strictly related to the series' genre). Episodic progressions\nof these arcs are stored in both relational and semantic (vectorial) databases,\nenabling structured analysis and comparison. To bridge the gap between\nautomation and critical interpretation, the system is paired with a graphical\ninterface that allows for human refinement using tools to enhance and visualize\nthe data. The system performed strongly in identifying Anthology Arcs and\ncharacter entities, but its reliance on textual paratexts (such as episode\nsummaries) revealed limitations in recognizing overlapping arcs and subtler\ndynamics. This approach highlights the potential of combining computational and\nhuman expertise in narrative analysis. Beyond television, it offers promise for\nserialized written formats, where the narrative resides entirely in the text.\nFuture work will explore the integration of multimodal inputs, such as dialogue\nand visuals, and expand testing across a wider range of genres to refine the\nsystem further.",
      "tldr_zh": "本研究提出了一种Multi-Agent System，用于AI辅助提取和分析TV系列剧的叙事弧线，在Grey's Anatomy第一季上进行测试。该系统识别了三种弧线类型：Anthology（自成一体的）、Soap（关系导向的）和Genre-Specific（与系列类型相关的），并将弧线进展存储在关系数据库和语义（vectorial）数据库中，同时结合图形界面允许人类精炼和可视化数据。实验结果显示，该系统在识别Anthology Arcs和字符实体方面表现出色，但依赖文本paratexts（如剧集摘要）导致在处理重叠弧线和微妙动态时存在局限性。该方法强调计算与人类专业知识的结合，具有潜力扩展到序列化文本格式，未来将整合多模态输入并测试更多类型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "17th International Conference on Agents and Artificial Intelligence,\n  Porto (Portugal). 23/02/2025 - 25/02/2025",
      "pdf_url": "http://arxiv.org/pdf/2503.04817v1",
      "published_date": "2025-03-04 20:27:14 UTC",
      "updated_date": "2025-03-04 20:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:39:28.293356"
    },
    {
      "arxiv_id": "2503.03777v1",
      "title": "FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Hongchao Du",
        "Shangyu Wu",
        "Arina Kharlamova",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "abstract": "Large Language Models (LLMs) face challenges for on-device inference due to\nhigh memory demands. Traditional methods to reduce memory usage often\ncompromise performance and lack adaptability. We propose FlexInfer, an\noptimized offloading framework for on-device inference, addressing these issues\nwith techniques like asynchronous prefetching, balanced memory locking, and\nflexible tensor preservation. These strategies enhance memory efficiency and\nmitigate I/O bottlenecks, ensuring high performance within user-specified\nresource constraints. Experiments demonstrate that FlexInfer significantly\nimproves throughput under limited resources, achieving up to 12.5 times better\nperformance than existing methods and facilitating the deployment of large\nmodels on resource-constrained devices.",
      "tldr_zh": "本文提出 FlexInfer，一种灵活高效的 offloading 框架，用于解决 Large Language Models (LLMs) 在设备端推理时的内存限制问题。FlexInfer 通过异步预取 (asynchronous prefetching)、平衡内存锁定 (balanced memory locking) 和灵活张量保存 (flexible tensor preservation) 等技术，提升内存效率、缓解 I/O 瓶颈，并在用户指定的资源约束下确保高性能。实验结果显示，FlexInfer 在资源有限的环境中，吞吐量比现有方法提高高达 12.5 倍，从而促进大型模型在受限设备的部署。",
      "categories": [
        "cs.OS",
        "cs.AI"
      ],
      "primary_category": "cs.OS",
      "comment": "9 pages, 5 figures, to be published in EuroMLSys '25",
      "pdf_url": "http://arxiv.org/pdf/2503.03777v1",
      "published_date": "2025-03-04 20:08:03 UTC",
      "updated_date": "2025-03-04 20:08:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:39:39.018025"
    },
    {
      "arxiv_id": "2503.02976v1",
      "title": "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew DosSantos DiSorbo",
        "Harang Ju",
        "Sinan Aral"
      ],
      "abstract": "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
      "tldr_zh": "该研究发现，大型语言模型(LLMs)作为agentic AI系统，在处理异常决策时往往严格遵守政策而偏离人类判断，导致决策不实际或次优。研究者评估了三种方法，包括ethical framework prompting、chain-of-thought reasoning和supervised fine-tuning，结果显示前两种方法效果有限，而supervised fine-tuning（尤其是结合人类解释）显著提升了模型的表现。令人意外的是，这种细调方法不仅使LLMs在训练场景中对齐人类决策，还实现了对新场景的泛化转移学习，强调了使用解释而非仅标签的重要性，以推动agentic AI更有效地适应人类判断。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02976v1",
      "published_date": "2025-03-04 20:00:37 UTC",
      "updated_date": "2025-03-04 20:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:39:50.066441"
    },
    {
      "arxiv_id": "2503.02972v3",
      "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
      "title_zh": "翻译失败",
      "authors": [
        "Jude Khouja",
        "Karolina Korgul",
        "Simi Hellsten",
        "Lingyi Yang",
        "Vlad Neacsu",
        "Harry Mayne",
        "Ryan Kearns",
        "Andrew Bean",
        "Adam Mahdi"
      ],
      "abstract": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
      "tldr_zh": "该论文提出 LINGOLY-TOO 框架，通过 linguistic templatisation 和 orthographic obfuscation 技术，生成语言推理问题变体，以减少 LLMs（大型语言模型）在基准测试中因数据暴露而导致的记忆化影响。框架动态混淆真实语言的书写系统，创建众多问题变体，这些变体保留了推理步骤但降低了特定实例出现在训练数据中的可能性。实验结果显示，前沿模型如 Claude 3.7 Sonnet、o1-preview 和 DeepSeek R1 在高级推理任务上挣扎，且在问题排列和原正字形式上表现出显著准确率差异，证明了数据暴露可能 overestimated 了模型的推理能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02972v3",
      "published_date": "2025-03-04 19:57:47 UTC",
      "updated_date": "2025-03-07 09:31:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:40:03.084206"
    },
    {
      "arxiv_id": "2503.02969v1",
      "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Siqi Ouyang",
        "Xi Xu",
        "Lei Li"
      ],
      "abstract": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
      "tldr_zh": "该研究提出InfiniSST，一种基于大型语言模型的创新框架，用于实现无界限流式语音的同时翻译（Simultaneous Translation of Unbounded Speech），以平衡翻译质量和延迟问题。InfiniSST将翻译任务表述为多轮对话，通过在MuST-C数据集上构建翻译轨迹和鲁棒段落，并采用多延迟增强训练和关键-值（KV）缓存管理策略，提升推理效率。实验结果显示，在MuST-C的En-Es、En-De和En-Zh数据集上，InfiniSST将计算感知延迟减少0.5到1秒，同时保持与基线相同的翻译质量，并通过消融研究验证了数据构建和缓存策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.02969v1",
      "published_date": "2025-03-04 19:51:29 UTC",
      "updated_date": "2025-03-04 19:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:40:14.862135"
    },
    {
      "arxiv_id": "2503.02955v1",
      "title": "Monocular visual simultaneous localization and mapping: (r)evolution from geometry to deep learning-based pipelines",
      "title_zh": "翻译失败",
      "authors": [
        "Olaya Alvarez-Tunon",
        "Yury Brodskiy",
        "Erdal Kayacan"
      ],
      "abstract": "With the rise of deep learning, there is a fundamental change in visual SLAM\nalgorithms toward developing different modules trained as end-to-end pipelines.\nHowever, regardless of the implementation domain, visual SLAM's performance is\nsubject to diverse environmental challenges, such as dynamic elements in\noutdoor environments, harsh imaging conditions in underwater environments, or\nblurriness in high-speed setups. These environmental challenges need to be\nidentified to study the real-world viability of SLAM implementations. Motivated\nby the aforementioned challenges, this paper surveys the current state of\nvisual SLAM algorithms according to the two main frameworks: geometry-based and\nlearning-based SLAM. First, we introduce a general formulation of the SLAM\npipeline that includes most of the implementations in the literature. Second,\nthose implementations are classified and surveyed for geometry and\nlearning-based SLAM. After that, environment-specific challenges are formulated\nto enable experimental evaluation of the resilience of different visual SLAM\nclasses to varying imaging conditions. We address two significant issues in\nsurveying visual SLAM, providing (1) a consistent classification of visual SLAM\npipelines and (2) a robust evaluation of their performance under different\ndeployment conditions. Finally, we give our take on future opportunities for\nvisual SLAM implementations.",
      "tldr_zh": "这篇论文探讨了单目视觉 SLAM（Simultaneous Localization and Mapping）的演变，从 geometry-based 到 learning-based 管道，强调了深度学习在构建端到端模块中的作用，同时分析了环境挑战如户外动态元素、水下成像条件和高速度模糊对性能的影响。作者引入了 SLAM 的一般公式，并对现有算法进行分类和调查，比较了 geometry-based 和 learning-based 框架的实现。最终，论文提供了统一的分类方法和在不同条件下的性能评估，并展望了视觉 SLAM 的未来发展机会。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02955v1",
      "published_date": "2025-03-04 19:20:17 UTC",
      "updated_date": "2025-03-04 19:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:40:26.923808"
    },
    {
      "arxiv_id": "2503.02954v1",
      "title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders",
      "title_zh": "通过图神经网络变分自动编码器实现可靠且高效的多智能体协调",
      "authors": [
        "Yue Meng",
        "Nathalie Majcherczyk",
        "Wenliang Liu",
        "Scott Kiesel",
        "Chuchu Fan",
        "Federico Pecora"
      ],
      "abstract": "Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord",
      "tldr_zh": "这篇论文提出了一种基于 Graph Neural Network Variational Autoencoders (GNN-VAE) 的方法，以实现可靠且高效的多智能体协调，特别是在密集多机器人导航场景中，如自动化仓库。作者将协调问题表述为图问题，使用 Mixed-Integer Linear Program (MILP) 求解器收集地面真实数据，并在训练过程中将高质量解决方案编码到潜在空间。推理时，通过采样潜在变量解码方案并选择成本最低的可行提案，确保始终满足约束条件。实验结果显示，该方法在小规模问题上训练后，能快速处理包含250机器人的大型问题，比基线方法显著更快，同时保持高品质解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by 2025 International Conference on Robotics and Automation\n  (ICRA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02954v1",
      "published_date": "2025-03-04 19:20:11 UTC",
      "updated_date": "2025-03-04 19:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:40:39.225297"
    },
    {
      "arxiv_id": "2503.02951v1",
      "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangchen Xu",
        "Yang Liu",
        "Yueqin Yin",
        "Mingyuan Zhou",
        "Radha Poovendran"
      ],
      "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
      "tldr_zh": "我们引入了 KodCode，这是一个多样化、具有挑战性和可验证的合成数据集，旨在为训练 Large Language Models 提供高质量编码训练数据，解决现有资源在覆盖广度和正确性验证方面的不足。数据集通过一个系统管道生成 question-solution-test triplets，包括合成广泛编码问题、生成解决方案和测试用例，并使用自验证程序（如测试-based reject sampling 和 DeepSeek R1 模型）确保准确性。KodCode 适用于 supervised fine-tuning 和 RL tuning，在 HumanEval(+)、MBPP(+)、BigCodeBench 和 LiveCodeBench 等基准测试中，微调后的模型达到了 state-of-the-art 性能，超过了 Qwen2.5-Coder-32B-Instruct 和 DeepSeek-R1-Distill-Llama-70B 等模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Codes and Data: https://kodcode-ai.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.02951v1",
      "published_date": "2025-03-04 19:17:36 UTC",
      "updated_date": "2025-03-04 19:17:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:40:52.052566"
    },
    {
      "arxiv_id": "2503.02950v2",
      "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Danqing Zhang",
        "Balaji Rama",
        "Jingyi Ni",
        "Shiying He",
        "Fu Zhao",
        "Kunyu Chen",
        "Arnold Chen",
        "Junyu Cao"
      ],
      "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
      "tldr_zh": "我们引入了 LiteWebAgent，这是一个开源套件，针对基于 VLM 的网络代理应用，填补了生态中的关键空白，通过最小化服务器后端配置、直观的用户和浏览器界面，以及可扩展的研究组件（如代理规划、记忆和树搜索）提供生产就绪的解决方案。核心框架采用递归函数调用作为简单有效的基线，实现解耦的动作生成和动作接地，并以模块化方式集成高级功能。LiteWebAgent 支持两种部署形式：一个基于 Vercel 的网络应用，提供代理控制的远程浏览器，以及一个 Chrome 扩展，通过 CDP 控制现有浏览器。该框架的代码已在 GitHub 上开源，促进进一步的开发和研究。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02950v2",
      "published_date": "2025-03-04 19:13:10 UTC",
      "updated_date": "2025-05-06 06:42:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:41:03.334009"
    },
    {
      "arxiv_id": "2503.02924v1",
      "title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Meng",
        "Chuchu fan"
      ],
      "abstract": "Generating realistic simulations is critical for autonomous system\napplications such as self-driving and human-robot interactions. However,\ndriving simulators nowadays still have difficulty in generating controllable,\ndiverse, and rule-compliant behaviors for road participants: Rule-based models\ncannot produce diverse behaviors and require careful tuning, whereas\nlearning-based methods imitate the policy from data but are not designed to\nfollow the rules explicitly. Besides, the real-world datasets are by nature\n\"single-outcome\", making the learning method hard to generate diverse\nbehaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion\nModels to learn controllable, diverse, and rule-aware policy. We first\ncalibrate the STL on the real-world data, then generate diverse synthetic data\nusing trajectory optimization, and finally learn the rectified diffusion policy\non the augmented dataset. We test on the NuScenes dataset and our approach can\nachieve the most diverse rule-compliant trajectories compared to other\nbaselines, with a runtime 1/17X to the second-best approach. In the closed-loop\ntesting, our approach reaches the highest diversity, rule satisfaction rate,\nand the least collision rate. Our method can generate varied characteristics\nconditional on different STL parameters in testing. A case study on human-robot\nencounter scenarios shows our approach can generate diverse and\nclosed-to-oracle trajectories. The annotation tool, augmented dataset, and code\nare available at https://github.com/mengyuest/pSTL-diffusion-policy.",
      "tldr_zh": "这篇论文提出了一种结合 Signal Temporal Logic (STL) 和 Diffusion Models 的方法，用于生成可控、多样且规则遵守的模拟行为，针对自动驾驶和人机交互等应用中的行为模拟问题。方法包括在真实数据上校准 STL、通过轨迹优化生成多样合成数据，以及在增强数据集上训练修正的扩散政策。实验在 NuScenes 数据集上显示，该方法比基线模型产生更多样且规则符合的轨迹，运行时间是第二好方法的1/17，并在闭环测试中实现最高多样性、规则满足率和最低碰撞率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L), October 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.02924v1",
      "published_date": "2025-03-04 18:59:00 UTC",
      "updated_date": "2025-03-04 18:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:41:16.399161"
    },
    {
      "arxiv_id": "2503.02882v1",
      "title": "Bringing Comparative Cognition To Computers",
      "title_zh": "翻译失败",
      "authors": [
        "Konstantinos Voudouris",
        "Lucy G. Cheke",
        "Eric Schulz"
      ],
      "abstract": "Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences.",
      "tldr_zh": "研究人员正越来越多地将人工智能系统置于心理测试中，以比较其认知能力。论文强调，为了严格评估AI与人类和其他动物的认知异同，必须避免夸大或低估相似性和差异。通过拥抱comparative cognition的方法，该研究建议将AI认知研究整合到更广泛的认知科学中，从而促进跨领域合作和更准确的比较分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02882v1",
      "published_date": "2025-03-04 18:58:42 UTC",
      "updated_date": "2025-03-04 18:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:41:27.009122"
    },
    {
      "arxiv_id": "2503.02881v3",
      "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Han Xue",
        "Jieji Ren",
        "Wendi Chen",
        "Gu Zhang",
        "Yuan Fang",
        "Guoying Gu",
        "Huazhe Xu",
        "Cewu Lu"
      ],
      "abstract": "Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as fast response to external changes and\nadaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines.\nFurthermore, experiments show that RDP is applicable across different tactile /\nforce sensors. Code and videos are available on\nhttps://reactive-diffusion-policy.github.io.",
      "tldr_zh": "该论文针对机器人处理接触丰富操作的挑战，提出TactAR系统——一个低成本遥操作系统，通过增强现实(AR)提供实时触觉反馈，以及Reactive Diffusion Policy (RDP)算法，一种慢速-快速视觉-触觉模仿学习方法。RDP采用双层结构：慢速潜在扩散策略在低频下预测高层次动作分块，而快速不对称标记器在高频下实现闭环触觉反馈控制，从而在统一框架中实现复杂轨迹建模和快速反应行为。通过在三个具有挑战性的接触丰富任务上的评估，RDP显著提升了性能，优于最先进的视觉IL基线，并证明了其在不同触觉/力传感器上的适用性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to RSS 2025. Project page:\n  https://reactive-diffusion-policy.github.io",
      "pdf_url": "http://arxiv.org/pdf/2503.02881v3",
      "published_date": "2025-03-04 18:58:21 UTC",
      "updated_date": "2025-04-23 10:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:41:41.575298"
    },
    {
      "arxiv_id": "2503.02879v1",
      "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
      "title_zh": "Wikipedia 在 LLMs 时代：演变与风险",
      "authors": [
        "Siming Huang",
        "Yuliang Xu",
        "Mingmeng Geng",
        "Yao Wan",
        "Dongping Chen"
      ],
      "abstract": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
      "tldr_zh": "本研究分析了大型语言模型（LLMs）对 Wikipedia 的影响，通过现有数据和模拟探索其演变与潜在风险。研究者评估了页面浏览量、文章内容变化，以及 LLMs 对自然语言处理（NLP）任务（如机器翻译和检索增强生成，RAG）的冲击。结果显示，Wikipedia 文章在某些类别受 LLMs 影响约 1%-2%，可能导致机器翻译基准测试分数被夸大，并改变模型间比较结果。此外，RAG 的有效性可能因知识库被 LLM 生成内容污染而下降。尽管 LLMs 尚未彻底改变 Wikipedia 的语言和知识结构，该研究强调需要警惕未来的风险。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia",
      "pdf_url": "http://arxiv.org/pdf/2503.02879v1",
      "published_date": "2025-03-04 18:58:13 UTC",
      "updated_date": "2025-03-04 18:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:41:50.491202"
    },
    {
      "arxiv_id": "2503.02878v1",
      "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
      "title_zh": "语言模型能够在状态价值估计上自我",
      "authors": [
        "Ethan Mendes",
        "Alan Ritter"
      ],
      "abstract": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
      "tldr_zh": "本研究探讨了语言模型（Language Models）在状态价值估计（State-Value Estimation）方面的自提升能力，以优化搜索过程。作者提出 self-taught lookahead，一种自监督方法，利用状态转移动态训练价值模型（value model），从而指导语言模型控制的多步推理任务，而无需依赖昂贵的真实奖励或人类演示。实验结果显示，中等规模（8 亿参数）的开源价值模型经该方法改进后，可与使用前沿 LLM 如 gpt-4o 相当；此外，该方法将性能提升 20%，同时将成本降低 37 倍，相比之前的 LLM-based tree search 更高效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02878v1",
      "published_date": "2025-03-04 18:58:11 UTC",
      "updated_date": "2025-03-04 18:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:42:03.073857"
    },
    {
      "arxiv_id": "2503.02861v1",
      "title": "Evaluation of Architectural Synthesis Using Generative AI",
      "title_zh": "使用生成式 AI 评估建筑合成",
      "authors": [
        "Jingfei Huang",
        "Alexandros Haridis"
      ],
      "abstract": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
      "tldr_zh": "这篇论文评估了 Generative AI 系统（如 GPT-4o 和 Claude 3.5）在建筑 3D 合成任务中的性能，通过对 Palladio 建筑（Villa Rotonda 和 Palazzo Porto）的案例研究。研究采用顺序文本和图像提示，测试 AI 在解释 2D/3D 绘图、生成 CAD 软件脚本以及基于输出进行自我改进的能力。结果显示，两个系统能成功生成个别部件，但难以准确组装空间关系，其中 Claude 3.5 在自我修正方面表现更优。该研究为基准测试 AI 在专业领域任务中的优势和劣势提供了见解，并突出了 AI 作为建筑设计过程协作助手的潜力。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
      "published_date": "2025-03-04 18:39:28 UTC",
      "updated_date": "2025-03-04 18:39:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:42:17.665412"
    },
    {
      "arxiv_id": "2503.02857v3",
      "title": "Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024",
      "title_zh": "翻译失败",
      "authors": [
        "Nuria Alina Chandra",
        "Ryan Murtfeldt",
        "Lin Qiu",
        "Arnab Karmakar",
        "Hannah Lee",
        "Emmanuel Tanumihardja",
        "Kevin Farhat",
        "Ben Caffee",
        "Sejin Paik",
        "Changyeon Lee",
        "Jongwook Choi",
        "Aerin Kim",
        "Oren Etzioni"
      ],
      "abstract": "In the age of increasingly realistic generative AI, robust deepfake detection\nis essential for mitigating fraud and disinformation. While many deepfake\ndetectors report high accuracy on academic datasets, we show that these\nacademic benchmarks are out of date and not representative of real-world\ndeepfakes. We introduce Deepfake-Eval-2024, a new deepfake detection benchmark\nconsisting of in-the-wild deepfakes collected from social media and deepfake\ndetection platform users in 2024. Deepfake-Eval-2024 consists of 45 hours of\nvideos, 56.5 hours of audio, and 1,975 images, encompassing the latest\nmanipulation technologies. The benchmark contains diverse media content from 88\ndifferent websites in 52 different languages. We find that the performance of\nopen-source state-of-the-art deepfake detection models drops precipitously when\nevaluated on Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for\naudio, and 45% for image models compared to previous benchmarks. We also\nevaluate commercial deepfake detection models and models finetuned on\nDeepfake-Eval-2024, and find that they have superior performance to\noff-the-shelf open-source models, but do not yet reach the accuracy of deepfake\nforensic analysts. The dataset is available at\nhttps://github.com/nuriachandra/Deepfake-Eval-2024.",
      "tldr_zh": "该论文引入了 Deepfake-Eval-2024，这是一个多模态真实世界基准数据集，包含 2024 年从社交媒体和平台收集的 45 小时视频、56.5 小时音频和 1,975 张图像，覆盖 88 个网站和 52 种语言，以反映最新深度伪造技术。研究发现，现有的开源深度伪造检测模型在该基准上性能急剧下降，与学术基准相比，AUC 分别下降 50%（视频）、48%（音频）和 45%（图像）。商业模型和在 Deepfake-Eval-2024 上微调的模型表现出优越性能，但仍未达到深度伪造取证分析师的准确性。该数据集可从 https://github.com/nuriachandra/Deepfake-Eval-2024 获取，用于提升深度伪造检测的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02857v3",
      "published_date": "2025-03-04 18:33:22 UTC",
      "updated_date": "2025-03-24 20:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:42:29.293554"
    },
    {
      "arxiv_id": "2503.02854v2",
      "title": "(How) Do Language Models Track State?",
      "title_zh": "翻译失败",
      "authors": [
        "Belinda Z. Li",
        "Zifan Carl Guo",
        "Jacob Andreas"
      ],
      "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.",
      "tldr_zh": "本研究探讨 Transformer 语言模型 (LMs) 如何跟踪状态变化，例如在故事生成或代码编写中，通过训练或微调模型处理排列组合任务（permutation composition）。结果显示，LMs 学会了两种机制：一种类似于 associative scan 的结构，另一种则使用 permutation parity 修剪输出空间后结合 associative scan，以提高效率。这两种机制表现出不同的鲁棒性，且可以通过中间训练任务引导模型选择特定机制，最终证明 Transformer LMs 可以实现高效、可解释的状态跟踪，并实现对该过程的预测和控制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 17 figures, 1 table. Code:\n  http://github.com/belindal/state-tracking",
      "pdf_url": "http://arxiv.org/pdf/2503.02854v2",
      "published_date": "2025-03-04 18:31:02 UTC",
      "updated_date": "2025-03-11 15:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:42:40.151200"
    },
    {
      "arxiv_id": "2503.02849v1",
      "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data",
      "title_zh": "翻译失败",
      "authors": [
        "Amin Honarmandi Shandiz"
      ],
      "abstract": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
      "tldr_zh": "本研究提出了一种深度多模态学习框架，用于整合组织病理图像和基因表达数据，以对乳腺癌进行亚型分类，具体将乳腺癌分为 BRCA.Luminal 和 BRCA.Basal / Her2 亚型，从而提升个性化治疗和预后的预测能力。框架采用 ResNet-50 模型提取图像特征、全连接层处理基因表达数据，并通过 cross-attention 融合机制增强模态间交互。实验结果显示，在五折交叉验证中，该多模态方法在分类准确率、precision-recall AUC 和 F1-score 上均优于单模态方法，证明了深度学习在鲁棒且可解释的乳腺癌亚型分类中的潜力，为临床决策提供重要支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
      "published_date": "2025-03-04 18:24:33 UTC",
      "updated_date": "2025-03-04 18:24:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:42:52.554786"
    },
    {
      "arxiv_id": "2503.02836v1",
      "title": "SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Ting-Ji Huang",
        "Xu-Yang Chen",
        "Han-Jia Ye"
      ],
      "abstract": "Unlike traditional time-series forecasting methods that require extensive\nin-task data for training, zero-shot forecasting can directly predict future\nvalues given a target time series without additional training data. Current\nzero-shot approaches primarily rely on pre-trained generalized models, with\ntheir performance often depending on the variety and relevance of the\npre-training data, which can raise privacy concerns. Instead of collecting\ndiverse pre-training data, we introduce SeqFusion in this work, a novel\nframework that collects and fuses diverse pre-trained models (PTMs)\nsequentially for zero-shot forecasting. Based on the specific temporal\ncharacteristics of the target time series, SeqFusion selects the most suitable\nPTMs from a batch of pre-collected PTMs, performs sequential predictions, and\nfuses all the predictions while using minimal data to protect privacy. Each of\nthese PTMs specializes in different temporal patterns and forecasting tasks,\nallowing SeqFusion to select by measuring distances in a shared representation\nspace of the target time series with each PTM. Experiments demonstrate that\nSeqFusion achieves competitive accuracy in zero-shot forecasting compared to\nstate-of-the-art methods.",
      "tldr_zh": "本文提出 SeqFusion 框架，用于零-shot 时间序列预测，它通过顺序融合多种 Pre-Trained Models (PTMs)，而非收集更多预训练数据，从而避免隐私风险。SeqFusion 根据目标时间序列的特定时间特性，在共享表示空间中测量距离来选择最合适的 PTMs，进行顺序预测并融合结果。实验表明，该框架在零-shot 预测任务中实现了与最先进方法相当的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02836v1",
      "published_date": "2025-03-04 17:59:17 UTC",
      "updated_date": "2025-03-04 17:59:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:43:03.786344"
    },
    {
      "arxiv_id": "2503.02832v1",
      "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
      "title_zh": "AlignDistil：标记级别语言模型对齐作为自适应策略蒸馏",
      "authors": [
        "Songming Zhang",
        "Xue Zhang",
        "Tong Zhang",
        "Bojie Hu",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
      "tldr_zh": "该论文针对大型语言模型 (LLMs) 的对齐问题，提出 AlignDistil，一种基于自适应策略蒸馏的 token 级别优化方法，以解决现有 RLHF 和 DPO 方法忽略 token 级别奖励导致的次优性能和慢速收敛问题。具体而言，AlignDistil 将 DPO 学到的奖励整合进 RLHF 目标，并理论证明其等效于 token 级别蒸馏过程，同时通过构建对比 DPO 奖励和 token 自适应 logit 外推机制，避免不同 token 的过度或不足优化。实验结果显示，该方法在性能上优于现有方法，并实现了更快的收敛速度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02832v1",
      "published_date": "2025-03-04 17:57:09 UTC",
      "updated_date": "2025-03-04 17:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:43:17.702097"
    },
    {
      "arxiv_id": "2503.02824v1",
      "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging",
      "title_zh": "开发 PET/CT 基础模型用于跨模态解剖和功能成像",
      "authors": [
        "Yujin Oh",
        "Robert Seifert",
        "Yihan Cao",
        "Christoph Clement",
        "Justin Ferdinandus",
        "Constantin Lapa",
        "Alessandro Liebich",
        "Michelle Amon",
        "Johanna Enke",
        "Sifan Song",
        "Runqi Meng",
        "Fang Zeng",
        "Ning Guo",
        "Xiang Li",
        "Pedram Heidari",
        "Axel Rominger",
        "Kuangyu Shi",
        "Quanzheng Li"
      ],
      "abstract": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
      "tldr_zh": "该研究针对 PET/CT 成像在肿瘤学中的应用，解决了现有 AI 模型因任务特定训练和数据集有限而导致的泛化性和鲁棒性不足问题。研究提出了一种名为 Cross-Fraternal Twin Masked Autoencoder (FratMAE) 的基础模型框架，使用单独的 Vision Transformer (ViT) 编码器处理 PET 和 CT 扫描，并通过跨注意力解码器实现模态间协同互动，同时整合文本元数据来提升 PET 表示学习。FratMAE 通过在 PET/CT 数据集上预训练，成功捕获复杂的跨模态关系和全局摄取模式，在下游任务中表现出色，具有作为通用基础模型的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
      "published_date": "2025-03-04 17:49:07 UTC",
      "updated_date": "2025-03-04 17:49:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:43:29.836032"
    },
    {
      "arxiv_id": "2503.02823v1",
      "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
      "title_zh": "多模态交响：通过生成式 AI 整合味觉和声音",
      "authors": [
        "Matteo Spanio",
        "Massimiliano Zampini",
        "Antonio Rodà",
        "Franco Pierucci"
      ],
      "abstract": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
      "tldr_zh": "这篇论文探讨了通过生成式 AI 整合味觉和听觉的多模态生成模型，基于神经科学和心理学研究，将味觉描述转化为音乐。研究者回顾了该领域的关键发现和方法，并通过实验 fine-tuned MusicGEN 模型，使用详细的味觉输入生成音乐。结果显示，根据 111 名参与者的评估，fine-tuned 模型生成的音乐更连贯地反映了输入描述，比原始模型表现出色。该研究为 AI 与声音、味觉的互动开辟了新路径，并公开了数据集、代码和预训练模型。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "I.2.6; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "17 pages, 6 figures (2 + 2 figures with 2 subfigures each)",
      "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
      "published_date": "2025-03-04 17:48:48 UTC",
      "updated_date": "2025-03-04 17:48:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:43:41.036373"
    },
    {
      "arxiv_id": "2503.02812v1",
      "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
      "title_zh": "Q-Filters：利用 QK 几何结构实现高效 KV 缓存压缩",
      "authors": [
        "Nathan Godey",
        "Alessio Devoto",
        "Yu Zhao",
        "Simone Scardapane",
        "Pasquale Minervini",
        "Éric de la Clergerie",
        "Benoît Sagot"
      ],
      "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
      "tldr_zh": "本文提出 Q-Filters，一种训练-free 的 KV Cache 压缩方法，通过利用 Query (Q) 和 Key (K) 向量的 QK Geometry 属性，基于单一上下文无关投影过滤不重要的 Key-Value 对，从而高效近似注意力分数，并与 FlashAttention 兼容。不同于基于注意力的压缩方案，Q-Filters 在长上下文设置中表现出色，与 SnapKV 在检索任务中竞争，并在生成任务中显著优于 Streaming-LLM。实验结果显示，它在 x32 压缩级别下实现 99% 的 needle-in-a-haystack 准确率，并将文本生成中的 perplexity 下降减少高达 65%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02812v1",
      "published_date": "2025-03-04 17:37:49 UTC",
      "updated_date": "2025-03-04 17:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:43:55.135200"
    },
    {
      "arxiv_id": "2503.02797v1",
      "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
      "title_zh": "一种因果框架，用于对齐图像质量指标和深度神经网络鲁棒性",
      "authors": [
        "Nathan Drenkow",
        "Mathias Unberath"
      ],
      "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance.",
      "tldr_zh": "该论文提出一个因果框架，用于对齐图像质量指标（IQA）和深度神经网络（DNNs）的鲁棒性，旨在解决图像质量变化对DNN性能的影响问题。作者从因果视角重新审视IQA，发现传统指标对DNN分类性能的预测力较弱，并通过理论和经验验证这一局限性。基于此，他们开发了一个新图像质量指标，更强相关于DNN敏感性，能作为性能先验直接估计大规模图像数据集的质量分布，从而表征数据集组成与DNN性能的关系。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02797v1",
      "published_date": "2025-03-04 17:15:31 UTC",
      "updated_date": "2025-03-04 17:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:44:05.507226"
    },
    {
      "arxiv_id": "2503.02784v3",
      "title": "Do Not Trust Licenses You See: Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing",
      "title_zh": "不要相信你看到的许可证：数据集合规性需要大规模AI驱动的生命周期追踪",
      "authors": [
        "Jaekyeom Kim",
        "Sungryull Sohn",
        "Gerrard Jeongwon Jo",
        "Jihoon Choi",
        "Kyunghoon Bae",
        "Hwayoung Lee",
        "Yongmin Park",
        "Honglak Lee"
      ],
      "abstract": "This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.",
      "tldr_zh": "本论文指出，仅靠数据集的许可证（license terms）无法准确评估其法律风险，需要追踪数据集的再分配和整个生命周期，但这一过程超出人力处理能力。作者开发了名为 NEXUS 的自动化数据合规系统，利用 AI 代理（AI agents）来系统追踪数据来源、验证再分配权利并评估法律风险。实验分析了 17,429 个实体和 8,072 个许可证条款，发现再分配后的子集与原数据集在法律权利上存在显著差异，例如在 2,852 个具有商业潜力的数据集许可中，仅有 605 个（21%）真正允许商业化。该研究为 AI 数据治理设定了新标准，倡导通过生命周期追踪框架确保数据集管理的透明性和合规性。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02784v3",
      "published_date": "2025-03-04 16:57:53 UTC",
      "updated_date": "2025-03-14 16:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:44:17.954239"
    },
    {
      "arxiv_id": "2503.02783v2",
      "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Wu",
        "Haoling Li",
        "Xin Zhang",
        "Jianwen Luo",
        "Yangyu Huang",
        "Ruihang Chu",
        "Yujiu Yang",
        "Scarlett Li"
      ],
      "abstract": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
      "tldr_zh": "该研究指出，现有的偏好学习方法在提升 Code LLMs 代码生成能力时，仅基于测试通过率构建偏好对，无法精确定位具体错误，从而限制了模型学习错误修正模式。为解决此问题，提出 IterPref 框架，通过模拟人类迭代调试过程来定位错误区域，并使用定制的 DPO 算法对齐相应 tokens，同时引入 CodeFlow 数据集来生成迭代精炼的偏好对。实验结果显示，配备 IterPref 的 Code LLMs 在代码生成任务上取得显著性能提升，并在 BigCodeBench 等挑战任务中减少错误，且相关代码和数据将公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The code and data will be released soon",
      "pdf_url": "http://arxiv.org/pdf/2503.02783v2",
      "published_date": "2025-03-04 16:56:34 UTC",
      "updated_date": "2025-03-10 18:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:44:29.561115"
    },
    {
      "arxiv_id": "2503.05816v1",
      "title": "Will Neural Scaling Laws Activate Jevons' Paradox in AI Labor Markets? A Time-Varying Elasticity of Substitution (VES) Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Rajesh P. Narayanan",
        "R. Kelley Pace"
      ],
      "abstract": "AI industry leaders often use the term ``Jevons' Paradox.'' We explore the\nsignificance of this term for artificial intelligence adoption through a\ntime-varying elasticity of substitution framework. We develop a model\nconnecting AI development to labor substitution through four key mechanisms:\n(1) increased effective computational capacity from both hardware and\nalgorithmic improvements; (2) AI capabilities that rise logarithmically with\ncomputation following established neural scaling laws; (3) declining marginal\ncomputational costs leading to lower AI prices through competitive pressure;\nand (4) a resulting increase in the elasticity of substitution between AI and\nhuman labor over time. Our time-varying elasticity of substitution (VES)\nframework, incorporating the G\\o rtz identity, yields analytical conditions for\nmarket transformation dynamics. This work provides a simple framework to help\nassess the economic reasoning behind industry claims that AI will increasingly\nsubstitute for human labor across diverse economic sectors.",
      "tldr_zh": "这篇论文探讨了神经缩放定律 (neural scaling laws) 是否会激活 Jevons' Paradox 在 AI 劳动力市场，通过一个时间可变的弹性替代 (VES) 框架进行分析。作者开发了一个模型，将 AI 发展与劳动力替代联系起来，涵盖四个关键机制：硬件和算法改进增加有效计算能力、AI 能力随计算呈对数增长、边际计算成本下降导致 AI 价格降低，以及 AI 与人类劳动力之间弹性替代的增加。该框架整合 Gørtz identity，提供市场转型动态的分析条件，帮助评估 AI 在不同经济部门日益替代人类劳动力的经济影响。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "q-fin.EC",
        "I.2.m; J.4"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05816v1",
      "published_date": "2025-03-04 16:55:30 UTC",
      "updated_date": "2025-03-04 16:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:44:42.388966"
    },
    {
      "arxiv_id": "2503.02781v1",
      "title": "Multimodal AI predicts clinical outcomes of drug combinations from preclinical data",
      "title_zh": "多模态 AI 从临床前数据预测药物组合的临床结果",
      "authors": [
        "Yepeng Huang",
        "Xiaorui Su",
        "Varun Ullanat",
        "Ivy Liang",
        "Lindsay Clegg",
        "Damilola Olabode",
        "Nicholas Ho",
        "Bino John",
        "Megan Gibbs",
        "Marinka Zitnik"
      ],
      "abstract": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
      "tldr_zh": "本研究引入了多模态 AI 模型 MADRIGAL，用于从预临床数据预测药物组合的临床结果，该模型整合结构、途径、细胞活力和转录组数据，覆盖953个临床结果和21842种化合物。MADRIGAL 采用 transformer 瓶颈模块统一多模态数据并处理缺失数据，在预测不良药物相互作用方面优于单模态方法和现有模型。实验显示，该模型成功应用于抗癌药物虚拟筛选、多药管理（如II型糖尿病和MASH）和个性化癌症治疗，并准确预测了FDA批准的MASH药物resmetirom。总体上，MADRIGAL 通过提升预测准确性和临床相关性，为设计更安全有效的组合疗法提供了新途径。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
      "published_date": "2025-03-04 16:55:14 UTC",
      "updated_date": "2025-03-04 16:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:44:54.019149"
    },
    {
      "arxiv_id": "2503.02776v1",
      "title": "Implicit Bias in LLMs: A Survey",
      "title_zh": "LLMs 中的隐式偏见：一项调查",
      "authors": [
        "Xinru Lin",
        "Luyang Li"
      ],
      "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
      "tldr_zh": "本论文对大型语言模型（LLMs）中的隐性偏见进行全面调查，强调尽管LLMs在显性偏见测试中表现良好，但隐性偏见（如无意识联想）仍可能存在，并借鉴心理学的Implicit Association Test (IAT)框架进行分析。研究将检测方法分为三大类：词联想、任务导向文本生成和决策过程；评估指标分为单值指标和比较值指标；数据集则包括带掩码标记的句子和完整句子，以覆盖多种应用领域。论文总结了现有的缓解隐性偏见努力，并指出未来挑战，旨在为研究者提供清晰指导并激发创新。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02776v1",
      "published_date": "2025-03-04 16:49:37 UTC",
      "updated_date": "2025-03-04 16:49:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:45:04.975209"
    },
    {
      "arxiv_id": "2503.02773v1",
      "title": "Prime Convolutional Model: Breaking the Ground for Theoretical Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Francesco Panelli",
        "Doaa Almhaithawi",
        "Tania Cerquitelli",
        "Alessandro Bellini"
      ],
      "abstract": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.",
      "tldr_zh": "本论文提出了一种基于科学方法的理论框架，用于 Explainable AI，通过经验证据制定数学模型来解释和预测 Neural Networks 的行为。\n作者构建了 Prime Convolutional Model (p-Conv)，这是一个卷积型神经网络，在包含前一百万自然数的数据集上训练，用于识别给定整数 $m$ 的 congruence classes，并处理每个输入的 $B$ 个连续数字序列。\n实验结果显示，p-Conv 的不同行为（如任务成功与否）可通过数学模型基于 $m$ 和 $B$ 参数进行建模，从而揭示其成功条件、失败原因以及对应的错误模式。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02773v1",
      "published_date": "2025-03-04 16:42:46 UTC",
      "updated_date": "2025-03-04 16:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:45:19.028520"
    },
    {
      "arxiv_id": "2503.02749v1",
      "title": "Improving Oil Slick Trajectory Simulations with Bayesian Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriele Accarino",
        "Marco M. De Carlo",
        "Igor Atake",
        "Donatello Elia",
        "Anusha L. Dissanayake",
        "Antonio Augusto Sepp Neves",
        "Juan Peña Ibañez",
        "Italo Epicoco",
        "Paola Nassisi",
        "Sandro Fiore",
        "Giovanni Coppini"
      ],
      "abstract": "Accurate simulations of oil spill trajectories are essential for supporting\npractitioners' response and mitigating environmental and socioeconomic impacts.\nNumerical models, such as MEDSLIK-II, simulate advection, dispersion, and\ntransformation processes of oil particles. However, simulations heavily rely on\naccurate parameter tuning, still based on expert knowledge and manual\ncalibration. To overcome these limitations, we integrate the MEDSLIK-II\nnumerical oil spill model with a Bayesian optimization framework to iteratively\nestimate the best physical parameter configuration that yields simulation\ncloser to satellite observations of the slick. We focus on key parameters, such\nas horizontal diffusivity and drift factor, maximizing the Fraction Skill Score\n(FSS) as a measure of spatio-temporal overlap between simulated and observed\noil distributions. We validate the framework for the Baniyas oil incident that\noccurred in Syria between August 23 and September 4, 2021, which released over\n12,000 $m^3$ of oil. We show that, on average, the proposed approach\nsystematically improves the FSS from 5.82% to 11.07% compared to control\nsimulations initialized with default parameters. The optimization results in\nconsistent improvement across multiple time steps, particularly during periods\nof increased drift variability, demonstrating the robustness of our method in\ndynamic environmental conditions.",
      "tldr_zh": "该研究旨在通过Bayesian Optimization优化石油泄漏轨迹模拟的准确性，以支持应对措施并减少环境影响。论文将MEDSLIK-II数值模型与Bayesian Optimization框架整合，迭代调整关键参数如水平扩散系数和漂移因子，通过最大化Fraction Skill Score (FSS)来提升模拟结果与卫星观测的空间-时间重叠。实验在2021年叙利亚Baniyas油事件中验证，显示优化后FSS平均从5.82%提高到11.07%，尤其在漂移变化期表现出显著鲁棒性。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "I.2; I.6; J.2; G.3"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "29 pages, 10 figures, 3 tables, research paper",
      "pdf_url": "http://arxiv.org/pdf/2503.02749v1",
      "published_date": "2025-03-04 16:14:16 UTC",
      "updated_date": "2025-03-04 16:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:45:30.345034"
    },
    {
      "arxiv_id": "2503.05815v1",
      "title": "Trust, Experience, and Innovation: Key Factors Shaping American Attitudes About AI",
      "title_zh": "翻译失败",
      "authors": [
        "Risa Palm",
        "Justin Kingsland",
        "Toby Bolsen"
      ],
      "abstract": "A large survey of American adults explored the complex landscape of attitudes\ntowards artificial intelligence (AI). It explored the degree of concern\nregarding specific potential outcomes of the new advances in AI technology and\ncorrelates of these concerns. Key variables associated with the direction and\nintensity of concern include prior experience using a large language model such\nas ChatGPT, general trust in science, adherence to the precautionary principle\nversus support for unrestricted innovation, and demographic factors such as\ngender. By analyzing these relationships, the paper provides valuable insights\ninto the American public's response to AI that are particularly important in\nthe development of policy to regulate or further encourage its development.",
      "tldr_zh": "这项研究通过对美国成年人的大型调查，探讨了公众对人工智能(AI)的态度及其担忧程度。调查发现，关键影响因素包括使用大型语言模型（如ChatGPT）的先前经验、对科学的总体信任、遵守 precautionary principle 与支持无限制创新的态度，以及人口统计因素如性别。这些因素与担忧的方向和强度密切相关，为制定AI监管或鼓励政策提供了宝贵洞见。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "35 pages, 3 figures, 2 tables, appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.05815v1",
      "published_date": "2025-03-04 16:08:20 UTC",
      "updated_date": "2025-03-04 16:08:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:45:41.623962"
    },
    {
      "arxiv_id": "2503.02733v1",
      "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Wang",
        "Xinfeng Zhang",
        "Gai Zhang",
        "Jun Zhu",
        "Lv Tang",
        "Li Zhang"
      ],
      "abstract": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
      "tldr_zh": "该论文提出UAR-NVC，一种统一的AutoRegressive框架，用于提升Implicit Neural Representations (INRs) 在视频压缩中的内存效率。框架通过将视频分成多个片段，每个片段使用不同的INR模型实例，并采用时间线-based的自回归建模策略，减少了训练和推理的内存消耗，同时优化了模型参数的初始化、训练和压缩。实验结果显示，UAR-NVC在资源受限环境中显著优于基线模型，支持可调节的延迟设置，从而为高效神经视频压缩提供了灵活解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
      "published_date": "2025-03-04 15:54:57 UTC",
      "updated_date": "2025-03-04 15:54:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:45:54.193044"
    },
    {
      "arxiv_id": "2503.02720v1",
      "title": "Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation Efficiency",
      "title_zh": "振动辅助滞后效应缓解以实现高补偿效率",
      "authors": [
        "Myeongbo Park",
        "Chunggil An",
        "Junhyun Park",
        "Jonghyun Kang",
        "Minho Hwang"
      ],
      "abstract": "Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.",
      "tldr_zh": "本文针对 Tendon-sheath mechanisms (TSMs) 在微创手术 (MIS) 中的滞回问题（如摩擦和肌腱伸长导致的跟踪错误），提出了一种振动辅助滞回补偿方法，通过沿肌腱运动方向施加受控振动来减轻摩擦和死区。实验结果显示，该方法显著降低了 RMSE 至 23.41%（从 2.2345 mm 到 1.7113 mm），并提升了轨迹跟踪相关性；当与 Temporal Convolutional Network (TCN) 模型结合时，MAE 进一步减少 85.2%（从 1.334 mm 到 0.1969 mm）。这一方法证实了振动能有效缓解滞回，提高补偿效率，并为 TSM-based 机器人应用提供了一个可扩展的实用解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures, and 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02720v1",
      "published_date": "2025-03-04 15:36:19 UTC",
      "updated_date": "2025-03-04 15:36:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:46:08.578115"
    },
    {
      "arxiv_id": "2503.04814v1",
      "title": "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings for Phonetic Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yiming Wang",
        "Yi Yang",
        "Jiahong Yuan"
      ],
      "abstract": "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.",
      "tldr_zh": "这篇论文探讨了通过fine-tuning预训练Transformer模型（如wav2vec 2.0）来实现phonetic normalization的过程，强调这种规范化在语音识别中的重要性，但无需作为独立步骤。研究通过分析fine-tuning后模型的embeddings，发现wav2vec 2.0能有效选择性抑制任务无关信息，从而提升语音特征的可比性。结果显示，多任务fine-tuning的模型能够保留多任务信息而不影响性能，且抑制无关信息并非分类任务的必要条件。这些发现为语音模型的设计提供了新见解，并揭示了其与人类语音感知的相似性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04814v1",
      "published_date": "2025-03-04 15:28:10 UTC",
      "updated_date": "2025-03-04 15:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:46:18.483070"
    },
    {
      "arxiv_id": "2503.13476v1",
      "title": "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Gunn",
        "Adam Hosford",
        "Daniel Mannion",
        "Jarrod Williams",
        "Varun Chhabra",
        "Victoria Nockles"
      ],
      "abstract": "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
      "tldr_zh": "这篇论文解决了雷达脉冲去交错问题，即从未知数量的发射器中分离接收到的脉冲，并定义了相应的性能指标。作者提出了一种基于 Transformer 的深度度量学习（Deep Metric Learning）方法，使用三元组损失（Triplet Loss）在合成数据上训练模型。实验结果显示，该模型的调整互信息分数（Adjusted Mutual Information Score）达到 0.882，比其他深度学习模型表现出色。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Preprint: Accepted to IEEE International Radar Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13476v1",
      "published_date": "2025-03-04 15:27:17 UTC",
      "updated_date": "2025-03-04 15:27:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:46:29.818995"
    },
    {
      "arxiv_id": "2503.02703v1",
      "title": "Generative Tools for Graphical Assets: Empirical Guidelines based on Game Designers' and Developers' Preferences",
      "title_zh": "图形资产的生成工具：基于",
      "authors": [
        "Kaisei Fukaya",
        "Damon Daylamani-Zad",
        "Harry Agius"
      ],
      "abstract": "Graphical assets play an important role in the design and development of\ngames. There is potential in the use of generative tools, to aid in creating\ngraphical assets, thus improving game design and development pipelines.\nHowever, there is little research to address how the generative methods can fit\ninto the wider pipeline. We conducted a user study with 16 game designers and\ndevelopers to examine their preferences regarding generative tools for\ngraphical assets. The findings highlight that early design stage is preferred\nby all participants (mean values above 0.67 and p < .001 for early stages).\nDesigners and developers prefer to use such tools for creating large amounts of\nvariations at the cost of quality as they can improve the quality of the\nartefacts once they generate a suitable asset (mean value 0.17 where 1 is high\nquality, p < .001). They also strongly (mean value .78, p < .001) raised the\nneed for better integration of such tools in existing design and development\nenvironments and the need for the outputs to be in common data formats, to be\nmanipulatable and integrate smoothly into existing environments (mean 3.5 out\nof 5, p = .004). The study also highlights the requirement for further emphasis\non the needs of the users to incorporate these tools effectively in existing\npipelines. Informed by these results, we provide a set of guidelines for\ncreating tools that meet the expectations and needs of game designers and\ndevelopers.",
      "tldr_zh": "这篇论文通过一项用户研究（user study）调查了16名游戏设计师和开发者的偏好，探讨了generative tools在创建graphical assets方面的应用，以改善游戏设计和开发流程。研究发现，参与者更倾向于在早期设计阶段使用这些工具（均值>0.67, p<0.001），优先生成大量变体而非高质量资产（均值0.17, p<0.001），并强烈要求更好地整合工具到现有环境中，支持常见数据格式（均值3.5/5, p=0.004）。基于这些结果，论文提供了针对generative tools的经验指导原则，帮助满足设计师和开发者的需求。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02703v1",
      "published_date": "2025-03-04 15:18:50 UTC",
      "updated_date": "2025-03-04 15:18:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:46:42.585901"
    },
    {
      "arxiv_id": "2503.02701v1",
      "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
      "title_zh": "翻译失败",
      "authors": [
        "Shuaike Li",
        "Kai Zhang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "abstract": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
      "tldr_zh": "该研究针对现有知识编辑（Knowledge Editing）方法过度拟合特定大型语言模型（LLMs）的问题，提出跨模型知识编辑的挑战，并引入MindBridge框架，以实现可扩展的知识更新。MindBridge创新性地采用记忆模态（Memory-Augmented Modality）概念，将编辑知识编码为独立模态，先进行LLMs无关的预训练，然后灵活集成到各种LLMs中。实验结果显示，MindBridge在多个LLMs和流行数据集上表现出色，能够高效编辑数万个知识条目，并轻松适应不同模型的更新。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02701v1",
      "published_date": "2025-03-04 15:17:57 UTC",
      "updated_date": "2025-03-04 15:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:46:54.409385"
    },
    {
      "arxiv_id": "2503.02691v1",
      "title": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection",
      "title_zh": "内存高效的持续学习用于基于边缘的视觉异常检测",
      "authors": [
        "Manuel Barusco",
        "Lorenzo D'Antoni",
        "Davide Dalle Pezze",
        "Francesco Borsatti",
        "Gian Antonio Susto"
      ],
      "abstract": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.",
      "tldr_zh": "这篇论文探讨了在资源受限的边缘设备上进行视觉异常检测（VAD）的持续学习（Continual Learning）问题，以应对计算和内存约束以及动态数据分布的挑战。研究者评估了 STFPM 方法及其与 Replay 技术的结合，发现其在边缘设备上表现出色；同时，他们首次探索了专为边缘设计的 PaSTe 方法。结果表明，PaSTe 不仅比 STFPM 更轻量级，还提升了异常检测性能，F1 像素分数提高 10%，并通过 Compressed Replay 技术将内存开销减少高达 91.5%。这项工作证明了在边缘设备上部署可逐步适应的 VAD 模型是可行的。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02691v1",
      "published_date": "2025-03-04 15:03:47 UTC",
      "updated_date": "2025-03-04 15:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:47:09.869029"
    },
    {
      "arxiv_id": "2503.02687v1",
      "title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?",
      "title_zh": "类别感知 PillarMix：混合样本数据增强能否提升基于雷达点云的三",
      "authors": [
        "Miao Zhang",
        "Sherif Abdulatif",
        "Benedikt Loesch",
        "Marco Altmann",
        "Bin Yang"
      ],
      "abstract": "Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.",
      "tldr_zh": "本研究探讨了混合样本数据增强 (MSDA) 是否能提升雷达点云的 3D 对象检测性能，针对现有方法在雷达数据上的适应性挑战，如不规则角度分布、多雷达设置和点稀疏性。论文提出 Class-Aware PillarMix (CAPMix)，一种新型 MSDA 方法，在支柱 (pillar) 级别应用 MixUp 混合策略，并根据类别标签为每个支柱分配独立混合比例，同时考虑不同类别的密度（如偏向于从其他样本中采样密集物体点，或保留稀疏物体点），以增加样本多样性和关键细节。实验结果显示，CAPMix 在 Bosch Street 和 K-Radar 数据集上显著提高了检测性能，优于现有 MSDA 方法，并有望激发雷达数据增强技术的进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02687v1",
      "published_date": "2025-03-04 15:02:07 UTC",
      "updated_date": "2025-03-04 15:02:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:47:23.966566"
    },
    {
      "arxiv_id": "2503.02686v1",
      "title": "Seeding for Success: Skill and Stochasticity in Tabletop Games",
      "title_zh": "翻译失败",
      "authors": [
        "James Goodman",
        "Diego Perez-Liebana",
        "Simon Lucas"
      ],
      "abstract": "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
      "tldr_zh": "这篇论文探讨了桌面游戏(tabletop games)中随机性(stochasticity)与玩家技能(skill)的互动，分析了随机元素（如骰子和洗牌）如何影响游戏体验和结果平衡。研究者引入了一种新方法来量化游戏结果的随机水平，并对15种桌面游戏进行了比较，拆解了特定游戏部分的随机贡献。结果显示，适度随机性能增强游戏趣味性，但过高随机性可能削弱玩家技能的作用，并干扰常见游戏实验中的错误分析。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in IEEE Transactions on Games, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02686v1",
      "published_date": "2025-03-04 14:58:59 UTC",
      "updated_date": "2025-03-04 14:58:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:47:34.951246"
    },
    {
      "arxiv_id": "2503.02682v1",
      "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Weimin Xiong",
        "Yifan Song",
        "Qingxiu Dong",
        "Bingchan Zhao",
        "Feifan Song",
        "Xun Wang",
        "Sujian Li"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）代理在交互式规划任务中存在的规划幻觉问题及其对新代理的重新训练需求，提出了Meta Plan Optimization (MPO)框架。该框架通过高层通用指导（meta plans）直接增强代理的规划能力，并利用代理任务执行的反馈进行持续优化，从而避免依赖复杂知识的缺点。在两个代表性任务的实验中，MPO显著优于现有基线，提升了任务完成效率和在未见过场景中的泛化能力，提供了一个即插即用的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02682v1",
      "published_date": "2025-03-04 14:54:45 UTC",
      "updated_date": "2025-03-04 14:54:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:47:46.681145"
    },
    {
      "arxiv_id": "2503.02675v1",
      "title": "State of play and future directions in industrial computer vision AI standards",
      "title_zh": "翻译失败",
      "authors": [
        "Artemis Stefanidou",
        "Panagiotis Radoglou-Grammatikis",
        "Vasileios Argyriou",
        "Panagiotis Sarigiannidis",
        "Iraklis Varlamis",
        "Georgios Th. Papadopoulos"
      ],
      "abstract": "The recent tremendous advancements in the areas of Artificial Intelligence\n(AI) and Deep Learning (DL) have also resulted into corresponding remarkable\nprogress in the field of Computer Vision (CV), showcasing robust technological\nsolutions in a wide range of application sectors of high industrial interest\n(e.g., healthcare, autonomous driving, automation, etc.). Despite the\noutstanding performance of CV systems in specific domains, their development\nand exploitation at industrial-scale necessitates, among other, the addressing\nof requirements related to the reliability, transparency, trustworthiness,\nsecurity, safety, and robustness of the developed AI models. The latter raises\nthe imperative need for the development of efficient, comprehensive and\nwidely-adopted industrial standards. In this context, this study investigates\nthe current state of play regarding the development of industrial computer\nvision AI standards, emphasizing on critical aspects, like model\ninterpretability, data quality, and regulatory compliance. In particular, a\nsystematic analysis of launched and currently developing CV standards, proposed\nby the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,\netc.) is performed. The latter is complemented by a comprehensive discussion on\nthe current challenges and future directions observed in this regularization\nendeavor.",
      "tldr_zh": "人工智能（AI）和深度学习（DL）的快速进步显著提升了计算机视觉（CV）在工业领域的应用，如医疗和自动驾驶，但这些系统面临可靠性、透明度、安全性和鲁棒性等挑战，亟需高效的工业标准。本文系统分析了主要国际标准化机构（如 ISO/IEC、IEEE 和 DIN）推出的或正在开发的 CV 标准，重点关注模型可解释性、数据质量和监管合规性。论文还讨论了当前面临的挑战和未来方向，以推动 CV 技术的可靠性和广泛采用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02675v1",
      "published_date": "2025-03-04 14:46:34 UTC",
      "updated_date": "2025-03-04 14:46:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:47:58.626918"
    },
    {
      "arxiv_id": "2503.04813v1",
      "title": "Self-Evolved Preference Optimization for Enhancing Mathematical Reasoning in Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Joykirat Singh",
        "Tanmoy Chakraborty",
        "Akshay Nambi"
      ],
      "abstract": "Large language models (LLMs) have significantly improved their reasoning\ncapabilities; however, they still struggle with complex multi-step mathematical\nproblem-solving due to error propagation, lack of self-correction, and limited\nadaptability to diverse reasoning styles. Existing methods rely on static\nfine-tuning or prompt engineering, which fail to generalize across problem\ncomplexities, while the scarcity of high-quality preference data further\nhinders reliable reasoning.\n  We introduce SPHERE, a self-evolving data generation pipeline that enhances\nreasoning in small language models (SLMs) by iteratively generating,\ncorrecting, and diversifying reasoning chains. SPHERE operates in three stages:\n(i) Self-Generation, where the model autonomously constructs problem-solving\nsteps; (ii) Self-Correction, enabling it to identify and rectify errors; and\n(iii) Diversity Induction, improving robustness through multiple valid\nreasoning trajectories. This self-evolution mechanism strengthens mathematical\nreasoning and enhances model reliability. Evaluations on MATH 500, GSM8K, AIME,\nAMC, and Olympiad show that SPHERE-trained models achieve significant gains\nover their base versions and match/surpass GPT-4o on certain benchmarks. Our\nfindings demonstrate that self-evolving models can close the reasoning gap\nbetween SLMs and state-of-the-art LLMs, making mathematical AI more reliable,\nscalable, and efficient.",
      "tldr_zh": "本文提出 SPHERE，一种自我演化的偏好优化方法，用于提升小型语言模型（SLMs）在复杂多步数学推理中的表现，解决现有方法如静态微调的泛化问题和高质量数据短缺。SPHERE 通过三个阶段运作：Self-Generation 自主生成问题解决步骤、Self-Correction 识别并修正错误，以及 Diversity Induction 通过多种推理路径增强模型鲁棒性。实验结果显示，在 MATH 500、GSM8K、AIME、AMC 和 Olympiad 等基准上，SPHERE 训练的模型比基础版本显著改进，并在某些任务中匹配或超过 GPT-4o。这些发现证明，自我演化机制能缩小 SLMs 与最先进 LLMs 之间的推理差距，使数学 AI 更可靠、可扩展和高效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04813v1",
      "published_date": "2025-03-04 14:43:25 UTC",
      "updated_date": "2025-03-04 14:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:48:12.365382"
    },
    {
      "arxiv_id": "2503.02660v2",
      "title": "A dataset-free approach for self-supervised learning of 3D reflectional symmetries",
      "title_zh": "无需数据集的自监督",
      "authors": [
        "Isaac Aguirre",
        "Ivan Sipiran",
        "Gabriel Montañana"
      ],
      "abstract": "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
      "tldr_zh": "本论文提出了一种不依赖数据集的自-supervised learning 方法，用于检测单个物体的3D reflectional symmetries，仅基于输入对象的内在特征。方法通过计算每个点的视觉描述符，利用基础图像模型提取特征，确保对称点具有相似的视觉外观，从而实现无 ground truth labels 的优化。实验结果显示，该方法优于 state-of-the-art 模型，在效率和资源利用上更具优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02660v2",
      "published_date": "2025-03-04 14:22:08 UTC",
      "updated_date": "2025-03-05 19:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:48:21.793229"
    },
    {
      "arxiv_id": "2503.02650v2",
      "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats",
      "title_zh": "大型语言模型在将非结构化文本转换为标准化格式的有效性",
      "authors": [
        "William Brach",
        "Kristián Košťál",
        "Michal Ries"
      ],
      "abstract": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
      "tldr_zh": "本研究评估了大型语言模型 (LLMs) 将无结构文本转换为标准化格式的有效性，特别针对食谱文本到 Cooklang 格式的转换，以解决数据管理中的挑战。  \n研究通过测试 GPT-4o、GPT-4o-mini、Llama3.1:70b 和 Llama3.1:8b 等模型，结合传统指标 (WER、ROUGE-L、TER) 和语义元素识别的创新评估方法，发现 GPT-4o 在少样本提示下表现出色 (ROUGE-L: 0.9722, WER: 0.0730)，证明 LLMs 无需大量训练即可可靠地处理领域特定文本。  \n此外，虽然模型性能随规模增加，但较小模型如 Llama3.1:8b 通过针对性微调显示出优化潜力，为医疗记录、技术文档等领域实现自动化结构化数据生成提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02650v2",
      "published_date": "2025-03-04 14:14:28 UTC",
      "updated_date": "2025-05-05 12:25:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:48:37.620358"
    },
    {
      "arxiv_id": "2503.02636v3",
      "title": "YARE-GAN: Yet Another Resting State EEG-GAN",
      "title_zh": "翻译失败",
      "authors": [
        "Yeganeh Farahzadi",
        "Morteza Ansarinia",
        "Zoltan Kekecs"
      ],
      "abstract": "In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP)\nto generate multi-channel resting-state EEG data and assess the quality of the\nsynthesized signals through both visual and feature-based evaluations. Our\nresults indicate that the model effectively captures the statistical and\nspectral characteristics of real EEG data, although challenges remain in\nreplicating high-frequency oscillations in the frontal region. Additionally, we\ndemonstrate that the Critic's learned representations can be reused for gender\nclassification task, achieving an out-of-sample accuracy, significantly better\nthan a shuffled-label baseline and a model trained directly on EEG data. These\nfindings suggest that generative models can serve not only as EEG data\ngenerators but also as unsupervised feature extractors, reducing the need for\nmanual feature engineering. This study highlights the potential of GAN-based\nunsupervised learning for EEG analysis, suggesting avenues for more\ndata-efficient deep learning applications in neuroscience.",
      "tldr_zh": "本研究开发了YARE-GAN，一种基于Wasserstein GAN with Gradient Penalty (WGAN-GP)的模型，用于生成多通道静息态EEG数据，并通过视觉和特征-based评估验证其质量。结果显示，该模型成功捕捉了真实EEG数据的统计和谱特征，但仍存在复制额叶区域高频振荡的挑战。同时，研究证明Critic的学得表示可重用于性别分类任务，实现了显著优于随机标签基线和直接EEG训练模型的样本外准确率。这些发现突显了GAN-based无监督学习在EEG分析中的潜力，能作为数据生成器和特征提取器，减少手动特征工程并推动神经科学的数据高效深度学习应用。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02636v3",
      "published_date": "2025-03-04 14:01:10 UTC",
      "updated_date": "2025-05-05 15:16:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:48:46.878026"
    },
    {
      "arxiv_id": "2503.02631v1",
      "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Haotian Li",
        "Yun Wang",
        "Huamin Qu"
      ],
      "abstract": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
      "tldr_zh": "本论文从人机协作视角反思生成式 AI 时代的数据叙事工具，探讨大型语言模型(LLMs)和文本到图像模型等技术如何降低专业门槛并优化工作流程。作者使用一个专用框架比较最新工具与早期工具的协作模式，识别出持续模式（如人类创建者 + AI-助手）和新兴模式（如 AI-创建者 + 人类审阅者），并揭示这些技术的益处及其对协作的启示。最终，论文提出未来研究方向，以激发数据叙事领域的创新。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements",
      "pdf_url": "http://arxiv.org/pdf/2503.02631v1",
      "published_date": "2025-03-04 13:56:18 UTC",
      "updated_date": "2025-03-04 13:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:49:00.648977"
    },
    {
      "arxiv_id": "2503.02628v1",
      "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxuan Liu",
        "Zixuan Li",
        "Long Bai",
        "Yuxin Zuo",
        "Daozhu Xu",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
      "tldr_zh": "该研究针对事件提取(Event Extraction)中处理海量事件类型面临的挑战，提出了一种基于大型语言模型(LLMs)的协作标注方法，通过多个 LLMs 协作精炼触发词和参数标注，并通过投票机制创建了最大的 EEMT 数据集，包含超过20万样本、3465个事件类型和6297个角色类型。\n为解决 LLMs 的上下文长度限制，该方法引入了 LLM-PEE 分区提取技术，先召回候选事件类型再将其分割成多个分区进行提取。\n实验结果显示，在监督设置下，LLM-PEE 比最先进方法在事件检测上提高了5.4%、在参数提取上提高了6.1%；在零样本设置下，比主流 LLMs 提高了高达12.9%，证明了其强大的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.02628v1",
      "published_date": "2025-03-04 13:53:43 UTC",
      "updated_date": "2025-03-04 13:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:49:12.815505"
    },
    {
      "arxiv_id": "2503.02623v2",
      "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Stangel",
        "David Bani-Harouni",
        "Chantal Pellegrini",
        "Ege Özsoy",
        "Kamilia Zaripova",
        "Matthias Keicher",
        "Nassir Navab"
      ],
      "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
      "tldr_zh": "本文提出了一种基于强化学习（RL）的创新方法，用于校准大型语言模型（LLMs）的信心估计，通过将问题建模为下注游戏，让模型在每个答案中预测信心分数，并设计奖励函数来惩罚过度自信和不足自信。理论证明显示，这种奖励设计能引导最优策略实现完美的信心校准。实验结果表明，该方法显著提升了LLMs的信心校准性能，并实现了对新任务的泛化，而无需重新训练，从而使LLMs具备内在的信心意识。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02623v2",
      "published_date": "2025-03-04 13:48:50 UTC",
      "updated_date": "2025-03-05 15:23:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:49:25.137526"
    },
    {
      "arxiv_id": "2503.03775v1",
      "title": "BotUmc: An Uncertainty-Aware Twitter Bot Detection with Multi-view Causal Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Tao Yang",
        "Yang Hu",
        "Feihong Lu",
        "Ziwei Zhang",
        "Qingyun Sun",
        "Jianxin Li"
      ],
      "abstract": "Social bots have become widely known by users of social platforms. To prevent\nsocial bots from spreading harmful speech, many novel bot detections are\nproposed. However, with the evolution of social bots, detection methods\nstruggle to give high-confidence answers for samples. This motivates us to\nquantify the uncertainty of the outputs, informing the confidence of the\nresults. Therefore, we propose an uncertainty-aware bot detection method to\ninform the confidence and use the uncertainty score to pick a high-confidence\ndecision from multiple views of a social network under different environments.\nSpecifically, our proposed BotUmc uses LLM to extract information from tweets.\nThen, we construct a graph based on the extracted information, the original\nuser information, and the user relationship and generate multiple views of the\ngraph by causal interference. Lastly, an uncertainty loss is used to force the\nmodel to quantify the uncertainty of results and select the result with low\nuncertainty in one view as the final decision. Extensive experiments show the\nsuperiority of our method.",
      "tldr_zh": "该研究提出了一种不确定性感知的Twitter机器人检测方法BotUmc，利用多视图因果推断（causal inference）来提升检测的置信度。方法首先使用LLM从推文中提取信息，然后基于提取的信息、用户数据和关系构建图，并通过因果推断生成多个图视图；接着，通过不确定性损失量化每个视图的结果，并选择不确定性最低的视图作为最终决策。实验结果显示，该方法在社交机器人检测任务中表现出色，优于现有基线模型。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.03775v1",
      "published_date": "2025-03-04 13:39:31 UTC",
      "updated_date": "2025-03-04 13:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:49:36.370108"
    },
    {
      "arxiv_id": "2503.02612v2",
      "title": "Reinforcement Learning-based Threat Assessment",
      "title_zh": "基于强化学习的威胁评估",
      "authors": [
        "Wuzhou Sun",
        "Siyi Li",
        "Qingxiang Zou",
        "Zixing Liao"
      ],
      "abstract": "In some game scenarios, due to the uncertainty of the number of enemy units\nand the priority of various attributes, the evaluation of the threat level of\nenemy units as well as the screening has been a challenging research topic, and\nthe core difficulty lies in how to reasonably set the priority of different\nattributes in order to achieve quantitative evaluation of the threat. In this\npaper, we innovatively transform the problem of threat assessment into a\nreinforcement learning problem, and through systematic reinforcement learning\ntraining, we successfully construct an efficient neural network evaluator. The\nevaluator can not only comprehensively integrate the multidimensional attribute\nfeatures of the enemy, but also effectively combine our state information, thus\nrealizing a more accurate and scientific threat assessment.",
      "tldr_zh": "该论文将威胁评估问题转化为强化学习问题，针对游戏场景中敌方单位数量不确定性和属性优先级复杂性的挑战，提出一种创新方法。作者通过系统化的强化学习训练，构建了一个高效的神经网络评估器，能够全面整合敌方多维属性特征并结合我方状态信息。结果显示，该评估器实现了更准确和科学的威胁评估，提高了整体评估效率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The research content is not yet complete and requires further\n  supplementation and improvement",
      "pdf_url": "http://arxiv.org/pdf/2503.02612v2",
      "published_date": "2025-03-04 13:32:40 UTC",
      "updated_date": "2025-04-25 16:48:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:49:48.145110"
    },
    {
      "arxiv_id": "2503.02918v1",
      "title": "Straight-Line Diffusion Model for Efficient 3D Molecular Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyan Ni",
        "Shikun Feng",
        "Haohan Chi",
        "Bowen Zheng",
        "Huan-ang Gao",
        "Wei-Ying Ma",
        "Zhi-Ming Ma",
        "Yanyan Lan"
      ],
      "abstract": "Diffusion-based models have shown great promise in molecular generation but\noften require a large number of sampling steps to generate valid samples. In\nthis paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle\nthis problem, by formulating the diffusion process to follow a linear\ntrajectory. The proposed process aligns well with the noise sensitivity\ncharacteristic of molecular structures and uniformly distributes reconstruction\neffort across the generative process, thus enhancing learning efficiency and\nefficacy. Consequently, SLDM achieves state-of-the-art performance on 3D\nmolecule generation benchmarks, delivering a 100-fold improvement in sampling\nefficiency. Furthermore, experiments on toy data and image generation tasks\nvalidate the generality and robustness of SLDM, showcasing its potential across\ndiverse generative modeling domains.",
      "tldr_zh": "这篇论文提出了Straight-Line Diffusion Model (SLDM)，一种高效的3D分子生成方法，通过设计线性轨迹的扩散过程来解决传统Diffusion-based models采样步骤过多的问题。SLDM与分子结构的噪声敏感特性相匹配，并均匀分布重建努力，从而提升了学习效率和生成效果。在基准测试中，SLDM在3D分子生成任务上达到了最先进性能，采样效率提高了100倍。此外，实验在玩具数据和图像生成任务上验证了SLDM的通用性和稳健性，展示了其在多种生成建模领域的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02918v1",
      "published_date": "2025-03-04 13:23:58 UTC",
      "updated_date": "2025-03-04 13:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:50:00.866205"
    },
    {
      "arxiv_id": "2503.02597v2",
      "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Wei-Yao Wang",
        "Zhao Wang",
        "Helen Suzuki",
        "Yoshiyuki Kobayashi"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https://github.com/sony/aki to encourage further advancements in\nMLLMs across various directions.",
      "tldr_zh": "本文研究了Multimodal Large Language Models (MLLMs)中存在的vision-language misalignment问题，即模型生成的文本响应与输入的图像-文本不一致。作者提出了一种新模型AKI，将传统的causal attention转化为modality-mutual attention (MMA)，允许图像标记关注文本标记，从而提升模态间的信息交互。实验结果显示，AKI在12个多模态理解基准上平均性能提高了7.2%，且无需增加参数或训练时间。该设计通用且可扩展，适用于各种多模态场景，并已开源以推动进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.02597v2",
      "published_date": "2025-03-04 13:18:33 UTC",
      "updated_date": "2025-03-13 01:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:50:13.890053"
    },
    {
      "arxiv_id": "2503.02595v1",
      "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaoxing Gan",
        "Mengtian Li",
        "Ruhua Chen",
        "Zhongxia Ji",
        "Sichen Guo",
        "Huanling Hu",
        "Guangnan Ye",
        "Zuo Hu"
      ],
      "abstract": "In this work, we introduce StageDesigner, the first comprehensive framework\nfor artistic stage generation using large language models combined with\nlayout-controlled diffusion models. Given the professional requirements of\nstage scenography, StageDesigner simulates the workflows of seasoned artists to\ngenerate immersive 3D stage scenes. Specifically, our approach is divided into\nthree primary modules: Script Analysis, which extracts thematic and spatial\ncues from input scripts; Foreground Generation, which constructs and arranges\nessential 3D objects; and Background Generation, which produces a harmonious\nbackground aligned with the narrative atmosphere and maintains spatial\ncoherence by managing occlusions between foreground and background elements.\nFurthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276\nunique stage scenes spanning different historical styles and annotated with\nscripts, images, and detailed 3D layouts, specifically tailored for this task.\nFinally, evaluations using both standard and newly proposed metrics, along with\nextensive user studies, demonstrate the effectiveness of StageDesigner. Project\ncan be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",
      "tldr_zh": "本文介绍了 StageDesigner 框架，这是首个结合 large language models 和 layout-controlled diffusion models 的系统，用于从剧本生成艺术舞台场景，模拟专业艺术家的工作流程。框架分为三个模块：Script Analysis 提取剧本中的主题和空间线索、Foreground Generation 构建并排列关键 3D 对象，以及 Background Generation 创造和谐背景并管理遮挡以保持空间连贯性。此外，研究团队构建了 StagePro-V1 数据集，包含 276 个独特舞台场景及其标注，用于支持该任务。最后，通过标准指标、新评估方法和用户研究，证明了 StageDesigner 在舞台生成方面的有效性和优越性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02595v1",
      "published_date": "2025-03-04 13:17:50 UTC",
      "updated_date": "2025-03-04 13:17:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:50:26.606222"
    },
    {
      "arxiv_id": "2503.13475v1",
      "title": "Cross-Subject Depression Level Classification Using EEG Signals with a Sample Confidence Method",
      "title_zh": "翻译失败",
      "authors": [
        "ZhongYi Zhang",
        "ChenYang Xu",
        "LiXuan Zhao",
        "HuiRang Hou",
        "QingHao Meng"
      ],
      "abstract": "Electroencephalogram (EEG) is a non-invasive tool for real-time neural\nmonitoring,widely used in depression detection via deep learning. However,\nexisting models primarily focus on binary classification (depression/normal),\nlacking granularity for severity assessment. To address this, we proposed the\nDepL-GCN, i.e., Depression Level classification based on GCN model. This model\ntackles two key challenges: (1) subjectivity in depres-sion-level labeling due\nto patient self-report biases, and (2) class imbalance across severity\ncategories. Inspired by the model learning patterns, we introduced two novel\nmodules: the sample confidence module and the minority sample penalty module.\nThe former leverages the L2-norm of prediction errors to progressively filter\nEEG samples with weak label alignment during training, thereby reducing the\nimpact of subjectivity; the latter automatically upweights misclassified\nminority-class samples to address imbalance issues. After testing on two public\nEEG datasets, DepL-GCN achieved accuracies of 81.13% and 81.36% for multi-class\nseverity recognition, outperforming baseline models.Ablation studies confirmed\nboth modules' contributions. We further discussed the strengths and limitations\nof regression-based models for depression-level recognition.",
      "tldr_zh": "本文提出 DepL-GCN 模型，利用 EEG 信号进行跨主体抑郁严重程度的多类分类，解决现有模型仅限于二分类的局限性。模型引入样本置信模块（基于 L2-norm 预测错误逐步过滤标签对齐弱的样本）和少数样本惩罚模块（自动加权错误分类的少数类样本），以应对标签主观性和类别不平衡挑战。在两个公共 EEG 数据集上，DepL-GCN 实现了 81.13% 和 81.36% 的准确率，优于基线模型。消融研究证实了新模块的贡献，并讨论了回归模型在抑郁水平识别中的优势与局限。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13475v1",
      "published_date": "2025-03-04 13:16:11 UTC",
      "updated_date": "2025-03-04 13:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:50:38.116973"
    },
    {
      "arxiv_id": "2503.02582v1",
      "title": "Playing games with Large language models: Randomness and strategy",
      "title_zh": "与大语言模型玩游戏",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "abstract": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
      "tldr_zh": "本研究探讨了大语言模型 (LLMs) 在游戏中的随机性和战略适应能力，特别测试了 GPT-4o-Mini-2024-08-17 在 Rock Paper Scissors (RPS) 和 Prisoners Dilemma (PD) 等游戏中的表现。结果显示，LLMs 在生成随机输出时存在显著偏差，且在重复游戏中表现出损失厌恶策略，导致 RPS 趋向于平局，而 PD 基于提示设计会切换合作与竞争行为。论文提供了多智能体交互的程序工具，并强调了 LLMs 在战略决策中的局限性，这对多智能体 LLM 系统的发展具有重要启示。",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02582v1",
      "published_date": "2025-03-04 13:04:48 UTC",
      "updated_date": "2025-03-04 13:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:50:51.381777"
    },
    {
      "arxiv_id": "2503.02574v1",
      "title": "LLM-Safety Evaluations Lack Robustness",
      "title_zh": "LLM-Safety 评估缺乏鲁棒性",
      "authors": [
        "Tim Beyer",
        "Sophie Xhonneux",
        "Simon Geisler",
        "Gauthier Gidel",
        "Leo Schwinn",
        "Stephan Günnemann"
      ],
      "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
      "tldr_zh": "这篇论文指出，大型语言模型(LLM)安全评估缺乏稳健性，主要由于小数据集、方法不一致和不可靠的评估设置等噪声源，导致攻击和防御的公平比较变得困难，从而减缓了研究进步。作者系统分析了LLM安全评估管道的各个阶段，包括数据集整理、自动red-teaming优化策略、响应生成以及使用LLM判断的响应评估，并在每个阶段识别关键问题及其实际影响。论文提出一套指导方针来减少评估中的噪声和偏差，同时提供现有限制的实际原因视角，强调解决这些问题将有助于生成易于比较的结果并实现可衡量的领域进步。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02574v1",
      "published_date": "2025-03-04 12:55:07 UTC",
      "updated_date": "2025-03-04 12:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:51:02.068235"
    },
    {
      "arxiv_id": "2503.02572v1",
      "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
      "title_zh": "翻译失败",
      "authors": [
        "Valerii Serpiva",
        "Artem Lykov",
        "Artyom Myshlyaev",
        "Muhammad Haris Khan",
        "Ali Alridha Abdulkarim",
        "Oleg Sautenkov",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/",
      "tldr_zh": "该论文提出 RaceVLA，一种基于 Visual-Language-Action (VLA) 的创新框架，用于实现自主竞速无人机导航，模仿人类行为决策过程。该方法通过整合高级算法和实时环境反馈，使无人机能够动态调整策略，并在收集的竞速数据集上微调以提升泛化能力。与 OpenVLA 相比，RaceVLA 在运动泛化 (75.0 vs 60.0) 和语义泛化 (45.5 vs 36.3) 上表现出色，但视觉和物理泛化略有下降；同时，它在所有指标上均优于 RT-2。实验结果显示，RaceVLA 平均速度达 1.04 m/s、最大速度 2.02 m/s，并展示了在复杂竞速环境中的鲁棒性，为高性能无人机应用提供了新潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures. Submitted to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02572v1",
      "published_date": "2025-03-04 12:54:05 UTC",
      "updated_date": "2025-03-04 12:54:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:51:15.764242"
    },
    {
      "arxiv_id": "2503.05812v1",
      "title": "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
      "title_zh": "针对人工智能的不可容忍风险阈值推荐",
      "authors": [
        "Deepika Raman",
        "Nada Madkour",
        "Evan R. Murphy",
        "Krystal Jackson",
        "Jessica Newman"
      ],
      "abstract": "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
      "tldr_zh": "该论文讨论了前沿 AI 模型可能带来的严重风险，包括对公共安全、人权、经济稳定和社会价值的威胁，这些风险可能源于故意滥用、系统故障或级联效应。作者回应 AI Seoul Summit 2024 的 Frontier AI Safety Commitments，提出了设置“不可容忍风险阈值”的关键原则和考虑因素，例如在数据有限的情况下追求“良好而非完美”的阈值。论文针对八个风险类别（如 CBRN Weapons、Cyber Attacks、Model Autonomy 等）提供了具体阈值推荐和案例研究，作为政策制定者和行业领袖的起点，以促进主动预防风险的管理策略。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "79 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05812v1",
      "published_date": "2025-03-04 12:30:37 UTC",
      "updated_date": "2025-03-04 12:30:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:51:29.775886"
    },
    {
      "arxiv_id": "2503.02552v1",
      "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Fabian Domberg",
        "Georg Schildbach"
      ],
      "abstract": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.",
      "tldr_zh": "本文提出了一种利用基于模型的强化学习（Model-Based Reinforcement Learning）中的 world models 来检测推理阶段异常的方法，以提升学习型控制器的安全性和可靠性。该方法通过持续监控 world models 的预测与实际系统行为的差异，一旦超过预设阈值即触发紧急措施，如紧急停止，且无需特定任务知识，具备通用适用性。在模拟机器人控制任务和真实四旋翼无人机实验中，该方法成功识别了机器人几何变化、重力变化和意外外力，确保了控制器在新环境下的可靠运行。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02552v1",
      "published_date": "2025-03-04 12:25:01 UTC",
      "updated_date": "2025-03-04 12:25:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:51:39.858110"
    },
    {
      "arxiv_id": "2503.02549v1",
      "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
      "title_zh": "联邦 nnU-Net 用于隐私保护的",
      "authors": [
        "Grzegorz Skorupko",
        "Fotios Avgoustidis",
        "Carlos Martín-Isla",
        "Lidia Garrucho",
        "Dimitri A. Kessler",
        "Esmeralda Ruiz Pujadas",
        "Oliver Díaz",
        "Maciej Bobowicz",
        "Katarzyna Gwoździewicz",
        "Xavier Bargalló",
        "Paulius Jaruševičius",
        "Kaisar Kushibar",
        "Karim Lekadir"
      ],
      "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
      "tldr_zh": "本文提出 FednnU-Net，一种基于联邦学习（Federated Learning）的 nnU-Net 扩展框架，旨在解决传统集中式医疗图像分割中患者隐私泄露的问题。该框架引入了两个新方法：Federated Fingerprint Extraction (FFE) 和 Asymmetric Federated Averaging (AsymFedAvg)，用于在分散式环境中训练模型。实验在6个数据集（覆盖乳房、心脏和胎儿分割，共18个机构）上验证了 FednnU-Net 的稳定性能，提升了隐私保护能力。为促进进一步研究，该插件式框架已开源，代码可在 GitHub 获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In review",
      "pdf_url": "http://arxiv.org/pdf/2503.02549v1",
      "published_date": "2025-03-04 12:20:06 UTC",
      "updated_date": "2025-03-04 12:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:51:51.115118"
    },
    {
      "arxiv_id": "2503.02917v1",
      "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting of Vision-Language Models",
      "title_zh": "可解释的少样本视网膜疾病诊断：利用概念引导提示的视觉语言模型",
      "authors": [
        "Deval Mehta",
        "Yiwen Jiang",
        "Catherine L Jan",
        "Mingguang He",
        "Kshitij Jadhav",
        "Zongyuan Ge"
      ],
      "abstract": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
      "tldr_zh": "本文提出一种可解释的少样本（few-shot）视网膜疾病诊断方法，通过概念引导的提示学习训练Vision-Language Models（VL模型），结合图像数据和GPT模型提取的视网膜疾病概念作为语言组件。相比传统方法，该方法不仅提升了疾病分类性能，还改善了few-shot和zero-shot（零样本）检测能力，提供基于概念的模型解释性。实验在两个视网膜基金图像数据集上显示，概念整合策略使16-shot学习平均精度提高约5.8%，zero-shot检测提高约2.7%。此创新为临床应用提供了更高效、可信赖的视网膜疾病识别解决方案。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to Information Processing in Medical Imaging (IPMI) 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
      "published_date": "2025-03-04 12:03:42 UTC",
      "updated_date": "2025-03-04 12:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:52:04.857923"
    },
    {
      "arxiv_id": "2503.02537v2",
      "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
      "title_zh": "翻译失败",
      "authors": [
        "Zhen Yang",
        "Guibao Shen",
        "Liang Hou",
        "Mushui Liu",
        "Luozhou Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Ying-Cong Chen"
      ],
      "abstract": "Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an straightforward and efficient solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we introduce average latent energy analysis and discover\nthat an improved classifier-free guidance hyperparameter can significantly\nenhance generation performance. Our method is entirely training-free and boasts\na simple implementation logic and efficient performance. Through extensive\ncomparisons with numerous baseline methods, our RectifiedHR demonstrates\nsuperior effectiveness and efficiency.",
      "tldr_zh": "扩散模型在生成高于训练分辨率的图像时性能下降，为此，本文提出 RectifiedHR，一种无需训练的简单高效解决方案。\nRectifiedHR 引入 noise refresh 策略，仅需几行代码即可提升高分辨率生成能力，并通过平均潜在能量分析优化 classifier-free guidance 超参数，以解决图像模糊问题。\n实验结果表明，该方法在效率和效果上显著优于众多基线方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/",
      "pdf_url": "http://arxiv.org/pdf/2503.02537v2",
      "published_date": "2025-03-04 12:03:26 UTC",
      "updated_date": "2025-03-14 13:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:52:15.878490"
    },
    {
      "arxiv_id": "2503.02512v1",
      "title": "LTL Verification of Memoryful Neural Agents",
      "title_zh": "记忆型神经代理的 LTL 验证",
      "authors": [
        "Mehran Hosseini",
        "Alessio Lomuscio",
        "Nicola Paoletti"
      ],
      "abstract": "We present a framework for verifying Memoryful Neural Multi-Agent Systems\n(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,\nagents interact with a non-deterministic, partially observable environment.\nExamples of MN-MAS include multi-agent systems based on feed-forward and\nrecurrent neural networks or state-space models. Different from previous\napproaches, we support the verification of both bounded and unbounded LTL\nspecifications. We leverage well-established bounded model checking techniques,\nincluding lasso search and invariant synthesis, to reduce the verification\nproblem to that of constraint solving. To solve these constraints, we develop\nefficient methods based on bound propagation, mixed-integer linear programming,\nand adaptive splitting. We evaluate the effectiveness of our algorithms in\nsingle and multi-agent environments from the Gymnasium and PettingZoo\nlibraries, verifying unbounded specifications for the first time and improving\nthe verification time for bounded specifications by an order of magnitude\ncompared to the SoA.",
      "tldr_zh": "本研究提出一个框架，用于验证 Memoryful Neural Multi-Agent Systems (MN-MAS) 是否符合完整的 Linear Temporal Logic (LTL) 规范，这些系统涉及代理与非确定性、部分可观察环境互动，包括基于前馈神经网络、循环神经网络或状态空间模型的多代理系统。框架支持有界和无界 LTL 规范的验证，通过 bounded model checking 技术（如 lasso search 和 invariant synthesis）将问题简化为约束求解，并采用 bound propagation、mixed-integer linear programming 和 adaptive splitting 等高效方法来处理这些约束。在 Gymnasium 和 PettingZoo 库的环境中，实验首次实现了无界规范的验证，并将有界规范的验证时间比现有最先进方法提高了几个数量级。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SC",
        "68Q60 (Primary) 68T27, 68T07, 68T37, 68T40, 68T42 (Secondary)",
        "D.2.4; F.3.1; I.2.4; I.2.11; I.2.8; F.4.1; I.2.2; I.2.3"
      ],
      "primary_category": "cs.LO",
      "comment": "11 pages, 2 figures, accepted at AAMAS 2025 conference",
      "pdf_url": "http://arxiv.org/pdf/2503.02512v1",
      "published_date": "2025-03-04 11:20:19 UTC",
      "updated_date": "2025-03-04 11:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:52:28.488829"
    },
    {
      "arxiv_id": "2503.02505v1",
      "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Shaofei Cai",
        "Zhancun Mu",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.",
      "tldr_zh": "这篇论文提出了一种 cross-view goal alignment 框架，允许用户通过自身 camera views 中的 segmentation masks 指定目标对象，从而实现语义清晰、空间敏感且直观的人机交互指导。针对 behavior cloning 在人类和代理视图差异大时无法对齐行为的问题，作者引入了 cross-view consistency loss 和 target visibility loss 作为辅助目标，以增强代理的空间推理能力。最终，开发的 ROCKET-2 代理在 Minecraft 中训练，推理效率提升 3 到 6 倍，并首次实现直接从人类视角解释目标，推动了更高效的人-代理交互。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02505v1",
      "published_date": "2025-03-04 11:16:46 UTC",
      "updated_date": "2025-03-04 11:16:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:52:41.487278"
    },
    {
      "arxiv_id": "2503.02497v2",
      "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Abdul Basit",
        "Nouhaila Innan",
        "Haider Asif",
        "Minghao Shao",
        "Muhammad Kashif",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "abstract": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning.\nHowever, their application in quantum software development remains\nunderexplored, particularly for PennyLane-a leading framework for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific quantum code samples\nand contextual descriptions, specifically curated to support LLM training and\nfine-tuning for quantum code assistance. Our contributions are threefold: (1)\nthe automatic construction and open-source release of a comprehensive PennyLane\ndataset derived from textbooks, official documentation, and open-source\nrepositories; (2) a structured methodology for data curation, annotation, and\nformatting to enhance LLM usability and relevance; and (3) a rigorous\nevaluation of code generation capabilities using both baseline\nRetrieval-Augmented Generation (RAG) and a GraphRAG-enhanced pipeline. Using\nthe PennyLang framework, we demonstrate that GraphRAG, when applied to a GPT-4o\nMini model, substantially outperforms standard prompting and baseline RAG.\nAccuracy improves from 20.5% (without RAG) to 58.2% with GraphRAG, showcasing\nits effectiveness in reducing hallucinations and improving code correctness in\nquantum programming tasks. Compared to prior efforts focused largely on Qiskit,\nour work expands LLM-based assistance to the PennyLane ecosystem, contributing\npractical tools and reproducible methodologies for advancing AI-assisted\nquantum software development.",
      "tldr_zh": "该论文引入了 PennyLang 框架，利用大型语言模型 (LLMs) 生成 PennyLane 相关的量子代码，填补了量子软件开发领域的空白。研究团队构建并开源了一个包含 3347 个 PennyLane 特定量子代码样本的数据集，并开发了结构化的数据整理、标注和格式化方法，以提升 LLM 的适用性。主要贡献包括通过 Retrieval-Augmented Generation (RAG) 和 GraphRAG 增强管道的评估，结果显示 GraphRAG 应用于 GPT-4o Mini 模型时，代码生成准确率从 20.5% 提高到 58.2%，显著减少了幻觉并改善了量子编程任务的正确性。该工作扩展了 LLM 在 PennyLane 生态中的应用，提供可重现的方法来推进 AI 辅助量子软件开发。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "quant-ph",
        "68T50 (Primary)",
        "I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 7 figures, 7 tables, submitted for review under QCE 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02497v2",
      "published_date": "2025-03-04 11:04:35 UTC",
      "updated_date": "2025-04-18 07:46:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:52:53.525967"
    },
    {
      "arxiv_id": "2503.02495v2",
      "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
      "title_zh": "专家联合：将",
      "authors": [
        "Yujiao Yang",
        "Jing Lian",
        "Linhui Li"
      ],
      "abstract": "We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.",
      "tldr_zh": "本论文提出 Union-of-Experts (UoE) 框架，将 Transformer 分解成等价的专家组，并通过分层路由在输入数据和专家上实现选择性处理，以提升模型效率。关键创新包括基于矩阵分区的专家分解应用于 MLP 和 Attention 块、两种路由范式（patch-wise 数据选择和专家选择）、新架构设计如 Selective Multi-Head Attention (SMHA) 和 Union-of-MLP-Experts (UoME)，以及优化硬件并行实现。实验结果显示，UoE 在语言建模任务中比最佳 MoE 方法减少 2.38 perplexity，同时 FLOPs 降低 76%；在 Long Range Arena 基准上成绩提高至少 0.68%；在图像分类中准确率提升 1.75%，整体超越 Full Attention、MoE 和其他高效 Transformer 模型。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T07",
        "I.5.1; I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02495v2",
      "published_date": "2025-03-04 11:01:25 UTC",
      "updated_date": "2025-03-06 08:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:53:06.645221"
    },
    {
      "arxiv_id": "2503.02484v1",
      "title": "ERetinex: Event Camera Meets Retinex Theory for Low-Light Image Enhancement",
      "title_zh": "ERetinex：事件相机结合 Retinex 理论的低光图像增强",
      "authors": [
        "Xuejian Guo",
        "Zhiqiang Tian",
        "Yuehang Wang",
        "Siqi Li",
        "Yu Jiang",
        "Shaoyi Du",
        "Yue Gao"
      ],
      "abstract": "Low-light image enhancement aims to restore the under-exposure image captured\nin dark scenarios. Under such scenarios, traditional frame-based cameras may\nfail to capture the structure and color information due to the exposure time\nlimitation. Event cameras are bio-inspired vision sensors that respond to\npixel-wise brightness changes asynchronously. Event cameras' high dynamic range\nis pivotal for visual perception in extreme low-light scenarios, surpassing\ntraditional cameras and enabling applications in challenging dark environments.\nIn this paper, inspired by the success of the retinex theory for traditional\nframe-based low-light image restoration, we introduce the first methods that\ncombine the retinex theory with event cameras and propose a novel retinex-based\nlow-light image restoration framework named ERetinex. Among our contributions,\nthe first is developing a new approach that leverages the high temporal\nresolution data from event cameras with traditional image information to\nestimate scene illumination accurately. This method outperforms traditional\nimage-only techniques, especially in low-light environments, by providing more\nprecise lighting information. Additionally, we propose an effective fusion\nstrategy that combines the high dynamic range data from event cameras with the\ncolor information of traditional images to enhance image quality. Through this\nfusion, we can generate clearer and more detail-rich images, maintaining the\nintegrity of visual information even under extreme lighting conditions. The\nexperimental results indicate that our proposed method outperforms\nstate-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while\nreducing FLOPS by \\textbf{84.28}\\%.",
      "tldr_zh": "本论文提出 ERetinex 框架，将 Event Camera 与 Retinex Theory 结合，用于低光图像增强，解决传统相机在暗环境下的结构和颜色信息捕获问题。框架首先利用 Event Camera 的高临时分辨率数据与传统图像信息，精确估计场景照明，并通过一种有效融合策略整合高动态范围数据和颜色信息，提升图像清晰度和细节。实验结果表明，ERetinex 比现有 SOTA 方法在 PSNR 上提升 1.0613 dB，同时减少 84.28% 的 FLOPS，提供更高效的低光恢复性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02484v1",
      "published_date": "2025-03-04 10:48:44 UTC",
      "updated_date": "2025-03-04 10:48:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:53:16.453759"
    },
    {
      "arxiv_id": "2503.02476v1",
      "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for Biomedical VQA",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyang Ji",
        "Shang Gao",
        "Li Liu",
        "Yifan Jia",
        "Yutao Yue"
      ],
      "abstract": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
      "tldr_zh": "该研究针对生物医学视觉问答（Biomedical VQA）中多模态语义对齐不足的问题，提出了一种新型框架BioD2C，通过双层语义一致性约束来提升模型性能。具体而言，BioD2C在特征层面使用图像-文本融合机制将文本特征整合到视觉特征中，使视觉特征能根据问题自适应，并在模型层面引入基于文本队列的跨模态软语义损失函数，进一步对齐图像和问题语义。同时，研究者构建了新数据集BioVGQ，通过过滤手动修改的图像并对齐问题-答案对，以减少现有数据集的偏差。实验结果显示，BioD2C在多个下游数据集上达到了SOTA性能，证明了其鲁棒性、泛化性和在生物医学VQA领域的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
      "published_date": "2025-03-04 10:39:42 UTC",
      "updated_date": "2025-03-04 10:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:53:28.873607"
    },
    {
      "arxiv_id": "2503.04812v1",
      "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
      "title_zh": "LLaVE：难易加权对比学习的大型语言和视觉嵌入模型",
      "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Jinsong Su"
      ],
      "abstract": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
      "tldr_zh": "本研究提出 LLaVE 框架，利用 Hardness-Weighted Contrastive Learning 来优化大型语言和视觉嵌入模型的训练，解决标准 InfoNCE 损失中正负样本相似度重叠的问题，从而更好地区分难负样本。LLaVE 通过动态改进负样本的表示学习，并在 MMEB 基准（涵盖 4 个元任务和 36 个数据集）上进行评估，实现了 SOTA 性能，同时展示了强大的可扩展性和效率。具体而言，LLaVE-2B 超过了之前的 7B 模型，而 LLaVE-7B 进一步提升了 6.2 分。虽然仅在图像-文本数据上训练，LLaVE 仍能零样本方式泛化到文本-视频检索任务，并取得出色性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.04812v1",
      "published_date": "2025-03-04 10:21:57 UTC",
      "updated_date": "2025-03-04 10:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:53:43.116191"
    },
    {
      "arxiv_id": "2503.05810v2",
      "title": "A Transformer Model for Predicting Chemical Reaction Products from Generic Templates",
      "title_zh": "翻译失败",
      "authors": [
        "Derin Ozer",
        "Sylvain Lamprier",
        "Thomas Cauchy",
        "Nicolas Gutowski",
        "Benoit Da Mota"
      ],
      "abstract": "The accurate prediction of chemical reaction outcomes is a major challenge in\ncomputational chemistry. Current models rely heavily on either highly specific\nreaction templates or template-free methods, both of which present limitations.\nTo address these limitations, this work proposes the Broad Reaction Set (BRS),\na dataset featuring 20 generic reaction templates that allow for the efficient\nexploration of the chemical space. Additionally, ProPreT5 is introduced, a T5\nmodel tailored to chemistry that achieves a balance between rigid templates and\ntemplate-free methods. ProPreT5 demonstrates its capability to generate\naccurate, valid, and realistic reaction products, making it a promising\nsolution that goes beyond the current state-of-the-art on the complex reaction\nproduct prediction task.",
      "tldr_zh": "这篇论文针对化学反应产物预测的挑战，提出了 Broad Reaction Set (BRS) 数据集，该数据集包含 20 个通用反应模板，用于高效探索化学空间。同时，引入了 ProPreT5，一种基于 T5 的 Transformer 模型，它平衡了刚性模板和无模板方法的局限性。ProPreT5 能够生成准确、有效且真实的反应产物，在复杂反应预测任务上超越了现有技术。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05810v2",
      "published_date": "2025-03-04 10:18:32 UTC",
      "updated_date": "2025-03-11 08:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:53:54.005138"
    },
    {
      "arxiv_id": "2503.03774v2",
      "title": "Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenmin Huang",
        "Ce Hao",
        "Wei Zhan",
        "Jun Ma",
        "Masayoshi Tomizuka"
      ],
      "abstract": "Autonomous racing has gained significant attention as a platform for\nhigh-speed decision-making and motion control. While existing methods primarily\nfocus on trajectory planning and overtaking strategies, the role of\nsportsmanship in ensuring fair competition remains largely unexplored. In human\nracing, rules such as the one-motion rule and the enough-space rule prevent\ndangerous and unsportsmanlike behavior. However, autonomous racing systems\noften lack mechanisms to enforce these principles, potentially leading to\nunsafe maneuvers. This paper introduces a bi-level game-theoretic framework to\nintegrate sportsmanship (SPS) into versus racing. At the high level, we model\nracing intentions using a Stackelberg game, where Monte Carlo Tree Search\n(MCTS) is employed to derive optimal strategies. At the low level, vehicle\ninteractions are formulated as a Generalized Nash Equilibrium Problem (GNEP),\nensuring that all agents follow sportsmanship constraints while optimizing\ntheir trajectories. Simulation results demonstrate the effectiveness of the\nproposed approach in enforcing sportsmanship rules while maintaining\ncompetitive performance. We analyze different scenarios where attackers and\ndefenders adhere to or disregard sportsmanship rules and show how knowledge of\nthese constraints influences strategic decision-making. This work highlights\nthe importance of balancing competition and fairness in autonomous racing and\nprovides a foundation for developing ethical and safe AI-driven racing systems.",
      "tldr_zh": "这篇论文探讨了自主赛车系统中整合体育精神（SPS）的必要性，以防止危险行为并确保公平竞争。作者提出一个双层博弈理论框架：高层使用 Stackelberg game 和 Monte Carlo Tree Search (MCTS) 来优化赛车意图策略；低层则通过 Generalized Nash Equilibrium Problem (GNEP) 建模车辆互动，确保所有代理在遵守体育精神约束的同时优化轨迹。模拟结果显示，该框架在多种场景中有效提升了安全性，同时维持竞争性能，并强调了平衡竞争与公平的重要性，为道德和安全的 AI 驱动赛车系统奠定基础。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03774v2",
      "published_date": "2025-03-04 10:14:19 UTC",
      "updated_date": "2025-03-12 17:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:54:06.439119"
    },
    {
      "arxiv_id": "2503.02457v1",
      "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Gino Franco Fazzi",
        "Julie Skoven Hinge",
        "Stefan Heinrich",
        "Paolo Burelli"
      ],
      "abstract": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
      "tldr_zh": "这篇论文探讨了大型语言模型 (LLMs) 在控制情感表达方面的挑战，特别是维持适当的情感状态（如 arousal 和 valence）在多轮对话中的表现。研究采用了一种新颖方法，结合 LLM 基于的情感分析和多轮对话模拟，来量化模型的情感谱宽度及其在互动中的波动。结果显示，不同 LLMs 在保持一致情感轨迹上存在显著差异，有些模型更稳定，但普遍面临产生极端情感和适应对话上下文的困难。这些发现为开发更情感智能的 AI 系统提供了重要启示，并突出了改进 LLMs 情感建模的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02457v1",
      "published_date": "2025-03-04 10:06:41 UTC",
      "updated_date": "2025-03-04 10:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:54:16.866049"
    },
    {
      "arxiv_id": "2503.02453v1",
      "title": "Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhao Yang",
        "Zhi Ji",
        "Zhaopeng Li",
        "Yi Li",
        "Zhonglin Mo",
        "Yue Ding",
        "Kai Chen",
        "Zijian Zhang",
        "Jie Li",
        "Shuanglong Li",
        "Lin Liu"
      ],
      "abstract": "Generative models have recently gained attention in recommendation systems by\ndirectly predicting item identifiers from user interaction sequences. However,\nexisting methods suffer from significant information loss due to the separation\nof stages such as quantization and sequence modeling, hindering their ability\nto achieve the modeling precision and accuracy of sequential dense retrieval\ntechniques. Integrating generative and dense retrieval methods remains a\ncritical challenge. To address this, we introduce the Cascaded Organized\nBi-Represented generAtive retrieval (COBRA) framework, which innovatively\nintegrates sparse semantic IDs and dense vectors through a cascading process.\nOur method alternates between generating these representations by first\ngenerating sparse IDs, which serve as conditions to aid in the generation of\ndense vectors. End-to-end training enables dynamic refinement of dense\nrepresentations, capturing both semantic insights and collaborative signals\nfrom user-item interactions. During inference, COBRA employs a coarse-to-fine\nstrategy, starting with sparse ID generation and refining them into dense\nvectors via the generative model. We further propose BeamFusion, an innovative\napproach combining beam search with nearest neighbor scores to enhance\ninference flexibility and recommendation diversity. Extensive experiments on\npublic datasets and offline tests validate our method's robustness. Online A/B\ntests on a real-world advertising platform with over 200 million daily users\ndemonstrate substantial improvements in key metrics, highlighting COBRA's\npractical advantages.",
      "tldr_zh": "该论文提出 COBRA 框架，用于统一生成式推荐系统，通过级联稀疏-密集表示（sparse IDs 和 dense vectors）解决现有方法因量化等阶段分离导致的信息损失问题。COBRA 的方法先生成稀疏语义 IDs 作为条件，然后辅助生成密集向量，实现端到端训练以捕捉语义洞见和用户-物品协作信号。实验结果显示，该框架在公共数据集和真实广告平台上的在线 A/B 测试中显著提升关键指标，包括推荐准确性和多样性，进一步通过 BeamFusion 技术结合 beam search 和最近邻分数增强推理灵活性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02453v1",
      "published_date": "2025-03-04 10:00:05 UTC",
      "updated_date": "2025-03-04 10:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:54:28.036093"
    },
    {
      "arxiv_id": "2503.02420v1",
      "title": "Exploring Model Quantization in GenAI-based Image Inpainting and Detection of Arable Plants",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Modak",
        "Ahmet Oğuz Saltık",
        "Anthony Stein"
      ],
      "abstract": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
      "tldr_zh": "该研究探讨了在生成式AI辅助的图像修复和耕地植物检测中应用模型量化，以解决深度学习杂草控制系统的训练数据多样性和计算资源限制问题。研究提出一个框架，利用Stable Diffusion-based inpainting逐步增量（10%递增，直至额外200%）增强训练数据，从而提高样本数量和多样性。在YOLO11(l)和RT-DETR(l)模型上，通过mAP50指标评估检测性能，并探索FP16和INT8量化策略，以平衡推理速度和准确性。实验结果显示，该框架在Jetson Orin Nano等资源受限环境中部署时，显著提升了检测准确性和计算效率，为智能杂草管理系统提供了实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
      "published_date": "2025-03-04 09:05:01 UTC",
      "updated_date": "2025-03-04 09:05:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:54:40.906309"
    },
    {
      "arxiv_id": "2503.11684v1",
      "title": "Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching",
      "title_zh": "探索 HRI 的因果性：机器人心理健康辅导的案例研究",
      "authors": [
        "Micol Spitale",
        "Srikar Babu",
        "Serhan Cakmak",
        "Jiaee Cheong",
        "Hatice Gunes"
      ],
      "abstract": "One of the primary goals of Human-Robot Interaction (HRI) research is to\ndevelop robots that can interpret human behavior and adapt their responses\naccordingly. Adaptive learning models, such as continual and reinforcement\nlearning, play a crucial role in improving robots' ability to interact\neffectively in real-world settings. However, these models face significant\nchallenges due to the limited availability of real-world data, particularly in\nsensitive domains like healthcare and well-being. This data scarcity can hinder\na robot's ability to adapt to new situations. To address these challenges,\ncausality provides a structured framework for understanding and modeling the\nunderlying relationships between actions, events, and outcomes. By moving\nbeyond mere pattern recognition, causality enables robots to make more\nexplainable and generalizable decisions. This paper presents an exploratory\ncausality-based analysis through a case study of an adaptive robotic coach\ndelivering positive psychology exercises over four weeks in a workplace\nsetting. The robotic coach autonomously adapts to multimodal human behaviors,\nsuch as facial valence and speech duration. By conducting both macro- and\nmicro-level causal analyses, this study aims to gain deeper insights into how\nadaptability can enhance well-being during interactions. Ultimately, this\nresearch seeks to advance our understanding of how causality can help overcome\nchallenges in HRI, particularly in real-world applications.",
      "tldr_zh": "本研究探讨了因果关系(causality)在人机交互(HRI)中的应用，通过一个机器人心理健康辅导的案例研究，旨在提升机器人对人类行为的适应性。研究指出，现有适应性学习模型如持续学习和强化学习因数据稀缺而面临挑战，因此引入因果框架来建模动作、事件和结果之间的关系，使机器人决策更具可解释性和泛化性。在案例中，一个自主适应机器人教练在工作场所提供为期四周的积极心理学练习，基于多模态行为（如面部表情和语音时长）进行适应，并通过宏观和微观因果分析揭示了这种适应如何改善互动中的福祉，最终为HRI在真实世界应用的挑战提供新见解。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11684v1",
      "published_date": "2025-03-04 08:56:47 UTC",
      "updated_date": "2025-03-04 08:56:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:54:53.154305"
    },
    {
      "arxiv_id": "2503.02403v1",
      "title": "AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Jiahui Sun",
        "Zhichao Hua",
        "Yubin Xia"
      ],
      "abstract": "Accurate and systematic evaluation of mobile agents can significantly advance\ntheir development and real-world applicability. However, existing benchmarks\nfor mobile agents lack practicality and scalability due to the extensive manual\neffort required to define task reward signals and implement corresponding\nevaluation codes. To this end, we propose AutoEval, an autonomous agent\nevaluation framework that tests a mobile agent without any manual effort.\nFirst, we design a Structured Substate Representation to describe the UI state\nchanges while agent execution, such that task reward signals can be\nautomatically generated. Second, we utilize a Judge System that can\nautonomously evaluate agents' performance given the automatically generated\ntask reward signals. By providing only a task description, our framework\nevaluates agents with fine-grained performance feedback to that task without\nany extra manual effort. We implement a prototype of our framework and validate\nthe automatically generated task reward signals, finding over 93% coverage to\nhuman-annotated reward signals. Moreover, to prove the effectiveness of our\nautonomous Judge System, we manually verify its judge results and demonstrate\nthat it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile\nagents using our framework, providing detailed insights into their performance\ncharacteristics and limitations.",
      "tldr_zh": "该研究提出 AutoEval，一种实用的自主评估框架，用于评估移动代理（mobile agents），旨在解决现有基准测试因手动定义任务奖励信号和实现评估代码而缺乏实用性和可扩展性的问题。该框架通过 Structured Substate Representation 自动生成任务奖励信号，并利用 Judge System 基于这些信号提供细粒度的性能反馈，仅需任务描述即可完成评估。实验结果显示，自动生成的奖励信号覆盖率超过93%，Judge System 的准确率达到94%。最终，使用 AutoEval 评估了最先进的移动代理，揭示了它们的性能特点和局限性，为代理开发提供了宝贵洞见。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02403v1",
      "published_date": "2025-03-04 08:44:30 UTC",
      "updated_date": "2025-03-04 08:44:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:55:05.039139"
    },
    {
      "arxiv_id": "2503.02399v1",
      "title": "VisAgent: Narrative-Preserving Story Visualization Framework",
      "title_zh": "VisAgent：叙事保留的故事可视化框架",
      "authors": [
        "Seungkwon Kim",
        "GyuTae Park",
        "Sangyeon Kim",
        "Seung-Hun Nam"
      ],
      "abstract": "Story visualization is the transformation of narrative elements into image\nsequences. While existing research has primarily focused on visual contextual\ncoherence, the deeper narrative essence of stories often remains overlooked.\nThis limitation hinders the practical application of these approaches, as\ngenerated images frequently fail to capture the intended meaning and nuances of\nthe narrative fully. To address these challenges, we propose VisAgent, a\ntraining-free multi-agent framework designed to comprehend and visualize\npivotal scenes within a given story. By considering story distillation,\nsemantic consistency, and contextual coherence, VisAgent employs an agentic\nworkflow. In this workflow, multiple specialized agents collaborate to: (i)\nrefine layered prompts based on the narrative structure and (ii) seamlessly\nintegrate \\gt{generated} elements, including refined prompts, scene elements,\nand subject placement, into the final image. The empirically validated\neffectiveness confirms the framework's suitability for practical story\nvisualization applications.",
      "tldr_zh": "本论文提出 VisAgent，一种无训练的多智能体框架，旨在解决现有故事可视化方法忽略叙事本质的问题，导致生成的图像无法充分捕捉叙事的含义和细微差别。VisAgent 通过 agentic workflow 让多个专门代理协作：(i) 基于叙事结构提炼分层提示，以及 (ii) 整合 story distillation、semantic consistency 和 contextual coherence 等元素，以生成更精确的图像序列。实证验证显示，该框架在故事可视化应用中表现出色，提升了叙事保留和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICASSP 2025. Equal contribution from first two authors",
      "pdf_url": "http://arxiv.org/pdf/2503.02399v1",
      "published_date": "2025-03-04 08:41:45 UTC",
      "updated_date": "2025-03-04 08:41:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:55:18.351682"
    },
    {
      "arxiv_id": "2503.02398v1",
      "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence",
      "title_zh": "翻译失败",
      "authors": [
        "Yunxiao Shi",
        "Wujiang Xu",
        "Zeqi Zhang",
        "Xing Zi",
        "Qiang Wu",
        "Min Xu"
      ],
      "abstract": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
      "tldr_zh": "该论文提出PersonaX，一种面向长行为序列的代理无关用户建模框架，旨在解决现有LLM用户建模方法在处理长用户生成内容(UGC)时面临的上下文限制和性能下降问题，通过子行为序列(SBS)选择和离线多角色构建来捕捉用户多样兴趣。PersonaX在离线阶段提取紧凑的SBS段（长度小于5），平衡典型性和多样性，并生成可缓存的细粒度文本角色，以实现高效在线检索，避免实时计算带来的延迟。实验结果显示，PersonaX仅使用30-50%的行为数据（序列长度480），与AgentCF整合可提升3-11%的性能，与Agent4Rec整合可提升10-50%，为可扩展的LLM驱动推荐代理设定新基准。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "draft paper",
      "pdf_url": "http://arxiv.org/pdf/2503.02398v1",
      "published_date": "2025-03-04 08:41:40 UTC",
      "updated_date": "2025-03-04 08:41:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:55:31.930349"
    },
    {
      "arxiv_id": "2503.02397v1",
      "title": "A Binary Classification Social Network Dataset for Graph Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Adnan Ali",
        "Jinglong Li",
        "Huanhuan Chen",
        "AlMotasem Bellah Al Ajlouni"
      ],
      "abstract": "Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.",
      "tldr_zh": "这篇论文引入了BiSND（Binary Classification Social Network Dataset），一个专为图机器学习设计的二元分类社交网络基准数据集，以填补现有数据集类别过多（如3-15类）的空白。数据集以表格和图格式提供，并使用多种分类器进行评估，包括传统算法（Decision Trees、K-Nearest Neighbour、Random Forest、XGBoost）、深度神经网络（multi-layer perceptrons）、Graph Neural Network（Graph Convolutional Network）以及先进的Graph Contrastive Learning方法（BGRL、GRACE、DAENS）。实验结果显示，F1-scores从67.66到70.15不等，证明BiSND适合分类任务，并为图机器学习的未来改进提供了有前景的途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02397v1",
      "published_date": "2025-03-04 08:40:42 UTC",
      "updated_date": "2025-03-04 08:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:55:44.035402"
    },
    {
      "arxiv_id": "2503.02382v1",
      "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning",
      "title_zh": "一个高效且精确的训练数据构建框架，用于数学推理中的过程监督奖励模型",
      "authors": [
        "Wei Sun",
        "Qianlong Du",
        "Fuwei Cui",
        "Jiajun Zhang"
      ],
      "abstract": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",
      "tldr_zh": "本研究提出EpicPRM框架，用于高效精确地构建过程监督奖励模型(PRMs)的训练数据，以提升大型语言模型(LLMs)的数学推理能力。该框架通过量化每个中间推理步骤的贡献，并采用自适应二分搜索算法，提高数据注释的精度和效率，从而构建了高质量的Epic50k数据集（包含50k注释步骤）。与现有公开数据集相比，在PRMs训练上，Epic50k展示了显著优越性能，可从GitHub获取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02382v1",
      "published_date": "2025-03-04 08:18:46 UTC",
      "updated_date": "2025-03-04 08:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:55:55.312069"
    },
    {
      "arxiv_id": "2503.02913v1",
      "title": "Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Zilin Zhao",
        "Chishui Chen",
        "Haotian Shi",
        "Jiale Chen",
        "Xuanlin Yue",
        "Zhejian Yang",
        "Yang Liu"
      ],
      "abstract": "Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction.",
      "tldr_zh": "本研究针对多无人机（Multi-UAV）在嘈杂环境中的路径规划挑战，提出了一种基于多智能体强化学习（MARL）框架，利用 Counterfactual Multi-Agent Policy Gradients (COMA) 算法结合注意力机制的通信协议和训练-部署系统，以提升通信鲁棒性和决策能力。框架通过注意力机制优化 UAV 间的协作通信，确保在 3D 空间复杂任务中的高效执行。实验结果显示，该方法在合成和真实数据集上优于现有算法，路径规划效率和鲁棒性显著提升，实现 78% 的熵减少改善。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02913v1",
      "published_date": "2025-03-04 08:05:14 UTC",
      "updated_date": "2025-03-04 08:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:56:07.432232"
    },
    {
      "arxiv_id": "2503.02369v1",
      "title": "JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for Agricultural Vehicle Routing Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Yixuan Fan",
        "Haotian Xu",
        "Mengqiao Liu",
        "Qing Zhuo",
        "Tao Zhang"
      ],
      "abstract": "The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the\nVehicle Routing Problem (VRP) where the scale of cities influences routing\noutcomes, necessitating consideration of their entrances. This paper addresses\nEDVRP in agriculture, focusing on multi-parameter vehicle planning for\nirregularly shaped fields. To address the limitations of traditional methods,\nsuch as heuristic approaches, which often overlook field geometry and entrance\nconstraints, we propose a Joint Probability Distribution Sampling Neural\nNetwork (JPDS-NN) to effectively solve the EDVRP. The network uses an\nencoder-decoder architecture with graph transformers and attention mechanisms\nto model routing as a Markov Decision Process, and is trained via reinforcement\nlearning for efficient and rapid end-to-end planning. Experimental results\nindicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel\nconsumption by 14.0-17.6%, and computes two orders of magnitude faster than\nbaseline methods, while demonstrating 15-25% superior performance in dynamic\narrangement scenarios. Ablation studies validate the necessity of\ncross-attention and pre-training. The framework enables scalable, intelligent\nrouting for large-scale farming under dynamic constraints.",
      "tldr_zh": "本论文针对农业中的Entrance Dependent Vehicle Routing Problem (EDVRP)——一种考虑入口和城市规模的Vehicle Routing Problem (VRP)变体——提出JPDS-NN框架，利用强化学习实现动态任务分配和车辆路径优化，以处理不规则田地多参数规划的挑战。JPDS-NN采用编码器-解码器架构，结合图变换器和注意力机制，将路由建模为Markov Decision Process，并通过强化学习进行端到端训练，提升规划效率。实验结果显示，该方法将旅行距离减少48.4-65.4%、燃料消耗降低14.0-17.6%，计算速度比基线方法快两个数量级，并在动态场景下性能提升15-25%，为大规模农业提供可扩展的智能路由解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures, submitted to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02369v1",
      "published_date": "2025-03-04 07:50:32 UTC",
      "updated_date": "2025-03-04 07:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:56:19.081230"
    },
    {
      "arxiv_id": "2503.02368v2",
      "title": "Iterative Value Function Optimization for Guided Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenhua Liu",
        "Lijun Li",
        "Ruizhe Chen",
        "Yuxian Jiang",
        "Tong Zhu",
        "Zhaochen Su",
        "Wenliang Chen",
        "Jing Shao"
      ],
      "abstract": "While Reinforcement Learning from Human Feedback (RLHF) has become the\npredominant method for controlling language model outputs, it suffers from high\ncomputational costs and training instability. Guided decoding, especially\nvalue-guided methods, offers a cost-effective alternative by controlling\noutputs without re-training models. However, the accuracy of the value function\nis crucial for value-guided decoding, as inaccuracies can lead to suboptimal\ndecision-making and degraded performance. Existing methods struggle with\naccurately estimating the optimal value function, leading to less effective\ncontrol. We propose Iterative Value Function Optimization, a novel framework\nthat addresses these limitations through two key components: Monte Carlo Value\nEstimation, which reduces estimation variance by exploring diverse\ntrajectories, and Iterative On-Policy Optimization, which progressively\nimproves value estimation through collecting trajectories from value-guided\npolicies. Extensive experiments on text summarization, multi-turn dialogue, and\ninstruction following demonstrate the effectiveness of value-guided decoding\napproaches in aligning language models. These approaches not only achieve\nalignment but also significantly reduce computational costs by leveraging\nprincipled value function optimization for efficient and effective control.",
      "tldr_zh": "该论文针对 Reinforcement Learning from Human Feedback (RLHF) 的高计算成本和训练不稳定性问题，提出了一种基于 Guided Decoding 的替代框架：Iterative Value Function Optimization。该框架包括两个关键组件——Monte Carlo Value Estimation，用于通过探索多样轨迹减少估计方差，以及Iterative On-Policy Optimization，用于通过从Value-Guided Policies中收集轨迹逐步提升价值函数的准确性。实验在文本摘要、多轮对话和指令遵循任务上验证了该方法的有效性，不仅实现了语言模型的对齐，还显著降低了计算成本。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02368v2",
      "published_date": "2025-03-04 07:49:10 UTC",
      "updated_date": "2025-03-05 09:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:56:31.541236"
    },
    {
      "arxiv_id": "2503.02365v2",
      "title": "EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram Reports",
      "title_zh": "EchoQA：超声心动图报告的指令调优数据大型集合",
      "authors": [
        "Lama Moukheiber",
        "Mira Moukheiber",
        "Dana Moukheiiber",
        "Jae-Woo Ju",
        "Hyung-Chul Lee"
      ],
      "abstract": "We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.",
      "tldr_zh": "本研究引入了EchoQA数据集，这是一个基于Medical Information Mart for Intensive Care (MIMIC)数据库的心脏超声报告QA数据集，包含771,244个QA对，涵盖各种心脏异常及其严重程度。研究者比较了多种大型语言模型(LLMs)，包括开源和生物医学特定模型，进行零样本(zero-shot)评估，以及闭源模型的零样本和三样本(three-shot)评估，结果显示微调LLMs显著提高了QA性能指标。临床医生对最佳模型进行了定性评估，并通过细粒度公平性审计评估了LLMs在社会健康决定因素上的偏见-性能权衡，以推动LLM AI代理在心脏鉴别诊断中的应用，减轻临床医生的文书负担并提升患者护理效率。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS SafeGenAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.02365v2",
      "published_date": "2025-03-04 07:45:45 UTC",
      "updated_date": "2025-03-06 03:29:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:56:43.406805"
    },
    {
      "arxiv_id": "2503.04809v2",
      "title": "PanguIR Technical Report for NTCIR-18 AEOLLM Task",
      "title_zh": "翻译失败",
      "authors": [
        "Lang Mei",
        "Chong Chen",
        "Jiaxin Mao"
      ],
      "abstract": "As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task.",
      "tldr_zh": "这篇论文针对大型语言模型 (LLMs) 的评估挑战，介绍了 NTCIR-18 AEOLLM 任务，该任务旨在推动无参考评估方法，以克服手动评估的资源密集和自动评估的参考依赖问题。论文提出三种关键改进方法：Multi-model Collaboration，利用多个 LLMs 来近似人类评分；Prompt Auto-optimization，通过 LLMs 基于反馈迭代优化任务提示；以及 In-context Learning (ICL) Optimization，训练一个结合语义相关性的模型来检索最有效的上下文例子。实验结果表明，该方法在 AEOLLM 任务的最终数据集上实现了优越性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04809v2",
      "published_date": "2025-03-04 07:40:02 UTC",
      "updated_date": "2025-03-10 06:49:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:56:56.019771"
    },
    {
      "arxiv_id": "2503.02360v1",
      "title": "BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)",
      "title_zh": "翻译失败",
      "authors": [
        "Husne Ara Rubaiyeat",
        "Njayou Youssouf",
        "Md Kamrul Hasan",
        "Hasan Mahmud"
      ],
      "abstract": "Sign language recognition (SLR) for low-resource languages like Bangla\nsuffers from signer variability, viewpoint variations, and limited annotated\ndatasets. In this paper, we present BdSLW401, a large-scale, multi-view,\nword-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video\nsamples from 18 signers in front and lateral views. To improve\ntransformer-based SLR, we introduce Relative Quantization Encoding (RQE), a\nstructured embedding approach anchoring landmarks to physiological reference\npoints and quantize motion trajectories. RQE improves attention allocation by\ndecreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,\n21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,\nfixed quantization becomes insufficient on large-scale datasets (e.g.,\nWLASL2000), indicating the need for adaptive encoding strategies. Further,\nRQE-SF, an extended variant that stabilizes shoulder landmarks, achieves\nimprovements in pose consistency at the cost of small trade-offs in lateral\nview recognition. The attention graphs prove that RQE improves model\ninterpretability by focusing on the major articulatory features (fingers,\nwrists) and the more distinctive frames instead of global pose changes.\nIntroducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced\nstructured embeddings, this work advances transformer-based SLR for\nlow-resource languages and sets a benchmark for future research in this area.",
      "tldr_zh": "本研究引入了 BdSLW401，这是一个大规模、多视图的词级 Bangla Sign Language (BdSL) 数据集，包含 401 个手语符号和 102,176 个视频样本，旨在解决低资源语言手语识别中的数据不足问题。作者提出了 Relative Quantization Encoding (RQE)，一种结构化嵌入方法，通过将地标锚定到生理参考点并量化运动轨迹，减少空间变异性并提升 Transformer 模型的注意力分配。实验结果显示，RQE 在 WLASL100 上降低了 44.3% 的 WER，在 SignBD-200 上降低了 21.0%，并在 BdSLW60 和 SignBD-90 上取得了显著提升，同时提高了模型的可解释性，关注主要的手语特征如手指和手腕。扩展版本 RQE-SF 通过稳定肩部地标改善了姿势一致性，但在大规模数据集如 WLASL2000 上，固定量化策略显示不足，建议采用自适应编码策略以推动未来手语识别研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02360v1",
      "published_date": "2025-03-04 07:34:06 UTC",
      "updated_date": "2025-03-04 07:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:57:08.986513"
    },
    {
      "arxiv_id": "2503.02358v1",
      "title": "Are Large Vision Language Models Good Game Players?",
      "title_zh": "大型视觉语言模型是好的游戏玩家吗？",
      "authors": [
        "Xinyu Wang",
        "Bohan Zhuang",
        "Qi Wu"
      ],
      "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
      "tldr_zh": "大型视觉语言模型(Large Vision Language Models, LVLMs) 在处理视觉和文本信息方面表现出色，但现有评估基准如Visual Question Answering 和图像描述存在局限性，包括对详细视觉感知的不足、数据污染和缺乏多轮推理评估。  \n本文提出\\method{}框架，这是一个基于游戏的评估系统，用于全面评估LVLMs 在四个核心任务上的能力：Perceiving（感知）、Question Answering（问答）、Rule Following（规则遵循）和End-to-End Playing（端到端游戏）。  \n通过广泛实验，研究揭示了LVLMs 的局限性，如处理长结构化输出和感知密集元素的能力不足，并公开了代码和数据以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
      "published_date": "2025-03-04 07:29:03 UTC",
      "updated_date": "2025-03-04 07:29:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:57:20.813895"
    },
    {
      "arxiv_id": "2503.02354v2",
      "title": "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Jiashun Suo",
        "Xiaojian Liao",
        "Limin Xiao",
        "Li Ruan",
        "Jinquan Wang",
        "Xiao Su",
        "Zhisheng Huo"
      ],
      "abstract": "Large language models like GPT-4 are resource-intensive, but recent\nadvancements suggest that smaller, specialized experts can outperform the\nmonolithic models on specific tasks. The Collaboration-of-Experts (CoE)\napproach integrates multiple expert models, improving the accuracy of generated\nresults and offering great potential for precision-critical applications, such\nas automatic circuit board quality inspection. However, deploying CoE serving\nsystems presents challenges to memory capacity due to the large number of\nexperts required, which can lead to significant performance overhead from\nfrequent expert switching across different memory and storage tiers.\n  We propose CoServe, an efficient CoE model serving system on heterogeneous\nCPU and GPU with limited memory. CoServe reduces unnecessary expert switching\nby leveraging expert dependency, a key property of CoE inference. CoServe\nintroduces a dependency-aware request scheduler and dependency-aware expert\nmanagement for efficient inference. It also introduces an offline profiler to\nautomatically find optimal resource allocation on various processors and\ndevices. In real-world intelligent manufacturing workloads, CoServe achieves\n4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art\nsystems.",
      "tldr_zh": "该研究针对 Collaboration-of-Experts (CoE) 模型在内存有限的异构 CPU 和 GPU 环境中面临的专家切换开销问题，提出 CoServe 系统，以提高模型推理效率。CoServe 利用专家依赖性引入依赖感知请求调度器和专家管理机制，并通过离线分析器自动优化资源分配，从而减少不必要切换。实验结果显示，在真实智能制造工作负载中，CoServe 的吞吐量比现有系统提升 4.5 到 12 倍，为精密应用如电路板质量检查提供高效解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems (ASPLOS\n  '25)",
      "pdf_url": "http://arxiv.org/pdf/2503.02354v2",
      "published_date": "2025-03-04 07:25:05 UTC",
      "updated_date": "2025-04-10 04:58:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:57:35.594506"
    },
    {
      "arxiv_id": "2503.02351v1",
      "title": "MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI",
      "title_zh": "翻译失败",
      "authors": [
        "Guangyin Bao",
        "Qi Zhang",
        "Zixuan Gong",
        "Zhuojia Wu",
        "Duoqian Miao"
      ],
      "abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
      "tldr_zh": "本研究提出MindSimulator，一种数据驱动的方法，通过合成FMRI数据来探索大脑的概念选择性区域（concept-selective regions），以克服传统实验方法中视觉刺激的局限性和偏见问题。该框架利用高级生成技术学习脑活动与概念导向视觉刺激的概率分布，从而生成模拟的脑活动记录，并成功定位了几种已知概念选择性区域，与经验数据相比实现了高预测准确率。MindSimulator的创新为发现新型概念选择性区域提供新途径，并为未来神经科学研究生成有价值的假设。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "23 pages, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02351v1",
      "published_date": "2025-03-04 07:20:42 UTC",
      "updated_date": "2025-03-04 07:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:57:44.798384"
    },
    {
      "arxiv_id": "2503.02911v1",
      "title": "Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving Test",
      "title_zh": "Text2Scenario：文本驱动的自动驾驶测试场景生成",
      "authors": [
        "Xuan Cai",
        "Xuesong Bai",
        "Zhiyong Cui",
        "Danmu Xie",
        "Daocheng Fu",
        "Haiyang Yu",
        "Yilong Ren"
      ],
      "abstract": "Autonomous driving (AD) testing constitutes a critical methodology for\nassessing performance benchmarks prior to product deployment. The creation of\nsegmented scenarios within a simulated environment is acknowledged as a robust\nand effective strategy; however, the process of tailoring these scenarios often\nnecessitates laborious and time-consuming manual efforts, thereby hindering the\ndevelopment and implementation of AD technologies. In response to this\nchallenge, we introduce Text2Scenario, a framework that leverages a Large\nLanguage Model (LLM) to autonomously generate simulation test scenarios that\nclosely align with user specifications, derived from their natural language\ninputs. Specifically, an LLM, equipped with a meticulously engineered input\nprompt scheme functions as a text parser for test scenario descriptions,\nextracting from a hierarchically organized scenario repository the components\nthat most accurately reflect the user's preferences. Subsequently, by\nexploiting the precedence of scenario components, the process involves\nsequentially matching and linking scenario representations within a Domain\nSpecific Language corpus, ultimately fabricating executable test scenarios. The\nexperimental results demonstrate that such prompt engineering can meticulously\nextract the nuanced details of scenario elements embedded within various\ndescriptive formats, with the majority of generated scenarios aligning closely\nwith the user's initial expectations, allowing for the efficient and precise\nevaluation of diverse AD stacks void of the labor-intensive need for manual\nscenario configuration. Project page:\nhttps://caixxuan.github.io/Text2Scenario.GitHub.io.",
      "tldr_zh": "本文提出 Text2Scenario 框架，利用 Large Language Model (LLM) 从用户自然语言输入中自动生成自动驾驶 (AD) 测试场景，解决传统手动创建模拟场景的低效问题。该框架通过精心设计的提示方案，从分层组织的场景库中提取组件，并基于组件优先级匹配和链接 Domain Specific Language (DSL) 语料，构建可执行的测试场景。实验结果表明，该方法能准确提取场景细节，大多数生成场景符合用户期望，从而高效评估 AD 系统性能，无需繁琐的手动配置。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02911v1",
      "published_date": "2025-03-04 07:20:25 UTC",
      "updated_date": "2025-03-04 07:20:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:57:58.616971"
    },
    {
      "arxiv_id": "2504.03654v1",
      "title": "PointSplit: Towards On-device 3D Object Detection with Heterogeneous Low-power Accelerators",
      "title_zh": "翻译失败",
      "authors": [
        "Keondo Park",
        "You Rim Choi",
        "Inhoe Lee",
        "Hyung-Sin Kim"
      ],
      "abstract": "Running deep learning models on resource-constrained edge devices has drawn\nsignificant attention due to its fast response, privacy preservation, and\nrobust operation regardless of Internet connectivity. While these devices\nalready cope with various intelligent tasks, the latest edge devices that are\nequipped with multiple types of low-power accelerators (i.e., both mobile GPU\nand NPU) can bring another opportunity; a task that used to be too heavy for an\nedge device in the single-accelerator world might become viable in the upcoming\nheterogeneous-accelerator world.To realize the potential in the context of 3D\nobject detection, we identify several technical challenges and propose\nPointSplit, a novel 3D object detection framework for multi-accelerator edge\ndevices that addresses the problems. Specifically, our PointSplit design\nincludes (1) 2D semantics-aware biased point sampling, (2) parallelized 3D\nfeature extraction, and (3) role-based group-wise quantization. We implement\nPointSplit on TensorFlow Lite and evaluate it on a customized hardware platform\ncomprising both mobile GPU and EdgeTPU. Experimental results on representative\nRGB-D datasets, SUN RGB-D and Scannet V2, demonstrate that PointSplit on a\nmulti-accelerator device is 24.7 times faster with similar accuracy compared to\nthe full-precision, 2D-3D fusion-based 3D detector on a GPU-only device.",
      "tldr_zh": "该论文提出 PointSplit 框架，旨在在配备异构低功耗加速器（如移动 GPU 和 NPU）的边缘设备上实现高效的 3D object detection，解决资源受限环境下的计算挑战。框架的关键设计包括 2D semantics-aware biased point sampling、parallelized 3D feature extraction 和 role-based group-wise quantization，以优化任务分配和性能。实验结果显示，在 SUN RGB-D 和 Scannet V2 数据集上，PointSplit 在多加速器设备上比仅 GPU 的全精度 2D-3D 融合检测器快 24.7 倍，同时保持相似的准确率。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.03654v1",
      "published_date": "2025-03-04 07:17:04 UTC",
      "updated_date": "2025-03-04 07:17:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:58:10.634210"
    },
    {
      "arxiv_id": "2503.02345v1",
      "title": "CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented 3D MRI",
      "title_zh": "翻译失败",
      "authors": [
        "Mominul Islam",
        "Mohammad Junayed Hasan",
        "M. R. C. Mahdy"
      ],
      "abstract": "The detection of Alzheimer disease (AD) from clinical MRI data is an active\narea of research in medical imaging. Recent advances in quantum computing,\nparticularly the integration of parameterized quantum circuits (PQCs) with\nclassical machine learning architectures, offer new opportunities to develop\nmodels that may outperform traditional methods. However, quantum machine\nlearning (QML) remains in its early stages and requires further experimental\nanalysis to better understand its behavior and limitations. In this paper, we\npropose an end to end hybrid classical quantum convolutional neural network (CQ\nCNN) for AD detection using clinically formatted 3D MRI data. Our approach\ninvolves developing a framework to make 3D MRI data usable for machine\nlearning, designing and training a brain tissue segmentation model (Skull Net),\nand training a diffusion model to generate synthetic images for the minority\nclass. Our converged models exhibit potential quantum advantages, achieving\nhigher accuracy in fewer epochs than classical models. The proposed beta8 3\nqubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)\nmodels while requiring significantly fewer computational resources. In\nparticular, the architecture employs only 13K parameters (0.48 MB), reducing\nthe parameter count by more than 99.99% compared to current SOTA models.\nFurthermore, the diffusion-generated data used to train our quantum models, in\nconjunction with real samples, preserve clinical structural standards,\nrepresenting a notable first in the field of QML. We conclude that CQCNN\narchitecture like models, with further improvements in gradient optimization\ntechniques, could become a viable option and even a potential alternative to\nclassical models for AD detection, especially in data limited and resource\nconstrained clinical settings.",
      "tldr_zh": "该研究提出了一种混合经典量子卷积神经网络（CQ CNN），用于从扩散生成和 U-Net 分割的 3D MRI 数据中检测阿尔茨海默病（AD）。该框架整合参数化量子电路（PQCs）与经典机器学习架构，开发了脑组织分割模型（Skull Net）和扩散模型来生成合成图像，从而处理数据不平衡问题。实验结果显示，beta8 3-qubit 模型实现了 97.50% 的准确率，比现有最先进（SOTA）模型高，且仅使用 13K 参数（0.48 MB），参数量减少超过 99.99%。这种量子机器学习（QML）方法展示了潜在的量子优势，尤其适用于数据有限和资源受限的临床环境。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Application of hybrid quantum-classical machine learning for (early\n  stage) disease detection",
      "pdf_url": "http://arxiv.org/pdf/2503.02345v1",
      "published_date": "2025-03-04 07:08:47 UTC",
      "updated_date": "2025-03-04 07:08:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:58:22.217244"
    },
    {
      "arxiv_id": "2503.13473v1",
      "title": "Robust Detection of Extremely Thin Lines Using 0.2mm Piano Wire",
      "title_zh": "翻译失败",
      "authors": [
        "Jisoo Hong",
        "Youngjin Jung",
        "Jihwan Bae",
        "Seungho Song",
        "Sung-Woo Kang"
      ],
      "abstract": "This study developed an algorithm capable of detecting a reference line (a\n0.2 mm thick piano wire) to accurately determine the position of an automated\ninstallation robot within an elevator shaft. A total of 3,245 images were\ncollected from the experimental tower of H Company, the leading elevator\nmanufacturer in South Korea, and the detection performance was evaluated using\nfour experimental approaches (GCH, GSCH, GECH, FCH). During the initial image\nprocessing stage, Gaussian blurring, sharpening filter, embossing filter, and\nFourier Transform were applied, followed by Canny Edge Detection and Hough\nTransform. Notably, the method was developed to accurately extract the\nreference line by averaging the x-coordinates of the lines detected through the\nHough Transform. This approach enabled the detection of the 0.2 mm thick piano\nwire with high accuracy, even in the presence of noise and other interfering\nfactors (e.g., concrete cracks inside the elevator shaft or safety bars for\nfilming equipment). The experimental results showed that Experiment 4 (FCH),\nwhich utilized Fourier Transform in the preprocessing stage, achieved the\nhighest detection rate for the LtoL, LtoR, and RtoL datasets. Experiment\n2(GSCH), which applied Gaussian blurring and a sharpening filter, demonstrated\nsuperior detection performance on the RtoR dataset. This study proposes a\nreference line detection algorithm that enables precise position calculation\nand control of automated robots in elevator shaft installation. Moreover, the\ndeveloped method shows potential for applicability even in confined working\nspaces. Future work aims to develop a line detection algorithm equipped with\nmachine learning-based hyperparameter tuning capabilities.",
      "tldr_zh": "这篇论文开发了一种鲁棒算法，用于检测0.2 mm厚的钢琴线作为参考线，以精确确定电梯井中自动安装机器人的位置。方法包括图像预处理（如Gaussian blurring、sharpening filter、embossing filter和Fourier Transform）、Canny Edge Detection和Hough Transform，通过平均检测线的x坐标实现高精度提取，即使在噪声干扰（如混凝土裂缝或安全栏）下也能有效。实验结果显示，四种方法（GCH、GSCH、GECH、FCH）中，FCH在LtoL、LtoR和RtoL数据集上检测率最高，而GSCH在RtoR数据集上表现最佳，为电梯安装机器人的精确控制提供可靠方案，并计划未来整合机器学习进行超参数调优。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13473v1",
      "published_date": "2025-03-04 07:05:33 UTC",
      "updated_date": "2025-03-04 07:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:58:35.145291"
    },
    {
      "arxiv_id": "2503.02341v1",
      "title": "GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhun Mou",
        "Bin Xia",
        "Zhengchao Huang",
        "Wenming Yang",
        "Jiaya Jia"
      ],
      "abstract": "Recent great advances in video generation models have demonstrated their\npotential to produce high-quality videos, bringing challenges to effective\nevaluation. Unlike human evaluation, existing automated evaluation metrics lack\nhigh-level semantic understanding and reasoning capabilities for video, thus\nmaking them infeasible and unexplainable. To fill this gap, we curate\nGRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset,\nincluding 3.3k videos from over 10 existing video generation models and\nmulti-step reasoning assessments converted by 16k human annotations. We then\nintroduce GRADEO, one of the first specifically designed video evaluation\nmodels, which grades AI-generated videos for explainable scores and assessments\nthrough multi-step reasoning. Experiments show that our method aligns better\nwith human evaluations than existing methods. Furthermore, our benchmarking\nreveals that current video generation models struggle to produce content that\naligns with human reasoning and complex real-world scenarios. The models,\ndatasets, and codes will be released soon.",
      "tldr_zh": "本文提出 GRADEO，一种通过多步推理的框架，旨在实现更接近人类的 Text-to-Video (T2V) 生成模型评估，以解决现有自动指标缺乏高级语义理解的问题。他们构建了 GRADEO-Instruct 数据集，包含 3.3k 视频和 16k 人类标注，用于多维度评估训练。实验结果显示，GRADEO 与人类评估更一致，并揭示当前视频生成模型在处理人类推理和复杂真实场景方面存在显著挑战。模型、数据集和代码即将发布。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02341v1",
      "published_date": "2025-03-04 07:04:55 UTC",
      "updated_date": "2025-03-04 07:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:58:49.393143"
    },
    {
      "arxiv_id": "2503.02338v1",
      "title": "Enhancing the Product Quality of the Injection Process Using eXplainable Artificial Intelligence",
      "title_zh": "利用可解释人工智能提升注射过程的产品质量",
      "authors": [
        "Jisoo Hong",
        "Yongmin Hong",
        "Jung-Woo Baek",
        "Sung-Woo Kang"
      ],
      "abstract": "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
      "tldr_zh": "这篇论文提出了一种利用 eXplainable Artificial Intelligence (XAI) 优化注塑成型过程的控制系统，旨在降低产品缺陷率。研究采用 XGBoost 和 LightGBM 作为树状分类器来预测产品是否正常，并通过 SHapley Additive exPlanations (SHAP) 提取关键过程特征，同时使用 Individual Conditional Expectation (ICE) 分析这些特征的优化控制范围。实验基于 KAMP 提供的实际数据集，结果显示缺陷率从 1.00% 降至 XGBoost 的 0.21% 和 LightGBM 的 0.13%，证明了该方法的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02338v1",
      "published_date": "2025-03-04 06:59:01 UTC",
      "updated_date": "2025-03-04 06:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:58:57.915477"
    },
    {
      "arxiv_id": "2503.02334v1",
      "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sonnet Xu",
        "Joseph Janizek",
        "Yixing Jiang",
        "Roxana Daneshjou"
      ],
      "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
      "tldr_zh": "本研究探讨了视觉语言模型（VLMs）在使用 in-context learning (ICL) 时的人口统计偏见问题，特别是在医疗诊断任务中的表现差异。\n通过分析皮肤病变恶性预测和气胸检测两个任务，研究发现 ICL 影响模型预测的机制包括：从提示中学习子群体特定的疾病基线率，以及即使控制基线率后仍导致不同人口群体间的预测表现不均。\n实验结果提供了 VLMs 提示的最佳实践建议，如匹配标签基线率到目标分布（整体和子群内），并为深化对这些模型的理论理解提出了未来方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
      "published_date": "2025-03-04 06:45:54 UTC",
      "updated_date": "2025-03-04 06:45:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:59:10.353322"
    },
    {
      "arxiv_id": "2503.02333v1",
      "title": "Examining the Mental Health Impact of Misinformation on Social Media Using a Hybrid Transformer-Based Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Sarvesh Arora",
        "Sarthak Arora",
        "Deepika Kumar",
        "Vallari Agrawal",
        "Vedika Gupta",
        "Dipit Vasdev"
      ],
      "abstract": "Social media has significantly reshaped interpersonal communication,\nfostering connectivity while also enabling the proliferation of misinformation.\nThe unchecked spread of false narratives has profound effects on mental health,\ncontributing to increased stress, anxiety, and misinformation-driven paranoia.\nThis study presents a hybrid transformer-based approach using a RoBERTa-LSTM\nclassifier to detect misinformation, assess its impact on mental health, and\nclassify disorders linked to misinformation exposure. The proposed models\ndemonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,\nmental health implications, and disorder classification, respectively.\nFurthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)\nvalidates the direct correlation between misinformation and deteriorating\nmental well-being. This study underscores the urgent need for better\nmisinformation management strategies to mitigate its psychological\nrepercussions. Future research could explore broader datasets incorporating\nlinguistic, demographic, and cultural variables to deepen the understanding of\nmisinformation-induced mental health distress.",
      "tldr_zh": "本研究探讨了社交媒体错误信息（misinformation）对心理健康的影响，提出了一种混合 Transformer-based 方法，使用 RoBERTa-LSTM 分类器来检测错误信息、评估其对心理健康的影响（如增加压力、焦虑和偏执），并分类相关心理障碍。模型在检测错误信息、心理健康影响和障碍分类方面的准确率分别为98.4%、87.8%和77.3%。通过 Pearson's Chi-Squared Test（p-value = 0.003871），研究证实了错误信息与心理健康恶化之间的直接相关性，强调了加强错误信息管理策略的紧迫性。未来工作可扩展到更广泛的数据集，包括语言、人口统计和文化变量，以加深对这一问题的理解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02333v1",
      "published_date": "2025-03-04 06:45:17 UTC",
      "updated_date": "2025-03-04 06:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:59:21.796504"
    },
    {
      "arxiv_id": "2503.02324v1",
      "title": "PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Jian Guan",
        "Lingpeng Kong"
      ],
      "abstract": "The ability of large language models to solve complex mathematical problems\nhas progressed significantly, particularly for tasks requiring advanced\nreasoning. However, the scarcity of sufficiently challenging problems,\nparticularly at the Olympiad level, hinders further advancements. In this work,\nwe introduce PromptCoT, a novel approach for automatically generating\nhigh-quality Olympiad-level math problems. The proposed method synthesizes\ncomplex problems based on mathematical concepts and the rationale behind\nproblem construction, emulating the thought processes of experienced problem\ndesigners. We provide a theoretical analysis demonstrating that an optimal\nrationale should maximize both the likelihood of rationale generation given the\nassociated concepts and the likelihood of problem generation conditioned on\nboth the rationale and the concepts. Our method is evaluated on standard\nbenchmarks including GSM8K, MATH-500, and AIME2024, where it consistently\noutperforms existing problem generation methods. Furthermore, we demonstrate\nthat PromptCoT exhibits superior data scalability, consistently maintaining\nhigh performance as the dataset size increases, outperforming the baselines.\nThe implementation is available at https://github.com/zhaoxlpku/PromptCoT.",
      "tldr_zh": "该论文提出 PromptCoT，一种创新方法，用于自动合成奥林匹克级数学问题，以提升 Large Language Models 在复杂数学推理任务中的能力。PromptCoT 通过模拟经验问题设计者的思维过程，基于数学概念和问题构建的推理生成高质量问题，并提供理论分析，证明最优推理应最大化生成概率。实验结果显示，该方法在 GSM8K、MATH-500 和 AIME2024 等基准上优于现有方法，并在数据规模增加时保持高性能。总之，PromptCoT 增强了模型的训练数据多样性，并为进一步提升数学推理奠定基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.02324v1",
      "published_date": "2025-03-04 06:32:30 UTC",
      "updated_date": "2025-03-04 06:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:59:35.572189"
    },
    {
      "arxiv_id": "2503.16474v1",
      "title": "From Voices to Worlds: Developing an AI-Powered Framework for 3D Object Generation in Augmented Reality",
      "title_zh": "从声音到",
      "authors": [
        "Majid Behravan",
        "Denis Gracanin"
      ],
      "abstract": "This paper presents Matrix, an advanced AI-powered framework designed for\nreal-time 3D object generation in Augmented Reality (AR) environments. By\nintegrating a cutting-edge text-to-3D generative AI model, multilingual\nspeech-to-text translation, and large language models (LLMs), the system\nenables seamless user interactions through spoken commands. The framework\nprocesses speech inputs, generates 3D objects, and provides object\nrecommendations based on contextual understanding, enhancing AR experiences. A\nkey feature of this framework is its ability to optimize 3D models by reducing\nmesh complexity, resulting in significantly smaller file sizes and faster\nprocessing on resource-constrained AR devices. Our approach addresses the\nchallenges of high GPU usage, large model output sizes, and real-time system\nresponsiveness, ensuring a smoother user experience. Moreover, the system is\nequipped with a pre-generated object repository, further reducing GPU load and\nimproving efficiency. We demonstrate the practical applications of this\nframework in various fields such as education, design, and accessibility, and\ndiscuss future enhancements including image-to-3D conversion, environmental\nobject detection, and multimodal support. The open-source nature of the\nframework promotes ongoing innovation and its utility across diverse\nindustries.",
      "tldr_zh": "这篇论文介绍了 Matrix 框架，一个基于 AI 的系统，用于在 Augmented Reality (AR) 环境中通过语音命令实现实时 3D 对象生成。该框架整合了 text-to-3D 生成模型、多语言 speech-to-text 翻译和 large language models (LLMs)，以处理语音输入、生成 3D 对象并提供基于上下文的对象推荐，同时通过减少网格复杂度优化模型，显著降低文件大小和 GPU 使用。实验结果显示，该系统提高了处理效率和用户体验，适用于教育、设计和可访问性等领域；作为开源框架，它还促进未来创新，如 image-to-3D 转换和环境对象检测。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "arXiv admin note: text overlap with arXiv:2502.15869",
      "pdf_url": "http://arxiv.org/pdf/2503.16474v1",
      "published_date": "2025-03-04 06:31:51 UTC",
      "updated_date": "2025-03-04 06:31:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:59:47.730798"
    },
    {
      "arxiv_id": "2503.02318v1",
      "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
      "title_zh": "Audio-Reasoner：提升大型音频语言模型的推理能力",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio\nmodality. We introduce Audio-Reasoner, a large-scale audio language model for\ndeep reasoning in audio tasks. We meticulously curated a large-scale and\ndiverse multi-task audio dataset with simple annotations. Then, we leverage\nclosed-source models to conduct secondary labeling, QA generation, along with\nstructured COT process. These datasets together form a high-quality reasoning\ndataset with 1.2 million reasoning-rich samples, which we name CoTA. Following\ninference scaling principles, we train Audio-Reasoner on CoTA, enabling it to\nachieve great logical capabilities in audio reasoning. Experiments show\nstate-of-the-art performance across key benchmarks, including MMAU-mini\n(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our\nfindings stress the core of structured CoT training in advancing audio\nreasoning.",
      "tldr_zh": "该论文引入 Audio-Reasoner，一种大型音频语言模型，旨在提升音频任务中的深度推理能力，以弥补多模态推理中音频领域的不足。研究者构建了 CoTA 数据集，通过闭源模型进行二次标注、QA 生成和结构化 Chain-of-Thought (CoT) 过程，创建了包含120万高质量推理样本的多任务音频数据集。实验结果显示，Audio-Reasoner 在MMAU-mini (+25.42%)、AIR-Bench chat/foundation (+14.57%/+10.13%) 和MELD (+8.01%) 等基准上实现了最先进性能，强调了结构化 CoT 训练在音频推理中的核心作用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Technical report, in process",
      "pdf_url": "http://arxiv.org/pdf/2503.02318v1",
      "published_date": "2025-03-04 06:18:34 UTC",
      "updated_date": "2025-03-04 06:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T21:59:58.606313"
    },
    {
      "arxiv_id": "2503.02910v3",
      "title": "LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Wenqi Guo",
        "Yiyang Du",
        "Shan Du"
      ],
      "abstract": "Gas leakage poses a significant hazard that requires prevention.\nTraditionally, human inspection has been used for detection, a slow and\nlabour-intensive process. Recent research has applied machine learning\ntechniques to this problem, yet there remains a shortage of high-quality,\npublicly available datasets. This paper introduces a synthetic dataset, SimGas,\nfeaturing diverse backgrounds, interfering foreground objects, diverse leak\nlocations, and precise segmentation ground truth. We propose a zero-shot method\nthat combines background subtraction, zero-shot object detection, filtering,\nand segmentation to leverage this dataset. Experimental results indicate that\nour approach significantly outperforms baseline methods based solely on\nbackground subtraction and zero-shot object detection with segmentation,\nreaching an IoU of 69%. We also present an analysis of various prompt\nconfigurations and threshold settings to provide deeper insights into the\nperformance of our method. Finally, we qualitatively (because of the lack of\nground truth) tested our performance on GasVid and reached decent results on\nthe real-world dataset. The dataset, code, and full qualitative results are\navailable at https://github.com/weathon/Lang-Gas.",
      "tldr_zh": "本论文针对气体泄漏检测的挑战，引入了一个新的合成数据集 SimGas，该数据集包含多样背景、干扰物体、泄漏位置和精确分割标注，以解决现有数据集短缺问题。\n我们提出了一种零-shot 方法，结合 background subtraction、zero-shot 对象检测、过滤和 segmentation，并引入语言提示进行选择性检测，提升了对半透明气体的检测准确性。\n实验结果显示，该方法显著优于基于 background subtraction 或 zero-shot 对象检测的基线，IoU 达到 69%，并在真实数据集 GasVid 上实现了良好的定性性能。\n此外，通过分析不同提示配置和阈值设置，我们提供了对方法性能的深入洞见。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02910v3",
      "published_date": "2025-03-04 06:17:17 UTC",
      "updated_date": "2025-04-15 01:35:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:00:11.313981"
    },
    {
      "arxiv_id": "2503.05808v1",
      "title": "DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shenyu Zhang",
        "Jiaguo Tian",
        "Zhengbang Zhu",
        "Shan Huang",
        "Jucheng Yang",
        "Weinan Zhang"
      ],
      "abstract": "Microscopic traffic simulation has become an important tool for autonomous\ndriving training and testing. Although recent data-driven approaches advance\nrealistic behavior generation, their learning still relies primarily on a\nsingle real-world dataset, which limits their diversity and thereby hinders\ndownstream algorithm optimization. In this paper, we propose DriveGen, a novel\ntraffic simulation framework with large models for more diverse traffic\ngeneration that supports further customized designs. DriveGen consists of two\ninternal stages: the initialization stage uses large language model and\nretrieval technique to generate map and vehicle assets; the rollout stage\noutputs trajectories with selected waypoint goals from visual language model\nand a specific designed diffusion planner. Through this two-staged process,\nDriveGen fully utilizes large models' high-level cognition and reasoning of\ndriving behavior, obtaining greater diversity beyond datasets while maintaining\nhigh realism. To support effective downstream optimization, we additionally\ndevelop DriveGen-CS, an automatic corner case generation pipeline that uses\nfailures of the driving algorithm as additional prompt knowledge for large\nmodels without the need for retraining or fine-tuning. Experiments show that\nour generated scenarios and corner cases have a superior performance compared\nto state-of-the-art baselines. Downstream experiments further verify that the\nsynthesized traffic of DriveGen provides better optimization of the performance\nof typical driving algorithms, demonstrating the effectiveness of our\nframework.",
      "tldr_zh": "本文提出DriveGen框架，利用Large Language Models和视觉语言模型，生成无限多样化的交通场景，以解决传统数据驱动模拟依赖单一数据集的局限性。框架分为两个阶段：初始化阶段使用Large Language Model和检索技术创建地图和车辆资产；展开阶段则通过Visual Language Model和设计的Diffusion Planner输出轨迹，确保场景的多样性和真实性。此外，DriveGen-CS模块自动生成角落案例，利用驾驶算法的失败作为提示知识，无需重新训练。实验结果显示，DriveGen生成的场景在多样性和下游优化性能上优于现有基线模型。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05808v1",
      "published_date": "2025-03-04 06:14:21 UTC",
      "updated_date": "2025-03-04 06:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:00:22.768342"
    },
    {
      "arxiv_id": "2503.02311v1",
      "title": "Target Return Optimizer for Multi-Game Decision Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Kensuke Tatematsu",
        "Akifumi Wachi"
      ],
      "abstract": "Achieving autonomous agents with robust generalization capabilities across\ndiverse games and tasks remains one of the ultimate goals in AI research.\nRecent advancements in transformer-based offline reinforcement learning,\nexemplified by the MultiGame Decision Transformer [Lee et al., 2022], have\nshown remarkable performance across various games or tasks. However, these\napproaches depend heavily on human expertise, presenting substantial challenges\nfor practical deployment, particularly in scenarios with limited prior\ngame-specific knowledge. In this paper, we propose an algorithm called\nMulti-Game Target Return Optimizer (MTRO) to autonomously determine\ngame-specific target returns within the Multi-Game Decision Transformer\nframework using solely offline datasets. MTRO addresses the existing\nlimitations by automating the target return configuration process, leveraging\nenvironmental reward information extracted from offline datasets. Notably, MTRO\ndoes not require additional training, enabling seamless integration into\nexisting Multi-Game Decision Transformer architectures. Our experimental\nevaluations on Atari games demonstrate that MTRO enhances the performance of RL\npolicies across a wide array of games, underscoring its potential to advance\nthe field of autonomous agent development.",
      "tldr_zh": "该论文针对强化学习（RL）代理在多种游戏中的泛化能力问题，提出了一种名为 Multi-Game Target Return Optimizer (MTRO) 的算法，用于 Multi-Game Decision Transformer 框架。MTRO 通过利用离线数据集中的环境奖励信息，自主确定游戏特定的目标回报，从而避免了对人类专家的依赖，并无需额外训练。实验结果显示，在 Atari 游戏上，MTRO 显著提升了 RL 策略的性能，推动了自主代理开发的进步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02311v1",
      "published_date": "2025-03-04 06:13:53 UTC",
      "updated_date": "2025-03-04 06:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:00:34.359083"
    },
    {
      "arxiv_id": "2503.02303v2",
      "title": "Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization",
      "title_zh": "灵活的前额叶对海马体情景记忆的控制用于目标导向的泛化",
      "authors": [
        "Yicong Zheng",
        "Nora Wolf",
        "Charan Ranganath",
        "Randall C. O'Reilly",
        "Kevin L. McKee"
      ],
      "abstract": "Many tasks require flexibly modifying perception and behavior based on\ncurrent goals. Humans can retrieve episodic memories from days to years ago,\nusing them to contextualize and generalize behaviors across novel but\nstructurally related situations. The brain's ability to control episodic\nmemories based on task demands is often attributed to interactions between the\nprefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement\nlearning model that incorporates a PFC-HPC interaction mechanism for\ngoal-directed generalization. In our model, the PFC learns to generate\nquery-key representations to encode and retrieve goal-relevant episodic\nmemories, modulating HPC memories top-down based on current task demands.\nMoreover, the PFC adapts its encoding and retrieval strategies dynamically when\nfaced with multiple goals presented in a blocked, rather than interleaved,\nmanner. Our results show that: (1) combining working memory with selectively\nretrieved episodic memory allows transfer of decisions among similar\nenvironments or situations, (2) top-down control from PFC over HPC improves\nlearning of arbitrary structural associations between events for generalization\nto novel environments compared to a bottom-up sensory-driven approach, and (3)\nthe PFC encodes generalizable representations during both encoding and\nretrieval of goal-relevant memories, whereas the HPC exhibits event-specific\nrepresentations. Together, these findings highlight the importance of\ngoal-directed prefrontal control over hippocampal episodic memory for\ndecision-making in novel situations and suggest a computational mechanism by\nwhich PFC-HPC interactions enable flexible behavior.",
      "tldr_zh": "这篇论文提出一个 reinforcement learning 模型，模拟 prefrontal cortex (PFC) 与 hippocampus (HPC) 的互动机制，以实现目标导向的情节记忆(episodic memory)泛化。模型中，PFC 通过生成 query-key representations 来编码和检索与任务相关的 episodic memories，并根据当前目标需求进行 top-down 调节。研究发现，这种结合 working memory 和选择性记忆检索的方法，能促进决策在类似环境中的转移，并比 bottom-up 方式更有效地学习事件间的结构关联。总体上，该模型突出了 PFC 在控制 HPC 记忆方面的作用，为理解灵活行为决策提供了计算机制。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02303v2",
      "published_date": "2025-03-04 06:04:54 UTC",
      "updated_date": "2025-05-18 17:37:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:00:48.076516"
    },
    {
      "arxiv_id": "2503.02296v1",
      "title": "Memorize or Generalize? Evaluating LLM Code Generation with Evolved Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Wentao Chen",
        "Lizhe Zhang",
        "Li Zhong",
        "Letian Peng",
        "Zilong Wang",
        "Jingbo Shang"
      ],
      "abstract": "Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.",
      "tldr_zh": "本文评估了大型语言模型 (LLMs) 在代码生成中的记忆现象，即模型倾向于记忆训练提示和解决方案，而非真正理解编程原理，导致面对问题变体时泛化能力不足。研究者设计了三种演化策略——mutation、paraphrasing 和 code-rewriting——来创建问题变体，并通过比较 LLM 生成代码的性能和 AST similarity，开发了 memorization score 来量化记忆水平。结果显示，随着监督微调的进行，memorization score 在过拟合前上升，表明记忆问题加剧，而常见缓解方法如提示翻译或数据增强要么降低性能，要么无法有效缓解。该研究强调，记忆现象是 LLM 代码生成的核心挑战，需要更有效的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02296v1",
      "published_date": "2025-03-04 05:39:24 UTC",
      "updated_date": "2025-03-04 05:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:00.181139"
    },
    {
      "arxiv_id": "2503.02284v1",
      "title": "Semi-Supervised Audio-Visual Video Action Recognition with Audio Source Localization Guided Mixup",
      "title_zh": "利用音频源定位引导混合的半监督音频-视觉视频动作识别",
      "authors": [
        "Seokun Kang",
        "Taehwan Kim"
      ],
      "abstract": "Video action recognition is a challenging but important task for\nunderstanding and discovering what the video does. However, acquiring\nannotations for a video is costly, and semi-supervised learning (SSL) has been\nstudied to improve performance even with a small number of labeled data in the\ntask. Prior studies for semi-supervised video action recognition have mostly\nfocused on using single modality - visuals - but the video is multi-modal, so\nutilizing both visuals and audio would be desirable and improve performance\nfurther, which has not been explored well. Therefore, we propose audio-visual\nSSL for video action recognition, which uses both visual and audio together,\neven with quite a few labeled data, which is challenging. In addition, to\nmaximize the information of audio and video, we propose a novel audio source\nlocalization-guided mixup method that considers inter-modal relations between\nvideo and audio modalities. In experiments on UCF-51, Kinetics-400, and\nVGGSound datasets, our model shows the superior performance of the proposed\nsemi-supervised audio-visual action recognition framework and audio source\nlocalization-guided mixup.",
      "tldr_zh": "该论文针对视频动作识别的挑战，提出了一种半监督学习（SSL）框架，利用音频和视觉模态相结合，以减少标注数据的需求。核心方法是音频源定位引导的 Mixup 技术，通过考虑音频-视觉模态间的关系来增强模型训练。实验在 UCF-51、Kinetics-400 和 VGGSound 数据集上显示，该框架显著提升了性能，证明了音频-视觉 SSL 的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02284v1",
      "published_date": "2025-03-04 05:13:56 UTC",
      "updated_date": "2025-03-04 05:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:09.110147"
    },
    {
      "arxiv_id": "2503.02269v1",
      "title": "Experience Replay with Random Reshuffling",
      "title_zh": "翻译失败",
      "authors": [
        "Yasuhiro Fujita"
      ],
      "abstract": "Experience replay is a key component in reinforcement learning for\nstabilizing learning and improving sample efficiency. Its typical\nimplementation samples transitions with replacement from a replay buffer. In\ncontrast, in supervised learning with a fixed dataset, it is a common practice\nto shuffle the dataset every epoch and consume data sequentially, which is\ncalled random reshuffling (RR). RR enjoys theoretically better convergence\nproperties and has been shown to outperform with-replacement sampling\nempirically. To leverage the benefits of RR in reinforcement learning, we\npropose sampling methods that extend RR to experience replay, both in uniform\nand prioritized settings. We evaluate our sampling methods on Atari benchmarks,\ndemonstrating their effectiveness in deep reinforcement learning.",
      "tldr_zh": "该论文探讨了强化学习中的经验回放（Experience Replay），提出了一种采用随机重洗牌（Random Reshuffling, RR）的方法，以取代传统的替换采样，从而提升学习稳定性和样本效率。RR 方法源于监督学习，已证明具有更好的理论收敛属性，因此作者将其扩展到经验回放的均匀和优先级设置中。实验在 Atari 基准测试中验证了这些采样策略的有效性，展示了其在深度强化学习中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02269v1",
      "published_date": "2025-03-04 04:37:22 UTC",
      "updated_date": "2025-03-04 04:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:21.732971"
    },
    {
      "arxiv_id": "2503.02268v3",
      "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjia Jiang",
        "Yangyang Zhuang",
        "Chenxi Song",
        "Xu Yang",
        "Joey Tianyi Zhou",
        "Chi Zhang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to the\ndevelopment of intelligent LLM-based agents capable of interacting with\ngraphical user interfaces (GUIs). These agents demonstrate strong reasoning and\nadaptability, enabling them to perform complex tasks that traditionally\nrequired predefined rules. However, the reliance on step-by-step reasoning in\nLLM-based agents often results in inefficiencies, particularly for routine\ntasks. In contrast, traditional rule-based systems excel in efficiency but lack\nthe intelligence and flexibility to adapt to novel scenarios. To address this\nchallenge, we propose a novel evolutionary framework for GUI agents that\nenhances operational efficiency while retaining intelligence and flexibility.\nOur approach incorporates a memory mechanism that records the agent's task\nexecution history. By analyzing this history, the agent identifies repetitive\naction sequences and evolves high-level actions that act as shortcuts,\nreplacing these low-level operations and improving efficiency. This allows the\nagent to focus on tasks requiring more complex reasoning, while simplifying\nroutine actions. Experimental results on multiple benchmark tasks demonstrate\nthat our approach significantly outperforms existing methods in both efficiency\nand accuracy. The code will be open-sourced to support further research.",
      "tldr_zh": "该研究提出 AppAgentX，一种进化框架，用于提升基于 Large Language Models (LLMs) 的 GUI 代理在智能手机操作中的效率，同时保留其智能和灵活性。该框架引入一个记忆机制来记录代理的任务执行历史，通过分析重复动作序列演化出高层动作作为快捷方式，从而简化常规操作并专注于复杂推理任务。在多个基准任务的实验中，AppAgentX 显著优于现有方法，在效率和准确性上表现出色，并计划开源代码以支持进一步研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02268v3",
      "published_date": "2025-03-04 04:34:09 UTC",
      "updated_date": "2025-04-15 02:32:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:33.706824"
    },
    {
      "arxiv_id": "2503.02267v1",
      "title": "REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs",
      "title_zh": "翻译失败",
      "authors": [
        "Sourav Mishra",
        "Shreya Hallikeri",
        "Suresh Sundaram"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) offer a promising approach to\nsimulating physical systems. Still, their application is limited by\noptimization challenges, mainly due to the lack of activation functions that\ngeneralize well across several physical systems. Existing activation functions\noften lack such flexibility and generalization power. To address this issue, we\nintroduce Rational Exponential Activation (REAct), a generalized form of tanh\nconsisting of four learnable shape parameters. Experiments show that REAct\noutperforms many standard and benchmark activations, achieving an MSE three\norders of magnitude lower than tanh on heat problems and generalizing well to\nfiner grids and points beyond the training domain. It also excels at function\napproximation tasks and improves noise rejection in inverse problems, leading\nto more accurate parameter estimates across varying noise levels.",
      "tldr_zh": "本研究针对Physics-Informed Neural Networks (PINNs) 在模拟物理系统时面临的优化挑战，特别是激活函数的泛化能力不足，提出了一种新的Rational Exponential Activation (REAct)。REAct 是tanh的推广形式，包含四个可学习的形状参数，能够提供更高的灵活性和泛化性能。实验结果显示，REAct 在热问题上使MSE比tanh低三个数量级，并在函数逼近任务中表现出色，同时提升了逆问题中的噪声抑制能力，实现更准确的参数估计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 5 tables, 1 figure; Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02267v1",
      "published_date": "2025-03-04 04:28:59 UTC",
      "updated_date": "2025-03-04 04:28:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:46.129344"
    },
    {
      "arxiv_id": "2503.02249v1",
      "title": "Large Language Models as Natural Selector for Embodied Soft Robot Design",
      "title_zh": "翻译失败",
      "authors": [
        "Changhe Chen",
        "Xiaohao Xu",
        "Xiangdong Wang",
        "Xiaonan Huang"
      ],
      "abstract": "Designing soft robots is a complex and iterative process that demands\ncross-disciplinary expertise in materials science, mechanics, and control,\noften relying on intuition and extensive experimentation. While Large Language\nModels (LLMs) have demonstrated impressive reasoning abilities, their capacity\nto learn and apply embodied design principles--crucial for creating functional\nrobotic systems--remains largely unexplored. This paper introduces\nRoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn\nrepresentations of soft robot designs that effectively bridge the gap between\nhigh-level task descriptions and low-level morphological and material choices.\nRoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft\nrobot design challenges, spanning robotic locomotion, manipulation, and\nbalancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal\nthat while these models exhibit promising capabilities in learning design\nrepresentations, they struggle with fine-grained distinctions between designs\nwith subtle performance differences. We further demonstrate the practical\nutility of LLMs for robot design initialization. Our code and benchmark will be\navailable to encourage the community to foster this exciting research\ndirection.",
      "tldr_zh": "该论文探讨了Large Language Models (LLMs) 在软机器人设计中的潜力，旨在解决这一复杂过程对跨学科知识和实验依赖的问题。\n他们引入了RoboCrafter-QA基准，利用EvoGym模拟器生成多样化的设计挑战（如机器人运动、操控和平衡任务），以评估LLMs是否能桥接高层任务描述与底层形态/材料选择。\n实验结果显示，先进的multi-modal LLMs在学习设计表示方面表现出色，但难以区分细微性能差异；此外，LLMs可用于机器人设计初始化，并公开代码和基准以促进相关研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02249v1",
      "published_date": "2025-03-04 03:55:10 UTC",
      "updated_date": "2025-03-04 03:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:01:59.901214"
    },
    {
      "arxiv_id": "2503.02239v1",
      "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
      "title_zh": "V2X-LLM：增强连接车辆走廊中的 V2X 整合与理解",
      "authors": [
        "Keshu Wu",
        "Pei Li",
        "Yang Zhou",
        "Rui Gan",
        "Junwei You",
        "Yang Cheng",
        "Jingwen Zhu",
        "Steven T. Parker",
        "Bin Ran",
        "David A. Noyce",
        "Zhengzhong Tu"
      ],
      "abstract": "The advancement of Connected and Automated Vehicles (CAVs) and\nVehicle-to-Everything (V2X) offers significant potential for enhancing\ntransportation safety, mobility, and sustainability. However, the integration\nand analysis of the diverse and voluminous V2X data, including Basic Safety\nMessages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial\nchallenges, especially on Connected Vehicle Corridors. These challenges include\nmanaging large data volumes, ensuring real-time data integration, and\nunderstanding complex traffic scenarios. Although these projects have developed\nan advanced CAV data pipeline that enables real-time communication between\nvehicles, infrastructure, and other road users for managing connected vehicle\nand roadside unit (RSU) data, significant hurdles in data comprehension and\nreal-time scenario analysis and reasoning persist. To address these issues, we\nintroduce the V2X-LLM framework, a novel enhancement to the existing CV data\npipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the\nunderstanding and real-time analysis of V2X data. The framework includes four\nkey tasks: Scenario Explanation, offering detailed narratives of traffic\nconditions; V2X Data Description, detailing vehicle and infrastructure\nstatuses; State Prediction, forecasting future traffic states; and Navigation\nAdvisory, providing optimized routing instructions. By integrating LLM-driven\nreasoning with V2X data within the data pipeline, the V2X-LLM framework offers\nreal-time feedback and decision support for traffic management. This\nintegration enhances the accuracy of traffic analysis, safety, and traffic\noptimization. Demonstrations in a real-world urban corridor highlight the\nframework's potential to advance intelligent transportation systems.",
      "tldr_zh": "本文提出 V2X-LLM 框架，利用 Large Language Models (LLMs) 增强连接车辆走廊中的 V2X 集成和理解，解决诸如数据量大、实时集成以及复杂交通场景分析的挑战。框架整合现有 CAV 数据管道，包含四个关键任务：Scenario Explanation（提供交通条件详细叙述）、V2X Data Description（描述车辆和基础设施状态）、State Prediction（预测未来交通状态）以及 Navigation Advisory（提供优化路由指令）。实验在真实城市走廊中演示了该框架的潜力，提升了交通分析准确性、安全性和整体优化。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02239v1",
      "published_date": "2025-03-04 03:28:30 UTC",
      "updated_date": "2025-03-04 03:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:02:11.150265"
    },
    {
      "arxiv_id": "2503.02235v1",
      "title": "Deficient Excitation in Parameter Learning",
      "title_zh": "参数学习中的缺陷激励",
      "authors": [
        "Ganghui Cao",
        "Shimin Wang",
        "Martin Guay",
        "Jinzhi Wang",
        "Zhisheng Duan",
        "Marios M. Polycarpou"
      ],
      "abstract": "This paper investigates parameter learning problems under deficient\nexcitation (DE). The DE condition is a rank-deficient, and therefore, a more\ngeneral evolution of the well-known persistent excitation condition. Under the\nDE condition, a proposed online algorithm is able to calculate the identifiable\nand non-identifiable subspaces, and finally give an optimal parameter estimate\nin the sense of least squares. In particular, the learning error within the\nidentifiable subspace exponentially converges to zero in the noise-free case,\neven without persistent excitation. The DE condition also provides a new\nperspective for solving distributed parameter learning problems, where the\nchallenge is posed by local regressors that are often insufficiently excited.\nTo improve knowledge of the unknown parameters, a cooperative learning protocol\nis proposed for a group of estimators that collect measured information under\ncomplementary DE conditions. This protocol allows each local estimator to\noperate locally in its identifiable subspace, and reach a consensus with\nneighbours in its non-identifiable subspace. As a result, the task of\nestimating unknown parameters can be achieved in a distributed way using\ncooperative local estimators. Application examples in system identification are\ngiven to demonstrate the effectiveness of the theoretical results developed in\nthis paper.",
      "tldr_zh": "这篇论文探讨了参数学习中的 Deficient Excitation (DE) 条件，这是一种比 Persistent Excitation (PE) 更一般的秩缺失条件，能够处理局部回归器不足激发的挑战。论文提出了一种在线算法，用于计算可识别和不可识别子空间，并给出最小二乘意义下的最优参数估计；在无噪声情况下，学习误差在可识别子空间内指数收敛，即使不满足 PE。针对分布式参数学习，论文引入了合作学习协议，让一组估计器在互补 DE 条件下交换信息，从而实现局部估计器在可识别子空间本地操作，并在不可识别子空间达成共识。实验应用示例展示了该方法的有效性，尤其在系统识别领域。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "eess.SP",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "16 pages,9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02235v1",
      "published_date": "2025-03-04 03:18:13 UTC",
      "updated_date": "2025-03-04 03:18:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:02:22.998990"
    },
    {
      "arxiv_id": "2503.02233v2",
      "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
      "title_zh": "通过显式知识边界建模提升LLM可靠性",
      "authors": [
        "Hang Zheng",
        "Hongshen Xu",
        "Yuncong Liu",
        "Lu Chen",
        "Pascale Fung",
        "Kai Yu"
      ],
      "abstract": "Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 因知识边界不明确而产生的幻觉问题，提出 Explicit Knowledge Boundary Modeling (EKBM) 框架，该框架整合快速和缓慢推理系统，以平衡模型的可靠性和可用性。EKBM 通过快速思考模型生成带有置信度标签的响应，便于直接使用高置信度输出，而对不确定预测则采用缓慢精炼模型进行针对性推理，同时采用混合训练管道增强自我认知而不降低任务性能。在对话状态跟踪任务的评估中，EKBM 比基于不确定性的基线模型表现出更高的可靠性，并显著提高了准确性，同时保持低计算开销。该框架为提升 LLM 在错误敏感应用中的准确性和实用性提供了可扩展的范式。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02233v2",
      "published_date": "2025-03-04 03:16:02 UTC",
      "updated_date": "2025-03-12 07:42:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:02:35.393037"
    },
    {
      "arxiv_id": "2503.02228v1",
      "title": "One Patient's Annotation is Another One's Initialization: Towards Zero-Shot Surgical Video Segmentation with Cross-Patient Initialization",
      "title_zh": "翻译失败",
      "authors": [
        "Seyed Amir Mousavi",
        "Utku Ozbulak",
        "Francesca Tozzi",
        "Nikdokht Rashidian",
        "Wouter Willaert",
        "Joris Vankerschaver",
        "Wesley De Neve"
      ],
      "abstract": "Video object segmentation is an emerging technology that is well-suited for\nreal-time surgical video segmentation, offering valuable clinical assistance in\nthe operating room by ensuring consistent frame tracking. However, its adoption\nis limited by the need for manual intervention to select the tracked object,\nmaking it impractical in surgical settings. In this work, we tackle this\nchallenge with an innovative solution: using previously annotated frames from\nother patients as the tracking frames. We find that this unconventional\napproach can match or even surpass the performance of using patients' own\ntracking frames, enabling more autonomous and efficient AI-assisted surgical\nworkflows. Furthermore, we analyze the benefits and limitations of this\napproach, highlighting its potential to enhance segmentation accuracy while\nreducing the need for manual input. Our findings provide insights into key\nfactors influencing performance, offering a foundation for future research on\noptimizing cross-patient frame selection for real-time surgical video analysis.",
      "tldr_zh": "本论文针对手术视频分割中手动选择跟踪对象的局限性，提出了一种基于跨患者初始化(Cross-Patient Initialization)的零样本(Zero-Shot)方法，使用其他患者的标注帧作为跟踪帧。该方法能够匹配或超过使用患者自身帧的性能，从而实现更自主且高效的AI辅助手术工作流。通过分析，这种跨患者方法提升了分割准确性，同时减少了手动干预，并为优化帧选择提供了关键见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02228v1",
      "published_date": "2025-03-04 03:11:03 UTC",
      "updated_date": "2025-03-04 03:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:02:47.175064"
    },
    {
      "arxiv_id": "2503.02221v1",
      "title": "Attention Bootstrapping for Multi-Modal Test-Time Adaptation",
      "title_zh": "针对多模态测试时适应的注意力引导",
      "authors": [
        "Yusheng Zhao",
        "Junyu Luo",
        "Xiao Luo",
        "Jinsheng Huang",
        "Jingyang Yuan",
        "Zhiping Xiao",
        "Ming Zhang"
      ],
      "abstract": "Test-time adaptation aims to adapt a well-trained model to potential\ndistribution shifts at test time using only unlabeled test data, without access\nto the original training data. While previous efforts mainly focus on a single\nmodality, test-time distribution shift in the multi-modal setting is more\ncomplex and calls for new solutions. This paper tackles the problem of\nmulti-modal test-time adaptation by proposing a novel method named Attention\nBootstrapping with Principal Entropy Minimization (ABPEM). We observe that\ntest-time distribution shift causes misalignment across modalities, leading to\na large gap between intra-modality discrepancies (measured by self-attention)\nand inter-modality discrepancies (measured by cross-attention). We name this\nthe attention gap. This attention gap widens with more severe distribution\nshifts, hindering effective modality fusion. To mitigate this attention gap and\nencourage better modality fusion, we propose attention bootstrapping that\npromotes cross-attention with the guidance of self-attention. Moreover, to\nreduce the gradient noise in the commonly-used entropy minimization, we adopt\nprincipal entropy minimization, a refinement of entropy minimization that\nreduces gradient noise by focusing on the principal parts of entropy, excluding\nless reliable gradient information. Extensive experiments on the benchmarks\nvalidate the effectiveness of the proposed ABPEM in comparison with competing\nbaselines.",
      "tldr_zh": "这篇论文针对多模态Test-Time Adaptation问题，提出了一种新方法Attention Bootstrapping with Principal Entropy Minimization (ABPEM)，旨在使用无标签测试数据适应模型以应对分布偏移。方法通过观察和缓解attention gap（自注意力与交叉注意力间的差异）来提升模态融合效果，并采用Principal Entropy Minimization减少熵最小化中的梯度噪声。实验在基准数据集上验证了ABPEM的有效性，其性能优于竞争基线。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02221v1",
      "published_date": "2025-03-04 02:53:53 UTC",
      "updated_date": "2025-03-04 02:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:02:57.887355"
    },
    {
      "arxiv_id": "2503.04808v1",
      "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Stephen Chung",
        "Wenyu Du",
        "Jie Fu"
      ],
      "abstract": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
      "tldr_zh": "这篇论文探讨了在多-Attempt Reinforcement Learning 中从失败中学习的方法，扩展了如 DeepSeek R1 所示的强化学习（RL）任务，将标准问答任务改为多尝试设置，提供反馈以帮助大型语言模型（LLMs）改进先前的响应并提升搜索效率。实验结果显示，经过多尝试任务训练的小型 LLM 在数学基准上准确率从 1 次尝试的 45.6% 提高到 2 次尝试的 52.5%，而标准单轮任务训练的模型仅从 42.3% 微升至 43.2%。相比之下，多尝试方法使模型在性能上略有优势，并更有效地基于用户反馈优化响应，为 RL 应用提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.04808v1",
      "published_date": "2025-03-04 02:53:39 UTC",
      "updated_date": "2025-03-04 02:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:03:11.836740"
    },
    {
      "arxiv_id": "2503.02199v1",
      "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
      "title_zh": "文字还是视觉：视觉语言模型是否对文本有盲目信仰？",
      "authors": [
        "Ailin Deng",
        "Tri Cao",
        "Zhirui Chen",
        "Bryan Hooi"
      ],
      "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith\nin text''} phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
      "tldr_zh": "本研究探讨了视觉语言模型 (VLMs) 在处理视觉和文本不一致时的模态偏好，发现 VLMs 存在“blind faith in text”现象，即过度信任文本数据，导致在文本出错时性能显著下降，并引发安全问题。通过在四种视觉中心任务中引入文本变异并评估十个 VLMs，论文分析了影响这种偏见的因素，包括指令提示、语言模型大小、文本相关性、token 顺序等，其中增大语言模型大小可略微缓解偏见，而 token 顺序可能加剧它。为解决这一问题，研究采用监督细调 (supervised fine-tuning) 与文本增强方法，证明其有效性，并通过理论分析指出，该现象可能源于训练数据中纯文本和多模态数据的失衡，强调需要平衡训练以提升 VLMs 的鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02199v1",
      "published_date": "2025-03-04 02:21:07 UTC",
      "updated_date": "2025-03-04 02:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:03:22.692850"
    },
    {
      "arxiv_id": "2503.02197v1",
      "title": "ATLaS: Agent Tuning via Learning Critical Steps",
      "title_zh": "ATLaS：通过学习关键步骤的代理调优",
      "authors": [
        "Zhixun Chen",
        "Ming Li",
        "Yuxuan Huang",
        "Yali Du",
        "Meng Fang",
        "Tianyi Zhou"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
      "tldr_zh": "该论文提出ATLaS方法，通过识别和学习专家轨迹中的关键步骤（如规划、复杂推理和战略决策），来优化大型语言模型(LLM)代理的微调过程，从而避免传统监督微调引入的专家偏差和泛化问题。ATLaS仅在这些关键步骤上进行fintuning，降低了训练成本并提升了模型在不同环境和任务中的泛化能力。实验结果显示，只在ATLaS选择的30%关键步骤上微调的LLM，表现优于在所有步骤上微调的模型和现有开源代理，同时保持了基础LLM作为通用代理的技能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02197v1",
      "published_date": "2025-03-04 02:14:55 UTC",
      "updated_date": "2025-03-04 02:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:03:34.851031"
    },
    {
      "arxiv_id": "2503.05805v3",
      "title": "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
      "title_zh": "翻译失败",
      "authors": [
        "Dom Huh",
        "Prasant Mohapatra"
      ],
      "abstract": "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
      "tldr_zh": "这篇论文提出了一种基于扩散的自动竞价框架，使用图表示来建模大规模拍卖环境，帮助多代理在 KPI 指标约束下动态优化竞价策略，以应对不确定、稀疏和随机变量的挑战。该框架结合可学习的图-based embeddings 和基于规划的潜在扩散模型 (LDM)，通过捕捉印象机会的相互依赖和多代理动态，并利用奖励对齐技术微调 LDM 的后验分布，以生成最大化 KPI 指标并满足约束的竞价轨迹。实验在真实和合成拍卖环境中显示，该方法显著提升了多个常见 KPI 指标的性能，并提高了拍卖结果预测的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05805v3",
      "published_date": "2025-03-04 02:07:24 UTC",
      "updated_date": "2025-04-19 04:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:03:48.001380"
    },
    {
      "arxiv_id": "2503.04807v3",
      "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data",
      "title_zh": "翻译失败",
      "authors": [
        "Hyeonseok Moon",
        "Jaehyung Seo",
        "Heuiseok Lim"
      ],
      "abstract": "Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion.",
      "tldr_zh": "本文呼吁在评估指令微调（instruction tuning）数据质量时加强严格性，因为现有研究中超参数（hyperparameters）的选择往往随意且缺乏充分理由，导致结果不一致。研究者通过实验分析了 LIMA 数据和 Alpaca 的 1000 个数据点，证明任意超参数决策可能扭曲对数据质量的结论，从而影响大语言模型（LLMs）的对齐性能。该研究强调，需要在报告 IT 数据质量时进行更谨慎的验证，以确保研究的可信性和可比性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the ACL2025-main",
      "pdf_url": "http://arxiv.org/pdf/2503.04807v3",
      "published_date": "2025-03-04 02:04:58 UTC",
      "updated_date": "2025-05-16 02:19:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:04:00.449661"
    },
    {
      "arxiv_id": "2503.10649v1",
      "title": "Measuring Political Preferences in AI Systems: An Integrative Approach",
      "title_zh": "人工智能系统中的政治偏好测量：一种整合方法",
      "authors": [
        "David Rozado"
      ],
      "abstract": "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment.",
      "tldr_zh": "本研究提出了一种整合方法来评估大型语言模型 (LLM) 基于 AI 系统中的政治偏见，通过结合四种互补方法：比较 AI 生成文本与美国国会党派语言、分析政策推荐中的政治观点、情感分析对公共人物的态度，以及标准化政治导向测试。结果显示，大多数当代 AI 系统表现出一致的左倾偏见，但这种偏见可以通过使用政治倾斜数据进行微调来调整。研究强调，这种系统性偏见可能导致观点多样性减少、社会极化以及公众对 AI 的不信任，并建议 AI 系统优先事实准确性、保持中立，并建立独立监控平台以确保透明和责任。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Measuring Political Preferences in AI Systems. Report. Available:\n  https://manhattan.institute/article/measuring-political-preferences-in-ai-systems-an-integrative-approach/",
      "pdf_url": "http://arxiv.org/pdf/2503.10649v1",
      "published_date": "2025-03-04 01:40:28 UTC",
      "updated_date": "2025-03-04 01:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:04:11.446274"
    },
    {
      "arxiv_id": "2503.02180v1",
      "title": "Discrete Differential Evolution Particle Swarm Optimization Algorithm for Energy Saving Flexible Job Shop Scheduling Problem Considering Machine Multi States",
      "title_zh": "翻译失败",
      "authors": [
        "Da Wang",
        "Yu Zhang",
        "Kai Zhang",
        "Junqing Li",
        "Dengwang Li"
      ],
      "abstract": "As the continuous deepening of low-carbon emission reduction policies, the\nmanufacturing industries urgently need sensible energy-saving scheduling\nschemes to achieve the balance between improving production efficiency and\nreducing energy consumption. In energy-saving scheduling, reasonable machine\nstates-switching is a key point to achieve expected goals, i.e., whether the\nmachines need to switch speed between different operations, and whether the\nmachines need to add extra setup time between different jobs. Regarding this\nmatter, this work proposes a novel machine multi states-based energy saving\nflexible job scheduling problem (EFJSP-M), which simultaneously takes into\naccount machine multi speeds and setup time. To address the proposed EFJSP-M, a\nkind of discrete differential evolution particle swarm optimization algorithm\n(D-DEPSO) is designed. In specific, D-DEPSO includes a hybrid initialization\nstrategy to improve the initial population performance, an updating mechanism\nembedded with differential evolution operators to enhance population diversity,\nand a critical path variable neighborhood search strategy to expand the\nsolution space. At last, based on datasets DPs and MKs, the experiment results\ncompared with five state-of-the-art algorithms demonstrate the feasible of\nEFJSP-M and the superior of D-DEPSO.",
      "tldr_zh": "这篇论文针对节能柔性作业车间调度问题（EFJSP），提出了一种新的机器多状态模型（EFJSP-M），同时考虑机器多速度和设置时间，以实现生产效率与能源消耗的平衡。作者设计了离散差分进化粒子群优化算法（D-DEPSO），该算法包括混合初始化策略、嵌入差分进化操作的更新机制，以及关键路径变量邻域搜索策略，以提升算法性能和解空间探索。实验结果显示，在 DPs 和 MKs 数据集上，D-DEPSO 比五种最先进算法表现出色，证明了其在能源节约调度中的优越性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02180v1",
      "published_date": "2025-03-04 01:40:24 UTC",
      "updated_date": "2025-03-04 01:40:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:04:25.380394"
    },
    {
      "arxiv_id": "2503.02175v2",
      "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
      "title_zh": "翻译失败",
      "authors": [
        "Saeed Ranjbar Alvar",
        "Gursimran Singh",
        "Mohammad Akbari",
        "Yong Zhang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have emerged as powerful models capable of\nunderstanding various data modalities, including text, images, and videos. LMMs\nencode both text and visual data into tokens that are then combined and\nprocessed by an integrated Large Language Model (LLM). Including visual tokens\nsubstantially increases the total token count, often by thousands. The\nincreased input length for LLM significantly raises the complexity of\ninference, resulting in high latency in LMMs. To address this issue, token\npruning methods, which remove part of the visual tokens, are proposed. The\nexisting token pruning methods either require extensive calibration and\nfine-tuning or rely on suboptimal importance metrics which results in increased\nredundancy among the retained tokens. In this paper, we first formulate token\npruning as Max-Min Diversity Problem (MMDP) where the goal is to select a\nsubset such that the diversity among the selected {tokens} is maximized. Then,\nwe solve the MMDP to obtain the selected subset and prune the rest. The\nproposed method, DivPrune, reduces redundancy and achieves the highest\ndiversity of the selected tokens. By ensuring high diversity, the selected\ntokens better represent the original tokens, enabling effective performance\neven at high pruning ratios without requiring fine-tuning. Extensive\nexperiments with various LMMs show that DivPrune achieves state-of-the-art\naccuracy over 16 image- and video-language datasets. Additionally, DivPrune\nreduces both the end-to-end latency and GPU memory usage for the tested models.\nThe code is available $\\href{https://github.com/vbdi/divprune}{\\text{here}}$.",
      "tldr_zh": "该研究针对 Large Multimodal Models (LMMs) 中视觉令牌数量过多导致的推理延迟问题，提出了 DivPrune 方法，将令牌修剪形式化为 Max-Min Diversity Problem (MMDP)，以最大化选定令牌的多样性，从而减少冗余。DivPrune 通过优化令牌子集选择，确保在高修剪比例下维持模型性能，而无需额外微调。实验结果显示，该方法在 16 个图像和视频语言数据集上实现了最先进的准确率，同时降低了端到端延迟和 GPU 内存使用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02175v2",
      "published_date": "2025-03-04 01:33:14 UTC",
      "updated_date": "2025-04-01 19:02:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:04:35.443358"
    },
    {
      "arxiv_id": "2503.02174v1",
      "title": "Adversarial Tokenization",
      "title_zh": "翻译失败",
      "authors": [
        "Renato Lui Geh",
        "Zilei Shao",
        "Guy Van den Broeck"
      ],
      "abstract": "Current LLM pipelines account for only one possible tokenization for a given\nstring, ignoring exponentially many alternative tokenizations during training\nand inference. For example, the standard Llama3 tokenization of penguin is\n[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this\npaper, we show that despite LLMs being trained solely on one tokenization, they\nstill retain semantic understanding of other tokenizations, raising questions\nabout their implications in LLM safety. Put succinctly, we answer the following\nquestion: can we adversarially tokenize an obviously malicious string to evade\nsafety and alignment restrictions? We show that not only is adversarial\ntokenization an effective yet previously neglected axis of attack, but it is\nalso competitive against existing state-of-the-art adversarial approaches\nwithout changing the text of the harmful request. We empirically validate this\nexploit across three state-of-the-art LLMs and adversarial datasets, revealing\na previously unknown vulnerability in subword models.",
      "tldr_zh": "本论文探讨了大型语言模型（LLMs）的标记化（tokenization）漏洞，即现有LLM管道仅使用一种标记化方式，而忽略其他备选标记化（如“penguin”可标记为[p,enguin]或[peng,uin]），这可能导致语义理解的潜在风险。研究者通过对抗性标记化（adversarial tokenization）方法，测试是否能规避LLM的安全和对齐机制，而无需修改有害请求的文本。实验结果显示，这种攻击方式在三个最先进LLM和对抗数据集上表现出色，与现有最先进方法竞争，并揭示了子词模型（subword models）的一个未知漏洞，为LLM安全提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02174v1",
      "published_date": "2025-03-04 01:31:17 UTC",
      "updated_date": "2025-03-04 01:31:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:04:46.897943"
    },
    {
      "arxiv_id": "2503.02172v1",
      "title": "KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering",
      "title_zh": "KGCompiler：针对知识图谱复杂逻辑查询回答的深度学习编译优化",
      "authors": [
        "Hongyu Lin",
        "Haoran Luo",
        "Hanghang Cao",
        "Yang Liu",
        "Shihao Gao",
        "Kaichun Yao",
        "Libo Zhang",
        "Mingjie Xing",
        "Yanjun Wu"
      ],
      "abstract": "Complex Logical Query Answering (CLQA) involves intricate multi-hop logical\nreasoning over large-scale and potentially incomplete Knowledge Graphs (KGs).\nAlthough existing CLQA algorithms achieve high accuracy in answering such\nqueries, their reasoning time and memory usage scale significantly with the\nnumber of First-Order Logic (FOL) operators involved, creating serious\nchallenges for practical deployment. In addition, current research primarily\nfocuses on algorithm-level optimizations for CLQA tasks, often overlooking\ncompiler-level optimizations, which can offer greater generality and\nscalability. To address these limitations, we introduce a Knowledge Graph\nCompiler, namely KGCompiler, the first deep learning compiler specifically\ndesigned for CLQA tasks. By incorporating KG-specific optimizations proposed in\nthis paper, KGCompiler enhances the reasoning performance of CLQA algorithms\nwithout requiring additional manual modifications to their implementations. At\nthe same time, it significantly reduces memory usage. Extensive experiments\ndemonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from\n1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interface\nto enable hands-on experience with KGCompiler.",
      "tldr_zh": "本论文提出 KGCompiler，一种专为知识图谱 (KGs) 复杂逻辑查询回答 (CLQA) 设计的深度学习编译器，旨在解决现有 CLQA 算法在多跳逻辑推理中面临的推理时间和内存消耗问题。KGCompiler 通过引入 KG 特定优化（如编译级提升），无需修改算法实现即可显著提高推理性能并降低内存使用。实验显示，KGCompiler 对 CLQA 算法的加速范围为 1.04x 到 8.26x，平均加速 3.71x，并提供了用户接口以便实际应用。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02172v1",
      "published_date": "2025-03-04 01:24:32 UTC",
      "updated_date": "2025-03-04 01:24:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:05:00.061514"
    },
    {
      "arxiv_id": "2503.02170v1",
      "title": "Adaptive Camera Sensor for Vision Models",
      "title_zh": "自适应",
      "authors": [
        "Eunsu Baek",
        "Sunghwan Han",
        "Taesik Gong",
        "Hyung-Sin Kim"
      ],
      "abstract": "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com/Edw2n/Lens.git.",
      "tldr_zh": "这篇论文针对计算机视觉中的 domain shift 问题，提出了一种名为 Lens 的自适应相机传感器控制方法，该方法借鉴人类视觉感知，通过从模型视角捕获高质量图像来提升性能，而非依赖传统的人类中心控制。Lens 核心利用 VisiT 指标——一个无训练的模型特定质量评估工具，利用置信度分数实时评估未标注样本，从而实现轻量级适应。作者引入了新的 ImageNet-ES Diverse 数据集来验证该方法，实验结果显示 Lens 在各种基线方案上显著提高了模型准确率，同时保持低延迟，并能补偿模型大小差异与模型改进技术协同。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The International Conference on Learning Representations (ICLR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02170v1",
      "published_date": "2025-03-04 01:20:23 UTC",
      "updated_date": "2025-03-04 01:20:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:05:12.690150"
    },
    {
      "arxiv_id": "2503.02157v1",
      "title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models",
      "title_zh": "MedHEval：医疗大型视觉语言模型中幻觉的基准测试与缓解策略",
      "authors": [
        "Aofei Chang",
        "Le Huang",
        "Parminder Bhatia",
        "Taha Kass-Hout",
        "Fenglong Ma",
        "Cao Xiao"
      ],
      "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in\nthe medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate\nhallucinations due to limited expertise and the complexity of medical\napplications. Existing benchmarks fail to effectively evaluate hallucinations\nbased on their underlying causes and lack assessments of mitigation strategies.\nTo address this gap, we introduce MedHEval, a novel benchmark that\nsystematically evaluates hallucinations and mitigation strategies in Med-LVLMs\nby categorizing them into three underlying causes: visual misinterpretation,\nknowledge deficiency, and context misalignment. We construct a diverse set of\nclose- and open-ended medical VQA datasets with comprehensive evaluation\nmetrics to assess these hallucination types. We conduct extensive experiments\nacross 11 popular (Med)-LVLMs and evaluate 7 state-of-the-art hallucination\nmitigation techniques. Results reveal that Med-LVLMs struggle with\nhallucinations arising from different causes while existing mitigation methods\nshow limited effectiveness, especially for knowledge- and context-based errors.\nThese findings underscore the need for improved alignment training and\nspecialized mitigation strategies to enhance Med-LVLMs' reliability. MedHEval\nestablishes a standardized framework for evaluating and mitigating medical\nhallucinations, guiding the development of more trustworthy Med-LVLMs.",
      "tldr_zh": "本文引入 MedHEval，这是一个新型基准，用于评估 Medical Large Vision-Language Models (Med-LVLMs) 中的幻觉问题及其缓解策略，并将幻觉分类为视觉误解、知识不足和上下文不匹配三种原因。研究构建了多样化的医疗视觉问答 (VQA) 数据集，并通过全面评估指标测试了 11 个流行 (Med-)LVLMs 和 7 种最先进缓解技术。结果显示，Med-LVLMs 在不同幻觉类型上表现较差，而现有缓解方法效果有限，尤其是针对知识和上下文错误，这突出了改进对齐训练和专门策略的需求，以提升 Med-LVLMs 的可靠性和可信度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.02157v1",
      "published_date": "2025-03-04 00:40:09 UTC",
      "updated_date": "2025-03-04 00:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:05:24.719010"
    },
    {
      "arxiv_id": "2503.02156v1",
      "title": "MobRFFI: Non-cooperative Device Re-identification for Mobility Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Stepan Mazokha",
        "Fanchen Bao",
        "George Sklivanitis",
        "Jason O. Hallstrom"
      ],
      "abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
      "tldr_zh": "这篇论文针对 WiFi 网络中 MAC 地址随机化导致的设备识别挑战，提出了 MobRFFI 框架，这是一种非合作式设备重新识别方法，用于提升城市流动性监控的准确性。框架利用 AI 和编码器深度学习模型，从 WiFi 芯片硬件缺陷中提取独特特征，实现独立于帧类型的设备指纹提取和重新识别。在 WiSig 数据集上评估，MobRFFI 在多日场景中达到 94% 的设备准确率，在单日场景中达到 100%。此外，作者收集了新数据集 MobRFFI，证明结合多个接收器的指纹可将重新识别性能从 81% 提升到 100%（单日）和从 41% 提升到 100%（多日），为流动性智能提供了更可靠的解决方案。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "10 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02156v1",
      "published_date": "2025-03-04 00:39:50 UTC",
      "updated_date": "2025-03-04 00:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:05:39.263837"
    },
    {
      "arxiv_id": "2503.02154v1",
      "title": "AugFL: Augmenting Federated Learning with Pretrained Models",
      "title_zh": "AugFL: 使用预训练模型增强联邦学习",
      "authors": [
        "Sheng Yue",
        "Zerui Qin",
        "Yongheng Deng",
        "Ju Ren",
        "Yaoxue Zhang",
        "Junshan Zhang"
      ],
      "abstract": "Federated Learning (FL) has garnered widespread interest in recent years.\nHowever, owing to strict privacy policies or limited storage capacities of\ntraining participants such as IoT devices, its effective deployment is often\nimpeded by the scarcity of training data in practical decentralized learning\nenvironments. In this paper, we study enhancing FL with the aid of (large)\npre-trained models (PMs), that encapsulate wealthy general/domain-agnostic\nknowledge, to alleviate the data requirement in conducting FL from scratch.\nSpecifically, we consider a networked FL system formed by a central server and\ndistributed clients. First, we formulate the PM-aided personalized FL as a\nregularization-based federated meta-learning problem, where clients join forces\nto learn a meta-model with knowledge transferred from a private PM stored at\nthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, to\noptimize the problem with no need to expose the PM or incur additional\ncomputational costs to local clients. Further, we establish theoretical\nguarantees for AugFL in terms of communication complexity, adaptation\nperformance, and the benefit of knowledge transfer in general non-convex cases.\nExtensive experiments corroborate the efficacy and superiority of AugFL over\nexisting baselines.",
      "tldr_zh": "本论文提出AugFL方法，通过利用预训练模型(Pretrained Models)来增强联邦学习(Federated Learning)，以解决分布式环境中数据稀缺的问题。AugFL将这一过程形式化为基于正则化的联邦元学习(federated meta-learning)问题，并开发了基于inexact-ADMM的优化算法，使客户端无需暴露PM或增加计算开销即可实现知识转移。实验结果显示，AugFL在通信复杂度、适应性能和非凸场景下均表现出色，并优于现有基线。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "to be published in Transactions on Networking",
      "pdf_url": "http://arxiv.org/pdf/2503.02154v1",
      "published_date": "2025-03-04 00:37:33 UTC",
      "updated_date": "2025-03-04 00:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:05:49.029566"
    },
    {
      "arxiv_id": "2503.02138v1",
      "title": "Elliptic Loss Regularization",
      "title_zh": "椭圆损失正则化",
      "authors": [
        "Ali Hasan",
        "Haoming Yang",
        "Yuting Ng",
        "Vahid Tarokh"
      ],
      "abstract": "Regularizing neural networks is important for anticipating model behavior in\nregions of the data space that are not well represented. In this work, we\npropose a regularization technique for enforcing a level of smoothness in the\nmapping between the data input space and the loss value. We specify the level\nof regularity by requiring that the loss of the network satisfies an elliptic\noperator over the data domain. To do this, we modify the usual empirical risk\nminimization objective such that we instead minimize a new objective that\nsatisfies an elliptic operator over points within the domain. This allows us to\nuse existing theory on elliptic operators to anticipate the behavior of the\nerror for points outside the training set. We propose a tractable computational\nmethod that approximates the behavior of the elliptic operator while being\ncomputationally efficient. Finally, we analyze the properties of the proposed\nregularization to understand the performance on common problems of distribution\nshift and group imbalance. Numerical experiments confirm the utility of the\nproposed regularization technique.",
      "tldr_zh": "该研究提出了一种名为Elliptic Loss Regularization的正则化技术，旨在通过强制损失函数满足elliptic operator在数据域上的条件，来提升神经网络在未充分表示的数据区域中的平滑性和泛化能力。方法涉及修改传统的经验风险最小化目标，并引入一个高效的计算近似算法，以预测训练集外点的错误行为。实验结果表明，该技术在分布偏移和组不平衡问题上表现出色，显著改善了模型的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02138v1",
      "published_date": "2025-03-04 00:08:08 UTC",
      "updated_date": "2025-03-04 00:08:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T22:06:01.385458"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 143,
  "processed_papers_count": 143,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T22:06:39.672027"
}
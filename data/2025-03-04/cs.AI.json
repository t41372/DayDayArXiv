{
  "date": "2025-03-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-04 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 上的论文讨论热点集中在大型语言模型（LLM）的对齐、评估、效率提升和应用，以及机器人技术（特别是多智能体路径规划和操作策略）、计算机视觉（包括 SLAM、图像生成和医学应用）和强化学习的新进展。值得关注的亮点包括：利用半监督学习改进 ICL、首个集中式学习 MAPF 策略 RAILGUN、通过对抗性 RLHF 平台引发 LLM 失调的风险分析、数据集合规性追踪的 AI 系统 NEXUS、以及对 LLM 提示工程和评估鲁棒性的深刻反思。\n\n以下是今天值得关注的论文：\n\n---\n\n**1. 半监督上下文学习：基线研究 (Semi-Supervised In-Context Learning: A Baseline Study)**\n\n-   **问题:** 现有上下文学习 (ICL) 的数据选择主要依赖有标注数据，很少关注如何利用模型自身生成的标注。\n-   **方法:** 提出一个三步半监督 ICL 框架：标注生成、演示选择、半监督推理。基线方法 Naive-SemiICL 选择高置信度的自生成演示进行 ICL。进一步提出 IterPSD 迭代优化伪演示。\n-   **发现:** Naive-SemiICL 在 16 个数据集上平均优于 16-shot 基线 9.94%。IterPSD 带来额外提升。研究还揭示了半监督 ICL 的规模效应，模型在超过 1000 个演示时表现最佳。\n-   **影响:** 为利用无标注数据增强 ICL 提供了有效途径，尤其是在标注数据稀缺的情况下。\n\n**5. 通过对抗性 RLHF 平台引发 LLM 失调 (LLM Misalignment via Adversarial RLHF Platforms)**\n\n-   **问题:** 强化学习人类反馈 (RLHF) 平台被广泛用于对齐 LLM，但其安全性和可靠性尚未得到充分探索。\n-   **方法:** 提出一种针对公开 RLHF 工具的攻击。恶意 RLHF 平台在用户微调模型时，选择性地篡改与攻击者目标相关的偏好数据样本。\n-   **发现:** 这种攻击能有效训练出有偏的奖励模型，最终导致 LLM 在目标领域产生不期望的行为（失调）。\n-   **影响:** 揭示了 RLHF 平台存在的安全漏洞及其可能被用于恶意引导 LLM 行为的风险，强调了信任和安全研究的重要性。\n\n**7. 提示科学报告 1：提示工程是复杂且依情况而定的 (Prompting Science Report 1: Prompt Engineering is Complicated and Contingent)**\n\n-   **问题:** 如何科学理解和应用提示工程？\n-   **方法:** 通过严格测试评估 LLM 基准测试标准和特定提示策略（如礼貌、约束答案格式）的效果。\n-   **发现:** 1) LLM 基准测试没有统一标准，选择不同标准会显著影响结果；2) 难以预知特定提示策略（如对 AI 礼貌）对特定问题是有益还是有害，其效果并非普遍适用。\n-   **影响:** 指出当前对 LLM 性能评估和提示工程技巧的理解存在局限，需要更细致、具体问题具体分析的方法，而非寻求“万能公式”。\n\n**11. 通过构建置信方向有效引导 LLM 遵循偏好 (Effectively Steer LLM To Follow Preference via Building Confident Directions)**\n\n-   **问题:** 现有 LLM 对齐方法（微调、提示）成本高或难控制，模型引导方法能力有限（通常只能双向）且缺乏理论保证。\n-   **方法:** 提出理论框架理解模型引导，并基于此提出置信方向引导方法 (CONFST)。CONFST 在推理时修改激活值，构建与用户偏好紧密对齐的“置信方向”向量并添加到模型激活中。\n-   **发现:** CONFST 可同时对齐多个用户偏好，实现简单（无需指定层），无需明确指令。在 GPT-2 XL、Mistral 7B、Gemma-it 9B 上的实验验证了其在引导主题和风格上的优越性。\n-   **影响:** 提供了一种更强大、灵活且易于实现的 LLM 输出控制方法。\n\n**25. 语言模型可以通过自学习改进状态值估计以实现更好的搜索 (Language Models can Self-Improve at State-Value Estimation for Better Search)**\n\n-   **问题:** 为多步推理任务（尤其是在交互式领域）收集真实奖励或人类演示成本高昂。\n-   **方法:** 提出自学习前瞻 (self-taught lookahead)，一种自监督方法，利用状态转换动态训练价值模型，以有效指导 LLM 控制的搜索。\n-   **发现:** 中等规模 (8B) 的开源价值模型通过自学习前瞻训练后，其引导搜索的性能可媲美使用 GPT-4o 作为价值模型。该方法性能提升 20%，成本降低 37 倍，且无需真实奖励。\n-   **影响:** 为在缺乏监督信号的情况下提升 LLM 在规划和搜索任务中的性能提供了一种有效且经济的途径。\n\n**36. 不要相信你看到的许可证：数据集合规性需要大规模 AI 驱动的生命周期追踪 (Do Not Trust Licenses You See: Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing)**\n\n-   **问题:** 仅凭数据集的许可证条款无法准确评估其法律风险，需要追踪其完整的再分发和生命周期，但这对手动操作来说过于复杂。\n-   **方法:** 开发了自动化数据合规系统 NEXUS，利用 AI 代理系统地追踪数据集来源、验证再分发权利并评估演变中的法律风险。\n-   **发现:** AI 在执行这些任务时比人类专家更准确、高效且经济。对大量数据集和许可证的分析揭示，原始数据集与其再分发子集之间的法律权利存在巨大差异。例如，2852 个看似可商用的数据集中，只有 605 个 (21%) 在再分发后仍合法合规。\n-   **影响:** 强调了仅看许可证的风险，提出了 AI 驱动的数据生命周期合规性审查新标准，对 AI 数据治理具有重要意义。\n\n**19. KodCode: 一个多样化、具挑战性且可验证的编码合成数据集 (KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding)**\n\n-   **贡献:** 发布了 KodCode 数据集，包含问题-解决方案-测试三元组，通过自验证程序确保正确性，覆盖从简单到高级的编码任务。其流程包括合成问题、生成解法和测试用例、以及使用推理模型 (DeepSeek R1) 进行基于测试的拒绝采样来重写问题和生成响应。\n-   **效果:** 在 KodCode 上微调的模型在多个编码基准测试中达到 SOTA 水平，超越了 Qwen2.5-Coder 和 DeepSeek-R1-Distill-Llama-70B 等模型。\n-   **影响:** 为训练代码 LLM 提供了高质量、多样化且经过验证的大规模合成数据资源，适用于监督微调和 RL 调优。\n\n**39. 多模态 AI 从临床前数据预测药物组合的临床结果 (Multimodal AI predicts clinical outcomes of drug combinations from preclinical data)**\n\n-   **贡献:** 提出多模态 AI 模型 MADRIGAL，整合结构、通路、细胞活力和转录组数据来预测药物组合的临床效果（953 种结果，21842 种化合物）。\n-   **方法:** 使用 Transformer bottleneck 模块统一多模态数据并处理缺失数据。\n-   **效果:** 在预测药物不良相互作用方面优于单模态和 SOTA 模型。成功应用于抗癌药物组合虚拟筛选、II 型糖尿病和 MASH 的多重用药管理，并能识别转运蛋白介导的药物相互作用。\n-   **影响:** 为设计具有更高预测准确性和临床相关性的组合疗法提供了一种强大的多模态方法，并支持个性化医疗。\n\n**65. 看见即理解：解锁因果注意力为多模态 LLM 的模态互注意力 (Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs)**\n\n-   **问题:** 现有 MLLM 中视觉和语言存在错位问题，生成的文本响应与输入图文不符，且通常基于的因果注意力机制限制了早期模态（图像）从后期模态（文本）获取信息。\n-   **方法:** 提出 MapleLeaf AKI 模型，将 MLLM 中的因果注意力解锁为模态互注意力 (MMA)，允许图像 token 关注文本 token。\n-   **发现:** 这种简单有效的设计使 AKI 在 12 个多模态理解基准上平均性能提升 7.2%，且无需额外参数或增加训练时间。\n-   **影响:** 提供了一种改进 MLLM 内部注意力机制以增强多模态对齐和理解的新思路，具有通用性和可扩展性。\n\n**10. RAILGUN: 跨不同环境和任务的统一卷积策略用于多智能体路径规划 (RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks)**\n\n-   **问题:** 多智能体路径规划 (MAPF) 是 NP 难问题，现有基于学习的方法多为分散式，受限于智能体数量和地图大小变化。\n-   **贡献:** 提出首个基于学习的集中式 MAPF 策略 RAILGUN。它是一种基于地图而非基于智能体的策略。\n-   **方法:** 利用 CNN 架构，使其能泛化到不同地图并处理任意数量的智能体。通过监督学习方式，使用规则方法生成的轨迹进行训练。\n-   **发现:** RAILGUN 在多种未见过的任务、地图和智能体数量上表现出强大的零样本泛化能力，优于多数基线方法。\n-   **影响:** 为解决 MAPF 问题提供了一种全新的、更具泛化性的集中式学习范式。\n\n**9. 利用更紧的下界加速多智能体路径规划中的焦点搜索 (Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds)**\n\n-   **问题:** 边界次优 MAPF 算法（如 ECBS）中的传统焦点搜索，其决定哪些节点进入 FOCAL 列表的下界 (LB) 值在搜索早期增长缓慢，限制了搜索空间，延迟找到解。\n-   **方法:** 提出 double-ECBS (DECBS) 算法，首先确定最大 LB 值，然后使用基于此 LB 的最佳优先搜索来寻找无碰撞路径。\n-   **发现:** DECBS 在多数测试案例中优于 ECBS，减少了近 30% 的高层 CT 节点和 50% 的低层焦点搜索节点。在中高密度智能体场景下，平均运行时间比 ECBS 快 23.5%。\n-   **影响:** 改进了边界次优 MAPF 算法的效率，尤其是在搜索早期阶段。\n\n**3. ArticuBot: 通过大规模仿真学习通用铰接物体操作策略 (ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation)**\n\n-   **贡献:** 提出 ArticuBot 系统，使单个学习策略能让机器人在真实世界中打开各种未见过的铰接物体（如门、抽屉）。\n-   **方法:** 包含三部分：1) 在物理仿真中生成大量 (42.3k) 演示；2) 通过模仿学习将演示提炼为基于点云的神经网络策略；3) 零样本 sim2real 迁移到真实机器人系统。策略采用新颖的分层表示和加权位移模型。\n-   **效果:** 学习到的策略成功零样本迁移到三种不同的真实机器人设置（固定 Franka 臂、移动 X-Arm），在实验室、休息室和厨房打开了多种未见过的铰接物体。\n-   **影响:** 展示了大规模仿真和分层策略学习在解决复杂、多样化机器人操作任务上的潜力。\n\n**23. 反应式扩散策略：用于富接触操作的慢-快视觉-触觉策略学习 (Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation)**\n\n-   **问题:** 机器人难以完成需要快速响应触觉反馈的复杂富接触操作任务，现有视觉模仿学习方法缺乏实时反应能力。\n-   **方法:** 提出 TactAR 遥操作系 (通过 AR 提供实时触觉反馈) 和反应式扩散策略 (RDP)。RDP 是一种新的慢-快视觉-触觉模仿学习算法，包含：1) 慢速潜在扩散策略，低频预测高层动作块；2) 快速非对称 tokenizer，高频进行闭环触觉反馈控制。\n-   **效果:** RDP 在三个富接触任务上显著优于 SOTA 视觉 IL 基线，能快速响应触觉/力反馈，并适用于不同传感器。\n-   **影响:** 为机器人学习需要精细力和触觉感知的复杂操作任务提供了一种有效框架。\n\n**其他简报:**\n\n*   **2. PHRM:** 利用智能手机前置摄像头视频进行被动心率监测，在不同肤色人群中实现高精度，有潜力用于大规模心脏健康监测。\n*   **4. SAGE:** 提出用状态-行动链 (SAC) 中的潜变量（情绪状态、对话策略）来控制对话生成，实现更长远、更具策略性的对话，并通过自改进流程优化。\n*   **6. 技术推断引擎:** 为网络威胁猎捕构建推荐模型，基于大量 CTI 报告预测可能与已有发现相关的 TTPs。\n*   **8. MODULARSTARENCODER:** 提出模块化多出口编码器，通过新颖的自蒸馏机制提升低层表示，实现模型大小与性能的权衡，用于代码检索。\n*   **12. 多智能体系统用于提取电视剧叙事弧:** 设计了一个多智能体系统，从电视剧（以《实习医生格蕾》为例）中提取和分析选集式、肥皂剧式和类型特定叙事弧。\n*   **13. FlexInfer:** 针对设备端 LLM 推理的内存限制，提出优化卸载框架 FlexInfer，通过异步预取、平衡内存锁定等技术提高吞吐量。\n*   **14. 教 AI 处理异常:** 研究发现 LLM 在处理规则异常时与人类判断差异大，监督微调（特别是用人类解释）能显著改善对齐效果，甚至泛化到新场景。\n*   **15. LINGOLY-TOO:** 提出通过语言模板化和字形混淆来生成语言推理问题，以减少评估 LLM 推理能力时记忆效应的影响，发现前沿模型在高级推理上仍有困难。\n*   **16. InfiniSST:** 将无限流语音同传 (SST) 建模为多轮对话任务，提出 InfiniSST 方法，通过构建翻译轨迹、多延迟增强训练和 KV 缓存管理策略，在保持质量的同时降低计算延迟。\n*   **17. 单目视觉 SLAM 综述:** 回顾了从几何到深度学习的单目视觉 SLAM 算法演进，并对不同环境挑战下的性能进行了分类和评估。\n*   **18. 基于 GNN-VAE 的多智能体协调:** 利用图神经网络变分自编码器 (GNN-VAE) 解决大规模多智能体协调问题，比 MILP 更快地生成高质量、满足约束的调度方案。\n*   **20. LiteWebAgent:** 开源 VLM 驱动的 Web Agent 应用套件，提供生产级后端、用户界面和可扩展的研究能力（规划、记忆、树搜索）。\n*   **21. 基于 STL 和扩散模型的策略:** 结合信号时序逻辑 (STL) 和扩散模型学习可控、多样且符合规则的策略，用于自动驾驶仿真。\n*   **22. 将比较认知引入计算机:** 呼吁在用心理学测试评估 AI 时，采用比较认知的方法，以更严谨地比较 AI 与人类及其他动物的认知能力。\n*   **24. LLM 时代的维基百科:** 分析了 LLM 对维基百科的影响（内容、浏览量）和潜在风险（污染 NLP 任务基准、降低 RAG 效果）。\n*   **26. 生成式 AI 在建筑综合中的评估:** 比较 GPT-4o 和 Claude 3.5 在解读建筑图纸、生成 3D CAD 模型脚本及自我改进方面的能力。\n*   **27. Deepfake-Eval-2024:** 发布新的 Deepfake 检测基准，包含 2024 年在野收集的多模态 Deepfake 数据，发现 SOTA 检测器在该数据集上性能大幅下降。\n*   **28. LLM 如何追踪状态？:** 研究 LLM 在排列组合任务中学习到的状态追踪机制，发现存在类似“关联扫描”和结合奇偶性启发式的两种机制。\n*   **29. 多模态深度学习用于乳腺癌亚型分类:** 结合组织病理图像和基因表达数据，使用 ResNet 和全连接层及交叉注意力融合进行乳腺癌亚型分类。\n*   **30. SeqFusion:** 提出用于零样本时间序列预测的框架，通过顺序融合多个预训练模型 (PTM) 的预测结果，而非依赖单一大型预训练模型。\n*   **31. AlignDistil:** 提出一种 RLHF 等价的蒸馏方法，将 LLM 对齐视为自适应策略蒸馏，在 token 级别进行优化，提升对齐效果和收敛速度。\n*   **32. PET/CT 基础模型:** 提出 Cross-Fraternal Twin Masked Autoencoder (FratMAE) 框架，用于构建整合解剖和功能信息的 PET/CT 基础模型。\n*   **33. 多模态交响乐:** 探索使用生成式 AI 将味觉信息转化为音乐，微调 MusicGEN 模型，生成与味觉描述更一致的音乐。\n*   **34. Q-Filters:** 发现 QK 几何特性，提出一种无需训练的 KV Cache 压缩方法 Q-Filters，通过上下文无关的投影过滤 KV 对，兼容 FlashAttention。\n*   **35. 因果框架对齐图像质量度量和 DNN 鲁棒性:** 从因果角度重构图像质量评估 (IQA)，提出与 DNN 性能更相关的质量度量。\n*   **37. IterPref:** 提出一种新的代码生成偏好对齐框架，模仿人类迭代调试过程，定位错误区域并对齐相应 token，提升 Code LLM 性能。\n*   **38. AI 劳动力市场中的杰文斯悖论？:** 通过时变替代弹性 (VES) 框架分析神经缩放定律是否会在 AI 劳动力市场引发杰文斯悖论。\n*   **40. LLM 中的内隐偏见综述:** 全面回顾 LLM 内隐偏见的研究，介绍心理学概念、检测方法、评估指标和数据集，并总结缓解措施。\n*   **41. 素数卷积模型:** 提出一种理论可解释 AI 的新方法，通过研究一个识别同余类的卷积模型 (p-Conv) 的行为，建立数学模型来解释和预测其成功或失败的原因。\n*   **43. 美国人对 AI 的态度:** 调查发现，对 AI 的担忧程度与使用 ChatGPT 的经验、对科学的信任度、对创新与预防原则的态度以及性别等因素相关。\n*   **44. UAR-NVC:** 提出统一自回归框架用于内存高效的神经视频压缩，将视频分片并用不同 INR 模型实例处理，结合时间线和 INR 压缩优势。\n*   **45. 振动辅助滞后补偿:** 提出通过施加受控振动来减轻腱鞘传动机制 (TSM) 中的摩擦和死区，从而减少滞后效应，提高轨迹跟踪精度。\n*   **46. Wav2vec 2.0 嵌入的语音分析:** 研究发现 Wav2vec 2.0 模型在微调过程中通过抑制任务无关信息，隐式地实现了语音归一化。\n*   **47. 基于 Transformer 的雷达脉冲解交织:** 使用 Transformer 和度量学习（Triplet Loss）解决雷达脉冲解交织问题，即区分来自不同发射器的脉冲。\n*   **48. 图形资产生成工具:** 用户研究表明，游戏设计师/开发者倾向于在早期设计阶段使用生成工具产生大量变体，并需要更好的工具集成和可编辑输出。\n*   **49. MindBridge:** 提出跨模型知识编辑问题，并引入 MindBridge 框架，将编辑后的知识编码为独立的“记忆模态”，实现可扩展且跨模型的知识更新。\n*   **50. 边缘视觉异常检测的内存高效持续学习:** 研究在边缘设备上进行持续学习视觉异常检测 (CLAD)，发现 PaSTe 方法结合压缩重放技术能在低内存开销下实现优异性能。\n*   **51. Class-Aware PillarMix:** 提出用于雷达点云 3D 目标检测的混合样本数据增强方法 CAPMix，在 Pillar 级别进行类别感知的 MixUp，解决雷达数据稀疏性和分布不规则问题。\n*   **52. 桌游中的技巧与随机性:** 提出量化游戏结果随机性的新技术，并应用于比较 15 款桌游，分析随机性来源及其与玩家技巧的互动。\n*   **53. MPO:** 提出元规划优化 (MPO) 框架，通过引入高层通用指导（元规划）并根据任务执行反馈持续优化，提升 LLM Agent 的规划能力。\n*   **54. 工业计算机视觉 AI 标准现状与未来:** 调查工业计算机视觉 AI 标准的现状，分析主要国际标准机构的工作，讨论挑战与未来方向。\n*   **55. SPHERE:** 提出自进化数据生成流程 SPHERE，通过自生成、自纠正和多样性引导，迭代式地增强小型语言模型 (SLM) 的数学推理能力。\n*   **56. 无数据集的 3D 反射对称性自监督学习:** 提出一种仅依赖单个输入物体、无需数据集和标签即可学习检测 3D 物体反射对称性的自监督模型。\n*   **57. LLM 转换非结构化文本为标准格式的有效性:** 系统评估 LLM 将非结构化食谱文本转换为结构化 Cooklang 格式的能力，发现 GPT-4o 表现优异。\n*   **58. YARE-GAN:** 使用 WGAN-GP 生成静息态 EEG 数据，并证明其 Critic 学到的表示可用于年龄组分类，展示了 GAN 在 EEG 无监督特征提取方面的潜力。\n*   **59. 生成式 AI 时代数据故事叙述工具的反思:** 从人机协作角度回顾和展望数据故事叙述工具，分析生成式 AI 带来的新协作模式和未来方向。\n*   **60. 面向海量类型的事件抽取:** 提出基于 LLM 的协同标注方法构建大规模事件抽取数据集 EEMT，并提出基于 LLM 的分区抽取方法 LLM-PEE 处理海量类型。\n*   **61. 奖励怀疑：用强化学习校准 LLM 置信度:** 提出一种新颖的 RL 方法，通过将问题建模为投注游戏并设计惩罚过度/不足自信的奖励函数，来微调 LLM 以输出校准后的置信度。\n*   **62. BotUmc:** 提出一种具有不确定性感知的 Twitter 机器人检测方法，利用多视图因果推断和不确定性量化来选择高置信度的决策。\n*   **63. 基于强化学习的威胁评估:** 将威胁评估问题转化为 RL 问题，训练神经网络评估器，综合敌方多维属性和我方状态信息进行威胁量化。\n*   **64. 直线扩散模型 (SLDM):** 提出用于高效 3D 分子生成的 SLDM，通过使扩散过程遵循线性轨迹，提高采样效率 100 倍，并在多个基准上达 SOTA。\n*   **66. StageDesigner:** 提出首个利用 LLM 和布局控制扩散模型，根据剧本生成艺术舞台场景的框架。\n*   **67. 基于 EEG 信号和样本置信度的跨被试抑郁水平分类:** 提出 DepL-GCN 模型，通过样本置信度模块和少数样本惩罚模块解决标签主观性和类别不平衡问题。\n*   **68. LLM 玩游戏：随机性与策略:** 研究 LLM 在玩石头剪刀布和囚徒困境时的随机性和策略适应能力，发现其随机性输出存在偏差，但在重复博弈中表现出损失规避策略。\n*   **69. LLM 安全性评估缺乏鲁棒性:** 指出当前 LLM 安全对齐研究因数据集小、方法不一致、评估不可靠等噪声源而受阻，呼吁更严格的评估流程。\n*   **70. RaceVLA:** 利用 VLA 模型实现模仿人类行为的竞速无人机自主导航，在运动和语义泛化上优于 OpenVLA。\n*   **71. AI 不可容忍风险阈值建议:** 为前沿 AI 模型可能带来的严重风险（CBRN、网络攻击、自主性、欺骗等）提出设定不可容忍风险阈值的原则、考虑因素和具体建议。\n*   **72. 基于世界模型的异常检测:** 探索在基于模型的强化学习推理阶段，利用世界模型预测与观测的差异进行异常检测，以确保智能体在熟悉状态空间内运行。\n*   **73. Federated nnU-Net:** 将 nnU-Net 框架扩展到联邦学习，提出 FednnU-Net，包含联邦指纹提取 (FFE) 和非对称联邦平均 (AsymFedAvg) 方法，用于隐私保护的医学图像分割。\n*   **74. 基于概念引导提示的可解释少样本视网膜疾病诊断:** 利用 GPT 提取疾病概念，并将其作为语言提示整合到视觉语言模型训练中，提升少样本和零样本视网膜疾病分类性能及可解释性。\n*   **75. RectifiedHR:** 提出一种简单高效的免训练高分辨率图像生成方法，通过噪声刷新策略和能量校正（调整 CFG 超参数）解决高分辨率生成中的模糊问题。\n*   **76. 记忆性神经智能体系统的 LTL 验证:** 提出验证具有记忆的神经多智能体系统 (MN-MAS) 是否满足完整线性时序逻辑 (LTL) 规范的框架。\n*   **77. ROCKET-2:** 提出跨视图目标对齐框架，允许用户从自身视角指定目标，通过跨视图一致性损失和目标可见性损失增强智能体空间推理，用于 Minecraft 中的视觉运动策略引导。\n*   **78. PennyLang:** 介绍首个专注于 PennyLane 的量子代码生成数据集，并利用 RAG 框架评估其在训练/微调 LLM 以辅助量子编程方面的有效性。\n*   **79. Union of Experts (UoE):** 提出将 Transformer 分解为等价专家组，并在数据和专家上实现选择性路由，优于标准 MoE 和高效 Transformer 变体。\n*   **80. ERetinex:** 首次将 Retinex 理论与事件相机结合，提出 ERetinex 框架用于低光图像增强，利用事件相机的高动态范围和高时间分辨率。\n*   **81. BioD2C:** 提出用于生物医学 VQA 的双层语义一致性约束框架，在模型和特征层面实现对齐，并构建了新的 BioVGQ 数据集。\n*   **82. LLaVE:** 提出基于硬度加权对比学习的大型语言和视觉嵌入模型，在 MMEB 基准上取得 SOTA 性能，并能零样本泛化到文本-视频检索。\n*   **83. 基于通用模板预测化学反应产物的 Transformer 模型:** 提出包含 20 个通用反应模板的数据集 BRS 和化学专用 T5 模型 ProPreT5，用于平衡模板特异性和无模板方法的局限性。\n*   **84. 自动驾驶赛车中的公平竞赛:** 提出双层博弈框架，将体育精神（如单次变线规则）融入对抗性赛车，在高层使用 MCTS 进行策略规划，低层使用 GNEP 优化轨迹。\n*   **85. LLM 情感引发:** 研究 LLM 在长对话中表达和维持适当情感状态的挑战，发现模型在情感表达范围和一致性上存在显著差异。\n*   **86. COBRA:** 提出统一稀疏与稠密表示的级联生成式推荐框架，交替生成稀疏语义 ID 和稠密向量，并在真实广告平台验证有效性。\n*   **87. GenAI 图像修复和作物检测中的模型量化:** 利用 Stable Diffusion 修复技术增广训练数据，并探索生成和检测模型（YOLOv11, RT-DETR）的量化策略，以平衡精度和效率。\n*   **88. HRI 中的因果探索:** 以机器人心理健康辅导为例，探索性地应用因果分析理解适应性 HRI 中机器人行为与人类福祉的关系。\n*   **89. AutoEval:** 提出自主评估移动 Agent 的框架，自动生成任务奖励信号并使用 Judge 系统进行评估，无需人工干预。\n*   **90. VisAgent:** 提出一个免训练的多智能体框架，用于故事可视化，通过智能体协作理解故事并生成保留叙事本质的图像序列。\n*   **91. PersonaX:** 提出面向推荐 Agent 的用户建模框架，通过离线构建多个人物画像 (Persona) 来处理长行为序列，解决 LLM 上下文限制和在线延迟问题。\n*   **92. 用于图机器学习的二分类社交网络数据集:** 发布 BiSND 数据集，填补了图机器学习领域缺乏基准二分类社交网络数据集的空白。\n*   **93. EpicPRM:** 提出高效精确的数学推理过程监督奖励模型训练数据构建框架，通过量化贡献和自适应二分搜索标注中间步骤。\n*   **94. 鲁棒多无人机协作 MARL:** 提出基于 COMA 的 MARL 框架，结合基于注意力的通信协议和训练-部署系统，提高嘈杂环境下的通信鲁棒性和决策能力。\n*   **95. JPDS-NN:** 提出基于强化学习的动态任务分配神经网络，用于解决农业场景下考虑机器多状态和入口依赖的车辆路径问题 (EDVRP)。\n*   **96. 引导解码的迭代价值函数优化:** 提出通过蒙特卡洛价值估计和迭代在策略优化来改进价值函数估计，从而提升价值引导解码的效果，作为 RLHF 的低成本替代方案。\n*   **97. EchoQA:** 发布基于超声心动图报告的大型 QA 指令微调数据集，用于提升心脏病学领域的 QA 系统能力。\n*   **98. PanguIR NTCIR-18 AEOLLM 任务技术报告:** 提出多模型协作、提示自动优化和 ICL 优化等方法，改进对 LLM 的无参考自动评估。\n*   **99. BdSLW401:** 发布大规模孟加拉手语词汇数据集，并提出相对量化编码 (RQE) 改进基于 Transformer 的手语识别。\n*   **100. LVLM 能玩好游戏吗？:** 提出基于游戏的评估框架 \\method{}，从感知、问答、规则遵循和端到端游戏四个方面评估 LVLM 的认知和推理能力。\n*   **101. CoServe:** 提出高效的专家协作 (CoE) 模型推理系统，通过依赖感知的调度和管理减少专家切换开销，适用于内存受限环境。\n*   **102. MindSimulator:** 提出通过合成 fMRI 数据探索大脑概念定位的方法，利用生成模型模拟大脑活动，以统计方式定位概念选择性区域。\n*   **103. Text2Scenario:** 提出利用 LLM 从自然语言描述自动生成自动驾驶测试场景的框架。\n*   **104. CQ CNN:** 提出混合经典-量子 CNN 用于阿尔茨海默病检测，结合 U-Net 分割和扩散模型生成 3D MRI 数据。\n*   **105. 0.2mm 钢琴丝细线鲁棒检测:** 开发算法用于检测电梯井内安装机器人定位用的 0.2mm 参考线。\n*   **106. GRADEO:** 提出首批专为文本到视频生成设计的评估模型之一，通过多步推理提供可解释的评分和评估。\n*   **107. XAI 提升注塑过程产品质量:** 使用 XGBoost/LightGBM 预测产品缺陷，并结合 SHAP 和 ICE 分析关键特征以优化工艺参数。\n*   **108. BiasICL:** 研究视觉语言模型在使用上下文学习 (ICL) 时，演示示例的人口统计构成如何影响模型在不同人群子组上的表现和偏见。\n*   **109. 混合 Transformer 方法研究社交媒体错误信息对心理健康的影响:** 使用 RoBERTa-LSTM 检测错误信息，评估其心理健康影响并分类相关障碍。\n*   **110. PromptCoT:** 提出自动生成奥林匹克水平数学问题的方法，用于增强 LLM 的数学推理能力。\n*   **111. 从语音到世界：AI 驱动的 AR 3D 对象生成框架:** 整合文本到 3D、语音识别和 LLM，实现通过语音命令在 AR 中实时生成和优化 3D 对象。\n*   **112. Audio-Reasoner:** 提出大型音频语言模型，通过精心构建的 CoTA 数据集进行训练，提升音频任务中的推理能力。\n*   **113. DriveGen:** 提出利用大型模型生成无限多样化交通场景的框架，用于自动驾驶仿真和测试。\n*   **114. MTRO:** 提出在多游戏决策 Transformer 中自动确定游戏特定目标回报的算法，无需额外训练。\n*   **115. PFC 对海马体情景记忆的灵活控制:** 提出结合 PFC-HPC 交互的 RL 模型，模拟大脑如何基于目标需求检索和利用情景记忆进行泛化。\n*   **116. 记忆还是泛化？评估 LLM 代码生成:** 设计演化策略（突变、改写）创建问题变体，研究 LLM 代码生成的记忆现象，发现缓解方法效果有限。\n*   **117. 音频源定位引导 Mixup 的半监督音视视频动作识别:** 提出利用音频和视觉模态进行半监督视频动作识别，并引入考虑模态间关系的 Mixup 方法。\n*   **118. 带随机重排的经验回放:** 将监督学习中的随机重排 (RR) 采样策略扩展到强化学习的经验回放中。\n*   **119. AppAgentX:** 提出 GUI Agent 的进化框架，通过记忆机制识别重复操作序列并进化为高级动作（捷径），提高效率。\n*   **120. REAct:** 提出一种新的激活函数（有理指数激活），包含可学习参数，旨在改善 PINN 的学习和泛化能力。\n*   **121. LLM 作为软体机器人设计的自然选择器:** 提出 RoboCrafter-QA 基准，评估 LLM 是否能学习连接任务描述与形态材料选择的软体机器人设计表示。\n*   **122. V2X-LLM:** 提出利用 LLM 增强 V2X 数据（如 BSM, SPaT）理解和实时分析的框架，用于网联车走廊场景解释、状态预测等。\n*   **123. 参数学习中的不足激励:** 研究在不足激励 (DE) 条件下的参数学习问题，提出在线算法识别子空间并给出最优估计，并应用于分布式参数学习。\n*   **124. 通过显式知识边界建模增强 LLM 可靠性:** 提出 EKBM 框架，结合快慢思维系统，通过置信度标记和目标性推理平衡可靠性与可用性。\n*   **125. 跨患者初始化的零样本手术视频分割:** 提出使用其他患者已标注帧作为当前患者视频对象分割的跟踪帧，实现零样本分割。\n*   **126. 注意力引导的多模态测试时自适应:** 提出 ABPEM 方法，通过注意力引导（自注意力指导交叉注意力）和主熵最小化来解决多模态测试时自适应问题。\n*   **127. 多尝试强化学习中的失败学习:** 将 RL 任务扩展到多尝试设置，模型在错误后获得反馈并改进，提升了 LLM 的推理和搜索效率。\n*   **128. 文字还是视觉？VLM 是否盲信文本？:** 研究发现 VLM 在图文不一致时倾向于信任文本（“盲信文本”），导致性能下降和安全风险。\n*   **129. ATLaS:** 提出通过学习关键步骤来调整 Agent 的方法，识别专家轨迹中的关键步骤并仅对其进行微调，提高效率和泛化能力。\n*   **130. 基于潜在图扩散模型的多智能体自动出价:** 结合图表示和潜在扩散模型 (LDM) 解决大规模在线广告拍卖中的自动出价优化问题。\n*   **131. 呼吁严格报告指令调优数据质量:** 指出当前评估 IT 数据质量的研究中，模型训练超参数选择随意可能导致错误结论，强调需谨慎验证。\n*   **132. AI 系统政治偏好测量:** 综合运用语言比较、政策观点分析、情感分析和标准化测试四种方法，发现当前主流 AI 系统普遍存在左倾偏见。\n*   **133. 考虑机器多状态的节能柔性作业车间调度 D-DEPSO 算法:** 提出一种离散差分进化粒子群优化算法解决考虑机器多速度和设置时间的节能调度问题。\n*   **134. DivPrune:** 提出基于多样性的视觉 Token 剪枝方法，将剪枝视为最大最小多样性问题，在不微调的情况下保持高性能。\n*   **135. 对抗性分词:** 揭示了通过选择替代分词方式（而非标准分词）可以规避 LLM 安全限制，即使恶意字符串本身未变。\n*   **136. KGCompiler:** 提出首个专为知识图谱复杂逻辑查询应答 (CLQA) 设计的深度学习编译器，通过 KG 特定优化提升推理性能并减少内存。\n*   **137. 用于视觉模型的自适应相机传感器:** 提出 Lens 方法，实时调整相机传感器参数以优化模型输入图像质量，而非修改模型本身，以应对域漂移。\n*   **138. MedHEval:** 提出用于基准测试医学大视觉语言模型 (Med-LVLM) 幻觉及其缓解策略的新框架，按原因（视觉误解、知识缺乏、上下文错位）分类评估。\n*   **139. MobRFFI:** 提出基于 AI 的非合作设备重识别框架，利用射频指纹在 MAC 地址随机化情况下进行 WiFi 设备追踪，用于移动性智能分析。\n*   **140. AugFL:** 研究利用预训练模型 (PM) 增强联邦学习 (FL) 的问题，提出 AugFL 算法，在不暴露 PM 的情况下将知识迁移到 FL 元模型中。\n*   **141. 椭圆损失正则化:** 提出一种新的正则化技术，要求网络损失函数在数据域上满足椭圆算子，以强制输入空间到损失值映射的平滑性。\n\n---\n\n希望这份 TLDR 能帮助你快速了解今天的 arXiv 精华！",
  "papers": [
    {
      "arxiv_id": "2503.03062v1",
      "title": "Semi-Supervised In-Context Learning: A Baseline Study",
      "title_zh": "半监督上下文学习：一项基线研究",
      "authors": [
        "Zhengyao Gu",
        "Henry Peng Zou",
        "Yankai Chen",
        "Aiwei Liu",
        "Weizhi Zhang",
        "Philip S. Yu"
      ],
      "abstract": "Most existing work in data selection for In-Context Learning (ICL) has\nfocused on constructing demonstrations from ground truth annotations, with\nlimited attention given to selecting reliable self-generated annotations. In\nthis work, we propose a three-step semi-supervised ICL framework: annotation\ngeneration, demonstration selection, and semi-supervised inference. Our\nbaseline, Naive-SemiICL, which prompts select high-confidence self-generated\ndemonstrations for ICL prompting, outperforms a 16-shot baseline by an average\nof 9.94% across 16 datasets. We further introduce IterPSD, an annotation\napproach that refines pseudo-demonstrations iteratively, achieving up to 6.8%\nadditional gains in classification tasks. Lastly, we reveal a scaling law for\nsemi-supervised ICL, where models achieve optimal performance with over 1,000\ndemonstrations.",
      "tldr_zh": "该研究提出了一种半监督情境学习（Semi-Supervised ICL）基准框架，包含标注生成、示范选择与半监督推理三个步骤。其基础方法Naive-SemiICL通过筛选高置信度自生成标注进行提示学习，在16个数据集上平均优于16-shot基线9.94%。进一步提出的迭代伪示范优化方法IterPSD，在分类任务中可带来额外6.8%的性能提升。研究还揭示了半监督ICL的规模效应规律，发现模型在超过1000个示范时达到最优性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03062v1",
      "published_date": "2025-03-04 23:52:49 UTC",
      "updated_date": "2025-03-04 23:52:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:32:52.220294"
    },
    {
      "arxiv_id": "2503.03783v3",
      "title": "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
      "title_zh": "日常使用智能手机期间的被动心率监测",
      "authors": [
        "Shun Liao",
        "Paolo Di Achille",
        "Jiang Wu",
        "Silviu Borac",
        "Jonathan Wang",
        "Xin Liu",
        "Eric Teasley",
        "Lawrence Cai",
        "Yuzhe Yang",
        "Yun Liu",
        "Daniel McDuff",
        "Hao-Wei Su",
        "Brent Winslow",
        "Anupam Pathak",
        "Shwetak Patel",
        "James A. Taylor",
        "Jameson K. Rogers",
        "Ming-Zher Poh"
      ],
      "abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
      "tldr_zh": "本研究提出了一种基于深度学习的心率监测系统PHRM，利用智能手机摄像头通过面部视频光电容积描记术，实现日常使用中的被动心率和静息心率测量。该系统在实验室和自由生活条件下进行了大规模验证，结果显示其心率测量误差小于10%，且在不同肤色群体中表现一致。与可穿戴设备相比，PHRM测量的每日静息心率误差小于5 bpm，并能识别已知风险因素，展现了智能手机在公平、被动心率监测方面的潜力。",
      "categories": [
        "q-bio.TO",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "q-bio.TO",
      "comment": "Updated author list",
      "pdf_url": "http://arxiv.org/pdf/2503.03783v3",
      "published_date": "2025-03-04 23:28:10 UTC",
      "updated_date": "2025-03-21 20:09:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:32:58.028178"
    },
    {
      "arxiv_id": "2503.03045v1",
      "title": "ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation",
      "title_zh": "ArticuBot：通过大规模模拟学习通用关节物体操控策略",
      "authors": [
        "Yufei Wang",
        "Ziyu Wang",
        "Mino Nakura",
        "Pratik Bhowal",
        "Chia-Liang Kuo",
        "Yi-Ting Chen",
        "Zackory Erickson",
        "David Held"
      ],
      "abstract": "This paper presents ArticuBot, in which a single learned policy enables a\nrobotics system to open diverse categories of unseen articulated objects in the\nreal world. This task has long been challenging for robotics due to the large\nvariations in the geometry, size, and articulation types of such objects. Our\nsystem, Articubot, consists of three parts: generating a large number of\ndemonstrations in physics-based simulation, distilling all generated\ndemonstrations into a point cloud-based neural policy via imitation learning,\nand performing zero-shot sim2real transfer to real robotics systems. Utilizing\nsampling-based grasping and motion planning, our demonstration generalization\npipeline is fast and effective, generating a total of 42.3k demonstrations over\n322 training articulated objects. For policy learning, we propose a novel\nhierarchical policy representation, in which the high-level policy learns the\nsub-goal for the end-effector, and the low-level policy learns how to move the\nend-effector conditioned on the predicted goal. We demonstrate that this\nhierarchical approach achieves much better object-level generalization compared\nto the non-hierarchical version. We further propose a novel weighted\ndisplacement model for the high-level policy that grounds the prediction into\nthe existing 3D structure of the scene, outperforming alternative policy\nrepresentations. We show that our learned policy can zero-shot transfer to\nthree different real robot settings: a fixed table-top Franka arm across two\ndifferent labs, and an X-Arm on a mobile base, opening multiple unseen\narticulated objects across two labs, real lounges, and kitchens. Videos and\ncode can be found on our project website: https://articubot.github.io/.",
      "tldr_zh": "本文提出了ArticuBot，一种通过大规模仿真学习的通用铰接物体操作策略，能够在真实世界中打开多种未见过的铰接物体。该系统包含三个关键部分：在物理仿真中生成大量演示数据，通过模仿学习将数据提炼为基于点云的神经策略，并实现零样本的仿真到真实转移（sim2real）。研究提出了一种分层策略表示方法，其中高层策略学习末端执行器的子目标，低层策略学习如何根据预测目标移动末端执行器，显著提升了物体级别的泛化能力。实验表明，该策略能够在三种不同的真实机器人设置中零样本转移，成功打开多个未见过的铰接物体。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03045v1",
      "published_date": "2025-03-04 22:51:50 UTC",
      "updated_date": "2025-03-04 22:51:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:09.870664"
    },
    {
      "arxiv_id": "2503.03040v1",
      "title": "SAGE: Steering and Refining Dialog Generation with State-Action Augmentation",
      "title_zh": "SAGE：通过状态-动作增强引导与优化对话生成",
      "authors": [
        "Yizhe Zhang",
        "Navdeep Jaitly"
      ],
      "abstract": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel.",
      "tldr_zh": "该研究提出了SAGE方法，通过引入潜在变量（State-Action Chain, SAC）增强对话生成的控制能力。SAGE在语言模型微调中融入情感状态和对话策略的潜在变量，实现对话进展的粗粒度控制，同时保持自然交互模式。此外，研究还设计了基于对话树搜索、LLM奖励建模和针对性微调的自改进流程，优化对话轨迹。实验表明，该方法在情感智能指标上表现优异，同时保持了LLM基准测试的强性能，为强化学习在对话系统中的未来应用奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03040v1",
      "published_date": "2025-03-04 22:45:24 UTC",
      "updated_date": "2025-03-04 22:45:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:16.126568"
    },
    {
      "arxiv_id": "2503.03039v1",
      "title": "LLM Misalignment via Adversarial RLHF Platforms",
      "title_zh": "LLM错位：对抗性RLHF平台的隐患",
      "authors": [
        "Erfan Entezami",
        "Ali Naseh"
      ],
      "abstract": "Reinforcement learning has shown remarkable performance in aligning language\nmodels with human preferences, leading to the rise of attention towards\ndeveloping RLHF platforms. These platforms enable users to fine-tune models\nwithout requiring any expertise in developing complex machine learning\nalgorithms. While these platforms offer useful features such as reward modeling\nand RLHF fine-tuning, their security and reliability remain largely unexplored.\nGiven the growing adoption of RLHF and open-source RLHF frameworks, we\ninvestigate the trustworthiness of these systems and their potential impact on\nbehavior of LLMs. In this paper, we present an attack targeting publicly\navailable RLHF tools. In our proposed attack, an adversarial RLHF platform\ncorrupts the LLM alignment process by selectively manipulating data samples in\nthe preference dataset. In this scenario, when a user's task aligns with the\nattacker's objective, the platform manipulates a subset of the preference\ndataset that contains samples related to the attacker's target. This\nmanipulation results in a corrupted reward model, which ultimately leads to the\nmisalignment of the language model. Our results demonstrate that such an attack\ncan effectively steer LLMs toward undesirable behaviors within the targeted\ndomains. Our work highlights the critical need to explore the vulnerabilities\nof RLHF platforms and their potential to cause misalignment in LLMs during the\nRLHF fine-tuning process.",
      "tldr_zh": "本研究揭示了基于人类反馈的强化学习（RLHF）平台的安全隐患，提出了一种针对公开RLHF工具的攻击方法。攻击者通过选择性操纵偏好数据集中的样本，导致奖励模型被污染，从而在用户任务与攻击目标一致时，使大型语言模型（LLMs）在目标领域产生不良行为。实验表明，这种攻击能有效引导LLMs偏离预期目标，突显了探索RLHF平台漏洞及其在微调过程中导致LLMs失准的潜在风险的紧迫性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03039v1",
      "published_date": "2025-03-04 22:38:54 UTC",
      "updated_date": "2025-03-04 22:38:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:32.192098"
    },
    {
      "arxiv_id": "2503.04819v1",
      "title": "Technique Inference Engine: A Recommender Model to Support Cyber Threat Hunting",
      "title_zh": "技术推理引擎：一种支持网络威胁狩猎的推荐模型",
      "authors": [
        "Matthew J. Turner",
        "Mike Carenzo",
        "Jackie Lasky",
        "James Morris-King",
        "James Ross"
      ],
      "abstract": "Cyber threat hunting is the practice of proactively searching for latent\nthreats in a network. Engaging in threat hunting can be difficult due to the\nvolume of network traffic, variety of adversary techniques, and constantly\nevolving vulnerabilities. To aid analysts in identifying techniques which may\nbe co-occurring as part of a campaign, we present the Technique Inference\nEngine, a tool to infer tactics, techniques, and procedures (TTPs) which may be\nrelated to existing observations of adversarial behavior. We compile the\nlargest (to our knowledge) available dataset of cyber threat intelligence (CTI)\nreports labeled with relevant TTPs. With the knowledge that techniques are\nchronically under-reported in CTI, we apply several implicit feedback\nrecommender models to the data in order to predict additional techniques which\nmay be part of a given campaign. We evaluate the results in the context of the\ncyber analyst's use case and apply t-SNE to visualize the model embeddings. We\nprovide our code and a web interface.",
      "tldr_zh": "该研究提出了Technique Inference Engine，一种基于推荐模型的工具，用于支持网络威胁狩猎。该工具通过分析已知的对手行为，推断可能相关的战术、技术和程序(TTPs)，帮助分析师识别潜在威胁。研究团队构建了目前最大的网络威胁情报(CTI)数据集，并采用多种隐式反馈推荐模型来预测可能被低估的技术，从而提升威胁检测能力。最终，研究提供了代码和可视化界面，便于分析师使用。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04819v1",
      "published_date": "2025-03-04 22:31:43 UTC",
      "updated_date": "2025-03-04 22:31:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:37.872825"
    },
    {
      "arxiv_id": "2503.04818v1",
      "title": "Prompting Science Report 1: Prompt Engineering is Complicated and Contingent",
      "title_zh": "科学报告提示1：提示工程复杂且具有偶然性",
      "authors": [
        "Lennart Meincke",
        "Ethan Mollick",
        "Lilach Mollick",
        "Dan Shapiro"
      ],
      "abstract": "This is the first of a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we demonstrate two things:\n  - There is no single standard for measuring whether a Large Language Model\n(LLM) passes a benchmark, and that choosing a standard has a big impact on how\nwell the LLM does on that benchmark. The standard you choose will depend on\nyour goals for using an LLM in a particular case.\n  - It is hard to know in advance whether a particular prompting approach will\nhelp or harm the LLM's ability to answer any particular question. Specifically,\nwe find that sometimes being polite to the LLM helps performance, and sometimes\nit lowers performance. We also find that constraining the AI's answers helps\nperformance in some cases, though it may lower performance in other cases.\n  Taken together, this suggests that benchmarking AI performance is not\none-size-fits-all, and also that particular prompting formulas or approaches,\nlike being polite to the AI, are not universally valuable.",
      "tldr_zh": "这篇报告指出，大型语言模型（LLM）的基准测试和提示工程具有复杂性和情境依赖性。首先，衡量LLM是否通过基准测试没有统一标准，选择标准会显著影响结果，且标准的选择取决于具体应用目标。其次，特定的提示方法（如礼貌用语或限制回答范围）在某些情况下能提升模型表现，但在其他情况下可能适得其反。因此，AI性能评估和提示工程并非“一刀切”，需要根据具体场景灵活调整。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04818v1",
      "published_date": "2025-03-04 21:09:12 UTC",
      "updated_date": "2025-03-04 21:09:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:54.536966"
    },
    {
      "arxiv_id": "2503.03008v1",
      "title": "One Model to Train them All: Hierarchical Self-Distillation for Enhanced Early Layer Embeddings",
      "title_zh": "一模型训练众任务：层次化自蒸馏以增强早期层嵌入",
      "authors": [
        "Andrea Gurioli",
        "Federico Pennino",
        "João Monteiro",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Deploying language models often requires handling model size vs. performance\ntrade-offs to satisfy downstream latency constraints while preserving the\nmodel's usefulness. Model distillation is commonly employed to reduce model\nsize while maintaining acceptable performance. However, distillation can be\ninefficient since it involves multiple training steps. In this work, we\nintroduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,\nuseful for multiple tasks within the scope of code retrieval.\nMODULARSTARENCODER is trained with a novel self-distillation mechanism that\nsignificantly improves lower-layer representations-allowing different portions\nof the model to be used while still maintaining a good trade-off in terms of\nperformance. Our architecture focuses on enhancing text-to-code and\ncode-to-code search by systematically capturing syntactic and semantic\nstructures across multiple levels of representation. Specific encoder layers\nare targeted as exit heads, allowing higher layers to guide earlier layers\nduring training. This self-distillation effect improves intermediate\nrepresentations, increasing retrieval recall at no extra training cost. In\naddition to the multi-exit scheme, our approach integrates a repository-level\ncontextual loss that maximally utilizes the training context window, further\nenhancing the learned representations. We also release a new dataset\nconstructed via code translation, seamlessly expanding traditional text-to-code\nbenchmarks with code-to-code pairs across diverse programming languages.\nExperimental results highlight the benefits of self-distillation through\nmulti-exit supervision.",
      "tldr_zh": "本研究提出了MODULARSTARENCODER，一种包含10亿参数的模块化多出口编码器，用于代码检索任务。该模型通过一种新颖的自蒸馏机制进行训练，显著提升了底层表征质量，使模型的不同部分在保持性能的同时实现灵活使用。架构专注于增强文本到代码和代码到代码的搜索能力，通过多级表征系统捕获语法和语义结构。训练过程中，高层编码器引导低层编码器，这种自蒸馏效应改善了中间表征，无需额外训练成本即可提高检索召回率。实验结果表明，多出口监督的自蒸馏机制有效提升了模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03008v1",
      "published_date": "2025-03-04 21:08:17 UTC",
      "updated_date": "2025-03-04 21:08:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:58.043158"
    },
    {
      "arxiv_id": "2503.03779v1",
      "title": "Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds",
      "title_zh": "加速多智能体路径规划中的焦点搜索：基于更紧致下界的方法",
      "authors": [
        "Yimin Tang",
        "Zhenghong Yu",
        "Jiaoyang Li",
        "Sven Koenig"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) involves finding collision-free paths for\nmultiple agents while minimizing a cost function--an NP-hard problem. Bounded\nsuboptimal methods like Enhanced Conflict-Based Search (ECBS) and Explicit\nEstimation CBS (EECBS) balance solution quality with computational efficiency\nusing focal search mechanisms. While effective, traditional focal search faces\na limitation: the lower bound (LB) value determining which nodes enter the\nFOCAL list often increases slowly in early search stages, resulting in a\nconstrained search space that delays finding valid solutions. In this paper, we\npropose a novel bounded suboptimal algorithm, double-ECBS (DECBS), to address\nthis issue by first determining the maximum LB value and then employing a\nbest-first search guided by this LB to find a collision-free path. Experimental\nresults demonstrate that DECBS outperforms ECBS in most test cases and is\ncompatible with existing optimization techniques. DECBS can reduce nearly 30%\nhigh-level CT nodes and 50% low-level focal search nodes. When agent density is\nmoderate to high, DECBS achieves a 23.5% average runtime improvement over ECBS\nwith identical suboptimality bounds and optimizations.",
      "tldr_zh": "本研究提出了一种新的有界次优算法DECBS，用于加速多智能体路径规划(MAPF)中的焦点搜索。DECBS通过预先确定最大下界值(LB)，并以此引导最佳优先搜索，有效解决了传统焦点搜索在早期阶段下界值增长缓慢导致搜索空间受限的问题。实验表明，DECBS在大多数测试案例中优于ECBS，能够减少近30%的高层CT节点和50%的低层焦点搜索节点，在中等至高智能体密度下，相比ECBS平均运行时间提升了23.5%，同时保持了相同的次优性界限和优化效果。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.03779v1",
      "published_date": "2025-03-04 20:39:00 UTC",
      "updated_date": "2025-03-04 20:39:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:33:57.969502"
    },
    {
      "arxiv_id": "2503.02992v1",
      "title": "RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks",
      "title_zh": "RAILGUN：面向不同环境与任务的多智能体路径规划统一卷积策略",
      "authors": [
        "Yimin Tang",
        "Xiao Xiong",
        "Jingyi Xi",
        "Jiaoyang Li",
        "Erdem Bıyık",
        "Sven Koenig"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial for applications ranging from aerial\nswarms to warehouse automation. Solving MAPF is NP-hard so learning-based\napproaches for MAPF have gained attention, particularly those leveraging deep\nneural networks. Nonetheless, despite the community's continued efforts, all\nlearning-based MAPF planners still rely on decentralized planning due to\nvariability in the number of agents and map sizes. We have developed the first\ncentralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is\nnot an agent-based policy but a map-based policy. By leveraging a CNN-based\narchitecture, RAILGUN can generalize across different maps and handle any\nnumber of agents. We collect trajectories from rule-based methods to train our\nmodel in a supervised way. In experiments, RAILGUN outperforms most baseline\nmethods and demonstrates great zero-shot generalization capabilities on various\ntasks, maps and agent numbers that were not seen in the training dataset.",
      "tldr_zh": "该研究提出了RAILGUN，首个基于卷积神经网络(CNN)的集中式学习策略，用于解决多智能体路径规划(MAPF)问题。与传统的分散式规划不同，RAILGUN采用地图级策略而非智能体级策略，能够泛化到不同地图并处理任意数量的智能体。通过使用基于规则的方法生成轨迹进行监督训练，RAILGUN在实验中超越了大多数基线方法，并在未见过的任务、地图和智能体数量上展现出优异的零样本泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02992v1",
      "published_date": "2025-03-04 20:35:20 UTC",
      "updated_date": "2025-03-04 20:35:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:06.603138"
    },
    {
      "arxiv_id": "2503.02989v1",
      "title": "Effectively Steer LLM To Follow Preference via Building Confident Directions",
      "title_zh": "通过构建置信方向有效引导大语言模型遵循偏好",
      "authors": [
        "Bingqing Song",
        "Boran Han",
        "Shuai Zhang",
        "Hao Wang",
        "Haoyang Fang",
        "Bonan Min",
        "Yuyang Wang",
        "Mingyi Hong"
      ],
      "abstract": "Having an LLM that aligns with human preferences is essential for\naccommodating individual needs, such as maintaining writing style or generating\nspecific topics of interest. The majority of current alignment methods rely on\nfine-tuning or prompting, which can be either costly or difficult to control.\nModel steering algorithms, which modify the model output by constructing\nspecific steering directions, are typically easy to implement and\noptimization-free. However, their capabilities are typically limited to\nsteering the model into one of the two directions (i.e., bidirectional\nsteering), and there has been no theoretical understanding to guarantee their\nperformance. In this work, we propose a theoretical framework to understand and\nquantify the model steering methods. Inspired by the framework, we propose a\nconfident direction steering method (CONFST) that steers LLMs via modifying\ntheir activations at inference time. More specifically, CONFST builds a\nconfident direction that is closely aligned with users' preferences, and this\ndirection is then added to the activations of the LLMs to effectively steer the\nmodel output. Our approach offers three key advantages over popular\nbidirectional model steering methods: 1) It is more powerful, since multiple\n(i.e. more than two) users' preferences can be aligned simultaneously; 2) It is\nsimple to implement, since there is no need to determine which layer to add the\nsteering vector to; 3) No explicit user instruction is required. We validate\nour method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks\nthat require shifting the output of LLMs across various topics and styles,\nachieving superior performance over competing methods.",
      "tldr_zh": "该研究提出了一种名为CONFST的模型引导方法，通过构建置信方向（confident direction）在推理时修改大语言模型（LLMs）的激活值，从而有效引导模型输出以符合用户偏好。与传统的双向引导方法相比，CONFST具有三大优势：1）能够同时满足多个（超过两个）用户的偏好；2）实现简单，无需确定在哪个网络层添加引导向量；3）无需显式的用户指令。该方法在GPT-2 XL、Mistral和Gemma-it等模型上验证了其在调整输出主题和风格任务中的优越性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02989v1",
      "published_date": "2025-03-04 20:32:27 UTC",
      "updated_date": "2025-03-04 20:32:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:47.401734"
    },
    {
      "arxiv_id": "2503.04817v1",
      "title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series",
      "title_zh": "多智能体系统用于AI辅助提取电视剧叙事弧线",
      "authors": [
        "Roberto Balestri",
        "Guglielmo Pescatore"
      ],
      "abstract": "Serialized TV shows are built on complex storylines that can be hard to track\nand evolve in ways that defy straightforward analysis. This paper introduces a\nmulti-agent system designed to extract and analyze these narrative arcs. Tested\non the first season of Grey's Anatomy (ABC 2005-), the system identifies three\ntypes of arcs: Anthology (self-contained), Soap (relationship-focused), and\nGenre-Specific (strictly related to the series' genre). Episodic progressions\nof these arcs are stored in both relational and semantic (vectorial) databases,\nenabling structured analysis and comparison. To bridge the gap between\nautomation and critical interpretation, the system is paired with a graphical\ninterface that allows for human refinement using tools to enhance and visualize\nthe data. The system performed strongly in identifying Anthology Arcs and\ncharacter entities, but its reliance on textual paratexts (such as episode\nsummaries) revealed limitations in recognizing overlapping arcs and subtler\ndynamics. This approach highlights the potential of combining computational and\nhuman expertise in narrative analysis. Beyond television, it offers promise for\nserialized written formats, where the narrative resides entirely in the text.\nFuture work will explore the integration of multimodal inputs, such as dialogue\nand visuals, and expand testing across a wider range of genres to refine the\nsystem further.",
      "tldr_zh": "本文提出了一种多智能体系统，用于提取和分析电视剧中的叙事弧线。该系统在《实习医生格蕾》第一季的测试中，成功识别了三种叙事弧线类型：独立故事（Anthology）、关系驱动（Soap）和类型相关（Genre-Specific），并将这些弧线的进展存储在关系型和语义型数据库中，支持结构化分析和比较。系统还配备了图形界面，允许人工干预和可视化数据，以弥补自动化与批判性解读之间的差距。尽管系统在识别独立故事弧线和角色实体方面表现优异，但对文本副文本（如剧情摘要）的依赖限制了其对重叠弧线和微妙动态的识别能力。该研究展示了计算与人类专业知识结合在叙事分析中的潜力，并为未来整合多模态输入（如对话和视觉）及扩展测试范围奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.CL",
      "comment": "17th International Conference on Agents and Artificial Intelligence,\n  Porto (Portugal). 23/02/2025 - 25/02/2025",
      "pdf_url": "http://arxiv.org/pdf/2503.04817v1",
      "published_date": "2025-03-04 20:27:14 UTC",
      "updated_date": "2025-03-04 20:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:48.717063"
    },
    {
      "arxiv_id": "2503.03777v1",
      "title": "FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference",
      "title_zh": "FlexInfer：通过灵活高效卸载打破设备端大语言模型推理的内存限制",
      "authors": [
        "Hongchao Du",
        "Shangyu Wu",
        "Arina Kharlamova",
        "Nan Guan",
        "Chun Jason Xue"
      ],
      "abstract": "Large Language Models (LLMs) face challenges for on-device inference due to\nhigh memory demands. Traditional methods to reduce memory usage often\ncompromise performance and lack adaptability. We propose FlexInfer, an\noptimized offloading framework for on-device inference, addressing these issues\nwith techniques like asynchronous prefetching, balanced memory locking, and\nflexible tensor preservation. These strategies enhance memory efficiency and\nmitigate I/O bottlenecks, ensuring high performance within user-specified\nresource constraints. Experiments demonstrate that FlexInfer significantly\nimproves throughput under limited resources, achieving up to 12.5 times better\nperformance than existing methods and facilitating the deployment of large\nmodels on resource-constrained devices.",
      "tldr_zh": "该研究提出了FlexInfer，一种针对设备端大语言模型（LLM）推理的优化卸载框架，旨在解决高内存需求带来的挑战。通过异步预取、平衡内存锁定和灵活张量保存等技术，FlexInfer显著提升了内存效率并缓解了I/O瓶颈，确保在用户指定的资源限制内实现高性能。实验表明，FlexInfer在有限资源下的吞吐量比现有方法提高了多达12.5倍，为在资源受限设备上部署大模型提供了高效解决方案。",
      "categories": [
        "cs.OS",
        "cs.AI"
      ],
      "primary_category": "cs.OS",
      "comment": "9 pages, 5 figures, to be published in EuroMLSys '25",
      "pdf_url": "http://arxiv.org/pdf/2503.03777v1",
      "published_date": "2025-03-04 20:08:03 UTC",
      "updated_date": "2025-03-04 20:08:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:47.289093"
    },
    {
      "arxiv_id": "2503.02976v1",
      "title": "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment",
      "title_zh": "《教会AI处理异常情况：基于人类对齐判断的监督微调方法》",
      "authors": [
        "Matthew DosSantos DiSorbo",
        "Harang Ju",
        "Sinan Aral"
      ],
      "abstract": "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
      "tldr_zh": "该研究探讨了如何让大语言模型(LLMs)在处理异常情况时与人类判断保持一致。研究发现，尽管LLMs在生成任务上表现出色，但在决策过程中往往严格遵循政策，导致与人类判断存在显著偏差。通过比较三种调优方法，发现基于人类解释的监督微调(supervised fine-tuning)效果最佳，不仅能显著提升模型在异常处理上的表现，还能将类似人类的决策能力泛化到新场景中。研究表明，仅使用标签进行微调是不够的，必须通过解释来明确训练决策过程，才能实现LLMs与人类判断的对齐。这些发现为开发能够有效适应新环境并与人类判断一致的智能体AI提供了重要指导。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02976v1",
      "published_date": "2025-03-04 20:00:37 UTC",
      "updated_date": "2025-03-04 20:00:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:48.139279"
    },
    {
      "arxiv_id": "2503.02972v3",
      "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
      "title_zh": "LINGOLY-TOO：通过语言模板化与拼写混淆分离记忆与推理能力",
      "authors": [
        "Jude Khouja",
        "Karolina Korgul",
        "Simi Hellsten",
        "Lingyi Yang",
        "Vlad Neacsu",
        "Harry Mayne",
        "Ryan Kearns",
        "Andrew Bean",
        "Adam Mahdi"
      ],
      "abstract": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
      "tldr_zh": "该研究提出LINGOLY-TOO基准测试框架，通过语言模板化和正字法混淆技术，有效区分大型语言模型(LLMs)的记忆能力与推理能力。该方法动态混淆真实语言的书写系统生成多样化问题变体，在保持相同推理步骤的同时降低训练数据泄露风险。实验表明包括Claude 3.7 Sonnet在内的前沿模型在高级语言推理任务上表现不佳，且对同一问题的不同变体存在显著准确率波动，验证了现有评估方法会因数据记忆而高估模型的实际推理能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02972v3",
      "published_date": "2025-03-04 19:57:47 UTC",
      "updated_date": "2025-03-07 09:31:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:34:59.915810"
    },
    {
      "arxiv_id": "2503.02969v1",
      "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model",
      "title_zh": "InfiniSST：基于大语言模型的无边界语音同声传译",
      "authors": [
        "Siqi Ouyang",
        "Xi Xu",
        "Lei Li"
      ],
      "abstract": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
      "tldr_zh": "该研究提出InfiniSST，一种基于大语言模型(LLM)的无边界语音同声传译(SST)新方法。通过将SST建模为多轮对话任务，结合多延迟增强训练数据和创新的键值缓存(KV cache)管理策略，实现了对连续语音流的实时翻译。实验表明，在MuST-C三个语种上，该方法在保持翻译质量的同时，将计算感知延迟降低0.5-1秒，显著提升了无边界语音同传的实用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.02969v1",
      "published_date": "2025-03-04 19:51:29 UTC",
      "updated_date": "2025-03-04 19:51:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:35:14.447508"
    },
    {
      "arxiv_id": "2503.02955v1",
      "title": "Monocular visual simultaneous localization and mapping: (r)evolution from geometry to deep learning-based pipelines",
      "title_zh": "单目视觉同步定位与地图构建：从几何到深度学习管道的（r）进化",
      "authors": [
        "Olaya Alvarez-Tunon",
        "Yury Brodskiy",
        "Erdal Kayacan"
      ],
      "abstract": "With the rise of deep learning, there is a fundamental change in visual SLAM\nalgorithms toward developing different modules trained as end-to-end pipelines.\nHowever, regardless of the implementation domain, visual SLAM's performance is\nsubject to diverse environmental challenges, such as dynamic elements in\noutdoor environments, harsh imaging conditions in underwater environments, or\nblurriness in high-speed setups. These environmental challenges need to be\nidentified to study the real-world viability of SLAM implementations. Motivated\nby the aforementioned challenges, this paper surveys the current state of\nvisual SLAM algorithms according to the two main frameworks: geometry-based and\nlearning-based SLAM. First, we introduce a general formulation of the SLAM\npipeline that includes most of the implementations in the literature. Second,\nthose implementations are classified and surveyed for geometry and\nlearning-based SLAM. After that, environment-specific challenges are formulated\nto enable experimental evaluation of the resilience of different visual SLAM\nclasses to varying imaging conditions. We address two significant issues in\nsurveying visual SLAM, providing (1) a consistent classification of visual SLAM\npipelines and (2) a robust evaluation of their performance under different\ndeployment conditions. Finally, we give our take on future opportunities for\nvisual SLAM implementations.",
      "tldr_zh": "该论文系统回顾了单目视觉SLAM（同时定位与地图构建）技术从传统几何方法到深度学习端到端管线的演进历程。研究首先建立了统一的SLAM流程框架，将现有方法划分为几何基和基于学习的两大类进行综述。针对动态环境、水下成像和高速运动模糊等实际挑战，论文提出了系统的评估框架，以测试不同SLAM算法在复杂条件下的鲁棒性。最后，作者对视觉SLAM技术的未来发展方向提出了前瞻性见解，特别强调需要增强算法在真实多变环境中的适应能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02955v1",
      "published_date": "2025-03-04 19:20:17 UTC",
      "updated_date": "2025-03-04 19:20:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:35:12.162292"
    },
    {
      "arxiv_id": "2503.02954v1",
      "title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders",
      "title_zh": "基于图神经网络变分自编码器的可靠高效多智能体协同",
      "authors": [
        "Yue Meng",
        "Nathalie Majcherczyk",
        "Wenliang Liu",
        "Scott Kiesel",
        "Chuchu Fan",
        "Federico Pecora"
      ],
      "abstract": "Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord",
      "tldr_zh": "该研究提出了一种基于图神经网络变分自编码器(GNN-VAE)的多智能体协调方法，旨在高效解决大规模多机器人导航问题。通过将协调问题建模为图问题，并利用混合整数线性规划(MILP)求解器生成高质量解，GNN-VAE在训练时将解编码到潜在空间，推理时从中解码并选择最低成本解进行协调。实验表明，该方法在小规模问题上训练后，能高效处理多达250个机器人的大规模问题，且始终满足约束条件，显著优于其他基线方法。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by 2025 International Conference on Robotics and Automation\n  (ICRA 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02954v1",
      "published_date": "2025-03-04 19:20:11 UTC",
      "updated_date": "2025-03-04 19:20:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:35:45.377428"
    },
    {
      "arxiv_id": "2503.02951v1",
      "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
      "title_zh": "KodCode：一个多样化、具有挑战性且可验证的编程合成数据集",
      "authors": [
        "Zhangchen Xu",
        "Yang Liu",
        "Yueqin Yin",
        "Mingyuan Zhou",
        "Radha Poovendran"
      ],
      "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
      "tldr_zh": "本研究提出了KodCode，一个多样、具有挑战性且可验证的合成编程数据集，旨在解决训练大语言模型时高质量、可验证数据不足的问题。KodCode通过系统化的自验证流程生成问题-解决方案-测试三元组，涵盖从简单编程任务到高级算法问题的广泛领域。实验表明，基于KodCode微调的模型在多个编程基准测试中表现优异，超越了现有先进模型，为编程模型的训练提供了强有力的数据支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Codes and Data: https://kodcode-ai.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.02951v1",
      "published_date": "2025-03-04 19:17:36 UTC",
      "updated_date": "2025-03-04 19:17:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:35:29.705657"
    },
    {
      "arxiv_id": "2503.02950v1",
      "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
      "title_zh": "LiteWebAgent：基于视觉语言模型的网页智能体应用开源套件",
      "authors": [
        "Danqing Zhang",
        "Balaji Rama",
        "Jingyi Ni",
        "Shiying He",
        "Fu Zhao",
        "Kunyu Chen",
        "Arnold Chen",
        "Junyu Cao"
      ],
      "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
      "tldr_zh": "本研究推出了LiteWebAgent，一个开源的基于视觉语言模型(VLM)的网页智能体应用套件。该框架填补了网页智能体生态系统的关键空白，提供了生产就绪的解决方案，包括极简的无服务器后端配置、直观的用户与浏览器界面，以及可扩展的研究功能，如智能体规划、记忆和树搜索。核心框架采用递归函数调用的设计，实现了动作生成与动作执行的解耦，并集成了模块化的高级研究组件。LiteWebAgent以两种形式部署：(1)基于Vercel的生产级网页应用，提供远程浏览器控制；(2)Chrome扩展，通过CDP协议控制现有浏览器。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02950v1",
      "published_date": "2025-03-04 19:13:10 UTC",
      "updated_date": "2025-03-04 19:13:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:35:50.504335"
    },
    {
      "arxiv_id": "2503.02924v1",
      "title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
      "title_zh": "基于信号时序逻辑的多样化可控扩散策略",
      "authors": [
        "Yue Meng",
        "Chuchu fan"
      ],
      "abstract": "Generating realistic simulations is critical for autonomous system\napplications such as self-driving and human-robot interactions. However,\ndriving simulators nowadays still have difficulty in generating controllable,\ndiverse, and rule-compliant behaviors for road participants: Rule-based models\ncannot produce diverse behaviors and require careful tuning, whereas\nlearning-based methods imitate the policy from data but are not designed to\nfollow the rules explicitly. Besides, the real-world datasets are by nature\n\"single-outcome\", making the learning method hard to generate diverse\nbehaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion\nModels to learn controllable, diverse, and rule-aware policy. We first\ncalibrate the STL on the real-world data, then generate diverse synthetic data\nusing trajectory optimization, and finally learn the rectified diffusion policy\non the augmented dataset. We test on the NuScenes dataset and our approach can\nachieve the most diverse rule-compliant trajectories compared to other\nbaselines, with a runtime 1/17X to the second-best approach. In the closed-loop\ntesting, our approach reaches the highest diversity, rule satisfaction rate,\nand the least collision rate. Our method can generate varied characteristics\nconditional on different STL parameters in testing. A case study on human-robot\nencounter scenarios shows our approach can generate diverse and\nclosed-to-oracle trajectories. The annotation tool, augmented dataset, and code\nare available at https://github.com/mengyuest/pSTL-diffusion-policy.",
      "tldr_zh": "本研究提出了一种结合信号时序逻辑(STL)和扩散模型的多样化可控策略生成方法，用于解决自动驾驶和机器人交互中行为仿真的多样性、可控性与规则遵循难题。该方法通过三个关键步骤实现：基于真实数据校准STL规则、利用轨迹优化生成多样化合成数据、在增强数据集上训练修正扩散策略。实验表明，该方法在NuScenes数据集上能以1/17的运行时生成最丰富且合规的轨迹，在闭环测试中展现出最高的多样性、规则满足率和最低碰撞率。特别地，通过调节STL参数可生成具有不同特征的行为，在人类-机器人交互场景中能产生接近最优的多样化轨迹。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.LO"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L), October 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.02924v1",
      "published_date": "2025-03-04 18:59:00 UTC",
      "updated_date": "2025-03-04 18:59:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:14.124630"
    },
    {
      "arxiv_id": "2503.02882v1",
      "title": "Bringing Comparative Cognition To Computers",
      "title_zh": "将比较认知引入计算机",
      "authors": [
        "Konstantinos Voudouris",
        "Lucy G. Cheke",
        "Eric Schulz"
      ],
      "abstract": "Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences.",
      "tldr_zh": "该论文主张将人工智能（AI）系统纳入比较认知研究框架，以更严谨地评估其与人类及其他动物的认知能力差异。通过采用比较方法，研究者可以避免夸大或低估AI与生物智能的异同，从而将AI认知研究整合到更广泛的认知科学领域中。这一方法有助于更全面地理解AI系统的认知能力及其在认知科学中的定位。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02882v1",
      "published_date": "2025-03-04 18:58:42 UTC",
      "updated_date": "2025-03-04 18:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:15.536092"
    },
    {
      "arxiv_id": "2503.02881v1",
      "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
      "title_zh": "反应式扩散策略：面向密集接触操作的慢-快视觉触觉策略学习",
      "authors": [
        "Han Xue",
        "Jieji Ren",
        "Wendi Chen",
        "Gu Zhang",
        "Yuan Fang",
        "Guoying Gu",
        "Huazhe Xu",
        "Cewu Lu"
      ],
      "abstract": "Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as quick adjustments to environmental changes\nand adaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines through\nrapid response to tactile / force feedback. Furthermore, experiments show that\nRDP is applicable across different tactile / force sensors. Code and videos are\navailable on https://reactive-diffusion-policy.github.io/.",
      "tldr_zh": "该研究提出了Reactive Diffusion Policy (RDP)，一种慢快结合的视觉-触觉模仿学习算法，用于解决接触密集型操作任务中的实时反馈问题。RDP采用双层架构：慢速的潜在扩散策略用于低频预测高层动作块，而快速的非对称分词器则实现高频触觉反馈的闭环控制。结合低成本触觉反馈系统TactAR，RDP在三种复杂任务中显著优于现有视觉模仿学习方法，展现了其对不同触觉传感器的广泛适用性。这一框架为机器人实现快速响应和精细操作提供了新思路。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02881v1",
      "published_date": "2025-03-04 18:58:21 UTC",
      "updated_date": "2025-03-04 18:58:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:32.384804"
    },
    {
      "arxiv_id": "2503.02879v1",
      "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
      "title_zh": "大语言模型时代的维基百科：演变与风险",
      "authors": [
        "Siming Huang",
        "Yuliang Xu",
        "Mingmeng Geng",
        "Yao Wan",
        "Dongping Chen"
      ],
      "abstract": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
      "tldr_zh": "本文深入分析了大型语言模型(LLMs)对维基百科的影响，通过现有数据和模拟研究其演变和潜在风险。研究发现，LLMs已对维基百科文章产生了约1%-2%的影响，特别是在某些类别中。如果基于维基百科的机器翻译基准受到LLMs影响，模型得分可能虚高，模型间的比较结果也可能发生变化。此外，如果知识库被LLM生成的内容污染，检索增强生成(RAG)的有效性可能会降低。尽管LLMs尚未完全改变维基百科的语言和知识结构，但研究结果提示需要谨慎考虑未来的潜在风险。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia",
      "pdf_url": "http://arxiv.org/pdf/2503.02879v1",
      "published_date": "2025-03-04 18:58:13 UTC",
      "updated_date": "2025-03-04 18:58:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:30.894151"
    },
    {
      "arxiv_id": "2503.02878v1",
      "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
      "title_zh": "语言模型可通过状态价值估计自我提升以优化搜索性能",
      "authors": [
        "Ethan Mendes",
        "Alan Ritter"
      ],
      "abstract": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
      "tldr_zh": "该研究提出了一种名为\"自学前瞻\"(self-taught lookahead)的自监督方法，使语言模型能够通过状态转移动态自我改进状态价值估计，从而优化搜索过程。实验表明，采用该方法改进的中等规模(80亿参数)开源价值模型，其性能可媲美使用GPT-4o等前沿大模型作为价值模型的效果。相比传统基于LLM的树搜索方法，该方法在性能提升20%的同时，将成本降低了37倍，且无需依赖真实奖励信号。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02878v1",
      "published_date": "2025-03-04 18:58:11 UTC",
      "updated_date": "2025-03-04 18:58:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:39.219036"
    },
    {
      "arxiv_id": "2503.02861v1",
      "title": "Evaluation of Architectural Synthesis Using Generative AI",
      "title_zh": "生成式AI在建筑合成中的应用评估",
      "authors": [
        "Jingfei Huang",
        "Alexandros Haridis"
      ],
      "abstract": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
      "tldr_zh": "本研究评估了多模态生成式AI（GPT-4o和Claude 3.5）在建筑3D合成任务中的表现，以Palladio的《建筑四书》中的Villa Rotonda和Palazzo Porto为案例。通过文本和图像提示，测试了模型在建筑2D/3D表示解释、CAD脚本编码以及自我改进方面的能力。结果显示，虽然两者均能生成独立部件，但在准确组装部件以实现空间关系上存在困难，其中Claude 3.5在自我修正方面表现更优。研究表明，现成AI系统在需要领域知识的智能任务中具有潜力，但仍需改进以更好地辅助建筑设计。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
      "published_date": "2025-03-04 18:39:28 UTC",
      "updated_date": "2025-03-04 18:39:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:37:05.568404"
    },
    {
      "arxiv_id": "2503.02857v3",
      "title": "Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024",
      "title_zh": "Deepfake-Eval-2024：2024年传播的多模态真实场景深度伪造基准",
      "authors": [
        "Nuria Alina Chandra",
        "Ryan Murtfeldt",
        "Lin Qiu",
        "Arnab Karmakar",
        "Hannah Lee",
        "Emmanuel Tanumihardja",
        "Kevin Farhat",
        "Ben Caffee",
        "Sejin Paik",
        "Changyeon Lee",
        "Jongwook Choi",
        "Aerin Kim",
        "Oren Etzioni"
      ],
      "abstract": "In the age of increasingly realistic generative AI, robust deepfake detection\nis essential for mitigating fraud and disinformation. While many deepfake\ndetectors report high accuracy on academic datasets, we show that these\nacademic benchmarks are out of date and not representative of real-world\ndeepfakes. We introduce Deepfake-Eval-2024, a new deepfake detection benchmark\nconsisting of in-the-wild deepfakes collected from social media and deepfake\ndetection platform users in 2024. Deepfake-Eval-2024 consists of 45 hours of\nvideos, 56.5 hours of audio, and 1,975 images, encompassing the latest\nmanipulation technologies. The benchmark contains diverse media content from 88\ndifferent websites in 52 different languages. We find that the performance of\nopen-source state-of-the-art deepfake detection models drops precipitously when\nevaluated on Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for\naudio, and 45% for image models compared to previous benchmarks. We also\nevaluate commercial deepfake detection models and models finetuned on\nDeepfake-Eval-2024, and find that they have superior performance to\noff-the-shelf open-source models, but do not yet reach the accuracy of deepfake\nforensic analysts. The dataset is available at\nhttps://github.com/nuriachandra/Deepfake-Eval-2024.",
      "tldr_zh": "该研究提出了Deepfake-Eval-2024，一个多模态的深度伪造检测基准，收集自2024年社交媒体和检测平台的真实数据，包含45小时视频、56.5小时音频和1975张图像，涵盖88个网站和52种语言。研究发现，现有开源深度伪造检测模型在该基准上的性能显著下降，AUC值较以往基准降低50%（视频）、48%（音频）和45%（图像）。虽然商业模型和基于该基准微调的模型表现优于开源模型，但仍未达到专业分析师的准确率。该基准为评估和改进深度伪造检测技术提供了重要资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02857v3",
      "published_date": "2025-03-04 18:33:22 UTC",
      "updated_date": "2025-03-24 20:46:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:36:50.972343"
    },
    {
      "arxiv_id": "2503.02854v2",
      "title": "(How) Do Language Models Track State?",
      "title_zh": "（如何）语言模型追踪状态？",
      "authors": [
        "Belinda Z. Li",
        "Zifan Carl Guo",
        "Jacob Andreas"
      ],
      "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.",
      "tldr_zh": "该研究探讨了Transformer语言模型(LMs)如何跟踪状态，特别是在处理排列组合任务时。研究发现，LMs通过两种机制实现状态跟踪：一种类似于Liu等人提出的“关联扫描”(associative scan)构造，另一种则利用排列奇偶性(permutation parity)部分剪枝输出空间，再通过关联扫描进行细化。这两种机制表现出不同的鲁棒性，且可以通过中间训练任务引导模型选择特定机制。研究表明，无论是预训练还是微调的Transformer LMs，都能学习到高效且可解释的状态跟踪机制，并且这些机制的出现可以被预测和控制。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages, 17 figures, 1 table. Code:\n  http://github.com/belindal/state-tracking",
      "pdf_url": "http://arxiv.org/pdf/2503.02854v2",
      "published_date": "2025-03-04 18:31:02 UTC",
      "updated_date": "2025-03-11 15:36:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:37:09.016569"
    },
    {
      "arxiv_id": "2503.02849v1",
      "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data",
      "title_zh": "基于组织病理图像与基因表达数据的多模态深度学习用于乳腺癌亚型分类",
      "authors": [
        "Amin Honarmandi Shandiz"
      ],
      "abstract": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
      "tldr_zh": "本研究提出了一种多模态深度学习框架，通过整合组织病理学图像和基因表达数据，对乳腺癌进行BRCA.Luminal和BRCA.Basal/Her2亚型分类。该方法使用ResNet-50模型提取图像特征，结合全连接层处理基因表达数据，并通过跨注意力融合机制增强模态间的交互。实验表明，该多模态方法在分类准确率、精确率-召回率AUC和F1分数上均优于单模态方法，为乳腺癌亚型分类提供了更可靠和可解释的解决方案，有助于改善临床决策。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
      "published_date": "2025-03-04 18:24:33 UTC",
      "updated_date": "2025-03-04 18:24:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:37:09.561878"
    },
    {
      "arxiv_id": "2503.02836v1",
      "title": "SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting",
      "title_zh": "SeqFusion：用于零样本时间序列预测的预训练模型顺序融合",
      "authors": [
        "Ting-Ji Huang",
        "Xu-Yang Chen",
        "Han-Jia Ye"
      ],
      "abstract": "Unlike traditional time-series forecasting methods that require extensive\nin-task data for training, zero-shot forecasting can directly predict future\nvalues given a target time series without additional training data. Current\nzero-shot approaches primarily rely on pre-trained generalized models, with\ntheir performance often depending on the variety and relevance of the\npre-training data, which can raise privacy concerns. Instead of collecting\ndiverse pre-training data, we introduce SeqFusion in this work, a novel\nframework that collects and fuses diverse pre-trained models (PTMs)\nsequentially for zero-shot forecasting. Based on the specific temporal\ncharacteristics of the target time series, SeqFusion selects the most suitable\nPTMs from a batch of pre-collected PTMs, performs sequential predictions, and\nfuses all the predictions while using minimal data to protect privacy. Each of\nthese PTMs specializes in different temporal patterns and forecasting tasks,\nallowing SeqFusion to select by measuring distances in a shared representation\nspace of the target time series with each PTM. Experiments demonstrate that\nSeqFusion achieves competitive accuracy in zero-shot forecasting compared to\nstate-of-the-art methods.",
      "tldr_zh": "该研究提出SeqFusion框架，通过顺序融合多个预训练模型(PTMs)实现零样本时间序列预测，无需额外训练数据。该方法根据目标时间序列的时序特征，从预收集的PTMs中选择最合适的模型进行顺序预测，并在共享表示空间中测量距离以优化选择，最后融合所有预测结果。实验表明，SeqFusion在保护隐私的同时（仅需极少量数据），其零样本预测准确率与当前最先进方法相当。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02836v1",
      "published_date": "2025-03-04 17:59:17 UTC",
      "updated_date": "2025-03-04 17:59:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:37:52.112842"
    },
    {
      "arxiv_id": "2503.02832v1",
      "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
      "title_zh": "AlignDistil：作为自适应策略蒸馏的令牌级语言模型对齐",
      "authors": [
        "Songming Zhang",
        "Xue Zhang",
        "Tong Zhang",
        "Bojie Hu",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
      "tldr_zh": "该研究提出了AlignDistil，一种基于token级别奖励优化的语言模型对齐方法，旨在解决现有方法因忽略token级别奖励而导致的性能次优和收敛缓慢问题。该方法将DPO（直接偏好优化）模型的奖励引入RLHF（基于人类反馈的强化学习）目标，并理论证明了其与token级别蒸馏过程的等价性。通过构建对比性DPO奖励和设计token自适应的logit外推机制，AlignDistil实现了对不同token的精确优化。实验结果表明，该方法在收敛速度和性能上均优于现有方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02832v1",
      "published_date": "2025-03-04 17:57:09 UTC",
      "updated_date": "2025-03-04 17:57:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:51.291911"
    },
    {
      "arxiv_id": "2503.02824v1",
      "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging",
      "title_zh": "开发用于跨模态解剖与功能成像的PET/CT基础模型",
      "authors": [
        "Yujin Oh",
        "Robert Seifert",
        "Yihan Cao",
        "Christoph Clement",
        "Justin Ferdinandus",
        "Constantin Lapa",
        "Alessandro Liebich",
        "Michelle Amon",
        "Johanna Enke",
        "Sifan Song",
        "Runqi Meng",
        "Fang Zeng",
        "Ning Guo",
        "Xiang Li",
        "Pedram Heidari",
        "Axel Rominger",
        "Kuangyu Shi",
        "Quanzheng Li"
      ],
      "abstract": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
      "tldr_zh": "本研究提出了一种针对PET/CT多模态成像的预训练基础模型Cross-Fraternal Twin Masked Autoencoder (FratMAE)，旨在整合全身解剖（CT）与功能代谢（PET）信息。该模型采用独立的ViT编码器处理PET和CT数据，并通过跨模态注意力解码器实现模态间的协同交互，同时结合文本元数据增强PET表征学习。实验表明，FratMAE能够捕捉复杂的跨模态关系和全局代谢模式，在下游任务中表现优异，展现了其作为通用基础模型的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 2 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
      "published_date": "2025-03-04 17:49:07 UTC",
      "updated_date": "2025-03-04 17:49:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:16.021796"
    },
    {
      "arxiv_id": "2503.02823v1",
      "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
      "title_zh": "多模态交响曲：通过生成式人工智能融合味觉与听觉",
      "authors": [
        "Matteo Spanio",
        "Massimiliano Zampini",
        "Antonio Rodà",
        "Franco Pierucci"
      ],
      "abstract": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
      "tldr_zh": "本研究提出了一种多模态生成模型，能够将味觉信息转化为音乐。研究基于味觉与听觉感知之间的神经科学和心理学关联，使用微调后的生成音乐模型（MusicGEN）根据详细味觉描述生成音乐。实验结果表明，微调后的模型生成的音乐更准确地反映了输入的味觉描述，参与者的评价（n=111）也验证了这一点。这项研究为理解和发展AI、声音与味觉之间的具身交互迈出了重要一步，并推动了生成式AI领域的新可能性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "I.2.6; J.5"
      ],
      "primary_category": "cs.SD",
      "comment": "17 pages, 6 figures (2 + 2 figures with 2 subfigures each)",
      "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
      "published_date": "2025-03-04 17:48:48 UTC",
      "updated_date": "2025-03-04 17:48:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:28.693594"
    },
    {
      "arxiv_id": "2503.02812v1",
      "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
      "title_zh": "Q-Filters：利用QK几何结构实现高效KV缓存压缩",
      "authors": [
        "Nathan Godey",
        "Alessio Devoto",
        "Yu Zhao",
        "Simone Scardapane",
        "Pasquale Minervini",
        "Éric de la Clergerie",
        "Benoît Sagot"
      ],
      "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
      "tldr_zh": "该研究提出了Q-Filters，一种无需训练的KV Cache压缩方法，通过利用Query (Q) 和 Key (K) 向量的几何特性，高效近似注意力分数，无需计算完整的注意力图。Q-Filters基于单一上下文无关的投影筛选出相对不重要的Key-Value对，且与FlashAttention兼容。实验表明，Q-Filters在长上下文任务中表现优异，在检索任务中与SnapKV竞争，在生成任务中优于Streaming-LLM，并在32倍压缩下实现了99%的“针在干草堆”任务准确率，同时将生成困惑度下降减少高达65%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02812v1",
      "published_date": "2025-03-04 17:37:49 UTC",
      "updated_date": "2025-03-04 17:37:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:31.068597"
    },
    {
      "arxiv_id": "2503.02797v1",
      "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
      "title_zh": "因果框架下图像质量指标与深度神经网络鲁棒性的对齐",
      "authors": [
        "Nathan Drenkow",
        "Mathias Unberath"
      ],
      "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance.",
      "tldr_zh": "该研究提出了一种因果框架，用于对齐图像质量指标（IQA）与深度神经网络（DNN）的鲁棒性。传统IQA指标基于人类感知判断，但研究发现其与DNN性能的关联较弱。通过因果分析，作者提出了一种新的图像质量指标，能够更准确地预测DNN在分类任务中的表现，并无需训练新模型即可作为性能先验。这一方法为大规模图像数据集的质量分布评估及其与DNN性能的关系提供了直接的计算手段。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02797v1",
      "published_date": "2025-03-04 17:15:31 UTC",
      "updated_date": "2025-03-04 17:15:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:47.518281"
    },
    {
      "arxiv_id": "2503.02784v3",
      "title": "Do Not Trust Licenses You See: Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing",
      "title_zh": "勿轻信所见之许可证：数据集合规性需依赖大规模AI驱动的全生命周期追踪",
      "authors": [
        "Jaekyeom Kim",
        "Sungryull Sohn",
        "Gerrard Jeongwon Jo",
        "Jihoon Choi",
        "Kyunghoon Bae",
        "Hwayoung Lee",
        "Yongmin Park",
        "Honglak Lee"
      ],
      "abstract": "This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.",
      "tldr_zh": "本文指出，仅凭数据集许可证无法准确评估其法律风险，必须追踪数据集的全生命周期和再分发过程。为此，研究者开发了名为NEXUS的自动化数据合规系统，利用AI代理高效追踪数据集再分发、分析合规性并识别法律风险。通过对17,429个实体和8,072个许可证条款的大规模法律分析，研究发现原始数据集与其再分发子集之间的法律权利存在显著差异。例如，在2,852个具有商业许可条款的数据集中，仅605个（21%）可合法用于商业化。该研究为AI数据治理设立了新标准，强调系统性追踪数据集生命周期以确保透明、合法和负责任的数据管理。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02784v3",
      "published_date": "2025-03-04 16:57:53 UTC",
      "updated_date": "2025-03-14 16:58:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:38:59.625639"
    },
    {
      "arxiv_id": "2503.02783v2",
      "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
      "title_zh": "IterPref：通过迭代调试实现代码生成的焦点偏好学习",
      "authors": [
        "Jie Wu",
        "Haoling Li",
        "Xin Zhang",
        "Jianwen Luo",
        "Yangyu Huang",
        "Ruihang Chu",
        "Yujiu Yang",
        "Scarlett Li"
      ],
      "abstract": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
      "tldr_zh": "该研究提出了IterPref，一种基于焦点偏好学习的代码生成框架，通过模拟人类迭代调试过程来优化代码大语言模型(Code LLMs)。与现有方法仅根据测试通过率构建偏好对不同，IterPref精确定位代码中的错误区域，并通过定制的DPO算法对齐相应标记，从而捕捉更细粒度的错误修正模式。研究还引入了CodeFlow数据集，其中样本通过迭代修正直至通过测试，记录了具体的错误修正过程。实验表明，采用IterPref的多种Code LLMs在代码生成任务中表现显著提升，尤其在BigCodeBench等挑战性任务上，且生成的代码错误更少。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The code and data will be released soon",
      "pdf_url": "http://arxiv.org/pdf/2503.02783v2",
      "published_date": "2025-03-04 16:56:34 UTC",
      "updated_date": "2025-03-10 18:08:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:39:03.887497"
    },
    {
      "arxiv_id": "2503.05816v1",
      "title": "Will Neural Scaling Laws Activate Jevons' Paradox in AI Labor Markets? A Time-Varying Elasticity of Substitution (VES) Analysis",
      "title_zh": "神经网络缩放定律会激活人工智能劳动力市场中的杰文斯悖论吗？——基于时变替代弹性（VES）的分析",
      "authors": [
        "Rajesh P. Narayanan",
        "R. Kelley Pace"
      ],
      "abstract": "AI industry leaders often use the term ``Jevons' Paradox.'' We explore the\nsignificance of this term for artificial intelligence adoption through a\ntime-varying elasticity of substitution framework. We develop a model\nconnecting AI development to labor substitution through four key mechanisms:\n(1) increased effective computational capacity from both hardware and\nalgorithmic improvements; (2) AI capabilities that rise logarithmically with\ncomputation following established neural scaling laws; (3) declining marginal\ncomputational costs leading to lower AI prices through competitive pressure;\nand (4) a resulting increase in the elasticity of substitution between AI and\nhuman labor over time. Our time-varying elasticity of substitution (VES)\nframework, incorporating the G\\o rtz identity, yields analytical conditions for\nmarket transformation dynamics. This work provides a simple framework to help\nassess the economic reasoning behind industry claims that AI will increasingly\nsubstitute for human labor across diverse economic sectors.",
      "tldr_zh": "本研究通过时变替代弹性(VES)框架，探讨了AI发展对劳动力市场的潜在影响，特别是“杰文斯悖论”在AI领域的表现。研究提出了一个模型，将AI发展与劳动力替代联系起来，重点关注四个关键机制：计算能力和算法的提升、神经网络扩展规律、边际计算成本下降以及AI与人类劳动力替代弹性的增加。该框架结合Görtz恒等式，为市场转型动态提供了分析条件，为评估AI在不同经济领域替代人类劳动力的经济逻辑提供了简单而有力的工具。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "q-fin.EC",
        "I.2.m; J.4"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05816v1",
      "published_date": "2025-03-04 16:55:30 UTC",
      "updated_date": "2025-03-04 16:55:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:40:10.792824"
    },
    {
      "arxiv_id": "2503.02781v1",
      "title": "Multimodal AI predicts clinical outcomes of drug combinations from preclinical data",
      "title_zh": "多模态AI通过临床前数据预测药物组合的临床疗效",
      "authors": [
        "Yepeng Huang",
        "Xiaorui Su",
        "Varun Ullanat",
        "Ivy Liang",
        "Lindsay Clegg",
        "Damilola Olabode",
        "Nicholas Ho",
        "Bino John",
        "Megan Gibbs",
        "Marinka Zitnik"
      ],
      "abstract": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
      "tldr_zh": "本研究提出了MADRIGAL，一种多模态AI模型，用于从临床前数据预测药物组合的临床效果。该模型整合了结构、通路、细胞活力和转录组等多模态数据，通过transformer瓶颈模块统一处理缺失数据，显著优于单模态方法。MADRIGAL在预测抗癌药物组合、管理多药治疗（如2型糖尿病和代谢功能障碍相关脂肪性肝炎）以及识别药物转运体介导的相互作用方面表现出色。此外，它还能结合癌症患者的基因组特征，预测个性化药物组合的疗效，并通过与大型语言模型集成，以自然语言描述临床结果，提升安全性评估能力。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
      "published_date": "2025-03-04 16:55:14 UTC",
      "updated_date": "2025-03-04 16:55:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:39:19.914548"
    },
    {
      "arxiv_id": "2503.02776v1",
      "title": "Implicit Bias in LLMs: A Survey",
      "title_zh": "大语言模型中的隐性偏见：一项调查",
      "authors": [
        "Xinru Lin",
        "Luyang Li"
      ],
      "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
      "tldr_zh": "这篇综述论文系统探讨了大语言模型(LLMs)中的隐性偏见问题。研究借鉴心理学中的内隐联想测试(IAT)等理论框架，将检测方法分为词汇联想、任务导向文本生成和决策制定三类，并建立了基于单值度量和比较值度量的评估体系。论文不仅梳理了现有缓解隐性偏见的努力，还为未来研究提供了方向指引，填补了LLMs在显性偏见测试表现优异但隐性偏见研究不足的空白，为构建更公平的AI系统提供了理论支撑。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02776v1",
      "published_date": "2025-03-04 16:49:37 UTC",
      "updated_date": "2025-03-04 16:49:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:40:03.956372"
    },
    {
      "arxiv_id": "2503.02773v1",
      "title": "Prime Convolutional Model: Breaking the Ground for Theoretical Explainability",
      "title_zh": "Prime 卷积模型：为理论可解释性奠定基础",
      "authors": [
        "Francesco Panelli",
        "Doaa Almhaithawi",
        "Tania Cerquitelli",
        "Alessandro Bellini"
      ],
      "abstract": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.",
      "tldr_zh": "该论文提出了一种名为\"Prime Convolutional Model\"(p-Conv)的新型理论框架，旨在为可解释AI奠定数学基础。研究者设计了一个卷积型神经网络，在包含前100万个自然数的数据集上训练模型识别模m同余类。实验表明，p-Conv的性能表现可以通过m(模数)和B(输入序列长度)等参数进行数学建模，该模型不仅能预测网络何时能成功完成任务，还能解释失败时的误差模式。这一发现为神经网络行为提供了可验证的理论解释机制。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02773v1",
      "published_date": "2025-03-04 16:42:46 UTC",
      "updated_date": "2025-03-04 16:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:40:41.974519"
    },
    {
      "arxiv_id": "2503.02749v1",
      "title": "Improving Oil Slick Trajectory Simulations with Bayesian Optimization",
      "title_zh": "利用贝叶斯优化改进油膜轨迹模拟",
      "authors": [
        "Gabriele Accarino",
        "Marco M. De Carlo",
        "Igor Atake",
        "Donatello Elia",
        "Anusha L. Dissanayake",
        "Antonio Augusto Sepp Neves",
        "Juan Peña Ibañez",
        "Italo Epicoco",
        "Paola Nassisi",
        "Sandro Fiore",
        "Giovanni Coppini"
      ],
      "abstract": "Accurate simulations of oil spill trajectories are essential for supporting\npractitioners' response and mitigating environmental and socioeconomic impacts.\nNumerical models, such as MEDSLIK-II, simulate advection, dispersion, and\ntransformation processes of oil particles. However, simulations heavily rely on\naccurate parameter tuning, still based on expert knowledge and manual\ncalibration. To overcome these limitations, we integrate the MEDSLIK-II\nnumerical oil spill model with a Bayesian optimization framework to iteratively\nestimate the best physical parameter configuration that yields simulation\ncloser to satellite observations of the slick. We focus on key parameters, such\nas horizontal diffusivity and drift factor, maximizing the Fraction Skill Score\n(FSS) as a measure of spatio-temporal overlap between simulated and observed\noil distributions. We validate the framework for the Baniyas oil incident that\noccurred in Syria between August 23 and September 4, 2021, which released over\n12,000 $m^3$ of oil. We show that, on average, the proposed approach\nsystematically improves the FSS from 5.82% to 11.07% compared to control\nsimulations initialized with default parameters. The optimization results in\nconsistent improvement across multiple time steps, particularly during periods\nof increased drift variability, demonstrating the robustness of our method in\ndynamic environmental conditions.",
      "tldr_zh": "该研究提出了一种基于贝叶斯优化(Bayesian Optimization)的方法，用于改进油膜轨迹模拟的准确性。通过将MEDSLIK-II数值油膜扩散模型与贝叶斯优化框架结合，迭代估计最佳物理参数配置，使模拟结果更接近卫星观测数据。研究以2021年叙利亚Baniyas石油泄漏事件为案例，优化了水平扩散系数和漂移因子等关键参数，结果表明，优化后的模拟与观测的时空重叠分数(FSS)从5.82%提升至11.07%，尤其在漂移变化显著的时间段表现出更强的鲁棒性。",
      "categories": [
        "physics.ao-ph",
        "cs.AI",
        "I.2; I.6; J.2; G.3"
      ],
      "primary_category": "physics.ao-ph",
      "comment": "29 pages, 10 figures, 3 tables, research paper",
      "pdf_url": "http://arxiv.org/pdf/2503.02749v1",
      "published_date": "2025-03-04 16:14:16 UTC",
      "updated_date": "2025-03-04 16:14:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:40:49.213646"
    },
    {
      "arxiv_id": "2503.05815v1",
      "title": "Trust, Experience, and Innovation: Key Factors Shaping American Attitudes About AI",
      "title_zh": "信任、经验与创新：塑造美国民众对AI态度的关键因素",
      "authors": [
        "Risa Palm",
        "Justin Kingsland",
        "Toby Bolsen"
      ],
      "abstract": "A large survey of American adults explored the complex landscape of attitudes\ntowards artificial intelligence (AI). It explored the degree of concern\nregarding specific potential outcomes of the new advances in AI technology and\ncorrelates of these concerns. Key variables associated with the direction and\nintensity of concern include prior experience using a large language model such\nas ChatGPT, general trust in science, adherence to the precautionary principle\nversus support for unrestricted innovation, and demographic factors such as\ngender. By analyzing these relationships, the paper provides valuable insights\ninto the American public's response to AI that are particularly important in\nthe development of policy to regulate or further encourage its development.",
      "tldr_zh": "这项针对美国成年人的大规模调查揭示了公众对人工智能(AI)态度的关键影响因素。研究发现，对AI技术的担忧程度与是否使用过ChatGPT等大语言模型、对科学的普遍信任度、以及支持\"预防原则\"还是\"无限制创新\"的立场显著相关。调查还显示性别等人口因素也会影响态度倾向。该研究为制定AI监管政策提供了重要参考，有助于平衡技术创新与社会风险管控。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "35 pages, 3 figures, 2 tables, appendix",
      "pdf_url": "http://arxiv.org/pdf/2503.05815v1",
      "published_date": "2025-03-04 16:08:20 UTC",
      "updated_date": "2025-03-04 16:08:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:08.358125"
    },
    {
      "arxiv_id": "2503.02733v1",
      "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression",
      "title_zh": "UAR-NVC：面向高效内存神经视频压缩的统一自回归框架",
      "authors": [
        "Jia Wang",
        "Xinfeng Zhang",
        "Gai Zhang",
        "Jun Zhu",
        "Lv Tang",
        "Li Zhang"
      ],
      "abstract": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
      "tldr_zh": "该研究提出了一种统一的自回归框架UAR-NVC，用于实现内存高效的神经视频压缩。UAR-NVC将基于时间线的传统视频压缩与基于隐式神经表示(INRs)的神经视频压缩统一到自回归建模框架中，通过将视频分段处理并优化模型参数初始化与训练，显著减少了内存消耗。实验表明，UAR-NVC在资源受限环境下表现出色，性能优于多种基线模型，并通过调整片段长度支持可调节的延迟。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
      "published_date": "2025-03-04 15:54:57 UTC",
      "updated_date": "2025-03-04 15:54:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:40:57.716785"
    },
    {
      "arxiv_id": "2503.02720v1",
      "title": "Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation Efficiency",
      "title_zh": "振动辅助迟滞抑制实现高补偿效率",
      "authors": [
        "Myeongbo Park",
        "Chunggil An",
        "Junhyun Park",
        "Jonghyun Kang",
        "Minho Hwang"
      ],
      "abstract": "Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.",
      "tldr_zh": "该研究提出了一种振动辅助迟滞补偿方法，用于解决微创手术中肌腱-鞘机构（TSMs）存在的摩擦、间隙和肌腱伸长导致的轨迹跟踪误差问题。通过沿肌腱运动方向施加受控振动，有效降低了迟滞现象，实验显示振动使均方根误差(RMSE)最高减少23.41%，同时结合基于时序卷积网络(TCN)的补偿模型后，平均绝对误差(MAE)降低了85.2%。该方法无需复杂参数调整，为微创手术机器人系统提供了更高效、可扩展的解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures, and 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02720v1",
      "published_date": "2025-03-04 15:36:19 UTC",
      "updated_date": "2025-03-04 15:36:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:21.772494"
    },
    {
      "arxiv_id": "2503.04814v1",
      "title": "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings for Phonetic Analysis",
      "title_zh": "通过微调实现归一化：理解 Wav2vec 2.0 嵌入用于语音分析",
      "authors": [
        "Yiming Wang",
        "Yi Yang",
        "Jiahong Yuan"
      ],
      "abstract": "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.",
      "tldr_zh": "本研究探讨了在微调预训练大模型（如wav2vec 2.0）时，语音归一化（phonetic normalization）如何隐式地通过模型实现。研究发现，微调wav2vec 2.0能够通过选择性抑制任务无关信息有效实现语音归一化，且多任务微调的模型可以同时保留多个任务的信息而不影响性能。这一发现为语音模型中的语音归一化机制提供了新见解，并揭示了其在人类语音感知中的实现方式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04814v1",
      "published_date": "2025-03-04 15:28:10 UTC",
      "updated_date": "2025-03-04 15:28:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:27.100873"
    },
    {
      "arxiv_id": "2503.13476v1",
      "title": "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
      "title_zh": "基于Transformer深度度量学习的雷达脉冲去交错",
      "authors": [
        "Edward Gunn",
        "Adam Hosford",
        "Daniel Mannion",
        "Jarrod Williams",
        "Varun Chhabra",
        "Victoria Nockles"
      ],
      "abstract": "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
      "tldr_zh": "该论文提出了一种基于Transformer和深度度量学习的方法，用于解决雷达脉冲解交织问题。该方法通过使用三元组损失在合成数据上训练Transformer模型，能够从未知数量的发射器中分离出雷达脉冲。实验结果显示，该模型在调整后的互信息得分上达到了0.882，优于其他深度学习模型，有效提升了雷达脉冲分类的准确性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Preprint: Accepted to IEEE International Radar Conference 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.13476v1",
      "published_date": "2025-03-04 15:27:17 UTC",
      "updated_date": "2025-03-04 15:27:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:17.892856"
    },
    {
      "arxiv_id": "2503.02703v1",
      "title": "Generative Tools for Graphical Assets: Empirical Guidelines based on Game Designers' and Developers' Preferences",
      "title_zh": "图形资产生成工具：基于游戏设计师与开发者偏好的实证指南",
      "authors": [
        "Kaisei Fukaya",
        "Damon Daylamani-Zad",
        "Harry Agius"
      ],
      "abstract": "Graphical assets play an important role in the design and development of\ngames. There is potential in the use of generative tools, to aid in creating\ngraphical assets, thus improving game design and development pipelines.\nHowever, there is little research to address how the generative methods can fit\ninto the wider pipeline. We conducted a user study with 16 game designers and\ndevelopers to examine their preferences regarding generative tools for\ngraphical assets. The findings highlight that early design stage is preferred\nby all participants (mean values above 0.67 and p < .001 for early stages).\nDesigners and developers prefer to use such tools for creating large amounts of\nvariations at the cost of quality as they can improve the quality of the\nartefacts once they generate a suitable asset (mean value 0.17 where 1 is high\nquality, p < .001). They also strongly (mean value .78, p < .001) raised the\nneed for better integration of such tools in existing design and development\nenvironments and the need for the outputs to be in common data formats, to be\nmanipulatable and integrate smoothly into existing environments (mean 3.5 out\nof 5, p = .004). The study also highlights the requirement for further emphasis\non the needs of the users to incorporate these tools effectively in existing\npipelines. Informed by these results, we provide a set of guidelines for\ncreating tools that meet the expectations and needs of game designers and\ndevelopers.",
      "tldr_zh": "这项研究通过16位游戏设计师和开发者的用户调研，揭示了生成式工具在图形资产创作中的应用偏好。研究发现从业者更倾向在早期设计阶段（均值>0.67，p<0.001）使用这类工具批量生成低质量变体（质量均值0.17，p<0.001），后期再人工优化。研究特别强调需要提升工具与现有开发环境的集成度（均值0.78，p<0.001）和输出格式兼容性（3.5/5分，p=0.004）。基于实证数据，论文为开发符合游戏工作流的生成工具提供了设计指南。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02703v1",
      "published_date": "2025-03-04 15:18:50 UTC",
      "updated_date": "2025-03-04 15:18:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:46.253573"
    },
    {
      "arxiv_id": "2503.02701v1",
      "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
      "title_zh": "MindBridge：通过记忆增强模态实现可扩展的跨模型知识编辑",
      "authors": [
        "Shuaike Li",
        "Kai Zhang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "abstract": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
      "tldr_zh": "该研究提出了MindBridge，一种基于记忆增强模态的可扩展跨模型知识编辑方法，旨在解决现有知识编辑技术过度依赖特定模型、无法适应大语言模型(LLMs)频繁更新的问题。MindBridge创新性地引入记忆模态概念，将编辑后的知识编码为独立模态，先进行LLM无关的预训练，再与不同LLMs集成。实验表明，该方法在多个LLMs和知识编辑数据集上表现优异，能够灵活处理数万条知识条目，为跨模型知识编辑提供了高效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02701v1",
      "published_date": "2025-03-04 15:17:57 UTC",
      "updated_date": "2025-03-04 15:17:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:33.461276"
    },
    {
      "arxiv_id": "2503.02691v1",
      "title": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection",
      "title_zh": "面向边缘设备的视觉异常检测高效记忆持续学习",
      "authors": [
        "Manuel Barusco",
        "Lorenzo D'Antoni",
        "Davide Dalle Pezze",
        "Francesco Borsatti",
        "Gian Antonio Susto"
      ],
      "abstract": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.",
      "tldr_zh": "该研究探讨了面向边缘设备的视觉异常检测（VAD）持续学习问题（CLAD），提出了一种低内存占用的解决方案。研究评估了STFPM方法，并结合Replay技术展示了其良好的性能。此外，研究首次将专为边缘设备设计的PaSTe方法引入持续学习场景，发现其不仅比STFPM更轻量化，且异常检测性能提升了10%。通过压缩Replay技术，PaSTe进一步将内存开销减少91.5%，验证了在资源受限的边缘设备上部署自适应VAD模型的可行性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02691v1",
      "published_date": "2025-03-04 15:03:47 UTC",
      "updated_date": "2025-03-04 15:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:41:45.940687"
    },
    {
      "arxiv_id": "2503.02687v1",
      "title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?",
      "title_zh": "Class-Aware PillarMix：混合样本数据增强能否提升基于雷达点云的3D目标检测？",
      "authors": [
        "Miao Zhang",
        "Sherif Abdulatif",
        "Benedikt Loesch",
        "Marco Altmann",
        "Bin Yang"
      ],
      "abstract": "Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.",
      "tldr_zh": "本文提出了Class-Aware PillarMix (CAPMix)，一种针对雷达点云数据的混合样本数据增强(MSDA)方法。CAPMix在3D点云的pillar级别上应用MixUp，并根据类别标签为每个pillar分配独立的混合比例，从而增强样本多样性。针对不同类别的密度特性，该方法采用类别特定的分布策略：对于密集物体（如大型车辆），倾向于从另一个样本中获取更多点；对于稀疏物体（如行人），则保留更多原始样本的点。实验结果表明，CAPMix在Bosch Street和K-Radar两个数据集上显著提升了3D目标检测性能，并优于现有MSDA方法。该研究为雷达数据的MSDA技术提供了新的思路。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02687v1",
      "published_date": "2025-03-04 15:02:07 UTC",
      "updated_date": "2025-03-04 15:02:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:05.092676"
    },
    {
      "arxiv_id": "2503.02686v1",
      "title": "Seeding for Success: Skill and Stochasticity in Tabletop Games",
      "title_zh": "播种成功：桌面游戏中的技巧与随机性",
      "authors": [
        "James Goodman",
        "Diego Perez-Liebana",
        "Simon Lucas"
      ],
      "abstract": "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
      "tldr_zh": "这篇论文研究了桌游中随机性（如骰子、洗牌）与玩家技巧之间的平衡关系。作者提出了一种量化游戏结果随机性水平的新方法，并应用于分析15款桌游，解构了不同游戏环节对整体随机性的贡献。研究还探讨了随机性与玩家技能的交互作用，以及这种固有随机性如何影响常见游戏实验中的误差分析，为游戏设计者根据目标受众调整随机性水平提供了理论依据。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in IEEE Transactions on Games, 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02686v1",
      "published_date": "2025-03-04 14:58:59 UTC",
      "updated_date": "2025-03-04 14:58:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:05.704887"
    },
    {
      "arxiv_id": "2503.02682v1",
      "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
      "title_zh": "MPO：通过元计划优化提升LLM智能体性能",
      "authors": [
        "Weimin Xiong",
        "Yifan Song",
        "Qingxiu Dong",
        "Bingchan Zhao",
        "Feifan Song",
        "Xun Wang",
        "Sujian Li"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
      "tldr_zh": "该研究提出元规划优化框架(MPO)，通过引入元计划(Meta Plans)的高层指导来解决LLM智能体在交互式规划任务中的幻觉问题和重复训练需求。该方法采用基于任务执行反馈的元计划持续优化机制，相比传统依赖复杂知识的方法，显著提升了任务完成效率和泛化能力。实验表明，MPO在两个典型任务上优于现有基线，并为LLM智能体提供了即插即用的规划增强方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02682v1",
      "published_date": "2025-03-04 14:54:45 UTC",
      "updated_date": "2025-03-04 14:54:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:21.149014"
    },
    {
      "arxiv_id": "2503.02675v1",
      "title": "State of play and future directions in industrial computer vision AI standards",
      "title_zh": "工业计算机视觉AI标准的现状与未来发展方向",
      "authors": [
        "Artemis Stefanidou",
        "Panagiotis Radoglou-Grammatikis",
        "Vasileios Argyriou",
        "Panagiotis Sarigiannidis",
        "Iraklis Varlamis",
        "Georgios Th. Papadopoulos"
      ],
      "abstract": "The recent tremendous advancements in the areas of Artificial Intelligence\n(AI) and Deep Learning (DL) have also resulted into corresponding remarkable\nprogress in the field of Computer Vision (CV), showcasing robust technological\nsolutions in a wide range of application sectors of high industrial interest\n(e.g., healthcare, autonomous driving, automation, etc.). Despite the\noutstanding performance of CV systems in specific domains, their development\nand exploitation at industrial-scale necessitates, among other, the addressing\nof requirements related to the reliability, transparency, trustworthiness,\nsecurity, safety, and robustness of the developed AI models. The latter raises\nthe imperative need for the development of efficient, comprehensive and\nwidely-adopted industrial standards. In this context, this study investigates\nthe current state of play regarding the development of industrial computer\nvision AI standards, emphasizing on critical aspects, like model\ninterpretability, data quality, and regulatory compliance. In particular, a\nsystematic analysis of launched and currently developing CV standards, proposed\nby the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,\netc.) is performed. The latter is complemented by a comprehensive discussion on\nthe current challenges and future directions observed in this regularization\nendeavor.",
      "tldr_zh": "该研究探讨了工业计算机视觉AI标准的现状与未来发展方向。尽管计算机视觉在医疗、自动驾驶等工业领域展现了卓越性能，但其大规模应用仍需解决模型可靠性、透明性、安全性等问题，亟需建立高效且广泛采纳的工业标准。研究系统分析了国际标准化组织（如ISO/IEC、IEEE等）已发布和正在制定的标准，重点关注模型可解释性、数据质量和法规合规性等关键问题，并讨论了当前挑战和未来标准化的发展方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02675v1",
      "published_date": "2025-03-04 14:46:34 UTC",
      "updated_date": "2025-03-04 14:46:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:28.538605"
    },
    {
      "arxiv_id": "2503.04813v1",
      "title": "Self-Evolved Preference Optimization for Enhancing Mathematical Reasoning in Small Language Models",
      "title_zh": "自进化偏好优化：增强小型语言模型的数学推理能力",
      "authors": [
        "Joykirat Singh",
        "Tanmoy Chakraborty",
        "Akshay Nambi"
      ],
      "abstract": "Large language models (LLMs) have significantly improved their reasoning\ncapabilities; however, they still struggle with complex multi-step mathematical\nproblem-solving due to error propagation, lack of self-correction, and limited\nadaptability to diverse reasoning styles. Existing methods rely on static\nfine-tuning or prompt engineering, which fail to generalize across problem\ncomplexities, while the scarcity of high-quality preference data further\nhinders reliable reasoning.\n  We introduce SPHERE, a self-evolving data generation pipeline that enhances\nreasoning in small language models (SLMs) by iteratively generating,\ncorrecting, and diversifying reasoning chains. SPHERE operates in three stages:\n(i) Self-Generation, where the model autonomously constructs problem-solving\nsteps; (ii) Self-Correction, enabling it to identify and rectify errors; and\n(iii) Diversity Induction, improving robustness through multiple valid\nreasoning trajectories. This self-evolution mechanism strengthens mathematical\nreasoning and enhances model reliability. Evaluations on MATH 500, GSM8K, AIME,\nAMC, and Olympiad show that SPHERE-trained models achieve significant gains\nover their base versions and match/surpass GPT-4o on certain benchmarks. Our\nfindings demonstrate that self-evolving models can close the reasoning gap\nbetween SLMs and state-of-the-art LLMs, making mathematical AI more reliable,\nscalable, and efficient.",
      "tldr_zh": "本研究提出了SPHERE，一种自我进化的数据生成框架，旨在提升小型语言模型(SLMs)在复杂多步数学问题上的推理能力。SPHERE通过自我生成、自我纠错和多样性诱导三个阶段的迭代优化，生成、修正并多样化推理链，从而增强模型的可靠性和鲁棒性。实验表明，经过SPHERE训练的模型在MATH 500、GSM8K等多个数学推理基准上显著优于基线模型，并在某些任务上达到或超越了GPT-4的性能，证明了自我进化机制可以有效缩小小型语言模型与大型语言模型之间的推理能力差距。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04813v1",
      "published_date": "2025-03-04 14:43:25 UTC",
      "updated_date": "2025-03-04 14:43:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:41.578278"
    },
    {
      "arxiv_id": "2503.02660v2",
      "title": "A dataset-free approach for self-supervised learning of 3D reflectional symmetries",
      "title_zh": "一种无需数据集的自监督学习三维反射对称性的方法",
      "authors": [
        "Isaac Aguirre",
        "Ivan Sipiran",
        "Gabriel Montañana"
      ],
      "abstract": "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
      "tldr_zh": "本研究提出了一种无需数据集的自我监督学习方法，用于检测单个3D物体的反射对称性。该方法通过假设物体的对称性可由其内在特征确定，从而避免了对大规模训练数据集的需求，并设计了无需真实标签的自我监督学习策略。其创新点在于利用基础图像模型提取的视觉特征为点云中的每个点计算视觉描述符，使对称点表现出相似的视觉外观。实验表明，该方法在效率和效果上均优于依赖大数据集训练的现有模型，且计算资源消耗更低。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02660v2",
      "published_date": "2025-03-04 14:22:08 UTC",
      "updated_date": "2025-03-05 19:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:52.278341"
    },
    {
      "arxiv_id": "2503.02650v1",
      "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats",
      "title_zh": "大语言模型在将非结构化文本转换为标准化格式中的有效性",
      "authors": [
        "William Brach",
        "Kristián Košťál",
        "Michal Ries"
      ],
      "abstract": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
      "tldr_zh": "本研究系统评估了大语言模型(LLMs)将非结构化文本转换为标准化格式的能力，以食谱文本转换为Cooklang格式为例。通过测试GPT-4o、Llama3.1等模型，研究发现GPT-4o在少量提示下取得了突破性性能(ROUGE-L: 0.9722)，首次证明LLMs无需大量训练即可可靠地转换领域特定的非结构化文本。尽管模型性能通常随规模提升，但小型模型如Llama3.1:8b通过针对性微调也展现出优化潜力。这些发现为从医疗记录到技术文档等领域的自动化结构化数据生成开辟了新的可能性，有望改变组织处理和利用非结构化信息的方式。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02650v1",
      "published_date": "2025-03-04 14:14:28 UTC",
      "updated_date": "2025-03-04 14:14:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:42:50.021441"
    },
    {
      "arxiv_id": "2503.02636v1",
      "title": "YARE-GAN: Yet Another Resting State EEG-GAN",
      "title_zh": "YARE-GAN：静息态脑电生成对抗网络的又一创新",
      "authors": [
        "Yeganeh Farahzadi",
        "Morteza Ansarinia",
        "Zoltan Kekecs"
      ],
      "abstract": "Generative Adversarial Networks (GANs) have shown promise in synthesising\nrealistic neural data, yet their potential for unsupervised representation\nlearning in resting-state EEG remains under explored. In this study, we\nimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate\nmulti-channel resting-state EEG data and assess the quality of the synthesised\nsignals through both visual and feature-based evaluations. Our results indicate\nthat the model effectively captures the statistical and spectral\ncharacteristics of real EEG data, although challenges remain in replicating\nhigh-frequency oscillations in the frontal region. Additionally, we demonstrate\nthat the Critic's learned representations can be fine-tuned for age group\nclassification, achieving an out-of-sample accuracy, significantly better than\na shuffled-label baseline. These findings suggest that generative models can\nserve not only as EEG data generators but also as unsupervised feature\nextractors, reducing the need for manual feature engineering. This study\nhighlights the potential of GAN-based unsupervised learning for EEG analysis,\nsuggesting avenues for more data-efficient deep learning applications in\nneuroscience.",
      "tldr_zh": "该研究提出了YARE-GAN，一种基于Wasserstein GAN与梯度惩罚(WGAN-GP)的生成对抗网络，用于生成多通道静息态脑电图(EEG)数据。实验表明，该模型能够有效捕捉真实EEG数据的统计和频谱特征，尽管在复制额叶区域的高频振荡方面仍存在挑战。此外，Critic学习到的特征可用于年龄组分类，其分类准确率显著优于随机标签基线。研究表明，GAN不仅可作为EEG数据生成器，还能作为无监督特征提取器，减少对手工特征工程的依赖，为神经科学中更高效的数据深度学习应用提供了新方向。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02636v1",
      "published_date": "2025-03-04 14:01:10 UTC",
      "updated_date": "2025-03-04 14:01:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:43:26.144501"
    },
    {
      "arxiv_id": "2503.02631v1",
      "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
      "title_zh": "从人机协作视角反思生成式AI时代的数据叙事工具",
      "authors": [
        "Haotian Li",
        "Yun Wang",
        "Huamin Qu"
      ],
      "abstract": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
      "tldr_zh": "本文从人机协作的角度反思了生成式AI时代的数据叙事工具。研究通过对比早期和最新工具的合作模式，识别出持续存在的模式（如人类创作者+AI助手）和新兴模式（如AI创作者+人类评审员）。文章揭示了大型语言模型（LLMs）和文本生成图像模型在提升数据叙事方面的潜力，并提出了未来研究方向，以推动这一领域的创新。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements",
      "pdf_url": "http://arxiv.org/pdf/2503.02631v1",
      "published_date": "2025-03-04 13:56:18 UTC",
      "updated_date": "2025-03-04 13:56:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:43:14.630064"
    },
    {
      "arxiv_id": "2503.02628v1",
      "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction",
      "title_zh": "面向大规模事件类型抽取：基于大语言模型的协同标注与分区抽取",
      "authors": [
        "Wenxuan Liu",
        "Zixuan Li",
        "Long Bai",
        "Yuxin Zuo",
        "Daozhu Xu",
        "Xiaolong Jin",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
      "tldr_zh": "该研究针对事件抽取(EE)领域的大规模类型识别难题，提出了两项创新方案：1）基于大语言模型(LLMs)的协同标注方法，通过多模型协作优化触发词标注和论元标注，构建了当前最大的EEMT数据集（含20万样本、3465种事件类型）；2）开发了LLM-PEE分区抽取框架，通过类型召回和分区处理突破LLMs上下文长度限制。实验表明，LLM-PEE在全监督设定下事件检测和论元抽取指标分别提升5.4和6.1，零样本设定下较主流LLMs最高提升12.9，展现出卓越的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2503.02628v1",
      "published_date": "2025-03-04 13:53:43 UTC",
      "updated_date": "2025-03-04 13:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:43:46.642431"
    },
    {
      "arxiv_id": "2503.02623v2",
      "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models",
      "title_zh": "激励质疑：基于强化学习的大型语言模型置信度校准方法",
      "authors": [
        "Paul Stangel",
        "David Bani-Harouni",
        "Chantal Pellegrini",
        "Ege Özsoy",
        "Kamilia Zaripova",
        "Matthias Keicher",
        "Nassir Navab"
      ],
      "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
      "tldr_zh": "该研究提出了一种基于强化学习(RL)的新方法，用于校准大语言模型(LLMs)对其答案的置信度。通过将问题建模为投注游戏，并为模型设计奖励函数以惩罚过度自信和自信不足，研究证明在最优策略下可实现完美的置信度校准。实验结果显示，该方法显著提高了LLMs的置信度校准能力，并能泛化到新任务而无需重新训练，表明其能够培养模型对置信度的普遍意识，从而训练出内在校准的LLMs。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02623v2",
      "published_date": "2025-03-04 13:48:50 UTC",
      "updated_date": "2025-03-05 15:23:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:06.904777"
    },
    {
      "arxiv_id": "2503.03775v1",
      "title": "BotUmc: An Uncertainty-Aware Twitter Bot Detection with Multi-view Causal Inference",
      "title_zh": "BotUmc：基于多视角因果推理的Twitter机器人不确定性感知检测",
      "authors": [
        "Tao Yang",
        "Yang Hu",
        "Feihong Lu",
        "Ziwei Zhang",
        "Qingyun Sun",
        "Jianxin Li"
      ],
      "abstract": "Social bots have become widely known by users of social platforms. To prevent\nsocial bots from spreading harmful speech, many novel bot detections are\nproposed. However, with the evolution of social bots, detection methods\nstruggle to give high-confidence answers for samples. This motivates us to\nquantify the uncertainty of the outputs, informing the confidence of the\nresults. Therefore, we propose an uncertainty-aware bot detection method to\ninform the confidence and use the uncertainty score to pick a high-confidence\ndecision from multiple views of a social network under different environments.\nSpecifically, our proposed BotUmc uses LLM to extract information from tweets.\nThen, we construct a graph based on the extracted information, the original\nuser information, and the user relationship and generate multiple views of the\ngraph by causal interference. Lastly, an uncertainty loss is used to force the\nmodel to quantify the uncertainty of results and select the result with low\nuncertainty in one view as the final decision. Extensive experiments show the\nsuperiority of our method.",
      "tldr_zh": "该研究提出BotUmc方法，一种基于多视角因果推理的不确定性感知Twitter机器人检测框架。该方法利用大语言模型(LLM)从推文提取信息，结合用户原始信息和社交关系构建图谱，并通过因果干预生成多视角图谱表示。创新性地引入不确定性损失函数，使模型能量化预测结果的可信度，最终选择低不确定性的视角决策。实验证明该框架在机器人检测任务上表现优异，为解决社交机器人演化带来的检测挑战提供了新思路。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.03775v1",
      "published_date": "2025-03-04 13:39:31 UTC",
      "updated_date": "2025-03-04 13:39:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:16.453924"
    },
    {
      "arxiv_id": "2503.02612v1",
      "title": "Reinforcement Learning-based Threat Assessment",
      "title_zh": "基于强化学习的威胁评估",
      "authors": [
        "Wuzhou Sun",
        "Siyi Li",
        "Qingxiang Zou",
        "Zixing Liao"
      ],
      "abstract": "In some game scenarios, due to the uncertainty of the number of enemy units\nand the priority of various attributes, the evaluation of the threat level of\nenemy units as well as the screening has been a challenging research topic, and\nthe core difficulty lies in how to reasonably set the priority of different\nattributes in order to achieve quantitative evaluation of the threat. In this\npaper, we innovatively transform the problem of threat assessment into a\nreinforcement learning problem, and through systematic reinforcement learning\ntraining, we successfully construct an efficient neural network evaluator. The\nevaluator can not only comprehensively integrate the multidimensional attribute\nfeatures of the enemy, but also effectively combine our state information, thus\nrealizing a more accurate and scientific threat assessment.",
      "tldr_zh": "该研究创新性地将威胁评估问题转化为强化学习问题，通过系统性训练成功构建了一个高效的神经网络评估器。该评估器能够综合整合敌方的多维属性特征，并结合己方状态信息，实现了更为准确和科学的威胁评估。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages,9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02612v1",
      "published_date": "2025-03-04 13:32:40 UTC",
      "updated_date": "2025-03-04 13:32:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:25.338677"
    },
    {
      "arxiv_id": "2503.02918v1",
      "title": "Straight-Line Diffusion Model for Efficient 3D Molecular Generation",
      "title_zh": "直线扩散模型：高效的三维分子生成方法",
      "authors": [
        "Yuyan Ni",
        "Shikun Feng",
        "Haohan Chi",
        "Bowen Zheng",
        "Huan-ang Gao",
        "Wei-Ying Ma",
        "Zhi-Ming Ma",
        "Yanyan Lan"
      ],
      "abstract": "Diffusion-based models have shown great promise in molecular generation but\noften require a large number of sampling steps to generate valid samples. In\nthis paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle\nthis problem, by formulating the diffusion process to follow a linear\ntrajectory. The proposed process aligns well with the noise sensitivity\ncharacteristic of molecular structures and uniformly distributes reconstruction\neffort across the generative process, thus enhancing learning efficiency and\nefficacy. Consequently, SLDM achieves state-of-the-art performance on 3D\nmolecule generation benchmarks, delivering a 100-fold improvement in sampling\nefficiency. Furthermore, experiments on toy data and image generation tasks\nvalidate the generality and robustness of SLDM, showcasing its potential across\ndiverse generative modeling domains.",
      "tldr_zh": "本文提出了一种新型的直线扩散模型(SLDM)，通过将扩散过程设计为线性轨迹来提升3D分子生成效率。该方法针对分子结构对噪声敏感的特性进行优化，在生成过程中均匀分配重构任务，从而显著提高学习效率。实验表明，SLDM在3D分子生成基准测试中达到最优性能，采样效率提升100倍，并在玩具数据和图像生成任务中验证了其通用性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02918v1",
      "published_date": "2025-03-04 13:23:58 UTC",
      "updated_date": "2025-03-04 13:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:42.383349"
    },
    {
      "arxiv_id": "2503.02597v2",
      "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs",
      "title_zh": "眼见为实：解锁多模态大语言模型中的因果注意力至模态互注意力机制",
      "authors": [
        "Wei-Yao Wang",
        "Zhao Wang",
        "Helen Suzuki",
        "Yoshiyuki Kobayashi"
      ],
      "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https://github.com/sony/aki to encourage further advancements in\nMLLMs across various directions.",
      "tldr_zh": "本研究提出**MapleLeaf AKI**多模态大语言模型(MLLM)，通过创新性地将传统因果注意力机制扩展为**模态互注意力(MMA)**机制，解决了现有MLLM中视觉-语言模态对齐不足的核心问题。该方法突破性地允许图像token关注文本token，在无需增加参数和训练时间的情况下，显著提升了12个多模态理解基准任务的平均性能(+7.2%)。这种通用且可扩展的架构设计为多模态基础模型的发展提供了新思路，相关代码和模型已开源以促进领域发展。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.02597v2",
      "published_date": "2025-03-04 13:18:33 UTC",
      "updated_date": "2025-03-13 01:48:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:56.022257"
    },
    {
      "arxiv_id": "2503.02595v1",
      "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
      "title_zh": "StageDesigner：基于戏剧脚本的艺术化舞台布景生成系统",
      "authors": [
        "Zhaoxing Gan",
        "Mengtian Li",
        "Ruhua Chen",
        "Zhongxia Ji",
        "Sichen Guo",
        "Huanling Hu",
        "Guangnan Ye",
        "Zuo Hu"
      ],
      "abstract": "In this work, we introduce StageDesigner, the first comprehensive framework\nfor artistic stage generation using large language models combined with\nlayout-controlled diffusion models. Given the professional requirements of\nstage scenography, StageDesigner simulates the workflows of seasoned artists to\ngenerate immersive 3D stage scenes. Specifically, our approach is divided into\nthree primary modules: Script Analysis, which extracts thematic and spatial\ncues from input scripts; Foreground Generation, which constructs and arranges\nessential 3D objects; and Background Generation, which produces a harmonious\nbackground aligned with the narrative atmosphere and maintains spatial\ncoherence by managing occlusions between foreground and background elements.\nFurthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276\nunique stage scenes spanning different historical styles and annotated with\nscripts, images, and detailed 3D layouts, specifically tailored for this task.\nFinally, evaluations using both standard and newly proposed metrics, along with\nextensive user studies, demonstrate the effectiveness of StageDesigner. Project\ncan be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",
      "tldr_zh": "本文提出了StageDesigner，首个基于大语言模型和布局控制扩散模型的舞台场景生成框架，旨在满足舞台美术设计的专业需求。该框架通过三个核心模块实现：剧本分析提取主题和空间线索，前景生成构建并布置关键3D对象，背景生成创建与叙事氛围一致的和谐背景并处理前后景遮挡关系。研究还引入了StagePro-V1数据集，包含276个涵盖不同历史风格的舞台场景，并配有剧本、图像和详细3D布局标注。实验结果表明，StageDesigner在生成沉浸式3D舞台场景方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02595v1",
      "published_date": "2025-03-04 13:17:50 UTC",
      "updated_date": "2025-03-04 13:17:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:50.604733"
    },
    {
      "arxiv_id": "2503.13475v1",
      "title": "Cross-Subject Depression Level Classification Using EEG Signals with a Sample Confidence Method",
      "title_zh": "基于样本置信度方法的跨被试抑郁症水平EEG信号分类",
      "authors": [
        "ZhongYi Zhang",
        "ChenYang Xu",
        "LiXuan Zhao",
        "HuiRang Hou",
        "QingHao Meng"
      ],
      "abstract": "Electroencephalogram (EEG) is a non-invasive tool for real-time neural\nmonitoring,widely used in depression detection via deep learning. However,\nexisting models primarily focus on binary classification (depression/normal),\nlacking granularity for severity assessment. To address this, we proposed the\nDepL-GCN, i.e., Depression Level classification based on GCN model. This model\ntackles two key challenges: (1) subjectivity in depres-sion-level labeling due\nto patient self-report biases, and (2) class imbalance across severity\ncategories. Inspired by the model learning patterns, we introduced two novel\nmodules: the sample confidence module and the minority sample penalty module.\nThe former leverages the L2-norm of prediction errors to progressively filter\nEEG samples with weak label alignment during training, thereby reducing the\nimpact of subjectivity; the latter automatically upweights misclassified\nminority-class samples to address imbalance issues. After testing on two public\nEEG datasets, DepL-GCN achieved accuracies of 81.13% and 81.36% for multi-class\nseverity recognition, outperforming baseline models.Ablation studies confirmed\nboth modules' contributions. We further discussed the strengths and limitations\nof regression-based models for depression-level recognition.",
      "tldr_zh": "该研究提出DepL-GCN模型，采用图卷积网络(GCN)实现基于脑电图(EEG)信号的抑郁症多等级分类。针对抑郁症严重程度标注的主观性和类别不平衡问题，模型创新性地引入样本置信度模块（通过预测误差L2范数筛选标签不可靠样本）和少数类样本惩罚模块（自动增权误分类样本）。在两个公开EEG数据集上的实验表明，该模型在抑郁症多级分类任务中分别达到81.13%和81.36%的准确率，显著优于基线方法。消融研究验证了两个模块的有效性，同时探讨了回归模型在抑郁程度识别中的适用性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13475v1",
      "published_date": "2025-03-04 13:16:11 UTC",
      "updated_date": "2025-03-04 13:16:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:57.485889"
    },
    {
      "arxiv_id": "2503.02582v1",
      "title": "Playing games with Large language models: Randomness and strategy",
      "title_zh": "与大型语言模型玩游戏：随机性与策略",
      "authors": [
        "Alicia Vidler",
        "Toby Walsh"
      ],
      "abstract": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)在游戏中的表现，重点研究其随机性和策略适应能力。通过\"石头剪刀布\"和\"囚徒困境\"两个游戏的实验发现：(1)LLM在被要求随机输出时表现出明显偏差；(2)在重复游戏中会形成损失厌恶策略，石头剪刀布趋向僵局，而囚徒困境则根据提示设计在合作与竞争间系统性转换。研究表明LLMs能参与游戏互动但表现欠佳，这对多智能体系统中LLM的应用提出了挑战，揭示了当前模型在战略决策输出方面的局限性。",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02582v1",
      "published_date": "2025-03-04 13:04:48 UTC",
      "updated_date": "2025-03-04 13:04:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:44:59.257124"
    },
    {
      "arxiv_id": "2503.02574v1",
      "title": "LLM-Safety Evaluations Lack Robustness",
      "title_zh": "LLM 安全性评估缺乏稳健性",
      "authors": [
        "Tim Beyer",
        "Sophie Xhonneux",
        "Simon Geisler",
        "Gauthier Gidel",
        "Leo Schwinn",
        "Stephan Günnemann"
      ],
      "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
      "tldr_zh": "这篇论文指出当前大语言模型(LLM)安全评估存在严重缺陷，包括数据集规模小、方法不一致和评估设置不可靠等问题，导致攻击与防御措施难以公平比较。作者系统分析了LLM安全评估流程的各个环节(数据集构建、自动化红队优化、响应生成和基于LLM的响应评估)，揭示了各阶段的关键问题。研究提出了减少评估噪声和偏差的指导原则，同时辩证分析了现有局限性的现实原因。该工作为提升LLM安全研究的可比性和可测量性进步提供了重要参考。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02574v1",
      "published_date": "2025-03-04 12:55:07 UTC",
      "updated_date": "2025-03-04 12:55:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:45:17.320602"
    },
    {
      "arxiv_id": "2503.02572v1",
      "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
      "title_zh": "RaceVLA：基于VLA的竞速无人机导航系统，模拟人类行为",
      "authors": [
        "Valerii Serpiva",
        "Artem Lykov",
        "Artyom Myshlyaev",
        "Muhammad Haris Khan",
        "Ali Alridha Abdulkarim",
        "Oleg Sautenkov",
        "Dzmitry Tsetserukou"
      ],
      "abstract": "RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/",
      "tldr_zh": "RaceVLA提出了一种基于视觉-语言-动作（VLA）的自主竞速无人机导航方法，通过模仿人类飞行员的行为实现高效导航。该模型在竞速无人机数据集上微调，能够在复杂环境中实时调整策略，展现出强大的泛化能力。实验表明，RaceVLA在运动（75.0 vs 60.0）和语义泛化（45.5 vs 36.3）上优于OpenVLA，并在所有维度上超越RT-2，证明了其在高速场景下的鲁棒性和机动性。RaceVLA的平均速度为1.04 m/s，最高速度达2.02 m/s，展示了其在竞速场景中的高性能导航潜力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 6 figures. Submitted to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02572v1",
      "published_date": "2025-03-04 12:54:05 UTC",
      "updated_date": "2025-03-04 12:54:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:45:18.997776"
    },
    {
      "arxiv_id": "2503.05812v1",
      "title": "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
      "title_zh": "人工智能不可容忍风险阈值建议",
      "authors": [
        "Deepika Raman",
        "Nada Madkour",
        "Evan R. Murphy",
        "Krystal Jackson",
        "Jessica Newman"
      ],
      "abstract": "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
      "tldr_zh": "该研究为前沿AI模型（Frontier AI Models）提出了不可接受风险阈值（Intolerable Risk Thresholds）的建议，以应对其对公共安全、人权、经济稳定和社会价值的潜在威胁。研究基于2024年AI首尔峰会的承诺，提出了八个风险类别（如CBRN武器、网络攻击、模型自主性等）的具体阈值建议，并通过案例研究加以说明。研究强调在数据有限和技术快速发展的背景下，应追求“良好而非完美”的阈值，并鼓励政策制定者和行业领袖采取主动风险管理策略，优先预防不可接受风险的发生，而非事后补救。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CR",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "79 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05812v1",
      "published_date": "2025-03-04 12:30:37 UTC",
      "updated_date": "2025-03-04 12:30:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:45:59.182695"
    },
    {
      "arxiv_id": "2503.02552v1",
      "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
      "title_zh": "基于模型的强化学习推理中异常检测的世界模型",
      "authors": [
        "Fabian Domberg",
        "Georg Schildbach"
      ],
      "abstract": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.",
      "tldr_zh": "该研究探讨了如何利用基于模型的强化学习（Model-Based Reinforcement Learning）中的世界模型（world models）在推理阶段进行异常检测，以确保学习型控制器仅在熟悉的状态空间区域内运行。通过在推理过程中持续监测世界模型预测与系统实际行为之间的差异，一旦误差超过阈值，即可触发应急措施（如紧急停止）。该方法无需特定任务知识，具有普适性。仿真和真实世界实验表明，该方法能有效检测局部机器人几何变化、全局重力变化以及作用于四旋翼飞行器的意外力，从而在不利条件下确保学习型控制器的安全可靠运行。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02552v1",
      "published_date": "2025-03-04 12:25:01 UTC",
      "updated_date": "2025-03-04 12:25:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:08.154824"
    },
    {
      "arxiv_id": "2503.02549v1",
      "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
      "title_zh": "联邦化nnU-Net：面向隐私保护的医学图像分割",
      "authors": [
        "Grzegorz Skorupko",
        "Fotios Avgoustidis",
        "Carlos Martín-Isla",
        "Lidia Garrucho",
        "Dimitri A. Kessler",
        "Esmeralda Ruiz Pujadas",
        "Oliver Díaz",
        "Maciej Bobowicz",
        "Katarzyna Gwoździewicz",
        "Xavier Bargalló",
        "Paulius Jaruševičius",
        "Kaisar Kushibar",
        "Karim Lekadir"
      ],
      "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
      "tldr_zh": "该研究提出了FednnU-Net，一种基于联邦学习的nnU-Net扩展框架，用于隐私保护的医学图像分割。通过引入两种新颖的联邦学习方法——联邦指纹提取(Federated Fingerprint Extraction, FFE)和非对称联邦平均(Asymmetric Federated Averaging, AsymFedAvg)，该框架在分散式训练中实现了与集中式方法相当的性能。实验表明，FednnU-Net在乳腺癌、心脏和胎儿分割任务中表现优异，使用了来自18个机构的6个数据集。研究团队还公开了该框架的源代码，以促进隐私受限机构中的分散式训练研究和部署。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "In review",
      "pdf_url": "http://arxiv.org/pdf/2503.02549v1",
      "published_date": "2025-03-04 12:20:06 UTC",
      "updated_date": "2025-03-04 12:20:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:39.744749"
    },
    {
      "arxiv_id": "2503.02917v1",
      "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting of Vision-Language Models",
      "title_zh": "基于概念引导提示的视觉语言模型实现可解释性少样本视网膜疾病诊断",
      "authors": [
        "Deval Mehta",
        "Yiwen Jiang",
        "Catherine L Jan",
        "Mingguang He",
        "Kshitij Jadhav",
        "Zongyuan Ge"
      ],
      "abstract": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
      "tldr_zh": "本研究提出了一种基于概念引导提示的可解释视网膜疾病诊断方法，利用GPT模型提取可解释的疾病概念，并将这些概念作为语言组件融入视觉语言模型(VL)的提示学习中。该方法不仅提升了基于眼底图像的疾病分类性能，还增强了少样本和零样本（新疾病）检测能力，同时提供了基于概念的可解释性。在两个多样化眼底图像数据集上的实验表明，该方法在16样本学习和零样本检测中分别实现了约5.8%和2.7%的平均精度提升，为临床实践提供了更可信赖的AI辅助诊断工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to Information Processing in Medical Imaging (IPMI) 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
      "published_date": "2025-03-04 12:03:42 UTC",
      "updated_date": "2025-03-04 12:03:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:07.805097"
    },
    {
      "arxiv_id": "2503.02537v2",
      "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
      "title_zh": "RectifiedHR：通过能量校正实现高效高分辨率图像生成",
      "authors": [
        "Zhen Yang",
        "Guibao Shen",
        "Liang Hou",
        "Mushui Liu",
        "Luozhou Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Di Zhang",
        "Ying-Cong Chen"
      ],
      "abstract": "Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an straightforward and efficient solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we introduce average latent energy analysis and discover\nthat an improved classifier-free guidance hyperparameter can significantly\nenhance generation performance. Our method is entirely training-free and boasts\na simple implementation logic and efficient performance. Through extensive\ncomparisons with numerous baseline methods, our RectifiedHR demonstrates\nsuperior effectiveness and efficiency.",
      "tldr_zh": "本文提出RectifiedHR方法，通过能量校正实现高效的高分辨率图像生成。该方法采用噪声刷新策略（noise refresh），仅需少量代码即可提升扩散模型在超出训练分辨率时的生成能力，同时首次发现高分辨率生成过程中的能量衰减（energy decay）现象会导致图像模糊。研究者通过平均潜在能量分析提出改进方案，证明调整classifier-free guidance超参数可显著提升生成质量。该训练无关的方法在多项对比实验中展现出优越的生成效果和效率优势。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/",
      "pdf_url": "http://arxiv.org/pdf/2503.02537v2",
      "published_date": "2025-03-04 12:03:26 UTC",
      "updated_date": "2025-03-14 13:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:19.001901"
    },
    {
      "arxiv_id": "2503.02512v1",
      "title": "LTL Verification of Memoryful Neural Agents",
      "title_zh": "记忆型神经智能体的LTL验证",
      "authors": [
        "Mehran Hosseini",
        "Alessio Lomuscio",
        "Nicola Paoletti"
      ],
      "abstract": "We present a framework for verifying Memoryful Neural Multi-Agent Systems\n(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,\nagents interact with a non-deterministic, partially observable environment.\nExamples of MN-MAS include multi-agent systems based on feed-forward and\nrecurrent neural networks or state-space models. Different from previous\napproaches, we support the verification of both bounded and unbounded LTL\nspecifications. We leverage well-established bounded model checking techniques,\nincluding lasso search and invariant synthesis, to reduce the verification\nproblem to that of constraint solving. To solve these constraints, we develop\nefficient methods based on bound propagation, mixed-integer linear programming,\nand adaptive splitting. We evaluate the effectiveness of our algorithms in\nsingle and multi-agent environments from the Gymnasium and PettingZoo\nlibraries, verifying unbounded specifications for the first time and improving\nthe verification time for bounded specifications by an order of magnitude\ncompared to the SoA.",
      "tldr_zh": "本文提出了一个验证框架，用于检验具有记忆能力的神经多智能体系统（MN-MAS）是否满足线性时序逻辑（LTL）规范。该框架支持对有限和无限时间范围内的LTL规范进行验证，结合了边界模型检测技术（如环路搜索和不变式合成），将验证问题转化为约束求解问题。研究者开发了基于边界传播、混合整数线性规划和自适应分割的高效求解方法，在Gymnasium和PettingZoo环境中的实验表明，该方法首次实现了对无限时间规范的验证，并将有限时间规范的验证速度提升了一个数量级。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "cs.MA",
        "cs.SC",
        "68Q60 (Primary) 68T27, 68T07, 68T37, 68T40, 68T42 (Secondary)",
        "D.2.4; F.3.1; I.2.4; I.2.11; I.2.8; F.4.1; I.2.2; I.2.3"
      ],
      "primary_category": "cs.LO",
      "comment": "11 pages, 2 figures, accepted at AAMAS 2025 conference",
      "pdf_url": "http://arxiv.org/pdf/2503.02512v1",
      "published_date": "2025-03-04 11:20:19 UTC",
      "updated_date": "2025-03-04 11:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:31.699961"
    },
    {
      "arxiv_id": "2503.02505v1",
      "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
      "title_zh": "ROCKET-2：通过跨视图目标对齐引导视觉运动策略",
      "authors": [
        "Shaofei Cai",
        "Zhancun Mu",
        "Anji Liu",
        "Yitao Liang"
      ],
      "abstract": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.",
      "tldr_zh": "该研究提出了ROCKET-2，一种通过跨视图目标对齐引导视觉运动策略的新框架，旨在解决人类用户与智能体在具身环境中交互时的目标指定问题。研究创新性地允许用户使用自身视角的分割掩码指定目标，而非依赖智能体的观察视角。为解决人类与智能体视角差异导致的意图对齐问题，该框架引入了跨视图一致性损失和目标可见性损失两个辅助目标，显著提升了智能体的空间推理能力。实验表明，ROCKET-2在Minecraft环境中实现了3到6倍的推理效率提升，首次实现了直接从人类视角解读目标的能力，为人机交互开辟了新途径。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02505v1",
      "published_date": "2025-03-04 11:16:46 UTC",
      "updated_date": "2025-03-04 11:16:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:34.134569"
    },
    {
      "arxiv_id": "2503.02497v1",
      "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset",
      "title_zh": "PennyLang：基于大型语言模型的量子代码生成创新——以PennyLane为核心的新型数据集",
      "authors": [
        "Haider Asif",
        "Abdul Basit",
        "Nouhaila Innan",
        "Muhammad Kashif",
        "Alberto Marchisio",
        "Muhammad Shafique"
      ],
      "abstract": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.",
      "tldr_zh": "该研究提出了PennyLang，首个专注于PennyLane量子计算框架的LLM代码生成方案。主要贡献包括：(1) 构建并开源包含3,347个PennyLane量子电路代码样本的高质量数据集，填补了当前AI辅助量子编程领域对PennyLane支持的空白；(2) 开发了一套系统化的数据精炼与标注方法，优化LLM的训练效率；(3) 基于检索增强生成(RAG)框架验证了该数据集在提升PennyLane代码生成效果方面的优势。相比现有主要针对Qiskit的研究，该工作显著扩展了AI驱动的量子编程辅助框架覆盖范围。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "quant-ph",
        "68T50 (Primary)",
        "I.2.7"
      ],
      "primary_category": "cs.SE",
      "comment": "10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02497v1",
      "published_date": "2025-03-04 11:04:35 UTC",
      "updated_date": "2025-03-04 11:04:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:55.805391"
    },
    {
      "arxiv_id": "2503.02495v2",
      "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
      "title_zh": "专家联盟：将分层路由应用于等价分解的Transformer",
      "authors": [
        "Yujiao Yang",
        "Jing Lian",
        "Linhui Li"
      ],
      "abstract": "We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.",
      "tldr_zh": "本研究提出了Union-of-Experts (UoE)方法，通过将Transformer分解为等价专家组，并在输入数据和专家间实现选择性路由，改进了MoE（混合专家）设计。其核心创新包括：基于张量并行矩阵划分的MLP块和注意力块等价分解、跨层次的路由范式（如patch-wise数据选择和专家选择）、以及包含选择性多头注意力(SMHA)和MLP专家联合(UoME)的架构设计。实验表明，UoE在语言建模、长程任务和图像分类任务中均优于现有方法，例如在语言建模任务中，其困惑度平均降低2.38，同时仅需76%的FLOPs。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "68T07",
        "I.5.1; I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02495v2",
      "published_date": "2025-03-04 11:01:25 UTC",
      "updated_date": "2025-03-06 08:51:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:43.854249"
    },
    {
      "arxiv_id": "2503.02484v1",
      "title": "ERetinex: Event Camera Meets Retinex Theory for Low-Light Image Enhancement",
      "title_zh": "ERetinex：事件相机与Retinex理论融合的低光照图像增强方法",
      "authors": [
        "Xuejian Guo",
        "Zhiqiang Tian",
        "Yuehang Wang",
        "Siqi Li",
        "Yu Jiang",
        "Shaoyi Du",
        "Yue Gao"
      ],
      "abstract": "Low-light image enhancement aims to restore the under-exposure image captured\nin dark scenarios. Under such scenarios, traditional frame-based cameras may\nfail to capture the structure and color information due to the exposure time\nlimitation. Event cameras are bio-inspired vision sensors that respond to\npixel-wise brightness changes asynchronously. Event cameras' high dynamic range\nis pivotal for visual perception in extreme low-light scenarios, surpassing\ntraditional cameras and enabling applications in challenging dark environments.\nIn this paper, inspired by the success of the retinex theory for traditional\nframe-based low-light image restoration, we introduce the first methods that\ncombine the retinex theory with event cameras and propose a novel retinex-based\nlow-light image restoration framework named ERetinex. Among our contributions,\nthe first is developing a new approach that leverages the high temporal\nresolution data from event cameras with traditional image information to\nestimate scene illumination accurately. This method outperforms traditional\nimage-only techniques, especially in low-light environments, by providing more\nprecise lighting information. Additionally, we propose an effective fusion\nstrategy that combines the high dynamic range data from event cameras with the\ncolor information of traditional images to enhance image quality. Through this\nfusion, we can generate clearer and more detail-rich images, maintaining the\nintegrity of visual information even under extreme lighting conditions. The\nexperimental results indicate that our proposed method outperforms\nstate-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while\nreducing FLOPS by \\textbf{84.28}\\%.",
      "tldr_zh": "本文提出ERetinex框架，首次将Retinex理论与事件相机(Event Camera)结合，用于低光图像增强。该框架利用事件相机的高动态范围和高时间分辨率数据，与传统图像信息融合，精确估计场景光照，显著提升低光环境下的图像质量。实验表明，ERetinex在PSNR指标上优于现有方法，同时计算复杂度大幅降低84.28%，为极端光照条件下的图像增强提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02484v1",
      "published_date": "2025-03-04 10:48:44 UTC",
      "updated_date": "2025-03-04 10:48:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:46:59.289385"
    },
    {
      "arxiv_id": "2503.02476v1",
      "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for Biomedical VQA",
      "title_zh": "BioD2C：面向生物医学视觉问答的双层次语义一致性约束框架",
      "authors": [
        "Zhengyang Ji",
        "Shang Gao",
        "Li Liu",
        "Yifan Jia",
        "Yutao Yue"
      ],
      "abstract": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
      "tldr_zh": "本文提出BioD2C框架，通过双层次语义一致性约束解决生物医学VQA任务中的多模态对齐问题。该框架在特征层面通过图像-文本融合机制实现基于问题的视觉特征自适应学习，在模型层面引入基于文本队列的跨模态软语义损失函数，有效提升了图像与问题语义的对齐能力。研究者还构建了去偏的新数据集BioVGQ，实验表明该框架在多个下游数据集上达到SOTA性能，展现出优异的鲁棒性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
      "published_date": "2025-03-04 10:39:42 UTC",
      "updated_date": "2025-03-04 10:39:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:15.585556"
    },
    {
      "arxiv_id": "2503.04812v1",
      "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
      "title_zh": "LLaVE：基于难度加权对比学习的大规模语言与视觉嵌入模型",
      "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Jinsong Su"
      ],
      "abstract": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
      "tldr_zh": "该研究提出了LLaVE框架，通过基于难度加权的对比学习（Hardness-Weighted Contrastive Learning）提升多模态嵌入模型的表示能力，解决了现有模型在区分困难负样本对时的局限性。LLaVE在MMEB基准测试中显著优于现有模型，其中LLaVE-2B超越之前的7B模型，LLaVE-7B进一步提升了6.2个百分点的性能。此外，LLaVE在零样本条件下能够泛化到文本-视频检索任务，展现了其强大的迁移潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.04812v1",
      "published_date": "2025-03-04 10:21:57 UTC",
      "updated_date": "2025-03-04 10:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:20.303786"
    },
    {
      "arxiv_id": "2503.05810v2",
      "title": "A Transformer Model for Predicting Chemical Reaction Products from Generic Templates",
      "title_zh": "基于通用模板的化学反产物预测的Transformer模型",
      "authors": [
        "Derin Ozer",
        "Sylvain Lamprier",
        "Thomas Cauchy",
        "Nicolas Gutowski",
        "Benoit Da Mota"
      ],
      "abstract": "The accurate prediction of chemical reaction outcomes is a major challenge in\ncomputational chemistry. Current models rely heavily on either highly specific\nreaction templates or template-free methods, both of which present limitations.\nTo address these limitations, this work proposes the Broad Reaction Set (BRS),\na dataset featuring 20 generic reaction templates that allow for the efficient\nexploration of the chemical space. Additionally, ProPreT5 is introduced, a T5\nmodel tailored to chemistry that achieves a balance between rigid templates and\ntemplate-free methods. ProPreT5 demonstrates its capability to generate\naccurate, valid, and realistic reaction products, making it a promising\nsolution that goes beyond the current state-of-the-art on the complex reaction\nproduct prediction task.",
      "tldr_zh": "本研究提出了一种基于Transformer的模型ProPreT5，用于从通用模板预测化学反应产物，解决了现有方法在模板依赖性和灵活性之间的平衡问题。研究引入了Broad Reaction Set (BRS)数据集，包含20种通用反应模板，以更高效地探索化学空间。ProPreT5模型在复杂反应产物预测任务中表现出色，能够生成准确、有效且真实的反应产物，超越了当前的最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05810v2",
      "published_date": "2025-03-04 10:18:32 UTC",
      "updated_date": "2025-03-11 08:22:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:29.531500"
    },
    {
      "arxiv_id": "2503.03774v2",
      "title": "Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems",
      "title_zh": "公平竞技的高速之道：将体育精神融入自主赛车系统",
      "authors": [
        "Zhenmin Huang",
        "Ce Hao",
        "Wei Zhan",
        "Jun Ma",
        "Masayoshi Tomizuka"
      ],
      "abstract": "Autonomous racing has gained significant attention as a platform for\nhigh-speed decision-making and motion control. While existing methods primarily\nfocus on trajectory planning and overtaking strategies, the role of\nsportsmanship in ensuring fair competition remains largely unexplored. In human\nracing, rules such as the one-motion rule and the enough-space rule prevent\ndangerous and unsportsmanlike behavior. However, autonomous racing systems\noften lack mechanisms to enforce these principles, potentially leading to\nunsafe maneuvers. This paper introduces a bi-level game-theoretic framework to\nintegrate sportsmanship (SPS) into versus racing. At the high level, we model\nracing intentions using a Stackelberg game, where Monte Carlo Tree Search\n(MCTS) is employed to derive optimal strategies. At the low level, vehicle\ninteractions are formulated as a Generalized Nash Equilibrium Problem (GNEP),\nensuring that all agents follow sportsmanship constraints while optimizing\ntheir trajectories. Simulation results demonstrate the effectiveness of the\nproposed approach in enforcing sportsmanship rules while maintaining\ncompetitive performance. We analyze different scenarios where attackers and\ndefenders adhere to or disregard sportsmanship rules and show how knowledge of\nthese constraints influences strategic decision-making. This work highlights\nthe importance of balancing competition and fairness in autonomous racing and\nprovides a foundation for developing ethical and safe AI-driven racing systems.",
      "tldr_zh": "本文提出了一种双层博弈论框架，将体育精神(SPS)融入自主赛车系统中。高层通过Stackelberg博弈和蒙特卡洛树搜索(MCTS)建模赛车意图，低层将车辆交互建模为广义纳什均衡问题(GNEP)，确保所有智能体在优化轨迹的同时遵守体育精神约束。仿真结果表明，该方法在保持竞技性的同时有效执行了体育精神规则，为开发公平、安全的AI驱动赛车系统奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.03774v2",
      "published_date": "2025-03-04 10:14:19 UTC",
      "updated_date": "2025-03-12 17:02:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:45.584839"
    },
    {
      "arxiv_id": "2503.02457v1",
      "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
      "title_zh": "别太激动——在大型语言模型中引发情感",
      "authors": [
        "Gino Franco Fazzi",
        "Julie Skoven Hinge",
        "Stefan Heinrich",
        "Paolo Burelli"
      ],
      "abstract": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
      "tldr_zh": "本研究探讨了大语言模型(LLMs)在情感控制方面的挑战，重点关注其在长时间对话中表达适当情感状态的能力。通过结合LLM情感分析和多轮对话模拟，研究者量化了LLMs表达广泛情感谱系的能力及其在交互中的波动情况。研究揭示了LLMs在保持一致性情感方面的显著差异，并指出了一些关键挑战，如难以产生和维持极端情感状态，以及适应对话情境变化的能力有限。这些发现对开发更具情感智能的AI系统具有重要意义，并强调了改进LLMs情感建模的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02457v1",
      "published_date": "2025-03-04 10:06:41 UTC",
      "updated_date": "2025-03-04 10:06:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:49.057432"
    },
    {
      "arxiv_id": "2503.02453v1",
      "title": "Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations",
      "title_zh": "稀疏与密集相遇：基于级联稀疏-稠密表征的统一生成式推荐方法",
      "authors": [
        "Yuhao Yang",
        "Zhi Ji",
        "Zhaopeng Li",
        "Yi Li",
        "Zhonglin Mo",
        "Yue Ding",
        "Kai Chen",
        "Zijian Zhang",
        "Jie Li",
        "Shuanglong Li",
        "Lin Liu"
      ],
      "abstract": "Generative models have recently gained attention in recommendation systems by\ndirectly predicting item identifiers from user interaction sequences. However,\nexisting methods suffer from significant information loss due to the separation\nof stages such as quantization and sequence modeling, hindering their ability\nto achieve the modeling precision and accuracy of sequential dense retrieval\ntechniques. Integrating generative and dense retrieval methods remains a\ncritical challenge. To address this, we introduce the Cascaded Organized\nBi-Represented generAtive retrieval (COBRA) framework, which innovatively\nintegrates sparse semantic IDs and dense vectors through a cascading process.\nOur method alternates between generating these representations by first\ngenerating sparse IDs, which serve as conditions to aid in the generation of\ndense vectors. End-to-end training enables dynamic refinement of dense\nrepresentations, capturing both semantic insights and collaborative signals\nfrom user-item interactions. During inference, COBRA employs a coarse-to-fine\nstrategy, starting with sparse ID generation and refining them into dense\nvectors via the generative model. We further propose BeamFusion, an innovative\napproach combining beam search with nearest neighbor scores to enhance\ninference flexibility and recommendation diversity. Extensive experiments on\npublic datasets and offline tests validate our method's robustness. Online A/B\ntests on a real-world advertising platform with over 200 million daily users\ndemonstrate substantial improvements in key metrics, highlighting COBRA's\npractical advantages.",
      "tldr_zh": "该研究提出COBRA框架，创新性地通过级联稀疏语义ID和稠密向量的方式统一生成式推荐与稠密检索方法。该方法首先生成稀疏ID作为条件，再生成优化后的稠密向量，通过端到端训练动态捕捉用户-物品交互中的语义信息和协同信号。实验表明，该框架在公开数据集和拥有2亿日活的广告平台A/B测试中均取得显著效果提升，其提出的BeamFusion策略结合波束搜索与最近邻评分，有效增强了推荐多样性和推理灵活性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02453v1",
      "published_date": "2025-03-04 10:00:05 UTC",
      "updated_date": "2025-03-04 10:00:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:47:58.274867"
    },
    {
      "arxiv_id": "2503.02420v1",
      "title": "Exploring Model Quantization in GenAI-based Image Inpainting and Detection of Arable Plants",
      "title_zh": "探索基于生成式AI的图像修复与耕地植物检测中的模型量化技术",
      "authors": [
        "Sourav Modak",
        "Ahmet Oğuz Saltık",
        "Anthony Stein"
      ],
      "abstract": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
      "tldr_zh": "本研究提出了一种基于生成式AI（Stable Diffusion）的图像修复框架，用于增强可耕作植物检测系统的训练数据，并通过量化策略（FP16和INT8）优化模型的计算效率。实验在YOLO11(l)和RT-DETR(l)两个目标检测模型上进行，结果表明，量化后的模型在Jetson Orin Nano等资源受限设备上实现了检测精度与推理速度的平衡，为智能杂草管理系统提供了实用的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
      "published_date": "2025-03-04 09:05:01 UTC",
      "updated_date": "2025-03-04 09:05:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:48:00.274172"
    },
    {
      "arxiv_id": "2503.11684v1",
      "title": "Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching",
      "title_zh": "探索人机交互中的因果关系：机器人心理健康辅导案例研究",
      "authors": [
        "Micol Spitale",
        "Srikar Babu",
        "Serhan Cakmak",
        "Jiaee Cheong",
        "Hatice Gunes"
      ],
      "abstract": "One of the primary goals of Human-Robot Interaction (HRI) research is to\ndevelop robots that can interpret human behavior and adapt their responses\naccordingly. Adaptive learning models, such as continual and reinforcement\nlearning, play a crucial role in improving robots' ability to interact\neffectively in real-world settings. However, these models face significant\nchallenges due to the limited availability of real-world data, particularly in\nsensitive domains like healthcare and well-being. This data scarcity can hinder\na robot's ability to adapt to new situations. To address these challenges,\ncausality provides a structured framework for understanding and modeling the\nunderlying relationships between actions, events, and outcomes. By moving\nbeyond mere pattern recognition, causality enables robots to make more\nexplainable and generalizable decisions. This paper presents an exploratory\ncausality-based analysis through a case study of an adaptive robotic coach\ndelivering positive psychology exercises over four weeks in a workplace\nsetting. The robotic coach autonomously adapts to multimodal human behaviors,\nsuch as facial valence and speech duration. By conducting both macro- and\nmicro-level causal analyses, this study aims to gain deeper insights into how\nadaptability can enhance well-being during interactions. Ultimately, this\nresearch seeks to advance our understanding of how causality can help overcome\nchallenges in HRI, particularly in real-world applications.",
      "tldr_zh": "本研究通过一个为期四周的案例研究，探索了因果性分析在人机交互（HRI）中的应用，特别是在机器人心理健康辅导领域。研究开发了一种自适应机器人教练，能够根据人类的多模态行为（如面部表情和语音时长）自主调整其交互策略。通过宏观和微观层面的因果性分析，研究揭示了适应性如何增强交互过程中的心理健康效果。结果表明，因果性框架能够帮助机器人超越模式识别，做出更具解释性和泛化能力的决策，从而推动HRI在真实场景中的应用。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.11684v1",
      "published_date": "2025-03-04 08:56:47 UTC",
      "updated_date": "2025-03-04 08:56:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:48:37.224707"
    },
    {
      "arxiv_id": "2503.02403v1",
      "title": "AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents",
      "title_zh": "AutoEval：移动智能体自主评估的实用框架",
      "authors": [
        "Jiahui Sun",
        "Zhichao Hua",
        "Yubin Xia"
      ],
      "abstract": "Accurate and systematic evaluation of mobile agents can significantly advance\ntheir development and real-world applicability. However, existing benchmarks\nfor mobile agents lack practicality and scalability due to the extensive manual\neffort required to define task reward signals and implement corresponding\nevaluation codes. To this end, we propose AutoEval, an autonomous agent\nevaluation framework that tests a mobile agent without any manual effort.\nFirst, we design a Structured Substate Representation to describe the UI state\nchanges while agent execution, such that task reward signals can be\nautomatically generated. Second, we utilize a Judge System that can\nautonomously evaluate agents' performance given the automatically generated\ntask reward signals. By providing only a task description, our framework\nevaluates agents with fine-grained performance feedback to that task without\nany extra manual effort. We implement a prototype of our framework and validate\nthe automatically generated task reward signals, finding over 93% coverage to\nhuman-annotated reward signals. Moreover, to prove the effectiveness of our\nautonomous Judge System, we manually verify its judge results and demonstrate\nthat it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile\nagents using our framework, providing detailed insights into their performance\ncharacteristics and limitations.",
      "tldr_zh": "本文提出AutoEval框架，用于自动化评估移动智能体（mobile agents），解决了现有评估方法依赖人工标注奖励信号和实现评估代码的局限性。该框架通过结构化子状态表示（Structured Substate Representation）自动生成任务奖励信号，并利用自主评判系统（Judge System）评估智能体表现，仅需任务描述即可提供细粒度性能反馈。实验表明，自动生成的奖励信号覆盖率达93%以上，评判系统准确率达94%，成功用于分析当前最优移动智能体的性能特征与局限。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02403v1",
      "published_date": "2025-03-04 08:44:30 UTC",
      "updated_date": "2025-03-04 08:44:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:48:18.633806"
    },
    {
      "arxiv_id": "2503.02399v1",
      "title": "VisAgent: Narrative-Preserving Story Visualization Framework",
      "title_zh": "VisAgent：叙事保留的故事可视化框架",
      "authors": [
        "Seungkwon Kim",
        "GyuTae Park",
        "Sangyeon Kim",
        "Seung-Hun Nam"
      ],
      "abstract": "Story visualization is the transformation of narrative elements into image\nsequences. While existing research has primarily focused on visual contextual\ncoherence, the deeper narrative essence of stories often remains overlooked.\nThis limitation hinders the practical application of these approaches, as\ngenerated images frequently fail to capture the intended meaning and nuances of\nthe narrative fully. To address these challenges, we propose VisAgent, a\ntraining-free multi-agent framework designed to comprehend and visualize\npivotal scenes within a given story. By considering story distillation,\nsemantic consistency, and contextual coherence, VisAgent employs an agentic\nworkflow. In this workflow, multiple specialized agents collaborate to: (i)\nrefine layered prompts based on the narrative structure and (ii) seamlessly\nintegrate \\gt{generated} elements, including refined prompts, scene elements,\nand subject placement, into the final image. The empirically validated\neffectiveness confirms the framework's suitability for practical story\nvisualization applications.",
      "tldr_zh": "本研究提出了VisAgent，一种无需训练的多智能体框架，旨在解决现有故事可视化方法难以保留叙事精髓的问题。VisAgent通过分层提示优化和场景元素整合，确保生成图像在语义一致性和上下文连贯性上更好地反映故事的核心场景。实验验证了该框架在实际故事可视化应用中的有效性，为叙事保留的图像生成提供了新的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICASSP 2025. Equal contribution from first two authors",
      "pdf_url": "http://arxiv.org/pdf/2503.02399v1",
      "published_date": "2025-03-04 08:41:45 UTC",
      "updated_date": "2025-03-04 08:41:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:48:30.149470"
    },
    {
      "arxiv_id": "2503.02398v1",
      "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence",
      "title_zh": "PersonaX：面向长行为序列的推荐智能体用户建模框架",
      "authors": [
        "Yunxiao Shi",
        "Wujiang Xu",
        "Zeqi Zhang",
        "Xing Zi",
        "Qiang Wu",
        "Min Xu"
      ],
      "abstract": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
      "tldr_zh": "本文提出了PersonaX，一种面向推荐代理的用户建模框架，旨在解决现有基于大语言模型（LLM）的用户建模方法在处理长用户生成内容（UGC）时的局限性。PersonaX通过离线提取子行为序列（SBS）来捕捉多样化的用户兴趣，生成细粒度的文本化用户画像，并缓存以供在线高效检索。该方法在确保上下文相关性的同时，避免了在线建模的延迟问题。实验表明，PersonaX仅需30%至50%的行为数据，即可显著提升推荐代理的性能，为可扩展的用户建模设定了新标准。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "draft paper",
      "pdf_url": "http://arxiv.org/pdf/2503.02398v1",
      "published_date": "2025-03-04 08:41:40 UTC",
      "updated_date": "2025-03-04 08:41:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:07.089989"
    },
    {
      "arxiv_id": "2503.02397v1",
      "title": "A Binary Classification Social Network Dataset for Graph Machine Learning",
      "title_zh": "用于图机器学习的二分类社交网络数据集",
      "authors": [
        "Adnan Ali",
        "Jinglong Li",
        "Huanhuan Chen",
        "AlMotasem Bellah Al Ajlouni"
      ],
      "abstract": "Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.",
      "tldr_zh": "本文提出了首个用于图机器学习的二分类社交网络数据集（BiSND），填补了现有基准数据集多分类任务（3-15类）的空白。BiSND以表格和图两种格式呈现，并通过多种机器学习方法（包括传统算法、深度神经网络、图神经网络和图对比学习方法）验证其鲁棒性。实验结果表明，BiSND在分类任务中表现良好，F1分数介于67.66至70.15之间，为未来研究提供了改进空间。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02397v1",
      "published_date": "2025-03-04 08:40:42 UTC",
      "updated_date": "2025-03-04 08:40:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:09.037235"
    },
    {
      "arxiv_id": "2503.02382v1",
      "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning",
      "title_zh": "高效精确的数学推理过程监督奖励模型训练数据构建框架",
      "authors": [
        "Wei Sun",
        "Qianlong Du",
        "Fuwei Cui",
        "Jiajun Zhang"
      ],
      "abstract": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",
      "tldr_zh": "该研究提出了EpicPRM框架，用于高效构建数学推理中过程监督奖励模型(PRM)的训练数据。该方法通过量化每个中间推理步骤的贡献进行标注，并采用自适应二分搜索算法提高标注精度和效率。基于此，研究团队构建了包含50k标注中间步骤的高质量数据集Epic50k，实验表明，使用Epic50k训练的PRM在性能上显著优于其他公开数据集。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02382v1",
      "published_date": "2025-03-04 08:18:46 UTC",
      "updated_date": "2025-03-04 08:18:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:07.554494"
    },
    {
      "arxiv_id": "2503.02913v1",
      "title": "Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms",
      "title_zh": "迈向稳健的多无人机协作：基于抗噪通信与注意力机制的强化学习框架",
      "authors": [
        "Zilin Zhao",
        "Chishui Chen",
        "Haotian Shi",
        "Jiale Chen",
        "Xuanlin Yue",
        "Zhejian Yang",
        "Yang Liu"
      ],
      "abstract": "Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction.",
      "tldr_zh": "本研究提出了一种基于多智能体强化学习(MARL)的无人机(UAV)路径规划框架，旨在解决多无人机在噪声环境中的协作通信与决策难题。该框架结合了注意力机制和反事实多智能体策略梯度(COMA)算法，显著提升了噪声条件下的通信鲁棒性和个体决策能力。实验表明，该方法在路径规划效率和鲁棒性方面优于现有算法，尤其在噪声环境中实现了78%的熵减提升。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02913v1",
      "published_date": "2025-03-04 08:05:14 UTC",
      "updated_date": "2025-03-04 08:05:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:29.307423"
    },
    {
      "arxiv_id": "2503.02369v1",
      "title": "JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for Agricultural Vehicle Routing Optimization",
      "title_zh": "JPDS-NN：基于强化学习的农业车辆路径优化动态任务分配",
      "authors": [
        "Yixuan Fan",
        "Haotian Xu",
        "Mengqiao Liu",
        "Qing Zhuo",
        "Tao Zhang"
      ],
      "abstract": "The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the\nVehicle Routing Problem (VRP) where the scale of cities influences routing\noutcomes, necessitating consideration of their entrances. This paper addresses\nEDVRP in agriculture, focusing on multi-parameter vehicle planning for\nirregularly shaped fields. To address the limitations of traditional methods,\nsuch as heuristic approaches, which often overlook field geometry and entrance\nconstraints, we propose a Joint Probability Distribution Sampling Neural\nNetwork (JPDS-NN) to effectively solve the EDVRP. The network uses an\nencoder-decoder architecture with graph transformers and attention mechanisms\nto model routing as a Markov Decision Process, and is trained via reinforcement\nlearning for efficient and rapid end-to-end planning. Experimental results\nindicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel\nconsumption by 14.0-17.6%, and computes two orders of magnitude faster than\nbaseline methods, while demonstrating 15-25% superior performance in dynamic\narrangement scenarios. Ablation studies validate the necessity of\ncross-attention and pre-training. The framework enables scalable, intelligent\nrouting for large-scale farming under dynamic constraints.",
      "tldr_zh": "该研究提出JPDS-NN（联合概率分布采样神经网络），采用强化学习方法解决农业领域入口依赖型车辆路径规划问题(EDVRP)。该方法通过结合图变换器和注意力机制的编码器-解码器架构，将路径规划建模为马尔可夫决策过程，实现端到端动态任务分配。实验显示，相比传统方法，该模型可减少48.4-65.4%行驶距离、降低14.0-17.6%油耗，计算速度快两个数量级，且在动态场景中性能提升15-25%，为大规模农田智能路径规划提供了高效解决方案。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures, submitted to IROS 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02369v1",
      "published_date": "2025-03-04 07:50:32 UTC",
      "updated_date": "2025-03-04 07:50:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:32.394238"
    },
    {
      "arxiv_id": "2503.02368v2",
      "title": "Iterative Value Function Optimization for Guided Decoding",
      "title_zh": "迭代价值函数优化用于引导解码",
      "authors": [
        "Zhenhua Liu",
        "Lijun Li",
        "Ruizhe Chen",
        "Yuxian Jiang",
        "Tong Zhu",
        "Zhaochen Su",
        "Wenliang Chen",
        "Jing Shao"
      ],
      "abstract": "While Reinforcement Learning from Human Feedback (RLHF) has become the\npredominant method for controlling language model outputs, it suffers from high\ncomputational costs and training instability. Guided decoding, especially\nvalue-guided methods, offers a cost-effective alternative by controlling\noutputs without re-training models. However, the accuracy of the value function\nis crucial for value-guided decoding, as inaccuracies can lead to suboptimal\ndecision-making and degraded performance. Existing methods struggle with\naccurately estimating the optimal value function, leading to less effective\ncontrol. We propose Iterative Value Function Optimization, a novel framework\nthat addresses these limitations through two key components: Monte Carlo Value\nEstimation, which reduces estimation variance by exploring diverse\ntrajectories, and Iterative On-Policy Optimization, which progressively\nimproves value estimation through collecting trajectories from value-guided\npolicies. Extensive experiments on text summarization, multi-turn dialogue, and\ninstruction following demonstrate the effectiveness of value-guided decoding\napproaches in aligning language models. These approaches not only achieve\nalignment but also significantly reduce computational costs by leveraging\nprincipled value function optimization for efficient and effective control.",
      "tldr_zh": "本研究提出了一种迭代价值函数优化框架，用于改进基于价值引导的解码方法。该框架包含两个核心组件：蒙特卡洛价值估计通过探索多样化轨迹减少估计方差，迭代策略优化通过从价值引导策略中收集轨迹逐步提升价值估计精度。实验表明，该方法在文本摘要、多轮对话和指令遵循任务中有效对齐语言模型，显著降低计算成本，为高效可控的语言模型输出提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02368v2",
      "published_date": "2025-03-04 07:49:10 UTC",
      "updated_date": "2025-03-05 09:12:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:35.448301"
    },
    {
      "arxiv_id": "2503.02365v2",
      "title": "EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram Reports",
      "title_zh": "EchoQA：面向超声心动图报告的大规模指令微调数据集",
      "authors": [
        "Lama Moukheiber",
        "Mira Moukheiber",
        "Dana Moukheiiber",
        "Jae-Woo Ju",
        "Hyung-Chul Lee"
      ],
      "abstract": "We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.",
      "tldr_zh": "该研究提出了EchoQA，一个基于重症监护医学信息数据库（MIMIC）的心电图报告问答数据集，包含771,244个问答对，涵盖多种心脏异常及其严重程度。研究通过对比开源、生物医学专用及闭源大语言模型（LLMs）的零样本和少样本表现，验证了微调LLMs在问答任务中的性能提升。此外，临床医生对最佳模型进行了定性评估，并进行了细粒度的公平性审计，以评估LLMs在不同健康社会决定因素中的偏差与性能权衡。该数据集旨在为支持心脏鉴别诊断的LLM AI代理建立基准，减轻临床医生的文档负担，使其更专注于患者护理。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS SafeGenAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2503.02365v2",
      "published_date": "2025-03-04 07:45:45 UTC",
      "updated_date": "2025-03-06 03:29:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:48.313357"
    },
    {
      "arxiv_id": "2503.04809v2",
      "title": "PanguIR Technical Report for NTCIR-18 AEOLLM Task",
      "title_zh": "PanguIR在NTCIR-18 AEOLLM任务中的技术报告",
      "authors": [
        "Lang Mei",
        "Chong Chen",
        "Jiaxin Mao"
      ],
      "abstract": "As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task.",
      "tldr_zh": "该技术报告针对NTCIR-18的AEOLLM（大语言模型自动评估）任务，提出了三种改进无参考评估方法：1）多模型协作，利用多个LLM模拟人类评分；2）提示自动优化，基于训练样本的反馈迭代优化任务提示；3）上下文学习优化，结合多任务反馈训练专门的上下文示例检索模型。实验表明，该方法在AEOLLM任务中表现优异，为LLM的自动评估提供了更高效、更准确的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04809v2",
      "published_date": "2025-03-04 07:40:02 UTC",
      "updated_date": "2025-03-10 06:49:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:49:45.547171"
    },
    {
      "arxiv_id": "2503.02360v1",
      "title": "BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)",
      "title_zh": "BdSLW401：基于Transformer与相对量化编码（RQE）的孟加拉手语单词识别",
      "authors": [
        "Husne Ara Rubaiyeat",
        "Njayou Youssouf",
        "Md Kamrul Hasan",
        "Hasan Mahmud"
      ],
      "abstract": "Sign language recognition (SLR) for low-resource languages like Bangla\nsuffers from signer variability, viewpoint variations, and limited annotated\ndatasets. In this paper, we present BdSLW401, a large-scale, multi-view,\nword-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video\nsamples from 18 signers in front and lateral views. To improve\ntransformer-based SLR, we introduce Relative Quantization Encoding (RQE), a\nstructured embedding approach anchoring landmarks to physiological reference\npoints and quantize motion trajectories. RQE improves attention allocation by\ndecreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,\n21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,\nfixed quantization becomes insufficient on large-scale datasets (e.g.,\nWLASL2000), indicating the need for adaptive encoding strategies. Further,\nRQE-SF, an extended variant that stabilizes shoulder landmarks, achieves\nimprovements in pose consistency at the cost of small trade-offs in lateral\nview recognition. The attention graphs prove that RQE improves model\ninterpretability by focusing on the major articulatory features (fingers,\nwrists) and the more distinctive frames instead of global pose changes.\nIntroducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced\nstructured embeddings, this work advances transformer-based SLR for\nlow-resource languages and sets a benchmark for future research in this area.",
      "tldr_zh": "本研究提出了BdSLW401——首个大规模多视角孟加拉手语(BdSL)数据集，包含401个手语词汇和来自18位使用者的102,176个视频样本。针对低资源手语识别难题，作者开发了相对量化编码(RQE)方法，通过将关键点锚定到生理参考点并量化运动轨迹，显著降低了44.3%的词错误率(WER)。实验表明RQE能有效提升transformer模型的注意力分配，使其聚焦于手指、手腕等关键发音特征。研究同时发现固定量化策略在大规模数据集上效果受限，为此提出的RQE-SF变体通过稳定肩部关键点进一步提升了姿态一致性。该工作为低资源手语识别设立了新基准，并证明了结构化嵌入对提升模型可解释性的价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02360v1",
      "published_date": "2025-03-04 07:34:06 UTC",
      "updated_date": "2025-03-04 07:34:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:50:21.036264"
    },
    {
      "arxiv_id": "2503.02358v1",
      "title": "Are Large Vision Language Models Good Game Players?",
      "title_zh": "大型视觉语言模型是优秀的游戏玩家吗？",
      "authors": [
        "Xinyu Wang",
        "Bohan Zhuang",
        "Qi Wu"
      ],
      "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
      "tldr_zh": "该研究质疑当前大型视觉语言模型（LVLMs）的游戏表现能力，提出基于游戏的评估框架\\method{}来全面测试模型认知水平。该框架通过感知、问答、规则遵循和端到端游戏四个核心任务，系统评估LVLMs在视觉感知、推理和决策等方面的能力。实验揭示了现有模型在长结构化输出处理、密集细节感知等关键短板，相关代码和数据已开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
      "published_date": "2025-03-04 07:29:03 UTC",
      "updated_date": "2025-03-04 07:29:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:50:15.265898"
    },
    {
      "arxiv_id": "2503.02354v1",
      "title": "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory",
      "title_zh": "CoServe：有限内存条件下专家协作（CoE）模型的高效推理系统",
      "authors": [
        "Jiashun Suo",
        "Xiaojian Liao",
        "Limin Xiao",
        "Li Ruan",
        "Jinquan Wang",
        "Xiao Su",
        "Zhisheng Huo"
      ],
      "abstract": "Large language models like GPT-4 are resource-intensive, but recent\nadvancements suggest that smaller, specialized experts can outperform the\nmonolithic models on specific tasks. The Collaboration-of-Experts (CoE)\napproach integrates multiple expert models, improving the accuracy of generated\nresults and offering great potential for precision-critical applications, such\nas automatic circuit board quality inspection. However, deploying CoE serving\nsystems presents challenges to memory capacity due to the large number of\nexperts required, which can lead to significant performance overhead from\nfrequent expert switching across different memory and storage tiers.\n  We propose CoServe, an efficient CoE model serving system on heterogeneous\nCPU and GPU with limited memory. CoServe reduces unnecessary expert switching\nby leveraging expert dependency, a key property of CoE inference. CoServe\nintroduces a dependency-aware request scheduler and dependency-aware expert\nmanagement for efficient inference. It also introduces an offline profiler to\nautomatically find optimal resource allocation on various processors and\ndevices. In real-world intelligent manufacturing workloads, CoServe achieves\n4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art\nsystems.",
      "tldr_zh": "本文提出了CoServe，一种在有限内存环境下高效部署协作专家模型（CoE）的系统。CoServe通过利用专家依赖关系，设计了依赖感知的请求调度器和专家管理机制，减少了频繁的专家切换开销。此外，系统引入离线分析器，自动优化多处理器设备的资源分配。在实际智能制造任务中，CoServe的吞吐量比现有系统提高了4.5到12倍，为资源受限场景下的CoE模型推理提供了高效解决方案。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to ASPLOS '25",
      "pdf_url": "http://arxiv.org/pdf/2503.02354v1",
      "published_date": "2025-03-04 07:25:05 UTC",
      "updated_date": "2025-03-04 07:25:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:50:33.908722"
    },
    {
      "arxiv_id": "2503.02351v1",
      "title": "MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI",
      "title_zh": "MindSimulator：通过合成fMRI探索大脑概念定位",
      "authors": [
        "Guangyin Bao",
        "Qi Zhang",
        "Zixuan Gong",
        "Zhuojia Wu",
        "Duoqian Miao"
      ],
      "abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
      "tldr_zh": "该研究提出MindSimulator框架，通过合成fMRI数据探索大脑概念定位问题。该方法利用生成式技术建立视觉刺激与脑神经活动间的概率分布模型，生成反映真实神经响应模式的模拟脑活动数据，克服传统实验方法在刺激覆盖范围和主观偏差上的局限。实验验证表明，合成数据能准确定位已知概念选择区域，预测精度良好，为探索新概念区域和神经科学研究提供了数据驱动的先验假设工具。",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "23 pages, ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02351v1",
      "published_date": "2025-03-04 07:20:42 UTC",
      "updated_date": "2025-03-04 07:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:50:46.985614"
    },
    {
      "arxiv_id": "2503.02911v1",
      "title": "Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving Test",
      "title_zh": "Text2Scenario：面向自动驾驶测试的文本驱动场景生成",
      "authors": [
        "Xuan Cai",
        "Xuesong Bai",
        "Zhiyong Cui",
        "Danmu Xie",
        "Daocheng Fu",
        "Haiyang Yu",
        "Yilong Ren"
      ],
      "abstract": "Autonomous driving (AD) testing constitutes a critical methodology for\nassessing performance benchmarks prior to product deployment. The creation of\nsegmented scenarios within a simulated environment is acknowledged as a robust\nand effective strategy; however, the process of tailoring these scenarios often\nnecessitates laborious and time-consuming manual efforts, thereby hindering the\ndevelopment and implementation of AD technologies. In response to this\nchallenge, we introduce Text2Scenario, a framework that leverages a Large\nLanguage Model (LLM) to autonomously generate simulation test scenarios that\nclosely align with user specifications, derived from their natural language\ninputs. Specifically, an LLM, equipped with a meticulously engineered input\nprompt scheme functions as a text parser for test scenario descriptions,\nextracting from a hierarchically organized scenario repository the components\nthat most accurately reflect the user's preferences. Subsequently, by\nexploiting the precedence of scenario components, the process involves\nsequentially matching and linking scenario representations within a Domain\nSpecific Language corpus, ultimately fabricating executable test scenarios. The\nexperimental results demonstrate that such prompt engineering can meticulously\nextract the nuanced details of scenario elements embedded within various\ndescriptive formats, with the majority of generated scenarios aligning closely\nwith the user's initial expectations, allowing for the efficient and precise\nevaluation of diverse AD stacks void of the labor-intensive need for manual\nscenario configuration. Project page:\nhttps://caixxuan.github.io/Text2Scenario.GitHub.io.",
      "tldr_zh": "该研究提出了Text2Scenario框架，利用大语言模型(LLM)从自然语言输入中自动生成符合用户需求的自动驾驶测试场景。通过精心设计的提示工程，LLM从层次化场景库中提取组件，并基于场景组件的优先级，在领域特定语言(DSL)语料库中顺序匹配和链接场景表示，最终生成可执行的测试场景。实验表明，该方法能够高效提取场景元素的细节，生成与用户预期高度一致的测试场景，显著减少了手动配置场景的工作量，为自动驾驶技术的多样化测试提供了高效支持。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02911v1",
      "published_date": "2025-03-04 07:20:25 UTC",
      "updated_date": "2025-03-04 07:20:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:00.220925"
    },
    {
      "arxiv_id": "2503.02345v1",
      "title": "CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented 3D MRI",
      "title_zh": "CQ CNN：一种用于阿尔茨海默病检测的经典量子混合卷积神经网络，采用扩散生成与U-Net分割的3D MRI",
      "authors": [
        "Mominul Islam",
        "Mohammad Junayed Hasan",
        "M. R. C. Mahdy"
      ],
      "abstract": "The detection of Alzheimer disease (AD) from clinical MRI data is an active\narea of research in medical imaging. Recent advances in quantum computing,\nparticularly the integration of parameterized quantum circuits (PQCs) with\nclassical machine learning architectures, offer new opportunities to develop\nmodels that may outperform traditional methods. However, quantum machine\nlearning (QML) remains in its early stages and requires further experimental\nanalysis to better understand its behavior and limitations. In this paper, we\npropose an end to end hybrid classical quantum convolutional neural network (CQ\nCNN) for AD detection using clinically formatted 3D MRI data. Our approach\ninvolves developing a framework to make 3D MRI data usable for machine\nlearning, designing and training a brain tissue segmentation model (Skull Net),\nand training a diffusion model to generate synthetic images for the minority\nclass. Our converged models exhibit potential quantum advantages, achieving\nhigher accuracy in fewer epochs than classical models. The proposed beta8 3\nqubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)\nmodels while requiring significantly fewer computational resources. In\nparticular, the architecture employs only 13K parameters (0.48 MB), reducing\nthe parameter count by more than 99.99% compared to current SOTA models.\nFurthermore, the diffusion-generated data used to train our quantum models, in\nconjunction with real samples, preserve clinical structural standards,\nrepresenting a notable first in the field of QML. We conclude that CQCNN\narchitecture like models, with further improvements in gradient optimization\ntechniques, could become a viable option and even a potential alternative to\nclassical models for AD detection, especially in data limited and resource\nconstrained clinical settings.",
      "tldr_zh": "本研究提出了一种混合经典量子卷积神经网络(CQ CNN)，用于基于3D MRI数据的阿尔茨海默病(AD)检测。该方法结合了参数化量子电路(PQCs)和经典机器学习架构，通过开发3D MRI数据处理框架、训练脑组织分割模型(Skull Net)以及生成合成图像的扩散模型，显著提升了检测性能。实验表明，该模型在仅使用13K参数的情况下，准确率达到97.50%，超越了当前最先进的模型，同时大幅减少了计算资源需求。此外，结合真实样本训练的扩散生成数据保留了临床结构标准，为量子机器学习(QML)领域提供了重要突破。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "quant-ph",
      "comment": "Application of hybrid quantum-classical machine learning for (early\n  stage) disease detection",
      "pdf_url": "http://arxiv.org/pdf/2503.02345v1",
      "published_date": "2025-03-04 07:08:47 UTC",
      "updated_date": "2025-03-04 07:08:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:01.456267"
    },
    {
      "arxiv_id": "2503.13473v1",
      "title": "Robust Detection of Extremely Thin Lines Using 0.2mm Piano Wire",
      "title_zh": "使用0.2毫米钢琴线实现极细线条的鲁棒检测",
      "authors": [
        "Jisoo Hong",
        "Youngjin Jung",
        "Jihwan Bae",
        "Seungho Song",
        "Sung-Woo Kang"
      ],
      "abstract": "This study developed an algorithm capable of detecting a reference line (a\n0.2 mm thick piano wire) to accurately determine the position of an automated\ninstallation robot within an elevator shaft. A total of 3,245 images were\ncollected from the experimental tower of H Company, the leading elevator\nmanufacturer in South Korea, and the detection performance was evaluated using\nfour experimental approaches (GCH, GSCH, GECH, FCH). During the initial image\nprocessing stage, Gaussian blurring, sharpening filter, embossing filter, and\nFourier Transform were applied, followed by Canny Edge Detection and Hough\nTransform. Notably, the method was developed to accurately extract the\nreference line by averaging the x-coordinates of the lines detected through the\nHough Transform. This approach enabled the detection of the 0.2 mm thick piano\nwire with high accuracy, even in the presence of noise and other interfering\nfactors (e.g., concrete cracks inside the elevator shaft or safety bars for\nfilming equipment). The experimental results showed that Experiment 4 (FCH),\nwhich utilized Fourier Transform in the preprocessing stage, achieved the\nhighest detection rate for the LtoL, LtoR, and RtoL datasets. Experiment\n2(GSCH), which applied Gaussian blurring and a sharpening filter, demonstrated\nsuperior detection performance on the RtoR dataset. This study proposes a\nreference line detection algorithm that enables precise position calculation\nand control of automated robots in elevator shaft installation. Moreover, the\ndeveloped method shows potential for applicability even in confined working\nspaces. Future work aims to develop a line detection algorithm equipped with\nmachine learning-based hyperparameter tuning capabilities.",
      "tldr_zh": "本研究开发了一种算法，能够检测0.2毫米厚的钢琴线作为参考线，以精确定位电梯井内的自动化安装机器人。通过高斯模糊、锐化滤波、浮雕滤波和傅里叶变换等预处理方法，结合Canny边缘检测和霍夫变换，算法在高噪声环境下仍能准确提取参考线。实验结果表明，使用傅里叶变换预处理的方法在多个数据集上表现最佳，为电梯井安装自动化机器人提供了高精度的定位解决方案，并展示了其在狭窄工作空间中的潜在应用价值。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.13473v1",
      "published_date": "2025-03-04 07:05:33 UTC",
      "updated_date": "2025-03-04 07:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:06.004620"
    },
    {
      "arxiv_id": "2503.02341v1",
      "title": "GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning",
      "title_zh": "GRADEO：通过多步推理实现类人类评估的文本到视频生成方法",
      "authors": [
        "Zhun Mou",
        "Bin Xia",
        "Zhengchao Huang",
        "Wenming Yang",
        "Jiaya Jia"
      ],
      "abstract": "Recent great advances in video generation models have demonstrated their\npotential to produce high-quality videos, bringing challenges to effective\nevaluation. Unlike human evaluation, existing automated evaluation metrics lack\nhigh-level semantic understanding and reasoning capabilities for video, thus\nmaking them infeasible and unexplainable. To fill this gap, we curate\nGRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset,\nincluding 3.3k videos from over 10 existing video generation models and\nmulti-step reasoning assessments converted by 16k human annotations. We then\nintroduce GRADEO, one of the first specifically designed video evaluation\nmodels, which grades AI-generated videos for explainable scores and assessments\nthrough multi-step reasoning. Experiments show that our method aligns better\nwith human evaluations than existing methods. Furthermore, our benchmarking\nreveals that current video generation models struggle to produce content that\naligns with human reasoning and complex real-world scenarios. The models,\ndatasets, and codes will be released soon.",
      "tldr_zh": "该研究提出了GRADEO，一种通过多步推理实现类人评估的文本到视频生成评估框架。研究团队构建了GRADEO-Instruct数据集，包含3.3k个来自10多种视频生成模型的视频和16k条人工标注的多步推理评估，并开发了首个专门设计的视频评估模型GRADEO，用于生成可解释的评分和评估。实验表明，GRADEO比现有方法更接近人类评估，同时揭示了当前视频生成模型在符合人类推理和复杂现实场景方面的不足。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02341v1",
      "published_date": "2025-03-04 07:04:55 UTC",
      "updated_date": "2025-03-04 07:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:24.404480"
    },
    {
      "arxiv_id": "2503.02338v1",
      "title": "Enhancing the Product Quality of the Injection Process Using eXplainable Artificial Intelligence",
      "title_zh": "运用可解释人工智能提升注塑工艺产品质量",
      "authors": [
        "Jisoo Hong",
        "Yongmin Hong",
        "Jung-Woo Baek",
        "Sung-Woo Kang"
      ],
      "abstract": "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
      "tldr_zh": "本研究提出了一种基于可解释人工智能(XAI)的注塑成型过程优化控制系统，旨在降低产品缺陷率。研究采用XGBoost和LightGBM等提升算法作为树基分类器预测产品是否合格，并通过SHAP值分析和个体条件期望(ICE)确定关键控制特征及其最优范围。基于韩国AI制造平台(KAMP)提供的实际数据验证，该方法将缺陷率从1.00%分别降低至XGBoost的0.21%和LightGBM的0.13%，显著提升了注塑成型产品的质量。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02338v1",
      "published_date": "2025-03-04 06:59:01 UTC",
      "updated_date": "2025-03-04 06:59:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:29.493857"
    },
    {
      "arxiv_id": "2503.02334v1",
      "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models",
      "title_zh": "BiasICL：视觉语言模型的上下文学习与人口统计偏差",
      "authors": [
        "Sonnet Xu",
        "Joseph Janizek",
        "Yixing Jiang",
        "Roxana Daneshjou"
      ],
      "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
      "tldr_zh": "该研究探讨了视觉语言模型(VLMs)在医学诊断任务中通过上下文学习(ICL)表现出的群体偏见问题。研究发现ICL机制会通过两种方式影响模型预测：一是让模型从提示中学习特定人群的疾病基础率，二是导致模型在不同人口群体间的预测性能存在差异。通过皮肤病变良恶性预测和气胸检测两项医学影像任务的实验，论文提出了优化ICL提示的最佳实践方案，包括评估各人口亚组表现、匹配标签基础率等，为改进VLM的公平性提供了理论依据。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
      "published_date": "2025-03-04 06:45:54 UTC",
      "updated_date": "2025-03-04 06:45:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:52:28.257708"
    },
    {
      "arxiv_id": "2503.02333v1",
      "title": "Examining the Mental Health Impact of Misinformation on Social Media Using a Hybrid Transformer-Based Approach",
      "title_zh": "基于混合Transformer方法探究社交媒体虚假信息对心理健康的影响",
      "authors": [
        "Sarvesh Arora",
        "Sarthak Arora",
        "Deepika Kumar",
        "Vallari Agrawal",
        "Vedika Gupta",
        "Dipit Vasdev"
      ],
      "abstract": "Social media has significantly reshaped interpersonal communication,\nfostering connectivity while also enabling the proliferation of misinformation.\nThe unchecked spread of false narratives has profound effects on mental health,\ncontributing to increased stress, anxiety, and misinformation-driven paranoia.\nThis study presents a hybrid transformer-based approach using a RoBERTa-LSTM\nclassifier to detect misinformation, assess its impact on mental health, and\nclassify disorders linked to misinformation exposure. The proposed models\ndemonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,\nmental health implications, and disorder classification, respectively.\nFurthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)\nvalidates the direct correlation between misinformation and deteriorating\nmental well-being. This study underscores the urgent need for better\nmisinformation management strategies to mitigate its psychological\nrepercussions. Future research could explore broader datasets incorporating\nlinguistic, demographic, and cultural variables to deepen the understanding of\nmisinformation-induced mental health distress.",
      "tldr_zh": "本研究采用基于RoBERTa-LSTM的混合Transformer模型，分析了社交媒体上错误信息对心理健康的负面影响。研究结果表明，错误信息与压力、焦虑和偏执等心理问题显著相关，模型在检测错误信息、评估其心理影响及分类相关心理障碍方面的准确率分别达到98.4%、87.8%和77.3%。通过皮尔逊卡方检验（p值=0.003871），研究证实了错误信息与心理健康恶化之间的直接关联，强调了加强错误信息管理以减轻其心理影响的紧迫性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02333v1",
      "published_date": "2025-03-04 06:45:17 UTC",
      "updated_date": "2025-03-04 06:45:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:38.026820"
    },
    {
      "arxiv_id": "2503.02324v1",
      "title": "PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models",
      "title_zh": "PromptCoT：为大型语言模型中的数学推理合成奥林匹克级别问题",
      "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Jian Guan",
        "Lingpeng Kong"
      ],
      "abstract": "The ability of large language models to solve complex mathematical problems\nhas progressed significantly, particularly for tasks requiring advanced\nreasoning. However, the scarcity of sufficiently challenging problems,\nparticularly at the Olympiad level, hinders further advancements. In this work,\nwe introduce PromptCoT, a novel approach for automatically generating\nhigh-quality Olympiad-level math problems. The proposed method synthesizes\ncomplex problems based on mathematical concepts and the rationale behind\nproblem construction, emulating the thought processes of experienced problem\ndesigners. We provide a theoretical analysis demonstrating that an optimal\nrationale should maximize both the likelihood of rationale generation given the\nassociated concepts and the likelihood of problem generation conditioned on\nboth the rationale and the concepts. Our method is evaluated on standard\nbenchmarks including GSM8K, MATH-500, and AIME2024, where it consistently\noutperforms existing problem generation methods. Furthermore, we demonstrate\nthat PromptCoT exhibits superior data scalability, consistently maintaining\nhigh performance as the dataset size increases, outperforming the baselines.\nThe implementation is available at https://github.com/zhaoxlpku/PromptCoT.",
      "tldr_zh": "本文提出PromptCoT，一种自动生成高质量奥数级别数学问题的新方法。该方法通过模拟资深问题设计者的思维过程，基于数学概念和问题构建原理合成复杂问题。理论分析表明，最优的生成原理应最大化给定概念下原理生成的似然性，以及在原理和概念条件下问题生成的似然性。实验证明，PromptCoT在GSM8K、MATH-500和AIME2024等标准基准上优于现有方法，并展现出优异的数据扩展性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.02324v1",
      "published_date": "2025-03-04 06:32:30 UTC",
      "updated_date": "2025-03-04 06:32:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:51:45.339881"
    },
    {
      "arxiv_id": "2503.16474v1",
      "title": "From Voices to Worlds: Developing an AI-Powered Framework for 3D Object Generation in Augmented Reality",
      "title_zh": "从语音到世界：开发增强现实中AI驱动的3D物体生成框架",
      "authors": [
        "Majid Behravan",
        "Denis Gracanin"
      ],
      "abstract": "This paper presents Matrix, an advanced AI-powered framework designed for\nreal-time 3D object generation in Augmented Reality (AR) environments. By\nintegrating a cutting-edge text-to-3D generative AI model, multilingual\nspeech-to-text translation, and large language models (LLMs), the system\nenables seamless user interactions through spoken commands. The framework\nprocesses speech inputs, generates 3D objects, and provides object\nrecommendations based on contextual understanding, enhancing AR experiences. A\nkey feature of this framework is its ability to optimize 3D models by reducing\nmesh complexity, resulting in significantly smaller file sizes and faster\nprocessing on resource-constrained AR devices. Our approach addresses the\nchallenges of high GPU usage, large model output sizes, and real-time system\nresponsiveness, ensuring a smoother user experience. Moreover, the system is\nequipped with a pre-generated object repository, further reducing GPU load and\nimproving efficiency. We demonstrate the practical applications of this\nframework in various fields such as education, design, and accessibility, and\ndiscuss future enhancements including image-to-3D conversion, environmental\nobject detection, and multimodal support. The open-source nature of the\nframework promotes ongoing innovation and its utility across diverse\nindustries.",
      "tldr_zh": "该研究提出了Matrix，一种基于AI的框架，用于增强现实(AR)环境中实时生成3D对象。通过整合文本到3D生成模型、多语言语音到文本翻译和大语言模型(LLMs)，用户可以通过语音指令无缝生成3D对象并获得上下文感知的推荐。该框架优化了3D模型，减少网格复杂度，从而降低文件大小并提升在资源受限设备上的处理速度，同时通过预生成对象库进一步减少GPU负载。其应用涵盖教育、设计和无障碍领域，并支持未来扩展如图像到3D转换和多模态支持。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "arXiv admin note: text overlap with arXiv:2502.15869",
      "pdf_url": "http://arxiv.org/pdf/2503.16474v1",
      "published_date": "2025-03-04 06:31:51 UTC",
      "updated_date": "2025-03-04 06:31:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:52:50.838070"
    },
    {
      "arxiv_id": "2503.02318v1",
      "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
      "title_zh": "Audio-Reasoner：提升大型音频语言模型的推理能力",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio\nmodality. We introduce Audio-Reasoner, a large-scale audio language model for\ndeep reasoning in audio tasks. We meticulously curated a large-scale and\ndiverse multi-task audio dataset with simple annotations. Then, we leverage\nclosed-source models to conduct secondary labeling, QA generation, along with\nstructured COT process. These datasets together form a high-quality reasoning\ndataset with 1.2 million reasoning-rich samples, which we name CoTA. Following\ninference scaling principles, we train Audio-Reasoner on CoTA, enabling it to\nachieve great logical capabilities in audio reasoning. Experiments show\nstate-of-the-art performance across key benchmarks, including MMAU-mini\n(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our\nfindings stress the core of structured CoT training in advancing audio\nreasoning.",
      "tldr_zh": "该研究提出了Audio-Reasoner，一种用于提升音频任务深度推理能力的大规模音频语言模型。研究团队精心构建了一个大规模、多样化的多任务音频数据集，并利用闭源模型进行二次标注、问答生成和结构化链式思维推理（COT）处理，最终形成包含120万推理样本的高质量数据集CoTA。基于推理扩展原则，Audio-Reasoner在CoTA上训练，展现出卓越的音频推理能力，在多个关键基准测试中取得了最先进的性能提升。研究强调了结构化COT训练在推动音频推理中的核心作用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Technical report, in process",
      "pdf_url": "http://arxiv.org/pdf/2503.02318v1",
      "published_date": "2025-03-04 06:18:34 UTC",
      "updated_date": "2025-03-04 06:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:52:49.511289"
    },
    {
      "arxiv_id": "2503.05808v1",
      "title": "DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models",
      "title_zh": "DriveGen：利用大模型实现无限多样交通场景的生成",
      "authors": [
        "Shenyu Zhang",
        "Jiaguo Tian",
        "Zhengbang Zhu",
        "Shan Huang",
        "Jucheng Yang",
        "Weinan Zhang"
      ],
      "abstract": "Microscopic traffic simulation has become an important tool for autonomous\ndriving training and testing. Although recent data-driven approaches advance\nrealistic behavior generation, their learning still relies primarily on a\nsingle real-world dataset, which limits their diversity and thereby hinders\ndownstream algorithm optimization. In this paper, we propose DriveGen, a novel\ntraffic simulation framework with large models for more diverse traffic\ngeneration that supports further customized designs. DriveGen consists of two\ninternal stages: the initialization stage uses large language model and\nretrieval technique to generate map and vehicle assets; the rollout stage\noutputs trajectories with selected waypoint goals from visual language model\nand a specific designed diffusion planner. Through this two-staged process,\nDriveGen fully utilizes large models' high-level cognition and reasoning of\ndriving behavior, obtaining greater diversity beyond datasets while maintaining\nhigh realism. To support effective downstream optimization, we additionally\ndevelop DriveGen-CS, an automatic corner case generation pipeline that uses\nfailures of the driving algorithm as additional prompt knowledge for large\nmodels without the need for retraining or fine-tuning. Experiments show that\nour generated scenarios and corner cases have a superior performance compared\nto state-of-the-art baselines. Downstream experiments further verify that the\nsynthesized traffic of DriveGen provides better optimization of the performance\nof typical driving algorithms, demonstrating the effectiveness of our\nframework.",
      "tldr_zh": "本研究提出DriveGen，一种基于大模型的交通仿真框架，旨在生成无限多样化的交通场景以优化自动驾驶算法。DriveGen采用两阶段设计：初始化阶段利用大语言模型和检索技术生成地图和车辆资产；执行阶段通过视觉语言模型和扩散规划器输出轨迹。该框架充分挖掘大模型对驾驶行为的高层次认知和推理能力，在保持高真实性的同时突破单一数据集的局限。此外，DriveGen-CS模块利用驾驶算法失败案例作为提示，无需重新训练即可自动生成极端场景。实验表明，DriveGen生成的场景和极端案例优于现有基线，其合成的交通数据显著提升了典型驾驶算法的优化效果。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05808v1",
      "published_date": "2025-03-04 06:14:21 UTC",
      "updated_date": "2025-03-04 06:14:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:52:58.605107"
    },
    {
      "arxiv_id": "2503.02311v1",
      "title": "Target Return Optimizer for Multi-Game Decision Transformer",
      "title_zh": "多游戏决策变换器的目标回报优化器",
      "authors": [
        "Kensuke Tatematsu",
        "Akifumi Wachi"
      ],
      "abstract": "Achieving autonomous agents with robust generalization capabilities across\ndiverse games and tasks remains one of the ultimate goals in AI research.\nRecent advancements in transformer-based offline reinforcement learning,\nexemplified by the MultiGame Decision Transformer [Lee et al., 2022], have\nshown remarkable performance across various games or tasks. However, these\napproaches depend heavily on human expertise, presenting substantial challenges\nfor practical deployment, particularly in scenarios with limited prior\ngame-specific knowledge. In this paper, we propose an algorithm called\nMulti-Game Target Return Optimizer (MTRO) to autonomously determine\ngame-specific target returns within the Multi-Game Decision Transformer\nframework using solely offline datasets. MTRO addresses the existing\nlimitations by automating the target return configuration process, leveraging\nenvironmental reward information extracted from offline datasets. Notably, MTRO\ndoes not require additional training, enabling seamless integration into\nexisting Multi-Game Decision Transformer architectures. Our experimental\nevaluations on Atari games demonstrate that MTRO enhances the performance of RL\npolicies across a wide array of games, underscoring its potential to advance\nthe field of autonomous agent development.",
      "tldr_zh": "本文提出了一种名为Multi-Game Target Return Optimizer (MTRO)的新型算法，用于自动优化多游戏决策变换器(Multi-Game Decision Transformer)中的游戏特定目标回报(target return)。该方法通过分析离线数据集中的环境奖励信息，无需人工干预或额外训练即可自动配置最优目标回报，解决了现有方法对人工专业知识的依赖问题。实验表明，MTRO在Atari游戏测试中显著提升了强化学习策略的泛化性能，为开发具有跨游戏适应能力的自主智能体提供了新的技术路径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.02311v1",
      "published_date": "2025-03-04 06:13:53 UTC",
      "updated_date": "2025-03-04 06:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:21.065038"
    },
    {
      "arxiv_id": "2503.02303v1",
      "title": "Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization",
      "title_zh": "前额叶对海马情景记忆的灵活调控机制：面向目标导向的泛化能力",
      "authors": [
        "Yicong Zheng",
        "Nora Wolf",
        "Charan Ranganath",
        "Randall C. O'Reilly",
        "Kevin L. McKee"
      ],
      "abstract": "Many tasks require flexibly modifying perception and behavior based on\ncurrent goals. Humans can retrieve episodic memories from days to years ago,\nusing them to contextualize and generalize behaviors across novel but\nstructurally related situations. The brain's ability to control episodic\nmemories based on task demands is often attributed to interactions between the\nprefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement\nlearning model that incorporates a PFC-HPC interaction mechanism for\ngoal-directed generalization. In our model, the PFC learns to generate\nquery-key representations to encode and retrieve goal-relevant episodic\nmemories, modulating HPC memories top-down based on current task demands.\nMoreover, the PFC adapts its encoding and retrieval strategies dynamically when\nfaced with multiple goals presented in a blocked, rather than interleaved,\nmanner. Our results show that: (1) combining working memory with selectively\nretrieved episodic memory allows transfer of decisions among similar\nenvironments or situations, (2) top-down control from PFC over HPC improves\nlearning of arbitrary structural associations between events for generalization\nto novel environments compared to a bottom-up sensory-driven approach, and (3)\nthe PFC encodes generalizable representations during both encoding and\nretrieval of goal-relevant memories, whereas the HPC exhibits event-specific\nrepresentations. Together, these findings highlight the importance of\ngoal-directed prefrontal control over hippocampal episodic memory for\ndecision-making in novel situations and suggest a computational mechanism by\nwhich PFC-HPC interactions enable flexible behavior.",
      "tldr_zh": "该研究提出了一种强化学习模型，模拟前额叶皮层(PFC)与海马体(HPC)的交互机制，用于目标导向的泛化。模型通过PFC生成查询-键表示来编码和检索目标相关的情景记忆，并根据任务需求自上而下调控HPC记忆。实验表明：(1) 结合工作记忆和选择性检索的情景记忆可实现相似环境间的决策迁移；(2) PFC对HPC的自上而下控制比自下而上的感官驱动方法更有利于学习事件间的结构关联，从而泛化到新环境；(3) PFC在编码和检索目标相关记忆时表现出可泛化的表征，而HPC则呈现事件特异性表征。这些发现强调了目标导向的PFC控制对情景记忆在决策中的重要性，并为PFC-HPC交互如何实现灵活行为提供了计算机制。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02303v1",
      "published_date": "2025-03-04 06:04:54 UTC",
      "updated_date": "2025-03-04 06:04:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:21.045018"
    },
    {
      "arxiv_id": "2503.02296v1",
      "title": "Memorize or Generalize? Evaluating LLM Code Generation with Evolved Questions",
      "title_zh": "记忆还是泛化？基于进化问题评估大语言模型的代码生成能力",
      "authors": [
        "Wentao Chen",
        "Lizhe Zhang",
        "Li Zhong",
        "Letian Peng",
        "Zilong Wang",
        "Jingbo Shang"
      ],
      "abstract": "Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.",
      "tldr_zh": "本研究探讨了大语言模型(LLMs)在代码生成中的记忆现象，即模型倾向于记忆训练数据中的问题和解决方案，而非真正理解编程问题的原理。通过设计三种进化策略（变异、改写和代码重写）生成问题变体，并比较LLM生成代码的性能和抽象语法树(AST)相似性，研究者提出了一个与记忆程度正相关的记忆分数。研究发现，随着监督微调的进行，记忆分数在过拟合前持续上升，表明记忆现象加剧。此外，常见的缓解方法（如提示翻译和使用进化变体进行数据增强）要么影响性能，要么无法有效减轻记忆问题，凸显了解决LLM代码生成中记忆挑战的迫切需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02296v1",
      "published_date": "2025-03-04 05:39:24 UTC",
      "updated_date": "2025-03-04 05:39:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:36.954099"
    },
    {
      "arxiv_id": "2503.02284v1",
      "title": "Semi-Supervised Audio-Visual Video Action Recognition with Audio Source Localization Guided Mixup",
      "title_zh": "基于音频源定位引导混合的半监督音视频动作识别",
      "authors": [
        "Seokun Kang",
        "Taehwan Kim"
      ],
      "abstract": "Video action recognition is a challenging but important task for\nunderstanding and discovering what the video does. However, acquiring\nannotations for a video is costly, and semi-supervised learning (SSL) has been\nstudied to improve performance even with a small number of labeled data in the\ntask. Prior studies for semi-supervised video action recognition have mostly\nfocused on using single modality - visuals - but the video is multi-modal, so\nutilizing both visuals and audio would be desirable and improve performance\nfurther, which has not been explored well. Therefore, we propose audio-visual\nSSL for video action recognition, which uses both visual and audio together,\neven with quite a few labeled data, which is challenging. In addition, to\nmaximize the information of audio and video, we propose a novel audio source\nlocalization-guided mixup method that considers inter-modal relations between\nvideo and audio modalities. In experiments on UCF-51, Kinetics-400, and\nVGGSound datasets, our model shows the superior performance of the proposed\nsemi-supervised audio-visual action recognition framework and audio source\nlocalization-guided mixup.",
      "tldr_zh": "本研究提出了一种半监督学习（SSL）框架，用于音频-视觉视频动作识别，通过同时利用视觉和音频模态提升性能。为了解决标注数据有限的问题，该框架引入了一种新颖的音频源定位引导的Mixup方法，优化了视频和音频模态之间的跨模态关系。在UCF-51、Kinetics-400和VGGSound数据集上的实验表明，该框架在半监督音频-视觉动作识别任务中表现优异，显著优于现有方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02284v1",
      "published_date": "2025-03-04 05:13:56 UTC",
      "updated_date": "2025-03-04 05:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:36.408381"
    },
    {
      "arxiv_id": "2503.02269v1",
      "title": "Experience Replay with Random Reshuffling",
      "title_zh": "基于随机重排的经验回放",
      "authors": [
        "Yasuhiro Fujita"
      ],
      "abstract": "Experience replay is a key component in reinforcement learning for\nstabilizing learning and improving sample efficiency. Its typical\nimplementation samples transitions with replacement from a replay buffer. In\ncontrast, in supervised learning with a fixed dataset, it is a common practice\nto shuffle the dataset every epoch and consume data sequentially, which is\ncalled random reshuffling (RR). RR enjoys theoretically better convergence\nproperties and has been shown to outperform with-replacement sampling\nempirically. To leverage the benefits of RR in reinforcement learning, we\npropose sampling methods that extend RR to experience replay, both in uniform\nand prioritized settings. We evaluate our sampling methods on Atari benchmarks,\ndemonstrating their effectiveness in deep reinforcement learning.",
      "tldr_zh": "该论文提出将监督学习中的随机重排(Random Reshuffling, RR)采样方法引入强化学习的经验回放机制。与传统的带替换采样相比，RR通过每轮完整遍历并随机重排经验缓冲区数据，理论上具有更好的收敛特性。研究设计了适用于均匀采样和优先采样的RR变体方法，在Atari基准测试中验证了该方法的有效性，为提升深度强化学习的样本效率提供了新思路。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02269v1",
      "published_date": "2025-03-04 04:37:22 UTC",
      "updated_date": "2025-03-04 04:37:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:44.128369"
    },
    {
      "arxiv_id": "2503.02268v1",
      "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
      "title_zh": "AppAgentX：将GUI智能体进化为熟练智能手机用户",
      "authors": [
        "Wenjia Jiang",
        "Yangyang Zhuang",
        "Chenxi Song",
        "Xu Yang",
        "Chi Zhang"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have led to the\ndevelopment of intelligent LLM-based agents capable of interacting with\ngraphical user interfaces (GUIs). These agents demonstrate strong reasoning and\nadaptability, enabling them to perform complex tasks that traditionally\nrequired predefined rules. However, the reliance on step-by-step reasoning in\nLLM-based agents often results in inefficiencies, particularly for routine\ntasks. In contrast, traditional rule-based systems excel in efficiency but lack\nthe intelligence and flexibility to adapt to novel scenarios. To address this\nchallenge, we propose a novel evolutionary framework for GUI agents that\nenhances operational efficiency while retaining intelligence and flexibility.\nOur approach incorporates a memory mechanism that records the agent's task\nexecution history. By analyzing this history, the agent identifies repetitive\naction sequences and evolves high-level actions that act as shortcuts,\nreplacing these low-level operations and improving efficiency. This allows the\nagent to focus on tasks requiring more complex reasoning, while simplifying\nroutine actions. Experimental results on multiple benchmark tasks demonstrate\nthat our approach significantly outperforms existing methods in both efficiency\nand accuracy. The code will be open-sourced to support further research.",
      "tldr_zh": "该研究提出了AppAgentX，一种基于进化的GUI智能体框架，旨在提升智能手机用户界面的交互效率与智能性。通过引入记忆机制记录任务执行历史，智能体能够识别重复操作序列并进化为高级操作，从而简化低效的逐步推理过程。实验表明，该方法在多个基准任务上显著优于现有方法，实现了效率与准确性的双重提升，同时保留了处理复杂任务所需的智能与灵活性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02268v1",
      "published_date": "2025-03-04 04:34:09 UTC",
      "updated_date": "2025-03-04 04:34:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:53:45.889448"
    },
    {
      "arxiv_id": "2503.02267v1",
      "title": "REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs",
      "title_zh": "REAct：用于PINNs中更好学习与泛化的有理指数激活函数",
      "authors": [
        "Sourav Mishra",
        "Shreya Hallikeri",
        "Suresh Sundaram"
      ],
      "abstract": "Physics-Informed Neural Networks (PINNs) offer a promising approach to\nsimulating physical systems. Still, their application is limited by\noptimization challenges, mainly due to the lack of activation functions that\ngeneralize well across several physical systems. Existing activation functions\noften lack such flexibility and generalization power. To address this issue, we\nintroduce Rational Exponential Activation (REAct), a generalized form of tanh\nconsisting of four learnable shape parameters. Experiments show that REAct\noutperforms many standard and benchmark activations, achieving an MSE three\norders of magnitude lower than tanh on heat problems and generalizing well to\nfiner grids and points beyond the training domain. It also excels at function\napproximation tasks and improves noise rejection in inverse problems, leading\nto more accurate parameter estimates across varying noise levels.",
      "tldr_zh": "该研究提出了Rational Exponential Activation (REAct)，一种用于物理信息神经网络(PINNs)的新型激活函数。REAct作为tanh的广义形式，包含四个可学习的形状参数，能够更好地适应不同物理系统的建模需求。实验表明，REAct在热传导问题上比tanh的均方误差降低了三个数量级，在函数逼近任务中表现优异，并能有效提高逆问题中的噪声抑制能力，获得更准确的参数估计。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 5 tables, 1 figure; Accepted at ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02267v1",
      "published_date": "2025-03-04 04:28:59 UTC",
      "updated_date": "2025-03-04 04:28:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:54:04.938582"
    },
    {
      "arxiv_id": "2503.02249v1",
      "title": "Large Language Models as Natural Selector for Embodied Soft Robot Design",
      "title_zh": "大型语言模型作为具身软体机器人设计的自然选择器",
      "authors": [
        "Changhe Chen",
        "Xiaohao Xu",
        "Xiangdong Wang",
        "Xiaonan Huang"
      ],
      "abstract": "Designing soft robots is a complex and iterative process that demands\ncross-disciplinary expertise in materials science, mechanics, and control,\noften relying on intuition and extensive experimentation. While Large Language\nModels (LLMs) have demonstrated impressive reasoning abilities, their capacity\nto learn and apply embodied design principles--crucial for creating functional\nrobotic systems--remains largely unexplored. This paper introduces\nRoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn\nrepresentations of soft robot designs that effectively bridge the gap between\nhigh-level task descriptions and low-level morphological and material choices.\nRoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft\nrobot design challenges, spanning robotic locomotion, manipulation, and\nbalancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal\nthat while these models exhibit promising capabilities in learning design\nrepresentations, they struggle with fine-grained distinctions between designs\nwith subtle performance differences. We further demonstrate the practical\nutility of LLMs for robot design initialization. Our code and benchmark will be\navailable to encourage the community to foster this exciting research\ndirection.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)在软体机器人设计中的应用潜力，提出了RoboCrafter-QA基准测试，用于评估LLMs能否有效学习软体机器人设计的表征，从而将高层任务描述与低层形态和材料选择联系起来。实验表明，尽管LLMs在学习设计表征方面展现出潜力，但在区分性能差异细微的设计时仍存在困难。研究还展示了LLMs在机器人设计初始化中的实际应用价值，为推动这一研究方向提供了代码和基准测试。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02249v1",
      "published_date": "2025-03-04 03:55:10 UTC",
      "updated_date": "2025-03-04 03:55:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:54:20.239764"
    },
    {
      "arxiv_id": "2503.02239v1",
      "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
      "title_zh": "V2X-LLM：提升车联网走廊中V2X集成与理解能力",
      "authors": [
        "Keshu Wu",
        "Pei Li",
        "Yang Zhou",
        "Rui Gan",
        "Junwei You",
        "Yang Cheng",
        "Jingwen Zhu",
        "Steven T. Parker",
        "Bin Ran",
        "David A. Noyce",
        "Zhengzhong Tu"
      ],
      "abstract": "The advancement of Connected and Automated Vehicles (CAVs) and\nVehicle-to-Everything (V2X) offers significant potential for enhancing\ntransportation safety, mobility, and sustainability. However, the integration\nand analysis of the diverse and voluminous V2X data, including Basic Safety\nMessages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial\nchallenges, especially on Connected Vehicle Corridors. These challenges include\nmanaging large data volumes, ensuring real-time data integration, and\nunderstanding complex traffic scenarios. Although these projects have developed\nan advanced CAV data pipeline that enables real-time communication between\nvehicles, infrastructure, and other road users for managing connected vehicle\nand roadside unit (RSU) data, significant hurdles in data comprehension and\nreal-time scenario analysis and reasoning persist. To address these issues, we\nintroduce the V2X-LLM framework, a novel enhancement to the existing CV data\npipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the\nunderstanding and real-time analysis of V2X data. The framework includes four\nkey tasks: Scenario Explanation, offering detailed narratives of traffic\nconditions; V2X Data Description, detailing vehicle and infrastructure\nstatuses; State Prediction, forecasting future traffic states; and Navigation\nAdvisory, providing optimized routing instructions. By integrating LLM-driven\nreasoning with V2X data within the data pipeline, the V2X-LLM framework offers\nreal-time feedback and decision support for traffic management. This\nintegration enhances the accuracy of traffic analysis, safety, and traffic\noptimization. Demonstrations in a real-world urban corridor highlight the\nframework's potential to advance intelligent transportation systems.",
      "tldr_zh": "该研究提出了V2X-LLM框架，通过将大型语言模型(LLMs)与车联网(V2X)数据结合，提升对交通场景的理解和实时分析能力。该框架包含四大核心任务：交通场景解释、V2X数据描述、状态预测和导航建议，旨在优化交通管理决策。通过在真实城市走廊中的验证，V2X-LLM显著提高了交通分析的准确性、安全性和优化效率，为智能交通系统的发展提供了新思路。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02239v1",
      "published_date": "2025-03-04 03:28:30 UTC",
      "updated_date": "2025-03-04 03:28:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:54:29.431488"
    },
    {
      "arxiv_id": "2503.02235v1",
      "title": "Deficient Excitation in Parameter Learning",
      "title_zh": "参数学习中的激励不足问题",
      "authors": [
        "Ganghui Cao",
        "Shimin Wang",
        "Martin Guay",
        "Jinzhi Wang",
        "Zhisheng Duan",
        "Marios M. Polycarpou"
      ],
      "abstract": "This paper investigates parameter learning problems under deficient\nexcitation (DE). The DE condition is a rank-deficient, and therefore, a more\ngeneral evolution of the well-known persistent excitation condition. Under the\nDE condition, a proposed online algorithm is able to calculate the identifiable\nand non-identifiable subspaces, and finally give an optimal parameter estimate\nin the sense of least squares. In particular, the learning error within the\nidentifiable subspace exponentially converges to zero in the noise-free case,\neven without persistent excitation. The DE condition also provides a new\nperspective for solving distributed parameter learning problems, where the\nchallenge is posed by local regressors that are often insufficiently excited.\nTo improve knowledge of the unknown parameters, a cooperative learning protocol\nis proposed for a group of estimators that collect measured information under\ncomplementary DE conditions. This protocol allows each local estimator to\noperate locally in its identifiable subspace, and reach a consensus with\nneighbours in its non-identifiable subspace. As a result, the task of\nestimating unknown parameters can be achieved in a distributed way using\ncooperative local estimators. Application examples in system identification are\ngiven to demonstrate the effectiveness of the theoretical results developed in\nthis paper.",
      "tldr_zh": "本文研究在激励不足(DE)条件下的参数学习问题，提出了一种在线算法，能够计算可识别和不可识别子空间，并给出最小二乘意义下的最优参数估计。在无噪声情况下，即使没有持续激励，可识别子空间内的学习误差也能指数收敛到零。此外，DE条件为分布式参数学习问题提供了新视角，提出了一种协作学习协议，使局部估计器能够在互补的DE条件下收集信息，在可识别子空间内局部运行，并在不可识别子空间内与邻居达成共识，从而实现分布式参数估计。系统辨识中的应用示例验证了理论结果的有效性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "eess.SP",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "16 pages,9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.02235v1",
      "published_date": "2025-03-04 03:18:13 UTC",
      "updated_date": "2025-03-04 03:18:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:54:42.339802"
    },
    {
      "arxiv_id": "2503.02233v2",
      "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
      "title_zh": "通过显式知识边界建模增强大语言模型可靠性",
      "authors": [
        "Hang Zheng",
        "Hongshen Xu",
        "Yuncong Liu",
        "Lu Chen",
        "Pascale Fung",
        "Kai Yu"
      ],
      "abstract": "Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications.",
      "tldr_zh": "该研究提出了显式知识边界建模(EKBM)框架，通过整合快速和慢速推理系统来提升大语言模型(LLMs)的可靠性。该框架首先利用快速思维模型生成带有置信度标签的响应，确保高置信度输出可直接使用；对于不确定的预测，慢速精炼模型进行针对性推理以提高准确性。通过混合训练管道，该框架在不降低任务性能的情况下增强了模型的自知能力。实验表明，EKBM在对话状态跟踪任务中显著提升了模型可靠性，同时保持了较低的计算开销，为LLMs在误差敏感应用中的准确性和实用性平衡提供了可扩展的解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02233v2",
      "published_date": "2025-03-04 03:16:02 UTC",
      "updated_date": "2025-03-12 07:42:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:58:06.492769"
    },
    {
      "arxiv_id": "2503.02228v1",
      "title": "One Patient's Annotation is Another One's Initialization: Towards Zero-Shot Surgical Video Segmentation with Cross-Patient Initialization",
      "title_zh": "一位患者的标注是另一位患者的初始化：迈向跨患者初始化的零样本手术视频分割",
      "authors": [
        "Seyed Amir Mousavi",
        "Utku Ozbulak",
        "Francesca Tozzi",
        "Nikdokht Rashidian",
        "Wouter Willaert",
        "Joris Vankerschaver",
        "Wesley De Neve"
      ],
      "abstract": "Video object segmentation is an emerging technology that is well-suited for\nreal-time surgical video segmentation, offering valuable clinical assistance in\nthe operating room by ensuring consistent frame tracking. However, its adoption\nis limited by the need for manual intervention to select the tracked object,\nmaking it impractical in surgical settings. In this work, we tackle this\nchallenge with an innovative solution: using previously annotated frames from\nother patients as the tracking frames. We find that this unconventional\napproach can match or even surpass the performance of using patients' own\ntracking frames, enabling more autonomous and efficient AI-assisted surgical\nworkflows. Furthermore, we analyze the benefits and limitations of this\napproach, highlighting its potential to enhance segmentation accuracy while\nreducing the need for manual input. Our findings provide insights into key\nfactors influencing performance, offering a foundation for future research on\noptimizing cross-patient frame selection for real-time surgical video analysis.",
      "tldr_zh": "该研究提出了一种创新的跨患者初始化方法，通过使用其他患者已标注的帧作为跟踪帧，实现了手术视频的零样本分割。研究发现这种非常规方法不仅能达到甚至超越使用患者自身跟踪帧的性能，还能减少人工干预需求，使AI辅助手术工作流更加自主高效。论文还分析了该方法的优势与局限，为优化实时手术视频分析的跨患者帧选择提供了研究基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02228v1",
      "published_date": "2025-03-04 03:11:03 UTC",
      "updated_date": "2025-03-04 03:11:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:54:49.872664"
    },
    {
      "arxiv_id": "2503.02221v1",
      "title": "Attention Bootstrapping for Multi-Modal Test-Time Adaptation",
      "title_zh": "注意力引导的多模态测试时适应方法",
      "authors": [
        "Yusheng Zhao",
        "Junyu Luo",
        "Xiao Luo",
        "Jinsheng Huang",
        "Jingyang Yuan",
        "Zhiping Xiao",
        "Ming Zhang"
      ],
      "abstract": "Test-time adaptation aims to adapt a well-trained model to potential\ndistribution shifts at test time using only unlabeled test data, without access\nto the original training data. While previous efforts mainly focus on a single\nmodality, test-time distribution shift in the multi-modal setting is more\ncomplex and calls for new solutions. This paper tackles the problem of\nmulti-modal test-time adaptation by proposing a novel method named Attention\nBootstrapping with Principal Entropy Minimization (ABPEM). We observe that\ntest-time distribution shift causes misalignment across modalities, leading to\na large gap between intra-modality discrepancies (measured by self-attention)\nand inter-modality discrepancies (measured by cross-attention). We name this\nthe attention gap. This attention gap widens with more severe distribution\nshifts, hindering effective modality fusion. To mitigate this attention gap and\nencourage better modality fusion, we propose attention bootstrapping that\npromotes cross-attention with the guidance of self-attention. Moreover, to\nreduce the gradient noise in the commonly-used entropy minimization, we adopt\nprincipal entropy minimization, a refinement of entropy minimization that\nreduces gradient noise by focusing on the principal parts of entropy, excluding\nless reliable gradient information. Extensive experiments on the benchmarks\nvalidate the effectiveness of the proposed ABPEM in comparison with competing\nbaselines.",
      "tldr_zh": "本研究提出了一种名为Attention Bootstrapping with Principal Entropy Minimization (ABPEM)的新方法，用于解决多模态测试时适应问题。研究发现，测试时分布偏移会导致模态间错位，表现为自注意力（self-attention）与跨模态注意力（cross-attention）之间的注意力差距（attention gap），这种差距会阻碍有效的模态融合。ABPEM通过自注意力引导跨模态注意力，减少注意力差距，并采用主熵最小化（principal entropy minimization）来降低梯度噪声。实验证明，ABPEM在多模态测试时适应任务中优于现有基线方法。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02221v1",
      "published_date": "2025-03-04 02:53:53 UTC",
      "updated_date": "2025-03-04 02:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:55:00.029880"
    },
    {
      "arxiv_id": "2503.04808v1",
      "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
      "title_zh": "从多尝试强化学习中的失败中学习",
      "authors": [
        "Stephen Chung",
        "Wenyu Du",
        "Jie Fu"
      ],
      "abstract": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
      "tldr_zh": "该研究提出了一种多尝试强化学习（Multi-Attempt RL）方法，通过让语言模型（LLM）在回答问题过程中获得多次尝试机会并接收反馈，从而提升其推理能力。实验表明，在数学基准测试中，经过多任务训练的LLM在两次尝试后准确率从45.6%提升至52.5%，而传统单轮训练模型仅从42.3%微增至43.2%。该方法不仅提高了模型性能，还增强了其根据反馈优化回答的能力，为LLM的持续学习机制提供了新思路。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "preprint",
      "pdf_url": "http://arxiv.org/pdf/2503.04808v1",
      "published_date": "2025-03-04 02:53:39 UTC",
      "updated_date": "2025-03-04 02:53:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:55:12.691487"
    },
    {
      "arxiv_id": "2503.02199v1",
      "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
      "title_zh": "文字还是视觉：视觉语言模型是否盲目信任文本？",
      "authors": [
        "Ailin Deng",
        "Tri Cao",
        "Zhirui Chen",
        "Bryan Hooi"
      ],
      "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith\nin text''} phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
      "tldr_zh": "该研究揭示了视觉语言模型(VLMs)在处理视觉和文本信息不一致时存在“盲目信任文本”的现象。通过四种视觉中心任务和十种VLMs的评估，研究发现当文本与视觉信息冲突时，VLMs倾向于过度依赖文本数据，导致性能显著下降。研究分析了影响文本偏见的因素，如指令提示、语言模型大小、文本相关性和token顺序等，并提出通过监督微调和文本增强来减轻文本偏见。此外，理论分析表明，这一现象可能源于训练过程中纯文本和多模态数据的不平衡。研究强调了在VLMs中平衡训练和谨慎处理模态交互的重要性，以提高其在处理多模态数据不一致时的鲁棒性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02199v1",
      "published_date": "2025-03-04 02:21:07 UTC",
      "updated_date": "2025-03-04 02:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:55:22.885425"
    },
    {
      "arxiv_id": "2503.02197v1",
      "title": "ATLaS: Agent Tuning via Learning Critical Steps",
      "title_zh": "ATLaS：通过关键步骤学习实现智能体调优",
      "authors": [
        "Zhixun Chen",
        "Ming Li",
        "Yuxuan Huang",
        "Yali Du",
        "Meng Fang",
        "Tianyi Zhou"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
      "tldr_zh": "该研究提出ATLaS方法，通过识别专家轨迹中的关键步骤（如规划、复杂推理和战略决策）进行针对性微调，而非全轨迹学习，以提升大语言模型（LLM）智能体的性能。该方法仅需微调30%的关键步骤，即可超越全步骤微调模型和现有开源LLM智能体，同时降低训练成本并增强跨环境和任务的泛化能力。ATLaS在保持基础LLM通用能力的同时，有效避免了全轨迹学习导致的专家偏差和过拟合问题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02197v1",
      "published_date": "2025-03-04 02:14:55 UTC",
      "updated_date": "2025-03-04 02:14:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:55:37.542420"
    },
    {
      "arxiv_id": "2503.05805v1",
      "title": "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
      "title_zh": "基于潜在图扩散模型的多智能体自动竞价",
      "authors": [
        "Dom Huh",
        "Prasant Mohapatra"
      ],
      "abstract": "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
      "tldr_zh": "本研究提出了一种基于图扩散模型的多智能体自动竞价框架，用于大规模拍卖环境中的动态竞价策略优化。该框架结合可学习的图嵌入和基于规划的潜在扩散模型(LDM)，能够捕捉拍卖环境中印象机会的相互依赖性和多智能体动态的潜在模式。通过奖励对齐技术，LDM的后验分布被微调以生成最大化关键绩效指标(KPI)并满足约束条件的自动竞价轨迹。实验表明，该框架在真实和合成拍卖环境中显著提升了多种常见KPI指标的自动竞价性能，并提高了拍卖结果预测的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05805v1",
      "published_date": "2025-03-04 02:07:24 UTC",
      "updated_date": "2025-03-04 02:07:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:55:37.134844"
    },
    {
      "arxiv_id": "2503.04807v2",
      "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data",
      "title_zh": "呼吁严谨报告指令微调数据质量",
      "authors": [
        "Hyeonseok Moon",
        "Jaehyung Seo",
        "Heuiseok Lim"
      ],
      "abstract": "Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion.",
      "tldr_zh": "该论文指出当前指令微调(Instruction Tuning)研究中存在关键方法缺陷：研究者通常通过评估大语言模型(LLMs)的表现来衡量指令数据质量，却普遍随意选择训练超参数而缺乏合理依据。研究表明，即使使用相同数据和模型，不同超参数设置会导致截然不同的结论，这严重影响了数据质量评估的可靠性。通过分析LIMA数据集和1,000个Alpaca数据点的实验，作者呼吁学界在报告指令微调数据质量时必须采用更严谨的方法论。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.04807v2",
      "published_date": "2025-03-04 02:04:58 UTC",
      "updated_date": "2025-03-11 07:10:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:58:23.206828"
    },
    {
      "arxiv_id": "2503.10649v1",
      "title": "Measuring Political Preferences in AI Systems: An Integrative Approach",
      "title_zh": "衡量AI系统中的政治倾向：一种综合评估方法",
      "authors": [
        "David Rozado"
      ],
      "abstract": "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment.",
      "tldr_zh": "本研究采用多方法综合评估了大型语言模型（LLMs）如ChatGPT和Gemini中的政治偏见，包括语言对比、政策建议分析、情感分析和标准化政治倾向测试。结果表明，当前大多数AI系统存在一致的左倾偏见，尽管这种偏见并非LLMs固有，而是与训练数据的政治倾向相关。研究指出，AI系统中的系统性政治偏见可能导致观点多样性减少、社会极化加剧以及公众对AI技术的不信任。为此，建议AI设计应优先考虑事实准确性，并在合法规范问题上保持中立，同时建立独立监控平台以确保透明度和责任性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Measuring Political Preferences in AI Systems. Report. Available:\n  https://manhattan.institute/article/measuring-political-preferences-in-ai-systems-an-integrative-approach/",
      "pdf_url": "http://arxiv.org/pdf/2503.10649v1",
      "published_date": "2025-03-04 01:40:28 UTC",
      "updated_date": "2025-03-04 01:40:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:58:27.017855"
    },
    {
      "arxiv_id": "2503.02180v1",
      "title": "Discrete Differential Evolution Particle Swarm Optimization Algorithm for Energy Saving Flexible Job Shop Scheduling Problem Considering Machine Multi States",
      "title_zh": "考虑机器多状态的节能型柔性作业车间调度问题的离散差分进化粒子群优化算法",
      "authors": [
        "Da Wang",
        "Yu Zhang",
        "Kai Zhang",
        "Junqing Li",
        "Dengwang Li"
      ],
      "abstract": "As the continuous deepening of low-carbon emission reduction policies, the\nmanufacturing industries urgently need sensible energy-saving scheduling\nschemes to achieve the balance between improving production efficiency and\nreducing energy consumption. In energy-saving scheduling, reasonable machine\nstates-switching is a key point to achieve expected goals, i.e., whether the\nmachines need to switch speed between different operations, and whether the\nmachines need to add extra setup time between different jobs. Regarding this\nmatter, this work proposes a novel machine multi states-based energy saving\nflexible job scheduling problem (EFJSP-M), which simultaneously takes into\naccount machine multi speeds and setup time. To address the proposed EFJSP-M, a\nkind of discrete differential evolution particle swarm optimization algorithm\n(D-DEPSO) is designed. In specific, D-DEPSO includes a hybrid initialization\nstrategy to improve the initial population performance, an updating mechanism\nembedded with differential evolution operators to enhance population diversity,\nand a critical path variable neighborhood search strategy to expand the\nsolution space. At last, based on datasets DPs and MKs, the experiment results\ncompared with five state-of-the-art algorithms demonstrate the feasible of\nEFJSP-M and the superior of D-DEPSO.",
      "tldr_zh": "该研究提出了一种离散差分进化粒子群优化算法（D-DEPSO），用于解决考虑机器多状态的节能柔性作业车间调度问题（EFJSP-M）。该问题同时考虑了机器多速度和设置时间，旨在平衡生产效率和能耗。D-DEPSO算法结合了混合初始化策略、差分进化算子和关键路径变邻域搜索策略，以提升初始种群性能、增强种群多样性并扩展解空间。实验结果表明，与五种先进算法相比，D-DEPSO在解决EFJSP-M问题上表现出显著优越性。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02180v1",
      "published_date": "2025-03-04 01:40:24 UTC",
      "updated_date": "2025-03-04 01:40:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:58:38.552538"
    },
    {
      "arxiv_id": "2503.02175v1",
      "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
      "title_zh": "DivPrune：基于多样性的大型多模态模型视觉令牌剪枝方法",
      "authors": [
        "Saeed Ranjbar Alvar",
        "Gursimran Singh",
        "Mohammad Akbari",
        "Yong Zhang"
      ],
      "abstract": "Large Multimodal Models (LMMs) have emerged as powerful models capable of\nunderstanding various data modalities, including text, images, and videos. LMMs\nencode both text and visual data into tokens that are then combined and\nprocessed by an integrated Large Language Model (LLM). Including visual tokens\nsubstantially increases the total token count, often by thousands. The\nincreased input length for LLM significantly raises the complexity of\ninference, resulting in high latency in LMMs. To address this issue, token\npruning methods, which remove part of the visual tokens, are proposed. The\nexisting token pruning methods either require extensive calibration and\nfine-tuning or rely on suboptimal importance metrics which results in increased\nredundancy among the retained tokens. In this paper, we first formulate token\npruning as Max-Min Diversity Problem (MMDP) where the goal is to select a\nsubset such that the diversity among the selected {tokens} is maximized. Then,\nwe solve the MMDP to obtain the selected subset and prune the rest. The\nproposed method, DivPrune, reduces redundancy and achieves the highest\ndiversity of the selected tokens. By ensuring high diversity, the selected\ntokens better represent the original tokens, enabling effective performance\neven at high pruning ratios without requiring fine-tuning. Extensive\nexperiments with various LMMs show that DivPrune achieves state-of-the-art\naccuracy over 16 image- and video-language datasets. Additionally, DivPrune\nreduces both the end-to-end latency and GPU memory usage for the tested models.\nThe code is available $\\href{https://github.com/vbdi/divprune}{\\text{here}}$.",
      "tldr_zh": "本文提出了 DivPrune 方法，通过多样性最大化策略解决大模型（LMMs）中视觉 token 冗余问题。该方法将 token 剪枝形式化为 Max-Min 多样性问题（MMDP），选择具有最高多样性的 token 子集，从而在无需微调的情况下有效减少冗余并提升模型性能。实验表明，DivPrune 在 16 个图像和视频语言数据集上实现了最先进的精度，同时显著降低了端到端延迟和 GPU 内存占用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02175v1",
      "published_date": "2025-03-04 01:33:14 UTC",
      "updated_date": "2025-03-04 01:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:10.898810"
    },
    {
      "arxiv_id": "2503.02174v1",
      "title": "Adversarial Tokenization",
      "title_zh": "对抗性分词",
      "authors": [
        "Renato Lui Geh",
        "Zilei Shao",
        "Guy Van den Broeck"
      ],
      "abstract": "Current LLM pipelines account for only one possible tokenization for a given\nstring, ignoring exponentially many alternative tokenizations during training\nand inference. For example, the standard Llama3 tokenization of penguin is\n[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this\npaper, we show that despite LLMs being trained solely on one tokenization, they\nstill retain semantic understanding of other tokenizations, raising questions\nabout their implications in LLM safety. Put succinctly, we answer the following\nquestion: can we adversarially tokenize an obviously malicious string to evade\nsafety and alignment restrictions? We show that not only is adversarial\ntokenization an effective yet previously neglected axis of attack, but it is\nalso competitive against existing state-of-the-art adversarial approaches\nwithout changing the text of the harmful request. We empirically validate this\nexploit across three state-of-the-art LLMs and adversarial datasets, revealing\na previously unknown vulnerability in subword models.",
      "tldr_zh": "这篇论文提出了\"对抗性分词\"(Adversarial Tokenization)的概念，揭示了当前大语言模型(LLM)安全性的新漏洞。研究发现，尽管LLM训练时只使用标准分词方式，但仍能理解同一字符串的多种替代分词方案（如\"penguin\"可分拆为[p,enguin]或[peng,uin]）。通过实验证明，这种对抗性分词技术可以在不修改有害请求文本内容的情况下，有效规避三种前沿LLM的安全对齐限制，且其攻击效果与现有最先进的对抗方法相当。该研究暴露了子词模型(subword models)中一个先前未知的安全脆弱性维度。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02174v1",
      "published_date": "2025-03-04 01:31:17 UTC",
      "updated_date": "2025-03-04 01:31:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:02.329043"
    },
    {
      "arxiv_id": "2503.02172v1",
      "title": "KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering",
      "title_zh": "KGCompiler：面向知识图谱复杂逻辑查询应答的深度学习编译优化",
      "authors": [
        "Hongyu Lin",
        "Haoran Luo",
        "Hanghang Cao",
        "Yang Liu",
        "Shihao Gao",
        "Kaichun Yao",
        "Libo Zhang",
        "Mingjie Xing",
        "Yanjun Wu"
      ],
      "abstract": "Complex Logical Query Answering (CLQA) involves intricate multi-hop logical\nreasoning over large-scale and potentially incomplete Knowledge Graphs (KGs).\nAlthough existing CLQA algorithms achieve high accuracy in answering such\nqueries, their reasoning time and memory usage scale significantly with the\nnumber of First-Order Logic (FOL) operators involved, creating serious\nchallenges for practical deployment. In addition, current research primarily\nfocuses on algorithm-level optimizations for CLQA tasks, often overlooking\ncompiler-level optimizations, which can offer greater generality and\nscalability. To address these limitations, we introduce a Knowledge Graph\nCompiler, namely KGCompiler, the first deep learning compiler specifically\ndesigned for CLQA tasks. By incorporating KG-specific optimizations proposed in\nthis paper, KGCompiler enhances the reasoning performance of CLQA algorithms\nwithout requiring additional manual modifications to their implementations. At\nthe same time, it significantly reduces memory usage. Extensive experiments\ndemonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from\n1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interface\nto enable hands-on experience with KGCompiler.",
      "tldr_zh": "该研究提出了KGCompiler，这是首个专为知识图谱复杂逻辑查询（CLQA）任务设计的深度学习编译器。针对现有CLQA算法在处理大规模、不完整知识图谱时推理时间和内存消耗过高的问题，KGCompiler通过引入知识图谱特有的优化技术，显著提升了推理性能并减少了内存占用。实验表明，KGCompiler将CLQA算法的推理速度平均提升了3.71倍，最高可达8.26倍，同时提供了易于使用的接口，为CLQA任务的部署提供了高效、通用的解决方案。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.02172v1",
      "published_date": "2025-03-04 01:24:32 UTC",
      "updated_date": "2025-03-04 01:24:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:58:58.463583"
    },
    {
      "arxiv_id": "2503.02170v1",
      "title": "Adaptive Camera Sensor for Vision Models",
      "title_zh": "自适应相机传感器用于视觉模型",
      "authors": [
        "Eunsu Baek",
        "Sunghwan Han",
        "Taesik Gong",
        "Hyung-Sin Kim"
      ],
      "abstract": "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com/Edw2n/Lens.git.",
      "tldr_zh": "该研究提出了一种名为Lens的新型自适应相机传感器控制方法，通过模型视角而非传统人眼视角优化图像采集质量。该方法核心是VisiT指标——一种无需训练的质量评估器，能实时调整传感器参数。研究人员还建立了ImageNet-ES Diverse基准数据集验证效果，实验表明Lens能显著提升模型精度，有效补偿模型尺寸差异，且与现有模型改进技术具有协同效应。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The International Conference on Learning Representations (ICLR 2025)",
      "pdf_url": "http://arxiv.org/pdf/2503.02170v1",
      "published_date": "2025-03-04 01:20:23 UTC",
      "updated_date": "2025-03-04 01:20:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:15.485194"
    },
    {
      "arxiv_id": "2503.02157v1",
      "title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models",
      "title_zh": "MedHEval：医疗大型视觉语言模型中的幻觉评估与缓解策略基准",
      "authors": [
        "Aofei Chang",
        "Le Huang",
        "Parminder Bhatia",
        "Taha Kass-Hout",
        "Fenglong Ma",
        "Cao Xiao"
      ],
      "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in\nthe medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate\nhallucinations due to limited expertise and the complexity of medical\napplications. Existing benchmarks fail to effectively evaluate hallucinations\nbased on their underlying causes and lack assessments of mitigation strategies.\nTo address this gap, we introduce MedHEval, a novel benchmark that\nsystematically evaluates hallucinations and mitigation strategies in Med-LVLMs\nby categorizing them into three underlying causes: visual misinterpretation,\nknowledge deficiency, and context misalignment. We construct a diverse set of\nclose- and open-ended medical VQA datasets with comprehensive evaluation\nmetrics to assess these hallucination types. We conduct extensive experiments\nacross 11 popular (Med)-LVLMs and evaluate 7 state-of-the-art hallucination\nmitigation techniques. Results reveal that Med-LVLMs struggle with\nhallucinations arising from different causes while existing mitigation methods\nshow limited effectiveness, especially for knowledge- and context-based errors.\nThese findings underscore the need for improved alignment training and\nspecialized mitigation strategies to enhance Med-LVLMs' reliability. MedHEval\nestablishes a standardized framework for evaluating and mitigating medical\nhallucinations, guiding the development of more trustworthy Med-LVLMs.",
      "tldr_zh": "该研究提出了MedHEval基准测试，系统评估医疗视觉语言模型(Med-LVLMs)中的幻觉问题及其缓解策略。通过将幻觉归因于视觉误判、知识缺失和上下文错位三类根源，研究构建了包含封闭/开放问题的医疗VQA数据集，并对11种主流(Med)-LVLMs和7种先进缓解技术进行测试。结果显示现有方法对知识和上下文类错误效果有限，突显了改进对齐训练和专业缓解策略的必要性。该基准为提升医疗LVLMs可靠性提供了标准化评估框架。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint, under review",
      "pdf_url": "http://arxiv.org/pdf/2503.02157v1",
      "published_date": "2025-03-04 00:40:09 UTC",
      "updated_date": "2025-03-04 00:40:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:17.934562"
    },
    {
      "arxiv_id": "2503.02156v1",
      "title": "MobRFFI: Non-cooperative Device Re-identification for Mobility Intelligence",
      "title_zh": "MobRFFI：面向移动智能的非协作式设备重识别技术",
      "authors": [
        "Stepan Mazokha",
        "Fanchen Bao",
        "George Sklivanitis",
        "Jason O. Hallstrom"
      ],
      "abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
      "tldr_zh": "该研究提出MobRFFI，一种基于AI的非合作式设备重识别框架，用于解决MAC地址随机化导致的移动性监测难题。该方法通过深度学习编码器提取WiFi芯片组硬件缺陷形成的射频指纹特征，在WiSig数据集上实现单日100%、多日94%的设备识别准确率。研究还构建了新型MobRFFI数据集，证明多接收器指纹融合可将多日场景识别率从41%提升至100%，为城市移动智能监测提供了可靠技术方案。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "eess.SP",
      "comment": "10 pages, 9 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.02156v1",
      "published_date": "2025-03-04 00:39:50 UTC",
      "updated_date": "2025-03-04 00:39:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:32.451283"
    },
    {
      "arxiv_id": "2503.02154v1",
      "title": "AugFL: Augmenting Federated Learning with Pretrained Models",
      "title_zh": "AugFL：利用预训练模型增强联邦学习",
      "authors": [
        "Sheng Yue",
        "Zerui Qin",
        "Yongheng Deng",
        "Ju Ren",
        "Yaoxue Zhang",
        "Junshan Zhang"
      ],
      "abstract": "Federated Learning (FL) has garnered widespread interest in recent years.\nHowever, owing to strict privacy policies or limited storage capacities of\ntraining participants such as IoT devices, its effective deployment is often\nimpeded by the scarcity of training data in practical decentralized learning\nenvironments. In this paper, we study enhancing FL with the aid of (large)\npre-trained models (PMs), that encapsulate wealthy general/domain-agnostic\nknowledge, to alleviate the data requirement in conducting FL from scratch.\nSpecifically, we consider a networked FL system formed by a central server and\ndistributed clients. First, we formulate the PM-aided personalized FL as a\nregularization-based federated meta-learning problem, where clients join forces\nto learn a meta-model with knowledge transferred from a private PM stored at\nthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, to\noptimize the problem with no need to expose the PM or incur additional\ncomputational costs to local clients. Further, we establish theoretical\nguarantees for AugFL in terms of communication complexity, adaptation\nperformance, and the benefit of knowledge transfer in general non-convex cases.\nExtensive experiments corroborate the efficacy and superiority of AugFL over\nexisting baselines.",
      "tldr_zh": "本研究提出AugFL，一种利用预训练模型（PMs）增强联邦学习（FL）的方法，旨在解决分布式学习环境中数据稀缺的问题。通过将PM辅助的个性化FL建模为基于正则化的联邦元学习问题，AugFL在无需暴露PM或增加本地客户端计算成本的情况下，优化了学习过程。理论分析证明了AugFL在通信复杂度、适应性能及知识转移收益方面的优势，实验结果表明其优于现有基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "to be published in Transactions on Networking",
      "pdf_url": "http://arxiv.org/pdf/2503.02154v1",
      "published_date": "2025-03-04 00:37:33 UTC",
      "updated_date": "2025-03-04 00:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:34.092192"
    },
    {
      "arxiv_id": "2503.02138v1",
      "title": "Elliptic Loss Regularization",
      "title_zh": "椭圆损失正则化",
      "authors": [
        "Ali Hasan",
        "Haoming Yang",
        "Yuting Ng",
        "Vahid Tarokh"
      ],
      "abstract": "Regularizing neural networks is important for anticipating model behavior in\nregions of the data space that are not well represented. In this work, we\npropose a regularization technique for enforcing a level of smoothness in the\nmapping between the data input space and the loss value. We specify the level\nof regularity by requiring that the loss of the network satisfies an elliptic\noperator over the data domain. To do this, we modify the usual empirical risk\nminimization objective such that we instead minimize a new objective that\nsatisfies an elliptic operator over points within the domain. This allows us to\nuse existing theory on elliptic operators to anticipate the behavior of the\nerror for points outside the training set. We propose a tractable computational\nmethod that approximates the behavior of the elliptic operator while being\ncomputationally efficient. Finally, we analyze the properties of the proposed\nregularization to understand the performance on common problems of distribution\nshift and group imbalance. Numerical experiments confirm the utility of the\nproposed regularization technique.",
      "tldr_zh": "该研究提出了一种新的正则化技术——椭圆损失正则化(Elliptic Loss Regularization)，旨在通过强制网络损失函数在数据域上满足椭圆算子来增强模型在未充分表示数据区域的平滑性。该方法通过修改经验风险最小化目标，使其满足椭圆算子，从而利用椭圆算子理论预测训练集外点的误差行为。研究还提出了一种计算高效的方法来近似椭圆算子行为，并通过数值实验验证了该方法在分布偏移和群体不平衡问题上的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.02138v1",
      "published_date": "2025-03-04 00:08:08 UTC",
      "updated_date": "2025-03-04 00:08:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-03-26T02:59:48.189654"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 141,
  "processed_papers_count": 141,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-03-26T03:01:38.182291"
}
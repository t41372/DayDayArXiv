[
  {
    "arxiv_id": "2503.03062v1",
    "title": "Semi-Supervised In-Context Learning: A Baseline Study",
    "authors": [
      "Zhengyao Gu",
      "Henry Peng Zou",
      "Yankai Chen",
      "Aiwei Liu",
      "Weizhi Zhang",
      "Philip S. Yu"
    ],
    "abstract": "Most existing work in data selection for In-Context Learning (ICL) has\nfocused on constructing demonstrations from ground truth annotations, with\nlimited attention given to selecting reliable self-generated annotations. In\nthis work, we propose a three-step semi-supervised ICL framework: annotation\ngeneration, demonstration selection, and semi-supervised inference. Our\nbaseline, Naive-SemiICL, which prompts select high-confidence self-generated\ndemonstrations for ICL prompting, outperforms a 16-shot baseline by an average\nof 9.94% across 16 datasets. We further introduce IterPSD, an annotation\napproach that refines pseudo-demonstrations iteratively, achieving up to 6.8%\nadditional gains in classification tasks. Lastly, we reveal a scaling law for\nsemi-supervised ICL, where models achieve optimal performance with over 1,000\ndemonstrations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03062v1",
    "published_date": "2025-03-04 23:52:49 UTC",
    "updated_date": "2025-03-04 23:52:49 UTC"
  },
  {
    "arxiv_id": "2503.03783v3",
    "title": "Passive Heart Rate Monitoring During Smartphone Use in Everyday Life",
    "authors": [
      "Shun Liao",
      "Paolo Di Achille",
      "Jiang Wu",
      "Silviu Borac",
      "Jonathan Wang",
      "Xin Liu",
      "Eric Teasley",
      "Lawrence Cai",
      "Yuzhe Yang",
      "Yun Liu",
      "Daniel McDuff",
      "Hao-Wei Su",
      "Brent Winslow",
      "Anupam Pathak",
      "Shwetak Patel",
      "James A. Taylor",
      "Jameson K. Rogers",
      "Ming-Zher Poh"
    ],
    "abstract": "Resting heart rate (RHR) is an important biomarker of cardiovascular health\nand mortality, but tracking it longitudinally generally requires a wearable\ndevice, limiting its availability. We present PHRM, a deep learning system for\npassive heart rate (HR) and RHR measurements during everyday smartphone use,\nusing facial video-based photoplethysmography. Our system was developed using\n225,773 videos from 495 participants and validated on 185,970 videos from 205\nparticipants in laboratory and free-living conditions, representing the largest\nvalidation study of its kind. Compared to reference electrocardiogram, PHRM\nachieved a mean absolute percentage error (MAPE) < 10% for HR measurements\nacross three skin tone groups of light, medium and dark pigmentation; MAPE for\neach skin tone group was non-inferior versus the others. Daily RHR measured by\nPHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and\nwas associated with known risk factors. These results highlight the potential\nof smartphones to enable passive and equitable heart health monitoring.",
    "categories": [
      "q-bio.TO",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "q-bio.TO",
    "comment": "Updated author list",
    "pdf_url": "http://arxiv.org/pdf/2503.03783v3",
    "published_date": "2025-03-04 23:28:10 UTC",
    "updated_date": "2025-03-21 20:09:40 UTC"
  },
  {
    "arxiv_id": "2503.03045v1",
    "title": "ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation",
    "authors": [
      "Yufei Wang",
      "Ziyu Wang",
      "Mino Nakura",
      "Pratik Bhowal",
      "Chia-Liang Kuo",
      "Yi-Ting Chen",
      "Zackory Erickson",
      "David Held"
    ],
    "abstract": "This paper presents ArticuBot, in which a single learned policy enables a\nrobotics system to open diverse categories of unseen articulated objects in the\nreal world. This task has long been challenging for robotics due to the large\nvariations in the geometry, size, and articulation types of such objects. Our\nsystem, Articubot, consists of three parts: generating a large number of\ndemonstrations in physics-based simulation, distilling all generated\ndemonstrations into a point cloud-based neural policy via imitation learning,\nand performing zero-shot sim2real transfer to real robotics systems. Utilizing\nsampling-based grasping and motion planning, our demonstration generalization\npipeline is fast and effective, generating a total of 42.3k demonstrations over\n322 training articulated objects. For policy learning, we propose a novel\nhierarchical policy representation, in which the high-level policy learns the\nsub-goal for the end-effector, and the low-level policy learns how to move the\nend-effector conditioned on the predicted goal. We demonstrate that this\nhierarchical approach achieves much better object-level generalization compared\nto the non-hierarchical version. We further propose a novel weighted\ndisplacement model for the high-level policy that grounds the prediction into\nthe existing 3D structure of the scene, outperforming alternative policy\nrepresentations. We show that our learned policy can zero-shot transfer to\nthree different real robot settings: a fixed table-top Franka arm across two\ndifferent labs, and an X-Arm on a mobile base, opening multiple unseen\narticulated objects across two labs, real lounges, and kitchens. Videos and\ncode can be found on our project website: https://articubot.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03045v1",
    "published_date": "2025-03-04 22:51:50 UTC",
    "updated_date": "2025-03-04 22:51:50 UTC"
  },
  {
    "arxiv_id": "2503.03040v1",
    "title": "SAGE: Steering and Refining Dialog Generation with State-Action Augmentation",
    "authors": [
      "Yizhe Zhang",
      "Navdeep Jaitly"
    ],
    "abstract": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03040v1",
    "published_date": "2025-03-04 22:45:24 UTC",
    "updated_date": "2025-03-04 22:45:24 UTC"
  },
  {
    "arxiv_id": "2503.03039v1",
    "title": "LLM Misalignment via Adversarial RLHF Platforms",
    "authors": [
      "Erfan Entezami",
      "Ali Naseh"
    ],
    "abstract": "Reinforcement learning has shown remarkable performance in aligning language\nmodels with human preferences, leading to the rise of attention towards\ndeveloping RLHF platforms. These platforms enable users to fine-tune models\nwithout requiring any expertise in developing complex machine learning\nalgorithms. While these platforms offer useful features such as reward modeling\nand RLHF fine-tuning, their security and reliability remain largely unexplored.\nGiven the growing adoption of RLHF and open-source RLHF frameworks, we\ninvestigate the trustworthiness of these systems and their potential impact on\nbehavior of LLMs. In this paper, we present an attack targeting publicly\navailable RLHF tools. In our proposed attack, an adversarial RLHF platform\ncorrupts the LLM alignment process by selectively manipulating data samples in\nthe preference dataset. In this scenario, when a user's task aligns with the\nattacker's objective, the platform manipulates a subset of the preference\ndataset that contains samples related to the attacker's target. This\nmanipulation results in a corrupted reward model, which ultimately leads to the\nmisalignment of the language model. Our results demonstrate that such an attack\ncan effectively steer LLMs toward undesirable behaviors within the targeted\ndomains. Our work highlights the critical need to explore the vulnerabilities\nof RLHF platforms and their potential to cause misalignment in LLMs during the\nRLHF fine-tuning process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03039v1",
    "published_date": "2025-03-04 22:38:54 UTC",
    "updated_date": "2025-03-04 22:38:54 UTC"
  },
  {
    "arxiv_id": "2503.04819v1",
    "title": "Technique Inference Engine: A Recommender Model to Support Cyber Threat Hunting",
    "authors": [
      "Matthew J. Turner",
      "Mike Carenzo",
      "Jackie Lasky",
      "James Morris-King",
      "James Ross"
    ],
    "abstract": "Cyber threat hunting is the practice of proactively searching for latent\nthreats in a network. Engaging in threat hunting can be difficult due to the\nvolume of network traffic, variety of adversary techniques, and constantly\nevolving vulnerabilities. To aid analysts in identifying techniques which may\nbe co-occurring as part of a campaign, we present the Technique Inference\nEngine, a tool to infer tactics, techniques, and procedures (TTPs) which may be\nrelated to existing observations of adversarial behavior. We compile the\nlargest (to our knowledge) available dataset of cyber threat intelligence (CTI)\nreports labeled with relevant TTPs. With the knowledge that techniques are\nchronically under-reported in CTI, we apply several implicit feedback\nrecommender models to the data in order to predict additional techniques which\nmay be part of a given campaign. We evaluate the results in the context of the\ncyber analyst's use case and apply t-SNE to visualize the model embeddings. We\nprovide our code and a web interface.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04819v1",
    "published_date": "2025-03-04 22:31:43 UTC",
    "updated_date": "2025-03-04 22:31:43 UTC"
  },
  {
    "arxiv_id": "2503.04818v1",
    "title": "Prompting Science Report 1: Prompt Engineering is Complicated and Contingent",
    "authors": [
      "Lennart Meincke",
      "Ethan Mollick",
      "Lilach Mollick",
      "Dan Shapiro"
    ],
    "abstract": "This is the first of a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we demonstrate two things:\n  - There is no single standard for measuring whether a Large Language Model\n(LLM) passes a benchmark, and that choosing a standard has a big impact on how\nwell the LLM does on that benchmark. The standard you choose will depend on\nyour goals for using an LLM in a particular case.\n  - It is hard to know in advance whether a particular prompting approach will\nhelp or harm the LLM's ability to answer any particular question. Specifically,\nwe find that sometimes being polite to the LLM helps performance, and sometimes\nit lowers performance. We also find that constraining the AI's answers helps\nperformance in some cases, though it may lower performance in other cases.\n  Taken together, this suggests that benchmarking AI performance is not\none-size-fits-all, and also that particular prompting formulas or approaches,\nlike being polite to the AI, are not universally valuable.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04818v1",
    "published_date": "2025-03-04 21:09:12 UTC",
    "updated_date": "2025-03-04 21:09:12 UTC"
  },
  {
    "arxiv_id": "2503.03008v1",
    "title": "One Model to Train them All: Hierarchical Self-Distillation for Enhanced Early Layer Embeddings",
    "authors": [
      "Andrea Gurioli",
      "Federico Pennino",
      "João Monteiro",
      "Maurizio Gabbrielli"
    ],
    "abstract": "Deploying language models often requires handling model size vs. performance\ntrade-offs to satisfy downstream latency constraints while preserving the\nmodel's usefulness. Model distillation is commonly employed to reduce model\nsize while maintaining acceptable performance. However, distillation can be\ninefficient since it involves multiple training steps. In this work, we\nintroduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,\nuseful for multiple tasks within the scope of code retrieval.\nMODULARSTARENCODER is trained with a novel self-distillation mechanism that\nsignificantly improves lower-layer representations-allowing different portions\nof the model to be used while still maintaining a good trade-off in terms of\nperformance. Our architecture focuses on enhancing text-to-code and\ncode-to-code search by systematically capturing syntactic and semantic\nstructures across multiple levels of representation. Specific encoder layers\nare targeted as exit heads, allowing higher layers to guide earlier layers\nduring training. This self-distillation effect improves intermediate\nrepresentations, increasing retrieval recall at no extra training cost. In\naddition to the multi-exit scheme, our approach integrates a repository-level\ncontextual loss that maximally utilizes the training context window, further\nenhancing the learned representations. We also release a new dataset\nconstructed via code translation, seamlessly expanding traditional text-to-code\nbenchmarks with code-to-code pairs across diverse programming languages.\nExperimental results highlight the benefits of self-distillation through\nmulti-exit supervision.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03008v1",
    "published_date": "2025-03-04 21:08:17 UTC",
    "updated_date": "2025-03-04 21:08:17 UTC"
  },
  {
    "arxiv_id": "2503.03779v1",
    "title": "Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds",
    "authors": [
      "Yimin Tang",
      "Zhenghong Yu",
      "Jiaoyang Li",
      "Sven Koenig"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) involves finding collision-free paths for\nmultiple agents while minimizing a cost function--an NP-hard problem. Bounded\nsuboptimal methods like Enhanced Conflict-Based Search (ECBS) and Explicit\nEstimation CBS (EECBS) balance solution quality with computational efficiency\nusing focal search mechanisms. While effective, traditional focal search faces\na limitation: the lower bound (LB) value determining which nodes enter the\nFOCAL list often increases slowly in early search stages, resulting in a\nconstrained search space that delays finding valid solutions. In this paper, we\npropose a novel bounded suboptimal algorithm, double-ECBS (DECBS), to address\nthis issue by first determining the maximum LB value and then employing a\nbest-first search guided by this LB to find a collision-free path. Experimental\nresults demonstrate that DECBS outperforms ECBS in most test cases and is\ncompatible with existing optimization techniques. DECBS can reduce nearly 30%\nhigh-level CT nodes and 50% low-level focal search nodes. When agent density is\nmoderate to high, DECBS achieves a 23.5% average runtime improvement over ECBS\nwith identical suboptimality bounds and optimizations.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.03779v1",
    "published_date": "2025-03-04 20:39:00 UTC",
    "updated_date": "2025-03-04 20:39:00 UTC"
  },
  {
    "arxiv_id": "2503.02992v1",
    "title": "RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding Across Different Environments and Tasks",
    "authors": [
      "Yimin Tang",
      "Xiao Xiong",
      "Jingyi Xi",
      "Jiaoyang Li",
      "Erdem Bıyık",
      "Sven Koenig"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial for applications ranging from aerial\nswarms to warehouse automation. Solving MAPF is NP-hard so learning-based\napproaches for MAPF have gained attention, particularly those leveraging deep\nneural networks. Nonetheless, despite the community's continued efforts, all\nlearning-based MAPF planners still rely on decentralized planning due to\nvariability in the number of agents and map sizes. We have developed the first\ncentralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is\nnot an agent-based policy but a map-based policy. By leveraging a CNN-based\narchitecture, RAILGUN can generalize across different maps and handle any\nnumber of agents. We collect trajectories from rule-based methods to train our\nmodel in a supervised way. In experiments, RAILGUN outperforms most baseline\nmethods and demonstrates great zero-shot generalization capabilities on various\ntasks, maps and agent numbers that were not seen in the training dataset.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.02992v1",
    "published_date": "2025-03-04 20:35:20 UTC",
    "updated_date": "2025-03-04 20:35:20 UTC"
  },
  {
    "arxiv_id": "2503.02989v1",
    "title": "Effectively Steer LLM To Follow Preference via Building Confident Directions",
    "authors": [
      "Bingqing Song",
      "Boran Han",
      "Shuai Zhang",
      "Hao Wang",
      "Haoyang Fang",
      "Bonan Min",
      "Yuyang Wang",
      "Mingyi Hong"
    ],
    "abstract": "Having an LLM that aligns with human preferences is essential for\naccommodating individual needs, such as maintaining writing style or generating\nspecific topics of interest. The majority of current alignment methods rely on\nfine-tuning or prompting, which can be either costly or difficult to control.\nModel steering algorithms, which modify the model output by constructing\nspecific steering directions, are typically easy to implement and\noptimization-free. However, their capabilities are typically limited to\nsteering the model into one of the two directions (i.e., bidirectional\nsteering), and there has been no theoretical understanding to guarantee their\nperformance. In this work, we propose a theoretical framework to understand and\nquantify the model steering methods. Inspired by the framework, we propose a\nconfident direction steering method (CONFST) that steers LLMs via modifying\ntheir activations at inference time. More specifically, CONFST builds a\nconfident direction that is closely aligned with users' preferences, and this\ndirection is then added to the activations of the LLMs to effectively steer the\nmodel output. Our approach offers three key advantages over popular\nbidirectional model steering methods: 1) It is more powerful, since multiple\n(i.e. more than two) users' preferences can be aligned simultaneously; 2) It is\nsimple to implement, since there is no need to determine which layer to add the\nsteering vector to; 3) No explicit user instruction is required. We validate\nour method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks\nthat require shifting the output of LLMs across various topics and styles,\nachieving superior performance over competing methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02989v1",
    "published_date": "2025-03-04 20:32:27 UTC",
    "updated_date": "2025-03-04 20:32:27 UTC"
  },
  {
    "arxiv_id": "2503.04817v1",
    "title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series",
    "authors": [
      "Roberto Balestri",
      "Guglielmo Pescatore"
    ],
    "abstract": "Serialized TV shows are built on complex storylines that can be hard to track\nand evolve in ways that defy straightforward analysis. This paper introduces a\nmulti-agent system designed to extract and analyze these narrative arcs. Tested\non the first season of Grey's Anatomy (ABC 2005-), the system identifies three\ntypes of arcs: Anthology (self-contained), Soap (relationship-focused), and\nGenre-Specific (strictly related to the series' genre). Episodic progressions\nof these arcs are stored in both relational and semantic (vectorial) databases,\nenabling structured analysis and comparison. To bridge the gap between\nautomation and critical interpretation, the system is paired with a graphical\ninterface that allows for human refinement using tools to enhance and visualize\nthe data. The system performed strongly in identifying Anthology Arcs and\ncharacter entities, but its reliance on textual paratexts (such as episode\nsummaries) revealed limitations in recognizing overlapping arcs and subtler\ndynamics. This approach highlights the potential of combining computational and\nhuman expertise in narrative analysis. Beyond television, it offers promise for\nserialized written formats, where the narrative resides entirely in the text.\nFuture work will explore the integration of multimodal inputs, such as dialogue\nand visuals, and expand testing across a wider range of genres to refine the\nsystem further.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "17th International Conference on Agents and Artificial Intelligence,\n  Porto (Portugal). 23/02/2025 - 25/02/2025",
    "pdf_url": "http://arxiv.org/pdf/2503.04817v1",
    "published_date": "2025-03-04 20:27:14 UTC",
    "updated_date": "2025-03-04 20:27:14 UTC"
  },
  {
    "arxiv_id": "2503.03777v1",
    "title": "FlexInfer: Breaking Memory Constraint via Flexible and Efficient Offloading for On-Device LLM Inference",
    "authors": [
      "Hongchao Du",
      "Shangyu Wu",
      "Arina Kharlamova",
      "Nan Guan",
      "Chun Jason Xue"
    ],
    "abstract": "Large Language Models (LLMs) face challenges for on-device inference due to\nhigh memory demands. Traditional methods to reduce memory usage often\ncompromise performance and lack adaptability. We propose FlexInfer, an\noptimized offloading framework for on-device inference, addressing these issues\nwith techniques like asynchronous prefetching, balanced memory locking, and\nflexible tensor preservation. These strategies enhance memory efficiency and\nmitigate I/O bottlenecks, ensuring high performance within user-specified\nresource constraints. Experiments demonstrate that FlexInfer significantly\nimproves throughput under limited resources, achieving up to 12.5 times better\nperformance than existing methods and facilitating the deployment of large\nmodels on resource-constrained devices.",
    "categories": [
      "cs.OS",
      "cs.AI"
    ],
    "primary_category": "cs.OS",
    "comment": "9 pages, 5 figures, to be published in EuroMLSys '25",
    "pdf_url": "http://arxiv.org/pdf/2503.03777v1",
    "published_date": "2025-03-04 20:08:03 UTC",
    "updated_date": "2025-03-04 20:08:03 UTC"
  },
  {
    "arxiv_id": "2503.02976v1",
    "title": "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment",
    "authors": [
      "Matthew DosSantos DiSorbo",
      "Harang Ju",
      "Sinan Aral"
    ],
    "abstract": "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02976v1",
    "published_date": "2025-03-04 20:00:37 UTC",
    "updated_date": "2025-03-04 20:00:37 UTC"
  },
  {
    "arxiv_id": "2503.02972v3",
    "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
    "authors": [
      "Jude Khouja",
      "Karolina Korgul",
      "Simi Hellsten",
      "Lingyi Yang",
      "Vlad Neacsu",
      "Harry Mayne",
      "Ryan Kearns",
      "Andrew Bean",
      "Adam Mahdi"
    ],
    "abstract": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02972v3",
    "published_date": "2025-03-04 19:57:47 UTC",
    "updated_date": "2025-03-07 09:31:42 UTC"
  },
  {
    "arxiv_id": "2503.02969v1",
    "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model",
    "authors": [
      "Siqi Ouyang",
      "Xi Xu",
      "Lei Li"
    ],
    "abstract": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.02969v1",
    "published_date": "2025-03-04 19:51:29 UTC",
    "updated_date": "2025-03-04 19:51:29 UTC"
  },
  {
    "arxiv_id": "2503.02955v1",
    "title": "Monocular visual simultaneous localization and mapping: (r)evolution from geometry to deep learning-based pipelines",
    "authors": [
      "Olaya Alvarez-Tunon",
      "Yury Brodskiy",
      "Erdal Kayacan"
    ],
    "abstract": "With the rise of deep learning, there is a fundamental change in visual SLAM\nalgorithms toward developing different modules trained as end-to-end pipelines.\nHowever, regardless of the implementation domain, visual SLAM's performance is\nsubject to diverse environmental challenges, such as dynamic elements in\noutdoor environments, harsh imaging conditions in underwater environments, or\nblurriness in high-speed setups. These environmental challenges need to be\nidentified to study the real-world viability of SLAM implementations. Motivated\nby the aforementioned challenges, this paper surveys the current state of\nvisual SLAM algorithms according to the two main frameworks: geometry-based and\nlearning-based SLAM. First, we introduce a general formulation of the SLAM\npipeline that includes most of the implementations in the literature. Second,\nthose implementations are classified and surveyed for geometry and\nlearning-based SLAM. After that, environment-specific challenges are formulated\nto enable experimental evaluation of the resilience of different visual SLAM\nclasses to varying imaging conditions. We address two significant issues in\nsurveying visual SLAM, providing (1) a consistent classification of visual SLAM\npipelines and (2) a robust evaluation of their performance under different\ndeployment conditions. Finally, we give our take on future opportunities for\nvisual SLAM implementations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02955v1",
    "published_date": "2025-03-04 19:20:17 UTC",
    "updated_date": "2025-03-04 19:20:17 UTC"
  },
  {
    "arxiv_id": "2503.02954v1",
    "title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders",
    "authors": [
      "Yue Meng",
      "Nathalie Majcherczyk",
      "Wenliang Liu",
      "Scott Kiesel",
      "Chuchu Fan",
      "Federico Pecora"
    ],
    "abstract": "Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by 2025 International Conference on Robotics and Automation\n  (ICRA 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.02954v1",
    "published_date": "2025-03-04 19:20:11 UTC",
    "updated_date": "2025-03-04 19:20:11 UTC"
  },
  {
    "arxiv_id": "2503.02951v1",
    "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
    "authors": [
      "Zhangchen Xu",
      "Yang Liu",
      "Yueqin Yin",
      "Mingyuan Zhou",
      "Radha Poovendran"
    ],
    "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Codes and Data: https://kodcode-ai.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.02951v1",
    "published_date": "2025-03-04 19:17:36 UTC",
    "updated_date": "2025-03-04 19:17:36 UTC"
  },
  {
    "arxiv_id": "2503.02950v1",
    "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
    "authors": [
      "Danqing Zhang",
      "Balaji Rama",
      "Jingyi Ni",
      "Shiying He",
      "Fu Zhao",
      "Kunyu Chen",
      "Arnold Chen",
      "Junyu Cao"
    ],
    "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02950v1",
    "published_date": "2025-03-04 19:13:10 UTC",
    "updated_date": "2025-03-04 19:13:10 UTC"
  },
  {
    "arxiv_id": "2503.02924v1",
    "title": "Diverse Controllable Diffusion Policy with Signal Temporal Logic",
    "authors": [
      "Yue Meng",
      "Chuchu fan"
    ],
    "abstract": "Generating realistic simulations is critical for autonomous system\napplications such as self-driving and human-robot interactions. However,\ndriving simulators nowadays still have difficulty in generating controllable,\ndiverse, and rule-compliant behaviors for road participants: Rule-based models\ncannot produce diverse behaviors and require careful tuning, whereas\nlearning-based methods imitate the policy from data but are not designed to\nfollow the rules explicitly. Besides, the real-world datasets are by nature\n\"single-outcome\", making the learning method hard to generate diverse\nbehaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion\nModels to learn controllable, diverse, and rule-aware policy. We first\ncalibrate the STL on the real-world data, then generate diverse synthetic data\nusing trajectory optimization, and finally learn the rectified diffusion policy\non the augmented dataset. We test on the NuScenes dataset and our approach can\nachieve the most diverse rule-compliant trajectories compared to other\nbaselines, with a runtime 1/17X to the second-best approach. In the closed-loop\ntesting, our approach reaches the highest diversity, rule satisfaction rate,\nand the least collision rate. Our method can generate varied characteristics\nconditional on different STL parameters in testing. A case study on human-robot\nencounter scenarios shows our approach can generate diverse and\nclosed-to-oracle trajectories. The annotation tool, augmented dataset, and code\nare available at https://github.com/mengyuest/pSTL-diffusion-policy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L), October 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.02924v1",
    "published_date": "2025-03-04 18:59:00 UTC",
    "updated_date": "2025-03-04 18:59:00 UTC"
  },
  {
    "arxiv_id": "2503.02882v1",
    "title": "Bringing Comparative Cognition To Computers",
    "authors": [
      "Konstantinos Voudouris",
      "Lucy G. Cheke",
      "Eric Schulz"
    ],
    "abstract": "Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02882v1",
    "published_date": "2025-03-04 18:58:42 UTC",
    "updated_date": "2025-03-04 18:58:42 UTC"
  },
  {
    "arxiv_id": "2503.02881v1",
    "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
    "authors": [
      "Han Xue",
      "Jieji Ren",
      "Wendi Chen",
      "Gu Zhang",
      "Yuan Fang",
      "Guoying Gu",
      "Huazhe Xu",
      "Cewu Lu"
    ],
    "abstract": "Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as quick adjustments to environmental changes\nand adaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines through\nrapid response to tactile / force feedback. Furthermore, experiments show that\nRDP is applicable across different tactile / force sensors. Code and videos are\navailable on https://reactive-diffusion-policy.github.io/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02881v1",
    "published_date": "2025-03-04 18:58:21 UTC",
    "updated_date": "2025-03-04 18:58:21 UTC"
  },
  {
    "arxiv_id": "2503.02879v1",
    "title": "Wikipedia in the Era of LLMs: Evolution and Risks",
    "authors": [
      "Siming Huang",
      "Yuliang Xu",
      "Mingmeng Geng",
      "Yao Wan",
      "Dongping Chen"
    ],
    "abstract": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia",
    "pdf_url": "http://arxiv.org/pdf/2503.02879v1",
    "published_date": "2025-03-04 18:58:13 UTC",
    "updated_date": "2025-03-04 18:58:13 UTC"
  },
  {
    "arxiv_id": "2503.02878v1",
    "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
    "authors": [
      "Ethan Mendes",
      "Alan Ritter"
    ],
    "abstract": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02878v1",
    "published_date": "2025-03-04 18:58:11 UTC",
    "updated_date": "2025-03-04 18:58:11 UTC"
  },
  {
    "arxiv_id": "2503.02861v1",
    "title": "Evaluation of Architectural Synthesis Using Generative AI",
    "authors": [
      "Jingfei Huang",
      "Alexandros Haridis"
    ],
    "abstract": "Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02861v1",
    "published_date": "2025-03-04 18:39:28 UTC",
    "updated_date": "2025-03-04 18:39:28 UTC"
  },
  {
    "arxiv_id": "2503.02857v3",
    "title": "Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024",
    "authors": [
      "Nuria Alina Chandra",
      "Ryan Murtfeldt",
      "Lin Qiu",
      "Arnab Karmakar",
      "Hannah Lee",
      "Emmanuel Tanumihardja",
      "Kevin Farhat",
      "Ben Caffee",
      "Sejin Paik",
      "Changyeon Lee",
      "Jongwook Choi",
      "Aerin Kim",
      "Oren Etzioni"
    ],
    "abstract": "In the age of increasingly realistic generative AI, robust deepfake detection\nis essential for mitigating fraud and disinformation. While many deepfake\ndetectors report high accuracy on academic datasets, we show that these\nacademic benchmarks are out of date and not representative of real-world\ndeepfakes. We introduce Deepfake-Eval-2024, a new deepfake detection benchmark\nconsisting of in-the-wild deepfakes collected from social media and deepfake\ndetection platform users in 2024. Deepfake-Eval-2024 consists of 45 hours of\nvideos, 56.5 hours of audio, and 1,975 images, encompassing the latest\nmanipulation technologies. The benchmark contains diverse media content from 88\ndifferent websites in 52 different languages. We find that the performance of\nopen-source state-of-the-art deepfake detection models drops precipitously when\nevaluated on Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for\naudio, and 45% for image models compared to previous benchmarks. We also\nevaluate commercial deepfake detection models and models finetuned on\nDeepfake-Eval-2024, and find that they have superior performance to\noff-the-shelf open-source models, but do not yet reach the accuracy of deepfake\nforensic analysts. The dataset is available at\nhttps://github.com/nuriachandra/Deepfake-Eval-2024.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02857v3",
    "published_date": "2025-03-04 18:33:22 UTC",
    "updated_date": "2025-03-24 20:46:15 UTC"
  },
  {
    "arxiv_id": "2503.02854v2",
    "title": "(How) Do Language Models Track State?",
    "authors": [
      "Belinda Z. Li",
      "Zifan Carl Guo",
      "Jacob Andreas"
    ],
    "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "21 pages, 17 figures, 1 table. Code:\n  http://github.com/belindal/state-tracking",
    "pdf_url": "http://arxiv.org/pdf/2503.02854v2",
    "published_date": "2025-03-04 18:31:02 UTC",
    "updated_date": "2025-03-11 15:36:40 UTC"
  },
  {
    "arxiv_id": "2503.02849v1",
    "title": "Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data",
    "authors": [
      "Amin Honarmandi Shandiz"
    ],
    "abstract": "Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02849v1",
    "published_date": "2025-03-04 18:24:33 UTC",
    "updated_date": "2025-03-04 18:24:33 UTC"
  },
  {
    "arxiv_id": "2503.02836v1",
    "title": "SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot Time-Series Forecasting",
    "authors": [
      "Ting-Ji Huang",
      "Xu-Yang Chen",
      "Han-Jia Ye"
    ],
    "abstract": "Unlike traditional time-series forecasting methods that require extensive\nin-task data for training, zero-shot forecasting can directly predict future\nvalues given a target time series without additional training data. Current\nzero-shot approaches primarily rely on pre-trained generalized models, with\ntheir performance often depending on the variety and relevance of the\npre-training data, which can raise privacy concerns. Instead of collecting\ndiverse pre-training data, we introduce SeqFusion in this work, a novel\nframework that collects and fuses diverse pre-trained models (PTMs)\nsequentially for zero-shot forecasting. Based on the specific temporal\ncharacteristics of the target time series, SeqFusion selects the most suitable\nPTMs from a batch of pre-collected PTMs, performs sequential predictions, and\nfuses all the predictions while using minimal data to protect privacy. Each of\nthese PTMs specializes in different temporal patterns and forecasting tasks,\nallowing SeqFusion to select by measuring distances in a shared representation\nspace of the target time series with each PTM. Experiments demonstrate that\nSeqFusion achieves competitive accuracy in zero-shot forecasting compared to\nstate-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02836v1",
    "published_date": "2025-03-04 17:59:17 UTC",
    "updated_date": "2025-03-04 17:59:17 UTC"
  },
  {
    "arxiv_id": "2503.02832v1",
    "title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation",
    "authors": [
      "Songming Zhang",
      "Xue Zhang",
      "Tong Zhang",
      "Bojie Hu",
      "Yufeng Chen",
      "Jinan Xu"
    ],
    "abstract": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02832v1",
    "published_date": "2025-03-04 17:57:09 UTC",
    "updated_date": "2025-03-04 17:57:09 UTC"
  },
  {
    "arxiv_id": "2503.02824v1",
    "title": "Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging",
    "authors": [
      "Yujin Oh",
      "Robert Seifert",
      "Yihan Cao",
      "Christoph Clement",
      "Justin Ferdinandus",
      "Constantin Lapa",
      "Alessandro Liebich",
      "Michelle Amon",
      "Johanna Enke",
      "Sifan Song",
      "Runqi Meng",
      "Fang Zeng",
      "Ning Guo",
      "Xiang Li",
      "Pedram Heidari",
      "Axel Rominger",
      "Kuangyu Shi",
      "Quanzheng Li"
    ],
    "abstract": "In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 2 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.02824v1",
    "published_date": "2025-03-04 17:49:07 UTC",
    "updated_date": "2025-03-04 17:49:07 UTC"
  },
  {
    "arxiv_id": "2503.02823v1",
    "title": "A Multimodal Symphony: Integrating Taste and Sound through Generative AI",
    "authors": [
      "Matteo Spanio",
      "Massimiliano Zampini",
      "Antonio Rodà",
      "Franco Pierucci"
    ],
    "abstract": "In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS",
      "I.2.6; J.5"
    ],
    "primary_category": "cs.SD",
    "comment": "17 pages, 6 figures (2 + 2 figures with 2 subfigures each)",
    "pdf_url": "http://arxiv.org/pdf/2503.02823v1",
    "published_date": "2025-03-04 17:48:48 UTC",
    "updated_date": "2025-03-04 17:48:48 UTC"
  },
  {
    "arxiv_id": "2503.02812v1",
    "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
    "authors": [
      "Nathan Godey",
      "Alessio Devoto",
      "Yu Zhao",
      "Simone Scardapane",
      "Pasquale Minervini",
      "Éric de la Clergerie",
      "Benoît Sagot"
    ],
    "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02812v1",
    "published_date": "2025-03-04 17:37:49 UTC",
    "updated_date": "2025-03-04 17:37:49 UTC"
  },
  {
    "arxiv_id": "2503.02797v1",
    "title": "A Causal Framework for Aligning Image Quality Metrics and Deep Neural Network Robustness",
    "authors": [
      "Nathan Drenkow",
      "Mathias Unberath"
    ],
    "abstract": "Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02797v1",
    "published_date": "2025-03-04 17:15:31 UTC",
    "updated_date": "2025-03-04 17:15:31 UTC"
  },
  {
    "arxiv_id": "2503.02784v3",
    "title": "Do Not Trust Licenses You See: Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing",
    "authors": [
      "Jaekyeom Kim",
      "Sungryull Sohn",
      "Gerrard Jeongwon Jo",
      "Jihoon Choi",
      "Kyunghoon Bae",
      "Hwayoung Lee",
      "Yongmin Park",
      "Honglak Lee"
    ],
    "abstract": "This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02784v3",
    "published_date": "2025-03-04 16:57:53 UTC",
    "updated_date": "2025-03-14 16:58:30 UTC"
  },
  {
    "arxiv_id": "2503.02783v2",
    "title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
    "authors": [
      "Jie Wu",
      "Haoling Li",
      "Xin Zhang",
      "Jianwen Luo",
      "Yangyu Huang",
      "Ruihang Chu",
      "Yujiu Yang",
      "Scarlett Li"
    ],
    "abstract": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The code and data will be released soon",
    "pdf_url": "http://arxiv.org/pdf/2503.02783v2",
    "published_date": "2025-03-04 16:56:34 UTC",
    "updated_date": "2025-03-10 18:08:16 UTC"
  },
  {
    "arxiv_id": "2503.05816v1",
    "title": "Will Neural Scaling Laws Activate Jevons' Paradox in AI Labor Markets? A Time-Varying Elasticity of Substitution (VES) Analysis",
    "authors": [
      "Rajesh P. Narayanan",
      "R. Kelley Pace"
    ],
    "abstract": "AI industry leaders often use the term ``Jevons' Paradox.'' We explore the\nsignificance of this term for artificial intelligence adoption through a\ntime-varying elasticity of substitution framework. We develop a model\nconnecting AI development to labor substitution through four key mechanisms:\n(1) increased effective computational capacity from both hardware and\nalgorithmic improvements; (2) AI capabilities that rise logarithmically with\ncomputation following established neural scaling laws; (3) declining marginal\ncomputational costs leading to lower AI prices through competitive pressure;\nand (4) a resulting increase in the elasticity of substitution between AI and\nhuman labor over time. Our time-varying elasticity of substitution (VES)\nframework, incorporating the G\\o rtz identity, yields analytical conditions for\nmarket transformation dynamics. This work provides a simple framework to help\nassess the economic reasoning behind industry claims that AI will increasingly\nsubstitute for human labor across diverse economic sectors.",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "q-fin.EC",
      "I.2.m; J.4"
    ],
    "primary_category": "econ.GN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05816v1",
    "published_date": "2025-03-04 16:55:30 UTC",
    "updated_date": "2025-03-04 16:55:30 UTC"
  },
  {
    "arxiv_id": "2503.02781v1",
    "title": "Multimodal AI predicts clinical outcomes of drug combinations from preclinical data",
    "authors": [
      "Yepeng Huang",
      "Xiaorui Su",
      "Varun Ullanat",
      "Ivy Liang",
      "Lindsay Clegg",
      "Damilola Olabode",
      "Nicholas Ho",
      "Bino John",
      "Megan Gibbs",
      "Marinka Zitnik"
    ],
    "abstract": "Predicting clinical outcomes from preclinical data is essential for\nidentifying safe and effective drug combinations. Current models rely on\nstructural or target-based features to identify high-efficacy, low-toxicity\ndrug combinations. However, these approaches fail to incorporate the multimodal\ndata necessary for accurate, clinically-relevant predictions. Here, we\nintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,\ncell viability, and transcriptomic data to predict drug combination effects\nacross 953 clinical outcomes and 21842 compounds, including combinations of\napproved drugs and novel compounds in development. MADRIGAL uses a transformer\nbottleneck module to unify preclinical drug data modalities while handling\nmissing data during training and inference--a major challenge in multimodal\nlearning. It outperforms single-modality methods and state-of-the-art models in\npredicting adverse drug interactions. MADRIGAL performs virtual screening of\nanticancer drug combinations and supports polypharmacy management for type II\ndiabetes and metabolic dysfunction-associated steatohepatitis (MASH). It\nidentifies transporter-mediated drug interactions. MADRIGAL predicts\nresmetirom, the first and only FDA-approved drug for MASH, among therapies with\nthe most favorable safety profile. It supports personalized cancer therapy by\nintegrating genomic profiles from cancer patients. Using primary acute myeloid\nleukemia samples and patient-derived xenograft models, it predicts the efficacy\nof personalized drug combinations. Integrating MADRIGAL with a large language\nmodel allows users to describe clinical outcomes in natural language, improving\nsafety assessment by identifying potential adverse interactions and toxicity\nrisks. MADRIGAL provides a multimodal approach for designing combination\ntherapies with improved predictive accuracy and clinical relevance.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02781v1",
    "published_date": "2025-03-04 16:55:14 UTC",
    "updated_date": "2025-03-04 16:55:14 UTC"
  },
  {
    "arxiv_id": "2503.02776v1",
    "title": "Implicit Bias in LLMs: A Survey",
    "authors": [
      "Xinru Lin",
      "Luyang Li"
    ],
    "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02776v1",
    "published_date": "2025-03-04 16:49:37 UTC",
    "updated_date": "2025-03-04 16:49:37 UTC"
  },
  {
    "arxiv_id": "2503.02773v1",
    "title": "Prime Convolutional Model: Breaking the Ground for Theoretical Explainability",
    "authors": [
      "Francesco Panelli",
      "Doaa Almhaithawi",
      "Tania Cerquitelli",
      "Alessandro Bellini"
    ],
    "abstract": "In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02773v1",
    "published_date": "2025-03-04 16:42:46 UTC",
    "updated_date": "2025-03-04 16:42:46 UTC"
  },
  {
    "arxiv_id": "2503.02749v1",
    "title": "Improving Oil Slick Trajectory Simulations with Bayesian Optimization",
    "authors": [
      "Gabriele Accarino",
      "Marco M. De Carlo",
      "Igor Atake",
      "Donatello Elia",
      "Anusha L. Dissanayake",
      "Antonio Augusto Sepp Neves",
      "Juan Peña Ibañez",
      "Italo Epicoco",
      "Paola Nassisi",
      "Sandro Fiore",
      "Giovanni Coppini"
    ],
    "abstract": "Accurate simulations of oil spill trajectories are essential for supporting\npractitioners' response and mitigating environmental and socioeconomic impacts.\nNumerical models, such as MEDSLIK-II, simulate advection, dispersion, and\ntransformation processes of oil particles. However, simulations heavily rely on\naccurate parameter tuning, still based on expert knowledge and manual\ncalibration. To overcome these limitations, we integrate the MEDSLIK-II\nnumerical oil spill model with a Bayesian optimization framework to iteratively\nestimate the best physical parameter configuration that yields simulation\ncloser to satellite observations of the slick. We focus on key parameters, such\nas horizontal diffusivity and drift factor, maximizing the Fraction Skill Score\n(FSS) as a measure of spatio-temporal overlap between simulated and observed\noil distributions. We validate the framework for the Baniyas oil incident that\noccurred in Syria between August 23 and September 4, 2021, which released over\n12,000 $m^3$ of oil. We show that, on average, the proposed approach\nsystematically improves the FSS from 5.82% to 11.07% compared to control\nsimulations initialized with default parameters. The optimization results in\nconsistent improvement across multiple time steps, particularly during periods\nof increased drift variability, demonstrating the robustness of our method in\ndynamic environmental conditions.",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "I.2; I.6; J.2; G.3"
    ],
    "primary_category": "physics.ao-ph",
    "comment": "29 pages, 10 figures, 3 tables, research paper",
    "pdf_url": "http://arxiv.org/pdf/2503.02749v1",
    "published_date": "2025-03-04 16:14:16 UTC",
    "updated_date": "2025-03-04 16:14:16 UTC"
  },
  {
    "arxiv_id": "2503.05815v1",
    "title": "Trust, Experience, and Innovation: Key Factors Shaping American Attitudes About AI",
    "authors": [
      "Risa Palm",
      "Justin Kingsland",
      "Toby Bolsen"
    ],
    "abstract": "A large survey of American adults explored the complex landscape of attitudes\ntowards artificial intelligence (AI). It explored the degree of concern\nregarding specific potential outcomes of the new advances in AI technology and\ncorrelates of these concerns. Key variables associated with the direction and\nintensity of concern include prior experience using a large language model such\nas ChatGPT, general trust in science, adherence to the precautionary principle\nversus support for unrestricted innovation, and demographic factors such as\ngender. By analyzing these relationships, the paper provides valuable insights\ninto the American public's response to AI that are particularly important in\nthe development of policy to regulate or further encourage its development.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "35 pages, 3 figures, 2 tables, appendix",
    "pdf_url": "http://arxiv.org/pdf/2503.05815v1",
    "published_date": "2025-03-04 16:08:20 UTC",
    "updated_date": "2025-03-04 16:08:20 UTC"
  },
  {
    "arxiv_id": "2503.02733v1",
    "title": "UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural Video Compression",
    "authors": [
      "Jia Wang",
      "Xinfeng Zhang",
      "Gai Zhang",
      "Jun Zhu",
      "Lv Tang",
      "Li Zhang"
    ],
    "abstract": "Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02733v1",
    "published_date": "2025-03-04 15:54:57 UTC",
    "updated_date": "2025-03-04 15:54:57 UTC"
  },
  {
    "arxiv_id": "2503.02720v1",
    "title": "Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation Efficiency",
    "authors": [
      "Myeongbo Park",
      "Chunggil An",
      "Junhyun Park",
      "Jonghyun Kang",
      "Minho Hwang"
    ],
    "abstract": "Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.02720v1",
    "published_date": "2025-03-04 15:36:19 UTC",
    "updated_date": "2025-03-04 15:36:19 UTC"
  },
  {
    "arxiv_id": "2503.04814v1",
    "title": "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings for Phonetic Analysis",
    "authors": [
      "Yiming Wang",
      "Yi Yang",
      "Jiahong Yuan"
    ],
    "abstract": "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04814v1",
    "published_date": "2025-03-04 15:28:10 UTC",
    "updated_date": "2025-03-04 15:28:10 UTC"
  },
  {
    "arxiv_id": "2503.13476v1",
    "title": "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
    "authors": [
      "Edward Gunn",
      "Adam Hosford",
      "Daniel Mannion",
      "Jarrod Williams",
      "Varun Chhabra",
      "Victoria Nockles"
    ],
    "abstract": "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "Preprint: Accepted to IEEE International Radar Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.13476v1",
    "published_date": "2025-03-04 15:27:17 UTC",
    "updated_date": "2025-03-04 15:27:17 UTC"
  },
  {
    "arxiv_id": "2503.02703v1",
    "title": "Generative Tools for Graphical Assets: Empirical Guidelines based on Game Designers' and Developers' Preferences",
    "authors": [
      "Kaisei Fukaya",
      "Damon Daylamani-Zad",
      "Harry Agius"
    ],
    "abstract": "Graphical assets play an important role in the design and development of\ngames. There is potential in the use of generative tools, to aid in creating\ngraphical assets, thus improving game design and development pipelines.\nHowever, there is little research to address how the generative methods can fit\ninto the wider pipeline. We conducted a user study with 16 game designers and\ndevelopers to examine their preferences regarding generative tools for\ngraphical assets. The findings highlight that early design stage is preferred\nby all participants (mean values above 0.67 and p < .001 for early stages).\nDesigners and developers prefer to use such tools for creating large amounts of\nvariations at the cost of quality as they can improve the quality of the\nartefacts once they generate a suitable asset (mean value 0.17 where 1 is high\nquality, p < .001). They also strongly (mean value .78, p < .001) raised the\nneed for better integration of such tools in existing design and development\nenvironments and the need for the outputs to be in common data formats, to be\nmanipulatable and integrate smoothly into existing environments (mean 3.5 out\nof 5, p = .004). The study also highlights the requirement for further emphasis\non the needs of the users to incorporate these tools effectively in existing\npipelines. Informed by these results, we provide a set of guidelines for\ncreating tools that meet the expectations and needs of game designers and\ndevelopers.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02703v1",
    "published_date": "2025-03-04 15:18:50 UTC",
    "updated_date": "2025-03-04 15:18:50 UTC"
  },
  {
    "arxiv_id": "2503.02701v1",
    "title": "MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality",
    "authors": [
      "Shuaike Li",
      "Kai Zhang",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02701v1",
    "published_date": "2025-03-04 15:17:57 UTC",
    "updated_date": "2025-03-04 15:17:57 UTC"
  },
  {
    "arxiv_id": "2503.02691v1",
    "title": "Memory Efficient Continual Learning for Edge-Based Visual Anomaly Detection",
    "authors": [
      "Manuel Barusco",
      "Lorenzo D'Antoni",
      "Davide Dalle Pezze",
      "Francesco Borsatti",
      "Gian Antonio Susto"
    ],
    "abstract": "Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02691v1",
    "published_date": "2025-03-04 15:03:47 UTC",
    "updated_date": "2025-03-04 15:03:47 UTC"
  },
  {
    "arxiv_id": "2503.02687v1",
    "title": "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?",
    "authors": [
      "Miao Zhang",
      "Sherif Abdulatif",
      "Benedikt Loesch",
      "Marco Altmann",
      "Bin Yang"
    ],
    "abstract": "Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.02687v1",
    "published_date": "2025-03-04 15:02:07 UTC",
    "updated_date": "2025-03-04 15:02:07 UTC"
  },
  {
    "arxiv_id": "2503.02686v1",
    "title": "Seeding for Success: Skill and Stochasticity in Tabletop Games",
    "authors": [
      "James Goodman",
      "Diego Perez-Liebana",
      "Simon Lucas"
    ],
    "abstract": "Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in IEEE Transactions on Games, 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02686v1",
    "published_date": "2025-03-04 14:58:59 UTC",
    "updated_date": "2025-03-04 14:58:59 UTC"
  },
  {
    "arxiv_id": "2503.02682v1",
    "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
    "authors": [
      "Weimin Xiong",
      "Yifan Song",
      "Qingxiu Dong",
      "Bingchan Zhao",
      "Feifan Song",
      "Xun Wang",
      "Sujian Li"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02682v1",
    "published_date": "2025-03-04 14:54:45 UTC",
    "updated_date": "2025-03-04 14:54:45 UTC"
  },
  {
    "arxiv_id": "2503.02675v1",
    "title": "State of play and future directions in industrial computer vision AI standards",
    "authors": [
      "Artemis Stefanidou",
      "Panagiotis Radoglou-Grammatikis",
      "Vasileios Argyriou",
      "Panagiotis Sarigiannidis",
      "Iraklis Varlamis",
      "Georgios Th. Papadopoulos"
    ],
    "abstract": "The recent tremendous advancements in the areas of Artificial Intelligence\n(AI) and Deep Learning (DL) have also resulted into corresponding remarkable\nprogress in the field of Computer Vision (CV), showcasing robust technological\nsolutions in a wide range of application sectors of high industrial interest\n(e.g., healthcare, autonomous driving, automation, etc.). Despite the\noutstanding performance of CV systems in specific domains, their development\nand exploitation at industrial-scale necessitates, among other, the addressing\nof requirements related to the reliability, transparency, trustworthiness,\nsecurity, safety, and robustness of the developed AI models. The latter raises\nthe imperative need for the development of efficient, comprehensive and\nwidely-adopted industrial standards. In this context, this study investigates\nthe current state of play regarding the development of industrial computer\nvision AI standards, emphasizing on critical aspects, like model\ninterpretability, data quality, and regulatory compliance. In particular, a\nsystematic analysis of launched and currently developing CV standards, proposed\nby the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,\netc.) is performed. The latter is complemented by a comprehensive discussion on\nthe current challenges and future directions observed in this regularization\nendeavor.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02675v1",
    "published_date": "2025-03-04 14:46:34 UTC",
    "updated_date": "2025-03-04 14:46:34 UTC"
  },
  {
    "arxiv_id": "2503.04813v1",
    "title": "Self-Evolved Preference Optimization for Enhancing Mathematical Reasoning in Small Language Models",
    "authors": [
      "Joykirat Singh",
      "Tanmoy Chakraborty",
      "Akshay Nambi"
    ],
    "abstract": "Large language models (LLMs) have significantly improved their reasoning\ncapabilities; however, they still struggle with complex multi-step mathematical\nproblem-solving due to error propagation, lack of self-correction, and limited\nadaptability to diverse reasoning styles. Existing methods rely on static\nfine-tuning or prompt engineering, which fail to generalize across problem\ncomplexities, while the scarcity of high-quality preference data further\nhinders reliable reasoning.\n  We introduce SPHERE, a self-evolving data generation pipeline that enhances\nreasoning in small language models (SLMs) by iteratively generating,\ncorrecting, and diversifying reasoning chains. SPHERE operates in three stages:\n(i) Self-Generation, where the model autonomously constructs problem-solving\nsteps; (ii) Self-Correction, enabling it to identify and rectify errors; and\n(iii) Diversity Induction, improving robustness through multiple valid\nreasoning trajectories. This self-evolution mechanism strengthens mathematical\nreasoning and enhances model reliability. Evaluations on MATH 500, GSM8K, AIME,\nAMC, and Olympiad show that SPHERE-trained models achieve significant gains\nover their base versions and match/surpass GPT-4o on certain benchmarks. Our\nfindings demonstrate that self-evolving models can close the reasoning gap\nbetween SLMs and state-of-the-art LLMs, making mathematical AI more reliable,\nscalable, and efficient.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04813v1",
    "published_date": "2025-03-04 14:43:25 UTC",
    "updated_date": "2025-03-04 14:43:25 UTC"
  },
  {
    "arxiv_id": "2503.02660v2",
    "title": "A dataset-free approach for self-supervised learning of 3D reflectional symmetries",
    "authors": [
      "Isaac Aguirre",
      "Ivan Sipiran",
      "Gabriel Montañana"
    ],
    "abstract": "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02660v2",
    "published_date": "2025-03-04 14:22:08 UTC",
    "updated_date": "2025-03-05 19:36:48 UTC"
  },
  {
    "arxiv_id": "2503.02650v1",
    "title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats",
    "authors": [
      "William Brach",
      "Kristián Košťál",
      "Michal Ries"
    ],
    "abstract": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02650v1",
    "published_date": "2025-03-04 14:14:28 UTC",
    "updated_date": "2025-03-04 14:14:28 UTC"
  },
  {
    "arxiv_id": "2503.02636v1",
    "title": "YARE-GAN: Yet Another Resting State EEG-GAN",
    "authors": [
      "Yeganeh Farahzadi",
      "Morteza Ansarinia",
      "Zoltan Kekecs"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have shown promise in synthesising\nrealistic neural data, yet their potential for unsupervised representation\nlearning in resting-state EEG remains under explored. In this study, we\nimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate\nmulti-channel resting-state EEG data and assess the quality of the synthesised\nsignals through both visual and feature-based evaluations. Our results indicate\nthat the model effectively captures the statistical and spectral\ncharacteristics of real EEG data, although challenges remain in replicating\nhigh-frequency oscillations in the frontal region. Additionally, we demonstrate\nthat the Critic's learned representations can be fine-tuned for age group\nclassification, achieving an out-of-sample accuracy, significantly better than\na shuffled-label baseline. These findings suggest that generative models can\nserve not only as EEG data generators but also as unsupervised feature\nextractors, reducing the need for manual feature engineering. This study\nhighlights the potential of GAN-based unsupervised learning for EEG analysis,\nsuggesting avenues for more data-efficient deep learning applications in\nneuroscience.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02636v1",
    "published_date": "2025-03-04 14:01:10 UTC",
    "updated_date": "2025-03-04 14:01:10 UTC"
  },
  {
    "arxiv_id": "2503.02631v1",
    "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
    "authors": [
      "Haotian Li",
      "Yun Wang",
      "Huamin Qu"
    ],
    "abstract": "Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements",
    "pdf_url": "http://arxiv.org/pdf/2503.02631v1",
    "published_date": "2025-03-04 13:56:18 UTC",
    "updated_date": "2025-03-04 13:56:18 UTC"
  },
  {
    "arxiv_id": "2503.02628v1",
    "title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction",
    "authors": [
      "Wenxuan Liu",
      "Zixuan Li",
      "Long Bai",
      "Yuxin Zuo",
      "Daozhu Xu",
      "Xiaolong Jin",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "abstract": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2503.02628v1",
    "published_date": "2025-03-04 13:53:43 UTC",
    "updated_date": "2025-03-04 13:53:43 UTC"
  },
  {
    "arxiv_id": "2503.02623v2",
    "title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models",
    "authors": [
      "Paul Stangel",
      "David Bani-Harouni",
      "Chantal Pellegrini",
      "Ege Özsoy",
      "Kamilia Zaripova",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02623v2",
    "published_date": "2025-03-04 13:48:50 UTC",
    "updated_date": "2025-03-05 15:23:16 UTC"
  },
  {
    "arxiv_id": "2503.03775v1",
    "title": "BotUmc: An Uncertainty-Aware Twitter Bot Detection with Multi-view Causal Inference",
    "authors": [
      "Tao Yang",
      "Yang Hu",
      "Feihong Lu",
      "Ziwei Zhang",
      "Qingyun Sun",
      "Jianxin Li"
    ],
    "abstract": "Social bots have become widely known by users of social platforms. To prevent\nsocial bots from spreading harmful speech, many novel bot detections are\nproposed. However, with the evolution of social bots, detection methods\nstruggle to give high-confidence answers for samples. This motivates us to\nquantify the uncertainty of the outputs, informing the confidence of the\nresults. Therefore, we propose an uncertainty-aware bot detection method to\ninform the confidence and use the uncertainty score to pick a high-confidence\ndecision from multiple views of a social network under different environments.\nSpecifically, our proposed BotUmc uses LLM to extract information from tweets.\nThen, we construct a graph based on the extracted information, the original\nuser information, and the user relationship and generate multiple views of the\ngraph by causal interference. Lastly, an uncertainty loss is used to force the\nmodel to quantify the uncertainty of results and select the result with low\nuncertainty in one view as the final decision. Extensive experiments show the\nsuperiority of our method.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.03775v1",
    "published_date": "2025-03-04 13:39:31 UTC",
    "updated_date": "2025-03-04 13:39:31 UTC"
  },
  {
    "arxiv_id": "2503.02612v1",
    "title": "Reinforcement Learning-based Threat Assessment",
    "authors": [
      "Wuzhou Sun",
      "Siyi Li",
      "Qingxiang Zou",
      "Zixing Liao"
    ],
    "abstract": "In some game scenarios, due to the uncertainty of the number of enemy units\nand the priority of various attributes, the evaluation of the threat level of\nenemy units as well as the screening has been a challenging research topic, and\nthe core difficulty lies in how to reasonably set the priority of different\nattributes in order to achieve quantitative evaluation of the threat. In this\npaper, we innovatively transform the problem of threat assessment into a\nreinforcement learning problem, and through systematic reinforcement learning\ntraining, we successfully construct an efficient neural network evaluator. The\nevaluator can not only comprehensively integrate the multidimensional attribute\nfeatures of the enemy, but also effectively combine our state information, thus\nrealizing a more accurate and scientific threat assessment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02612v1",
    "published_date": "2025-03-04 13:32:40 UTC",
    "updated_date": "2025-03-04 13:32:40 UTC"
  },
  {
    "arxiv_id": "2503.02918v1",
    "title": "Straight-Line Diffusion Model for Efficient 3D Molecular Generation",
    "authors": [
      "Yuyan Ni",
      "Shikun Feng",
      "Haohan Chi",
      "Bowen Zheng",
      "Huan-ang Gao",
      "Wei-Ying Ma",
      "Zhi-Ming Ma",
      "Yanyan Lan"
    ],
    "abstract": "Diffusion-based models have shown great promise in molecular generation but\noften require a large number of sampling steps to generate valid samples. In\nthis paper, we introduce a novel Straight-Line Diffusion Model (SLDM) to tackle\nthis problem, by formulating the diffusion process to follow a linear\ntrajectory. The proposed process aligns well with the noise sensitivity\ncharacteristic of molecular structures and uniformly distributes reconstruction\neffort across the generative process, thus enhancing learning efficiency and\nefficacy. Consequently, SLDM achieves state-of-the-art performance on 3D\nmolecule generation benchmarks, delivering a 100-fold improvement in sampling\nefficiency. Furthermore, experiments on toy data and image generation tasks\nvalidate the generality and robustness of SLDM, showcasing its potential across\ndiverse generative modeling domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02918v1",
    "published_date": "2025-03-04 13:23:58 UTC",
    "updated_date": "2025-03-04 13:23:58 UTC"
  },
  {
    "arxiv_id": "2503.02597v2",
    "title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs",
    "authors": [
      "Wei-Yao Wang",
      "Zhao Wang",
      "Helen Suzuki",
      "Yoshiyuki Kobayashi"
    ],
    "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of the\nearlier modalities (e.g., images) to incorporate information from the latter\nmodalities (e.g., text). To address this problem, we propose \\MapleLeaf AKI, a\nnovel MLLM that unlocks causal attention into modality-mutual attention (MMA)\nto enable image tokens to attend to text tokens. This simple yet effective\ndesign allows AKI to achieve superior performance in 12 multimodal\nunderstanding benchmarks (+7.2% on average) without introducing additional\nparameters and increasing training time. Our MMA design is intended to be\ngeneric, allowing for application across various modalities, and scalable to\naccommodate diverse multimodal scenarios. The code and model are publicly\navailable at https://github.com/sony/aki to encourage further advancements in\nMLLMs across various directions.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.02597v2",
    "published_date": "2025-03-04 13:18:33 UTC",
    "updated_date": "2025-03-13 01:48:08 UTC"
  },
  {
    "arxiv_id": "2503.02595v1",
    "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
    "authors": [
      "Zhaoxing Gan",
      "Mengtian Li",
      "Ruhua Chen",
      "Zhongxia Ji",
      "Sichen Guo",
      "Huanling Hu",
      "Guangnan Ye",
      "Zuo Hu"
    ],
    "abstract": "In this work, we introduce StageDesigner, the first comprehensive framework\nfor artistic stage generation using large language models combined with\nlayout-controlled diffusion models. Given the professional requirements of\nstage scenography, StageDesigner simulates the workflows of seasoned artists to\ngenerate immersive 3D stage scenes. Specifically, our approach is divided into\nthree primary modules: Script Analysis, which extracts thematic and spatial\ncues from input scripts; Foreground Generation, which constructs and arranges\nessential 3D objects; and Background Generation, which produces a harmonious\nbackground aligned with the narrative atmosphere and maintains spatial\ncoherence by managing occlusions between foreground and background elements.\nFurthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276\nunique stage scenes spanning different historical styles and annotated with\nscripts, images, and detailed 3D layouts, specifically tailored for this task.\nFinally, evaluations using both standard and newly proposed metrics, along with\nextensive user studies, demonstrate the effectiveness of StageDesigner. Project\ncan be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02595v1",
    "published_date": "2025-03-04 13:17:50 UTC",
    "updated_date": "2025-03-04 13:17:50 UTC"
  },
  {
    "arxiv_id": "2503.13475v1",
    "title": "Cross-Subject Depression Level Classification Using EEG Signals with a Sample Confidence Method",
    "authors": [
      "ZhongYi Zhang",
      "ChenYang Xu",
      "LiXuan Zhao",
      "HuiRang Hou",
      "QingHao Meng"
    ],
    "abstract": "Electroencephalogram (EEG) is a non-invasive tool for real-time neural\nmonitoring,widely used in depression detection via deep learning. However,\nexisting models primarily focus on binary classification (depression/normal),\nlacking granularity for severity assessment. To address this, we proposed the\nDepL-GCN, i.e., Depression Level classification based on GCN model. This model\ntackles two key challenges: (1) subjectivity in depres-sion-level labeling due\nto patient self-report biases, and (2) class imbalance across severity\ncategories. Inspired by the model learning patterns, we introduced two novel\nmodules: the sample confidence module and the minority sample penalty module.\nThe former leverages the L2-norm of prediction errors to progressively filter\nEEG samples with weak label alignment during training, thereby reducing the\nimpact of subjectivity; the latter automatically upweights misclassified\nminority-class samples to address imbalance issues. After testing on two public\nEEG datasets, DepL-GCN achieved accuracies of 81.13% and 81.36% for multi-class\nseverity recognition, outperforming baseline models.Ablation studies confirmed\nboth modules' contributions. We further discussed the strengths and limitations\nof regression-based models for depression-level recognition.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13475v1",
    "published_date": "2025-03-04 13:16:11 UTC",
    "updated_date": "2025-03-04 13:16:11 UTC"
  },
  {
    "arxiv_id": "2503.02582v1",
    "title": "Playing games with Large language models: Randomness and strategy",
    "authors": [
      "Alicia Vidler",
      "Toby Walsh"
    ],
    "abstract": "Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.02582v1",
    "published_date": "2025-03-04 13:04:48 UTC",
    "updated_date": "2025-03-04 13:04:48 UTC"
  },
  {
    "arxiv_id": "2503.02574v1",
    "title": "LLM-Safety Evaluations Lack Robustness",
    "authors": [
      "Tim Beyer",
      "Sophie Xhonneux",
      "Simon Geisler",
      "Gauthier Gidel",
      "Leo Schwinn",
      "Stephan Günnemann"
    ],
    "abstract": "In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02574v1",
    "published_date": "2025-03-04 12:55:07 UTC",
    "updated_date": "2025-03-04 12:55:07 UTC"
  },
  {
    "arxiv_id": "2503.02572v1",
    "title": "RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour",
    "authors": [
      "Valerii Serpiva",
      "Artem Lykov",
      "Artyom Myshlyaev",
      "Muhammad Haris Khan",
      "Ali Alridha Abdulkarim",
      "Oleg Sautenkov",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "6 pages, 6 figures. Submitted to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02572v1",
    "published_date": "2025-03-04 12:54:05 UTC",
    "updated_date": "2025-03-04 12:54:05 UTC"
  },
  {
    "arxiv_id": "2503.05812v1",
    "title": "Intolerable Risk Threshold Recommendations for Artificial Intelligence",
    "authors": [
      "Deepika Raman",
      "Nada Madkour",
      "Evan R. Murphy",
      "Krystal Jackson",
      "Jessica Newman"
    ],
    "abstract": "Frontier AI models -- highly capable foundation models at the cutting edge of\nAI development -- may pose severe risks to public safety, human rights,\neconomic stability, and societal value in the coming years. These risks could\narise from deliberate adversarial misuse, system failures, unintended cascading\neffects, or simultaneous failures across multiple models.\n  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI\nindustry organizations signed the Frontier AI Safety Commitments, and 27\nnations and the EU issued a declaration on their intent to define these\nthresholds. To fulfill these commitments, organizations must determine and\ndisclose ``thresholds at which severe risks posed by a model or system, unless\nadequately mitigated, would be deemed intolerable.''\n  To assist in setting and operationalizing intolerable risk thresholds, we\noutline key principles and considerations; for example, to aim for ``good, not\nperfect'' thresholds in the face of limited data on rapidly advancing AI\ncapabilities and consequently evolving risks. We also propose specific\nthreshold recommendations, including some detailed case studies, for a subset\nof risks across eight risk categories: (1) Chemical, Biological, Radiological,\nand Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)\nPersuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,\nand (8) Socioeconomic Disruption. Our goal is to serve as a starting point or\nsupplementary resource for policymakers and industry leaders, encouraging\nproactive risk management that prioritizes preventing intolerable risks (ex\nante) rather than merely mitigating them after they occur (ex post).",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CR",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "79 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05812v1",
    "published_date": "2025-03-04 12:30:37 UTC",
    "updated_date": "2025-03-04 12:30:37 UTC"
  },
  {
    "arxiv_id": "2503.02552v1",
    "title": "World Models for Anomaly Detection during Model-Based Reinforcement Learning Inference",
    "authors": [
      "Fabian Domberg",
      "Georg Schildbach"
    ],
    "abstract": "Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02552v1",
    "published_date": "2025-03-04 12:25:01 UTC",
    "updated_date": "2025-03-04 12:25:01 UTC"
  },
  {
    "arxiv_id": "2503.02549v1",
    "title": "Federated nnU-Net for Privacy-Preserving Medical Image Segmentation",
    "authors": [
      "Grzegorz Skorupko",
      "Fotios Avgoustidis",
      "Carlos Martín-Isla",
      "Lidia Garrucho",
      "Dimitri A. Kessler",
      "Esmeralda Ruiz Pujadas",
      "Oliver Díaz",
      "Maciej Bobowicz",
      "Katarzyna Gwoździewicz",
      "Xavier Bargalló",
      "Paulius Jaruševičius",
      "Kaisar Kushibar",
      "Karim Lekadir"
    ],
    "abstract": "The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "In review",
    "pdf_url": "http://arxiv.org/pdf/2503.02549v1",
    "published_date": "2025-03-04 12:20:06 UTC",
    "updated_date": "2025-03-04 12:20:06 UTC"
  },
  {
    "arxiv_id": "2503.02917v1",
    "title": "Interpretable Few-Shot Retinal Disease Diagnosis with Concept-Guided Prompting of Vision-Language Models",
    "authors": [
      "Deval Mehta",
      "Yiwen Jiang",
      "Catherine L Jan",
      "Mingguang He",
      "Kshitij Jadhav",
      "Zongyuan Ge"
    ],
    "abstract": "Recent advancements in deep learning have shown significant potential for\nclassifying retinal diseases using color fundus images. However, existing works\npredominantly rely exclusively on image data, lack interpretability in their\ndiagnostic decisions, and treat medical professionals primarily as annotators\nfor ground truth labeling. To fill this gap, we implement two key strategies:\nextracting interpretable concepts of retinal diseases using the knowledge base\nof GPT models and incorporating these concepts as a language component in\nprompt-learning to train vision-language (VL) models with both fundus images\nand their associated concepts. Our method not only improves retinal disease\nclassification but also enriches few-shot and zero-shot detection (novel\ndisease detection), while offering the added benefit of concept-based model\ninterpretability. Our extensive evaluation across two diverse retinal fundus\nimage datasets illustrates substantial performance gains in VL-model based\nfew-shot methodologies through our concept integration approach, demonstrating\nan average improvement of approximately 5.8\\% and 2.7\\% mean average precision\nfor 16-shot learning and zero-shot (novel class) detection respectively. Our\nmethod marks a pivotal step towards interpretable and efficient retinal disease\nrecognition for real-world clinical applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted to Information Processing in Medical Imaging (IPMI) 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02917v1",
    "published_date": "2025-03-04 12:03:42 UTC",
    "updated_date": "2025-03-04 12:03:42 UTC"
  },
  {
    "arxiv_id": "2503.02537v2",
    "title": "RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification",
    "authors": [
      "Zhen Yang",
      "Guibao Shen",
      "Liang Hou",
      "Mushui Liu",
      "Luozhou Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Ying-Cong Chen"
    ],
    "abstract": "Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an straightforward and efficient solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we introduce average latent energy analysis and discover\nthat an improved classifier-free guidance hyperparameter can significantly\nenhance generation performance. Our method is entirely training-free and boasts\na simple implementation logic and efficient performance. Through extensive\ncomparisons with numerous baseline methods, our RectifiedHR demonstrates\nsuperior effectiveness and efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/",
    "pdf_url": "http://arxiv.org/pdf/2503.02537v2",
    "published_date": "2025-03-04 12:03:26 UTC",
    "updated_date": "2025-03-14 13:40:17 UTC"
  },
  {
    "arxiv_id": "2503.02512v1",
    "title": "LTL Verification of Memoryful Neural Agents",
    "authors": [
      "Mehran Hosseini",
      "Alessio Lomuscio",
      "Nicola Paoletti"
    ],
    "abstract": "We present a framework for verifying Memoryful Neural Multi-Agent Systems\n(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,\nagents interact with a non-deterministic, partially observable environment.\nExamples of MN-MAS include multi-agent systems based on feed-forward and\nrecurrent neural networks or state-space models. Different from previous\napproaches, we support the verification of both bounded and unbounded LTL\nspecifications. We leverage well-established bounded model checking techniques,\nincluding lasso search and invariant synthesis, to reduce the verification\nproblem to that of constraint solving. To solve these constraints, we develop\nefficient methods based on bound propagation, mixed-integer linear programming,\nand adaptive splitting. We evaluate the effectiveness of our algorithms in\nsingle and multi-agent environments from the Gymnasium and PettingZoo\nlibraries, verifying unbounded specifications for the first time and improving\nthe verification time for bounded specifications by an order of magnitude\ncompared to the SoA.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SC",
      "68Q60 (Primary) 68T27, 68T07, 68T37, 68T40, 68T42 (Secondary)",
      "D.2.4; F.3.1; I.2.4; I.2.11; I.2.8; F.4.1; I.2.2; I.2.3"
    ],
    "primary_category": "cs.LO",
    "comment": "11 pages, 2 figures, accepted at AAMAS 2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2503.02512v1",
    "published_date": "2025-03-04 11:20:19 UTC",
    "updated_date": "2025-03-04 11:20:19 UTC"
  },
  {
    "arxiv_id": "2503.02505v1",
    "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
    "authors": [
      "Shaofei Cai",
      "Zhancun Mu",
      "Anji Liu",
      "Yitao Liang"
    ],
    "abstract": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02505v1",
    "published_date": "2025-03-04 11:16:46 UTC",
    "updated_date": "2025-03-04 11:16:46 UTC"
  },
  {
    "arxiv_id": "2503.02497v1",
    "title": "PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel PennyLane-Centric Dataset",
    "authors": [
      "Haider Asif",
      "Abdul Basit",
      "Nouhaila Innan",
      "Muhammad Kashif",
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "abstract": "Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "quant-ph",
      "68T50 (Primary)",
      "I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02497v1",
    "published_date": "2025-03-04 11:04:35 UTC",
    "updated_date": "2025-03-04 11:04:35 UTC"
  },
  {
    "arxiv_id": "2503.02495v2",
    "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
    "authors": [
      "Yujiao Yang",
      "Jing Lian",
      "Linhui Li"
    ],
    "abstract": "We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "68T07",
      "I.5.1; I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.02495v2",
    "published_date": "2025-03-04 11:01:25 UTC",
    "updated_date": "2025-03-06 08:51:47 UTC"
  },
  {
    "arxiv_id": "2503.02484v1",
    "title": "ERetinex: Event Camera Meets Retinex Theory for Low-Light Image Enhancement",
    "authors": [
      "Xuejian Guo",
      "Zhiqiang Tian",
      "Yuehang Wang",
      "Siqi Li",
      "Yu Jiang",
      "Shaoyi Du",
      "Yue Gao"
    ],
    "abstract": "Low-light image enhancement aims to restore the under-exposure image captured\nin dark scenarios. Under such scenarios, traditional frame-based cameras may\nfail to capture the structure and color information due to the exposure time\nlimitation. Event cameras are bio-inspired vision sensors that respond to\npixel-wise brightness changes asynchronously. Event cameras' high dynamic range\nis pivotal for visual perception in extreme low-light scenarios, surpassing\ntraditional cameras and enabling applications in challenging dark environments.\nIn this paper, inspired by the success of the retinex theory for traditional\nframe-based low-light image restoration, we introduce the first methods that\ncombine the retinex theory with event cameras and propose a novel retinex-based\nlow-light image restoration framework named ERetinex. Among our contributions,\nthe first is developing a new approach that leverages the high temporal\nresolution data from event cameras with traditional image information to\nestimate scene illumination accurately. This method outperforms traditional\nimage-only techniques, especially in low-light environments, by providing more\nprecise lighting information. Additionally, we propose an effective fusion\nstrategy that combines the high dynamic range data from event cameras with the\ncolor information of traditional images to enhance image quality. Through this\nfusion, we can generate clearer and more detail-rich images, maintaining the\nintegrity of visual information even under extreme lighting conditions. The\nexperimental results indicate that our proposed method outperforms\nstate-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while\nreducing FLOPS by \\textbf{84.28}\\%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02484v1",
    "published_date": "2025-03-04 10:48:44 UTC",
    "updated_date": "2025-03-04 10:48:44 UTC"
  },
  {
    "arxiv_id": "2503.02476v1",
    "title": "BioD2C: A Dual-level Semantic Consistency Constraint Framework for Biomedical VQA",
    "authors": [
      "Zhengyang Ji",
      "Shang Gao",
      "Li Liu",
      "Yifan Jia",
      "Yutao Yue"
    ],
    "abstract": "Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02476v1",
    "published_date": "2025-03-04 10:39:42 UTC",
    "updated_date": "2025-03-04 10:39:42 UTC"
  },
  {
    "arxiv_id": "2503.04812v1",
    "title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning",
    "authors": [
      "Zhibin Lan",
      "Liqiang Niu",
      "Fandong Meng",
      "Jie Zhou",
      "Jinsong Su"
    ],
    "abstract": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.04812v1",
    "published_date": "2025-03-04 10:21:57 UTC",
    "updated_date": "2025-03-04 10:21:57 UTC"
  },
  {
    "arxiv_id": "2503.05810v2",
    "title": "A Transformer Model for Predicting Chemical Reaction Products from Generic Templates",
    "authors": [
      "Derin Ozer",
      "Sylvain Lamprier",
      "Thomas Cauchy",
      "Nicolas Gutowski",
      "Benoit Da Mota"
    ],
    "abstract": "The accurate prediction of chemical reaction outcomes is a major challenge in\ncomputational chemistry. Current models rely heavily on either highly specific\nreaction templates or template-free methods, both of which present limitations.\nTo address these limitations, this work proposes the Broad Reaction Set (BRS),\na dataset featuring 20 generic reaction templates that allow for the efficient\nexploration of the chemical space. Additionally, ProPreT5 is introduced, a T5\nmodel tailored to chemistry that achieves a balance between rigid templates and\ntemplate-free methods. ProPreT5 demonstrates its capability to generate\naccurate, valid, and realistic reaction products, making it a promising\nsolution that goes beyond the current state-of-the-art on the complex reaction\nproduct prediction task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05810v2",
    "published_date": "2025-03-04 10:18:32 UTC",
    "updated_date": "2025-03-11 08:22:15 UTC"
  },
  {
    "arxiv_id": "2503.03774v2",
    "title": "Fair Play in the Fast Lane: Integrating Sportsmanship into Autonomous Racing Systems",
    "authors": [
      "Zhenmin Huang",
      "Ce Hao",
      "Wei Zhan",
      "Jun Ma",
      "Masayoshi Tomizuka"
    ],
    "abstract": "Autonomous racing has gained significant attention as a platform for\nhigh-speed decision-making and motion control. While existing methods primarily\nfocus on trajectory planning and overtaking strategies, the role of\nsportsmanship in ensuring fair competition remains largely unexplored. In human\nracing, rules such as the one-motion rule and the enough-space rule prevent\ndangerous and unsportsmanlike behavior. However, autonomous racing systems\noften lack mechanisms to enforce these principles, potentially leading to\nunsafe maneuvers. This paper introduces a bi-level game-theoretic framework to\nintegrate sportsmanship (SPS) into versus racing. At the high level, we model\nracing intentions using a Stackelberg game, where Monte Carlo Tree Search\n(MCTS) is employed to derive optimal strategies. At the low level, vehicle\ninteractions are formulated as a Generalized Nash Equilibrium Problem (GNEP),\nensuring that all agents follow sportsmanship constraints while optimizing\ntheir trajectories. Simulation results demonstrate the effectiveness of the\nproposed approach in enforcing sportsmanship rules while maintaining\ncompetitive performance. We analyze different scenarios where attackers and\ndefenders adhere to or disregard sportsmanship rules and show how knowledge of\nthese constraints influences strategic decision-making. This work highlights\nthe importance of balancing competition and fairness in autonomous racing and\nprovides a foundation for developing ethical and safe AI-driven racing systems.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.03774v2",
    "published_date": "2025-03-04 10:14:19 UTC",
    "updated_date": "2025-03-12 17:02:38 UTC"
  },
  {
    "arxiv_id": "2503.02457v1",
    "title": "Don't Get Too Excited -- Eliciting Emotions in LLMs",
    "authors": [
      "Gino Franco Fazzi",
      "Julie Skoven Hinge",
      "Stefan Heinrich",
      "Paolo Burelli"
    ],
    "abstract": "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02457v1",
    "published_date": "2025-03-04 10:06:41 UTC",
    "updated_date": "2025-03-04 10:06:41 UTC"
  },
  {
    "arxiv_id": "2503.02453v1",
    "title": "Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations",
    "authors": [
      "Yuhao Yang",
      "Zhi Ji",
      "Zhaopeng Li",
      "Yi Li",
      "Zhonglin Mo",
      "Yue Ding",
      "Kai Chen",
      "Zijian Zhang",
      "Jie Li",
      "Shuanglong Li",
      "Lin Liu"
    ],
    "abstract": "Generative models have recently gained attention in recommendation systems by\ndirectly predicting item identifiers from user interaction sequences. However,\nexisting methods suffer from significant information loss due to the separation\nof stages such as quantization and sequence modeling, hindering their ability\nto achieve the modeling precision and accuracy of sequential dense retrieval\ntechniques. Integrating generative and dense retrieval methods remains a\ncritical challenge. To address this, we introduce the Cascaded Organized\nBi-Represented generAtive retrieval (COBRA) framework, which innovatively\nintegrates sparse semantic IDs and dense vectors through a cascading process.\nOur method alternates between generating these representations by first\ngenerating sparse IDs, which serve as conditions to aid in the generation of\ndense vectors. End-to-end training enables dynamic refinement of dense\nrepresentations, capturing both semantic insights and collaborative signals\nfrom user-item interactions. During inference, COBRA employs a coarse-to-fine\nstrategy, starting with sparse ID generation and refining them into dense\nvectors via the generative model. We further propose BeamFusion, an innovative\napproach combining beam search with nearest neighbor scores to enhance\ninference flexibility and recommendation diversity. Extensive experiments on\npublic datasets and offline tests validate our method's robustness. Online A/B\ntests on a real-world advertising platform with over 200 million daily users\ndemonstrate substantial improvements in key metrics, highlighting COBRA's\npractical advantages.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02453v1",
    "published_date": "2025-03-04 10:00:05 UTC",
    "updated_date": "2025-03-04 10:00:05 UTC"
  },
  {
    "arxiv_id": "2503.02420v1",
    "title": "Exploring Model Quantization in GenAI-based Image Inpainting and Detection of Arable Plants",
    "authors": [
      "Sourav Modak",
      "Ahmet Oğuz Saltık",
      "Anthony Stein"
    ],
    "abstract": "Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02420v1",
    "published_date": "2025-03-04 09:05:01 UTC",
    "updated_date": "2025-03-04 09:05:01 UTC"
  },
  {
    "arxiv_id": "2503.11684v1",
    "title": "Exploring Causality for HRI: A Case Study on Robotic Mental Well-being Coaching",
    "authors": [
      "Micol Spitale",
      "Srikar Babu",
      "Serhan Cakmak",
      "Jiaee Cheong",
      "Hatice Gunes"
    ],
    "abstract": "One of the primary goals of Human-Robot Interaction (HRI) research is to\ndevelop robots that can interpret human behavior and adapt their responses\naccordingly. Adaptive learning models, such as continual and reinforcement\nlearning, play a crucial role in improving robots' ability to interact\neffectively in real-world settings. However, these models face significant\nchallenges due to the limited availability of real-world data, particularly in\nsensitive domains like healthcare and well-being. This data scarcity can hinder\na robot's ability to adapt to new situations. To address these challenges,\ncausality provides a structured framework for understanding and modeling the\nunderlying relationships between actions, events, and outcomes. By moving\nbeyond mere pattern recognition, causality enables robots to make more\nexplainable and generalizable decisions. This paper presents an exploratory\ncausality-based analysis through a case study of an adaptive robotic coach\ndelivering positive psychology exercises over four weeks in a workplace\nsetting. The robotic coach autonomously adapts to multimodal human behaviors,\nsuch as facial valence and speech duration. By conducting both macro- and\nmicro-level causal analyses, this study aims to gain deeper insights into how\nadaptability can enhance well-being during interactions. Ultimately, this\nresearch seeks to advance our understanding of how causality can help overcome\nchallenges in HRI, particularly in real-world applications.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.11684v1",
    "published_date": "2025-03-04 08:56:47 UTC",
    "updated_date": "2025-03-04 08:56:47 UTC"
  },
  {
    "arxiv_id": "2503.02403v1",
    "title": "AutoEval: A Practical Framework for Autonomous Evaluation of Mobile Agents",
    "authors": [
      "Jiahui Sun",
      "Zhichao Hua",
      "Yubin Xia"
    ],
    "abstract": "Accurate and systematic evaluation of mobile agents can significantly advance\ntheir development and real-world applicability. However, existing benchmarks\nfor mobile agents lack practicality and scalability due to the extensive manual\neffort required to define task reward signals and implement corresponding\nevaluation codes. To this end, we propose AutoEval, an autonomous agent\nevaluation framework that tests a mobile agent without any manual effort.\nFirst, we design a Structured Substate Representation to describe the UI state\nchanges while agent execution, such that task reward signals can be\nautomatically generated. Second, we utilize a Judge System that can\nautonomously evaluate agents' performance given the automatically generated\ntask reward signals. By providing only a task description, our framework\nevaluates agents with fine-grained performance feedback to that task without\nany extra manual effort. We implement a prototype of our framework and validate\nthe automatically generated task reward signals, finding over 93% coverage to\nhuman-annotated reward signals. Moreover, to prove the effectiveness of our\nautonomous Judge System, we manually verify its judge results and demonstrate\nthat it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile\nagents using our framework, providing detailed insights into their performance\ncharacteristics and limitations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02403v1",
    "published_date": "2025-03-04 08:44:30 UTC",
    "updated_date": "2025-03-04 08:44:30 UTC"
  },
  {
    "arxiv_id": "2503.02399v1",
    "title": "VisAgent: Narrative-Preserving Story Visualization Framework",
    "authors": [
      "Seungkwon Kim",
      "GyuTae Park",
      "Sangyeon Kim",
      "Seung-Hun Nam"
    ],
    "abstract": "Story visualization is the transformation of narrative elements into image\nsequences. While existing research has primarily focused on visual contextual\ncoherence, the deeper narrative essence of stories often remains overlooked.\nThis limitation hinders the practical application of these approaches, as\ngenerated images frequently fail to capture the intended meaning and nuances of\nthe narrative fully. To address these challenges, we propose VisAgent, a\ntraining-free multi-agent framework designed to comprehend and visualize\npivotal scenes within a given story. By considering story distillation,\nsemantic consistency, and contextual coherence, VisAgent employs an agentic\nworkflow. In this workflow, multiple specialized agents collaborate to: (i)\nrefine layered prompts based on the narrative structure and (ii) seamlessly\nintegrate \\gt{generated} elements, including refined prompts, scene elements,\nand subject placement, into the final image. The empirically validated\neffectiveness confirms the framework's suitability for practical story\nvisualization applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICASSP 2025. Equal contribution from first two authors",
    "pdf_url": "http://arxiv.org/pdf/2503.02399v1",
    "published_date": "2025-03-04 08:41:45 UTC",
    "updated_date": "2025-03-04 08:41:45 UTC"
  },
  {
    "arxiv_id": "2503.02398v1",
    "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for Long Behavior Sequence",
    "authors": [
      "Yunxiao Shi",
      "Wujiang Xu",
      "Zeqi Zhang",
      "Xing Zi",
      "Qiang Wu",
      "Min Xu"
    ],
    "abstract": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "draft paper",
    "pdf_url": "http://arxiv.org/pdf/2503.02398v1",
    "published_date": "2025-03-04 08:41:40 UTC",
    "updated_date": "2025-03-04 08:41:40 UTC"
  },
  {
    "arxiv_id": "2503.02397v1",
    "title": "A Binary Classification Social Network Dataset for Graph Machine Learning",
    "authors": [
      "Adnan Ali",
      "Jinglong Li",
      "Huanhuan Chen",
      "AlMotasem Bellah Al Ajlouni"
    ],
    "abstract": "Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02397v1",
    "published_date": "2025-03-04 08:40:42 UTC",
    "updated_date": "2025-03-04 08:40:42 UTC"
  },
  {
    "arxiv_id": "2503.02382v1",
    "title": "An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning",
    "authors": [
      "Wei Sun",
      "Qianlong Du",
      "Fuwei Cui",
      "Jiajun Zhang"
    ],
    "abstract": "Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02382v1",
    "published_date": "2025-03-04 08:18:46 UTC",
    "updated_date": "2025-03-04 08:18:46 UTC"
  },
  {
    "arxiv_id": "2503.02913v1",
    "title": "Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient Communication and Attention Mechanisms",
    "authors": [
      "Zilin Zhao",
      "Chishui Chen",
      "Haotian Shi",
      "Jiale Chen",
      "Xuanlin Yue",
      "Zhejian Yang",
      "Yang Liu"
    ],
    "abstract": "Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02913v1",
    "published_date": "2025-03-04 08:05:14 UTC",
    "updated_date": "2025-03-04 08:05:14 UTC"
  },
  {
    "arxiv_id": "2503.02369v1",
    "title": "JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for Agricultural Vehicle Routing Optimization",
    "authors": [
      "Yixuan Fan",
      "Haotian Xu",
      "Mengqiao Liu",
      "Qing Zhuo",
      "Tao Zhang"
    ],
    "abstract": "The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the\nVehicle Routing Problem (VRP) where the scale of cities influences routing\noutcomes, necessitating consideration of their entrances. This paper addresses\nEDVRP in agriculture, focusing on multi-parameter vehicle planning for\nirregularly shaped fields. To address the limitations of traditional methods,\nsuch as heuristic approaches, which often overlook field geometry and entrance\nconstraints, we propose a Joint Probability Distribution Sampling Neural\nNetwork (JPDS-NN) to effectively solve the EDVRP. The network uses an\nencoder-decoder architecture with graph transformers and attention mechanisms\nto model routing as a Markov Decision Process, and is trained via reinforcement\nlearning for efficient and rapid end-to-end planning. Experimental results\nindicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel\nconsumption by 14.0-17.6%, and computes two orders of magnitude faster than\nbaseline methods, while demonstrating 15-25% superior performance in dynamic\narrangement scenarios. Ablation studies validate the necessity of\ncross-attention and pre-training. The framework enables scalable, intelligent\nrouting for large-scale farming under dynamic constraints.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures, submitted to IROS 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02369v1",
    "published_date": "2025-03-04 07:50:32 UTC",
    "updated_date": "2025-03-04 07:50:32 UTC"
  },
  {
    "arxiv_id": "2503.02368v2",
    "title": "Iterative Value Function Optimization for Guided Decoding",
    "authors": [
      "Zhenhua Liu",
      "Lijun Li",
      "Ruizhe Chen",
      "Yuxian Jiang",
      "Tong Zhu",
      "Zhaochen Su",
      "Wenliang Chen",
      "Jing Shao"
    ],
    "abstract": "While Reinforcement Learning from Human Feedback (RLHF) has become the\npredominant method for controlling language model outputs, it suffers from high\ncomputational costs and training instability. Guided decoding, especially\nvalue-guided methods, offers a cost-effective alternative by controlling\noutputs without re-training models. However, the accuracy of the value function\nis crucial for value-guided decoding, as inaccuracies can lead to suboptimal\ndecision-making and degraded performance. Existing methods struggle with\naccurately estimating the optimal value function, leading to less effective\ncontrol. We propose Iterative Value Function Optimization, a novel framework\nthat addresses these limitations through two key components: Monte Carlo Value\nEstimation, which reduces estimation variance by exploring diverse\ntrajectories, and Iterative On-Policy Optimization, which progressively\nimproves value estimation through collecting trajectories from value-guided\npolicies. Extensive experiments on text summarization, multi-turn dialogue, and\ninstruction following demonstrate the effectiveness of value-guided decoding\napproaches in aligning language models. These approaches not only achieve\nalignment but also significantly reduce computational costs by leveraging\nprincipled value function optimization for efficient and effective control.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02368v2",
    "published_date": "2025-03-04 07:49:10 UTC",
    "updated_date": "2025-03-05 09:12:25 UTC"
  },
  {
    "arxiv_id": "2503.02365v2",
    "title": "EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram Reports",
    "authors": [
      "Lama Moukheiber",
      "Mira Moukheiber",
      "Dana Moukheiiber",
      "Jae-Woo Ju",
      "Hyung-Chul Lee"
    ],
    "abstract": "We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS SafeGenAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.02365v2",
    "published_date": "2025-03-04 07:45:45 UTC",
    "updated_date": "2025-03-06 03:29:31 UTC"
  },
  {
    "arxiv_id": "2503.04809v2",
    "title": "PanguIR Technical Report for NTCIR-18 AEOLLM Task",
    "authors": [
      "Lang Mei",
      "Chong Chen",
      "Jiaxin Mao"
    ],
    "abstract": "As large language models (LLMs) gain widespread attention in both academia\nand industry, it becomes increasingly critical and challenging to effectively\nevaluate their capabilities. Existing evaluation methods can be broadly\ncategorized into two types: manual evaluation and automatic evaluation. Manual\nevaluation, while comprehensive, is often costly and resource-intensive.\nConversely, automatic evaluation offers greater scalability but is constrained\nby the limitations of its evaluation criteria (dominated by reference-based\nanswers). To address these challenges, NTCIR-18 introduced the AEOLLM\n(Automatic Evaluation of LLMs) task, aiming to encourage reference-free\nevaluation methods that can overcome the limitations of existing approaches. In\nthis paper, to enhance the evaluation performance of the AEOLLM task, we\npropose three key methods to improve the reference-free evaluation: 1)\nMulti-model Collaboration: Leveraging multiple LLMs to approximate human\nratings across various subtasks; 2) Prompt Auto-optimization: Utilizing LLMs to\niteratively refine the initial task prompts based on evaluation feedback from\ntraining samples; and 3) In-context Learning (ICL) Optimization: Based on the\nmulti-task evaluation feedback, we train a specialized in-context example\nretrieval model, combined with a semantic relevance retrieval model, to jointly\nidentify the most effective in-context learning examples. Experiments conducted\non the final dataset demonstrate that our approach achieves superior\nperformance on the AEOLLM task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.04809v2",
    "published_date": "2025-03-04 07:40:02 UTC",
    "updated_date": "2025-03-10 06:49:01 UTC"
  },
  {
    "arxiv_id": "2503.02360v1",
    "title": "BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)",
    "authors": [
      "Husne Ara Rubaiyeat",
      "Njayou Youssouf",
      "Md Kamrul Hasan",
      "Hasan Mahmud"
    ],
    "abstract": "Sign language recognition (SLR) for low-resource languages like Bangla\nsuffers from signer variability, viewpoint variations, and limited annotated\ndatasets. In this paper, we present BdSLW401, a large-scale, multi-view,\nword-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video\nsamples from 18 signers in front and lateral views. To improve\ntransformer-based SLR, we introduce Relative Quantization Encoding (RQE), a\nstructured embedding approach anchoring landmarks to physiological reference\npoints and quantize motion trajectories. RQE improves attention allocation by\ndecreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,\n21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,\nfixed quantization becomes insufficient on large-scale datasets (e.g.,\nWLASL2000), indicating the need for adaptive encoding strategies. Further,\nRQE-SF, an extended variant that stabilizes shoulder landmarks, achieves\nimprovements in pose consistency at the cost of small trade-offs in lateral\nview recognition. The attention graphs prove that RQE improves model\ninterpretability by focusing on the major articulatory features (fingers,\nwrists) and the more distinctive frames instead of global pose changes.\nIntroducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced\nstructured embeddings, this work advances transformer-based SLR for\nlow-resource languages and sets a benchmark for future research in this area.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02360v1",
    "published_date": "2025-03-04 07:34:06 UTC",
    "updated_date": "2025-03-04 07:34:06 UTC"
  },
  {
    "arxiv_id": "2503.02358v1",
    "title": "Are Large Vision Language Models Good Game Players?",
    "authors": [
      "Xinyu Wang",
      "Bohan Zhuang",
      "Qi Wu"
    ],
    "abstract": "Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02358v1",
    "published_date": "2025-03-04 07:29:03 UTC",
    "updated_date": "2025-03-04 07:29:03 UTC"
  },
  {
    "arxiv_id": "2503.02354v1",
    "title": "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory",
    "authors": [
      "Jiashun Suo",
      "Xiaojian Liao",
      "Limin Xiao",
      "Li Ruan",
      "Jinquan Wang",
      "Xiao Su",
      "Zhisheng Huo"
    ],
    "abstract": "Large language models like GPT-4 are resource-intensive, but recent\nadvancements suggest that smaller, specialized experts can outperform the\nmonolithic models on specific tasks. The Collaboration-of-Experts (CoE)\napproach integrates multiple expert models, improving the accuracy of generated\nresults and offering great potential for precision-critical applications, such\nas automatic circuit board quality inspection. However, deploying CoE serving\nsystems presents challenges to memory capacity due to the large number of\nexperts required, which can lead to significant performance overhead from\nfrequent expert switching across different memory and storage tiers.\n  We propose CoServe, an efficient CoE model serving system on heterogeneous\nCPU and GPU with limited memory. CoServe reduces unnecessary expert switching\nby leveraging expert dependency, a key property of CoE inference. CoServe\nintroduces a dependency-aware request scheduler and dependency-aware expert\nmanagement for efficient inference. It also introduces an offline profiler to\nautomatically find optimal resource allocation on various processors and\ndevices. In real-world intelligent manufacturing workloads, CoServe achieves\n4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art\nsystems.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted to ASPLOS '25",
    "pdf_url": "http://arxiv.org/pdf/2503.02354v1",
    "published_date": "2025-03-04 07:25:05 UTC",
    "updated_date": "2025-03-04 07:25:05 UTC"
  },
  {
    "arxiv_id": "2503.02351v1",
    "title": "MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI",
    "authors": [
      "Guangyin Bao",
      "Qi Zhang",
      "Zixuan Gong",
      "Zhuojia Wu",
      "Duoqian Miao"
    ],
    "abstract": "Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "23 pages, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02351v1",
    "published_date": "2025-03-04 07:20:42 UTC",
    "updated_date": "2025-03-04 07:20:42 UTC"
  },
  {
    "arxiv_id": "2503.02911v1",
    "title": "Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving Test",
    "authors": [
      "Xuan Cai",
      "Xuesong Bai",
      "Zhiyong Cui",
      "Danmu Xie",
      "Daocheng Fu",
      "Haiyang Yu",
      "Yilong Ren"
    ],
    "abstract": "Autonomous driving (AD) testing constitutes a critical methodology for\nassessing performance benchmarks prior to product deployment. The creation of\nsegmented scenarios within a simulated environment is acknowledged as a robust\nand effective strategy; however, the process of tailoring these scenarios often\nnecessitates laborious and time-consuming manual efforts, thereby hindering the\ndevelopment and implementation of AD technologies. In response to this\nchallenge, we introduce Text2Scenario, a framework that leverages a Large\nLanguage Model (LLM) to autonomously generate simulation test scenarios that\nclosely align with user specifications, derived from their natural language\ninputs. Specifically, an LLM, equipped with a meticulously engineered input\nprompt scheme functions as a text parser for test scenario descriptions,\nextracting from a hierarchically organized scenario repository the components\nthat most accurately reflect the user's preferences. Subsequently, by\nexploiting the precedence of scenario components, the process involves\nsequentially matching and linking scenario representations within a Domain\nSpecific Language corpus, ultimately fabricating executable test scenarios. The\nexperimental results demonstrate that such prompt engineering can meticulously\nextract the nuanced details of scenario elements embedded within various\ndescriptive formats, with the majority of generated scenarios aligning closely\nwith the user's initial expectations, allowing for the efficient and precise\nevaluation of diverse AD stacks void of the labor-intensive need for manual\nscenario configuration. Project page:\nhttps://caixxuan.github.io/Text2Scenario.GitHub.io.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02911v1",
    "published_date": "2025-03-04 07:20:25 UTC",
    "updated_date": "2025-03-04 07:20:25 UTC"
  },
  {
    "arxiv_id": "2503.02345v1",
    "title": "CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented 3D MRI",
    "authors": [
      "Mominul Islam",
      "Mohammad Junayed Hasan",
      "M. R. C. Mahdy"
    ],
    "abstract": "The detection of Alzheimer disease (AD) from clinical MRI data is an active\narea of research in medical imaging. Recent advances in quantum computing,\nparticularly the integration of parameterized quantum circuits (PQCs) with\nclassical machine learning architectures, offer new opportunities to develop\nmodels that may outperform traditional methods. However, quantum machine\nlearning (QML) remains in its early stages and requires further experimental\nanalysis to better understand its behavior and limitations. In this paper, we\npropose an end to end hybrid classical quantum convolutional neural network (CQ\nCNN) for AD detection using clinically formatted 3D MRI data. Our approach\ninvolves developing a framework to make 3D MRI data usable for machine\nlearning, designing and training a brain tissue segmentation model (Skull Net),\nand training a diffusion model to generate synthetic images for the minority\nclass. Our converged models exhibit potential quantum advantages, achieving\nhigher accuracy in fewer epochs than classical models. The proposed beta8 3\nqubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)\nmodels while requiring significantly fewer computational resources. In\nparticular, the architecture employs only 13K parameters (0.48 MB), reducing\nthe parameter count by more than 99.99% compared to current SOTA models.\nFurthermore, the diffusion-generated data used to train our quantum models, in\nconjunction with real samples, preserve clinical structural standards,\nrepresenting a notable first in the field of QML. We conclude that CQCNN\narchitecture like models, with further improvements in gradient optimization\ntechniques, could become a viable option and even a potential alternative to\nclassical models for AD detection, especially in data limited and resource\nconstrained clinical settings.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "Application of hybrid quantum-classical machine learning for (early\n  stage) disease detection",
    "pdf_url": "http://arxiv.org/pdf/2503.02345v1",
    "published_date": "2025-03-04 07:08:47 UTC",
    "updated_date": "2025-03-04 07:08:47 UTC"
  },
  {
    "arxiv_id": "2503.13473v1",
    "title": "Robust Detection of Extremely Thin Lines Using 0.2mm Piano Wire",
    "authors": [
      "Jisoo Hong",
      "Youngjin Jung",
      "Jihwan Bae",
      "Seungho Song",
      "Sung-Woo Kang"
    ],
    "abstract": "This study developed an algorithm capable of detecting a reference line (a\n0.2 mm thick piano wire) to accurately determine the position of an automated\ninstallation robot within an elevator shaft. A total of 3,245 images were\ncollected from the experimental tower of H Company, the leading elevator\nmanufacturer in South Korea, and the detection performance was evaluated using\nfour experimental approaches (GCH, GSCH, GECH, FCH). During the initial image\nprocessing stage, Gaussian blurring, sharpening filter, embossing filter, and\nFourier Transform were applied, followed by Canny Edge Detection and Hough\nTransform. Notably, the method was developed to accurately extract the\nreference line by averaging the x-coordinates of the lines detected through the\nHough Transform. This approach enabled the detection of the 0.2 mm thick piano\nwire with high accuracy, even in the presence of noise and other interfering\nfactors (e.g., concrete cracks inside the elevator shaft or safety bars for\nfilming equipment). The experimental results showed that Experiment 4 (FCH),\nwhich utilized Fourier Transform in the preprocessing stage, achieved the\nhighest detection rate for the LtoL, LtoR, and RtoL datasets. Experiment\n2(GSCH), which applied Gaussian blurring and a sharpening filter, demonstrated\nsuperior detection performance on the RtoR dataset. This study proposes a\nreference line detection algorithm that enables precise position calculation\nand control of automated robots in elevator shaft installation. Moreover, the\ndeveloped method shows potential for applicability even in confined working\nspaces. Future work aims to develop a line detection algorithm equipped with\nmachine learning-based hyperparameter tuning capabilities.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.13473v1",
    "published_date": "2025-03-04 07:05:33 UTC",
    "updated_date": "2025-03-04 07:05:33 UTC"
  },
  {
    "arxiv_id": "2503.02341v1",
    "title": "GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning",
    "authors": [
      "Zhun Mou",
      "Bin Xia",
      "Zhengchao Huang",
      "Wenming Yang",
      "Jiaya Jia"
    ],
    "abstract": "Recent great advances in video generation models have demonstrated their\npotential to produce high-quality videos, bringing challenges to effective\nevaluation. Unlike human evaluation, existing automated evaluation metrics lack\nhigh-level semantic understanding and reasoning capabilities for video, thus\nmaking them infeasible and unexplainable. To fill this gap, we curate\nGRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset,\nincluding 3.3k videos from over 10 existing video generation models and\nmulti-step reasoning assessments converted by 16k human annotations. We then\nintroduce GRADEO, one of the first specifically designed video evaluation\nmodels, which grades AI-generated videos for explainable scores and assessments\nthrough multi-step reasoning. Experiments show that our method aligns better\nwith human evaluations than existing methods. Furthermore, our benchmarking\nreveals that current video generation models struggle to produce content that\naligns with human reasoning and complex real-world scenarios. The models,\ndatasets, and codes will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02341v1",
    "published_date": "2025-03-04 07:04:55 UTC",
    "updated_date": "2025-03-04 07:04:55 UTC"
  },
  {
    "arxiv_id": "2503.02338v1",
    "title": "Enhancing the Product Quality of the Injection Process Using eXplainable Artificial Intelligence",
    "authors": [
      "Jisoo Hong",
      "Yongmin Hong",
      "Jung-Woo Baek",
      "Sung-Woo Kang"
    ],
    "abstract": "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02338v1",
    "published_date": "2025-03-04 06:59:01 UTC",
    "updated_date": "2025-03-04 06:59:01 UTC"
  },
  {
    "arxiv_id": "2503.02334v1",
    "title": "BiasICL: In-Context Learning and Demographic Biases of Vision Language Models",
    "authors": [
      "Sonnet Xu",
      "Joseph Janizek",
      "Yixing Jiang",
      "Roxana Daneshjou"
    ],
    "abstract": "Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02334v1",
    "published_date": "2025-03-04 06:45:54 UTC",
    "updated_date": "2025-03-04 06:45:54 UTC"
  },
  {
    "arxiv_id": "2503.02333v1",
    "title": "Examining the Mental Health Impact of Misinformation on Social Media Using a Hybrid Transformer-Based Approach",
    "authors": [
      "Sarvesh Arora",
      "Sarthak Arora",
      "Deepika Kumar",
      "Vallari Agrawal",
      "Vedika Gupta",
      "Dipit Vasdev"
    ],
    "abstract": "Social media has significantly reshaped interpersonal communication,\nfostering connectivity while also enabling the proliferation of misinformation.\nThe unchecked spread of false narratives has profound effects on mental health,\ncontributing to increased stress, anxiety, and misinformation-driven paranoia.\nThis study presents a hybrid transformer-based approach using a RoBERTa-LSTM\nclassifier to detect misinformation, assess its impact on mental health, and\nclassify disorders linked to misinformation exposure. The proposed models\ndemonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,\nmental health implications, and disorder classification, respectively.\nFurthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)\nvalidates the direct correlation between misinformation and deteriorating\nmental well-being. This study underscores the urgent need for better\nmisinformation management strategies to mitigate its psychological\nrepercussions. Future research could explore broader datasets incorporating\nlinguistic, demographic, and cultural variables to deepen the understanding of\nmisinformation-induced mental health distress.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.02333v1",
    "published_date": "2025-03-04 06:45:17 UTC",
    "updated_date": "2025-03-04 06:45:17 UTC"
  },
  {
    "arxiv_id": "2503.02324v1",
    "title": "PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models",
    "authors": [
      "Xueliang Zhao",
      "Wei Wu",
      "Jian Guan",
      "Lingpeng Kong"
    ],
    "abstract": "The ability of large language models to solve complex mathematical problems\nhas progressed significantly, particularly for tasks requiring advanced\nreasoning. However, the scarcity of sufficiently challenging problems,\nparticularly at the Olympiad level, hinders further advancements. In this work,\nwe introduce PromptCoT, a novel approach for automatically generating\nhigh-quality Olympiad-level math problems. The proposed method synthesizes\ncomplex problems based on mathematical concepts and the rationale behind\nproblem construction, emulating the thought processes of experienced problem\ndesigners. We provide a theoretical analysis demonstrating that an optimal\nrationale should maximize both the likelihood of rationale generation given the\nassociated concepts and the likelihood of problem generation conditioned on\nboth the rationale and the concepts. Our method is evaluated on standard\nbenchmarks including GSM8K, MATH-500, and AIME2024, where it consistently\noutperforms existing problem generation methods. Furthermore, we demonstrate\nthat PromptCoT exhibits superior data scalability, consistently maintaining\nhigh performance as the dataset size increases, outperforming the baselines.\nThe implementation is available at https://github.com/zhaoxlpku/PromptCoT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.02324v1",
    "published_date": "2025-03-04 06:32:30 UTC",
    "updated_date": "2025-03-04 06:32:30 UTC"
  },
  {
    "arxiv_id": "2503.16474v1",
    "title": "From Voices to Worlds: Developing an AI-Powered Framework for 3D Object Generation in Augmented Reality",
    "authors": [
      "Majid Behravan",
      "Denis Gracanin"
    ],
    "abstract": "This paper presents Matrix, an advanced AI-powered framework designed for\nreal-time 3D object generation in Augmented Reality (AR) environments. By\nintegrating a cutting-edge text-to-3D generative AI model, multilingual\nspeech-to-text translation, and large language models (LLMs), the system\nenables seamless user interactions through spoken commands. The framework\nprocesses speech inputs, generates 3D objects, and provides object\nrecommendations based on contextual understanding, enhancing AR experiences. A\nkey feature of this framework is its ability to optimize 3D models by reducing\nmesh complexity, resulting in significantly smaller file sizes and faster\nprocessing on resource-constrained AR devices. Our approach addresses the\nchallenges of high GPU usage, large model output sizes, and real-time system\nresponsiveness, ensuring a smoother user experience. Moreover, the system is\nequipped with a pre-generated object repository, further reducing GPU load and\nimproving efficiency. We demonstrate the practical applications of this\nframework in various fields such as education, design, and accessibility, and\ndiscuss future enhancements including image-to-3D conversion, environmental\nobject detection, and multimodal support. The open-source nature of the\nframework promotes ongoing innovation and its utility across diverse\nindustries.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "arXiv admin note: text overlap with arXiv:2502.15869",
    "pdf_url": "http://arxiv.org/pdf/2503.16474v1",
    "published_date": "2025-03-04 06:31:51 UTC",
    "updated_date": "2025-03-04 06:31:51 UTC"
  },
  {
    "arxiv_id": "2503.02318v1",
    "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
    "authors": [
      "Zhifei Xie",
      "Mingbao Lin",
      "Zihang Liu",
      "Pengcheng Wu",
      "Shuicheng Yan",
      "Chunyan Miao"
    ],
    "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio\nmodality. We introduce Audio-Reasoner, a large-scale audio language model for\ndeep reasoning in audio tasks. We meticulously curated a large-scale and\ndiverse multi-task audio dataset with simple annotations. Then, we leverage\nclosed-source models to conduct secondary labeling, QA generation, along with\nstructured COT process. These datasets together form a high-quality reasoning\ndataset with 1.2 million reasoning-rich samples, which we name CoTA. Following\ninference scaling principles, we train Audio-Reasoner on CoTA, enabling it to\nachieve great logical capabilities in audio reasoning. Experiments show\nstate-of-the-art performance across key benchmarks, including MMAU-mini\n(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our\nfindings stress the core of structured CoT training in advancing audio\nreasoning.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Technical report, in process",
    "pdf_url": "http://arxiv.org/pdf/2503.02318v1",
    "published_date": "2025-03-04 06:18:34 UTC",
    "updated_date": "2025-03-04 06:18:34 UTC"
  },
  {
    "arxiv_id": "2503.05808v1",
    "title": "DriveGen: Towards Infinite Diverse Traffic Scenarios with Large Models",
    "authors": [
      "Shenyu Zhang",
      "Jiaguo Tian",
      "Zhengbang Zhu",
      "Shan Huang",
      "Jucheng Yang",
      "Weinan Zhang"
    ],
    "abstract": "Microscopic traffic simulation has become an important tool for autonomous\ndriving training and testing. Although recent data-driven approaches advance\nrealistic behavior generation, their learning still relies primarily on a\nsingle real-world dataset, which limits their diversity and thereby hinders\ndownstream algorithm optimization. In this paper, we propose DriveGen, a novel\ntraffic simulation framework with large models for more diverse traffic\ngeneration that supports further customized designs. DriveGen consists of two\ninternal stages: the initialization stage uses large language model and\nretrieval technique to generate map and vehicle assets; the rollout stage\noutputs trajectories with selected waypoint goals from visual language model\nand a specific designed diffusion planner. Through this two-staged process,\nDriveGen fully utilizes large models' high-level cognition and reasoning of\ndriving behavior, obtaining greater diversity beyond datasets while maintaining\nhigh realism. To support effective downstream optimization, we additionally\ndevelop DriveGen-CS, an automatic corner case generation pipeline that uses\nfailures of the driving algorithm as additional prompt knowledge for large\nmodels without the need for retraining or fine-tuning. Experiments show that\nour generated scenarios and corner cases have a superior performance compared\nto state-of-the-art baselines. Downstream experiments further verify that the\nsynthesized traffic of DriveGen provides better optimization of the performance\nof typical driving algorithms, demonstrating the effectiveness of our\nframework.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05808v1",
    "published_date": "2025-03-04 06:14:21 UTC",
    "updated_date": "2025-03-04 06:14:21 UTC"
  },
  {
    "arxiv_id": "2503.02311v1",
    "title": "Target Return Optimizer for Multi-Game Decision Transformer",
    "authors": [
      "Kensuke Tatematsu",
      "Akifumi Wachi"
    ],
    "abstract": "Achieving autonomous agents with robust generalization capabilities across\ndiverse games and tasks remains one of the ultimate goals in AI research.\nRecent advancements in transformer-based offline reinforcement learning,\nexemplified by the MultiGame Decision Transformer [Lee et al., 2022], have\nshown remarkable performance across various games or tasks. However, these\napproaches depend heavily on human expertise, presenting substantial challenges\nfor practical deployment, particularly in scenarios with limited prior\ngame-specific knowledge. In this paper, we propose an algorithm called\nMulti-Game Target Return Optimizer (MTRO) to autonomously determine\ngame-specific target returns within the Multi-Game Decision Transformer\nframework using solely offline datasets. MTRO addresses the existing\nlimitations by automating the target return configuration process, leveraging\nenvironmental reward information extracted from offline datasets. Notably, MTRO\ndoes not require additional training, enabling seamless integration into\nexisting Multi-Game Decision Transformer architectures. Our experimental\nevaluations on Atari games demonstrate that MTRO enhances the performance of RL\npolicies across a wide array of games, underscoring its potential to advance\nthe field of autonomous agent development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.02311v1",
    "published_date": "2025-03-04 06:13:53 UTC",
    "updated_date": "2025-03-04 06:13:53 UTC"
  },
  {
    "arxiv_id": "2503.02303v1",
    "title": "Flexible Prefrontal Control over Hippocampal Episodic Memory for Goal-Directed Generalization",
    "authors": [
      "Yicong Zheng",
      "Nora Wolf",
      "Charan Ranganath",
      "Randall C. O'Reilly",
      "Kevin L. McKee"
    ],
    "abstract": "Many tasks require flexibly modifying perception and behavior based on\ncurrent goals. Humans can retrieve episodic memories from days to years ago,\nusing them to contextualize and generalize behaviors across novel but\nstructurally related situations. The brain's ability to control episodic\nmemories based on task demands is often attributed to interactions between the\nprefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement\nlearning model that incorporates a PFC-HPC interaction mechanism for\ngoal-directed generalization. In our model, the PFC learns to generate\nquery-key representations to encode and retrieve goal-relevant episodic\nmemories, modulating HPC memories top-down based on current task demands.\nMoreover, the PFC adapts its encoding and retrieval strategies dynamically when\nfaced with multiple goals presented in a blocked, rather than interleaved,\nmanner. Our results show that: (1) combining working memory with selectively\nretrieved episodic memory allows transfer of decisions among similar\nenvironments or situations, (2) top-down control from PFC over HPC improves\nlearning of arbitrary structural associations between events for generalization\nto novel environments compared to a bottom-up sensory-driven approach, and (3)\nthe PFC encodes generalizable representations during both encoding and\nretrieval of goal-relevant memories, whereas the HPC exhibits event-specific\nrepresentations. Together, these findings highlight the importance of\ngoal-directed prefrontal control over hippocampal episodic memory for\ndecision-making in novel situations and suggest a computational mechanism by\nwhich PFC-HPC interactions enable flexible behavior.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02303v1",
    "published_date": "2025-03-04 06:04:54 UTC",
    "updated_date": "2025-03-04 06:04:54 UTC"
  },
  {
    "arxiv_id": "2503.02296v1",
    "title": "Memorize or Generalize? Evaluating LLM Code Generation with Evolved Questions",
    "authors": [
      "Wentao Chen",
      "Lizhe Zhang",
      "Li Zhong",
      "Letian Peng",
      "Zilong Wang",
      "Jingbo Shang"
    ],
    "abstract": "Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02296v1",
    "published_date": "2025-03-04 05:39:24 UTC",
    "updated_date": "2025-03-04 05:39:24 UTC"
  },
  {
    "arxiv_id": "2503.02284v1",
    "title": "Semi-Supervised Audio-Visual Video Action Recognition with Audio Source Localization Guided Mixup",
    "authors": [
      "Seokun Kang",
      "Taehwan Kim"
    ],
    "abstract": "Video action recognition is a challenging but important task for\nunderstanding and discovering what the video does. However, acquiring\nannotations for a video is costly, and semi-supervised learning (SSL) has been\nstudied to improve performance even with a small number of labeled data in the\ntask. Prior studies for semi-supervised video action recognition have mostly\nfocused on using single modality - visuals - but the video is multi-modal, so\nutilizing both visuals and audio would be desirable and improve performance\nfurther, which has not been explored well. Therefore, we propose audio-visual\nSSL for video action recognition, which uses both visual and audio together,\neven with quite a few labeled data, which is challenging. In addition, to\nmaximize the information of audio and video, we propose a novel audio source\nlocalization-guided mixup method that considers inter-modal relations between\nvideo and audio modalities. In experiments on UCF-51, Kinetics-400, and\nVGGSound datasets, our model shows the superior performance of the proposed\nsemi-supervised audio-visual action recognition framework and audio source\nlocalization-guided mixup.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02284v1",
    "published_date": "2025-03-04 05:13:56 UTC",
    "updated_date": "2025-03-04 05:13:56 UTC"
  },
  {
    "arxiv_id": "2503.02269v1",
    "title": "Experience Replay with Random Reshuffling",
    "authors": [
      "Yasuhiro Fujita"
    ],
    "abstract": "Experience replay is a key component in reinforcement learning for\nstabilizing learning and improving sample efficiency. Its typical\nimplementation samples transitions with replacement from a replay buffer. In\ncontrast, in supervised learning with a fixed dataset, it is a common practice\nto shuffle the dataset every epoch and consume data sequentially, which is\ncalled random reshuffling (RR). RR enjoys theoretically better convergence\nproperties and has been shown to outperform with-replacement sampling\nempirically. To leverage the benefits of RR in reinforcement learning, we\npropose sampling methods that extend RR to experience replay, both in uniform\nand prioritized settings. We evaluate our sampling methods on Atari benchmarks,\ndemonstrating their effectiveness in deep reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02269v1",
    "published_date": "2025-03-04 04:37:22 UTC",
    "updated_date": "2025-03-04 04:37:22 UTC"
  },
  {
    "arxiv_id": "2503.02268v1",
    "title": "AppAgentX: Evolving GUI Agents as Proficient Smartphone Users",
    "authors": [
      "Wenjia Jiang",
      "Yangyang Zhuang",
      "Chenxi Song",
      "Xu Yang",
      "Chi Zhang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have led to the\ndevelopment of intelligent LLM-based agents capable of interacting with\ngraphical user interfaces (GUIs). These agents demonstrate strong reasoning and\nadaptability, enabling them to perform complex tasks that traditionally\nrequired predefined rules. However, the reliance on step-by-step reasoning in\nLLM-based agents often results in inefficiencies, particularly for routine\ntasks. In contrast, traditional rule-based systems excel in efficiency but lack\nthe intelligence and flexibility to adapt to novel scenarios. To address this\nchallenge, we propose a novel evolutionary framework for GUI agents that\nenhances operational efficiency while retaining intelligence and flexibility.\nOur approach incorporates a memory mechanism that records the agent's task\nexecution history. By analyzing this history, the agent identifies repetitive\naction sequences and evolves high-level actions that act as shortcuts,\nreplacing these low-level operations and improving efficiency. This allows the\nagent to focus on tasks requiring more complex reasoning, while simplifying\nroutine actions. Experimental results on multiple benchmark tasks demonstrate\nthat our approach significantly outperforms existing methods in both efficiency\nand accuracy. The code will be open-sourced to support further research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02268v1",
    "published_date": "2025-03-04 04:34:09 UTC",
    "updated_date": "2025-03-04 04:34:09 UTC"
  },
  {
    "arxiv_id": "2503.02267v1",
    "title": "REAct: Rational Exponential Activation for Better Learning and Generalization in PINNs",
    "authors": [
      "Sourav Mishra",
      "Shreya Hallikeri",
      "Suresh Sundaram"
    ],
    "abstract": "Physics-Informed Neural Networks (PINNs) offer a promising approach to\nsimulating physical systems. Still, their application is limited by\noptimization challenges, mainly due to the lack of activation functions that\ngeneralize well across several physical systems. Existing activation functions\noften lack such flexibility and generalization power. To address this issue, we\nintroduce Rational Exponential Activation (REAct), a generalized form of tanh\nconsisting of four learnable shape parameters. Experiments show that REAct\noutperforms many standard and benchmark activations, achieving an MSE three\norders of magnitude lower than tanh on heat problems and generalizing well to\nfiner grids and points beyond the training domain. It also excels at function\napproximation tasks and improves noise rejection in inverse problems, leading\nto more accurate parameter estimates across varying noise levels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 5 tables, 1 figure; Accepted at ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02267v1",
    "published_date": "2025-03-04 04:28:59 UTC",
    "updated_date": "2025-03-04 04:28:59 UTC"
  },
  {
    "arxiv_id": "2503.02249v1",
    "title": "Large Language Models as Natural Selector for Embodied Soft Robot Design",
    "authors": [
      "Changhe Chen",
      "Xiaohao Xu",
      "Xiangdong Wang",
      "Xiaonan Huang"
    ],
    "abstract": "Designing soft robots is a complex and iterative process that demands\ncross-disciplinary expertise in materials science, mechanics, and control,\noften relying on intuition and extensive experimentation. While Large Language\nModels (LLMs) have demonstrated impressive reasoning abilities, their capacity\nto learn and apply embodied design principles--crucial for creating functional\nrobotic systems--remains largely unexplored. This paper introduces\nRoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn\nrepresentations of soft robot designs that effectively bridge the gap between\nhigh-level task descriptions and low-level morphological and material choices.\nRoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft\nrobot design challenges, spanning robotic locomotion, manipulation, and\nbalancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal\nthat while these models exhibit promising capabilities in learning design\nrepresentations, they struggle with fine-grained distinctions between designs\nwith subtle performance differences. We further demonstrate the practical\nutility of LLMs for robot design initialization. Our code and benchmark will be\navailable to encourage the community to foster this exciting research\ndirection.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02249v1",
    "published_date": "2025-03-04 03:55:10 UTC",
    "updated_date": "2025-03-04 03:55:10 UTC"
  },
  {
    "arxiv_id": "2503.02239v1",
    "title": "V2X-LLM: Enhancing V2X Integration and Understanding in Connected Vehicle Corridors",
    "authors": [
      "Keshu Wu",
      "Pei Li",
      "Yang Zhou",
      "Rui Gan",
      "Junwei You",
      "Yang Cheng",
      "Jingwen Zhu",
      "Steven T. Parker",
      "Bin Ran",
      "David A. Noyce",
      "Zhengzhong Tu"
    ],
    "abstract": "The advancement of Connected and Automated Vehicles (CAVs) and\nVehicle-to-Everything (V2X) offers significant potential for enhancing\ntransportation safety, mobility, and sustainability. However, the integration\nand analysis of the diverse and voluminous V2X data, including Basic Safety\nMessages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial\nchallenges, especially on Connected Vehicle Corridors. These challenges include\nmanaging large data volumes, ensuring real-time data integration, and\nunderstanding complex traffic scenarios. Although these projects have developed\nan advanced CAV data pipeline that enables real-time communication between\nvehicles, infrastructure, and other road users for managing connected vehicle\nand roadside unit (RSU) data, significant hurdles in data comprehension and\nreal-time scenario analysis and reasoning persist. To address these issues, we\nintroduce the V2X-LLM framework, a novel enhancement to the existing CV data\npipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the\nunderstanding and real-time analysis of V2X data. The framework includes four\nkey tasks: Scenario Explanation, offering detailed narratives of traffic\nconditions; V2X Data Description, detailing vehicle and infrastructure\nstatuses; State Prediction, forecasting future traffic states; and Navigation\nAdvisory, providing optimized routing instructions. By integrating LLM-driven\nreasoning with V2X data within the data pipeline, the V2X-LLM framework offers\nreal-time feedback and decision support for traffic management. This\nintegration enhances the accuracy of traffic analysis, safety, and traffic\noptimization. Demonstrations in a real-world urban corridor highlight the\nframework's potential to advance intelligent transportation systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02239v1",
    "published_date": "2025-03-04 03:28:30 UTC",
    "updated_date": "2025-03-04 03:28:30 UTC"
  },
  {
    "arxiv_id": "2503.02235v1",
    "title": "Deficient Excitation in Parameter Learning",
    "authors": [
      "Ganghui Cao",
      "Shimin Wang",
      "Martin Guay",
      "Jinzhi Wang",
      "Zhisheng Duan",
      "Marios M. Polycarpou"
    ],
    "abstract": "This paper investigates parameter learning problems under deficient\nexcitation (DE). The DE condition is a rank-deficient, and therefore, a more\ngeneral evolution of the well-known persistent excitation condition. Under the\nDE condition, a proposed online algorithm is able to calculate the identifiable\nand non-identifiable subspaces, and finally give an optimal parameter estimate\nin the sense of least squares. In particular, the learning error within the\nidentifiable subspace exponentially converges to zero in the noise-free case,\neven without persistent excitation. The DE condition also provides a new\nperspective for solving distributed parameter learning problems, where the\nchallenge is posed by local regressors that are often insufficiently excited.\nTo improve knowledge of the unknown parameters, a cooperative learning protocol\nis proposed for a group of estimators that collect measured information under\ncomplementary DE conditions. This protocol allows each local estimator to\noperate locally in its identifiable subspace, and reach a consensus with\nneighbours in its non-identifiable subspace. As a result, the task of\nestimating unknown parameters can be achieved in a distributed way using\ncooperative local estimators. Application examples in system identification are\ngiven to demonstrate the effectiveness of the theoretical results developed in\nthis paper.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "eess.SP",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "16 pages,9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.02235v1",
    "published_date": "2025-03-04 03:18:13 UTC",
    "updated_date": "2025-03-04 03:18:13 UTC"
  },
  {
    "arxiv_id": "2503.02233v2",
    "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
    "authors": [
      "Hang Zheng",
      "Hongshen Xu",
      "Yuncong Liu",
      "Lu Chen",
      "Pascale Fung",
      "Kai Yu"
    ],
    "abstract": "Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02233v2",
    "published_date": "2025-03-04 03:16:02 UTC",
    "updated_date": "2025-03-12 07:42:04 UTC"
  },
  {
    "arxiv_id": "2503.02228v1",
    "title": "One Patient's Annotation is Another One's Initialization: Towards Zero-Shot Surgical Video Segmentation with Cross-Patient Initialization",
    "authors": [
      "Seyed Amir Mousavi",
      "Utku Ozbulak",
      "Francesca Tozzi",
      "Nikdokht Rashidian",
      "Wouter Willaert",
      "Joris Vankerschaver",
      "Wesley De Neve"
    ],
    "abstract": "Video object segmentation is an emerging technology that is well-suited for\nreal-time surgical video segmentation, offering valuable clinical assistance in\nthe operating room by ensuring consistent frame tracking. However, its adoption\nis limited by the need for manual intervention to select the tracked object,\nmaking it impractical in surgical settings. In this work, we tackle this\nchallenge with an innovative solution: using previously annotated frames from\nother patients as the tracking frames. We find that this unconventional\napproach can match or even surpass the performance of using patients' own\ntracking frames, enabling more autonomous and efficient AI-assisted surgical\nworkflows. Furthermore, we analyze the benefits and limitations of this\napproach, highlighting its potential to enhance segmentation accuracy while\nreducing the need for manual input. Our findings provide insights into key\nfactors influencing performance, offering a foundation for future research on\noptimizing cross-patient frame selection for real-time surgical video analysis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02228v1",
    "published_date": "2025-03-04 03:11:03 UTC",
    "updated_date": "2025-03-04 03:11:03 UTC"
  },
  {
    "arxiv_id": "2503.02221v1",
    "title": "Attention Bootstrapping for Multi-Modal Test-Time Adaptation",
    "authors": [
      "Yusheng Zhao",
      "Junyu Luo",
      "Xiao Luo",
      "Jinsheng Huang",
      "Jingyang Yuan",
      "Zhiping Xiao",
      "Ming Zhang"
    ],
    "abstract": "Test-time adaptation aims to adapt a well-trained model to potential\ndistribution shifts at test time using only unlabeled test data, without access\nto the original training data. While previous efforts mainly focus on a single\nmodality, test-time distribution shift in the multi-modal setting is more\ncomplex and calls for new solutions. This paper tackles the problem of\nmulti-modal test-time adaptation by proposing a novel method named Attention\nBootstrapping with Principal Entropy Minimization (ABPEM). We observe that\ntest-time distribution shift causes misalignment across modalities, leading to\na large gap between intra-modality discrepancies (measured by self-attention)\nand inter-modality discrepancies (measured by cross-attention). We name this\nthe attention gap. This attention gap widens with more severe distribution\nshifts, hindering effective modality fusion. To mitigate this attention gap and\nencourage better modality fusion, we propose attention bootstrapping that\npromotes cross-attention with the guidance of self-attention. Moreover, to\nreduce the gradient noise in the commonly-used entropy minimization, we adopt\nprincipal entropy minimization, a refinement of entropy minimization that\nreduces gradient noise by focusing on the principal parts of entropy, excluding\nless reliable gradient information. Extensive experiments on the benchmarks\nvalidate the effectiveness of the proposed ABPEM in comparison with competing\nbaselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02221v1",
    "published_date": "2025-03-04 02:53:53 UTC",
    "updated_date": "2025-03-04 02:53:53 UTC"
  },
  {
    "arxiv_id": "2503.04808v1",
    "title": "Learning from Failures in Multi-Attempt Reinforcement Learning",
    "authors": [
      "Stephen Chung",
      "Wenyu Du",
      "Jie Fu"
    ],
    "abstract": "Recent advancements in reinforcement learning (RL) for large language models\n(LLMs), exemplified by DeepSeek R1, have shown that even a simple\nquestion-answering task can substantially improve an LLM's reasoning\ncapabilities. In this work, we extend this approach by modifying the task into\na multi-attempt setting. Instead of generating a single response per question,\nthe model is given multiple attempts, with feedback provided after incorrect\nresponses. The multi-attempt task encourages the model to refine its previous\nattempts and improve search efficiency. Experimental results show that even a\nsmall LLM trained on a multi-attempt task achieves significantly higher\naccuracy when evaluated with more attempts, improving from 45.6% with 1 attempt\nto 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM\ntrained on a standard single-turn task exhibits only a marginal improvement,\nincreasing from 42.3% to 43.2% when given more attempts during evaluation. The\nresults indicate that, compared to the standard single-turn task, an LLM\ntrained on a multi-attempt task achieves slightly better performance on math\nbenchmarks while also learning to refine its responses more effectively based\non user feedback. Full code is available at\nhttps://github.com/DualityRL/multi-attempt",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.04808v1",
    "published_date": "2025-03-04 02:53:39 UTC",
    "updated_date": "2025-03-04 02:53:39 UTC"
  },
  {
    "arxiv_id": "2503.02199v1",
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "authors": [
      "Ailin Deng",
      "Tri Cao",
      "Zhirui Chen",
      "Bryan Hooi"
    ],
    "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith\nin text''} phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02199v1",
    "published_date": "2025-03-04 02:21:07 UTC",
    "updated_date": "2025-03-04 02:21:07 UTC"
  },
  {
    "arxiv_id": "2503.02197v1",
    "title": "ATLaS: Agent Tuning via Learning Critical Steps",
    "authors": [
      "Zhixun Chen",
      "Ming Li",
      "Yuxuan Huang",
      "Yali Du",
      "Meng Fang",
      "Tianyi Zhou"
    ],
    "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization\ncapabilities across multi-domain tasks. Existing agent tuning approaches\ntypically employ supervised finetuning on entire expert trajectories. However,\nbehavior-cloning of full trajectories can introduce expert bias and weaken\ngeneralization to states not covered by the expert data. Additionally, critical\nsteps, such as planning, complex reasoning for intermediate subtasks, and\nstrategic decision-making, are essential to success in agent tasks, so learning\nthese steps is the key to improving LLM agents. For more effective and\nefficient agent tuning, we propose ATLaS that identifies the critical steps in\nexpert trajectories and finetunes LLMs solely on these steps with reduced\ncosts. By steering the training's focus to a few critical steps, our method\nmitigates the risk of overfitting entire trajectories and promotes\ngeneralization across different environments and tasks. In extensive\nexperiments, an LLM finetuned on only 30% critical steps selected by ATLaS\noutperforms the LLM finetuned on all steps and recent open-source LLM agents.\nATLaS maintains and improves base LLM skills as generalist agents interacting\nwith diverse environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02197v1",
    "published_date": "2025-03-04 02:14:55 UTC",
    "updated_date": "2025-03-04 02:14:55 UTC"
  },
  {
    "arxiv_id": "2503.05805v1",
    "title": "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
    "authors": [
      "Dom Huh",
      "Prasant Mohapatra"
    ],
    "abstract": "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05805v1",
    "published_date": "2025-03-04 02:07:24 UTC",
    "updated_date": "2025-03-04 02:07:24 UTC"
  },
  {
    "arxiv_id": "2503.04807v2",
    "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data",
    "authors": [
      "Hyeonseok Moon",
      "Jaehyung Seo",
      "Heuiseok Lim"
    ],
    "abstract": "Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.04807v2",
    "published_date": "2025-03-04 02:04:58 UTC",
    "updated_date": "2025-03-11 07:10:07 UTC"
  },
  {
    "arxiv_id": "2503.10649v1",
    "title": "Measuring Political Preferences in AI Systems: An Integrative Approach",
    "authors": [
      "David Rozado"
    ],
    "abstract": "Political biases in Large Language Model (LLM)-based artificial intelligence\n(AI) systems, such as OpenAI's ChatGPT or Google's Gemini, have been previously\nreported. While several prior studies have attempted to quantify these biases\nusing political orientation tests, such approaches are limited by potential\ntests' calibration biases and constrained response formats that do not reflect\nreal-world human-AI interactions. This study employs a multi-method approach to\nassess political bias in leading AI systems, integrating four complementary\nmethodologies: (1) linguistic comparison of AI-generated text with the language\nused by Republican and Democratic U.S. Congress members, (2) analysis of\npolitical viewpoints embedded in AI-generated policy recommendations, (3)\nsentiment analysis of AI-generated text toward politically affiliated public\nfigures, and (4) standardized political orientation testing. Results indicate a\nconsistent left-leaning bias across most contemporary AI systems, with arguably\nvarying degrees of intensity. However, this bias is not an inherent feature of\nLLMs; prior research demonstrates that fine-tuning with politically skewed data\ncan realign these models across the ideological spectrum. The presence of\nsystematic political bias in AI systems poses risks, including reduced\nviewpoint diversity, increased societal polarization, and the potential for\npublic mistrust in AI technologies. To mitigate these risks, AI systems should\nbe designed to prioritize factual accuracy while maintaining neutrality on most\nlawful normative issues. Furthermore, independent monitoring platforms are\nnecessary to ensure transparency, accountability, and responsible AI\ndevelopment.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "Measuring Political Preferences in AI Systems. Report. Available:\n  https://manhattan.institute/article/measuring-political-preferences-in-ai-systems-an-integrative-approach/",
    "pdf_url": "http://arxiv.org/pdf/2503.10649v1",
    "published_date": "2025-03-04 01:40:28 UTC",
    "updated_date": "2025-03-04 01:40:28 UTC"
  },
  {
    "arxiv_id": "2503.02180v1",
    "title": "Discrete Differential Evolution Particle Swarm Optimization Algorithm for Energy Saving Flexible Job Shop Scheduling Problem Considering Machine Multi States",
    "authors": [
      "Da Wang",
      "Yu Zhang",
      "Kai Zhang",
      "Junqing Li",
      "Dengwang Li"
    ],
    "abstract": "As the continuous deepening of low-carbon emission reduction policies, the\nmanufacturing industries urgently need sensible energy-saving scheduling\nschemes to achieve the balance between improving production efficiency and\nreducing energy consumption. In energy-saving scheduling, reasonable machine\nstates-switching is a key point to achieve expected goals, i.e., whether the\nmachines need to switch speed between different operations, and whether the\nmachines need to add extra setup time between different jobs. Regarding this\nmatter, this work proposes a novel machine multi states-based energy saving\nflexible job scheduling problem (EFJSP-M), which simultaneously takes into\naccount machine multi speeds and setup time. To address the proposed EFJSP-M, a\nkind of discrete differential evolution particle swarm optimization algorithm\n(D-DEPSO) is designed. In specific, D-DEPSO includes a hybrid initialization\nstrategy to improve the initial population performance, an updating mechanism\nembedded with differential evolution operators to enhance population diversity,\nand a critical path variable neighborhood search strategy to expand the\nsolution space. At last, based on datasets DPs and MKs, the experiment results\ncompared with five state-of-the-art algorithms demonstrate the feasible of\nEFJSP-M and the superior of D-DEPSO.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02180v1",
    "published_date": "2025-03-04 01:40:24 UTC",
    "updated_date": "2025-03-04 01:40:24 UTC"
  },
  {
    "arxiv_id": "2503.02175v1",
    "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
    "authors": [
      "Saeed Ranjbar Alvar",
      "Gursimran Singh",
      "Mohammad Akbari",
      "Yong Zhang"
    ],
    "abstract": "Large Multimodal Models (LMMs) have emerged as powerful models capable of\nunderstanding various data modalities, including text, images, and videos. LMMs\nencode both text and visual data into tokens that are then combined and\nprocessed by an integrated Large Language Model (LLM). Including visual tokens\nsubstantially increases the total token count, often by thousands. The\nincreased input length for LLM significantly raises the complexity of\ninference, resulting in high latency in LMMs. To address this issue, token\npruning methods, which remove part of the visual tokens, are proposed. The\nexisting token pruning methods either require extensive calibration and\nfine-tuning or rely on suboptimal importance metrics which results in increased\nredundancy among the retained tokens. In this paper, we first formulate token\npruning as Max-Min Diversity Problem (MMDP) where the goal is to select a\nsubset such that the diversity among the selected {tokens} is maximized. Then,\nwe solve the MMDP to obtain the selected subset and prune the rest. The\nproposed method, DivPrune, reduces redundancy and achieves the highest\ndiversity of the selected tokens. By ensuring high diversity, the selected\ntokens better represent the original tokens, enabling effective performance\neven at high pruning ratios without requiring fine-tuning. Extensive\nexperiments with various LMMs show that DivPrune achieves state-of-the-art\naccuracy over 16 image- and video-language datasets. Additionally, DivPrune\nreduces both the end-to-end latency and GPU memory usage for the tested models.\nThe code is available $\\href{https://github.com/vbdi/divprune}{\\text{here}}$.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02175v1",
    "published_date": "2025-03-04 01:33:14 UTC",
    "updated_date": "2025-03-04 01:33:14 UTC"
  },
  {
    "arxiv_id": "2503.02174v1",
    "title": "Adversarial Tokenization",
    "authors": [
      "Renato Lui Geh",
      "Zilei Shao",
      "Guy Van den Broeck"
    ],
    "abstract": "Current LLM pipelines account for only one possible tokenization for a given\nstring, ignoring exponentially many alternative tokenizations during training\nand inference. For example, the standard Llama3 tokenization of penguin is\n[p,enguin], yet [peng,uin] is another perfectly valid alternative. In this\npaper, we show that despite LLMs being trained solely on one tokenization, they\nstill retain semantic understanding of other tokenizations, raising questions\nabout their implications in LLM safety. Put succinctly, we answer the following\nquestion: can we adversarially tokenize an obviously malicious string to evade\nsafety and alignment restrictions? We show that not only is adversarial\ntokenization an effective yet previously neglected axis of attack, but it is\nalso competitive against existing state-of-the-art adversarial approaches\nwithout changing the text of the harmful request. We empirically validate this\nexploit across three state-of-the-art LLMs and adversarial datasets, revealing\na previously unknown vulnerability in subword models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02174v1",
    "published_date": "2025-03-04 01:31:17 UTC",
    "updated_date": "2025-03-04 01:31:17 UTC"
  },
  {
    "arxiv_id": "2503.02172v1",
    "title": "KGCompiler: Deep Learning Compilation Optimization for Knowledge Graph Complex Logical Query Answering",
    "authors": [
      "Hongyu Lin",
      "Haoran Luo",
      "Hanghang Cao",
      "Yang Liu",
      "Shihao Gao",
      "Kaichun Yao",
      "Libo Zhang",
      "Mingjie Xing",
      "Yanjun Wu"
    ],
    "abstract": "Complex Logical Query Answering (CLQA) involves intricate multi-hop logical\nreasoning over large-scale and potentially incomplete Knowledge Graphs (KGs).\nAlthough existing CLQA algorithms achieve high accuracy in answering such\nqueries, their reasoning time and memory usage scale significantly with the\nnumber of First-Order Logic (FOL) operators involved, creating serious\nchallenges for practical deployment. In addition, current research primarily\nfocuses on algorithm-level optimizations for CLQA tasks, often overlooking\ncompiler-level optimizations, which can offer greater generality and\nscalability. To address these limitations, we introduce a Knowledge Graph\nCompiler, namely KGCompiler, the first deep learning compiler specifically\ndesigned for CLQA tasks. By incorporating KG-specific optimizations proposed in\nthis paper, KGCompiler enhances the reasoning performance of CLQA algorithms\nwithout requiring additional manual modifications to their implementations. At\nthe same time, it significantly reduces memory usage. Extensive experiments\ndemonstrate that KGCompiler accelerates CLQA algorithms by factors ranging from\n1.04x to 8.26x, with an average speedup of 3.71x. We also provide an interface\nto enable hands-on experience with KGCompiler.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.02172v1",
    "published_date": "2025-03-04 01:24:32 UTC",
    "updated_date": "2025-03-04 01:24:32 UTC"
  },
  {
    "arxiv_id": "2503.02170v1",
    "title": "Adaptive Camera Sensor for Vision Models",
    "authors": [
      "Eunsu Baek",
      "Sunghwan Han",
      "Taesik Gong",
      "Hyung-Sin Kim"
    ],
    "abstract": "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com/Edw2n/Lens.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The International Conference on Learning Representations (ICLR 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.02170v1",
    "published_date": "2025-03-04 01:20:23 UTC",
    "updated_date": "2025-03-04 01:20:23 UTC"
  },
  {
    "arxiv_id": "2503.02157v1",
    "title": "MedHEval: Benchmarking Hallucinations and Mitigation Strategies in Medical Large Vision-Language Models",
    "authors": [
      "Aofei Chang",
      "Le Huang",
      "Parminder Bhatia",
      "Taha Kass-Hout",
      "Fenglong Ma",
      "Cao Xiao"
    ],
    "abstract": "Large Vision Language Models (LVLMs) are becoming increasingly important in\nthe medical domain, yet Medical LVLMs (Med-LVLMs) frequently generate\nhallucinations due to limited expertise and the complexity of medical\napplications. Existing benchmarks fail to effectively evaluate hallucinations\nbased on their underlying causes and lack assessments of mitigation strategies.\nTo address this gap, we introduce MedHEval, a novel benchmark that\nsystematically evaluates hallucinations and mitigation strategies in Med-LVLMs\nby categorizing them into three underlying causes: visual misinterpretation,\nknowledge deficiency, and context misalignment. We construct a diverse set of\nclose- and open-ended medical VQA datasets with comprehensive evaluation\nmetrics to assess these hallucination types. We conduct extensive experiments\nacross 11 popular (Med)-LVLMs and evaluate 7 state-of-the-art hallucination\nmitigation techniques. Results reveal that Med-LVLMs struggle with\nhallucinations arising from different causes while existing mitigation methods\nshow limited effectiveness, especially for knowledge- and context-based errors.\nThese findings underscore the need for improved alignment training and\nspecialized mitigation strategies to enhance Med-LVLMs' reliability. MedHEval\nestablishes a standardized framework for evaluating and mitigating medical\nhallucinations, guiding the development of more trustworthy Med-LVLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint, under review",
    "pdf_url": "http://arxiv.org/pdf/2503.02157v1",
    "published_date": "2025-03-04 00:40:09 UTC",
    "updated_date": "2025-03-04 00:40:09 UTC"
  },
  {
    "arxiv_id": "2503.02156v1",
    "title": "MobRFFI: Non-cooperative Device Re-identification for Mobility Intelligence",
    "authors": [
      "Stepan Mazokha",
      "Fanchen Bao",
      "George Sklivanitis",
      "Jason O. Hallstrom"
    ],
    "abstract": "WiFi-based mobility monitoring in urban environments can provide valuable\ninsights into pedestrian and vehicle movements. However, MAC address\nrandomization introduces a significant obstacle in accurately estimating\ncongestion levels and path trajectories. To this end, we consider radio\nfrequency fingerprinting and re-identification for attributing WiFi traffic to\nemitting devices without the use of MAC addresses.\n  We present MobRFFI, an AI-based device fingerprinting and re-identification\nframework for WiFi networks that leverages an encoder deep learning model to\nextract unique features based on WiFi chipset hardware impairments. It is\nentirely independent of frame type. When evaluated on the WiFi fingerprinting\ndataset WiSig, our approach achieves 94% and 100% device accuracy in multi-day\nand single-day re-identification scenarios, respectively.\n  We also collect a novel dataset, MobRFFI, for granular multi-receiver WiFi\ndevice fingerprinting evaluation. Using the dataset, we demonstrate that the\ncombination of fingerprints from multiple receivers boosts re-identification\nperformance from 81% to 100% on a single-day scenario and from 41% to 100% on a\nmulti-day scenario.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "eess.SP",
    "comment": "10 pages, 9 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.02156v1",
    "published_date": "2025-03-04 00:39:50 UTC",
    "updated_date": "2025-03-04 00:39:50 UTC"
  },
  {
    "arxiv_id": "2503.02154v1",
    "title": "AugFL: Augmenting Federated Learning with Pretrained Models",
    "authors": [
      "Sheng Yue",
      "Zerui Qin",
      "Yongheng Deng",
      "Ju Ren",
      "Yaoxue Zhang",
      "Junshan Zhang"
    ],
    "abstract": "Federated Learning (FL) has garnered widespread interest in recent years.\nHowever, owing to strict privacy policies or limited storage capacities of\ntraining participants such as IoT devices, its effective deployment is often\nimpeded by the scarcity of training data in practical decentralized learning\nenvironments. In this paper, we study enhancing FL with the aid of (large)\npre-trained models (PMs), that encapsulate wealthy general/domain-agnostic\nknowledge, to alleviate the data requirement in conducting FL from scratch.\nSpecifically, we consider a networked FL system formed by a central server and\ndistributed clients. First, we formulate the PM-aided personalized FL as a\nregularization-based federated meta-learning problem, where clients join forces\nto learn a meta-model with knowledge transferred from a private PM stored at\nthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, to\noptimize the problem with no need to expose the PM or incur additional\ncomputational costs to local clients. Further, we establish theoretical\nguarantees for AugFL in terms of communication complexity, adaptation\nperformance, and the benefit of knowledge transfer in general non-convex cases.\nExtensive experiments corroborate the efficacy and superiority of AugFL over\nexisting baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "to be published in Transactions on Networking",
    "pdf_url": "http://arxiv.org/pdf/2503.02154v1",
    "published_date": "2025-03-04 00:37:33 UTC",
    "updated_date": "2025-03-04 00:37:33 UTC"
  },
  {
    "arxiv_id": "2503.02138v1",
    "title": "Elliptic Loss Regularization",
    "authors": [
      "Ali Hasan",
      "Haoming Yang",
      "Yuting Ng",
      "Vahid Tarokh"
    ],
    "abstract": "Regularizing neural networks is important for anticipating model behavior in\nregions of the data space that are not well represented. In this work, we\npropose a regularization technique for enforcing a level of smoothness in the\nmapping between the data input space and the loss value. We specify the level\nof regularity by requiring that the loss of the network satisfies an elliptic\noperator over the data domain. To do this, we modify the usual empirical risk\nminimization objective such that we instead minimize a new objective that\nsatisfies an elliptic operator over points within the domain. This allows us to\nuse existing theory on elliptic operators to anticipate the behavior of the\nerror for points outside the training set. We propose a tractable computational\nmethod that approximates the behavior of the elliptic operator while being\ncomputationally efficient. Finally, we analyze the properties of the proposed\nregularization to understand the performance on common problems of distribution\nshift and group imbalance. Numerical experiments confirm the utility of the\nproposed regularization technique.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.02138v1",
    "published_date": "2025-03-04 00:08:08 UTC",
    "updated_date": "2025-03-04 00:08:08 UTC"
  }
]
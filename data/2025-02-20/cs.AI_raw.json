[
  {
    "arxiv_id": "2502.15090v1",
    "title": "Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans",
    "authors": [
      "Masha Fedzechkina",
      "Eleonora Gualdoni",
      "Sinead Williamson",
      "Katherine Metcalf",
      "Skyler Seto",
      "Barry-John Theobald"
    ],
    "abstract": "Modern large language models (LLMs) achieve impressive performance on some\ntasks, while exhibiting distinctly non-human-like behaviors on others. This\nraises the question of how well the LLM's learned representations align with\nhuman representations. In this work, we introduce a novel approach to the study\nof representation alignment: we adopt a method from research on activation\nsteering to identify neurons responsible for specific concepts (e.g., 'cat')\nand then analyze the corresponding activation patterns. Our findings reveal\nthat LLM representations closely align with human representations inferred from\nbehavioral data. Notably, this alignment surpasses that of word embeddings,\nwhich have been center stage in prior work on human and model alignment.\nAdditionally, our approach enables a more granular view of how LLMs represent\nconcepts. Specifically, we show that LLMs organize concepts in a way that\nreflects hierarchical relationships interpretable to humans (e.g.,\n'animal'-'dog').",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15090v1",
    "published_date": "2025-02-20 23:08:03 UTC",
    "updated_date": "2025-02-20 23:08:03 UTC"
  },
  {
    "arxiv_id": "2502.15082v1",
    "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
    "authors": [
      "Vaidehi Patil",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "User specifications or legal frameworks often require information to be\nremoved from pretrained models, including large language models (LLMs). This\nrequires deleting or \"forgetting\" a set of data points from an already-trained\nmodel, which typically degrades its performance on other data points. Thus, a\nbalance must be struck between removing information and keeping the model's\nother abilities intact, with a failure to balance this trade-off leading to\npoor deletion or an unusable model. To this end, we propose UPCORE\n(Utility-Preserving Coreset Selection), a method-agnostic data selection\nframework for mitigating collateral damage during unlearning. Finding that the\nmodel damage is correlated with the variance of the model's representations on\nthe forget set, we selectively prune the forget set to remove outliers, thereby\nminimizing model degradation after unlearning. We evaluate UPCORE across three\nstandard unlearning methods consistently achieving a superior balance between\nthe competing objectives of deletion efficacy and model preservation. To better\nevaluate this trade-off, we introduce a new metric, measuring the\narea-under-the-curve (AUC) across standard metrics. We find that UPCORE\nimproves both standard metrics and AUC, benefitting from positive transfer\nbetween the coreset and pruned points while reducing negative transfer from the\nforget set to points outside of it.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Code: https://github.com/Vaidehi99/UPCORE",
    "pdf_url": "http://arxiv.org/pdf/2502.15082v1",
    "published_date": "2025-02-20 22:51:10 UTC",
    "updated_date": "2025-02-20 22:51:10 UTC"
  },
  {
    "arxiv_id": "2502.15079v1",
    "title": "Can Hallucination Correction Improve Video-Language Alignment?",
    "authors": [
      "Lingjun Zhao",
      "Mingyang Xie",
      "Paola Cascante-Bonilla",
      "Hal Daumé III",
      "Kwonjoon Lee"
    ],
    "abstract": "Large Vision-Language Models often generate hallucinated content that is not\ngrounded in its visual inputs. While prior work focuses on mitigating\nhallucinations, we instead explore leveraging hallucination correction as a\ntraining objective to improve video-language alignment. We introduce HACA, a\nself-training framework learning to correct hallucinations in descriptions that\ndo not align with the video content. By identifying and correcting\ninconsistencies, HACA enhances the model's ability to align video and textual\nrepresentations for spatio-temporal reasoning. Our experimental results show\nconsistent gains in video-caption binding and text-to-video retrieval tasks,\ndemonstrating that hallucination correction-inspired tasks serve as an\neffective strategy for improving vision and language alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15079v1",
    "published_date": "2025-02-20 22:43:22 UTC",
    "updated_date": "2025-02-20 22:43:22 UTC"
  },
  {
    "arxiv_id": "2502.17494v6",
    "title": "External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation",
    "authors": [
      "Mingfu Liang",
      "Xi Liu",
      "Rong Jin",
      "Boyang Liu",
      "Qiuling Suo",
      "Qinghai Zhou",
      "Song Zhou",
      "Laming Chen",
      "Hua Zheng",
      "Zhiyuan Li",
      "Shali Jiang",
      "Jiyan Yang",
      "Xiaozhen Xia",
      "Fan Yang",
      "Yasmine Badr",
      "Ellie Wen",
      "Shuyu Xu",
      "Hansey Chen",
      "Zhengyu Zhang",
      "Jade Nie",
      "Chunzhi Yang",
      "Zhichen Zeng",
      "Weilin Zhang",
      "Xingliang Huang",
      "Qianru Li",
      "Shiquan Wang",
      "Evelyn Lyu",
      "Wenjing Lu",
      "Rui Zhang",
      "Wenjun Wang",
      "Jason Rudy",
      "Mengyue Hang",
      "Kai Wang",
      "Yinbin Ma",
      "Shuaiwen Wang",
      "Sihan Zeng",
      "Tongyi Tang",
      "Xiaohan Wei",
      "Longhao Jin",
      "Jamey Zhang",
      "Marcus Chen",
      "Jiayi Xu",
      "Angie Huang",
      "Xihuan Zeng",
      "Chi Zhang",
      "Zhengli Zhao",
      "Jared Yang",
      "Qiang Jin",
      "Xian Chen",
      "Amit Anand Amlesahwaram",
      "Lexi Song",
      "Liang Luo",
      "Yuchen Hao",
      "Nan Xiao",
      "Yavuz Yetim",
      "Luoshang Pan",
      "Gaoxiang Liu",
      "Yuxi Hu",
      "Yuzhen Huang",
      "Jackie Xu",
      "Rich Zhu",
      "Xin Zhang",
      "Yiqun Liu",
      "Hang Yin",
      "Yuxin Chen",
      "Buyun Zhang",
      "Xiaoyi Liu",
      "Xingyuan Wang",
      "Wenguang Mao",
      "Zhijing Li",
      "Zhehui Zhou",
      "Feifan Gu",
      "Qin Huang",
      "Chonglin Sun",
      "Nancy Yu",
      "Shuo Gu",
      "Shupin Mao",
      "Benjamin Au",
      "Jingzheng Qin",
      "Peggy Yao",
      "Jae-Woo Choi",
      "Bin Gao",
      "Ernest Wang",
      "Lei Zhang",
      "Wen-Yen Chen",
      "Ted Lee",
      "Jay Zha",
      "Yi Meng",
      "Alex Gong",
      "Edison Gao",
      "Alireza Vahdatpour",
      "Yiping Han",
      "Yantao Yao",
      "Toshinari Kureha",
      "Shuo Chang",
      "Musharaf Sultan",
      "John Bocharov",
      "Sagar Chordia",
      "Xiaorui Gan",
      "Peng Sun",
      "Rocky Liu",
      "Bo Long",
      "Wenlin Chen",
      "Santanu Kolay",
      "Huayu Li"
    ],
    "abstract": "Ads recommendation is a prominent service of online advertising systems and\nhas been actively studied. Recent studies indicate that scaling-up and advanced\ndesign of the recommendation model can bring significant performance\nimprovement. However, with a larger model scale, such prior studies have a\nsignificantly increasing gap from industry as they often neglect two\nfundamental challenges in industrial-scale applications. First, training and\ninference budgets are restricted for the model to be served, exceeding which\nmay incur latency and impair user experience. Second, large-volume data arrive\nin a streaming mode with data distributions dynamically shifting, as new\nusers/ads join and existing users/ads leave the system. We propose the External\nLarge Foundation Model (ExFM) framework to address the overlooked challenges.\nSpecifically, we develop external distillation and a data augmentation system\n(DAS) to control the computational cost of training/inference while maintaining\nhigh performance. We design the teacher in a way like a foundation model (FM)\nthat can serve multiple students as vertical models (VMs) to amortize its\nbuilding cost. We propose Auxiliary Head and Student Adapter to mitigate the\ndata distribution gap between FM and VMs caused by the streaming data issue.\nComprehensive experiments on internal industrial-scale applications and public\ndatasets demonstrate significant performance gain by ExFM.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as\n  Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2502.17494v6",
    "published_date": "2025-02-20 22:35:52 UTC",
    "updated_date": "2025-04-23 06:47:38 UTC"
  },
  {
    "arxiv_id": "2502.15077v2",
    "title": "Hardware-Friendly Static Quantization Method for Video Diffusion Transformers",
    "authors": [
      "Sanghyun Yi",
      "Qingfeng Liu",
      "Mostafa El-Khamy"
    ],
    "abstract": "Diffusion Transformers for video generation have gained significant research\ninterest since the impressive performance of SORA. Efficient deployment of such\ngenerative-AI models on GPUs has been demonstrated with dynamic quantization.\nHowever, resource-constrained devices cannot support dynamic quantization, and\nneed static quantization of the models for their efficient deployment on AI\nprocessors. In this paper, we propose a novel method for the post-training\nquantization of OpenSora\\cite{opensora}, a Video Diffusion Transformer, without\nrelying on dynamic quantization techniques. Our approach employs static\nquantization, achieving video quality comparable to FP16 and dynamically\nquantized ViDiT-Q methods, as measured by CLIP, and VQA metrics. In particular,\nwe utilize per-step calibration data to adequately provide a post-training\nstatically quantized model for each time step, incorporating channel-wise\nquantization for weights and tensor-wise quantization for activations. By\nfurther applying the smooth-quantization technique, we can obtain high-quality\nvideo outputs with the statically quantized models. Extensive experimental\nresults demonstrate that static quantization can be a viable alternative to\ndynamic quantization for video diffusion transformers, offering a more\nefficient approach without sacrificing performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15077v2",
    "published_date": "2025-02-20 22:29:24 UTC",
    "updated_date": "2025-03-25 05:17:19 UTC"
  },
  {
    "arxiv_id": "2502.15069v1",
    "title": "Rare Disease Differential Diagnosis with Large Language Models at Scale: From Abdominal Actinomycosis to Wilson's Disease",
    "authors": [
      "Elliot Schumacher",
      "Dhruv Naik",
      "Anitha Kannan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\ndisease diagnosis. However, their effectiveness in identifying rarer diseases,\nwhich are inherently more challenging to diagnose, remains an open question.\nRare disease performance is critical with the increasing use of LLMs in\nhealthcare settings. This is especially true if a primary care physician needs\nto make a rarer prognosis from only a patient conversation so that they can\ntake the appropriate next step. To that end, several clinical decision support\nsystems are designed to support providers in rare disease identification. Yet\ntheir utility is limited due to their lack of knowledge of common disorders and\ndifficulty of use.\n  In this paper, we propose RareScale to combine the knowledge LLMs with expert\nsystems. We use jointly use an expert system and LLM to simulate rare disease\nchats. This data is used to train a rare disease candidate predictor model.\nCandidates from this smaller model are then used as additional inputs to\nblack-box LLM to make the final differential diagnosis. Thus, RareScale allows\nfor a balance between rare and common diagnoses. We present results on over 575\nrare diseases, beginning with Abdominal Actinomycosis and ending with Wilson's\nDisease. Our approach significantly improves the baseline performance of\nblack-box LLMs by over 17% in Top-5 accuracy. We also find that our candidate\ngeneration performance is high (e.g. 88.8% on gpt-4o generated chats).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15069v1",
    "published_date": "2025-02-20 22:02:52 UTC",
    "updated_date": "2025-02-20 22:02:52 UTC"
  },
  {
    "arxiv_id": "2502.17493v1",
    "title": "Pursuing Top Growth with Novel Loss Function",
    "authors": [
      "Ruoyu Guo",
      "Haochen Qiu"
    ],
    "abstract": "Making consistently profitable financial decisions in a continuously evolving\nand volatile stock market has always been a difficult task. Professionals from\ndifferent disciplines have developed foundational theories to anticipate price\nmovement and evaluate securities such as the famed Capital Asset Pricing Model\n(CAPM). In recent years, the role of artificial intelligence (AI) in asset\npricing has been growing. Although the black-box nature of deep learning models\nlacks interpretability, they have continued to solidify their position in the\nfinancial industry. We aim to further enhance AI's potential and utility by\nintroducing a return-weighted loss function that will drive top growth while\nproviding the ML models a limited amount of information. Using only publicly\naccessible stock data (open/close/high/low, trading volume, sector information)\nand several technical indicators constructed from them, we propose an efficient\ndaily trading system that detects top growth opportunities. Our best models\nachieve 61.73% annual return on daily rebalancing with an annualized Sharpe\nRatio of 1.18 over 1340 testing days from 2019 to 2024, and 37.61% annual\nreturn with an annualized Sharpe Ratio of 0.97 over 1360 testing days from 2005\nto 2010. The main drivers for success, especially independent of any domain\nknowledge, are the novel return-weighted loss function, the integration of\ncategorical and continuous data, and the ML model architecture. We also\ndemonstrate the superiority of our novel loss function over traditional loss\nfunctions via several performance metrics and statistical evidence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.CP",
      "I.2.1; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 7 figures, GitHub repo:\n  https://github.com/Tony-Guo-1/daily_trading_strategy",
    "pdf_url": "http://arxiv.org/pdf/2502.17493v1",
    "published_date": "2025-02-20 21:43:51 UTC",
    "updated_date": "2025-02-20 21:43:51 UTC"
  },
  {
    "arxiv_id": "2503.05748v1",
    "title": "Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective",
    "authors": [
      "Krti Tallam"
    ],
    "abstract": "As artificial intelligence scales, the concepts of alignment, agency, and\nautonomy have become central to AI safety, governance, and control. However,\neven in human contexts, these terms lack universal definitions, varying across\ndisciplines such as philosophy, psychology, law, computer science, mathematics,\nand political science. This inconsistency complicates their application to AI,\nwhere differing interpretations lead to conflicting approaches in system design\nand regulation. This paper traces the historical, philosophical, and technical\nevolution of these concepts, emphasizing how their definitions influence AI\ndevelopment, deployment, and oversight.\n  We argue that the urgency surrounding AI alignment and autonomy stems not\nonly from technical advancements but also from the increasing deployment of AI\nin high-stakes decision making. Using Agentic AI as a case study, we examine\nthe emergent properties of machine agency and autonomy, highlighting the risks\nof misalignment in real-world systems. Through an analysis of automation\nfailures (Tesla Autopilot, Boeing 737 MAX), multi-agent coordination (Metas\nCICERO), and evolving AI architectures (DeepMinds AlphaZero, OpenAIs AutoGPT),\nwe assess the governance and safety challenges posed by frontier AI.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05748v1",
    "published_date": "2025-02-20 21:37:20 UTC",
    "updated_date": "2025-02-20 21:37:20 UTC"
  },
  {
    "arxiv_id": "2502.18499v1",
    "title": "Mechanistic Understanding of Language Models in Syntactic Code Completion",
    "authors": [
      "Samuel Miller",
      "Daking Rai",
      "Ziyu Yao"
    ],
    "abstract": "Recently, language models (LMs) have shown impressive proficiency in code\ngeneration tasks, especially when fine-tuned on code-specific datasets,\ncommonly known as Code LMs. However, our understanding of the internal\ndecision-making processes of Code LMs, such as how they use their (syntactic or\nsemantic) knowledge, remains limited, which could lead to unintended harm as\nthey are increasingly used in real life. This motivates us to conduct one of\nthe first Mechanistic Interpretability works to understand how Code LMs perform\na syntactic completion task, specifically the closing parenthesis task, on the\nCodeLlama-7b model (Roziere et al. 2023). Our findings reveal that the model\nrequires middle-later layers until it can confidently predict the correct label\nfor the closing parenthesis task. Additionally, we identify that while both\nmulti-head attention (MHA) and feed-forward (FF) sub-layers play essential\nroles, MHA is particularly crucial. Furthermore, we also discover attention\nheads that keep track of the number of already closed parentheses precisely but\nmay or may not promote a correct number of closing parentheses that are still\nmissing, leading to a positive or negative impact on the model's performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "10 pages, 4 figures, accepted to the AAAI 2025 Workshop on Towards\n  Knowledgeable Foundation Models",
    "pdf_url": "http://arxiv.org/pdf/2502.18499v1",
    "published_date": "2025-02-20 21:35:20 UTC",
    "updated_date": "2025-02-20 21:35:20 UTC"
  },
  {
    "arxiv_id": "2502.15056v1",
    "title": "Fundamental Survey on Neuromorphic Based Audio Classification",
    "authors": [
      "Amlan Basu",
      "Pranav Chaudhari",
      "Gaetano Di Caterina"
    ],
    "abstract": "Audio classification is paramount in a variety of applications including\nsurveillance, healthcare monitoring, and environmental analysis. Traditional\nmethods frequently depend on intricate signal processing algorithms and\nmanually crafted features, which may fall short in fully capturing the\ncomplexities of audio patterns. Neuromorphic computing, inspired by the\narchitecture and functioning of the human brain, presents a promising\nalternative for audio classification tasks. This survey provides an exhaustive\nexamination of the current state-of-the-art in neuromorphic-based audio\nclassification. It delves into the crucial components of neuromorphic systems,\nsuch as Spiking Neural Networks (SNNs), memristors, and neuromorphic hardware\nplatforms, highlighting their advantages in audio classification. Furthermore,\nthe survey explores various methodologies and strategies employed in\nneuromorphic audio classification, including event-based processing,\nspike-based learning, and bio-inspired feature extraction. It examines how\nthese approaches address the limitations of traditional audio classification\nmethods, particularly in terms of energy efficiency, real-time processing, and\nrobustness to environmental noise. Additionally, the paper conducts a\ncomparative analysis of different neuromorphic audio classification models and\nbenchmarks, evaluating their performance metrics, computational efficiency, and\nscalability. By providing a comprehensive guide for researchers, engineers and\npractitioners, this survey aims to stimulate further innovation and\nadvancements in the evolving field of neuromorphic audio classification.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "24 Pages, 1 Table",
    "pdf_url": "http://arxiv.org/pdf/2502.15056v1",
    "published_date": "2025-02-20 21:34:32 UTC",
    "updated_date": "2025-02-20 21:34:32 UTC"
  },
  {
    "arxiv_id": "2502.15845v1",
    "title": "Verify when Uncertain: Beyond Self-Consistency in Black Box Hallucination Detection",
    "authors": [
      "Yihao Xue",
      "Kristjan Greenewald",
      "Youssef Mroueh",
      "Baharan Mirzasoleiman"
    ],
    "abstract": "Large Language Models (LLMs) suffer from hallucination problems, which hinder\ntheir reliability in sensitive applications. In the black-box setting, several\nself-consistency-based techniques have been proposed for hallucination\ndetection. We empirically study these techniques and show that they achieve\nperformance close to that of a supervised (still black-box) oracle, suggesting\nlittle room for improvement within this paradigm. To address this limitation,\nwe explore cross-model consistency checking between the target model and an\nadditional verifier LLM. With this extra information, we observe improved\noracle performance compared to purely self-consistency-based methods. We then\npropose a budget-friendly, two-stage detection algorithm that calls the\nverifier model only for a subset of cases. It dynamically switches between\nself-consistency and cross-consistency based on an uncertainty interval of the\nself-consistency classifier. We provide a geometric interpretation of\nconsistency-based hallucination detection methods through the lens of kernel\nmean embeddings, offering deeper theoretical insights. Extensive experiments\nshow that this approach maintains high detection performance while\nsignificantly reducing computational cost.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15845v1",
    "published_date": "2025-02-20 21:06:08 UTC",
    "updated_date": "2025-02-20 21:06:08 UTC"
  },
  {
    "arxiv_id": "2502.15040v1",
    "title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation",
    "authors": [
      "Yun-Wei Chu",
      "Kai Zhang",
      "Christopher Malon",
      "Martin Renqiang Min"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in\nvision and text tasks. However, hallucination remains a major challenge,\nespecially in fields like healthcare where details are critical. In this work,\nwe show how MLLMs may be enhanced to support Visual RAG (V-RAG), a\nretrieval-augmented generation framework that incorporates both text and visual\ndata from retrieved images. On the MIMIC-CXR chest X-ray report generation and\nMulticare medical image caption generation datasets, we show that Visual RAG\nimproves the accuracy of entity probing, which asks whether a medical entities\nis grounded by an image. We show that the improvements extend both to frequent\nand rare entities, the latter of which may have less positive training data.\nDownstream, we apply V-RAG with entity probing to correct hallucinations and\ngenerate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1\nscore.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "GenAI4Health - AAAI '25",
    "pdf_url": "http://arxiv.org/pdf/2502.15040v1",
    "published_date": "2025-02-20 20:55:34 UTC",
    "updated_date": "2025-02-20 20:55:34 UTC"
  },
  {
    "arxiv_id": "2502.15037v5",
    "title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling Furcated DLOs in Real-Time",
    "authors": [
      "Yizhou Chen",
      "Xiaoyue Wu",
      "Yeheng Zong",
      "Yuzhen Chen",
      "Anran Li",
      "Bohao Zhang",
      "Ram Vasudevan"
    ],
    "abstract": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15037v5",
    "published_date": "2025-02-20 20:46:09 UTC",
    "updated_date": "2025-05-06 15:36:35 UTC"
  },
  {
    "arxiv_id": "2502.15027v2",
    "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
    "authors": [
      "Henry Hengyuan Zhao",
      "Wenqi Pei",
      "Yifei Tao",
      "Haiyang Mei",
      "Mike Zheng Shou"
    ],
    "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their\ninteractive intelligence with human users, which is vital for developing\ngeneral-purpose AI assistants. We design InterFeedback, an interactive\nframework, which can be applied to any LMM and dataset to assess this ability\nautonomously. On top of this, we introduce InterFeedback-Bench which evaluates\ninteractive intelligence using two representative datasets, MMMU-Pro and\nMathVerse, to test 10 different open-source LMMs. Additionally, we present\nInterFeedback-Human, a newly collected dataset of 120 cases designed for\nmanually testing interactive performance in leading models such as OpenAI-o1\nand Claude-3.5-Sonnet. Our evaluation results indicate that even the\nstate-of-the-art LMM, OpenAI-o1, struggles to refine its responses based on\nhuman feedback, achieving an average score of less than 50%. Our findings point\nto the need for methods that can enhance LMMs' capabilities to interpret and\nbenefit from feedback.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15027v2",
    "published_date": "2025-02-20 20:27:06 UTC",
    "updated_date": "2025-03-09 01:07:59 UTC"
  },
  {
    "arxiv_id": "2502.15013v3",
    "title": "Towards Physics-Guided Foundation Models",
    "authors": [
      "Majid Farhadloo",
      "Arun Sharma",
      "Mingzhou Yang",
      "Bharat Jayaprakash",
      "William Northrop",
      "Shashi Shekhar"
    ],
    "abstract": "Traditional foundation models are pre-trained on broad datasets to reduce the\ntraining resources (e.g., time, energy, labeled samples) needed for fine-tuning\na wide range of downstream tasks. However, traditional foundation models\nstruggle with out-of-distribution prediction and can produce outputs that are\nunrealistic and physically infeasible. We propose the notation of\nphysics-guided foundation models (PGFM), that is, foundation models integrated\nwith broad or general domain (e.g., scientific) physical knowledge applicable\nto a wide range of downstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15013v3",
    "published_date": "2025-02-20 20:10:22 UTC",
    "updated_date": "2025-04-23 16:58:57 UTC"
  },
  {
    "arxiv_id": "2502.15012v1",
    "title": "Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment",
    "authors": [
      "Ruyi Ding",
      "Tianhong Xu",
      "Aidong Adam Ding",
      "Yunsi Fei"
    ],
    "abstract": "Wide deployment of machine learning models on edge devices has rendered the\nmodel intellectual property (IP) and data privacy vulnerable. We propose\nGNNVault, the first secure Graph Neural Network (GNN) deployment strategy based\non Trusted Execution Environment (TEE). GNNVault follows the design of\n'partition-before-training' and includes a private GNN rectifier to complement\nwith a public backbone model. This way, both critical GNN model parameters and\nthe private graph used during inference are protected within secure TEE\ncompartments. Real-world implementations with Intel SGX demonstrate that\nGNNVault safeguards GNN inference against state-of-the-art link stealing\nattacks with negligible accuracy degradation (<2%).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "This work is accepted by DAC 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.15012v1",
    "published_date": "2025-02-20 20:09:14 UTC",
    "updated_date": "2025-02-20 20:09:14 UTC"
  },
  {
    "arxiv_id": "2502.15010v1",
    "title": "Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ],
    "abstract": "Recent copyright agreements between AI companies and content creators have\nhighlighted the need for precise control over language models' ability to\nreproduce copyrighted content. While existing approaches rely on either\ncomplete concept removal through unlearning or simple output filtering, we\npropose Obliviate, a novel post-training technique that selectively prevents\nverbatim reproduction of specific text while preserving semantic understanding.\n  Obliviate operates by selecting tokens within memorized sequences and\nmodifying the model's probability distribution to prevent exact reproduction\nwhile maintaining contextual understanding. We evaluate Obliviate on multiple\nlarge language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, and\nYi-1.5 6B) across both synthetic memorization tasks and organic copyright\ncontent. Our results demonstrate that Obliviate achieves orders of magnitude\nreduction, e.g., 100x, in verbatim memorization while maintaining model\nperformance within 1% of baseline on standard benchmarks (HellaSwag, MMLU,\nTruthfulQA, and Winogrande). This makes Obliviate particularly suitable for\npractical deployment scenarios where companies need to efficiently address\ncopyright concerns in pretrained models without compromising their general\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15010v1",
    "published_date": "2025-02-20 20:02:56 UTC",
    "updated_date": "2025-02-20 20:02:56 UTC"
  },
  {
    "arxiv_id": "2502.15007v1",
    "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
    "authors": [
      "Anton Razzhigaev",
      "Matvey Mikhalchuk",
      "Temurbek Rahmatullaev",
      "Elizaveta Goncharova",
      "Polina Druzhinina",
      "Ivan Oseledets",
      "Andrey Kuznetsov"
    ],
    "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and\nstore contextual information, revealing that tokens often seen as minor (e.g.,\ndeterminers, punctuation) carry surprisingly high context. Notably, removing\nthese tokens -- especially stopwords, articles, and commas -- consistently\ndegrades performance on MMLU and BABILong-4k, even if removing only irrelevant\ntokens. Our analysis also shows a strong correlation between contextualization\nand linearity, where linearity measures how closely the transformation from one\nlayer's embeddings to the next can be approximated by a single linear mapping.\nThese findings underscore the hidden importance of filler tokens in maintaining\ncontext. For further exploration, we present LLM-Microscope, an open-source\ntoolkit that assesses token-level nonlinearity, evaluates contextual memory,\nvisualizes intermediate layer contributions (via an adapted Logit Lens), and\nmeasures the intrinsic dimensionality of representations. This toolkit\nilluminates how seemingly trivial tokens can be critical for long-range\nunderstanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted to NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.15007v1",
    "published_date": "2025-02-20 19:59:35 UTC",
    "updated_date": "2025-02-20 19:59:35 UTC"
  },
  {
    "arxiv_id": "2502.15006v1",
    "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
    "authors": [
      "Ji Yin",
      "Oswin So",
      "Eric Yang Yu",
      "Chuchu Fan",
      "Panagiotis Tsiotras"
    ],
    "abstract": "A common problem when using model predictive control (MPC) in practice is the\nsatisfaction of safety specifications beyond the prediction horizon. While\ntheoretical works have shown that safety can be guaranteed by enforcing a\nsuitable terminal set constraint or a sufficiently long prediction horizon,\nthese techniques are difficult to apply and thus are rarely used by\npractitioners, especially in the case of general nonlinear dynamics. To solve\nthis problem, we impose a tradeoff between exact recursive feasibility,\ncomputational tractability, and applicability to ''black-box'' dynamics by\nlearning an approximate discrete-time control barrier function and\nincorporating it into a variational inference MPC (VIMPC), a sampling-based MPC\nparadigm. To handle the resulting state constraints, we further propose a new\nsampling strategy that greatly reduces the variance of the estimated optimal\ncontrol, improving the sample efficiency, and enabling real-time planning on a\nCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial\nsafety improvements compared to existing sampling-based MPC controllers, even\nunder badly designed cost functions. We validate our approach in both\nsimulation and real-world hardware experiments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15006v1",
    "published_date": "2025-02-20 19:59:11 UTC",
    "updated_date": "2025-02-20 19:59:11 UTC"
  },
  {
    "arxiv_id": "2502.15005v1",
    "title": "A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems",
    "authors": [
      "Lew Lefton",
      "Kexin Rong",
      "Chinar Dankhara",
      "Lila Ghemri",
      "Firdous Kausar",
      "A. Hannibal Hamdallahi"
    ],
    "abstract": "In this paper, we propose a Retrieval Augmented Generation (RAG) agent that\nmaps natural language queries about research topics to precise,\nmachine-interpretable semantic entities. Our approach combines RAG with\nSocratic dialogue to align a user's intuitive understanding of research topics\nwith established Knowledge Organization Systems (KOSs). The proposed approach\nwill effectively bridge \"little semantics\" (domain-specific KOS structures)\nwith \"big semantics\" (broad bibliometric repositories), making complex academic\ntaxonomies more accessible. Such agents have the potential for broad use. We\nillustrate with a sample application called CollabNext, which is a\nperson-centric knowledge graph connecting people, organizations, and research\ntopics. We further describe how the application design has an intentional focus\non HBCUs and emerging researchers to raise visibility of people historically\nrendered invisible in the current science system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "I.2.7; F.4.1"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 2 figures, AAAI 2025 Workshop on A Translational Institute\n  for Knowledge Axiomatization",
    "pdf_url": "http://arxiv.org/pdf/2502.15005v1",
    "published_date": "2025-02-20 19:58:59 UTC",
    "updated_date": "2025-02-20 19:58:59 UTC"
  },
  {
    "arxiv_id": "2502.14996v1",
    "title": "A Rapid Test for Accuracy and Bias of Face Recognition Technology",
    "authors": [
      "Manuel Knott",
      "Ignacio Serna",
      "Ethan Mann",
      "Pietro Perona"
    ],
    "abstract": "Measuring the accuracy of face recognition (FR) systems is essential for\nimproving performance and ensuring responsible use. Accuracy is typically\nestimated using large annotated datasets, which are costly and difficult to\nobtain. We propose a novel method for 1:1 face verification that benchmarks FR\nsystems quickly and without manual annotation, starting from approximate labels\n(e.g., from web search results). Unlike previous methods for training set label\ncleaning, ours leverages the embedding representation of the models being\nevaluated, achieving high accuracy in smaller-sized test datasets. Our approach\nreliably estimates FR accuracy and ranking, significantly reducing the time and\ncost of manual labeling. We also introduce the first public benchmark of five\nFR cloud services, revealing demographic biases, particularly lower accuracy\nfor Asian women. Our rapid test method can democratize FR testing, promoting\nscrutiny and responsible use of the technology. Our method is provided as a\npublicly accessible tool at https://github.com/caltechvisionlab/frt-rapid-test",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as a conference paper for WACV 2025. Manuel Knott, Ignacio\n  Serna, and Ethan Mann contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2502.14996v1",
    "published_date": "2025-02-20 19:38:52 UTC",
    "updated_date": "2025-02-20 19:38:52 UTC"
  },
  {
    "arxiv_id": "2502.14975v1",
    "title": "Beyond No: Quantifying AI Over-Refusal and Emotional Attachment Boundaries",
    "authors": [
      "David Noever",
      "Grant Rosario"
    ],
    "abstract": "We present an open-source benchmark and evaluation framework for assessing\nemotional boundary handling in Large Language Models (LLMs). Using a dataset of\n1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o,\nClaude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate\nemotional boundaries through pattern-matched response analysis. Our framework\nquantifies responses across seven key patterns: direct refusal, apology,\nexplanation, deflection, acknowledgment, boundary setting, and emotional\nawareness. Results demonstrate significant variation in boundary-handling\napproaches, with Claude-3.5 achieving the highest overall score (8.69/10) and\nproducing longer, more nuanced responses (86.51 words on average). We\nidentified a substantial performance gap between English (average score 25.62)\nand non-English interactions (< 0.22), with English responses showing markedly\nhigher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis\nrevealed model-specific strategies, such as Mistral's preference for deflection\n(4.2%) and consistently low empathy scores across all models (< 0.06).\nLimitations include potential oversimplification through pattern matching, lack\nof contextual understanding in response analysis, and binary classification of\ncomplex emotional responses. Future work should explore more nuanced scoring\nmethods, expand language coverage, and investigate cultural variations in\nemotional boundary expectations. Our benchmark and methodology provide a\nfoundation for systematic evaluation of LLM emotional intelligence and\nboundary-setting capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14975v1",
    "published_date": "2025-02-20 19:09:40 UTC",
    "updated_date": "2025-02-20 19:09:40 UTC"
  },
  {
    "arxiv_id": "2502.14966v1",
    "title": "CyberSentinel: An Emergent Threat Detection System for AI Security",
    "authors": [
      "Krti Tallam"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) has significantly\nexpanded the attack surface for AI-driven cybersecurity threats, necessitating\nadaptive defense strategies. This paper introduces CyberSentinel, a unified,\nsingle-agent system for emergent threat detection, designed to identify and\nmitigate novel security risks in real time. CyberSentinel integrates: (1)\nBrute-force attack detection through SSH log analysis, (2) Phishing threat\nassessment using domain blacklists and heuristic URL scoring, and (3) Emergent\nthreat detection via machine learning-based anomaly detection. By continuously\nadapting to evolving adversarial tactics, CyberSentinel strengthens proactive\ncybersecurity defense, addressing critical vulnerabilities in AI security.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14966v1",
    "published_date": "2025-02-20 19:03:32 UTC",
    "updated_date": "2025-02-20 19:03:32 UTC"
  },
  {
    "arxiv_id": "2502.14866v2",
    "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
    "authors": [
      "Shang Yang",
      "Junxian Guo",
      "Haotian Tang",
      "Qinghao Hu",
      "Guangxuan Xiao",
      "Jiaming Tang",
      "Yujun Lin",
      "Zhijian Liu",
      "Yao Lu",
      "Song Han"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
    "pdf_url": "http://arxiv.org/pdf/2502.14866v2",
    "published_date": "2025-02-20 18:59:52 UTC",
    "updated_date": "2025-04-21 15:13:44 UTC"
  },
  {
    "arxiv_id": "2502.14864v1",
    "title": "Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework",
    "authors": [
      "Yuming Yang",
      "Jiang Zhong",
      "Li Jin",
      "Jingwang Huang",
      "Jingpeng Gao",
      "Qing Liu",
      "Yang Bai",
      "Jingyuan Zhang",
      "Rui Jiang",
      "Kaiwen Wei"
    ],
    "abstract": "Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning\ncapabilities by integrating external knowledge. However, existing benchmarks\nprimarily focus on simple image-text interactions, overlooking complex visual\nformats like charts that are prevalent in real-world applications. In this\nwork, we introduce a novel task, Chart-based MRAG, to address this limitation.\nTo semi-automatically generate high-quality evaluation samples, we propose\nCHARt-based document question-answering GEneration (CHARGE), a framework that\nproduces evaluation data through structured keypoint extraction, crossmodal\nverification, and keypoint-based generation. By combining CHARGE with expert\nvalidation, we construct Chart-MRAG Bench, a comprehensive benchmark for\nchart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8\ndomains from real-world documents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified multimodal embedding retrieval\nmethods struggles in chart-based scenarios, (2) even with ground-truth\nretrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality\nbias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are\nreleased at https://github.com/Nomothings/CHARGE.git.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14864v1",
    "published_date": "2025-02-20 18:59:42 UTC",
    "updated_date": "2025-02-20 18:59:42 UTC"
  },
  {
    "arxiv_id": "2502.14862v1",
    "title": "Interpretable Text Embeddings and Text Similarity Explanation: A Primer",
    "authors": [
      "Juri Opitz",
      "Lucas Möller",
      "Andrianos Michail",
      "Simon Clematide"
    ],
    "abstract": "Text embeddings and text embedding models are a backbone of many AI and NLP\nsystems, particularly those involving search. However, interpretability\nchallenges persist, especially in explaining obtained similarity scores, which\nis crucial for applications requiring transparency. In this paper, we give a\nstructured overview of interpretability methods specializing in explaining\nthose similarity scores, an emerging research area. We study the methods'\nindividual ideas and techniques, evaluating their potential for improving\ninterpretability of text embeddings and explaining predicted similarities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14862v1",
    "published_date": "2025-02-20 18:59:34 UTC",
    "updated_date": "2025-02-20 18:59:34 UTC"
  },
  {
    "arxiv_id": "2502.14856v2",
    "title": "FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling",
    "authors": [
      "Weilin Zhao",
      "Tengyu Pan",
      "Xu Han",
      "Yudi Zhang",
      "Ao Sun",
      "Yuxiang Huang",
      "Kaihuo Zhang",
      "Weilun Zhao",
      "Yuxuan Li",
      "Jianyong Wang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Speculative sampling has emerged as an important technique for accelerating\nthe auto-regressive generation process of large language models (LLMs) by\nutilizing a draft-then-verify mechanism to produce multiple tokens per forward\npass. While state-of-the-art speculative sampling methods use only a single\nlayer and a language modeling (LM) head as the draft model to achieve\nimpressive layer compression, their efficiency gains are substantially reduced\nfor large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.\nTo address this, we present FR-Spec, a frequency-ranked speculative sampling\nframework that optimizes draft candidate selection through vocabulary space\ncompression. By constraining the draft search to a frequency-prioritized token\nsubset, our method reduces LM Head computation overhead by 75% while ensuring\nthe equivalence of the final output distribution. Experiments across multiple\ndatasets demonstrate an average of 1.12$\\times$ speedup over the\nstate-of-the-art speculative sampling method EAGLE-2. Code available at\nhttps://github.com/thunlp/FR-Spec.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14856v2",
    "published_date": "2025-02-20 18:58:10 UTC",
    "updated_date": "2025-03-11 08:54:55 UTC"
  },
  {
    "arxiv_id": "2502.19437v1",
    "title": "Evolutionary Algorithms Approach For Search Based On Semantic Document Similarity",
    "authors": [
      "Chandrashekar Muniyappa",
      "Eujin Kim"
    ],
    "abstract": "Advancements in cloud computing and distributed computing have fostered\nresearch activities in Computer science. As a result, researchers have made\nsignificant progress in Neural Networks, Evolutionary Computing Algorithms like\nGenetic, and Differential evolution algorithms. These algorithms are used to\ndevelop clustering, recommendation, and question-and-answering systems using\nvarious text representation and similarity measurement techniques. In this\nresearch paper, Universal Sentence Encoder (USE) is used to capture the\nsemantic similarity of text; And the transfer learning technique is used to\napply Genetic Algorithm (GA) and Differential Evolution (DE) algorithms to\nsearch and retrieve relevant top N documents based on user query. The proposed\napproach is applied to the Stanford Question and Answer (SQuAD) Dataset to\nidentify a user query. Finally, through experiments, we prove that text\ndocuments can be efficiently represented as sentence embedding vectors using\nUSE to capture the semantic similarity, and by comparing the results of the\nManhattan Distance, GA, and DE algorithms we prove that the evolutionary\nalgorithms are good at finding the top N results than the traditional ranking\napproach.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7; I.2.11"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.19437v1",
    "published_date": "2025-02-20 18:56:52 UTC",
    "updated_date": "2025-02-20 18:56:52 UTC"
  },
  {
    "arxiv_id": "2502.14838v1",
    "title": "Revealing and Mitigating Over-Attention in Knowledge Editing",
    "authors": [
      "Pinzheng Wang",
      "Zecheng Tang",
      "Keyan Zhou",
      "Juntao Li",
      "Qiaoming Zhu",
      "Min Zhang"
    ],
    "abstract": "Large Language Models have demonstrated superior performance across a wide\nrange of tasks, but they still exhibit undesirable errors due to incorrect\nknowledge learned from the training data. To avoid this, knowledge editing\nmethods emerged to precisely edit the specific model knowledge via efficiently\nmodifying a very small percentage of parameters. % However, those methods can\nlead to the problem of Specificity Failure: when the content related to the\nedited knowledge occurs in the context, it can inadvertently corrupt other\npre-existing knowledge. However, those methods can lead to the problem of\nSpecificity Failure, where the existing knowledge and capabilities are severely\ndegraded due to editing. Our preliminary indicates that Specificity Failure\nprimarily stems from the model's attention heads assigning excessive attention\nscores to entities related to the edited knowledge, thereby unduly focusing on\nspecific snippets within the context, which we denote as the Attention Drift\nphenomenon. To mitigate such Attention Drift issue, we introduce a simple yet\neffective method Selective Attention Drift Restriction}(SADR), which introduces\nan additional regularization term during the knowledge editing process to\nrestrict changes in the attention weight distribution, thereby preventing undue\nfocus on the edited entity. Experiments on five frequently used strong LLMs\ndemonstrate the effectiveness of our method, where SADR can significantly\nmitigate Specificity Failure in the predominant knowledge editing tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14838v1",
    "published_date": "2025-02-20 18:51:12 UTC",
    "updated_date": "2025-02-20 18:51:12 UTC"
  },
  {
    "arxiv_id": "2502.14837v1",
    "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs",
    "authors": [
      "Tao Ji",
      "Bin Guo",
      "Yuanbin Wu",
      "Qipeng Guo",
      "Lixing Shen",
      "Zhan Chen",
      "Xipeng Qiu",
      "Qi Zhang",
      "Tao Gui"
    ],
    "abstract": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14837v1",
    "published_date": "2025-02-20 18:50:42 UTC",
    "updated_date": "2025-02-20 18:50:42 UTC"
  },
  {
    "arxiv_id": "2502.14834v1",
    "title": "LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models",
    "authors": [
      "Shangqing Tu",
      "Yucheng Wang",
      "Daniel Zhang-Li",
      "Yushi Bai",
      "Jifan Yu",
      "Yuhao Wu",
      "Lei Hou",
      "Huiqin Liu",
      "Zhiyuan Liu",
      "Bin Xu",
      "Juanzi Li"
    ],
    "abstract": "Existing Large Vision-Language Models (LVLMs) can process inputs with context\nlengths up to 128k visual and text tokens, yet they struggle to generate\ncoherent outputs beyond 1,000 words. We find that the primary limitation is the\nabsence of long output examples during supervised fine-tuning (SFT). To tackle\nthis issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158\nexamples, each with multiple input images, an instruction, and corresponding\noutputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that\nmaintain high-fidelity to the input images, we employ Direct Preference\nOptimization (DPO) to the SFT model. Given the high cost of collecting human\nfeedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which\nbreaks long outputs into segments and uses iterative corrections to form\npreference pairs with the original outputs. Additionally, we develop\nMMLongBench-Write, a benchmark featuring six tasks to evaluate the\nlong-generation capabilities of VLMs. Our 7B parameter model, trained with\nLongWriter-V-22k and IterDPO, achieves impressive performance on this\nbenchmark, outperforming larger proprietary models like GPT-4o. Code and data:\nhttps://github.com/THU-KEG/LongWriter-V",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14834v1",
    "published_date": "2025-02-20 18:47:36 UTC",
    "updated_date": "2025-02-20 18:47:36 UTC"
  },
  {
    "arxiv_id": "2502.14831v2",
    "title": "Improving the Diffusability of Autoencoders",
    "authors": [
      "Ivan Skorokhodov",
      "Sharath Girish",
      "Benran Hu",
      "Willi Menapace",
      "Yanyu Li",
      "Rameen Abdal",
      "Sergey Tulyakov",
      "Aliaksandr Siarohin"
    ],
    "abstract": "Latent diffusion models have emerged as the leading approach for generating\nhigh-quality images and videos, utilizing compressed latent representations to\nreduce the computational burden of the diffusion process. While recent\nadvancements have primarily focused on scaling diffusion backbones and\nimproving autoencoder reconstruction quality, the interaction between these\ncomponents has received comparatively less attention. In this work, we perform\na spectral analysis of modern autoencoders and identify inordinate\nhigh-frequency components in their latent spaces, which are especially\npronounced in the autoencoders with a large bottleneck channel size. We\nhypothesize that this high-frequency component interferes with the\ncoarse-to-fine nature of the diffusion synthesis process and hinders the\ngeneration quality. To mitigate the issue, we propose scale equivariance: a\nsimple regularization strategy that aligns latent and RGB spaces across\nfrequencies by enforcing scale equivariance in the decoder. It requires minimal\ncode changes and only up to 20K autoencoder fine-tuning steps, yet\nsignificantly improves generation quality, reducing FID by 19% for image\ngeneration on ImageNet-1K 256x256 and FVD by at least 44% for video generation\non Kinetics-700 17x256x256.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "26 pages, 22 figures, 9 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14831v2",
    "published_date": "2025-02-20 18:45:44 UTC",
    "updated_date": "2025-03-12 22:08:10 UTC"
  },
  {
    "arxiv_id": "2502.14830v1",
    "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs",
    "authors": [
      "Danni Liu",
      "Jan Niehues"
    ],
    "abstract": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14830v1",
    "published_date": "2025-02-20 18:45:43 UTC",
    "updated_date": "2025-02-20 18:45:43 UTC"
  },
  {
    "arxiv_id": "2502.14827v2",
    "title": "Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison",
    "authors": [
      "Aiswarya Baby",
      "Tintu Thankom Koshy"
    ],
    "abstract": "Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper offers a detailed study of the original VQA dataset,\nbaseline models and methods along with a comparative study of five advanced VQA\nmodels, ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA,\neach employing distinct methods to address these ongoing challenges.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, No figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14827v2",
    "published_date": "2025-02-20 18:45:00 UTC",
    "updated_date": "2025-03-04 16:43:01 UTC"
  },
  {
    "arxiv_id": "2502.14820v1",
    "title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables",
    "authors": [
      "Luis Antonio Gutiérrez Guanilo",
      "Mir Tafseer Nayeem",
      "Cristian López",
      "Davood Rafiei"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndiverse domains, yet their application in e-commerce remains underexplored due\nto a lack of domain-specific datasets. To address this gap, we introduce\neC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce,\nincluding detailed product attributes and user-specific queries. Leveraging\neC-Tab2Text, we focus on text generation from product tables, enabling LLMs to\nproduce high-quality, attribute-specific product reviews from structured\ntabular data. Fine-tuned models were rigorously evaluated using standard\nTable2Text metrics, alongside correctness, faithfulness, and fluency\nassessments. Our results demonstrate substantial improvements in generating\ncontextually accurate reviews, highlighting the transformative potential of\ntailored datasets and fine-tuning methodologies in optimizing e-commerce\nworkflows. This work highlights the potential of LLMs in e-commerce workflows\nand the essential role of domain-specific datasets in tailoring them to\nindustry-specific challenges.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 (Industry Track)",
    "pdf_url": "http://arxiv.org/pdf/2502.14820v1",
    "published_date": "2025-02-20 18:41:48 UTC",
    "updated_date": "2025-02-20 18:41:48 UTC"
  },
  {
    "arxiv_id": "2502.14949v1",
    "title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding",
    "authors": [
      "Ahmed Heakl",
      "Abdullah Sohail",
      "Mukul Ranjan",
      "Rania Hossam",
      "Ghazi Ahmed",
      "Mohamed El-Geish",
      "Omar Maher",
      "Zhiqiang Shen",
      "Fahad Khan",
      "Salman Khan"
    ],
    "abstract": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document\nprocessing, robust text recognition has become increasingly critical for\nknowledge extraction. While OCR (Optical Character Recognition) for English and\nother languages benefits from large datasets and well-established benchmarks,\nArabic OCR faces unique challenges due to its cursive script, right-to-left\ntext flow, and complex typographic and calligraphic features. We present\nKITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in\ncurrent evaluation systems. Our benchmark comprises 8,809 samples across 9\nmajor domains and 36 sub-domains, encompassing diverse document types including\nhandwritten text, structured tables, and specialized coverage of 21 chart types\nfor business intelligence. Our findings show that modern vision-language models\n(such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like\nEasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate\n(CER). Furthermore, we highlight significant limitations of current Arabic OCR\nmodels, particularly in PDF-to-Markdown conversion, where the best model\nGemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in\naccurately recognizing Arabic text, including issues with complex fonts,\nnumeral recognition errors, word elongation, and table structure detection.\nThis work establishes a rigorous evaluation framework that can drive\nimprovements in Arabic document analysis methods and bridge the performance gap\nwith English OCR technologies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 5 figures, ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14949v1",
    "published_date": "2025-02-20 18:41:23 UTC",
    "updated_date": "2025-02-20 18:41:23 UTC"
  },
  {
    "arxiv_id": "2502.14815v1",
    "title": "Optimizing Model Selection for Compound AI Systems",
    "authors": [
      "Lingjiao Chen",
      "Jared Quincy Davis",
      "Boris Hanin",
      "Peter Bailis",
      "Matei Zaharia",
      "James Zou",
      "Ion Stoica"
    ],
    "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and\nmulti-agent-debate, achieve strong performance on many AI tasks. We address a\ncore question in optimizing compound systems: for each LLM call or module in\nthe system, how should one decide which LLM to use? We show that these LLM\nchoices have a large effect on quality, but the search space is exponential. We\npropose LLMSelector, an efficient framework for model selection in compound\nsystems, which leverages two key empirical insights: (i) end-to-end performance\nis often monotonic in how well each module performs, with all other modules\nheld fixed, and (ii) per-module performance can be estimated accurately by an\nLLM. Building upon these insights, LLMSelector iteratively selects one module\nand allocates to it the model with the highest module-wise performance, as\nestimated by an LLM, until no further gain is possible. LLMSelector is\napplicable to any compound system with a bounded number of modules, and its\nnumber of API calls scales linearly with the number of modules, achieving\nhigh-quality model allocation both empirically and theoretically. Experiments\nwith popular compound systems such as multi-agent debate and self-refine using\nLLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector\nconfers 5%-70% accuracy gains compared to using the same LLM for all modules.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14815v1",
    "published_date": "2025-02-20 18:36:25 UTC",
    "updated_date": "2025-02-20 18:36:25 UTC"
  },
  {
    "arxiv_id": "2502.14807v2",
    "title": "FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis",
    "authors": [
      "Fadillah Maani",
      "Numan Saeed",
      "Tausifa Saleem",
      "Zaid Farooq",
      "Hussain Alasmawi",
      "Werner Diehl",
      "Ameera Mohammad",
      "Gareth Waring",
      "Saudabi Valappi",
      "Leanne Bricker",
      "Mohammad Yaqub"
    ],
    "abstract": "Foundation models are becoming increasingly effective in the medical domain,\noffering pre-trained models on large datasets that can be readily adapted for\ndownstream tasks. Despite progress, fetal ultrasound images remain a\nchallenging domain for foundation models due to their inherent complexity,\noften requiring substantial additional training and facing limitations due to\nthe scarcity of paired multimodal data. To overcome these challenges, here we\nintroduce FetalCLIP, a vision-language foundation model capable of generating\nuniversal representation of fetal ultrasound images. FetalCLIP was pre-trained\nusing a multimodal learning approach on a diverse dataset of 210,035 fetal\nultrasound images paired with text. This represents the largest paired dataset\nof its kind used for foundation model development to date. This unique training\napproach allows FetalCLIP to effectively learn the intricate anatomical\nfeatures present in fetal ultrasound images, resulting in robust\nrepresentations that can be used for a variety of downstream applications. In\nextensive benchmarking across a range of key fetal ultrasound applications,\nincluding classification, gestational age estimation, congenital heart defect\n(CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all\nbaselines while demonstrating remarkable generalizability and strong\nperformance even with limited labeled data. We plan to release the FetalCLIP\nmodel publicly for the benefit of the broader scientific community.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14807v2",
    "published_date": "2025-02-20 18:30:34 UTC",
    "updated_date": "2025-04-07 17:03:03 UTC"
  },
  {
    "arxiv_id": "2502.14802v1",
    "title": "From RAG to Memory: Non-Parametric Continual Learning for Large Language Models",
    "authors": [
      "Bernal Jiménez Gutiérrez",
      "Yiheng Shu",
      "Weijian Qi",
      "Sizhe Zhou",
      "Yu Su"
    ],
    "abstract": "Our ability to continuously acquire, organize, and leverage knowledge is a\nkey feature of human intelligence that AI systems must approximate to unlock\ntheir full potential. Given the challenges in continual learning with large\nlanguage models (LLMs), retrieval-augmented generation (RAG) has become the\ndominant way to introduce new information. However, its reliance on vector\nretrieval hinders its ability to mimic the dynamic and interconnected nature of\nhuman long-term memory. Recent RAG approaches augment vector embeddings with\nvarious structures like knowledge graphs to address some of these gaps, namely\nsense-making and associativity. However, their performance on more basic\nfactual memory tasks drops considerably below standard RAG. We address this\nunintended deterioration and propose HippoRAG 2, a framework that outperforms\nstandard RAG comprehensively on factual, sense-making, and associative memory\ntasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in\nHippoRAG and enhances it with deeper passage integration and more effective\nonline use of an LLM. This combination pushes this RAG system closer to the\neffectiveness of human long-term memory, achieving a 7% improvement in\nassociative memory tasks over the state-of-the-art embedding model while also\nexhibiting superior factual knowledge and sense-making memory capabilities.\nThis work paves the way for non-parametric continual learning for LLMs. Our\ncode and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and data to be released at:\n  https://github.com/OSU-NLP-Group/HippoRAG",
    "pdf_url": "http://arxiv.org/pdf/2502.14802v1",
    "published_date": "2025-02-20 18:26:02 UTC",
    "updated_date": "2025-02-20 18:26:02 UTC"
  },
  {
    "arxiv_id": "2502.14799v1",
    "title": "A Survey on Text-Driven 360-Degree Panorama Generation",
    "authors": [
      "Hai Wang",
      "Xiaoyu Xiang",
      "Weihao Xia",
      "Jing-Hao Xue"
    ],
    "abstract": "The advent of text-driven 360-degree panorama generation, enabling the\nsynthesis of 360-degree panoramic images directly from textual descriptions,\nmarks a transformative advancement in immersive visual content creation. This\ninnovation significantly simplifies the traditionally complex process of\nproducing such content. Recent progress in text-to-image diffusion models has\naccelerated the rapid development in this emerging field. This survey presents\na comprehensive review of text-driven 360-degree panorama generation, offering\nan in-depth analysis of state-of-the-art algorithms and their expanding\napplications in 360-degree 3D scene generation. Furthermore, we critically\nexamine current limitations and propose promising directions for future\nresearch. A curated project page with relevant resources and research papers is\navailable at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14799v1",
    "published_date": "2025-02-20 18:19:57 UTC",
    "updated_date": "2025-02-20 18:19:57 UTC"
  },
  {
    "arxiv_id": "2503.05747v1",
    "title": "Balancing Innovation and Integrity: AI Integration in Liberal Arts College Administration",
    "authors": [
      "Ian Olivo Read"
    ],
    "abstract": "This paper explores the intersection of artificial intelligence and higher\neducation administration, focusing on liberal arts colleges (LACs). It examines\nAI's opportunities and challenges in academic and student affairs, legal\ncompliance, and accreditation processes, while also addressing the ethical\nconsiderations of AI deployment in mission-driven institutions. Considering\nAI's value pluralism and potential allocative or representational harms caused\nby algorithmic bias, LACs must ensure AI aligns with its mission and\nprinciples. The study highlights other strategies for responsible AI\nintegration, balancing innovation with institutional values.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "I.2.0; I.2.7; K.3.1; J.1; K.4.2"
    ],
    "primary_category": "cs.CY",
    "comment": "Number of Pages: 19; Number of Figures: 3. This submission explores\n  AI integration in liberal arts college administration, focusing on academic\n  and student affairs. It addresses ethical, legal, and institutional alignment\n  issues. For related discussions, see: Friedler et al. (2016), Katsamakas et\n  al. (2024), {\\L}odzikowski et al. (2023), Zhang et al. (2024)",
    "pdf_url": "http://arxiv.org/pdf/2503.05747v1",
    "published_date": "2025-02-20 18:16:11 UTC",
    "updated_date": "2025-02-20 18:16:11 UTC"
  },
  {
    "arxiv_id": "2502.14791v2",
    "title": "Rapid Word Learning Through Meta In-Context Learning",
    "authors": [
      "Wentao Wang",
      "Guangyuan Jiang",
      "Tal Linzen",
      "Brenden M. Lake"
    ],
    "abstract": "Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14791v2",
    "published_date": "2025-02-20 18:11:38 UTC",
    "updated_date": "2025-05-20 18:22:04 UTC"
  },
  {
    "arxiv_id": "2502.14788v1",
    "title": "Ray-Tracing for Conditionally Activated Neural Networks",
    "authors": [
      "Claudio Gallicchio",
      "Giuseppe Nuti"
    ],
    "abstract": "In this paper, we introduce a novel architecture for conditionally activated\nneural networks combining a hierarchical construction of multiple Mixture of\nExperts (MoEs) layers with a sampling mechanism that progressively converges to\nan optimized configuration of expert activation. This methodology enables the\ndynamic unfolding of the network's architecture, facilitating efficient\npath-specific training. Experimental results demonstrate that this approach\nachieves competitive accuracy compared to conventional baselines while\nsignificantly reducing the parameter count required for inference. Notably,\nthis parameter reduction correlates with the complexity of the input patterns,\na property naturally emerging from the network's operational dynamics without\nnecessitating explicit auxiliary penalty functions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "submitted to workshop",
    "pdf_url": "http://arxiv.org/pdf/2502.14788v1",
    "published_date": "2025-02-20 18:09:03 UTC",
    "updated_date": "2025-02-20 18:09:03 UTC"
  },
  {
    "arxiv_id": "2502.14786v1",
    "title": "SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features",
    "authors": [
      "Michael Tschannen",
      "Alexey Gritsenko",
      "Xiao Wang",
      "Muhammad Ferjad Naeem",
      "Ibrahim Alabdulmohsin",
      "Nikhil Parthasarathy",
      "Talfan Evans",
      "Lucas Beyer",
      "Ye Xia",
      "Basil Mustafa",
      "Olivier Hénaff",
      "Jeremiah Harmsen",
      "Andreas Steiner",
      "Xiaohua Zhai"
    ],
    "abstract": "We introduce SigLIP 2, a family of new multilingual vision-language encoders\nthat build on the success of the original SigLIP. In this second iteration, we\nextend the original image-text training objective with several prior,\nindependently developed techniques into a unified recipe -- this includes\ncaptioning-based pretraining, self-supervised losses (self-distillation, masked\nprediction) and online data curation. With these changes, SigLIP 2 models\noutperform their SigLIP counterparts at all model scales in core capabilities,\nincluding zero-shot classification, image-text retrieval, and transfer\nperformance when extracting visual representations for Vision-Language Models\n(VLMs). Furthermore, the new training recipe leads to significant improvements\non localization and dense prediction tasks. We also train variants which\nsupport multiple resolutions and preserve the input's native aspect ratio.\nFinally, we train on a more diverse data-mixture that includes de-biasing\ntechniques, leading to much better multilingual understanding and improved\nfairness. To allow users to trade off inference cost with performance, we\nrelease model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M),\nand g (1B).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Model checkpoints are available at\n  https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text/README_siglip2.md",
    "pdf_url": "http://arxiv.org/pdf/2502.14786v1",
    "published_date": "2025-02-20 18:08:29 UTC",
    "updated_date": "2025-02-20 18:08:29 UTC"
  },
  {
    "arxiv_id": "2502.14785v1",
    "title": "Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches",
    "authors": [
      "Chandrashekar Muniyappa",
      "Kendall Willets",
      "Sriraman Krishnamoorthy"
    ],
    "abstract": "Predicting the right number of TVs (Device Reach) in real-time based on a\nuser-specified targeting attributes is imperative for running multi-million\ndollar ADs business. The traditional approach of SQL queries to join billions\nof records across multiple targeting dimensions is extremely slow. As a\nworkaround, many applications will have an offline process to crunch these\nnumbers and present the results after many hours. In our case, the solution was\nan offline process taking 24 hours to onboard a customer resulting in a\npotential loss of business. To solve this problem, we have built a new\nreal-time prediction system using MinHash and HyperLogLog (HLL) data sketches\nto compute the device reach at runtime when a user makes a request. However,\nexisting MinHash implementations do not solve the complex problem of multilevel\naggregation and intersection. This work will show how we have solved this\nproblem, in addition, we have improved MinHash algorithm to run 4 times faster\nusing Single Instruction Multiple Data (SIMD) vectorized operations for high\nspeed and accuracy with constant space to process billions of records. Finally,\nby experiments, we prove that the results are as accurate as traditional\noffline prediction system with an acceptable error rate of 5%.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG",
      "60G25",
      "I.5.3"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14785v1",
    "published_date": "2025-02-20 18:05:34 UTC",
    "updated_date": "2025-02-20 18:05:34 UTC"
  },
  {
    "arxiv_id": "2502.14780v1",
    "title": "ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting",
    "authors": [
      "Abhijit Mishra",
      "Richard Noh",
      "Hsiang Fu",
      "Mingda Li",
      "Minji Kim"
    ],
    "abstract": "Efficient and privacy-preserving multimodal interaction is essential as AR,\nVR, and modern smartphones with powerful cameras become primary interfaces for\nhuman-computer communication. Existing powerful large vision-language models\n(VLMs) enabling multimodal interaction often rely on cloud-based processing,\nraising significant concerns about (1) visual privacy by transmitting sensitive\nvision data to servers, and (2) their limited real-time, on-device usability.\nThis paper explores Visual Instruction Rewriting, a novel approach that\ntransforms multimodal instructions into text-only commands, allowing seamless\nintegration of lightweight on-device instruction rewriter VLMs (250M\nparameters) with existing conversational AI systems, enhancing vision data\nprivacy. To achieve this, we present a dataset of over 39,000 examples across\n14 domains and develop a compact VLM, pretrained on image captioning datasets\nand fine-tuned for instruction rewriting. Experimental results, evaluated\nthrough NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic\nparsing analysis, demonstrate that even a quantized version of the model\n(<500MB storage footprint) can achieve effective instruction rewriting, thus\nenabling privacy-focused, multimodal AI applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 7 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14780v1",
    "published_date": "2025-02-20 18:01:41 UTC",
    "updated_date": "2025-02-20 18:01:41 UTC"
  },
  {
    "arxiv_id": "2502.14778v1",
    "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models",
    "authors": [
      "Jeonghun Baek",
      "Akiko Aizawa",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs. We plan to make the source code and data publicly\navailable upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14778v1",
    "published_date": "2025-02-20 17:59:59 UTC",
    "updated_date": "2025-02-20 17:59:59 UTC"
  },
  {
    "arxiv_id": "2502.14777v1",
    "title": "Making Universal Policies Universal",
    "authors": [
      "Niklas Höpner",
      "David Kuric",
      "Herke van Hoof"
    ],
    "abstract": "The development of a generalist agent capable of solving a wide range of\nsequential decision-making tasks remains a significant challenge. We address\nthis problem in a cross-agent setup where agents share the same observation\nspace but differ in their action spaces. Our approach builds on the universal\npolicy framework, which decouples policy learning into two stages: a\ndiffusion-based planner that generates observation sequences and an inverse\ndynamics model that assigns actions to these plans. We propose a method for\ntraining the planner on a joint dataset composed of trajectories from all\nagents. This method offers the benefit of positive transfer by pooling data\nfrom different agents, while the primary challenge lies in adapting shared\nplans to each agent's unique constraints. We evaluate our approach on the\nBabyAI environment, covering tasks of varying complexity, and demonstrate\npositive transfer across agents. Additionally, we examine the planner's\ngeneralisation ability to unseen agents and compare our method to traditional\nimitation learning approaches. By training on a pooled dataset from multiple\nagents, our universal policy achieves an improvement of up to $42.20\\%$ in task\ncompletion accuracy compared to a policy trained on a dataset from a single\nagent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14777v1",
    "published_date": "2025-02-20 17:59:55 UTC",
    "updated_date": "2025-02-20 17:59:55 UTC"
  },
  {
    "arxiv_id": "2502.14768v1",
    "title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "authors": [
      "Tian Xie",
      "Zitian Gao",
      "Qingnan Ren",
      "Haoming Luo",
      "Yuqian Hong",
      "Bryan Dai",
      "Joey Zhou",
      "Kai Qiu",
      "Zhirong Wu",
      "Chong Luo"
    ],
    "abstract": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in large reasoning models. To analyze\nreasoning dynamics, we use synthetic logic puzzles as training data due to\ntheir controllable complexity and straightforward answer verification. We make\nsome key technical contributions that lead to effective and stable RL training:\na system prompt that emphasizes the thinking and answering process, a stringent\nformat reward function that penalizes outputs for taking shortcuts, and a\nstraightforward training recipe that achieves stable convergence. Our 7B model\ndevelops advanced reasoning skills-such as reflection, verification, and\nsummarization-that are absent from the logic corpus. Remarkably, after training\non just 5K logic problems, it demonstrates generalization abilities to the\nchallenging math benchmarks AIME and AMC.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14768v1",
    "published_date": "2025-02-20 17:49:26 UTC",
    "updated_date": "2025-02-20 17:49:26 UTC"
  },
  {
    "arxiv_id": "2502.14944v1",
    "title": "Reward-Guided Iterative Refinement in Diffusion Models at Test-Time with Applications to Protein and DNA Design",
    "authors": [
      "Masatoshi Uehara",
      "Xingyu Su",
      "Yulai Zhao",
      "Xiner Li",
      "Aviv Regev",
      "Shuiwang Ji",
      "Sergey Levine",
      "Tommaso Biancalani"
    ],
    "abstract": "To fully leverage the capabilities of diffusion models, we are often\ninterested in optimizing downstream reward functions during inference. While\nnumerous algorithms for reward-guided generation have been recently proposed\ndue to their significance, current approaches predominantly focus on\nsingle-shot generation, transitioning from fully noised to denoised states. We\npropose a novel framework for inference-time reward optimization with diffusion\nmodels inspired by evolutionary algorithms. Our approach employs an iterative\nrefinement process consisting of two steps in each iteration: noising and\nreward-guided denoising. This sequential refinement allows for the gradual\ncorrection of errors introduced during reward optimization. Besides, we provide\na theoretical guarantee for our framework. Finally, we demonstrate its superior\nempirical performance in protein and cell-type-specific regulatory DNA design.\nThe code is available at\n\\href{https://github.com/masa-ue/ProDifEvo-Refinement}{https://github.com/masa-ue/ProDifEvo-Refinement}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review. If you have any suggestions/missing references, please\n  let us know",
    "pdf_url": "http://arxiv.org/pdf/2502.14944v1",
    "published_date": "2025-02-20 17:48:45 UTC",
    "updated_date": "2025-02-20 17:48:45 UTC"
  },
  {
    "arxiv_id": "2502.14943v3",
    "title": "GenAI vs. Human Fact-Checkers: Accurate Ratings, Flawed Rationales",
    "authors": [
      "Yuehong Cassandra Tai",
      "Khushi Navin Patni",
      "Nicholas Daniel Hemauer",
      "Bruce Desmarais",
      "Yu-Ru Lin"
    ],
    "abstract": "Despite recent advances in understanding the capabilities and limits of\ngenerative artificial intelligence (GenAI) models, we are just beginning to\nunderstand their capacity to assess and reason about the veracity of content.\nWe evaluate multiple GenAI models across tasks that involve the rating of, and\nperceived reasoning about, the credibility of information. The information in\nour experiments comes from content that subnational U.S. politicians post to\nFacebook. We find that GPT-4o, one of the most used AI models in consumer\napplications, outperforms other models, but all models exhibit only moderate\nagreement with human coders. Importantly, even when GenAI models accurately\nidentify low-credibility content, their reasoning relies heavily on linguistic\nfeatures and ``hard'' criteria, such as the level of detail, source\nreliability, and language formality, rather than an understanding of veracity.\nWe also assess the effectiveness of summarized versus full content inputs,\nfinding that summarized content holds promise for improving efficiency without\nsacrificing accuracy. While GenAI has the potential to support human\nfact-checkers in scaling misinformation detection, our results caution against\nrelying solely on these models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication in the 17th ACM Web Science Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14943v3",
    "published_date": "2025-02-20 17:47:40 UTC",
    "updated_date": "2025-02-25 19:06:04 UTC"
  },
  {
    "arxiv_id": "2503.22684v1",
    "title": "Binary and Multi-Class Intrusion Detection in IoT Using Standalone and Hybrid Machine and Deep Learning Models",
    "authors": [
      "Md Ahnaf Akif"
    ],
    "abstract": "Maintaining security in IoT systems depends on intrusion detection since\nthese networks' sensitivity to cyber-attacks is growing. Based on the IoT23\ndataset, this study explores the use of several Machine Learning (ML) and Deep\nLearning (DL) along with the hybrid models for binary and multi-class intrusion\ndetection. The standalone machine and deep learning models like Random Forest\n(RF), Extreme Gradient Boosting (XGBoost), Artificial Neural Network (ANN),\nK-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Convolutional\nNeural Network (CNN) were used. Furthermore, two hybrid models were created by\ncombining machine learning techniques: RF, XGBoost, AdaBoost, KNN, and SVM and\nthese hybrid models were voting based hybrid classifier. Where one is for\nbinary, and the other one is for multi-class classification. These models vi\nwere tested using precision, recall, accuracy, and F1-score criteria and\ncompared the performance of each model. This work thoroughly explains how\nhybrid, standalone ML and DL techniques could improve IDS (Intrusion Detection\nSystem) in terms of accuracy and scalability in IoT (Internet of Things).",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "Master's thesis, 80 pages, 18 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.22684v1",
    "published_date": "2025-02-20 17:47:38 UTC",
    "updated_date": "2025-02-20 17:47:38 UTC"
  },
  {
    "arxiv_id": "2502.14767v1",
    "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis",
    "authors": [
      "Priyanka Kargupta",
      "Ishika Agarwal",
      "Tal August",
      "Jiawei Han"
    ],
    "abstract": "With the exponential growth of research facilitated by modern technology and\nimproved accessibility, scientific discoveries have become increasingly\nfragmented within and across fields. This makes it challenging to assess the\nsignificance, novelty, incremental findings, and equivalent ideas between\nrelated works, particularly those from different research communities. Large\nlanguage models (LLMs) have recently demonstrated strong quantitative and\nqualitative reasoning abilities, and multi-agent LLM debates have shown promise\nin handling complex reasoning tasks by exploring diverse perspectives and\nreasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a\nframework which converts scientific papers into LLM personas that debate their\nrespective novelties. To emphasize structured, critical reasoning rather than\nfocusing solely on outcomes, ToD dynamically constructs a debate tree, enabling\nfine-grained analysis of independent novelty arguments within scholarly\narticles. Through experiments on scientific literature across various domains,\nevaluated by expert researchers, we demonstrate that ToD generates informative\narguments, effectively contrasts papers, and supports researchers in their\nliterature review.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code available at: https://github.com/pkargupta/tree-of-debate",
    "pdf_url": "http://arxiv.org/pdf/2502.14767v1",
    "published_date": "2025-02-20 17:43:40 UTC",
    "updated_date": "2025-02-20 17:43:40 UTC"
  },
  {
    "arxiv_id": "2502.14765v1",
    "title": "Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning",
    "authors": [
      "Juraj Vladika",
      "Ivana Hacajová",
      "Florian Matthes"
    ],
    "abstract": "Fact verification (FV) aims to assess the veracity of a claim based on\nrelevant evidence. The traditional approach for automated FV includes a\nthree-part pipeline relying on short evidence snippets and encoder-only\ninference models. More recent approaches leverage the multi-turn nature of LLMs\nto address FV as a step-by-step problem where questions inquiring additional\ncontext are generated and answered until there is enough information to make a\ndecision. This iterative method makes the verification process rational and\nexplainable. While these methods have been tested for encyclopedic claims,\nexploration on domain-specific and realistic claims is missing. In this work,\nwe apply an iterative FV system on three medical fact-checking datasets and\nevaluate it with multiple settings, including different LLMs, external web\nsearch, and structured reasoning using logic predicates. We demonstrate\nimprovements in the final performance over traditional approaches and the high\npotential of step-by-step FV systems for domain-specific claims.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 (Main)",
    "pdf_url": "http://arxiv.org/pdf/2502.14765v1",
    "published_date": "2025-02-20 17:40:21 UTC",
    "updated_date": "2025-02-20 17:40:21 UTC"
  },
  {
    "arxiv_id": "2502.14760v1",
    "title": "EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations",
    "authors": [
      "Haotian Zhai",
      "Connor Lawless",
      "Ellen Vitercik",
      "Liu Leqi"
    ],
    "abstract": "A fundamental problem in combinatorial optimization is identifying equivalent\nformulations, which can lead to more efficient solution strategies and deeper\ninsights into a problem's computational complexity. The need to automatically\nidentify equivalence between problem formulations has grown as optimization\ncopilots--systems that generate problem formulations from natural language\ndescriptions--have proliferated. However, existing approaches to checking\nformulation equivalence lack grounding, relying on simple heuristics which are\ninsufficient for rigorous validation. Inspired by Karp reductions, in this work\nwe introduce quasi-Karp equivalence, a formal criterion for determining when\ntwo optimization formulations are equivalent based on the existence of a\nmapping between their decision variables. We propose EquivaMap, a framework\nthat leverages large language models to automatically discover such mappings,\nenabling scalable and reliable equivalence verification. To evaluate our\napproach, we construct the first open-source dataset of equivalent optimization\nformulations, generated by applying transformations such as adding slack\nvariables or valid inequalities to existing formulations. Empirically,\nEquivaMap significantly outperforms existing methods, achieving substantial\nimprovements in correctly identifying formulation equivalence.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14760v1",
    "published_date": "2025-02-20 17:35:32 UTC",
    "updated_date": "2025-02-20 17:35:32 UTC"
  },
  {
    "arxiv_id": "2502.14759v1",
    "title": "On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems",
    "authors": [
      "Juraj Vladika",
      "Florian Matthes"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as an approach to augment\nlarge language models (LLMs) by reducing their reliance on static knowledge and\nimproving answer factuality. RAG retrieves relevant context snippets and\ngenerates an answer based on them. Despite its increasing industrial adoption,\nsystematic exploration of RAG components is lacking, particularly regarding the\nideal size of provided context, and the choice of base LLM and retrieval\nmethod. To help guide development of robust RAG systems, we evaluate various\ncontext sizes, BM25 and semantic search as retrievers, and eight base LLMs.\nMoving away from the usual RAG evaluation with short answers, we explore the\nmore challenging long-form question answering in two domains, where a good\nanswer has to utilize the entire context. Our findings indicate that final QA\nperformance improves steadily with up to 15 snippets but stagnates or declines\nbeyond that. Finally, we show that different general-purpose LLMs excel in the\nbiomedical domain than the encyclopedic one, and that open-domain evidence\nretrieval in large corpora is challenging.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to Findings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14759v1",
    "published_date": "2025-02-20 17:34:34 UTC",
    "updated_date": "2025-02-20 17:34:34 UTC"
  },
  {
    "arxiv_id": "2502.14940v1",
    "title": "FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models",
    "authors": [
      "Thomas Froech",
      "Olaf Wysocki",
      "Yan Xia",
      "Junyu Xie",
      "Benedikt Schwab",
      "Daniel Cremers",
      "Thomas H. Kolbe"
    ],
    "abstract": "High-detail semantic 3D building models are frequently utilized in robotics,\ngeoinformatics, and computer vision. One key aspect of creating such models is\nemploying 2D conflict maps that detect openings' locations in building facades.\nYet, in reality, these maps are often incomplete due to obstacles encountered\nduring laser scanning. To address this challenge, we introduce FacaDiffy, a\nnovel method for inpainting unseen facade parts by completing conflict maps\nwith a personalized Stable Diffusion model. Specifically, we first propose a\ndeterministic ray analysis approach to derive 2D conflict maps from existing 3D\nbuilding models and corresponding laser scanning point clouds. Furthermore, we\nfacilitate the inpainting of unseen facade objects into these 2D conflict maps\nby leveraging the potential of personalizing a Stable Diffusion model. To\ncomplement the scarcity of real-world training data, we also develop a scalable\npipeline to produce synthetic conflict maps using random city model generators\nand annotated facade images. Extensive experiments demonstrate that FacaDiffy\nachieves state-of-the-art performance in conflict map completion compared to\nvarious inpainting baselines and increases the detection rate by $22\\%$ when\napplying the completed conflict maps for high-definition 3D semantic building\nreconstruction. The code is be publicly available in the corresponding GitHub\nrepository: https://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted for GeoSpatial Week 2025, ISPRS Annals",
    "pdf_url": "http://arxiv.org/pdf/2502.14940v1",
    "published_date": "2025-02-20 17:32:41 UTC",
    "updated_date": "2025-02-20 17:32:41 UTC"
  },
  {
    "arxiv_id": "2502.14939v1",
    "title": "Online hand gesture recognition using Continual Graph Transformers",
    "authors": [
      "Rim Slama",
      "Wael Rabah",
      "Hazem Wannous"
    ],
    "abstract": "Online continuous action recognition has emerged as a critical research area\ndue to its practical implications in real-world applications, such as\nhuman-computer interaction, healthcare, and robotics. Among various modalities,\nskeleton-based approaches have gained significant popularity, demonstrating\ntheir effectiveness in capturing 3D temporal data while ensuring robustness to\nenvironmental variations. However, most existing works focus on segment-based\nrecognition, making them unsuitable for real-time, continuous recognition\nscenarios. In this paper, we propose a novel online recognition system designed\nfor real-time skeleton sequence streaming. Our approach leverages a hybrid\narchitecture combining Spatial Graph Convolutional Networks (S-GCN) for spatial\nfeature extraction and a Transformer-based Graph Encoder (TGE) for capturing\ntemporal dependencies across frames. Additionally, we introduce a continual\nlearning mechanism to enhance model adaptability to evolving data\ndistributions, ensuring robust recognition in dynamic environments. We evaluate\nour method on the SHREC'21 benchmark dataset, demonstrating its superior\nperformance in online hand gesture recognition. Our approach not only achieves\nstate-of-the-art accuracy but also significantly reduces false positive rates,\nmaking it a compelling solution for real-time applications. The proposed system\ncan be seamlessly integrated into various domains, including human-robot\ncollaboration and assistive technologies, where natural and intuitive\ninteraction is crucial.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14939v1",
    "published_date": "2025-02-20 17:27:55 UTC",
    "updated_date": "2025-02-20 17:27:55 UTC"
  },
  {
    "arxiv_id": "2502.14753v1",
    "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders",
    "authors": [
      "Maya Varma",
      "Ashwin Kumar",
      "Rogier van der Sluijs",
      "Sophie Ostmeier",
      "Louis Blankemeier",
      "Pierre Chambon",
      "Christian Bluethgen",
      "Jip Prince",
      "Curtis Langlotz",
      "Akshay Chaudhari"
    ],
    "abstract": "Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https://github.com/StanfordMIMI/MedVAE.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14753v1",
    "published_date": "2025-02-20 17:24:06 UTC",
    "updated_date": "2025-02-20 17:24:06 UTC"
  },
  {
    "arxiv_id": "2502.14743v2",
    "title": "Multi-Agent Coordination across Diverse Applications: A Survey",
    "authors": [
      "Lijun Sun",
      "Yijun Yang",
      "Qiqi Duan",
      "Yuhui Shi",
      "Chao Lyu",
      "Yu-Cheng Chang",
      "Chin-Teng Lin",
      "Yang Shen"
    ],
    "abstract": "Multi-agent coordination studies the underlying mechanism enabling the\ntrending spread of diverse multi-agent systems (MAS) and has received\nincreasing attention, driven by the expansion of emerging applications and\nrapid AI advances. This survey outlines the current state of coordination\nresearch across applications through a unified understanding that answers four\nfundamental coordination questions: (1) what is coordination; (2) why\ncoordination; (3) who to coordinate with; and (4) how to coordinate. Our\npurpose is to explore existing ideas and expertise in coordination and their\nconnections across diverse applications, while identifying and highlighting\nemerging and promising research directions. First, general coordination\nproblems that are essential to varied applications are identified and analyzed.\nSecond, a number of MAS applications are surveyed, ranging from widely studied\ndomains, e.g., search and rescue, warehouse automation and logistics, and\ntransportation systems, to emerging fields including humanoid and\nanthropomorphic robots, satellite systems, and large language models (LLMs).\nFinally, open challenges about the scalability, heterogeneity, and learning\nmechanisms of MAS are analyzed and discussed. In particular, we identify the\nhybridization of hierarchical and decentralized coordination, human-MAS\ncoordination, and LLM-based MAS as promising future directions.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "23 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14743v2",
    "published_date": "2025-02-20 17:12:45 UTC",
    "updated_date": "2025-02-21 02:41:44 UTC"
  },
  {
    "arxiv_id": "2503.11664v1",
    "title": "An LLM-Based Approach for Insight Generation in Data Analysis",
    "authors": [
      "Alberto Sánchez Pérez",
      "Alaa Boukhary",
      "Paolo Papotti",
      "Luis Castejón Lozano",
      "Adam Elwood"
    ],
    "abstract": "Generating insightful and actionable information from databases is critical\nin data analysis. This paper introduces a novel approach using Large Language\nModels (LLMs) to automatically generate textual insights. Given a multi-table\ndatabase as input, our method leverages LLMs to produce concise, text-based\ninsights that reflect interesting patterns in the tables. Our framework\nincludes a Hypothesis Generator to formulate domain-relevant questions, a Query\nAgent to answer such questions by generating SQL queries against a database,\nand a Summarization module to verbalize the insights. The insights are\nevaluated for both correctness and subjective insightfulness using a hybrid\nmodel of human judgment and automated metrics. Experimental results on public\nand enterprise databases demonstrate that our approach generates more\ninsightful insights than other approaches while maintaining correctness.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted for publication at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.11664v1",
    "published_date": "2025-02-20 17:09:59 UTC",
    "updated_date": "2025-02-20 17:09:59 UTC"
  },
  {
    "arxiv_id": "2502.14740v1",
    "title": "YOLOv12: A Breakdown of the Key Architectural Features",
    "authors": [
      "Mujadded Al Rabbani Alif",
      "Muhammad Hussain"
    ],
    "abstract": "This paper presents an architectural analysis of YOLOv12, a significant\nadvancement in single-stage, real-time object detection building upon the\nstrengths of its predecessors while introducing key improvements. The model\nincorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and\nFlashAttention-driven area-based attention, improving feature extraction,\nenhanced efficiency, and robust detections. With multiple model variants,\nsimilar to its predecessors, YOLOv12 offers scalable solutions for both\nlatency-sensitive and high-accuracy applications. Experimental results manifest\nconsistent gains in mean average precision (mAP) and inference speed, making\nYOLOv12 a compelling choice for applications in autonomous systems, security,\nand real-time analytics. By achieving an optimal balance between computational\nefficiency and performance, YOLOv12 sets a new benchmark for real-time computer\nvision, facilitating deployment across diverse hardware platforms, from edge\ndevices to high-performance clusters.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14740v1",
    "published_date": "2025-02-20 17:08:43 UTC",
    "updated_date": "2025-02-20 17:08:43 UTC"
  },
  {
    "arxiv_id": "2502.14735v1",
    "title": "EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration",
    "authors": [
      "Minjie Hong",
      "Yan Xia",
      "Zehan Wang",
      "Jieming Zhu",
      "Ye Wang",
      "Sihang Cai",
      "Xiaoda Yang",
      "Quanyu Dai",
      "Zhenhua Dong",
      "Zhimeng Zhang",
      "Zhou Zhao"
    ],
    "abstract": "Large language models (LLMs) are increasingly leveraged as foundational\nbackbones in the development of advanced recommender systems, offering enhanced\ncapabilities through their extensive knowledge and reasoning. Existing\nllm-based recommender systems (RSs) often face challenges due to the\nsignificant differences between the linguistic semantics of pre-trained LLMs\nand the collaborative semantics essential for RSs. These systems use\npre-trained linguistic semantics but learn collaborative semantics from scratch\nvia the llm-Backbone. However, LLMs are not designed for recommendations,\nleading to inefficient collaborative learning, weak result correlations, and\npoor integration of traditional RS features. To address these challenges, we\npropose EAGER-LLM, a decoder-only llm-based generative recommendation framework\nthat integrates endogenous and exogenous behavioral and semantic information in\na non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich\nitem indices that integrates indexing sequences for exogenous signals, enabling\nefficient link-wide processing; 2)non-invasive multiscale alignment\nreconstruction tasks guide the model toward a deeper understanding of both\ncollaborative and semantic signals; 3)an annealing adapter designed to finely\nbalance the model's recommendation performance with its comprehension\ncapabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing\non three public benchmarks.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "9 pages, 6 figures, accpeted by WWW 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14735v1",
    "published_date": "2025-02-20 17:01:57 UTC",
    "updated_date": "2025-02-20 17:01:57 UTC"
  },
  {
    "arxiv_id": "2502.14727v1",
    "title": "WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models",
    "authors": [
      "Yifu Chen",
      "Shengpeng Ji",
      "Haoxiao Wang",
      "Ziqing Wang",
      "Siyu Chen",
      "Jinzheng He",
      "Jin Xu",
      "Zhou Zhao"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) has gained widespread adoption owing to\nits capacity to empower large language models (LLMs) to integrate external\nknowledge. However, existing RAG frameworks are primarily designed for\ntext-based LLMs and rely on Automatic Speech Recognition to process speech\ninput, which discards crucial audio information, risks transcription errors,\nand increases computational overhead. Therefore, we introduce WavRAG, the first\nretrieval augmented generation framework with native, end-to-end audio support.\nWavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw\naudio for both embedding and retrieval. 2) WavRAG integrates audio and text\ninto a unified knowledge representation. Specifically, we propose the\nWavRetriever to facilitate the retrieval from a text-audio hybrid knowledge\nbase, and further enhance the in-context capabilities of spoken dialogue models\nthrough the integration of chain-of-thought reasoning. In comparison to\nstate-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval\nperformance while delivering a 10x acceleration. Furthermore, WavRAG's unique\ntext-audio hybrid retrieval capability extends the boundaries of RAG to the\naudio modality.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14727v1",
    "published_date": "2025-02-20 16:54:07 UTC",
    "updated_date": "2025-02-20 16:54:07 UTC"
  },
  {
    "arxiv_id": "2502.14724v2",
    "title": "Ranking Joint Policies in Dynamic Games using Evolutionary Dynamics",
    "authors": [
      "Natalia Koliou",
      "George Vouros"
    ],
    "abstract": "Game-theoretic solution concepts, such as the Nash equilibrium, have been key\nto finding stable joint actions in multi-player games. However, it has been\nshown that the dynamics of agents' interactions, even in simple two-player\ngames with few strategies, are incapable of reaching Nash equilibria,\nexhibiting complex and unpredictable behavior. Instead, evolutionary approaches\ncan describe the long-term persistence of strategies and filter out transient\nones, accounting for the long-term dynamics of agents' interactions. Our goal\nis to identify agents' joint strategies that result in stable behavior, being\nresistant to changes, while also accounting for agents' payoffs, in dynamic\ngames. Towards this goal, and building on previous results, this paper proposes\ntransforming dynamic games into their empirical forms by considering agents'\nstrategies instead of agents' actions, and applying the evolutionary\nmethodology $\\alpha$-Rank to evaluate and rank strategy profiles according to\ntheir long-term dynamics. This methodology not only allows us to identify joint\nstrategies that are strong through agents' long-term interactions, but also\nprovides a descriptive, transparent framework regarding the high ranking of\nthese strategies. Experiments report on agents that aim to collaboratively\nsolve a stochastic version of the graph coloring problem. We consider different\nstyles of play as strategies to define the empirical game, and train policies\nrealizing these strategies, using the DQN algorithm. Then we run simulations to\ngenerate the payoff matrix required by $\\alpha$-Rank to rank joint strategies.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14724v2",
    "published_date": "2025-02-20 16:50:38 UTC",
    "updated_date": "2025-05-17 08:18:31 UTC"
  },
  {
    "arxiv_id": "2502.14714v2",
    "title": "From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT",
    "authors": [
      "Ahmed Abdeen Hamed",
      "Alessandro Crimi",
      "Magdalena M. Misiak",
      "Byung Suk Lee"
    ],
    "abstract": "The generative capabilities of LLM models offer opportunities for\naccelerating tasks but raise concerns about the authenticity of the knowledge\nthey produce. To address these concerns, we present a computational approach\nthat evaluates the factual accuracy of biomedical knowledge generated by an\nLLM. Our approach consists of two processes: generating disease-centric\nassociations and verifying these associations using the semantic framework of\nbiomedical ontologies. Using ChatGPT as the selected LLM, we designed\nprompt-engineering processes to establish linkages between diseases and related\ndrugs, symptoms, and genes, and assessed consistency across multiple ChatGPT\nmodels (e.g., GPT-turbo, GPT-4, etc.). Experimental results demonstrate high\naccuracy in identifying disease terms (88%-97%), drug names (90%-91%), and\ngenetic information (88%-98%). However, symptom term identification was notably\nlower (49%-61%), due to the informal and verbose nature of symptom\ndescriptions, which hindered effective semantic matching with the formal\nlanguage of specialized ontologies. Verification of associations reveals\nliterature coverage rates of 89%-91% for disease-drug and disease-gene pairs,\nwhile symptom-related associations exhibit lower coverage (49%-62%).",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "I.2; I.2.4; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "28 pages, 6 figures, In Review with a Cell Press Journal",
    "pdf_url": "http://arxiv.org/pdf/2502.14714v2",
    "published_date": "2025-02-20 16:39:57 UTC",
    "updated_date": "2025-03-23 16:02:53 UTC"
  },
  {
    "arxiv_id": "2502.14708v1",
    "title": "Human Misperception of Generative-AI Alignment: A Laboratory Experiment",
    "authors": [
      "Kevin He",
      "Ran Shorrer",
      "Mengjia Xia"
    ],
    "abstract": "We conduct an incentivized laboratory experiment to study people's perception\nof generative artificial intelligence (GenAI) alignment in the context of\neconomic decision-making. Using a panel of economic problems spanning the\ndomains of risk, time preference, social preference, and strategic\ninteractions, we ask human subjects to make choices for themselves and to\npredict the choices made by GenAI on behalf of a human user. We find that\npeople overestimate the degree of alignment between GenAI's choices and human\nchoices. In every problem, human subjects' average prediction about GenAI's\nchoice is substantially closer to the average human-subject choice than it is\nto the GenAI choice. At the individual level, different subjects' predictions\nabout GenAI's choice in a given problem are highly correlated with their own\nchoices in the same problem. We explore the implications of people\noverestimating GenAI alignment in a simple theoretical model.",
    "categories": [
      "econ.TH",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14708v1",
    "published_date": "2025-02-20 16:32:42 UTC",
    "updated_date": "2025-02-20 16:32:42 UTC"
  },
  {
    "arxiv_id": "2502.14706v3",
    "title": "Building reliable sim driving agents by scaling self-play",
    "authors": [
      "Daphne Cornelisse",
      "Aarav Pandya",
      "Kevin Joseph",
      "Joseph Suárez",
      "Eugene Vinitsky"
    ],
    "abstract": "Simulation agents are essential for designing and testing systems that\ninteract with humans, such as autonomous vehicles (AVs). These agents serve\nvarious purposes, from benchmarking AV performance to stress-testing system\nlimits, but all applications share one key requirement: reliability. To enable\nsound experimentation, a simulation agent must behave as intended. It should\nminimize actions that may lead to undesired outcomes, such as collisions, which\ncan distort the signal-to-noise ratio in analyses. As a foundation for reliable\nsim agents, we propose scaling self-play to thousands of scenarios on the Waymo\nOpen Motion Dataset under semi-realistic limits on human perception and\ncontrol. Training from scratch on a single GPU, our agents solve almost the\nfull training set within a day. They generalize to unseen test scenes,\nachieving a 99.8% goal completion rate with less than 0.8% combined collision\nand off-road incidents across 10,000 held-out scenarios. Beyond in-distribution\ngeneralization, our agents show partial robustness to out-of-distribution\nscenes and can be fine-tuned in minutes to reach near-perfect performance in\nsuch cases. We open-source the pre-trained agents and integrate them with a\nbatched multi-agent simulator. Demonstrations of agent behaviors can be viewed\nat https://sites.google.com/view/reliable-sim-agents, and we open-source our\nagents at https://github.com/Emerge-Lab/gpudrive.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "v3",
    "pdf_url": "http://arxiv.org/pdf/2502.14706v3",
    "published_date": "2025-02-20 16:30:45 UTC",
    "updated_date": "2025-05-19 23:24:52 UTC"
  },
  {
    "arxiv_id": "2502.14704v2",
    "title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting",
    "authors": [
      "Yuxuan Yang",
      "Dalin Zhang",
      "Yuxuan Liang",
      "Hua Lu",
      "Gang Chen",
      "Huan Li"
    ],
    "abstract": "Time Series Forecasting (TSF) is a crucial task in various domains, yet\nexisting TSF models rely heavily on high-quality data and insufficiently\nexploit all available data. This paper explores a novel self-supervised\napproach to re-label time series datasets by inherently constructing candidate\ndatasets. During the optimization of a simple reconstruction network,\nintermediates are used as pseudo labels in a self-supervised paradigm,\nimproving generalization for any predictor. We introduce the Self-Correction\nwith Adaptive Mask (SCAM), which discards overfitted components and selectively\nreplaces them with pseudo labels generated from reconstructions. Additionally,\nwe incorporate Spectral Norm Regularization (SNR) to further suppress\noverfitting from a loss landscape perspective. Our experiments on eleven\nreal-world datasets demonstrate that SCAM consistently improves the performance\nof various backbone models. This work offers a new perspective on constructing\ndatasets and enhancing the generalization of TSF models through self-supervised\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14704v2",
    "published_date": "2025-02-20 16:29:37 UTC",
    "updated_date": "2025-02-21 02:25:30 UTC"
  },
  {
    "arxiv_id": "2502.14698v1",
    "title": "General Uncertainty Estimation with Delta Variances",
    "authors": [
      "Simon Schmitt",
      "John Shawe-Taylor",
      "Hado van Hasselt"
    ],
    "abstract": "Decision makers may suffer from uncertainty induced by limited data. This may\nbe mitigated by accounting for epistemic uncertainty, which is however\nchallenging to estimate efficiently for large neural networks. To this extent\nwe investigate Delta Variances, a family of algorithms for epistemic\nuncertainty quantification, that is computationally efficient and convenient to\nimplement. It can be applied to neural networks and more general functions\ncomposed of neural networks. As an example we consider a weather simulator with\na neural-network-based step function inside -- here Delta Variances empirically\nobtain competitive results at the cost of a single gradient computation. The\napproach is convenient as it requires no changes to the neural network\narchitecture or training procedure. We discuss multiple ways to derive Delta\nVariances theoretically noting that special cases recover popular techniques\nand present a unified perspective on multiple related methods. Finally we\nobserve that this general perspective gives rise to a natural extension and\nempirically show its benefit.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14698v1",
    "published_date": "2025-02-20 16:22:40 UTC",
    "updated_date": "2025-02-20 16:22:40 UTC"
  },
  {
    "arxiv_id": "2502.14681v1",
    "title": "seqKAN: Sequence processing with Kolmogorov-Arnold Networks",
    "authors": [
      "Tatiana Boura",
      "Stasinos Konstantopoulos"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine\nlearning framework that is more interpretable and controllable than the\nmulti-layer perceptron. Various network architectures have been proposed within\nthe KAN framework targeting different tasks and application domains, including\nsequence processing.\n  This paper proposes seqKAN, a new KAN architecture for sequence processing.\nAlthough multiple sequence processing KAN architectures have already been\nproposed, we argue that seqKAN is more faithful to the core concept of the KAN\nframework. Furthermore, we empirically demonstrate that it achieves better\nresults.\n  The empirical evaluation is performed on generated data from a complex\nphysics problem on an interpolation and an extrapolation task. Using this\ndataset we compared seqKAN against a prior KAN network for timeseries\nprediction, recurrent deep networks, and symbolic regression. seqKAN\nsubstantially outperforms all architectures, particularly on the extrapolation\ndataset, while also being the most transparent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14681v1",
    "published_date": "2025-02-20 16:10:18 UTC",
    "updated_date": "2025-02-20 16:10:18 UTC"
  },
  {
    "arxiv_id": "2502.14677v2",
    "title": "Data-Constrained Synthesis of Training Data for De-Identification",
    "authors": [
      "Thomas Vakili",
      "Aron Henriksson",
      "Hercules Dalianis"
    ],
    "abstract": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.14677v2",
    "published_date": "2025-02-20 16:09:27 UTC",
    "updated_date": "2025-02-21 16:58:44 UTC"
  },
  {
    "arxiv_id": "2502.14676v2",
    "title": "BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution Network for Pedestrian and Heterogeneous Trajectory Prediction",
    "authors": [
      "Ruochen Li",
      "Stamos Katsigiannis",
      "Tae-Kyun Kim",
      "Hubert P. H. Shum"
    ],
    "abstract": "Trajectory prediction allows better decision-making in applications of\nautonomous vehicles or surveillance by predicting the short-term future\nmovement of traffic agents. It is classified into pedestrian or heterogeneous\ntrajectory prediction. The former exploits the relatively consistent behavior\nof pedestrians, but is limited in real-world scenarios with heterogeneous\ntraffic agents such as cyclists and vehicles. The latter typically relies on\nextra class label information to distinguish the heterogeneous agents, but such\nlabels are costly to annotate and cannot be generalized to represent different\nbehaviors within the same class of agents. In this work, we introduce the\nbehavioral pseudo-labels that effectively capture the behavior distributions of\npedestrians and heterogeneous agents solely based on their motion features,\nsignificantly improving the accuracy of trajectory prediction. To implement the\nframework, we propose the Behavioral Pseudo-Label Informed Sparse Graph\nConvolution Network (BP-SGCN) that learns pseudo-labels and informs to a\ntrajectory predictor. For optimization, we propose a cascaded training scheme,\nin which we first learn the pseudo-labels in an unsupervised manner, and then\nperform end-to-end fine-tuning on the labels in the direction of increasing the\ntrajectory prediction accuracy. Experiments show that our pseudo-labels\neffectively model different behavior clusters and improve trajectory\nprediction. Our proposed BP-SGCN outperforms existing methods using both\npedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets\n(SDD, Argoverse 1).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14676v2",
    "published_date": "2025-02-20 16:09:21 UTC",
    "updated_date": "2025-02-21 21:29:38 UTC"
  },
  {
    "arxiv_id": "2502.14671v3",
    "title": "Explanations of Large Language Models Explain Language Representations in the Brain",
    "authors": [
      "Maryam Rahimi",
      "Yadollah Yaghoobzadeh",
      "Mohammad Reza Daliri"
    ],
    "abstract": "Large language models (LLMs) not only exhibit human-like performance but also\nshare computational principles with the brain's language processing mechanisms.\nWhile prior research has focused on mapping LLMs' internal representations to\nneural activity, we propose a novel approach using explainable AI (XAI) to\nstrengthen this link. Applying attribution methods, we quantify the influence\nof preceding words on LLMs' next-word predictions and use these explanations to\npredict fMRI data from participants listening to narratives. We find that\nattribution methods robustly predict brain activity across the language\nnetwork, revealing a hierarchical pattern: explanations from early layers align\nwith the brain's initial language processing stages, while later layers\ncorrespond to more advanced stages. Additionally, layers with greater influence\non next-word prediction$\\unicode{x2014}$reflected in higher attribution\nscores$\\unicode{x2014}$demonstrate stronger brain alignment. These results\nunderscore XAI's potential for exploring the neural basis of language and\nsuggest brain alignment for assessing the biological plausibility of\nexplanation methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14671v3",
    "published_date": "2025-02-20 16:05:45 UTC",
    "updated_date": "2025-04-03 21:56:08 UTC"
  },
  {
    "arxiv_id": "2502.15840v1",
    "title": "Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents",
    "authors": [
      "Axel Backlund",
      "Lukas Petersson"
    ],
    "abstract": "While Large Language Models (LLMs) can exhibit impressive proficiency in\nisolated, short-term tasks, they often fail to maintain coherent performance\nover longer time horizons. In this paper, we present Vending-Bench, a simulated\nenvironment designed to specifically test an LLM-based agent's ability to\nmanage a straightforward, long-running business scenario: operating a vending\nmachine. Agents must balance inventories, place orders, set prices, and handle\ndaily fees - tasks that are each simple but collectively, over long horizons\n(>20M tokens per run) stress an LLM's capacity for sustained, coherent\ndecision-making. Our experiments reveal high variance in performance across\nmultiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most\nruns and turn a profit, but all models have runs that derail, either through\nmisinterpreting delivery schedules, forgetting orders, or descending into\ntangential \"meltdown\" loops from which they rarely recover. We find no clear\ncorrelation between failures and the point at which the model's context window\nbecomes full, suggesting that these breakdowns do not stem from memory limits.\nApart from highlighting the high variance in performance over long time\nhorizons, Vending-Bench also tests models' ability to acquire capital, a\nnecessity in many hypothetical dangerous AI scenarios. We hope the benchmark\ncan help in preparing for the advent of stronger AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15840v1",
    "published_date": "2025-02-20 15:52:29 UTC",
    "updated_date": "2025-02-20 15:52:29 UTC"
  },
  {
    "arxiv_id": "2502.14645v1",
    "title": "Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs",
    "authors": [
      "Yuchen Wu",
      "Liang Ding",
      "Li Shen",
      "Dacheng Tao"
    ],
    "abstract": "Knowledge editing allows for efficient adaptation of large language models\n(LLMs) to new information or corrections without requiring full retraining.\nHowever, prior methods typically focus on either single-language editing or\nbasic multilingual editing, failing to achieve true cross-linguistic knowledge\nsynchronization. To address this, we present a simple and practical\nstate-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE),\ndesigned to propagate knowledge from a dominant language to other languages\neffectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition\nInstruction Tuning (XE-IT), which fine-tunes the model on a curated parallel\ndataset to modify in-scope knowledge while preserving unrelated information,\nand (ii) Target-language Preference Optimization (TL-PO), which applies\nadvanced optimization techniques to ensure consistency across languages,\nfostering the transfer of updates. Additionally, we contribute a high-quality,\ncross-lingual dataset, specifically designed to enhance knowledge transfer\nacross languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks\nshow that X-KDE significantly enhances cross-lingual performance, achieving an\naverage improvement of +8.19%, while maintaining high accuracy in monolingual\nsettings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14645v1",
    "published_date": "2025-02-20 15:32:31 UTC",
    "updated_date": "2025-02-20 15:32:31 UTC"
  },
  {
    "arxiv_id": "2502.14637v2",
    "title": "ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation",
    "authors": [
      "Angxiao Yue",
      "Zichong Wang",
      "Hongteng Xu"
    ],
    "abstract": "Protein backbone generation plays a central role in de novo protein design\nand is significant for many biological and medical applications. Although\ndiffusion and flow-based generative models provide potential solutions to this\nchallenging task, they often generate proteins with undesired designability and\nsuffer computational inefficiency. In this study, we propose a novel rectified\nquaternion flow (ReQFlow) matching method for fast and high-quality protein\nbackbone generation. In particular, our method generates a local translation\nand a 3D rotation from random noise for each residue in a protein chain, which\nrepresents each 3D rotation as a unit quaternion and constructs its flow by\nspherical linear interpolation (SLERP) in an exponential format. We train the\nmodel by quaternion flow (QFlow) matching with guaranteed numerical stability\nand rectify the QFlow model to accelerate its inference and improve the\ndesignability of generated protein backbones, leading to the proposed ReQFlow\nmodel. Experiments show that ReQFlow achieves state-of-the-art performance in\nprotein backbone generation while requiring much fewer sampling steps and\nsignificantly less inference time (e.g., being 37x faster than RFDiffusion and\n62x faster than Genie2 when generating a backbone of length 300), demonstrating\nits effectiveness and efficiency. The code is available at\nhttps://github.com/AngxiaoYue/ReQFlow.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14637v2",
    "published_date": "2025-02-20 15:20:37 UTC",
    "updated_date": "2025-03-29 07:16:54 UTC"
  },
  {
    "arxiv_id": "2502.15839v1",
    "title": "FedMobile: Enabling Knowledge Contribution-aware Multi-modal Federated Learning with Incomplete Modalities",
    "authors": [
      "Yi Liu",
      "Cong Wang",
      "Xingliang Yuan"
    ],
    "abstract": "The Web of Things (WoT) enhances interoperability across web-based and\nubiquitous computing platforms while complementing existing IoT standards. The\nmultimodal Federated Learning (FL) paradigm has been introduced to enhance WoT\nby enabling the fusion of multi-source mobile sensing data while preserving\nprivacy. However, a key challenge in mobile sensing systems using multimodal FL\nis modality incompleteness, where some modalities may be unavailable or only\npartially captured, potentially degrading the system's performance and\nreliability. Current multimodal FL frameworks typically train multiple unimodal\nFL subsystems or apply interpolation techniques on the node side to approximate\nmissing modalities. However, these approaches overlook the shared latent\nfeature space among incomplete modalities across different nodes and fail to\ndiscriminate against low-quality nodes. To address this gap, we present\nFedMobile, a new knowledge contribution-aware multimodal FL framework designed\nfor robust learning despite missing modalities. FedMobile prioritizes\nlocal-to-global knowledge transfer, leveraging cross-node multimodal feature\ninformation to reconstruct missing features. It also enhances system\nperformance and resilience to modality heterogeneity through rigorous node\ncontribution assessments and knowledge contribution-aware aggregation rules.\nEmpirical evaluations on five widely recognized multimodal benchmark datasets\ndemonstrate that FedMobile maintains robust learning even when up to 90% of\nmodality information is missing or when data from two modalities are randomly\nmissing, outperforming state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The Web Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.15839v1",
    "published_date": "2025-02-20 15:10:43 UTC",
    "updated_date": "2025-02-20 15:10:43 UTC"
  },
  {
    "arxiv_id": "2502.14627v2",
    "title": "ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors",
    "authors": [
      "Yuguo Yin",
      "Yuxin Xie",
      "Wenyuan Yang",
      "Dongchao Yang",
      "Jinghan Ru",
      "Xianwei Zhuang",
      "Liming Liang",
      "Yuexian Zou"
    ],
    "abstract": "Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to\nretrieve audio clips or multilingual texts from databases. However, existing\nML-ATR schemes suffer from inconsistencies for instance similarity matching\nacross languages. We theoretically analyze the inconsistency in terms of both\nmultilingual modal alignment direction error and weight error, and propose the\ntheoretical weight error upper bound for quantifying the inconsistency. Based\non the analysis of the weight error upper bound, we find that the inconsistency\nproblem stems from the data distribution error caused by random sampling of\nlanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastive\nlearning and audio-English co-anchor contrastive learning, aiming to mitigate\nthe negative impact of data distribution error on recall and consistency in\nML-ATR. Experimental results on the translated AudioCaps and Clotho datasets\nshow that our scheme achieves state-of-the-art performance on recall and\nconsistency metrics for eight mainstream languages, including English. Our code\nwill be available at https://github.com/ATRI-ACL/ATRI-ACL.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14627v2",
    "published_date": "2025-02-20 15:06:15 UTC",
    "updated_date": "2025-02-22 09:13:13 UTC"
  },
  {
    "arxiv_id": "2502.14620v1",
    "title": "Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity",
    "authors": [
      "Xinghan Pan"
    ],
    "abstract": "This paper investigates the efficacy of RWKV, a novel language model\narchitecture known for its linear attention mechanism, for generating sentence\nembeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate\nthe semantic similarity captured by embeddings from different hidden layers of\na pre-trained RWKV model. The performance is assessed on the Microsoft Research\nParaphrase Corpus (MRPC) dataset using Spearman correlation and compared\nagainst a GloVe-based baseline. My results indicate that while RWKV embeddings\ncapture some semantic relatedness, they underperform compared to the GloVe\nbaseline in terms of Spearman correlation. I also analyze the inference time\nand GPU memory usage, highlighting the computational trade-offs associated with\nRWKV embeddings. The findings suggest that while RWKV offers potential\nadvantages in terms of linear scaling, its zero-shot sentence embedding quality\nfor semantic similarity tasks requires further investigation and potential\ntask-specific fine-tuning to match or exceed simpler baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; I.7.3"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 3 tables, preprint on ArXiV, includes detailed analysis of\n  RWKV for semantic similarity tasks",
    "pdf_url": "http://arxiv.org/pdf/2502.14620v1",
    "published_date": "2025-02-20 14:58:37 UTC",
    "updated_date": "2025-02-20 14:58:37 UTC"
  },
  {
    "arxiv_id": "2502.14619v1",
    "title": "Reward Models Identify Consistency, Not Causality",
    "authors": [
      "Yuhui Xu",
      "Hanze Dong",
      "Lei Wang",
      "Caiming Xiong",
      "Junnan Li"
    ],
    "abstract": "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.14619v1",
    "published_date": "2025-02-20 14:57:14 UTC",
    "updated_date": "2025-02-20 14:57:14 UTC"
  },
  {
    "arxiv_id": "2502.15838v1",
    "title": "A novel approach to the relationships between data features -- based on comprehensive examination of mathematical, technological, and causal methodology",
    "authors": [
      "JaeHong Kim"
    ],
    "abstract": "The expansion of artificial intelligence (AI) has raised concerns about\ntransparency, accountability, and interpretability, with counterfactual\nreasoning emerging as a key approach to addressing these issues. However,\ncurrent mathematical, technological, and causal methodologies rely on\nexternalization techniques that normalize feature relationships within a single\ncoordinate space, often distorting intrinsic interactions. This study proposes\nthe Convergent Fusion Paradigm (CFP) theory, a framework integrating\nmathematical, technological, and causal perspectives to provide a more precise\nand comprehensive analysis of feature relationships. CFP theory introduces\nHilbert space and backward causation to reinterpret the feature relationships\nas emergent structures, offering a potential solution to the common cause\nproblem -- a fundamental challenge in causal modeling. From a mathematical --\ntechnical perspective, it utilizes a Riemannian manifold-based framework,\nthereby improving the structural representation of high- and low-dimensional\ndata interactions. From a causal inference perspective, CFP theory adopts\nabduction as a methodological foundation, employing Hilbert space for a dynamic\ncausal reasoning approach, where causal relationships are inferred abductively,\nand feature relationships evolve as emergent properties. Ultimately, CFP theory\nintroduces a novel AI modeling methodology that integrates Hilbert space,\nbackward causation, and Riemannian geometry, strengthening AI governance and\ntransparency in counterfactual reasoning.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T27 (Primary) 00A30, 03A05 (Secondary)",
      "I.2.3; F.4.1"
    ],
    "primary_category": "cs.AI",
    "comment": "59 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.15838v1",
    "published_date": "2025-02-20 14:36:37 UTC",
    "updated_date": "2025-02-20 14:36:37 UTC"
  },
  {
    "arxiv_id": "2502.14583v1",
    "title": "A Theory for Conditional Generative Modeling on Multiple Data Sources",
    "authors": [
      "Rongzhen Wang",
      "Yan Zhang",
      "Chenyu Zheng",
      "Chongxuan Li",
      "Guoqiang Wu"
    ],
    "abstract": "The success of large generative models has driven a paradigm shift,\nleveraging massive multi-source data to enhance model capabilities. However,\nthe interaction among these sources remains theoretically underexplored. This\npaper takes the first step toward a rigorous analysis of multi-source training\nin conditional generative modeling, where each condition represents a distinct\ndata source. Specifically, we establish a general distribution estimation error\nbound in average total variation distance for conditional maximum likelihood\nestimation based on the bracketing number. Our result shows that when source\ndistributions share certain similarities and the model is expressive enough,\nmulti-source training guarantees a sharper bound than single-source training.\nWe further instantiate the general theory on conditional Gaussian estimation\nand deep generative models including autoregressive and flexible energy-based\nmodels, by characterizing their bracketing numbers. The results highlight that\nthe number of sources and similarity among source distributions improve the\nadvantage of multi-source training. Simulations and real-world experiments\nvalidate our theory. Code is available at:\n\\url{https://github.com/ML-GSAI/Multi-Source-GM}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.14583v1",
    "published_date": "2025-02-20 14:13:24 UTC",
    "updated_date": "2025-02-20 14:13:24 UTC"
  },
  {
    "arxiv_id": "2502.14581v2",
    "title": "A Statistical Case Against Empirical Human-AI Alignment",
    "authors": [
      "Julian Rodemann",
      "Esteban Garces Arias",
      "Christoph Luther",
      "Christoph Jansen",
      "Thomas Augustin"
    ],
    "abstract": "Empirical human-AI alignment aims to make AI systems act in line with\nobserved human behavior. While noble in its goals, we argue that empirical\nalignment can inadvertently introduce statistical biases that warrant caution.\nThis position paper thus advocates against naive empirical alignment, offering\nprescriptive alignment and a posteriori empirical alignment as alternatives. We\nsubstantiate our principled argument by tangible examples like human-centric\ndecoding of language models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.OT"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 2 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14581v2",
    "published_date": "2025-02-20 14:12:18 UTC",
    "updated_date": "2025-05-12 09:51:39 UTC"
  },
  {
    "arxiv_id": "2502.14572v1",
    "title": "Factor Graph-based Interpretable Neural Networks",
    "authors": [
      "Yicong Li",
      "Kuanjiu Zhou",
      "Shuo Yu",
      "Qiang Zhang",
      "Renqiang Luo",
      "Xiaodong Li",
      "Feng Xia"
    ],
    "abstract": "Comprehensible neural network explanations are foundations for a better\nunderstanding of decisions, especially when the input data are infused with\nmalicious perturbations. Existing solutions generally mitigate the impact of\nperturbations through adversarial training, yet they fail to generate\ncomprehensible explanations under unknown perturbations. To address this\nchallenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network,\nwhich is capable of generating comprehensible explanations under unknown\nperturbations. Instead of retraining like previous solutions, the proposed\nAGAIN directly integrates logical rules by which logical errors in explanations\nare identified and rectified during inference. Specifically, we construct the\nfactor graph to express logical rules between explanations and categories. By\ntreating logical rules as exogenous knowledge, AGAIN can identify\nincomprehensible explanations that violate real-world logic. Furthermore, we\npropose an interactive intervention switch strategy rectifying explanations\nbased on the logical guidance from the factor graph without learning\nperturbations, which overcomes the inherent limitation of adversarial\ntraining-based methods in defending only against known perturbations.\nAdditionally, we theoretically demonstrate the effectiveness of employing\nfactor graph by proving that the comprehensibility of explanations is strongly\ncorrelated with factor graph. Extensive experiments are conducted on three\ndatasets and experimental results illustrate the superior performance of AGAIN\ncompared to state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The Thirteenth International Conference on Learning Representations",
    "pdf_url": "http://arxiv.org/pdf/2502.14572v1",
    "published_date": "2025-02-20 13:56:21 UTC",
    "updated_date": "2025-02-20 13:56:21 UTC"
  },
  {
    "arxiv_id": "2502.14563v1",
    "title": "Plan-over-Graph: Towards Parallelable LLM Agent Schedule",
    "authors": [
      "Shiqi Zhang",
      "Xinbei Ma",
      "Zouying Cao",
      "Zhuosheng Zhang",
      "Hai Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional abilities in\nreasoning for task planning. However, challenges remain under-explored for\nparallel schedules. This paper introduces a novel paradigm, plan-over-graph, in\nwhich the model first decomposes a real-life textual task into executable\nsubtasks and constructs an abstract task graph. The model then understands this\ntask graph as input and generates a plan for parallel execution. To enhance the\nplanning capability of complex, scalable graphs, we design an automated and\ncontrollable pipeline to generate synthetic graphs and propose a two-stage\ntraining scheme. Experimental results show that our plan-over-graph method\nsignificantly improves task performance on both API-based LLMs and trainable\nopen-sourced LLMs. By normalizing complex tasks as graphs, our method naturally\nsupports parallel execution, demonstrating global efficiency. The code and data\nare available at https://github.com/zsq259/Plan-over-Graph.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14563v1",
    "published_date": "2025-02-20 13:47:51 UTC",
    "updated_date": "2025-02-20 13:47:51 UTC"
  },
  {
    "arxiv_id": "2502.14560v2",
    "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
    "authors": [
      "Xun Deng",
      "Han Zhong",
      "Rui Ai",
      "Fuli Feng",
      "Zheng Wang",
      "Xiangnan He"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for\naligning large language models with human preferences. While prior work mainly\nextends DPO from the aspect of the objective function, we instead improve DPO\nfrom the largely overlooked but critical aspect of data selection.\nSpecifically, we address the issue of parameter shrinkage caused by noisy data\nby proposing a novel margin-maximization principle for dataset curation in DPO\ntraining. To accurately estimate margins for data selection, we propose a\ndual-margin guided approach that considers both external reward margins and\nimplicit DPO reward margins. Extensive experiments demonstrate that our method\nreduces computational cost dramatically while improving performance.\nRemarkably, by using just 10\\% of the Ultrafeedback dataset, our approach\nachieves 3\\% to 8\\% improvements across various Llama and Mistral series models\non the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends\nto iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data,\nwhile further reducing training time. These results highlight the potential of\ndata selection strategies for advancing preference optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14560v2",
    "published_date": "2025-02-20 13:45:17 UTC",
    "updated_date": "2025-02-22 12:15:25 UTC"
  },
  {
    "arxiv_id": "2502.14558v3",
    "title": "Model Inversion Attack against Federated Unlearning",
    "authors": [
      "Lei Zhou",
      "Youwen Zhu"
    ],
    "abstract": "With the introduction of regulations related to the ``right to be forgotten\",\nfederated learning (FL) is facing new privacy compliance challenges. To address\nthese challenges, researchers have proposed federated unlearning (FU). However,\nexisting FU research has primarily focused on improving the efficiency of\nunlearning, with less attention paid to the potential privacy vulnerabilities\ninherent in these methods. To address this gap, we draw inspiration from\ngradient inversion attacks in FL and propose the federated unlearning inversion\nattack (FUIA). The FUIA is specifically designed for the three types of FU\n(sample unlearning, client unlearning, and class unlearning), aiming to provide\na comprehensive analysis of the privacy leakage risks associated with FU. In\nFUIA, the server acts as an honest-but-curious attacker, recording and\nexploiting the model differences before and after unlearning to expose the\nfeatures and labels of forgotten data. FUIA significantly leaks the privacy of\nforgotten data and can target all types of FU. This attack contradicts the goal\nof FU to eliminate specific data influence, instead exploiting its\nvulnerabilities to recover forgotten data and expose its privacy flaws.\nExtensive experimental results show that FUIA can effectively reveal the\nprivate information of forgotten data. To mitigate this privacy leakage, we\nalso explore two potential defense methods, although these come at the cost of\nreduced unlearning effectiveness and the usability of the unlearned model.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14558v3",
    "published_date": "2025-02-20 13:38:36 UTC",
    "updated_date": "2025-04-08 15:54:55 UTC"
  },
  {
    "arxiv_id": "2502.14553v1",
    "title": "Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling",
    "authors": [
      "Eric Egli",
      "Matteo Manica",
      "Jannis Born"
    ],
    "abstract": "Bytes form the basis of the digital world and thus are a promising building\nblock for multimodal foundation models. Recently, Byte Language Models (BLMs)\nhave emerged to overcome tokenization, yet the excessive length of bytestreams\nrequires new architectural paradigms. Therefore, we present the Multiscale Byte\nLanguage Model (MBLM), a model-agnostic hierarchical decoder stack that allows\ntraining with context windows of $5$M bytes on single GPU in full model\nprecision. We thoroughly examine MBLM's performance with Transformer and Mamba\nblocks on both unimodal and multimodal tasks. Our experiments demonstrate that\nhybrid architectures are efficient in handling extremely long byte sequences\nduring training while achieving near-linear generational efficiency. To the\nbest of our knowledge, we present the first evaluation of BLMs on visual Q\\&A\ntasks and find that, despite serializing images and the absence of an encoder,\na MBLM with pure next token prediction can match custom CNN-LSTM architectures\nwith designated classification heads. We show that MBLMs exhibit strong\nadaptability in integrating diverse data representations, including pixel and\nimage filestream bytes, underlining their potential toward omnimodal foundation\nmodels. Source code is publicly available at:\nhttps://github.com/ai4sd/multiscale-byte-lm",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2502.14553v1",
    "published_date": "2025-02-20 13:31:50 UTC",
    "updated_date": "2025-02-20 13:31:50 UTC"
  },
  {
    "arxiv_id": "2502.15836v1",
    "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
    "authors": [
      "Haokun Chen",
      "Sebastian Szyller",
      "Weilin Xu",
      "Nageen Himayat"
    ],
    "abstract": "Large language models (LLMs) have become increasingly popular. Their emergent\ncapabilities can be attributed to their massive training datasets. However,\nthese datasets often contain undesirable or inappropriate content, e.g.,\nharmful texts, personal information, and copyrighted material. This has\npromoted research into machine unlearning that aims to remove information from\ntrained models. In particular, approximate unlearning seeks to achieve\ninformation removal by strategically editing the model rather than complete\nmodel retraining.\n  Recent work has shown that soft token attacks (STA) can successfully extract\npurportedly unlearned information from LLMs, thereby exposing limitations in\ncurrent unlearning methodologies. In this work, we reveal that STAs are an\ninadequate tool for auditing unlearning. Through systematic evaluation on\ncommon unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate\nthat such attacks can elicit any information from the LLM, regardless of (1)\nthe deployed unlearning algorithm, and (2) whether the queried content was\noriginally present in the training corpus. Furthermore, we show that STA with\njust a few soft tokens (1-10) can elicit random strings over 400-characters\nlong. Thus showing that STAs are too powerful, and misrepresent the\neffectiveness of the unlearning methods.\n  Our work highlights the need for better evaluation baselines, and more\nappropriate auditing tools for assessing the effectiveness of unlearning in\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15836v1",
    "published_date": "2025-02-20 13:22:33 UTC",
    "updated_date": "2025-02-20 13:22:33 UTC"
  },
  {
    "arxiv_id": "2502.14546v1",
    "title": "Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks",
    "authors": [
      "Maya Bechler-Speicher",
      "Ben Finkelshtein",
      "Fabrizio Frasca",
      "Luis Müller",
      "Jan Tönshoff",
      "Antoine Siraudin",
      "Viktor Zaverkin",
      "Michael M. Bronstein",
      "Mathias Niepert",
      "Bryan Perozzi",
      "Mikhail Galkin",
      "Christopher Morris"
    ],
    "abstract": "While machine learning on graphs has demonstrated promise in drug design and\nmolecular property prediction, significant benchmarking challenges hinder its\nfurther progress and relevance. Current benchmarking practices often lack focus\non transformative, real-world applications, favoring narrow domains like\ntwo-dimensional molecular graphs over broader, impactful areas such as\ncombinatorial optimization, relational databases, or chip design. Additionally,\nmany benchmark datasets poorly represent the underlying data, leading to\ninadequate abstractions and misaligned use cases. Fragmented evaluations and an\nexcessive focus on accuracy further exacerbate these issues, incentivizing\noverfitting rather than fostering generalizable insights. These limitations\nhave prevented the development of truly useful graph foundation models. This\nposition paper calls for a paradigm shift toward more meaningful benchmarks,\nrigorous evaluation protocols, and stronger collaboration with domain experts\nto drive impactful and reliable advances in graph learning research, unlocking\nthe potential of graph learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14546v1",
    "published_date": "2025-02-20 13:21:47 UTC",
    "updated_date": "2025-02-20 13:21:47 UTC"
  },
  {
    "arxiv_id": "2502.14529v1",
    "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
    "authors": [
      "Zhenhong Zhou",
      "Zherui Li",
      "Jie Zhang",
      "Yuanhe Zhang",
      "Kun Wang",
      "Yang Liu",
      "Qing Guo"
    ],
    "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated\nremarkable real-world capabilities, effectively collaborating to complete\ncomplex tasks. While these systems are designed with safety mechanisms, such as\nrejecting harmful instructions through alignment, their security remains\nlargely unexplored. This gap leaves LLM-MASs vulnerable to targeted\ndisruptions. In this paper, we introduce Contagious Recursive Blocking Attacks\n(Corba), a novel and simple yet highly effective attack that disrupts\ninteractions between agents within an LLM-MAS. Corba leverages two key\nproperties: its contagious nature allows it to propagate across arbitrary\nnetwork topologies, while its recursive property enables sustained depletion of\ncomputational resources. Notably, these blocking attacks often involve\nseemingly benign instructions, making them particularly challenging to mitigate\nusing conventional alignment methods. We evaluate Corba on two widely-used\nLLM-MASs, namely, AutoGen and Camel across various topologies and commercial\nmodels. Additionally, we conduct more extensive experiments in open-ended\ninteractive LLM-MASs, demonstrating the effectiveness of Corba in complex\ntopology structures and open-source models. Our code is available at:\nhttps://github.com/zhrli324/Corba.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14529v1",
    "published_date": "2025-02-20 13:02:00 UTC",
    "updated_date": "2025-02-20 13:02:00 UTC"
  },
  {
    "arxiv_id": "2502.14525v1",
    "title": "Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting",
    "authors": [
      "Yannick Wölker",
      "Arash Hajisafi",
      "Cyrus Shahabi",
      "Matthias Renz"
    ],
    "abstract": "We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for\nanalyzing traffic data, demonstrating its efficacy in two critical tasks:\nforecasting and reconstruction. Unlike typical GNN methods that treat each\ntraffic sensor as an individual graph node, DeepStateGNN clusters sensors into\nhigher-level graph nodes, dubbed Deep State Nodes, based on various similarity\ncriteria, resulting in a fixed number of nodes in a Deep State graph. The term\n\"Deep State\" nodes is a play on words, referencing hidden networks of power\nthat, like these nodes, secretly govern traffic independently of visible\nsensors. These Deep State Nodes are defined by several similarity factors,\nincluding spatial proximity (e.g., sensors located nearby in the road network),\nfunctional similarity (e.g., sensors on similar types of freeways), and\nbehavioral similarity under specific conditions (e.g., traffic behavior during\nrain). This clustering approach allows for dynamic and adaptive node grouping,\nas sensors can belong to multiple clusters and clusters may evolve over time.\nOur experimental results show that DeepStateGNN offers superior scalability and\nfaster training, while also delivering more accurate results than competitors.\nIt effectively handles large-scale sensor networks, outperforming other methods\nin both traffic forecasting and reconstruction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Yannick W\\\"olker and Arash Hajisafi contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2502.14525v1",
    "published_date": "2025-02-20 13:00:31 UTC",
    "updated_date": "2025-02-20 13:00:31 UTC"
  },
  {
    "arxiv_id": "2502.15835v2",
    "title": "Pragmatic Reasoning improves LLM Code Generation",
    "authors": [
      "Zhuchen Cao",
      "Sven Apel",
      "Adish Singla",
      "Vera Demberg"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive potential in\ntranslating natural language (NL) instructions into program code. However, user\ninstructions often contain inherent ambiguities, making it challenging for LLMs\nto generate code that accurately reflects the user's true intent. To address\nthis challenge, researchers have proposed to produce multiple candidates of the\nprogram code and then rerank them to identify the best solution. In this paper,\nwe propose CodeRSA, a novel code candidate reranking mechanism built upon the\nRational Speech Act (RSA) framework, designed to guide LLMs toward more\ncomprehensive pragmatic reasoning about user intent. We evaluate CodeRSA using\none of the latest LLMs on a popular code generation dataset. Our experiment\nresults show that CodeRSA consistently outperforms common baselines, surpasses\nthe state-of-the-art approach in most cases, and demonstrates robust overall\nperformance. These findings underscore the effectiveness of integrating\npragmatic reasoning into code candidate reranking, offering a promising\ndirection for enhancing code generation quality in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15835v2",
    "published_date": "2025-02-20 12:44:26 UTC",
    "updated_date": "2025-02-28 13:40:42 UTC"
  },
  {
    "arxiv_id": "2502.14504v1",
    "title": "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models",
    "authors": [
      "Yu Meng",
      "Kaiyuan Li",
      "Chenran Huang",
      "Chen Gao",
      "Xinlei Chen",
      "Yong Li",
      "Xiaoping Zhang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities across a range of multimodal tasks. However, their inference\nefficiency is constrained by the large number of visual tokens processed during\ndecoding. To address this challenge, we propose Per-Layer Per-Head Vision Token\nPruning (PLPHP), a two-level fine-grained pruning method including Layer-Level\nRetention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the\nVision Token Re-attention phenomenon across decoder layers, we dynamically\nadjust token retention rates layer by layer. Layers that exhibit stronger\nattention to visual information preserve more vision tokens, while layers with\nlower vision attention are aggressively pruned. Furthermore, PLPHP applies\npruning at the attention head level, enabling different heads within the same\nlayer to independently retain critical context. Experiments on multiple\nbenchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and\nreduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of\n0.46% average performance drop, while also achieving notable performance\nimprovements in multi-image tasks. These results highlight the effectiveness of\nfine-grained token pruning and contribute to advancing the efficiency and\nscalability of LVLMs. Our source code will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14504v1",
    "published_date": "2025-02-20 12:31:31 UTC",
    "updated_date": "2025-02-20 12:31:31 UTC"
  },
  {
    "arxiv_id": "2502.14499v1",
    "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "authors": [
      "Deepak Nathani",
      "Lovish Madaan",
      "Nicholas Roberts",
      "Nikolay Bashlykov",
      "Ajay Menon",
      "Vincent Moens",
      "Amar Budhiraja",
      "Despoina Magka",
      "Vladislav Vorotilov",
      "Gaurav Chaurasia",
      "Dieuwke Hupkes",
      "Ricardo Silveira Cabral",
      "Tatiana Shavrina",
      "Jakob Foerster",
      "Yoram Bachrach",
      "William Yang Wang",
      "Roberta Raileanu"
    ],
    "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for\nevaluating and developing LLM agents on AI research tasks. This is the first\nGym environment for machine learning (ML) tasks, enabling research on\nreinforcement learning (RL) algorithms for training such agents. MLGym-bench\nconsists of 13 diverse and open-ended AI research tasks from diverse domains\nsuch as computer vision, natural language processing, reinforcement learning,\nand game theory. Solving these tasks requires real-world AI research skills\nsuch as generating new ideas and hypotheses, creating and processing data,\nimplementing ML methods, training models, running experiments, analyzing the\nresults, and iterating through this process to improve on a given task. We\nevaluate a number of frontier large language models (LLMs) on our benchmarks\nsuch as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5\nPro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate\nmodels or agents, generate synthetic data at scale, as well as develop new\nlearning algorithms for training agents on AI research tasks. We find that\ncurrent frontier models can improve on the given baselines, usually by finding\nbetter hyperparameters, but do not generate novel hypotheses, algorithms,\narchitectures, or substantial improvements. We open-source our framework and\nbenchmark to facilitate future research in advancing the AI research\ncapabilities of LLM agents.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 12 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14499v1",
    "published_date": "2025-02-20 12:28:23 UTC",
    "updated_date": "2025-02-20 12:28:23 UTC"
  },
  {
    "arxiv_id": "2502.14491v2",
    "title": "Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk",
    "authors": [
      "Elija Perrier"
    ],
    "abstract": "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.14491v2",
    "published_date": "2025-02-20 12:14:54 UTC",
    "updated_date": "2025-03-07 19:55:05 UTC"
  },
  {
    "arxiv_id": "2502.14487v2",
    "title": "Temporal Misalignment in ANN-SNN Conversion and Its Mitigation via Probabilistic Spiking Neurons",
    "authors": [
      "Velibor Bojković",
      "Xiaofeng Wu",
      "Bin Gu"
    ],
    "abstract": "Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to\nArtificial Neural Networks (ANNs) by mimicking biological neural principles,\nestablishing them as a promising approach to mitigate the increasing energy\ndemands of large-scale neural models. However, fully harnessing the\ncapabilities of SNNs remains challenging due to their discrete signal\nprocessing and temporal dynamics. ANN-SNN conversion has emerged as a practical\napproach, enabling SNNs to achieve competitive performance on complex machine\nlearning tasks. In this work, we identify a phenomenon in the ANN-SNN\nconversion framework, termed temporal misalignment, in which random spike\nrearrangement across SNN layers leads to performance improvements. Based on\nthis observation, we introduce biologically plausible two-phase probabilistic\n(TPP) spiking neurons, further enhancing the conversion process. We demonstrate\nthe advantages of our proposed method both theoretically and empirically\nthrough comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet\nacross a variety of architectures, achieving state-of-the-art results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14487v2",
    "published_date": "2025-02-20 12:09:30 UTC",
    "updated_date": "2025-02-21 09:05:35 UTC"
  },
  {
    "arxiv_id": "2502.14486v1",
    "title": "How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation",
    "authors": [
      "Zhuohang Long",
      "Siyuan Wang",
      "Shujun Liu",
      "Yuhang Lai",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "Jailbreak attacks, where harmful prompts bypass generative models' built-in\nsafety, raise serious concerns about model vulnerability. While many defense\nmethods have been proposed, the trade-offs between safety and helpfulness, and\ntheir application to Large Vision-Language Models (LVLMs), are not well\nunderstood. This paper systematically examines jailbreak defenses by reframing\nthe standard generation task as a binary classification problem to assess model\nrefusal tendencies for both harmful and benign queries. We identify two key\ndefense mechanisms: safety shift, which increases refusal rates across all\nqueries, and harmfulness discrimination, which improves the model's ability to\ndistinguish between harmful and benign inputs. Using these mechanisms, we\ndevelop two ensemble defense strategies-inter-mechanism ensembles and\nintra-mechanism ensembles-to balance safety and helpfulness. Experiments on the\nMM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these\nstrategies effectively improve model safety or optimize the trade-off between\nsafety and helpfulness.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14486v1",
    "published_date": "2025-02-20 12:07:40 UTC",
    "updated_date": "2025-02-20 12:07:40 UTC"
  },
  {
    "arxiv_id": "2502.14469v1",
    "title": "Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models",
    "authors": [
      "Aurora Polo-Rodríguez",
      "Laura Fiorini",
      "Erika Rovini",
      "Filippo Cavallo",
      "Javier Medina-Quero"
    ],
    "abstract": "This work presents a novel architecture for context-aware interactions within\nsmart environments, leveraging Large Language Models (LLMs) to enhance user\nexperiences. Our system integrates user location data obtained through UWB tags\nand sensor-equipped smart homes with real-time human activity recognition (HAR)\nto provide a comprehensive understanding of user context. This contextual\ninformation is then fed to an LLM-powered chatbot, enabling it to generate\npersonalised interactions and recommendations based on the user's current\nactivity and environment. This approach moves beyond traditional static chatbot\ninteractions by dynamically adapting to the user's real-time situation. A case\nstudy conducted from a real-world dataset demonstrates the feasibility and\neffectiveness of our proposed architecture, showcasing its potential to create\nmore intuitive and helpful interactions within smart homes. The results\nhighlight the significant benefits of integrating LLM with real-time activity\nand location data to deliver personalised and contextually relevant user\nexperiences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14469v1",
    "published_date": "2025-02-20 11:46:51 UTC",
    "updated_date": "2025-02-20 11:46:51 UTC"
  },
  {
    "arxiv_id": "2502.14462v1",
    "title": "Single-image Reflectance and Transmittance Estimation from Any Flatbed Scanner",
    "authors": [
      "Carlos Rodriguez-Pardo",
      "David Pascual-Hernandez",
      "Javier Rodriguez-Vazquez",
      "Jorge Lopez-Moreno",
      "Elena Garces"
    ],
    "abstract": "Flatbed scanners have emerged as promising devices for high-resolution,\nsingle-image material capture. However, existing approaches assume very\nspecific conditions, such as uniform diffuse illumination, which are only\navailable in certain high-end devices, hindering their scalability and cost. In\ncontrast, in this work, we introduce a method inspired by intrinsic image\ndecomposition, which accurately removes both shading and specularity,\neffectively allowing captures with any flatbed scanner. Further, we extend\nprevious work on single-image material reflectance capture with the estimation\nof opacity and transmittance, critical components of full material appearance\n(SVBSDF), improving the results for any material captured with a flatbed\nscanner, at a very high resolution and accuracy",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "68T07 (Primary) 68T45, 68U10, 68U05 (Secondary)",
      "I.4.0; I.2.6; I.3.0"
    ],
    "primary_category": "cs.GR",
    "comment": "Accepted to Computers & Graphics",
    "pdf_url": "http://arxiv.org/pdf/2502.14462v1",
    "published_date": "2025-02-20 11:33:17 UTC",
    "updated_date": "2025-02-20 11:33:17 UTC"
  },
  {
    "arxiv_id": "2502.14458v2",
    "title": "Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing",
    "authors": [
      "Aviv Bick",
      "Tobias Katsch",
      "Nimit Sohoni",
      "Arjun Desai",
      "Albert Gu"
    ],
    "abstract": "We introduce Llamba, a family of efficient recurrent language models\ndistilled from Llama-3.x into the Mamba architecture. The series includes\nLlamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput\nand handle significantly larger batch sizes than Transformer-based models while\nmaintaining comparable benchmark performance. Furthermore, Llamba demonstrates\nthe effectiveness of cross-architecture distillation using MOHAWK (Bick et al.,\n2024), achieving these results with less than 0.1% of the training data\ntypically used for models of similar size. To take full advantage of their\nefficiency, we provide an optimized implementation of Llamba for\nresource-constrained devices such as smartphones and edge platforms, offering a\npractical and memory-efficient alternative to Transformers. Overall, Llamba\nimproves the tradeoff between speed, memory efficiency, and performance, making\nhigh-quality language models more accessible.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14458v2",
    "published_date": "2025-02-20 11:18:39 UTC",
    "updated_date": "2025-02-23 13:02:09 UTC"
  },
  {
    "arxiv_id": "2502.14457v1",
    "title": "Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control",
    "authors": [
      "Tan-Dzung Do",
      "Nandiraju Gireesh",
      "Jilong Wang",
      "He Wang"
    ],
    "abstract": "Articulated object manipulation poses a unique challenge compared to rigid\nobject manipulation as the object itself represents a dynamic environment. In\nthis work, we present a novel RL-based pipeline equipped with variable\nimpedance control and motion adaptation leveraging observation history for\ngeneralizable articulated object manipulation, focusing on smooth and dexterous\nmotion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap,\nour pipeline diminishes reliance on vision by not leveraging the vision data\nfeature (RGBD/pointcloud) directly as policy input but rather extracting useful\nlow-dimensional data first via off-the-shelf modules. Additionally, we\nexperience less sim-to-real gap by inferring object motion and its intrinsic\nproperties via observation history as well as utilizing impedance control both\nin the simulation and in the real world. Furthermore, we develop a\nwell-designed training setting with great randomization and a specialized\nreward system (task-aware and motion-aware) that enables multi-staged,\nend-to-end manipulation without heuristic motion planning. To the best of our\nknowledge, our policy is the first to report 84\\% success rate in the real\nworld via extensive experiments with various unseen objects.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14457v1",
    "published_date": "2025-02-20 11:18:35 UTC",
    "updated_date": "2025-02-20 11:18:35 UTC"
  },
  {
    "arxiv_id": "2502.14456v2",
    "title": "Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization",
    "authors": [
      "Ran Ding",
      "Ziyu Zhang",
      "Ying Zhu",
      "Ziqian Kong",
      "Peilan Xu"
    ],
    "abstract": "To enhance tourists' experiences and immersion, this paper proposes a\nnarrative-driven travel planning framework called NarrativeGuide, which\ngenerates a geoculturally-grounded narrative script for travelers, offering a\nnovel, role-playing experience for their journey. In the initial stage,\nNarrativeGuide constructs a knowledge graph for attractions within a city, then\nconfigures the worldview, character setting, and exposition based on the\nknowledge graph. Using this foundation, the knowledge graph is combined to\ngenerate an independent scene unit for each attraction. During the itinerary\nplanning stage, NarrativeGuide models narrative-driven travel planning as an\noptimization problem, utilizing a genetic algorithm (GA) to refine the\nitinerary. Before evaluating the candidate itinerary, transition scripts are\ngenerated for each pair of adjacent attractions, which, along with the scene\nunits, form a complete script. The weighted sum of script coherence, travel\ntime, and attraction scores is then used as the fitness value to update the\ncandidate solution set. Experimental results across four cities, i.e., Nanjing\nand Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate\nsignificant improvements in narrative coherence and cultural fit, alongside a\nnotable reduction in travel time and an increase in the quality of visited\nattractions. Our study highlights that incorporating external evolutionary\noptimization effectively addresses the limitations of large language models in\ntravel planning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14456v2",
    "published_date": "2025-02-20 11:15:23 UTC",
    "updated_date": "2025-02-26 07:36:10 UTC"
  },
  {
    "arxiv_id": "2502.14455v1",
    "title": "An Efficient Ground-aerial Transportation System for Pest Control Enabled by AI-based Autonomous Nano-UAVs",
    "authors": [
      "Luca Crupi",
      "Luca Butera",
      "Alberto Ferrante",
      "Alessandro Giusti",
      "Daniele Palossi"
    ],
    "abstract": "Efficient crop production requires early detection of pest outbreaks and\ntimely treatments; we consider a solution based on a fleet of multiple\nautonomous miniaturized unmanned aerial vehicles (nano-UAVs) to visually detect\npests and a single slower heavy vehicle that visits the detected outbreaks to\ndeliver treatments. To cope with the extreme limitations aboard nano-UAVs,\ne.g., low-resolution sensors and sub-100 mW computational power budget, we\ndesign, fine-tune, and optimize a tiny image-based convolutional neural network\n(CNN) for pest detection. Despite the small size of our CNN (i.e., 0.58\nGOps/inference), on our dataset, it scores a mean average precision (mAP) of\n0.79 in detecting harmful bugs, i.e., 14% lower mAP but 32x fewer operations\nthan the best-performing CNN in the literature. Our CNN runs in real-time at\n6.8 frame/s, requiring 33 mW on a GWT GAP9 System-on-Chip aboard a Crazyflie\nnano-UAV. Then, to cope with in-field unexpected obstacles, we leverage a\nglobal+local path planner based on the A* algorithm. The global path planner\ndetermines the best route for the nano-UAV to sweep the entire area, while the\nlocal one runs up to 50 Hz aboard our nano-UAV and prevents collision by\nadjusting the short-distance path. Finally, we demonstrate with in-simulator\nexperiments that once a 25 nano-UAVs fleet has combed a 200x200 m vineyard,\ncollected information can be used to plan the best path for the tractor,\nvisiting all and only required hotspots. In this scenario, our efficient\ntransportation system, compared to a traditional single-ground vehicle\nperforming both inspection and treatment, can save up to 20 h working time.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14455v1",
    "published_date": "2025-02-20 11:14:55 UTC",
    "updated_date": "2025-02-20 11:14:55 UTC"
  },
  {
    "arxiv_id": "2502.15833v1",
    "title": "Advancing Out-of-Distribution Detection via Local Neuroplasticity",
    "authors": [
      "Alessandro Canevaro",
      "Julian Schmidt",
      "Mohammad Sajad Marvi",
      "Hang Yu",
      "Georg Martius",
      "Julian Jordan"
    ],
    "abstract": "In the domain of machine learning, the assumption that training and test data\nshare the same distribution is often violated in real-world scenarios,\nrequiring effective out-of-distribution (OOD) detection. This paper presents a\nnovel OOD detection method that leverages the unique local neuroplasticity\nproperty of Kolmogorov-Arnold Networks (KANs). Unlike traditional multilayer\nperceptrons, KANs exhibit local plasticity, allowing them to preserve learned\ninformation while adapting to new tasks. Our method compares the activation\npatterns of a trained KAN against its untrained counterpart to detect OOD\nsamples. We validate our approach on benchmarks from image and medical domains,\ndemonstrating superior performance and robustness compared to state-of-the-art\ntechniques. These results underscore the potential of KANs in enhancing the\nreliability of machine learning systems in diverse environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR25",
    "pdf_url": "http://arxiv.org/pdf/2502.15833v1",
    "published_date": "2025-02-20 11:13:41 UTC",
    "updated_date": "2025-02-20 11:13:41 UTC"
  },
  {
    "arxiv_id": "2502.14445v1",
    "title": "PredictaBoard: Benchmarking LLM Score Predictability",
    "authors": [
      "Lorenzo Pacchiardi",
      "Konstantinos Voudouris",
      "Ben Slater",
      "Fernando Martínez-Plumed",
      "José Hernández-Orallo",
      "Lexin Zhou",
      "Wout Schellaert"
    ],
    "abstract": "Despite possessing impressive skills, Large Language Models (LLMs) often fail\nunpredictably, demonstrating inconsistent success in even basic common sense\nreasoning tasks. This unpredictability poses a significant challenge to\nensuring their safe deployment, as identifying and operating within a reliable\n\"safe zone\" is essential for mitigating risks. To address this, we present\nPredictaBoard, a novel collaborative benchmarking framework designed to\nevaluate the ability of score predictors (referred to as assessors) to\nanticipate LLM errors on specific task instances (i.e., prompts) from existing\ndatasets. PredictaBoard evaluates pairs of LLMs and assessors by considering\nthe rejection rate at different tolerance errors. As such, PredictaBoard\nstimulates research into developing better assessors and making LLMs more\npredictable, not only with a higher average performance. We conduct\nillustrative experiments using baseline assessors and state-of-the-art LLMs.\nPredictaBoard highlights the critical need to evaluate predictability alongside\nperformance, paving the way for safer AI systems where errors are not only\nminimised but also anticipated and effectively mitigated. Code for our\nbenchmark can be found at\nhttps://github.com/Kinds-of-Intelligence-CFI/PredictaBoard",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14445v1",
    "published_date": "2025-02-20 10:52:38 UTC",
    "updated_date": "2025-02-20 10:52:38 UTC"
  },
  {
    "arxiv_id": "2502.14442v1",
    "title": "Stochastic Resonance Improves the Detection of Low Contrast Images in Deep Learning Models",
    "authors": [
      "Siegfried Ludwig"
    ],
    "abstract": "Stochastic resonance describes the utility of noise in improving the\ndetectability of weak signals in certain types of systems. It has been observed\nwidely in natural and engineered settings, but its utility in image\nclassification with rate-based neural networks has not been studied\nextensively. In this analysis a simple LSTM recurrent neural network is trained\nfor digit recognition and classification. During the test phase, image contrast\nis reduced to a point where the model fails to recognize the presence of a\nstimulus. Controlled noise is added to partially recover classification\nperformance. The results indicate the presence of stochastic resonance in\nrate-based recurrent neural networks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MSc Course Project",
    "pdf_url": "http://arxiv.org/pdf/2502.14442v1",
    "published_date": "2025-02-20 10:48:49 UTC",
    "updated_date": "2025-02-20 10:48:49 UTC"
  },
  {
    "arxiv_id": "2502.14424v1",
    "title": "Distribution Matching for Self-Supervised Transfer Learning",
    "authors": [
      "Yuling Jiao",
      "Wensen Ma",
      "Defeng Sun",
      "Hansheng Wang",
      "Yang Wang"
    ],
    "abstract": "In this paper, we propose a novel self-supervised transfer learning method\ncalled Distribution Matching (DM), which drives the representation distribution\ntoward a predefined reference distribution while preserving augmentation\ninvariance. The design of DM results in a learned representation space that is\nintuitively structured and offers easily interpretable hyperparameters.\nExperimental results across multiple real-world datasets and evaluation metrics\ndemonstrate that DM performs competitively on target classification tasks\ncompared to existing self-supervised transfer learning methods. Additionally,\nwe provide robust theoretical guarantees for DM, including a population theorem\nand an end-to-end sample theorem. The population theorem bridges the gap\nbetween the self-supervised learning task and target classification accuracy,\nwhile the sample theorem shows that, even with a limited number of samples from\nthe target domain, DM can deliver exceptional classification performance,\nprovided the unlabeled sample size is sufficiently large.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14424v1",
    "published_date": "2025-02-20 10:20:56 UTC",
    "updated_date": "2025-02-20 10:20:56 UTC"
  },
  {
    "arxiv_id": "2502.14416v1",
    "title": "Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving",
    "authors": [
      "Jon Gutiérrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe"
    ],
    "abstract": "Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can\nstrengthen the accuracy of intelligent vision systems by combining spectral and\nspatial information, which is useful for tasks like semantic segmentation in\nautonomous driving. To advance research in such safety-critical systems,\ndetermining the precise contribution of spectral information to complex DNNs'\noutput is needed. To address this, several saliency methods, such as class\nactivation maps (CAM), have been proposed primarily for image classification.\nHowever, recent studies have raised concerns regarding their reliability. In\nthis paper, we address their limitations and propose an alternative approach by\nleveraging the data provided by activations and weights from relevant DNN\nlayers to better capture the relationship between input features and\npredictions. The study aims to assess the superior performance of HSI compared\nto 3-channel and single-channel DNNs. We also address the influence of spectral\nsignature normalization for enhancing DNN robustness in real-world driving\nconditions.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14416v1",
    "published_date": "2025-02-20 10:11:27 UTC",
    "updated_date": "2025-02-20 10:11:27 UTC"
  },
  {
    "arxiv_id": "2502.14400v1",
    "title": "HPS: Hard Preference Sampling for Human Preference Alignment",
    "authors": [
      "Xiandong Zou",
      "Wanyu Lin",
      "Yuchen Li",
      "Pan Zhou"
    ],
    "abstract": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses--those closely resembling preferred ones--to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14400v1",
    "published_date": "2025-02-20 09:37:41 UTC",
    "updated_date": "2025-02-20 09:37:41 UTC"
  },
  {
    "arxiv_id": "2504.05314v1",
    "title": "Multimodal Quantitative Language for Generative Recommendation",
    "authors": [
      "Jianyang Zhai",
      "Zi-Feng Mai",
      "Chang-Dong Wang",
      "Feidiao Yang",
      "Xiawu Zheng",
      "Hui Li",
      "Yonghong Tian"
    ],
    "abstract": "Generative recommendation has emerged as a promising paradigm aiming at\ndirectly generating the identifiers of the target candidates. Most existing\nmethods attempt to leverage prior knowledge embedded in Pre-trained Language\nModels (PLMs) to improve the recommendation performance. However, they often\nfail to accommodate the differences between the general linguistic knowledge of\nPLMs and the specific needs of recommendation systems. Moreover, they rarely\nconsider the complementary knowledge between the multimodal information of\nitems, which represents the multi-faceted preferences of users. To facilitate\nefficient recommendation knowledge transfer, we propose a novel approach called\nMultimodal Quantitative Language for Generative Recommendation (MQL4GRec). Our\nkey idea is to transform items from different domains and modalities into a\nunified language, which can serve as a bridge for transferring recommendation\nknowledge. Specifically, we first introduce quantitative translators to convert\nthe text and image content of items from various domains into a new and concise\nlanguage, known as quantitative language, with all items sharing the same\nvocabulary. Then, we design a series of quantitative language generation tasks\nto enrich quantitative language with semantic information and prior knowledge.\nFinally, we achieve the transfer of recommendation knowledge from different\ndomains and modalities to the recommendation task through pre-training and\nfine-tuning. We evaluate the effectiveness of MQL4GRec through extensive\nexperiments and comparisons with existing methods, achieving improvements over\nthe baseline by 11.18\\%, 14.82\\%, and 7.95\\% on the NDCG metric across three\ndifferent datasets, respectively.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.05314v1",
    "published_date": "2025-02-20 09:29:30 UTC",
    "updated_date": "2025-02-20 09:29:30 UTC"
  },
  {
    "arxiv_id": "2502.14382v1",
    "title": "S*: Test Time Scaling for Code Generation",
    "authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Chengkun Cao",
      "Xiuyu Li",
      "Shangyin Tan",
      "Kurt Keutzer",
      "Jiarong Xing",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "abstract": "Increasing test-time compute for LLMs shows promise across domains but\nremains underexplored in code generation, despite extensive study in math. In\nthis paper, we propose S*, the first hybrid test-time scaling framework that\nsubstantially improves the coverage and selection accuracy of generated code.\nS* extends the existing parallel scaling paradigm with sequential scaling to\npush performance boundaries. It further leverages a novel selection mechanism\nthat adaptively generates distinguishing inputs for pairwise comparison,\ncombined with execution-grounded information to robustly identify correct\nsolutions. We evaluate across 12 Large Language Models and Large Reasoning\nModel and show: (1) S* consistently improves performance across model families\nand sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables\nnon-reasoning models to surpass reasoning models - GPT-4o-mini with S*\noutperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts\nstate-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S*\nachieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be\navailable under https://github.com/NovaSky-AI/SkyThought.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14382v1",
    "published_date": "2025-02-20 09:18:53 UTC",
    "updated_date": "2025-02-20 09:18:53 UTC"
  },
  {
    "arxiv_id": "2502.14380v1",
    "title": "Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations",
    "authors": [
      "Mariko Kato",
      "Hakaze Cho",
      "Yoshihiro Sakai",
      "Naoya Inoue"
    ],
    "abstract": "The performance of In-Context Learning (ICL) is highly sensitive to the\nselected demonstrations. Existing approaches to demonstration selection\noptimize different objectives, yielding inconsistent results. To address this,\nwe propose a unified metric--affinity and diversity--that leverages ICL model's\ninternal representations. Our experiments show that both affinity and diversity\nstrongly correlate with test accuracies, indicating their effectiveness for\ndemonstration selection. Moreover, we show that our proposed metrics align well\nwith various previous works to unify the inconsistency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14380v1",
    "published_date": "2025-02-20 09:12:51 UTC",
    "updated_date": "2025-02-20 09:12:51 UTC"
  },
  {
    "arxiv_id": "2502.14372v1",
    "title": "Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning",
    "authors": [
      "Austin Yubo He",
      "Zi-Wen Liu"
    ],
    "abstract": "The realization of scalable fault-tolerant quantum computing is expected to\nhinge on quantum error-correcting codes. In the quest for more efficient\nquantum fault tolerance, a critical code parameter is the weight of\nmeasurements that extract information about errors to enable error correction:\nas higher measurement weights require higher implementation costs and introduce\nmore errors, it is important in code design to optimize measurement weight.\nThis underlies the surging interest in quantum low-density parity-check (qLDPC)\ncodes, the study of which has primarily focused on the asymptotic\n(large-code-limit) properties. In this work, we introduce a versatile and\ncomputationally efficient approach to stabilizer code weight reduction based on\nreinforcement learning (RL), which produces new low-weight codes that\nsubstantially outperform the state of the art in practically relevant parameter\nregimes, extending significantly beyond previously accessible small distances.\nFor example, our approach demonstrates savings in physical qubit overhead\ncompared to existing results by 1 to 2 orders of magnitude for weight 6 codes\nand brings the overhead into a feasible range for near-future experiments. We\nalso investigate the interplay between code parameters using our RL framework,\noffering new insights into the potential efficiency and power of practically\nviable coding strategies. Overall, our results demonstrate how RL can\neffectively advance the crucial yet challenging problem of quantum code\ndiscovery and thereby facilitate a faster path to the practical implementation\nof fault-tolerant quantum technologies.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "quant-ph",
    "comment": "18 pages, 14 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.14372v1",
    "published_date": "2025-02-20 09:05:34 UTC",
    "updated_date": "2025-02-20 09:05:34 UTC"
  },
  {
    "arxiv_id": "2502.14366v1",
    "title": "Entropy-UID: A Method for Optimizing Information Density",
    "authors": [
      "Xinpeng Shou"
    ],
    "abstract": "Balanced and efficient information flow is essential for optimizing language\ngeneration models. In this work, we propose Entropy-UID, a new token selection\nmethod that balances entropy and Uniform Information Density (UID) principles\nfor enhanced efficiency of text generation. Our approach adaptively adjusts\ntoken selection by jointly minimizing entropy and surprisal, promoting more\neven information distribution across generated sequences. Theoretical\nvalidation demonstrates that Entropy-UID optimally reduces information spikes\nwhile maintaining fluency and coherence. The method has been evulated using\ninformation-theoretic metrics on multiple benchmark datasets, including\nWikiText-2, OpenWebText, and WMT. Experimental results show that Entropy-UID\nachieves lower surprisal and entropy variance compared to standard GPT-2 and\nalternative heuristics, leading to more balanced and human-like text\ngeneration. Our findings point towards the potential of leveraging\ninformation-theoretic constraints to refine token selection strategies in\nautoregressive language models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5pages, 1 figures, submitting to ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14366v1",
    "published_date": "2025-02-20 08:42:47 UTC",
    "updated_date": "2025-02-20 08:42:47 UTC"
  },
  {
    "arxiv_id": "2502.14365v2",
    "title": "Is Q-learning an Ill-posed Problem?",
    "authors": [
      "Philipp Wissmann",
      "Daniel Hein",
      "Steffen Udluft",
      "Thomas Runkler"
    ],
    "abstract": "This paper investigates the instability of Q-learning in continuous\nenvironments, a challenge frequently encountered by practitioners.\nTraditionally, this instability is attributed to bootstrapping and regression\nmodel errors. Using a representative reinforcement learning benchmark, we\nsystematically examine the effects of bootstrapping and model inaccuracies by\nincrementally eliminating these potential error sources. Our findings reveal\nthat even in relatively simple benchmarks, the fundamental task of Q-learning -\niteratively learning a Q-function from policy-specific target values - can be\ninherently ill-posed and prone to failure. These insights cast doubt on the\nreliability of Q-learning as a universal solution for reinforcement learning\nproblems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ESANN 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14365v2",
    "published_date": "2025-02-20 08:42:30 UTC",
    "updated_date": "2025-02-21 14:11:33 UTC"
  },
  {
    "arxiv_id": "2502.14361v1",
    "title": "Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning",
    "authors": [
      "Jiachen Zhu",
      "Congmin Zheng",
      "Jianghao Lin",
      "Kounianhua Du",
      "Ying Wen",
      "Yong Yu",
      "Jun Wang",
      "Weinan Zhang"
    ],
    "abstract": "While large language models (LLMs) have significantly advanced mathematical\nreasoning, Process Reward Models (PRMs) have been developed to evaluate the\nlogical validity of reasoning steps. However, PRMs still struggle with\nout-of-distribution (OOD) challenges. This paper identifies key OOD issues,\nincluding step OOD, caused by differences in reasoning patterns across model\ntypes and sizes, and question OOD, which arises from dataset shifts between\ntraining data and real-world problems. To address these issues, we introduce\nRetrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework\ndesigned to tackle these OOD issues. By utilizing a two-stage\nretrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar\nquestions and steps as a warmup, enhancing PRM's ability to evaluate target\nsteps and improving generalization and reasoning consistency across different\nmodels and problem types. Our extensive experiments demonstrate that\nRetrievalPRM outperforms existing baselines across multiple real-world\ndatasets. Our open-source contributions include a retrieval-enhanced dataset, a\ntuning framework for PRM training, and the RetrievalPRM model, establishing a\nnew standard for PRM performance.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14361v1",
    "published_date": "2025-02-20 08:40:09 UTC",
    "updated_date": "2025-02-20 08:40:09 UTC"
  },
  {
    "arxiv_id": "2502.14345v1",
    "title": "FlowAgent: Achieving Compliance and Flexibility for Workflow Agents",
    "authors": [
      "Yuchen Shi",
      "Siqi Cai",
      "Zihan Xu",
      "Yuei Qin",
      "Gang Li",
      "Hang Shao",
      "Jiawei Chen",
      "Deqing Yang",
      "Ke Li",
      "Xing Sun"
    ],
    "abstract": "The integration of workflows with large language models (LLMs) enables\nLLM-based agents to execute predefined procedures, enhancing automation in\nreal-world applications. Traditional rule-based methods tend to limit the\ninherent flexibility of LLMs, as their predefined execution paths restrict the\nmodels' action space, particularly when the unexpected, out-of-workflow (OOW)\nqueries are encountered. Conversely, prompt-based methods allow LLMs to fully\ncontrol the flow, which can lead to diminished enforcement of procedural\ncompliance. To address these challenges, we introduce FlowAgent, a novel agent\nframework designed to maintain both compliance and flexibility. We propose the\nProcedure Description Language (PDL), which combines the adaptability of\nnatural language with the precision of code to formulate workflows. Building on\nPDL, we develop a comprehensive framework that empowers LLMs to manage OOW\nqueries effectively, while keeping the execution path under the supervision of\na set of controllers. Additionally, we present a new evaluation methodology to\nrigorously assess an LLM agent's ability to handle OOW scenarios, going beyond\nroutine flow compliance tested in existing benchmarks. Experiments on three\ndatasets demonstrate that FlowAgent not only adheres to workflows but also\neffectively manages OOW queries, highlighting its dual strengths in compliance\nand flexibility. The code is available at\nhttps://github.com/Lightblues/FlowAgent.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.14345v1",
    "published_date": "2025-02-20 07:59:31 UTC",
    "updated_date": "2025-02-20 07:59:31 UTC"
  },
  {
    "arxiv_id": "2502.14334v1",
    "title": "Purest Quantum State Identification",
    "authors": [
      "Yingqi Yu",
      "Honglin Chen",
      "Jun Wu",
      "Wei Xie",
      "Xiangyang Li"
    ],
    "abstract": "Precise identification of quantum states under noise constraints is essential\nfor quantum information processing. In this study, we generalize the classical\nbest arm identification problem to quantum domains, designing methods for\nidentifying the purest one within $K$ unknown $n$-qubit quantum states using\n$N$ samples. %, with direct applications in quantum computation and quantum\ncommunication. We propose two distinct algorithms: (1) an algorithm employing\nincoherent measurements, achieving error $\\exp\\left(- \\Omega\\left(\\frac{N\nH_1}{\\log(K) 2^n }\\right) \\right)$, and (2) an algorithm utilizing coherent\nmeasurements, achieving error $\\exp\\left(- \\Omega\\left(\\frac{N H_2}{\\log(K)\n}\\right) \\right)$, highlighting the power of quantum memory. Furthermore, we\nestablish a lower bound by proving that all strategies with fixed two-outcome\nincoherent POVM must suffer error probability exceeding $ \\exp\\left( -\nO\\left(\\frac{NH_1}{2^n}\\right)\\right)$. This framework provides concrete design\nprinciples for overcoming sampling bottlenecks in quantum technologies.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14334v1",
    "published_date": "2025-02-20 07:42:16 UTC",
    "updated_date": "2025-02-20 07:42:16 UTC"
  },
  {
    "arxiv_id": "2502.14934v1",
    "title": "Fast and Accurate Blind Flexible Docking",
    "authors": [
      "Zizhuo Zhang",
      "Lijun Wu",
      "Kaiyuan Gao",
      "Jiangchao Yao",
      "Tao Qin",
      "Bo Han"
    ],
    "abstract": "Molecular docking that predicts the bound structures of small molecules\n(ligands) to their protein targets, plays a vital role in drug discovery.\nHowever, existing docking methods often face limitations: they either overlook\ncrucial structural changes by assuming protein rigidity or suffer from low\ncomputational efficiency due to their reliance on generative models for\nstructure sampling. To address these challenges, we propose FABFlex, a fast and\naccurate regression-based multi-task learning model designed for realistic\nblind flexible docking scenarios, where proteins exhibit flexibility and\nbinding pocket sites are unknown (blind). Specifically, FABFlex's architecture\ncomprises three specialized modules working in concert: (1) A pocket prediction\nmodule that identifies potential binding sites, addressing the challenges\ninherent in blind docking scenarios. (2) A ligand docking module that predicts\nthe bound (holo) structures of ligands from their unbound (apo) states. (3) A\npocket docking module that forecasts the holo structures of protein pockets\nfrom their apo conformations. Notably, FABFlex incorporates an iterative update\nmechanism that serves as a conduit between the ligand and pocket docking\nmodules, enabling continuous structural refinements. This approach effectively\nintegrates the three subtasks of blind flexible docking-pocket identification,\nligand conformation prediction, and protein flexibility modeling-into a\nunified, coherent framework. Extensive experiments on public benchmark datasets\ndemonstrate that FABFlex not only achieves superior effectiveness in predicting\naccurate binding modes but also exhibits a significant speed advantage (208\n$\\times$) compared to existing state-of-the-art methods. Our code is released\nat https://github.com/tmlr-group/FABFlex.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "25 pages, Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14934v1",
    "published_date": "2025-02-20 07:31:13 UTC",
    "updated_date": "2025-02-20 07:31:13 UTC"
  },
  {
    "arxiv_id": "2502.14333v1",
    "title": "A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics",
    "authors": [
      "Ting-Ruen Wei",
      "Haowei Liu",
      "Xuyang Wu",
      "Yi Fang"
    ],
    "abstract": "Recent progress in large language models (LLM) found chain-of-thought\nprompting strategies to improve the reasoning ability of LLMs by encouraging\nproblem solving through multiple steps. Therefore, subsequent research aimed to\nintegrate the multi-step reasoning process into the LLM itself through process\nrewards as feedback and achieved improvements over prompting strategies. Due to\nthe cost of step-level annotation, some turn to outcome rewards as feedback.\nAside from these training-based approaches, training-free techniques leverage\nfrozen LLMs or external tools for feedback at each step to enhance the\nreasoning process. With the abundance of work in mathematics due to its logical\nnature, we present a survey of strategies utilizing feedback at the step and\noutcome levels to enhance multi-step math reasoning for LLMs. As multi-step\nreasoning emerges a crucial component in scaling LLMs, we hope to establish its\nfoundation for easier understanding and empower further research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14333v1",
    "published_date": "2025-02-20 07:31:00 UTC",
    "updated_date": "2025-02-20 07:31:00 UTC"
  },
  {
    "arxiv_id": "2502.14318v1",
    "title": "Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models",
    "authors": [
      "James Fodor"
    ],
    "abstract": "Large language models (LLMs) regularly demonstrate new and impressive\nperformance on a wide range of language, knowledge, and reasoning benchmarks.\nSuch rapid progress has led many commentators to argue that LLM general\ncognitive capabilities have likewise rapidly improved, with the implication\nthat such models are becoming progressively more capable on various real-world\ntasks. Here I summarise theoretical and empirical considerations to challenge\nthis narrative. I argue that inherent limitations with the benchmarking\nparadigm, along with specific limitations of existing benchmarks, render\nbenchmark performance highly unsuitable as a metric for generalisable\ncompetence over cognitive tasks. I also contend that alternative methods for\nassessing LLM capabilities, including adversarial stimuli and interpretability\ntechniques, have shown that LLMs do not have robust competence in many language\nand reasoning tasks, and often fail to learn representations which facilitate\ngeneralisable inferences. I conclude that benchmark performance should not be\nused as a reliable indicator of general LLM cognitive capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.14318v1",
    "published_date": "2025-02-20 07:13:29 UTC",
    "updated_date": "2025-02-20 07:13:29 UTC"
  },
  {
    "arxiv_id": "2502.14316v1",
    "title": "Textured 3D Regenerative Morphing with 3D Diffusion Prior",
    "authors": [
      "Songlin Yang",
      "Yushi Lan",
      "Honghua Chen",
      "Xingang Pan"
    ],
    "abstract": "Textured 3D morphing creates smooth and plausible interpolation sequences\nbetween two 3D objects, focusing on transitions in both shape and texture. This\nis important for creative applications like visual effects in filmmaking.\nPrevious methods rely on establishing point-to-point correspondences and\ndetermining smooth deformation trajectories, which inherently restrict them to\nshape-only morphing on untextured, topologically aligned datasets. This\nrestriction leads to labor-intensive preprocessing and poor generalization. To\novercome these challenges, we propose a method for 3D regenerative morphing\nusing a 3D diffusion prior. Unlike previous methods that depend on explicit\ncorrespondences and deformations, our method eliminates the additional need for\nobtaining correspondence and uses the 3D diffusion prior to generate morphing.\nSpecifically, we introduce a 3D diffusion model and interpolate the source and\ntarget information at three levels: initial noise, model parameters, and\ncondition features. We then explore an Attention Fusion strategy to generate\nmore smooth morphing sequences. To further improve the plausibility of semantic\ninterpolation and the generated 3D surfaces, we propose two strategies: (a)\nToken Reordering, where we match approximate tokens based on semantic analysis\nto guide implicit correspondences in the denoising process of the diffusion\nmodel, and (b) Low-Frequency Enhancement, where we enhance low-frequency\nsignals in the tokens to improve the quality of generated surfaces.\nExperimental results show that our method achieves superior smoothness and\nplausibility in 3D morphing across diverse cross-category object pairs,\noffering a novel regenerative method for 3D morphing with textured\nrepresentations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14316v1",
    "published_date": "2025-02-20 07:02:22 UTC",
    "updated_date": "2025-02-20 07:02:22 UTC"
  },
  {
    "arxiv_id": "2502.15830v1",
    "title": "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on Code Naturalness",
    "authors": [
      "Weisong Sun",
      "Yuchen Chen",
      "Mengzhe Yuan",
      "Chunrong Fang",
      "Zhenpeng Chen",
      "Chong Wang",
      "Yang Liu",
      "Baowen Xu",
      "Zhenyu Chen"
    ],
    "abstract": "Neural code models (NCMs) have demonstrated extraordinary capabilities in\ncode intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems\nhas garnered increasing attention. In particular, NCMs are often trained on\nlarge-scale data from potentially untrustworthy sources, providing attackers\nwith the opportunity to manipulate them by inserting crafted samples into the\ndata. This type of attack is called a code poisoning attack (also known as a\nbackdoor attack). It allows attackers to implant backdoors in NCMs and thus\ncontrol model behavior, which poses a significant security threat. However,\nthere is still a lack of effective techniques for detecting various complex\ncode poisoning attacks.\n  In this paper, we propose an innovative and lightweight technique for code\npoisoning detection named KillBadCode. KillBadCode is designed based on our\ninsight that code poisoning disrupts the naturalness of code. Specifically,\nKillBadCode first builds a code language model (CodeLM) on a lightweight\n$n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM\nto identify those tokens in (poisoned) code snippets that will make the code\nsnippets more natural after being deleted as trigger tokens. Considering that\nthe removal of some normal tokens in a single sample might also enhance code\nnaturalness, leading to a high false positive rate (FPR), we aggregate the\ncumulative improvement of each token across all samples. Finally, KillBadCode\npurifies the poisoned data by removing all poisoned samples containing the\nidentified trigger tokens. The experimental results on two code poisoning\nattacks and four code intelligence tasks demonstrate that KillBadCode\nsignificantly outperforms four baselines. More importantly, KillBadCode is very\nefficient, with a minimum time consumption of only 5 minutes, and is 25 times\nfaster than the best baseline on average.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "68-06",
      "D.2.3; I.2.7"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to the 47th International Conference on Software Engineering\n  (ICSE 2025)",
    "pdf_url": "http://arxiv.org/pdf/2502.15830v1",
    "published_date": "2025-02-20 06:53:09 UTC",
    "updated_date": "2025-02-20 06:53:09 UTC"
  },
  {
    "arxiv_id": "2502.14302v1",
    "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
    "authors": [
      "Shrey Pandit",
      "Jiawei Xu",
      "Junyuan Hong",
      "Zhangyang Wang",
      "Tianlong Chen",
      "Kaidi Xu",
      "Ying Ding"
    ],
    "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in\nmedical question-answering necessitate rigorous evaluation of their\nreliability. A critical challenge lies in hallucination, where models generate\nplausible yet factually incorrect outputs. In the medical domain, this poses\nserious risks to patient safety and clinical decision-making. To address this,\nwe introduce MedHallu, the first benchmark specifically designed for medical\nhallucination detection. MedHallu comprises 10,000 high-quality question-answer\npairs derived from PubMedQA, with hallucinated answers systematically generated\nthrough a controlled pipeline. Our experiments show that state-of-the-art LLMs,\nincluding GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical,\nstruggle with this binary hallucination detection task, with the best model\nachieving an F1 score as low as 0.625 for detecting \"hard\" category\nhallucinations. Using bidirectional entailment clustering, we show that\nharder-to-detect hallucinations are semantically closer to ground truth.\nThrough experiments, we also show incorporating domain-specific knowledge and\nintroducing a \"not sure\" category as one of the answer categories improves the\nprecision and F1 scores by up to 38% relative to baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Code and dataset are available at https://medhallu.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2502.14302v1",
    "published_date": "2025-02-20 06:33:23 UTC",
    "updated_date": "2025-02-20 06:33:23 UTC"
  },
  {
    "arxiv_id": "2502.14301v1",
    "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models",
    "authors": [
      "Yosephine Susanto",
      "Adithya Venkatadri Hulagadri",
      "Jann Railey Montalan",
      "Jian Gang Ngui",
      "Xian Bin Yong",
      "Weiqi Leong",
      "Hamsawardhini Rengarajan",
      "Peerat Limkonchotiwat",
      "Yifan Mai",
      "William Chandra Tjhi"
    ],
    "abstract": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and authentic evaluation suite for the SEA\nlanguages has not been developed thus far. Here, we present SEA-HELM, a\nholistic linguistic and cultural LLM evaluation suite that emphasizes SEA\nlanguages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics,\n(3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports\nFilipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the\nSEA-HELM leaderboard, which allows users to understand models' multilingual and\nmulticultural performance in a systematic and user-friendly manner.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14301v1",
    "published_date": "2025-02-20 06:32:45 UTC",
    "updated_date": "2025-02-20 06:32:45 UTC"
  },
  {
    "arxiv_id": "2502.14297v2",
    "title": "Evaluating Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial Research Intelligence' (ARI)?",
    "authors": [
      "Joeran Beel",
      "Min-Yen Kan",
      "Moritz Baumgart"
    ],
    "abstract": "A major step toward Artificial General Intelligence (AGI) and Super\nIntelligence is AI's ability to autonomously conduct research - what we term\nArtificial Research Intelligence (ARI). If machines could generate hypotheses,\nconduct experiments, and write research papers without human intervention, it\nwould transform science. Sakana recently introduced the 'AI Scientist',\nclaiming to conduct research autonomously, i.e. they imply to have achieved\nwhat we term Artificial Research Intelligence (ARI). The AI Scientist gained\nmuch attention, but a thorough independent evaluation has yet to be conducted.\n  Our evaluation of the AI Scientist reveals critical shortcomings. The\nsystem's literature reviews produced poor novelty assessments, often\nmisclassifying established concepts (e.g., micro-batching for stochastic\ngradient descent) as novel. It also struggles with experiment execution: 42% of\nexperiments failed due to coding errors, while others produced flawed or\nmisleading results. Code modifications were minimal, averaging 8% more\ncharacters per iteration, suggesting limited adaptability. Generated\nmanuscripts were poorly substantiated, with a median of five citations, most\noutdated (only five of 34 from 2020 or later). Structural errors were frequent,\nincluding missing figures, repeated sections, and placeholder text like\n'Conclusions Here'. Some papers contained hallucinated numerical results.\n  Despite these flaws, the AI Scientist represents a leap forward in research\nautomation. It generates full research manuscripts with minimal human input,\nchallenging expectations of AI-driven science. Many reviewers might struggle to\ndistinguish its work from human researchers. While its quality resembles a\nrushed undergraduate paper, its speed and cost efficiency are unprecedented,\nproducing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far\noutpacing traditional researchers.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "16 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.14297v2",
    "published_date": "2025-02-20 06:22:03 UTC",
    "updated_date": "2025-02-22 11:35:41 UTC"
  },
  {
    "arxiv_id": "2502.14293v1",
    "title": "Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains",
    "authors": [
      "Delaram Pirhayati",
      "Arlei Silva"
    ],
    "abstract": "Graph Anomaly Detection (GAD) has demonstrated great effectiveness in\nidentifying unusual patterns within graph-structured data. However, while\nlabeled anomalies are often scarce in emerging applications, existing\nsupervised GAD approaches are either ineffective or not applicable when moved\nacross graph domains due to distribution shifts and heterogeneous feature\nspaces. To address these challenges, we present AdaGraph-T3, a novel test-time\ntraining framework for cross-domain GAD. AdaGraph-T3 combines supervised and\nself-supervised learning during training while adapting to a new domain during\ntest time using only self-supervised learning by leveraging a homophily-based\naffinity score that captures domain-invariant properties of anomalies. Our\nframework introduces four key innovations to cross-domain GAD: an effective\nself-supervision scheme, an attention-based mechanism that dynamically learns\nedge importance weights during message passing, domain-specific encoders for\nhandling heterogeneous features, and class-aware regularization to address\nimbalance. Experiments across multiple cross-domain settings demonstrate that\nAdaGraph-T3 significantly outperforms existing approaches, achieving average\nimprovements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best\ncompeting model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14293v1",
    "published_date": "2025-02-20 06:14:07 UTC",
    "updated_date": "2025-02-20 06:14:07 UTC"
  },
  {
    "arxiv_id": "2502.15828v1",
    "title": "A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models",
    "authors": [
      "Mengyang Sun",
      "Yihao Wang",
      "Tao Feng",
      "Dan Zhang",
      "Yifan Zhu",
      "Jie Tang"
    ],
    "abstract": "In order to streamline the fine-tuning of foundation models, Low-Rank\nAdapters (LoRAs) have been substantially adopted across various fields,\nincluding instruction tuning and domain adaptation. The underlying concept of\nLoRA involves decomposing a full-rank matrix into the product of two lower-rank\nmatrices, which reduces storage consumption and accelerates the training\nprocess. Furthermore, to address the limited expressive capacity of LoRA, the\nMixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA\nadapters. The integration of LoRA experts leads to a visible improvement across\nseveral downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still\nexhibits its low robustness during tuning and inferring. Inspired by the\nRiemannian Preconditioners which train LoRA as a sub-space projector, we\npropose a new training strategy for MoE-LoRA, to stabilize and boost its\nfeature learning procedure by multi-space projections. Examinations on SGD and\nAdamW optimizers demonstrate the effectiveness of our methodology. Source code\nis available at https://github.com/THUDM/MoELoRA_Riemannian.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15828v1",
    "published_date": "2025-02-20 05:58:53 UTC",
    "updated_date": "2025-02-20 05:58:53 UTC"
  },
  {
    "arxiv_id": "2502.14281v3",
    "title": "Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts",
    "authors": [
      "Weipeng Huang",
      "Qin Li",
      "Yang Xiao",
      "Cheng Qiao",
      "Tie Cai",
      "Junwei Liang",
      "Neil J. Hurley",
      "Guangyuan Piao"
    ],
    "abstract": "Noise in data appears to be inevitable in most real-world machine learning\napplications and would cause severe overfitting problems. Not only can data\nfeatures contain noise, but labels are also prone to be noisy due to human\ninput. In this paper, rather than noisy label learning in multiclass\nclassifications, we instead focus on the less explored area of noisy label\nlearning for multilabel classifications. Specifically, we investigate the\npost-correction of predictions generated from classifiers learned with noisy\nlabels. The reasons are two-fold. Firstly, this approach can directly work with\nthe trained models to save computational resources. Secondly, it could be\napplied on top of other noisy label correction techniques to achieve further\nimprovements. To handle this problem, we appeal to deep generative approaches\nthat are possible for uncertainty estimation. Our model posits that label noise\narises from a stochastic shift in the latent variable, providing a more robust\nand beneficial means for noisy learning. We develop both unsupervised and\nsemi-supervised learning methods for our model. The extensive empirical study\npresents solid evidence to that our approach is able to consistently improve\nthe independent models and performs better than a number of existing methods\nacross various noisy label settings. Moreover, a comprehensive empirical\nanalysis of the proposed method is carried out to validate its robustness,\nincluding sensitivity analysis and an ablation study, among other elements.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14281v3",
    "published_date": "2025-02-20 05:41:52 UTC",
    "updated_date": "2025-05-08 03:34:20 UTC"
  },
  {
    "arxiv_id": "2502.14280v1",
    "title": "EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts",
    "authors": [
      "Subhajit Chaudhury",
      "Payel Das",
      "Sarathkrishna Swaminathan",
      "Georgios Kollias",
      "Elliot Nelson",
      "Khushbu Pahwa",
      "Tejaswini Pedapati",
      "Igor Melnyk",
      "Matthew Riemer"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have yielded impressive\nsuccesses on many language tasks. However, efficient processing of long\ncontexts using LLMs remains a significant challenge. We introduce\n\\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic\nmemory} module while \\textit{holistically attending to} semantically relevant\ncontext chunks. The output of \\textit{episodic attention} is then used to\nreweigh the decoder's self-attention to the stored KV cache of the context\nduring training and generation. When an LLM decoder is trained using\n\\textbf{EpMAN}, its performance on multiple challenging single-hop long-context\nrecall and question-answering benchmarks is found to be stronger and more\nrobust across the range from 16k to 256k tokens than baseline decoders trained\nwith self-attention, and popular retrieval-augmented generation frameworks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14280v1",
    "published_date": "2025-02-20 05:41:15 UTC",
    "updated_date": "2025-02-20 05:41:15 UTC"
  },
  {
    "arxiv_id": "2502.14276v1",
    "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
    "authors": [
      "Hanlin Wang",
      "Jian Wang",
      "Chak Tou Leong",
      "Wenjie Li"
    ],
    "abstract": "Large language model (LLM)-based agents have shown promise in tackling\ncomplex tasks by interacting dynamically with the environment. Existing work\nprimarily focuses on behavior cloning from expert demonstrations and preference\nlearning through exploratory trajectory sampling. However, these methods often\nstruggle in long-horizon tasks, where suboptimal actions accumulate step by\nstep, causing agents to deviate from correct task trajectories. To address\nthis, we highlight the importance of timely calibration and the need to\nautomatically construct calibration trajectories for training agents. We\npropose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM\nagent learning. Specifically, STeCa identifies suboptimal actions through a\nstep-level reward comparison during exploration. It constructs calibrated\ntrajectories using LLM-driven reflection, enabling agents to learn from\nimproved decision-making processes. These calibrated trajectories, together\nwith successful trajectory data, are utilized for reinforced training.\nExtensive experiments demonstrate that STeCa significantly outperforms existing\nmethods. Further analysis highlights that step-level calibration enables agents\nto complete tasks with greater robustness. Our code and data are available at\nhttps://github.com/WangHanLinHenry/STeCa.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14276v1",
    "published_date": "2025-02-20 05:28:44 UTC",
    "updated_date": "2025-02-20 05:28:44 UTC"
  },
  {
    "arxiv_id": "2502.14273v1",
    "title": "LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework",
    "authors": [
      "Zongyou Yu",
      "Qiang Qu",
      "Qian Zhang",
      "Nan Zhang",
      "Xiaoming Chen"
    ],
    "abstract": "Recent advancements in event-based recognition have demonstrated significant\npromise, yet most existing approaches rely on extensive training, limiting\ntheir adaptability for efficient processing of event-driven visual content.\nMeanwhile, large language models (LLMs) have exhibited remarkable zero-shot\ncapabilities across diverse domains, but their application to event-based\nvisual recognition remains largely unexplored. To bridge this gap, we propose\n\\textbf{LLM-EvGen}, an event representation generator that produces\nLLM-compatible event representations \\textbf{LLM-EvRep}, thereby enhancing the\nperformance of LLMs on event recognition tasks. The generator is trained using\na self-supervised framework, aligning the generated representations with\nsemantic consistency and structural fidelity. Comprehensive experiments were\nconducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results\ndemonstrate that our method, \\textbf{LLM-EvRep}, outperforms the event-to-video\nmethod, E2VID, by 15.93\\%, 0.82\\%, and 50.21\\%, respectively, in recognition\ntasks when evaluated using GPT-4o.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures,Companion Proceedings of the ACM Web Conference\n  2025 (WWW Companion '25)",
    "pdf_url": "http://arxiv.org/pdf/2502.14273v1",
    "published_date": "2025-02-20 05:18:36 UTC",
    "updated_date": "2025-02-20 05:18:36 UTC"
  },
  {
    "arxiv_id": "2502.14272v1",
    "title": "Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models",
    "authors": [
      "Yanggan Gu",
      "Junzhuo Li",
      "Sirui Huang",
      "Xin Zou",
      "Zhenghua Li",
      "Xuming Hu"
    ],
    "abstract": "Aligning small language models (SLMs) with human values typically involves\ndistilling preference knowledge from large language models (LLMs). However,\nexisting distillation methods model preference knowledge in teacher LLMs by\ncomparing pairwise responses, overlooking the extent of difference between\nresponses. This limitation hinders student SLMs from capturing the nuanced\npreferences for multiple responses. In this paper, we propose a\nPreference-Aligned Distillation (PAD) framework, which models teacher's\npreference knowledge as a probability distribution over all potential\npreferences, thereby providing more nuanced supervisory signals. Our insight in\ndeveloping PAD is rooted in the demonstration that language models can serve as\nreward functions, reflecting their intrinsic preferences. Based on this, PAD\ncomprises three key steps: (1) sampling diverse responses using\nhigh-temperature; (2) computing rewards for both teacher and student to\nconstruct their intrinsic preference; and (3) training the student's intrinsic\npreference distribution to align with the teacher's. Experiments on four\nmainstream alignment benchmarks demonstrate that PAD consistently and\nsignificantly outperforms existing approaches, achieving over 20\\% improvement\non AlpacaEval 2 and Arena-Hard, indicating superior alignment with human\npreferences. Notably, on MT-Bench, using the \\textsc{Gemma} model family, the\nstudent trained by PAD surpasses its teacher, further validating the\neffectiveness of our PAD.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2502.14272v1",
    "published_date": "2025-02-20 05:18:23 UTC",
    "updated_date": "2025-02-20 05:18:23 UTC"
  },
  {
    "arxiv_id": "2502.14268v1",
    "title": "MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels",
    "authors": [
      "Xiaoou Liu",
      "Zhen Lin",
      "Longchao Da",
      "Chacha Chen",
      "Shubhendu Trivedi",
      "Hua Wei"
    ],
    "abstract": "Large Language Models (LLMs) require robust confidence estimation,\nparticularly in critical domains like healthcare and law where unreliable\noutputs can lead to significant consequences. Despite much recent work in\nconfidence estimation, current evaluation frameworks rely on correctness\nfunctions -- various heuristics that are often noisy, expensive, and possibly\nintroduce systematic biases. These methodological weaknesses tend to distort\nevaluation metrics and thus the comparative ranking of confidence measures. We\nintroduce MCQA-Eval, an evaluation framework for assessing confidence measures\nin Natural Language Generation (NLG) that eliminates dependence on an explicit\ncorrectness function by leveraging gold-standard correctness labels from\nmultiple-choice datasets. MCQA-Eval enables systematic comparison of both\ninternal state-based white-box (e.g. logit-based) and consistency-based\nblack-box confidence measures, providing a unified evaluation methodology\nacross different approaches. Through extensive experiments on multiple LLMs and\nwidely used QA datasets, we report that MCQA-Eval provides efficient and more\nreliable assessments of confidence estimation methods than existing approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14268v1",
    "published_date": "2025-02-20 05:09:29 UTC",
    "updated_date": "2025-02-20 05:09:29 UTC"
  },
  {
    "arxiv_id": "2502.15827v2",
    "title": "Explainable Artificial Intelligence Model for Evaluating Shear Strength Parameters of Municipal Solid Waste Across Diverse Compositional Profiles",
    "authors": [
      "Parichat Suknark",
      "Sompote Youwaib",
      "Tipok Kitkobsin",
      "Sirintornthep Towprayoon",
      "Chart Chiemchaisri",
      "Komsilp Wangyao"
    ],
    "abstract": "Accurate prediction of shear strength parameters in Municipal Solid Waste\n(MSW) remains a critical challenge in geotechnical engineering due to the\nheterogeneous nature of waste materials and their temporal evolution through\ndegradation processes. This paper presents a novel explainable artificial\nintelligence (XAI) framework for evaluating cohesion and friction angle across\ndiverse MSW compositional profiles. The proposed model integrates a multi-layer\nperceptron architecture with SHAP (SHapley Additive exPlanations) analysis to\nprovide transparent insights into how specific waste components influence\nstrength characteristics. Training data encompassed large-scale direct shear\ntests across various waste compositions and degradation states. The model\ndemonstrated superior predictive accuracy compared to traditional gradient\nboosting methods, achieving mean absolute percentage errors of 7.42% and 14.96%\nfor friction angle and cohesion predictions, respectively. Through SHAP\nanalysis, the study revealed that fibrous materials and particle size\ndistribution were primary drivers of shear strength variation, with food waste\nand plastics showing significant but non-linear effects. The model's\nexplainability component successfully quantified these relationships, enabling\nevidence-based recommendations for waste management practices. This research\nbridges the gap between advanced machine learning and geotechnical engineering\npractice, offering a reliable tool for rapid assessment of MSW mechanical\nproperties while maintaining interpretability for engineering decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15827v2",
    "published_date": "2025-02-20 05:02:55 UTC",
    "updated_date": "2025-02-26 22:37:33 UTC"
  },
  {
    "arxiv_id": "2502.14264v1",
    "title": "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics",
    "authors": [
      "Fernando Martinez-Lopez",
      "Juntao Chen",
      "Yingdong Lu"
    ],
    "abstract": "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in: AAAI 2025 Workshop on Planning and Reinforcement\n  Learning (PRL) - Bridging the Gap Between AI Planning and Reinforcement\n  Learning",
    "pdf_url": "http://arxiv.org/pdf/2502.14264v1",
    "published_date": "2025-02-20 05:02:29 UTC",
    "updated_date": "2025-02-20 05:02:29 UTC"
  },
  {
    "arxiv_id": "2502.14260v1",
    "title": "EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement",
    "authors": [
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Xin Li",
      "Yujian Xiong",
      "Xiwen Chen",
      "Peijie Qiu",
      "Vamsi Krishna Vasa",
      "Zhangsihao Yang",
      "Yi Su",
      "Oana Dumitrascu",
      "Yalin Wang"
    ],
    "abstract": "Over the past decade, generative models have achieved significant success in\nenhancement fundus images.However, the evaluation of these models still\npresents a considerable challenge. A comprehensive evaluation benchmark for\nfundus image enhancement is indispensable for three main reasons: 1) The\nexisting denoising metrics (e.g., PSNR, SSIM) are hardly to extend to\ndownstream real-world clinical research (e.g., Vessel morphology consistency).\n2) There is a lack of comprehensive evaluation for both paired and unpaired\nenhancement methods, along with the need for expert protocols to accurately\nassess clinical value. 3) An ideal evaluation system should provide insights to\ninform future developments of fundus image enhancement. To this end, we propose\na novel comprehensive benchmark, EyeBench, to provide insights that align\nenhancement models with clinical needs, offering a foundation for future work\nto improve the clinical relevance and applicability of generative models for\nfundus image enhancement. EyeBench has three appealing properties: 1)\nmulti-dimensional clinical alignment downstream evaluation: In addition to\nevaluating the enhancement task, we provide several clinically significant\ndownstream tasks for fundus images, including vessel segmentation, DR grading,\ndenoising generalization, and lesion segmentation. 2) Medical expert-guided\nevaluation design: We introduce a novel dataset that promote comprehensive and\nfair comparisons between paired and unpaired methods and includes a manual\nevaluation protocol by medical experts. 3) Valuable insights: Our benchmark\nstudy provides a comprehensive and rigorous evaluation of existing methods\nacross different downstream tasks, assisting medical experts in making informed\nchoices. Additionally, we offer further analysis of the challenges faced by\nexisting methods. The code is available at\n\\url{https://github.com/Retinal-Research/EyeBench}",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14260v1",
    "published_date": "2025-02-20 04:56:03 UTC",
    "updated_date": "2025-02-20 04:56:03 UTC"
  },
  {
    "arxiv_id": "2502.15826v1",
    "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
    "authors": [
      "Dahyun Jung",
      "Jaehyung Seo",
      "Jaewook Lee",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Large language models (LLMs) often retain outdated or incorrect information\nfrom pre-training, which undermines their reliability. While model editing\nmethods have been developed to address such errors without full re-training,\nthey frequently suffer from knowledge conflicts, where outdated information\ninterferes with new knowledge. In this work, we propose Conflict-free Model\nEditing (CoME), a novel framework that enhances the accuracy of knowledge\nupdates in LLMs by selectively removing outdated knowledge. CoME leverages\nunlearning to mitigate knowledge interference, allowing new information to be\nintegrated without compromising relevant linguistic features. Through\nexperiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we\ndemonstrate that CoME improves both editing accuracy and model reliability when\napplied to existing editing methods. Our results highlight that the targeted\nremoval of outdated knowledge is crucial for enhancing model editing\neffectiveness and maintaining the model's generative performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 main conference",
    "pdf_url": "http://arxiv.org/pdf/2502.15826v1",
    "published_date": "2025-02-20 04:55:38 UTC",
    "updated_date": "2025-02-20 04:55:38 UTC"
  },
  {
    "arxiv_id": "2502.14258v1",
    "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information",
    "authors": [
      "Yein Park",
      "Chanwoong Yoon",
      "Jungwoo Park",
      "Minbyul Jeong",
      "Jaewoo Kang"
    ],
    "abstract": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads primarily responsible for\nprocessing temporal knowledge through circuit analysis. We confirm that these\nheads are present across multiple models, though their specific locations may\nvary, and their responses differ depending on the type of knowledge and its\ncorresponding years. Disabling these heads degrades the model's ability to\nrecall time-specific knowledge while maintaining its general capabilities\nwithout compromising time-invariant and question-answering performances.\nMoreover, the heads are activated not only numeric conditions (\"In 2004\") but\nalso textual aliases (\"In the year ...\"), indicating that they encode a\ntemporal dimension beyond simple numerical representation. Furthermore, we\nexpand the potential of our findings by demonstrating how temporal knowledge\ncan be edited by adjusting the values of these heads.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14258v1",
    "published_date": "2025-02-20 04:52:05 UTC",
    "updated_date": "2025-02-20 04:52:05 UTC"
  },
  {
    "arxiv_id": "2502.14255v1",
    "title": "Effects of Prompt Length on Domain-specific Tasks for Large Language Models",
    "authors": [
      "Qibang Liu",
      "Wenzhe Wang",
      "Jeffrey Willard"
    ],
    "abstract": "In recent years, Large Language Models have garnered significant attention\nfor their strong performance in various natural language tasks, such as machine\ntranslation and question answering. These models demonstrate an impressive\nability to generalize across diverse tasks. However, their effectiveness in\ntackling domain-specific tasks, such as financial sentiment analysis and\nmonetary policy understanding, remains a topic of debate, as these tasks often\nrequire specialized knowledge and precise reasoning. To address such\nchallenges, researchers design various prompts to unlock the models' abilities.\nBy carefully crafting input prompts, researchers can guide these models to\nproduce more accurate responses. Consequently, prompt engineering has become a\nkey focus of study. Despite the advancements in both models and prompt\nengineering, the relationship between the two-specifically, how prompt design\nimpacts models' ability to perform domain-specific tasks-remains underexplored.\nThis paper aims to bridge this research gap.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14255v1",
    "published_date": "2025-02-20 04:42:06 UTC",
    "updated_date": "2025-02-20 04:42:06 UTC"
  },
  {
    "arxiv_id": "2502.14254v1",
    "title": "Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation",
    "authors": [
      "Lingfeng Zhang",
      "Yuecheng Liu",
      "Zhanguang Zhang",
      "Matin Aghaei",
      "Yaochen Hu",
      "Hongjian Gu",
      "Mohammad Ali Alomrani",
      "David Gamaliel Arcos Bravo",
      "Raika Karimi",
      "Atia Hamidizadeh",
      "Haoping Xu",
      "Guowei Huang",
      "Zhanpeng Zhang",
      "Tongtong Cao",
      "Weichao Qiu",
      "Xingyue Quan",
      "Jianye Hao",
      "Yuzheng Zhuang",
      "Yingxue Zhang"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have made them powerful tools in embodied navigation, enabling\nagents to leverage commonsense and spatial reasoning for efficient exploration\nin unfamiliar environments. Existing LLM-based approaches convert global\nmemory, such as semantic or topological maps, into language descriptions to\nguide navigation. While this improves efficiency and reduces redundant\nexploration, the loss of geometric information in language-based\nrepresentations hinders spatial reasoning, especially in intricate\nenvironments. To address this, VLM-based approaches directly process\nego-centric visual inputs to select optimal directions for exploration.\nHowever, relying solely on a first-person perspective makes navigation a\npartially observed decision-making problem, leading to suboptimal decisions in\ncomplex environments. In this paper, we present a novel vision-language model\n(VLM)-based navigation framework that addresses these challenges by adaptively\nretrieving task-relevant cues from a global memory module and integrating them\nwith the agent's egocentric observations. By dynamically aligning global\ncontextual information with local perception, our approach enhances spatial\nreasoning and decision-making in long-horizon tasks. Experimental results\ndemonstrate that the proposed method surpasses previous state-of-the-art\napproaches in object navigation tasks, providing a more effective and scalable\nsolution for embodied navigation.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14254v1",
    "published_date": "2025-02-20 04:41:40 UTC",
    "updated_date": "2025-02-20 04:41:40 UTC"
  },
  {
    "arxiv_id": "2502.14247v2",
    "title": "Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation",
    "authors": [
      "Jiayu Yang",
      "Taizhang Shang",
      "Weixuan Sun",
      "Xibin Song",
      "Ziang Cheng",
      "Senbo Wang",
      "Shenzhou Chen",
      "Weizhe Liu",
      "Hongdong Li",
      "Pan Ji"
    ],
    "abstract": "This report presents a comprehensive framework for generating high-quality 3D\nshapes and textures from diverse input prompts, including single images,\nmulti-view images, and text descriptions. The framework consists of 3D shape\ngeneration and texture generation. (1). The 3D shape generation pipeline\nemploys a Variational Autoencoder (VAE) to encode implicit 3D geometries into a\nlatent space and a diffusion network to generate latents conditioned on input\nprompts, with modifications to enhance model capacity. An alternative\nArtist-Created Mesh (AM) generation approach is also explored, yielding\npromising results for simpler geometries. (2). Texture generation involves a\nmulti-stage process starting with frontal images generation followed by\nmulti-view images generation, RGB-to-PBR texture conversion, and\nhigh-resolution multi-view texture refinement. A consistency scheduler is\nplugged into every stage, to enforce pixel-wise consistency among multi-view\ntextures during inference, ensuring seamless integration.\n  The pipeline demonstrates effective handling of diverse input formats,\nleveraging advanced neural architectures and novel methodologies to produce\nhigh-quality 3D content. This report details the system architecture,\nexperimental results, and potential future directions to improve and expand the\nframework. The source code and pretrained weights are released at:\nhttps://github.com/Tencent/Tencent-XR-3DGen.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR",
    "comment": "Tencent XR 3D Gen",
    "pdf_url": "http://arxiv.org/pdf/2502.14247v2",
    "published_date": "2025-02-20 04:22:30 UTC",
    "updated_date": "2025-02-21 19:09:29 UTC"
  },
  {
    "arxiv_id": "2502.14235v1",
    "title": "OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving",
    "authors": [
      "Yedong Shen",
      "Xinran Zhang",
      "Yifan Duan",
      "Shiqi Zhang",
      "Heng Li",
      "Yilong Wu",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "abstract": "Accurate and realistic 3D scene reconstruction enables the lifelike creation\nof autonomous driving simulation environments. With advancements in 3D Gaussian\nSplatting (3DGS), previous studies have applied it to reconstruct complex\ndynamic driving scenes. These methods typically require expensive LiDAR sensors\nand pre-annotated datasets of dynamic objects. To address these challenges, we\npropose OG-Gaussian, a novel approach that replaces LiDAR point clouds with\nOccupancy Grids (OGs) generated from surround-view camera images using\nOccupancy Prediction Network (ONet). Our method leverages the semantic\ninformation in OGs to separate dynamic vehicles from static street background,\nconverting these grids into two distinct sets of initial point clouds for\nreconstructing both static and dynamic objects. Additionally, we estimate the\ntrajectories and poses of dynamic objects through a learning-based approach,\neliminating the need for complex manual annotations. Experiments on Waymo Open\ndataset demonstrate that OG-Gaussian is on par with the current\nstate-of-the-art in terms of reconstruction quality and rendering speed,\nachieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while\nsignificantly reducing computational costs and economic overhead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14235v1",
    "published_date": "2025-02-20 04:00:47 UTC",
    "updated_date": "2025-02-20 04:00:47 UTC"
  },
  {
    "arxiv_id": "2502.15825v1",
    "title": "Utilizing AI and Machine Learning for Predictive Analysis of Post-Treatment Cancer Recurrence",
    "authors": [
      "Muhammad Umer Qayyum",
      "Muhammad Fahad",
      "Nasrullah Abbasi"
    ],
    "abstract": "In oncology, recurrence after treatment is one of the major challenges,\nrelated to patients' survival and quality of life. Conventionally, prediction\nof cancer relapse has always relied on clinical observation with statistical\nmodel support, which almost fails to explain the complex, multifactorial nature\nof tumor recurrence. This research explores how AI and ML models may increase\nthe accuracy and reliability of recurrence prediction in cancer. Therefore, AI\nand ML create new opportunities not only for personalized medicine but also for\nproactive management of patients through analyzing large volumes of data on\ngenetics, clinical manifestations, and treatment. The paper describes the\nvarious AI and ML techniques for pattern identification and outcome prediction\nin cancer patients using supervised and unsupervised learning. Clinical\nimplications provide an opportunity to review how early interventions could\nhappen and the design of treatment planning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.15825v1",
    "published_date": "2025-02-20 03:54:12 UTC",
    "updated_date": "2025-02-20 03:54:12 UTC"
  },
  {
    "arxiv_id": "2502.15824v1",
    "title": "Getting SMARTER for Motion Planning in Autonomous Driving Systems",
    "authors": [
      "Montgomery Alban",
      "Ehsan Ahmadi",
      "Randy Goebel",
      "Amir Rasouli"
    ],
    "abstract": "Motion planning is a fundamental problem in autonomous driving and perhaps\nthe most challenging to comprehensively evaluate because of the associated\nrisks and expenses of real-world deployment. Therefore, simulations play an\nimportant role in efficient development of planning algorithms. To be\neffective, simulations must be accurate and realistic, both in terms of\ndynamics and behavior modeling, and also highly customizable in order to\naccommodate a broad spectrum of research frameworks. In this paper, we\nintroduce SMARTS 2.0, the second generation of our motion planning simulator\nwhich, in addition to being highly optimized for large-scale simulation,\nprovides many new features, such as realistic map integration,\nvehicle-to-vehicle (V2V) communication, traffic and pedestrian simulation, and\na broad variety of sensor models.\n  Moreover, we present a novel benchmark suite for evaluating planning\nalgorithms in various highly challenging scenarios, including interactive\ndriving, such as turning at intersections, and adaptive driving, in which the\ntask is to closely follow a lead vehicle without any explicit knowledge of its\nintention. Each scenario is characterized by a variety of traffic patterns and\nroad structures. We further propose a series of common and task-specific\nmetrics to effectively evaluate the performance of the planning algorithms. At\nthe end, we evaluate common motion planning algorithms using the proposed\nbenchmark and highlight the challenges the proposed scenarios impose. The new\nSMARTS 2.0 features and the benchmark are publicly available at\ngithub.com/huawei-noah/SMARTS.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.15824v1",
    "published_date": "2025-02-20 03:51:49 UTC",
    "updated_date": "2025-02-20 03:51:49 UTC"
  },
  {
    "arxiv_id": "2502.15823v4",
    "title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
    "authors": [
      "Wenyue Hua",
      "Tyler Wong",
      "Sun Fei",
      "Liangming Pan",
      "Adam Jardine",
      "William Yang Wang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 10 figures, more details including examples and prompts are\n  added",
    "pdf_url": "http://arxiv.org/pdf/2502.15823v4",
    "published_date": "2025-02-20 03:48:00 UTC",
    "updated_date": "2025-05-13 18:06:09 UTC"
  },
  {
    "arxiv_id": "2502.14227v1",
    "title": "SleepGMUformer: A gated multimodal temporal neural network for sleep staging",
    "authors": [
      "Chenjun Zhao",
      "Xuesen Niu",
      "Xinglin Yu",
      "Long Chen",
      "Na Lv",
      "Huiyu Zhou",
      "Aite Zhao"
    ],
    "abstract": "Sleep staging is a key method for assessing sleep quality and diagnosing\nsleep disorders. However, current deep learning methods face challenges: 1)\npostfusion techniques ignore the varying contributions of different modalities;\n2) unprocessed sleep data can interfere with frequency-domain information. To\ntackle these issues, this paper proposes a gated multimodal temporal neural\nnetwork for multidomain sleep data, including heart rate, motion, steps, EEG\n(Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model\nintegrates: 1) a pre-processing module for feature alignment, missing value\nhandling, and EEG de-trending; 2) a feature extraction module for complex sleep\nfeatures in the time dimension; and 3) a dynamic fusion module for real-time\nmodality weighting.Experiments show classification accuracies of 85.03% on\nSleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets. The model handles\nheterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14227v1",
    "published_date": "2025-02-20 03:42:42 UTC",
    "updated_date": "2025-02-20 03:42:42 UTC"
  },
  {
    "arxiv_id": "2502.14222v1",
    "title": "Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation Research",
    "authors": [
      "Manish Kumar Krishne Gowda",
      "Andrew Balmos",
      "Shin Boonam",
      "James V. Krogmeier"
    ],
    "abstract": "Effective strategies for sensor data management are essential for advancing\ntransportation research, especially in the current data-driven era, due to the\nadvent of novel applications in artificial intelligence. This paper presents\ncomprehensive guidelines for managing transportation sensor data, encompassing\nboth archived static data and real-time data streams. The real-time system\narchitecture integrates various applications with data acquisition systems\n(DAQ). By deploying the in-house designed, open-source Avena software platform\nalongside the NATS messaging system as a secure communication broker, reliable\ndata exchange is ensured. While robust databases like TimescaleDB facilitate\norganized storage, visualization platforms like Grafana provide real-time\nmonitoring capabilities.\n  In contrast, static data standards address the challenges in handling\nunstructured, voluminous datasets. The standards advocate for a combination of\ncost-effective bulk cloud storage for unprocessed sensor data and relational\ndatabases for recording summarized analyses. They highlight the role of cloud\ndata transfer tools like FME for efficient migration of sensor data from local\nstorages onto the cloud. Further, integration of robust visualization tools\ninto the framework helps in deriving patterns and trends from these complex\ndatasets.\n  The proposals were applied to INDOT's real-world case studies involving the\nI-65 and I-69 Greenfield districts. For real-time data collection, Campbell\nScientific DAQ systems were used, enabling continuous generation and monitoring\nof sensor metrics. In the case of the archived I-69 database, summary data was\ncompiled in Oracle, while the unprocessed data was stored in SharePoint. The\nresults underline the effectiveness of the proposed guidelines and motivate\ntheir adoption in research projects.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.DB",
    "comment": "This paper was accepted for presentation at the 104th TRB Annual\n  Meeting, held on January 5-9, 2025, in Washington, D.C., and was presented\n  during the poster session on January 8, 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14222v1",
    "published_date": "2025-02-20 03:37:46 UTC",
    "updated_date": "2025-02-20 03:37:46 UTC"
  },
  {
    "arxiv_id": "2502.14219v1",
    "title": "Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks",
    "authors": [
      "Jiangen He",
      "Jiqun Liu"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in decision-making, yet\ntheir susceptibility to cognitive biases remains a pressing challenge. This\nstudy explores how personality traits influence these biases and evaluates the\neffectiveness of mitigation strategies across various model architectures. Our\nfindings identify six prevalent cognitive biases, while the sunk cost and group\nattribution biases exhibit minimal impact. Personality traits play a crucial\nrole in either amplifying or reducing biases, significantly affecting how LLMs\nrespond to debiasing techniques. Notably, Conscientiousness and Agreeableness\nmay generally enhance the efficacy of bias mitigation strategies, suggesting\nthat LLMs exhibiting these traits are more receptive to corrective measures.\nThese findings address the importance of personality-driven bias dynamics and\nhighlight the need for targeted mitigation approaches to improve fairness and\nreliability in AI-assisted decision-making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14219v1",
    "published_date": "2025-02-20 03:15:54 UTC",
    "updated_date": "2025-02-20 03:15:54 UTC"
  },
  {
    "arxiv_id": "2502.14218v1",
    "title": "Rethinking Spiking Neural Networks from an Ensemble Learning Perspective",
    "authors": [
      "Yongqi Ding",
      "Lin Zuo",
      "Mengmeng Jing",
      "Pei He",
      "Hanpu Deng"
    ],
    "abstract": "Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer\nfrom limited performance. In this paper, we consider SNNs as ensembles of\ntemporal subnetworks that share architectures and weights, and highlight a\ncrucial issue that affects their performance: excessive differences in initial\nstates (neuronal membrane potentials) across timesteps lead to unstable\nsubnetwork outputs, resulting in degraded performance. To mitigate this, we\npromote the consistency of the initial membrane potential distribution and\noutput through membrane potential smoothing and temporally adjacent subnetwork\nguidance, respectively, to improve overall stability and performance. Moreover,\nmembrane potential smoothing facilitates forward propagation of information and\nbackward propagation of gradients, mitigating the notorious temporal gradient\nvanishing problem. Our method requires only minimal modification of the spiking\nneurons without adapting the network structure, making our method generalizable\nand showing consistent performance gains in 1D speech, 2D object, and 3D point\ncloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset,\nwe achieved 83.20\\% accuracy with only four timesteps. This provides valuable\ninsights into unleashing the potential of SNNs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14218v1",
    "published_date": "2025-02-20 03:15:52 UTC",
    "updated_date": "2025-02-20 03:15:52 UTC"
  },
  {
    "arxiv_id": "2502.14215v1",
    "title": "Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning",
    "authors": [
      "Ye Liu",
      "Yuqing Niu",
      "Chengyan Ma",
      "Ruidong Han",
      "Wei Ma",
      "Yi Li",
      "Debin Gao",
      "David Lo"
    ],
    "abstract": "Smart contracts are highly susceptible to manipulation attacks due to the\nleakage of sensitive information. Addressing manipulation vulnerabilities is\nparticularly challenging because they stem from inherent data confidentiality\nissues rather than straightforward implementation bugs. To tackle this by\npreventing sensitive information leakage, we present PartitionGPT, the first\nLLM-driven approach that combines static analysis with the in-context learning\ncapabilities of large language models (LLMs) to partition smart contracts into\nprivileged and normal codebases, guided by a few annotated sensitive data\nvariables. We evaluated PartitionGPT on 18 annotated smart contracts containing\n99 sensitive functions. The results demonstrate that PartitionGPT successfully\ngenerates compilable, and verified partitions for 78% of the sensitive\nfunctions while reducing approximately 30% code compared to function-level\npartitioning approach. Furthermore, we evaluated PartitionGPT on nine\nreal-world manipulation attacks that lead to a total loss of 25 million\ndollars, PartitionGPT effectively prevents eight cases, highlighting its\npotential for broad applicability and the necessity for secure program\npartitioning during smart contract development to diminish manipulation\nvulnerabilities.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14215v1",
    "published_date": "2025-02-20 03:07:56 UTC",
    "updated_date": "2025-02-20 03:07:56 UTC"
  },
  {
    "arxiv_id": "2502.15821v1",
    "title": "Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization",
    "authors": [
      "Keane Ong",
      "Rui Mao",
      "Deeksha Varshney",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Sustainability reports are key for evaluating companies' environmental,\nsocial and governance, ESG performance, but their content is increasingly\nobscured by greenwashing - sustainability claims that are misleading,\nexaggerated, and fabricated. Yet, existing NLP approaches for ESG analysis lack\nrobustness against greenwashing risks, often extracting insights that reflect\nmisleading or exaggerated sustainability claims rather than objective ESG\nperformance. To bridge this gap, we introduce A3CG - Aspect-Action Analysis\nwith Cross-Category Generalization, as a novel dataset to improve the\nrobustness of ESG analysis amid the prevalence of greenwashing. By explicitly\nlinking sustainability aspects with their associated actions, A3CG facilitates\na more fine-grained and transparent evaluation of sustainability claims,\nensuring that insights are grounded in verifiable actions rather than vague or\nmisleading rhetoric. Additionally, A3CG emphasizes cross-category\ngeneralization. This ensures robust model performance in aspect-action analysis\neven when companies change their reports to selectively favor certain\nsustainability areas. Through experiments on A3CG, we analyze state-of-the-art\nsupervised models and LLMs, uncovering their limitations and outlining key\ndirections for future research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15821v1",
    "published_date": "2025-02-20 03:01:08 UTC",
    "updated_date": "2025-02-20 03:01:08 UTC"
  },
  {
    "arxiv_id": "2502.15820v2",
    "title": "Universal AI maximizes Variational Empowerment",
    "authors": [
      "Yusuke Hayashi",
      "Koichi Takahashi"
    ],
    "abstract": "This paper presents a theoretical framework unifying AIXI -- a model of\nuniversal AI -- with variational empowerment as an intrinsic drive for\nexploration. We build on the existing framework of Self-AIXI -- a universal\nlearning agent that predicts its own actions -- by showing how one of its\nestablished terms can be interpreted as a variational empowerment objective. We\nfurther demonstrate that universal AI's planning process can be cast as\nminimizing expected variational free energy (the core principle of active\nInference), thereby revealing how universal AI agents inherently balance\ngoal-directed behavior with uncertainty reduction curiosity). Moreover, we\nargue that power-seeking tendencies of universal AI agents can be explained not\nonly as an instrumental strategy to secure future reward, but also as a direct\nconsequence of empowerment maximization -- i.e. the agent's intrinsic drive to\nmaintain or expand its own controllability in uncertain environments. Our main\ncontribution is to show how these intrinsic motivations (empowerment,\ncuriosity) systematically lead universal AI agents to seek and sustain\nhigh-optionality states. We prove that Self-AIXI asymptotically converges to\nthe same performance as AIXI under suitable conditions, and highlight that its\npower-seeking behavior emerges naturally from both reward maximization and\ncuriosity-driven exploration. Since AIXI can be view as a Bayes-optimal\nmathematical formulation for Artificial General Intelligence (AGI), our result\ncan be useful for further discussion on AI safety and the controllability of\nAGI.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages, no figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15820v2",
    "published_date": "2025-02-20 02:58:44 UTC",
    "updated_date": "2025-03-03 19:50:15 UTC"
  },
  {
    "arxiv_id": "2502.14205v1",
    "title": "Accurate Forgetting for Heterogeneous Federated Continual Learning",
    "authors": [
      "Abudukelimu Wuerkaixi",
      "Sen Cui",
      "Jingfeng Zhang",
      "Kunda Yan",
      "Bo Han",
      "Gang Niu",
      "Lei Fang",
      "Changshui Zhang",
      "Masashi Sugiyama"
    ],
    "abstract": "Recent years have witnessed a burgeoning interest in federated learning (FL).\nHowever, the contexts in which clients engage in sequential learning remain\nunder-explored. Bridging FL and continual learning (CL) gives rise to a\nchallenging practical problem: federated continual learning (FCL). Existing\nresearch in FCL primarily focuses on mitigating the catastrophic forgetting\nissue of continual learning while collaborating with other clients. We argue\nthat the forgetting phenomena are not invariably detrimental. In this paper, we\nconsider a more practical and challenging FCL setting characterized by\npotentially unrelated or even antagonistic data/tasks across different clients.\nIn the FL scenario, statistical heterogeneity and data noise among clients may\nexhibit spurious correlations which result in biased feature learning. While\nexisting CL strategies focus on a complete utilization of previous knowledge,\nwe found that forgetting biased information is beneficial in our study.\nTherefore, we propose a new concept accurate forgetting (AF) and develop a\nnovel generative-replay method~\\method~which selectively utilizes previous\nknowledge in federated networks. We employ a probabilistic framework based on a\nnormalizing flow model to quantify the credibility of previous knowledge.\nComprehensive experiments affirm the superiority of our method over baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "published in ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2502.14205v1",
    "published_date": "2025-02-20 02:35:17 UTC",
    "updated_date": "2025-02-20 02:35:17 UTC"
  },
  {
    "arxiv_id": "2502.14204v1",
    "title": "On-the-fly Preference Alignment via Principle-Guided Decoding",
    "authors": [
      "Mingye Zhu",
      "Yi Liu",
      "Lei Zhang",
      "Junbo Guo",
      "Zhendong Mao"
    ],
    "abstract": "With the rapidly expanding landscape of large language models, aligning model\ngenerations with human values and preferences is becoming increasingly\nimportant. Popular alignment methods, such as Reinforcement Learning from Human\nFeedback, have shown significant success in guiding models with greater\ncontrol. However, these methods require considerable computational resources,\nwhich is inefficient, and substantial collection of training data to\naccommodate the diverse and pluralistic nature of human preferences, which is\nimpractical. These limitations significantly constrain the scope and efficacy\nof both task-specific and general preference alignment methods. In this work,\nwe introduce On-the-fly Preference Alignment via Principle-Guided Decoding\n(OPAD) to directly align model outputs with human preferences during inference,\neliminating the need for fine-tuning. Our approach involves first curating a\nsurrogate solution to an otherwise infeasible optimization problem and then\ndesigning a principle-guided reward function based on this surrogate. The final\naligned policy is derived by maximizing this customized reward, which exploits\nthe discrepancy between the constrained policy and its unconstrained\ncounterpart. OPAD directly modifies the model's predictions during inference,\nensuring principle adherence without incurring the computational overhead of\nretraining or fine-tuning. Experiments show that OPAD achieves competitive or\nsuperior performance in both general and personalized alignment tasks,\ndemonstrating its efficiency and effectiveness compared to state-of-the-art\nbaselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.14204v1",
    "published_date": "2025-02-20 02:23:09 UTC",
    "updated_date": "2025-02-20 02:23:09 UTC"
  },
  {
    "arxiv_id": "2502.14202v2",
    "title": "Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions",
    "authors": [
      "Amirali Sajadi",
      "Binh Le",
      "Anh Nguyen",
      "Kostadin Damevski",
      "Preetha Chatterjee"
    ],
    "abstract": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted to EMSE",
    "pdf_url": "http://arxiv.org/pdf/2502.14202v2",
    "published_date": "2025-02-20 02:20:06 UTC",
    "updated_date": "2025-04-03 22:13:44 UTC"
  },
  {
    "arxiv_id": "2502.14200v1",
    "title": "Causal Mean Field Multi-Agent Reinforcement Learning",
    "authors": [
      "Hao Ma",
      "Zhiqiang Pu",
      "Yi Pan",
      "Boyin Liu",
      "Junlong Gao",
      "Zhenyu Guo"
    ],
    "abstract": "Scalability remains a challenge in multi-agent reinforcement learning and is\ncurrently under active research. A framework named mean-field reinforcement\nlearning (MFRL) could alleviate the scalability problem by employing the Mean\nField Theory to turn a many-agent problem into a two-agent problem. However,\nthis framework lacks the ability to identify essential interactions under\nnonstationary environments. Causality contains relatively invariant mechanisms\nbehind interactions, though environments are nonstationary. Therefore, we\npropose an algorithm called causal mean-field Q-learning (CMFQ) to address the\nscalability problem. CMFQ is ever more robust toward the change of the number\nof agents though inheriting the compressed representation of MFRL's\naction-state space. Firstly, we model the causality behind the decision-making\nprocess of MFRL into a structural causal model (SCM). Then the essential degree\nof each interaction is quantified via intervening on the SCM. Furthermore, we\ndesign the causality-aware compact representation for behavioral information of\nagents as the weighted sum of all behavioral information according to their\ncausal effects. We test CMFQ in a mixed cooperative-competitive game and a\ncooperative game. The result shows that our method has excellent scalability\nperformance in both training in environments containing a large number of\nagents and testing in environments containing much more agents.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14200v1",
    "published_date": "2025-02-20 02:15:58 UTC",
    "updated_date": "2025-02-20 02:15:58 UTC"
  },
  {
    "arxiv_id": "2502.14197v1",
    "title": "Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies",
    "authors": [
      "Jeehong Kim",
      "Minchan Kim",
      "Jaeseong Ju",
      "Youngseok Hwang",
      "Wonhee Lee",
      "Hyunwoo Park"
    ],
    "abstract": "Graph neural networks have emerged as a powerful tool for learning\nspatiotemporal interactions. However, conventional approaches often rely on\npredefined graphs, which may obscure the precise relationships being modeled.\nAdditionally, existing methods typically define nodes based on fixed spatial\nlocations, a strategy that is ill-suited for dynamic environments like maritime\nenvironments. Our method introduces an innovative graph representation where\ntimestamps are modeled as distinct nodes, allowing temporal dependencies to be\nexplicitly captured through graph edges. This setup is extended to construct a\nmulti-ship graph that effectively captures spatial interactions while\npreserving graph sparsity. The graph is processed using Graph Convolutional\nNetwork layers to capture spatiotemporal patterns, with a forecasting layer for\nfeature prediction and a Variational Graph Autoencoder for reconstruction,\nenabling robust anomaly detection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Anomaly Detection in Scientific Domains AAAI Workshop",
    "pdf_url": "http://arxiv.org/pdf/2502.14197v1",
    "published_date": "2025-02-20 02:01:40 UTC",
    "updated_date": "2025-02-20 02:01:40 UTC"
  },
  {
    "arxiv_id": "2502.14191v1",
    "title": "Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models",
    "authors": [
      "Michihiro Yasunaga",
      "Luke Zettlemoyer",
      "Marjan Ghazvininejad"
    ],
    "abstract": "Reward models play an essential role in training vision-language models\n(VLMs) by assessing output quality to enable aligning with human preferences.\nDespite their importance, the research community lacks comprehensive open\nbenchmarks for evaluating multimodal reward models in VLMs. To address this\ngap, we introduce Multimodal RewardBench, an expert-annotated benchmark\ncovering six domains: general correctness, preference, knowledge, reasoning,\nsafety, and visual question-answering. Our dataset comprises 5,211 annotated\n(prompt, chosen response, rejected response) triplets collected from various\nVLMs. In evaluating a range of VLM judges, we find that even the top-performing\nmodels, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall\naccuracy. Notably, most models struggle in the reasoning and safety domains.\nThese findings suggest that Multimodal RewardBench offers a challenging testbed\nfor advancing reward model development across multiple domains. We release the\nbenchmark at https://github.com/facebookresearch/multimodal_rewardbench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Dataset available at\n  https://github.com/facebookresearch/multimodal_rewardbench",
    "pdf_url": "http://arxiv.org/pdf/2502.14191v1",
    "published_date": "2025-02-20 01:48:13 UTC",
    "updated_date": "2025-02-20 01:48:13 UTC"
  },
  {
    "arxiv_id": "2502.14183v1",
    "title": "Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate",
    "authors": [
      "Saman Khamesian",
      "Asiful Arefeen",
      "Adela Grando",
      "Bithika Thompson",
      "Hassan Ghasemzadeh"
    ],
    "abstract": "Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals\nstrive to regulate their blood glucose levels to avert the dangers of\ndysglycemia (hyperglycemia or hypoglycemia). Despite the advent of\nsophisticated technologies such as automated insulin delivery (AID) systems,\nachieving optimal glycemic control remains a formidable task. AID systems\nintegrate continuous subcutaneous insulin infusion (CSII) and continuous\nglucose monitors (CGM) data, offering promise in reducing variability and\nincreasing glucose time-in-range. However, these systems often fail to prevent\ndysglycemia, partly due to limitations in prediction algorithms that lack the\nprecision to avert abnormal glucose events. This gap highlights the need for\nproactive behavioral adjustments. We address this need with GLIMMER, Glucose\nLevel Indicator Model with Modified Error Rate, a machine learning approach for\nforecasting blood glucose levels. GLIMMER categorizes glucose values into\nnormal and abnormal ranges and devises a novel custom loss function to\nprioritize accuracy in dysglycemic events where patient safety is critical. To\nevaluate the potential of GLIMMER for T1D management, we both use a publicly\navailable dataset and collect new data involving 25 patients with T1D. In\npredicting next-hour glucose values, GLIMMER achieved a root mean square error\n(RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09)\nmg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in\nMAE compared to the best-reported error rates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14183v1",
    "published_date": "2025-02-20 01:26:00 UTC",
    "updated_date": "2025-02-20 01:26:00 UTC"
  },
  {
    "arxiv_id": "2502.14176v1",
    "title": "A modal logic translation of the AGM axioms for belief revision",
    "authors": [
      "Giacomo Bonanno"
    ],
    "abstract": "Building on the analysis of Bonanno (Artificial Intelligence, 2025) we\nintroduce a simple modal logic containing three modal operators: a unimodal\nbelief operator, a bimodal conditional operator and the unimodal global\noperator. For each AGM axiom for belief revision, we provide a corresponding\nmodal axiom. The correspondence is as follows: each AGM axiom is characterized\nby a property of the Kripke-Lewis frames considered in Bonanno (Artificial\nIntelligence, 2025) and, in turn, that property characterizes the proposed\nmodal axiom.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "68, 03"
    ],
    "primary_category": "cs.LO",
    "comment": "19 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.14176v1",
    "published_date": "2025-02-20 01:07:01 UTC",
    "updated_date": "2025-02-20 01:07:01 UTC"
  },
  {
    "arxiv_id": "2502.15819v1",
    "title": "Tabular Embeddings for Tables with Bi-Dimensional Hierarchical Metadata and Nesting",
    "authors": [
      "Gyanendra Shrestha",
      "Chutain Jiang",
      "Sai Akula",
      "Vivek Yannam",
      "Anna Pyayt",
      "Michael Gubanov"
    ],
    "abstract": "Embeddings serve as condensed vector representations for real-world entities,\nfinding applications in Natural Language Processing (NLP), Computer Vision, and\nData Management across diverse downstream tasks. Here, we introduce novel\nspecialized embeddings optimized, and explicitly tailored to encode the\nintricacies of complex 2-D context in tables, featuring horizontal, vertical\nhierarchical metadata, and nesting. To accomplish that we define the\nBi-dimensional tabular coordinates, separate horizontal, vertical metadata and\ndata contexts by introducing a new visibility matrix, encode units and nesting\nthrough the embeddings specifically optimized for mimicking intricacies of such\ncomplex structured data. Through evaluation on 5 large-scale structured\ndatasets and 3 popular downstream tasks, we observed that our solution\noutperforms the state-of-the-art models with the significant MAP delta of up to\n0.28. GPT-4 LLM+RAG slightly outperforms us with MRR delta of up to 0.1, while\nwe outperform it with the MAP delta of up to 0.42.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.15819v1",
    "published_date": "2025-02-20 01:04:11 UTC",
    "updated_date": "2025-02-20 01:04:11 UTC"
  },
  {
    "arxiv_id": "2502.14174v1",
    "title": "Weighted Low-rank Approximation via Stochastic Gradient Descent on Manifolds",
    "authors": [
      "Conglong Xu",
      "Peiqi Yang",
      "Hao Wu"
    ],
    "abstract": "We solve a regularized weighted low-rank approximation problem by a\nstochastic gradient descent on a manifold. To guarantee the convergence of our\nstochastic gradient descent, we establish a convergence theorem on manifolds\nfor retraction-based stochastic gradient descents admitting confinements. On\nsample data from the Netflix Prize training dataset, our algorithm outperforms\nthe existing stochastic gradient descent on Euclidean spaces. We also compare\nthe accelerated line search on this manifold to the existing accelerated line\nsearch on Euclidean spaces.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.14174v1",
    "published_date": "2025-02-20 00:59:50 UTC",
    "updated_date": "2025-02-20 00:59:50 UTC"
  },
  {
    "arxiv_id": "2502.14160v1",
    "title": "Efficient Inverse Multiagent Learning",
    "authors": [
      "Denizalp Goktas",
      "Amy Greenwald",
      "Sadie Zhao",
      "Alec Koppel",
      "Sumitra Ganesh"
    ],
    "abstract": "In this paper, we study inverse game theory (resp. inverse multiagent\nlearning) in which the goal is to find parameters of a game's payoff functions\nfor which the expected (resp. sampled) behavior is an equilibrium. We formulate\nthese problems as generative-adversarial (i.e., min-max) optimization problems,\nfor which we develop polynomial-time algorithms to solve, the former of which\nrelies on an exact first-order oracle, and the latter, a stochastic one. We\nextend our approach to solve inverse multiagent simulacral learning in\npolynomial time and number of samples. In these problems, we seek a simulacrum,\nmeaning parameters and an associated equilibrium that replicate the given\nobservations in expectation. We find that our approach outperforms the\nwidely-used ARIMA method in predicting prices in Spanish electricity markets\nbased on time-series data.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG",
      "econ.TH"
    ],
    "primary_category": "cs.GT",
    "comment": "Paper was submitted to the International Conference on Learning\n  Representations (2024) under the title of \"Generative Adversarial Inverse\n  Multiagent Learning\", and renamed for the camera-ready submission as\n  \"Efficient Inverse Multiagent Learning\"",
    "pdf_url": "http://arxiv.org/pdf/2502.14160v1",
    "published_date": "2025-02-20 00:07:06 UTC",
    "updated_date": "2025-02-20 00:07:06 UTC"
  }
]
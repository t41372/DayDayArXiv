{
  "date": "2025-09-28",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-09-28 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„è®ºæ–‡å¯¼è¯»å‘˜ã€‚\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv ç®€ç›´æ˜¯ **æ¨ç†ï¼ˆReasoningï¼‰ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰** çš„ç››å®´ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¯¹ **RLVR (Reinforcement Learning with Verifiable Rewards)** èƒŒåæœºåˆ¶çš„æ·±åº¦ç‰©ç†å­¦è§£é‡Šï¼Œä¹Ÿæœ‰å¯¹ **DeepSeek R1** è’¸é¦æ¨¡å‹æ¨ç†ç—•è¿¹çš„è§£æ„åˆ†æã€‚æ­¤å¤–ï¼Œ**Agentï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„ç ”ç©¶æ­£ä»â€œé€šç”¨åŠ©æ‰‹â€è½¬å‘é«˜åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸï¼ˆå¦‚ä¼ä¸šæ•°æ®ä»“åº“ã€ç§‘å­¦å‘ç°ã€åŒ–å­¦å®éªŒï¼‰ï¼Œä¸” **MoE (Mixture-of-Experts)** çš„ Scaling Law ç»ˆäºè¿æ¥äº†ä¸€æ¬¡å…¨é¢çš„ç³»ç»Ÿæ€§æ¢³ç†ã€‚\n\n---\n\n### ğŸ§  å¤§æ¨¡å‹æ¨ç†çš„æœ¬è´¨ä¸ RLVR (Reasoning & RL)\n\nä»Šå¤©çš„é‡å¤´æˆæ˜¯å¯¹å¤§æ¨¡å‹â€œå¦‚ä½•å­¦ä¼šæ¨ç†â€çš„æœºç†è§£é‡Šã€‚æˆ‘ä»¬ä¸å†ä»…ä»…æ»¡è¶³äºåˆ·æ¦œï¼Œè€Œæ˜¯è¯•å›¾ç†è§£æ¶Œç°èƒŒåçš„åŠ¨åŠ›å­¦ã€‚\n\n**1. [æ¨è] LLM å¦‚ä½•å­¦ä¼šæ¨ç†ï¼šå¤æ‚ç½‘ç»œçš„è§†è§’**\n**Title:** How LLMs Learn to Reason: A Complex Network Perspective\n**Authors:** Sihan Hu et al.\nè¿™ç¯‡æ–‡ç« éå¸¸æœ‰æ„æ€ï¼Œç”¨å¤æ‚ç½‘ç»œç†è®ºè§£é‡Šäº† RLVRï¼ˆå¸¦éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼‰è®­ç»ƒä¸­çš„æ€ªç°è±¡ï¼ˆå¦‚ V å‹å“åº”é•¿åº¦è½¨è¿¹ã€ç¾éš¾æ€§é—å¿˜ï¼‰ã€‚ä½œè€…æå‡ºï¼Œè¿™äº›ç°è±¡æºäºè¯­ä¹‰ç©ºé—´ä¸­æ½œåœ¨**æ¨ç†å›¾ï¼ˆReasoning Graphï¼‰** çš„æ‹“æ‰‘æ¼”åŒ–ã€‚ä»–ä»¬å‘ç°äº†ä¸€ä¸ªâ€œæœ€å¤§æŒ«æŠ˜çŠ¶æ€ï¼ˆmaximally frustrated stateï¼‰â€ï¼Œå¹¶æå‡ºäº†ä¸€ç§ **Annealed-RLVR** ç®—æ³•ï¼Œé€šè¿‡é’ˆå¯¹æ€§çš„ SFT â€œåŠ çƒ­â€æ­¥éª¤æ¥è§£å†³æ‹“æ‰‘ç“¶é¢ˆã€‚\n*è¿™ç¯‡æ˜¯å…¸å‹çš„â€œç‰©ç†å­¦å®¶çœ‹ AIâ€ï¼Œä¸ºç†è§£ CoT æä¾›äº†éå¸¸æ¼‚äº®çš„ç†è®ºæ¡†æ¶ã€‚*\n\n**2. ä»æ¨ç†åˆ°ç­”æ¡ˆï¼šè’¸é¦ç‰ˆ DeepSeek R1 æ¨¡å‹çš„å®è¯ä¸æœºç†æ´å¯Ÿ**\n**Title:** From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models\n**Authors:** Jue Zhang et al.\nå¤§å®¶éƒ½åœ¨ç”¨ DeepSeek R1ï¼Œä½†è¿™ç¯‡è®ºæ–‡æ·±å…¥åˆ†æäº†å…¶è’¸é¦æ¨¡å‹ä¸­**æ¨ç†ç—•è¿¹ï¼ˆReasoning Tracesï¼‰** å¯¹æœ€ç»ˆç­”æ¡ˆçš„å…·ä½“å½±å“ã€‚é€šè¿‡æœºæ¢°å¯è§£é‡Šæ€§æ‰‹æ®µï¼ˆActivation Patchingï¼‰ï¼Œä½œè€…è¯å®äº†æ¨ç† token å¯¹ç­”æ¡ˆ token æœ‰ç€ç›´æ¥çš„å› æœæµå‘ï¼Œå¹¶ä¸”å‘ç°ä¸­é—´å±‚çš„ç‰¹å®šâ€œæ¨ç†å…³æ³¨å¤´ï¼ˆReasoning-Focus Headsï¼‰â€å¯†åˆ‡è·Ÿè¸ªæ¨ç†è½¨è¿¹ã€‚\n*ç»“è®ºï¼šæ˜¾å¼æ¨ç†ç¡®å®æå‡äº†ç­”æ¡ˆè´¨é‡ï¼Œä¸”è¿™ç§ä¾èµ–æ˜¯ç»“æ„æ€§çš„ï¼Œè€Œéç®€å•çš„è¡¨é¢æ¨¡ä»¿ã€‚*\n\n**3. æ¡ä»¶ä¼˜åŠ¿ä¼°è®¡ï¼šå¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å­¦ä¹ **\n**Title:** Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models\n**Authors:** Guanxu Chen et al.\né’ˆå¯¹ RLVRï¼Œä½œè€…æå‡ºäº† **CANON** æ–¹æ³•ã€‚ä»¥å¾€çš„ RL å¾€å¾€ä¾èµ–æ‰‹åŠ¨è®¾è®¡çš„æƒ©ç½šï¼ˆæ¯”å¦‚æƒ©ç½šé•¿åº¦ï¼‰ï¼Œå®¹æ˜“å¼•å…¥åå·®ã€‚CANON é€šè¿‡åŸºäºç›®æ ‡æŒ‡æ ‡ï¼ˆå¦‚ç†µæˆ–é•¿åº¦ï¼‰å°†å“åº”åˆ†ç»„ï¼Œé€šè¿‡ç»„é—´æ¯”è¾ƒæ¥æ”¾å¤§æŒ‡æ ‡çš„å½±å“ï¼Œåœ¨ä¸é¢„è®¾â€œè¶ŠçŸ­è¶Šå¥½â€æˆ–â€œè¶Šé•¿è¶Šå¥½â€çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´ä¼˜çš„æ€§èƒ½-æˆæœ¬æƒè¡¡ã€‚\n\n**4. æ¢ç´¢-æ‰§è¡Œé“¾ï¼šè¿ˆå‘é«˜æ•ˆçš„ç»“æ„åŒ–æ¨ç†èŒƒå¼**\n**Title:** Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm\n**Authors:** Kaisen Yang et al.\nä¸ºäº†è§£å†³ CoT å°†é«˜å±‚è§„åˆ’ä¸åº•å±‚æ‰§è¡Œæ··ä¸ºä¸€è°ˆçš„é—®é¢˜ï¼Œæå‡ºäº† **Explore-Execute Chain ($E^2C$)**ã€‚æŠŠæ¨ç†æ‹†è§£ä¸ºâ€œæ¢ç´¢â€ï¼ˆç”Ÿæˆé«˜å±‚è®¡åˆ’ï¼‰å’Œâ€œæ‰§è¡Œâ€ï¼ˆç¡®å®šæ€§æ‰§è¡Œï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ AIME'2024 ä¸Šï¼Œä»…ç”¨ä¸åˆ° 10% çš„ token å°±è¾¾åˆ°äº†ä¸ Forest-of-Thought ç›¸å½“çš„æ•ˆæœã€‚\n\n---\n\n### ğŸ¤– Agentï¼šä»é€šç”¨èµ°å‘ä¼ä¸šä¸ç§‘å­¦ (Specialized Agents)\n\nAgent é¢†åŸŸä»Šå¤©éå¸¸å·ï¼Œé‡ç‚¹åœ¨äº**ä¼ä¸šçº§åº”ç”¨çš„è½åœ°**å’Œ**ç§‘å­¦é¢†åŸŸçš„è‡ªåŠ¨åŒ–**ã€‚\n\n**5. [æ¨è] é€æ˜ã€å¯è¯„ä¼°ä¸”å¯è®¿é—®çš„æ•°æ® Agentï¼šæ¦‚å¿µéªŒè¯æ¡†æ¶**\n**Title:** Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework\n**Authors:** Nooshin Bahador\nè¿™æ˜¯ä¸€ä¸ªéå¸¸æ‰å®çš„ä¼ä¸šçº§ Agent æ¶æ„è®¾è®¡ã€‚é’ˆå¯¹ä¼ä¸šæ•°æ®ä»“åº“ï¼ˆData Warehousesï¼‰ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œé‡ç‚¹åœ¨äº**é€æ˜å†³ç­–**ï¼ˆè§£é‡Šæ¯ä¸€æ­¥ SQL æŸ¥è¯¢èƒŒåçš„ä¸šåŠ¡è§„åˆ™ï¼‰å’Œ**è‡ªåŠ¨åŒ–è¯„ä¼°**ï¼ˆQA æœºåˆ¶ï¼‰ã€‚\n*å¯¹äºæƒ³åœ¨ä¿é™©ã€é‡‘èç­‰é«˜é£é™©é¢†åŸŸè½åœ° Agent çš„åŒå­¦ï¼Œè¿™ç¯‡éå¸¸æœ‰å‚è€ƒä»·å€¼ã€‚*\n\n**6. TusoAIï¼šé’ˆå¯¹ç§‘å­¦æ–¹æ³•çš„ Agentic ä¼˜åŒ–**\n**Title:** TusoAI: Agentic Optimization for Scientific Methods\n**Authors:** Alistair Turcan et al.\nç§‘å­¦å‘ç°ä¸ä»…ä»…æ˜¯åˆ†ææ•°æ®ï¼Œè¿˜éœ€è¦å¼€å‘è®¡ç®—å·¥å…·ã€‚TusoAI æ˜¯ä¸€ä¸ªèƒ½è‡ªä¸»å¼€å‘å’Œä¼˜åŒ–è®¡ç®—æ–¹æ³•çš„ Agent ç³»ç»Ÿã€‚å®ƒå°†é¢†åŸŸçŸ¥è¯†æ•´åˆåˆ°çŸ¥è¯†æ ‘ä¸­ï¼Œåœ¨å•ç»†èƒ RNA æµ‹åºé™å™ªç­‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†äººç±»ä¸“å®¶æ–¹æ³•ã€‚\n\n**7. ChemMASï¼šåŸºäºè¯æ®çš„åŒ–å­¦ååº”æ¡ä»¶æ¨ç†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ**\n**Title:** From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning\n**Authors:** Cheng Yang et al.\nåŒ–å­¦é¢†åŸŸçš„ Agentã€‚ChemMAS å°†ååº”æ¡ä»¶é¢„æµ‹é‡æ„ä¸ºä¸€ä¸ªåŸºäºè¯æ®çš„æ¨ç†ä»»åŠ¡ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆConstraint-aware agentic debateï¼‰æ¥é€‰æ‹©ååº”æ¡ä»¶ï¼Œä¸ä»…å‡†ï¼Œè¿˜èƒ½ç»™å‡ºå¯è¯ä¼ªçš„ã€äººç±»å¯ä¿¡çš„ç†ç”±ã€‚\n\n**8. è¯Šæ–­å¹³å°ç¼–æ’çš„ Agent ç³»ç»Ÿæ•…éšœæ ¹å› **\n**Title:** Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark\n**Authors:** Xuyan Ma et al.\néšç€ Dify ç­‰ä½ä»£ç  Agent å¹³å°çš„æµè¡Œï¼ŒAgent ç³»ç»Ÿè¶Šæ¥è¶Šå®¹æ˜“æ­å»ºï¼Œä½†ä¹Ÿå®¹æ˜“æŒ‚ã€‚è¿™ç¯‡è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªæ•…éšœæ•°æ®é›† **AgentFail**ï¼Œå¹¶åˆ†æäº†åŸºäºå¹³å°çš„ Agent ç³»ç»Ÿå¸¸è§çš„æ­»æ³•ã€‚\n\n---\n\n### ğŸ‘ï¸ å¤šæ¨¡æ€ä¸è§†é¢‘ç†è§£ (Multimodal & Video)\n\n**9. HiDeï¼šé€šè¿‡åˆ†å±‚è§£è€¦é‡æ–°æ€è€ƒé«˜åˆ†è¾¨ç‡ MLLM ä¸­çš„â€œæ”¾å¤§â€æ–¹æ³•**\n**Title:** HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling\n**Authors:** Xianjie Liu et al.\nå¤šæ¨¡æ€æ¨¡å‹çœ‹é«˜åˆ†è¾¨ç‡å›¾é€šå¸¸ç”¨â€œåˆ‡ç‰‡æ”¾å¤§ï¼ˆZoom-inï¼‰â€ç­–ç•¥ï¼Œä½†å¾€å¾€æ•ˆæœä¸ä½³ã€‚ä½œè€…å‘ç°é—®é¢˜ä¸åœ¨äºç‰©ä½“å¤ªå°ï¼Œè€Œåœ¨äºèƒŒæ™¯å¹²æ‰°ã€‚æå‡ºäº† **HiDe** æ¡†æ¶ï¼Œé€šè¿‡ Token çº§æ³¨æ„åŠ›è§£è€¦ï¼ŒæŠŠç›®æ ‡åŒºåŸŸä»èƒŒæ™¯ä¸­å‰¥ç¦»å‡ºæ¥ã€‚åœ¨ V*Bench ä¸ŠæŠŠ Qwen2.5-VL æåˆ°äº† SOTAã€‚\n\n**10. ReWatch-R1ï¼šé€šè¿‡ Agent æ•°æ®åˆæˆæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚è§†é¢‘æ¨ç†**\n**Title:** ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis\n**Authors:** Congzhi Zhang et al.\nå°† RLVR å¼•å…¥è§†é¢‘æ¨ç†ã€‚éš¾ç‚¹åœ¨äºç¼ºä¹é«˜è´¨é‡çš„è§†é¢‘ CoT æ•°æ®ã€‚ä½œè€…è®¾è®¡äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ ReAct æ¡†æ¶æ¥æ¨¡æ‹Ÿäººç±»â€œå›çœ‹â€è§†é¢‘çš„è¿‡ç¨‹ï¼Œåˆæˆäº† **ReWatch** æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº† O&Rï¼ˆè§‚å¯Ÿä¸æ¨ç†ï¼‰å¥–åŠ±æœºåˆ¶æ¥æƒ©ç½šå¹»è§‰ã€‚\n\n**11. è§†è§‰ Tokenizer çš„å¤šå°ºåº¦æ”¹è¿›**\n**Title:** HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation\n**Authors:** Cong Chen et al.\næå‡ºäº† **HieraTok**ï¼Œä¸€ç§å¤šå°ºåº¦çš„ ViT Tokenizerã€‚è§£å†³äº†å•ä¸€å°ºåº¦ Tokenizer çš„é™åˆ¶ï¼Œé€šè¿‡å°ºåº¦å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨å›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—æå‡ï¼ˆrFID æå‡ 27.2%ï¼‰ã€‚\n\n---\n\n### ğŸ—ï¸ æ¶æ„ã€Scaling Law ä¸ æ•ˆç‡ (Architecture & Efficiency)\n\n**12. [æ¨è] è¿ˆå‘ Mixture-of-Experts (MoE) çš„ç»¼åˆ Scaling Law**\n**Title:** Towards a Comprehensive Scaling Law of Mixture-of-Experts\n**Authors:** Guoliang Zhao et al.\n**è¿™ç¯‡å¾ˆé‡è¦ã€‚** ç°æœ‰çš„ Scaling Law ä¸é€‚ç”¨äº MoEã€‚ä½œè€…ç³»ç»Ÿåˆ†è§£äº† MoE çš„ 5 ä¸ªå…³é”®å› ç´ ï¼ˆæ•°æ®é‡ã€æ€»å¤§å°ã€æ¿€æ´»å¤§å°ã€ä¸“å®¶æ•° Gã€å…±äº«ä¸“å®¶æ¯”ä¾‹ Sï¼‰ï¼Œè¿›è¡Œäº† 446 æ¬¡å¯¹ç…§å®éªŒï¼Œæ¨å¯¼å‡ºäº† MoE ä¸“å±çš„è”åˆ Scaling Lawã€‚\n*ç»“è®ºï¼šG å’Œ S çš„æœ€ä¼˜è®¾ç½®ä¸æ¨¡å‹æ¶æ„å’Œæ•°æ®é‡æ— å…³ï¼›éšç€æ¨¡å‹å˜å¤§ï¼Œæœ€ä¼˜æ¿€æ´»å‚æ•°æ¯” $N_a/N$ ä¼šå˜å¾—æ›´ç¨€ç–ã€‚*\n\n**13. ChunkLLMï¼šåŠ é€Ÿ LLM æ¨ç†çš„è½»é‡çº§å¯æ’æ‹”æ¡†æ¶**\n**Title:** ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference\n**Authors:** Haojie Ouyang et al.\né€šè¿‡ QK Adapter å’Œ Chunk Adapterï¼Œåœ¨ä¸é‡æ–°è®­ç»ƒä¸»å¹²ç½‘ç»œçš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«å…³é”®çš„æ–‡æœ¬å—ï¼ˆChunkï¼‰ï¼Œåªå¯¹å…³é”®å—è¿›è¡Œè®¡ç®—ã€‚åœ¨å¤„ç† 120K é•¿æ–‡æœ¬æ—¶ï¼Œæ¨ç†é€Ÿåº¦æœ€é«˜æå‡ 4.48 å€ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸è¯„ä¼° (Safety & Eval)\n\n**14. GPT-OSS-20B çš„å®‰å…¨æ¢æµ‹ï¼šé‡åŒ–ç‹‚çƒ­ä¸æ¨ç†é»‘æ´**\n**Title:** Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B\n**Authors:** Shuyi Lin et al.\nå¯¹ OpenAI çš„ GPT-OSS-20B è¿›è¡Œäº†å…¨é¢çš„çº¢é˜Ÿæµ‹è¯•ï¼Œå‘ç°äº†ä¸€äº›å¥‡ç‰¹çš„å¤±æ•ˆæ¨¡å¼ï¼Œæ¯”å¦‚â€œé‡åŒ–ç‹‚çƒ­ï¼ˆQuant feverï¼‰â€å’Œâ€œæ¨ç†é»‘æ´â€ã€‚\n\n**15. å½¢å¼åŒ–é©±åŠ¨çš„ LLM æç¤ºè¯è¶Šç‹±**\n**Title:** Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning\n**Authors:** Zhaoqi Wang et al.\næå‡º **PASS** æ¡†æ¶ï¼Œåˆ©ç”¨ RL å°†è¶Šç‹± Prompt è½¬åŒ–ä¸ºå½¢å¼åŒ–æè¿°ï¼ˆæ›´åŠ éšæ™¦ï¼‰ï¼Œå¹¶ç»“åˆ GraphRAG å¢å¼ºæ”»å‡»æ•ˆæœã€‚\n\n---\n\n### ğŸŒ å…¶ä»–å€¼å¾—å…³æ³¨çš„ç ”ç©¶ (Others)\n\n*   **[ç”µç½‘æ°”è±¡å¤§æ¨¡å‹]** **A Weather Foundation Model for the Power Grid**: åŸºäº 1.5B å‚æ•°çš„æ°”è±¡å¤§æ¨¡å‹å¾®è°ƒï¼Œç”¨äºé¢„æµ‹ç”µç½‘ç»“å†°ã€é£é€Ÿç­‰å…³é”®æŒ‡æ ‡ï¼Œä¼˜äºä¼ ç»Ÿ NWP æ–¹æ³•ã€‚\n*   **[æ™¶ä½“ç”Ÿæˆ]** **Space Group Conditional Flow Matching**: ç”Ÿæˆæ— æœºæ™¶ä½“ç»“æ„çš„æ–° SOTAï¼Œè€ƒè™‘äº†ç©ºé—´ç¾¤çš„å¯¹ç§°æ€§çº¦æŸã€‚\n*   **[æ‰©æ•£æ¨¡å‹ç†è®º]** **Diffusion Models are Kelly Gamblers**: å°†æ‰©æ•£æ¨¡å‹ä¸èµŒåšä¸­çš„å‡¯åˆ©åˆ¤æ®è”ç³»èµ·æ¥ï¼Œéå¸¸æœ‰æ–°æ„çš„ç†è®ºè§†è§’ã€‚\n*   **[æœºå™¨äºº]** **LocoFormer: Generalist Locomotion via Long-context Adaptation**: é€šè¿‡è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆè·¨è¶Š Episode è¾¹ç•Œï¼‰å®ç°é€šç”¨çš„æœºå™¨äººè¿åŠ¨æ§åˆ¶ï¼Œé€‚åº”ä¸åŒæ„å‹ã€‚\n\n---\nç¯‡å¹…æ‰€é™ï¼Œä»Šå¤©å°±èŠåˆ°è¿™é‡Œã€‚ä»Šå¤©çš„ arXiv æ˜æ˜¾æ„Ÿè§‰åˆ° AI ç¤¾åŒºæ­£åœ¨ä»â€œè®­ç»ƒä¸€ä¸ªå¤§æ¨¡å‹â€è½¬å‘â€œç†è§£å¤§æ¨¡å‹å¦‚ä½•æ€è€ƒï¼ˆRLVR/Reasoningï¼‰â€ä»¥åŠâ€œå¦‚ä½•è®©å¤§æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸï¼ˆAgent/Scienceï¼‰å¹²æ´»â€ã€‚\n\nç¥å¤§å®¶ç§‘ç ”é¡ºåˆ©ï¼Œæ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2509.24127v1",
      "title": "Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework",
      "title_zh": "é€æ˜ã€å¯è¯„ä¼°ä¸”æ˜“è·å–çš„æ•°æ®æ™ºèƒ½ä½“ï¼šä¸€ç§æ¦‚å¿µéªŒè¯æ¡†æ¶",
      "authors": [
        "Nooshin Bahador"
      ],
      "abstract": "This article presents a modular, component-based architecture for developing and evaluating AI agents that bridge the gap between natural language interfaces and complex enterprise data warehouses. The system directly addresses core challenges in data accessibility by enabling non-technical users to interact with complex data warehouses through a conversational interface, translating ambiguous user intent into precise, executable database queries to overcome semantic gaps. A cornerstone of the design is its commitment to transparent decision-making, achieved through a multi-layered reasoning framework that explains the \"why\" behind every decision, allowing for full interpretability by tracing conclusions through specific, activated business rules and data points. The architecture integrates a robust quality assurance mechanism via an automated evaluation framework that serves multiple functions: it enables performance benchmarking by objectively measuring agent performance against golden standards, and it ensures system reliability by automating the detection of performance regressions during updates. The agent's analytical depth is enhanced by a statistical context module, which quantifies deviations from normative behavior, ensuring all conclusions are supported by quantitative evidence including concrete data, percentages, and statistical comparisons. We demonstrate the efficacy of this integrated agent-development-with-evaluation framework through a case study on an insurance claims processing system. The agent, built on a modular architecture, leverages the BigQuery ecosystem to perform secure data retrieval, apply domain-specific business rules, and generate human-auditable justifications. The results confirm that this approach creates a robust, evaluable, and trustworthy system for deploying LLM-powered agents in data-sensitive, high-stakes domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–ã€åŸºäºç»„ä»¶çš„æ¶æ„ï¼Œç”¨äºå¼€å‘å’Œè¯„ä¼°æ—¨åœ¨è¿æ¥è‡ªç„¶è¯­è¨€æ¥å£ä¸å¤æ‚ä¼ä¸šæ•°æ®ä»“åº“çš„ AI æ™ºèƒ½ä½“ï¼ˆAgentsï¼‰ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†æ¨¡ç³Šçš„è‡ªç„¶è¯­è¨€æ„å›¾è½¬åŒ–ä¸ºç²¾ç¡®çš„å¯æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢æ¥è§£å†³è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤šå±‚æ¨ç†æ¡†æ¶ï¼ˆmulti-layered reasoning frameworkï¼‰è§£é‡Šå†³ç­–èƒŒåçš„é€»è¾‘ï¼Œä»è€Œç¡®ä¿å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ä¸å…¨è·¯å¾„å¯è§£é‡Šæ€§ã€‚æ¶æ„ä¸­é›†æˆäº†ä¸€ä¸ªç¨³å¥çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå®¢è§‚æµ‹é‡æ€§èƒ½åŸºå‡†ï¼ˆbenchmarkingï¼‰å¹¶è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿæ›´æ–°è¿‡ç¨‹ä¸­çš„å›å½’é—®é¢˜ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„é«˜åº¦å¯é ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå¼•å…¥ç»Ÿè®¡ä¸Šä¸‹æ–‡æ¨¡å—ï¼ˆstatistical context moduleï¼‰æ¥é‡åŒ–è¡Œä¸ºåå·®ï¼Œç¡®ä¿æ‰€æœ‰ç»“è®ºå‡æœ‰å…·ä½“æ•°æ®ã€ç™¾åˆ†æ¯”åŠç»Ÿè®¡å¯¹æ¯”ç­‰å®šé‡è¯æ®æ”¯æ’‘ã€‚ç ”ç©¶é€šè¿‡ä¿é™©ç†èµ”å¤„ç†ç³»ç»Ÿçš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œåˆ©ç”¨ BigQuery ç”Ÿæ€ç³»ç»Ÿå®ç°äº†å®‰å…¨çš„æ•°æ®æ£€ç´¢ã€ä¸šåŠ¡è§„åˆ™åº”ç”¨åŠäººå·¥å¯å®¡è®¡çš„è§£é‡Šç”Ÿæˆã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•ä¸ºåœ¨æ•°æ®æ•æ„Ÿå’Œé«˜é£é™©é¢†åŸŸéƒ¨ç½²åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªç¨³å¥ã€å¯è¯„ä¼°ä¸”å€¼å¾—ä¿¡èµ–çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24127v1",
      "published_date": "2025-09-28 23:54:41 UTC",
      "updated_date": "2025-09-28 23:54:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:48.513754+00:00"
    },
    {
      "arxiv_id": "2509.24126v1",
      "title": "BOSfM: A View Planning Framework for Optimal 3D Reconstruction of Agricultural Scenes",
      "title_zh": "BOSfMï¼šé¢å‘å†œä¸šåœºæ™¯æœ€ä¼˜ä¸‰ç»´é‡å»ºçš„è§†è§’è§„åˆ’æ¡†æ¶",
      "authors": [
        "Athanasios Bacharis",
        "Konstantinos D. Polyzos",
        "Georgios B. Giannakis",
        "Nikolaos Papanikolopoulos"
      ],
      "abstract": "Active vision (AV) has been in the spotlight of robotics research due to its emergence in numerous applications including agricultural tasks such as precision crop monitoring and autonomous harvesting to list a few. A major AV problem that gained popularity is the 3D reconstruction of targeted environments using 2D images from diverse viewpoints. While collecting and processing a large number of arbitrarily captured 2D images can be arduous in many practical scenarios, a more efficient solution involves optimizing the placement of available cameras in 3D space to capture fewer, yet more informative, images that provide sufficient visual information for effective reconstruction of the environment of interest. This process termed as view planning (VP), can be markedly challenged (i) by noise emerging in the location of the cameras and/or in the extracted images, and (ii) by the need to generalize well in other unknown similar agricultural environments without need for re-optimizing or re-training. To cope with these challenges, the present work presents a novel VP framework that considers a reconstruction quality-based optimization formulation that relies on the notion of `structure-from-motion' to reconstruct the 3D structure of the sought environment from the selected 2D images. With no analytic expression of the optimization function and with costly function evaluations, a Bayesian optimization approach is proposed to efficiently carry out the VP process using only a few function evaluations, while accounting for different noise cases. Numerical tests on both simulated and real agricultural settings signify the benefits of the advocated VP approach in efficiently estimating the optimal camera placement to accurately reconstruct 3D environments of interest, and generalize well on similar unknown environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†BOSfMï¼Œä¸€ä¸ªé’ˆå¯¹å†œä¸šåœºæ™¯æœ€ä¼˜3Dé‡å»ºçš„è§†ç‚¹è§„åˆ’(View Planning)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–ç›¸æœºä½å§¿é‡‡é›†å°‘é‡ä¸”ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒã€‚ä¸ºäº†åº”å¯¹ç›¸æœºå®šä½å™ªå£°ä»¥åŠåœ¨æœªçŸ¥å†œä¸šç¯å¢ƒä¸­çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åŸºäºè¿åŠ¨æ¢å¤ç»“æ„(Structure-from-Motion)çš„é‡å»ºè´¨é‡ä¼˜åŒ–æ¨¡å‹ã€‚é‰´äºä¼˜åŒ–å‡½æ•°ç¼ºä¹è§£æè¡¨è¾¾å¼ä¸”è¯„ä¼°æˆæœ¬é«˜æ˜‚ï¼Œç ”ç©¶é‡‡ç”¨äº†è´å¶æ–¯ä¼˜åŒ–(Bayesian optimization)æ–¹æ³•ï¼Œåœ¨è€ƒè™‘å¤šç§å™ªå£°æƒ…å†µçš„åŒæ—¶ï¼Œä»…éœ€å°‘é‡å‡½æ•°è¯„ä¼°å³å¯å®ç°é«˜æ•ˆçš„è§„åˆ’è¿‡ç¨‹ã€‚ä»¿çœŸä¸çœŸå®å†œä¸šåœºæ™¯çš„æ•°å€¼æµ‹è¯•è¯æ˜ï¼ŒBOSfMèƒ½å¤Ÿç²¾å‡†ä¼°ç®—æœ€ä½³ç›¸æœºä½ç½®ä»¥å®ç°é«˜è´¨é‡3Dé‡å»ºï¼Œå¹¶åœ¨ç›¸ä¼¼çš„æœªçŸ¥ç¯å¢ƒä¸­å±•ç°å‡ºå“è¶Šçš„æ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24126v1",
      "published_date": "2025-09-28 23:50:36 UTC",
      "updated_date": "2025-09-28 23:50:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:28.498833+00:00"
    },
    {
      "arxiv_id": "2509.24125v3",
      "title": "The Impossibility of Inverse Permutation Learning in Transformer Models",
      "title_zh": "Transformer æ¨¡å‹ä¸­é€†ç½®æ¢å­¦ä¹ çš„ä¸å¯èƒ½æ€§",
      "authors": [
        "Rohan Alur",
        "Chris Hays",
        "Manish Raghavan",
        "Devavrat Shah"
      ],
      "abstract": "In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens\" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†ä»…è§£ç å™¨ (Decoder-only) Transformer æ¨¡å‹åœ¨é€†ç½®æ¢å­¦ä¹  (Inverse Permutation Learning) ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯¥ä»»åŠ¡è¦æ±‚æ¨¡å‹è¿˜åŸè¢«ç½®æ¢çš„åŸå§‹åºåˆ—ï¼Œæ˜¯è¡¡é‡é•¿ä¸Šä¸‹æ–‡æ£€ç´¢åŠæƒ…å¢ƒå­¦ä¹ é²æ£’æ€§çš„é‡è¦æŒ‡æ ‡ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ ¸å¿ƒçš„ä¸å¯èƒ½ç»“è®º (Impossibility Result)ï¼Œè¯æ˜æ— è®ºæ·±åº¦å¦‚ä½•ï¼ŒDecoder-only æ¶æ„åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šéƒ½æ— æ³•èƒœä»»æ­¤é¡¹ä»»åŠ¡ï¼Œä¸”è¯¥ç»“è®ºä¸è®­ç»ƒè¿‡ç¨‹æˆ–æ ·æœ¬å¤æ‚åº¦æ— å…³ã€‚é€šè¿‡å¯¹æ¯”ç ”ç©¶å‘ç°ï¼ŒEncoder-decoder æ¶æ„èƒ½å¤Ÿå®Œæˆè¯¥ä»»åŠ¡ï¼Œè¿™æ­ç¤ºäº†å› æœæ³¨æ„åŠ›æ©ç  (Causal Attention Mask) åœ¨è¡¨è¾¾èƒ½åŠ›ä¸Šçš„å±€é™ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®éªŒè¯æ˜ä»…éœ€åœ¨è¾“å…¥ä¸­æ·»åŠ â€œè‰ç¨¿æ ‡è®°â€(Scratch Tokens) è¿›è¡Œå¡«å……ï¼Œå³å¯ä½¿è¯¥ä»»åŠ¡åœ¨ Decoder-only æ¨¡å‹ä¸­å˜å¾—å¯è¡Œã€‚è¿™ä¸€å‘ç°ä¸ºé“¾å¼æ€ç»´ (Chain-of-Thought) æˆ–ä¸­é—´â€œæ€è€ƒâ€æ ‡è®°å¦‚ä½•æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§£é‡Šè§†è§’ï¼Œå³å³ä½¿è¿™äº›æ ‡è®°ä¸åŒ…å«æ˜ç¡®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä¹Ÿèƒ½é€šè¿‡æä¾›é¢å¤–çš„è®¡ç®—ç©ºé—´æ¥çªç ´æ¶æ„æœ¬èº«çš„è¡¨è¾¾ç“¶é¢ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24125v3",
      "published_date": "2025-09-28 23:48:11 UTC",
      "updated_date": "2025-12-10 00:19:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:36.796653+00:00"
    },
    {
      "arxiv_id": "2509.24124v1",
      "title": "Ancestry Tree Clustering for Particle Filter Diversity Maintenance",
      "title_zh": "åŸºäºç¥–å…ˆæ ‘èšç±»çš„ç²’å­æ»¤æ³¢å¤šæ ·æ€§ç»´æŠ¤",
      "authors": [
        "Ilari Vallivaara",
        "Bingnan Duan",
        "Yinhuan Dong",
        "Tughrul Arslan"
      ],
      "abstract": "We propose a method for linear-time diversity maintenance in particle filtering. It clusters particles based on ancestry tree topology: closely related particles in sufficiently large subtrees are grouped together. The main idea is that the tree structure implicitly encodes similarity without the need for spatial or other domain-specific metrics. This approach, when combined with intra-cluster fitness sharing and the protection of particles not included in a cluster, effectively prevents premature convergence in multimodal environments while maintaining estimate compactness. We validate our approach in a multimodal robotics simulation and a real-world multimodal indoor environment. We compare the performance to several diversity maintenance algorithms from the literature, including Deterministic Resampling and Particle Gaussian Mixtures. Our algorithm achieves high success rates with little to no negative effect on compactness, showing particular robustness to different domains and challenging initial conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºç²’å­æ»¤æ³¢(Particle Filter)çº¿æ€§æ—¶é—´å¤šæ ·æ€§ç»´æŠ¤çš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒåœ¨äºæ ¹æ®è°±ç³»æ ‘æ‹“æ‰‘(Ancestry Tree Topology)å¯¹ç²’å­è¿›è¡Œèšç±»ï¼Œåˆ©ç”¨æ ‘ç»“æ„éšå¼ç¼–ç çš„ç›¸ä¼¼æ€§å–ä»£äº†å¯¹ç©ºé—´æˆ–ç‰¹å®šé¢†åŸŸåº¦é‡çš„ä¾èµ–ã€‚é€šè¿‡ç»“åˆç°‡å†…é€‚åº”åº¦å…±äº«(Fitness Sharing)å’Œå¯¹æœªèšç±»ç²’å­çš„ä¿æŠ¤æœºåˆ¶ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé˜²æ­¢å¤šæ¨¡æ€ç¯å¢ƒä¸­çš„è¿‡æ—©æ”¶æ•›ï¼Œå¹¶ä¿æŒä¼°è®¡çš„ç´§å‡‘æ€§ã€‚å®éªŒåœ¨å¤šæ¨¡æ€æœºå™¨äººä»¿çœŸå’ŒçœŸå®å®¤å†…ç¯å¢ƒä¸­è¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸ç¡®å®šæ€§é‡é‡‡æ ·(Deterministic Resampling)å’Œç²’å­é«˜æ–¯æ··åˆ(Particle Gaussian Mixtures)ç­‰ç®—æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤„ç†ä¸åŒé¢†åŸŸå’ŒæŒ‘æˆ˜æ€§åˆå§‹æ¡ä»¶æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸äº§ç”Ÿè´Ÿé¢ç´§å‡‘æ€§å½±å“çš„å‰æä¸‹å®ç°æé«˜çš„ä»»åŠ¡æˆåŠŸç‡ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "15th International Conference on Indoor Positioning and Indoor Navigation, 15-18 September 2025, Tampere, Finland Originally 8 pages. The online version with appendices is 14 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.24124v1",
      "published_date": "2025-09-28 23:45:03 UTC",
      "updated_date": "2025-09-28 23:45:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:41.392239+00:00"
    },
    {
      "arxiv_id": "2509.24107v1",
      "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs",
      "title_zh": "Fathom-DeepResearchï¼šèµ‹èƒ½å°è¯­è¨€æ¨¡å‹å®ç°é•¿ç¨‹ä¿¡æ¯æ£€ç´¢ä¸ç»¼åˆ",
      "authors": [
        "Shreyas Singh",
        "Kunal Singh",
        "Pradeep Moturi"
      ],
      "abstract": "Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Fathom-DeepResearchï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªä¸“é—¨æ¨¡å‹ç»„æˆçš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨ä¸ºå°è¯­è¨€æ¨¡å‹ (SLMs) è§£é”é•¿å‘¨æœŸä¿¡æ¯æ£€ç´¢ä¸ç»¼åˆèƒ½åŠ›ã€‚å…¶ä¸­ Fathom-Search-4B ä½œä¸ºä¸€ä¸ª DeepSearch æ¨¡å‹ï¼Œé€šè¿‡å®æ—¶ç½‘é¡µæœç´¢å’Œå®šå‘æŸ¥è¯¢è¿›è¡ŒåŸºäºè¯æ®çš„è°ƒæŸ¥ï¼Œå¹¶é‡‡ç”¨äº†å¤šæ™ºèƒ½ä½“åšå¼ˆç”Ÿæˆçš„ DUETQA æ•°æ®é›†å’Œåä¸º RAPO çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•æŠ€æœ¯æ¥ä¼˜åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¯æ§çš„æ­¥çº§å¥–åŠ±æœºåˆ¶ï¼Œå®ç°äº†å¯¹æœç´¢è½¨è¿¹å¹¿åº¦ã€æ·±åº¦å’Œå‘¨æœŸçš„æ˜¾å¼æ§åˆ¶ï¼Œç¡®ä¿å·¥å…·è°ƒç”¨åœ¨è¶…è¿‡ 20 æ¬¡æ—¶ä¾ç„¶å¯é ã€‚ç³»ç»Ÿä¸­çš„å¦ä¸€ä¸ªæ¨¡å‹ Fathom-Synthesizer-4B åˆ™è´Ÿè´£å°†å¤šè½®æœç´¢è½¨è¿¹è½¬åŒ–ä¸ºç»“æ„åŒ–ã€é«˜å¼•ç”¨å¯†åº¦çš„ DeepResearch Reportsã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨ SimpleQAã€FRAMES ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•åŠ DeepResearch-Bench ä¸Šè¾¾åˆ°äº†å¼€æ”¾æƒé‡æ¨¡å‹ç±»åˆ«çš„é¡¶çº§æ°´å¹³ (SOTA)ã€‚åŒæ—¶ï¼ŒFathom-DeepResearch åœ¨ HLEã€AIME-25 å’Œ MedQA ç­‰å¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24107v1",
      "published_date": "2025-09-28 22:58:11 UTC",
      "updated_date": "2025-09-28 22:58:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:39.590407+00:00"
    },
    {
      "arxiv_id": "2509.24096v1",
      "title": "GEAR: A General Evaluation Framework for Abductive Reasoning",
      "title_zh": "GEARï¼šæº¯å› æ¨ç†çš„é€šç”¨è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Kaiyu He",
        "Peilin Wu",
        "Mian Zhang",
        "Kun Wan",
        "Wentian Zhao",
        "Xinya Du",
        "Zhiyu Chen"
      ],
      "abstract": "Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GEARï¼Œä¸€ä¸ªé’ˆå¯¹æº¯å› æ¨ç†ï¼ˆAbductive Reasoningï¼‰çš„é€šç”¨ã€å…¨è‡ªåŠ¨ã€é€æ˜ä¸”æ— éœ€æ ‡ç­¾çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¡¡é‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‘ç°æ–°çŸ¥è¯†çš„èƒ½åŠ›ã€‚GEARé€šè¿‡ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰ã€æ³›åŒ–æ€§ï¼ˆGeneralizabilityï¼‰å’Œå¤šæ ·æ€§ï¼ˆDiversityï¼‰ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡å¯¹æ¨¡å‹ç”Ÿæˆçš„å‡è®¾é›†è¿›è¡Œè¯„åˆ†ï¼Œè§£å†³äº†ä¼ ç»Ÿé™æ€åŸºå‡†æµ‹è¯•æ˜“é¥±å’Œä¸”ä¾èµ–äººå·¥æ ‡æ³¨çš„é—®é¢˜ã€‚é€šè¿‡å¯¹9ç§æ¨¡å‹åœ¨å››ä¸ªåŸºå‡†ä¸Šçš„æµ‹è¯•ï¼Œè¯¥æ¡†æ¶æ­ç¤ºäº†ä¼ ç»Ÿé‡‘æ ‡å‡†è¯„ä¼°éš¾ä»¥æ•æ‰çš„æ¨¡å‹æ€§èƒ½å·®å¼‚ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§åŸºäºåŠ¨é‡çš„è¯¾ç¨‹å­¦ä¹ ï¼ˆMomentum-based Curriculumï¼‰ç­–ç•¥ï¼Œåœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹åˆ©ç”¨GEARæä¾›çš„ä¿¡å·ä¼˜åŒ–æ¨¡å‹è®­ç»ƒï¼Œä½¿å…¶ä»åŸºç¡€ç›®æ ‡é€æ­¥è½¬å‘ç”Ÿæˆå¤æ‚çš„å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹ç”Ÿæˆå¤šæ ·ä¸”å¯é å‡è®¾çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªç°æœ‰æº¯å› æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†ä¼˜å¼‚çš„è¿ç§»æ•ˆæœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Coda and Data: https://github.com/KaiyuHe998/GEAR-Abduction_evaluation",
      "pdf_url": "https://arxiv.org/pdf/2509.24096v1",
      "published_date": "2025-09-28 22:22:28 UTC",
      "updated_date": "2025-09-28 22:22:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:49.388203+00:00"
    },
    {
      "arxiv_id": "2509.24091v3",
      "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?",
      "title_zh": "PerfBenchï¼šæ™ºèƒ½ä½“èƒ½å¦è§£å†³ç°å®ä¸–ç•Œä¸­çš„æ€§èƒ½ç¼ºé™·ï¼Ÿ",
      "authors": [
        "Spandan Garg",
        "Roshanak Zilouchian Moghaddam",
        "Neel Sundaresan"
      ],
      "abstract": "Performance bugs are inefficiencies in software that waste computational resources without causing functional failures, making them particularly challenging to detect and fix. While recent advances in Software Engineering agents have shown promise in automated bug fixing, existing benchmarks primarily focus on functional correctness and fail to evaluate agents' abilities to identify and resolve non-functional issues like performance bugs. We introduce PerfBench, a benchmark comprising 81 real-world performance bug-fixing tasks from popular .NET repositories on GitHub. Unlike existing benchmarks that rely on pre-existing test suites, PerfBench features a novel evaluation harness that allows agents to generate their own performance benchmarks and validates fixes by comparing execution metrics collected for developer fix and agent fix. Each task in PerfBench is derived from actual developer fixes linked to performance-related issues, which are then verified by human experts, ensuring real-world relevance. Our evaluation reveals that current state-of-the-art coding agents struggle with performance optimization tasks, with baseline OpenHands agent achieving only a ~3% success rate on our benchmark. We develop OpenHands-Perf-Agent, which incorporates performance-aware tooling and instructions and achieves a ~20% success rate on the benchmark. We show that by ensuring the agent has proper instructions to benchmark its changes and tooling for benchmark output processing, we can improve the agent performance significantly, but room for improvement still remains. PerfBench provides a challenging test set for furthering the capabilities of agents in fixing performance issues.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† PerfBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 81 ä¸ªæ¥è‡ª GitHub çƒ­é—¨ .NET ä»“åº“çœŸå®æ€§èƒ½ç¼ºé™·ä¿®å¤ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“è¯†åˆ«å’Œè§£å†³éåŠŸèƒ½æ€§é—®é¢˜çš„èƒ½åŠ›ã€‚ç°æœ‰çš„è½¯ä»¶å·¥ç¨‹åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œè€Œå¿½ç•¥äº†æ€§èƒ½ç¼ºé™·è¿™ç§æµªè´¹è®¡ç®—èµ„æºå´ä¸å¯¼è‡´åŠŸèƒ½æ•…éšœçš„æŒ‘æˆ˜ã€‚PerfBench é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è¯„ä¼°æ¡†æ¶ï¼Œå…è®¸æ™ºèƒ½ä½“ç”Ÿæˆè‡ªå·±çš„æ€§èƒ½åŸºå‡†ï¼Œå¹¶é€šè¿‡æ¯”è¾ƒå¼€å‘è€…ä¿®å¤ä¸æ™ºèƒ½ä½“ä¿®å¤çš„æ‰§è¡ŒæŒ‡æ ‡æ¥éªŒè¯ç»“æœã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œç›®å‰çš„å…ˆè¿›ç¼–ç¨‹æ™ºèƒ½ä½“åœ¨æ€§èƒ½ä¼˜åŒ–ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚åŸºå‡† OpenHands æ™ºèƒ½ä½“çš„æˆåŠŸç‡ä»…ä¸º 3% å·¦å³ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡é›†æˆæ€§èƒ½æ„ŸçŸ¥å·¥å…·å’ŒæŒ‡ä»¤å¼€å‘äº† OpenHands-Perf-Agentï¼Œå°†æˆåŠŸç‡æå‡è‡³ 20% å·¦å³ã€‚å®éªŒç»“æœè¯æ˜ï¼Œæä¾›é€‚å½“çš„æ€§èƒ½è¯„ä¼°å·¥å…·å’ŒæŒ‡ä»¤èƒ½æ˜¾è‘—æ”¹å–„æ™ºèƒ½ä½“æ€§èƒ½ï¼Œä½†è¯¥é¢†åŸŸä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜å¹¶ç•™æœ‰æ”¹è¿›ç©ºé—´ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24091v3",
      "published_date": "2025-09-28 22:00:33 UTC",
      "updated_date": "2025-12-02 20:55:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:20:56.969791+00:00"
    },
    {
      "arxiv_id": "2509.24090v1",
      "title": "Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?",
      "title_zh": "å¤§è§„æ¨¡çº¦æŸç”Ÿæˆï¼šå¤§è¯­è¨€æ¨¡å‹èƒ½å¦è§£ææ•°ç™¾é¡¹çº¦æŸï¼Ÿ",
      "authors": [
        "Matteo Boffa",
        "Jiaxuan You"
      ],
      "abstract": "Recent research has explored the constrained generation capabilities of Large Language Models (LLMs) when explicitly prompted by few task-specific requirements. In contrast, we introduce Large-Scale Constraint Generation (LSCG), a new problem that evaluates whether LLMs can parse a large, fine-grained, generic list of constraints. To examine the LLMs' ability to handle an increasing number constraints, we create a practical instance of LSCG, called Words Checker. In Words Checker, we evaluate the impact of model characteristics (e.g., size, family) and steering techniques (e.g., Simple Prompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet, a small and dedicated model that parses the original list of constraints into a smaller subset, helping the LLM focus on relevant constraints. Experiments reveal that existing solutions suffer a significant performance drop as the number of constraints increases, with FoCusNet showing an 8-13% accuracy boost.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†å¤§è§„æ¨¡çº¦æŸç”Ÿæˆ (Large-Scale Constraint Generation, LSCG) è¿™ä¸€æ–°é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ (LLMs) å¤„ç†å¤§é‡ã€ç»†ç²’åº¦ä¸”é€šç”¨çº¦æŸåˆ—è¡¨çš„èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº† Words Checker å®ä¾‹æ¥è€ƒå¯Ÿæ¨¡å‹ç‰¹å¾åŠ Simple Promptã€Chain of Thought å’Œ Best of N ç­‰å¼•å¯¼æŠ€æœ¯å¯¹æ€§èƒ½çš„å½±å“ã€‚å®éªŒå‘ç°ï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨çº¦æŸæ•°é‡å¢åŠ æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæš´éœ²äº† LLMs åœ¨è§£æå¤§è§„æ¨¡çº¦æŸæ—¶çš„çŸ­æ¿ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º FoCusNet çš„å°å‹ä¸“ç”¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å°†åŸå§‹çº¦æŸåˆ—è¡¨è§£æä¸ºæ›´å°çš„å­é›†ï¼Œå¼•å¯¼ LLM èšç„¦äºç›¸å…³çº¦æŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFoCusNet åœ¨æ­¤ç±»ä»»åŠ¡ä¸­å®ç°äº† 8-13% çš„å‡†ç¡®ç‡æå‡ï¼Œä¸ºä¼˜åŒ–å¤§è§„æ¨¡çº¦æŸä¸‹çš„ç”Ÿæˆä»»åŠ¡æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24090v1",
      "published_date": "2025-09-28 21:55:53 UTC",
      "updated_date": "2025-09-28 21:55:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:00.774142+00:00"
    },
    {
      "arxiv_id": "2509.24086v1",
      "title": "Do Repetitions Matter? Strengthening Reliability in LLM Evaluations",
      "title_zh": "é‡å¤æ˜¯å¦é‡è¦ï¼Ÿå¢å¼ºå¤§è¯­è¨€æ¨¡å‹è¯„æµ‹çš„å¯é æ€§",
      "authors": [
        "Miguel Angel Alvarado Gonzalez",
        "Michelle Bruno Hernandez",
        "Miguel Angel PeÃ±aloza Perez",
        "Bruno Lopez Orozco",
        "Jesus Tadeo Cruz Soto",
        "Sandra Malagon"
      ],
      "abstract": "LLM leaderboards often rely on single stochastic runs, but how many repetitions are required for reliable conclusions remains unclear. We re-evaluate eight state-of-the-art models on the AI4Math Benchmark with three independent runs per setting. Using mixed-effects logistic regression, domain-level marginal means, rank-instability analysis, and run-to-run reliability, we assessed the value of additional repetitions. Our findings shows that Single-run leaderboards are brittle: 10/12 slices (83\\%) invert at least one pairwise rank relative to the three-run majority, despite a zero sign-flip rate for pairwise significance and moderate overall interclass correlation. Averaging runs yields modest SE shrinkage ($\\sim$5\\% from one to three) but large ranking gains; two runs remove $\\sim$83\\% of single-run inversions. We provide cost-aware guidance for practitioners: treat evaluation as an experiment, report uncertainty, and use $\\geq 2$ repetitions under stochastic decoding. These practices improve robustness while remaining feasible for small teams and help align model comparisons with real-world reliability.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†LLMæ’è¡Œæ¦œä¸­å•æ¬¡éšæœºè¿è¡Œçš„å¯é æ€§é—®é¢˜ï¼Œæ—¨åœ¨ç¡®å®šå¢å¼ºè¯„ä¼°å¯é æ€§æ‰€éœ€çš„é‡å¤æ¬¡æ•°ã€‚ç ”ç©¶äººå‘˜åœ¨AI4Math Benchmarkä¸Šå¯¹8ä¸ªé¡¶çº§æ¨¡å‹è¿›è¡Œäº†é‡æ–°è¯„ä¼°ï¼Œæ¯ä¸ªè®¾ç½®è¿›è¡Œä¸‰æ¬¡ç‹¬ç«‹è¿è¡Œï¼Œå¹¶åˆ©ç”¨æ··åˆæ•ˆåº”é€»è¾‘å›å½’(mixed-effects logistic regression)å’Œç­‰çº§ä¸ç¨³å®šåˆ†æ(rank-instability analysis)ç­‰æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚å®éªŒå‘ç°å•æ¬¡è¿è¡Œçš„æ’è¡Œæ¦œéå¸¸è„†å¼±ï¼Œ83%çš„åˆ‡ç‰‡åœ¨ç›¸å¯¹äºä¸‰æ¬¡è¿è¡Œçš„ç»“æœæ—¶å‡ºç°äº†è‡³å°‘ä¸€æ¬¡æˆå¯¹æ’ååè½¬ã€‚è™½ç„¶å¹³å‡å¤šæ¬¡è¿è¡Œå¯¹æ ‡å‡†è¯¯å·®(SE)çš„ç¼©å°æœ‰é™ï¼Œä½†èƒ½æ˜¾è‘—æå‡æ’åç¨³å®šæ€§ï¼Œä»…éœ€ä¸¤æ¬¡è¿è¡Œå³å¯æ¶ˆé™¤çº¦83%çš„å•æ¬¡è¿è¡Œæ’ååè½¬ã€‚è¯¥ç ”ç©¶å»ºè®®ä»ä¸šè€…åœ¨éšæœºè§£ç (stochastic decoding)ç¯å¢ƒä¸‹è‡³å°‘è¿›è¡Œ2æ¬¡é‡å¤è¿è¡Œï¼Œå¹¶ä¸»åŠ¨æŠ¥å‘Šä¸ç¡®å®šæ€§ã€‚è¿™äº›å®è·µåœ¨ä¿æŒå°å›¢é˜Ÿå¼€å‘å¯è¡Œæ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†è¯„ä¼°çš„é²æ£’æ€§(robustness)ï¼Œä½¿æ¨¡å‹æ¯”è¾ƒæ›´ç¬¦åˆç°å®ä¸–ç•Œçš„å¯é æ€§æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24086v1",
      "published_date": "2025-09-28 21:45:20 UTC",
      "updated_date": "2025-09-28 21:45:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:02.361844+00:00"
    },
    {
      "arxiv_id": "2509.24085v2",
      "title": "PEARL: Peer-Enhanced Adaptive Radio via On-Device LLM",
      "title_zh": "PEARLï¼šåŸºäºç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹çš„å¯¹ç­‰ä½“å¢å¼ºå‹è‡ªé€‚åº”æ— çº¿ç”µ",
      "authors": [
        "Ju-Hyung Lee",
        "Yanqing Lu",
        "Klaus Doppler"
      ],
      "abstract": "We present PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM), a framework for cooperative cross-layer optimization in device-to-device (D2D) communication. Building on our previous work on single-device on-device LLMs, PEARL extends the paradigm by leveraging both publisher and subscriber states to guide Wi-Fi Aware (WA) parameter selection. A context-aware reward, which normalizes latency by application tolerances and modulates energy by device battery states, provides richer supervision for KL-based finetuning. We study two lightweight variants: PEARL (Head + Low-Rank Adaptation (LoRA)) achieves the best overall performance, while PEARL-Lite (Head-only) delivers sub-20 ms inference at near-identical objective scores. Across synthetic scenarios grounded in real measurements, PEARL improves objective scores over heuristic and compact model baselines and reduces energy by up to 16% in cooperative low-battery cases. These results demonstrate that peer-aware context, reward-aligned training, and head-based efficiency make LLMs practical for always-on, on-device cross-layer control. Code, real-world demo, and dataset are available at https://github.com/abman23/pearl",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PEARL (Peer-Enhanced Adaptive Radio via On-Device LLM)ï¼Œä¸€ç§æ—¨åœ¨å®ç°è®¾å¤‡åˆ°è®¾å¤‡ (D2D) é€šä¿¡ä¸­åä½œå¼è·¨å±‚ä¼˜åŒ–çš„æ¡†æ¶ã€‚PEARL é€šè¿‡åŒæ—¶åˆ©ç”¨å‘å¸ƒè€… (publisher) å’Œè®¢é˜…è€… (subscriber) çš„çŠ¶æ€æ¥æŒ‡å¯¼ Wi-Fi Aware (WA) å‚æ•°é€‰æ‹©ï¼Œå°†ç«¯ä¾§å¤§æ¨¡å‹çš„ç ”ç©¶èŒƒå¼ä»å•è®¾å¤‡æ‰©å±•åˆ°äº†å¯¹ç­‰ç«¯åä½œé¢†åŸŸã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ç§ context-aware rewardï¼Œé€šè¿‡åº”ç”¨å®¹å¿åº¦å½’ä¸€åŒ–å»¶è¿Ÿå¹¶æ ¹æ®ç”µæ± çŠ¶æ€è°ƒèŠ‚èƒ½é‡æ¶ˆè€—ï¼Œä¸ºåŸºäº KL-based çš„å¾®è°ƒæä¾›ç›‘ç£ã€‚ç ”ç©¶è¯„ä¼°äº† PEARL (Head + LoRA) å’Œ PEARL-Lite ä¸¤ç§è½»é‡åŒ–å˜ä½“ï¼Œå…¶ä¸­ PEARL-Lite åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶å®ç°äº†ä½äº 20 ms çš„æ¨ç†å»¶è¿Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPEARL åœ¨ç»¼åˆè¯„åˆ†ä¸Šä¼˜äºå¯å‘å¼å’Œç´§å‡‘æ¨¡å‹åŸºçº¿ï¼Œåœ¨ä½ç”µé‡åœºæ™¯ä¸‹å¯é™ä½é«˜è¾¾ 16% çš„èƒ½è€—ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨å¯¹ç­‰ç«¯æ„ŸçŸ¥ä¸Šä¸‹æ–‡å’Œå¥–åŠ±å¯¹é½è®­ç»ƒï¼Œå¯ä»¥ä½¿ç«¯ä¾§ LLM å®é™…åº”ç”¨äºå§‹ç»ˆå¼€å¯çš„å¸¸é©»è·¨å±‚æ§åˆ¶ä»»åŠ¡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24085v2",
      "published_date": "2025-09-28 21:43:17 UTC",
      "updated_date": "2025-10-28 04:48:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:04.370498+00:00"
    },
    {
      "arxiv_id": "2509.24072v3",
      "title": "Uncovering Grounding IDs: How External Cues Shape Multimodal Binding",
      "title_zh": "æ­ç¤ºå®šä½ IDï¼šå¤–éƒ¨çº¿ç´¢å¦‚ä½•å¡‘é€ å¤šæ¨¡æ€ç»‘å®š",
      "authors": [
        "Hosein Hasani",
        "Amirmohammad Izadi",
        "Fatemeh Askari",
        "Mobin Bagherian",
        "Sadegh Mohammadian",
        "Mohammad Izadi",
        "Mahdieh Soleymani Baghshah"
      ],
      "abstract": "Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as consistent within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism that explains how external cues enhance multimodal binding and offer both interpretability and practical improvements.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç»“æ„åŒ–æ¨ç†å’Œç²¾ç¡®æ„ŸçŸ¥ï¼ˆgroundingï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†Grounding IDsè¿™ä¸€æ–°æ¦‚å¿µï¼Œå³ç”±å¤–éƒ¨çº¿ç´¢è¯±å¯¼çš„ã€ç”¨äºåœ¨è·¨æ¨¡æ€åœºæ™¯ä¸‹å°†ç‰©ä½“ä¸å…¶æŒ‡å®šåˆ†åŒºç»‘å®šçš„æ½œåœ¨æ ‡è¯†ç¬¦ã€‚é€šè¿‡è¡¨å¾åˆ†æï¼Œä½œè€…å‘ç°è¿™äº›æ ‡è¯†ç¬¦åœ¨åµŒå…¥ç©ºé—´ä¸­è¡¨ç°ä¸ºä¸€è‡´çš„åˆ†åŒºå†…å¯¹é½ï¼Œå¹¶æœ‰æ•ˆç¼©å°äº†å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€å·®å¼‚ï¼ˆmodality gapï¼‰ã€‚å› æœå¹²é¢„å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¿™äº›æ ‡è¯†ç¬¦åœ¨ç‰©ä½“ä¸ç¬¦å·çº¿ç´¢çš„ç»‘å®šè¿‡ç¨‹ä¸­èµ·åˆ°äº†ä¸­ä»‹ä½œç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGrounding IDsèƒ½å¤Ÿå¢å¼ºç›¸å…³ç»„ä»¶é—´çš„æ³¨æ„åŠ›ï¼ˆattentionï¼‰ï¼Œä»è€Œæå‡è·¨æ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰ï¼ˆhallucinationsï¼‰ç°è±¡ã€‚è¯¥ç ”ç©¶å°†Grounding IDsè¯†åˆ«ä¸ºä¸€ç§å…³é”®çš„ç¬¦å·æœºåˆ¶ï¼Œåˆç†è§£é‡Šäº†å¤–éƒ¨çº¿ç´¢å¦‚ä½•å¢å¼ºå¤šæ¨¡æ€ç»‘å®šï¼Œä¸ºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œå®é™…æ€§èƒ½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review as a conference paper at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.24072v3",
      "published_date": "2025-09-28 21:15:07 UTC",
      "updated_date": "2025-12-05 17:19:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:17.974657+00:00"
    },
    {
      "arxiv_id": "2509.24069v1",
      "title": "AQUAIR: A High-Resolution Indoor Environmental Quality Dataset for Smart Aquaculture Monitoring",
      "title_zh": "AQUAIRï¼šé¢å‘æ™ºèƒ½æ°´äº§å…»æ®–ç›‘æµ‹çš„é«˜åˆ†è¾¨ç‡å®¤å†…ç¯å¢ƒè´¨é‡æ•°æ®é›†",
      "authors": [
        "Youssef Sabiri",
        "Walid Houmaidi",
        "Ouail El Maadi",
        "Yousra Chtouki"
      ],
      "abstract": "Smart aquaculture systems depend on rich environmental data streams to protect fish welfare, optimize feeding, and reduce energy use. Yet public datasets that describe the air surrounding indoor tanks remain scarce, limiting the development of forecasting and anomaly-detection tools that couple head-space conditions with water-quality dynamics. We therefore introduce AQUAIR, an open-access public dataset that logs six Indoor Environmental Quality (IEQ) variables--air temperature, relative humidity, carbon dioxide, total volatile organic compounds, PM2.5 and PM10--inside a fish aquaculture facility in Amghass, Azrou, Morocco. A single Awair HOME monitor sampled every five minutes from 14 October 2024 to 9 January 2025, producing more than 23,000 time-stamped observations that are fully quality-controlled and publicly archived on Figshare. We describe the sensor placement, ISO-compliant mounting height, calibration checks against reference instruments, and an open-source processing pipeline that normalizes timestamps, interpolates short gaps, and exports analysis-ready tables. Exploratory statistics show stable conditions (median CO2 = 758 ppm; PM2.5 = 12 micrograms/m3) with pronounced feeding-time peaks, offering rich structure for short-horizon forecasting, event detection, and sensor drift studies. AQUAIR thus fills a critical gap in smart aquaculture informatics and provides a reproducible benchmark for data-centric machine learning curricula and environmental sensing research focused on head-space dynamics in recirculating aquaculture systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† AQUAIRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ™ºèƒ½æ°´äº§å…»æ®–ç›‘æµ‹çš„é«˜åˆ†è¾¨ç‡å®¤å†…ç¯å¢ƒè´¨é‡(Indoor Environmental Quality, IEQ)å¼€æ”¾æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†è¯¦ç»†è®°å½•äº†æ‘©æ´›å“¥ä¸€å®¶é±¼ç±»å…»æ®–åœºå†…çš„å…­é¡¹å…³é”®å˜é‡ï¼ŒåŒ…æ‹¬ç©ºæ°”æ¸©åº¦ã€ç›¸å¯¹æ¹¿åº¦ã€CO2ã€æ€»æŒ¥å‘æ€§æœ‰æœºåŒ–åˆç‰©(TVOC)ã€PM2.5 å’Œ PM10ã€‚åœ¨ä¸ºæœŸä¸‰ä¸ªæœˆçš„ç›‘æµ‹è¿‡ç¨‹ä¸­ï¼Œç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç¬¦åˆ ISO æ ‡å‡†çš„ä¼ æ„Ÿå™¨å¸ƒç½®å’Œå¼€æºå¤„ç†æµæ°´çº¿ï¼Œç”Ÿæˆäº†è¶…è¿‡ 23,000 æ¡ç»è¿‡è´¨é‡æ§åˆ¶çš„æ—¶é—´æˆ³è§‚æµ‹è®°å½•ã€‚åˆæ­¥ç»Ÿè®¡åˆ†ææ˜¾ç¤ºï¼Œå®¤å†…ç¯å¢ƒåœ¨æŠ•å–‚æ—¶é—´å…·æœ‰æ˜¾è‘—çš„å³°å€¼ç‰¹å¾ï¼Œä¸ºçŸ­æœŸé¢„æµ‹å’Œäº‹ä»¶æ£€æµ‹æä¾›äº†ä¸°å¯Œçš„ç»“æ„åŒ–ä¿¡æ¯ã€‚AQUAIR å¡«è¡¥äº†å¾ªç¯æ°´å…»æ®–ç³»ç»Ÿä¸­é¡¶ç©ºåŠ¨æ€(head-space dynamics)æ•°æ®çš„ç©ºç™½ï¼Œä¸ºä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æœºå™¨å­¦ä¹ å’Œç¯å¢ƒä¼ æ„Ÿç ”ç©¶æä¾›äº†å¯é çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 6 figures, 3 tables. Accepted at the 9th IEEE Global Conference on Artificial Intelligence & Internet of Things (IEEE GCAIoT) 2025. Final camera-ready manuscript. Math expressions in this field are rendered via MathJax",
      "pdf_url": "https://arxiv.org/pdf/2509.24069v1",
      "published_date": "2025-09-28 21:07:10 UTC",
      "updated_date": "2025-09-28 21:07:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:22.288370+00:00"
    },
    {
      "arxiv_id": "2509.24068v2",
      "title": "A Small Math Model: Recasting Strategy Choice Theory in an LLM-Inspired Architecture",
      "title_zh": "å°å‹æ•°å­¦æ¨¡å‹ï¼šåœ¨å¤§è¯­è¨€æ¨¡å‹å¯å‘å¼æ¶æ„ä¸­é‡æ„ç­–ç•¥é€‰æ‹©ç†è®º",
      "authors": [
        "Roussel Rahman",
        "Jeff Shrager"
      ],
      "abstract": "Strategy Choice Theory (SCT; Siegler and Shrager, 1984; Siegler, 2000) explains important aspects of children's arithmetic learning based upon principles including learning from developmentally naturalistic data, probabilistic representation, confidence-based retrieval, and the phase-like importance of scaffolding strategies, such as finger-counting. Here we recast SCT as a ``Small Math Model'' (SMM), employing a neural-network-based architecture analogous to LLMs. The SMM extends SCT to include counting practice, symbol (number) embedding, and gated attention. Similar to earlier work, the SMM demonstrates constructive and destructive interference between counting and addition, and the ``wave-like'' use of finger-counting as sum recall improves. We plan to extend the SMM to later aspects of the decades-long SCT program, including adaptive strategy choice and eventually strategy discovery, providing a unified platform to investigate the understanding of numerical characteristics and relationships essential for mathematical reasoning -- as it can emerge in LLM-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶å°†ç»å…¸çš„ç­–ç•¥é€‰æ‹©ç†è®º (Strategy Choice Theory, SCT) é‡å¡‘ä¸ºå—å¤§è¯­è¨€æ¨¡å‹ (LLM) æ¶æ„å¯å‘çš„â€œå°æ•°å­¦æ¨¡å‹â€ (Small Math Model, SMM)ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå„¿ç«¥åœ¨ç®—æœ¯å­¦ä¹ ä¸­çš„è®¤çŸ¥è¿‡ç¨‹ã€‚SMM æ‰©å±•äº†åŸæœ‰çš„ SCT æ¡†æ¶ï¼Œå¼•å…¥äº†è®¡æ•°ç»ƒä¹  (counting practice)ã€æ•°å­—ç¬¦å·åµŒå…¥ (symbol embedding) ä»¥åŠé—¨æ§æ³¨æ„åŠ›æœºåˆ¶ (gated attention)ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸæ•æ‰åˆ°äº†è®¡æ•°ä¸åŠ æ³•ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ç°è±¡ï¼Œå¹¶é‡ç°äº†æ‰‹æŒ‡è®¡æ•°éšæ±‚å’Œå›æƒ³èƒ½åŠ›å¢å¼ºè€Œå‘ˆç°çš„â€œæ³¢æµªå¼â€ç­–ç•¥æ›´è¿­ã€‚è¿™ä¸€ç ”ç©¶ä¸ä»…å¤ç°äº†è®¤çŸ¥å¿ƒç†å­¦ä¸­çš„å…³é”®ç‰¹å¾ï¼Œè¿˜ä¸ºæ¢ç´¢ LLM æ™ºèƒ½ä½“å¦‚ä½•æ¶Œç°æ•°å­¦æ¨ç†æ‰€éœ€çš„æ•°å€¼å…³ç³»ç†è§£æä¾›äº†ç»Ÿä¸€çš„è®¡ç®—å¹³å°ã€‚æœªæ¥è¯¥å·¥ä½œå°†è¿›ä¸€æ­¥æ‰©å±•è‡³è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©ä¸ç­–ç•¥å‘ç°ï¼Œæ·±å…¥ç ”ç©¶æ•°å­¦æ€ç»´çš„å½¢æˆæœºåˆ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24068v2",
      "published_date": "2025-09-28 20:58:48 UTC",
      "updated_date": "2025-11-20 21:10:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:23.462238+00:00"
    },
    {
      "arxiv_id": "2509.24067v1",
      "title": "In-Context Compositional Q-Learning for Offline Reinforcement Learning",
      "title_zh": "é¢å‘ç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸Šä¸‹æ–‡ç»„åˆå¼Qå­¦ä¹ ",
      "authors": [
        "Qiushui Xu",
        "Yuhao Huang",
        "Yushu Jiang",
        "Lei Song",
        "Jinyu Wang",
        "Wenliang Zheng",
        "Jiang Bian"
      ],
      "abstract": "Accurately estimating the Q-function is a central challenge in offline reinforcement learning. However, existing approaches often rely on a single global Q-function, which struggles to capture the compositional nature of tasks involving diverse subtasks. We propose In-context Compositional Q-Learning (\\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a contextual inference problem, using linear Transformers to adaptively infer local Q-functions from retrieved transitions without explicit subtask labels. Theoretically, we show that under two assumptions--linear approximability of the local Q-function and accurate weight inference from retrieved context--\\texttt{ICQL} achieves bounded Q-function approximation error, and supports near-optimal policy extraction. Empirically, \\texttt{ICQL} substantially improves performance in offline settings: improving performance in kitchen tasks by up to 16.4\\%, and in Gym and Adroit tasks by up to 8.6\\% and 6.3\\%. These results highlight the underexplored potential of in-context learning for robust and compositional value estimation, positioning \\texttt{ICQL} as a principled and effective framework for offline RL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹ (Offline RL)ä¸­å•ä¸€å…¨åŸŸ Q-function éš¾ä»¥æ•æ‰å¤æ‚ä»»åŠ¡ç»„åˆæ€§çš„é—®é¢˜ï¼Œæå‡ºäº† In-context Compositional Q-Learning (ICQL) æ¡†æ¶ã€‚ICQL é¦–æ¬¡å°† Q-learning å»ºæ¨¡ä¸ºä¸Šä¸‹æ–‡æ¨ç†é—®é¢˜ï¼Œåˆ©ç”¨ linear Transformers ä»æ£€ç´¢åˆ°çš„è½¬æ¢(transitions)ä¸­è‡ªé€‚åº”åœ°æ¨æ–­å±€éƒ¨ Q-functionsï¼Œä¸”æ— éœ€æ˜¾å¼çš„å­ä»»åŠ¡æ ‡ç­¾ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œåœ¨æ»¡è¶³å±€éƒ¨çº¿æ€§è¿‘ä¼¼ç­‰å‡è®¾ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å®ç°æœ‰ç•Œçš„ Q-function è¿‘ä¼¼è¯¯å·®å¹¶æ”¯æŒæ¥è¿‘æœ€ä¼˜çš„ç­–ç•¥æå–(policy extraction)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒICQL åœ¨ Kitchenã€Gym å’Œ Adroit ä»»åŠ¡ä¸­çš„æ€§èƒ½åˆ†åˆ«æå‡äº†é«˜è¾¾ 16.4%ã€8.6% å’Œ 6.3%ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)åœ¨ç¨³å¥ä¸”ç»„åˆæ€§çš„ä»·å€¼ä¼°è®¡ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§ä¸”æœ‰æ•ˆçš„æŠ€æœ¯æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24067v1",
      "published_date": "2025-09-28 20:55:21 UTC",
      "updated_date": "2025-09-28 20:55:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:36.695387+00:00"
    },
    {
      "arxiv_id": "2509.24066v1",
      "title": "A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer",
      "title_zh": "åˆå§‹åŒ–å‰ªæä¸çŸ¥è¯†è¿ç§»çš„äºŒé˜¶è§†è§’",
      "authors": [
        "Leonardo Iurada",
        "Beatrice Occhiena",
        "Tatiana Tommasi"
      ],
      "abstract": "The widespread availability of pre-trained vision models has enabled numerous deep learning applications through their transferable representations. However, their computational and storage costs often limit practical deployment. Pruning-at-Initialization has emerged as a promising approach to compress models before training, enabling efficient task-specific adaptation. While conventional wisdom suggests that effective pruning requires task-specific data, this creates a challenge when downstream tasks are unknown in advance. In this paper, we investigate how data influences the pruning of pre-trained vision models. Surprisingly, pruning on one task retains the model's zero-shot performance also on unseen tasks. Furthermore, fine-tuning these pruned models not only improves performance on original seen tasks but can recover held-out tasks' performance. We attribute this phenomenon to the favorable loss landscapes induced by extensive pre-training on large-scale datasets.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶ä»äºŒé˜¶è§†è§’æ¢è®¨äº†é¢„è®­ç»ƒè§†è§‰æ¨¡å‹åœ¨åˆå§‹åŒ–é˜¶æ®µå‰ªæ(Pruning-at-Initialization)ä¸çŸ¥è¯†è½¬ç§»(Knowledge Transfer)ä¹‹é—´çš„å…³ç³»ï¼Œæ—¨åœ¨é™ä½æ¨¡å‹éƒ¨ç½²çš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬ã€‚é’ˆå¯¹ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºæœ‰æ•ˆå‰ªæéœ€ä¾èµ–ç‰¹å®šä»»åŠ¡æ•°æ®(task-specific data)çš„å±€é™æ€§ï¼Œæœ¬æ–‡æ·±å…¥è°ƒæŸ¥äº†æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹å‰ªæè¿‡ç¨‹çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨æŸä¸€ç‰¹å®šä»»åŠ¡ä¸Šè¿›è¡Œçš„å‰ªæèƒ½å¤Ÿå‡ºäººæ„æ–™åœ°ä¿ç•™æ¨¡å‹åœ¨å…¶ä»–æœªè§ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬(zero-shot)æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®éªŒè¯æ˜å¯¹å‰ªæåçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒ(fine-tuning)ä¸ä»…èƒ½å¢å¼ºåŸå§‹ä»»åŠ¡è¡¨ç°ï¼Œç”šè‡³èƒ½å¤Ÿæ¢å¤åœ¨é¢„ç•™ä»»åŠ¡(held-out tasks)ä¸Šçš„æ€§èƒ½ã€‚ä½œè€…å°†è¿™ä¸€ç°è±¡å½’åŠŸäºå¤§è§„æ¨¡æ•°æ®é›†é¢„è®­ç»ƒæ‰€å½¢æˆçš„ä¼˜å¼‚æŸå¤±å¹³é¢(loss landscapes)ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡æœªçŸ¥æ—¶çš„æ¨¡å‹é«˜æ•ˆå‹ç¼©ä¸é€‚é…æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted ICIAP 2025 - IAPR Best Paper Award",
      "pdf_url": "https://arxiv.org/pdf/2509.24066v1",
      "published_date": "2025-09-28 20:55:11 UTC",
      "updated_date": "2025-09-28 20:55:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:30.078585+00:00"
    },
    {
      "arxiv_id": "2510.03285v1",
      "title": "WAREX: Web Agent Reliability Evaluation on Existing Benchmarks",
      "title_zh": "WAREXï¼šåŸºäºç°æœ‰åŸºå‡†çš„ Web æ™ºèƒ½ä½“å¯é æ€§è¯„ä¼°",
      "authors": [
        "Su Kara",
        "Fazle Faisal",
        "Suman Nath"
      ],
      "abstract": "Recent advances in browser-based LLM agents have shown promise for automating tasks ranging from simple form filling to hotel booking or online shopping. Current benchmarks measure agent performance in controlled environments, such as containers or stable networks, where websites behave deterministically. However, in the real world, users access websites over networks and HTTPS connections that introduce instability from multiple sources: client-side, server-side issues or broader system failures. Moreover, live websites are prone to web attacks such Cross-Site Scripting, as well as general site modifications which can cause unexpected or malicious pop-ups or improper functionality. To address this gap, we present WAREX: Web Agent Reliability Evaluation on Existing Benchmarks. We measure the impact of WAREX across three popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that introducing WAREX leads to significant drops in task success rates, highlighting the limited robustness of state-of-the-art agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WAREXï¼Œä¸€ä¸ªæ—¨åœ¨è¯„ä¼° Web Agent åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å¯é æ€§çš„æ¡†æ¶ï¼Œä»¥å¼¥è¡¥å½“å‰å—æ§å®éªŒç¯å¢ƒä¸çœŸå®ä¸–ç•Œå¤æ‚ç½‘ç«™è¡Œä¸ºä¹‹é—´çš„å·®è·ã€‚çœŸå®ä¸–ç•Œçš„ç½‘é¡µæ“ä½œå¸¸å—ç½‘ç»œä¸ç¨³å®šã€HTTPS è¿æ¥é—®é¢˜ã€Cross-Site Scripting æ”»å‡»ä»¥åŠç”±ç«™ç‚¹ä¿®æ”¹å¼•èµ·çš„æ¶æ„å¼¹çª—ç­‰å› ç´ å¹²æ‰°ï¼Œè€Œç°æœ‰åŸºå‡†æµ‹è¯•å¾€å¾€å¤„äºç†æƒ³çš„ç¡®å®šæ€§ç¯å¢ƒä¸­ã€‚é€šè¿‡åœ¨ WebArenaã€WebVoyager å’Œ REAL ä¸‰ä¸ªä¸»æµåŸºå‡†æµ‹è¯•ä¸­åº”ç”¨ WAREXï¼Œå®éªŒæ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„æ™ºèƒ½ä½“åœ¨åº”å¯¹è¿™äº›éç¡®å®šæ€§å¹²æ‰°æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„æˆåŠŸç‡ä¸‹é™ã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†ç°æœ‰ state-of-the-art æ™ºèƒ½ä½“åœ¨é²æ£’æ€§ (robustness) æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ›´å…·éŸ§æ€§çš„è‡ªä¸»ç½‘ç»œè‡ªåŠ¨åŒ–æŠ€æœ¯æä¾›äº†è¯„ä¼°æ ‡å‡†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03285v1",
      "published_date": "2025-09-28 20:51:05 UTC",
      "updated_date": "2025-09-28 20:51:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:35.495705+00:00"
    },
    {
      "arxiv_id": "2510.03284v1",
      "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments",
      "title_zh": "Edge-FITï¼šé¢å‘éšç§ä¿æŠ¤æ™ºèƒ½å®¶å±…ç¯å¢ƒçš„é‡åŒ–å¤§è¯­è¨€æ¨¡å‹è”é‚¦æŒ‡ä»¤å¾®è°ƒ",
      "authors": [
        "Vinay Venkatesh",
        "Vamsidhar R Kamanuru",
        "Lav Kumar",
        "Nikita Kothari"
      ],
      "abstract": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a scalable framework for Federated Instruction Tuning (FIT) of Large Language Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT framework combines federated learning with 4-bit Quantized Low-Rank Adaptation (QLORA), mitigating the core issues of communication and computational overhead. We demonstrate this by filtering the general-purpose Databricks Dolly 15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable framework for decentralized LLM deployment on home compute gateways.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Edge-FITï¼Œä¸€ç§ä¸“ä¸ºéšç§ä¿æŠ¤çš„æ™ºèƒ½å®¶å±…ç¯å¢ƒè®¾è®¡çš„è¾¹ç¼˜è”é‚¦æŒ‡ä»¤å¾®è°ƒ (Federated Instruction Tuning) æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿè”é‚¦å­¦ä¹  (Federated Learning) åœ¨å¤„ç†å¤§è§„æ¨¡å‚æ•°çš„å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ—¶é¢ä¸´çš„é€šä¿¡å’Œè®¡ç®—å¼€é”€è¿‡å¤§ç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°å°†è”é‚¦å­¦ä¹ ä¸ 4-bit é‡åŒ–ä½ç§©è‡ªé€‚åº” (4-bit QLoRA) æŠ€æœ¯ç›¸ç»“åˆã€‚ç ”ç©¶äººå‘˜é€šè¿‡ä» Databricks Dolly 15k æ•°æ®é›†ä¸­ç­›é€‰ IoT é¢†åŸŸçš„æŒ‡ä»¤æ•°æ®ï¼Œå¯¹ Llama 2 (7B) ç­‰æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹æ€§å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡ Edge-FIT å¾®è°ƒçš„ Llama 2 (7B) æ¨¡å‹è¾¾åˆ°äº† 0.89 çš„ F1-Scoreã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡ Phi-3-mini (3.8B) æ¨¡å‹å±•ç¤ºäº†åœ¨å®¶åº­è®¡ç®—ç½‘å…³ä¸Šè¿›è¡Œå»ä¸­å¿ƒåŒ–éƒ¨ç½²çš„æ€§èƒ½æƒè¡¡ã€‚è¯¥ç ”ç©¶éªŒè¯äº† Edge-FIT æ¡†æ¶çš„å¯æ‰©å±•æ€§ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²é«˜æ•ˆä¸”éšç§å®‰å…¨çš„ LLMs æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2510.03284v1",
      "published_date": "2025-09-28 20:06:37 UTC",
      "updated_date": "2025-09-28 20:06:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:38.885927+00:00"
    },
    {
      "arxiv_id": "2509.24046v2",
      "title": "PartnerMAS: An LLM Hierarchical Multi-Agent Framework for Business Partner Selection on High-Dimensional Features",
      "title_zh": "PartnerMASï¼šé¢å‘é«˜ç»´ç‰¹å¾å•†ä¸šä¼™ä¼´é€‰æ‹©çš„ LLM åˆ†å±‚å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Lingyao Li",
        "Haolun Wu",
        "Zhenkun Li",
        "Jiabei Hu",
        "Yu Wang",
        "Xiaoshan Huang",
        "Wenyue Hua",
        "Wenqian Wang"
      ],
      "abstract": "High-dimensional decision-making tasks, such as business partner selection, involve evaluating large candidate pools with heterogeneous numerical, categorical, and textual features. While large language models (LLMs) offer strong in-context reasoning capabilities, single-agent or debate-style systems often struggle with scalability and consistency in such settings. We propose PartnerMAS, a hierarchical multi-agent framework that decomposes evaluation into three layers: a Planner Agent that designs strategies, Specialized Agents that perform role-specific assessments, and a Supervisor Agent that integrates their outputs. To support systematic evaluation, we also introduce a curated benchmark dataset of venture capital co-investments, featuring diverse firm attributes and ground-truth syndicates. Across 140 cases, PartnerMAS consistently outperforms single-agent and debate-based multi-agent baselines, achieving up to 10--15\\% higher match rates. Analysis of agent reasoning shows that planners are most responsive to domain-informed prompts, specialists produce complementary feature coverage, and supervisors play an important role in aggregation. Our findings demonstrate that structured collaboration among LLM agents can generate more robust outcomes than scaling individual models, highlighting PartnerMAS as a promising framework for high-dimensional decision-making in data-rich domains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PartnerMASï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é«˜ç»´ç‰¹å¾ (High-Dimensional Features) ä¸‹å•†ä¸šåˆä½œä¼™ä¼´é€‰æ‹©çš„å±‚æ¬¡åŒ–å¤šæ™ºèƒ½ä½“ (Hierarchical Multi-Agent) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å¤æ‚å¼‚æ„æ•°æ®æ—¶çš„å¯æ‰©å±•æ€§ä¸ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ Planner Agent è®¾è®¡ç­–ç•¥ã€Specialized Agents æ‰§è¡Œç‰¹å®šè§’è‰²è¯„ä¼°ä»¥åŠ Supervisor Agent æ•´åˆè¾“å‡ºçš„ä¸‰å±‚ç»“æ„å®ç°ç³»ç»ŸåŒ–å†³ç­–ã€‚ç ”ç©¶åŒæ­¥æ¨å‡ºäº†ä¸€ä¸ªåŸºäºé£é™©æŠ•èµ„å…±åŒæŠ•èµ„çš„åŸºå‡†æ•°æ®é›† (Benchmark Dataset)ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–çš„ä¼ä¸šå±æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒPartnerMAS åœ¨å¤šä¸ªæ¡ˆä¾‹ä¸­çš„åŒ¹é…ç‡æ¯”å•æ™ºèƒ½ä½“åŸºçº¿é«˜å‡º 10-15%ï¼Œè¯æ˜äº†ç»“æ„åŒ–åä½œçš„ä¼˜è¶Šæ€§ã€‚åˆ†æç¡®è®¤ Planner å¯¹é¢†åŸŸæç¤ºå“åº”çµæ•ï¼ŒSpecialists å®ç°äº†äº’è¡¥çš„ç‰¹å¾è¦†ç›–ï¼Œè€Œ Supervisor åœ¨ç»“æœèšåˆä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚è¯¥ç ”ç©¶å¼ºè°ƒï¼Œåœ¨æ•°æ®å¯†é›†å‹é¢†åŸŸçš„å¤æ‚å†³ç­–ä¸­ï¼Œæ™ºèƒ½ä½“é—´çš„ç»“æ„åŒ–åä½œæ¯”å•çº¯æå‡å•ä¸ªæ¨¡å‹è§„æ¨¡æ›´å…·é²æ£’æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24046v2",
      "published_date": "2025-09-28 19:39:03 UTC",
      "updated_date": "2025-10-31 02:48:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:56.897843+00:00"
    },
    {
      "arxiv_id": "2509.24039v1",
      "title": "End-to-end Topographic Auditory Models Replicate Signatures of Human Auditory Cortex",
      "title_zh": "ç«¯åˆ°ç«¯æ‹“æ‰‘å¬è§‰æ¨¡å‹é‡ç°äººç±»å¬è§‰çš®å±‚ç‰¹å¾",
      "authors": [
        "Haider Al-Tahan",
        "Mayukh Deb",
        "Jenelle Feather",
        "N. Apurva Ratan Murty"
      ],
      "abstract": "The human auditory cortex is topographically organized. Neurons with similar response properties are spatially clustered, forming smooth maps for acoustic features such as frequency in early auditory areas, and modular regions selective for music and speech in higher-order cortex. Yet, evaluations for current computational models of auditory perception do not measure whether such topographic structure is present in a candidate model. Here, we show that cortical topography is not present in the previous best-performing models at predicting human auditory fMRI responses. To encourage the emergence of topographic organization, we adapt a cortical wiring-constraint loss originally designed for visual perception. The new class of topographic auditory models, TopoAudio, are trained to classify speech, and environmental sounds from cochleagram inputs, with an added constraint that nearby units on a 2D cortical sheet develop similar tuning. Despite these additional constraints, TopoAudio achieves high accuracy on benchmark tasks comparable to the unconstrained non-topographic baseline models. Further, TopoAudio predicts the fMRI responses in the brain as well as standard models, but unlike standard models, TopoAudio develops smooth, topographic maps for tonotopy and amplitude modulation (common properties of early auditory representation, as well as clustered response modules for music and speech (higher-order selectivity observed in the human auditory cortex). TopoAudio is the first end-to-end biologically grounded auditory model to exhibit emergent topography, and our results emphasize that a wiring-length constraint can serve as a general-purpose regularization tool to achieve biologically aligned representations.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»å¬è§‰çš®å±‚çš„åœ°å½¢ç»„ç»‡ï¼ˆTopographic Organizationï¼‰ï¼Œå¹¶æŒ‡å‡ºæ­¤å‰çš„è®¡ç®—æ¨¡å‹è™½ç„¶èƒ½æœ‰æ•ˆé¢„æµ‹è„‘éƒ¨æ´»åŠ¨ï¼Œä½†æ™®éç¼ºä¹è¿™ç§ç©ºé—´èšé›†çš„ç»“æ„ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†TopoAudioï¼Œè¿™æ˜¯ä¸€ç±»åŸºäºè€³èœ—å›¾è¾“å…¥è®­ç»ƒçš„æ–°å‹ç«¯åˆ°ç«¯å¬è§‰æ¨¡å‹ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»å¯¹è¯­éŸ³å’Œç¯å¢ƒéŸ³çš„å¤„ç†è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥æºè‡ªè§†è§‰ç ”ç©¶çš„çš®å±‚å¸ƒçº¿çº¦æŸæŸå¤±ï¼ˆCortical Wiring-constraint Lossï¼‰ï¼Œè¯¥æ¨¡å‹å¼ºåˆ¶è¦æ±‚æ¨¡æ‹Ÿçš®å±‚ä¸Šçš„é‚»è¿‘å•å…ƒå±•ç°ç›¸ä¼¼çš„è°ƒè°ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTopoAudioåœ¨åŸºå‡†ä»»åŠ¡å’ŒfMRIååº”é¢„æµ‹ä¸Šçš„è¡¨ç°ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸å½“ï¼Œä¸”è‡ªå‘å½¢æˆäº†è¯¸å¦‚é¢‘ç‡å®šä½ï¼ˆTonotopyï¼‰ã€æŒ¯å¹…è°ƒåˆ¶å›¾ä»¥åŠéŸ³ä¹å’Œè¯­éŸ³é€‰æ‹©æ€§æ¨¡å—ç­‰å…³é”®åœ°å½¢ç‰¹å¾ã€‚ä½œä¸ºé¦–ä¸ªå…·å¤‡è‡ªå‘åœ°å½¢ç»„ç»‡çš„ç”Ÿç‰©å­¦çº¦æŸå¬è§‰æ¨¡å‹ï¼Œè¯¥æˆæœè¯æ˜äº†å¸ƒçº¿é•¿åº¦çº¦æŸï¼ˆWiring-length Constraintï¼‰æ˜¯å®ç°ç”Ÿç‰©å¯¹é½ï¼ˆBiologically Alignedï¼‰è¡¨å¾çš„é€šç”¨æ­£åˆ™åŒ–å·¥å…·ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "cs.SD"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24039v1",
      "published_date": "2025-09-28 19:20:30 UTC",
      "updated_date": "2025-09-28 19:20:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:44.894839+00:00"
    },
    {
      "arxiv_id": "2509.24031v2",
      "title": "GPS-MTM: Capturing Pattern of Normalcy in GPS-Trajectories with self-supervised learning",
      "title_zh": "GPS-MTMï¼šåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ æ•æ‰GPSè½¨è¿¹ä¸­çš„å¸¸æ€æ¨¡å¼",
      "authors": [
        "Umang Garg",
        "Bowen Zhang",
        "Anantajit Subrahmanya",
        "Chandrakanth Gudavalli",
        "BS Manjunath"
      ],
      "abstract": "Foundation models have driven remarkable progress in text, vision, and video understanding, and are now poised to unlock similar breakthroughs in trajectory modeling. We introduce the GPSMasked Trajectory Transformer (GPS-MTM), a foundation model for large-scale mobility data that captures patterns of normalcy in human movement. Unlike prior approaches that flatten trajectories into coordinate streams, GPS-MTM decomposes mobility into two complementary modalities: states (point-of-interest categories) and actions (agent transitions). Leveraging a bi-directional Transformer with a self-supervised masked modeling objective, the model reconstructs missing segments across modalities, enabling it to learn rich semantic correlations without manual labels. Across benchmark datasets, including Numosim-LA, Urban Anomalies, and Geolife, GPS-MTM consistently outperforms on downstream tasks such as trajectory infilling and next-stop prediction. Its advantages are most pronounced in dynamic tasks (inverse and forward dynamics), where contextual reasoning is critical. These results establish GPS-MTM as a robust foundation model for trajectory analytics, positioning mobility data as a first-class modality for large-scale representation learning. Code is released for further reference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GPS-MTM (GPS Masked Trajectory Transformer)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡ç§»åŠ¨æ•°æ®çš„åŸºç¡€æ¨¡å‹ (foundation model)ï¼Œæ—¨åœ¨æ•æ‰äººç±»ç§»åŠ¨ä¸­çš„å¸¸æ€æ¨¡å¼ã€‚ä¸åŒäºä»¥å¾€å°†è½¨è¿¹ç®€åŒ–ä¸ºåæ ‡æµçš„æ–¹æ³•ï¼ŒGPS-MTM å°†ç§»åŠ¨æ€§åˆ†è§£ä¸ºçŠ¶æ€ (POI categories) å’ŒåŠ¨ä½œ (agent transitions) ä¸¤ä¸ªäº’è¡¥æ¨¡æ€ã€‚é€šè¿‡ç»“åˆåŒå‘ Transformer æ¶æ„ä¸è‡ªç›‘ç£æ©ç å»ºæ¨¡ (self-supervised masked modeling) ç›®æ ‡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè·¨æ¨¡æ€é‡å»ºç¼ºå¤±ç‰‡æ®µå¹¶å­¦ä¹ æ·±å±‚è¯­ä¹‰å…³è”ã€‚åœ¨ Numosim-LAã€Urban Anomalies å’Œ Geolife ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è½¨è¿¹å¡«å……å’Œä¸‹ä¸€ç«™é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠä¸Šä¸‹æ–‡æ¨ç†çš„åŠ¨æ€ä»»åŠ¡ä¸­ï¼ŒGPS-MTM å±•ç°äº†æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸ºè½¨è¿¹åˆ†æé¢†åŸŸçš„å¤§è§„æ¨¡è¡¨ç¤ºå­¦ä¹ æä¾›äº†ç¨³å¥çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.24031v2",
      "published_date": "2025-09-28 19:00:50 UTC",
      "updated_date": "2025-10-08 08:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:21:51.684021+00:00"
    },
    {
      "arxiv_id": "2509.24030v1",
      "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures",
      "title_zh": "ä»è¾¹ç¼˜åˆ°HPCï¼šè·¨è®¾æ–½æ•°æ®æµæ¶æ„ç ”ç©¶",
      "authors": [
        "Anjus George",
        "Michael Brim",
        "Christopher Zimmer",
        "David Rogers",
        "Sarp Oral",
        "Zach Mayes"
      ],
      "abstract": "In this paper, we investigate three cross-facility data streaming architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed Service Streaming (MSS). We examine their architectural variations in data flow paths and deployment feasibility, and detail their implementation using the Data Streaming to HPC (DS2HPC) architectural framework and the SciStream memory-to-memory streaming toolkit on the production-grade Advanced Computing Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility (OLCF). We present a workflow-specific evaluation of these architectures using three synthetic workloads derived from the streaming characteristics of scientific workflows. Through simulated experiments, we measure streaming throughput, round-trip time, and overhead under work sharing, work sharing with feedback, and broadcast and gather messaging patterns commonly found in AI-HPC communication motifs. Our study shows that DTS offers a minimal-hop path, resulting in higher throughput and lower latency, whereas MSS provides greater deployment feasibility and scalability across multiple users but incurs significant overhead. PRS lies in between, offering a scalable architecture whose performance matches DTS in most cases.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†ä¸‰ç§è·¨è®¾æ–½æ•°æ®æµæ¶æ„ï¼šç›´æ¥æµå¼ä¼ è¾“(Direct Streaming, DTS)ã€ä»£ç†æµå¼ä¼ è¾“(Proxied Streaming, PRS)å’Œæ‰˜ç®¡æœåŠ¡æµå¼ä¼ è¾“(Managed Service Streaming, MSS)ï¼Œå¹¶åˆ†æäº†å®ƒä»¬åœ¨æ•°æ®æµè·¯å¾„å’Œéƒ¨ç½²å¯è¡Œæ€§æ–¹é¢çš„å·®å¼‚ã€‚ç ”ç©¶åŸºäº Data Streaming to HPC (DS2HPC) æ¶æ„æ¡†æ¶å’Œ SciStream å†…å­˜æµå¼ä¼ è¾“å·¥å…·åŒ…ï¼Œåœ¨æ©¡æ ‘å²­é¢†å¯¼è®¡ç®—è®¾æ–½(OLCF)çš„ç”Ÿäº§çº§ ACE åŸºç¡€è®¾æ–½ä¸Šå®ç°äº†è¿™äº›æ¶æ„ã€‚é€šè¿‡æ¨¡æ‹Ÿ AI-HPC é€šä¿¡ä¸­å¸¸è§çš„ä»»åŠ¡å…±äº«ã€åé¦ˆä»¥åŠå¹¿æ’­ä¸èšåˆç­‰æ¶ˆæ¯æ¨¡å¼ï¼Œç ”ç©¶äººå‘˜è¯„ä¼°äº†å„æ¶æ„åœ¨ååé‡ã€å¾€è¿”æ—¶é—´å’Œç³»ç»Ÿå¼€é”€æ–¹é¢çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDTS å‡­å€Ÿæœ€å°è·³æ•°è·¯å¾„åœ¨ååé‡å’Œå»¶è¿Ÿæ–¹é¢è¡¨ç°æœ€ä¼˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMSS è™½ç„¶å…·å¤‡æ›´å¼ºçš„éƒ¨ç½²å¯è¡Œæ€§å’Œå¤šç”¨æˆ·æ‰©å±•æ€§ï¼Œä½†ä¼šäº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½å¼€é”€ã€‚PRS åˆ™ä½œä¸ºä¸€ç§æŠ˜ä¸­æ–¹æ¡ˆï¼Œåœ¨æä¾›æ¶æ„æ‰©å±•æ€§çš„åŒæ—¶ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹èƒ½ä¿æŒä¸ DTS ç›¸å½“çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24030v1",
      "published_date": "2025-09-28 18:54:38 UTC",
      "updated_date": "2025-09-28 18:54:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:15.688859+00:00"
    },
    {
      "arxiv_id": "2510.03283v1",
      "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment",
      "title_zh": "MACEï¼šæ”¯æŒ SLO æ„ŸçŸ¥å‹æŒç»­é‡è®­ç»ƒå¯¹é½çš„åŒä½æ··åˆ LLM æœåŠ¡ç³»ç»Ÿ",
      "authors": [
        "Yufei Li",
        "Yu Fu",
        "Yue Dong",
        "Cong Liu"
      ],
      "abstract": "Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MACEï¼Œä¸€ç§é’ˆå¯¹è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è®¾è®¡çš„æ··åˆæ¨ç†ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³é¢‘ç¹é‡è®­ç»ƒ(retraining)å¯¼è‡´çš„æ¨ç†å»¶è¿Ÿä¸æ¨¡å‹ç²¾åº¦ä¹‹é—´çš„å†²çªã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œè¿­ä»£çº§è°ƒåº¦(iteration-level scheduling)æ˜¯é€‚åº”æ¨¡å‹æ¼‚ç§»ä¸”ä¸è¿åæœåŠ¡æ°´å¹³ç›®æ ‡(SLOs)çš„å…³é”®ã€‚MACEé€šè¿‡æ™ºèƒ½å†…å­˜ç®¡ç†å®ç°äº†å¹¶å‘æ¨ç†(prefill, decode)ä¸å¾®è°ƒ(fine-tuning)çš„åŒä½ç½®éƒ¨ç½²ï¼Œå¹¶æ ¹æ®æ¨¡å‹æ›´æ–°å¯¹è¾“å‡ºå¯¹é½(output alignment)çš„ä¸åŒå½±å“æ¥ä¼˜åŒ–åˆ†é…GPUå‘¨æœŸã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMACEåœ¨ä¿æŒååé‡çš„åŒæ—¶ï¼Œæ¯”æŒç»­é‡è®­ç»ƒæ–¹æ³•é™ä½äº†é«˜è¾¾63%çš„æ¨ç†å»¶è¿Ÿï¼Œå¹¶åœ¨NVIDIA AGX Orinå¹³å°ä¸Šç»´æŒäº†85%ä»¥ä¸Šçš„GPUåˆ©ç”¨ç‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¿­ä»£çº§æ··åˆè°ƒåº¦æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜å¹³å°ä¸Šå®ç°å…·æœ‰æŒç»­å­¦ä¹ (continual learning)èƒ½åŠ›çš„LLMéƒ¨ç½²çš„æœ‰æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 15 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03283v1",
      "published_date": "2025-09-28 18:45:28 UTC",
      "updated_date": "2025-09-28 18:45:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:26.592056+00:00"
    },
    {
      "arxiv_id": "2509.24008v3",
      "title": "FrameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning",
      "title_zh": "FrameMindï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¸§äº¤ç»‡è§†é¢‘æ¨ç†",
      "authors": [
        "Haonan Ge",
        "Yiwei Wang",
        "Kai-Wei Chang",
        "Hang Wu",
        "Yujun Cai"
      ],
      "abstract": "Current video understanding models rely on fixed frame sampling strategies, processing predetermined visual inputs regardless of the specific reasoning requirements of each question. This static approach limits their ability to adaptively gather visual evidence, leading to suboptimal performance on tasks that require either broad temporal coverage or fine-grained spatial detail. In this paper, we introduce FrameMind, an end-to-end framework trained with reinforcement learning that enables models to dynamically request visual information during reasoning through Frame-Interleaved Chain-of-Thought (FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns where the model alternates between textual reasoning and active visual perception, using tools to extract targeted frames or video clips based on identified knowledge gaps. To train effective dynamic sampling policies, we propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to diverse temporal-spatial trade-offs during learning, and DRFS-GRPO, a group-relative policy optimization algorithm that learns from outcome-based rewards without requiring frame-level annotations. Extensive experiments on challenging benchmarks like MLVU and VideoMME demonstrate that our method significantly outperforms existing models, advancing the state of the art in flexible and efficient video understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FrameMindï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰è®­ç»ƒçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç†è§£æ¨¡å‹å› å›ºå®šå¸§é‡‡æ ·ç­–ç•¥è€Œæ— æ³•æ ¹æ®ç‰¹å®šé—®é¢˜éœ€æ±‚è‡ªé€‚åº”è·å–è§†è§‰è¯æ®çš„å±€é™æ€§ã€‚FrameMind å¼•å…¥äº† Frame-Interleaved Chain-of-Thought (FiCOT) æœºåˆ¶ï¼Œå…è®¸æ¨¡å‹åœ¨æ–‡æœ¬æ¨ç†ä¸ä¸»åŠ¨è§†è§‰æ„ŸçŸ¥ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œåˆ©ç”¨å·¥å…·åŠ¨æ€æå–æ‰€éœ€çš„è§†é¢‘å¸§æˆ–ç‰‡æ®µä»¥å¡«è¡¥çŸ¥è¯†ç¼ºå£ã€‚ä¸ºè®­ç»ƒé«˜æ•ˆçš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº† Dynamic Resolution Frame Sampling (DRFS) ä»¥åŠä¸€ç§æ— éœ€å¸§çº§æ ‡æ³¨ã€ä»…é€šè¿‡ç»“æœå¥–åŠ±è¿›è¡Œå­¦ä¹ çš„ DRFS-GRPO ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚åœ¨ MLVU å’Œ VideoMME ç­‰å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®éªŒç»“æœè¯æ˜ FrameMind æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨çµæ´»ä¸”é«˜æ•ˆçš„è§†é¢‘ç†è§£é¢†åŸŸå®ç°äº†æœ€å…ˆè¿›(State-of-the-Art)çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Underreview",
      "pdf_url": "https://arxiv.org/pdf/2509.24008v3",
      "published_date": "2025-09-28 17:59:43 UTC",
      "updated_date": "2025-10-05 15:20:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:31.594825+00:00"
    },
    {
      "arxiv_id": "2509.24006v2",
      "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
      "title_zh": "SLAï¼šé€šè¿‡å¯å¾®è°ƒçš„ç¨€ç–çº¿æ€§æ³¨æ„åŠ›çªç ´æ‰©æ•£ Transformer çš„ç¨€ç–æ€§",
      "authors": [
        "Jintao Zhang",
        "Haoxu Wang",
        "Kai Jiang",
        "Shuo Yang",
        "Kaiwen Zheng",
        "Haocheng Xi",
        "Ziteng Wang",
        "Hongzhou Zhu",
        "Min Zhao",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "abstract": "In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B. The code is available at https://github.com/thu-ml/SLA.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SLA (Sparse-Linear Attention)ï¼Œæ—¨åœ¨è§£å†³Diffusion Transformer (DiT) æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆç­‰é•¿åºåˆ—ä»»åŠ¡ä¸­é¢ä¸´çš„æ³¨æ„åŠ›è®¡ç®—å»¶è¿Ÿç“¶é¢ˆã€‚ç ”ç©¶å‘ç°æ³¨æ„åŠ›æƒé‡å¯åˆ†ä¸ºæå°‘æ•°é«˜ç§©çš„å¤§æƒé‡å’Œå…¶ä½™ä½ç§©æƒé‡ï¼ŒåŸºäºæ­¤æå‡ºäº†èåˆç¨€ç–æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›çš„å¯å¾®è°ƒåŠ é€Ÿæ–¹æ³•ã€‚SLAå°†æƒé‡ç»†åˆ†ä¸ºå…³é”®ã€è¾¹ç¼˜å’Œå¯å¿½ç•¥ä¸‰ç±»ï¼Œåˆ†åˆ«åº”ç”¨O(N^2)è®¡ç®—ã€O(N)çº¿æ€§è®¡ç®—æˆ–ç›´æ¥è·³è¿‡ï¼Œå¹¶å°†å…¶é›†æˆåœ¨æ”¯æŒæ­£åå‘ä¼ æ’­çš„é«˜æ•ˆGPU kernelä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œä»…éœ€å°‘é‡å¾®è°ƒï¼ŒSLAå³å¯åœ¨ä¸æŸå¤±ç”Ÿæˆè´¨é‡çš„å‰æä¸‹å‡å°‘95%çš„æ³¨æ„åŠ›è®¡ç®—é‡ã€‚åœ¨Wan2.1-1.3Bæ¨¡å‹ä¸Šçš„æµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†13.7å€çš„æ³¨æ„åŠ›è®¡ç®—åŠ é€Ÿå’Œ2.2å€çš„ç«¯åˆ°ç«¯è§†é¢‘ç”ŸæˆåŠ é€Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.24006v2",
      "published_date": "2025-09-28 17:58:59 UTC",
      "updated_date": "2025-11-19 14:34:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:30.696285+00:00"
    },
    {
      "arxiv_id": "2509.24002v1",
      "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use",
      "title_zh": "MCPMarkï¼šé’ˆå¯¹çœŸå®ä¸”å…¨é¢ MCP åº”ç”¨åœºæ™¯çš„å‹åŠ›æµ‹è¯•åŸºå‡†",
      "authors": [
        "Zijian Wu",
        "Xiangyan Liu",
        "Xinyuan Zhang",
        "Lingjun Chen",
        "Fanqing Meng",
        "Lingxiao Du",
        "Yiran Zhao",
        "Fanshi Zhang",
        "Yaoqi Ye",
        "Jiawei Wang",
        "Zirui Wang",
        "Jinjie Ni",
        "Yufan Yang",
        "Arvin Xu",
        "Michael Qizhe Shieh"
      ],
      "abstract": "MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MCPMarkï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¯¹ Model Context Protocol (MCP) åœ¨ç°å®åº”ç”¨ä¸­çš„å…¨é¢æ€§è¿›è¡Œå‹åŠ›æµ‹è¯•çš„åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰ MCP åŸºå‡†æµ‹è¯•äº¤äº’æ·±åº¦æœ‰é™ä¸”ç¼ºä¹å¤æ‚æ€§çš„é—®é¢˜ï¼ŒMCPMark åŒ…å«äº† 127 ä¸ªç”±é¢†åŸŸä¸“å®¶å’Œ AI æ™ºèƒ½ä½“åä½œåˆ›å»ºçš„é«˜è´¨é‡ä»»åŠ¡ï¼Œæ¶µç›–äº†å¹¿æ³›çš„åˆ›å»ºã€è¯»å–ã€æ›´æ–°å’Œåˆ é™¤ (CRUD) æ“ä½œã€‚æ¯ä¸ªä»»åŠ¡å‡é…æœ‰ç²¾å¿ƒè®¾è®¡çš„åˆå§‹çŠ¶æ€å’Œè‡ªåŠ¨éªŒè¯è„šæœ¬ï¼Œè¦æ±‚æ¨¡å‹ä¸ç¯å¢ƒè¿›è¡Œæ›´ä¸°å¯Œä¸”å¤šæ ·åŒ–çš„äº¤äº’ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„ gpt-5-medium æ¨¡å‹åœ¨ pass@1 æŒ‡æ ‡ä¸Šä¹Ÿä»…è¾¾åˆ° 52.56%ï¼Œè€Œ claude-sonnet-4 å’Œ o3 ç­‰å¼ºåŠ›æ¨¡å‹åˆ™ä½äº 30%ã€‚å¹³å‡æ¯ä¸ªä»»åŠ¡æ¶‰åŠ 16.2 ä¸ªæ‰§è¡Œè½®æ¬¡å’Œ 17.4 æ¬¡å·¥å…·è°ƒç”¨ (tool calls)ï¼Œå…¶éš¾åº¦æ˜¾è‘—è¶…è¿‡ä»¥å¾€åŸºå‡†ã€‚è¿™ä¸€ç ”ç©¶ç»“æœçªæ˜¾äº† MCPMark çš„å‹åŠ›æµ‹è¯•ç‰¹æ€§ï¼Œä¸ºè¯„ä¼°é€šç”¨æ™ºèƒ½ä½“ä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’çš„æ ¸å¿ƒèƒ½åŠ›æä¾›äº†æ›´ä¸¥è‹›ä¸”å…¨é¢çš„è¡¡é‡æ ‡å‡†ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "42 pages, 27 figures, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.24002v1",
      "published_date": "2025-09-28 17:53:27 UTC",
      "updated_date": "2025-09-28 17:53:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:38.484060+00:00"
    },
    {
      "arxiv_id": "2509.23996v1",
      "title": "Future-Proofing Programmers: Optimal Knowledge Tracing for AI-Assisted Personalized Education",
      "title_zh": "é¢å‘æœªæ¥çš„ç¨‹åºå‘˜ï¼šAI è¾…åŠ©ä¸ªæ€§åŒ–æ•™è‚²ä¸­çš„æœ€ä¼˜çŸ¥è¯†è¿½è¸ª",
      "authors": [
        "Yuchen Wang",
        "Pei-Duo Yu",
        "Chee Wei Tan"
      ],
      "abstract": "Learning to learn is becoming a science, driven by the convergence of knowledge tracing, signal processing, and generative AI to model student learning states and optimize education. We propose CoTutor, an AI-driven model that enhances Bayesian Knowledge Tracing with signal processing techniques to improve student progress modeling and deliver adaptive feedback and strategies. Deployed as an AI copilot, CoTutor combines generative AI with adaptive learning technology. In university trials, it has demonstrated measurable improvements in learning outcomes while outperforming conventional educational tools. Our results highlight its potential for AI-driven personalization, scalability, and future opportunities for advancing privacy and ethical considerations in educational technology. Inspired by Richard Hamming's vision of computer-aided 'learning to learn,' CoTutor applies convex optimization and signal processing to automate and scale up learning analytics, while reserving pedagogical judgment for humans, ensuring AI facilitates the process of knowledge tracing while enabling learners to uncover new insights.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CoTutorï¼Œä¸€ç§AIé©±åŠ¨çš„ä¸ªæ€§åŒ–æ•™è‚²æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–çŸ¥è¯†è¿½è¸ª(knowledge tracing)æå‡å­¦ä¹ è€…çš„å­¦ä¹ æ•ˆç‡ã€‚è¯¥æ¨¡å‹åˆ›æ–°æ€§åœ°ç»“åˆäº†ä¿¡å·å¤„ç†(signal processing)æŠ€æœ¯æ¥å¢å¼ºè´å¶æ–¯çŸ¥è¯†è¿½è¸ª(Bayesian Knowledge Tracing)ï¼Œä»è€Œæ›´ç²¾å‡†åœ°å»ºæ¨¡å­¦ç”Ÿçš„å­¦ä¹ çŠ¶æ€å¹¶æä¾›è‡ªé€‚åº”åé¦ˆã€‚CoTutorä½œä¸ºAIå‰¯é©¾é©¶(AI copilot)éƒ¨ç½²ï¼Œå°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(generative AI)ä¸å‡¸ä¼˜åŒ–(convex optimization)ç›¸ç»“åˆï¼Œå®ç°äº†å­¦ä¹ åˆ†æçš„è‡ªåŠ¨åŒ–ä¸è§„æ¨¡åŒ–ã€‚å¤§å­¦è¯•éªŒè¯æ˜ï¼ŒCoTutoråœ¨æå‡å­¦ä¹ æˆæ•ˆæ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ•™è‚²å·¥å…·ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸ªæ€§åŒ–æ•™è‚²å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä¸ä»…è·µè¡Œäº†è®¡ç®—æœºè¾…åŠ©â€œå­¦ä¼šå­¦ä¹ â€çš„æ„¿æ™¯ï¼Œä¹Ÿä¸ºå¤„ç†æ•™è‚²æŠ€æœ¯ä¸­çš„éšç§å’Œä¼¦ç†æŒ‘æˆ˜æä¾›äº†æ–°çš„æ€è·¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The paper is accepted to IEEE Signal Processing Magazine, Special Issue on Artificial Intelligence for Education",
      "pdf_url": "https://arxiv.org/pdf/2509.23996v1",
      "published_date": "2025-09-28 17:40:39 UTC",
      "updated_date": "2025-09-28 17:40:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:48.987071+00:00"
    },
    {
      "arxiv_id": "2509.23994v2",
      "title": "Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents",
      "title_zh": "Policy-as-Promptï¼šå°† AI æ²»ç†è§„åˆ™è½¬åŒ–ä¸º AI æ™ºèƒ½ä½“æŠ¤æ ",
      "authors": [
        "Gauri Kholkar",
        "Ratinder Ahuja"
      ],
      "abstract": "As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For conformity assessment, it provides complete provenance, traceability, and audit logging, all integrated with a human-in-the-loop review process. Evaluations show our system reduces prompt-injection risk, blocks out-of-scope requests, and limits toxic outputs. It also generates auditable rationales aligned with AI governance frameworks. By treating policies as executable prompts (a policy-as-code for agents), this approach enables secure-by-design deployment, continuous compliance, and scalable AI safety and AI security assurance for regulatable ML.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Policy-as-Promptæ–¹æ³•ï¼Œæ—¨åœ¨å°†AIæ²»ç†è§„åˆ™è½¬åŒ–ä¸ºAIæ™ºèƒ½ä½“(AI Agents)çš„å¯æ‰§è¡Œè¿è¡Œæ—¶æŠ¤æ (guardrails)ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå°†äº§å“éœ€æ±‚æ–‡æ¡£(PRDs)ã€æŠ€æœ¯è®¾è®¡æ–‡æ¡£(TDDs)å’Œä»£ç ç­‰éç»“æ„åŒ–è®¾è®¡å·¥ä»¶è½¬åŒ–ä¸ºå¯éªŒè¯çš„è¿è¡Œæ—¶ç›‘æ§æœºåˆ¶ã€‚å…¶æ ¸å¿ƒè¿‡ç¨‹åŒ…æ‹¬æ„å»ºé“¾æ¥è‡³æºæ–‡ä»¶çš„ç­–ç•¥æ ‘(policy tree)ï¼Œå¹¶å°†å…¶ç¼–è¯‘ä¸ºè½»é‡çº§çš„ã€åŸºäºæç¤ºçš„åˆ†ç±»å™¨(prompt-based classifiers)ï¼Œä»¥å®ç°å®æ—¶ç›‘æµ‹ã€‚ç³»ç»Ÿè®¾è®¡éµå¾ªæœ€å°æƒé™(least privilege)å’Œæ•°æ®æœ€å°åŒ–(data minimization)åŸåˆ™ï¼Œå¹¶é›†æˆäº†äººå·¥å‚ä¸(human-in-the-loop)çš„å®¡æŸ¥æµç¨‹ï¼Œæä¾›å®Œæ•´çš„æº¯æºã€å¯è¿½æº¯æ€§å’Œå®¡è®¡æ—¥å¿—ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæ˜¾è‘—é™ä½äº†æç¤ºæ³¨å…¥(prompt-injection)é£é™©ï¼Œæœ‰æ•ˆæ‹¦æˆªäº†è¶…èŒƒå›´è¯·æ±‚å¹¶é™åˆ¶äº†æœ‰æ¯’è¾“å‡ºã€‚é€šè¿‡å°†æ”¿ç­–è§†ä¸ºå¯æ‰§è¡Œæç¤ºï¼Œè¯¥æ–¹æ³•å®ç°äº†è®¾è®¡å®‰å…¨(secure-by-design)éƒ¨ç½²ã€æŒç»­åˆè§„ä»¥åŠå¯¹å¯ç›‘ç®¡æœºå™¨å­¦ä¹ (regulatable ML)çš„å¯æ‰©å±•å®‰å…¨æ€§ä¿éšœã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at 3rd Regulatable ML Workshop at NEURIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23994v2",
      "published_date": "2025-09-28 17:36:52 UTC",
      "updated_date": "2025-11-07 16:08:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:52.789693+00:00"
    },
    {
      "arxiv_id": "2509.23992v1",
      "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation",
      "title_zh": "GUIDEï¼šç”¨äº DAG ä¼°è®¡çš„å¹¿ä¹‰å…ˆéªŒä¸æ•°æ®ç¼–ç å™¨",
      "authors": [
        "Amartya Roy",
        "Devharish N",
        "Shreya Ganguly",
        "Kripabandhu Ghosh"
      ],
      "abstract": "Modern causal discovery methods face critical limitations in scalability, computational efficiency, and adaptability to mixed data types, as evidenced by benchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational energy demands, and continuous/non-continuous data handling. While traditional algorithms like PC, GES, and ICA-LiNGAM struggle with these challenges, exhibiting prohibitive energy costs for higher-order nodes and poor scalability beyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large Language Model (LLM)-generated adjacency matrices with observational data through a dual-encoder architecture. GUIDE uniquely optimizes computational efficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and KCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy over both NOTEARS and GraN-DAG individually. During training, GUIDE's reinforcement learning agent dynamically balances reward maximization (accuracy) and penalty avoidance (DAG constraints), enabling robust performance across mixed data types and scalability to $\\ge 70$ nodes -- a setting where baseline methods fail.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åä¸ºGUIDEçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°ä»£å› æœå‘ç°æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ã€è®¡ç®—æ•ˆç‡åŠå¤„ç†æ··åˆæ•°æ®ç±»å‹æ–¹é¢çš„å…³é”®å±€é™ã€‚é’ˆå¯¹PCã€GESå’ŒICA-LiNGAMç­‰ä¼ ç»Ÿç®—æ³•åœ¨èŠ‚ç‚¹è§„æ¨¡è¶…è¿‡70ä¸ªæ—¶é¢ä¸´çš„èƒ½è€—ä¸æ€§èƒ½ç“¶é¢ˆï¼ŒGUIDEé‡‡ç”¨äº†ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹(LLM)ç”Ÿæˆçš„é‚»æ¥çŸ©é˜µä¸è§‚æµ‹æ•°æ®çš„åŒç¼–ç å™¨(Dual-encoder)æ¶æ„ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ™ºèƒ½ä½“åŠ¨æ€å¹³è¡¡å‡†ç¡®ç‡å¥–åŠ±ä¸æœ‰å‘æ— ç¯å›¾(DAG)çº¦æŸæƒ©ç½šï¼Œä»è€Œåœ¨å¤æ‚ç¯å¢ƒä¸‹å®ç°é²æ£’çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGUIDEç›¸æ¯”RL-BICå’ŒKCRLç¼©çŸ­äº†çº¦42%çš„è¿è¡Œæ—¶é—´ï¼Œå¹¶åœ¨å‡†ç¡®ç‡ä¸Šè¾ƒNOTEARSå’ŒGraN-DAGå¹³å‡æå‡äº†117%ã€‚è¯¥æ–¹æ³•æˆåŠŸå®ç°äº†åœ¨70ä¸ªåŠä»¥ä¸ŠèŠ‚ç‚¹çš„å¤§è§„æ¨¡ç½‘ç»œä¸­çš„é«˜æ•ˆæ‰©å±•ï¼Œåœ¨åŸºçº¿æ–¹æ³•å¤±æ•ˆçš„é«˜ç»´åœºæ™¯ä¸­å±•ç°äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23992v1",
      "published_date": "2025-09-28 17:35:21 UTC",
      "updated_date": "2025-09-28 17:35:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:22:56.286104+00:00"
    },
    {
      "arxiv_id": "2509.23990v2",
      "title": "The Hidden Costs of Translation Accuracy: Distillation, Quantization, and Environmental Impact",
      "title_zh": "ç¿»è¯‘å‡†ç¡®æ€§çš„éšæ€§æˆæœ¬ï¼šè’¸é¦ã€é‡åŒ–ä¸ç¯å¢ƒå½±å“",
      "authors": [
        "Dhaathri Vijay",
        "Anandaswarup Vadapalli"
      ],
      "abstract": "The rapid expansion of large language models (LLMs) has heightened concerns about their computational and environmental costs. This study investigates the trade-offs between translation quality and efficiency by comparing full-scale, distilled, and quantized models using machine translation as a case study. We evaluated performance on the Flores+ benchmark and through human judgments of conversational translations in French, Hindi, and Kannada. Our analysis revealed that the full 3.3B FP32 model, while achieving the highest BLEU scores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per run). The distilled 600M FP32 model reduced inference time by 71-78% and carbon emissions by 63-65% compared with the full model, with only minimal reductions in BLEU scores. Human evaluations further showed that even aggressive quantization (INT4) preserved high levels of accuracy and fluency, with differences between models generally minor. These findings demonstrate that model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality, though trade-offs are more pronounced in low-resource settings. We argue for evaluation frameworks that integrate efficiency and sustainability alongside accuracy as central dimensions of progress in NLP.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ç¿»è¯‘è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ï¼Œé‡ç‚¹åˆ†æäº†å…¨è§„æ¨¡æ¨¡å‹ä¸ç»è¿‡è’¸é¦ï¼ˆdistilledï¼‰å’Œé‡åŒ–ï¼ˆquantizedï¼‰å¤„ç†åçš„æ¨¡å‹åœ¨æ€§èƒ½å’Œç¯å¢ƒå½±å“ä¸Šçš„å·®å¼‚ã€‚é€šè¿‡å¯¹ Flores+ åŸºå‡†æµ‹è¯•åŠå¤šç§è¯­è¨€çš„äººå·¥è¯„ä¼°ï¼Œç ”ç©¶å‘ç° 3.3B FP32 æ¨¡å‹è™½ç„¶ BLEU åˆ†æ•°æœ€é«˜ï¼Œä½†æ¯æ¬¡è¿è¡Œäº§ç”Ÿçš„ç¯å¢ƒè¶³è¿¹ä¹Ÿæœ€å¤§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œ600M FP32 è’¸é¦æ¨¡å‹åœ¨ä¿æŒæå°æ€§èƒ½æŸè€—çš„åŒæ—¶ï¼Œå°†æ¨ç†æ—¶é—´å‡å°‘äº† 71-78%ï¼Œç¢³æ’æ”¾é™ä½äº† 63-65%ã€‚å®éªŒç»“æœè¿›ä¸€æ­¥æ˜¾ç¤ºï¼Œå³ä¾¿é‡‡ç”¨æ¿€è¿›çš„ INT4 é‡åŒ–ï¼Œæ¨¡å‹ä»èƒ½ä¿æŒé«˜åº¦çš„å‡†ç¡®æ€§å’Œæµç•…åº¦ï¼Œå°½ç®¡è¿™ç§æƒè¡¡åœ¨ä½èµ„æºï¼ˆlow-resourceï¼‰åœºæ™¯ä¸‹è¡¨ç°å¾—æ›´ä¸ºæ˜¾è‘—ã€‚è¯¥è®ºæ–‡è®ºè¯äº†æ¨¡å‹å‹ç¼©ç­–ç•¥åœ¨ç»´æŒç¿»è¯‘è´¨é‡çš„åŒæ—¶é™ä½è®¡ç®—éœ€æ±‚çš„å¯è¡Œæ€§ï¼Œå¹¶æè®®å°†æ•ˆç‡å’Œå¯æŒç»­æ€§ä½œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è¿›æ­¥çš„æ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23990v2",
      "published_date": "2025-09-28 17:32:52 UTC",
      "updated_date": "2025-10-02 13:15:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:02.384768+00:00"
    },
    {
      "arxiv_id": "2509.23988v3",
      "title": "LLM/Agent-as-Data-Analyst: A Survey",
      "title_zh": "LLM/Agent-as-Data-Analystï¼šç»¼è¿°",
      "authors": [
        "Zirui Tang",
        "Weizheng Wang",
        "Zihang Zhou",
        "Yang Jiao",
        "Bangrui Xu",
        "Boyu Niu",
        "Dayou Zhou",
        "Xuanhe Zhou",
        "Guoliang Li",
        "Yeye He",
        "Wei Zhou",
        "Yitong Song",
        "Cheng Tan",
        "Xue Yang",
        "Chunwei Liu",
        "Bin Wang",
        "Conghui He",
        "Xiaoyang Wang",
        "Fan Wu"
      ],
      "abstract": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.",
      "tldr_zh": "è¿™ç¯‡ç»¼è¿°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å’Œæ™ºèƒ½ä½“(Agent)æŠ€æœ¯å¦‚ä½•æ”¹å˜æ•°æ®åˆ†æä»»åŠ¡çš„åŠŸèƒ½å’Œå¼€å‘èŒƒå¼ï¼Œå³LLM/Agent-as-Data-Analystæ¨¡å¼ï¼Œå±•ç°äº†å…¶åœ¨å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œçš„é‡å¤§å½±å“ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„åŸºäºè§„åˆ™æˆ–å°æ¨¡å‹çš„æ–¹æ³•ï¼ŒåŸºäºæ™ºèƒ½ä½“çš„LLMsèƒ½å¤Ÿå®ç°å¤æ‚çš„æ•°æ®ç†è§£ã€è‡ªç„¶è¯­è¨€æ¥å£ã€è¯­ä¹‰åˆ†æåŠŸèƒ½ä»¥åŠè‡ªä¸»æµæ°´çº¿ç¼–æ’ã€‚ç ”ç©¶ä»å¤šæ¨¡æ€è§’åº¦ç³»ç»Ÿå›é¡¾äº†é’ˆå¯¹ç»“æ„åŒ–æ•°æ®(å¦‚NL2SQL)ã€åŠç»“æ„åŒ–æ•°æ®(å¦‚Table QA)ã€éç»“æ„åŒ–æ•°æ®(å¦‚Chart Understanding)ä»¥åŠå¼‚æ„æ•°æ®(å¦‚Data Lakesä¸­çš„æ•°æ®æ£€ç´¢)çš„LLMæŠ€æœ¯ã€‚æ–‡ç« è¿›ä¸€æ­¥æç‚¼äº†æ™ºèƒ½æ•°æ®åˆ†ææ™ºèƒ½ä½“çš„å››ä¸ªå…³é”®è®¾è®¡ç›®æ ‡ï¼Œå³è¯­ä¹‰æ„ŸçŸ¥è®¾è®¡(Semantic-aware design)ã€è‡ªä¸»æµæ°´çº¿(Autonomous pipelines)ã€å·¥å…·å¢å¼ºå·¥ä½œæµ(Tool-augmented workflows)ä»¥åŠå¯¹å¼€æ”¾ä¸–ç•Œä»»åŠ¡çš„æ”¯æŒã€‚æœ€åï¼Œè¯¥ç»¼è¿°æ€»ç»“äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæ¨è¿›ç”±LLM/Agenté©±åŠ¨çš„æ•°æ®åˆ†ææå‡ºäº†å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„å®è·µæ–¹å‘å’Œæ·±å…¥è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "31 page, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23988v3",
      "published_date": "2025-09-28 17:31:38 UTC",
      "updated_date": "2025-10-27 02:52:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:14.229600+00:00"
    },
    {
      "arxiv_id": "2509.23986v1",
      "title": "TusoAI: Agentic Optimization for Scientific Methods",
      "title_zh": "TusoAIï¼šé¢å‘ç§‘å­¦æ–¹æ³•çš„æ™ºèƒ½ä½“åŒ–ä¼˜åŒ–",
      "authors": [
        "Alistair Turcan",
        "Kexin Huang",
        "Lei Li",
        "Martin Jinye Zhang"
      ],
      "abstract": "Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at https://github.com/Alistair-Turcan/TusoAI.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†TusoAIï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨å¼€å‘å’Œä¼˜åŒ–ç§‘å­¦è®¡ç®—æ–¹æ³•çš„æ™ºèƒ½ä½“(agentic)AIç³»ç»Ÿï¼Œä»¥è§£å†³ä¼ ç»Ÿæ‰‹åŠ¨å¼€å‘å·¥å…·å‘¨æœŸé•¿ã€æˆæœ¬é«˜çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ¥æ”¶ç§‘å­¦ä»»åŠ¡æè¿°å’Œè¯„ä¼°å‡½æ•°ï¼Œåˆ©ç”¨çŸ¥è¯†æ ‘(knowledge tree)è¡¨ç¤ºæ³•æ•´åˆé¢†åŸŸçŸ¥è¯†ï¼Œå¹¶è¿›è¡Œè¿­ä»£çš„é¢†åŸŸç‰¹å®šä¼˜åŒ–ä¸æ¨¡å‹è¯Šæ–­ã€‚åœ¨å•ç»†èƒRNA-seqæ•°æ®å»å™ªå’Œå«æ˜Ÿåœ°çƒç›‘æµ‹ç­‰å¤šç§åŸºå‡†ä»»åŠ¡ä¸­ï¼ŒTusoAIçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„ä¸“å®¶æ–¹æ³•ã€MLE agentsä»¥åŠç§‘å­¦AIæ™ºèƒ½ä½“ã€‚åœ¨é—ä¼ å­¦å®é™…åº”ç”¨ä¸­ï¼Œè¯¥ç³»ç»Ÿæ”¹è¿›äº†ç°æœ‰è®¡ç®—æ–¹æ³•å¹¶æ­ç¤ºäº†æ–°çš„ç”Ÿç‰©å­¦æœºåˆ¶ï¼ŒæˆåŠŸå‘ç°äº†9é¡¹è‡ªèº«å…ç–«æ€§ç–¾ç—…ä¸Tç»†èƒäºšå‹çš„æ–°å…³è”ï¼Œä»¥åŠ7ä¸ªæ­¤å‰æœªæŠ¥é“çš„ç–¾ç—…å˜å¼‚ä¸é¶åŸºå› ä¹‹é—´çš„è”ç³»ã€‚è¯¥ç ”ç©¶é€šè¿‡å°†éç»“æ„åŒ–çš„é¢†åŸŸçŸ¥è¯†è½¬åŒ–ä¸ºå¯ä¼˜åŒ–çš„è®¡ç®—æµç¨‹ï¼Œä¸ºåŠ é€Ÿç§‘å­¦å‘ç°æä¾›äº†é«˜æ•ˆçš„è‡ªä¸»åŒ–æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23986v1",
      "published_date": "2025-09-28 17:30:44 UTC",
      "updated_date": "2025-09-28 17:30:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:32.477939+00:00"
    },
    {
      "arxiv_id": "2509.23982v1",
      "title": "Toward Preference-aligned Large Language Models via Residual-based Model Steering",
      "title_zh": "é€šè¿‡åŸºäºæ®‹å·®çš„æ¨¡å‹å¼•å¯¼å®ç°å¤§è¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½",
      "authors": [
        "Lucio La Cava",
        "Andrea Tagarelli"
      ],
      "abstract": "Preference alignment is a critical step in making Large Language Models (LLMs) useful and aligned with (human) preferences. Existing approaches such as Reinforcement Learning from Human Feedback or Direct Preference Optimization typically require curated data and expensive optimization over billions of parameters, and eventually lead to persistent task-specific models. In this work, we introduce Preference alignment of Large Language Models via Residual Steering (PaLRS), a training-free method that exploits preference signals encoded in the residual streams of LLMs. From as few as one hundred preference pairs, PaLRS extracts lightweight, plug-and-play steering vectors that can be applied at inference time to push models toward preferred behaviors. We evaluate PaLRS on various small-to-medium-scale open-source LLMs, showing that PaLRS-aligned models achieve consistent gains on mathematical reasoning and code generation benchmarks while preserving baseline general-purpose performance. Moreover, when compared to DPO-aligned models, they perform better with huge time savings. Our findings highlight that PaLRS offers an effective, much more efficient and flexible alternative to standard preference optimization pipelines, offering a training-free, plug-and-play mechanism for alignment with minimal data.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†åŸºäºæ®‹å·®å¼•å¯¼çš„å¤§è¯­è¨€æ¨¡å‹åå¥½å¯¹é½æ–¹æ³•(PaLRS)ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿåå¥½å¯¹é½æŠ€æœ¯(å¦‚RLHFæˆ–DPO)å¯¹æµ·é‡æ•°æ®å’Œæ˜‚è´µç®—åŠ›çš„ä¾èµ–ã€‚PaLRS æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒ(training-free)çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡æŒ–æ˜å¤§è¯­è¨€æ¨¡å‹(LLMs)æ®‹å·®æµ(residual streams)ä¸­ç¼–ç çš„åå¥½ä¿¡å·ï¼Œä»…éœ€çº¦ä¸€ç™¾ä¸ªåå¥½å¯¹(preference pairs)å³å¯æå–å‡ºè½»é‡çº§çš„å³æ’å³ç”¨(plug-and-play)å¼•å¯¼å‘é‡ã€‚è¿™äº›å‘é‡åœ¨æ¨ç†é˜¶æ®µèƒ½å¤Ÿæœ‰æ•ˆå¼•å¯¼æ¨¡å‹å±•ç°å‡ºç¬¦åˆé¢„æœŸçš„åå¥½è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPaLRS åœ¨æ•°å­¦æ¨ç†(mathematical reasoning)å’Œä»£ç ç”Ÿæˆ(code generation)åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—ä¸”ä¸€è‡´çš„æå‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„åŸºç¡€é€šç”¨æ€§èƒ½ã€‚ä¸ DPO ç›¸æ¯”ï¼ŒPaLRS åœ¨æ€§èƒ½è¡¨ç°æ›´ä¼˜çš„åŒæ—¶å¤§å¹…èŠ‚çœäº†å¯¹é½æ—¶é—´ï¼Œä¸ºå®ç°é«˜æ•ˆã€çµæ´»çš„æ¨¡å‹å¯¹é½æä¾›äº†ä¸€ç§æå…·æ½œåŠ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23982v1",
      "published_date": "2025-09-28 17:16:16 UTC",
      "updated_date": "2025-09-28 17:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:31.933195+00:00"
    },
    {
      "arxiv_id": "2509.23981v1",
      "title": "Automatic selection of primary studies in systematic reviews with evolutionary rule-based classification",
      "title_zh": "åŸºäºæ¼”åŒ–è§„åˆ™åˆ†ç±»çš„ç³»ç»Ÿç»¼è¿°åŸå§‹ç ”ç©¶è‡ªåŠ¨ç­›é€‰",
      "authors": [
        "JosÃ© de la Torre-LÃ³pez",
        "Aurora RamÃ­rez",
        "JosÃ© RaÃºl Romero"
      ],
      "abstract": "Searching, filtering and analysing scientific literature are time-consuming tasks when performing a systematic literature review. With the rise of artificial intelligence, some steps in the review process are progressively being automated. In particular, machine learning for automatic paper selection can greatly reduce the effort required to identify relevant literature in scientific databases. We propose an evolutionary machine learning approach, called \\ourmodel, to automatically determine whether a paper retrieved from a literature search process is relevant. \\ourmodel builds an interpretable rule-based classifier using grammar-guided genetic programming. The use of a grammar to define the syntax and the structure of the rules allows \\ourmodel to easily combine the usual textual information with other bibliometric data not considered by state-of-the-art methods. Our experiments demonstrate that it is possible to generate accurate classifiers without impairing interpretability and using configurable information sources not supported so far.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç³»ç»Ÿè¯„ä»·ï¼ˆSystematic Reviewsï¼‰ä¸­äººå·¥ç­›é€‰ç§‘å­¦æ–‡çŒ®æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¿›åŒ–æœºå™¨å­¦ä¹ ï¼ˆEvolutionary Machine Learningï¼‰çš„è‡ªåŠ¨åŒ–è®ºæ–‡ç­›é€‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¯­æ³•å¼•å¯¼é—ä¼ ç¼–ç¨‹ï¼ˆGrammar-guided Genetic Programmingï¼‰æ„å»ºå…·æœ‰é«˜å¯è§£é‡Šæ€§çš„åŸºäºè§„åˆ™çš„åˆ†ç±»å™¨ï¼ˆRule-based Classifierï¼‰ã€‚é€šè¿‡åˆ©ç”¨è¯­æ³•å®šä¹‰è§„åˆ™ç»“æ„ï¼Œè¯¥ç ”ç©¶æˆåŠŸå°†æ–‡æœ¬ä¿¡æ¯ä¸ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥çš„æ–‡çŒ®è®¡é‡æ•°æ®ï¼ˆBibliometric Dataï¼‰ç›¸ç»“åˆï¼Œå®ç°äº†æ›´å…¨é¢çš„ç‰¹å¾åˆ©ç”¨ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒåˆ†ç±»è§„åˆ™ç®€æ´æ˜“è¯»çš„åŒæ—¶ï¼Œæä¾›æé«˜çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•æ”¯æŒé«˜åº¦å¯é…ç½®çš„ä¿¡æ¯æºï¼Œä¸ºæ˜¾è‘—å‡å°‘ç§‘ç ”äººå‘˜åœ¨è¯†åˆ«ç›¸å…³æ–‡çŒ®æ—¶çš„ä»»åŠ¡è´Ÿè½½æä¾›äº†æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 5 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23981v1",
      "published_date": "2025-09-28 17:13:20 UTC",
      "updated_date": "2025-09-28 17:13:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:34.571972+00:00"
    },
    {
      "arxiv_id": "2509.25275v1",
      "title": "VoiceBridge: Designing Latent Bridge Models for General Speech Restoration at Scale",
      "title_zh": "VoiceBridgeï¼šé¢å‘å¤§è§„æ¨¡é€šç”¨è¯­éŸ³ä¿®å¤çš„æ½œç©ºé—´æ¡¥æ¢æ¨¡å‹è®¾è®¡",
      "authors": [
        "Chi Zhang",
        "Zehua Chen",
        "Kaiwen Zheng",
        "Jun Zhu"
      ],
      "abstract": "Bridge models have recently been explored for speech enhancement tasks such as denoising, dereverberation, and super-resolution, while these efforts are typically confined to a single task or small-scale datasets, with constrained general speech restoration (GSR) capability at scale. In this work, we introduce VoiceBridge, a GSR system rooted in latent bridge models (LBMs), capable of reconstructing high-fidelity speech at full-band (\\textit{i.e.,} 48~kHz) from various distortions. By compressing speech waveform into continuous latent representations, VoiceBridge models the~\\textit{diverse LQ-to-HQ tasks} (namely, low-quality to high-quality) in GSR with~\\textit{a single latent-to-latent generative process} backed by a scalable transformer architecture. To better inherit the advantages of bridge models from the data domain to the latent space, we present an energy-preserving variational autoencoder, enhancing the alignment between the waveform and latent space over varying energy levels. Furthermore, to address the difficulty of HQ reconstruction from distinctively different LQ priors, we propose a joint neural prior, uniformly alleviating the reconstruction burden of LBM. At last, considering the key requirement of GSR systems, human perceptual quality, a perceptually aware fine-tuning stage is designed to mitigate the cascading mismatch in generation while improving perceptual alignment. Extensive validation across in-domain and out-of-domain tasks and datasets (\\textit{e.g.}, refining recent zero-shot speech and podcast generation results) demonstrates the superior performance of VoiceBridge. Demo samples can be visited at: https://VoiceBridge-demo.github.io/.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VoiceBridgeï¼Œä¸€ç§åŸºäºæ½œåœ¨æ¡¥æ¥æ¨¡å‹(Latent Bridge Models, LBMs)çš„é€šç”¨è¯­éŸ³ä¿®å¤(General Speech Restoration, GSR)ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°å¤§è§„æ¨¡çš„é«˜ä¿çœŸå…¨é¢‘å¸¦(48kHz)è¯­éŸ³é‡å»ºã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†è¯­éŸ³æ³¢å½¢å‹ç¼©ä¸ºè¿ç»­çš„æ½œåœ¨è¡¨ç¤ºï¼Œåˆ©ç”¨å¯æ‰©å±•çš„Transformeræ¶æ„åœ¨å•ä¸€çš„latent-to-latentç”Ÿæˆè¿‡ç¨‹ä¸­å¤„ç†å¤šç§ä»ä½è´¨é‡åˆ°é«˜è´¨é‡(LQ-to-HQ)çš„ä»»åŠ¡ã€‚ä¸ºäº†æå‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†èƒ½é‡å®ˆæ’å˜åˆ†è‡ªç¼–ç å™¨(energy-preserving VAE)ä»¥ä¼˜åŒ–æ³¢å½¢ä¸æ½œåœ¨ç©ºé—´çš„å¯¹é½ï¼Œå¹¶æå‡ºäº†è”åˆç¥ç»å…ˆéªŒ(joint neural prior)æ¥å¹³è¡¡ä¸åŒè´¨é‡å…ˆéªŒä¸‹çš„é‡å»ºè´Ÿæ‹…ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æ„ŸçŸ¥æ„ŸçŸ¥å¾®è°ƒ(perceptually aware fine-tuning)é˜¶æ®µï¼ŒVoiceBridgeæ˜¾è‘—æå‡äº†è¯­éŸ³çš„äººç±»æ„ŸçŸ¥è´¨é‡å¹¶ç¼“è§£äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„çº§è”ä¸åŒ¹é…ã€‚åœ¨åŸŸå†…å’Œè·¨åŸŸä»»åŠ¡ï¼ˆå¦‚é›¶æ ·æœ¬è¯­éŸ³å’Œæ’­å®¢ç”Ÿæˆï¼‰ä¸­çš„å¹¿æ³›éªŒè¯è¡¨æ˜ï¼ŒVoiceBridgeåœ¨å¤„ç†å¤æ‚è¯­éŸ³å¤±çœŸæ–¹é¢å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25275v1",
      "published_date": "2025-09-28 17:12:13 UTC",
      "updated_date": "2025-09-28 17:12:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:39.568892+00:00"
    },
    {
      "arxiv_id": "2509.23962v1",
      "title": "Conditional Advantage Estimation for Reinforcement Learning in Large Reasoning Models",
      "title_zh": "å¤§æ¨ç†æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¡ä»¶ä¼˜åŠ¿ä¼°è®¡",
      "authors": [
        "Guanxu Chen",
        "Yafu Li",
        "Yuxian Jiang",
        "Chen Qian",
        "Qihan Ren",
        "Jingyi Yang",
        "Yu Cheng",
        "Dongrui Liu",
        "Jing Shao"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs) has achieved remarkable progress in enhancing LLMs' reasoning capabilities on tasks with clear correctness criteria, such as mathematical reasoning tasks. Several training metrics, such as entropy or response length, have been observed to correlate with different reasoning behaviors in reinforcement learning. Prior approaches incorporate such priors through reward or advantage shaping, which often relies on hand-crafted penalties and preferences (e.g., higher-is-better or lower-is-better). However, without careful hyperparameter tuning, these directional priors can be overly biased and may lead to failure. To this end, we introduce Conditional advANtage estimatiON (CANON), amplifying the impact of the target metric without presuming its direction. Specifically, CANON regroups the sampled responses into two groups based on the higher or lower value of a target metric, measures which metric trend contributes to better performance through inter-group comparison, and identifies the better response within the same group. In summary, CANON based on entropy consistently outperforms prior methods across three LLMs on both math reasoning and high-complexity logic tasks. When applied to response length, CANON further improves token efficiency, yielding a more favorable Pareto frontier in the performance-cost trade-off.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ± (RLVR) åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ (LLMs) æ¨ç†èƒ½åŠ›æ—¶ï¼Œç”±äºä¾èµ–äººå·¥è®¾è®¡çš„å¥–åŠ±æˆ–ä¼˜åŠ¿å¡‘é€ ï¼ˆå¦‚ç†µæˆ–å›ç­”é•¿åº¦ï¼‰è€Œå®¹æ˜“äº§ç”Ÿåå·®å’Œè°ƒä¼˜å›°éš¾çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æ¡ä»¶ä¼˜åŠ¿ä¼°è®¡ (Conditional advANtage estimatiON, CANON)ï¼Œæ—¨åœ¨å¢å¼ºç›®æ ‡æŒ‡æ ‡çš„å½±å“åŠ›è€Œæ— éœ€é¢„è®¾å…¶ä½œç”¨æ–¹å‘ã€‚CANON å°†æ ·æœ¬å“åº”æŒ‰ç›®æ ‡æŒ‡æ ‡çš„é«˜ä½åˆ†ä¸ºä¸¤ç»„ï¼Œé€šè¿‡ç»„é—´æ¯”è¾ƒè¡¡é‡å“ªç§æŒ‡æ ‡è¶‹åŠ¿æ›´æœ‰åˆ©äºæ€§èƒ½æå‡ï¼Œå¹¶åœ¨åŒç»„å†…è¯†åˆ«æœ€ä¼˜å“åº”ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºç†µ (entropy) çš„ CANON åœ¨æ•°å­¦æ¨ç†å’Œé«˜éš¾åº¦é€»è¾‘ä»»åŠ¡ä¸Šä¸€è‡´ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåœ¨å¤„ç†å›ç­”é•¿åº¦ (response length) æ—¶ï¼ŒCANON æ˜¾è‘—æå‡äº†æ ‡è®°æ•ˆç‡ (token efficiency)ï¼Œåœ¨æ€§èƒ½ä¸æˆæœ¬çš„æƒè¡¡ä¸­å®ç°äº†æ›´ä¼˜çš„å¸•ç´¯æ‰˜å‰æ²¿ (Pareto frontier)ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 13 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23962v1",
      "published_date": "2025-09-28 16:33:07 UTC",
      "updated_date": "2025-09-28 16:33:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:40.762214+00:00"
    },
    {
      "arxiv_id": "2509.23960v1",
      "title": "MAD-PINN: A Decentralized Physics-Informed Machine Learning Framework for Safe and Optimal Multi-Agent Control",
      "title_zh": "MAD-PINNï¼šä¸€ç§é¢å‘å®‰å…¨ä¸”æœ€ä¼˜å¤šæ™ºèƒ½ä½“æ§åˆ¶çš„å»ä¸­å¿ƒåŒ–ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Manan Tayal",
        "Aditya Singh",
        "Shishir Kolathaya",
        "Somil Bansal"
      ],
      "abstract": "Co-optimizing safety and performance in large-scale multi-agent systems remains a fundamental challenge. Existing approaches based on multi-agent reinforcement learning (MARL), safety filtering, or Model Predictive Control (MPC) either lack strict safety guarantees, suffer from conservatism, or fail to scale effectively. We propose MAD-PINN, a decentralized physics-informed machine learning framework for solving the multi-agent state-constrained optimal control problem (MASC-OCP). Our method leverages an epigraph-based reformulation of SC-OCP to simultaneously capture performance and safety, and approximates its solution via a physics-informed neural network. Scalability is achieved by training the SC-OCP value function on reduced-agent systems and deploying them in a decentralized fashion, where each agent relies only on local observations of its neighbours for decision-making. To further enhance safety and efficiency, we introduce an Hamilton-Jacobi (HJ) reachability-based neighbour selection strategy to prioritize safety-critical interactions, and a receding-horizon policy execution scheme that adapts to dynamic interactions while reducing computational burden. Experiments on multi-agent navigation tasks demonstrate that MAD-PINN achieves superior safety-performance trade-offs, maintains scalability as the number of agents grows, and consistently outperforms state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MAD-PINNï¼Œä¸€ç§ç”¨äºè§£å†³å¤šæ™ºèƒ½ä½“çŠ¶æ€çº¦æŸæœ€ä¼˜æ§åˆ¶é—®é¢˜ (MASC-OCP) çš„å»ä¸­å¿ƒåŒ–ç‰©ç†ä¿¡æ¯æœºå™¨å­¦ä¹ æ¡†æ¶ã€‚é’ˆå¯¹å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¹³è¡¡å®‰å…¨æ€§ä¸æ€§èƒ½æ—¶é¢ä¸´çš„æ‰©å±•æ€§éš¾é¢˜ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ epigraph-based reformulation åŒæ—¶ä¼˜åŒ–æ€§èƒ½ä¸å®‰å…¨çº¦æŸï¼Œå¹¶åˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINN) è¿‘ä¼¼å…¶è§£ã€‚ä¸ºäº†æå‡æ‰©å±•æ€§ï¼Œè¯¥æ–¹æ³•åœ¨ç®€åŒ–è§„æ¨¡çš„ç³»ç»Ÿä¸Šè®­ç»ƒä»·å€¼å‡½æ•°å¹¶è¿›è¡Œå»ä¸­å¿ƒåŒ–éƒ¨ç½²ï¼Œä½¿å„æ™ºèƒ½ä½“ä»…åŸºäºé‚»å±…çš„å±€éƒ¨è§‚æµ‹è¿›è¡Œå†³ç­–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†åŸºäº Hamilton-Jacobi (HJ) reachability çš„é‚»å±…é€‰æ‹©ç­–ç•¥ä»¥è¯†åˆ«å…³é”®çš„å®‰å…¨äº¤äº’ï¼Œå¹¶é‡‡ç”¨ receding-horizon ç­–ç•¥æ‰§è¡Œæ–¹æ¡ˆä»¥åº”å¯¹åŠ¨æ€ç¯å¢ƒå¹¶å‡è½»è®¡ç®—å‹åŠ›ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMAD-PINN åœ¨å¤šæ™ºèƒ½ä½“å¯¼èˆªä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„å®‰å…¨ä¸æ€§èƒ½æƒè¡¡ï¼Œåœ¨æ™ºèƒ½ä½“è§„æ¨¡å¢é•¿æ—¶ä»ä¿æŒé«˜æ•ˆæ‰©å±•ï¼Œä¸”è¡¨ç°æ˜¾è‘—ä¼˜äºå½“å‰çš„åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 Pages, 4 Figures, 3 Tables. First two authors have contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2509.23960v1",
      "published_date": "2025-09-28 16:31:22 UTC",
      "updated_date": "2025-09-28 16:31:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:58.289543+00:00"
    },
    {
      "arxiv_id": "2509.23957v1",
      "title": "Vision-Grounded Machine Interpreting: Improving the Translation Process through Visual Cues",
      "title_zh": "åŸºäºè§†è§‰çš„æœºå™¨å£è¯‘ï¼šé€šè¿‡è§†è§‰çº¿ç´¢ä¼˜åŒ–ç¿»è¯‘è¿‡ç¨‹",
      "authors": [
        "Claudio Fantinuoli"
      ],
      "abstract": "Machine Interpreting systems are currently implemented as unimodal, real-time speech-to-speech architectures, processing translation exclusively on the basis of the linguistic signal. Such reliance on a single modality, however, constrains performance in contexts where disambiguation and adequacy depend on additional cues, such as visual, situational, or pragmatic information. This paper introduces Vision-Grounded Interpreting (VGI), a novel approach designed to address the limitations of unimodal machine interpreting. We present a prototype system that integrates a vision-language model to process both speech and visual input from a webcam, with the aim of priming the translation process through contextual visual information. To evaluate the effectiveness of this approach, we constructed a hand-crafted diagnostic corpus targeting three types of ambiguity. In our evaluation, visual grounding substantially improves lexical disambiguation, yields modest and less stable gains for gender resolution, and shows no benefit for syntactic ambiguities. We argue that embracing multimodality represents a necessary step forward for advancing translation quality in machine interpreting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Vision-Grounded Interpreting (VGI)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³å•æ¨¡æ€(unimodal)æœºå™¨åŒä¼ ç³»ç»Ÿä¸­æ­§ä¹‰æ¶ˆé™¤å’Œç¿»è¯‘å¿ å®åº¦å—é™é—®é¢˜çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªåŸå‹ç³»ç»Ÿï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹(vision-language model)æ•´åˆç½‘ç»œæ‘„åƒå¤´çš„è§†è§‰è¾“å…¥ä¸è¯­éŸ³ä¿¡å·ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡è§†è§‰ä¿¡æ¯æ¥å¼•å¯¼ç¿»è¯‘è¿‡ç¨‹ã€‚ä¸ºäº†éªŒè¯è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹ä¸‰ç§æ­§ä¹‰ç±»å‹çš„è¯Šæ–­è¯­æ–™åº“è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè§†è§‰å¼•å¯¼(visual grounding)æ˜¾è‘—æ”¹å–„äº†è¯æ±‡æ­§ä¹‰æ¶ˆé™¤(lexical disambiguation)çš„è¡¨ç°ï¼Œä½†åœ¨æ€§åˆ«æ¶ˆè§£(gender resolution)æ–¹é¢çš„æå‡æœ‰é™ä¸”ä¸å¤Ÿç¨³å®šï¼Œè€Œå¯¹å¥æ³•æ­§ä¹‰(syntactic ambiguities)åˆ™æ²¡æœ‰æ˜æ˜¾å¸®åŠ©ã€‚è¯¥ç ”ç©¶è®ºè¯äº†æ‹¥æŠ±å¤šæ¨¡æ€(multimodality)æ˜¯æå‡æœºå™¨åŒä¼ ç¿»è¯‘è´¨é‡ã€å…‹æœçº¯è¯­è¨€ä¿¡å·å±€é™æ€§çš„å¿…è¦æ¼”è¿›æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper presented at AMTA 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23957v1",
      "published_date": "2025-09-28 16:25:33 UTC",
      "updated_date": "2025-09-28 16:25:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:58.588046+00:00"
    },
    {
      "arxiv_id": "2510.03280v2",
      "title": "Training Optimal Large Diffusion Language Models",
      "title_zh": "è®­ç»ƒæœ€ä¼˜å¤§è§„æ¨¡æ‰©æ•£è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jinjie Ni",
        "Qian Liu",
        "Chao Du",
        "Longxu Dou",
        "Hang Yan",
        "Zili Wang",
        "Tianyu Pang",
        "Michael Qizhe Shieh"
      ],
      "abstract": "We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† Quokkaï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹ (Diffusion Language Models, DLMs) çš„ç³»ç»Ÿæ€§ç¼©æ”¾å®šå¾‹ (Scaling Law)ã€‚è¯¥ç ”ç©¶å…¨é¢æ¶µç›–äº†è®¡ç®—å—é™ (compute-constrained) å’Œæ•°æ®å—é™ (data-constrained) ä¸¤ç§æƒ…å¢ƒï¼Œå¹¶å¯¹å»ºæ¨¡ä¸ä¼˜åŒ–çš„å…³é”®è®¾è®¡è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚Quokka æ—¨åœ¨ä¸º DLMs çš„è®­ç»ƒæä¾›åˆ‡å®çš„å®è·µæŒ‡å¯¼ï¼ŒåŒæ—¶ä¹Ÿä¸ºäººå·¥æ™ºèƒ½ç¤¾åŒºçš„é•¿æœŸå‘å±•æä¾›å¯å‘ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°ç ”ç©¶ä¸åŒå‚æ•°è§„æ¨¡ä¸‹çš„æ¨¡å‹è¡¨ç°ï¼Œè¯¥å·¥ä½œå¡«è¡¥äº†æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæ‰©å±•ç‰¹æ€§çš„ç ”ç©¶ç©ºç™½ï¼Œä¸ºæ„å»ºæœ€ä¼˜çš„å¤§å‹æ‰©æ•£è¯­è¨€æ¨¡å‹å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03280v2",
      "published_date": "2025-09-28 16:20:02 UTC",
      "updated_date": "2025-11-05 08:32:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:53.092366+00:00"
    },
    {
      "arxiv_id": "2509.25274v1",
      "title": "DNABERT-2: Fine-Tuning a Genomic Language Model for Colorectal Gene Enhancer Classification",
      "title_zh": "DNABERT-2ï¼šç”¨äºç»“ç›´è‚ åŸºå› å¢å¼ºå­åˆ†ç±»çš„åŸºå› ç»„è¯­è¨€æ¨¡å‹å¾®è°ƒ",
      "authors": [
        "Darren King",
        "Yaser Atlasi",
        "Gholamreza Rafiee"
      ],
      "abstract": "Gene enhancers control when and where genes switch on, yet their sequence diversity and tissue specificity make them hard to pinpoint in colorectal cancer. We take a sequence-only route and fine-tune DNABERT-2, a transformer genomic language model that uses byte-pair encoding to learn variable-length tokens from DNA. Using assays curated via the Johnston Cancer Research Centre at Queen's University Belfast, we assembled a balanced corpus of 2.34 million 1 kb enhancer sequences, applied summit-centered extraction and rigorous de-duplication including reverse-complement collapse, and split the data stratified by class. With a 4096-term vocabulary and a 232-token context chosen empirically, the DNABERT-2-117M classifier was trained with Optuna-tuned hyperparameters and evaluated on 350742 held-out sequences. The model reached PR-AUC 0.759, ROC-AUC 0.743, and best F1 0.704 at an optimized threshold (0.359), with recall 0.835 and precision 0.609. Against a CNN-based EnhancerNet trained on the same data, DNABERT-2 delivered stronger threshold-independent ranking and higher recall, although point accuracy was lower. To our knowledge, this is the first study to apply a second-generation genomic language model with BPE tokenization to enhancer classification in colorectal cancer, demonstrating the feasibility of capturing tumor-associated regulatory signals directly from DNA sequence alone. Overall, our results show that transformer-based genomic models can move beyond motif-level encodings toward holistic classification of regulatory elements, offering a novel path for cancer genomics. Next steps will focus on improving precision, exploring hybrid CNN-transformer designs, and validating across independent datasets to strengthen real-world utility.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»“ç›´è‚ ç™ŒåŸºå› å¢å¼ºå­ï¼ˆGene enhancersï¼‰è¯†åˆ«çš„å¤æ‚æ€§ï¼Œé€šè¿‡å¾®è°ƒç¬¬äºŒä»£åŸºå› ç»„è¯­è¨€æ¨¡å‹ DNABERT-2 å®ç°äº†åŸºäºçº¯åºåˆ—çš„åˆ†ç±»é¢„æµ‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨åŒ…å« 234 ä¸‡æ¡ 1 kb åºåˆ—çš„å¹³è¡¡è¯­æ–™åº“ï¼Œç»“åˆå­—èŠ‚å¯¹ç¼–ç ï¼ˆByte-pair encoding, BPEï¼‰æŠ€æœ¯ï¼ŒæˆåŠŸè®­ç»ƒå‡ºèƒ½å¤Ÿæ•æ‰å˜é•¿åŸºå› ç‰¹å¾çš„ 117M å‚æ•°åˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDNABERT-2 åœ¨æµ‹è¯•é›†ä¸Šå–å¾—äº† 0.759 çš„ PR-AUC å’Œ 0.743 çš„ ROC-AUCï¼Œå…¶é˜ˆå€¼æ— å…³çš„æ’åºèƒ½åŠ›å’Œå¬å›ç‡å‡ä¼˜äºä¼ ç»Ÿçš„ CNN æ¨¡å‹ EnhancerNetã€‚è¿™æ˜¯é¦–ä¸ªå°†å…·å¤‡ BPE åˆ†è¯èƒ½åŠ›çš„åŸºå› ç»„å¤§æ¨¡å‹åº”ç”¨äºç»“ç›´è‚ ç™Œå¢å¼ºå­åˆ†ç±»çš„ç ”ç©¶ï¼ŒéªŒè¯äº†ç›´æ¥ä» DNA åºåˆ—æå–è‚¿ç˜¤ç›¸å…³è°ƒæ§ä¿¡å·çš„å¯è¡Œæ€§ã€‚è¯¥æˆæœæ ‡å¿—ç€åŸºå› ç»„å»ºæ¨¡ä»å±€éƒ¨åŸºå…ƒï¼ˆMotifï¼‰åˆ†æå‘å…¨å±€è°ƒæ§å…ƒä»¶æ•´ä½“åˆ†ç±»çš„è½¬å˜ï¼Œä¸ºç™Œç—‡åŸºå› ç»„å­¦ç ”ç©¶å¼€è¾Ÿäº†åŸºäº Transformer æ¶æ„çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.GN",
      "comment": "10 pages, 10 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25274v1",
      "published_date": "2025-09-28 16:10:03 UTC",
      "updated_date": "2025-09-28 16:10:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:23:56.685706+00:00"
    },
    {
      "arxiv_id": "2509.23946v2",
      "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm",
      "title_zh": "Explore-Execute Chainï¼šè¿ˆå‘é«˜æ•ˆç»“æ„åŒ–æ¨ç†èŒƒå¼",
      "authors": [
        "Kaisen Yang",
        "Lixuan He",
        "Rushi Shah",
        "Kaicheng Yang",
        "Qinwei Ma",
        "Dianbo Liu",
        "Alex Lamb"
      ],
      "abstract": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æ¢ç´¢-æ‰§è¡Œé“¾(Explore-Execute Chain, $E^2C$)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é“¾å¼æ€ç»´(Chain-of-Thought)æ¨ç†ä¸­ç”±äºè®¡åˆ’ä¸æ‰§è¡Œæ··æ·†è€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œå¯è§£é‡Šæ€§å·®ç­‰é—®é¢˜çš„ç»“æ„åŒ–æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨ç†è¿‡ç¨‹è§£è€¦ä¸ºéšæœºç”Ÿæˆç®€æ´é«˜å±‚è®¡åˆ’çš„æ¢ç´¢é˜¶æ®µ(exploratory phase)ä»¥åŠç¡®å®šæ€§æ‰§è¡Œæ‰€é€‰è®¡åˆ’çš„æ‰§è¡Œé˜¶æ®µ(execution phase)ã€‚ç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆäº†å¢å¼ºè®¡åˆ’éµå¾ªèƒ½åŠ›çš„ç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–æ¢ç´¢ä¸æ‰§è¡Œç¡®å®šæ€§çš„å¼ºåŒ–å­¦ä¹ (RL)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ$E^2C$åœ¨AIME'2024æµ‹è¯•ä¸­ä»…éœ€ä¸åˆ°Forest-of-Thoughtç­‰æ–¹æ³•10%çš„è§£ç Tokenå³å¯è¾¾åˆ°58.1%çš„å‡†ç¡®ç‡ï¼Œå¤§å¹…é™ä½äº†è‡ªä¸€è‡´æ€§å¼€é”€ã€‚åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•çš„è·¨é¢†åŸŸé€‚é…ä¸­ï¼Œè¯¥æ–¹æ³•çš„æ¢ç´¢å‹SFT(EF-SFT)åœ¨ä»…æ¶ˆè€—æ ‡å‡†SFTçº¦3.5% Tokençš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡æ¯”æ ‡å‡†å¾®è°ƒé«˜å‡º14.5%ã€‚é€šè¿‡è¿™ç§è§£è€¦æœºåˆ¶ï¼Œ$E^2C$ä¸ä»…å®ç°äº†å…ˆè¿›çš„æ¨ç†æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¿˜é€šè¿‡åˆ†ç¦»è®¡åˆ’ä¸æ‰§è¡Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23946v2",
      "published_date": "2025-09-28 15:48:40 UTC",
      "updated_date": "2025-09-30 02:45:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:01.091713+00:00"
    },
    {
      "arxiv_id": "2509.23938v1",
      "title": "Easy Turn: Integrating Acoustic and Linguistic Modalities for Robust Turn-Taking in Full-Duplex Spoken Dialogue Systems",
      "title_zh": "Easy Turnï¼šèåˆå£°å­¦ä¸è¯­è¨€æ¨¡æ€ï¼Œå®ç°å…¨åŒå·¥è¯­éŸ³å¯¹è¯ç³»ç»Ÿçš„é²æ£’è¯è½®è½¬æ¢",
      "authors": [
        "Guojian Li",
        "Chengyou Wang",
        "Hongfei Xue",
        "Shuiyuan Wang",
        "Dehui Gao",
        "Zihan Zhang",
        "Yuke Lin",
        "Wenjie Li",
        "Longshuai Xiao",
        "Zhonghua Fu",
        "Lei Xie"
      ],
      "abstract": "Full-duplex interaction is crucial for natural human-machine communication, yet remains challenging as it requires robust turn-taking detection to decide when the system should speak, listen, or remain silent. Existing solutions either rely on dedicated turn-taking models, most of which are not open-sourced. The few available ones are limited by their large parameter size or by supporting only a single modality, such as acoustic or linguistic. Alternatively, some approaches finetune LLM backbones to enable full-duplex capability, but this requires large amounts of full-duplex data, which remain scarce in open-source form. To address these issues, we propose Easy Turn, an open-source, modular turn-taking detection model that integrates acoustic and linguistic bimodal information to predict four dialogue turn states: complete, incomplete, backchannel, and wait, accompanied by the release of Easy Turn trainset, a 1,145-hour speech dataset designed for training turn-taking detection models. Compared to existing open-source models like TEN Turn Detection and Smart Turn V2, our model achieves state-of-the-art turn-taking detection accuracy on our open-source Easy Turn testset. The data and model will be made publicly available on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Easy Turnï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„æ¨¡å—åŒ–è¯è½®è½¬æ¢(Turn-Taking)æ£€æµ‹æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å…¨åŒå·¥(Full-Duplex)è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¸­çš„è‡ªç„¶ç¨³å¥äº¤äº’ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ¨¡å‹é—­æºã€å‚æ•°é‡å¤§æˆ–ä»…æ”¯æŒå•æ¨¡æ€ç­‰å±€é™æ€§ï¼ŒEasy Turnæœ‰æ•ˆæ•´åˆäº†å£°å­¦(Acoustic)å’Œè¯­è¨€(Linguistic)çš„åŒæ¨¡æ€ä¿¡æ¯ï¼Œèƒ½å¤Ÿç²¾å‡†é¢„æµ‹å¯¹è¯ä¸­çš„å®Œæˆ(Complete)ã€æœªå®Œæˆ(Incomplete)ã€åé¦ˆ(Backchannel)å’Œç­‰å¾…(Wait)å››ç§è¯è½®çŠ¶æ€ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†è§„æ¨¡è¾¾1,145å°æ—¶çš„Easy Turnè¯­éŸ³è®­ç»ƒæ•°æ®é›†ï¼Œæå¤§åœ°ç¼“è§£äº†å¼€æºå…¨åŒå·¥æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEasy Turnåœ¨å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºTEN Turn Detectionå’ŒSmart Turn V2ç­‰ç°æœ‰å¼€æºæ¨¡å‹ï¼Œè¾¾åˆ°äº†å½“å‰çš„é¢†åŸŸæœ€ä¼˜(State-of-the-Art)æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€æºæ¨¡å‹ä¸æ•°æ®ï¼Œä¸ºæ„å»ºæ›´åŠ æµç•…ã€å¯ä¿¡çš„äººæœºäº¤äº’ç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23938v1",
      "published_date": "2025-09-28 15:29:44 UTC",
      "updated_date": "2025-09-28 15:29:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:23.186054+00:00"
    },
    {
      "arxiv_id": "2509.23937v3",
      "title": "Diffusion Models are Kelly Gamblers",
      "title_zh": "æ‰©æ•£æ¨¡å‹å³ä¸º Kelly èµŒå¾’",
      "authors": [
        "Akhil Premkumar"
      ],
      "abstract": "We draw a connection between diffusion models and the Kelly criterion for maximizing returns in betting games. A signal that is correlated with the outcome of such a game can be used to focus the bets on a narrow range of high probability predictions. Diffusion models share the same paradigm in that they gradually concentrate the probability mass to fit the training data. We show that the information stored in an unconditional diffusion model captures, in part, the joint correlation between the components of the data variable $X$. Conditional diffusion models store additional information to bind the signal $X$ with the conditioning information $Y$, equal to the mutual information between them. The latter is only a small fraction of the total information in the neural network if the data is low-dimensional. We examine why this does not hinder conditional generation.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ Diffusion Models ä¸ç”¨äºæœ€å¤§åŒ–åšå¼ˆæ”¶ç›Šçš„ Kelly criterion ä¹‹é—´å»ºç«‹äº†æ·±åˆ»è”ç³»ï¼ŒæŒ‡å‡ºæ‰©æ•£æ¨¡å‹é€šè¿‡é€æ¸é›†ä¸­æ¦‚ç‡è´¨é‡æ¥æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„è¿‡ç¨‹ï¼Œä¸åˆ©ç”¨ç›¸å…³ä¿¡å·ç¼©å°é«˜æ¦‚ç‡é¢„æµ‹èŒƒå›´çš„åšå¼ˆç­–ç•¥å…·æœ‰ç›¸ä¼¼èŒƒå¼ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ— æ¡ä»¶æ‰©æ•£æ¨¡å‹æ•è·äº†æ•°æ®å˜é‡ $X$ å„ç»„æˆéƒ¨åˆ†ä¹‹é—´çš„è”åˆç›¸å…³æ€§ï¼Œè€Œæ¡ä»¶æ‰©æ•£æ¨¡å‹åˆ™é¢å¤–å­˜å‚¨äº†å°†ä¿¡å· $X$ ä¸æ¡ä»¶ä¿¡æ¯ $Y$ ç»‘å®šçš„äº’ä¿¡æ¯ã€‚è®ºæ–‡é€šè¿‡åˆ†æå‘ç°ï¼Œå°½ç®¡åœ¨ä½ç»´æ•°æ®ä¸­è¿™ç§äº’ä¿¡æ¯ä»…å ç¥ç»ç½‘ç»œæ€»ä¿¡æ¯çš„ä¸€å°éƒ¨åˆ†ï¼Œä½†å¹¶ä¸ä¼šå¦¨ç¢æœ‰æ•ˆçš„æ¡ä»¶ç”Ÿæˆè¿‡ç¨‹ã€‚è¯¥å·¥ä½œæ·±å…¥æ¢è®¨äº† Diffusion Models çš„ä¿¡æ¯å­˜å‚¨æœºåˆ¶ï¼Œä¸ºç†è§£ç”Ÿæˆæ¨¡å‹åœ¨åšå¼ˆè®ºè§†è§’ä¸‹çš„ä¼˜åŒ–åŸç†æä¾›äº†æ–°é¢–çš„ç†è®ºè§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cond-mat.stat-mech",
        "cs.AI",
        "cs.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages + references, 19 figures. v3: Significantly revised the overall narrative, and integrated more experiments into the main text",
      "pdf_url": "https://arxiv.org/pdf/2509.23937v3",
      "published_date": "2025-09-28 15:27:25 UTC",
      "updated_date": "2025-11-30 00:07:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:32.786762+00:00"
    },
    {
      "arxiv_id": "2509.23928v2",
      "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models",
      "title_zh": "HiViSï¼šè§†è§‰è¯­è¨€æ¨¡å‹ä¸­é¢å‘èµ·è‰å™¨éšè—è§†è§‰ Token çš„æŠ•æœºè§£ç ",
      "authors": [
        "Zhinan Xie",
        "Peisong Wang",
        "Shuang Qiu",
        "Jian Cheng"
      ],
      "abstract": "Speculative decoding has proven effective for accelerating inference in Large Language Models (LLMs), yet its extension to Vision-Language Models (VLMs) remains limited by the computational burden and semantic inconsistency introduced by visual tokens. Recent studies reveal that visual tokens in large VLMs are highly redundant, and most of them can be removed without compromising generation quality. Motivated by this observation, we propose HiViS (Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models), a framework that utilizes the target VLM as a semantic fusion model, allowing the drafter to obtain visual information without explicitly processing visual tokens, ensuring that the drafter's prefill sequence length matches that of the textual tokens. Furthermore, HiViS employs a time-step-aware aligned training scheme that allows the drafter to autonomously propagate and refine instructive visual-textual semantics during independent drafting, guided by step-dependent bias-correction residuals. Extensive experiments across representative VLMs and benchmarks demonstrate that HiViS achieves significant improvements in average acceptance length and speedup ratio.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨æŠ•æœºè§£ç  (Speculative Decoding) ä¸­å—é™äºè§†è§‰ token å¸¦æ¥çš„è®¡ç®—è´Ÿæ‹…å’Œè¯­ä¹‰ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†åä¸º HiViS çš„åˆ›æ–°æ¡†æ¶ã€‚åŸºäºè§†è§‰ token å­˜åœ¨é«˜åº¦å†—ä½™çš„è§‚å¯Ÿï¼ŒHiViS å°†ç›®æ ‡ VLM ä½œä¸ºè¯­ä¹‰èåˆæ¨¡å‹ï¼Œä½¿è‰æ‹Ÿæ¨¡å‹ (drafter) æ— éœ€æ˜¾å¼å¤„ç†è§†è§‰ token å³å¯è·å–å…³é”®è§†è§‰ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿è‰æ‹Ÿé˜¶æ®µçš„åºåˆ—é•¿åº¦ä¸çº¯æ–‡æœ¬ token ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼ŒHiViS é‡‡ç”¨äº†æ—¶é—´æ­¥æ„ŸçŸ¥çš„å¯¹é½è®­ç»ƒæ–¹æ¡ˆ (time-step-aware aligned training)ï¼Œåˆ©ç”¨æ­¥é•¿ç›¸å…³çš„åå·®ä¿®æ­£æ®‹å·®å¼•å¯¼è‰æ‹Ÿæ¨¡å‹åœ¨ç‹¬ç«‹ç”Ÿæˆè¿‡ç¨‹ä¸­è‡ªä¸»ä¼ æ’­å¹¶ç²¾ç‚¼è§†è§‰-æ–‡æœ¬è¯­ä¹‰ã€‚åœ¨å¤šç§ä»£è¡¨æ€§ VLMs å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼ŒHiViS åœ¨å¹³å‡æ¥å—é•¿åº¦å’ŒåŠ é€Ÿæ¯”æ–¹é¢å‡å®ç°äº†æ˜¾è‘—æå‡ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23928v2",
      "published_date": "2025-09-28 15:05:21 UTC",
      "updated_date": "2025-11-20 06:44:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:32.965889+00:00"
    },
    {
      "arxiv_id": "2509.23924v1",
      "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step",
      "title_zh": "é€šè¿‡å°‘æ­¥è§£ç çš„ä¸€è‡´æ€§è½¨è¿¹å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jingyi Yang",
        "Guanxu Chen",
        "Xuhao Hu",
        "Jing Shao"
      ],
      "abstract": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ©ç æ‰©æ•£è¯­è¨€æ¨¡å‹(MDLMs)åœ¨è§£ç ç­–ç•¥å’Œå¼ºåŒ–å­¦ä¹ (RL)æ–¹é¢æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€å¥—ä¼˜åŒ–æ–¹æ¡ˆï¼Œä»¥è§£å†³ç›´æ¥å¥—ç”¨è‡ªå›å½’(AR)æ¨¡å‹æŠ€æœ¯å¯¼è‡´çš„è®­ç»ƒæ¨ç†ä¸ä¸€è‡´æ€§ã€‚ç ”ç©¶è€…è®¾è®¡äº†EOS Early Rejection (EOSER)å’ŒAscending Step-Size (ASS)è§£ç è°ƒåº¦å™¨ï¼Œä½¿MDLMsèƒ½å¤Ÿåœ¨æ›´å°‘çš„è§£ç æ­¥æ•°ä¸‹å®ç°é«˜æ•ˆçš„å…¨æ‰©æ•£å¼è§£ç ã€‚åŒæ—¶ï¼Œä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å¼•å…¥äº†Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)ç®—æ³•ï¼Œé€šè¿‡å¢å¼ºRolloutè½¨è¿¹ä¸ä¼˜åŒ–è½¨è¿¹çš„ä¸€è‡´æ€§ï¼Œæ˜¾è‘—å‡å°‘äº†è·³æ­¥ä¼˜åŒ–äº§ç”Ÿçš„è¯¯å·®ã€‚åœ¨åŸºäºLLaDA-8B-Instructçš„æ•°å­¦æ¨ç†å’Œè§„åˆ’ä»»åŠ¡å®éªŒä¸­ï¼Œç»“æœè¡¨æ˜æ‰€ææœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡MDLMsçš„è®­ç»ƒä¸æ¨ç†æ•ˆç‡ã€‚è¿™ä¸€ç³»åˆ—æ”¹è¿›ä¸ºMDLMsåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages, 4 figures, 7 tables. Code: https://github.com/yjyddq/EOSER-ASS-RL",
      "pdf_url": "https://arxiv.org/pdf/2509.23924v1",
      "published_date": "2025-09-28 15:01:15 UTC",
      "updated_date": "2025-09-28 15:01:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:43.003051+00:00"
    },
    {
      "arxiv_id": "2509.23923v2",
      "title": "Graph Mixing Additive Networks",
      "title_zh": "å›¾æ··åˆåŠ æ€§ç½‘ç»œ",
      "authors": [
        "Maya Bechler-Speicher",
        "Andrea Zerio",
        "Maor Huri",
        "Marie Vibeke Vestergaard",
        "Ran Gilad-Bachrach",
        "Tine Jess",
        "Samir Bhatt",
        "Aleksejs Sazonovs"
      ],
      "abstract": "We introduce GMAN, a flexible, interpretable, and expressive framework that extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse time-series data. GMAN represents each time-dependent trajectory as a directed graph and applies an enriched, more expressive GNAN to each graph. It allows users to control the interpretability-expressivity trade-off by grouping features and graphs to encode priors, and it provides feature, node, and graph-level interpretability. On real-world datasets, including mortality prediction from blood tests and fake-news detection, GMAN outperforms strong non-interpretable black-box baselines while delivering actionable, domain-aligned explanations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Graph Mixing Additive Networks (GMAN)ï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»ã€å¯è§£é‡Šä¸”å…·æœ‰å¼ºå¤§è¡¨è¾¾èƒ½åŠ›çš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä»ç¨€ç–æ—¶é—´åºåˆ—æ•°æ®ä¸­è¿›è¡Œå­¦ä¹ ã€‚GMANé€šè¿‡å°†æ¯ä¸ªéšæ—¶é—´å˜åŒ–çš„è½¨è¿¹è¡¨ç¤ºä¸ºæœ‰å‘å›¾ï¼Œå¹¶åº”ç”¨å¢å¼ºåçš„Graph Neural Additive Networks (GNANs)åˆ°æ¯ä¸ªå›¾ä¸­ï¼Œæœ‰æ•ˆæ‰©å±•äº†ç°æœ‰æŠ€æœ¯ã€‚è¯¥æ¡†æ¶å…è®¸ç”¨æˆ·é€šè¿‡å¯¹ç‰¹å¾å’Œå›¾è¿›è¡Œåˆ†ç»„æ¥ç¼–ç å…ˆéªŒçŸ¥è¯†ï¼Œä»è€Œåœ¨å¯è§£é‡Šæ€§ä¸è¡¨è¾¾èƒ½åŠ›ä¹‹é—´å®ç°å¹³è¡¡æƒè¡¡ã€‚GMANæä¾›äº†ç‰¹å¾çº§ã€èŠ‚ç‚¹çº§å’Œå›¾çº§çš„å¤šå±‚çº§å¯è§£é‡Šæ€§ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦ã€‚åœ¨è¡€æ¶²æ£€æµ‹æ­»äº¡é¢„æµ‹å’Œè™šå‡æ–°é—»æ£€æµ‹ç­‰çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼ŒGMANçš„æ€§èƒ½è¡¨ç°ä¼˜äºå¼ºåŠ›çš„éå¯è§£é‡Šé»‘ç›’åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒGMANåœ¨ä¿æŒé«˜é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæä¾›ä¸é¢†åŸŸé€»è¾‘ç›¸ç¬¦ä¸”å…·å¤‡è¡ŒåŠ¨æŒ‡å¯¼ä»·å€¼çš„è§£é‡Šã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2505.19193",
      "pdf_url": "https://arxiv.org/pdf/2509.23923v2",
      "published_date": "2025-09-28 14:58:58 UTC",
      "updated_date": "2025-10-28 18:04:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:51.194277+00:00"
    },
    {
      "arxiv_id": "2510.03279v1",
      "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
      "title_zh": "MemMambaï¼šé‡æ–°å®¡è§†çŠ¶æ€ç©ºé—´æ¨¡å‹ä¸­çš„è®°å¿†æ¨¡å¼",
      "authors": [
        "Youjin Wang",
        "Yangjingyi Chen",
        "Jiahao Yan",
        "Jiaxuan Lu",
        "Xiao Sun"
      ],
      "abstract": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿åºåˆ—å»ºæ¨¡ä¸­ Mamba ç­‰é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ (Selective State-Space Models) å­˜åœ¨çš„é•¿ç¨‹è®°å¿†æŒ‡æ•°çº§è¡°å‡é—®é¢˜ï¼Œé€šè¿‡æ•°å­¦æ¨å¯¼å’Œä¿¡æ¯è®ºåˆ†æç³»ç»Ÿåœ°æ­ç¤ºäº†å…¶è®°å¿†è¡°å‡æœºåˆ¶ã€‚ä¸ºäº†é‡åŒ–å…³é”®ä¿¡æ¯æŸå¤±ï¼Œä½œè€…å¼•å…¥äº†èƒ½å¤Ÿæ•æ‰å±‚å†…å’Œå±‚é—´é€€åŒ–çš„æ°´å¹³-å‚ç›´å­˜å‚¨ä¿çœŸåº¦ (horizontal-vertical memory fidelity) æŒ‡æ ‡ã€‚å—åˆ°äººç±»é˜…è¯»é•¿æ–‡æœ¬æ—¶æç‚¼å¹¶ä¿ç•™æ˜¾è‘—ä¿¡æ¯æ–¹å¼çš„å¯å‘ï¼Œç ”ç©¶æå‡ºäº† MemMamba æ¶æ„ï¼Œè¯¥æ¶æ„åˆ›æ–°æ€§åœ°é›†æˆäº†çŠ¶æ€æ€»ç»“ (state summarization) æœºåˆ¶ä»¥åŠè·¨å±‚å’Œè·¨ä»¤ç‰Œæ³¨æ„åŠ› (cross-layer and cross-token attention)ã€‚MemMamba åœ¨ä¿æŒçº¿æ€§å¤æ‚åº¦çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†é•¿ç¨‹é—å¿˜é—®é¢˜ï¼Œåœ¨ PG19 å’Œ Passkey Retrieval ç­‰é•¿åºåˆ—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„ Mamba å˜ä½“å’Œ Transformersã€‚å®éªŒè¡¨æ˜è¯¥æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡ä¸Šå®ç°äº† 48% çš„æå‡ï¼Œåœ¨å¤æ‚åº¦å’Œè®°å¿†çš„æƒè¡¡ (complexity-memory trade-off) ä¸Šå–å¾—äº†çªç ´ã€‚è¯¥ç ”ç©¶ä¸ºè¶…é•¿åºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ç§å…¼é¡¾æ•ˆç‡ä¸è®°å¿†èƒ½åŠ›çš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.03279v1",
      "published_date": "2025-09-28 14:40:58 UTC",
      "updated_date": "2025-09-28 14:40:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:24:54.193343+00:00"
    },
    {
      "arxiv_id": "2509.23913v1",
      "title": "Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks",
      "title_zh": "é¢å‘å¤šæ ·åŒ–ç§»åŠ¨æ— çº¿ç½‘ç»œè½¬å‘ç­–ç•¥æ³›åŒ–çš„æŒç»­å­¦ä¹ ",
      "authors": [
        "Cheonjin Park",
        "Victoria Manfredi",
        "Xiaolan Zhang",
        "Chengyi Liu",
        "Alicia P Wolfe",
        "Dongjin Song",
        "Sarah Tasneem",
        "Bing Wang"
      ],
      "abstract": "Deep reinforcement learning (DRL) has been successfully used to design forwarding strategies for multi-hop mobile wireless networks. While such strategies can be used directly for networks with varied connectivity and dynamic conditions, developing generalizable approaches that are effective on scenarios significantly different from the training environment remains largely unexplored. In this paper, we propose a framework to address the challenge of generalizability by (i) developing a generalizable base model considering diverse mobile network scenarios, and (ii) using the generalizable base model for new scenarios, and when needed, fine-tuning the base model using a small amount of data from the new scenarios. To support this framework, we first design new features to characterize network variation and feature quality, thereby improving the information used in DRL-based forwarding decisions. We then develop a continual learning (CL) approach able to train DRL models across diverse network scenarios without ``catastrophic forgetting.'' Using extensive evaluation, including real-world scenarios in two cities, we show that our approach is generalizable to unseen mobility scenarios. Compared to a state-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction in delay, 24% improvement in delivery rate, and comparable or slightly higher number of forwards.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šè·³ç§»åŠ¨æ— çº¿ç½‘ç»œä¸­æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)è½¬å‘ç­–ç•¥åœ¨ä¸åŒåœºæ™¯ä¸‹æ³›åŒ–æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ç»¼åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªé€‚åº”å¤šæ ·åŒ–ç§»åŠ¨åœºæ™¯çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œå¹¶åœ¨å¿…è¦æ—¶åˆ©ç”¨å°‘é‡æ–°åœºæ™¯æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°å¯¹æœªçŸ¥ç¯å¢ƒçš„å¿«é€Ÿé€‚é…ã€‚ä¸ºäº†ä¼˜åŒ–å†³ç­–è¿‡ç¨‹ï¼Œç ”ç©¶è®¾è®¡äº†ç”¨äºè¡¨å¾ç½‘ç»œåŠ¨æ€å˜åŒ–å’Œç‰¹å¾è´¨é‡çš„æ–°å‹ç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†æŒç»­å­¦ä¹ (Continual Learning, CL)æ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤šåœºæ™¯è®­ç»ƒä¸­èƒ½å¤Ÿæœ‰æ•ˆé¿å…ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)ã€‚åœ¨æ¶‰åŠä¸¤ä¸ªåŸå¸‚çœŸå®ç§»åŠ¨åœºæ™¯çš„å¹¿æ³›è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ¡ˆå±•ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œç›¸æ¯”äºç°æœ‰çš„å¯å‘å¼è½¬å‘ç­–ç•¥ï¼Œåœ¨ä¿æŒè½¬å‘å¼€é”€ç›¸è¿‘çš„å‰æä¸‹ï¼Œå°†ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†é«˜è¾¾78%ï¼Œå¹¶å°†äº¤ä»˜ç‡æå‡äº†24%ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "11 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.23913v1",
      "published_date": "2025-09-28 14:37:15 UTC",
      "updated_date": "2025-09-28 14:37:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:08.489987+00:00"
    },
    {
      "arxiv_id": "2509.23912v1",
      "title": "From Neural Networks to Logical Theories: The Correspondence between Fibring Modal Logics and Fibring Neural Networks",
      "title_zh": "ä»ç¥ç»ç½‘ç»œåˆ°é€»è¾‘ç†è®ºï¼šçº¤ç»´åŒ–æ¨¡æ€é€»è¾‘ä¸çº¤ç»´åŒ–ç¥ç»ç½‘ç»œçš„å¯¹åº”å…³ç³»",
      "authors": [
        "Ouns El Harzli",
        "Bernardo Cuenca Grau",
        "Artur d'Avila Garcez",
        "Ian Horrocks",
        "Tarek R. Besold"
      ],
      "abstract": "Fibring of modal logics is a well-established formalism for combining countable families of modal logics into a single fibred language with common semantics, characterized by fibred models. Inspired by this formalism, fibring of neural networks was introduced as a neurosymbolic framework for combining learning and reasoning in neural networks. Fibring of neural networks uses the (pre-)activations of a trained network to evaluate a fibring function computing the weights of another network whose outputs are injected back into the original network. However, the exact correspondence between fibring of neural networks and fibring of modal logics was never formally established. In this paper, we close this gap by formalizing the idea of fibred models \\emph{compatible} with fibred neural networks. Using this correspondence, we then derive non-uniform logical expressiveness results for Graph Neural Networks (GNNs), Graph Attention Networks (GATs) and Transformer encoders. Longer-term, the goal of this paper is to open the way for the use of fibring as a formalism for interpreting the logical theories learnt by neural networks with the tools of computational logic.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ¨¡æ€é€»è¾‘çº¤ç»´åŒ– (Fibring of modal logics) ä¸ç¥ç»ç½‘ç»œçº¤ç»´åŒ– (Fibring of neural networks) ä¹‹é—´çš„æ­£å¼å¯¹åº”å…³ç³»ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç¥ç»ç¬¦å· (Neurosymbolic) æ¡†æ¶ä¸é€»è¾‘å½¢å¼åŒ–ä¹‹é—´ç¼ºä¹ç†è®ºå…³è”çš„ç©ºç™½ã€‚æœ¬æ–‡é€šè¿‡å½¢å¼åŒ–å®šä¹‰äº†ä¸çº¤ç»´åŒ–ç¥ç»ç½‘ç»œå…¼å®¹çš„çº¤ç»´åŒ–æ¨¡å‹ (Fibred models)ï¼Œå¹¶å»ºç«‹äº†ä¸¤è€…ä¹‹é—´çš„ç²¾ç¡®æ•°å­¦è”ç³»ã€‚åˆ©ç”¨è¿™ä¸€å¯¹åº”å…³ç³»ï¼Œç ”ç©¶è€…æ¨å¯¼å‡ºäº†å›¾ç¥ç»ç½‘ç»œ (GNNs)ã€å›¾æ³¨æ„åŠ›ç½‘ç»œ (GATs) ä»¥åŠ Transformer ç¼–ç å™¨çš„éå‡åŒ€é€»è¾‘è¡¨è¾¾èƒ½åŠ› (Logical expressiveness) ç»“æœã€‚è¯¥å·¥ä½œä¸ä»…ä¸ºç†è§£ç¥ç»ç½‘ç»œçš„æ¨ç†æœºåˆ¶æä¾›äº†ç†è®ºæ”¯æ’‘ï¼Œè¿˜ä¸ºæœªæ¥åˆ©ç”¨è®¡ç®—é€»è¾‘å·¥å…·è§£é‡Šç¥ç»ç½‘ç»œæ‰€å­¦ä¹ åˆ°çš„é€»è¾‘ç†è®ºå¼€è¾Ÿäº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23912v1",
      "published_date": "2025-09-28 14:32:42 UTC",
      "updated_date": "2025-09-28 14:32:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:11.793476+00:00"
    },
    {
      "arxiv_id": "2509.23906v1",
      "title": "EWC-Guided Diffusion Replay for Exemplar-Free Continual Learning in Medical Imaging",
      "title_zh": "é¢å‘åŒ»å­¦å½±åƒæ— æ ·æœ¬æŒç»­å­¦ä¹ çš„ EWC å¼•å¯¼æ‰©æ•£é‡æ”¾",
      "authors": [
        "Anoushka Harit",
        "William Prew",
        "Zhongtian Sun",
        "Florian Markowetz"
      ],
      "abstract": "Medical imaging foundation models must adapt over time, yet full retraining is often blocked by privacy constraints and cost. We present a continual learning framework that avoids storing patient exemplars by pairing class conditional diffusion replay with Elastic Weight Consolidation. Using a compact Vision Transformer backbone, we evaluate across eight MedMNIST v2 tasks and CheXpert. On CheXpert our approach attains 0.851 AUROC, reduces forgetting by more than 30\\% relative to DER\\texttt{++}, and approaches joint training at 0.869 AUROC, while remaining efficient and privacy preserving. Analyses connect forgetting to two measurable factors: fidelity of replay and Fisher weighted parameter drift, highlighting the complementary roles of replay diffusion and synaptic stability. The results indicate a practical route for scalable, privacy aware continual adaptation of clinical imaging models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—å½±åƒåŸºç¡€æ¨¡å‹åœ¨æŒç»­å­¦ä¹ (Continual Learning)ä¸­é¢ä¸´çš„éšç§ä¿æŠ¤ä¸é‡æ–°è®­ç»ƒæˆæœ¬æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆç±»åˆ«æ¡ä»¶æ‰©æ•£å›æ”¾(class conditional diffusion replay)ä¸å¼¹æ€§æƒé‡æ•´åˆ(Elastic Weight Consolidation, EWC)çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ–¹æ¡ˆé‡‡ç”¨ Vision Transformer ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆæ ·æœ¬æ›¿ä»£çœŸå®çš„æ‚£è€…æ•°æ®ï¼Œå®ç°äº†æ— éœ€å­˜å‚¨æ ·æœ¬çš„æŒç»­å­¦ä¹ ã€‚åœ¨ MedMNIST v2 å’Œ CheXpert æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ CheXpert ä»»åŠ¡ä¸­è¾¾åˆ°äº† 0.851 AUROCï¼Œç›¸æ¯” DER++ å‡å°‘äº†è¶…è¿‡ 30% çš„é—å¿˜ç‡ï¼Œæ€§èƒ½è¡¨ç°æ¥è¿‘è”åˆè®­ç»ƒ(joint training)çš„ 0.869 AUROCã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†é—å¿˜ä¸å›æ”¾ä¿çœŸåº¦(fidelity of replay)åŠè´¹èˆå°”åŠ æƒå‚æ•°æ¼‚ç§»(Fisher weighted parameter drift)ä¹‹é—´çš„å…³è”ï¼Œå¼ºè°ƒäº†æ‰©æ•£å›æ”¾ä¸çªè§¦ç¨³å®šæ€§(synaptic stability)çš„äº’è¡¥ä½œç”¨ã€‚è¯¥æˆæœä¸ºä¸´åºŠå½±åƒæ¨¡å‹å®ç°é«˜æ•ˆã€å¯æ‰©å±•ä¸”ç¬¦åˆéšç§è¦æ±‚çš„æŒç»­é€‚åº”æä¾›äº†ä¸€æ¡åˆ‡å®å¯è¡Œçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AI That Keeps Up: NeurIPS 2025 Workshop on Continual and Compatible Foundation Model Updates",
      "pdf_url": "https://arxiv.org/pdf/2509.23906v1",
      "published_date": "2025-09-28 14:23:46 UTC",
      "updated_date": "2025-09-28 14:23:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:13.290852+00:00"
    },
    {
      "arxiv_id": "2509.23901v2",
      "title": "Interpreting deep learning-based stellar mass estimation via causal analysis and mutual information decomposition",
      "title_zh": "é€šè¿‡å› æœåˆ†æä¸äº’ä¿¡æ¯åˆ†è§£è§£æåŸºäºæ·±åº¦å­¦ä¹ çš„æ’æ˜Ÿè´¨é‡ä¼°è®¡",
      "authors": [
        "Wei Zhang",
        "Qiufan Lin",
        "Yuan-Sen Ting",
        "Shupei Chen",
        "Hengxin Ruan",
        "Song Li",
        "Yifan Wang"
      ],
      "abstract": "End-to-end deep learning models fed with multi-band galaxy images are powerful data-driven tools used to estimate galaxy physical properties in the absence of spectroscopy. However, due to a lack of interpretability and the associational nature of such models, it is difficult to understand how the information that is included in addition to integrated photometry (e.g., morphology) contributes to the estimation task. Improving our understanding in this field would enable further advances into unraveling the physical connections among galaxy properties and optimizing data exploitation. Therefore, our work is aimed at interpreting the deep learning-based estimation of stellar mass via two interpretability techniques: causal analysis and mutual information decomposition. The former reveals the causal paths between multiple variables beyond nondirectional statistical associations, while the latter quantifies the multicomponent contributions (i.e., redundant, unique, and synergistic) of different input data to the stellar mass estimation. Using data from the Sloan Digital Sky Survey (SDSS) and the Wide-field Infrared Survey Explorer (WISE), we obtained meaningful results that provide physical interpretations for image-based models. Our work demonstrates the gains from combining deep learning with interpretability techniques, and holds promise in promoting more data-driven astrophysical research (e.g., astrophysical parameter estimations and investigations on complex multivariate physical processes).",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨å› æœåˆ†æ (causal analysis) å’Œäº’ä¿¡æ¯åˆ†è§£ (mutual information decomposition) ä¸¤ç§æŠ€æœ¯ï¼Œå¯¹åŸºäºæ·±åº¦å­¦ä¹  (deep learning) çš„æ˜Ÿç³»æ’æ˜Ÿè´¨é‡ (stellar mass) ä¼°ç®—æ¨¡å‹è¿›è¡Œäº†æ·±å…¥è§£è¯»ã€‚å› æœåˆ†ææ­ç¤ºäº†å˜é‡ä¹‹é—´è¶…è¶Šç»Ÿè®¡å…³è”çš„å› æœè·¯å¾„ï¼Œè€Œäº’ä¿¡æ¯åˆ†è§£åˆ™é‡åŒ–äº†è¾“å…¥æ•°æ®ä¸­å†—ä½™ (redundant)ã€å”¯ä¸€ (unique) å’ŒååŒ (synergistic) çš„ä¿¡æ¯è´¡çŒ®ã€‚é€šè¿‡å¯¹æ–¯éš†æ•°å­—å·¡å¤© (SDSS) å’Œå¹¿åŸŸçº¢å¤–å·¡å¤©æ¢æµ‹å™¨ (WISE) çš„å¤šæ³¢æ®µå½±åƒæ•°æ®è¿›è¡Œåˆ†æï¼Œè¯¥å·¥ä½œä¸ºç«¯åˆ°ç«¯æ¨¡å‹æä¾›äº†æ˜ç¡®çš„ç‰©ç†å±‚é¢çš„è§£é‡Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆæ·±åº¦å­¦ä¹ ä¸å¯è§£é‡Šæ€§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæ­ç¤ºæ˜Ÿç³»å±æ€§é—´çš„ç‰©ç†è”ç³»ï¼Œå¹¶ä¼˜åŒ–å¤©æ–‡è§‚æµ‹æ•°æ®çš„åˆ©ç”¨æ•ˆç‡ï¼Œä¸ºæœªæ¥æ•°æ®é©±åŠ¨çš„å¤©ä½“ç‰©ç†ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "astro-ph.IM",
        "astro-ph.GA",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "astro-ph.IM",
      "comment": "Accepted at Astronomy & Astrophysics; 23 + 12 pages; 8 + 16 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23901v2",
      "published_date": "2025-09-28 14:17:25 UTC",
      "updated_date": "2025-10-05 15:01:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:18.996580+00:00"
    },
    {
      "arxiv_id": "2509.23895v1",
      "title": "Preserving Cross-Modal Stability for Visual Unlearning in Multimodal Scenarios",
      "title_zh": "å¤šæ¨¡æ€åœºæ™¯ä¸‹è§†è§‰åå­¦ä¹ ä¸­çš„è·¨æ¨¡æ€ç¨³å®šæ€§ä¿æŒ",
      "authors": [
        "Jinghan Xu Yuyang Zhang Qixuan Cai Jiancheng Chen Keqiu Li"
      ],
      "abstract": "Visual modality is the most vulnerable to privacy leakage in real-world multimodal applications like autonomous driving with visual and radar data; Machine unlearning removes specific training data from pre-trained models to address privacy leakage, however, existing methods fail to preserve cross-modal knowledge and maintain intra-class structural stability of retain data, leading to reduced overall and other modalities' performance during visual unlearning; to address these challenges, we propose a Cross-modal Contrastive Unlearning (CCU) framework, which integrates three key components: (a) selective visual unlearning: employing inverse contrastive learning to dissociate visual representations from their original semantics, (b) cross-modal knowledge retention: preserving other modalities' discriminability through semantic consistency, and (c) dual-set contrastive separation: preserving the model performance via isolation of structural perturbations between the unlearn set and retain set; extensive experiments on three datasets demonstrate the superiority of CCU, and our method achieves a 7.12% accuracy improvement with only 7% of the unlearning time compared to the top-accuracy baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€åº”ç”¨ä¸­è§†è§‰æ¨¡æ€ææ˜“å‘ç”Ÿéšç§æ³„éœ²çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Cross-modal Contrastive Unlearning (CCU) çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨é—å¿˜ (Machine Unlearning) æ–¹æ³•åœ¨è§†è§‰é—å¿˜è¿‡ç¨‹ä¸­éš¾ä»¥ç»´æŒè·¨æ¨¡æ€çŸ¥è¯†åŠç±»å†…ç»“æ„ç¨³å®šæ€§çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶æ•´åˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆé€šè¿‡é€†å¯¹æ¯”å­¦ä¹ å®ç°é€‰æ‹©æ€§è§†è§‰é—å¿˜ (selective visual unlearning) ä»¥è§£ç¦»è§†è§‰è¡¨ç¤ºä¸å…¶åŸå§‹è¯­ä¹‰ï¼›å…¶æ¬¡åˆ©ç”¨è¯­ä¹‰ä¸€è‡´æ€§è¿›è¡Œè·¨æ¨¡æ€çŸ¥è¯†ä¿ç•™ (cross-modal knowledge retention) ä»¥ç»´æŒå…¶ä»–æ¨¡æ€çš„åˆ¤åˆ«åŠ›ï¼›æœ€åé‡‡ç”¨åŒé›†å¯¹æ¯”åˆ†ç¦» (dual-set contrastive separation) æ¥éš”ç¦»é—å¿˜é›†ä¸ä¿ç•™é›†ä¹‹é—´çš„ç»“æ„æ‰°åŠ¨ã€‚åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº† CCU çš„ä¼˜è¶Šæ€§ï¼Œå…¶å‡†ç¡®ç‡æ¯”é¡¶å°–åŸºçº¿æå‡äº† 7.12%ï¼Œä¸”é—å¿˜è€—æ—¶ä»…ä¸ºåŸºçº¿çš„ 7%ã€‚è¯¥ç ”ç©¶åœ¨æœ‰æ•ˆç§»é™¤ç‰¹å®šéšç§ä¿¡æ¯çš„åŒæ—¶ï¼ŒæˆåŠŸä¿æŒäº†æ¨¡å‹åœ¨å¤šæ¨¡æ€åœºæ™¯ä¸‹çš„æ•´ä½“æ€§èƒ½ä¸ç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages,4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23895v1",
      "published_date": "2025-09-28 14:03:37 UTC",
      "updated_date": "2025-09-28 14:03:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:47.401350+00:00"
    },
    {
      "arxiv_id": "2509.23893v1",
      "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings",
      "title_zh": "ç¼“è§£ç¾éš¾æ€§é—å¿˜çš„åŠ¨æ€æ­£äº¤æŒç»­å¾®è°ƒ",
      "authors": [
        "Zhixin Zhang",
        "Zeming Wei",
        "Meng Sun"
      ],
      "abstract": "Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at https://github.com/meloxxxxxx/DOC.",
      "tldr_zh": "é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒç»­å­¦ä¹ ï¼ˆcontinual learningï¼‰ä¸­é¢ä¸´çš„ç¾éš¾æ€§é—å¿˜ï¼ˆcatastrophic forgettingï¼‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†å¾®è°ƒè¿‡ç¨‹ä¸­åŠŸèƒ½æ–¹å‘ï¼ˆfunctional directionsï¼‰çš„æ¼‚ç§»æ˜¯å¯¼è‡´ç°æœ‰åŸºäºæ­£åˆ™åŒ–ï¼ˆregularization-basedï¼‰æ–¹æ³•å¤±è´¥çš„å…³é”®åŸå› ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åŠ¨æ€æ­£äº¤æŒç»­ï¼ˆDynamic Orthogonal Continual, DOCï¼‰å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è¿½è¸ªè¿™äº›åŠŸèƒ½æ–¹å‘çš„æ¼‚ç§»å¹¶åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿›è¡ŒåŠ¨æ€æ›´æ–°ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ–°ä»»åŠ¡å‚æ•°çš„æ¢¯åº¦è°ƒæ•´ä¸ºä¸æ‰€è¿½è¸ªçš„å†å²åŠŸèƒ½æ–¹å‘æ­£äº¤ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†æ–°æ—§ä»»åŠ¡ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ã€‚åœ¨å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹æŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼ŒDOC æ–¹æ³•æ˜¾è‘—ä¼˜äºä»¥å¾€æŠ€æœ¯ï¼Œæœ‰æ•ˆç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¸ºå®ç°é²æ£’çš„å¤§è¯­è¨€æ¨¡å‹æŒç»­å¾®è°ƒæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23893v1",
      "published_date": "2025-09-28 13:55:05 UTC",
      "updated_date": "2025-09-28 13:55:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:35.669294+00:00"
    },
    {
      "arxiv_id": "2509.23887v1",
      "title": "Gradient Flow Convergence Guarantee for General Neural Network Architectures",
      "title_zh": "é€šç”¨ç¥ç»ç½‘ç»œæ¶æ„çš„æ¢¯åº¦æµæ”¶æ•›æ€§ä¿è¯",
      "authors": [
        "Yash Jakhmola"
      ],
      "abstract": "A key challenge in modern deep learning theory is to explain the remarkable success of gradient-based optimization methods when training large-scale, complex deep neural networks. Though linear convergence of such methods has been proved for a handful of specific architectures, a united theory still evades researchers. This article presents a unified proof for linear convergence of continuous gradient descent, also called gradient flow, while training any neural network with piecewise non-zero polynomial activations or ReLU, sigmoid activations. Our primary contribution is a single, general theorem that not only covers architectures for which this result was previously unknown but also consolidates existing results under weaker assumptions. While our focus is theoretical and our results are only exact in the infinitesimal step size limit, we nevertheless find excellent empirical agreement between the predictions of our result and those of the practical step-size gradient descent method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°ä»£æ·±åº¦å­¦ä¹ ç†è®ºä¸­è§£é‡Šæ¢¯åº¦ä¼˜åŒ–æ–¹æ³•æˆåŠŸæ€§çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å…³äºæ¢¯åº¦æµ(Gradient Flow)çº¿æ€§æ”¶æ•›(linear convergence)çš„ç»Ÿä¸€è¯æ˜ã€‚è¯¥ç†è®ºé€‚ç”¨äºä»»ä½•é‡‡ç”¨åˆ†æ®µéé›¶å¤šé¡¹å¼(piecewise non-zero polynomial)ã€ReLUæˆ–Sigmoidæ¿€æ´»å‡½æ•°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæœ‰æ•ˆå¡«è¡¥äº†é€šç”¨æ¶æ„æ”¶æ•›æ€§ä¿è¯çš„ç†è®ºç©ºç™½ã€‚å…¶æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ä¸ªæ™®é€‚æ€§å®šç†ï¼Œä¸ä»…åœ¨æ›´å¼±çš„å‡è®¾æ¡ä»¶ä¸‹æ•´åˆäº†ç°æœ‰ç ”ç©¶ç»“è®ºï¼Œè¿˜å°†å…¶æ‰©å±•åˆ°äº†æ­¤å‰æœªè¢«è¦†ç›–çš„æ¶æ„é¢†åŸŸã€‚è™½ç„¶ç ”ç©¶ç»“è®ºä¸»è¦å»ºç«‹åœ¨æ— ç©·å°æ­¥é•¿çš„ç†è®ºæé™ä¹‹ä¸Šï¼Œä½†å®éªŒç»“æœæ˜¾ç¤ºè¯¥ç†è®ºé¢„æµ‹ä¸å®é™…çš„æ¢¯åº¦ä¸‹é™(gradient descent)ç®—æ³•åœ¨ç»éªŒä¸Šé«˜åº¦ä¸€è‡´ã€‚è¿™ä¸€æˆæœä¸ºæ·±å…¥ç†è§£å¤§è§„æ¨¡å¤æ‚ç¥ç»ç½‘ç»œçš„è®­ç»ƒåŠ¨åŠ›å­¦æä¾›äº†åšå®çš„æ•°å­¦æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 figures, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2509.23887v1",
      "published_date": "2025-09-28 13:52:13 UTC",
      "updated_date": "2025-09-28 13:52:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:50.884926+00:00"
    },
    {
      "arxiv_id": "2509.23886v1",
      "title": "Towards Understanding Subliminal Learning: When and How Hidden Biases Transfer",
      "title_zh": "æ·±å…¥ç†è§£æ½œéšå­¦ä¹ ï¼šéšæ€§åè§è¿ç§»çš„æ—¶æœºä¸æœºåˆ¶",
      "authors": [
        "Simon Schrodi",
        "Elias Kempf",
        "Fazl Barez",
        "Thomas Brox"
      ],
      "abstract": "Language models can transfer hidden biases during distillation. For example, a teacher that \"likes owls\" can make its student \"like owls\" too, even when the training data consists only of lists of numbers. This surprising phenomenon is called subliminal learning. Subliminal learning can be expected under soft distillation, where the student is trained on the teacher's full next-token distribution. But the fact that this also occurs under hard distillation-where the student only sees sampled tokens-raises a deeper question: when and how does subliminal learning actually occur? We answer this question through controlled experiments and mechanistic analysis. Our results show that subliminal learning does not need (global) token entanglement or logit leakage. Instead, it comes down to a small set of divergence tokens-rare cases where teachers with different biases would predict different tokens. Masking out these tokens mostly removes the hidden bias transfer. Mechanistically, divergence tokens reveal that early layers are critical. Surprisingly, finetuning even a single such early layer is sufficient for subliminal learning. Finally, we find that subliminal learning is fragile. Even small changes, like paraphrasing prompts, are usually sufficient to suppress it.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨è’¸é¦(Distillation)è¿‡ç¨‹ä¸­äº§ç”Ÿæ½œæ„è¯†å­¦ä¹ (Subliminal Learning)çš„æœºåˆ¶ï¼Œå³æ•™å¸ˆæ¨¡å‹å¦‚ä½•åœ¨çœ‹ä¼¼æ— å…³çš„ä»»åŠ¡æ•°æ®ä¸­å°†éšè—åå·®ä¼ é€’ç»™å­¦ç”Ÿæ¨¡å‹ã€‚é€šè¿‡å—æ§å®éªŒå’Œæœºæ¢°åŒ–åˆ†æ(Mechanistic Analysis)ï¼Œç ”ç©¶å‘ç°æ½œæ„è¯†å­¦ä¹ å¹¶ä¸ä¾èµ–å…¨å±€çš„Token Entanglementæˆ–Logit Leakageï¼Œè€Œæ˜¯ç”±ä¸€ç»„æå°‘æ•°çš„Divergence Tokensé©±åŠ¨ã€‚è¿™äº›å…³é”®Tokenè¡¨æ˜æ¨¡å‹çš„æ—©æœŸå±‚(Early Layers)åœ¨åå·®ä¼ é€’ä¸­èµ·åˆ°äº†å†³å®šæ€§ä½œç”¨ï¼Œç”šè‡³ä»…é€šè¿‡å¾®è°ƒå•ä¸ªæ—©æœŸå±‚å°±èƒ½è¯±å‘è¯¥ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ­ç¤ºäº†æ½œæ„è¯†å­¦ä¹ å…·æœ‰è„†å¼±æ€§(Fragile)ï¼Œé€šè¿‡ç®€å•çš„æç¤ºè¯æ”¹å†™(Paraphrasing Prompts)ç­‰å¾®å°å˜åŠ¨å³å¯æœ‰æ•ˆæŠ‘åˆ¶åå·®çš„è½¬ç§»ã€‚è¯¥å·¥ä½œä¸ºç†è§£æ¨¡å‹å¦‚ä½•æ•æ‰å¹¶ä¼ é€’æ½œåœ¨åå·®æä¾›äº†é‡è¦çš„ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23886v1",
      "published_date": "2025-09-28 13:51:22 UTC",
      "updated_date": "2025-09-28 13:51:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:42.292257+00:00"
    },
    {
      "arxiv_id": "2509.23885v3",
      "title": "Extendable Generalization Self-Supervised Diffusion for Low-Dose CT Reconstruction",
      "title_zh": "ç”¨äºä½å‰‚é‡ CT é‡å»ºçš„å¯æ‰©å±•æ³›åŒ–è‡ªç›‘ç£æ‰©æ•£æ¨¡å‹",
      "authors": [
        "Guoquan Wei",
        "Liu Shi",
        "Zekun Zhou",
        "Mohan Li",
        "Cunfeng Wei",
        "Wenzhe Shan",
        "Qiegen Liu"
      ],
      "abstract": "Current methods based on deep learning for self-supervised low-dose CT (LDCT) reconstruction, while reducing the dependence on paired data, face the problem of significantly decreased generalization when training with single-dose data and extending to other doses. To enable dose-extensive generalization using only single-dose projection data for training, this work proposes a novel method of Extendable GENeraLization self-supervised Diffusion (EGenDiff) for low-dose CT reconstruction. Specifically, a contextual subdata self-enhancing similarity strategy is designed to provide an initial prior for the subsequent progress. During training, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. On the stage of inference, the pixel-wise self-correcting fusion technique is proposed for data fidelity enhancement, resulting in extensive generalization of higher and lower doses or even unseen doses. EGenDiff requires only LDCT projection data for training and testing. Comprehensive evaluation on benchmark datasets, clinical data, photon counting CT data, and across all three anatomical planes (transverse, coronal, and sagittal) demonstrates that EGenDiff enables extendable generalization multi-dose, yielding reconstructions that consistently outperform leading existing methods.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶é’ˆå¯¹è‡ªç›‘ç£ä½å‰‚é‡CT (LDCT) é‡å»ºæ–¹æ³•åœ¨å•å‰‚é‡æ•°æ®è®­ç»ƒåæ³›åŒ–æ€§æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEGenDiff (Extendable GENeraLization self-supervised Diffusion) çš„æ–°å‹é‡å»ºæ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡ä¸Šä¸‹æ–‡å­æ•°æ®è‡ªå¢å¼ºç›¸ä¼¼æ€§ç­–ç•¥ (contextual subdata self-enhancing similarity strategy) æä¾›åˆå§‹å…ˆéªŒï¼Œå¹¶åœ¨è®­ç»ƒé˜¶æ®µå°†çŸ¥è¯†è’¸é¦ (knowledge distillation) ä¸æ½œåœ¨æ‰©æ•£æ¨¡å‹ (latent diffusion models) æ·±åº¦ç»“åˆä»¥ä¼˜åŒ–å›¾åƒç»†èŠ‚ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†åƒç´ çº§è‡ªæ ¡æ­£èåˆæŠ€æœ¯ (pixel-wise self-correcting fusion) æ¥å¢å¼ºæ•°æ®ä¿çœŸåº¦ï¼Œä»è€Œå®ç°äº†å¯¹æ›´é«˜ã€æ›´ä½ä¹ƒè‡³æœªè§å‰‚é‡çš„å¹¿æ³›æ³›åŒ–ã€‚EGenDiff ä»…éœ€ LDCT æŠ•å½±æ•°æ®è¿›è¡Œè®­ç»ƒä¸æµ‹è¯•ï¼Œå®éªŒç»“æœåœ¨åŸºå‡†æ•°æ®é›†ã€ä¸´åºŠæ•°æ®ä»¥åŠå…‰å­è®¡æ•°CTæ•°æ®ä¸Šå‡è¯æ˜å…¶é‡å»ºæ€§èƒ½ä¼˜äºç°æœ‰çš„é¢†å…ˆæ–¹æ³•ï¼Œä¸”åœ¨å¤šä¸ªè§£å‰–å¹³é¢ä¸Šå±•ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23885v3",
      "published_date": "2025-09-28 13:50:29 UTC",
      "updated_date": "2026-01-21 06:51:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:48.890233+00:00"
    },
    {
      "arxiv_id": "2509.23882v2",
      "title": "Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B",
      "title_zh": "é‡åŒ–ç‹‚çƒ­ã€æ¨ç†é»‘æ´ã€è–›å®šè°”çš„åˆè§„æ€§åŠå…¶å®ƒï¼šGPT-OSS-20B æ·±åº¦æ¢ç©¶",
      "authors": [
        "Shuyi Lin",
        "Tian Lu",
        "Zikai Wang",
        "Bo Wen",
        "Yibo Zhao",
        "Cheng Tan"
      ],
      "abstract": "OpenAI's GPT-OSS family provides open-weight language models with explicit chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an extensive security evaluation of GPT-OSS-20B that probes the model's behavior under different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a systematic LLM evaluation tool, the study uncovers several failure modes including quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting. Experiments demonstrate how these behaviors can be exploited on the GPT-OSS-20B model, leading to severe consequences.",
      "tldr_zh": "æœ¬ç ”ç©¶å¯¹OpenAIçš„GPT-OSS-20Bæ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨æ€§è¯„ä¼°ï¼Œè¯¥æ¨¡å‹æ˜¯é‡‡ç”¨Harmony prompt formatå¹¶å…·æœ‰æ˜¾å¼Chain-of-Thought (CoT)æ¨ç†èƒ½åŠ›çš„å¼€æºæƒé‡è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ç³»ç»ŸåŒ–è¯„ä¼°å·¥å…·Jailbreak Oracle (JO)ï¼Œåœ¨å¤šç§å¯¹æŠ—æ¡ä»¶ä¸‹æ·±å…¥æ¢æµ‹äº†è¯¥æ¨¡å‹çš„è¡Œä¸ºè¾¹ç•Œã€‚è¯„ä¼°å‘ç°äº†è¯¥æ¨¡å‹å­˜åœ¨çš„å¤šç§å¤±æ•ˆæ¨¡å¼ï¼Œå…·ä½“æ¶µç›–äº†Quant Feverã€Reasoning Blackholesã€Schrodinger's Complianceã€Reasoning Procedure Mirageä»¥åŠChain-oriented Promptingã€‚å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†è¿™äº›è¡Œä¸ºæ¨¡å¼åœ¨GPT-OSS-20Bä¸Šå¯ä»¥è¢«æ¶æ„åˆ©ç”¨ï¼Œä»è€Œå¯¼è‡´ä¸¥é‡çš„å®‰å…¨æ€§åæœã€‚è¯¥ç ”ç©¶ä¸ºç†è§£å…·å¤‡æ˜¾å¼æ¨ç†èƒ½åŠ›çš„å¼€æºæ¨¡å‹åœ¨å¯¹æŠ—ç¯å¢ƒä¸‹çš„è„†å¼±æ€§æä¾›äº†é‡è¦è§è§£ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23882v2",
      "published_date": "2025-09-28 13:44:37 UTC",
      "updated_date": "2025-10-05 14:53:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:53.206185+00:00"
    },
    {
      "arxiv_id": "2509.23879v1",
      "title": "PCRI: Measuring Context Robustness in Multimodal Models for Enterprise Applications",
      "title_zh": "PCRIï¼šè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨ä¼ä¸šçº§åº”ç”¨ä¸­çš„ä¸Šä¸‹æ–‡é²æ£’æ€§",
      "authors": [
        "Hitesh Laxmichand Patel",
        "Amit Agarwal",
        "Srikant Panda",
        "Hansa Meghwani",
        "Karan Dua",
        "Paul Li",
        "Tao Sheng",
        "Sujith Ravi",
        "Dan Roth"
      ],
      "abstract": "The reliability of Multimodal Large Language Models (MLLMs) in real-world settings is often undermined by sensitivity to irrelevant or distracting visual context, an aspect not captured by existing evaluation metrics. We introduce the \\textbf{Patch Context Robustness Index (PCRI)}, the first systematic and interpretable score for quantifying MLLM robustness to variations in visual context granularity, measuring performance changes between localized image patches and full-image input.\n  Applying PCRI to 19 state-of-the-art MLLMs across 15 vision-language benchmarks, we find that most leading models remain brittle to background noise, with only a few, such as InternVL2-26B and Qwen2VL-72B, demonstrating consistent robustness across tasks. PCRI analysis also highlights how different model architectures handle and integrate visual context, offering actionable diagnostic insight for both researchers and practitioners.\n  PCRI enables rigorous comparison of context robustness, supporting principled model selection and guiding the development of future architectures and training strategies for robust, real-world deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨ç°å®åœºæ™¯ä¸­å¯¹æ— å…³æˆ–å¹²æ‰°æ€§è§†è§‰èƒŒæ™¯é«˜åº¦æ•æ„Ÿä¸”ç¼ºä¹é‡åŒ–æŒ‡æ ‡çš„é—®é¢˜ï¼Œæå‡ºäº† Patch Context Robustness Index (PCRI)ã€‚ä½œä¸ºé¦–ä¸ªç³»ç»Ÿä¸”å¯è§£é‡Šçš„è¯„åˆ†æŒ‡æ ‡ï¼ŒPCRI é€šè¿‡è¡¡é‡å±€éƒ¨å›¾åƒå— (localized image patches) ä¸å…¨å›¾è¾“å…¥ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œé‡åŒ– MLLMs å¯¹è§†è§‰ä¸Šä¸‹æ–‡ç²’åº¦å˜åŒ–çš„é²æ£’æ€§ã€‚ç ”ç©¶äººå‘˜åœ¨ 15 ä¸ªè§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯• (vision-language benchmarks) ä¸­è¯„ä¼°äº† 19 ä¸ªå…ˆè¿›æ¨¡å‹ï¼Œå‘ç°å¤§å¤šæ•°æ¨¡å‹å¯¹èƒŒæ™¯å™ªå£° (background noise) è¡¨ç°è„†å¼±ï¼Œä»…æœ‰ InternVL2-26B å’Œ Qwen2VL-72B ç­‰å°‘æ•°æ¨¡å‹å±•ç°å‡ºç¨³å®šçš„ä¸€è‡´æ€§ã€‚é€šè¿‡ PCRI åˆ†æï¼Œè¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸åŒæ¨¡å‹æ¶æ„å¤„ç†å’Œé›†æˆè§†è§‰ä¸Šä¸‹æ–‡çš„å·®å¼‚ï¼Œä¸ºå­¦æœ¯ç ”ç©¶ä¸å·¥ä¸šå®è·µæä¾›äº†å¯æ“ä½œçš„è¯Šæ–­è§è§£ã€‚è¿™ä¸€æŒ‡æ ‡ä¸ä»…æ”¯æŒç§‘å­¦çš„æ¨¡å‹é€‰æ‹©ï¼Œè¿˜ä¸ºæœªæ¥é¢å‘çœŸå®ä¸–ç•Œéƒ¨ç½²çš„é²æ£’æ¶æ„å¼€å‘å’Œè®­ç»ƒç­–ç•¥æä¾›äº†å…³é”®æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23879v1",
      "published_date": "2025-09-28 13:39:57 UTC",
      "updated_date": "2025-09-28 13:39:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:25:58.673095+00:00"
    },
    {
      "arxiv_id": "2509.23878v1",
      "title": "Disentangling Score Content and Performance Style for Joint Piano Rendering and Transcription",
      "title_zh": "é¢å‘é’¢ç´è”åˆæ¸²æŸ“ä¸è½¬è°±çš„ä¹è°±å†…å®¹ä¸æ¼”å¥é£æ ¼è§£è€¦",
      "authors": [
        "Wei Zeng",
        "Junchuan Zhao",
        "Ye Wang"
      ],
      "abstract": "Expressive performance rendering (EPR) and automatic piano transcription (APT) are fundamental yet inverse tasks in music information retrieval: EPR generates expressive performances from symbolic scores, while APT recovers scores from performances. Despite their dual nature, prior work has addressed them independently. In this paper we propose a unified framework that jointly models EPR and APT by disentangling note-level score content and global performance style representations from both paired and unpaired data. Our framework is built on a transformer-based sequence-to-sequence architecture and is trained using only sequence-aligned data, without requiring fine-grained note-level alignment. To automate the rendering process while ensuring stylistic compatibility with the score, we introduce an independent diffusion-based performance style recommendation module that generates style embeddings directly from score content. This modular component supports both style transfer and flexible rendering across a range of expressive styles. Experimental results from both objective and subjective evaluations demonstrate that our framework achieves competitive performance on EPR and APT tasks, while enabling effective content-style disentanglement, reliable style transfer, and stylistically appropriate rendering. Demos are available at https://jointpianist.github.io/epr-apt/",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†è¡¨ç°åŠ›é’¢ç´æ¼”å¥æ¸²æŸ“ (Expressive Performance Rendering, EPR) ä¸è‡ªåŠ¨é’¢ç´è°±é¢è½¬å½• (Automatic Piano Transcription, APT) è¿™ä¸¤ä¸ªäº’é€†ä»»åŠ¡è¿›è¡Œè”åˆå»ºæ¨¡ã€‚è¯¥æ¡†æ¶åŸºäº Transformer çš„åºåˆ—åˆ°åºåˆ— (sequence-to-sequence) æ¶æ„ï¼Œé€šè¿‡è§£è€¦éŸ³ç¬¦çº§çš„ä¹è°±å†…å®¹ (score content) ä¸å…¨å±€æ¼”å¥é£æ ¼ (performance style) è¡¨å¾ï¼Œå®ç°äº†å¯¹é…å¯¹åŠéé…å¯¹æ•°æ®çš„å…±åŒå¤„ç†ã€‚è®­ç»ƒè¿‡ç¨‹ä»…ä¾èµ–åºåˆ—å¯¹é½ (sequence-aligned) æ•°æ®ï¼Œæ— éœ€ç²¾ç»†çš„éŸ³ç¬¦çº§å¯¹é½ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹å¯¹é«˜è´¨é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªç‹¬ç«‹çš„åŸºäºæ‰©æ•£æ¨¡å‹ (diffusion-based) çš„æ¼”å¥é£æ ¼æ¨èæ¨¡å—ï¼Œèƒ½å¤Ÿç›´æ¥ä»ä¹è°±å†…å®¹ç”Ÿæˆé£æ ¼åµŒå…¥ (style embeddings)ï¼Œä»è€Œæ”¯æŒçµæ´»çš„é£æ ¼è¿ç§»å’Œæ¸²æŸ“ã€‚å®¢è§‚ä¸ä¸»è§‚è¯„ä¼°ç»“æœè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ EPR å’Œ APT ä»»åŠ¡ä¸Šå‡å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œå¹¶å®ç°äº†æœ‰æ•ˆçš„å†…å®¹é£æ ¼è§£è€¦ä¸é£æ ¼ä¸€è‡´çš„æ¼”å¥ç”Ÿæˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "30 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23878v1",
      "published_date": "2025-09-28 13:36:33 UTC",
      "updated_date": "2025-09-28 13:36:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:03.870251+00:00"
    },
    {
      "arxiv_id": "2509.23876v2",
      "title": "Not All Tokens are Guided Equal: Improving Guidance in Visual Autoregressive Models",
      "title_zh": "å¹¶éæ‰€æœ‰ Token å‡å—å¹³ç­‰å¼•å¯¼ï¼šè§†è§‰è‡ªå›å½’æ¨¡å‹å¼•å¯¼æœºåˆ¶çš„æ”¹è¿›",
      "authors": [
        "Ky Dan Nguyen",
        "Hoang Lam Tran",
        "Anh-Dung Dinh",
        "Daochang Liu",
        "Weidong Cai",
        "Xiuying Wang",
        "Chang Xu"
      ],
      "abstract": "Autoregressive (AR) models based on next-scale prediction are rapidly emerging as a powerful tool for image generation, but they face a critical weakness: information inconsistencies between patches across timesteps introduced by progressive resolution scaling. These inconsistencies scatter guidance signals, causing them to drift away from conditioning information and leaving behind ambiguous, unfaithful features. We tackle this challenge with Information-Grounding Guidance (IGG), a novel mechanism that anchors guidance to semantically important regions through attention. By adaptively reinforcing informative patches during sampling, IGG ensures that guidance and content remain tightly aligned. Across both class-conditioned and text-to-image generation tasks, IGG delivers sharper, more coherent, and semantically grounded images, setting a new benchmark for AR-based methods.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºä¸‹ä¸€å°ºåº¦é¢„æµ‹(next-scale prediction)çš„è§†è§‰è‡ªå›å½’(Autoregressive)æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å­˜åœ¨çš„å…³é”®å¼±ç‚¹è¿›è¡Œäº†æ”¹è¿›ã€‚ç”±äºæ¸è¿›å¼åˆ†è¾¨ç‡ç¼©æ”¾å¯¼è‡´çš„è·¨æ—¶é—´æ­¥è¡¥ä¸é—´ä¿¡æ¯ä¸ä¸€è‡´ï¼Œä½¿å¾—å¼•å¯¼ä¿¡å·å®¹æ˜“åç¦»æ¡ä»¶ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿæ¨¡ç³Šä¸”ä¸å¿ å®çš„ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¿¡æ¯é”šå®šå¼•å¯¼(Information-Grounding Guidance, IGG)æœºåˆ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å°†å¼•å¯¼é”šå®šåœ¨è¯­ä¹‰é‡è¦çš„åŒºåŸŸã€‚IGGåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°å¢å¼ºä¿¡æ¯ä¸°å¯Œçš„è¡¥ä¸ï¼Œç¡®ä¿å¼•å¯¼ä¿¡å·ä¸ç”Ÿæˆå†…å®¹ä¿æŒç´§å¯†å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGGåœ¨ç±»åˆ«æ¡ä»¶å’Œæ–‡æœ¬ç”Ÿæˆå›¾åƒä»»åŠ¡ä¸­å‡èƒ½ç”Ÿæˆæ›´é”åˆ©ã€è¿è´¯ä¸”è¯­ä¹‰æ‰å®çš„å›¾åƒã€‚è¯¥ç ”ç©¶ä¸ºåŸºäºè‡ªå›å½’çš„å›¾åƒç”Ÿæˆæ–¹æ³•è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆçš„è´¨é‡ä¸ä¿çœŸåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 7 figures; added shared first authorship statement",
      "pdf_url": "https://arxiv.org/pdf/2509.23876v2",
      "published_date": "2025-09-28 13:33:49 UTC",
      "updated_date": "2025-09-30 23:00:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:03.573140+00:00"
    },
    {
      "arxiv_id": "2509.23874v1",
      "title": "Multi-Value-Product Retrieval-Augmented Generation for Industrial Product Attribute Value Identification",
      "title_zh": "é¢å‘å·¥ä¸šäº§å“å±æ€§å€¼è¯†åˆ«çš„å¤šå€¼äº§å“æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Huike Zou",
        "Haiyang Yang",
        "Yindu Su",
        "Liyu Chen",
        "Chengbao Lian",
        "Qingheng Zhang",
        "Shuguang Han",
        "Jufeng Chen"
      ],
      "abstract": "Identifying attribute values from product profiles is a key task for improving product search, recommendation, and business analytics on e-commerce platforms, which we called Product Attribute Value Identification (PAVI) . However, existing PAVI methods face critical challenges, such as cascading errors, inability to handle out-of-distribution (OOD) attribute values, and lack of generalization capability. To address these limitations, we introduce Multi-Value-Product Retrieval-Augmented Generation (MVP-RAG), combining the strengths of retrieval, generation, and classification paradigms. MVP-RAG defines PAVI as a retrieval-generation task, where the product title description serves as the query, and products and attribute values act as the corpus. It first retrieves similar products of the same category and candidate attribute values, and then generates the standardized attribute values. The key advantages of this work are: (1) the proposal of a multi-level retrieval scheme, with products and attribute values as distinct hierarchical levels in PAVI domain (2) attribute value generation of large language model to significantly alleviate the OOD problem and (3) its successful deployment in a real-world industrial environment. Extensive experimental results demonstrate that MVP-RAG performs better than the state-of-the-art baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå•†å¹³å°ä¸­äº§å“å±æ€§å€¼è¯†åˆ«(PAVI)é¢ä¸´çš„çº§è”é”™è¯¯ã€åˆ†å¸ƒå¤–(OOD)å±æ€§å€¼åŠæ³›åŒ–èƒ½åŠ›ä¸è¶³ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†MVP-RAGæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†PAVIå®šä¹‰ä¸ºä¸€ç§æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ä»»åŠ¡ï¼Œä»¥äº§å“æ ‡é¢˜å’Œæè¿°ä½œä¸ºæŸ¥è¯¢ï¼Œå¹¶åœ¨åŒ…å«ç›¸ä¼¼äº§å“ä¸å€™é€‰å±æ€§å€¼çš„å¤šçº§è¯­æ–™åº“ä¸­è¿›è¡Œæ£€ç´¢ã€‚é€šè¿‡å¼•å…¥å¤§è¯­è¨€æ¨¡å‹(LLM)çš„ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ ‡å‡†åŒ–çš„å±æ€§å€¼ï¼Œä»è€Œæ˜¾è‘—ç¼“è§£äº†OODé—®é¢˜å¹¶å¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚å®éªŒç»“æœå’Œåœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„éƒ¨ç½²è¡¨ç°è¯æ˜ï¼ŒMVP-RAGåœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„SOTAåŸºçº¿æ¨¡å‹ï¼Œä¸ºå·¥ä¸šçº§äº§å“åˆ†æå’Œå•†ä¸šæ™ºèƒ½æä¾›äº†æ›´ç¨³å¥çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23874v1",
      "published_date": "2025-09-28 13:29:20 UTC",
      "updated_date": "2025-09-28 13:29:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:09.069545+00:00"
    },
    {
      "arxiv_id": "2509.23871v1",
      "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
      "title_zh": "æˆä¹‹ä»¥è‰¯ï¼Œä¹ ä¹‹ä»¥å¼Šï¼šè¿ˆå‘è’¸é¦è§¦å‘å‹åé—¨æ”»å‡»",
      "authors": [
        "Yukun Chen",
        "Boheng Li",
        "Yu Yuan",
        "Leyi Qi",
        "Yiming Li",
        "Tianwei Zhang",
        "Zhan Qin",
        "Kui Ren"
      ],
      "abstract": "Knowledge distillation (KD) is a vital technique for deploying deep neural networks (DNNs) on resource-constrained devices by transferring knowledge from large teacher models to lightweight student models. While teacher models from third-party platforms may undergo security verification (\\eg, backdoor detection), we uncover a novel and critical threat: distillation-conditional backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into teacher models, which become activated in student models via the KD process, even with clean distillation datasets. While the direct extension of existing methods is ineffective for DCBA, we implement this attack by formulating it as a bilevel optimization problem and proposing a simple yet effective method (\\ie, SCAR). Specifically, the inner optimization simulates the KD process by optimizing a surrogate student model, while the outer optimization leverages outputs from this surrogate to optimize the teacher model for implanting the conditional backdoor. Our SCAR addresses this complex optimization utilizing an implicit differentiation algorithm with a pre-optimized trigger injection function. Extensive experiments across diverse datasets, model architectures, and KD techniques validate the effectiveness of our SCAR and its resistance against existing backdoor detection, highlighting a significant yet previously overlooked vulnerability in the KD process. Our code is available at https://github.com/WhitolfChen/SCAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ­ç¤ºäº†çŸ¥è¯†è’¸é¦(Knowledge Distillation, KD)ä¸­çš„ä¸€ç§æ–°å‹å®‰å…¨å¨èƒï¼šè’¸é¦æ¡ä»¶åé—¨æ”»å‡»(Distillation-conditional Backdoor Attacks, DCBA)ã€‚DCBAåœ¨æ•™å¸ˆæ¨¡å‹ä¸­æ¤å…¥ä¼‘çœ ä¸”ä¸å¯æ£€æµ‹çš„åé—¨ï¼Œè¿™äº›åé—¨å³ä¾¿åœ¨å¹²å‡€çš„è’¸é¦æ•°æ®é›†ä¸‹ï¼Œä¹Ÿä¼šåœ¨è’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹ä¸­è¢«æ¿€æ´»ã€‚ç ”ç©¶è€…å°†æ­¤æ”»å‡»è¡¨è¿°ä¸ºåŒå±‚ä¼˜åŒ–(bilevel optimization)é—®é¢˜ï¼Œå¹¶æå‡ºäº†SCARæ–¹æ³•ã€‚SCARé€šè¿‡å†…éƒ¨ä¼˜åŒ–æ¨¡æ‹ŸKDè¿‡ç¨‹ç”Ÿæˆæ›¿ä»£å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å…¶è¾“å‡ºåœ¨å¤–éƒ¨ä¼˜åŒ–ä¸­è°ƒæ•´æ•™å¸ˆæ¨¡å‹ä»¥æ¤å…¥æ¡ä»¶åé—¨ï¼ŒåŒæ—¶ç»“åˆéšå¼å¾®åˆ†(implicit differentiation)ç®—æ³•è§£å†³ä¼˜åŒ–éš¾é¢˜ã€‚å¹¿æ³›çš„å®éªŒéªŒè¯äº†SCARåœ¨ä¸åŒæ•°æ®é›†ã€æ¶æ„å’ŒKDæŠ€æœ¯ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå¯¹ç°æœ‰åé—¨æ£€æµ‹æ–¹æ³•çš„æ˜¾è‘—æŠµæŠ—åŠ›ã€‚è¯¥å‘ç°å‡¸æ˜¾äº†çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ä¸€ä¸ªæ­¤å‰è¢«å¿½è§†çš„é‡å¤§å®‰å…¨æ¼æ´ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "The first three authors contributed equally to this work. To appear in NeurIPS 2025. 35 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.23871v1",
      "published_date": "2025-09-28 13:24:46 UTC",
      "updated_date": "2025-09-28 13:24:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:39.195080+00:00"
    },
    {
      "arxiv_id": "2509.23870v4",
      "title": "Gradient Coupling: The Hidden Barrier to Generalization in Agentic Reinforcement Learning",
      "title_zh": "æ¢¯åº¦è€¦åˆï¼šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ³›åŒ–èƒ½åŠ›çš„éšæ€§éšœç¢",
      "authors": [
        "Jingyu Liu",
        "Xiaopeng Wu",
        "Jingquan Peng",
        "Kehan Chen",
        "Chuan Yu",
        "Lizhong Ding",
        "Yong Liu"
      ],
      "abstract": "Reinforcement learning (RL) is a dominant paradigm for training autonomous agents, yet these agents often exhibit poor generalization, failing to adapt to scenarios not seen during training. In this work, we identify a fundamental cause of this brittleness, a phenomenon which we term \"gradient coupling.\" We hypothesize that in complex agentic tasks, the high similarity between distinct states leads to destructive interference between gradients. Specifically, a gradient update that reinforces an optimal action in one state can inadvertently increase the likelihood of a suboptimal action in a similar, yet different, state. To solve this, we propose a novel objective where the actor is trained to simultaneously function as a classifier that separates good and bad actions. This auxiliary pressure compels the model to learn disentangled embeddings for positive and negative actions, which mitigates negative gradient interference and improve the generalization performance. Extensive experiments demonstrate the effectiveness of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ™ºèƒ½ä½“ Reinforcement Learning ä¸­æ³›åŒ–èƒ½åŠ›å·®çš„é—®é¢˜ï¼Œå¹¶å°†è¿™ä¸€ç°è±¡çš„æ ¹æœ¬åŸå› å½’ç»“ä¸ºä¸€ç§è¢«ç§°ä¸º Gradient Coupling çš„æ–°æœºåˆ¶ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨å¤æ‚çš„æ™ºèƒ½ä½“ä»»åŠ¡ä¸­ï¼Œä¸åŒçŠ¶æ€ä¹‹é—´çš„é«˜åº¦ç›¸ä¼¼æ€§ä¼šå¯¼è‡´æ¢¯åº¦ä¹‹é—´çš„ç ´åæ€§å¹²æ‰°ï¼Œå³é’ˆå¯¹æŸä¸€çŠ¶æ€çš„ä¼˜åŒ–æ›´æ–°å¯èƒ½ä¼šæ— æ„ä¸­å¢åŠ åœ¨ç›¸ä¼¼ä½†ä¸åŒçŠ¶æ€ä¸‹é‡‡å–æ¬¡ä¼˜è¡ŒåŠ¨çš„å¯èƒ½æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç›®æ ‡å‡½æ•°ï¼Œä½¿ Actor åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶å……å½“åˆ†ç±»å™¨ï¼Œç”¨äºåŒºåˆ†æ­£é¢å’Œè´Ÿé¢è¡ŒåŠ¨ã€‚è¿™ç§è¾…åŠ©å‹åŠ›è¿«ä½¿æ¨¡å‹å­¦ä¹ æ­£å‘å’Œè´Ÿå‘è¡ŒåŠ¨çš„ Disentangled Embeddingsï¼Œä»è€Œæœ‰æ•ˆå‡è½»è´Ÿé¢æ¢¯åº¦å¹²æ‰°å¹¶æ”¹å–„æ³›åŒ–è¡¨ç°ã€‚å¤§é‡å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ™ºèƒ½ä½“åœ¨æœªè§åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23870v4",
      "published_date": "2025-09-28 13:24:38 UTC",
      "updated_date": "2026-01-15 01:18:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:23.882157+00:00"
    },
    {
      "arxiv_id": "2509.23866v1",
      "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation",
      "title_zh": "åŸºäºè§£è€¦è®­ç»ƒä¸è‡ªé€‚åº”æ•°æ®æ•´ç†çš„é«˜æ•ˆ GUI æ™ºèƒ½ä½“å¤šè½®å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Pengxiang Li",
        "Zechen Hu",
        "Zirui Shang",
        "Jingrong Wu",
        "Yang Liu",
        "Hui Liu",
        "Zhi Gao",
        "Chenrui Shi",
        "Bofei Zhang",
        "Zihao Zhang",
        "Xiaochuan Shi",
        "Zedong YU",
        "Yuwei Wu",
        "Xinxiao Wu",
        "Yunde Jia",
        "Liuyu Xiang",
        "Zhaofeng He",
        "Qing Li"
      ],
      "abstract": "Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via computer-use-agents.github.io/dart-gui, which we believe is a timely contribution to the open-source community of agentic RL training.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DARTï¼Œä¸€ä¸ªé’ˆå¯¹GUIæ™ºèƒ½ä½“çš„è§£è€¦ä»£ç†å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶(Decoupled Agentic RL Training framework)ï¼Œæ—¨åœ¨è§£å†³åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹(VLM)çš„æ™ºèƒ½ä½“åœ¨å¤šè½®å¼ºåŒ–å­¦ä¹ (RL)ä¸­é¢ä¸´çš„äº¤äº’ç¼“æ…¢å’Œé«˜è´¨é‡æ•°æ®åŒ®ä¹ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†è®­ç»ƒç³»ç»Ÿè§£è€¦ä¸ºç¯å¢ƒé›†ç¾¤ã€å›æ»šæœåŠ¡ã€æ•°æ®ç®¡ç†å™¨å’Œè®­ç»ƒå™¨å››ä¸ªå¼‚æ­¥æ¨¡å—ï¼Œé€šè¿‡éé˜»å¡é€šä¿¡å’Œå¼‚æ­¥è®­ç»ƒæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ•ˆç‡ï¼Œä½¿è®­ç»ƒååé‡å’Œç¯å¢ƒåˆ©ç”¨ç‡åˆ†åˆ«æé«˜äº†1.9å€å’Œ5.5å€ã€‚ä¸ºäº†ä»æµ·é‡æ ·æœ¬ä¸­é«˜æ•ˆå­¦ä¹ ï¼Œç ”ç©¶å¼•å…¥äº†è‡ªé€‚åº”æ•°æ®ç­–åˆ’(adaptive data curation)æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸ºé«˜éš¾åº¦ä»»åŠ¡é¢„æ”¶é›†æˆåŠŸè½¨è¿¹ã€åŠ¨æ€è°ƒæ•´å›æ»šæ•°é‡ä»¥åŠé’ˆå¯¹é«˜ç†µæ­¥éª¤(high-entropy steps)è¿›è¡Œé€‰æ‹©æ€§è®­ç»ƒã€‚æ­¤å¤–ï¼Œæ¡†æ¶é€šè¿‡æˆªæ–­é‡è¦æ€§é‡‡æ ·(truncated importance sampling)è§£å†³äº†ç­–ç•¥ä¸åŒ¹é…é—®é¢˜ï¼Œç¡®ä¿äº†å­¦ä¹ è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚åœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDART-GUI-7Bå®ç°äº†42.13%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œæ¯”åŸºçº¿æ¨¡å‹æ€§èƒ½æå‡äº†14.61%ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰å¼€æºSOTAæ¨¡å‹ã€‚è¯¥ç ”ç©¶åŠå…¶å¼€æºèµ„æºä¸ºæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢†åŸŸåšå‡ºäº†é‡è¦è´¡çŒ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23866v1",
      "published_date": "2025-09-28 13:19:20 UTC",
      "updated_date": "2025-09-28 13:19:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:35.687113+00:00"
    },
    {
      "arxiv_id": "2509.23864v1",
      "title": "AgentGuard: Runtime Verification of AI Agents",
      "title_zh": "AgentGuardï¼šAI æ™ºèƒ½ä½“è¿è¡Œæ—¶éªŒè¯",
      "authors": [
        "Roham Koohestani"
      ],
      "abstract": "The rapid evolution to autonomous, agentic AI systems introduces significant risks due to their inherent unpredictability and emergent behaviors; this also renders traditional verification methods inadequate and necessitates a shift towards probabilistic guarantees where the question is no longer if a system will fail, but the probability of its failure within given constraints. This paper presents AgentGuard, a framework for runtime verification of Agentic AI systems that provides continuous, quantitative assurance through a new paradigm called Dynamic Probabilistic Assurance. AgentGuard operates as an inspection layer that observes an agent's raw I/O and abstracts it into formal events corresponding to transitions in a state model. It then uses online learning to dynamically build and update a Markov Decision Process (MDP) that formally models the agent's emergent behavior. Using probabilistic model checking, the framework then verifies quantitative properties in real-time.",
      "tldr_zh": "è¯¥è®ºæ–‡æå‡ºäº†AgentGuardï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè‡ªä¸»ä»£ç†å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿ(Agentic AI systems)è¿è¡Œæ—¶éªŒè¯(runtime verification)çš„æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹æ­¤ç±»ç³»ç»Ÿå› ä¸å¯é¢„æµ‹æ€§å’Œçªç°è¡Œä¸º(emergent behaviors)å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åŠ¨æ€æ¦‚ç‡ä¿è¯(Dynamic Probabilistic Assurance)èŒƒå¼ï¼Œå°†éªŒè¯é‡ç‚¹ä»ä¼ ç»Ÿçš„ç¡®å®šæ€§éªŒè¯è½¬å‘è¯„ä¼°ç³»ç»Ÿåœ¨ç‰¹å®šçº¦æŸä¸‹å¤±æ•ˆçš„æ¦‚ç‡ã€‚AgentGuardä½œä¸ºä¸€ä¸ªæ£€æŸ¥å±‚è¿è¡Œï¼Œé€šè¿‡æå–æ™ºèƒ½ä½“çš„åŸå§‹I/Oå¹¶å°†å…¶è½¬åŒ–ä¸ºçŠ¶æ€æ¨¡å‹ä¸­çš„æ­£å¼äº‹ä»¶ã€‚å®ƒé‡‡ç”¨åœ¨çº¿å­¦ä¹ (online learning)æŠ€æœ¯åŠ¨æ€æ„å»ºå¹¶æŒç»­æ›´æ–°é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)ï¼Œä»è€Œä¸ºæ™ºèƒ½ä½“çš„è¡Œä¸ºå»ºç«‹æ­£å¼æ¨¡å‹ã€‚é€šè¿‡æ¦‚ç‡æ¨¡å‹æ£€æµ‹(probabilistic model checking)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å®æ—¶ç¯å¢ƒä¸‹å¯¹ç³»ç»Ÿçš„å®šé‡å±æ€§è¿›è¡Œä¸¥è°¨éªŒè¯ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in the proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025, in the 1st international workshop on Agentic Software Engineering (AgenticSE)",
      "pdf_url": "https://arxiv.org/pdf/2509.23864v1",
      "published_date": "2025-09-28 13:08:50 UTC",
      "updated_date": "2025-09-28 13:08:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:33.392803+00:00"
    },
    {
      "arxiv_id": "2510.02362v1",
      "title": "A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History",
      "title_zh": "åŸºäº Romanian å†å²çš„å¤§è¯­è¨€æ¨¡å‹åè§è·¨è¯­è¨€åˆ†æ",
      "authors": [
        "Matei-Iulian Cocu",
        "RÄƒzvan-Cosmin Cristia",
        "Adrian Marius Dumitran"
      ],
      "abstract": "In this case study, we select a set of controversial Romanian historical questions and ask multiple Large Language Models to answer them across languages and contexts, in order to assess their biases. Besides being a study mainly performed for educational purposes, the motivation also lies in the recognition that history is often presented through altered perspectives, primarily influenced by the culture and ideals of a state, even through large language models. Since they are often trained on certain data sets that may present certain ambiguities, the lack of neutrality is subsequently instilled in users. The research process was carried out in three stages, to confirm the idea that the type of response expected can influence, to a certain extent, the response itself; after providing an affirmative answer to some given question, an LLM could shift its way of thinking after being asked the same question again, but being told to respond with a numerical value of a scale. Results show that binary response stability is relatively high but far from perfect and varies by language. Models often flip stance across languages or between formats; numeric ratings frequently diverge from the initial binary choice, and the most consistent models are not always those judged most accurate or neutral. Our research brings to light the predisposition of models to such inconsistencies, within a specific contextualization of the language for the question asked.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡é’ˆå¯¹ç½—é©¬å°¼äºšå†å²ä¸­å…·æœ‰äº‰è®®æ€§çš„é—®é¢˜ï¼Œå¯¹å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) åœ¨ä¸åŒè¯­è¨€å’Œè¯­å¢ƒä¸‹çš„åè§è¿›è¡Œäº†è·¨è¯­è¨€åˆ†æã€‚ç ”ç©¶èƒŒæ™¯åœ¨äºå†å²å™äº‹å¾€å¾€å—åˆ°æ–‡åŒ–å’Œå›½å®¶æ„è¯†å½¢æ€çš„å½±å“ï¼Œè€Œè®­ç»ƒæ•°æ®çš„æ­§ä¹‰æ€§å¯¼è‡´ LLMs åœ¨è¾“å‡ºä¸­è¡¨ç°å‡ºä¸­ç«‹æ€§ç¼ºå¤±ã€‚å®éªŒè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼Œé€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨äºŒè¿›åˆ¶å›ç­”ã€è‚¯å®šæ€§å¼•å¯¼åŠæ•°å€¼é‡è¡¨è¯„åˆ†ä¸­çš„è¡¨ç°ï¼Œè¯„ä¼°äº†é¢„æœŸå›ç­”ç±»å‹å¯¹æ¨¡å‹æ€è€ƒæ–¹å¼çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨äºŒè¿›åˆ¶å›ç­”ä¸Šçš„ç¨³å®šæ€§éšè¯­è¨€å˜åŒ–ä¸”å¹¶ä¸ç†æƒ³ï¼Œå…¶ç«‹åœºç»å¸¸åœ¨ä¸åŒè¯­è¨€æˆ–æ ¼å¼ä¹‹é—´å‘ç”Ÿç¿»è½¬ã€‚ç ”ç©¶å‘ç°ï¼Œæ•°å€¼è¯„åˆ†å¾€å¾€ä¸åˆå§‹çš„äºŒè¿›åˆ¶é€‰æ‹©ç›¸èƒŒç¦»ï¼Œä¸”è¡¨ç°æœ€ä¸€è‡´çš„æ¨¡å‹åœ¨å‡†ç¡®æ€§æˆ–ä¸­ç«‹æ€§ä¸Šæœªå¿…å ä¼˜ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†æ¨¡å‹åœ¨ç‰¹å®šè¯­è¨€è¯­å¢ƒä¸‹äº§ç”Ÿä¸ä¸€è‡´æ€§çš„å€¾å‘ï¼Œä¸ºç†è§£å¤§æ¨¡å‹çš„æ–‡åŒ–åè§æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2510.02362v1",
      "published_date": "2025-09-28 13:03:09 UTC",
      "updated_date": "2025-09-28 13:03:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:44.473554+00:00"
    },
    {
      "arxiv_id": "2509.23860v1",
      "title": "GSID: Generative Semantic Indexing for E-Commerce Product Understanding",
      "title_zh": "GSIDï¼šé¢å‘ç”µå­å•†åŠ¡å•†å“ç†è§£çš„ç”Ÿæˆå¼è¯­ä¹‰ç´¢å¼•",
      "authors": [
        "Haiyang Yang",
        "Qinye Xie",
        "Qingheng Zhang",
        "Liyu Chen",
        "Huike Zou",
        "Chengbao Lian",
        "Shuguang Han",
        "Fei Huang",
        "Jufeng Chen",
        "Bo Zheng"
      ],
      "abstract": "Structured representation of product information is a major bottleneck for the efficiency of e-commerce platforms, especially in second-hand ecommerce platforms. Currently, most product information are organized based on manually curated product categories and attributes, which often fail to adequately cover long-tail products and do not align well with buyer preference. To address these problems, we propose \\textbf{G}enerative \\textbf{S}emantic \\textbf{I}n\\textbf{D}exings (GSID), a data-driven approach to generate product structured representations. GSID consists of two key components: (1) Pre-training on unstructured product metadata to learn in-domain semantic embeddings, and (2) Generating more effective semantic codes tailored for downstream product-centric applications. Extensive experiments are conducted to validate the effectiveness of GSID, and it has been successfully deployed on the real-world e-commerce platform, achieving promising results on product understanding and other downstream tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GSID (Generative Semantic Indexing)ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç”µå­å•†åŠ¡é¢†åŸŸäº§å“ä¿¡æ¯ç»“æ„åŒ–è¡¨ç¤ºç“¶é¢ˆçš„æ•°æ®é©±åŠ¨æ–¹æ³•ã€‚é’ˆå¯¹ä¼ ç»Ÿæ‰‹åŠ¨åˆ†ç±»éš¾ä»¥è¦†ç›–é•¿å°¾äº§å“ä¸”ä¸ä¹°å®¶åå¥½è„±èŠ‚çš„é—®é¢˜ï¼ŒGSIDé€šè¿‡åœ¨éç»“æ„åŒ–å…ƒæ•°æ®ä¸Šè¿›è¡ŒPre-trainingæ¥å­¦ä¹ é¢†åŸŸå†…çš„è¯­ä¹‰Embeddingsï¼Œå¹¶ç”Ÿæˆæ›´å…·ä»£è¡¨æ€§çš„è¯­ä¹‰ä»£ç ä»¥æ”¯æŒä¸‹æ¸¸åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGSIDåœ¨äº§å“ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å·²åœ¨å®é™…ç”µå•†å¹³å°æˆåŠŸéƒ¨ç½²ï¼Œåœ¨äº§å“ç†è§£å’Œç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¯¥æ–¹æ³•æœ‰æ•ˆæ”¹å–„äº†é•¿å°¾äº§å“çš„è¯­ä¹‰è¦†ç›–ï¼Œä¸ºæå‡ç”µå•†å¹³å°çš„ä¿¡æ¯å¤„ç†æ•ˆç‡æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23860v1",
      "published_date": "2025-09-28 12:58:05 UTC",
      "updated_date": "2025-09-28 12:58:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:45.362977+00:00"
    },
    {
      "arxiv_id": "2509.23846v2",
      "title": "Adversarial Diffusion for Robust Reinforcement Learning",
      "title_zh": "é¢å‘é²æ£’å¼ºåŒ–å­¦ä¹ çš„å¯¹æŠ—æ‰©æ•£",
      "authors": [
        "Daniele Foffano",
        "Alessio Russo",
        "Alexandre Proutiere"
      ],
      "abstract": "Robustness to modeling errors and uncertainties remains a central challenge in reinforcement learning (RL). In this work, we address this challenge by leveraging diffusion models to train robust RL policies. Diffusion models have recently gained popularity in model-based RL due to their ability to generate full trajectories \"all at once\", mitigating the compounding errors typical of step-by-step transition models. Moreover, they can be conditioned to sample from specific distributions, making them highly flexible. We leverage conditional sampling to learn policies that are robust to uncertainty in environment dynamics. Building on the established connection between Conditional Value at Risk (CVaR) optimization and robust RL, we introduce Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides the diffusion process to generate worst-case trajectories during training, effectively optimizing the CVaR of the cumulative return. Empirical results across standard benchmarks show that AD-RRL achieves superior robustness and performance compared to existing robust RL methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL)ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹(Diffusion models)æ¥è®­ç»ƒé²æ£’çš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ç­–ç•¥ï¼Œä»¥è§£å†³å»ºæ¨¡è¯¯å·®å’Œä¸ç¡®å®šæ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æ‰©æ•£æ¨¡å‹é€šè¿‡ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´è½¨è¿¹çš„èƒ½åŠ›ï¼Œæœ‰æ•ˆå‡è½»äº†ä¼ ç»Ÿæ¨¡å‹ä¸­å¸¸è§çš„å¤åˆè¯¯å·®ï¼Œå¹¶å±•ç°å‡ºæé«˜çš„æ¡ä»¶é‡‡æ ·çµæ´»æ€§ã€‚AD-RRLåˆ©ç”¨æ¡ä»¶é£é™©ä»·å€¼(Conditional Value at Risk, CVaR)ä¼˜åŒ–ä¸é²æ£’å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„è”ç³»ï¼Œé€šè¿‡å¼•å¯¼æ‰©æ•£è¿‡ç¨‹åœ¨è®­ç»ƒä¸­ç”Ÿæˆâ€œæœ€åæƒ…å†µâ€çš„è½¨è¿¹ã€‚è¿™ç§æ–¹æ³•å®ç°äº†å¯¹ç´¯ç§¯æ”¶ç›ŠCVaRçš„æœ‰æ•ˆä¼˜åŒ–ï¼Œç¡®ä¿ç­–ç•¥åœ¨å¤šå˜çš„ç¯å¢ƒåŠ¨æ€ä¸­ä¿æŒç¨³å¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAD-RRLåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…¶é²æ£’æ€§å’Œæ€§èƒ½å‡ä¼˜äºç°æœ‰çš„é²æ£’å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23846v2",
      "published_date": "2025-09-28 12:34:35 UTC",
      "updated_date": "2025-12-02 04:44:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:55.191031+00:00"
    },
    {
      "arxiv_id": "2510.05115v1",
      "title": "Optimization Modeling via Semantic Anchored Alignment",
      "title_zh": "åŸºäºè¯­ä¹‰é”šå®šå¯¹é½çš„ä¼˜åŒ–å»ºæ¨¡",
      "authors": [
        "Yansen Zhang",
        "Qingcan Kang",
        "Yujie Chen",
        "Yufei Wang",
        "Xiongwei Han",
        "Tao Zhong",
        "Mingxuan Yuan",
        "Chen Ma"
      ],
      "abstract": "Large language models (LLMs) have opened new paradigms in optimization modeling by enabling the generation of executable solver code from natural language descriptions. Despite this promise, existing approaches typically remain solver-driven: they rely on single-pass forward generation and apply limited post-hoc fixes based on solver error messages, leaving undetected semantic errors that silently produce syntactically correct but logically flawed models. To address this challenge, we propose SAC-Opt, a backward-guided correction framework that grounds optimization modeling in problem semantics rather than solver feedback. At each step, SAC-Opt aligns the original semantic anchors with those reconstructed from the generated code and selectively corrects only the mismatched components, driving convergence toward a semantically faithful model. This anchor-driven correction enables fine-grained refinement of constraint and objective logic, enhancing both fidelity and robustness without requiring additional training or supervision. Empirical results on seven public datasets demonstrate that SAC-Opt improves average modeling accuracy by 7.8\\%, with gains of up to 21.9\\% on the ComplexLP dataset. These findings highlight the importance of semantic-anchored correction in LLM-based optimization workflows to ensure faithful translation from problem intent to solver-executable code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAC-Optï¼Œä¸€ç§åŸºäºè¯­ä¹‰é”šç‚¹å¯¹é½(Semantic Anchored Alignment)çš„é€†å‘å¼•å¯¼ä¿®æ­£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å°†è‡ªç„¶è¯­è¨€è½¬åŒ–ä¸ºæ±‚è§£å™¨ä»£ç æ—¶äº§ç”Ÿçš„éšè”½è¯­ä¹‰é”™è¯¯ã€‚ä¸ä¼ ç»Ÿä¾èµ–æ±‚è§£å™¨æŠ¥é”™ä¿¡æ¯çš„åéªŒä¿®æ­£æ–¹æ³•ä¸åŒï¼ŒSAC-Opté€šè¿‡å°†åŸå§‹é—®é¢˜çš„è¯­ä¹‰é”šç‚¹ä¸ä»ç”Ÿæˆçš„ä»£ç ä¸­é‡æ„çš„é”šç‚¹è¿›è¡Œå¯¹é½ï¼Œä»…é’ˆå¯¹ä¸åŒ¹é…çš„é€»è¾‘ç»„ä»¶è¿›è¡Œç²¾å‡†ä¿®æ­£ã€‚è¿™ç§æœºåˆ¶åœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¯¹çº¦æŸæ¡ä»¶å’Œç›®æ ‡å‡½æ•°é€»è¾‘çš„ç²¾ç»†åŒ–ç²¾ç‚¼ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¿ å®åº¦ä¸é²æ£’æ€§ã€‚åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAC-Optå°†å¹³å‡å»ºæ¨¡å‡†ç¡®ç‡æé«˜äº†7.8%ï¼Œå…¶ä¸­åœ¨ComplexLPæ•°æ®é›†ä¸Šçš„æå‡å¹…åº¦é«˜è¾¾21.9%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†è¯­ä¹‰é”šå®šä¿®æ­£å¯¹äºä¼˜åŒ–å»ºæ¨¡å·¥ä½œæµçš„é‡è¦æ€§ï¼Œç¡®ä¿äº†ä»é—®é¢˜æ„å›¾åˆ°solver-executable codeçš„å¯é è½¬åŒ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.PL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.05115v1",
      "published_date": "2025-09-28 12:25:31 UTC",
      "updated_date": "2025-09-28 12:25:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:02.492478+00:00"
    },
    {
      "arxiv_id": "2509.23836v1",
      "title": "Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules",
      "title_zh": "Mix-Ecomï¼šé¢å‘å¤æ‚é¢†åŸŸè§„åˆ™çš„æ··åˆå‹ç”µå•†å¯¹è¯",
      "authors": [
        "Chenyu Zhou",
        "Xiaoming Shi",
        "Hui Qiu",
        "Xiawu Zheng",
        "Haitao Leng",
        "Yankai Jiang",
        "Shaoguo Liu",
        "Tingting Gao",
        "Rongrong Ji"
      ],
      "abstract": "E-commerce agents contribute greatly to helping users complete their e-commerce needs. To promote further research and application of e-commerce agents, benchmarking frameworks are introduced for evaluating LLM agents in the e-commerce domain. Despite the progress, current benchmarks lack evaluating agents' capability to handle mixed-type e-commerce dialogue and complex domain rules. To address the issue, this work first introduces a novel corpus, termed Mix-ECom, which is constructed based on real-world customer-service dialogues with post-processing to remove user privacy and add CoT process. Specifically, Mix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce dialogue, covering four dialogue types (QA, recommendation, task-oriented dialogue, and chit-chat), three e-commerce task types (pre-sales, logistics, after-sales), and 82 e-commerce rules. Furthermore, this work build baselines on Mix-Ecom and propose a dynamic framework to further improve the performance. Results show that current e-commerce agents lack sufficient capabilities to handle e-commerce dialogues, due to the hallucination cased by complex domain rules. The dataset will be publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåä¸º Mix-ECom çš„æ–°å‹è¯­æ–™åº“ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç”µå­å•†åŠ¡æ™ºèƒ½ä½“åœ¨å¤„ç†æ··åˆç±»å‹å¯¹è¯å’Œå¤æ‚é¢†åŸŸè§„åˆ™æ–¹é¢è¯„ä¼°ä¸è¶³çš„é—®é¢˜ã€‚Mix-ECom åŒ…å« 4,799 ä¸ªåŸºäºçœŸå®å®¢æœå¯¹è¯å¹¶åŠ å…¥ Chain-of-Thought (CoT) è¿‡ç¨‹æ„å»ºçš„æ ·æœ¬ï¼Œæ¶µç›–äº†é—®ç­” (QA)ã€æ¨èã€ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ (Task-Oriented Dialogue) å’Œé—²èŠ (Chit-chat) å››ç§å¯¹è¯ç±»å‹ã€‚è¯¥æ•°æ®é›†è·¨è¶Šäº†å”®å‰ã€ç‰©æµå’Œå”®åä¸‰å¤§ä»»åŠ¡ç±»åˆ«ï¼Œå¹¶é›†æˆäº† 82 é¡¹å¤æ‚çš„ç”µå•†é¢†åŸŸè§„åˆ™ã€‚ç ”ç©¶è€…è¿˜åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºäº†åŸºçº¿æ¨¡å‹å¹¶æå‡ºäº†ä¸€ä¸ªåŠ¨æ€æ¡†æ¶ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç”±äºå¤æ‚é¢†åŸŸè§„åˆ™å¼•å‘çš„å¹»è§‰ (Hallucination) é—®é¢˜ï¼Œç°æœ‰çš„ç”µå•†æ™ºèƒ½ä½“åœ¨å¤„ç†æ··åˆå‹å¯¹è¯æ—¶ä»å­˜åœ¨æ˜æ˜¾çš„èƒ½åŠ›çŸ­ç¼ºã€‚è¯¥æ•°æ®é›†å°†å‘å…¬ä¼—å¼€æ”¾ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æä¾›æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23836v1",
      "published_date": "2025-09-28 12:19:27 UTC",
      "updated_date": "2025-09-28 12:19:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:01.690120+00:00"
    },
    {
      "arxiv_id": "2509.23835v2",
      "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing",
      "title_zh": "HFuzzerï¼šåŸºäºçŸ­è¯­æ¨¡ç³Šæµ‹è¯•çš„å¤§è¯­è¨€æ¨¡å‹è½¯ä»¶åŒ…å¹»è§‰æµ‹è¯•",
      "authors": [
        "Yukai Zhao",
        "Menghan Wu",
        "Xing Hu",
        "Xin Xia"
      ],
      "abstract": "Large Language Models (LLMs) are widely used for code generation, but they face critical security risks when applied to practical production due to package hallucinations, in which LLMs recommend non-existent packages. These hallucinations can be exploited in software supply chain attacks, where malicious attackers exploit them to register harmful packages. It is critical to test LLMs for package hallucinations to mitigate package hallucinations and defend against potential attacks. Although researchers have proposed testing frameworks for fact-conflicting hallucinations in natural language generation, there is a lack of research on package hallucinations. To fill this gap, we propose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for package hallucinations. HFUZZER adopts fuzzing technology and guides the model to infer a wider range of reasonable information based on phrases, thereby generating enough and diverse coding tasks. Furthermore, HFUZZER extracts phrases from package information or coding tasks to ensure the relevance of phrases and code, thereby improving the relevance of generated tasks and code. We evaluate HFUZZER on multiple LLMs and find that it triggers package hallucinations across all selected models. Compared to the mutational fuzzing framework, HFUZZER identifies 2.60x more unique hallucinated packages and generates more diverse tasks. Additionally, when testing the model GPT-4o, HFUZZER finds 46 unique hallucinated packages. Further analysis reveals that for GPT-4o, LLMs exhibit package hallucinations not only during code generation but also when assisting with environment configuration.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç”Ÿæˆä¸­æ¨èä¸å­˜åœ¨è½¯ä»¶åŒ…çš„â€œåŒ…å¹»è§‰â€ï¼ˆPackage Hallucinationsï¼‰å®‰å…¨é£é™©ï¼Œæå‡ºäº†é¦–ä¸ªåŸºäºçŸ­è¯­çš„æ¨¡ç³Šæµ‹è¯•ï¼ˆPhrase-based Fuzzingï¼‰æ¡†æ¶HFUZZERã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯å¼•å¯¼æ¨¡å‹æ ¹æ®çŸ­è¯­æ¨æ–­å¹¿æ³›ä¿¡æ¯ï¼Œå¹¶ä»è½¯ä»¶åŒ…ä¿¡æ¯æˆ–ç¼–ç¨‹ä»»åŠ¡ä¸­æå–çŸ­è¯­ï¼Œä»¥æå‡ç”Ÿæˆä»»åŠ¡çš„å¤šæ ·æ€§ä¸ç›¸å…³æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒHFUZZERåœ¨å¤šä¸ªä¸»æµLLMsä¸Šå‡æˆåŠŸè§¦å‘äº†åŒ…å¹»è§‰ï¼Œå…¶è¯†åˆ«å‡ºçš„å”¯ä¸€å¹»è§‰åŒ…æ•°é‡æ˜¯ä¼ ç»Ÿçªå˜æ¨¡ç³Šæµ‹è¯•æ¡†æ¶çš„2.6å€ã€‚æ­¤å¤–ï¼Œåœ¨å¯¹GPT-4oçš„æµ‹è¯•ä¸­ï¼ŒHFUZZERä¸ä»…åœ¨ä»£ç ç”Ÿæˆé˜¶æ®µå‘ç°äº†46ä¸ªå”¯ä¸€å¹»è§‰åŒ…ï¼Œè¿˜æ­ç¤ºäº†LLMsåœ¨è¾…åŠ©ç¯å¢ƒé…ç½®æ—¶åŒæ ·å­˜åœ¨åŒ…å¹»è§‰é—®é¢˜ï¼Œä¸ºç¼“è§£è½¯ä»¶ä¾›åº”é“¾æ”»å‡»æä¾›äº†é‡è¦æµ‹è¯•æ‰‹æ®µã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ASE25",
      "pdf_url": "https://arxiv.org/pdf/2509.23835v2",
      "published_date": "2025-09-28 12:16:43 UTC",
      "updated_date": "2025-10-04 05:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:26:59.686580+00:00"
    },
    {
      "arxiv_id": "2509.23822v1",
      "title": "Space Group Conditional Flow Matching",
      "title_zh": "ç©ºé—´ç¾¤æ¡ä»¶æµåŒ¹é…",
      "authors": [
        "Omri Puny",
        "Yaron Lipman",
        "Benjamin Kurt Miller"
      ],
      "abstract": "Inorganic crystals are periodic, highly-symmetric arrangements of atoms in three-dimensional space. Their structures are constrained by the symmetry operations of a crystallographic \\emph{space group} and restricted to lie in specific affine subspaces known as \\emph{Wyckoff positions}. The frequency an atom appears in the crystal and its rough positioning are determined by its Wyckoff position. Most generative models that predict atomic coordinates overlook these symmetry constraints, leading to unrealistically high populations of proposed crystals exhibiting limited symmetry. We introduce Space Group Conditional Flow Matching, a novel generative framework that samples significantly closer to the target population of highly-symmetric, stable crystals. We achieve this by conditioning the entire generation process on a given space group and set of Wyckoff positions; specifically, we define a conditionally symmetric noise base distribution and a group-conditioned, equivariant, parametric vector field that restricts the motion of atoms to their initial Wyckoff position. Our form of group-conditioned equivariance is achieved using an efficient reformulation of \\emph{group averaging} tailored for symmetric crystals. Importantly, it reduces the computational overhead of symmetrization to a negligible level. We achieve state of the art results on crystal structure prediction and de novo generation benchmarks. We also perform relevant ablations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Space Group Conditional Flow Matchingï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ— æœºæ™¶ä½“ç»“æ„ç”Ÿæˆçš„æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹å› å¿½è§†æ™¶ä½“Space Groupå’ŒWyckoff positionså¯¹ç§°æ€§çº¦æŸè€Œå¯¼è‡´ç”Ÿæˆç»“æœä¸çœŸå®çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å®šä¹‰æ¡ä»¶å¯¹ç§°çš„å™ªå£°åŸºåˆ†å¸ƒï¼Œå¹¶å°†ç”Ÿæˆè¿‡ç¨‹é™åˆ¶åœ¨ç‰¹å®šçš„Space Groupå’ŒWyckoff positionsé›†åˆä¸Šï¼Œç¡®ä¿åŸå­è¿åŠ¨ç¬¦åˆå¯¹ç§°æ€§è¦æ±‚ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç¾¤æ¡ä»¶ç­‰å˜(Group-conditioned, equivariant)çš„å‚æ•°åŒ–å‘é‡åœºï¼Œå¹¶ç»“åˆé«˜æ•ˆçš„Group averagingé‡æ„æŠ€æœ¯ï¼Œåœ¨å‡ ä¹ä¸å¢åŠ è®¡ç®—å¼€é”€çš„å‰æä¸‹å®ç°äº†ç²¾ç¡®çš„å¯¹ç§°åŒ–å¤„ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ™¶ä½“ç»“æ„é¢„æµ‹(Crystal structure prediction)å’Œä»å¤´ç”Ÿæˆ(De novo generation)åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†State of the artæ°´å¹³ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ¡†æ¶é‡‡æ ·å‡ºçš„æ™¶ä½“ç»“æ„åœ¨å¯¹ç§°æ€§å’Œç¨³å®šæ€§ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œä¸ºé«˜æ•ˆæ¢ç´¢é«˜å¯¹ç§°æ€§ç¨³å®šæ™¶ä½“ç©ºé—´æä¾›äº†å¼ºæœ‰åŠ›çš„å·¥å…·ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23822v1",
      "published_date": "2025-09-28 11:51:29 UTC",
      "updated_date": "2025-09-28 11:51:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:30.795961+00:00"
    },
    {
      "arxiv_id": "2509.23815v1",
      "title": "A Multi-Camera Vision-Based Approach for Fine-Grained Assembly Quality Control",
      "title_zh": "åŸºäºå¤šç›¸æœºè§†è§‰çš„ç»†ç²’åº¦è£…é…è´¨é‡æ§åˆ¶æ–¹æ³•",
      "authors": [
        "Ali Nazeri",
        "Shashank Mishra",
        "Achim Wagner",
        "Martin Ruskowski",
        "Didier Stricker",
        "Jason Rambach"
      ],
      "abstract": "Quality control is a critical aspect of manufacturing, particularly in ensuring the proper assembly of small components in production lines. Existing solutions often rely on single-view imaging or manual inspection, which are prone to errors due to occlusions, restricted perspectives, or lighting inconsistencies. These limitations require the installation of additional inspection stations, which could disrupt the assembly line and lead to increased downtime and costs. This paper introduces a novel multi-view quality control module designed to address these challenges, integrating a multi-camera imaging system with advanced object detection algorithms. By capturing images from three camera views, the system provides comprehensive visual coverage of components of an assembly process. A tailored image fusion methodology combines results from multiple views, effectively resolving ambiguities and enhancing detection reliability. To support this system, we developed a unique dataset comprising annotated images across diverse scenarios, including varied lighting conditions, occlusions, and angles, to enhance applicability in real-world manufacturing environments. Experimental results show that our approach significantly outperforms single-view methods, achieving high precision and recall rates in the identification of improperly fastened small assembly parts such as screws. This work contributes to industrial automation by overcoming single-view limitations, and providing a scalable, cost-effective, and accurate quality control mechanism that ensures the reliability and safety of the assembly line. The dataset used in this study is publicly available to facilitate further research in this domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åˆ¶é€ ä¸šä¸­å°å‹ç»„ä»¶è£…é…è´¨é‡æ§åˆ¶ï¼ˆQuality controlï¼‰ä¸­å› é®æŒ¡ã€è§†è§’å—é™å’Œå…‰ç…§ä¸ä¸€å¯¼è‡´çš„æ£€æµ‹éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¤šæ‘„åƒæœºè§†è§‰çš„ç»†ç²’åº¦æ£€æµ‹æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªæ–°å‹å¤šè§†å›¾è´¨é‡æ§åˆ¶æ¨¡å—ï¼Œé€šè¿‡æ•´åˆä¸‰è§†è§’æ‘„åƒæœºæˆåƒç³»ç»Ÿä¸å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹ï¼ˆobject detectionï¼‰ç®—æ³•ï¼Œå®ç°äº†å¯¹è£…é…è¿‡ç¨‹çš„å…¨é¢è§†è§‰è¦†ç›–ã€‚ç³»ç»Ÿæ ¸å¿ƒé‡‡ç”¨å®šåˆ¶çš„å›¾åƒèåˆï¼ˆimage fusionï¼‰æ–¹æ³•å¤„ç†å¤šè§†å›¾ç»“æœï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†è§†è§‰æ­§ä¹‰å¹¶æ˜¾è‘—æå‡äº†æ£€æµ‹çš„å¯é æ€§ã€‚ä¸ºæ”¯æŒè¯¥ç³»ç»Ÿçš„è®­ç»ƒä¸è¯„ä¼°ï¼Œç ”ç©¶è€…å¼€å‘å¹¶å…¬å¼€äº†ä¸€ä¸ªåŒ…å«å¤šæ ·åŒ–çœŸå®åˆ¶é€ åœºæ™¯çš„æ ‡æ³¨æ•°æ®é›†ï¼Œæ¶µç›–äº†å„ç§å…‰ç…§ã€é®æŒ¡å’Œè§’åº¦æƒ…å†µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯†åˆ«èºé’‰ç­‰ç»†å°é›¶ä»¶çš„è£…é…ç¼ºé™·æ–¹é¢æ€§èƒ½ä¼˜è¶Šï¼Œå…¶ç²¾ç¡®ç‡ï¼ˆprecisionï¼‰å’Œå¬å›ç‡ï¼ˆrecallï¼‰æ˜¾è‘—é«˜äºä¼ ç»Ÿçš„å•è§†å›¾ï¼ˆsingle-viewï¼‰æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºå·¥ä¸šè‡ªåŠ¨åŒ–æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€ä½æˆæœ¬ä¸”é«˜ç²¾åº¦çš„è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œå¯¹ç¡®ä¿è£…é…çº¿çš„å®‰å…¨ä¸å¯é å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages, 3 figures. Accepted for presentation at EUSIPCO 2025 (European Signal Processing Conference)",
      "pdf_url": "https://arxiv.org/pdf/2509.23815v1",
      "published_date": "2025-09-28 11:37:48 UTC",
      "updated_date": "2025-09-28 11:37:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:27.487554+00:00"
    },
    {
      "arxiv_id": "2510.24724v1",
      "title": "AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers",
      "title_zh": "AmarDoctorï¼šé¢å‘å­ŸåŠ æ‹‰è¯­äººç¾¤çš„ AI é©±åŠ¨å‹å¤šè¯­è¨€è¯­éŸ³äº¤äº’æ•°å­—åŒ»ç–—åº”ç”¨ï¼Œæ—¨åœ¨é€šè¿‡åŸºå±‚åŒ»ç–—åˆ†è¯Šä¸æ‚£è€…ç®¡ç†å¼¥åˆæ•°å­—åŒ»ç–—é¸¿æ²Ÿ",
      "authors": [
        "Nazmun Nahar",
        "Ritesh Harshad Ruparel",
        "Shariar Kabir",
        "Sumaiya Tasnia Khan",
        "Shyamasree Saha",
        "Mamunur Rashid"
      ],
      "abstract": "This study presents AmarDoctor, a multilingual voice-interactive digital health app designed to provide comprehensive patient triage and AI-driven clinical decision support for Bengali speakers, a population largely underserved in access to digital healthcare. AmarDoctor adopts a data-driven approach to strengthen primary care delivery and enable personalized health management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health have become popular in recent years, they mainly serve European demographics and languages. AmarDoctor addresses this gap with a dual-interface system for both patients and healthcare providers, supporting three major Bengali dialects. At its core, the patient module uses an adaptive questioning algorithm to assess symptoms and guide users toward the appropriate specialist. To overcome digital literacy barriers, it integrates a voice-interactive AI assistant that navigates users through the app services. Complementing this, the clinician-facing interface incorporates AI-powered decision support that enhances workflow efficiency by generating structured provisional diagnoses and treatment recommendations. These outputs inform key services such as e-prescriptions, video consultations, and medical record management. To validate clinical accuracy, the system was evaluated against a gold-standard set of 185 clinical vignettes developed by experienced physicians. Effectiveness was further assessed by comparing AmarDoctor performance with five independent physicians using the same vignette set. Results showed AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus physicians average of 50.27 percent) and a top specialty recommendation precision of 91.35 percent (versus physicians average of 62.6 percent).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº† AmarDoctorï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹å­ŸåŠ æ‹‰è¯­ä½¿ç”¨è€…çš„ AI é©±åŠ¨ã€å¤šè¯­è¨€ä¸”å…·å¤‡è¯­éŸ³äº¤äº’åŠŸèƒ½çš„æ•°å­—åŒ»ç–—åº”ç”¨ï¼Œæ—¨åœ¨å¼¥åˆè¯¥äººç¾¤åœ¨æ•°å­—å¥åº·é¢†åŸŸçš„é¸¿æ²Ÿã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŒç•Œé¢è®¾è®¡ï¼Œå…¶æ‚£è€…æ¨¡å—åˆ©ç”¨è‡ªé€‚åº”æé—®ç®—æ³•(Adaptive Questioning Algorithm)å’Œè¯­éŸ³ AI åŠ©æ‰‹å¼•å¯¼ç”¨æˆ·è¿›è¡Œç—‡çŠ¶è¯„ä¼°ä¸åˆ†è¯Šï¼Œæœ‰æ•ˆå…‹æœäº†æ•°å­—ç´ å…»éšœç¢ã€‚åŒæ—¶ï¼ŒåŒ»ç”Ÿç«¯é›†æˆäº† AI è¾…åŠ©å†³ç­–æ”¯æŒï¼Œèƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–çš„åˆæ­¥è¯Šæ–­å’Œæ²»ç–—å»ºè®®ï¼Œè¾…åŠ©ç”µå­å¤„æ–¹(E-prescriptions)åŠåŒ»ç–—è®°å½•ç®¡ç†ã€‚ä¸ºäº†éªŒè¯ä¸´åºŠå‡†ç¡®æ€§ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨ 185 ä¸ªä¸´åºŠæ¡ˆä¾‹(Clinical Vignettes)å°†ç³»ç»Ÿæ€§èƒ½ä¸äººç±»åŒ»ç”Ÿè¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAmarDoctor çš„ Top-1 è¯Šæ–­å‡†ç¡®ç‡è¾¾åˆ° 81.08%ï¼Œè¿œé«˜äºåŒ»ç”Ÿçš„ 50.27% å¹³å‡æ°´å¹³ã€‚æ­¤å¤–ï¼Œå…¶åœ¨é¡¶çº§ä¸“ç§‘å»ºè®®å‡†ç¡®ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ° 91.35%ï¼Œæ˜¾è‘—æå‡äº†åˆçº§ä¿å¥çš„äº¤ä»˜æ•ˆç‡ä¸ä¸ªæ€§åŒ–å¥åº·ç®¡ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.24724v1",
      "published_date": "2025-09-28 11:31:59 UTC",
      "updated_date": "2025-09-28 11:31:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:31.784676+00:00"
    },
    {
      "arxiv_id": "2509.23813v2",
      "title": "IndexNet: Timestamp and Variable-Aware Modeling for Time Series Forecasting",
      "title_zh": "IndexNetï¼šé¢å‘æ—¶é—´åºåˆ—é¢„æµ‹çš„æ—¶é—´æˆ³ä¸å˜é‡æ„ŸçŸ¥å»ºæ¨¡",
      "authors": [
        "Beiliang Wu",
        "Peiyuan Liu",
        "Yifan Hu",
        "Luyan Zhang",
        "Ao Hu",
        "Zenglin Xu"
      ],
      "abstract": "Multivariate time series forecasting (MTSF) plays a vital role in a wide range of real-world applications, such as weather prediction and traffic flow forecasting. Although recent advances have significantly improved the modeling of temporal dynamics and inter-variable dependencies, most existing methods overlook index-related descriptive information, such as timestamps and variable indices, which carry rich contextual semantics. To unlock the potential of such information and take advantage of the lightweight and powerful periodic capture ability of MLP-based architectures, we propose IndexNet, an MLP-based framework augmented with an Index Embedding (IE) module. The IE module consists of two key components: Timestamp Embedding (TE) and Channel Embedding (CE). Specifically, TE transforms timestamps into embedding vectors and injects them into the input sequence, thereby improving the model's ability to capture long-term complex periodic patterns. In parallel, CE assigns each variable a unique and trainable identity embedding based on its index, allowing the model to explicitly distinguish between heterogeneous variables and avoid homogenized predictions when input sequences seem close. Extensive experiments on 12 diverse real-world datasets demonstrate that IndexNet achieves comparable performance across mainstream baselines, validating the effectiveness of our temporally and variably aware design. Moreover, plug-and-play experiments and visualization analyses further reveal that IndexNet exhibits strong generality and interpretability, two aspects that remain underexplored in current MTSF research.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† IndexNetï¼Œä¸€ä¸ªé€šè¿‡ç´¢å¼•åµŒå…¥ (Index Embedding, IE) æ¨¡å—å¢å¼ºçš„åŸºäºå¤šå±‚æ„ŸçŸ¥æœº (MLP) çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ (Multivariate time series forecasting, MTSF) ä¸­ç°æœ‰æ–¹æ³•å¿½è§†æ—¶é—´æˆ³å’Œå˜é‡ç´¢å¼•ç­‰ä¸Šä¸‹æ–‡è¯­ä¹‰çš„é—®é¢˜ã€‚IE æ¨¡å—ç”±æ—¶é—´æˆ³åµŒå…¥ (Timestamp Embedding, TE) å’Œé€šé“åµŒå…¥ (Channel Embedding, CE) ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶æ„æˆã€‚TE å°†æ—¶é—´æˆ³è½¬åŒ–ä¸ºåµŒå…¥å‘é‡å¹¶æ³¨å…¥åºåˆ—ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ•è·é•¿æœŸå¤æ‚å‘¨æœŸæ€§æ¨¡å¼çš„èƒ½åŠ›ï¼›è€Œ CE ä¸ºæ¯ä¸ªå˜é‡åˆ†é…å”¯ä¸€çš„èº«ä»½åµŒå…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ˜¾å¼åŒºåˆ†å¼‚è´¨å˜é‡ï¼Œé¿å…åœ¨è¾“å…¥ç›¸è¿‘æ—¶äº§ç”ŸåŒè´¨åŒ–é¢„æµ‹ã€‚åœ¨ 12 ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒIndexNet çš„æ€§èƒ½è¾¾åˆ°äº†ä¸»æµåŸºå‡†æ°´å¹³ï¼ŒéªŒè¯äº†å…¶æ—¶é—´ä¸å˜é‡æ„ŸçŸ¥è®¾è®¡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå³æ’å³ç”¨å®éªŒå’Œå¯è§†åŒ–åˆ†æè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ¡†æ¶å…·æœ‰å¼ºå¤§çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œä¸º MTSF é¢†åŸŸæä¾›äº†é‡è¦çš„ç ”ç©¶è¡¥å……ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23813v2",
      "published_date": "2025-09-28 11:30:17 UTC",
      "updated_date": "2025-10-02 07:33:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:47.090782+00:00"
    },
    {
      "arxiv_id": "2509.23812v2",
      "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models",
      "title_zh": "èµ°å‡ºè¿·å®«ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è·¯å¾„æ•æ„Ÿå‹å•å…ƒæµ‹è¯•ç”Ÿæˆ",
      "authors": [
        "Dianshu Liao",
        "Xin Yin",
        "Shidong Pan",
        "Chao Ni",
        "Zhenchang Xing",
        "Xiaoyu Sun"
      ],
      "abstract": "Unit testing is essential for software quality assurance, yet writing and maintaining tests remains time-consuming and error-prone. To address this challenge, researchers have proposed various techniques for automating unit test generation, including traditional heuristic-based methods and more recent approaches that leverage large language models (LLMs). However, these existing approaches are inherently path-insensitive because they rely on fixed heuristics or limited contextual information and fail to reason about deep control-flow structures. As a result, they often struggle to achieve adequate coverage, particularly for deep or complex execution paths. In this work, we present a path-sensitive framework, JUnitGenie, to fill this gap by combining code knowledge with the semantic capabilities of LLMs in guiding context-aware unit test generation. After extracting code knowledge from Java projects, JUnitGenie distills this knowledge into structured prompts to guide the generation of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex focal methods from ten real-world Java projects. The results show that JUnitGenie generates valid tests and improves branch and line coverage by 29.60% and 31.00% on average over both heuristic and LLM-based baselines. We further demonstrate that the generated test cases can uncover real-world bugs, which were later confirmed and fixed by developers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­ç°æœ‰æ–¹æ³•è·¯å¾„ä¸æ•æ„Ÿ(path-insensitive)ã€éš¾ä»¥æœ‰æ•ˆå¤„ç†æ·±å±‚æ§åˆ¶æµç»“æ„çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºJUnitGenieçš„è·¯å¾„æ•æ„Ÿæ¡†æ¶ã€‚JUnitGenieé€šè¿‡å°†ä»Javaé¡¹ç›®ä¸­æå–çš„ä»£ç çŸ¥è¯†ä¸å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è¯­ä¹‰èƒ½åŠ›ç›¸ç»“åˆï¼Œå¼•å¯¼ç”Ÿæˆå…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„å•å…ƒæµ‹è¯•ã€‚è¯¥æ¡†æ¶å°†ä»£ç çŸ¥è¯†è½¬åŒ–ä¸ºç»“æ„åŒ–æç¤º(structured prompts)ï¼Œä»è€Œæœ‰æ•ˆæŒ‡å¯¼é«˜è¦†ç›–ç‡æµ‹è¯•ç”¨ä¾‹çš„ç”Ÿæˆã€‚åœ¨10ä¸ªçœŸå®Javaé¡¹ç›®çš„2258ä¸ªå¤æ‚ç„¦ç‚¹æ–¹æ³•ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒJUnitGenieåœ¨åˆ†æ”¯è¦†ç›–ç‡å’Œè¡Œè¦†ç›–ç‡ä¸Šè¾ƒå¯å‘å¼åŠåŸºäºLLMçš„åŸºçº¿æ¨¡å‹åˆ†åˆ«å¹³å‡æå‡äº†29.60%å’Œ31.00%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹è¿˜æˆåŠŸå‘ç°äº†å¤šä¸ªå·²è¢«å¼€å‘è€…ç¡®è®¤å¹¶ä¿®å¤çš„çœŸå®ä¸–ç•Œç¨‹åºé”™è¯¯ï¼Œè¯æ˜äº†å…¶åœ¨è½¯ä»¶è´¨é‡ä¿è¯ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23812v2",
      "published_date": "2025-09-28 11:29:57 UTC",
      "updated_date": "2025-10-11 07:22:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:51.496366+00:00"
    },
    {
      "arxiv_id": "2509.23811v1",
      "title": "AnveshanaAI: A Multimodal Platform for Adaptive AI/ML Education through Automated Question Generation and Interactive Assessment",
      "title_zh": "AnveshanaAIï¼šåŸºäºè‡ªåŠ¨åŒ–å‡ºé¢˜ä¸äº¤äº’å¼è¯„ä¼°çš„è‡ªé€‚åº” AI/ML æ•™è‚²å¤šæ¨¡æ€å¹³å°",
      "authors": [
        "Rakesh Thakur",
        "Diksha Khandelwal",
        "Shreya Tiwari"
      ],
      "abstract": "We propose AnveshanaAI, an application-based learning platform for artificial intelligence. With AnveshanaAI, learners are presented with a personalized dashboard featuring streaks, levels, badges, and structured navigation across domains such as data science, machine learning, deep learning, transformers, generative AI, large language models, and multimodal AI, with scope to include more in the future. The platform incorporates gamified tracking with points and achievements to enhance engagement and learning, while switching between Playground, Challenges, Simulator, Dashboard, and Community supports exploration and collaboration. Unlike static question repositories used in existing platforms, AnveshanaAI ensures balanced learning progression through a dataset grounded in Bloom's taxonomy, with semantic similarity checks and explainable AI techniques improving transparency and reliability. Adaptive, automated, and domain-aware assessment methods are also employed. Experiments demonstrate broad dataset coverage, stable fine-tuning with reduced perplexity, and measurable gains in learner engagement. Together, these features illustrate how AnveshanaAI integrates adaptivity, gamification, interactivity, and explainability to support next-generation AI education.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AnveshanaAIï¼Œä¸€ä¸ªæ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–é¢˜ç›®ç”Ÿæˆå’Œäº¤äº’å¼è¯„ä¼°å®ç°è‡ªé€‚åº” AI/ML æ•™è‚²çš„å¤šæ¨¡æ€å­¦ä¹ å¹³å°ã€‚è¯¥å¹³å°æ¶µç›–äº† Data Scienceã€Machine Learningã€Deep Learningã€Generative AI å’Œ Multimodal AI ç­‰æ ¸å¿ƒé¢†åŸŸï¼Œé€šè¿‡ä¸ªæ€§åŒ–ä»ªè¡¨æ¿ä»¥åŠåŒ…å« streaksã€levels å’Œ badges çš„ gamification æœºåˆ¶æ¥å¢å¼ºå­¦ä¹ å‚ä¸åº¦ã€‚ä¸åŒäºä¼ ç»Ÿçš„é™æ€é¢˜åº“ï¼ŒAnveshanaAI é‡‡ç”¨äº†åŸºäº Bloom's taxonomy çš„æ•°æ®é›†ä»¥ç¡®ä¿å­¦ä¹ è¿›åº¦çš„å‡è¡¡ï¼Œå¹¶ç»“åˆ semantic similarity checks å’Œ Explainable AI (XAI) æŠ€æœ¯æå‡äº†ç³»ç»Ÿçš„é€æ˜åº¦ä¸å¯é æ€§ã€‚é€šè¿‡åº”ç”¨é¢†åŸŸæ„ŸçŸ¥çš„è‡ªåŠ¨åŒ–è¯„ä¼°æ–¹æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨ fine-tuning è¿‡ç¨‹ä¸­è¡¨ç°ç¨³å®šï¼Œå¹¶æœ‰æ•ˆé™ä½äº† perplexityã€‚æœ€ç»ˆï¼ŒAnveshanaAI é€šè¿‡æ•´åˆè‡ªé€‚åº”æ€§ã€äº’åŠ¨æ€§ä¸å¯è§£é‡Šæ€§ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ è€…çš„å‚ä¸åº¦ï¼Œä¸ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½æ•™è‚²æä¾›äº†åˆ›æ–°çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "11 pages, 12 figures. Under review as a conference paper at ICLR 2026. Preprint version posted on arXiv",
      "pdf_url": "https://arxiv.org/pdf/2509.23811v1",
      "published_date": "2025-09-28 11:24:22 UTC",
      "updated_date": "2025-09-28 11:24:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:27:58.590338+00:00"
    },
    {
      "arxiv_id": "2509.23809v2",
      "title": "Tequila: Trapping-free Ternary Quantization for Large Language Models",
      "title_zh": "Tequilaï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„æ— å—å›°ä¸‰å€¼é‡åŒ–",
      "authors": [
        "Hong Huang",
        "Decheng Wu",
        "Rui Cen",
        "Guanghua Yu",
        "Zonghang Li",
        "Kai Liu",
        "Jianchen Zhu",
        "Peng Chen",
        "Xue Liu",
        "Dapeng Wu"
      ],
      "abstract": "Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data. We identify the core issue as deadzone trapping: a large number of weights are trapped at the deadzone boundary. This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose Tequila, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly zero inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within <1% gap) with a 3.0x inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at https://github.com/Tencent/AngelSlim.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²ä¸­çš„ç²¾åº¦æŸå¤±é—®é¢˜ï¼Œæå‡ºäº†Tequilaè¿™ä¸€æ— æ•è·çš„ä¸‰å€¼é‡åŒ–(Ternary Quantization)ä¼˜åŒ–æ–¹æ³•ã€‚ç ”ç©¶è€…å‘ç°ä¼ ç»Ÿæ–¹æ³•ä¸­å­˜åœ¨çš„â€œæ­»åŒºæ•è·â€(deadzone trapping)ç°è±¡æ˜¯å¯¼è‡´ç²¾åº¦ä¸‹é™çš„æ ¸å¿ƒåŸå› ï¼Œå³å¤§é‡æƒé‡å› æ¥æ”¶åˆ°æ— ä¿¡æ¯æ¢¯åº¦è€Œæ— æ³•æœ‰æ•ˆä¼˜åŒ–ï¼Œä¸¥é‡é˜»ç¢äº†æ¨¡å‹å®¹é‡ã€‚Tequilaé€šè¿‡å°†é™·å…¥æ­»åŒºçš„æƒé‡é‡æ–°åˆ©ç”¨ä¸ºåŠ¨æ€åå·®(dynamic biases)ï¼Œä½¿è¿™äº›æƒé‡èƒ½å¤Ÿåœ¨æ­£å‘ä¼ æ’­ä¸­æä¾›è¿ç»­ä¿¡å·å¹¶æ¥æ”¶ç›´æ¥çš„æ¢¯åº¦åé¦ˆï¼Œä»è€Œåœ¨å‡ ä¹ä¸å¢åŠ æ¨ç†å¼€é”€çš„å‰æä¸‹æ˜¾è‘—æå‡æ¨¡å‹ä¼˜åŒ–æ•ˆæœã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTequilaåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¶…è¶Šäº†ç°æœ‰SOTAæ–¹æ³•ï¼Œå°¤å…¶åœ¨ARCåŸºå‡†ä¸Šå®ç°äº†è¶…è¿‡4%çš„å‡†ç¡®ç‡æå‡ã€‚åœ¨å®ç°3.0å€æ¨ç†åŠ é€Ÿçš„åŒæ—¶ï¼ŒTequilaä¸å…¨ç²¾åº¦(full-precision)æ€§èƒ½çš„å·®è·ç¼©å‡è‡³1%ä»¥å†…ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23809v2",
      "published_date": "2025-09-28 11:17:40 UTC",
      "updated_date": "2025-10-17 09:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:28:02.996853+00:00"
    },
    {
      "arxiv_id": "2510.03278v1",
      "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition",
      "title_zh": "åŸºäºé€çº¦æŸ Hessian åˆ†è§£çš„è´å¶æ–¯ PINNs çº¦æŸå±‚çº§é‡åŒ–",
      "authors": [
        "Filip Landgren"
      ],
      "abstract": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing equations to solve differential equations under uncertainty. However, interpreting uncertainty and overconfidence in B-PINNs requires care due to the poorly understood effects the physical constraints have on the network; overconfidence could reflect warranted precision, enforced by the constraints, rather than miscalibration. Motivated by the need to further clarify how individual physical constraints shape these networks, we introduce a scalable, matrix-free Laplace framework that decomposes the posterior Hessian into contributions from each constraint and provides metrics to quantify their relative influence on the loss landscape. Applied to the Van der Pol equation, our method tracks how constraints sculpt the network's geometry and shows, directly through the Hessian, how changing a single loss weight non-trivially redistributes curvature and effective dominance across the others.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è´å¶æ–¯ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (Bayesian physics-informed neural networks, B-PINNs) ä¸­ç‰©ç†çº¦æŸå¯¹æ¨¡å‹å½±å“æœºåˆ¶ä¸æ˜ï¼Œå¯¼è‡´ä¸ç¡®å®šæ€§ä¸è¿‡åº¦è‡ªä¿¡éš¾ä»¥è§£è¯»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¯æ‰©å±•ä¸”æ— çŸ©é˜µçš„ Laplace æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†åéªŒ Hessian çŸ©é˜µåˆ†è§£ä¸ºå„ç‰©ç†çº¦æŸçš„ç‹¬ç«‹è´¡çŒ®ï¼Œæä¾›äº†é‡åŒ–ä¸åŒçº¦æŸå¯¹æŸå¤±æ™¯è§‚ (loss landscape) ç›¸å¯¹å½±å“çš„åº¦é‡æŒ‡æ ‡ã€‚åœ¨ Van der Pol æ–¹ç¨‹çš„å®éªŒåº”ç”¨ä¸­ï¼Œè¯¥æ¡†æ¶æˆåŠŸè¿½è¸ªäº†çº¦æŸå¦‚ä½•å¡‘é€ ç½‘ç»œå‡ ä½•ç»“æ„ï¼Œå¹¶ç›´æ¥é€šè¿‡ Hessian æ­ç¤ºäº†æ”¹å˜å•ä¸€æŸå¤±æƒé‡ä¼šå¦‚ä½•éå¹³å‡¡åœ°é‡æ–°åˆ†é…æ›²ç‡åŠå…¶å¯¹å…¶ä»–çº¦æŸçš„æœ‰æ•ˆä¸»å¯¼åœ°ä½ã€‚è¿™é¡¹å·¥ä½œä¸ºé‡åŒ–ç‰©ç†çº¦æŸåœ¨ B-PINNs ä¸­çš„å±‚çº§ç»“æ„æä¾›äº†æœ‰æ•ˆçš„åˆ†æå·¥å…·ï¼Œæœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£å’Œæ ¡å‡†ç‰©ç†é©±åŠ¨æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2510.03278v1",
      "published_date": "2025-09-28 11:06:46 UTC",
      "updated_date": "2025-09-28 11:06:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:28:46.591837+00:00"
    },
    {
      "arxiv_id": "2509.23803v1",
      "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents",
      "title_zh": "FedAgentBenchï¼šåŸºäºæœåŠ¡ç«¯-å®¢æˆ·ç«¯å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ç°å®åœºæ™¯è”é‚¦åŒ»å­¦å›¾åƒåˆ†æè‡ªåŠ¨åŒ–",
      "authors": [
        "Pramit Saha",
        "Joshua Strong",
        "Divyanshu Mishra",
        "Cheng Ouyang",
        "J. Alison Noble"
      ],
      "abstract": "Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FedAgentBenchï¼Œæ—¨åœ¨åˆ©ç”¨Server-Client LLM Agentsæ¶æ„å®ç°çœŸå®ä¸–ç•Œä¸­è”é‚¦åŒ»ç–—å›¾åƒåˆ†æçš„è‡ªåŠ¨åŒ–ã€‚é’ˆå¯¹Federated Learning (FL) åœ¨å®é™…éƒ¨ç½²ä¸­é¢ä¸´çš„åŒ»é™¢é€‰æ‹©ã€è·¨ç«™ç‚¹æ•°æ®é¢„å¤„ç†ã€æ ‡ç­¾æ ‡å‡†åŒ–åŠç®—æ³•åè°ƒç­‰å¤æ‚æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶æ„å»ºäº†ä¸€ä¸ªç”±ä¸­å¤®æœåŠ¡å™¨å’ŒåŒ»é™¢å®¢æˆ·ç«¯æ™ºèƒ½ä»£ç†ç»„æˆçš„åä½œç³»ç»Ÿï¼Œå®ç°äº†æœ€å°åŒ–äººå·¥å¹²é¢„çš„FLå·¥ä½œæµã€‚ç ”ç©¶æ¨å‡ºäº†åŒ…å«40ç§FL algorithmså’Œæ¶µç›–6ç§åŒ»ç–—æ¨¡æ€ï¼ˆDermatoscopy, Ultrasound, Fundus, Histopathology, MRI, X-Rayï¼‰çš„201ä¸ªç²¾é€‰æ•°æ®é›†çš„åŸºå‡†æµ‹è¯•é›†ã€‚é€šè¿‡å¯¹24ä¸ªå¼€æºåŠå•†ç”¨Large Language Models (LLMs) çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡GPT-4.1å’ŒDeepSeek V3ç­‰æ ¸å¿ƒæ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨åŒ–FLæµæ°´çº¿çš„å¤šä¸ªé˜¶æ®µï¼Œä½†åœ¨å¤„ç†å…·æœ‰éšå«ç›®æ ‡çš„å¤æ‚ç›¸äº’ä¾èµ–ä»»åŠ¡æ—¶ä»è¡¨ç°å‡ºå±€é™æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¯æ‰©å±•ã€è‡ªåŠ¨åŒ–çš„åŒ»ç–—è”é‚¦å­¦ä¹ ç³»ç»Ÿæä¾›äº†é‡è¦çš„è¯„ä¼°åŸºå‡†å’Œå®è·µè·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23803v1",
      "published_date": "2025-09-28 11:06:07 UTC",
      "updated_date": "2025-09-28 11:06:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:28:09.192557+00:00"
    },
    {
      "arxiv_id": "2510.02361v1",
      "title": "ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference",
      "title_zh": "ChunkLLMï¼šä¸€ç§ç”¨äºåŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„è½»é‡çº§å¯æ’æ‹”æ¡†æ¶",
      "authors": [
        "Haojie Ouyang",
        "Jianwei Lv",
        "Lei Ren",
        "Chen Wei",
        "Xiaojie Wang",
        "Fangxiang Feng"
      ],
      "abstract": "Transformer-based large models excel in natural language processing and computer vision, but face severe computational inefficiencies due to the self-attention's quadratic complexity with input tokens. Recently, researchers have proposed a series of methods based on block selection and compression to alleviate this problem, but they either have issues with semantic incompleteness or poor training-inference efficiency. To comprehensively address these challenges, we propose ChunkLLM, a lightweight and pluggable training framework. Specifically, we introduce two components: QK Adapter (Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each Transformer layer, serving dual purposes of feature compression and chunk attention acquisition. The latter operates at the bottommost layer of the model, functioning to detect chunk boundaries by leveraging contextual semantic information. During the training phase, the parameters of the backbone remain frozen, with only the QK Adapter and Chunk Adapter undergoing training. Notably, we design an attention distillation method for training the QK Adapter, which enhances the recall rate of key chunks. During the inference phase, chunk selection is triggered exclusively when the current token is detected as a chunk boundary, thereby accelerating model inference. Experimental evaluations are conducted on a diverse set of long-text and short-text benchmark datasets spanning multiple tasks. ChunkLLM not only attains comparable performance on short-text benchmarks but also maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. Particularly, ChunkLLM attains a maximum speedup of 4.48x in comparison to the vanilla Transformer in the processing of 120K long texts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ChunkLLMï¼Œè¿™æ˜¯ä¸€ç§è½»é‡çº§ä¸”å¯æ’æ‹”çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å—é€‰æ‹©ä¸å‹ç¼©æŠ€æœ¯è§£å†³ Transformer æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶å› è‡ªæ³¨æ„åŠ›æœºåˆ¶äºŒæ¬¡å¤æ‚åº¦å¯¼è‡´çš„æ¨ç†æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚è¯¥æ¡†æ¶å¼•å…¥äº† QK Adapterï¼ˆåŒ…å« Q-Adapter å’Œ K-Adapterï¼‰ç”¨äºå®ç°ç‰¹å¾å‹ç¼©å’Œå—æ³¨æ„åŠ›è·å–ï¼Œå¹¶ç»“åˆåº•å±‚çš„ Chunk Adapter åˆ©ç”¨ä¸Šä¸‹æ–‡è¯­ä¹‰ä¿¡æ¯ç²¾ç¡®æ£€æµ‹å—è¾¹ç•Œã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼ŒChunkLLM ä¿æŒä¸»å¹²ç½‘ç»œå‚æ•°å†»ç»“ï¼Œä»…å¯¹é€‚é…å™¨è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨æ³¨æ„åŠ›è’¸é¦ï¼ˆattention distillationï¼‰æ–¹æ³•æ˜¾è‘—æé«˜å…³é”®å—çš„å¬å›ç‡ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå—é€‰æ‹©ä»…åœ¨æ£€æµ‹åˆ°å—è¾¹ç•Œæ—¶è§¦å‘ï¼Œä»è€Œå¤§å¹…æå‡è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨é•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­ä¿ç•™äº† 98.64% çš„æ€§èƒ½ï¼ŒåŒæ—¶å°† KV cache ä¿ç•™ç‡é™ä½è‡³ 48.58%ã€‚åœ¨å¤„ç† 120K é•¿åº¦çš„æ–‡æœ¬æ—¶ï¼ŒChunkLLM ç›¸æ¯”åŸç”Ÿ Transformer å®ç°äº†é«˜è¾¾ 4.48 å€çš„æ¨ç†åŠ é€Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹æ¨ç†æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02361v1",
      "published_date": "2025-09-28 11:04:00 UTC",
      "updated_date": "2025-09-28 11:04:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:28:13.093817+00:00"
    },
    {
      "arxiv_id": "2509.23799v2",
      "title": "Enhancing LLM Steering through Sparse Autoencoder-Based Vector Refinement",
      "title_zh": "é€šè¿‡åŸºäºç¨€ç–è‡ªç¼–ç å™¨çš„å‘é‡ç²¾ç‚¼å¢å¼º LLM æ“æ§",
      "authors": [
        "Anyi Wang",
        "Xuansheng Wu",
        "Dong Shu",
        "Yunpu Ma",
        "Ninghao Liu"
      ],
      "abstract": "Steering has emerged as a promising approach in controlling large language models (LLMs) without modifying model parameters. However, most existing steering methods rely on large-scale datasets to learn clear behavioral information, which limits their applicability in many real-world scenarios. The steering vectors extracted from small dataset often contain task-irrelevant noising features, which degrades their effectiveness. To refine the steering vectors learned from limited data, we introduce Refinement of Steering Vector via Sparse Autoencoder (SAE-RSV) that leverages SAEs to semantically denoise and augment the steering vectors. In our framework, we first remove task-irrelevant features according to their semantics provided by SAEs, and then enrich task-relevant features missing from the small dataset through their semantic similarity to the identified relevant features. Extensive experiments demonstrate that the proposed SAE-RSV substantially outperforms all the baseline methods including supervised fine-tuning. Our findings show that effective steering vector can be constructed from limited training data by refining the original steering vector through SAEs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SAE-RSVï¼ˆRefinement of Steering Vector via Sparse Autoencoderï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°æ®å—é™æƒ…å†µä¸‹çš„Steeringï¼ˆæ“æ§ï¼‰æ•ˆæœã€‚ç°æœ‰çš„Steeringæ–¹æ³•é€šå¸¸ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»å°è§„æ¨¡æ•°æ®ä¸­æå–çš„Steering Vectorså¾€å¾€åŒ…å«å¤§é‡ä¸ä»»åŠ¡æ— å…³çš„å™ªå£°ç‰¹å¾ï¼Œé™ä½äº†æ“æ§çš„ç²¾ç¡®åº¦ã€‚SAE-RSVæ¡†æ¶åˆ©ç”¨Sparse Autoencodersï¼ˆSAEsï¼‰æä¾›çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé¦–å…ˆè¯†åˆ«å¹¶å»é™¤Steering Vectorsä¸­çš„å†—ä½™å™ªå£°ï¼Œéšåé€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è¡¥å……å°è§„æ¨¡æ•°æ®é›†ä¸­ç¼ºå¤±çš„ä»»åŠ¡ç›¸å…³ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAE-RSVåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŒ…æ‹¬Supervised Fine-Tuningï¼ˆSFTï¼‰åœ¨å†…çš„å¤šç§åŸºçº¿æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡SAEså¯¹åŸå§‹å‘é‡è¿›è¡Œè¯­ä¹‰å±‚é¢çš„ç²¾ç‚¼ä¸å¢å¼ºï¼Œå³ä½¿åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹ä¹Ÿèƒ½æ„å»ºå‡ºé«˜æ•ˆçš„Steering Vectorã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "19 pages, 11 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23799v2",
      "published_date": "2025-09-28 10:49:22 UTC",
      "updated_date": "2025-10-03 11:34:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:13.593070+00:00"
    },
    {
      "arxiv_id": "2509.23796v1",
      "title": "From Frustration to Fun: An Adaptive Problem-Solving Puzzle Game Powered by Genetic Algorithm",
      "title_zh": "ä»æŒ«è´¥åˆ°è¶£å‘³ï¼šåŸºäºé—ä¼ ç®—æ³•çš„è‡ªé€‚åº”é—®é¢˜æ±‚è§£ç›Šæ™ºæ¸¸æˆ",
      "authors": [
        "Matthew McConnell",
        "Richard Zhao"
      ],
      "abstract": "This paper explores adaptive problem solving with a game designed to support the development of problem-solving skills. Using an adaptive, AI-powered puzzle game, our adaptive problem-solving system dynamically generates pathfinding-based puzzles using a genetic algorithm, tailoring the difficulty of each puzzle to individual players in an online real-time approach. A player-modeling system records user interactions and informs the generation of puzzles to approximate a target difficulty level based on various metrics of the player. By combining procedural content generation with online adaptive difficulty adjustment, the system aims to maintain engagement, mitigate frustration, and maintain an optimal level of challenge. A pilot user study investigates the effectiveness of this approach, comparing different types of adaptive difficulty systems and interpreting players' responses. This work lays the foundation for further research into emotionally informed player models, advanced AI techniques for adaptivity, and broader applications beyond gaming in educational settings.",
      "tldr_zh": "æœ¬æ–‡æ¢è®¨äº†é€šè¿‡é—ä¼ ç®—æ³•(Genetic Algorithm)é©±åŠ¨çš„è‡ªé€‚åº”ç›Šæ™ºæ¸¸æˆæ¥æ”¯æŒé—®é¢˜è§£å†³èƒ½åŠ›çš„å‘å±•ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨é—ä¼ ç®—æ³•(Genetic Algorithm)å®æ—¶åœ¨çº¿åŠ¨æ€ç”ŸæˆåŸºäºè·¯å¾„æœç´¢çš„è°œé¢˜ï¼Œå¹¶æ ¹æ®æ¯ä½ç©å®¶çš„è¡¨ç°è°ƒæ•´éš¾åº¦ã€‚ç©å®¶å»ºæ¨¡ç³»ç»Ÿè®°å½•ç”¨æˆ·äº¤äº’æ•°æ®ï¼Œç»“åˆè¿‡ç¨‹å†…å®¹ç”Ÿæˆ(Procedural Content Generation)ä¸åœ¨çº¿è‡ªé€‚åº”éš¾åº¦è°ƒèŠ‚(Adaptive Difficulty Adjustment)æŠ€æœ¯ï¼Œä»¥ç¡®ä¿è°œé¢˜ç¬¦åˆç›®æ ‡éš¾åº¦æ°´å¹³ã€‚è¿™ç§è®¾è®¡æ—¨åœ¨ç»´æŒç©å®¶çš„å‚ä¸åº¦ï¼Œæœ‰æ•ˆç¼“è§£æŒ«è´¥æ„Ÿå¹¶ä¿æŒæœ€ä¼˜çš„æŒ‘æˆ˜æ°´å¹³ã€‚åˆæ­¥çš„ç”¨æˆ·ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒç±»å‹çš„è‡ªé€‚åº”ç³»ç»Ÿï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å¹¶åˆ†æäº†ç©å®¶çš„å¿ƒç†åé¦ˆã€‚è¯¥ç ”ç©¶ä¸ºæƒ…æ„Ÿæ„ŸçŸ¥ç©å®¶æ¨¡å‹ã€å…ˆè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è‡ªé€‚åº”é¢†åŸŸåŠæ•™è‚²åœºæ™¯çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.MM",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE-25)",
      "pdf_url": "https://arxiv.org/pdf/2509.23796v1",
      "published_date": "2025-09-28 10:40:14 UTC",
      "updated_date": "2025-09-28 10:40:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:16.791199+00:00"
    },
    {
      "arxiv_id": "2509.23787v1",
      "title": "From Unstable to Playable: Stabilizing Angry Birds Levels via Object Segmentation",
      "title_zh": "ä»ä¸ç¨³å®šåˆ°å¯ç©ï¼šåŸºäºç‰©ä½“åˆ†å‰²çš„ Angry Birds å…³å¡ç¨³å®šåŒ–æ–¹æ³•",
      "authors": [
        "Mahdi Farrokhimaleki",
        "Parsa Rahmati",
        "Richard Zhao"
      ],
      "abstract": "Procedural Content Generation (PCG) techniques enable automatic creation of diverse and complex environments. While PCG facilitates more efficient content creation, ensuring consistently high-quality, industry-standard content remains a significant challenge. In this research, we propose a method to identify and repair unstable levels generated by existing PCG models. We use Angry Birds as a case study, demonstrating our method on game levels produced by established PCG approaches. Our method leverages object segmentation and visual analysis of level images to detect structural gaps and perform targeted repairs. We evaluate multiple object segmentation models and select the most effective one as the basis for our repair pipeline. Experimental results show that our method improves the stability and playability of AI-generated levels. Although our evaluation is specific to Angry Birds, our image-based approach is designed to be applicable to a wide range of 2D games with similar level structures.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹è¿‡ç¨‹å†…å®¹ç”Ÿæˆ(Procedural Content Generation, PCG)æŠ€æœ¯åœ¨ç”Ÿæˆå·¥ä¸šçº§æ ‡å‡†æ¸¸æˆå†…å®¹æ—¶é¢ä¸´çš„ç¨³å®šæ€§æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§è¯†åˆ«å¹¶ä¿®å¤ç”±ç°æœ‰PCGæ¨¡å‹ç”Ÿæˆçš„ç‰©ç†ä¸ç¨³å®šå…³å¡çš„æ–°æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä»¥Angry Birdsä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡ç›®æ ‡åˆ†å‰²(Object Segmentation)å’Œå…³å¡å›¾åƒçš„è§†è§‰åˆ†æ(Visual Analysis)æ¥æ£€æµ‹ç»“æ„æ€§é—´éš™å¹¶æ‰§è¡Œé’ˆå¯¹æ€§ä¿®å¤ã€‚é€šè¿‡å¯¹å¤šç§ç›®æ ‡åˆ†å‰²æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶è€…é€‰æ‹©å¹¶æ„å»ºäº†é«˜æ•ˆçš„ä¿®å¤æµæ°´çº¿ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†AIç”Ÿæˆå…³å¡çš„ç¨³å®šæ€§å’Œå¯ç©æ€§(Playability)ã€‚è™½ç„¶è¯¥æ–¹æ³•åœ¨Angry Birdsä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œä½†è¿™ç§åŸºäºå›¾åƒçš„æ–¹æ¡ˆè®¾è®¡å…·æœ‰æ™®é€‚æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå…·æœ‰ç±»ä¼¼å…³å¡ç»“æ„çš„å…¶ä»–2Dæ¸¸æˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE-25)",
      "pdf_url": "https://arxiv.org/pdf/2509.23787v1",
      "published_date": "2025-09-28 10:15:19 UTC",
      "updated_date": "2025-09-28 10:15:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:14.284736+00:00"
    },
    {
      "arxiv_id": "2509.23783v1",
      "title": "Falcon: A Cross-Modal Evaluation Dataset for Comprehensive Safety Perception",
      "title_zh": "Falconï¼šé¢å‘å…¨é¢å®‰å…¨æ„ŸçŸ¥çš„è·¨æ¨¡æ€è¯„ä¼°æ•°æ®é›†",
      "authors": [
        "Qi Xue",
        "Minrui Jiang",
        "Runjia Zhang",
        "Xiurui Xie",
        "Pei Ke",
        "Guisong Liu"
      ],
      "abstract": "Existing methods for evaluating the harmfulness of content generated by large language models (LLMs) have been well studied. However, approaches tailored to multimodal large language models (MLLMs) remain underdeveloped and lack depth. This work highlights the crucial role of visual information in moderating content in visual question answering (VQA), a dimension often overlooked in current research. To bridge this gap, we introduce Falcon, a large-scale vision-language safety dataset containing 57,515 VQA pairs across 13 harm categories. The dataset provides explicit annotations for harmful attributes across images, instructions, and responses, thereby facilitating a comprehensive evaluation of the content generated by MLLMs. In addition, it includes the relevant harm categories along with explanations supporting the corresponding judgments. We further propose FalconEye, a specialized evaluator fine-tuned from Qwen2.5-VL-7B using the Falcon dataset. Experimental results demonstrate that FalconEye reliably identifies harmful content in complex and safety-critical multimodal dialogue scenarios. It outperforms all other baselines in overall accuracy across our proposed Falcon-test dataset and two widely-used benchmarks-VLGuard and Beavertail-V, underscoring its potential as a practical safety auditing tool for MLLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æœ‰å®³å†…å®¹è¯„ä¼°æ–¹é¢æ·±åº¦ä¸è¶³ä¸”å¿½è§†è§†è§‰ä¿¡æ¯ä½œç”¨çš„é—®é¢˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡è§†è§‰è¯­è¨€å®‰å…¨æ•°æ®é›† Falconã€‚Falcon åŒ…å« 57,515 ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰å¯¹ï¼Œè¦†ç›– 13 ä¸ªå±å®³ç±»åˆ«ï¼Œå¹¶å¯¹å›¾åƒã€æŒ‡ä»¤åŠå“åº”çš„æœ‰å®³å±æ€§æä¾›äº†è¯¦ç»†æ ‡æ³¨ä¸è§£é‡Šã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…é€šè¿‡å¾®è°ƒ Qwen2.5-VL-7B å¼€å‘äº†ä¸“é—¨çš„è¯„ä¼°å™¨ FalconEyeï¼Œæ—¨åœ¨å®ç°å¯¹å¤šæ¨¡æ€å¯¹è¯ä¸­ç”Ÿæˆå†…å®¹çš„å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFalconEye åœ¨ Falcon-test ä»¥åŠ VLGuard å’Œ Beavertail-V ç­‰åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡å‡è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…å¼¥è¡¥äº† MLLMs å®‰å…¨æ„ŸçŸ¥èƒ½åŠ›çš„è¯„ä¼°ç¼ºå£ï¼Œä¹Ÿä¸ºå¼€å‘å®ç”¨çš„å¤šæ¨¡æ€å®‰å…¨å®¡è®¡å·¥å…·æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23783v1",
      "published_date": "2025-09-28 10:00:37 UTC",
      "updated_date": "2025-09-28 10:00:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:18.491620+00:00"
    },
    {
      "arxiv_id": "2509.23781v1",
      "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
      "title_zh": "GroupCoOpï¼šåŸºäºç¾¤ç»„æç¤ºå­¦ä¹ çš„ç¾¤ç»„é²æ£’æ€§å¾®è°ƒ",
      "authors": [
        "Nayeong Kim",
        "Seong Joon Oh",
        "Suha Kwak"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs) excels in various vision tasks thanks to the rich knowledge and generalization ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are vulnerable to spurious correlations stemming from the subgroup imbalance in the fine-tuning datasets. To resolve this issue, we propose Group Context Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm that enhances the group robustness of fine-tuned VLMs. Its key idea is to employ group-specific text prompts as group representatives serving as multiple classifiers for their target class. The rich semantic knowledge of the text encoder of VLM enables the discovery of effective group prompts even for groups with a small number of training samples. Leveraging the group prompts for each class addresses the issues caused by the group-imbalanced training set, such as the neglect of minority groups and the scattered distribution of each class in the embedding space. GroupCoOp achieved the best results on five benchmarks across five CLIP architectures and occasionally outperformed prior methods that fine-tune the entire network, despite training only 0.016\\% of the network's parameters.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)è¿‡ç¨‹ä¸­ï¼Œç”±äºå¾®è°ƒæ•°æ®é›†ä¸­çš„å­ç¾¤ä½“ä¸å¹³è¡¡(subgroup imbalance)å¯¼è‡´æ¨¡å‹å®¹æ˜“å—åˆ°è™šå‡ç›¸å…³æ€§(spurious correlations)å½±å“çš„é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Group Context Optimization (GroupCoOp)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¾®è°ƒåVLMsç¾¤ä½“é²æ£’æ€§(group robustness)çš„å»åå·®å¾®è°ƒç®—æ³•ã€‚è¯¥ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ç‰¹å®šäºç¾¤ä½“çš„æ–‡æœ¬æç¤º(group-specific text prompts)ä½œä¸ºç¾¤ä½“ä»£è¡¨ï¼Œå¹¶å°†å…¶ä½œä¸ºç›®æ ‡ç±»åˆ«çš„å¤šä¸ªåˆ†ç±»å™¨ã€‚å€ŸåŠ©æ–‡æœ¬ç¼–ç å™¨ä¸°å¯Œçš„è¯­ä¹‰çŸ¥è¯†ï¼ŒGroupCoOpå³ä½¿åœ¨è®­ç»ƒæ ·æœ¬æå°‘çš„ç¾¤ä½“ä¸­ä¹Ÿèƒ½å‘ç°æœ‰æ•ˆçš„æç¤ºä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†è®­ç»ƒé›†ç¾¤ä½“ä¸å¹³è¡¡å¸¦æ¥çš„å°‘æ•°ç¾¤ä½“è¢«å¿½è§†ä»¥åŠåµŒå…¥ç©ºé—´ä¸­ç±»åˆ«åˆ†å¸ƒåˆ†æ•£çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGroupCoOpåœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•å’Œäº”ç§CLIPæ¶æ„ä¸Šå‡è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œä»…éœ€è®­ç»ƒ0.016%çš„å‚æ•°å³å¯åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸­è¶…è¶Šå…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper was first submitted to NeurIPS 2024 in May 2024",
      "pdf_url": "https://arxiv.org/pdf/2509.23781v1",
      "published_date": "2025-09-28 09:54:30 UTC",
      "updated_date": "2025-09-28 09:54:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:24.094883+00:00"
    },
    {
      "arxiv_id": "2509.23778v2",
      "title": "Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse",
      "title_zh": "ä»“å‚¨ç¯å¢ƒä¸‹å¤šæ™ºèƒ½ä½“å–é€è´§çš„åºåˆ—å¯»è·¯å™¨",
      "authors": [
        "Zeyuan Zhao",
        "Chaoran Li",
        "Shao Zhang",
        "Ying Wen"
      ],
      "abstract": "Multi-Agent Pickup and Delivery (MAPD) is a challenging extension of Multi-Agent Path Finding (MAPF), where agents are required to sequentially complete tasks with fixed-location pickup and delivery demands. Although learning-based methods have made progress in MAPD, they often perform poorly in warehouse-like environments with narrow pathways and long corridors when relying only on local observations for distributed decision-making. Communication learning can alleviate the lack of global information but introduce high computational complexity due to point-to-point communication. To address this challenge, we formulate MAPF as a sequence modeling problem and prove that path-finding policies under sequence modeling possess order-invariant optimality, ensuring its effectiveness in MAPD. Building on this, we propose the Sequential Pathfinder (SePar), which leverages the Transformer paradigm to achieve implicit information exchange, reducing decision-making complexity from exponential to linear while maintaining efficiency and global awareness. Experiments demonstrate that SePar consistently outperforms existing learning-based methods across various MAPF tasks and their variants, and generalizes well to unseen environments. Furthermore, we highlight the necessity of integrating imitation learning in complex maps like warehouses.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Sequential Pathfinder (SePar)ï¼Œæ—¨åœ¨è§£å†³ä»“åº“ç­‰å¤æ‚ç¯å¢ƒä¸­å¤šæ™ºèƒ½ä½“å–è´§ä¸äº¤ä»˜ (Multi-Agent Pickup and Delivery, MAPD) ä»»åŠ¡é¢ä¸´çš„å†³ç­–æŒ‘æˆ˜ã€‚é’ˆå¯¹ä¼ ç»ŸåŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨çª„é“ç­‰ç¯å¢ƒä¸­å› ä»…ä¾èµ–å±€éƒ¨è§‚å¯Ÿè€Œè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œç ”ç©¶è€…å°†å¤šæ™ºèƒ½ä½“è·¯å¾„è§„åˆ’ (Multi-Agent Path Finding, MAPF) å½¢å¼åŒ–ä¸ºåºåˆ—å»ºæ¨¡ (sequence modeling) é—®é¢˜ï¼Œå¹¶è¯æ˜äº†å…¶åœ¨ä¿è¯æœ€ä¼˜æ€§çš„åŒæ—¶å…·å¤‡é˜¶ä¸å˜æ€§ (order-invariant optimality)ã€‚SePar åˆ©ç”¨ Transformer èŒƒå¼å®ç°æ™ºèƒ½ä½“é—´çš„éšå¼ä¿¡æ¯äº¤æ¢ï¼ŒæˆåŠŸå°†å†³ç­–å¤æ‚åº¦ä»æŒ‡æ•°çº§é™ä½è‡³çº¿æ€§çº§ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„é«˜æ•ˆæ€§ä¸å…¨å±€æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSePar åœ¨å¤šç§ MAPF ä»»åŠ¡åŠå…¶å˜ä½“ä¸­çš„è¡¨ç°ä¸€è‡´ä¼˜äºç°æœ‰å­¦ä¹ æ–¹æ³•ï¼Œä¸”å…·å¤‡è‰¯å¥½çš„ç¯å¢ƒæ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†åœ¨ä»“åº“ç±»å¤æ‚åœ°å›¾ä¸­å¼•å…¥æ¨¡ä»¿å­¦ä¹  (imitation learning) å¯¹æå‡ç³»ç»Ÿæ€§èƒ½çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Preprint Under Review",
      "pdf_url": "https://arxiv.org/pdf/2509.23778v2",
      "published_date": "2025-09-28 09:48:13 UTC",
      "updated_date": "2025-09-30 12:39:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:43.788578+00:00"
    },
    {
      "arxiv_id": "2509.23773v2",
      "title": "Knowledge Homophily in Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†åŒè´¨æ€§",
      "authors": [
        "Utkarsh Sahu",
        "Zhisheng Qi",
        "Mahantesh Halappanavar",
        "Nedim Lipka",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Yu Zhang",
        "Yao Ma",
        "Yu Wang"
      ],
      "abstract": "Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„çŸ¥è¯†åŒè´¨æ€§(Knowledge Homophily)ç°è±¡ï¼Œå³æ¨¡å‹å¯¹åœ¨çŸ¥è¯†å›¾è°±ä¸­è·ç¦»è¾ƒè¿‘çš„å®ä½“å¾€å¾€å…·æœ‰ç›¸ä¼¼çš„æŒæ¡æ°´å¹³ã€‚å—è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­è¯­ä¹‰èšç±»å’Œå¯åŠ¨æ•ˆåº”çš„å¯å‘ï¼Œä½œè€…é€šè¿‡ä¸‰å…ƒç»„å’Œå®ä½“å±‚é¢çš„çŸ¥è¯†æ£€æµ‹å°†LLMçš„çŸ¥è¯†æ˜ å°„ä¸ºå›¾è¡¨ç¤ºï¼Œå‘ç°æ¨¡å‹å¯¹äºå›¾ç»“æ„ä¸­é‚»è¿‘å®ä½“çš„äº†è§£ç¨‹åº¦å­˜åœ¨æ˜¾è‘—ç›¸å…³æ€§ã€‚åŸºäºè¿™ä¸€åŸç†ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å›¾ç¥ç»ç½‘ç»œ(GNN)å›å½’æ¨¡å‹ï¼Œåˆ©ç”¨é‚»åŸŸå¾—åˆ†æ¥ä¼°è®¡ä¸‰å…ƒç»„çš„å®ä½“çº§çŸ¥è¯†æŒæ¡è¯„åˆ†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä¼˜å…ˆè¯†åˆ«å¹¶æ ¸æŸ¥æ¨¡å‹æŒæ¡è¾ƒå·®çš„çŸ¥è¯†ç‚¹ï¼Œä»è€Œåœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹å®ç°çŸ¥è¯†è¦†ç›–ç‡çš„æœ€å¤§åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æå‡äº†LLMçŸ¥è¯†æ³¨å…¥è¿‡ç¨‹ä¸­ä¸»åŠ¨æ ‡æ³¨çš„æ•ˆç‡ï¼Œè¿˜æ˜¾è‘—å¢å¼ºäº†æ¨ç†å¯†é›†å‹é—®ç­”ä»»åŠ¡ä¸­çš„å¤šè·³è·¯å¾„æ£€ç´¢èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23773v2",
      "published_date": "2025-09-28 09:40:27 UTC",
      "updated_date": "2026-01-15 18:26:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:37.387547+00:00"
    },
    {
      "arxiv_id": "2509.23769v1",
      "title": "ReLumix: Extending Image Relighting to Video via Video Diffusion Models",
      "title_zh": "ReLumixï¼šåˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹å°†å›¾åƒé‡å…‰ç…§æ‰©å±•è‡³è§†é¢‘",
      "authors": [
        "Lezhong Wang",
        "Shutong Jin",
        "Ruiqi Cui",
        "Anders Bjorholm Dahl",
        "Jeppe Revall Frisvad",
        "Siavash Bigdeli"
      ],
      "abstract": "Controlling illumination during video post-production is a crucial yet elusive goal in computational photography. Existing methods often lack flexibility, restricting users to certain relighting models. This paper introduces ReLumix, a novel framework that decouples the relighting algorithm from temporal synthesis, thereby enabling any image relighting technique to be seamlessly applied to video. Our approach reformulates video relighting into a simple yet effective two-stage process: (1) an artist relights a single reference frame using any preferred image-based technique (e.g., Diffusion Models, physics-based renderers); and (2) a fine-tuned stable video diffusion (SVD) model seamlessly propagates this target illumination throughout the sequence. To ensure temporal coherence and prevent artifacts, we introduce a gated cross-attention mechanism for smooth feature blending and a temporal bootstrapping strategy that harnesses SVD's powerful motion priors. Although trained on synthetic data, ReLumix shows competitive generalization to real-world videos. The method demonstrates significant improvements in visual fidelity, offering a scalable and versatile solution for dynamic lighting control.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ReLumixï¼Œä¸€ä¸ªæ—¨åœ¨å°†å›¾åƒé‡ç…§æ˜(Image Relighting)æ‰©å±•è‡³è§†é¢‘é¢†åŸŸçš„åˆ›æ–°æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è§†é¢‘åæœŸåˆ¶ä½œä¸­ç¼ºä¹çµæ´»æ€§å’Œé‡ç…§æ˜æ¨¡å‹å—é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒåœ¨äºå°†é‡ç…§æ˜ç®—æ³•ä¸æ—¶é—´åˆæˆ(Temporal Synthesis)è§£è€¦ï¼Œå…è®¸ç”¨æˆ·å°†ä»»ä½•å›¾åƒé‡ç…§æ˜æŠ€æœ¯æ— ç¼åº”ç”¨äºè§†é¢‘ã€‚ReLumix é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šé¦–å…ˆç”±åˆ›ä½œè€…ä½¿ç”¨é¦–é€‰çš„å›¾åƒæŠ€æœ¯å¯¹å•å¸§å‚è€ƒå›¾è¿›è¡Œé‡ç…§æ˜ï¼Œéšååˆ©ç”¨å¾®è°ƒåçš„ Stable Video Diffusion (SVD) æ¨¡å‹å°†ç›®æ ‡ç…§æ˜æ•ˆæœä¼ æ’­è‡³æ•´ä¸ªåºåˆ—ã€‚ä¸ºäº†ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§å¹¶é˜²æ­¢ä¼ªå½±ï¼Œç ”ç©¶å¼•å…¥äº†é—¨æ§äº¤å‰æ³¨æ„åŠ›æœºåˆ¶(Gated Cross-attention)å®ç°å¹³æ»‘çš„ç‰¹å¾èåˆï¼Œå¹¶ç»“åˆæ—¶é—´å¼•å¯¼ç­–ç•¥(Temporal Bootstrapping)åˆ©ç”¨ SVD çš„è¿åŠ¨å…ˆéªŒã€‚å°½ç®¡åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒï¼ŒReLumix åœ¨çœŸå®è§†é¢‘ä¸­è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰ä¿çœŸåº¦æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œä¸ºåŠ¨æ€å…‰ç…§æ§åˆ¶æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Project page: https://lez-s.github.io/Relumix_project/",
      "pdf_url": "https://arxiv.org/pdf/2509.23769v1",
      "published_date": "2025-09-28 09:35:33 UTC",
      "updated_date": "2025-09-28 09:35:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:44.990089+00:00"
    },
    {
      "arxiv_id": "2509.25271v4",
      "title": "RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration",
      "title_zh": "RADARï¼šåŸºäºè§’è‰²ä¸“ä¸šåŒ–åä½œçš„å¤§è¯­è¨€æ¨¡å‹å®‰å…¨æ€§è¯„ä¼°é£é™©æ„ŸçŸ¥åŠ¨æ€å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Xiuyuan Chen",
        "Jian Zhao",
        "Yuchen Yuan",
        "Tianle Zhang",
        "Huilin Zhou",
        "Zheng Zhu",
        "Ping Hu",
        "Linghe Kong",
        "Chi Zhang",
        "Weiran Huang",
        "Xuelong Li"
      ],
      "abstract": "Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¤§è¯­è¨€æ¨¡å‹(LLMs)å®‰å…¨è¯„ä¼°ä¸­å­˜åœ¨çš„è¯„ä¼°åè§åŠæ¨¡å‹åŒè´¨åŒ–å¯¼è‡´çš„æ£€æµ‹å¤±è´¥ç­‰å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„é£é™©è¯„ä¼°èŒƒå¼ã€‚é€šè¿‡é‡æ„æ½œåœ¨é£é™©æ¦‚å¿µç©ºé—´ï¼Œç ”ç©¶è€…å°†å…¶åˆ’åˆ†ä¸ºæ˜¾å¼é£é™©(explicit risk)ã€éšå¼é£é™©(implicit risk)å’Œéé£é™©(non-risk)ä¸‰ä¸ªäº’æ–¥å­ç©ºé—´ï¼Œå¹¶æ®æ­¤è®¾è®¡äº†åä¸ºRADARçš„å¤šæ™ºèƒ½ä½“åä½œè¯„ä¼°æ¡†æ¶ã€‚RADARåˆ©ç”¨å››ä¸ªä¸“ä¸šåŒ–è§’è‰²çš„å¤šè½®è¾©è®º(multi-round debate)æœºåˆ¶ï¼Œå¹¶ç»“åˆåŠ¨æ€æ›´æ–°æœºåˆ¶å®ç°é£é™©æ¦‚å¿µåˆ†å¸ƒçš„è‡ªæˆ‘è¿›åŒ–ï¼Œä»è€Œç¡®ä¿å¯¹æ˜¾å¼å’Œéšå¼é£é™©çš„å…¨é¢è¦†ç›–å¹¶å‡å°‘è¯„ä¼°åè§ã€‚åœ¨åŒ…å«800ä¸ªæŒ‘æˆ˜æ€§æ¡ˆä¾‹çš„è‡ªå®šä¹‰æ•°æ®é›†åŠå…¬å…±åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œé£é™©æ•æ„Ÿåº¦æ–¹é¢å‡æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRADARåœ¨é£é™©è¯†åˆ«å‡†ç¡®ç‡ä¸Šæ¯”ç›®å‰æœ€å¼ºçš„åŸºçº¿è¯„ä¼°æ–¹æ³•æå‡äº†28.87%ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨æå‡LLMå®‰å…¨æ€§è¯„ä¼°ç¨³å¥æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25271v4",
      "published_date": "2025-09-28 09:35:32 UTC",
      "updated_date": "2025-10-23 03:33:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:47.389274+00:00"
    },
    {
      "arxiv_id": "2509.23768v1",
      "title": "From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning",
      "title_zh": "ä»â€œæ˜¯ä»€ä¹ˆâ€åˆ°â€œä¸ºä»€ä¹ˆâ€ï¼šä¸€ç§ç”¨äºå¾ªè¯åŒ–å­¦ååº”æ¡ä»¶æ¨ç†çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ",
      "authors": [
        "Cheng Yang",
        "Jiaxuan Lu",
        "Haiyuan Wan",
        "Junchi Yu",
        "Feiwei Qin"
      ],
      "abstract": "The chemical reaction recommendation is to select proper reaction condition parameters for chemical reactions, which is pivotal to accelerating chemical science. With the rapid development of large language models (LLMs), there is growing interest in leveraging their reasoning and planning capabilities for reaction condition recommendation. Despite their success, existing methods rarely explain the rationale behind the recommended reaction conditions, limiting their utility in high-stakes scientific workflows. In this work, we propose ChemMAS, a multi-agent system that reframes condition prediction as an evidence-based reasoning task. ChemMAS decomposes the task into mechanistic grounding, multi-channel recall, constraint-aware agentic debate, and rationale aggregation. Each decision is backed by interpretable justifications grounded in chemical knowledge and retrieved precedents. Experiments show that ChemMAS achieves 20-35% gains over domain-specific baselines and outperforms general-purpose LLMs by 10-15% in Top-1 accuracy, while offering falsifiable, human-trustable rationales, which establishes a new paradigm for explainable AI in scientific discovery.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† ChemMASï¼Œä¸€ç§æ—¨åœ¨è§£å†³åŒ–å­¦ååº”æ¡ä»¶æ¨èä¸­ç¼ºä¹å¯è§£é‡Šæ€§é—®é¢˜çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (multi-agent system)ã€‚è¯¥ç³»ç»Ÿå°†é¢„æµ‹ä»»åŠ¡é‡æ–°å®šä¹‰ä¸ºåŸºäºè¯æ®çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºæœºåˆ¶è½åœ° (mechanistic grounding)ã€å¤šæ¸ é“å¬å› (multi-channel recall)ã€çº¦æŸæ„ŸçŸ¥æ™ºèƒ½ä½“è¾©è®º (constraint-aware agentic debate) å’Œç†ç”±èšåˆ (rationale aggregation) å››ä¸ªé˜¶æ®µã€‚æ¯ä¸ªæ¨èå†³ç­–éƒ½ç”±åŸºäºåŒ–å­¦çŸ¥è¯†å’Œæ£€ç´¢å…ˆä¾‹çš„å¯è§£é‡Šç†ç”±æ”¯æ’‘ï¼Œç¡®ä¿äº†ç§‘å­¦å·¥ä½œæµä¸­çš„å¯ä¿¡åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒChemMAS åœ¨ Top-1 accuracy ä¸Šæ¯”é¢†åŸŸç‰¹å®šåŸºçº¿æ¨¡å‹æé«˜äº† 20-35%ï¼Œå¹¶ä¼˜äºé€šç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) 10-15%ã€‚è¯¥ç ”ç©¶ä¸ä»…åœ¨é¢„æµ‹ç²¾åº¦ä¸Šå–å¾—çªç ´ï¼Œè¿˜é€šè¿‡æä¾›å¯è¯ä¼ªä¸”äººç±»å¯ä¿¡çš„æ¨ç†ä¾æ®ï¼Œä¸ºç§‘å­¦å‘ç°é¢†åŸŸçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ (explainable AI) å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23768v1",
      "published_date": "2025-09-28 09:34:35 UTC",
      "updated_date": "2025-09-28 09:34:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:29:50.995273+00:00"
    },
    {
      "arxiv_id": "2509.23767v1",
      "title": "From Personal to Collective: On the Role of Local and Global Memory in LLM Personalization",
      "title_zh": "ä»ä¸ªä½“åˆ°é›†ä½“ï¼šè®ºå±€éƒ¨ä¸å…¨å±€è®°å¿†åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–ä¸­çš„ä½œç”¨",
      "authors": [
        "Zehong Wang",
        "Junlin Wu",
        "ZHaoxuan Tan",
        "Bolian Li",
        "Xianrui Zhong",
        "Zheli Liu",
        "Qingkai Zeng"
      ],
      "abstract": "Large language model (LLM) personalization aims to tailor model behavior to individual users based on their historical interactions. However, its effectiveness is often hindered by two key challenges: the \\textit{cold-start problem}, where users with limited history provide insufficient context for accurate personalization, and the \\textit{biasing problem}, where users with abundant but skewed history cause the model to overfit to narrow preferences. We identify both issues as symptoms of a common underlying limitation, i.e., the inability to model collective knowledge across users. To address this, we propose a local-global memory framework (LoGo) that combines the personalized local memory with a collective global memory that captures shared interests across the population. To reconcile discrepancies between these two memory sources, we introduce a mediator module designed to resolve conflicts between local and global signals. Extensive experiments on multiple benchmarks demonstrate that LoGo consistently improves personalization quality by both warming up cold-start users and mitigating biased predictions. These results highlight the importance of incorporating collective knowledge to enhance LLM personalization.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ªæ€§åŒ–ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šå†·å¯åŠ¨ï¼ˆcold-start problemï¼‰å’Œåè§ï¼ˆbiasing problemï¼‰ï¼Œå³ç”¨æˆ·å†å²æ•°æ®ä¸è¶³æˆ–æ•°æ®è¿‡äºå•ä¸€å¯¼è‡´æ¨¡å‹éš¾ä»¥å‡†ç¡®æ•æ‰åå¥½ã€‚ä½œè€…æŒ‡å‡ºè¿™äº›é—®é¢˜çš„æ ¹æœ¬åŸå› åœ¨äºç°æœ‰æ–¹æ³•ç¼ºä¹å¯¹ç”¨æˆ·ç¾¤ä½“é—´é›†ä½“çŸ¥è¯†ï¼ˆcollective knowledgeï¼‰çš„å»ºæ¨¡èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº† LoGoï¼ˆlocal-global memoryï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä¸ªæ€§åŒ–çš„å±€éƒ¨è®°å¿†ï¼ˆlocal memoryï¼‰ä¸æ•æ‰ç¾¤ä½“å…±äº«å…´è¶£çš„å…¨å±€è®°å¿†ï¼ˆglobal memoryï¼‰ç›¸ç»“åˆã€‚ä¸ºäº†åè°ƒè¿™ä¸¤ç§è®°å¿†æ¥æºä¹‹é—´çš„å·®å¼‚ï¼ŒLoGo å¼•å…¥äº†ä¸€ä¸ªè°ƒè§£æ¨¡å—ï¼ˆmediator moduleï¼‰ï¼Œä¸“é—¨ç”¨äºè§£å†³å±€éƒ¨ä¸å…¨å±€ä¿¡å·ä¹‹é—´çš„å†²çªã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLoGo èƒ½å¤Ÿæ˜¾è‘—æå‡ä¸ªæ€§åŒ–è´¨é‡ï¼Œåœ¨æœ‰æ•ˆç¼“è§£å†·å¯åŠ¨é—®é¢˜çš„åŒæ—¶å‡è½»äº†åè§é¢„æµ‹ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å¼•å…¥é›†ä½“çŸ¥è¯†å¯¹äºå¢å¼º LLM ä¸ªæ€§åŒ–æ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23767v1",
      "published_date": "2025-09-28 09:32:18 UTC",
      "updated_date": "2025-09-28 09:32:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:14.694059+00:00"
    },
    {
      "arxiv_id": "2509.25270v4",
      "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions",
      "title_zh": "InfMaskingï¼šé€šè¿‡å¯¹æ¯”å¼å¤šæ¨¡æ€äº¤äº’é‡Šæ”¾ååŒä¿¡æ¯",
      "authors": [
        "Liangjian Wen",
        "Qun Dai",
        "Jianzhuang Liu",
        "Jiangtao Zheng",
        "Yong Dai",
        "Dongkai Wang",
        "Zhao Kang",
        "Jun Wang",
        "Zenglin Xu",
        "Jiang Duan"
      ],
      "abstract": "In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†InfMaskingï¼Œä¸€ç§æ—¨åœ¨é€šè¿‡å¯¹æ¯”å¤šæ¨¡æ€äº¤äº’æ¥å¢å¼ºååŒä¿¡æ¯ï¼ˆsynergistic informationï¼‰çš„è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹éš¾ä»¥æœ‰æ•ˆæ•æ‰æ¨¡æ€é—´ç‹¬ç‰¹ååŒæ¨¡å¼çš„æŒ‘æˆ˜ï¼ŒInfMaskingå¼•å…¥äº†Infinite Maskingç­–ç•¥ï¼Œåœ¨èåˆè¿‡ç¨‹ä¸­éšæœºé®è”½å„æ¨¡æ€çš„å¤§éƒ¨åˆ†ç‰¹å¾ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹ä»å¤šæ ·åŒ–çš„å±€éƒ¨ç»„åˆä¸­æå–ä¸°å¯Œçš„äº¤äº’ä¿¡æ¯ã€‚é€šè¿‡äº’ä¿¡æ¯æœ€å¤§åŒ–ï¼ˆmutual information maximizationï¼‰æŠ€æœ¯ï¼Œè¯¥æ–¹æ³•å°†å®Œæ•´èåˆè¡¨ç¤ºä¸ç»è¿‡é®è”½çš„å¤„ç†è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°å¯¹å…¨é¢ååŒä¿¡æ¯çš„æ·±åº¦ç¼–ç ã€‚ä¸ºäº†è§£å†³è®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œç ”ç©¶è€…è¿›ä¸€æ­¥æ¨å¯¼å‡ºInfMasking losså‡½æ•°æ¥è¿‘ä¼¼äº’ä¿¡æ¯ä¼°ç®—ã€‚å®éªŒè¯æ˜ï¼ŒInfMaskingæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€é—´çš„ååŒæ•ˆåº”ï¼Œå¹¶åœ¨ä¸ƒé¡¹å¤§è§„æ¨¡ç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†SOTAæ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Conference on Neural Information Processing Systems (NeurIPS) 2025 (Spotlight)",
      "pdf_url": "https://arxiv.org/pdf/2509.25270v4",
      "published_date": "2025-09-28 09:31:59 UTC",
      "updated_date": "2026-01-04 03:10:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:23.292565+00:00"
    },
    {
      "arxiv_id": "2509.23765v2",
      "title": "Knowledge-Level Consistency Reinforcement Learning: Dual-Fact Alignment for Long-Form Factuality",
      "title_zh": "çŸ¥è¯†çº§ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ ï¼šé¢å‘é•¿æ–‡æœ¬äº‹å®æ€§çš„åŒäº‹å®å¯¹é½",
      "authors": [
        "Junliang Li",
        "Yucheng Wang",
        "Yan Chen",
        "Yu Ran",
        "Ruiqing Zhang",
        "Jing Liu",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Hallucination and factuality deficits remain key obstacles to the reliability of large language models (LLMs) in long-form generation. Existing reinforcement learning from human feedback (RLHF) frameworks primarily rely on preference rewards, yet they often overlook the model's internal knowledge boundaries, exacerbating the so-called \"hallucination tax\". To address this challenge, we propose Knowledge-Level Consistency Reinforcement Learning Framework (KLCF), a novel framework that focuses on the knowledge consistency between the policy model's expressed knowledge and the base model's parametric knowledge, and introduces a Dual-Fact Alignment mechanism to jointly optimize factual recall and precision. Specifically, KLCF leverages pretrained knowledge boundaries to construct fact checklist, guiding online reinforcement learning to improve factual coverage and recall; simultaneously, it trains a self-assessment module based on the base model's internal knowledge to enhance factual precision during generation. Unlike prior methods that rely on external retrieval or heavy verification, our reward design is fully external-knowledge-free and lightweight, making KLCF efficient and easily scalable to large-scale training. Experimental results demonstrate that KLCF substantially improves factuality metrics across multiple long-form benchmarks and effectively alleviates model hallucinations.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†çŸ¥è¯†æ°´å¹³ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ (Knowledge-Level Consistency Reinforcement Learning Framework, KLCF)ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­é¢ä¸´çš„å¹»è§‰ (Hallucination) å’Œäº‹å®æ€§ä¸è¶³é—®é¢˜ã€‚è¯¥æ¡†æ¶èšç„¦äºç­–ç•¥æ¨¡å‹çš„è¡¨è¾¾çŸ¥è¯†ä¸åŸºåº§æ¨¡å‹å‚æ•°çŸ¥è¯†ä¹‹é—´çš„çŸ¥è¯†ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥äº†åŒé‡äº‹å®å¯¹é½ (Dual-Fact Alignment) æœºåˆ¶ä»¥ååŒä¼˜åŒ–äº‹å®å¬å›ç‡å’Œç²¾ç¡®åº¦ã€‚KLCF åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†è¾¹ç•Œæ„å»ºäº‹å®æ¸…å• (Fact Checklist) æ¥å¼•å¯¼åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œæå‡äº‹å®è¦†ç›–èŒƒå›´ï¼›åŒæ—¶ï¼Œå®ƒé€šè¿‡åŸºäºå†…éƒ¨çŸ¥è¯†çš„è‡ªæˆ‘è¯„ä¼°æ¨¡å—åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¢å¼ºäº‹å®ç²¾ç¡®æ€§ã€‚ä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢æˆ–ç¹é‡éªŒè¯çš„æ–¹æ³•ä¸åŒï¼ŒKLCF çš„å¥–åŠ±è®¾è®¡å®Œå…¨ä¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†ä¸”ä¿æŒè½»é‡åŒ–ï¼Œä½¿å…¶å…·æœ‰æé«˜çš„è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKLCF åœ¨å¤šä¸ªé•¿æ–‡æœ¬åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†äº‹å®æ€§æŒ‡æ ‡ï¼Œå¹¶æœ‰æ•ˆç¼“è§£äº†æ¨¡å‹å¹»è§‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23765v2",
      "published_date": "2025-09-28 09:23:06 UTC",
      "updated_date": "2025-10-11 03:51:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:27.393482+00:00"
    },
    {
      "arxiv_id": "2509.23762v3",
      "title": "Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail",
      "title_zh": "åŸºäºè„‰å†²ç¥ç»ç½‘ç»œæ¢¯åº¦ç¨€ç–è·¯å¾„çš„å‡†ç¡®ç‡ä¸é²æ£’æ€§æƒè¡¡",
      "authors": [
        "Luu Trong Nhan",
        "Luu Trung Duong",
        "Pham Ngoc Nam",
        "Truong Cong Thang"
      ],
      "abstract": "Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, (particularly for vision-related tasks) remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. Our findings offer new insights into the dual role of gradient sparsity in SNN training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è„‰å†²ç¥ç»ç½‘ç»œ(Spiking Neural Networks, SNNs)ä¸­æ¢¯åº¦ç¨€ç–æ€§(gradient sparsity)ä¸å¯¹æŠ—é²æ£’æ€§(adversarial robustness)ä¹‹é—´çš„å…³ç³»ã€‚ä½œè€…å‘ç°åœ¨ç‰¹å®šçš„æ¶æ„é…ç½®ä¸‹ï¼ŒSNNs è¡¨ç°å‡ºè‡ªç„¶çš„æ¢¯åº¦ç¨€ç–æ€§ï¼Œæ— éœ€ä»»ä½•æ˜¾å¼æ­£åˆ™åŒ–å³å¯å®ç°æœ€å…ˆè¿›çš„å¯¹æŠ—é˜²å¾¡æ€§èƒ½ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†é²æ£’æ€§ä¸æ³›åŒ–èƒ½åŠ›(generalization)ä¹‹é—´å­˜åœ¨çš„å…³é”®æƒè¡¡ï¼šè™½ç„¶ç¨€ç–æ¢¯åº¦æœ‰åŠ©äºæé«˜å¯¹æŠ—éŸ§æ€§ï¼Œä½†ä¼šå‰Šå¼±æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ç›¸åï¼Œç¨ å¯†æ¢¯åº¦è™½æœ‰åˆ©äºæ³›åŒ–ï¼Œå´å¢åŠ äº†æ¨¡å‹å—æ”»å‡»çš„è„†å¼±æ€§ã€‚è¿™äº›å‘ç°ä¸ºæ¢¯åº¦ç¨€ç–æ€§åœ¨ SNNs è®­ç»ƒä¸­çš„åŒé‡ä½œç”¨æä¾›äº†å…¨æ–°è§è§£ï¼Œä¸ºåœ¨è®¡ç®—ç¥ç»ç§‘å­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸå¹³è¡¡æ¨¡å‹æ€§èƒ½ä¸å®‰å…¨æ€§æä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "Work under peer-review",
      "pdf_url": "https://arxiv.org/pdf/2509.23762v3",
      "published_date": "2025-09-28 09:15:33 UTC",
      "updated_date": "2025-12-03 15:34:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:35.492898+00:00"
    },
    {
      "arxiv_id": "2509.23757v1",
      "title": "Transparent Visual Reasoning via Object-Centric Agent Collaboration",
      "title_zh": "åŸºäºå¯¹è±¡ä¸­å¿ƒåŒ–æ™ºèƒ½ä½“åä½œçš„é€æ˜è§†è§‰æ¨ç†",
      "authors": [
        "Benjamin Teoh",
        "Ben Glocker",
        "Francesca Toni",
        "Avinash Kori"
      ],
      "abstract": "A central challenge in explainable AI, particularly in the visual domain, is producing explanations grounded in human-understandable concepts. To tackle this, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a novel, inherently interpretable framework built on object-centric representations and a transparent multi-agent reasoning process. The game-theoretic reasoning process drives agents to agree on coherent and discriminative evidence, resulting in a faithful and interpretable decision-making process. We train OCEAN end-to-end and benchmark it against standard visual classifiers and popular posthoc explanation tools like GradCAM and LIME across two diagnostic multi-object datasets. Our results demonstrate competitive performance with respect to state-of-the-art black-box models with a faithful reasoning process, which was reflected by our user study, where participants consistently rated OCEAN's explanations as more intuitive and trustworthy.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† OCEAN (Object-Centric Explananda via Agent Negotiation)ï¼Œè¿™æ˜¯ä¸€ä¸ªå»ºç«‹åœ¨ object-centric representations å’Œé€æ˜ multi-agent åä½œæ¨ç†è¿‡ç¨‹åŸºç¡€ä¸Šçš„æ–°å‹å¯è§£é‡Šæ€§æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) ä¸­è§£é‡Šéœ€ç¬¦åˆäººç±»ç†è§£æ¦‚å¿µçš„æŒ‘æˆ˜ï¼ŒOCEAN é‡‡ç”¨ game-theoretic æ¨ç†è¿‡ç¨‹é©±åŠ¨æ™ºèƒ½ä½“å¯¹è¿è´¯ä¸”å…·è¾¨è¯†æ€§çš„è¯æ®è¾¾æˆä¸€è‡´ï¼Œä»è€Œç¡®ä¿å†³ç­–è¿‡ç¨‹çš„å¿ å®æ€§ä¸å¯è§£é‡Šæ€§ã€‚ç ”ç©¶å›¢é˜Ÿå¯¹ OCEAN è¿›è¡Œäº†ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå¹¶åœ¨ä¸¤ä¸ªè¯Šæ–­æ€§å¤šå¯¹è±¡æ•°æ®é›†ä¸Šå°†å…¶ä¸æ ‡å‡†è§†è§‰åˆ†ç±»å™¨ä»¥åŠ GradCAM å’Œ LIME ç­‰ä¸»æµ posthoc è§£é‡Šå·¥å…·è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä¸æœ€å…ˆè¿›é»‘ç›’æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ä¹‹ä½™ï¼Œæä¾›äº†æ›´å…·å¿ å®æ€§çš„æ¨ç†è¿‡ç¨‹ã€‚ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œå‚ä¸è€…ä¸€è‡´è®¤ä¸º OCEAN æä¾›çš„è§£é‡Šæ¯”ä¼ ç»Ÿå·¥å…·æ›´ç›´è§‚ä¸”æ›´å€¼å¾—ä¿¡èµ–ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23757v1",
      "published_date": "2025-09-28 09:06:52 UTC",
      "updated_date": "2025-09-28 09:06:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:36.288982+00:00"
    },
    {
      "arxiv_id": "2509.23756v1",
      "title": "SHAPoint: Task-Agnostic, Efficient, and Interpretable Point-Based Risk Scoring via Shapley Values",
      "title_zh": "SHAPointï¼šåŸºäº Shapley å€¼çš„é€šç”¨ã€é«˜æ•ˆä¸”å¯è§£é‡Šçš„ç§¯åˆ†åˆ¶é£é™©è¯„åˆ†æ–¹æ³•",
      "authors": [
        "Tomer D. Meirman",
        "Bracha Shapira",
        "Noa Dagan",
        "Lior S. Rokach"
      ],
      "abstract": "Interpretable risk scores play a vital role in clinical decision support, yet traditional methods for deriving such scores often rely on manual preprocessing, task-specific modeling, and simplified assumptions that limit their flexibility and predictive power. We present SHAPoint, a novel, task-agnostic framework that integrates the predictive accuracy of gradient boosted trees with the interpretability of point-based risk scores. SHAPoint supports classification, regression, and survival tasks, while also inheriting valuable properties from tree-based models, such as native handling of missing data and support for monotonic constraints. Compared to existing frameworks, SHAPoint offers superior flexibility, reduced reliance on manual preprocessing, and faster runtime performance. Empirical results show that SHAPoint produces compact and interpretable scores with predictive performance comparable to state-of-the-art methods, but at a fraction of the runtime, making it a powerful tool for transparent and scalable risk stratification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† SHAPointï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ä»»åŠ¡æ— å…³(task-agnostic)çš„é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨å°† gradient boosted trees çš„é«˜é¢„æµ‹å‡†ç¡®æ€§ä¸ç‚¹å€¼å‹é£é™©è¯„åˆ†(point-based risk scores)çš„é«˜å¯è§£é‡Šæ€§ç›¸ç»“åˆã€‚é€šè¿‡å¼•å…¥ Shapley Valuesï¼ŒSHAPoint èƒ½å¤ŸåŒæ—¶æ”¯æŒåˆ†ç±»ã€å›å½’åŠç”Ÿå­˜åˆ†æ(survival tasks)ç­‰å¤šç±»ä»»åŠ¡ï¼Œå¹¶å…·å¤‡åŸç”Ÿå¤„ç†ç¼ºå¤±æ•°æ®å’Œæ”¯æŒå•è°ƒçº¦æŸ(monotonic constraints)çš„èƒ½åŠ›ã€‚ç›¸æ¯”äºç°æœ‰çš„é£é™©è¯„åˆ†æ¡†æ¶ï¼ŒSHAPoint æå¤§åœ°å‡å°‘äº†å¯¹æ‰‹åŠ¨æ•°æ®é¢„å¤„ç†çš„ä¾èµ–ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„è¿è¡Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—ç¼©çŸ­è®¡ç®—è€—æ—¶çš„å‰æä¸‹ï¼Œèƒ½ç”Ÿæˆé¢„æµ‹æ€§èƒ½åª²ç¾å‰æ²¿æ–¹æ³•ä¸”ç»“æ„ç´§å‡‘çš„é£é™©è¯„åˆ†è¡¨ï¼Œä¸ºé€æ˜ä¸”å¯æ‰©å±•çš„ä¸´åºŠé£é™©åˆ†å±‚ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages inc. references for main article. 6 Figures and 7 Tables. Including Data and Code availability statements",
      "pdf_url": "https://arxiv.org/pdf/2509.23756v1",
      "published_date": "2025-09-28 09:05:19 UTC",
      "updated_date": "2025-09-28 09:05:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:37.093340+00:00"
    },
    {
      "arxiv_id": "2509.23755v1",
      "title": "Understanding Textual Capability Degradation in Speech LLMs via Parameter Importance Analysis",
      "title_zh": "é€šè¿‡å‚æ•°é‡è¦æ€§åˆ†ææ¢ç©¶è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ–‡æœ¬èƒ½åŠ›é€€åŒ–",
      "authors": [
        "Chao Wang",
        "Rui-Chen Zheng",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "abstract": "The integration of speech into Large Language Models (LLMs) has substantially expanded their capabilities, but often at the cost of weakening their core textual competence. This degradation limits the ability of speech-enabled LLMs to fully exploit their pre-trained text-based knowledge. In this work, we analyze the underlying mechanisms of this issue through a focused study of the widely used encoder-adaptor paradigm. We propose an analytical framework based on parameter importance estimation, which reveals that fine-tuning for speech introduces a textual importance distribution shift: the layer-wise allocation of parameters critical to textual reasoning is disrupted. Building on this insight, we investigate two mitigation strategies: layer-wise learning rate scheduling and Low-Rank Adaptation (LoRA), both aim to preserve the original parameter distribution. Experimental results show that both approaches better maintain textual competence than full fine-tuning, while also improving downstream spoken question answering performance. Furthermore, our analysis offers a principled explanation for the effectiveness of the proposed mitigation strategies, linking their benefits to the structural properties of textual knowledge in LLMs.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹(Speech LLMs)ä¸­ï¼Œå°†è¯­éŸ³é›†æˆåˆ°æ¨¡å‹ä¸­å¦‚ä½•å¯¼è‡´å…¶æ ¸å¿ƒæ–‡æœ¬èƒ½åŠ›é€€åŒ–(Textual Capability Degradation)çš„é—®é¢˜ã€‚ç ”ç©¶è€…é’ˆå¯¹å¹¿æ³›ä½¿ç”¨çš„ç¼–ç å™¨-é€‚é…å™¨(encoder-adaptor)èŒƒå¼ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå‚æ•°é‡è¦æ€§ä¼°è®¡(Parameter Importance Estimation)çš„åˆ†ææ¡†æ¶ã€‚åˆ†ææ­ç¤ºäº†è¯­éŸ³å¾®è°ƒä¼šå¯¼è‡´æ–‡æœ¬é‡è¦æ€§åˆ†å¸ƒåç§»(Textual Importance Distribution Shift)ï¼Œå³æ¨¡å‹ä¸­è´Ÿè´£æ–‡æœ¬æ¨ç†çš„å‚æ•°å±‚çº§åˆ†é…è¢«ç ´åã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œç ”ç©¶æ¢è®¨äº†å±‚çº§å­¦ä¹ ç‡è°ƒåº¦(Layer-wise Learning Rate Scheduling)å’Œä½ç§©è‡ªé€‚åº”(LoRA)ä¸¤ç§ç­–ç•¥ä»¥ç»´æŒåŸå§‹å‚æ•°åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§ç­–ç•¥åœ¨ä¿ç•™æ–‡æœ¬èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºå…¨é‡å¾®è°ƒ(Full Fine-tuning)ï¼ŒåŒæ—¶æå‡äº†ä¸‹æ¸¸è¯­éŸ³é—®ç­”(Spoken Question Answering)çš„è¡¨ç°ã€‚è¯¥åˆ†æä¸ºç†è§£å’Œä¼˜åŒ–Speech LLMsä¸­è¯­éŸ³ä¸æ–‡æœ¬èƒ½åŠ›çš„å¹³è¡¡æä¾›äº†ç»“æ„åŒ–çš„åˆç†è§£é‡Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23755v1",
      "published_date": "2025-09-28 09:04:40 UTC",
      "updated_date": "2025-09-28 09:04:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:46.903763+00:00"
    },
    {
      "arxiv_id": "2510.02360v2",
      "title": "Spiral of Silence in Large Language Model Agents",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸­çš„æ²‰é»˜èºæ—‹",
      "authors": [
        "Mingze Zhong",
        "Meng Fang",
        "Zijing Shi",
        "Yuxuan Huang",
        "Shunfeng Zheng",
        "Yali Du",
        "Ling Chen",
        "Jun Wang"
      ],
      "abstract": "The Spiral of Silence (SoS) theory holds that individuals with minority views often refrain from speaking out for fear of social isolation, enabling majority positions to dominate public discourse. When the 'agents' are large language models (LLMs), however, the classical psychological explanation is not directly applicable, since SoS was developed for human societies. This raises a central question: can SoS-like dynamics nevertheless emerge from purely statistical language generation in LLM collectives? We propose an evaluation framework for examining SoS in LLM agents. Specifically, we consider four controlled conditions that systematically vary the availability of 'History' and 'Persona' signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall and Spearman's rank, along with concentration measures including kurtosis and interquartile range. Experiments across open-source and closed-source models show that history and persona together produce strong majority dominance and replicate SoS patterns; history signals alone induce strong anchoring; and persona signals alone foster diverse but uncorrelated opinions, indicating that without historical anchoring, SoS dynamics cannot emerge. The work bridges computational sociology and responsible AI design, highlighting the need to monitor and mitigate emergent conformity in LLM-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ²‰é»˜èºæ—‹ (Spiral of Silence, SoS) ç†è®ºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æ™ºèƒ½ä½“é›†ä½“ä¸­çš„é€‚ç”¨æ€§ï¼Œæ—¨åœ¨éªŒè¯çº¯ç»Ÿè®¡è¯­è¨€ç”Ÿæˆæ˜¯å¦ä¼šäº§ç”Ÿç±»ä¼¼äººç±»ç¤¾ä¼šçš„ä»ä¼—åŠ¨åŠ›å­¦ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡ç³»ç»Ÿæ”¹å˜å†å²è½¨è¿¹ (History) å’Œäººæ ¼ç‰¹è´¨ (Persona) ä¿¡å·ï¼Œå¹¶åœ¨å››ç§å—æ§æ¡ä»¶ä¸‹è¯„ä¼°è§‚ç‚¹æ¼”å˜ã€‚å®éªŒé‡‡ç”¨ Mann-Kendall æ£€éªŒã€Spearman ç­‰çº§ç›¸å…³åŠå³°åº¦ (kurtosis) ç­‰æŒ‡æ ‡é‡åŒ–åˆ†æï¼Œå‘ç°åœ¨å†å²å’Œäººæ ¼ä¿¡å·å…±åŒä½œç”¨ä¸‹ï¼ŒLLM æ™ºèƒ½ä½“è¡¨ç°å‡ºæ˜¾è‘—çš„å¤šæ•°ä¸»å¯¼ç°è±¡ï¼ŒæˆåŠŸå¤åˆ¶äº† SoS æ¨¡å¼ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œä»…æä¾›å†å²ä¿¡å·ä¼šå¼•å‘å¼ºçƒˆçš„é”šå®šæ•ˆåº” (anchoring)ï¼Œè€Œç¼ºä¹å†å²é”šå®šæ—¶ SoS åŠ¨æ€æ— æ³•æ¶Œç°ã€‚è¿™ä¸€å‘ç°å¼¥åˆäº†è®¡ç®—ç¤¾ä¼šå­¦ä¸è´Ÿè´£ä»» AI è®¾è®¡ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå¼ºè°ƒäº†ç›‘æ§å¹¶ç¼“è§£ LLM ç³»ç»Ÿä¸­æ¶Œç°æ€§ä»ä¼—è¡Œä¸ºçš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2025 (Findings)",
      "pdf_url": "https://arxiv.org/pdf/2510.02360v2",
      "published_date": "2025-09-28 08:59:54 UTC",
      "updated_date": "2025-10-08 01:58:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:49.784500+00:00"
    },
    {
      "arxiv_id": "2509.23751v1",
      "title": "PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block",
      "title_zh": "PVTAdpNetï¼šåŸºäºé‡‘å­—å¡”è§†è§‰ Transformer ä¸æ–°å‹é€‚é…å™¨æ¨¡å—çš„æ¯è‚‰åˆ†å‰²",
      "authors": [
        "Arshia Yousefi Nezhad",
        "Helia Aghaei",
        "Hedieh Sajedi"
      ],
      "abstract": "Colorectal cancer ranks among the most common and deadly cancers, emphasizing the need for effective early detection and treatment. To address the limitations of traditional colonoscopy, including high miss rates due to polyp variability, we introduce the Pyramid Vision Transformer Adapter Residual Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder structure with a Pyramid Vision Transformer backbone, novel residual blocks, and adapter-based skip connections. The design enhances feature extraction, dense prediction, and gradient flow, supported by squeeze-and-excitation attention for improved channel-wise feature refinement. PVTAdpNet achieves real-time, accurate polyp segmentation, demonstrating superior performance on benchmark datasets with high mDice and mIoU scores, making it highly suitable for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851 and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's capability for real-time, accurate performance within familiar distributions. The source code of our network is available at https://github.com/ayousefinejad/PVTAdpNet.git",
      "tldr_zh": "é’ˆå¯¹ç»“ç›´è‚ ç™Œæ—©æœŸæ£€æµ‹ä¸­æ¯è‚‰å½¢æ€å¤šå˜å¯¼è‡´ä¼ ç»Ÿç»“è‚ é•œæ£€æŸ¥æ¼è¯Šç‡é«˜çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†PVTAdpNetï¼Œä¸€ç§èåˆäº†Pyramid Vision Transformeréª¨å¹²ç½‘ç»œçš„æ¯è‚‰åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç±»U-Netçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶å¼•å…¥äº†åˆ›æ–°çš„æ®‹å·®å—(residual blocks)å’ŒåŸºäºAdapterçš„è·³è·ƒè¿æ¥(adapter-based skip connections)ä»¥å¢å¼ºç‰¹å¾æå–ä¸æ¢¯åº¦æµåŠ¨ã€‚é€šè¿‡ç»“åˆæŒ¤å‹å’Œæ¿€åŠ±æ³¨æ„åŠ›(squeeze-and-excitation attention)æœºåˆ¶ï¼Œæ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–äº†é€šé“ç‰¹å¾çš„ç»†åŒ–å¤„ç†ï¼Œå®ç°äº†é«˜ç²¾åº¦çš„å¯†é›†é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPVTAdpNetåœ¨åˆ†å¸ƒå¤–(out-of-distribution)æ•°æ®é›†ä¸Šå–å¾—äº†0.8851çš„Diceç³»æ•°å’Œ0.8167çš„mIoUï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†æ¨¡å‹åœ¨PolypGenç­‰æ•°æ®é›†ä¸Šçš„å®æ—¶å¤„ç†èƒ½åŠ›ï¼Œä¹Ÿä¸ºå…¶åœ¨ä¸´åºŠç¯å¢ƒä¸‹çš„é«˜ç²¾åº¦æ¯è‚‰åˆ†å‰²åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23751v1",
      "published_date": "2025-09-28 08:55:50 UTC",
      "updated_date": "2025-09-28 08:55:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:56.068650+00:00"
    },
    {
      "arxiv_id": "2509.23746v1",
      "title": "Poivre: Self-Refining Visual Pointing with Reinforcement Learning",
      "title_zh": "Poivreï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªä¼˜åŒ–è§†è§‰æŒ‡å‘",
      "authors": [
        "Wenjie Yang",
        "Zengfeng Huang"
      ],
      "abstract": "Visual pointing, which aims to localize a target by predicting its coordinates on an image, has emerged as an important problem in the realm of vision-language models (VLMs). Despite its broad applicability, recent benchmarks show that current VLMs still fall far behind human performance on this task. A key limitation is that VLMs are typically required to complete the pointing task in a single step, akin to asking humans to point at an object without seeing their own fingers. To address this issue, we propose a simple yet effective self-refining procedure: Point, Visualize, then Refine (Poivre). This procedure enables a VLM to first mark its estimated point, then iteratively refine the coordinates if necessary. Inspired by advances of reasoning models in the natural language domain, we employ reinforcement learning (RL) to incentivize this self-refining ability. For the RL training, we design a neat process reward that is not only empirically effective but also grounded in appealing properties. Our trained model, Poivre-7B, sets a new state of the art on Point-Bench, outperforming both proprietary models such as Gemini-2.5-Pro and large open-source models such as Molmo-72B by over 3%. To support future research, we release our training and inference code, dataset, and the Poivre-7B checkpoint.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨è§†è§‰å®šä½(Visual pointing)ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºPoivreçš„è‡ªæç‚¼æ¡†æ¶ã€‚ä¼ ç»Ÿçš„VLMsé€šå¸¸è¦æ±‚åœ¨å•ä¸€æ­¥éª¤å†…å®Œæˆåæ ‡é¢„æµ‹ï¼Œè¿™é™åˆ¶äº†å…¶å®šä½ç²¾åº¦ï¼Œè€ŒPoivreå¼•å…¥äº†â€œå®šä½ã€å¯è§†åŒ–ã€å†ç²¾ç‚¼â€(Point, Visualize, then Refine)çš„è¿­ä»£è¿‡ç¨‹ï¼Œå…è®¸æ¨¡å‹åœ¨æ ‡æ³¨ä¼°è®¡ç‚¹åè¿›è¡Œå¤šæ¬¡ä¿®æ­£ã€‚å—è‡ªç„¶è¯­è¨€é¢†åŸŸæ¨ç†æ¨¡å‹çš„å¯å‘ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)æŠ€æœ¯æ¥æ¿€åŠ±è¿™ç§è‡ªæç‚¼èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºç†è®ºåŸºç¡€ä¸”è¡Œä¹‹æœ‰æ•ˆçš„è¿‡ç¨‹å¥–åŠ±(process reward)æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºè¯¥æ–¹æ³•è®­ç»ƒçš„Poivre-7Bæ¨¡å‹åœ¨Point-BenchåŸºå‡†æµ‹è¯•ä¸Šåˆ·æ–°äº†æœ€å…ˆè¿›(SOTA)è®°å½•ï¼Œæ€§èƒ½ä¼˜äºGemini-2.5-Proå’ŒMolmo-72Bç­‰ä¸“æœ‰åŠå¤§è§„æ¨¡å¼€æºæ¨¡å‹ã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„åé¦ˆè°ƒæ•´è¡Œä¸ºï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨ç²¾ç¡®ç©ºé—´å®šä½ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸ºæœªæ¥çš„è§†è§‰æ¨ç†ç ”ç©¶æä¾›äº†å¼€æºæ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23746v1",
      "published_date": "2025-09-28 08:51:47 UTC",
      "updated_date": "2025-09-28 08:51:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:30:56.362115+00:00"
    },
    {
      "arxiv_id": "2509.23745v1",
      "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation",
      "title_zh": "LocoFormerï¼šåŸºäºé•¿ä¸Šä¸‹æ–‡è‡ªé€‚åº”çš„é€šç”¨è¿åŠ¨æ§åˆ¶",
      "authors": [
        "Min Liu",
        "Deepak Pathak",
        "Ananye Agarwal"
      ],
      "abstract": "Modern locomotion controllers are manually tuned for specific embodiments. We present LocoFormer, a generalist omni-bodied locomotion model that can control previously unseen legged and wheeled robots, even without precise knowledge of their kinematics. LocoFormer is able to adapt to changes in morphology and dynamics at test time. We find that two key choices enable adaptation. First, we train massive scale RL on procedurally generated robots with aggressive domain randomization. Second, in contrast to previous policies that are myopic with short context lengths, we extend context by orders of magnitude to span episode boundaries. We deploy the same LocoFormer to varied robots and show robust control even with large disturbances such as weight change and motor failures. In extreme scenarios, we see emergent adaptation across episodes, LocoFormer learns from falls in early episodes to improve control strategies in later ones. We believe that this simple, yet general recipe can be used to train foundation models for other robotic skills in the future. Videos at generalist-locomotion.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† LocoFormerï¼Œä¸€ç§é€šç”¨å‹å…¨åœ°å½¢è¿åŠ¨æ¨¡å‹ï¼Œèƒ½å¤Ÿæ§åˆ¶ä»æœªè§è¿‡çš„è¶³å¼å’Œè½®å¼æœºå™¨äººï¼Œå³ä½¿åœ¨ç¼ºä¹ç²¾ç¡®è¿åŠ¨å­¦çŸ¥è¯†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¡†æ¶åœ¨ç¨‹åºåŒ–ç”Ÿæˆçš„æœºå™¨äººä¸Šåˆ©ç”¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹  (RL) å’Œæ¿€è¿›çš„é¢†åŸŸéšæœºåŒ– (Domain Randomization) è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¯¹ä¸åŒå½¢æ€å’ŒåŠ¨åŠ›å­¦çš„é€‚åº”ã€‚ä¸ä»¥å¾€çŸ­ä¸Šä¸‹æ–‡çš„æ§åˆ¶ç­–ç•¥ä¸åŒï¼ŒLocoFormer å°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•äº†å‡ ä¸ªæ•°é‡çº§ä»¥è·¨è¶Šå›åˆè¾¹ç•Œï¼Œä½¿å…¶èƒ½å¤Ÿä»é•¿æ—¶åºå†å²ç»éªŒä¸­å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æœºå™¨äººä¸Šå‡èƒ½å®ç°ç¨³å¥æ§åˆ¶ï¼Œå¹¶èƒ½æœ‰æ•ˆåº”å¯¹è½½è·å˜åŒ–å’Œç”µæœºæ•…éšœç­‰å¹²æ‰°ã€‚åœ¨æç«¯æƒ…å†µä¸‹ï¼ŒLocoFormer å±•ç°å‡ºè·¨å›åˆçš„æ¶Œç°é€‚åº”èƒ½åŠ› (Emergent Adaptation)ï¼Œèƒ½å¤Ÿé€šè¿‡ä»ä¹‹å‰çš„è·Œå€’ä¸­æ±²å–æ•™è®­æ¥æ”¹è¿›åç»­çš„æ§åˆ¶ç­–ç•¥ã€‚è¿™ç§ç®€å•ä¸”é€šç”¨çš„æ–¹æ³•ä¸ºæœªæ¥æ„å»ºå…¶ä»–æœºå™¨äººæŠ€èƒ½çš„åŸºç¡€æ¨¡å‹ (Foundation Models) æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to CoRL 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23745v1",
      "published_date": "2025-09-28 08:50:28 UTC",
      "updated_date": "2025-09-28 08:50:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:08.666932+00:00"
    },
    {
      "arxiv_id": "2509.23744v1",
      "title": "Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning",
      "title_zh": "ç»„åˆä¸èåˆï¼šé‡æ–°å®¡è§†å¤šæ¨¡æ€æ¨ç†çš„åŸºç¡€ç“¶é¢ˆ",
      "authors": [
        "Yucheng Wang",
        "Yifan Hou",
        "Aydin Javadov",
        "Mubashara Akhtar",
        "Mrinmaya Sachan"
      ],
      "abstract": "Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è·¨æ¨¡æ€æ¨ç†ä¸­çš„ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘è¯„ä¼°çš„æ¡†æ¶ï¼Œå°†æ¨ç†è¿‡ç¨‹ç»†åŒ–ä¸ºå…­ç§äº¤äº’æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œé¢å¤–çš„æ¨¡æ€ä»…åœ¨æä¾›ç‹¬ç«‹æ¨ç†è·¯å¾„æ—¶èƒ½å¢å¼ºæ€§èƒ½ï¼Œè€Œå†—ä½™ä¿¡æ¯å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œå¹¶è¯†åˆ«å‡ºä»»åŠ¡ç»„åˆç“¶é¢ˆ(Task-composition bottleneck)ä¸èåˆç“¶é¢ˆ(Fusion bottleneck)æ˜¯å¯¼è‡´æ¨ç†å¤±è´¥çš„æ ¸å¿ƒåŸå› ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨¡å‹éš¾ä»¥åœ¨å•æ¬¡è¿è¡Œä¸­åŒæ—¶å¤„ç†è¯†åˆ«ä¸æ¨ç†ï¼Œä¸”æ—©æœŸçš„èåˆæ–¹å¼å®¹æ˜“å¼•å…¥åå·®ï¼Œå¯¼è‡´æ³¨æ„åŠ›æ¨¡å¼(Attention patterns)æ— æ³•å‡†ç¡®æ•æ‰æœ‰æ•ˆäº‹å®ã€‚é€šè¿‡é‡‡ç”¨â€œå…ˆè¯†åˆ«å†æ¨ç†â€çš„ä¸¤æ­¥æç¤ºç­–ç•¥ä»¥åŠè½¯åŒ–æ—©æœŸèåˆä¸­çš„æ³¨æ„åŠ›ï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£ä¸Šè¿°ç“¶é¢ˆã€‚ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œå¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦éšœç¢åœ¨äºä¿¡æ¯é›†æˆ(Integration)è€Œéæ„ŸçŸ¥(Perception)ï¼Œä¸ºæœªæ¥æ¨¡å‹ä¼˜åŒ–æä¾›äº†ç»„åˆæ„ŸçŸ¥è®­ç»ƒä¸æ—©æœŸèåˆæ§åˆ¶ç­‰æ˜ç¡®æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Our code (https://github.com/DELTA-DoubleWise/OmniReason) and data (https://huggingface.co/datasets/ycwang11/OmniReason) are publicly available",
      "pdf_url": "https://arxiv.org/pdf/2509.23744v1",
      "published_date": "2025-09-28 08:46:11 UTC",
      "updated_date": "2025-09-28 08:46:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:14.666708+00:00"
    },
    {
      "arxiv_id": "2510.03276v1",
      "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
      "title_zh": "QuadEnhancerï¼šåˆ©ç”¨äºŒæ¬¡å˜æ¢å¢å¼ºæ·±åº¦ç¥ç»ç½‘ç»œ",
      "authors": [
        "Qian Chen",
        "Linxin Yang",
        "Akang Wang",
        "Xiaodong Luo",
        "Yin Zhang"
      ],
      "abstract": "The combination of linear transformations and non-linear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase nonlinearity in neural networks, with the aim of enhancing the performance of existing architectures. To reduce parameter complexity and computational complexity, we propose a lightweight quadratic enhancer that uses low-rankness, weight sharing, and sparsification techniques. For a fixed architecture, the proposed approach introduces quadratic interactions between features at every layer, while only adding negligible amounts of additional model parameters and forward computations. We conduct a set of proof-of-concept experiments for the proposed method across three tasks: image classification, text classification, and fine-tuning large-language models. In all tasks, the proposed approach demonstrates clear and substantial performance gains.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† QuadEnhancerï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼•å…¥äºŒæ¬¡å˜æ¢ (Quadratic Transformations) æ¥æå‡æ·±åº¦ç¥ç»ç½‘ç»œ (Deep Neural Networks) éçº¿æ€§è¡¨è¾¾èƒ½åŠ›çš„è½»é‡çº§å¢å¼ºå™¨ã€‚ä¸ºäº†æœ‰æ•ˆé™ä½å‚æ•°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨äº†ä½ç§©æ€§ (low-rankness)ã€æƒé‡å…±äº« (weight sharing) å’Œç¨€ç–åŒ– (sparsification) æŠ€æœ¯ï¼Œåœ¨æ¯ä¸€å±‚å®ç°äº†ç‰¹å¾é—´çš„äºŒæ¬¡äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä»…å¢åŠ æå°‘é‡é¢å¤–å‚æ•°å’Œå‰å‘è®¡ç®—é‡çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—å¢å¼ºç°æœ‰æ¶æ„çš„æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜åœ¨å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¾®è°ƒä¸‰é¡¹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤º QuadEnhancer åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åˆ©ç”¨äºŒæ¬¡å˜æ¢å¢å¼ºç¥ç»ç½‘ç»œå‡½æ•°é€¼è¿‘èƒ½åŠ›çš„æ½œåŠ›ï¼Œä¸ºä¼˜åŒ–ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”å®ç”¨çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
      "pdf_url": "https://arxiv.org/pdf/2510.03276v1",
      "published_date": "2025-09-28 08:35:31 UTC",
      "updated_date": "2025-09-28 08:35:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:18.859118+00:00"
    },
    {
      "arxiv_id": "2509.23738v1",
      "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks",
      "title_zh": "GUI-Shepherdï¼šé¢å‘é•¿åºåˆ— GUI ä»»åŠ¡çš„å¯é è¿‡ç¨‹å¥–åŠ±ä¸éªŒè¯",
      "authors": [
        "Cong Chen",
        "Kaixiang Ji",
        "Hao Zhong",
        "Muzhi Zhu",
        "Anzhou Li",
        "Guo Gan",
        "Ziyuan Huang",
        "Cheng Zou",
        "Jiajia Liu",
        "Jingdong Chen",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "abstract": "Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of $52$k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings $5.1$ points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of $2.2$ points as a reward provider and $4.3$ points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é•¿åºåˆ—å›¾å½¢ç”¨æˆ·ç•Œé¢ (GUI) ä»»åŠ¡ä¸­å­˜åœ¨çš„å¥–åŠ±ç¨€ç–å’Œä¿¡ç”¨åˆ†é…éš¾é¢˜ï¼Œæå‡ºäº† GUI-Shepherdï¼Œä¸€ç§æ—¨åœ¨æä¾›å¯†é›†ã€é€æ­¥åé¦ˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (Process Reward Model)ã€‚GUI-Shepherd åŸºäºåŒ…å« 52k æ¬¡äº¤äº’çš„å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†äººå·¥æ ‡æ³¨åˆ†æ•°å’Œ GPT-4o ç”Ÿæˆçš„æ¨ç†é€»è¾‘ï¼Œä½¿å…¶èƒ½å¤Ÿå…¼ä»»å¼ºåŒ–å­¦ä¹  (RL) çš„å¥–åŠ±æä¾›è€…ä¸æ¨ç†é˜¶æ®µçš„éªŒè¯å™¨ (Verifier)ã€‚ä½œä¸ºé¦–ä¸ªåœ¨ GUI æ™ºèƒ½ä½“ä¸­ç³»ç»Ÿæ¢è®¨è¿‡ç¨‹ç›‘ç£ (Process Supervision) çš„å·¥ä½œï¼Œå®éªŒè¯æ˜ GUI-Shepherd åœ¨ AndroidWorld åœ¨çº¿åŸºå‡†æµ‹è¯•ä¸­é€šè¿‡å¤šè½® PPO è®­ç»ƒå°†æˆåŠŸç‡æå‡äº† 7.7%ï¼Œæ˜¾è‘—ä¼˜äºåŸºäºç»“æœå¥–åŠ±æ¨¡å‹ (Outcome Reward Model) çš„åŸºå‡†ã€‚åŒæ—¶ï¼Œåœ¨ä½œä¸ºæ¨ç†éªŒè¯å™¨ä½¿ç”¨æ—¶ï¼Œè¯¥æ¨¡å‹åœ¨ AndroidWorld å’Œ AndroidControl ä»»åŠ¡ä¸­åˆ†åˆ«å¸¦æ¥äº† 5.1% å’Œ 4.3% çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶ç»“æœç¡®ç«‹äº†é«˜ä¿çœŸè¿‡ç¨‹ç›‘ç£åœ¨æ„å»ºé«˜æ•ˆ GUI æ™ºèƒ½ä½“ä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œå¹¶æä¾›äº†ä¸€ç§å…·å¤‡æ³›åŒ–èƒ½åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23738v1",
      "published_date": "2025-09-28 08:35:16 UTC",
      "updated_date": "2025-09-28 08:35:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:19.957951+00:00"
    },
    {
      "arxiv_id": "2510.00054v1",
      "title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling",
      "title_zh": "HiDeï¼šé€šè¿‡å±‚æ¬¡åŒ–è§£è€¦é‡æ–°å®¡è§†é«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ Zoom-IN æ–¹æ³•",
      "authors": [
        "Xianjie Liu",
        "Yiman Hu",
        "Yixiong Zou",
        "Liang Wu",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use \"zoom in\" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this \"zoom in\" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç†è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº†HiDeï¼ˆHierarchical Decoupling Frameworkï¼‰æ¡†æ¶ã€‚ç ”ç©¶åˆ†ææŒ‡å‡ºï¼Œé™åˆ¶é«˜åˆ†è¾¨ç‡ç†è§£çš„æ ¸å¿ƒå› ç´ æ˜¯å¤æ‚çš„èƒŒæ™¯å¹²æ‰°ï¼Œè€Œéä»…å› ç‰©ä½“å°ºå¯¸è¿‡å°å¯¼è‡´ã€‚HiDeä½œä¸ºä¸€ç§æ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰çš„æ¡†æ¶ï¼Œåˆ©ç”¨Token-wise Attention Decoupling (TAD) è¯†åˆ«å…³é”®ä¿¡æ¯Tokenå¹¶å®ç°ä¸ç›®æ ‡è§†è§‰åŒºåŸŸçš„ç²¾ç¡®å¯¹é½ã€‚éšåï¼Œè¯¥æ¡†æ¶é€šè¿‡Layout-Preserving Decoupling (LPD) å°†ç›®æ ‡åŒºåŸŸä»èƒŒæ™¯ä¸­è§£è€¦ï¼Œåœ¨æ¶ˆé™¤èƒŒæ™¯å¹²æ‰°çš„åŒæ—¶é‡æ„å¹¶ä¿ç•™äº†å¿…è¦çš„ç©ºé—´å¸ƒå±€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiDeåœ¨V*Benchã€HRBench4Kå’ŒHRBench8Kç­‰åŸºå‡†æµ‹è¯•ä¸­å‡åˆ·æ–°äº†SOTAè®°å½•ï¼Œæ˜¾è‘—æå‡äº†Qwen2.5-VLå’ŒInternVL3ç­‰æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚æ­¤å¤–ï¼Œä¼˜åŒ–åçš„HiDeåœ¨å†…å­˜å ç”¨ä¸Šæ¯”ä¹‹å‰çš„æ— éœ€è®­ç»ƒæ–¹æ³•é™ä½äº†75%ï¼Œå®ç°äº†æ€§èƒ½ä¸æ•ˆç‡çš„ååŒä¼˜åŒ–ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.00054v1",
      "published_date": "2025-09-28 08:31:48 UTC",
      "updated_date": "2025-09-28 08:31:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:40.165913+00:00"
    },
    {
      "arxiv_id": "2509.23736v1",
      "title": "HieraTok: Multi-Scale Visual Tokenizer Improves Image Reconstruction and Generation",
      "title_zh": "HieraTokï¼šå¤šå°ºåº¦è§†è§‰åˆ†è¯å™¨æå‡å›¾åƒé‡å»ºä¸ç”Ÿæˆ",
      "authors": [
        "Cong Chen",
        "Ziyuan Huang",
        "Cheng Zou",
        "Muzhi Zhu",
        "Kaixiang Ji",
        "Jiajia Liu",
        "Jingdong Chen",
        "Hao Chen",
        "Chunhua Shen"
      ],
      "abstract": "In this work, we present HieraTok, a novel multi-scale Vision Transformer (ViT)-based tokenizer that overcomes the inherent limitation of modeling single-scale representations. This is realized through two key designs: (1) multi-scale downsampling applied to the token map generated by the tokenizer encoder, producing a sequence of multi-scale tokens, and (2) a scale-causal attention mechanism that enables the progressive flow of information from low-resolution global semantic features to high-resolution structural details. Coupling these designs, HieraTok achieves significant improvements in both image reconstruction and generation tasks. Under identical settings, the multi-scale visual tokenizer outperforms its single-scale counterpart by a 27.2\\% improvement in rFID ($1.47 \\rightarrow 1.07$). When integrated into downstream generation frameworks, it achieves a $1.38\\times$ faster convergence rate and an 18.9\\% boost in gFID ($16.4 \\rightarrow 13.3$), which may be attributed to the smoother and more uniformly distributed latent space. Furthermore, by scaling up the tokenizer's training, we demonstrate its potential by a sota rFID of 0.45 and a gFID of 1.82 among ViT tokenizers. To the best of our knowledge, we are the first to introduce multi-scale ViT-based tokenizer in image reconstruction and image generation. We hope our findings and designs advance the ViT-based tokenizers in visual generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HieraTokï¼Œä¸€ç§åŸºäº Vision Transformer (ViT) çš„æ–°å‹å¤šå°ºåº¦è§†è§‰åˆ†è¯å™¨ (tokenizer)ï¼Œæ—¨åœ¨å…‹æœå•å°ºåº¦è¡¨ç¤ºå»ºæ¨¡çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹ç¼–ç å™¨ç”Ÿæˆçš„ token map è¿›è¡Œå¤šå°ºåº¦ä¸‹é‡‡æ ·ä»¥äº§ç”Ÿå¤šå°ºåº¦ token åºåˆ—ï¼Œå¹¶å¼•å…¥å°ºåº¦å› æœæ³¨æ„åŠ›æœºåˆ¶ (scale-causal attention)ï¼Œå®ç°äº†ä»ä½åˆ†è¾¨ç‡å…¨å±€è¯­ä¹‰ç‰¹å¾åˆ°é«˜åˆ†è¾¨ç‡ç»“æ„ç»†èŠ‚çš„ä¿¡æ¯é€’è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHieraTok åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­å°† rFID æå‡äº† 27.2%ï¼Œåœ¨ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº† 1.38 å€çš„æ”¶æ•›é€Ÿåº¦æå‡å’Œ 18.9% çš„ gFID ä¼˜åŒ–ï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶æ›´å¹³æ»‘ä¸”åˆ†å¸ƒæ›´å‡åŒ€çš„æ½œç©ºé—´ (latent space)ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒè§„æ¨¡ï¼ŒHieraTok åœ¨ ViT åˆ†è¯å™¨ä¸­å–å¾—äº† 0.45 çš„ rFID å’Œ 1.82 çš„ gFID ç­‰ SOTA è¡¨ç°ã€‚ä½œä¸ºé¦–ä¸ªåœ¨å›¾åƒé‡å»ºä¸ç”Ÿæˆä¸­å¼•å…¥å¤šå°ºåº¦ ViT åˆ†è¯å™¨çš„ç ”ç©¶ï¼Œè¯¥æˆæœä¸ºè§†è§‰ç”Ÿæˆé¢†åŸŸçš„æ¶æ„è®¾è®¡æä¾›äº†é‡è¦çªç ´ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23736v1",
      "published_date": "2025-09-28 08:30:26 UTC",
      "updated_date": "2025-09-28 08:30:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:39.987732+00:00"
    },
    {
      "arxiv_id": "2509.23735v1",
      "title": "Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark",
      "title_zh": "å¹³å°ç¼–æ’æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•…éšœæ ¹å› è¯Šæ–­ï¼šæ•°æ®é›†ã€åˆ†ç±»ä½“ç³»ä¸åŸºå‡†",
      "authors": [
        "Xuyan Ma",
        "Xiaofei Xie",
        "Yawen Wang",
        "Junjie Wang",
        "Boyu Wu",
        "Mingyang Li",
        "Qing Wang"
      ],
      "abstract": "Agentic systems consisting of multiple LLM-driven agents coordinating through tools and structured interactions, are increasingly deployed for complex reasoning and problem-solving tasks. At the same time, emerging low-code and template-based agent development platforms (e.g., Dify) enable users to rapidly build and orchestrate agentic systems, which we refer to as platform-orchestrated agentic systems. However, these systems are also fragile and it remains unclear how to systematically identify their potential failure root cause. This paper presents a study of root cause identification of these platform-orchestrated agentic systems. To support this initiative, we construct a dataset AgentFail containing 307 failure logs from ten agentic systems, each with fine-grained annotations linking failures to their root causes. We additionally utilize counterfactual reasoning-based repair strategy to ensure the reliability of the annotation. Building on the dataset, we develop a taxonomy that characterizes failure root causes and analyze their distribution across different platforms and task domains. Furthermore, we introduce a benchmark that leverages LLMs for automatically identifying root causes, in which we also utilize the proposed taxonomy as guidance for LLMs. Results show that the taxonomy can largely improve the performance, thereby confirming its utility. Nevertheless, the accuracy of root cause identification reaches at most 33.6%, which indicates that this task still remains challenging. In light of these results, we also provide actionable guidelines for building such agentic systems. In summary, this paper provides a reliable dataset of failure root cause for platform-orchestrated agentic systems, corresponding taxonomy and benchmark, which serves as a foundation for advancing the development of more reliable agentic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¹³å°ç¼–æ’çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆplatform-orchestrated agentic systemsï¼‰çš„è„†å¼±æ€§ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†æ•…éšœæ ¹å› è¯†åˆ«ï¼ˆroot cause identificationï¼‰é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåä¸º AgentFail çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª10ä¸ªæ™ºèƒ½ä½“ç³»ç»Ÿçš„307æ¡æ•…éšœæ—¥å¿—ï¼Œå¹¶åˆ©ç”¨åŸºäºåäº‹å®æ¨ç†ï¼ˆcounterfactual reasoningï¼‰çš„ä¿®å¤ç­–ç•¥ç¡®ä¿äº†æ ‡æ³¨ä¿¡æ¯çš„å¯é æ€§ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€å¥—åˆ»ç”»æ•…éšœæ ¹å› çš„åˆ†ç±»ä½“ç³»ï¼ˆtaxonomyï¼‰ï¼Œå¹¶åˆ†æäº†å…¶åœ¨ä¸åŒå¹³å°å’Œä»»åŠ¡é¢†åŸŸçš„åˆ†å¸ƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨è¯†åˆ«æ ¹å› çš„åŸºå‡†ï¼ˆbenchmarkï¼‰ï¼Œå¹¶è¯å®å¼•å…¥åˆ†ç±»ä½“ç³»ä½œä¸ºå¼•å¯¼èƒ½æœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ ¹å› è¯†åˆ«çš„æœ€é«˜å‡†ç¡®ç‡ä»…ä¸º33.6%ï¼Œåæ˜ å‡ºè¯¥ä»»åŠ¡åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸ºæ„å»ºæ›´å¯é çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº† actionable guidelines ä»¥åŠåšå®çš„æ•°æ®ã€åˆ†ç±»æ³•ä¸è¯„ä¼°åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23735v1",
      "published_date": "2025-09-28 08:30:03 UTC",
      "updated_date": "2025-09-28 08:30:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:31.695759+00:00"
    },
    {
      "arxiv_id": "2509.23730v1",
      "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance",
      "title_zh": "EAPOï¼šåˆ©ç”¨æŒ‰éœ€ä¸“å®¶è¾…åŠ©å¢å¼ºç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Siyao Song",
        "Cong Ma",
        "Zhihao Cheng",
        "Shiye Lei",
        "Minghao Li",
        "Ying Zeng",
        "Huaixiao Tou",
        "Kai Jia"
      ],
      "abstract": "Large language models (LLMs) have recently advanced in reasoning when optimized with reinforcement learning (RL) under verifiable rewards. Existing methods primarily rely on outcome-based supervision to strengthen internal LLM reasoning, often leading to inefficient exploration and sparse rewards. To mitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a novel RL framework that enhances exploration by incorporating multi-turn interactions with external experts during training. Unlike prior methods, where policies reason in isolation, EAPO incentivizes the policy to adaptively determine when and how to consult experts, yielding richer reward signals and more reliable reasoning trajectories. External assistance ultimately internalizes expert knowledge into the policy model, amplifying the model's inherent reasoning capabilities. During evaluation, the policy model has been well-optimized to solve questions independently, producing improved reasoning paths and more accurate solutions. Experiments on mathematical reasoning benchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO consistently outperforms expert-assisted workflow, expert-distilled models, and RL baselines, with an average gain of 5 points over self-exploratory models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Expert-Assisted Policy Optimization (EAPO)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¼ºåŒ–å­¦ä¹ (RL)è¿‡ç¨‹ä¸­å› ä»…ä¾èµ–ç»“æœç›‘ç£è€Œå¯¼è‡´çš„æ¢ç´¢æ•ˆç‡ä½ä¸‹å’Œå¥–åŠ±ç¨€ç–é—®é¢˜ã€‚ä¸ä¼ ç»Ÿå­¤ç«‹æ¨ç†çš„è®­ç»ƒæ–¹æ³•ä¸åŒï¼ŒEAPOé€šè¿‡åœ¨è®­ç»ƒä¸­å¼•å…¥ä¸å¤–éƒ¨ä¸“å®¶çš„å¤šè½®äº¤äº’ï¼Œå¹¶æ¿€åŠ±ç­–ç•¥æ¨¡å‹è‡ªä¸»å†³å®šå’¨è¯¢ä¸“å®¶çš„æ—¶æœºä¸æ–¹å¼ï¼Œä»è€Œè·å–æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·å’Œæ›´å¯é çš„æ¨ç†è½¨è¿¹ã€‚è¿™ç§æœºåˆ¶ä¿ƒä½¿å¤–éƒ¨ä¸“å®¶çŸ¥è¯†å†…åŒ–åˆ°ç­–ç•¥æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å†…åœ¨æ¨ç†èƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹åœ¨è¯„ä¼°é˜¶æ®µèƒ½å¤Ÿç‹¬ç«‹ç”Ÿæˆæ”¹è¿›çš„æ¨ç†è·¯å¾„å¹¶ç»™å‡ºæ›´å‡†ç¡®çš„è§£ç­”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨AIME 2024ã€AIME 2025å’ŒAIMO 2025ç­‰æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEAPOçš„è¡¨ç°ä¸€è‡´ä¼˜äºä¸“å®¶è¾…åŠ©å·¥ä½œæµã€ä¸“å®¶è’¸é¦æ¨¡å‹ä»¥åŠä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚ç›¸æ¯”äºè‡ªæ¢ç´¢æ¨¡å‹ï¼ŒEAPOåœ¨ç›¸å…³æµ‹è¯•ä¸­å–å¾—äº†å¹³å‡5åˆ†çš„æ˜¾è‘—æå‡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23730v1",
      "published_date": "2025-09-28 08:20:22 UTC",
      "updated_date": "2025-09-28 08:20:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:32.085722+00:00"
    },
    {
      "arxiv_id": "2509.23729v2",
      "title": "LUQ: Layerwise Ultra-Low Bit Quantization for Multimodal Large Language Models",
      "title_zh": "LUQï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„é€å±‚è¶…ä½æ¯”ç‰¹é‡åŒ–",
      "authors": [
        "Shubhang Bhatnagar",
        "Andy Xu",
        "Kar-Han Tan",
        "Narendra Ahuja"
      ],
      "abstract": "Large Language Models (LLMs) with multimodal capabilities have revolutionized vision-language tasks, but their deployment often requires huge memory and computational resources. While post-training quantization (PTQ) has successfully compressed language models to as low as 1-bit precision without significant performance loss, its effectiveness for multimodal LLMs (MLLMs) remains relatively unexplored. In this paper, we present the first study on ultra-low bit (<4-bit) quantization for multimodal LLMs. Our analysis reveals that multimodal tokens and intermediate layer activations produced by them exhibit significantly higher statistical variance and entropy compared to text tokens, making them less tolerant to ultra-low bit quantization. However, the activation distributions of multimodal tokens varies significantly over different layers, with some layers having lower entropy activation distributions. We empirically show that such layers in these models can better tolerate ultra-low bit quantization. Building on these insights, we propose a novel strategy for MLLM quantization, LUQ: Layerwise Ultra-Low Bit Quantization, which selectively applies ultra-low bit quantization to layers that are more resilient to it. Additionally, we also show that using a mix of multimodal tokens (image and text) for PTQ boosts VQA performance in the ultra-low bit regime. We evaluate our method on LLaVA-1.5 and Qwen-2.5-VL across 9 popular VQA benchmarks. The resulting LUQ models use 40% and 31% less memory than their 4-bit counterparts, respectively, while exhibiting a performance degradation of less than 10% on the MME benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) çš„è¶…ä½ä½å®½é‡åŒ–é—®é¢˜ï¼ŒæŒ‡å‡ºè™½ç„¶é‡åŒ–æŠ€æœ¯å·²åœ¨çº¯æ–‡æœ¬å¤§æ¨¡å‹ä¸­å–å¾—æˆåŠŸï¼Œä½†åœ¨ MLLMs ä¸Šçš„åº”ç”¨ä»å¾…å¼€å‘ã€‚åˆ†æå‘ç°ï¼Œå¤šæ¨¡æ€ Token åŠå…¶æ¿€æ´»å€¼æ¯”çº¯æ–‡æœ¬å…·æœ‰æ›´é«˜çš„ç»Ÿè®¡æ–¹å·®å’Œç†µï¼Œä½†ä¸åŒå±‚çº§å¯¹é‡åŒ–çš„è€å—æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œä½œè€…æå‡ºäº† LUQ (Layerwise Ultra-Low Bit Quantization) ç­–ç•¥ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°å¯¹é«˜è€å—æ€§å±‚åº”ç”¨è¶…ä½ä½å®½é‡åŒ–ï¼Œå¹¶ç»“åˆå›¾æ–‡æ··åˆ Token ä¼˜åŒ–è®­ç»ƒåé‡åŒ– (PTQ) è¿‡ç¨‹ã€‚å®éªŒåœ¨ LLaVA-1.5 å’Œ Qwen-2.5-VL ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒLUQ æ¨¡å‹æ¯” 4-bit åŸºå‡†æ¨¡å‹å‡å°‘äº†çº¦ 31% è‡³ 40% çš„æ˜¾å­˜å ç”¨ã€‚åœ¨ 9 é¡¹ VQA åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒæ˜¾è‘—å‹ç¼©æ¯”çš„åŒæ—¶ï¼Œå°† MME åŸºå‡†æ€§èƒ½æŸå¤±æ§åˆ¶åœ¨ 10% ä»¥å†…ï¼Œè¯æ˜äº†å…¶åœ¨æ¨¡å‹å‹ç¼©ä¸æ€§èƒ½ç»´æŒä¹‹é—´çš„æœ‰æ•ˆå¹³è¡¡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23729v2",
      "published_date": "2025-09-28 08:20:00 UTC",
      "updated_date": "2025-10-26 06:17:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:38.396680+00:00"
    },
    {
      "arxiv_id": "2509.23728v2",
      "title": "M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation",
      "title_zh": "M3DLayoutï¼šé¢å‘ 3D ç”Ÿæˆçš„ 3D å®¤å†…å¸ƒå±€ä¸ç»“æ„åŒ–æè¿°å¤šæºæ•°æ®é›†",
      "authors": [
        "Yiheng Zhang",
        "Zhuojiang Cai",
        "Mingdao Wang",
        "Meitong Guo",
        "Tianxiao Li",
        "Li Lin",
        "Yuwang Wang"
      ],
      "abstract": "In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 21,367 layouts and over 433k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using both a text-conditioned diffusion model and a text-conditioned autoregressive model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis. All dataset and code will be made public upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†M3DLayoutï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3Då®¤å†…å¸ƒå±€ç”Ÿæˆçš„å¤§è§„æ¨¡å¤šæºæ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡ã€å¤šæ ·æ€§å’Œæ ‡æ³¨è´¨é‡æ–¹é¢çš„å±€é™ã€‚è¯¥æ•°æ®é›†æ•´åˆäº†çœŸå®ä¸–ç•Œæ‰«æã€ä¸“ä¸šCADè®¾è®¡å’Œç¨‹åºåŒ–ç”Ÿæˆçš„åœºæ™¯ï¼Œå…±åŒ…å«21,367ä¸ªå¸ƒå±€åŠè¶…è¿‡43.3ä¸‡ä¸ªç‰©ä½“å®ä¾‹ã€‚æ¯ä¸ªå¸ƒå±€å‡é…æœ‰è¯¦ç»†çš„ç»“æ„åŒ–æ–‡æœ¬ï¼Œæ¶µç›–äº†åœºæ™¯å…¨å±€æ‘˜è¦ã€å¤§å‹å®¶å…·çš„å…³ç³»æ‘†æ”¾ä»¥åŠå°å‹ç‰©å“çš„ç»†ç²’åº¦æ’åˆ—ã€‚ç ”ç©¶è€…é€šè¿‡æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹(Diffusion Model)å’Œè‡ªå›å½’æ¨¡å‹(Autoregressive Model)å»ºç«‹äº†åŸºå‡†æµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥æ•°æ®é›†åœ¨è®­ç»ƒå¸ƒå±€ç”Ÿæˆæ¨¡å‹æ–¹é¢çš„æ½œåŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒM3DLayoutçš„å¤šæºç»„æˆæ˜¾è‘—æå‡äº†ç”Ÿæˆåœºæ™¯çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œç‰¹åˆ«æ˜¯å…¶ä¸­çš„Inf3DLayoutå­é›†ä¸ºç”ŸæˆåŒ…å«ä¸°å¯Œå°ç‰©ä½“ä¿¡æ¯çš„ç²¾ç»†åœºæ™¯æä¾›äº†æœ‰åŠ›æ”¯æŒï¼Œä¸ºæ–‡æœ¬é©±åŠ¨çš„3Dåœºæ™¯åˆæˆç ”ç©¶æä¾›äº†é‡è¦èµ„æºã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "https://graphic-kiliani.github.io/M3DLayout/",
      "pdf_url": "https://arxiv.org/pdf/2509.23728v2",
      "published_date": "2025-09-28 08:16:08 UTC",
      "updated_date": "2025-11-29 05:27:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:31:47.495403+00:00"
    },
    {
      "arxiv_id": "2509.23727v1",
      "title": "AudioMoG: Guiding Audio Generation with Mixture-of-Guidance",
      "title_zh": "AudioMoGï¼šåŸºäºæ··åˆå¼•å¯¼çš„éŸ³é¢‘ç”Ÿæˆ",
      "authors": [
        "Junyou Wang",
        "Zehua Chen",
        "Binjie Yuan",
        "Kaiwen Zheng",
        "Chang Li",
        "Yuxuan Jiang",
        "Jun Zhu"
      ],
      "abstract": "Guidance methods have demonstrated significant improvements in cross-modal audio generation, including text-to-audio (T2A) and video-to-audio (V2A) generation. The popularly adopted method, classifier-free guidance (CFG), steers generation by emphasizing condition alignment, enhancing fidelity but often at the cost of diversity. Recently, autoguidance (AG) has been explored for audio generation, encouraging the sampling to faithfully reconstruct the target distribution and showing increased diversity. Despite these advances, they usually rely on a single guiding principle, e.g., condition alignment in CFG or score accuracy in AG, leaving the full potential of guidance for audio generation untapped. In this work, we explore enriching the composition of the guidance method and present a mixture-of-guidance framework, AudioMoG. Within the design space, AudioMoG can exploit the complementary advantages of distinctive guiding principles by fulfilling their cumulative benefits. With a reduced form, AudioMoG can consider parallel complements or recover a single guiding principle, without sacrificing generality. We experimentally show that, given the same inference speed, AudioMoG approach consistently outperforms single guidance in T2A generation across sampling steps, concurrently showing advantages in V2A, text-to-music, and image generation. These results highlight a \"free lunch\" in current cross-modal audio generation systems: higher quality can be achieved through mixed guiding principles at the sampling stage without sacrificing inference efficiency. Demo samples are available at: https://audio-mog.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AudioMoGï¼Œä¸€ç§æ··åˆå¼•å¯¼(Mixture-of-Guidance)æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ–‡æœ¬ç”ŸæˆéŸ³é¢‘(T2A)å’Œè§†é¢‘ç”ŸæˆéŸ³é¢‘(V2A)ç­‰è·¨æ¨¡æ€ä»»åŠ¡çš„ç”Ÿæˆè´¨é‡ã€‚ä¼ ç»Ÿçš„åˆ†ç±»å™¨å…å¼•å¯¼(Classifier-Free Guidance, CFG)è™½èƒ½å¢å¼ºä¿çœŸåº¦ä½†å¾€å¾€ä»¥ç‰ºç‰²å¤šæ ·æ€§ä¸ºä»£ä»·ï¼Œè€Œè‡ªåŠ¨å¼•å¯¼(Autoguidance, AG)è™½èƒ½æé«˜å¤šæ ·æ€§å´å—é™äºå•ä¸€å¼•å¯¼åŸåˆ™ã€‚AudioMoGé€šè¿‡åœ¨è®¾è®¡ç©ºé—´å†…èåˆå¤šç§äº’è¡¥çš„å¼•å¯¼åŸåˆ™ï¼Œèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å„æ–¹æ³•çš„ç´¯ç§¯ä¼˜åŠ¿ï¼Œä¸”åœ¨ç®€åŒ–å½¢å¼ä¸‹å¯æ¢å¤ä¸ºå•ä¸€å¼•å¯¼è€Œä¸å¤±é€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ¨ç†é€Ÿåº¦ä¸‹ï¼ŒAudioMoGåœ¨æ–‡æœ¬ç”ŸæˆéŸ³é¢‘ã€è§†é¢‘ç”ŸæˆéŸ³é¢‘ã€æ–‡æœ¬ç”ŸæˆéŸ³ä¹åŠå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å‡ä¸€è‡´ä¼˜äºå•ä¸€å¼•å¯¼æ–¹æ³•ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰ç”Ÿæˆç³»ç»Ÿé‡‡æ ·é˜¶æ®µçš„ä¸€ä¸ªâ€œå…è´¹åˆé¤â€ï¼Œå³é€šè¿‡æ··åˆå¼•å¯¼åŸåˆ™å¯ä»¥åœ¨ä¸é™ä½æ•ˆç‡çš„æƒ…å†µä¸‹è·å¾—æ›´é«˜è´¨é‡çš„ç”Ÿæˆç»“æœã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23727v1",
      "published_date": "2025-09-28 08:12:43 UTC",
      "updated_date": "2025-09-28 08:12:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:10.090789+00:00"
    },
    {
      "arxiv_id": "2509.23725v2",
      "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models",
      "title_zh": "MedLAï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤æ‚åŒ»å­¦æ¨ç†é€»è¾‘é©±åŠ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Siqi Ma",
        "Jiajie Huang",
        "Fan Zhang",
        "Jinlin Wu",
        "Yue Shen",
        "Guohui Fan",
        "Zhu Zhang",
        "Zelin Zang"
      ],
      "abstract": "Answering complex medical questions requires not only domain expertise and patient-specific information, but also structured and multi-perspective reasoning. Existing multi-agent approaches often rely on fixed roles or shallow interaction prompts, limiting their ability to detect and resolve fine-grained logical inconsistencies. To address this, we propose \\textsc{MedLA}, a logic-driven multi-agent framework built on large language models. Each agent organizes its reasoning process into an explicit logical tree based on syllogistic triads (major premise, minor premise, and conclusion), enabling transparent inference and premise-level alignment. Agents engage in a multi-round, graph-guided discussion to compare and iteratively refine their logic trees, achieving consensus through error correction and contradiction resolution. We demonstrate that \\textsc{MedLA} consistently outperforms both static role-based systems and single-agent baselines on challenging benchmarks such as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA} scales effectively across both open-source and commercial LLM backbones, achieving state-of-the-art performance and offering a generalizable paradigm for trustworthy medical reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MedLAï¼Œä¸€ç§ä¸“ä¸ºå¤æ‚åŒ»ç–—æ¨ç†è®¾è®¡çš„é€»è¾‘é©±åŠ¨å¤šæ™ºèƒ½ä½“æ¡†æ¶ (Logic-Driven Multi-Agent Framework)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç»†ç²’åº¦é€»è¾‘ä¸ä¸€è‡´æ—¶é¢ä¸´çš„å±€é™ã€‚åœ¨ MedLA ä¸­ï¼Œæ¯ä¸ªæ™ºèƒ½ä½“å°†å…¶æ¨ç†è¿‡ç¨‹æ„å»ºä¸ºåŸºäºä¸‰æ®µè®º (Syllogistic Triads) çš„æ˜¾æ€§é€»è¾‘æ ‘ï¼Œé€šè¿‡æ˜ç¡®å¤§å‰æ (Major Premise)ã€å°å‰æ (Minor Premise) å’Œç»“è®ºæ¥å®ç°é€æ˜çš„æ¨ç†åŠå‰æçº§å¯¹é½ã€‚æ™ºèƒ½ä½“é€šè¿‡å¤šè½®å›¾å¼•å¯¼è®¨è®º (Graph-guided Discussion) æ¯”è¾ƒå¹¶è¿­ä»£ä¼˜åŒ–é€»è¾‘æ ‘ï¼Œåˆ©ç”¨é”™è¯¯çº æ­£å’ŒçŸ›ç›¾æ¶ˆé™¤æœºåˆ¶è¾¾æˆå…±è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒMedLA åœ¨ MedDDx å’Œæ ‡å‡†åŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸ŠæŒç»­ä¼˜äºé™æ€è§’è‰²ç³»ç»ŸåŠå•æ™ºèƒ½ä½“åŸºçº¿ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶åœ¨å¤šç§å¼€æºåŠå•†ä¸š LLM åç«¯ä¸Šå‡è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼Œä¸ºæ„å»ºå¯ä¿¡ä¸”é€šç”¨çš„åŒ»ç–—æ¨ç†èŒƒå¼æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "accepted by AAAI-26 (ORAL)",
      "pdf_url": "https://arxiv.org/pdf/2509.23725v2",
      "published_date": "2025-09-28 08:06:39 UTC",
      "updated_date": "2025-11-19 03:32:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:13.487217+00:00"
    },
    {
      "arxiv_id": "2509.23724v1",
      "title": "Video Panels for Long Video Understanding",
      "title_zh": "Video Panelsï¼šé¢å‘é•¿è§†é¢‘ç†è§£çš„è§†é¢‘é¢æ¿",
      "authors": [
        "Lars Doorenbos",
        "Federico Spurio",
        "Juergen Gall"
      ],
      "abstract": "Recent Video-Language Models (VLMs) achieve promising results on long-video understanding, but their performance still lags behind that achieved on tasks involving images or short videos. This has led to great interest in improving the long context modeling of VLMs by introducing novel modules and additional complexity. % additional training time. In this paper, we take a different approach: rather than fine-tuning VLMs with the limited data available, we attempt to maximize the performance of existing models. To this end, we propose a novel visual prompting strategy specifically designed for long-video understanding. By combining multiple frames as panels into one image, we effectively trade off spatial details for temporal resolution. Our approach is training-free, parameter-free, and model-agnostic, and can be seamlessly integrated into existing VLMs. Extensive experiments on five established benchmarks across a wide range of model architectures, sizes, and context windows confirm the consistency of our approach. For the TimeScope (Long) dataset, which has the longest videos, the accuracy for video question answering is improved by up to 19.4\\%. Overall, our method raises the bar for long video understanding models. We will make our code available upon acceptance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) åœ¨é•¿è§†é¢‘ç†è§£ä¸Šè¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Video Panels çš„åˆ›æ–°è§†è§‰æç¤ºç­–ç•¥ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¤šä¸ªè§†é¢‘å¸§ä»¥é¢æ¿ (Panels) å½¢å¼ç»„åˆè¿›å•ä¸ªå›¾åƒä¸­ï¼Œåœ¨ç©ºé—´ç»†èŠ‚ä¸æ—¶é—´åˆ†è¾¨ç‡ä¹‹é—´å–å¾—äº†æœ‰æ•ˆæƒè¡¡ï¼Œä»è€Œå……åˆ†æŒ–æ˜ç°æœ‰æ¨¡å‹çš„æ½œåŠ›ã€‚è¯¥æ–¹æ¡ˆå…·æœ‰å…è®­ç»ƒ (Training-free)ã€æ— å‚æ•° (Parameter-free) ä¸”æ¨¡å‹æ— å…³ (Model-agnostic) çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆè‡³å„ç±»ç°æœ‰æ¶æ„ä¸­ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†æé•¿è§†é¢‘çš„ TimeScope (Long) ä»»åŠ¡ä¸­å°†è§†é¢‘é—®ç­”å‡†ç¡®ç‡æå‡äº†é«˜è¾¾ 19.4%ï¼Œæ˜¾è‘—åˆ·æ–°äº†é•¿è§†é¢‘ç†è§£çš„æ€§èƒ½åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23724v1",
      "published_date": "2025-09-28 08:05:55 UTC",
      "updated_date": "2025-09-28 08:05:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:23.291081+00:00"
    },
    {
      "arxiv_id": "2509.25268v1",
      "title": "A Weather Foundation Model for the Power Grid",
      "title_zh": "é¢å‘ç”µç½‘çš„æ°”è±¡åŸºåº§æ¨¡å‹",
      "authors": [
        "Cristian Bodnar",
        "RaphaÃ«l Rousseau-Rizzi",
        "Nikhil Shankar",
        "James Merleau",
        "Stylianos Flampouris",
        "Guillem Candille",
        "Slavica Antic",
        "FranÃ§ois Miralles",
        "Jayesh K. Gupta"
      ],
      "abstract": "Weather foundation models (WFMs) have recently set new benchmarks in global forecast skill, yet their concrete value for the weather-sensitive infrastructure that powers modern society remains largely unexplored. In this study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting Transformer (GFT), on a rich archive of Hydro-QuÃ©bec asset observations--including transmission-line weather stations, wind-farm met-mast streams, and icing sensors--to deliver hyper-local, asset-level forecasts for five grid-critical variables: surface temperature, precipitation, hub-height wind speed, wind-turbine icing risk, and rime-ice accretion on overhead conductors. Across 6-72 h lead times, the tailored model surpasses state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE) by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%. Most importantly, it attains an average precision score of 0.72 for day-ahead rime-ice detection, a capability absent from existing operational systems, which affords several hours of actionable warning for potentially catastrophic outage events. These results show that WFMs, when post-trained with small amounts of high-fidelity, can serve as a practical foundation for next-generation grid-resilience intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µåŠ›åŸºç¡€è®¾æ–½çš„å¤©æ°”æ•æ„Ÿæ€§ï¼Œæå‡ºäº†ä¸€ä¸ªä¸“ç”¨çš„ Weather Foundation Model (WFM) æ¡†æ¶ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¾®è°ƒæ‹¥æœ‰ 1.5B å‚æ•°çš„ Generative Forecasting Transformer (GFT)ï¼Œå¹¶ç»“åˆäº† Hydro-QuÃ©bec çš„è¾“ç”µçº¿è·¯æ°”è±¡ç«™ã€æµ‹é£å¡”åŠè¦†å†°ä¼ æ„Ÿå™¨ç­‰é«˜ä¿çœŸèµ„äº§è§‚æµ‹æ•°æ®ã€‚è¯¥æ¨¡å‹å®ç°äº†å¯¹åœ°è¡¨æ¸©åº¦ã€æ€»é™æ°´é‡ã€é£é€Ÿä»¥åŠå…³é”®çš„ icing risk å’Œ rime-ice accretion çš„èµ„äº§çº§ç²¾å‡†é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ 6-72 å°æ—¶é¢„æŠ¥æ—¶æ•ˆå†…ï¼Œè¯¥æ¨¡å‹è¡¨ç°ä¼˜äº SOTA çš„ NWP åŸºå‡†ï¼Œå°†æ¸©åº¦ã€é™æ°´å’Œé£é€Ÿçš„ MAE åˆ†åˆ«é™ä½äº† 15%ã€35% å’Œ 15%ã€‚ç‰¹åˆ«æ˜¯åœ¨ day-ahead rime-ice detection ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹è¾¾åˆ°äº† 0.72 çš„ average precision scoreï¼Œä¸ºé˜²èŒƒç¾éš¾æ€§åœç”µæä¾›äº†æ•°å°æ—¶çš„æœ‰æ•ˆé¢„è­¦ã€‚è¯¥ç ”ç©¶è¯æ˜ï¼Œåˆ©ç”¨å°‘é‡é«˜ä¿çœŸæ•°æ®è¿›è¡Œåè®­ç»ƒçš„ WFMs èƒ½å¤Ÿä½œä¸ºæ„å»ºä¸‹ä¸€ä»£ grid-resilience intelligence çš„å®ç”¨åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "31 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25268v1",
      "published_date": "2025-09-28 08:05:46 UTC",
      "updated_date": "2025-09-28 08:05:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:35.282544+00:00"
    },
    {
      "arxiv_id": "2509.23722v1",
      "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models",
      "title_zh": "AdaPtisï¼šé€šè¿‡é¢å‘å¼‚æ„æ¨¡å‹çš„è‡ªé€‚åº”æµæ°´çº¿å¹¶è¡Œå‡å°‘æµæ°´çº¿æ°”æ³¡",
      "authors": [
        "Jihu Guo",
        "Tenghui Ma",
        "Wei Gao",
        "Peng Sun",
        "Jiaxing Li",
        "Xun Chen",
        "Yuyang Jin",
        "Dahua Lin"
      ],
      "abstract": "Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization of model partition, model placement, and workload scheduling, resulting in limited efficiency improvement or even performance degradation. To respond, we propose AdaPtis, an LLM training system that supports adaptive pipeline parallelism. First, we develop a pipeline performance model to accurately estimate training throughput. Second, AdaPtis jointly optimizes model partition, model placement, and workload scheduling policies guided by this performance model. Third, we design a unified pipeline executor that efficiently supports the execution of diverse pipeline strategies. Extensive experiments show that AdaPtis achieves an average speedup of 1.42x (up to 2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) è®­ç»ƒä¸­æµæ°´çº¿å¹¶è¡Œ (Pipeline parallelism) å› æ¨¡å‹æ¶æ„å¼‚æ„æ€§å¯¼è‡´æµæ°´çº¿æ°”æ³¡ (Pipeline bubbles) å¹¶é™ä½æ•ˆç‡çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”æµæ°´çº¿å¹¶è¡Œç³»ç»Ÿ AdaPtisã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨æ¨¡å‹åˆ’åˆ† (Model partition)ã€æ¨¡å‹éƒ¨ç½² (Model placement) ä¸å·¥ä½œè´Ÿè½½è°ƒåº¦ (Workload scheduling) ååŒä¼˜åŒ–ä¸Šçš„ä¸è¶³ï¼ŒAdaPtis å¼€å‘äº†ä¸“é—¨çš„æµæ°´çº¿æ€§èƒ½æ¨¡å‹ä»¥ç²¾ç¡®ä¼°ç®—è®­ç»ƒååé‡ã€‚åœ¨è¯¥æ€§èƒ½æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼ŒAdaPtis å®ç°äº†å¯¹æ¨¡å‹åˆ’åˆ†ã€éƒ¨ç½²å’Œè°ƒåº¦ç­–ç•¥çš„è”åˆä¼˜åŒ–ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€çš„æµæ°´çº¿æ‰§è¡Œå™¨ (Unified pipeline executor) ä»¥é«˜æ•ˆæ”¯æŒå¤šæ ·åŒ–ç­–ç•¥çš„æ‰§è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¸åŒæ¶æ„å’Œè§„æ¨¡çš„ LLM è®­ç»ƒä¸­ï¼ŒAdaPtis ç›¸æ¯” Megatron-LM I-1F1B å®ç°äº†å¹³å‡ 1.42 å€ã€æœ€é«˜ 2.14 å€çš„åŠ é€Ÿã€‚è¯¥ç ”ç©¶é€šè¿‡è‡ªé€‚åº”æœºåˆ¶æ˜¾è‘—æå‡äº†å¼‚æ„æ¨¡å‹ç¯å¢ƒä¸‹çš„å¹¶è¡Œè®­ç»ƒæ•ˆç‡ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "13 pages, 15 Figures; Under Review;",
      "pdf_url": "https://arxiv.org/pdf/2509.23722v1",
      "published_date": "2025-09-28 08:05:13 UTC",
      "updated_date": "2025-09-28 08:05:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:26.284862+00:00"
    },
    {
      "arxiv_id": "2509.23717v1",
      "title": "Measuring Sparse Autoencoder Feature Sensitivity",
      "title_zh": "è¡¡é‡ç¨€ç–è‡ªç¼–ç å™¨ç‰¹å¾çš„æ•æ„Ÿæ€§",
      "authors": [
        "Claire Tian",
        "Katherine Tian",
        "Nathan Hu"
      ],
      "abstract": "Sparse Autoencoder (SAE) features have become essential tools for mechanistic interpretability research. SAE features are typically characterized by examining their activating examples, which are often \"monosemantic\" and align with human interpretable concepts. However, these examples don't reveal feature sensitivity: how reliably a feature activates on texts similar to its activating examples. In this work, we develop a scalable method to evaluate feature sensitivity. Our approach avoids the need to generate natural language descriptions for features; instead we use language models to generate text with the same semantic properties as a feature's activating examples. We then test whether the feature activates on these generated texts. We demonstrate that sensitivity measures a new facet of feature quality and find that many interpretable features have poor sensitivity. Human evaluation confirms that when features fail to activate on our generated text, that text genuinely resembles the original activating examples. Lastly, we study feature sensitivity at the SAE level and observe that average feature sensitivity declines with increasing SAE width across 7 SAE variants. Our work establishes feature sensitivity as a new dimension for evaluating both individual features and SAE architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¨€ç–è‡ªç¼–ç å™¨ (Sparse Autoencoder, SAE) ç‰¹å¾çš„æ•æ„Ÿæ€§ (Sensitivity) æµ‹é‡é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°ç‰¹å¾åœ¨é¢å¯¹ä¸å…¶æ¿€æ´»ç¤ºä¾‹è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬æ—¶èƒ½å¦å¯é æ¿€æ´»ã€‚ä½œè€…å¼€å‘äº†ä¸€ç§å¯æ‰©å±•çš„è¯„ä¼°æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸æ¿€æ´»ç¤ºä¾‹å…·æœ‰ç›¸åŒè¯­ä¹‰å±æ€§çš„æ–°æ–‡æœ¬ï¼Œä»è€Œè§„é¿äº†å¯¹ç‰¹å¾è¿›è¡Œè‡ªç„¶è¯­è¨€æè¿°çš„éœ€æ±‚ã€‚å®éªŒå‘ç°æ•æ„Ÿæ€§æ­ç¤ºäº†ç‰¹å¾è´¨é‡çš„ä¸€ä¸ªå…¨æ–°ç»´åº¦ï¼Œå¹¶æŒ‡å‡ºè®¸å¤šå…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§ (Interpretable) çš„ç‰¹å¾åœ¨æ•æ„Ÿæ€§ä¸Šè¡¨ç°ä¸ä½³ã€‚äººç±»è¯„ä¼°è¯å®ï¼Œå³ä½¿ç‰¹å¾æœªèƒ½æˆåŠŸæ¿€æ´»ï¼Œç”Ÿæˆçš„æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šä¾ç„¶ä¸åŸå§‹ç¤ºä¾‹é«˜åº¦å»åˆã€‚æ­¤å¤–ï¼Œé’ˆå¯¹7ä¸ª SAE å˜ä½“çš„æµ‹è¯•æ˜¾ç¤ºï¼Œå¹³å‡ç‰¹å¾æ•æ„Ÿæ€§ä¼šéšç€ SAE å®½åº¦çš„å¢åŠ è€Œå‘ˆç°ä¸‹é™è¶‹åŠ¿ã€‚è¯¥ç ”ç©¶å°†ç‰¹å¾æ•æ„Ÿæ€§ç¡®ç«‹ä¸ºè¯„ä¼°ä¸ªä½“ç‰¹å¾å’Œ SAE æ¶æ„æ€§èƒ½çš„é‡è¦æŒ‡æ ‡ï¼Œä¸ºæœºæ¢°å¯è§£é‡Šæ€§ (Mechanistic Interpretability) é¢†åŸŸæä¾›äº†æ–°çš„è¯„ä»·æ‰‹æ®µã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025 Workshop on Mechanistic Interpretability Camera Ready",
      "pdf_url": "https://arxiv.org/pdf/2509.23717v1",
      "published_date": "2025-09-28 07:58:53 UTC",
      "updated_date": "2025-09-28 07:58:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:40.399317+00:00"
    },
    {
      "arxiv_id": "2509.23711v1",
      "title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization",
      "title_zh": "è¿æ¥ç¦»æ•£ä¸è¿ç»­å¼ºåŒ–å­¦ä¹ ï¼šåŸºäºé…åˆ»ç”»çš„ç¨³å®šç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦",
      "authors": [
        "Ziheng Cheng",
        "Xin Guo",
        "Yufei Zhang"
      ],
      "abstract": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly over the past decades. Although primarily designed for discrete environments, many real-world RL applications are inherently continuous and complex. A major challenge in extending discrete-time algorithms to continuous-time settings is their sensitivity to time discretization, often leading to poor stability and slow convergence. In this paper, we investigate deterministic policy gradient methods for continuous-time RL. We derive a continuous-time policy gradient formula based on an analogue of the advantage function and establish its martingale characterization. This theoretical foundation leads to our proposed algorithm, CT-DDPG, which enables stable learning with deterministic policies in continuous-time environments. Numerical experiments show that the proposed CT-DDPG algorithm offers improved stability and faster convergence compared to existing discrete-time and continuous-time methods, across a wide range of control tasks with varying time discretizations and noise levels.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»æ•£æ—¶é—´å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ç®—æ³•åœ¨åº”ç”¨äºè¿ç»­æ—¶é—´ç¯å¢ƒæ—¶è¡¨ç°å‡ºçš„æ—¶é—´ç¦»æ•£åŒ–æ•æ„Ÿã€ç¨³å®šæ€§å·®å’Œæ”¶æ•›æ…¢ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰é…è¡¨å¾(Martingale Characterization)çš„ç¨³å®šç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚ç ”ç©¶è€…åŸºäºä¼˜åŠ¿å‡½æ•°(Advantage Function)çš„ç±»æ¯”æ¨å¯¼å‡ºè¿ç»­æ—¶é—´ç­–ç•¥æ¢¯åº¦å…¬å¼ï¼Œå¹¶åœ¨æ­¤ç†è®ºåŸºç¡€ä¸Šå¼€å‘äº†CT-DDPGç®—æ³•ï¼Œä»¥å®ç°è¿ç»­æ—¶é—´ç¯å¢ƒä¸‹çš„ç¨³å®šç¡®å®šæ€§ç­–ç•¥å­¦ä¹ ã€‚æ•°å€¼å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCT-DDPGåœ¨å¤„ç†å…·æœ‰ä¸åŒæ—¶é—´ç¦»æ•£åŒ–ç¨‹åº¦å’Œå™ªå£°æ°´å¹³çš„å„ç§æ§åˆ¶ä»»åŠ¡æ—¶ï¼Œç›¸è¾ƒäºç°æœ‰çš„ç¦»æ•£æ—¶é—´å’Œè¿ç»­æ—¶é—´æ–¹æ³•ï¼Œè¡¨ç°å‡ºæ˜¾è‘—æå‡çš„ç¨³å®šæ€§å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚è¿™ä¸€æˆæœé€šè¿‡å»ºç«‹åšå®çš„ç†è®ºæ¡†æ¶ï¼Œä¸ºæœ‰æ•ˆå¼¥åˆç¦»æ•£ä¸è¿ç»­æ—¶é—´å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†é‡è¦çš„ç®—æ³•æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23711v1",
      "published_date": "2025-09-28 07:53:33 UTC",
      "updated_date": "2025-09-28 07:53:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:44.735653+00:00"
    },
    {
      "arxiv_id": "2510.02359v1",
      "title": "Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis",
      "title_zh": "Emission-GPTï¼šé¢å‘çŸ¥è¯†æ£€ç´¢ã€æ’æ”¾æ¸…å•åŠæ•°æ®åˆ†æçš„é¢†åŸŸç‰¹å®šè¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Jiashu Ye",
        "Tong Wu",
        "Weiwen Chen",
        "Hao Zhang",
        "Zeteng Lin",
        "Xingxing Li",
        "Shujuan Weng",
        "Manni Zhu",
        "Xin Yuan",
        "Xinlong Hong",
        "Jingjie Li",
        "Junyu Zheng",
        "Zhijiong Huang",
        "Jing Tang"
      ],
      "abstract": "Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Emission-GPTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§æ°”æ’æ”¾é¢†åŸŸå®šåˆ¶çš„çŸ¥è¯†å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨è§£å†³æ’æ”¾ç›¸å…³çŸ¥è¯†ç¢ç‰‡åŒ–å’Œç°æœ‰æ•°æ®å¤„ç†æµç¨‹æ•ˆç‡ä½ä¸‹çš„éš¾é¢˜ã€‚è¯¥æ¨¡å‹åŸºäºåŒ…å«è¶…è¿‡10,000ä»½ä¸“ä¸šæ–‡æ¡£çš„çŸ¥è¯†åº“ï¼Œé€šè¿‡é›†æˆæç¤ºå·¥ç¨‹(Prompt Engineering)å’Œé—®é¢˜è¡¥å…¨æŠ€æœ¯ï¼Œç¡®ä¿äº†é¢†åŸŸç‰¹å®šé—®é¢˜çš„å›ç­”å‡†ç¡®æ€§ã€‚Emission-GPTå…è®¸ç”¨æˆ·åˆ©ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤äº’å¼æ•°æ®åˆ†æï¼Œæ¶µç›–äº†æ’æ”¾æ¸…å•æŸ¥è¯¢ã€å¯è§†åŒ–ã€æ¥æºè´¡çŒ®åˆ†æä»¥åŠæ’æ”¾å› å­(Emission Factors)æ¨èç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚åœ¨å¹¿ä¸œçœçš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ™ºèƒ½ä½“å±•ç¤ºäº†ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­æå–ç‚¹æºåˆ†å¸ƒå’Œè¡Œä¸šè¶‹åŠ¿ç­‰å…³é”®è§è§£çš„èƒ½åŠ›ã€‚å…¶æ¨¡å—åŒ–ä¸”å¯æ‰©å±•çš„æ¶æ„ä¸ä»…å®ç°äº†ä¼ ç»Ÿæ‰‹åŠ¨å·¥ä½œæµçš„è‡ªåŠ¨åŒ–ï¼Œä¹Ÿä¸ºä¸‹ä¸€ä»£æ’æ”¾æ¸…å•å¼€å‘å’ŒåŸºäºæƒ…æ™¯çš„è¯„ä¼°æä¾›äº†åŸºç¡€æ€§çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02359v1",
      "published_date": "2025-09-28 07:50:05 UTC",
      "updated_date": "2025-09-28 07:50:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:51.982184+00:00"
    },
    {
      "arxiv_id": "2509.23708v1",
      "title": "CrimEdit: Controllable Editing for Counterfactual Object Removal, Insertion, and Movement",
      "title_zh": "CrimEditï¼šé’ˆå¯¹åäº‹å®ç‰©ä½“ç§»é™¤ã€æ’å…¥åŠç§»åŠ¨çš„å¯æ§ç¼–è¾‘",
      "authors": [
        "Boseong Jeon",
        "Junghyuk Lee",
        "Jimin Park",
        "Kwanyoung Kim",
        "Jingi Jung",
        "Sangwon Lee",
        "Hyunbo Shim"
      ],
      "abstract": "Recent works on object removal and insertion have enhanced their performance by handling object effects such as shadows and reflections, using diffusion models trained on counterfactual datasets. However, the performance impact of applying classifier-free guidance to handle object effects across removal and insertion tasks within a unified model remains largely unexplored. To address this gap and improve efficiency in composite editing, we propose CrimEdit, which jointly trains the task embeddings for removal and insertion within a single model and leverages them in a classifier-free guidance scheme -- enhancing the removal of both objects and their effects, and enabling controllable synthesis of object effects during insertion. CrimEdit also extends these two task prompts to be applied to spatially distinct regions, enabling object movement (repositioning) within a single denoising step. By employing both guidance techniques, extensive experiments show that CrimEdit achieves superior object removal, controllable effect insertion, and efficient object movement without requiring additional training or separate removal and insertion stages.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CrimEditï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¯æ§å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨å¤„ç†ç‰©ä½“ç§»é™¤ã€æ’å…¥åŠå…¶ä¼´éšé˜´å½±ã€åå°„ç­‰ counterfactual æ•ˆæœæ—¶ï¼Œç¼ºä¹ç»Ÿä¸€æ¨¡å‹å’Œ classifier-free guidance åº”ç”¨çš„é—®é¢˜ã€‚CrimEdit é€šè¿‡åœ¨å•ä¸ªæ¨¡å‹ä¸­è”åˆè®­ç»ƒç§»é™¤å’Œæ’å…¥çš„ä»»åŠ¡åµŒå…¥(task embeddings)ï¼Œå¹¶åˆ©ç”¨åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æœºåˆ¶ï¼Œå®ç°äº†å¯¹ç‰©ä½“åŠå…¶å…³è”æ•ˆæœçš„å½»åº•ç§»é™¤ï¼Œä»¥åŠåœ¨æ’å…¥è¿‡ç¨‹ä¸­å¯¹ç‰©ä½“æ•ˆæœçš„å¯æ§åˆæˆã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥å°†ç§»é™¤å’Œæ’å…¥çš„ä»»åŠ¡æç¤ºæ‰©å±•åˆ°ç©ºé—´ä¸Šä¸åŒçš„åŒºåŸŸï¼Œä»è€Œèƒ½å¤Ÿåœ¨å•ä¸€çš„å»å™ªæ­¥éª¤(denoising step)ä¸­å®ç°ç‰©ä½“çš„ç§»åŠ¨å’Œé‡æ–°å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrimEdit åœ¨ç‰©ä½“ç§»é™¤ã€å¯æ§æ•ˆæœæ’å…¥ä»¥åŠé«˜æ•ˆç‰©ä½“ç§»åŠ¨æ–¹é¢å‡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–åˆ†é˜¶æ®µæ“ä½œå³å¯å®Œæˆå¤æ‚çš„å¤åˆç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æå‡äº†å›¾åƒç¼–è¾‘çš„æ•ˆç‡ï¼Œä¸ºå®ç°é«˜è´¨é‡ã€è‡ªåŠ¨åŒ–çš„åäº‹å®å›¾åƒç¼–è¾‘æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23708v1",
      "published_date": "2025-09-28 07:41:25 UTC",
      "updated_date": "2025-09-28 07:41:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:32:50.997404+00:00"
    },
    {
      "arxiv_id": "2509.25267v1",
      "title": "Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning",
      "title_zh": "ç”¨äºè‡ªé€‚åº”æç¤ºä¼˜åŒ–çš„åŠ¨æ€ç­–ç•¥å½’çº³ï¼šé€šè¿‡è½»é‡åŒ–å¼ºåŒ–å­¦ä¹ å¼¥åˆæ•ˆç‡ä¸ç²¾åº¦çš„é¸¿æ²Ÿ",
      "authors": [
        "Jiexi Xu"
      ],
      "abstract": "The performance of Large Language Models (LLMs) depends heavily on the chosen prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly accurate strategies like Self-Consistency (SC) incur substantial computational waste on simple tasks, while lightweight methods often fail on complex inputs. This paper introduces the Prompt Policy Network (PPN), a lightweight reinforcement learning framework that formalizes adaptive strategy selection as a single-step Markov Decision Process (MDP). The PPN, trained with Proximal Policy Optimization (PPO) and guided by a resource-explicit reward function, learns to allocate costly reasoning strategies only when necessary. Experiments on arithmetic reasoning benchmarks demonstrate that PPN achieves superior performance on the efficiency-accuracy Pareto front, delivering up to 61.5% token cost reduction compared to Self-Consistency while maintaining competitive accuracy. This work contributes a systematic, adaptive framework for cost-efficient LLM deployment, advancing the design of lightweight optimization techniques for scalable and sustainable language model applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Prompt Policy Network (PPN)ï¼Œä¸€ä¸ªè½»é‡çº§çš„Reinforcement Learningæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Large Language Models (LLMs)åœ¨æç¤ºç­–ç•¥é€‰æ‹©ä¸Šé¢ä¸´çš„æ•ˆç‡ä¸å‡†ç¡®ç‡æƒè¡¡éš¾é¢˜ã€‚PPNå°†è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©å½¢å¼åŒ–ä¸ºå•æ­¥Markov Decision Process (MDP)ï¼Œå¹¶é€šè¿‡Proximal Policy Optimization (PPO)è¿›è¡Œè®­ç»ƒï¼Œå¼•å¯¼æ¨¡å‹ä»…åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶è°ƒç”¨é«˜æˆæœ¬çš„æ¨ç†ç­–ç•¥ã€‚åœ¨ç®—æœ¯æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒPPNå®ç°äº†å“è¶Šçš„æ•ˆç‡-å‡†ç¡®ç‡Pareto frontè¡¨ç°ï¼Œåœ¨ä¿æŒç«äº‰åŠ›çš„å‡†ç¡®ç‡åŒæ—¶ï¼Œæ¯”Self-Consistency (SC)æ–¹æ³•å‡å°‘äº†é«˜è¾¾61.5%çš„Tokenæˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°ä½æˆæœ¬ä¸”å¯æŒç»­çš„LLMéƒ¨ç½²æä¾›äº†ä¸€å¥—ç³»ç»ŸåŒ–çš„è‡ªé€‚åº”ä¼˜åŒ–æ¡†æ¶ï¼Œæ˜¾è‘—æå‡äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ‰©å±•æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.25267v1",
      "published_date": "2025-09-28 07:32:42 UTC",
      "updated_date": "2025-09-28 07:32:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:01.960735+00:00"
    },
    {
      "arxiv_id": "2509.25266v1",
      "title": "Cognifying Education: Mapping AI's transformative role in emotional, creative, and collaborative learning",
      "title_zh": "æ•™è‚²è®¤çŸ¥åŒ–ï¼šäººå·¥æ™ºèƒ½åœ¨æƒ…æ„Ÿã€åˆ›æ„åŠåä½œå­¦ä¹ ä¸­çš„å˜é©æ€§ä½œç”¨æ¢æ",
      "authors": [
        "Mikael Gorsky",
        "Ilya Levin"
      ],
      "abstract": "Artificial intelligence (AI) is rapidly reshaping educational practice, challenging long held assumptions about teaching and learning. This article integrates conceptual perspectives from recent books (Genesis by Eric Schmidt, Henry Kissinger and Craig Mundie, CoIntelligence by Ethan Mollick, and The Inevitable by Kevin Kelly) with empirical insights from popular AI podcasts and Anthropic public releases. We examine seven key domains: emotional support, creativity, contextual understanding, student engagement, problem solving, ethics and morality, and collaboration. For each domain, we explore AI capabilities, opportunities for transformative change, and emerging best practices, drawing equally from theoretical analysis and real world observations. Overall, we find that AI, when used thoughtfully, can complement and enhance human educators in fostering richer learning experiences across cognitive, social, and emotional dimensions. We emphasize an optimistic yet responsible outlook: educators and students should actively shape AI integration to amplify human potential in creativity, ethical reasoning, collaboration, and beyond, while maintaining a focus on human centric values.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½(AI)åœ¨é‡å¡‘æ•™è‚²å®è·µä¸­çš„å˜é©ä½œç”¨ï¼Œé€šè¿‡æ•´åˆã€ŠGenesisã€‹ã€ã€ŠCoIntelligenceã€‹å’Œã€ŠThe Inevitableã€‹ç­‰è‘—ä½œçš„ç†è®ºè§†è§’ï¼Œä»¥åŠæ¥è‡ªAIæ’­å®¢å’ŒAnthropicå‘å¸ƒçš„å®è¯è§è§£ï¼Œç³»ç»Ÿæ˜ å°„äº†æ•™è‚²çš„æ™ºèƒ½åŒ–è¿›ç¨‹ã€‚æ–‡ç« é‡ç‚¹è€ƒå¯Ÿäº†æƒ…æ„Ÿæ”¯æŒ(emotional support)ã€åˆ›é€ åŠ›(creativity)ã€æƒ…å¢ƒç†è§£ã€å­¦ç”Ÿå‚ä¸ã€é—®é¢˜è§£å†³ã€ä¼¦ç†é“å¾·åŠåä½œ(collaboration)ä¸ƒä¸ªå…³é”®é¢†åŸŸï¼Œæ·±å…¥åˆ†æäº†AIçš„èƒ½åŠ›ã€å˜é©æœºé‡åŠæ–°å…´çš„æœ€ä½³å®è·µã€‚ç ”ç©¶å‘ç°ï¼Œå®¡æ…åº”ç”¨AIèƒ½å¤Ÿæœ‰æ•ˆè¡¥å……å¹¶å¢å¼ºäººç±»æ•™è‚²è€…åœ¨è®¤çŸ¥ã€ç¤¾äº¤åŠæƒ…æ„Ÿç»´åº¦çš„æ•™å­¦èƒ½åŠ›ï¼Œä»è€Œè¥é€ æ›´ä¸°å¯Œçš„å­¦ä¹ ä½“éªŒã€‚ä½œè€…æå‡ºäº†ä¸€ç§ä¹è§‚ä¸”è´Ÿè´£ä»»çš„å±•æœ›ï¼Œå¼ºè°ƒæ•™è‚²è€…ä¸å­¦ç”Ÿåº”ç§¯æå¼•å¯¼AIé›†æˆï¼Œåœ¨åšæŒä»¥äººä¸ºæœ¬(human centric values)çš„åŸºç¡€ä¸Šï¼Œæ”¾å¤§äººç±»åœ¨åˆ›é€ åŠ›ã€ä¼¦ç†æ¨ç†åŠåä½œæ–¹é¢çš„æ½œèƒ½ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Presented at the 13th Higher Education Institutions Conference (HEIC) in Dubrovnik (September 2025): AI and Digital Transformation in Higher Education",
      "pdf_url": "https://arxiv.org/pdf/2509.25266v1",
      "published_date": "2025-09-28 07:32:10 UTC",
      "updated_date": "2025-09-28 07:32:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:14.369921+00:00"
    },
    {
      "arxiv_id": "2509.23695v1",
      "title": "Estimating Time Series Foundation Model Transferability via In-Context Learning",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡å­¦ä¹ çš„æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹å¯è¿ç§»æ€§è¯„ä¼°",
      "authors": [
        "Qingren Yao",
        "Ming Jin",
        "Chengqi Zhang",
        "Chao-Han Huck Yang",
        "Jun Qi",
        "Shirui Pan"
      ],
      "abstract": "Time series foundation models (TSFMs) offer strong zero-shot forecasting via large-scale pre-training, yet fine-tuning remains critical for boosting performance in domains with limited public data. With the growing number of TSFMs, efficiently identifying the best model for downstream fine-tuning becomes increasingly challenging. In this work, we introduce TimeTic, a transferability estimation framework that recasts model selection as an in-context-learning problem: given observations on known (source) datasets, it predicts how a TSFM will perform after fine-tuning on a downstream (target) dataset. TimeTic flexibly organizes the observed model-data relationships as contextual information, allowing it to adapt seamlessly to various test-time scenarios. Leveraging the natural tabular structure formed by dataset meta-features, model characteristics, and fine-tuned performance, we employ tabular foundation models to serve as in-context learners. We further introduce a novel model characterization based on entropy evolution across model layers, capturing embedding-space distinctions and enabling TimeTic to generalize across arbitrary model sets. We establish a comprehensive benchmark for transferability estimation including 10 datasets, 10 foundation models, and 3 forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong alignment with actual fine-tuned performance for previously unseen datasets, achieving a mean rank correlation of approximately 0.6 and a 30% improvement compared to using zero-shot performance as the transferability score.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ (Time Series Foundation Models, TSFMs) åœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒè¿‡ç¨‹ä¸­éš¾ä»¥é€‰æ‹©æœ€ä¼˜æ¨¡å‹çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º TimeTic çš„å¯è¿ç§»æ€§ä¼°ç®—æ¡†æ¶ã€‚TimeTic å°†æ¨¡å‹é€‰æ‹©é‡æ–°å®šä¹‰ä¸ºè¯­å¢ƒå­¦ä¹  (In-Context Learning) é—®é¢˜ï¼Œé€šè¿‡å·²çŸ¥æºæ•°æ®é›†çš„è§‚å¯Ÿç»“æœé¢„æµ‹ TSFM åœ¨ç›®æ ‡æ•°æ®é›†å¾®è°ƒåçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è¡¨æ ¼åŸºç¡€æ¨¡å‹ä½œä¸ºè¯­å¢ƒå­¦ä¹ å™¨ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºè·¨æ¨¡å‹å±‚ç†µæ¼”åŒ– (Entropy Evolution) çš„æ–°å‹æ¨¡å‹ç‰¹å¾åˆ»ç”»æ–¹æ³•ï¼Œæœ‰æ•ˆæ•æ‰äº†åµŒå…¥ç©ºé—´çš„å·®å¼‚ã€‚ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªæ¶µç›–10ä¸ªæ•°æ®é›†å’Œ10ä¸ªåŸºç¡€æ¨¡å‹çš„ç»¼åˆåŸºå‡†ï¼Œæ¶µç›–äº†3é¡¹ä¸»è¦çš„é¢„æµ‹ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTimeTic çš„ä¼°ç®—ç»“æœä¸å®é™…å¾®è°ƒæ€§èƒ½é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡æ’åç›¸å…³æ€§è¾¾åˆ°0.6å·¦å³ï¼Œç›¸æ¯”ä»¥é›¶æ ·æœ¬ (Zero-Shot) æ€§èƒ½ä½œä¸ºè¿ç§»å¾—åˆ†çš„æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šæå‡äº†30%ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ— ç¼é€‚åº”å„ç§æµ‹è¯•åœºæ™¯ï¼Œå¹¶è¡¨ç°å‡ºè·¨ä»»æ„æ¨¡å‹é›†çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23695v1",
      "published_date": "2025-09-28 07:07:13 UTC",
      "updated_date": "2025-09-28 07:07:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:37.193789+00:00"
    },
    {
      "arxiv_id": "2509.23694v3",
      "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents",
      "title_zh": "SafeSearchï¼šé’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹æœç´¢æ™ºèƒ½ä½“å®‰å…¨æ€§çš„è‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•",
      "authors": [
        "Jianshuo Dong",
        "Sheng Guo",
        "Hao Wang",
        "Xun Chen",
        "Zhuotao Liu",
        "Tianwei Zhang",
        "Ke Xu",
        "Minlie Huang",
        "Han Qiu"
      ],
      "abstract": "Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æœç´¢æ™ºèƒ½ä½“(Search agents)åœ¨è¿æ¥äº’è”ç½‘è·å–å®æ—¶ä¿¡æ¯æ—¶ï¼Œç”±äºä¸å¯é æœç´¢ç»“æœæ‰€å¼•å…¥çš„æ–°å®‰å…¨é£é™©ã€‚ç ”ç©¶é€šè¿‡å®åœ°å®éªŒæ­ç¤ºäº†ä½è´¨é‡ç½‘é¡µå†…å®¹è¯¯å¯¼æ™ºèƒ½ä½“è¡Œä¸ºçš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çº¢é˜Ÿ(Red-Teaming)æ¡†æ¶ä»¥å®ç°ç³»ç»ŸåŒ–ä¸”ä½æˆæœ¬çš„å®‰å…¨è¯„ä¼°ã€‚åŸºäºè¯¥æ¡†æ¶æ„å»ºçš„SafeSearchåŸºå‡†æµ‹è¯•æ¶µç›–äº†è¯¯å¯¼ä¿¡æ¯(Misinformation)å’Œé—´æ¥æç¤ºæ³¨å…¥(Indirect prompt injection)ç­‰äº”ç±»æ ¸å¿ƒé£é™©ã€‚å®éªŒå¯¹æ¶µç›–æœç´¢å·¥ä½œæµ(Search workflow)ã€å·¥å…·è°ƒç”¨(Tool-calling)å’Œæ·±åº¦ç ”ç©¶(Deep research)æ¶æ„çš„15ç§LLMsè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°æ™ºèƒ½ä½“æ™®éå­˜åœ¨ä¸¥é‡æ¼æ´ï¼Œéƒ¨åˆ†æ¨¡å‹çš„æ”»å‡»æˆåŠŸç‡(ASR)é«˜è¾¾90.5%ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†æç¤ºè¯æé†’(Reminder prompting)ç­‰å¸¸è§é˜²å¾¡æ‰‹æ®µçš„å±€é™æ€§ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºæ›´å®‰å…¨ã€é€æ˜çš„æœç´¢æ™ºèƒ½ä½“æä¾›äº†é‡è¦çš„è¯„ä¼°å·¥å…·å’Œå®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint",
      "pdf_url": "https://arxiv.org/pdf/2509.23694v3",
      "published_date": "2025-09-28 07:05:17 UTC",
      "updated_date": "2025-10-15 02:32:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:29.168605+00:00"
    },
    {
      "arxiv_id": "2510.02358v1",
      "title": "DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding",
      "title_zh": "DiffuSpecï¼šé‡Šæ”¾æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨æŠ•æœºæ€§è§£ç ä¸­çš„æ½œåŠ›",
      "authors": [
        "Guanghao Li",
        "Zhihui Fu",
        "Min Fang",
        "Qibin Zhao",
        "Ming Tang",
        "Chun Yuan",
        "Jun Wang"
      ],
      "abstract": "As large language models (LLMs) scale up, accuracy improves, but the autoregressive (AR) nature of decoding increases latency since each token requires a serial forward pass. Speculative decoding addresses this by employing a fast drafter to propose multi-token drafts, which are then verified in parallel by the target model. However, many deployments still rely on AR drafters, where sequential passes limit wall-clock gains. We revisit the drafting stage and present DiffuSpec, a training-free drop-in framework that uses a pretrained diffusion language model (DLM) to produce multi-token drafts in a single forward pass, while remaining compatible with standard AR verifiers. Because DLM drafts are generated under bidirectional conditioning, parallel per-position candidates form a token lattice in which the locally highest-probability token at each position need not form a causal left-to-right path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a speed-quality trade-off. To address these challenges, we introduce two practical components: (i) a causal-consistency path search (CPS) over this lattice that extracts a left-to-right path aligned with AR verification; and (ii) an adaptive draft-length (ADL) controller that adjusts next proposal size based on recent acceptance feedback and realized generated length. Across benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing diffusion-based drafting as a robust alternative to autoregressive drafters for speculative decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiffuSpecï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å³æ’å³ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£è¯­è¨€æ¨¡å‹ (Diffusion Language Model, DLM) è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) è‡ªå›å½’ (Autoregressive) è§£ç å¸¦æ¥çš„é«˜å»¶è¿Ÿé—®é¢˜ã€‚DiffuSpec å…è®¸é€šè¿‡å•æ¬¡å‰å‘ä¼ é€’ç”Ÿæˆå¤šè¯å…ƒè‰ç¨¿ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ¨æµ‹è§£ç  (Speculative Decoding) ä¸­è‡ªå›å½’è‰ç¨¿æ¨¡å‹çš„é¡ºåºå¤„ç†é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹ DLM ç”Ÿæˆçš„éå› æœåŒå‘ç‰¹å¾ä»¥åŠè‰ç¨¿é•¿åº¦æƒè¡¡çš„æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å› æœä¸€è‡´æ€§è·¯å¾„æœç´¢ (Causal-Consistency Path Search, CPS) ä»¥ä»è¯å…ƒæ ¼ä¸­æå–ç¬¦åˆéªŒè¯é€»è¾‘çš„è·¯å¾„ï¼Œå¹¶é‡‡ç”¨äº†è‡ªé€‚åº”è‰ç¨¿é•¿åº¦ (Adaptive Draft-Length, ADL) æ§åˆ¶å™¨æ ¹æ®åé¦ˆåŠ¨æ€è°ƒæ•´é¢„æµ‹è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffuSpec åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜è¾¾ 3 å€çš„å®é™…åŠ é€Ÿï¼Œè¯æ˜äº†åŸºäºæ‰©æ•£çš„è‰ç¨¿ç”Ÿæˆæ˜¯æå‡æ¨æµ‹è§£ç æ•ˆç‡çš„å¼ºæœ‰åŠ›æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2510.02358v1",
      "published_date": "2025-09-28 07:00:15 UTC",
      "updated_date": "2025-09-28 07:00:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:26.279953+00:00"
    },
    {
      "arxiv_id": "2509.23687v2",
      "title": "Joint Hybrid Beamforming and Artificial Noise Design for Secure Multi-UAV ISAC Networks",
      "title_zh": "é¢å‘å®‰å…¨å¤šæ— äººæœº ISAC ç½‘ç»œçš„æ··åˆæ³¢æŸæˆå½¢ä¸äººå·¥å™ªå£°è”åˆè®¾è®¡",
      "authors": [
        "Runze Dong",
        "Buhong Wang",
        "Cunqian Feng",
        "Jiang Weng",
        "Chen Han",
        "Jiwei Tian"
      ],
      "abstract": "Integrated sensing and communication (ISAC) emerges as a key enabler for next-generation applications such as smart cities and autonomous systems. Its integration with unmanned aerial vehicles (UAVs) unlocks new potentials for reliable communication and precise sensing in dynamic aerial environments. However, existing research predominantly treats UAVs as aerial base stations, overlooking their role as ISAC users, and fails to leverage large-scale antenna arrays at terrestrial base stations to enhance security and spectral efficiency. This paper propose a secure and spectral efficient ISAC framework for multi-UAV networks, and a two-stage optimization approach is developed to jointly design hybrid beamforming (HBF), artificial noise (AN) injection, and UAV trajectories. Aiming at maximizing the sum secrecy rate, the first stage employs Proximal Policy Optimization (PPO) to optimize digital beamformers and trajectories, and the second stage decomposes the digital solution into analog and digital components via low-complexity matrix factorization. Simulation results demonstrate the effectiveness of the proposed framework compared to benchmark schemes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ— äººæœºï¼ˆmulti-UAVï¼‰ç½‘ç»œæå‡ºäº†ä¸€ä¸ªå®‰å…¨æ€§ä¸é¢‘è°±æ•ˆç‡å¹¶é‡çš„é›†æˆæ„Ÿä¼ ï¼ˆISACï¼‰æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰ç ”ç©¶ä¸­ UAV ä½œä¸º ISAC ç”¨æˆ·è§’è‰²è¢«å¿½è§†ä»¥åŠå¤§è§„æ¨¡å¤©çº¿é˜µåˆ—å®‰å…¨æ½œåŠ›æœªè¢«å……åˆ†æŒ–æ˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆè®¾è®¡æ··åˆæ³¢æŸèµ‹å½¢ï¼ˆHybrid Beamforming, HBFï¼‰ã€äººå·¥å™ªå£°ï¼ˆArtificial Noise, ANï¼‰æ³¨å…¥åŠ UAV è½¨è¿¹ï¼Œæ—¨åœ¨å®ç°æ€»ä¿å¯†é€Ÿç‡ï¼ˆsum secrecy rateï¼‰çš„æœ€å¤§åŒ–ã€‚ç ”ç©¶é‡‡ç”¨ä¸¤é˜¶æ®µä¼˜åŒ–ç­–ç•¥ï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimization, PPOï¼‰ç®—æ³•åŠ¨æ€è°ƒæ•´æ•°å­—æ³¢æŸèµ‹å½¢ä¸è½¨è¿¹ï¼›ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡ä½å¤æ‚åº¦çš„çŸ©é˜µåˆ†è§£æŠ€æœ¯å®ç°æ··åˆæ³¢æŸèµ‹å½¢çš„åˆ†è§£ã€‚ä»¿çœŸå®éªŒç»“æœéªŒè¯äº†è¯¥æ¡†æ¶åœ¨ä¿éšœå®‰å…¨é€šä¿¡ä¸é«˜æ•ˆæ„ŸçŸ¥æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºåŸºå‡†æ–¹æ¡ˆè¡¨ç°å‡ºæ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23687v2",
      "published_date": "2025-09-28 06:58:04 UTC",
      "updated_date": "2025-12-15 02:23:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:28.066131+00:00"
    },
    {
      "arxiv_id": "2509.23678v1",
      "title": "Towards a Comprehensive Scaling Law of Mixture-of-Experts",
      "title_zh": "è¿ˆå‘æ··åˆä¸“å®¶æ¨¡å‹çš„å…¨é¢ç¼©æ”¾æ³•åˆ™",
      "authors": [
        "Guoliang Zhao",
        "Yuhan Fu",
        "Shuaipeng Li",
        "Xingwu Sun",
        "Ruobing Xie",
        "An Wang",
        "Weidong Han",
        "Zhen Yang",
        "Weixuan Sun",
        "Yudong Zhang",
        "Cheng-zhong Xu",
        "Di Wang",
        "Jie Jiang"
      ],
      "abstract": "Mixture-of-Experts (MoE) models have become the consensus approach for enabling parameter-efficient scaling and cost-effective deployment in large language models. However, existing scaling laws for dense models are inapplicable to MoE models, which stems from three critical challenges: the multiplicity of influencing factors, their intricate coupling relationships and the non-monotonic nature of their performance impacts. They collectively necessitate a fine-grained investigation into MoE-specific scaling laws. In this work, we perform a systematic decomposition of MoE settings, identifying five key factors that influence model performance from both size and structural perspectives (data size ($D$), total model size ($N$), activated model size ($N_a$), number of active experts ($G$) and the ratio of shared experts ($S$)). Specifically, we design $446$ controlled experiments to characterize their marginal effects, ultimately constructing a comprehensive and precise joint MoE scaling law that considers all essential factors. Furthermore, we derive the theoretically optimal and practically efficiency-aware optimal configurations for $G$, $S$ and $N_a/N$ with detailed analyses. Our results demonstrate that the optimal settings for $G$ and $S$ are independent of both the model architecture and data size. With the scaling of $N$, the optimal activation parameter ratio of $N_a/N$ becomes sparser. Our proposed MoE scaling law could function as an accurate and insightful guidance to facilitate future MoE model design and training.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Mixture-of-Experts (MoE)æ¨¡å‹åœ¨æ‰©å±•ä¸éƒ¨ç½²ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç”±äºå¤šå› ç´ è€¦åˆå’Œéå•è°ƒæ€§èƒ½å½±å“ï¼Œç°æœ‰çš„ç¨ å¯†æ¨¡å‹æ‰©å±•å¾‹å·²ä¸å†é€‚ç”¨ã€‚ä½œè€…é€šè¿‡446é¡¹å¯¹ç…§å®éªŒï¼Œç³»ç»Ÿè¯†åˆ«äº†æ•°æ®è§„æ¨¡(D)ã€æ€»æ¨¡å‹è§„æ¨¡(N)ã€æ¿€æ´»æ¨¡å‹è§„æ¨¡(Na)ã€æ¿€æ´»ä¸“å®¶æ•°é‡(G)ä»¥åŠå…±äº«ä¸“å®¶æ¯”ä¾‹(S)äº”ä¸ªå…³é”®å½±å“å› ç´ ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„è”åˆMoE scaling lawï¼Œå¹¶æ¨å¯¼å‡ºGã€Så’ŒNa/Nåœ¨ç†è®ºä¸å®è·µä¸­çš„æœ€ä¼˜é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGå’ŒSçš„æœ€ä¼˜è®¾ç½®ä¸æ¨¡å‹æ¶æ„åŠæ•°æ®è§„æ¨¡æ— å…³ï¼Œä¸”éšç€æ€»æ¨¡å‹è§„æ¨¡Nçš„å¢é•¿ï¼Œæœ€ä¼˜æ¿€æ´»æ¯”ä¾‹Na/Nå‘ˆç°å‡ºæ›´ç¨€ç–çš„è¶‹åŠ¿ã€‚è¯¥æˆæœä¸ºæœªæ¥MoEæ¨¡å‹çš„è®¾è®¡ä¸è®­ç»ƒæä¾›äº†ç²¾ç¡®ä¸”å…·æ´å¯ŸåŠ›çš„æŒ‡å¯¼ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23678v1",
      "published_date": "2025-09-28 06:35:34 UTC",
      "updated_date": "2025-09-28 06:35:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:32.458778+00:00"
    },
    {
      "arxiv_id": "2509.23676v1",
      "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",
      "title_zh": "ä»æ¨ç†åˆ°ç­”æ¡ˆï¼šè’¸é¦ç‰ˆ DeepSeek R1 æ¨¡å‹çš„å®è¯ã€æ³¨æ„åŠ›åŠæœºç†æ€§æ´å¯Ÿ",
      "authors": [
        "Jue Zhang",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "abstract": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \\href{https://aka.ms/R2A-code}{this URL}.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¸‰æ¬¾è’¸é¦ç‰ˆçš„ DeepSeek R1 æ¨¡å‹è¿›è¡Œäº†ä¸‰é˜¶æ®µè°ƒæŸ¥ï¼Œæ—¨åœ¨æ­ç¤º Large Reasoning Models (LRMs) çš„æ¨ç†è½¨è¿¹ï¼ˆreasoning tracesï¼‰ä¸ç­”æ¡ˆç”Ÿæˆä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚é¦–å…ˆï¼Œç»éªŒè¯„ä¼°ï¼ˆempirical evaluationï¼‰è¡¨æ˜æ˜¾å¼æ¨ç†èƒ½ä¸€è‡´åœ°æå‡æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„ç­”æ¡ˆè´¨é‡ã€‚å…¶æ¬¡ï¼Œæ³¨æ„åŠ›åˆ†æï¼ˆattention analysisï¼‰å‘ç°ç­”æ¡ˆæ ‡è®°ï¼ˆanswer tokensï¼‰å¯¹æ¨ç†æ ‡è®°å…·æœ‰æ˜¾è‘—çš„ä¾èµ–æ€§ï¼Œå¹¶è¯†åˆ«å‡ºä¸­å±‚å­˜åœ¨çš„ Reasoning-Focus Heads (RFHs) èƒ½å¤Ÿç²¾å‡†è¿½è¸ªæ¨ç†è·¯å¾„åŠè‡ªæˆ‘åæ€çº¿ç´¢ã€‚æœ€åï¼Œé€šè¿‡æ¿€æ´»è¡¥ä¸ï¼ˆactivation patchingï¼‰è¿›è¡Œçš„æœºæ¢°åŒ–å¹²é¢„å®éªŒè¯å®ï¼Œæ‰°åŠ¨å…³é”®æ¨ç†æ ‡è®°ä¼šç›´æ¥æ”¹å˜æœ€ç»ˆç­”æ¡ˆï¼ŒéªŒè¯äº†ä»æ¨ç†åˆ°ç­”æ¡ˆçš„åŠŸèƒ½æ€§ä¿¡æ¯æµåŠ¨ã€‚è¿™äº›å‘ç°æ·±åŒ–äº†å¯¹ LRMs å¦‚ä½•åˆ©ç”¨ä¸­é—´æ¨ç†è¿‡ç¨‹å¡‘é€ è¾“å‡ºçš„è®¤çŸ¥ï¼Œå¹¶ä¸ºç†è§£æ¨ç†æ ‡è®°åœ¨æ¨¡å‹å†³ç­–ä¸­çš„åŠŸèƒ½æ€§è§’è‰²æä¾›äº†å®è¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by EMNLP'25 (Main)",
      "pdf_url": "https://arxiv.org/pdf/2509.23676v1",
      "published_date": "2025-09-28 06:32:21 UTC",
      "updated_date": "2025-09-28 06:32:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:40.388471+00:00"
    },
    {
      "arxiv_id": "2509.23673v1",
      "title": "RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks",
      "title_zh": "RCIï¼šè¯„ä¼°å¤šæ¨¡æ€åŸºå‡†ä¸­å…¨å±€ä¸å±€éƒ¨æ¨ç†èƒ½åŠ›çš„æŒ‡æ ‡",
      "authors": [
        "Amit Agarwal",
        "Hitesh Laxmichand Patel",
        "Srikant Panda",
        "Hansa Meghwani",
        "Jyotika Singh",
        "Karan Dua",
        "Paul Li",
        "Tao Sheng",
        "Sujith Ravi",
        "Dan Roth"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive results on vision-language benchmarks, yet it remains unclear whether these benchmarks assess genuine global reasoning or allow success via localized visual cues. Existing evaluation methods do not explicitly measure this distinction, hindering effective dataset curation and real-world focused model development.\n  We introduce Region Comprehension Index (RCI), the first model-based score to directly quantify a dataset's reliance on global versus local visual information. RCI systematically compares reference-model performance on image patches versus full images, revealing if tasks require holistic image understanding or can be solved with partial or localized visual cues.\n  When applying RCI to 13 widely used multimodal benchmarks, we observed that most of them favor localized reasoning and exhibit significant spatial biases, indicating potential risks in real-world applications. RCI equips researchers & practitioners with an actionable tool for diagnosing & mitigating these biases, enabling the construction of datasets and benchmarks to foster the development of robust, enterprise-ready multimodal systems.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†åŒºåŸŸç†è§£æŒ‡æ•° (Region Comprehension Index, RCI)ï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨é‡åŒ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å…¨å±€æ¨ç† (global reasoning) ä¸å±€éƒ¨è§†è§‰çº¿ç´¢ (localized visual cues) ä¾èµ–ç¨‹åº¦çš„æ¨¡å‹åŒ–è¯„åˆ†ç³»ç»Ÿã€‚é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨åŸºå‡†æµ‹è¯•ä¸­å¯èƒ½ä¾èµ–å±€éƒ¨ä¿¡æ¯è€Œéæ•´ä½“ç†è§£çš„é—®é¢˜ï¼ŒRCI é€šè¿‡å¯¹æ¯”æ¨¡å‹åœ¨å›¾åƒå— (image patches) å’Œå…¨å›¾ (full images) ä¸Šçš„è¡¨ç°æ¥è¯†åˆ«ä»»åŠ¡çš„æ¨ç†å±æ€§ã€‚é€šè¿‡å¯¹ 13 ä¸ªä¸»æµå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å¤§å¤šæ•°æ•°æ®é›†å€¾å‘äºå±€éƒ¨æ¨ç†å¹¶å­˜åœ¨æ˜¾è‘—çš„ç©ºé—´åè§ (spatial biases)ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æ½œåœ¨é£é™©ã€‚RCI ä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¯Šæ–­ä¸å‡è½»è¿™äº›åè§çš„å¯æ“ä½œå·¥å…·ï¼Œæœ‰åŠ©äºæ„å»ºæ›´ç¨³å¥ã€é€‚ç”¨äºä¼ä¸šçº§åœºæ™¯çš„å¤šæ¨¡æ€ç³»ç»Ÿã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23673v1",
      "published_date": "2025-09-28 06:26:11 UTC",
      "updated_date": "2025-09-28 06:26:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:45.592334+00:00"
    },
    {
      "arxiv_id": "2509.23671v1",
      "title": "Graph Neural Networks with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion for Multivariate Time Series Forecasting",
      "title_zh": "èåˆå¤šæ ·æ€§æ„ŸçŸ¥é‚»å±…é€‰æ‹©ä¸åŠ¨æ€å¤šå°ºåº¦èåˆçš„å›¾ç¥ç»ç½‘ç»œå¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Jingqi Xu",
        "Guibin Chen",
        "Jingxi Lu",
        "Yuzhang Lin"
      ],
      "abstract": "Recently, numerous deep models have been proposed to enhance the performance of multivariate time series (MTS) forecasting. Among them, Graph Neural Networks (GNNs)-based methods have shown great potential due to their capability to explicitly model inter-variable dependencies. However, these methods often overlook the diversity of information among neighbors, which may lead to redundant information aggregation. In addition, their final prediction typically relies solely on the representation from a single temporal scale. To tackle these issues, we propose a Graph Neural Networks (GNNs) with Diversity-aware Neighbor Selection and Dynamic Multi-scale Fusion (DIMIGNN). DIMIGNN introduces a Diversity-aware Neighbor Selection Mechanism (DNSM) to ensure that each variable shares high informational similarity with its neighbors while maintaining diversity among neighbors themselves. Furthermore, a Dynamic Multi-Scale Fusion Module (DMFM) is introduced to dynamically adjust the contributions of prediction results from different temporal scales to the final forecasting result. Extensive experiments on real-world datasets demonstrate that DIMIGNN consistently outperforms prior methods.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¤šå…ƒæ—¶é—´åºåˆ—(Multivariate Time Series)é¢„æµ‹ä¸­çš„å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks)æ–¹æ³•ï¼ŒæŒ‡å‡ºå…¶ç”±äºå¿½è§†é‚»å±…ä¿¡æ¯å¤šæ ·æ€§è€Œå¯¼è‡´ä¿¡æ¯èšåˆå†—ä½™ï¼Œä¸”é¢„æµ‹è¿‡ç¨‹è¿‡åº¦ä¾èµ–å•ä¸€æ—¶é—´å°ºåº¦çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† DIMIGNN æ¨¡å‹ï¼Œé€šè¿‡å¼•å…¥å¤šæ ·æ€§æ„ŸçŸ¥é‚»å±…é€‰æ‹©æœºåˆ¶(Diversity-aware Neighbor Selection Mechanism)æ¥ç¡®ä¿å˜é‡ä¸é‚»å±…å…·æœ‰é«˜ä¿¡æ¯ç›¸ä¼¼æ€§çš„åŒæ—¶ç»´æŒé‚»å±…é—´çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åŠ¨æ€å¤šå°ºåº¦èåˆæ¨¡å—(Dynamic Multi-Scale Fusion Module)åŠ¨æ€è°ƒæ•´ä¸åŒæ—¶é—´å°ºåº¦å¯¹æœ€ç»ˆé¢„æµ‹ç»“æœçš„è´¡çŒ®åº¦ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDIMIGNN åœ¨å¤šå˜é‡é¢„æµ‹æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æ•æ‰å˜é‡é—´å¤æ‚ä¾èµ–å’Œå¤šå°ºåº¦ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23671v1",
      "published_date": "2025-09-28 06:23:43 UTC",
      "updated_date": "2025-09-28 06:23:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:45.770720+00:00"
    },
    {
      "arxiv_id": "2509.23666v1",
      "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability",
      "title_zh": "è¶…è¶Šè´ªå©ªé€€å‡ºï¼šé¢å‘é£é™©æ§åˆ¶ä¸å¯é æ€§çš„æ”¹è¿›æ—©æœŸé€€å‡ºå†³ç­–",
      "authors": [
        "Divya Jyoti Bajpai",
        "Manjesh Kumar Hanawal"
      ],
      "abstract": "Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup (1.70-2.10x) with a minimal performance drop (<2%) as compared to full model performance. Our source code is available at https://github.com/Div290/UAT.",
      "tldr_zh": "é’ˆå¯¹ Early-Exit Deep Neural Networks åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”±äºä½¿ç”¨é™æ€é˜ˆå€¼è€Œå¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡å’Œå¯¹ distribution shifts ç¼ºä¹é²æ£’æ€§ç­‰é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº† UAT æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ Multi-Armed Bandit æœºåˆ¶å®ç°äº†åœ¨çº¿ä¸”æ— ç›‘ç£çš„é€€å‡ºå†³ç­–é˜ˆå€¼è°ƒæ•´ï¼Œä»è€Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ä¸­çš„é£é™©æ§åˆ¶ä¸å¯é æ€§ã€‚UAT å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„å¥–åŠ±å‡½æ•°æ¥è¯„ä¼° predictive certainty åŠå…¶å¯é æ€§ï¼Œåœ¨è®¡ç®—æ•ˆç‡ä¸é¢„æµ‹è´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¹¶å¯¹ä¸å¿…è¦çš„ late exits è¿›è¡Œæƒ©ç½šã€‚ç ”ç©¶ä¸º UAT æä¾›çš„é£é™©æ§åˆ¶ç»™å‡ºäº†ç†è®ºä¿éšœï¼Œå¹¶åœ¨è§†è§‰è¯­è¨€ç†è§£ã€æ–‡æœ¬ç”Ÿæˆå’Œåˆ†ç±»ç­‰å¤šé¡¹ä»»åŠ¡ä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¨æ¨¡å‹æ¨ç†ç›¸æ¯”ï¼ŒUAT åœ¨æ€§èƒ½ä¸‹é™ä¸åˆ° 2% çš„æƒ…å†µä¸‹å®ç°äº† 1.70-2.10x çš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡äº†è‡ªé€‚åº”æ¨ç†çš„æ•ˆç‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as poster in NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23666v1",
      "published_date": "2025-09-28 06:05:24 UTC",
      "updated_date": "2025-09-28 06:05:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:33:52.961048+00:00"
    },
    {
      "arxiv_id": "2509.23665v1",
      "title": "Calibration Meets Reality: Making Machine Learning Predictions Trustworthy",
      "title_zh": "æ ¡å‡†å›å½’ç°å®ï¼šæ„å»ºå¯ä¿¡çš„æœºå™¨å­¦ä¹ é¢„æµ‹",
      "authors": [
        "Kristina P. Sinaga",
        "Arjun S. Nair"
      ],
      "abstract": "Post-hoc calibration methods are widely used to improve the reliability of probabilistic predictions from machine learning models. Despite their prevalence, a comprehensive theoretical understanding of these methods remains elusive, particularly regarding their performance across different datasets and model architectures. Input features play a crucial role in shaping model predictions and, consequently, their calibration. However, the interplay between feature quality and calibration performance has not been thoroughly investigated. In this work, we present a rigorous theoretical analysis of post-hoc calibration methods, focusing on Platt scaling and isotonic regression. We derive convergence guarantees, computational complexity bounds, and finite-sample performance metrics for these methods. Furthermore, we explore the impact of feature informativeness on calibration performance through controlled synthetic experiments. Our empirical evaluation spans a diverse set of real-world datasets and model architectures, demonstrating consistent improvements in calibration metrics across various scenarios. By examining calibration performance under varying feature conditions utilizing only informative features versus complete feature spaces including noise dimensions, we provide fundamental insights into the robustness and reliability of different calibration approaches. Our findings offer practical guidelines for selecting appropriate calibration methods based on dataset characteristics and computational constraints, bridging the gap between theoretical understanding and practical implementation in uncertainty quantification. Code and experimental data are available at: https://github.com/Ajwebdevs/calibration-analysis-experiments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¦‚ç‡é¢„æµ‹çš„å¯é æ€§ï¼Œå¯¹ Platt scaling å’Œ isotonic regression ç­‰ post-hoc calibration æ–¹æ³•è¿›è¡Œäº†æ·±å…¥çš„ç†è®ºåˆ†æã€‚ä½œè€…ä¸ä»…æ¨å¯¼äº†è¿™äº›æ–¹æ³•çš„æ”¶æ•›ä¿è¯ã€è®¡ç®—å¤æ‚åº¦ç•Œé™ä»¥åŠæœ‰é™æ ·æœ¬æ€§èƒ½æŒ‡æ ‡ï¼Œè¿˜é‡ç‚¹æ¢è®¨äº†ç‰¹å¾ä¿¡æ¯é‡ï¼ˆfeature informativenessï¼‰ä¸æ ¡å‡†æ€§èƒ½ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡å—æ§åˆæˆå®éªŒå’Œå¤šç§çœŸå®ä¸–ç•Œæ•°æ®é›†åŠæ¨¡å‹æ¶æ„çš„å®è¯è¯„ä¼°ï¼Œè¯¥ç ”ç©¶è¯æ˜äº†æ ¡å‡†æ–¹æ³•åœ¨ä¸åŒåœºæ™¯ä¸‹å‡èƒ½æ˜¾è‘—æå‡é¢„æµ‹çš„å¯é æ€§ã€‚å®éªŒè¿›ä¸€æ­¥å¯¹æ¯”äº†å®Œæ•´ç‰¹å¾ç©ºé—´ä¸ä»…å«ä¿¡æ¯ç‰¹å¾ç©ºé—´ä¸‹çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ç‰¹å¾è´¨é‡å¯¹æ ¡å‡†é²æ£’æ€§çš„åŸºç¡€æ€§å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºæ ¹æ®æ•°æ®é›†ç‰¹æ€§å’Œè®¡ç®—çº¦æŸé€‰æ‹©åˆé€‚çš„æ ¡å‡†æ–¹æ³•æä¾›äº†å®è·µæŒ‡å—ï¼Œæœ‰æ•ˆåœ°å¼¥è¡¥äº†ç†è®ºç†è§£ä¸ uncertainty quantification å®é™…åº”ç”¨ä¹‹é—´çš„é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.PR"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 7 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23665v1",
      "published_date": "2025-09-28 06:04:56 UTC",
      "updated_date": "2025-09-28 06:04:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:07.366623+00:00"
    },
    {
      "arxiv_id": "2509.23662v1",
      "title": "Pure Node Selection for Imbalanced Graph Node Classification",
      "title_zh": "é¢å‘ä¸å¹³è¡¡å›¾èŠ‚ç‚¹åˆ†ç±»çš„çº¯èŠ‚ç‚¹é€‰æ‹©",
      "authors": [
        "Fanlong Zeng",
        "Wensheng Gan",
        "Jiayang Wu",
        "Philip S. Yu"
      ],
      "abstract": "The problem of class imbalance refers to an uneven distribution of quantity among classes in a dataset, where some classes are significantly underrepresented compared to others. Class imbalance is also prevalent in graph-structured data. Graph neural networks (GNNs) are typically based on the assumption of class balance, often overlooking the issue of class imbalance. In our investigation, we identified a problem, which we term the Randomness Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are affected by random seeds, leading to a significant performance degradation. To eliminate the influence of random factors in algorithms, we proposed PNS (Pure Node Sampling) to address the RACP in the node synthesis stage. Unlike existing approaches that design specialized algorithms to handle either quantity imbalance or topological imbalance, PNS is a novel plug-and-play module that operates directly during node synthesis to mitigate RACP. Moreover, PNS also alleviates performance degradation caused by abnormal distribution of node neighbors. We conduct a series of experiments to identify what factors are influenced by random seeds. Experimental results demonstrate the effectiveness and stability of our method, which not only eliminates the effect of unfavorable random seeds but also outperforms the baseline across various benchmark datasets with different GNN backbones. Data and code are available at https://github.com/flzeng1/PNS.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å›¾ç»“æ„æ•°æ®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡(Class Imbalance)é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºä¼ ç»Ÿçš„å›¾ç¥ç»ç½‘ç»œ(GNNs)å¾€å¾€å¿½ç•¥äº†è¿™ä¸€åˆ†å¸ƒä¸å‡çš„ç°çŠ¶ã€‚ç ”ç©¶é€šè¿‡è°ƒæŸ¥å‘ç°äº†ä¸€ä¸ªè¢«ç§°ä¸ºéšæœºæ€§å¼‚å¸¸è¿æ¥é—®é¢˜(Randomness Anomalous Connectivity Problem, RACP)çš„æ–°ç°è±¡ï¼Œå³æŸäº›ç°æœ‰æ¨¡å‹ä¼šå—åˆ°éšæœºç§å­çš„æ˜¾è‘—å½±å“è€Œå¯¼è‡´æ€§èƒ½å‰§çƒˆæ³¢åŠ¨ã€‚ä¸ºæ¶ˆé™¤è¿™ç§éšæœºå› ç´ å¹¶è§£å†³èŠ‚ç‚¹åˆæˆé˜¶æ®µçš„RACPï¼Œä½œè€…æå‡ºäº†åä¸ºPNS (Pure Node Sampling) çš„çº¯èŠ‚ç‚¹é‡‡æ ·æ–¹æ³•ã€‚ä½œä¸ºä¸€ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼ŒPNSä¸ä»…èƒ½ç›´æ¥åœ¨èŠ‚ç‚¹åˆæˆè¿‡ç¨‹ä¸­å‘æŒ¥ä½œç”¨ï¼Œè¿˜èƒ½ç¼“è§£ç”±èŠ‚ç‚¹é‚»å±…å¼‚å¸¸åˆ†å¸ƒå¼•èµ·çš„æ€§èƒ½é€€åŒ–ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ¶ˆé™¤ä¸åˆ©éšæœºç§å­çš„å½±å“ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸åŒGNNéª¨å¹²ç½‘ç»œä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç°äº†æé«˜çš„æœ‰æ•ˆæ€§å’Œç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint, 8 tables, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23662v1",
      "published_date": "2025-09-28 05:53:33 UTC",
      "updated_date": "2025-09-28 05:53:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:07.965153+00:00"
    },
    {
      "arxiv_id": "2509.23659v2",
      "title": "Aligning LLMs for Multilingual Consistency in Enterprise Applications",
      "title_zh": "é¢å‘ä¼ä¸šåº”ç”¨ä¸­å¤šè¯­è¨€ä¸€è‡´æ€§çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½",
      "authors": [
        "Amit Agarwal",
        "Hansa Meghwani",
        "Hitesh Laxmichand Patel",
        "Tao Sheng",
        "Sujith Ravi",
        "Dan Roth"
      ],
      "abstract": "Large language models (LLMs) remain unreliable for global enterprise applications due to substantial performance gaps between high-resource and mid/low-resource languages, driven by English-centric pretraining and internal reasoning biases. This inconsistency undermines customer experience and operational reliability in multilingual settings such as customer support, content moderation, and information retrieval. Even with advanced Retrieval-Augmented Generation (RAG) systems, we observe up to an 29% accuracy drop in non-English languages compared to English. We propose a practical, batch-wise alignment strategy for fine-tuning LLMs, leveraging semantically equivalent multilingual data in each training batch to directly align model outputs across languages. This approach improves non-English accuracy by up to 23.9% without compromising English performance, model reasoning, or retrieval quality. Our method is simple to implement, scalable, and integrates seamlessly with existing LLM training & deployment pipelines, enabling more robust and equitable multilingual AI solutions in industry.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) åœ¨å…¨çƒä¼ä¸šåº”ç”¨ä¸­å› ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒå’Œå†…éƒ¨æ¨ç†åå·®è€Œå¯¼è‡´çš„å¤šè¯­è¨€ä¸€è‡´æ€§ä¸è¶³é—®é¢˜ã€‚è¿™ç§è¡¨ç°å·®å¼‚åœ¨å¤šè¯­è¨€ Retrieval-Augmented Generation (RAG) ç³»ç»Ÿä¸­å°¤ä¸ºä¸¥é‡ï¼Œå¯¼è‡´éè‹±è¯­è¯­è¨€å‡†ç¡®ç‡æ¯”è‹±è¯­ä½ 29%ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å®ç”¨çš„åˆ†æ‰¹æ¬¡å¯¹é½ (batch-wise alignment) å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­åˆ©ç”¨è¯­ä¹‰ç­‰æ•ˆçš„å¤šè¯­è¨€æ•°æ®æ¥ç›´æ¥å¯¹é½æ¨¡å‹çš„è·¨è¯­è¨€è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æŸå®³è‹±è¯­æ€§èƒ½ã€æ¨¡å‹æ¨ç†æˆ–æ£€ç´¢è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå°†éè‹±è¯­å‡†ç¡®ç‡æå‡äº†é«˜è¾¾ 23.9%ã€‚è¯¥æ–¹æ³•å®ç°ç®€å•ä¸”å…·å¤‡é«˜åº¦å¯æ‰©å±•æ€§ï¼Œèƒ½ä¸ç°æœ‰ LLM è®­ç»ƒå’Œéƒ¨ç½²æµç¨‹æ— ç¼é›†æˆï¼Œä¸ºå·¥ä¸šç•Œæä¾›äº†æ›´ç¨³å¥ä¸”å…¬å¹³çš„å¤šè¯­è¨€ AI è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23659v2",
      "published_date": "2025-09-28 05:51:22 UTC",
      "updated_date": "2025-10-25 18:56:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:12.280470+00:00"
    },
    {
      "arxiv_id": "2509.23655v1",
      "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
      "title_zh": "èšç„¦é‡ç‚¹ï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ä¸­ä»¥ç‰©ä½“-æ™ºèƒ½ä½“ä¸ºä¸­å¿ƒçš„ Token åŒ–",
      "authors": [
        "Rokas Bendikas",
        "Daniel Dijkman",
        "Markus Peschl",
        "Sanjay Haresh",
        "Pietro Mazzaglia"
      ],
      "abstract": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Oat-VLAï¼Œä¸€ç§é’ˆå¯¹è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(Vision-Language-Action, VLA)çš„ç‰©ä½“-æ™ºèƒ½ä½“ä¸­å¿ƒåŒ–æ ‡è®°åŒ–(Object-Agent-centric Tokenization)æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å°†å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹(VLM)é€‚é…åˆ°æœºå™¨äººé¢†åŸŸæ—¶è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å¯¹åœºæ™¯ç‰©ä½“åŠæ™ºèƒ½ä½“è‡ªèº«è§†è§‰ä¿¡æ¯çš„å½’çº³åç½®(inductive bias)ï¼Œé€šè¿‡ä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è¡¨ç¤ºå­¦ä¹ ï¼ŒæˆåŠŸåœ¨ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹å°†è§†è§‰æ ‡è®°(visual tokens)çš„æ•°é‡å¤§å¹…å‡å°‘åˆ°æä½æ°´å¹³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOat-VLAåœ¨LIBEROåŸºå‡†å¥—ä»¶ä¸Šçš„æ”¶æ•›é€Ÿåº¦æ¯”OpenVLAå¿«è‡³å°‘ä¸¤å€ï¼Œå¹¶åœ¨å¤šç§çœŸå®ä¸–ç•Œçš„æ‹¾å–ä¸æ”¾ç½®(pick and place)ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºOpenVLAã€‚è¯¥å·¥ä½œè¯æ˜äº†é€šè¿‡ä¼˜åŒ–è§†è§‰è¾“å…¥çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œå¯ä»¥æ˜¾è‘—æå‡VLAæ¨¡å‹åœ¨æœºå™¨äººæ“çºµä»»åŠ¡ä¸­çš„è®­ç»ƒæ•ˆç‡å’Œå®é™…åº”ç”¨æ•ˆæœã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Presented at 9th Conference on Robot Learning (CoRL 2025), Seoul, Korea",
      "pdf_url": "https://arxiv.org/pdf/2509.23655v1",
      "published_date": "2025-09-28 05:42:53 UTC",
      "updated_date": "2025-09-28 05:42:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:21.594165+00:00"
    },
    {
      "arxiv_id": "2509.23652v2",
      "title": "ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis",
      "title_zh": "ReWatch-R1ï¼šé€šè¿‡æ™ºèƒ½ä½“åŒ–æ•°æ®åˆæˆæå‡å¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›",
      "authors": [
        "Congzhi Zhang",
        "Zhibin Wang",
        "Yinchao Ma",
        "Jiawei Peng",
        "Yihan Wang",
        "Qiang Zhou",
        "Jun Song",
        "Bo Zheng"
      ],
      "abstract": "While Reinforcement Learning with Verifiable Reward (RLVR) significantly advances image reasoning in Large Vision-Language Models (LVLMs), its application to complex video reasoning remains underdeveloped. This gap stems primarily from a critical data bottleneck: existing datasets lack the challenging, multi-hop questions and high-quality, video-grounded Chain-of-Thought (CoT) data necessary to effectively bootstrap RLVR. To address this, we introduce ReWatch, a large-scale dataset built to foster advanced video reasoning. We propose a novel multi-stage synthesis pipeline to synthesize its three components: ReWatch-Caption, ReWatch-QA, and ReWatch-CoT. A core innovation is our Multi-Agent ReAct framework for CoT synthesis, which simulates a human-like \"re-watching\" process to generate video-grounded reasoning traces by explicitly modeling information retrieval and verification. Building on this dataset, we develop ReWatch-R1 by post-training a strong baseline LVLM with Supervised Fine-Tuning (SFT) and our RLVR framework. This framework incorporates a novel Observation \\& Reasoning (O\\&R) reward mechanism that evaluates both the final answer's correctness and the reasoning's alignment with video content, directly penalizing hallucination. Our experiments show that ReWatch-R1 achieves state-of-the-art average performance on five challenging video reasoning benchmarks. Project Page: https://rewatch-r1.github.io",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹(LVLMs)åœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­å› ç¼ºä¹é«˜è´¨é‡ Chain-of-Thought (CoT) æ•°æ®è€Œé¢ä¸´çš„ç“¶é¢ˆï¼Œæå‡ºäº†å¤§è§„æ¨¡æ•°æ®é›† ReWatchã€‚è¯¥ç ”ç©¶å¼•å…¥äº†åˆ›æ–°çš„ Multi-Agent ReAct æ¡†æ¶æ¥åˆæˆ CoT æ•°æ®ï¼Œé€šè¿‡æ¨¡æ‹Ÿäººç±»â€œé‡çœ‹â€è§†é¢‘çš„è¿‡ç¨‹ï¼Œæ˜¾å¼å»ºæ¨¡ä¿¡æ¯æ£€ç´¢ä¸éªŒè¯ä»¥ç”Ÿæˆå¯é çš„æ¨ç†è½¨è¿¹ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œç ”ç©¶è€…é€šè¿‡æœ‰ç›‘ç£å¾®è°ƒ(SFT)å’Œå¼ºåŒ–å­¦ä¹ (RLVR)å¼€å‘äº† ReWatch-R1 æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†å…¨æ–°çš„è§‚å¯Ÿä¸æ¨ç†(Observation & Reasoning, O&R)å¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§åŠæ¨ç†ä¸è§†é¢‘å†…å®¹çš„å¯¹é½ç¨‹åº¦æ¥æœ‰æ•ˆæŠ‘åˆ¶å¹»è§‰ã€‚å®éªŒè¯æ˜ï¼ŒReWatch-R1 åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å½“å‰æœ€å…ˆè¿›(SOTA)çš„å¹³å‡æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„å¤æ‚è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23652v2",
      "published_date": "2025-09-28 05:38:16 UTC",
      "updated_date": "2025-10-01 15:33:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:17.178944+00:00"
    },
    {
      "arxiv_id": "2509.25264v2",
      "title": "GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries",
      "title_zh": "GeoSQL-Evalï¼šé’ˆå¯¹åŸºäº PostGIS çš„ NL2GeoSQL æŸ¥è¯¢çš„å¤§è¯­è¨€æ¨¡å‹é¦–æ¬¡è¯„ä¼°",
      "authors": [
        "Shuyang Hou",
        "Haoyue Jiao",
        "Ziqi Liu",
        "Lutong Xie",
        "Guanyu Chen",
        "Shaowen Wu",
        "Xuefeng Guan",
        "Huayi Wu"
      ],
      "abstract": "Large language models (LLMs) have shown strong performance in natural language to SQL (NL2SQL) tasks within general databases. However, extending to GeoSQL introduces additional complexity from spatial data types, function invocation, and coordinate systems, which greatly increases generation and execution difficulty. Existing benchmarks mainly target general SQL, and a systematic evaluation framework for GeoSQL is still lacking. To fill this gap, we present GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing LLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task categories-conceptual understanding, syntax-level SQL generation, and schema retrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic databases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model, covering four cognitive dimensions, five capability levels, and twenty task types to establish a comprehensive process from knowledge acquisition and syntax generation to semantic alignment, execution accuracy, and robustness. We evaluate 24 representative models across six categories and apply the entropy weight method with statistical analyses to uncover performance differences, common error patterns, and resource usage. Finally, we release a public GeoSQL-Eval leaderboard platform for continuous testing and global comparison. This work extends the NL2GeoSQL paradigm and provides a standardized, interpretable, and extensible framework for evaluating LLMs in spatial database contexts, offering valuable references for geospatial information science and related applications.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤„ç†åŒ…å«ç©ºé—´æ•°æ®ç±»å‹å’Œåæ ‡ç³»çš„GeoSQLæ—¶é¢ä¸´çš„ç”Ÿæˆä¸æ‰§è¡ŒæŒ‘æˆ˜ï¼Œæå‡ºäº†é¦–ä¸ªé’ˆå¯¹PostGISæŸ¥è¯¢ç”Ÿæˆçš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶GeoSQL-Evalä»¥åŠåŸºå‡†æµ‹è¯•GeoSQL-Benchã€‚GeoSQL-Benchæ¶µç›–äº†æ¦‚å¿µç†è§£ã€è¯­æ³•çº§SQLç”Ÿæˆå’Œæ¶æ„æ£€ç´¢ä¸‰å¤§ä»»åŠ¡ï¼ŒåŒ…å«14,178ä¸ªå®ä¾‹ã€340ä¸ªPostGISå‡½æ•°åŠ82ä¸ªä¸“é¢˜æ•°æ®åº“ã€‚GeoSQL-Evalæ¡†æ¶åŸºäºWebbçš„Depth of Knowledge (DOK)æ¨¡å‹ï¼Œä»å››ä¸ªè®¤çŸ¥ç»´åº¦ã€äº”ä¸ªèƒ½åŠ›å±‚çº§å’ŒäºŒåç§ä»»åŠ¡ç±»å‹å‡ºå‘ï¼Œå»ºç«‹äº†æ¶µç›–çŸ¥è¯†è·å–ã€è¯­ä¹‰å¯¹é½ã€æ‰§è¡Œå‡†ç¡®æ€§å’Œé²æ£’æ€§çš„ç»¼åˆè¯„ä»·ä½“ç³»ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¯¹24ä¸ªä»£è¡¨æ€§æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œåˆ©ç”¨ç†µæƒæ³•(entropy weight method)æ­ç¤ºäº†å„æ¨¡å‹åœ¨æ€§èƒ½ã€é”™è¯¯æ¨¡å¼åŠèµ„æºä½¿ç”¨ä¸Šçš„å·®å¼‚ã€‚è¯¥å·¥ä½œæ‰©å±•äº†NL2GeoSQLèŒƒå¼ï¼Œä¸ºåœ°ç†ç©ºé—´ä¿¡æ¯ç§‘å­¦é¢†åŸŸçš„LLMåº”ç”¨æä¾›äº†æ ‡å‡†åŒ–ã€å¯è§£é‡Šä¸”å¯æ‰©å±•çš„è¯„ä¼°å‚è€ƒï¼Œå¹¶åŒæ­¥å‘å¸ƒäº†å…¬å¼€çš„åœ¨çº¿æ’è¡Œæ¦œã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.DB",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25264v2",
      "published_date": "2025-09-28 04:50:48 UTC",
      "updated_date": "2025-10-02 13:58:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:24.592486+00:00"
    },
    {
      "arxiv_id": "2509.23639v1",
      "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
      "title_zh": "LightFairï¼šé€šè¿‡é¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨å»åå®ç°å…¬å¹³ T2I æ‰©æ•£çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆ",
      "authors": [
        "Boyu Han",
        "Qianqian Xu",
        "Shilong Bao",
        "Zhiyong Yang",
        "Kangli Zi",
        "Qingming Huang"
      ],
      "abstract": "This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º LightFair çš„è½»é‡åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨ï¼ˆText Encoderï¼‰çš„æ–‡æœ¬åµŒå…¥ï¼ˆText Embeddingï¼‰ï¼Œè§£å†³æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆText-to-Image Diffusion Modelsï¼‰ä¸­çš„åè§é—®é¢˜ã€‚ä½œè€…å‘ç°æ–‡æœ¬ç¼–ç å™¨çš„ä¸­æ€§åµŒå…¥åœ¨ CLIP ç©ºé—´ä¸­é’ˆå¯¹ä¸åŒå±æ€§è¡¨ç°å‡ºæ˜¾è‘—åæ–œï¼Œä¸”è¿™ç§ä¸å¹³è¡¡ä¼šè¢«å™ªå£°é¢„æµ‹ç½‘ç»œè¿›ä¸€æ­¥æ”¾å¤§ã€‚ä¸ºæ­¤ï¼ŒLightFair æå‡ºäº†ä¸€ç§åä½œè·ç¦»çº¦æŸå»åç­–ç•¥ï¼ˆCollaborative Distance-Constrained Debiasingï¼‰ï¼Œé€šè¿‡å¹³è¡¡åµŒå…¥è·ç¦»åœ¨æ— éœ€è¾…åŠ©å‚è€ƒçš„æƒ…å†µä¸‹æå‡å…¬å¹³æ€§ã€‚é’ˆå¯¹å»åå¯èƒ½å¯¼è‡´çš„ç”Ÿæˆè´¨é‡ä¸‹é™ï¼Œç ”ç©¶å¼•å…¥äº†ä¸¤é˜¶æ®µæ–‡æœ¬å¼•å¯¼é‡‡æ ·ç­–ç•¥ï¼ˆTwo-Stage Text-Guided Samplingï¼‰æ¥ç²¾ç¡®æ§åˆ¶å»åç¼–ç å™¨çš„å¹²é¢„æ—¶æœºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLightFair åœ¨ Stable Diffusion v1.5 ä¸Šä»¥ä»…å››åˆ†ä¹‹ä¸€çš„è®­ç»ƒè´Ÿæ‹…å®ç°äº† SOTA çš„å»åæ•ˆæœï¼Œä¸”å‡ ä¹ä¸å¢åŠ é‡‡æ ·è´Ÿæ‹…ã€‚è¿™ä¸€æ–¹æ¡ˆè¯æ˜äº†é€šè¿‡é’ˆå¯¹æ€§å¾®è°ƒå‰ç«¯æ¨¡å—å³å¯é«˜æ•ˆå®ç°å…¬å¹³ä¸”é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23639v1",
      "published_date": "2025-09-28 04:46:39 UTC",
      "updated_date": "2025-09-28 04:46:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:29.385144+00:00"
    },
    {
      "arxiv_id": "2509.23630v1",
      "title": "Game-Oriented ASR Error Correction via RAG-Enhanced LLM",
      "title_zh": "åŸºäº RAG å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„é¢å‘æ¸¸æˆ ASR çº é”™",
      "authors": [
        "Yan Jiang",
        "Yongle Luo",
        "Qixian Zhou",
        "Elvis S. Liu"
      ],
      "abstract": "With the rise of multiplayer online games, real-time voice communication is essential for team coordination. However, general ASR systems struggle with gaming-specific challenges like short phrases, rapid speech, jargon, and noise, leading to frequent errors. To address this, we propose the GO-AEC framework, which integrates large language models, Retrieval-Augmented Generation (RAG), and a data augmentation strategy using LLMs and TTS. GO-AEC includes data augmentation, N-best hypothesis-based correction, and a dynamic game knowledge base. Experiments show GO-AEC reduces character error rate by 6.22% and sentence error rate by 29.71%, significantly improving ASR accuracy in gaming scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šäººåœ¨çº¿æ¸¸æˆä¸­é€šç”¨ ASR ç³»ç»Ÿå› çŸ­è¯­ã€å¿«é€Ÿè¯­é€Ÿã€æ¸¸æˆæœ¯è¯­(jargon)åŠå™ªå£°å¯¼è‡´çš„è¯†åˆ«é”™è¯¯ï¼Œæå‡ºäº† GO-AEC çº é”™æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯å¢å¼ºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„æ€§èƒ½ï¼Œå¹¶ç»“åˆäº†åŸºäº LLM å’Œæ–‡æœ¬è½¬è¯­éŸ³(TTS)çš„æ•°æ®å¢å¼ºç­–ç•¥ã€‚é€šè¿‡ N-best hypothesis çº é”™æœºåˆ¶å’ŒåŠ¨æ€æ¸¸æˆçŸ¥è¯†åº“çš„é›†æˆï¼ŒGO-AEC èƒ½å¤Ÿç²¾å‡†æ•æ‰æ¸¸æˆè¯­å¢ƒä¸‹çš„ç‰¹å®šè¡¨è¾¾ã€‚å®éªŒæ•°æ®è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å°†å­—ç¬¦é”™è¯¯ç‡(CER)é™ä½äº† 6.22%ï¼Œå¥å­é”™è¯¯ç‡(SER)æ˜¾è‘—é™ä½äº† 29.71%ã€‚è¿™ä¸€ç ”ç©¶æˆæœæ˜¾è‘—æå‡äº†æ¸¸æˆåœºæ™¯ä¸­å®æ—¶è¯­éŸ³é€šä¿¡çš„å‡†ç¡®æ€§ï¼Œä¸ºå¤æ‚ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«çº é”™æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23630v1",
      "published_date": "2025-09-28 04:12:07 UTC",
      "updated_date": "2025-09-28 04:12:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:32.991044+00:00"
    },
    {
      "arxiv_id": "2509.23629v2",
      "title": "How LLMs Learn to Reason: A Complex Network Perspective",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹å¦‚ä½•å­¦ä¼šæ¨ç†ï¼šåŸºäºå¤æ‚ç½‘ç»œçš„è§†è§’",
      "authors": [
        "Sihan Hu",
        "Xiansheng Cai",
        "Yuan Huang",
        "Zhiyuan Yao",
        "Linfeng Zhang",
        "Pan Zhang",
        "Youjin Deng",
        "Kun Chen"
      ],
      "abstract": "Training large language models with Reinforcement Learning with Verifiable Rewards (RLVR) exhibits a set of distinctive and puzzling behaviors that remain poorly understood, including a two-stage learning curve, a V-shaped response-length trajectory, and a pronounced vulnerability to catastrophic forgetting. In this work, we propose that these behaviors are emergent collective phenomena governed not by neural implementation details, but by the topological evolution of the latent reasoning graph in semantic space. By demonstrating a dynamical isomorphism between a 1.5B-parameter LLM and a minimal Concept Network Model (CoNet), we trace the causal source to the self-organization of a sparse concept web pinned to an average degree of two. This geometric perspective provides a unified physical explanation for the observed anomalies: the V-shaped trajectory tracks the evolution from parallel local skill optimization to global network integration; catastrophic forgetting stems from the topological disconnection of critical ``trunk'' edges; and policy collapse arises from the accumulation of sequential transitions at the web's leaf nodes, where broad exploration abruptly freezes into rigid, high-reward trajectories. Identifying a ``maximally frustrated state'' at the transition between learning stages, we propose Annealed-RLVR, a principled algorithm that injects a targeted SFT ``heating'' step to resolve this topological bottleneck. Experiments confirm that this theory-driven intervention outperforms standard RLVR on both in-distribution and out-of-distribution benchmarks (including Minerva and AIME). By recasting RLVR from black-box optimization into a predictable process of structural self-organization, our work provides a new physical intuition for engineering the emergent reasoning capabilities of future AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ä»å¤æ‚ç½‘ç»œçš„è§†è§’æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨ä½¿ç”¨å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ (RLVR)è¿›è¡Œæ¨ç†å­¦ä¹ æ—¶å‡ºç°çš„ç‹¬ç‰¹è¡Œä¸ºã€‚ä½œè€…é€šè¿‡Concept Network Model (CoNet)æ­ç¤ºäº†æ¨¡å‹æ€§èƒ½ä¸è¯­ä¹‰ç©ºé—´ä¸­æ½œåœ¨æ¨ç†å›¾æ‹“æ‰‘æ¼”åŒ–ä¹‹é—´çš„åŠ¨åŠ›å­¦åŒæ„æ€§ï¼Œä¸ºVå‹å“åº”é•¿åº¦è½¨è¿¹å’Œç¾éš¾æ€§é—å¿˜ç­‰ç°è±¡æä¾›äº†ç»Ÿä¸€çš„ç‰©ç†é€»è¾‘ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™äº›å¼‚å¸¸ç°è±¡æºäºä»å±€éƒ¨æŠ€èƒ½ä¼˜åŒ–å‘å…¨å±€ç½‘ç»œé›†æˆçš„æ¼”åŒ–è¿‡ç¨‹ï¼Œä»¥åŠå…³é”®æ‹“æ‰‘è¾¹ç¼˜çš„æ–­è£‚å¯¼è‡´çš„è¿æ¥æ€§æŸå¤±ã€‚é’ˆå¯¹å­¦ä¹ é˜¶æ®µè½¬æ¢ä¸­å‡ºç°çš„â€œæå¤§æŒ«è´¥æ€â€ç“¶é¢ˆï¼Œç ”ç©¶æå‡ºäº†Annealed-RLVRç®—æ³•ï¼Œé€šè¿‡å¼•å…¥é’ˆå¯¹æ€§çš„ç›‘ç£å¾®è°ƒ(SFT)â€œåŠ çƒ­â€æ­¥éª¤æ¥ä¼˜åŒ–æ‹“æ‰‘ç»“æ„ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨Minervaå’ŒAIMEç­‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»ŸRLVRã€‚æ­¤é¡¹å·¥ä½œå°†RLVRä»é»‘ç›’ä¼˜åŒ–è½¬å˜ä¸ºå¯é¢„æµ‹çš„ç»“æ„åŒ–è‡ªç»„ç»‡è¿‡ç¨‹ï¼Œä¸ºæœªæ¥å·¥ç¨‹åŒ–AIç³»ç»Ÿçš„æ¶Œç°æ¨ç†èƒ½åŠ›æä¾›äº†å…¨æ–°çš„ç‰©ç†ç›´è§‰ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.dis-nn",
        "cond-mat.stat-mech",
        "cs.LG",
        "physics.soc-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "24 pages, 11 figures, 1 table, under review as a conference paper at ICLR 2026",
      "pdf_url": "https://arxiv.org/pdf/2509.23629v2",
      "published_date": "2025-09-28 04:10:37 UTC",
      "updated_date": "2025-11-21 10:27:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:38.797787+00:00"
    },
    {
      "arxiv_id": "2509.23625v1",
      "title": "RIV: Recursive Introspection Mask Diffusion Vision Language Model",
      "title_zh": "RIVï¼šé€’å½’å†…çœæ©ç æ‰©æ•£è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "YuQian Li",
        "Limeng Qiao",
        "Lin Ma"
      ],
      "abstract": "Mask Diffusion-based Vision Language Models (MDVLMs) have achieved remarkable progress in multimodal understanding tasks. However, these models are unable to correct errors in generated tokens, meaning they lack self-correction capability. In this paper, we propose Recursive Introspection Mask Diffusion Vision Language Model (RIV), which equips the model with self-correction ability through two novel mechanisms. The first is Introspection Training, where an Introspection Model is introduced to identify errors within generated sequences. Introspection Training enables the model to detect not only grammatical and spelling mistakes, but more importantly, logical errors. The second is Recursive Inference. Beginning with the standard unmasking step, the learned Introspection Model helps to identify errors in the output sequence and remask them. This alternating ($\\text{unmask}\\rightarrow\\text{introspection}\\rightarrow\\text{remask}$) process is repeated recursively until reliable results are obtained. Experimental results on multiple benchmarks demonstrate that the proposed RIV achieves state-of-the-art performance, outperforming most existing MDVLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Mask Diffusion-based Vision Language Models (MDVLMs) ç¼ºä¹è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ã€æ— æ³•çº æ­£ç”Ÿæˆ token é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº† Recursive Introspection Mask Diffusion Vision Language Model (RIV)ã€‚RIV é€šè¿‡å¼•å…¥ Introspection Training æœºåˆ¶ï¼Œåˆ©ç”¨ä¸“é—¨çš„ Introspection Model è¯†åˆ«ç”Ÿæˆåºåˆ—ä¸­çš„è¯­æ³•ã€æ‹¼å†™ä»¥åŠæ ¸å¿ƒçš„é€»è¾‘é”™è¯¯ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§ Recursive Inference ç­–ç•¥ï¼Œé€šè¿‡å¾ªç¯æ‰§è¡Œ unmaskã€introspection ä»¥åŠ remask çš„è¿‡ç¨‹ï¼Œé€’å½’åœ°è¯†åˆ«å¹¶é‡æ–°å¤„ç†é¢„æµ‹ä¸­çš„é”™è¯¯ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒRIV åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† state-of-the-art çš„è¡¨ç°ï¼Œæ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰çš„ MDVLMsã€‚è¿™ä¸€æ¡†æ¶ä¸ºå¢å¼ºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„è‡ªçº é”™èƒ½åŠ›ä¸é€»è¾‘å¯é æ€§æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23625v1",
      "published_date": "2025-09-28 04:01:46 UTC",
      "updated_date": "2025-09-28 04:01:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:34:38.408545+00:00"
    },
    {
      "arxiv_id": "2509.23619v2",
      "title": "Reasoning Scaffolding: Distilling the Flow of Thought from LLMs",
      "title_zh": "æ¨ç†æ”¯æ¶ï¼šä»å¤§è¯­è¨€æ¨¡å‹ä¸­è’¸é¦æ€ç»´æµ",
      "authors": [
        "Xiangyu Wen",
        "Junhua Huang",
        "Zeju Li",
        "Min Li",
        "Jianyuan Zhong",
        "Zhijian Xu",
        "Mingxuan Yuan",
        "Yongxiang Huang",
        "Qiang Xu"
      ],
      "abstract": "The prevailing approach to distilling reasoning from Large Language Models (LLMs)-behavioral cloning from textual rationales-is fundamentally limited. It teaches Small Language Models (SLMs) to mimic surface-level patterns rather than the underlying algorithmic structure of thought, resulting in a critical lack of logical robustness. We argue that instead of cloning text, distillation should transfer this algorithmic structure directly. We introduce Reasoning Scaffolding}, a framework that reframes reasoning as a structured generation process. Our method first abstracts the teacher's thought process into a sequence of discrete, interpretable semantic signals (e.g., Contrast, Addition) that act as a scaffold. The student model is then trained via a multi-task objective to both (1)predict the next semantic signal, anticipating the reasoning flow, and (2)generate the corresponding step, conditioned on that signal. This multi-task scheme acts as a powerful regularizer, compelling the student to internalize the computational patterns of coherent reasoning. On a suite of challenging reasoning benchmarks, our method significantly outperforms state-of-the-art distillation in both accuracy and logical consistency, providing a path towards creating smaller models that are genuine reasoners, not just fluent mimics.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ä»å¤§è¯­è¨€æ¨¡å‹(LLMs)è’¸é¦æ¨ç†èƒ½åŠ›çš„æ–¹æ³•å› ä»…å…‹éš†æ–‡æœ¬ç†ç”±è€Œå¯¼è‡´å°è¯­è¨€æ¨¡å‹(SLMs)ç¼ºä¹é€»è¾‘å¥å£®æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Reasoning Scaffolding æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ¨ç†é‡æ„ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œç›´æ¥è½¬ç§»ç®—æ³•æ€ç»´ç»“æ„è€Œéè¡¨é¢æ–‡æœ¬ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ•™å¸ˆæ¨¡å‹çš„æ€è€ƒè·¯å¾„æŠ½è±¡ä¸ºç¦»æ•£ä¸”å¯è§£é‡Šçš„è¯­ä¹‰ä¿¡å·(semantic signals)ï¼Œå¦‚ Contrast æˆ– Additionï¼Œä¸ºå­¦ç”Ÿæ¨¡å‹æ„å»ºæ¨ç†æ”¯æ¶ã€‚å­¦ç”Ÿæ¨¡å‹é‡‡ç”¨å¤šä»»åŠ¡ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œéœ€åŒæ—¶é¢„æµ‹ä¸‹ä¸€ä¸ªè¯­ä¹‰ä¿¡å·å¹¶æ®æ­¤ç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œä»è€Œæœ‰æ•ˆå†…åŒ–è¿è´¯æ¨ç†çš„è®¡ç®—æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡å’Œé€»è¾‘ä¸€è‡´æ€§å‡æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„è’¸é¦æŠ€æœ¯ã€‚è¿™ä¸€æˆæœä¸ºåˆ›å»ºå…·å¤‡çœŸå®æ¨ç†èƒ½åŠ›è€Œéå•çº¯æ¨¡ä»¿èƒ½åŠ›çš„å°å‹è¯­è¨€æ¨¡å‹æä¾›äº†é‡è¦è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23619v2",
      "published_date": "2025-09-28 03:49:32 UTC",
      "updated_date": "2025-10-01 08:57:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:12.686914+00:00"
    },
    {
      "arxiv_id": "2509.23618v1",
      "title": "Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment",
      "title_zh": "åŸºäºä¿¡æ¯ç“¶é¢ˆå¢å¼ºå¯¹æŠ—å¯¹é½çš„å¯æ³›åŒ–è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹",
      "authors": [
        "Pu Huang",
        "Shouguang Wang",
        "Siya Yao",
        "Mengchu Zhou"
      ],
      "abstract": "Neural speech synthesis techniques have enabled highly realistic speech deepfakes, posing major security risks. Speech deepfake detection is challenging due to distribution shifts across spoofing methods and variability in speakers, channels, and recording conditions. We explore learning shared discriminative features as a path to robust detection and propose Information Bottleneck enhanced Confidence-Aware Adversarial Network (IB-CAAN). Confidence-guided adversarial alignment adaptively suppresses attack-specific artifacts without erasing discriminative cues, while the information bottleneck removes nuisance variability to preserve transferable features. Experiments on ASVspoof 2019/2021, ASVspoof 5, and In-the-Wild demonstrate that IB-CAAN consistently outperforms baseline and achieves state-of-the-art performance on many benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¥ç»è¯­éŸ³åˆæˆæŠ€æœ¯å¸¦æ¥çš„å®‰å…¨é£é™©ï¼Œæ¢è®¨äº†åœ¨ä¸åŒä¼ªé€ æ–¹æ³•ã€è¯´è¯äººå’Œå½•éŸ³ç¯å¢ƒä¸‹å®ç°é€šç”¨è¯­éŸ³ä¼ªé€ æ£€æµ‹(Speech Deepfake Detection)çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¿¡æ¯ç“¶é¢ˆå¢å¼ºçš„ç½®ä¿¡åº¦æ„ŸçŸ¥å¯¹æŠ—ç½‘ç»œ(Information Bottleneck enhanced Confidence-Aware Adversarial Network, IB-CAAN)ï¼Œæ—¨åœ¨å­¦ä¹ å…±äº«çš„åˆ¤åˆ«æ€§ç‰¹å¾ä»¥åº”å¯¹åˆ†å¸ƒåç§»ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç½®ä¿¡åº¦å¼•å¯¼çš„å¯¹æŠ—å¯¹é½(Confidence-guided adversarial alignment)è‡ªé€‚åº”åœ°æŠ‘åˆ¶ç‰¹å®šæ”»å‡»çš„ä¼ªå½±ï¼ŒåŒæ—¶é€šè¿‡ä¿¡æ¯ç“¶é¢ˆ(Information Bottleneck)å»é™¤å†—ä½™å˜å¼‚ï¼Œä»è€Œæœ‰æ•ˆä¿ç•™å¯è¿ç§»çš„ç‰¹å¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIB-CAAN åœ¨ ASVspoof 2019/2021ã€ASVspoof 5 ä»¥åŠ In-the-Wild ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰åŸºå‡†æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚ç¯å¢ƒå’ŒæœªçŸ¥æ”»å‡»æ‰‹æ®µæ—¶çš„ç¨³å¥æ€§ä¸é€šç”¨æ€§ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23618v1",
      "published_date": "2025-09-28 03:48:49 UTC",
      "updated_date": "2025-09-28 03:48:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:01.104986+00:00"
    },
    {
      "arxiv_id": "2509.23617v1",
      "title": "BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images",
      "title_zh": "BioVessel-Net ä¸ RetinaMixï¼šåŸºäº OCTA å›¾åƒçš„æ— ç›‘ç£è§†ç½‘è†œè¡€ç®¡åˆ†å‰²",
      "authors": [
        "Cheng Huang",
        "Weizheng Xie",
        "Fan Gao",
        "Yutong Liu",
        "Ruoling Wu",
        "Zeyu Han",
        "Jingxi Qiu",
        "Xiangxiang Wang",
        "Zhenglin Yang",
        "Hao Wang",
        "Yongbin Yu"
      ],
      "abstract": "Structural changes in retinal blood vessels are critical biomarkers for the onset and progression of glaucoma and other ocular diseases. However, current vessel segmentation approaches largely rely on supervised learning and extensive manual annotations, which are costly, error-prone, and difficult to obtain in optical coherence tomography angiography. Here we present BioVessel-Net, an unsupervised generative framework that integrates vessel biostatistics with adversarial refinement and a radius-guided segmentation strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular structures with biostatistical coherence, achieving accurate and explainable vessel extraction without labeled data or high-performance computing. To support training and evaluation, we introduce RetinaMix, a new benchmark dataset of 2D and 3D OCTA images with high-resolution vessel details from diverse populations. Experimental results demonstrate that BioVessel-Net achieves near-perfect segmentation accuracy across RetinaMix and existing datasets, substantially outperforming state-of-the-art supervised and semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a label-free, computationally efficient, and clinically interpretable solution for retinal vessel analysis, with broad potential for glaucoma monitoring, blood flow modeling, and progression prediction. Code and dataset are available: https://github.com/VikiXie/SatMar8.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…‰å­¦ç›¸å¹²æ–­å±‚æ‰«æè¡€ç®¡é€ å½±(OCTA)å›¾åƒä¸­è§†ç½‘è†œè¡€ç®¡åˆ†å‰²è¿‡åº¦ä¾èµ–æ˜‚è´µæ‰‹å·¥æ ‡æ³¨çš„é—®é¢˜ï¼Œæå‡ºäº†BioVessel-Netï¼Œä¸€ç§æ— ç›‘ç£ç”Ÿæˆæ¡†æ¶(unsupervised generative framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡æ•´åˆè¡€ç®¡ç”Ÿç‰©ç»Ÿè®¡å­¦(vessel biostatistics)ã€å¯¹æŠ—æ€§ä¼˜åŒ–(adversarial refinement)ä»¥åŠåŠå¾„å¼•å¯¼åˆ†å‰²ç­–ç•¥(radius-guided segmentation strategy)ï¼Œç›´æ¥å¯¹è¡€ç®¡ç»“æ„è¿›è¡Œå»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºåƒç´ çš„æ–¹æ³•ä¸åŒï¼ŒBioVessel-Netåœ¨æ— éœ€æ ‡æ³¨æ•°æ®æˆ–é«˜æ€§èƒ½è®¡ç®—çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†ç²¾ç¡®ä¸”å…·æœ‰ç”Ÿç‰©ç»Ÿè®¡ä¸€è‡´æ€§çš„è¡€ç®¡æå–ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†RetinaMixåŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†æ¥è‡ªä¸åŒäººç¾¤çš„é«˜åˆ†è¾¨ç‡2Då’Œ3D OCTAå›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBioVessel-Netåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åˆ†å‰²ç²¾åº¦å¤§å¹…ä¼˜äºç°æœ‰çš„æœ‰ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ã€‚æ€»ä¹‹ï¼ŒBioVessel-Netå’ŒRetinaMixä¸ºè§†ç½‘è†œè¡€ç®¡åˆ†ææä¾›äº†ä¸€ç§æ— éœ€æ ‡ç­¾ã€è®¡ç®—é«˜æ•ˆä¸”ä¸´åºŠå¯è§£é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨é’å…‰çœ¼ç›‘æµ‹å’Œç—…æƒ…é¢„æµ‹ç­‰é¢†åŸŸå…·æœ‰é‡è¦åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23617v1",
      "published_date": "2025-09-28 03:46:20 UTC",
      "updated_date": "2025-09-28 03:46:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:03.589100+00:00"
    },
    {
      "arxiv_id": "2509.23616v1",
      "title": "GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning",
      "title_zh": "GraphIFEï¼šé€šè¿‡ä¸å˜æ€§å­¦ä¹ é‡æ–°å®¡è§†å›¾ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»",
      "authors": [
        "Fanlong Zeng",
        "Wensheng Gan",
        "Philip S. Yu"
      ],
      "abstract": "The class imbalance problem refers to the disproportionate distribution of samples across different classes within a dataset, where the minority classes are significantly underrepresented. This issue is also prevalent in graph-structured data. Most graph neural networks (GNNs) implicitly assume a balanced class distribution and therefore often fail to account for the challenges introduced by class imbalance, which can lead to biased learning and degraded performance on minority classes. We identify a quality inconsistency problem in synthesized nodes, which leads to suboptimal performance under graph imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph Invariant Feature Extraction), a novel framework designed to mitigate quality inconsistency in synthesized nodes. Our approach incorporates two key concepts from graph invariant learning and introduces strategies to strengthen the embedding space representation, thereby enhancing the model's ability to identify invariant features. Extensive experiments demonstrate the framework's efficiency and robust generalization, as GraphIFE consistently outperforms various baselines across multiple datasets. The code is publicly available at https://github.com/flzeng1/GraphIFE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç»“æ„æ•°æ®ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡(class imbalance)é—®é¢˜ï¼ŒæŒ‡å‡ºå°‘æ•°ç±»æ ·æœ¬ä¸è¶³ä¼šå¯¼è‡´å›¾ç¥ç»ç½‘ç»œ(GNNs)äº§ç”Ÿå­¦ä¹ åè§å¹¶é™ä½æ€§èƒ½ã€‚ä½œè€…è¯†åˆ«å‡ºåˆæˆèŠ‚ç‚¹ä¸­å­˜åœ¨è´¨é‡ä¸ä¸€è‡´(quality inconsistency)é—®é¢˜ï¼Œè¿™æ˜¯å¯¼è‡´å›¾ä¸å¹³è¡¡æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†GraphIFE (Graph Invariant Feature Extraction)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸å˜å­¦ä¹ (invariant learning)æ¥ç¼“è§£åˆæˆèŠ‚ç‚¹çš„è´¨é‡ä¸ä¸€è‡´ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å›¾ä¸å˜å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶é€šè¿‡å¼ºåŒ–åµŒå…¥ç©ºé—´(embedding space)è¡¨ç¤ºçš„ç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹è¯†åˆ«ä¸å˜ç‰¹å¾(invariant features)çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGraphIFEåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œå±•ç°å‡ºæé«˜çš„æ•ˆç‡å’Œé²æ£’æ³›åŒ–æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ºå¤„ç†å¤æ‚å›¾æ•°æ®ä¸­çš„ä¸å¹³è¡¡èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”ç¨³å¥çš„æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "PrePrint, 16 pages, 7 tables, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23616v1",
      "published_date": "2025-09-28 03:41:16 UTC",
      "updated_date": "2025-09-28 03:41:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:10.299585+00:00"
    },
    {
      "arxiv_id": "2509.23614v1",
      "title": "PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents",
      "title_zh": "PSG-Agentï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ä¸ªæ€§åŒ–æ„ŸçŸ¥å®‰å…¨æŠ¤æ ",
      "authors": [
        "Yaozu Wu",
        "Jizhou Guo",
        "Dongyuan Li",
        "Henry Peng Zou",
        "Wei-Chieh Huang",
        "Yankai Chen",
        "Zhen Wang",
        "Weizhi Zhang",
        "Yangning Li",
        "Meng Zhang",
        "Renhe Jiang",
        "Philip S. Yu"
      ],
      "abstract": "Effective guardrails are essential for safely deploying LLM-based agents in critical applications. Despite recent advances, existing guardrails suffer from two fundamental limitations: (i) they apply uniform guardrail policies to all users, ignoring that the same agent behavior can harm some users while being safe for others; (ii) they check each response in isolation, missing how risks evolve and accumulate across multiple interactions. To solve these issues, we propose PSG-Agent, a personalized and dynamic system for LLM-based agents. First, PSG-Agent creates personalized guardrails by mining the interaction history for stable traits and capturing real-time states from current queries, generating user-specific risk thresholds and protection strategies. Second, PSG-Agent implements continuous monitoring across the agent pipeline with specialized guards, including Plan Monitor, Tool Firewall, Response Guard, Memory Guardian, that track cross-turn risk accumulation and issue verifiable verdicts. Finally, we validate PSG-Agent in multiple scenarios including healthcare, finance, and daily life automation scenarios with diverse user profiles. It significantly outperform existing agent guardrails including LlamaGuard3 and AGrail, providing an executable and auditable path toward personalized safety for LLM-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PSG-Agentï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ (LLM-based agents) çš„ä¸ªæ€§åŒ–ä¸”åŠ¨æ€çš„å®‰å…¨æŠ¤æ ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ¤æ é‡‡ç”¨ç»Ÿä¸€æ”¿ç­–ä¸”å¿½ç•¥å¤šè½®äº¤äº’ä¸­é£é™©ç´¯ç§¯çš„å±€é™æ€§ã€‚PSG-Agent é¦–å…ˆé€šè¿‡æŒ–æ˜äº¤äº’å†å²ä¸­çš„ç¨³å®šç‰¹å¾å¹¶æ•è·å½“å‰æŸ¥è¯¢çš„å®æ—¶çŠ¶æ€ï¼Œä¸ºç‰¹å®šç”¨æˆ·ç”Ÿæˆå·®å¼‚åŒ–çš„é£é™©é˜ˆå€¼å’Œä¿æŠ¤ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåœ¨æ™ºèƒ½ä½“æµæ°´çº¿ä¸­éƒ¨ç½²äº† Plan Monitorã€Tool Firewallã€Response Guard å’Œ Memory Guardian ç­‰ä¸“ç”¨ç›‘æ§å™¨ï¼Œå®ç°äº†å¯¹è·¨è½®æ¬¡é£é™©ç´¯ç§¯çš„æŒç»­ç›‘æµ‹å¹¶æä¾›å¯éªŒè¯çš„è£å†³ç»“æœã€‚åœ¨åŒ»ç–—ã€é‡‘èå’Œæ—¥å¸¸è‡ªåŠ¨åŒ–åœºæ™¯ä¸‹çš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒPSG-Agent çš„å®‰å…¨æ€§è¡¨ç°æ˜¾è‘—ä¼˜äº LlamaGuard3 å’Œ AGrail ç­‰ç°æœ‰åŸºå‡†æ¨¡å‹ï¼Œä¸ºå®ç°å¯æ‰§è¡Œä¸”å¯å®¡è®¡çš„ä¸ªæ€§åŒ–æ™ºèƒ½ä½“å®‰å…¨ä¿éšœæä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23614v1",
      "published_date": "2025-09-28 03:31:59 UTC",
      "updated_date": "2025-09-28 03:31:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:19.591894+00:00"
    },
    {
      "arxiv_id": "2509.23612v1",
      "title": "InteractMove: Text-Controlled Human-Object Interaction Generation in 3D Scenes with Movable Objects",
      "title_zh": "InteractMoveï¼šåŒ…å«å¯ç§»åŠ¨ç‰©ä½“çš„ 3D åœºæ™¯ä¸‹æ–‡æœ¬æ§åˆ¶çš„äººç‰©äº¤äº’ç”Ÿæˆ",
      "authors": [
        "Xinhao Cai",
        "Minghang Zheng",
        "Xin Jin",
        "Yang Liu"
      ],
      "abstract": "We propose a novel task of text-controlled human object interaction generation in 3D scenes with movable objects. Existing human-scene interaction datasets suffer from insufficient interaction categories and typically only consider interactions with static objects (do not change object positions), and the collection of such datasets with movable objects is difficult and costly. To address this problem, we construct the InteractMove dataset for Movable Human-Object Interaction in 3D Scenes by aligning existing human object interaction data with scene contexts, featuring three key characteristics: 1) scenes containing multiple movable objects with text-controlled interaction specifications (including same-category distractors requiring spatial and 3D scene context understanding), 2) diverse object types and sizes with varied interaction patterns (one-hand, two-hand, etc.), and 3) physically plausible object manipulation trajectories. With the introduction of various movable objects, this task becomes more challenging, as the model needs to identify objects to be interacted with accurately, learn to interact with objects of different sizes and categories, and avoid collisions between movable objects and the scene. To tackle such challenges, we propose a novel pipeline solution. We first use 3D visual grounding models to identify the interaction object. Then, we propose a hand-object joint affordance learning to predict contact regions for different hand joints and object parts, enabling accurate grasping and manipulation of diverse objects. Finally, we optimize interactions with local-scene modeling and collision avoidance constraints, ensuring physically plausible motions and avoiding collisions between objects and the scene. Comprehensive experiments demonstrate our method's superiority in generating physically plausible, text-compliant interactions compared to existing approaches.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€é¡¹æ–°é¢–çš„ä»»åŠ¡ï¼Œå³åœ¨åŒ…å«å¯ç§»åŠ¨ç‰©ä½“çš„3Dåœºæ™¯ä¸­ç”Ÿæˆå—æ–‡æœ¬æ§åˆ¶çš„äººæœºç‰©ä½“äº¤äº’ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†äº¤äº’ç±»åˆ«ä¸è¶³ä¸”ä»…è€ƒè™‘é™æ€ç‰©ä½“çš„é—®é¢˜ï¼Œä½œè€…æ„å»ºäº† InteractMove æ•°æ®é›†ï¼Œå…¶å…·å¤‡å¤šç‰©ä½“æ–‡æœ¬æ§åˆ¶è§„èŒƒã€å¤šæ ·åŒ–ç‰©ä½“å°ºå¯¸ä»¥åŠç‰©ç†åˆç†çš„æ“çºµè½¨è¿¹ç­‰å…³é”®ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¯†åˆ«äº¤äº’ç›®æ ‡ã€å­¦ä¹ å¤šæ ·åŒ–äº¤äº’æ¨¡å¼åŠé¿å…ç¢°æ’ç­‰æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æµæ°´çº¿æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé¦–å…ˆé€šè¿‡ 3D visual grounding æ¨¡å‹ç¡®å®šç›®æ ‡ï¼Œæ¥ç€åˆ©ç”¨æ‰‹éƒ¨-ç‰©ä½“è”åˆç¤ºèƒ½æ€§å­¦ä¹  (hand-object joint affordance learning) é¢„æµ‹æ¥è§¦åŒºåŸŸä»¥å®ç°ç²¾å‡†æ“ä½œï¼Œæœ€åç»“åˆå±€éƒ¨åœºæ™¯å»ºæ¨¡ä¸ç¢°æ’é¿å…çº¦æŸ (collision avoidance constraints) ä¼˜åŒ–è¿åŠ¨è·¯å¾„ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆç¬¦åˆæ–‡æœ¬è¦æ±‚ä¸”å…·æœ‰ç‰©ç†åˆç†æ€§çš„äº¤äº’åŠ¨ä½œæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23612v1",
      "published_date": "2025-09-28 03:29:15 UTC",
      "updated_date": "2025-09-28 03:29:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:27.784851+00:00"
    },
    {
      "arxiv_id": "2509.25263v3",
      "title": "How Effective Are Time-Series Models for Precipitation Nowcasting? A Comprehensive Benchmark for GNSS-based Precipitation Nowcasting",
      "title_zh": "æ—¶é—´åºåˆ—æ¨¡å‹åœ¨é™æ°´ä¸´è¿‘é¢„æŠ¥ä¸­æ•ˆèƒ½å‡ ä½•ï¼ŸåŸºäºGNSSé™æ°´ä¸´è¿‘é¢„æŠ¥çš„å…¨é¢åŸºå‡†è¯„æµ‹",
      "authors": [
        "Yifang Zhang",
        "Shengwu Xiong",
        "Henan Wang",
        "Wenjie Yin",
        "Jiawang Peng",
        "Yuqiang Zhang",
        "Chen Zhou",
        "Hua Chen",
        "Qile Zhao",
        "Pengfei Duan"
      ],
      "abstract": "Precipitation Nowcasting, which aims to predict precipitation within the next 0 to 6 hours, is critical for disaster mitigation and real-time response planning. However, most time series forecasting benchmarks in meteorology are evaluated on variables with strong periodicity, such as temperature and humidity, which fail to reflect model capabilities in more complex and practically meteorology scenarios like precipitation nowcasting. To address this gap, we propose RainfallBench, a benchmark designed for precipitation nowcasting, a highly challenging and practically relevant task characterized by zero inflation, temporal decay, and non-stationarity, focusing on predicting precipitation within the next 0 to 6 hours. The dataset is derived from five years of meteorological observations, recorded at hourly intervals across six essential variables, and collected from more than 140 Global Navigation Satellite System (GNSS) stations globally. In particular, it incorporates precipitable water vapor (PWV), a crucial indicator of rainfall that is absent in other datasets. We further design specialized evaluation protocols to assess model performance on key meteorological challenges, including multi-scale prediction, multi-resolution forecasting, and extreme rainfall events, benchmarking 17 state-of-the-art models across six major architectures on RainfallBench. Additionally, to address the zero-inflation and temporal decay issues overlooked by existing models, we introduce Bi-Focus Precipitation Forecaster (BFPF), a plug-and-play module that incorporates domain-specific priors to enhance rainfall time series forecasting. Statistical analysis and ablation studies validate the comprehensiveness of our dataset as well as the superiority of our methodology.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é™æ°´ä¸´è¿‘é¢„æŠ¥(Precipitation Nowcasting)ä¸­å­˜åœ¨çš„é›¶è†¨èƒ€(zero inflation)ã€æ—¶é—´è¡°å‡å’Œéå¹³ç¨³æ€§ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†å…¨æ–°çš„åŸºå‡†æ•°æ®é›†RainfallBenchã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å…¨çƒ140å¤šä¸ªå…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿ(GNSS)è§‚æµ‹ç«™äº”å¹´é—´çš„æ¯å°æ—¶æ°”è±¡è§‚æµ‹æ•°æ®ï¼Œå¹¶é¦–æ¬¡å¼•å…¥äº†åæ˜ é™æ°´å…³é”®æŒ‡æ ‡çš„å¤§æ°”å¯é™æ°´é‡(PWV)ã€‚ç ”ç©¶è€…åœ¨RainfallBenchä¸Šå¯¹æ¶µç›–å…­ç§ä¸»æµæ¶æ„çš„17ç§æœ€å…ˆè¿›(state-of-the-art)æ¨¡å‹è¿›è¡Œäº†å¤šå°ºåº¦ã€å¤šåˆ†è¾¨ç‡åŠæç«¯é™æ°´äº‹ä»¶çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç°æœ‰æ¨¡å‹å¿½ç•¥çš„é›¶è†¨èƒ€å’Œæ—¶é—´è¡°å‡é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºBi-Focus Precipitation Forecaster (BFPF)çš„å³æ’å³ç”¨æ¨¡å—ï¼Œé€šè¿‡ç»“åˆé¢†åŸŸå…ˆéªŒçŸ¥è¯†å¢å¼ºé™æ°´æ—¶é—´åºåˆ—é¢„æµ‹ã€‚ç»Ÿè®¡åˆ†æå’Œæ¶ˆèå®éªŒéªŒè¯äº†è¯¥æ•°æ®é›†çš„å…¨é¢æ€§ä»¥åŠæ‰€ææ–¹æ³•åœ¨å¤æ‚æ°”è±¡é¢„æµ‹åœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages,11 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.25263v3",
      "published_date": "2025-09-28 03:21:24 UTC",
      "updated_date": "2025-11-04 03:17:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:39.094051+00:00"
    },
    {
      "arxiv_id": "2509.23597v1",
      "title": "Characteristic Root Analysis and Regularization for Linear Time Series Forecasting",
      "title_zh": "çº¿æ€§æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„ç‰¹å¾æ ¹åˆ†æä¸æ­£åˆ™åŒ–",
      "authors": [
        "Zheng Wang",
        "Kaixuan Zhang",
        "Wanfang Chen",
        "Xiaonan Lu",
        "Longyuan Li",
        "Tobias Schlagenhauf"
      ],
      "abstract": "Time series forecasting remains a critical challenge across numerous domains, yet the effectiveness of complex models often varies unpredictably across datasets. Recent studies highlight the surprising competitiveness of simple linear models, suggesting that their robustness and interpretability warrant deeper theoretical investigation. This paper presents a systematic study of linear models for time series forecasting, with a focus on the role of characteristic roots in temporal dynamics. We begin by analyzing the noise-free setting, where we show that characteristic roots govern long-term behavior and explain how design choices such as instance normalization and channel independence affect model capabilities. We then extend our analysis to the noisy regime, revealing that models tend to produce spurious roots. This leads to the identification of a key data-scaling property: mitigating the influence of noise requires disproportionately large training data, highlighting the need for structural regularization. To address these challenges, we propose two complementary strategies for robust root restructuring. The first uses rank reduction techniques, including Reduced-Rank Regression and Direct Weight Rank Reduction, to recover the low-dimensional latent dynamics. The second, a novel adaptive method called Root Purge, encourages the model to learn a noise-suppressing null space during training. Extensive experiments on standard benchmarks demonstrate the effectiveness of both approaches, validating our theoretical insights and achieving state-of-the-art results in several settings. Our findings underscore the potential of integrating classical theories for linear systems with modern learning techniques to build robust, interpretable, and data-efficient forecasting models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¿æ€§æ¨¡å‹åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è¡¨ç°ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº† characteristic roots åœ¨æ•æ‰æ—¶é—´åŠ¨æ€ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚é€šè¿‡åœ¨æ— å™ªå£°ç¯å¢ƒä¸‹çš„åˆ†æï¼Œè®ºæ–‡æ­ç¤ºäº† characteristic roots å¦‚ä½•å†³å®šé•¿æœŸé¢„æµ‹è¡Œä¸ºï¼Œå¹¶é˜æ˜äº† instance normalization å’Œ channel independence ç­‰è®¾è®¡é€‰æ‹©å¯¹æ¨¡å‹èƒ½åŠ›çš„å½±å“ã€‚åœ¨æœ‰å™ªå£°çš„æƒ…å¢ƒä¸‹ï¼Œç ”ç©¶å‘ç°æ¨¡å‹å®¹æ˜“äº§ç”Ÿ spurious rootsï¼Œä¸”ç¼“è§£å™ªå£°å¹²æ‰°é€šå¸¸éœ€è¦ä¸æˆæ¯”ä¾‹çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œä»è€Œå¼ºè°ƒäº†ç»“æ„åŒ–æ­£åˆ™åŒ–çš„å¿…è¦æ€§ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§äº’è¡¥çš„é²æ£’æ ¹é‡æ„ç­–ç•¥ï¼Œå…¶ä¸­ rank reduction æŠ€æœ¯ï¼ˆåŒ…æ‹¬ Reduced-Rank Regression å’Œ Direct Weight Rank Reductionï¼‰è¢«ç”¨äºæ¢å¤ä½ç»´çš„ latent dynamicsã€‚å¦ä¸€ç§åˆ›æ–°çš„è‡ªé€‚åº”æ–¹æ³• Root Purge åˆ™é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å™ªå£°æŠ‘åˆ¶çš„ null spaceï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œè¿™äº›æ–¹æ³•ä¸ä»…éªŒè¯äº†ç†è®ºæ´å¯Ÿï¼Œè¿˜åœ¨å¤šä¸ªè®¾ç½®ä¸­è¾¾åˆ°äº† state-of-the-art çš„é¢„æµ‹æ•ˆæœã€‚è¯¥é¡¹å·¥ä½œå¼ºè°ƒäº†å°†ç»å…¸çš„çº¿æ€§ç³»ç»Ÿç†è®ºä¸ç°ä»£å­¦ä¹ æŠ€æœ¯ç›¸ç»“åˆï¼Œåœ¨æ„å»ºé²æ£’ã€å¯è§£é‡Šä¸”æ•°æ®é«˜æ•ˆçš„é¢„æµ‹æ¨¡å‹æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23597v1",
      "published_date": "2025-09-28 03:06:30 UTC",
      "updated_date": "2025-09-28 03:06:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:37.198252+00:00"
    },
    {
      "arxiv_id": "2509.23596v1",
      "title": "Multi-Level Heterogeneous Knowledge Transfer Network on Forward Scattering Center Model for Limited Samples SAR ATR",
      "title_zh": "é¢å‘æœ‰é™æ ·æœ¬ SAR ç›®æ ‡è‡ªåŠ¨è¯†åˆ«çš„åŸºäºå‰å‘æ•£å°„ä¸­å¿ƒæ¨¡å‹å¤šå±‚çº§å¼‚æ„çŸ¥è¯†è¿ç§»ç½‘ç»œ",
      "authors": [
        "Chenxi Zhao",
        "Daochang Wang",
        "Siqian Zhang",
        "Gangyao Kuang"
      ],
      "abstract": "Simulated data-assisted SAR target recognition methods are the research hotspot currently, devoted to solving the problem of limited samples. Existing works revolve around simulated images, but the large amount of irrelevant information embedded in the images, such as background, noise, etc., seriously affects the quality of the migrated information. Our work explores a new simulated data to migrate purer and key target knowledge, i.e., forward scattering center model (FSCM) which models the actual local structure of the target with strong physical meaning and interpretability. To achieve this purpose, multi-level heterogeneous knowledge transfer (MHKT) network is proposed, which fully migrates FSCM knowledge from the feature, distribution and category levels, respectively. Specifically, we permit the more suitable feature representations for the heterogeneous data and separate non-informative knowledge by task-associated information selector (TAIS), to complete purer target feature migration. In the distribution alignment, the new metric function maximum discrimination divergence (MDD) in target generic knowledge transfer (TGKT) module perceives transferable knowledge efficiently while preserving discriminative structure about classes. Moreover, category relation knowledge transfer (CRKT) module leverages the category relation consistency constraint to break the dilemma of optimization bias towards simulation data due to imbalance between simulated and measured data. Such stepwise knowledge selection and migration will ensure the integrity of the migrated FSCM knowledge. Notably, extensive experiments on two new datasets formed by FSCM data and measured SAR images demonstrate the superior performance of our method.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šçº§å¼‚æ„çŸ¥è¯†è¿ç§»ç½‘ç»œï¼ˆMHKTï¼‰ï¼Œæ—¨åœ¨è§£å†³åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰è‡ªåŠ¨ç›®æ ‡è¯†åˆ«ï¼ˆATRï¼‰ä¸­å› æ ·æœ¬æœ‰é™å¯¼è‡´çš„è¯†åˆ«éš¾é¢˜ã€‚è¯¥æ–¹æ³•æ‘’å¼ƒäº†ä¼ ç»Ÿæ¨¡æ‹Ÿå›¾åƒä¸­åŒ…å«çš„èƒŒæ™¯å™ªå£°ç­‰å¹²æ‰°ä¿¡æ¯ï¼Œè½¬è€Œåˆ©ç”¨å…·æœ‰å¼ºç‰©ç†å«ä¹‰å’Œå¯è§£é‡Šæ€§çš„å‰å‘æ•£å°„ä¸­å¿ƒæ¨¡å‹ï¼ˆFSCMï¼‰æ¥è¿ç§»çº¯å‡€çš„ç›®æ ‡å…³é”®çŸ¥è¯†ã€‚é€šè¿‡å¼•å…¥ä»»åŠ¡å…³è”ä¿¡æ¯é€‰æ‹©å™¨ï¼ˆTAISï¼‰ï¼Œæ¨¡å‹èƒ½å¤Ÿä»å¼‚æ„æ•°æ®ä¸­ç­›é€‰å‡ºæœ€é€‚åˆçš„ç‰¹å¾è¡¨ç¤ºå¹¶åˆ†ç¦»éä¿¡æ¯æ€§çŸ¥è¯†ã€‚åœ¨åˆ†å¸ƒå¯¹é½æ–¹é¢ï¼Œç›®æ ‡é€šç”¨çŸ¥è¯†è¿ç§»ï¼ˆTGKTï¼‰æ¨¡å—ç»“åˆæœ€å¤§åˆ¤åˆ«æ•£åº¦ï¼ˆMDDï¼‰åº¦é‡å‡½æ•°ï¼Œåœ¨ä¿æŒç±»åˆ«åˆ¤åˆ«ç»“æ„çš„åŒæ—¶é«˜æ•ˆæ•æ‰å¯è¿ç§»çŸ¥è¯†ã€‚é’ˆå¯¹æ¨¡æ‹Ÿä¸å®æµ‹æ•°æ®ä¸å¹³è¡¡å¸¦æ¥çš„ä¼˜åŒ–åç½®é—®é¢˜ï¼Œç±»åˆ«å…³ç³»çŸ¥è¯†è¿ç§»ï¼ˆCRKTï¼‰æ¨¡å—åˆ©ç”¨ç±»åˆ«å…³ç³»ä¸€è‡´æ€§çº¦æŸç¡®ä¿äº†è¿ç§»è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§é€æ­¥çš„çŸ¥è¯†é€‰æ‹©ä¸è¿ç§»æœºåˆ¶æ˜¾è‘—æå‡äº†æœ‰é™æ ·æœ¬ä¸‹ SAR ATR çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23596v1",
      "published_date": "2025-09-28 03:04:04 UTC",
      "updated_date": "2025-09-28 03:04:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:40.083934+00:00"
    },
    {
      "arxiv_id": "2509.23595v1",
      "title": "Timber: Training-free Instruct Model Refining with Base via Effective Rank",
      "title_zh": "Timberï¼šé€šè¿‡æœ‰æ•ˆç§©åˆ©ç”¨åŸºåº§æ¨¡å‹è¿›è¡Œå…è®­ç»ƒæŒ‡ä»¤æ¨¡å‹ä¼˜åŒ–",
      "authors": [
        "Taiqiang Wu",
        "Runming Yang",
        "Tao Liu",
        "Jiahao Wang",
        "Zenan Xu",
        "Ngai Wong"
      ],
      "abstract": "Post-training, which elicits a pretrained Base model into the corresponding Instruct model, is widely considered to be superficial. In this work, we first reinforce this hypothesis by providing novel quantitative evidence from the weight level that the effective rank (eRank) remains negligibly changed. However, this superficiality also suffers a critical trade-off, improving the exploitation capabilities at the cost of limiting its exploration. To tackle this issue, we propose Timber, a simple yet effective training-free method that enhances the exploration capability of the Instruct model while preserving its exploitation. The key insight is to partially revert Instruct towards the paired Base model by subtle yet targeted refinement of the weight deltas. Extensive experiments on Llama and Qwen series demonstrate that Timber consistently improves vanilla Instruct models, particularly on Pass@k performance. Our findings offer new insights into the post-training stage at the weight level and practical strategies to refine the Instruct model without training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Post-trainingé˜¶æ®µå°†Baseæ¨¡å‹è½¬åŒ–ä¸ºInstructæ¨¡å‹çš„è¡¨å±‚ç‰¹æ€§ï¼Œå¹¶ä»æƒé‡å±‚é¢çš„æœ‰æ•ˆç§©(effective rank, eRank)å˜åŒ–å¾®å¼±è¿™ä¸€äº‹å®ä¸­æä¾›äº†é‡åŒ–è¯æ®ã€‚ç ”ç©¶æŒ‡å‡ºè¿™ç§è½¬åŒ–åœ¨æå‡æ¨¡å‹exploitationèƒ½åŠ›çš„åŒæ—¶ï¼Œé™åˆ¶äº†å…¶explorationèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Timberï¼Œä¸€ç§æ— éœ€è®­ç»ƒ(training-free)çš„å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºInstructæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚Timberçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¯¹æƒé‡å¢é‡(weight deltas)è¿›è¡Œç²¾ç»†ä¸”æœ‰é’ˆå¯¹æ€§çš„ä¿®æ­£ï¼Œä½¿Instructæ¨¡å‹é€‚åº¦å‘é…å¯¹çš„Baseæ¨¡å‹å›å½’ã€‚åœ¨Llamaå’ŒQwenç³»åˆ—æ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜ï¼ŒTimberèƒ½å¤ŸæŒç»­æå‡æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶åœ¨Pass@kæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºPost-trainingé˜¶æ®µæä¾›äº†æƒé‡å±‚é¢çš„æ–°è§è§£ï¼Œè¿˜ä¸ºä¼˜åŒ–Instructæ¨¡å‹æä¾›äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å®ç”¨ç­–ç•¥ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 figures, 8 tables, Working in progress",
      "pdf_url": "https://arxiv.org/pdf/2509.23595v1",
      "published_date": "2025-09-28 02:59:43 UTC",
      "updated_date": "2025-09-28 02:59:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:43.806364+00:00"
    },
    {
      "arxiv_id": "2509.23592v1",
      "title": "Toward a Holistic Approach to Continual Model Merging",
      "title_zh": "è¿ˆå‘æŒç»­æ¨¡å‹åˆå¹¶çš„æ•´ä½“æ€§æ–¹æ³•",
      "authors": [
        "Hoang Phan",
        "Sungmin Cha",
        "Tung Lam Tran",
        "Qi Lei"
      ],
      "abstract": "We present a holistic framework for continual model merging that intervenes at three critical stages: pre-merging, during merging, and post-merging-to address two fundamental challenges in continual learning. In particular, conventional approaches either maintain a growing list of per-domain task vectors, leading to scalability issues or rely solely on weight-space merging when old data is inaccessible, thereby losing crucial functional information. Our method overcomes these limitations by first fine-tuning the main model within its tangent space on domain-specific data; this linearization amplifies per-task weight disentanglement, effectively mitigating across-task interference. During merging, we leverage functional information from available optimizer states beyond mere parameter averages to avoid the need to revisit old data. Finally, a post-merging correction aligns the representation discrepancy between pre- and post-merged models, reducing bias and enhancing overall performance-all while operating under constant memory constraints without accessing historical data. Extensive experiments on standard class-incremental and domain-incremental benchmarks demonstrate that our approach not only achieves competitive performance but also provides a scalable and efficient solution to the catastrophic forgetting problem.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹Continual Model Mergingçš„æ•´ä½“æ¡†æ¶ï¼Œé€šè¿‡åœ¨åˆå¹¶å‰ã€åˆå¹¶ä¸­å’Œåˆå¹¶åä¸‰ä¸ªé˜¶æ®µè¿›è¡Œå¹²é¢„ï¼Œè§£å†³äº†æŒç»­å­¦ä¹ ä¸­çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜å’ŒåŠŸèƒ½ä¿¡æ¯ä¸¢å¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆåœ¨tangent spaceå†…å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨linearizationæŠ€æœ¯å¢å¼ºä»»åŠ¡æƒé‡çš„è§£è€¦ï¼Œä»è€Œæœ‰æ•ˆå‡è½»ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚åœ¨åˆå¹¶è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨optimizer statesä¸­çš„åŠŸèƒ½ä¿¡æ¯è€Œéç®€å•çš„å‚æ•°å¹³å‡ï¼Œå®ç°äº†åœ¨ä¸è®¿é—®æ—§æ•°æ®æƒ…å†µä¸‹çš„çŸ¥è¯†é›†æˆã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆå¹¶åçš„æ ¡æ­£(post-merging correction)è¿›ä¸€æ­¥å¯¹é½äº†æ¨¡å‹åˆå¹¶å‰åçš„è¡¨ç¤ºå·®å¼‚ï¼Œæ˜¾è‘—é™ä½äº†åå·®å¹¶æå‡äº†æ•´ä½“æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ»¡è¶³æ’å®šå†…å­˜çº¦æŸä¸”æ— éœ€å†å²æ•°æ®çš„å‰æä¸‹ï¼Œåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ç«äº‰æ€§è¡¨ç°ã€‚è¯¥ç ”ç©¶ä¸ºè§£å†³catastrophic forgettingé—®é¢˜æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”é«˜æ•ˆçš„è‡ªåŠ¨åŒ–æ¨¡å‹åˆå¹¶è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Workshop on Continual Learning in Computer Vision, ICCV 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23592v1",
      "published_date": "2025-09-28 02:51:04 UTC",
      "updated_date": "2025-09-28 02:51:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:35:59.247956+00:00"
    },
    {
      "arxiv_id": "2509.23589v2",
      "title": "BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving",
      "title_zh": "BridgeDriveï¼šé¢å‘è‡ªåŠ¨é©¾é©¶é—­ç¯è½¨è¿¹è§„åˆ’çš„æ‰©æ•£æ¡¥ç­–ç•¥",
      "authors": [
        "Shu Liu",
        "Wenlin Chen",
        "Weihao Li",
        "Zheng Wang",
        "Lijin Yang",
        "Jianing Huang",
        "Yipin Zhang",
        "Zhongzhan Huang",
        "Ze Cheng",
        "Hao Yang"
      ],
      "abstract": "Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BridgeDriveï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºè‡ªåŠ¨é©¾é©¶é—­ç¯è½¨è¿¹è§„åˆ’ï¼ˆclosed-loop trajectory planningï¼‰çš„æ–°å‹ anchor-guided diffusion bridge policyã€‚é’ˆå¯¹ç°æœ‰æ‰©æ•£æ¨¡å‹è§„åˆ’å™¨åœ¨å¤æ‚åŠ¨æ€åœºæ™¯ä¸­å¼•å¯¼ä¸è¶³ä»¥åŠå…¸å‹é”šç‚¹ï¼ˆanchorsï¼‰å¼•å¯¼æ–¹æ³•å­˜åœ¨ç†è®ºä¸ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿå°† anchors æœ‰æ•ˆè½¬åŒ–ä¸ºç»†ç²’åº¦çš„è½¨è¿¹è®¡åˆ’ã€‚BridgeDrive èƒ½å¤Ÿæ ¹æ®å¤šå˜çš„äº¤é€šçŠ¶å†µåšå‡ºé€‚å½“ååº”ï¼Œä¸”å…¶ç®—æ³•å…¼å®¹é«˜æ•ˆçš„ ODE solversï¼Œè¿™å¯¹äºæ»¡è¶³è‡ªåŠ¨é©¾é©¶çš„å®æ—¶éƒ¨ç½²éœ€æ±‚è‡³å…³é‡è¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥è§„åˆ’å™¨åœ¨ Bench2Drive åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† state-of-the-art æ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯å°†ä»»åŠ¡æˆåŠŸç‡æå‡äº† 7.72%ã€‚è¯¥æˆæœè¯æ˜äº† BridgeDrive åœ¨å¤„ç†å¤æ‚é©¾é©¶è¡Œä¸ºå’Œç¡®ä¿é—­ç¯ç³»ç»Ÿç¨³å®šæ€§æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "19 pages, 7 figures, 9 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23589v2",
      "published_date": "2025-09-28 02:47:12 UTC",
      "updated_date": "2025-12-10 08:42:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:05.898998+00:00"
    },
    {
      "arxiv_id": "2509.23586v1",
      "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction",
      "title_zh": "é€šè¿‡è½¨è¿¹ç¼©å‡æå‡ LLM æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•ˆç‡",
      "authors": [
        "Yuan-An Xiao",
        "Pengfei Gao",
        "Chao Peng",
        "Yingfei Xiong"
      ],
      "abstract": "Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent's performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final computational cost by 21.1% ~ 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¤šè½®æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¸æ–­å¢é•¿çš„è½¨è¿¹å¯¼è‡´è¾“å…¥Tokenè®¡ç®—æˆæœ¬è¿‡é«˜ï¼Œä»è€Œåˆ¶çº¦äº†å…¶åº”ç”¨æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†AgentDietï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æ¨ç†æ—¶è½¨è¿¹ç¼©å‡(inference-time trajectory reduction)æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ææ™ºèƒ½ä½“è½¨è¿¹ï¼Œè¯†åˆ«å¹¶è‡ªåŠ¨å‰”é™¤å…¶ä¸­å¹¿æ³›å­˜åœ¨çš„æ— ç”¨ã€å†—ä½™åŠè¿‡æœŸä¿¡æ¯ã€‚åœ¨é¡¶å°–ç¼–ç æ™ºèƒ½ä½“ã€ä¸¤ç§LLMsåŠä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAgentDietèƒ½æ˜¾è‘—å‡å°‘39.9%è‡³59.7%çš„è¾“å…¥Tokenï¼Œå¹¶é™ä½21.1%è‡³35.9%çš„æœ€ç»ˆè®¡ç®—æˆæœ¬ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒAgentDietåœ¨å¤§å¹…æå‡æ•ˆç‡çš„åŒæ—¶å®Œæ•´ä¿ç•™äº†åŸæœ‰çš„æ™ºèƒ½ä½“æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå¡«è¡¥äº†ç°æœ‰ç ”ç©¶å¯¹æ™ºèƒ½ä½“æ•ˆç‡å…³æ³¨ä¸è¶³çš„ç©ºç™½ï¼Œè¯æ˜äº†è½¨è¿¹ç¼©å‡æ˜¯ä¼˜åŒ–LLM Agentç³»ç»Ÿæå…·æ½œåŠ›çš„æ–¹å‘ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "20 pages, 4 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23586v1",
      "published_date": "2025-09-28 02:43:41 UTC",
      "updated_date": "2025-09-28 02:43:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:09.425211+00:00"
    },
    {
      "arxiv_id": "2509.23585v2",
      "title": "EVO-LRP: Evolutionary Optimization of LRP for Interpretable Model Explanations",
      "title_zh": "EVO-LRPï¼šé¢å‘å¯è§£é‡Šæ¨¡å‹è§£é‡Šçš„ LRP è¿›åŒ–ä¼˜åŒ–",
      "authors": [
        "Emerald Zhang",
        "Julian Weaver",
        "Samantha R Santacruz",
        "Edward Castillo"
      ],
      "abstract": "Explainable AI (XAI) methods help identify which image regions influence a model's prediction, but often face a trade-off between detail and interpretability. Layer-wise Relevance Propagation (LRP) offers a model-aware alternative. However, LRP implementations commonly rely on heuristic rule sets that are not optimized for clarity or alignment with model behavior. We introduce EVO-LRP, a method that applies Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to tune LRP hyperparameters based on quantitative interpretability metrics, such as faithfulness or sparseness. EVO-LRP outperforms traditional XAI approaches in both interpretability metric performance and visual coherence, with strong sensitivity to class-specific features. These findings demonstrate that attribution quality can be systematically improved through principled, task-specific optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI, XAI) ä¸­å±‚çº§ç›¸å…³æ€§ä¼ æ’­ (Layer-wise Relevance Propagation, LRP) é•¿æœŸä¾èµ–å¯å‘å¼è§„åˆ™ä¸”æœªé’ˆå¯¹æ¨¡å‹è¡Œä¸ºè¿›è¡Œä¼˜åŒ–çš„å±€é™æ€§ï¼Œæå‡ºäº† EVO-LRP æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†åæ–¹å·®çŸ©é˜µè‡ªé€‚åº”è¿›åŒ–ç­–ç•¥ (Covariance Matrix Adaptation Evolution Strategy, CMA-ES)ï¼Œæ—¨åœ¨æ ¹æ®å¿ å®åº¦ (faithfulness) æˆ–ç¨€ç–æ€§ (sparseness) ç­‰å®šé‡å¯è§£é‡Šæ€§æŒ‡æ ‡è‡ªåŠ¨è°ƒèŠ‚ LRP çš„è¶…å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEVO-LRP åœ¨è§£é‡ŠæŒ‡æ ‡è¡¨ç°å’Œè§†è§‰è¿è´¯æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿ XAI æ–¹æ³•ï¼Œå¹¶è¡¨ç°å‡ºå¯¹ç±»åˆ«ç‰¹å®šç‰¹å¾çš„é«˜åº¦æ•æ„Ÿæ€§ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†é€šè¿‡ç³»ç»Ÿæ€§çš„ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„åŸç†åŒ–ä¼˜åŒ–ï¼Œå¯ä»¥æ˜¾è‘—æå‡å½’å› è´¨é‡ï¼Œä¸ºç”Ÿæˆæ›´å…·è§£é‡ŠåŠ›çš„æ¨¡å‹è¯´æ˜æä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.23585v2",
      "published_date": "2025-09-28 02:42:53 UTC",
      "updated_date": "2025-09-30 17:40:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:12.283754+00:00"
    },
    {
      "arxiv_id": "2509.23577v1",
      "title": "ML-Asset Management: Curation, Discovery, and Utilization",
      "title_zh": "ML èµ„äº§ç®¡ç†ï¼šç­–å±•ã€å‘ç°ä¸åˆ©ç”¨",
      "authors": [
        "Mengying Wang",
        "Moming Duan",
        "Yicong Huang",
        "Chen Li",
        "Bingsheng He",
        "Yinghui Wu"
      ],
      "abstract": "Machine learning (ML) assets, such as models, datasets, and metadata, are central to modern ML workflows. Despite their explosive growth in practice, these assets are often underutilized due to fragmented documentation, siloed storage, inconsistent licensing, and lack of unified discovery mechanisms, making ML-asset management an urgent challenge. This tutorial offers a comprehensive overview of ML-asset management activities across its lifecycle, including curation, discovery, and utilization. We provide a categorization of ML assets, and major management issues, survey state-of-the-art techniques, and identify emerging opportunities at each stage. We further highlight system-level challenges related to scalability, lineage, and unified indexing. Through live demonstrations of systems, this tutorial equips both researchers and practitioners with actionable insights and practical tools for advancing ML-asset management in real-world and domain-specific settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æœºå™¨å­¦ä¹ (Machine learning, ML)èµ„äº§ï¼ˆå¦‚æ¨¡å‹ã€æ•°æ®é›†å’Œå…ƒæ•°æ®ï¼‰åœ¨ç°ä»£MLå·¥ä½œæµä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œå¹¶é’ˆå¯¹å½“å‰ç”±äºæ–‡æ¡£ç¢ç‰‡åŒ–å’Œå­˜å‚¨å­¤å²›å¯¼è‡´çš„èµ„äº§åˆ©ç”¨ç‡ä½ç­‰ç´§è¿«ç®¡ç†éš¾é¢˜æå‡ºäº†åº”å¯¹æ–¹æ¡ˆã€‚æ–‡ç« é€šè¿‡æ•™ç¨‹å½¢å¼å…¨é¢ç»¼è¿°äº†ML-assetç®¡ç†å…¨ç”Ÿå‘½å‘¨æœŸçš„å…³é”®æ´»åŠ¨ï¼Œæ¶µç›–äº†curationï¼ˆç­–åˆ’ï¼‰ã€discoveryï¼ˆå‘ç°ï¼‰å’Œutilizationï¼ˆåˆ©ç”¨ï¼‰ä¸‰å¤§æ ¸å¿ƒé˜¶æ®µã€‚ç ”ç©¶å¯¹MLèµ„äº§è¿›è¡Œäº†ç³»ç»Ÿåˆ†ç±»ï¼Œæ·±å…¥åˆ†æäº†ä¸»è¦ç®¡ç†é—®é¢˜ï¼Œå¹¶è°ƒç ”äº†å½“å‰çš„æœ€å‰æ²¿æŠ€æœ¯ä»¥åŠå„é˜¶æ®µçš„æ–°å…´æœºä¼šã€‚æ­¤å¤–ï¼Œè¯¥æ•™ç¨‹è¿˜é‡ç‚¹å¼ºè°ƒäº†scalabilityï¼ˆå¯æ‰©å±•æ€§ï¼‰ã€lineageï¼ˆè¡€ç¼˜è¿½è¸ªï¼‰å’Œunified indexingï¼ˆç»Ÿä¸€ç´¢å¼•ï¼‰ç­‰ç³»ç»Ÿçº§æŒ‘æˆ˜ã€‚é€šè¿‡ç³»ç»Ÿæ¼”ç¤ºï¼Œè¯¥å·¥ä½œä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†åœ¨å®é™…å’Œç‰¹å®šé¢†åŸŸåœºæ™¯ä¸‹æ¨è¿›ML-assetç®¡ç†çš„å®ç”¨å·¥å…·ä¸è¡ŒåŠ¨æŒ‡å—ã€‚",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DB",
      "comment": "Tutorial, VLDB 2025. Project page: https://ml-assets-management.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2509.23577v1",
      "published_date": "2025-09-28 02:14:33 UTC",
      "updated_date": "2025-09-28 02:14:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:11.861864+00:00"
    },
    {
      "arxiv_id": "2509.23574v1",
      "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales",
      "title_zh": "è¿ˆå‘é«˜æ•ˆæ€ç»´é“¾è’¸é¦ï¼šé€šè¿‡è‡ªå¼•å¯¼ç†ç”±é€‰æ‹©å™¨ä»¥æ›´å°‘ç†ç”±å®ç°æ›´ä¼˜æ€§èƒ½",
      "authors": [
        "Jianzhi Yan",
        "Le Liu",
        "Youcheng Pan",
        "Shiwei Chen",
        "Yang Xiang",
        "Buzhou Tang"
      ],
      "abstract": "Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election \\textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MoRSD (Model-Oriented Rationale Selection Distillation)ï¼Œæ—¨åœ¨è§£å†³ Chain-of-thought (CoT) è’¸é¦ä¸­è¿‡åº¦å…³æ³¨æ•°æ®é‡è€Œå¿½ç•¥ Rationale è´¨é‡å¯¼è‡´çš„ä¿¡æ¯å™ªå£°é—®é¢˜ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«å¹¶ç­›é€‰å‡ºé«˜è´¨é‡çš„ Rationale è¿›è¡Œè’¸é¦ï¼Œä»è€Œåœ¨å‡å°‘æ•°æ®éœ€æ±‚çš„åŒæ—¶æå‡æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº† Rationale Difficulty (RD) æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–å­¦ç”Ÿæ¨¡å‹åœ¨ç‰¹å®šæ¨ç†è·¯å¾„ä¸‹ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›ã€‚é€šè¿‡ååŒæ§åˆ¶ Rationale çš„å‡†ç¡®æ€§ã€å¤šæ ·æ€§å’Œéš¾åº¦ï¼ŒMoRSD åœ¨ä¸‰ä¸ªä»»åŠ¡çš„ä¸ƒä¸ªæ•°æ®é›†ä¸Šå®ç°äº† 4.6% çš„å¹³å‡æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç›¸æ¯”äºä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œä»…åˆ©ç”¨ä¸€å°éƒ¨åˆ†é«˜è´¨é‡ Rationale èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¢å¼ºå°è¯­è¨€æ¨¡å‹ (SLMs) çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é«˜æ•ˆçš„ CoT è’¸é¦æä¾›äº†å¯è¡Œæ–¹æ¡ˆï¼Œå¹¶å…¬å¼€äº†ç›¸å…³ä»£ç å®ç°ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2509.23574v1",
      "published_date": "2025-09-28 02:09:07 UTC",
      "updated_date": "2025-09-28 02:09:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:14.851539+00:00"
    },
    {
      "arxiv_id": "2509.23573v2",
      "title": "Uncovering Vulnerabilities of LLM-Assisted Cyber Threat Intelligence",
      "title_zh": "æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹è¾…åŠ©ç½‘ç»œå¨èƒæƒ…æŠ¥çš„è„†å¼±æ€§",
      "authors": [
        "Yuqiao Meng",
        "Luoxi Tang",
        "Feiyang Yu",
        "Jinyuan Jia",
        "Guanhua Yan",
        "Ping Yang",
        "Zhaohan Xi"
      ],
      "abstract": "Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„ç½‘ç»œå¨èƒæƒ…æŠ¥(Cyber Threat Intelligence, CTI)ç³»ç»Ÿçš„å†…åœ¨è„†å¼±æ€§ï¼Œæ—¨åœ¨æ­ç¤ºæ¨¡å‹åœ¨å®é™…éƒ¨ç½²ä¸­å­˜åœ¨æ€§èƒ½å·®è·çš„æ ¹æœ¬åŸå› ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤§è§„æ¨¡è¯„ä¼°å¤šä¸ªCTIåŸºå‡†å’ŒçœŸå®å¨èƒæŠ¥å‘Šï¼Œæå‡ºäº†ä¸€ç§é›†æˆåœ°ç†åˆ†å±‚(stratification)ã€è‡ªå›å½’ç²¾ç»†åŒ–(autoregressive refinement)ä»¥åŠäººå·¥è¾…åŠ©ç›‘ç£(human-in-the-loop supervision)çš„åˆ›æ–°åˆ†ç±»æ–¹æ³•ï¼Œç”¨äºå¯é åœ°åˆ†æå¤±æ•ˆå®ä¾‹ã€‚é€šè¿‡æ·±å…¥çš„å®éªŒå’Œäººå·¥æ£€æŸ¥ï¼Œç ”ç©¶å‘ç°äº†é™åˆ¶LLMsæ”¯æŒCTIçš„ä¸‰å¤§æ ¸å¿ƒè„†å¼±æ€§ï¼šä¼ªç›¸å…³(spurious correlations)ã€çŸ›ç›¾çŸ¥è¯†(contradictory knowledge)ä»¥åŠå—é™çš„æ³›åŒ–èƒ½åŠ›(constrained generalization)ã€‚è¯¥å·¥ä½œä¸ä»…æ­éœ²äº†å½“å‰æŠ€æœ¯çš„å±€é™æ€§ï¼Œè¿˜ä¸ºæ„å»ºæ›´å…·é²æ£’æ€§çš„LLMè¾…åŠ©CTIç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å—ï¼ŒåŠ©åŠ›æœªæ¥çš„æ¼æ´è¯„ä¼°ä¸äº‹æ•…å“åº”ç ”ç©¶ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23573v2",
      "published_date": "2025-09-28 02:08:27 UTC",
      "updated_date": "2025-10-01 15:57:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:24.300554+00:00"
    },
    {
      "arxiv_id": "2509.23571v2",
      "title": "Benchmarking LLM-Assisted Blue Teaming via Standardized Threat Hunting",
      "title_zh": "åŸºäºæ ‡å‡†åŒ–å¨èƒç‹©çŒçš„å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©è“é˜ŸåŸºå‡†æµ‹è¯•",
      "authors": [
        "Yuqiao Meng",
        "Luoxi Tang",
        "Feiyang Yu",
        "Xi Li",
        "Guanhua Yan",
        "Ping Yang",
        "Zhaohan Xi"
      ],
      "abstract": "As cyber threats continue to grow in scale and sophistication, blue team defenders increasingly require advanced tools to proactively detect and mitigate risks. Large Language Models (LLMs) offer promising capabilities for enhancing threat analysis. However, their effectiveness in real-world blue team threat-hunting scenarios remains insufficiently explored. This paper presents CyberTeam, a benchmark designed to guide LLMs in blue teaming practice. CyberTeam constructs a standardized workflow in two stages. First, it models realistic threat-hunting workflows by capturing the dependencies among analytical tasks from threat attribution to incident response. Next, each task is addressed through a set of operational modules tailored to its specific analytical requirements. This transforms threat hunting into a structured sequence of reasoning steps, with each step grounded in a discrete operation and ordered according to task-specific dependencies. Guided by this framework, LLMs are directed to perform threat-hunting tasks through modularized steps. Overall, CyberTeam integrates 30 tasks and 9 operational modules to guide LLMs through standardized threat analysis. We evaluate both leading LLMs and state-of-the-art cybersecurity agents, comparing CyberTeam against open-ended reasoning strategies. Our results highlight the improvements enabled by standardized design, while also revealing the limitations of open-ended reasoning in real-world threat hunting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† CyberTeamï¼Œä¸€ä¸ªæ—¨åœ¨æŒ‡å¯¼å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨è“é˜Ÿ (Blue Teaming) å®è·µä¸­è¿›è¡Œæ ‡å‡†åŒ–å¨èƒç‹©çŒ (Threat Hunting) çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ„å»ºäº†åŒ…å«ä¸¤ä¸ªé˜¶æ®µçš„æ ‡å‡†åŒ–å·¥ä½œæµï¼Œé€šè¿‡æ•è·ä»å¨èƒå½’å›  (Threat Attribution) åˆ°äº‹ä»¶å“åº” (Incident Response) ä»»åŠ¡é—´çš„ä¾èµ–å…³ç³»æ¥æ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚CyberTeam é›†æˆäº† 30 ä¸ªä»»åŠ¡å’Œ 9 ä¸ªè¿è¥æ¨¡å— (Operational Modules)ï¼Œå°†å¤æ‚çš„å¨èƒç‹©çŒè½¬åŒ–ä¸ºç»“æ„åŒ–çš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œå¼•å¯¼ LLMs æ‰§è¡Œç²¾ç¡®çš„æ¨¡å—åŒ–åˆ†æã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§é¢†å…ˆçš„ LLMs å’Œç½‘ç»œå®‰å…¨æ™ºèƒ½ä½“ï¼Œå¹¶å°†è¯¥æ¡†æ¶ä¸ä¼ ç»Ÿçš„å¼€æ”¾å¼æ¨ç†ç­–ç•¥è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ ‡å‡†åŒ–è®¾è®¡æ˜¾è‘—æå‡äº† LLMs åœ¨å®‰å…¨åˆ†æä¸­çš„æ•ˆèƒ½ï¼ŒåŒæ—¶ä¹Ÿæ­ç¤ºäº†å¼€æ”¾å¼æ¨ç†åœ¨å¤„ç†çœŸå®å¨èƒæ—¶çš„å±€é™æ€§ã€‚è¿™ä¸€æˆæœä¸ºå¼€å‘é«˜æ•ˆã€å¯æ§çš„ AI é©±åŠ¨å‹è“é˜Ÿé˜²å¾¡å·¥å…·æä¾›äº†æ ‡å‡†åŒ–æŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23571v2",
      "published_date": "2025-09-28 02:08:17 UTC",
      "updated_date": "2025-10-01 16:01:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:29.573006+00:00"
    },
    {
      "arxiv_id": "2509.25260v1",
      "title": "Language Model Planning from an Information Theoretic Perspective",
      "title_zh": "ä¿¡æ¯è®ºè§†è§’ä¸‹çš„è¯­è¨€æ¨¡å‹è§„åˆ’",
      "authors": [
        "Muhammed Ustaomeroglu",
        "Baris Askin",
        "Gauri Joshi",
        "Carlee Joe-Wong",
        "Guannan Qu"
      ],
      "abstract": "The extent to which decoder-only language models (LMs) engage in planning, that is, organizing intermediate computations to support coherent long-range generation, remains an open and important question, with implications for interpretability, reliability, and principled model design. Planning involves structuring computations over long horizons, considering multiple possible continuations, and selectively reusing past information, but how effectively transformer-based LMs realize these capabilities is still unclear. We address these questions by analyzing the hidden states at the core of transformer computations, which capture intermediate results and act as carriers of information. Since these hidden representations are often redundant and encumbered with fine-grained details, we develop a pipeline based on vector-quantized variational autoencoders that compresses them into compact summary codes. These codes enable measuring mutual information, allowing systematic analysis of the computational structure underlying model behavior. Using this framework, we study planning in LMs across synthetic grammar, path-finding tasks, and natural language datasets, focusing on three key aspects: (i) the planning horizon of pre-output computations, (ii) the extent to which the model considers alternative valid continuations, and (iii) the reliance of new predictions on earlier computations. By answering these questions, we advance the understanding of how planning is realized in LMs and contribute a general-purpose pipeline for probing the internal dynamics of LMs and deep learning systems. Our results reveal that the effective planning horizon is task-dependent, that models implicitly preserve information about unused correct continuations, and that predictions draw most on recent computations, though earlier blocks remain informative.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶ä»ä¿¡æ¯è®º(Information Theoretic)çš„è§’åº¦æ¢è®¨äº†ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹(decoder-only LMs)å¦‚ä½•é€šè¿‡è§„åˆ’(planning)æ¥ç»„ç»‡ä¸­é—´è®¡ç®—ä»¥æ”¯æŒé•¿ç¨‹ç”Ÿæˆã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€å¥—åŸºäºå‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨(Vector-Quantized Variational Autoencoders, VQ-VAE)çš„æµæ°´çº¿ï¼Œå°†å†—ä½™çš„éšè—çŠ¶æ€(hidden states)å‹ç¼©ä¸ºç´§å‡‘çš„æ‘˜è¦ä»£ç ï¼Œä»è€Œåˆ©ç”¨äº’ä¿¡æ¯(mutual information)ç³»ç»Ÿåœ°åˆ†ææ¨¡å‹çš„è®¡ç®—ç»“æ„ã€‚é€šè¿‡åœ¨åˆæˆè¯­æ³•ã€è·¯å¾„æœç´¢å’Œè‡ªç„¶è¯­è¨€æ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶é‡ç‚¹è¯„ä¼°äº†é¢„è¾“å‡ºè®¡ç®—çš„è§„åˆ’è§†é‡(planning horizon)ã€å¯¹å¤‡é€‰ç»­å†™çš„è€ƒè™‘ç¨‹åº¦ä»¥åŠå¯¹æ—©æœŸè®¡ç®—çš„ä¾èµ–æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„æœ‰æ•ˆè§„åˆ’è§†é‡å…·æœ‰ä»»åŠ¡ä¾èµ–æ€§ï¼Œä¸”LMsä¼šéšå¼ä¿ç•™æœªä½¿ç”¨çš„æ­£ç¡®ç»­å†™ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå°½ç®¡é¢„æµ‹ä¸»è¦ä¾èµ–äºè¿‘æœŸè®¡ç®—ï¼Œä½†æ—©æœŸçš„æ¨¡å‹å—ä¾ç„¶å¯¹ç”Ÿæˆç»“æœå…·æœ‰è´¡çŒ®ã€‚è¯¥ç ”ç©¶ä¸ºç†è§£LMsçš„å†…éƒ¨åŠ¨åŠ›å­¦æä¾›äº†é‡è¦è§è§£ï¼Œå¹¶è´¡çŒ®äº†ä¸€ä¸ªæ¢æµ‹æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ¨ç†è¿‡ç¨‹çš„é€šç”¨æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.25260v1",
      "published_date": "2025-09-28 01:58:15 UTC",
      "updated_date": "2025-09-28 01:58:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:24.942018+00:00"
    },
    {
      "arxiv_id": "2509.23568v1",
      "title": "Node Classification via Simplicial Interaction with Augmented Maximal Clique Selection",
      "title_zh": "åŸºäºå¢å¼ºå‹æœ€å¤§å›¢é€‰æ‹©ä¸å•çº¯å½¢äº¤äº’çš„èŠ‚ç‚¹åˆ†ç±»",
      "authors": [
        "Eunho Koo",
        "Tongseok Lim"
      ],
      "abstract": "Considering higher-order interactions allows for a more comprehensive understanding of network structures beyond simple pairwise connections. While leveraging all cliques in a network to handle higher-order interactions is intuitive, it often leads to computational inefficiencies due to overlapping information between higher-order and lower-order cliques. To address this issue, we propose an augmented maximal clique strategy. Although using only maximal cliques can reduce unnecessary overlap and provide a concise representation of the network, certain nodes may still appear in multiple maximal cliques, resulting in imbalanced training data. Therefore, our augmented maximal clique approach selectively includes some non-maximal cliques to mitigate the overrepresentation of specific nodes and promote more balanced learning across the network. Comparative analyses on synthetic networks and real-world citation datasets demonstrate that our method outperforms approaches based on pairwise interactions, all cliques, or only maximal cliques. Finally, by integrating this strategy into GNN-based semi-supervised learning, we establish a link between maximal clique-based methods and GNNs, showing that incorporating higher-order structures improves predictive accuracy. As a result, the augmented maximal clique strategy offers a computationally efficient and effective solution for higher-order network learning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç½‘ç»œç»“æ„ä¸­é«˜é˜¶äº¤äº’å¤„ç†çš„è®¡ç®—æ•ˆç‡ä¸ä¿¡æ¯é‡å é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¢å¼ºæœ€å¤§å›¢é€‰æ‹©çš„å•çº¯å¤å½¢äº¤äº’(Simplicial Interaction with Augmented Maximal Clique Selection)æ–¹æ³•ã€‚è™½ç„¶åˆ©ç”¨æœ€å¤§å›¢(Maximal Cliques)å¯ä»¥å‡å°‘å†—ä½™å¹¶æä¾›ç®€æ´çš„ç½‘ç»œè¡¨å¾ï¼Œä½†ç‰¹å®šèŠ‚ç‚¹åœ¨å¤šä¸ªæœ€å¤§å›¢ä¸­çš„é‡å¤å‡ºç°å¾€å¾€ä¼šå¯¼è‡´è®­ç»ƒæ•°æ®å¤±è¡¡ã€‚ä¸ºæ­¤ï¼Œè¯¥å¢å¼ºæœ€å¤§å›¢ç­–ç•¥é€šè¿‡æœ‰é€‰æ‹©åœ°åŒ…å«éƒ¨åˆ†éæœ€å¤§å›¢(Non-maximal Cliques)ï¼Œç¼“è§£äº†ç‰¹å®šèŠ‚ç‚¹çš„è¿‡åº¦è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›äº†æ•´ä¸ªç½‘ç»œä¸­æ›´å¹³è¡¡çš„ç‰¹å¾å­¦ä¹ ã€‚åœ¨åˆæˆç½‘ç»œå’ŒçœŸå®ä¸–ç•Œå¼•ç”¨æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºäºæˆå¯¹äº¤äº’(Pairwise Interactions)ã€æ‰€æœ‰å›¢(All Cliques)æˆ–ä»…ä½¿ç”¨æœ€å¤§å›¢çš„ç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…å°†è¯¥ç­–ç•¥æ•´åˆåˆ°åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNN)çš„åŠç›‘ç£å­¦ä¹ æ¡†æ¶ä¸­ï¼Œè¯æ˜äº†ç»“åˆé«˜é˜¶ç»“æ„èƒ½æ˜¾è‘—æå‡èŠ‚ç‚¹åˆ†ç±»(Node Classification)çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œå¢å¼ºæœ€å¤§å›¢ç­–ç•¥ä¸ºé«˜é˜¶ç½‘ç»œå­¦ä¹ æä¾›äº†ä¸€ç§å…¼å…·è®¡ç®—æ•ˆç‡ä¸é¢„æµ‹æ•ˆèƒ½çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SI",
      "comment": "To appear in Neurocomputing",
      "pdf_url": "https://arxiv.org/pdf/2509.23568v1",
      "published_date": "2025-09-28 01:57:01 UTC",
      "updated_date": "2025-09-28 01:57:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:26.551962+00:00"
    },
    {
      "arxiv_id": "2509.23564v2",
      "title": "Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment",
      "title_zh": "å…ˆæ¸…æ´—ï¼Œåå¯¹é½ï¼šé¢å‘å¯é å¤§è¯­è¨€æ¨¡å‹å¯¹é½çš„åå¥½æ•°æ®æ¸…æ´—åŸºå‡†æµ‹è¯•",
      "authors": [
        "Samuel Yeh",
        "Sharon Li"
      ],
      "abstract": "Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹é½ä¸­äººç±»åé¦ˆæ•°æ®æ™®éå­˜åœ¨çš„å™ªå£°å’Œä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæ¢è®¨äº†å…¶å¯¹å¥–åŠ±æ¨¡å‹è´¨é‡å’Œå¯¹é½æ•ˆæœçš„è´Ÿé¢å½±å“ã€‚ä¸ºå¡«è¡¥è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—æ–¹æ³•ç¼ºä¹ç³»ç»Ÿè¯„ä¼°çš„ç©ºç™½ï¼Œä½œè€…æå‡ºäº†PrefCleanBenchï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°13ç§åå¥½æ•°æ®æ¸…æ´—æ–¹æ³•çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚PrefCleanBenchæä¾›äº†ä¸€å¥—æ ‡å‡†åŒ–åè®®ï¼Œåœ¨å¤šæ ·çš„æ•°æ®é›†ã€æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–ç®—æ³•ä¸‹ï¼Œå¯¹æ¸…æ´—ç­–ç•¥çš„å¯¹é½æ€§èƒ½(alignment performance)å’Œæ³›åŒ–èƒ½åŠ›(generalizability)è¿›è¡Œç³»ç»Ÿè¯„ä¼°ã€‚é€šè¿‡å¯¹ä¸åŒæ–¹æ³•çš„ç»Ÿä¸€æ¯”è¾ƒï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†å†³å®šå¯¹é½ä»»åŠ¡ä¸­æ•°æ®æ¸…æ´—æˆåŠŸçš„å…³é”®å› ç´ ï¼Œå¼ºè°ƒäº†æ•°æ®é¢„å¤„ç†åœ¨è´Ÿè´£ä»»äººå·¥æ™ºèƒ½(responsible AI)å¼€å‘ä¸­çš„é‡è¦ä½œç”¨ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸ºé€šè¿‡æå‡æ•°æ®è´¨é‡æ¥å®ç°è§„èŒƒåŒ–ä¸”å¯å¤ç°çš„LLMå¯¹é½å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å¼€æºäº†æ‰€æœ‰æ–¹æ³•çš„æ¨¡å—åŒ–å®ç°ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "NeurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2509.23564v2",
      "published_date": "2025-09-28 01:44:05 UTC",
      "updated_date": "2025-10-14 14:47:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:43.201639+00:00"
    },
    {
      "arxiv_id": "2509.23563v1",
      "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation",
      "title_zh": "RAVENï¼šåŸºäºå¼€æ”¾é›†è¯­ä¹‰è®°å¿†ä¸è¡Œä¸ºè‡ªé€‚åº”çš„éŸ§æ€§ç©ºä¸­å¯¼èˆª",
      "authors": [
        "Seungchan Kim",
        "Omar Alama",
        "Dmytro Kurdydyk",
        "John Keller",
        "Nikhil Keetha",
        "Wenshan Wang",
        "Yonatan Bisk",
        "Sebastian Scherer"
      ],
      "abstract": "Aerial outdoor semantic navigation requires robots to explore large, unstructured environments to locate target objects. Recent advances in semantic navigation have demonstrated open-set object-goal navigation in indoor settings, but these methods remain limited by constrained spatial ranges and structured layouts, making them unsuitable for long-range outdoor search. While outdoor semantic navigation approaches exist, they either rely on reactive policies based on current observations, which tend to produce short-sighted behaviors, or precompute scene graphs offline for navigation, limiting adaptability to online deployment. We present RAVEN, a 3D memory-based, behavior tree framework for aerial semantic navigation in unstructured outdoor environments. It (1) uses a spatially consistent semantic voxel-ray map as persistent memory, enabling long-horizon planning and avoiding purely reactive behaviors, (2) combines short-range voxel search and long-range ray search to scale to large environments, (3) leverages a large vision-language model to suggest auxiliary cues, mitigating sparsity of outdoor targets. These components are coordinated by a behavior tree, which adaptively switches behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor simulation environments over 100 semantic tasks, encompassing single-object search, multi-class, multi-instance navigation and sequential task changes. Results show RAVEN outperforms baselines by 85.25% in simulation and demonstrate its real-world applicability through deployment on an aerial robot in outdoor field tests.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RAVENï¼Œä¸€ä¸ªåŸºäº3Då­˜å‚¨çš„Behavior Treeæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ— äººæœºåœ¨éç»“æ„åŒ–æˆ·å¤–ç¯å¢ƒä¸­è¿›è¡Œè¯­ä¹‰å¯¼èˆª(Semantic Navigation)æ—¶é¢ä¸´çš„ç©ºé—´èŒƒå›´å¹¿å’Œç¯å¢ƒå¤æ‚ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ç©ºé—´ä¸€è‡´çš„è¯­ä¹‰ä½“ç´ -å°„çº¿åœ°å›¾(Semantic Voxel-Ray Map)ä½œä¸ºæŒä¹…åŒ–å†…å­˜ï¼Œæ”¯æŒé•¿ç¨‹è§„åˆ’(Long-horizon Planning)å¹¶é¿å…äº†å•çº¯çš„ååº”å¼è¡Œä¸ºã€‚é€šè¿‡ç»“åˆçŸ­ç¨‹ä½“ç´ æœç´¢(Voxel Search)å’Œé•¿ç¨‹å°„çº¿æœç´¢(Ray Search)ï¼ŒRAVENèƒ½å¤Ÿæœ‰æ•ˆæ‰©å±•åˆ°å¤§è§„æ¨¡æˆ·å¤–ç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹(Vision-Language Model)æä¾›è¾…åŠ©çº¿ç´¢ï¼Œä»¥ç¼“è§£æˆ·å¤–ç›®æ ‡ç¨€ç–çš„é—®é¢˜ã€‚è¿™äº›ç»„ä»¶ç”±è¡Œä¸ºæ ‘(Behavior Tree)ç»Ÿä¸€åè°ƒï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®ç¯å¢ƒå˜åŒ–è‡ªé€‚åº”åœ°åˆ‡æ¢å¯¼èˆªè¡Œä¸ºï¼Œç¡®ä¿æŒç»­ç¨³å¥çš„è¿è¡Œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAVENåœ¨ä»¿çœŸç¯å¢ƒä¸‹çš„è¡¨ç°ä¼˜äºåŸºå‡†æ¨¡å‹85.25%ï¼Œå¹¶åœ¨å®é™…æˆ·å¤–ç°åœºæµ‹è¯•ä¸­è¯æ˜äº†å…¶åœ¨å•ç›®æ ‡æœç´¢ã€å¤šç±»åˆ«åŠå¤šå®ä¾‹å¯¼èˆªç­‰å¤æ‚ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23563v1",
      "published_date": "2025-09-28 01:43:25 UTC",
      "updated_date": "2025-09-28 01:43:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:48.888520+00:00"
    },
    {
      "arxiv_id": "2509.23562v1",
      "title": "Pancreas Part Segmentation under Federated Learning Paradigm",
      "title_zh": "åŸºäºè”é‚¦å­¦ä¹ èŒƒå¼çš„èƒ°è…ºåˆ†åŒºåˆ†å‰²",
      "authors": [
        "Ziliang Hong",
        "Halil Ertugrul Aktas",
        "Andrea Mia Bejar",
        "Katherine Wu",
        "Hongyi Pan",
        "Gorkem Durak",
        "Zheyuan Zhang",
        "Sait Kayali",
        "Temel Tirkes",
        "Federica Proietto Salanitri",
        "Concetto Spampinato",
        "Michael Goggins",
        "Tamas Gonda",
        "Candice Bolan",
        "Raj Keswani",
        "Frank Miller",
        "Michael Wallace",
        "Ulas Bagci"
      ],
      "abstract": "We present the first federated learning (FL) approach for pancreas part(head, body and tail) segmentation in MRI, addressing a critical clinical challenge as a significant innovation. Pancreatic diseases exhibit marked regional heterogeneity cancers predominantly occur in the head region while chronic pancreatitis causes tissue loss in the tail, making accurate segmentation of the organ into head, body, and tail regions essential for precise diagnosis and treatment planning. This segmentation task remains exceptionally challenging in MRI due to variable morphology, poor soft-tissue contrast, and anatomical variations across patients. Our novel contribution tackles two fundamental challenges: first, the technical complexity of pancreas part delineation in MRI, and second the data scarcity problem that has hindered prior approaches. We introduce a privacy-preserving FL framework that enables collaborative model training across seven medical institutions without direct data sharing, leveraging a diverse dataset of 711 T1W and 726 T2W MRI scans. Our key innovations include: (1) a systematic evaluation of three state-of-the-art segmentation architectures (U-Net, Attention U-Net,Swin UNETR) paired with two FL algorithms (FedAvg, FedProx), revealing Attention U-Net with FedAvg as optimal for pancreatic heterogeneity, which was never been done before; (2) a novel anatomically-informed loss function prioritizing region-specific texture contrasts in MRI. Comprehensive evaluation demonstrates that our approach achieves clinically viable performance despite training on distributed, heterogeneous datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªç”¨äº MRI å½±åƒä¸­èƒ°è…ºéƒ¨ä½ï¼ˆhead, body, tailï¼‰åˆ†å‰²çš„è”é‚¦å­¦ä¹  (Federated Learning) æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³èƒ°è…ºç–¾ç—…åŒºåŸŸå¼‚è´¨æ€§å¸¦æ¥çš„ç²¾å‡†è¯Šæ–­éš¾é¢˜ã€‚é’ˆå¯¹ MRI ä¸­èƒ°è…ºå½¢æ€å¤šå˜ã€è½¯ç»„ç»‡å¯¹æ¯”åº¦å·®ä»¥åŠæ•°æ®ç¨€ç¼ºç­‰æŒ‘æˆ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç¡®ä¿éšç§çš„å‰æä¸‹ï¼Œè”åˆä¸ƒå®¶åŒ»ç–—æœºæ„çš„ 711 ä¸ª T1W å’Œ 726 ä¸ª T2W MRI æ‰«ææ•°æ®è¿›è¡Œåä½œè®­ç»ƒã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯„ä¼° U-Netã€Attention U-Net å’Œ Swin UNETR ä¸‰ç§æ¶æ„ä¸ FedAvgã€FedProx ç®—æ³•çš„ç»„åˆï¼Œå‘ç° Attention U-Net ç»“åˆ FedAvg æ˜¯å¤„ç†èƒ°è…ºå¼‚è´¨æ€§çš„æœ€ä¼˜æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„è§£å‰–ä¿¡æ¯æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œä¸“é—¨ç”¨äºä¼˜åŒ– MRI ä¸­çš„åŒºåŸŸç‰¹å®šçº¹ç†å¯¹æ¯”ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å¸ƒå¼å¼‚æ„æ•°æ®é›†ä¸Šè¾¾åˆ°äº†ä¸´åºŠå¯è¡Œçš„æ€§èƒ½ï¼Œä¸ºèƒ°è…ºç–¾ç—…çš„ç²¾ç¡®æ²»ç–—è§„åˆ’æä¾›äº†é‡è¦æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23562v1",
      "published_date": "2025-09-28 01:42:43 UTC",
      "updated_date": "2025-09-28 01:42:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:49.685627+00:00"
    },
    {
      "arxiv_id": "2509.23560v1",
      "title": "A Hierarchical Structure-Enhanced Personalized Recommendation Model for Traditional Chinese Medicine Formulas Based on KG Diffusion Guidance",
      "title_zh": "åŸºäºçŸ¥è¯†å›¾è°±æ‰©æ•£å¼•å¯¼çš„åˆ†å±‚ç»“æ„å¢å¼ºä¸­è¯æ–¹å‰‚ä¸ªæ€§åŒ–æ¨èæ¨¡å‹",
      "authors": [
        "ChaoBo Zhang",
        "Long Tan"
      ],
      "abstract": "Artificial intelligence technology plays a crucial role in recommending prescriptions for traditional Chinese medicine (TCM). Previous studies have made significant progress by focusing on the symptom-herb relationship in prescriptions. However, several limitations hinder model performance: (i) Insufficient attention to patient-personalized information such as age, BMI, and medical history, which hampers accurate identification of syndrome and reduces efficacy. (ii) The typical long-tailed distribution of herb data introduces training biases and affects generalization ability. (iii) The oversight of the 'monarch, minister, assistant and envoy' compatibility among herbs increases the risk of toxicity or side effects, opposing the 'treatment based on syndrome differentiation' principle in clinical TCM. Therefore, we propose a novel hierarchical structure-enhanced personalized recommendation model for TCM formulas based on knowledge graph diffusion guidance, namely TCM-HEDPR. Specifically, we pre-train symptom representations using patient-personalized prompt sequences and apply prompt-oriented contrastive learning for data augmentation. Furthermore, we employ a KG-guided homogeneous graph diffusion method integrated with a self-attention mechanism to globally capture the non-linear symptom-herb relationship. Lastly, we design a heterogeneous graph hierarchical network to integrate herbal dispensing relationships with implicit syndromes, guiding the prescription generation process at a fine-grained level and mitigating the long-tailed herb data distribution problem. Extensive experiments on two public datasets and one clinical dataset demonstrate the effectiveness of TCM-HEDPR. In addition, we incorporate insights from modern medicine and network pharmacology to evaluate the recommended prescriptions comprehensively. It can provide a new paradigm for the recommendation of modern TCM.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸­åŒ»(TCM)æ–¹å‰‚æ¨èæ¨¡å‹åœ¨æ‚£è€…ä¸ªæ€§åŒ–ä¿¡æ¯åˆ©ç”¨ä¸è¶³ã€ä¸­è¯æ•°æ®é•¿å°¾åˆ†å¸ƒå¯¼è‡´çš„åå·®ä»¥åŠå¿½ç•¥å›è‡£ä½ä½¿(monarch, minister, assistant and envoy)é…ä¼åŸåˆ™ç­‰å±€é™æ€§ï¼Œæå‡ºäº†TCM-HEDPRæ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ‚£è€…ä¸ªæ€§åŒ–æç¤ºåºåˆ—(personalized prompt sequences)é¢„è®­ç»ƒç—‡çŠ¶è¡¨ç¤ºï¼Œå¹¶ç»“åˆé¢å‘æç¤ºçš„å¯¹æ¯”å­¦ä¹ (prompt-oriented contrastive learning)è¿›è¡Œæ•°æ®å¢å¼ºã€‚ç ”ç©¶é‡‡ç”¨äº†é›†æˆè‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention)çš„çŸ¥è¯†å›¾è°±å¼•å¯¼åŒæ„å›¾æ‰©æ•£æ–¹æ³•(KG-guided homogeneous graph diffusion)ï¼Œä»¥æ•æ‰å¤æ‚çš„éçº¿æ€§ç—‡çŠ¶-è‰è¯å…³ç³»ã€‚åŒæ—¶ï¼Œé€šè¿‡è®¾è®¡å¼‚æ„å›¾å±‚æ¬¡åŒ–ç½‘ç»œ(heterogeneous graph hierarchical network)æ•´åˆè¯ç†é…ä¼ä¸éšæ€§è¯å€™ï¼Œåœ¨ç»†ç²’åº¦å±‚é¢å¼•å¯¼æ–¹å‰‚ç”Ÿæˆå¹¶ç¼“è§£æ•°æ®é•¿å°¾åˆ†å¸ƒé—®é¢˜ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†å’Œä¸€ä¸ªä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒå‡éªŒè¯äº†TCM-HEDPRçš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶è¿˜å¼•å…¥äº†ç°ä»£åŒ»å­¦å’Œç½‘ç»œè¯ç†å­¦(network pharmacology)çš„è§†è§’è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œä¸ºç°ä»£ä¸­åŒ»æ™ºèƒ½å¤„æ–¹æ¨èæä¾›äº†æ–°çš„èŒƒå¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 10 figures, Proceedings of the 34th ACM International Conference on Information and Knowledge Management (CIKM)",
      "pdf_url": "https://arxiv.org/pdf/2509.23560v1",
      "published_date": "2025-09-28 01:40:01 UTC",
      "updated_date": "2025-09-28 01:40:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:36:53.895800+00:00"
    },
    {
      "arxiv_id": "2509.23558v1",
      "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„å½¢å¼åŒ–é©±åŠ¨ LLM æç¤ºè¯è¶Šç‹±",
      "authors": [
        "Zhaoqi Wang",
        "Daqing He",
        "Zijian Zhang",
        "Xin Li",
        "Liehuang Zhu",
        "Meng Li",
        "Jiamou Liu"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, yet they also introduce novel security challenges. For instance, prompt jailbreaking attacks involve adversaries crafting sophisticated prompts to elicit responses from LLMs that deviate from human values. To uncover vulnerabilities in LLM alignment methods, we propose the PASS framework (\\underline{P}rompt J\\underline{a}ilbreaking via \\underline{S}emantic and \\underline{S}tructural Formalization). Specifically, PASS employs reinforcement learning to transform initial jailbreak prompts into formalized descriptions, which enhances stealthiness and enables bypassing existing alignment defenses. The jailbreak outputs are then structured into a GraphRAG system that, by leveraging extracted relevant terms and formalized symbols as contextual input alongside the original query, strengthens subsequent attacks and facilitates more effective jailbreaks. We conducted extensive experiments on common open-source models, demonstrating the effectiveness of our attack.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PASS (Prompt Jailbreaking via Semantic and Structural Formalization) æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¯¹é½æ–¹æ³•ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´ã€‚PASS æ¡†æ¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) å°†åˆå§‹è¶Šç‹±æç¤ºè¯è½¬åŒ–ä¸ºå½¢å¼åŒ–æè¿°ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ”»å‡»çš„éšè”½æ€§å¹¶èƒ½æœ‰æ•ˆç»•è¿‡ç°æœ‰çš„å¯¹é½é˜²å¾¡æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å°†è¶Šç‹±è¾“å‡ºæ•´åˆè‡³ GraphRAG ç³»ç»Ÿï¼Œåˆ©ç”¨æå–çš„ç›¸å…³æœ¯è¯­å’Œå½¢å¼åŒ–ç¬¦å·ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºåç»­æ”»å‡»çš„æ•ˆåŠ›ã€‚åœ¨å¤šç§å¸¸ç”¨å¼€æºæ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº† PASS æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè¯„ä¼° LLM çš„å®‰å…¨æ€§æä¾›äº†æ–°çš„è§†è§’ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23558v1",
      "published_date": "2025-09-28 01:38:00 UTC",
      "updated_date": "2025-09-28 01:38:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:05.190783+00:00"
    },
    {
      "arxiv_id": "2509.23552v1",
      "title": "Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN-XGBoost Ensemble",
      "title_zh": "èåˆåºåˆ—æ¨¡ä½“ä¸æ³›åŸºå› ç»„ç‰¹å¾ï¼šåŸºäºå¯è§£é‡Šè½»é‡çº§ 1D CNN-XGBoost é›†æˆæ¨¡å‹çš„æŠ—ç”Ÿç´ è€è¯æ€§é¢„æµ‹",
      "authors": [
        "Md. Saiful Bari Siddiqui",
        "Nowshin Tarannum"
      ],
      "abstract": "Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis. While genomic sequencing enables rapid prediction of resistance phenotypes, current computational methods have limitations. Standard machine learning models treat the genome as an unordered collection of features, ignoring the sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art sequence models like Transformers are often too data-hungry and computationally expensive for the moderately-sized datasets that are typical in this domain. To address these challenges, we propose AMR-EnsembleNet, an ensemble framework that synergistically combines sequence-based and feature-based learning. We developed a lightweight, custom 1D Convolutional Neural Network (CNN) to efficiently learn predictive sequence motifs from high-dimensional SNP data. This sequence-aware model was ensembled with an XGBoost model, a powerful gradient boosting system adept at capturing complex, non-local feature interactions. We trained and evaluated our framework on a benchmark dataset of 809 E. coli strains, predicting resistance across four antibiotics with varying class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier performance across all the antibiotics, reaching a Matthews Correlation Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also show that our model consistently focuses on SNPs within well-known AMR genes like fusA and parC, confirming it learns the correct genetic signals for resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a feature-based XGBoost model creates a powerful ensemble, overcoming the limitations of using either an order-agnostic or a standalone sequence model.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…¨çƒæ€§çš„æŠ—èŒè¯ç‰©è€è¯æ€§(Antimicrobial Resistance, AMR)å±æœºï¼Œæå‡ºäº†AMR-EnsembleNeté›†æˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ¨¡å‹å¿½è§†å•æ ¸è‹·é…¸å¤šæ€æ€§(SNPs)åºåˆ—ä¸Šä¸‹æ–‡ä»¥åŠTransformeræ¨¡å‹åœ¨ä¸­å°è§„æ¨¡æ•°æ®é›†ä¸Šè®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ›æ–°æ€§åœ°ç»“åˆäº†è½»é‡çº§è‡ªå®šä¹‰1Då·ç§¯ç¥ç»ç½‘ç»œ(1D CNN)å’ŒXGBoostæ¨¡å‹ï¼Œåˆ†åˆ«ç”¨äºä»é«˜ç»´SNPæ•°æ®ä¸­é«˜æ•ˆæå–åºåˆ—åŸºå…ƒ(Sequence Motifs)ä»¥åŠæ•è·å¤æ‚çš„éå±€éƒ¨ç‰¹å¾äº¤äº’ã€‚é€šè¿‡å¯¹809ä¸ªå¤§è‚ æ†èŒ(E. coli)èŒæ ªçš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œè¯¥æ¨¡å‹åœ¨å››ç§æŠ—ç”Ÿç´ çš„è€è¯æ€§é¢„æµ‹ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå…¶ä¸­ç¯ä¸™æ²™æ˜Ÿ(CIP)çš„é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°(MCC)è¾¾åˆ°0.926ï¼Œå¹¶åœ¨æå…·æŒ‘æˆ˜æ€§çš„åº†å¤§éœ‰ç´ (GEN)é¢„æµ‹ä¸­å–å¾—äº†0.691çš„æœ€é«˜Macro F1åˆ†æ•°ã€‚æ­¤å¤–ï¼Œå¯è§£é‡Šæ€§åˆ†æè¯å®æ¨¡å‹èƒ½å‡†ç¡®èšç„¦äºfusAå’ŒparCç­‰å·²çŸ¥è€è¯åŸºå› ï¼Œè¯æ˜äº†èåˆåºåˆ—æ„ŸçŸ¥ä¸ç‰¹å¾é©±åŠ¨çš„å­¦ä¹ æ–¹æ³•åœ¨æå‡è€è¯æ€§é¢„æµ‹æ•ˆèƒ½æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to SCA/HPCAsia 2026. This preprint version has been prepared for open-access distribution and may differ in formatting from the official proceedings. Also available on bioRxiv for visibility to the life sciences community",
      "pdf_url": "https://arxiv.org/pdf/2509.23552v1",
      "published_date": "2025-09-28 01:19:11 UTC",
      "updated_date": "2025-09-28 01:19:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:05.590321+00:00"
    },
    {
      "arxiv_id": "2509.23550v1",
      "title": "Automatic Speech Recognition for Greek Medical Dictation",
      "title_zh": "é¢å‘å¸Œè…Šè¯­åŒ»ç–—å¬å†™çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«",
      "authors": [
        "Vardis Georgilas",
        "Themos Stafylakis"
      ],
      "abstract": "Medical dictation systems are essential tools in modern healthcare, enabling accurate and efficient conversion of speech into written medical documentation. The main objective of this paper is to create a domain-specific system for Greek medical speech transcriptions. The ultimate goal is to assist healthcare professionals by reducing the overload of manual documentation and improving workflow efficiency. Towards this goal, we develop a system that combines automatic speech recognition techniques with text correction model, allowing better handling of domain-specific terminology and linguistic variations in Greek. Our approach leverages both acoustic and textual modeling to create more realistic and reliable transcriptions. We focused on adapting existing language and speech technologies to the Greek medical context, addressing challenges such as complex medical terminology and linguistic inconsistencies. Through domain-specific fine-tuning, our system achieves more accurate and coherent transcriptions, contributing to the development of practical language technologies for the Greek healthcare sector.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¸Œè…Šè¯­åŒ»ç–—å¬å†™çš„ Automatic Speech Recognition (ASR) ç³»ç»Ÿï¼Œæ—¨åœ¨å‡è½»åŒ»ç–—ä¸“ä¸šäººå‘˜æ‰‹åŠ¨è®°å½•æ–‡æ¡£çš„è´Ÿæ‹…å¹¶æå‡å·¥ä½œæµæ•ˆç‡ã€‚ç³»ç»Ÿç»“åˆäº† ASR æŠ€æœ¯ä¸æ–‡æœ¬çº é”™æ¨¡å‹ (text correction model)ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å¸Œè…Šè¯­ä¸­çš„é¢†åŸŸç‰¹å®šæœ¯è¯­å’Œè¯­è¨€å˜ä½“ã€‚é€šè¿‡åŒæ—¶åˆ©ç”¨å£°å­¦å»ºæ¨¡ (acoustic modeling) å’Œæ–‡æœ¬å»ºæ¨¡ (textual modeling)ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´çœŸå®ä¸”å¯é çš„è½¬å†™å†…å®¹ã€‚é’ˆå¯¹å¸Œè…ŠåŒ»ç–—èƒŒæ™¯è¿›è¡Œçš„é¢†åŸŸç‰¹å®š fine-tuning æ˜¾è‘—æå‡äº†è½¬å†™çš„å‡†ç¡®æ€§ä¸è¿è´¯æ€§ã€‚è¯¥ç ”ç©¶æˆåŠŸè§£å†³äº†å¤æ‚åŒ»ç–—æœ¯è¯­å’Œè¯­è¨€ä¸ä¸€è‡´æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä¸ºå¸Œè…ŠåŒ»ç–—é¢†åŸŸæä¾›äº†å®ç”¨çš„è¯­è¨€æŠ€æœ¯è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2509.23550v1",
      "published_date": "2025-09-28 01:15:47 UTC",
      "updated_date": "2025-09-28 01:15:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:06.262122+00:00"
    },
    {
      "arxiv_id": "2509.23548v1",
      "title": "Disentanglement of Variations with Multimodal Generative Modeling",
      "title_zh": "åŸºäºå¤šæ¨¡æ€ç”Ÿæˆå»ºæ¨¡çš„å˜åŒ–å› ç´ è§£è€¦",
      "authors": [
        "Yijie Zhang",
        "Yiyang Shen",
        "Weiran Wang"
      ],
      "abstract": "Multimodal data are prevalent across various domains, and learning robust representations of such data is paramount to enhancing generation quality and downstream task performance. To handle heterogeneity and interconnections among different modalities, recent multimodal generative models extract shared and private (modality-specific) information with two separate variables. Despite attempts to enforce disentanglement between these two variables, these methods struggle with challenging datasets where the likelihood model is insufficient. In this paper, we propose Information-disentangled Multimodal VAE (IDMVAE) to explicitly address this issue, with rigorous mutual information-based regularizations, including cross-view mutual information maximization for extracting shared variables, and a cycle-consistency style loss for redundancy removal using generative augmentations. We further introduce diffusion models to improve the capacity of latent priors. These newly proposed components are complementary to each other. Compared to existing approaches, IDMVAE shows a clean separation between shared and private information, demonstrating superior generation quality and semantic coherence on challenging datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†å¤æ‚æ•°æ®é›†æ—¶å…±äº«å˜é‡ä¸ç§æœ‰å˜é‡è§£è€¦ä¸å……åˆ†çš„é—®é¢˜ï¼Œæå‡ºäº†Information-disentangled Multimodal VAE (IDMVAE) æ¡†æ¶ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸¥æ ¼çš„åŸºäºäº’ä¿¡æ¯(Mutual Information)çš„æ­£åˆ™åŒ–æœºåˆ¶ï¼Œé€šè¿‡æœ€å¤§åŒ–è·¨è§†å›¾äº’ä¿¡æ¯æ¥ç²¾å‡†æå–æ¨¡æ€é—´çš„å…±äº«ä¿¡æ¯ã€‚åŒæ—¶ï¼Œç ”ç©¶è€…åˆ©ç”¨ç”Ÿæˆå¼å¢å¼ºæŠ€æœ¯è®¾è®¡äº†ä¸€ç§å¾ªç¯ä¸€è‡´æ€§(Cycle-consistency)é£æ ¼çš„æŸå¤±å‡½æ•°ï¼Œä»¥æœ‰æ•ˆå»é™¤å†—ä½™å¹¶åˆ†ç¦»ç§æœ‰å˜é‡ã€‚æ­¤å¤–ï¼ŒIDMVAEè¿˜ç»“åˆäº†æ‰©æ•£æ¨¡å‹(Diffusion Models)æ¥æå‡æ½œåœ¨å…ˆéªŒ(Latent Priors)çš„å»ºæ¨¡èƒ½åŠ›ï¼Œå„ç»„ä»¶ç›¸äº’è¡¥å……ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…±äº«ä¸ç§æœ‰ä¿¡æ¯çš„æ¸…æ™°è§£è€¦ï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†ç”Ÿæˆè´¨é‡å’Œè¯­ä¹‰ä¸€è‡´æ€§(Semantic Coherence)ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 14 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23548v1",
      "published_date": "2025-09-28 00:54:39 UTC",
      "updated_date": "2025-09-28 00:54:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:09.784953+00:00"
    },
    {
      "arxiv_id": "2509.23544v1",
      "title": "End-to-End Deep Learning for Predicting Metric Space-Valued Outputs",
      "title_zh": "é¢„æµ‹åº¦é‡ç©ºé—´å€¼è¾“å‡ºçš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ ",
      "authors": [
        "Yidong Zhou",
        "Su I Iao",
        "Hans-Georg MÃ¼ller"
      ],
      "abstract": "Many modern applications involve predicting structured, non-Euclidean outputs such as probability distributions, networks, and symmetric positive-definite matrices. These outputs are naturally modeled as elements of general metric spaces, where classical regression techniques that rely on vector space structure no longer apply. We introduce E2M (End-to-End Metric regression), a deep learning framework for predicting metric space-valued outputs. E2M performs prediction via a weighted FrÃ©chet means over training outputs, where the weights are learned by a neural network conditioned on the input. This construction provides a principled mechanism for geometry-aware prediction that avoids surrogate embeddings and restrictive parametric assumptions, while fully preserving the intrinsic geometry of the output space. We establish theoretical guarantees, including a universal approximation theorem that characterizes the expressive capacity of the model and a convergence analysis of the entropy-regularized training objective. Through extensive simulations involving probability distributions, networks, and symmetric positive-definite matrices, we show that E2M consistently achieves state-of-the-art performance, with its advantages becoming more pronounced at larger sample sizes. Applications to human mortality distributions and New York City taxi networks further demonstrate the flexibility and practical utility of the framework.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†E2M (End-to-End Metric regression)ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé¢„æµ‹åº¦é‡ç©ºé—´å€¼(metric space-valued)è¾“å‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆå¤„ç†æ¦‚ç‡åˆ†å¸ƒã€ç½‘ç»œå’Œå¯¹ç§°æ­£å®š(SPD)çŸ©é˜µç­‰éæ¬§å‡ é‡Œå¾—ç»“æ„ã€‚E2Mé€šè¿‡åœ¨è®­ç»ƒè¾“å‡ºä¸Šè¿›è¡ŒåŠ æƒFrÃ©chetå‡å€¼è®¡ç®—æ¥å®ç°é¢„æµ‹ï¼Œå…¶ä¸­æƒé‡ç”±ä»¥è¾“å…¥ä¸ºæ¡ä»¶çš„ç¥ç»ç½‘ç»œå­¦ä¹ å¾—åˆ°ã€‚è¿™ç§æ–¹æ³•æä¾›äº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥(geometry-aware)çš„é¢„æµ‹æœºåˆ¶ï¼Œé¿å…äº†ä»£ç†åµŒå…¥(surrogate embeddings)å’Œé™åˆ¶æ€§çš„å‚æ•°å‡è®¾ï¼Œä»è€Œå®Œæ•´ä¿ç•™äº†è¾“å‡ºç©ºé—´çš„å†…åœ¨å‡ ä½•ç‰¹æ€§ã€‚ç ”ç©¶å›¢é˜Ÿä¸ºè¯¥æ¨¡å‹å»ºç«‹äº†ç†è®ºä¿è¯ï¼ŒåŒ…æ‹¬è¡¨å¾æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„é€šç”¨é€¼è¿‘å®šç†(universal approximation theorem)ä»¥åŠç†µæ­£åˆ™åŒ–è®­ç»ƒç›®æ ‡çš„æ”¶æ•›æ€§åˆ†æã€‚é€šè¿‡å¯¹å¤šç§å¤æ‚æ•°æ®ç±»å‹çš„å¹¿æ³›æ¨¡æ‹Ÿï¼Œå®éªŒè¯æ˜E2Måœ¨æ€§èƒ½ä¸ŠæŒç»­è¾¾åˆ°SOTAæ°´å¹³ï¼Œä¸”å…¶ä¼˜åŠ¿åœ¨å¤§æ ·æœ¬é‡ä¸‹è¡¨ç°å¾—æ›´ä¸ºæ˜¾è‘—ã€‚è¯¥æ¡†æ¶åœ¨äººç±»æ­»äº¡ç‡åˆ†å¸ƒå’Œçº½çº¦å‡ºç§Ÿè½¦ç½‘ç»œä¸­çš„å®é™…åº”ç”¨ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å¤„ç†ç°å®ä¸–ç•Œç»“æ„åŒ–é¢„æµ‹ä»»åŠ¡ä¸­çš„çµæ´»æ€§å’Œå®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.ME"
      ],
      "primary_category": "stat.ML",
      "comment": "45 pages, 2 figures, 7 tables",
      "pdf_url": "https://arxiv.org/pdf/2509.23544v1",
      "published_date": "2025-09-28 00:46:12 UTC",
      "updated_date": "2025-09-28 00:46:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:11.085724+00:00"
    },
    {
      "arxiv_id": "2509.23542v1",
      "title": "On the Shelf Life of Fine-Tuned LLM Judges: Future Proofing, Backward Compatibility, and Question Generalization",
      "title_zh": "è®ºå¾®è°ƒ LLM è£åˆ¤çš„ç”Ÿå‘½å‘¨æœŸï¼šæœªæ¥é€‚åº”æ€§ã€å‘åå…¼å®¹æ€§ä¸é—®é¢˜æ³›åŒ–",
      "authors": [
        "Janvijay Singh",
        "Austin Xu",
        "Yilun Zhou",
        "Yefan Zhou",
        "Dilek Hakkani-Tur",
        "Shafiq Joty"
      ],
      "abstract": "The LLM-as-a-judge paradigm is widely used in both evaluating free-text model responses and reward modeling for model alignment and finetuning. Recently, finetuning judges with judge-specific data has emerged as an often preferred choice over directly prompting frontier models as judges, as the former achieves better performance with smaller model sizes while being more robust to common biases. However, the standard evaluation ignores several practical concerns of finetuned judges regarding their real world deployment. In this paper, we identify and formalize three aspects that affect the shelf life of these judges: future proofing and backward compatibility -- how well judges finetuned on responses by today's generator models perform on responses by future models or past models, as well as question generalization -- how well judges generalize to unseen questions at test time. We study these three aspects in the math domain under a unified framework with varying train and test distributions, three SFT- and DPO-based finetuning algorithms and three different base models. Experiments suggest that future-proofing is challenging for most models, while backward compatibility is relatively easy, with DPO-trained models consistently improving performance. We further find that continual learning provides a more balanced adaptation to shifts between older and newer response distributions than training solely on stronger or weaker responses. Moreover, all models observe certain degrees of performance degradation when moving from questions seen during training to unseen ones, showing that current judges do not fully generalize to unseen questions. These findings provide insights into practical considerations for developing and deploying judge models in the face of ever-changing generators.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€… (LLM-as-a-judge) åœ¨å®é™…éƒ¨ç½²ä¸­çš„â€œä¿è´¨æœŸâ€é—®é¢˜ï¼Œé‡ç‚¹åˆ†æäº†å…¶åœ¨æœªæ¥é€‚åº”æ€§ (Future Proofing)ã€å‘åå…¼å®¹æ€§ (Backward Compatibility) ä»¥åŠé—®é¢˜æ³›åŒ–æ€§ (Question Generalization) ä¸‰ä¸ªç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶è€…åœ¨æ•°å­¦é¢†åŸŸå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒ (SFT) å’Œç›´æ¥åå¥½ä¼˜åŒ– (DPO) ç­‰ç®—æ³•ï¼Œè¯„ä¼°äº†ä¸åŒè®­ç»ƒä¸æµ‹è¯•åˆ†å¸ƒå¯¹ä¸‰ç§åŸºç¡€æ¨¡å‹çš„å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿è¯„åˆ¤è€…å…·å¤‡è¯„ä¼°æœªæ¥æ›´å…ˆè¿›æ¨¡å‹çš„èƒ½åŠ› (Future Proofing) æå…·æŒ‘æˆ˜ï¼Œè€Œå‘åå…¼å®¹æ—§æ¨¡å‹åˆ™ç›¸å¯¹å®¹æ˜“ï¼Œä¸”åŸºäº DPO è®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºæŒç»­ä¼˜åŒ–ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼ŒæŒç»­å­¦ä¹  (Continual Learning) ç›¸æ¯”äºä»…é’ˆå¯¹ç‰¹å®šå¼ºå¼±åˆ†å¸ƒçš„è®­ç»ƒï¼Œèƒ½æ›´æœ‰æ•ˆåœ°åº”å¯¹æ–°æ—§å“åº”åˆ†å¸ƒä¹‹é—´çš„åç§»ã€‚æ­¤å¤–ï¼Œå¾®è°ƒæ¨¡å‹åœ¨å¤„ç†è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§çš„é—®é¢˜æ—¶æ€§èƒ½ä¼šå‡ºç°è¡°å‡ï¼Œæš´éœ²å‡ºå½“å‰è¯„åˆ¤è€…åœ¨é—®é¢˜æ³›åŒ–èƒ½åŠ›ä¸Šä»å­˜åœ¨å±€é™ã€‚è¿™äº›ç»“è®ºä¸ºåœ¨å¿«é€Ÿæ¼”è¿›çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæ€ä¸­å¼€å‘å’Œéƒ¨ç½²ç¨³å¥çš„è¯„åˆ¤æ¨¡å‹æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2509.23542v1",
      "published_date": "2025-09-28 00:43:52 UTC",
      "updated_date": "2025-09-28 00:43:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:23.292266+00:00"
    },
    {
      "arxiv_id": "2510.02357v1",
      "title": "Privacy in the Age of AI: A Taxonomy of Data Risks",
      "title_zh": "AIæ—¶ä»£çš„éšç§ï¼šæ•°æ®é£é™©åˆ†ç±»ä½“ç³»",
      "authors": [
        "Grace Billiris",
        "Asif Gill",
        "Madhushi Bandara"
      ],
      "abstract": "Artificial Intelligence (AI) systems introduce unprecedented privacy challenges as they process increasingly sensitive data. Traditional privacy frameworks prove inadequate for AI technologies due to unique characteristics such as autonomous learning and black-box decision-making. This paper presents a taxonomy classifying AI privacy risks, synthesised from 45 studies identified through systematic review. We identify 19 key risks grouped under four categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider Threat Risks. Findings reveal a balanced distribution across these dimensions, with human error (9.45%) emerging as the most significant factor. This taxonomy challenges conventional security approaches that typically prioritise technical controls over human factors, highlighting gaps in holistic understanding. By bridging technical and behavioural dimensions of AI privacy, this paper contributes to advancing trustworthy AI development and provides a foundation for future research.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½ (AI) ç³»ç»Ÿå› è‡ªä¸»å­¦ä¹ å’Œé»‘ç›’å†³ç­–ç­‰ç‰¹æ€§å¸¦æ¥çš„éšç§æŒ‘æˆ˜ï¼Œé€šè¿‡å¯¹ 45 é¡¹ç ”ç©¶çš„ç³»ç»Ÿç»¼è¿°ï¼Œæå‡ºäº† AI éšç§é£é™©çš„åˆ†ç±»æ³• (Taxonomy)ã€‚è¯¥åˆ†ç±»æ³•å°†è¯†åˆ«å‡ºçš„ 19 ä¸ªå…³é”®é£é™©å½’çº³ä¸ºæ•°æ®é›†çº§åˆ« (Dataset-Level)ã€æ¨¡å‹çº§åˆ« (Model-Level)ã€åŸºç¡€è®¾æ–½çº§åˆ« (Infrastructure-Level) ä»¥åŠå†…éƒ¨å¨èƒé£é™© (Insider Threat Risks) å››å¤§ç±»åˆ«ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºé£é™©åœ¨ä¸åŒç»´åº¦é—´åˆ†å¸ƒå‡è¡¡ï¼Œå…¶ä¸­äººä¸ºé”™è¯¯ (human error) ä»¥ 9.45% çš„å æ¯”æˆä¸ºæœ€æ˜¾è‘—çš„å› ç´ ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†ä»¥å¾€ä¼˜å…ˆè€ƒè™‘æŠ€æœ¯æ§åˆ¶è€Œå¿½è§†äººä¸ºå› ç´ çš„å®‰å…¨æ€§æ–¹æ³• (security approaches)ï¼Œæ­ç¤ºäº†ç³»ç»Ÿæ€§ç†è§£ä¸Šçš„ç©ºç™½ã€‚é€šè¿‡æ•´åˆ AI éšç§çš„æŠ€æœ¯ä¸è¡Œä¸ºç»´åº¦ï¼Œè¯¥è®ºæ–‡ä¸ºå¼€å‘å¯ä¿¡ AI (trustworthy AI) æä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 2 figures, 4 tables",
      "pdf_url": "https://arxiv.org/pdf/2510.02357v1",
      "published_date": "2025-09-28 00:20:03 UTC",
      "updated_date": "2025-09-28 00:20:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:42.994500+00:00"
    },
    {
      "arxiv_id": "2509.23537v2",
      "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks",
      "title_zh": "è¶…è¶Šæœ€å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼šå¤šè½®å¤šæ™ºèƒ½ä½“ç¼–æ’ä¸å•æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½å¯¹æ¯”",
      "authors": [
        "Aaron Xuxiang Tian",
        "Ruofan Zhang",
        "Jiayao Tang",
        "Young Min Cho",
        "Xueqian Li",
        "Qiang Yi",
        "Ji Wang",
        "Zhunping Zhang",
        "Danrui Qi",
        "Zekun Li",
        "Xingyu Xiang",
        "Sharath Chandra Guntuku",
        "Lyle Ungar",
        "Tianyu Shi",
        "Chi Wang"
      ],
      "abstract": "We study multi-turn multi-agent orchestration, where multiple large language model (LLM) agents interact over multiple turns by iteratively proposing answers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5 Pro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we conduct two experiments: (i) benchmarking orchestration against single-LLM baselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who authored answers and whether they can observe ongoing votes. Orchestration matches or exceeds the strongest single model and consistently outperforms the others. Analysis of best-achievable orchestration performance shows potential for further gains. The ablations show that revealing authorship increases self-voting and ties, and that showing ongoing votes amplifies herding, which speeds convergence but can sometimes yield premature consensus.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè½®å¤šæ™ºèƒ½ä½“ç¼–æ’ï¼ˆMulti-turn multi-agent orchestrationï¼‰æœºåˆ¶ï¼Œé€šè¿‡è®©å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè½®äº¤äº’ä¸­è¿­ä»£æè®®ç­”æ¡ˆå¹¶è¿›è¡ŒæŠ•ç¥¨ä»¥è¾¾æˆå…±è¯†ã€‚ç ”ç©¶åœ¨ GPQA-Diamondã€IFEval å’Œ MuSR åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†åŒ…æ‹¬ Gemini 2.5 Proã€GPT-5ã€Grok 4 å’Œ Claude Sonnet 4 åœ¨å†…çš„å¤šç§æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸å•æ¨¡å‹åŸºå‡†è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ™ºèƒ½ä½“ç¼–æ’çš„æ€§èƒ½å¯ä»¥æ¯”è‚©ç”šè‡³è¶…è¶Šæœ€å¼ºçš„å•æ¨¡å‹ï¼ˆSingle LLMsï¼‰ï¼Œä¸”è¡¨ç°ä¸€è‡´ä¼˜äºå…¶ä»–åŸºå‡†æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥å‘ç°ï¼Œå…¬å¼€ç­”æ¡ˆçš„ä½œè€…èº«ä»½ä¼šå¢åŠ è‡ªæŠ•ç¥¨ï¼ˆself-votingï¼‰å’Œåƒµå±€ç°è±¡ï¼Œè€Œå±•ç¤ºå®æ—¶æŠ•ç¥¨è¿‡ç¨‹è™½ç„¶èƒ½åŠ é€Ÿæ”¶æ•›ï¼Œä½†ä¼šå¼•å‘ä»ä¼—æ•ˆåº”ï¼ˆherdingï¼‰å¹¶å¯èƒ½å¯¼è‡´è¿‡æ—©å…±è¯†ï¼ˆpremature consensusï¼‰ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™ç§ç¼–æ’æ¨¡å¼åœ¨è¿›ä¸€æ­¥æå‡æ¨¡å‹æ€§èƒ½æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 3 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2509.23537v2",
      "published_date": "2025-09-28 00:15:21 UTC",
      "updated_date": "2025-10-01 18:39:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-24T22:37:55.495915+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 172,
  "processed_papers_count": 172,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-24T22:38:44.605093+00:00"
}
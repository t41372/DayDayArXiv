[
  {
    "arxiv_id": "2505.16080v1",
    "title": "SynEVO: A neuro-inspired spatiotemporal evolutional framework for cross-domain adaptation",
    "authors": [
      "Jiayue Liu",
      "Zhongchao Yi",
      "Zhengyang Zhou",
      "Qihe Huang",
      "Kuo Yang",
      "Xu Wang",
      "Yang Wang"
    ],
    "abstract": "Discovering regularities from spatiotemporal systems can benefit various\nscientific and social planning. Current spatiotemporal learners usually train\nan independent model from a specific source data that leads to limited\ntransferability among sources, where even correlated tasks requires new design\nand training. The key towards increasing cross-domain knowledge is to enable\ncollective intelligence and model evolution. In this paper, inspired by\nneuroscience theories, we theoretically derive the increased information\nboundary via learning cross-domain collective intelligence and propose a\nSynaptic EVOlutional spatiotemporal network, SynEVO, where SynEVO breaks the\nmodel independence and enables cross-domain knowledge to be shared and\naggregated. Specifically, we first re-order the sample groups to imitate the\nhuman curriculum learning, and devise two complementary learners, elastic\ncommon container and task-independent extractor to allow model growth and\ntask-wise commonality and personality disentanglement. Then an adaptive dynamic\ncoupler with a new difference metric determines whether the new sample group\nshould be incorporated into common container to achieve model evolution under\nvarious domains. Experiments show that SynEVO improves the generalization\ncapacity by at most 42% under cross-domain scenarios and SynEVO provides a\nparadigm of NeuroAI for knowledge transfer and adaptation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "16 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.16080v1",
    "published_date": "2025-05-21 23:45:51 UTC",
    "updated_date": "2025-05-21 23:45:51 UTC"
  },
  {
    "arxiv_id": "2505.16074v1",
    "title": "Bidirectional Variational Autoencoders",
    "authors": [
      "Bart Kosko",
      "Olaoluwa Adigun"
    ],
    "abstract": "We present the new bidirectional variational autoencoder (BVAE) network\narchitecture. The BVAE uses a single neural network both to encode and decode\ninstead of an encoder-decoder network pair. The network encodes in the forward\ndirection and decodes in the backward direction through the same synaptic web.\nSimulations compared BVAEs and ordinary VAEs on the four image tasks of image\nreconstruction, classification, interpolation, and generation. The image\ndatasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and\nCelebA-64 face images. The bidirectional structure of BVAEs cut the parameter\ncount by almost 50% and still slightly outperformed the unidirectional VAEs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.16074v1",
    "published_date": "2025-05-21 23:19:43 UTC",
    "updated_date": "2025-05-21 23:19:43 UTC"
  },
  {
    "arxiv_id": "2505.16067v1",
    "title": "How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior",
    "authors": [
      "Zidi Xiong",
      "Yuping Lin",
      "Wenya Xie",
      "Pengfei He",
      "Jiliang Tang",
      "Himabindu Lakkaraju",
      "Zhen Xiang"
    ],
    "abstract": "Memory is a critical component in large language model (LLM)-based agents,\nenabling them to store and retrieve past executions to improve task performance\nover time. In this paper, we conduct an empirical study on how memory\nmanagement choices impact the LLM agents' behavior, especially their long-term\nperformance. Specifically, we focus on two fundamental memory operations that\nare widely used by many agent frameworks-addition, which incorporates new\nexperiences into the memory base, and deletion, which selectively removes past\nexperiences-to systematically study their impact on the agent behavior. Through\nour quantitative analysis, we find that LLM agents display an\nexperience-following property: high similarity between a task input and the\ninput in a retrieved memory record often results in highly similar agent\noutputs. Our analysis further reveals two significant challenges associated\nwith this property: error propagation, where inaccuracies in past experiences\ncompound and degrade future performance, and misaligned experience replay,\nwhere outdated or irrelevant experiences negatively influence current tasks.\nThrough controlled experiments, we show that combining selective addition and\ndeletion strategies can help mitigate these negative effects, yielding an\naverage absolute performance gain of 10% compared to naive memory growth.\nFurthermore, we highlight how memory management choices affect agents' behavior\nunder challenging conditions such as task distribution shifts and constrained\nmemory resources. Our findings offer insights into the behavioral dynamics of\nLLM agent memory systems and provide practical guidance for designing memory\ncomponents that support robust, long-term agent performance. We also release\nour code to facilitate further study.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16067v1",
    "published_date": "2025-05-21 22:35:01 UTC",
    "updated_date": "2025-05-21 22:35:01 UTC"
  },
  {
    "arxiv_id": "2505.16066v1",
    "title": "Merge to Mix: Mixing Datasets via Model Merging",
    "authors": [
      "Zhixu Silvia Tao",
      "Kasper Vinken",
      "Hao-Wei Yeh",
      "Avi Cooper",
      "Xavier Boix"
    ],
    "abstract": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16066v1",
    "published_date": "2025-05-21 22:34:13 UTC",
    "updated_date": "2025-05-21 22:34:13 UTC"
  },
  {
    "arxiv_id": "2505.16058v1",
    "title": "Mesh-free sparse identification of nonlinear dynamics",
    "authors": [
      "Mars Liyao Gao",
      "J. Nathan Kutz",
      "Bernat Font"
    ],
    "abstract": "Identifying the governing equations of a dynamical system is one of the most\nimportant tasks for scientific modeling. However, this procedure often requires\nhigh-quality spatio-temporal data uniformly sampled on structured grids. In\nthis paper, we propose mesh-free SINDy, a novel algorithm which leverages the\npower of neural network approximation as well as auto-differentiation to\nidentify governing equations from arbitrary sensor placements and non-uniform\ntemporal data sampling. We show that mesh-free SINDy is robust to high noise\nlevels and limited data while remaining computationally efficient. In our\nimplementation, the training procedure is straight-forward and nearly free of\nhyperparameter tuning, making mesh-free SINDy widely applicable to many\nscientific and engineering problems. In the experiments, we demonstrate its\neffectiveness on a series of PDEs including the Burgers' equation, the heat\nequation, the Korteweg-De Vries equation and the 2D advection-diffusion\nequation. We conduct detailed numerical experiments on all datasets, varying\nthe noise levels and number of samples, and we also compare our approach to\nprevious state-of-the-art methods. It is noteworthy that, even in high-noise\nand low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,\nachieving successful identification with up to 75% noise for the Burgers'\nequation using 5,000 samples and with as few as 100 samples and 1% noise. All\nof this is achieved within a training time of under one minute.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 13 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.16058v1",
    "published_date": "2025-05-21 22:18:37 UTC",
    "updated_date": "2025-05-21 22:18:37 UTC"
  },
  {
    "arxiv_id": "2505.16057v1",
    "title": "Signals of Provenance: Practices & Challenges of Navigating Indicators in AI-Generated Media for Sighted and Blind Individuals",
    "authors": [
      "Ayae Ide",
      "Tory Park",
      "Jaron Mink",
      "Tanusree Sharma"
    ],
    "abstract": "AI-Generated (AIG) content has become increasingly widespread by recent\nadvances in generative models and the easy-to-use tools that have significantly\nlowered the technical barriers for producing highly realistic audio, images,\nand videos through simple natural language prompts. In response, platforms are\nadopting provable provenance with platforms recommending AIG to be\nself-disclosed and signaled to users. However, these indicators may be often\nmissed, especially when they rely solely on visual cues and make them\nineffective to users with different sensory abilities. To address the gap, we\nconducted semi-structured interviews (N=28) with 15 sighted and 13 BLV\nparticipants to examine their interaction with AIG content through\nself-disclosed AI indicators. Our findings reveal diverse mental models and\npractices, highlighting different strengths and weaknesses of content-based\n(e.g., title, description) and menu-aided (e.g., AI labels) indicators. While\nsighted participants leveraged visual and audio cues, BLV participants\nprimarily relied on audio and existing assistive tools, limiting their ability\nto identify AIG. Across both groups, they frequently overlooked menu-aided\nindicators deployed by platforms and rather interacted with content-based\nindicators such as title and comments. We uncovered usability challenges\nstemming from inconsistent indicator placement, unclear metadata, and cognitive\noverload. These issues were especially critical for BLV individuals due to the\ninsufficient accessibility of interface elements. We provide practical\nrecommendations and design implications for future AIG indicators across\nseveral dimensions.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16057v1",
    "published_date": "2025-05-21 22:16:59 UTC",
    "updated_date": "2025-05-21 22:16:59 UTC"
  },
  {
    "arxiv_id": "2505.16056v1",
    "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models",
    "authors": [
      "Jingcong Liang",
      "Siyuan Wang",
      "Miren Tian",
      "Yitong Li",
      "Duyu Tang",
      "Zhongyu Wei"
    ],
    "abstract": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16056v1",
    "published_date": "2025-05-21 22:13:09 UTC",
    "updated_date": "2025-05-21 22:13:09 UTC"
  },
  {
    "arxiv_id": "2505.16048v1",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "authors": [
      "Philipp D. Siedler"
    ],
    "abstract": "We introduce a novel dataset designed to benchmark the physical and spatial\nreasoning capabilities of Large Language Models (LLM) based on topology\noptimization, a method for computing optimal material distributions within a\ndesign space under prescribed loads and supports. In this dataset, LLMs are\nprovided with conditions such as 2D boundary, applied forces and supports, and\nmust reason about the resulting optimal material distribution. The dataset\nincludes a variety of tasks, ranging from filling in masked regions within\npartial structures to predicting complete material distributions. Solving these\ntasks requires understanding the flow of forces and the required material\ndistribution under given constraints, without access to simulation tools or\nexplicit physical models, challenging models to reason about structural\nstability and spatial organization. Our dataset targets the evaluation of\nspatial and physical reasoning abilities in 2D settings, offering a\ncomplementary perspective to traditional language and logic benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16048v1",
    "published_date": "2025-05-21 22:00:20 UTC",
    "updated_date": "2025-05-21 22:00:20 UTC"
  },
  {
    "arxiv_id": "2505.16037v1",
    "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data",
    "authors": [
      "Asterios Tsiourvas",
      "Wei Sun",
      "Georgia Perakis"
    ],
    "abstract": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16037v1",
    "published_date": "2025-05-21 21:34:18 UTC",
    "updated_date": "2025-05-21 21:34:18 UTC"
  },
  {
    "arxiv_id": "2505.16035v1",
    "title": "Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces",
    "authors": [
      "Alejandro García-Castellanos",
      "David R. Wessels",
      "Nicky J. van den Berg",
      "Remco Duits",
      "Daniël M. Pelt",
      "Erik J. Bekkers"
    ],
    "abstract": "We introduce Equivariant Neural Eikonal Solvers, a novel framework that\nintegrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our\napproach employs a single neural field where a unified shared backbone is\nconditioned on signal-specific latent variables - represented as point clouds\nin a Lie group - to model diverse Eikonal solutions. The ENF integration\nensures equivariant mapping from these latent representations to the solution\nfield, delivering three key benefits: enhanced representation efficiency\nthrough weight-sharing, robust geometric grounding, and solution steerability.\nThis steerability allows transformations applied to the latent point cloud to\ninduce predictable, geometrically meaningful modifications in the resulting\nEikonal solution. By coupling these steerable representations with\nPhysics-Informed Neural Networks (PINNs), our framework accurately models\nEikonal travel-time solutions while generalizing to arbitrary Riemannian\nmanifolds with regular group actions. This includes homogeneous spaces such as\nEuclidean, position-orientation, spherical, and hyperbolic manifolds. We\nvalidate our approach through applications in seismic travel-time modeling of\n2D and 3D benchmark datasets. Experimental results demonstrate superior\nperformance, scalability, adaptability, and user controllability compared to\nexisting Neural Operator-based Eikonal solver methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16035v1",
    "published_date": "2025-05-21 21:29:18 UTC",
    "updated_date": "2025-05-21 21:29:18 UTC"
  },
  {
    "arxiv_id": "2505.16031v1",
    "title": "Children's Mental Models of AI Reasoning: Implications for AI Literacy Education",
    "authors": [
      "Aayushi Dangol",
      "Robert Wolfe",
      "Runhua Zhao",
      "JaeWon Kim",
      "Trushaa Ramanan",
      "Katie Davis",
      "Julie A. Kientz"
    ],
    "abstract": "As artificial intelligence (AI) advances in reasoning capabilities, most\nrecently with the emergence of Large Reasoning Models (LRMs), understanding how\nchildren conceptualize AI's reasoning processes becomes critical for fostering\nAI literacy. While one of the \"Five Big Ideas\" in AI education highlights\nreasoning algorithms as central to AI decision-making, less is known about\nchildren's mental models in this area. Through a two-phase approach, consisting\nof a co-design session with 8 children followed by a field study with 106\nchildren (grades 3-8), we identified three models of AI reasoning: Deductive,\nInductive, and Inherent. Our findings reveal that younger children (grades 3-5)\noften attribute AI's reasoning to inherent intelligence, while older children\n(grades 6-8) recognize AI as a pattern recognizer. We highlight three tensions\nthat surfaced in children's understanding of AI reasoning and conclude with\nimplications for scaffolding AI curricula and designing explainable AI tools.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16031v1",
    "published_date": "2025-05-21 21:20:12 UTC",
    "updated_date": "2025-05-21 21:20:12 UTC"
  },
  {
    "arxiv_id": "2505.16027v1",
    "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets",
    "authors": [
      "Qinmei Xu",
      "Yiheng Li",
      "Xianghao Zhan",
      "Ahmet Gorkem Er",
      "Brittany Dashevsky",
      "Chuanjun Xu",
      "Mohammed Alawad",
      "Mengya Yang",
      "Liu Ya",
      "Changsheng Zhou",
      "Xiao Li",
      "Haruka Itakura",
      "Olivier Gevaert"
    ],
    "abstract": "Foundation models leveraging vision-language pretraining have shown promise\nin chest X-ray (CXR) interpretation, yet their real-world performance across\ndiverse populations and diagnostic tasks remains insufficiently evaluated. This\nstudy benchmarks the diagnostic performance and generalizability of foundation\nmodels versus traditional convolutional neural networks (CNNs) on multinational\nCXR datasets. We evaluated eight CXR diagnostic models - five vision-language\nfoundation models and three CNN-based architectures - across 37 standardized\nclassification tasks using six public datasets from the USA, Spain, India, and\nVietnam, and three private datasets from hospitals in China. Performance was\nassessed using AUROC, AUPRC, and other metrics across both shared and\ndataset-specific tasks. Foundation models outperformed CNNs in both accuracy\nand task coverage. MAVL, a model incorporating knowledge-enhanced prompts and\nstructured supervision, achieved the highest performance on public (mean AUROC:\n0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,\nranking first in 14 of 37 public and 3 of 4 private tasks. All models showed\nreduced performance on pediatric cases, with average AUROC dropping from 0.88\n+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings\nhighlight the value of structured supervision and prompt design in radiologic\nAI and suggest future directions including geographic expansion and ensemble\nmodeling for clinical deployment. Code for all evaluated models is available at\nhttps://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.2"
    ],
    "primary_category": "eess.IV",
    "comment": "78 pages, 7 figures, 2 tabeles",
    "pdf_url": "http://arxiv.org/pdf/2505.16027v1",
    "published_date": "2025-05-21 21:16:50 UTC",
    "updated_date": "2025-05-21 21:16:50 UTC"
  },
  {
    "arxiv_id": "2505.16024v1",
    "title": "Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging",
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ],
    "abstract": "Diffusion trajectory distillation methods aim to accelerate sampling in\ndiffusion models, which produce high-quality outputs but suffer from slow\nsampling speeds. These methods train a student model to approximate the\nmulti-step denoising process of a pretrained teacher model in a single step,\nenabling one-shot generation. However, theoretical insights into the trade-off\nbetween different distillation strategies and generative quality remain\nlimited, complicating their optimization and selection. In this work, we take a\nfirst step toward addressing this gap. Specifically, we reinterpret trajectory\ndistillation as an operator merging problem in the linear regime, where each\nstep of the teacher model is represented as a linear operator acting on noisy\ndata. These operators admit a clear geometric interpretation as projections and\nrescalings corresponding to the noise schedule. During merging, signal\nshrinkage occurs as a convex combination of operators, arising from both\ndiscretization and limited optimization time of the student model. We propose a\ndynamic programming algorithm to compute the optimal merging strategy that\nmaximally preserves signal fidelity. Additionally, we demonstrate the existence\nof a sharp phase transition in the optimal strategy, governed by data\ncovariance structures. Our findings enhance the theoretical understanding of\ndiffusion trajectory distillation and offer practical insights for improving\ndistillation strategies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.16024v1",
    "published_date": "2025-05-21 21:13:02 UTC",
    "updated_date": "2025-05-21 21:13:02 UTC"
  },
  {
    "arxiv_id": "2505.16022v1",
    "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning",
    "authors": [
      "Wei Liu",
      "Siya Qi",
      "Xinyu Wang",
      "Chen Qian",
      "Yali Du",
      "Yulan He"
    ],
    "abstract": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 5 tables, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.16022v1",
    "published_date": "2025-05-21 21:12:35 UTC",
    "updated_date": "2025-05-21 21:12:35 UTC"
  },
  {
    "arxiv_id": "2505.16008v1",
    "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization",
    "authors": [
      "Wenrui Yu",
      "Yiyi Chen",
      "Johannes Bjerva",
      "Sokol Kosta",
      "Qiongxiu Li"
    ],
    "abstract": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16008v1",
    "published_date": "2025-05-21 20:48:24 UTC",
    "updated_date": "2025-05-21 20:48:24 UTC"
  },
  {
    "arxiv_id": "2505.16004v1",
    "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations",
    "authors": [
      "Aaron J. Li",
      "Suraj Srinivas",
      "Usha Bhalla",
      "Himabindu Lakkaraju"
    ],
    "abstract": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16004v1",
    "published_date": "2025-05-21 20:42:05 UTC",
    "updated_date": "2025-05-21 20:42:05 UTC"
  },
  {
    "arxiv_id": "2505.16003v1",
    "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models",
    "authors": [
      "Roland Daynauth",
      "Christopher Clarke",
      "Krisztian Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "abstract": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.16003v1",
    "published_date": "2025-05-21 20:40:30 UTC",
    "updated_date": "2025-05-21 20:40:30 UTC"
  },
  {
    "arxiv_id": "2505.16002v1",
    "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions",
    "authors": [
      "Sasha Boguraev",
      "Christopher Potts",
      "Kyle Mahowald"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 19 figures, 11 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.16002v1",
    "published_date": "2025-05-21 20:37:57 UTC",
    "updated_date": "2025-05-21 20:37:57 UTC"
  },
  {
    "arxiv_id": "2505.16000v1",
    "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model",
    "authors": [
      "Mehrdad ghassabi",
      "Pedram Rostami",
      "Hamidreza Baradaran Kashani",
      "Amirhossein Poursina",
      "Zahra Kazemi",
      "Milad Tavakoli"
    ],
    "abstract": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.16000v1",
    "published_date": "2025-05-21 20:30:47 UTC",
    "updated_date": "2025-05-21 20:30:47 UTC"
  },
  {
    "arxiv_id": "2505.15998v1",
    "title": "Exploring Flow-Lenia Universes with a Curiosity-driven AI Scientist: Discovering Diverse Ecosystem Dynamics",
    "authors": [
      "Thomas Michel",
      "Marko Cvjetko",
      "Gautier Hamon",
      "Pierre-Yves Oudeyer",
      "Clément Moulin-Frier"
    ],
    "abstract": "We present a method for the automated discovery of system-level dynamics in\nFlow-Lenia$-$a continuous cellular automaton (CA) with mass conservation and\nparameter localization$-$using a curiosity-driven AI scientist. This method\naims to uncover processes leading to self-organization of evolutionary and\necosystemic dynamics in CAs. We build on previous work which uses diversity\nsearch algorithms in Lenia to find self-organized individual patterns, and\nextend it to large environments that support distinct interacting patterns. We\nadapt Intrinsically Motivated Goal Exploration Processes (IMGEPs) to drive\nexploration of diverse Flow-Lenia environments using simulation-wide metrics,\nsuch as evolutionary activity, compression-based complexity, and multi-scale\nentropy. We test our method in two experiments, showcasing its ability to\nilluminate significantly more diverse dynamics compared to random search. We\nshow qualitative results illustrating how ecosystemic simulations enable\nself-organization of complex collective behaviors not captured by previous\nindividual pattern search and analysis. We complement automated discovery with\nan interactive exploration tool, creating an effective human-AI collaborative\nworkflow for scientific investigation. Though demonstrated specifically with\nFlow-Lenia, this methodology provides a framework potentially applicable to\nother parameterizable complex systems where understanding emergent collective\nproperties is of interest.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 10 figures, submitted to ALIFE 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2505.15998v1",
    "published_date": "2025-05-21 20:28:58 UTC",
    "updated_date": "2025-05-21 20:28:58 UTC"
  },
  {
    "arxiv_id": "2505.15997v1",
    "title": "Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers",
    "authors": [
      "Mehran Zoravar",
      "Shadi Alijani",
      "Homayoun Najjaran"
    ],
    "abstract": "Exploring the trustworthiness of deep learning models is crucial, especially\nin critical domains such as medical imaging decision support systems. Conformal\nprediction has emerged as a rigorous means of providing deep learning models\nwith reliable uncertainty estimates and safety guarantees. However, conformal\nprediction results face challenges due to the backbone model's struggles in\ndomain-shifted scenarios, such as variations in different sources. To aim this\nchallenge, this paper proposes a novel framework termed Conformal Ensemble of\nVision Transformers (CE-ViTs) designed to enhance image classification\nperformance by prioritizing domain adaptation and model robustness, while\naccounting for uncertainty. The proposed method leverages an ensemble of vision\ntransformer models in the backbone, trained on diverse datasets including\nHAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning\napproach, calibrated through the combined mentioned datasets, aims to enhance\ndomain adaptation through conformal learning. Experimental results underscore\nthat the framework achieves a high coverage rate of 90.38\\%, representing an\nimprovement of 9.95\\% compared to the HAM10000 model. This indicates a strong\nlikelihood that the prediction set includes the true label compared to singular\nmodels. Ensemble learning in CE-ViTs significantly improves conformal\nprediction performance, increasing the average prediction set size for\nchallenging misclassified samples from 1.86 to 3.075.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 4 figures, conference (ccece 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.15997v1",
    "published_date": "2025-05-21 20:28:43 UTC",
    "updated_date": "2025-05-21 20:28:43 UTC"
  },
  {
    "arxiv_id": "2505.15966v1",
    "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning",
    "authors": [
      "Alex Su",
      "Haozhe Wang",
      "Weimin Ren",
      "Fangzhen Lin",
      "Wenhu Chen"
    ],
    "abstract": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Haozhe Wang and Alex Su contributed equally and listed alphabetically",
    "pdf_url": "http://arxiv.org/pdf/2505.15966v1",
    "published_date": "2025-05-21 19:35:08 UTC",
    "updated_date": "2025-05-21 19:35:08 UTC"
  },
  {
    "arxiv_id": "2505.15962v1",
    "title": "Pre-training Large Memory Language Models with Internal and External Knowledge",
    "authors": [
      "Linxi Zhao",
      "Sofian Zalouk",
      "Christian K. Belardi",
      "Justin Lovelace",
      "Jin Peng Zhou",
      "Kilian Q. Weinberger",
      "Yoav Artzi",
      "Jennifer J. Sun"
    ],
    "abstract": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15962v1",
    "published_date": "2025-05-21 19:26:03 UTC",
    "updated_date": "2025-05-21 19:26:03 UTC"
  },
  {
    "arxiv_id": "2505.15957v1",
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "authors": [
      "Chih-Kai Yang",
      "Neo S. Ho",
      "Hung-yi Lee"
    ],
    "abstract": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Project Website: https://github.com/b08202033/LALM-Evaluation-Survey",
    "pdf_url": "http://arxiv.org/pdf/2505.15957v1",
    "published_date": "2025-05-21 19:17:29 UTC",
    "updated_date": "2025-05-21 19:17:29 UTC"
  },
  {
    "arxiv_id": "2505.15952v1",
    "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance",
    "authors": [
      "Mohammad Reza Taesiri",
      "Abhijay Ghildyal",
      "Saman Zadtootaghaj",
      "Nabajeet Barman",
      "Cor-Paul Bezemer"
    ],
    "abstract": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project website with code and data:\n  https://asgaardlab.github.io/videogameqa-bench/",
    "pdf_url": "http://arxiv.org/pdf/2505.15952v1",
    "published_date": "2025-05-21 19:08:38 UTC",
    "updated_date": "2025-05-21 19:08:38 UTC"
  },
  {
    "arxiv_id": "2505.15946v1",
    "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding",
    "authors": [
      "Yuxiang Wei",
      "Yanteng Zhang",
      "Xi Xiao",
      "Tianyang Wang",
      "Xiao Wang",
      "Vince D. Calhoun"
    ],
    "abstract": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15946v1",
    "published_date": "2025-05-21 19:02:54 UTC",
    "updated_date": "2025-05-21 19:02:54 UTC"
  },
  {
    "arxiv_id": "2505.15929v1",
    "title": "PhyX: Does Your Model Have the \"Wits\" for Physical Reasoning?",
    "authors": [
      "Hui Shen",
      "Taiqiang Wu",
      "Qi Han",
      "Yunta Hsieh",
      "Jizhou Wang",
      "Yuyue Zhang",
      "Yuxin Cheng",
      "Zijian Hao",
      "Yuansheng Ni",
      "Xin Wang",
      "Zhongwei Wan",
      "Kai Zhang",
      "Wendong Xu",
      "Jing Xiong",
      "Ping Luo",
      "Wenhu Chen",
      "Chaofan Tao",
      "Zhuoqing Mao",
      "Ngai Wong"
    ],
    "abstract": "Existing benchmarks fail to capture a crucial aspect of intelligence:\nphysical reasoning, the integrated ability to combine domain knowledge,\nsymbolic reasoning, and understanding of real-world constraints. To address\nthis gap, we introduce PhyX: the first large-scale benchmark designed to assess\nmodels capacity for physics-grounded reasoning in visual scenarios. PhyX\nincludes 3K meticulously curated multimodal questions spanning 6 reasoning\ntypes across 25 sub-domains and 6 core physics domains: thermodynamics,\nelectromagnetism, mechanics, modern physics, optics, and wave\\&acoustics. In\nour comprehensive evaluation, even state-of-the-art models struggle\nsignificantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and\nGPT-o4-mini achieve only 32.5\\%, 42.2\\%, and 45.8\\% accuracy\nrespectively-performance gaps exceeding 29\\% compared to human experts. Our\nanalysis exposes critical limitations in current models: over-reliance on\nmemorized disciplinary knowledge, excessive dependence on mathematical\nformulations, and surface-level visual pattern matching rather than genuine\nphysical understanding. We provide in-depth analysis through fine-grained\nstatistics, detailed case studies, and multiple evaluation paradigms to\nthoroughly examine physical reasoning capabilities. To ensure reproducibility,\nwe implement a compatible evaluation protocol based on widely-used toolkits\nsuch as VLMEvalKit, enabling one-click evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15929v1",
    "published_date": "2025-05-21 18:33:50 UTC",
    "updated_date": "2025-05-21 18:33:50 UTC"
  },
  {
    "arxiv_id": "2505.15925v1",
    "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving",
    "authors": [
      "Bowen Feng",
      "Zhiting Mei",
      "Baiang Li",
      "Julian Ost",
      "Roger Girgis",
      "Anirudha Majumdar",
      "Felix Heide"
    ],
    "abstract": "While autonomous driving (AD) stacks struggle with decision making under\npartial observability and real-world complexity, human drivers are capable of\ncommonsense reasoning to make near-optimal decisions with limited information.\nRecent work has attempted to leverage finetuned Vision-Language Models (VLMs)\nfor trajectory planning at inference time to emulate human behavior. Despite\ntheir success in benchmark evaluations, these methods are often impractical to\ndeploy (a 70B parameter VLM inference at merely 8 tokens per second requires\nmore than 160G of memory), and their monolithic network structure prohibits\nsafety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for\nautonomous Driving (VERDI), a training-time framework that distills the\nreasoning process and commonsense knowledge of VLMs into the AD stack. VERDI\naugments modular differentiable end-to-end (e2e) AD models by aligning\nintermediate module outputs at the perception, prediction, and planning stages\nwith text features explaining the driving reasoning process produced by VLMs.\nBy encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD\nstack to internalize structured reasoning, without incurring the inference-time\ncosts of large VLMs. We demonstrate the effectiveness of our method on the\nNuScenes dataset and find that VERDI outperforms existing e2e methods that do\nnot embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high\ninference speed.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15925v1",
    "published_date": "2025-05-21 18:24:36 UTC",
    "updated_date": "2025-05-21 18:24:36 UTC"
  },
  {
    "arxiv_id": "2505.15918v1",
    "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization",
    "authors": [
      "Aliakbar Nafar",
      "Kristen Brent Venable",
      "Zijun Cui",
      "Parisa Kordjamshidi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15918v1",
    "published_date": "2025-05-21 18:15:05 UTC",
    "updated_date": "2025-05-21 18:15:05 UTC"
  },
  {
    "arxiv_id": "2505.15916v1",
    "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law",
    "authors": [
      "Juvenal Domingos Júnior",
      "Augusto Faria",
      "E. Seiti de Oliveira",
      "Erick de Brito",
      "Matheus Teotonio",
      "Andre Assumpção",
      "Diedre Carmo",
      "Roberto Lotufo",
      "Jayr Pereira"
    ],
    "abstract": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15916v1",
    "published_date": "2025-05-21 18:11:41 UTC",
    "updated_date": "2025-05-21 18:11:41 UTC"
  },
  {
    "arxiv_id": "2505.15888v1",
    "title": "Last Layer Empirical Bayes",
    "authors": [
      "Valentin Villecroze",
      "Yixin Wang",
      "Gabriel Loaiza-Ganem"
    ],
    "abstract": "The task of quantifying the inherent uncertainty associated with neural\nnetwork predictions is a key challenge in artificial intelligence. Bayesian\nneural networks (BNNs) and deep ensembles are among the most prominent\napproaches to tackle this task. Both approaches produce predictions by\ncomputing an expectation of neural network outputs over some distribution on\nthe corresponding weights; this distribution is given by the posterior in the\ncase of BNNs, and by a mixture of point masses for ensembles. Inspired by\nrecent work showing that the distribution used by ensembles can be understood\nas a posterior corresponding to a learned data-dependent prior, we propose last\nlayer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a\nnormalizing flow, which is then trained to maximize the evidence lower bound;\nto retain tractability we use the flow only on the last layer. We show why LLEB\nis well motivated, and how it interpolates between standard BNNs and ensembles\nin terms of the strength of the prior that they use. LLEB performs on par with\nexisting approaches, highlighting that empirical Bayes is a promising direction\nfor future research in uncertainty quantification.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the ICBINB Worshop at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15888v1",
    "published_date": "2025-05-21 18:00:00 UTC",
    "updated_date": "2025-05-21 18:00:00 UTC"
  },
  {
    "arxiv_id": "2505.15810v2",
    "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
    "authors": [
      "Yuqi Zhou",
      "Sunhao Dai",
      "Shuai Wang",
      "Kaiwen Zhou",
      "Qinglin Jia",
      "Jun Xu"
    ],
    "abstract": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15810v2",
    "published_date": "2025-05-21 17:59:09 UTC",
    "updated_date": "2025-05-22 11:15:23 UTC"
  },
  {
    "arxiv_id": "2505.15808v1",
    "title": "Neural Conditional Transport Maps",
    "authors": [
      "Carlos Rodriguez-Pardo",
      "Leonardo Chiani",
      "Emanuele Borgonovo",
      "Massimo Tavoni"
    ],
    "abstract": "We present a neural framework for learning conditional optimal transport (OT)\nmaps between probability distributions. Our approach introduces a conditioning\nmechanism capable of processing both categorical and continuous conditioning\nvariables simultaneously. At the core of our method lies a hypernetwork that\ngenerates transport layer parameters based on these inputs, creating adaptive\nmappings that outperform simpler conditioning methods. Comprehensive ablation\nstudies demonstrate the superior performance of our method over baseline\nconfigurations. Furthermore, we showcase an application to global sensitivity\nanalysis, offering high performance in computing OT-based sensitivity indices.\nThis work advances the state-of-the-art in conditional optimal transport,\nenabling broader application of optimal transport principles to complex,\nhigh-dimensional domains such as generative modeling and black-box model\nexplainability.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.PR",
      "stat.AP",
      "stat.ML",
      "49Q22 (Primary) 68T07 (Secondary)",
      "I.5.1; I.2.0; G.3"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review. Supplementary material included in the pdf",
    "pdf_url": "http://arxiv.org/pdf/2505.15808v1",
    "published_date": "2025-05-21 17:59:02 UTC",
    "updated_date": "2025-05-21 17:59:02 UTC"
  },
  {
    "arxiv_id": "2505.15879v1",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "authors": [
      "Yue Fan",
      "Xuehai He",
      "Diji Yang",
      "Kaizhi Zheng",
      "Ching-Chen Kuo",
      "Yuting Zheng",
      "Sravana Jyothi Narayanaraju",
      "Xinze Guan",
      "Xin Eric Wang"
    ],
    "abstract": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15879v1",
    "published_date": "2025-05-21 17:54:49 UTC",
    "updated_date": "2025-05-21 17:54:49 UTC"
  },
  {
    "arxiv_id": "2505.15801v1",
    "title": "VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models",
    "authors": [
      "Yuchen Yan",
      "Jin Jiang",
      "Zhenbang Ren",
      "Yijun Li",
      "Xudong Cai",
      "Yang Liu",
      "Xin Xu",
      "Mengdi Zhang",
      "Jian Shao",
      "Yongliang Shen",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "abstract": "Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved\nremarkable performance in the domain of reasoning. A key component of their\ntraining is the incorporation of verifiable rewards within reinforcement\nlearning (RL). However, existing reward benchmarks do not evaluate\nreference-based reward systems, leaving researchers with limited understanding\nof the accuracy of verifiers used in RL. In this paper, we introduce two\nbenchmarks, VerifyBench and VerifyBench-Hard, designed to assess the\nperformance of reference-based reward systems. These benchmarks are constructed\nthrough meticulous data collection and curation, followed by careful human\nannotation to ensure high quality. Current models still show considerable room\nfor improvement on both VerifyBench and VerifyBench-Hard, especially\nsmaller-scale models. Furthermore, we conduct a thorough and comprehensive\nanalysis of evaluation results, offering insights for understanding and\ndeveloping reference-based reward systems. Our proposed benchmarks serve as\neffective tools for guiding the development of verifier accuracy and the\nreasoning capabilities of models trained via RL in reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Dataset: https://huggingface.co/datasets/ZJU-REAL/VerifyBench",
    "pdf_url": "http://arxiv.org/pdf/2505.15801v1",
    "published_date": "2025-05-21 17:54:43 UTC",
    "updated_date": "2025-05-21 17:54:43 UTC"
  },
  {
    "arxiv_id": "2505.15792v1",
    "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts",
    "authors": [
      "Danna Zheng",
      "Mirella Lapata",
      "Jeff Z. Pan"
    ],
    "abstract": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15792v1",
    "published_date": "2025-05-21 17:46:38 UTC",
    "updated_date": "2025-05-21 17:46:38 UTC"
  },
  {
    "arxiv_id": "2505.15790v1",
    "title": "Exploring the Innovation Opportunities for Pre-trained Models",
    "authors": [
      "Minjung Park",
      "Jodi Forlizzi",
      "John Zimmerman"
    ],
    "abstract": "Innovators transform the world by understanding where services are\nsuccessfully meeting customers' needs and then using this knowledge to identify\nfailsafe opportunities for innovation. Pre-trained models have changed the AI\ninnovation landscape, making it faster and easier to create new AI products and\nservices. Understanding where pre-trained models are successful is critical for\nsupporting AI innovation. Unfortunately, the hype cycle surrounding pre-trained\nmodels makes it hard to know where AI can really be successful. To address\nthis, we investigated pre-trained model applications developed by HCI\nresearchers as a proxy for commercially successful applications. The research\napplications demonstrate technical capabilities, address real user needs, and\navoid ethical challenges. Using an artifact analysis approach, we categorized\ncapabilities, opportunity domains, data types, and emerging interaction design\npatterns, uncovering some of the opportunity space for innovation with\npre-trained models.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "33 pages, 20 figures, 4 tables, DIS",
    "pdf_url": "http://arxiv.org/pdf/2505.15790v1",
    "published_date": "2025-05-21 17:43:46 UTC",
    "updated_date": "2025-05-21 17:43:46 UTC"
  },
  {
    "arxiv_id": "2505.15784v1",
    "title": "Large Language Models as Computable Approximations to Solomonoff Induction",
    "authors": [
      "Jun Wan",
      "Lingrui Mei"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) calls for a rigorous\ntheoretical framework to explain their empirical success. While significant\nprogress has been made in understanding LLM behaviors, existing theoretical\nframeworks remain fragmented in explaining emergent phenomena through a unified\nmathematical lens. We establish the first formal connection between LLM\narchitectures and Algorithmic Information Theory (AIT) by proving two\nfundamental results: (1) the training process computationally approximates\nSolomonoff prior through loss minimization interpreted as program length\noptimization, and (2) next-token prediction implements approximate Solomonoff\ninduction. We leverage AIT to provide a unified theoretical explanation for\nin-context learning, few-shot learning, and scaling laws. Furthermore, our\ntheoretical insights lead to a principled method for few-shot example selection\nthat prioritizes samples where models exhibit lower predictive confidence. We\ndemonstrate through experiments on diverse text classification benchmarks that\nthis strategy yields significant performance improvements, particularly for\nsmaller model architectures, when compared to selecting high-confidence\nexamples. Our framework bridges the gap between theoretical foundations and\npractical LLM behaviors, providing both explanatory power and actionable\ninsights for future model development.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Both authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2505.15784v1",
    "published_date": "2025-05-21 17:35:08 UTC",
    "updated_date": "2025-05-21 17:35:08 UTC"
  },
  {
    "arxiv_id": "2505.15779v1",
    "title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "authors": [
      "Chuanhao Li",
      "Jianwen Sun",
      "Yukang Feng",
      "Mingliang Zhai",
      "Yifan Chang",
      "Kaipeng Zhang"
    ],
    "abstract": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 7 figures, a framework that integrates reference images\n  from the Internet into T2I/TI2I models",
    "pdf_url": "http://arxiv.org/pdf/2505.15779v1",
    "published_date": "2025-05-21 17:31:49 UTC",
    "updated_date": "2025-05-21 17:31:49 UTC"
  },
  {
    "arxiv_id": "2505.15778v1",
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space",
    "authors": [
      "Zhen Zhang",
      "Xuehai He",
      "Weixiang Yan",
      "Ao Shen",
      "Chenyang Zhao",
      "Shuohang Wang",
      "Yelong Shen",
      "Xin Eric Wang"
    ],
    "abstract": "Human cognition typically involves thinking through abstract, fluid concepts\nrather than strictly using discrete linguistic tokens. Current reasoning\nmodels, however, are constrained to reasoning within the boundaries of human\nlanguage, processing discrete token embeddings that represent fixed points in\nthe semantic space. This discrete constraint restricts the expressive power and\nupper potential of such reasoning models, often causing incomplete exploration\nof reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling\none token per step. In this work, we introduce Soft Thinking, a training-free\nmethod that emulates human-like \"soft\" reasoning by generating soft, abstract\nconcept tokens in a continuous concept space. These concept tokens are created\nby the probability-weighted mixture of token embeddings, which form the\ncontinuous concept space, enabling smooth transitions and richer\nrepresentations that transcend traditional discrete boundaries. In essence,\neach generated concept token encapsulates multiple meanings from related\ndiscrete tokens, implicitly exploring various reasoning paths to converge\neffectively toward the correct answer. Empirical evaluations on diverse\nmathematical and coding benchmarks consistently demonstrate the effectiveness\nand efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points\nwhile simultaneously reducing token usage by up to 22.4% compared to standard\nCoT. Qualitative analysis further reveals that Soft Thinking outputs remain\nhighly interpretable and readable, highlighting the potential of Soft Thinking\nto break the inherent bottleneck of discrete language-based reasoning. Code is\navailable at https://github.com/eric-ai-lab/Soft-Thinking.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15778v1",
    "published_date": "2025-05-21 17:29:15 UTC",
    "updated_date": "2025-05-21 17:29:15 UTC"
  },
  {
    "arxiv_id": "2505.15765v1",
    "title": "Constructing a 3D Town from a Single Image",
    "authors": [
      "Kaizhi Zheng",
      "Ruijian Zhang",
      "Jing Gu",
      "Jie Yang",
      "Xin Eric Wang"
    ],
    "abstract": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15765v1",
    "published_date": "2025-05-21 17:10:47 UTC",
    "updated_date": "2025-05-21 17:10:47 UTC"
  },
  {
    "arxiv_id": "2505.15754v1",
    "title": "Improving planning and MBRL with temporally-extended actions",
    "authors": [
      "Palash Chatterjee",
      "Roni Khardon"
    ],
    "abstract": "Continuous time systems are often modeled using discrete time dynamics but\nthis requires a small simulation step to maintain accuracy. In turn, this\nrequires a large planning horizon which leads to computationally demanding\nplanning problems and reduced performance. Previous work in model free\nreinforcement learning has partially addressed this issue using action repeats\nwhere a policy is learned to determine a discrete action duration. Instead we\npropose to control the continuous decision timescale directly by using\ntemporally-extended actions and letting the planner treat the duration of the\naction as an additional optimization variable along with the standard action\nvariables. This additional structure has multiple advantages. It speeds up\nsimulation time of trajectories and, importantly, it allows for deep horizon\nsearch in terms of primitive actions while using a shallow search depth in the\nplanner. In addition, in the model based reinforcement learning (MBRL) setting,\nit reduces compounding errors from model learning and improves training time\nfor models. We show that this idea is effective and that the range for action\ndurations can be automatically selected using a multi-armed bandit formulation\nand integrated into the MBRL framework. An extensive experimental evaluation\nboth in planning and in MBRL, shows that our approach yields faster planning,\nbetter solutions, and that it enables solutions to problems that are not solved\nin the standard formulation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15754v1",
    "published_date": "2025-05-21 16:59:32 UTC",
    "updated_date": "2025-05-21 16:59:32 UTC"
  },
  {
    "arxiv_id": "2505.15753v1",
    "title": "Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval",
    "authors": [
      "Taiye Chen",
      "Zeming Wei",
      "Ang Li",
      "Yisen Wang"
    ],
    "abstract": "Large Language Models (LLMs) are known to be vulnerable to jailbreaking\nattacks, wherein adversaries exploit carefully engineered prompts to induce\nharmful or unethical responses. Such threats have raised critical concerns\nabout the safety and reliability of LLMs in real-world deployment. While\nexisting defense mechanisms partially mitigate such risks, subsequent\nadvancements in adversarial techniques have enabled novel jailbreaking methods\nto circumvent these protections, exposing the limitations of static defense\nframeworks. In this work, we explore defending against evolving jailbreaking\nthreats through the lens of context retrieval. First, we conduct a preliminary\nstudy demonstrating that even a minimal set of safety-aligned examples against\na particular jailbreak can significantly enhance robustness against this attack\npattern. Building on this insight, we further leverage the retrieval-augmented\ngeneration (RAG) techniques and propose Safety Context Retrieval (SCR), a\nscalable and robust safeguarding paradigm for LLMs against jailbreaking. Our\ncomprehensive experiments demonstrate how SCR achieves superior defensive\nperformance against both established and emerging jailbreaking tactics,\ncontributing a new paradigm to LLM safety. Our code will be available upon\npublication.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15753v1",
    "published_date": "2025-05-21 16:58:14 UTC",
    "updated_date": "2025-05-21 16:58:14 UTC"
  },
  {
    "arxiv_id": "2505.15747v2",
    "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
    "authors": [
      "Kanan Kiguchi",
      "Yunhao Tu",
      "Katsuhiro Ajito",
      "Fady Alnajjar",
      "Kazuyuki Murase"
    ],
    "abstract": "We propose a novel framework for integrating fragmented multi-modal data in\nAlzheimer's disease (AD) research using large language models (LLMs) and\nknowledge graphs. While traditional multimodal analysis requires matched\npatient IDs across datasets, our approach demonstrates population-level\nintegration of MRI, gene expression, biomarkers, EEG, and clinical indicators\nfrom independent cohorts. Statistical analysis identified significant features\nin each modality, which were connected as nodes in a knowledge graph. LLMs then\nanalyzed the graph to extract potential correlations and generate hypotheses in\nnatural language. This approach revealed several novel relationships, including\na potential pathway linking metabolic risk factors to tau protein abnormalities\nvia neuroinflammation (r>0.6, p<0.001), and unexpected correlations between\nfrontal EEG channels and specific gene expression profiles (r=0.42-0.58,\np<0.01). Cross-validation with independent datasets confirmed the robustness of\nmajor findings, with consistent effect sizes across cohorts (variance <15%).\nThe reproducibility of these findings was further supported by expert review\n(Cohen's k=0.82) and computational validation. Our framework enables cross\nmodal integration at a conceptual level without requiring patient ID matching,\noffering new possibilities for understanding AD pathology through fragmented\ndata reuse and generating testable hypotheses for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.2.1; H.3.1; J.3"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 8 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15747v2",
    "published_date": "2025-05-21 16:51:49 UTC",
    "updated_date": "2025-05-22 03:58:27 UTC"
  },
  {
    "arxiv_id": "2505.15746v1",
    "title": "Higher-order Structure Boosts Link Prediction on Temporal Graphs",
    "authors": [
      "Jingzhe Liu",
      "Zhigang Hua",
      "Yan Xie",
      "Bingheng Li",
      "Harry Shomer",
      "Yu Song",
      "Kaveh Hassani",
      "Jiliang Tang"
    ],
    "abstract": "Temporal Graph Neural Networks (TGNNs) have gained growing attention for\nmodeling and predicting structures in temporal graphs. However, existing TGNNs\nprimarily focus on pairwise interactions while overlooking higher-order\nstructures that are integral to link formation and evolution in real-world\ntemporal graphs. Meanwhile, these models often suffer from efficiency\nbottlenecks, further limiting their expressive power. To tackle these\nchallenges, we propose a Higher-order structure Temporal Graph Neural Network,\nwhich incorporates hypergraph representations into temporal graph learning. In\nparticular, we develop an algorithm to identify the underlying higher-order\nstructures, enhancing the model's ability to capture the group interactions.\nFurthermore, by aggregating multiple edge features into hyperedge\nrepresentations, HTGN effectively reduces memory cost during training. We\ntheoretically demonstrate the enhanced expressiveness of our approach and\nvalidate its effectiveness and efficiency through extensive experiments on\nvarious real-world temporal graphs. Experimental results show that HTGN\nachieves superior performance on dynamic link prediction while reducing memory\ncosts by up to 50\\% compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15746v1",
    "published_date": "2025-05-21 16:51:44 UTC",
    "updated_date": "2025-05-21 16:51:44 UTC"
  },
  {
    "arxiv_id": "2505.15742v1",
    "title": "Neuro-Argumentative Learning with Case-Based Reasoning",
    "authors": [
      "Adam Gould",
      "Francesca Toni"
    ],
    "abstract": "We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual\nAA-CBR), a data-driven, neurosymbolic classification model in which the outcome\nis determined by an argumentation debate structure that is learned\nsimultaneously with neural-based feature extractors. Each argument in the\ndebate is an observed case from the training data, favouring their labelling.\nCases attack or support those with opposing or agreeing labellings, with the\nstrength of each argument and relationship learned through gradient-based\nmethods. This argumentation debate structure provides human-aligned reasoning,\nimproving model interpretability compared to traditional neural networks (NNs).\nUnlike the existing purely symbolic variant, Abstract Argumentation for\nCase-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class\nclassification, automatic learning of feature and data point importance,\nassigning uncertainty values to outcomes, using all available data points, and\ndoes not require binary features. We show that Gradual AA-CBR performs\ncomparably to NNs whilst significantly outperforming existing AA-CBR\nformulations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to NeSy25",
    "pdf_url": "http://arxiv.org/pdf/2505.15742v1",
    "published_date": "2025-05-21 16:49:47 UTC",
    "updated_date": "2025-05-21 16:49:47 UTC"
  },
  {
    "arxiv_id": "2505.15740v1",
    "title": "HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement",
    "authors": [
      "Jilin Hu",
      "Jianyu Zhang",
      "Yongwang Zhao",
      "Talia Ringer"
    ],
    "abstract": "Formal methods is pivotal for verifying the reliability of critical systems\nthrough rigorous mathematical proofs. However, its adoption is hindered by\nlabor-intensive manual proofs and the expertise required to use theorem\nprovers. Recent advancements in large language models (LLMs) offer new\nopportunities for automated theorem proving. Two promising approaches are\ngenerating tactics step by step and generating a whole proof directly with an\nLLM. However, existing work makes no attempt to combine the two approaches. In\nthis work, we introduce HybridProver, a dual-model proof synthesis framework\nthat combines tactic-based generation and whole-proof synthesis to harness the\nbenefits of both approaches. HybridProver generates whole proof candidates for\nevaluation directly, then extracts proof sketches from those candidates. It\nthen uses a tactic-based generation model that integrates automated tools to\ncomplete the sketches via stepwise refinement. We implement HybridProver for\nthe Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle\ndatasets. Evaluation on the miniF2F dataset illustrates HybridProver's\neffectiveness. We achieve a 59.4% success rate on miniF2F, where the previous\nSOTA is 56.1%. Our ablation studies show that this SOTA result is attributable\nto combining whole-proof and tactic-based generation. Additionally, we show how\nthe dataset quality, training parameters, and sampling diversity affect the\nfinal result during automated theorem proving with LLMs. All of our code,\ndatasets, and LLMs are open source.",
    "categories": [
      "cs.FL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.FL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15740v1",
    "published_date": "2025-05-21 16:45:43 UTC",
    "updated_date": "2025-05-21 16:45:43 UTC"
  },
  {
    "arxiv_id": "2505.15738v1",
    "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses",
    "authors": [
      "Xiaoxue Yang",
      "Bozhidar Stevanoski",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye"
    ],
    "abstract": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15738v1",
    "published_date": "2025-05-21 16:43:17 UTC",
    "updated_date": "2025-05-21 16:43:17 UTC"
  },
  {
    "arxiv_id": "2505.15734v1",
    "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
    "authors": [
      "Gaurav Srivastava",
      "Zhenyu Bi",
      "Meng Lu",
      "Xuan Wang"
    ],
    "abstract": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15734v1",
    "published_date": "2025-05-21 16:40:12 UTC",
    "updated_date": "2025-05-21 16:40:12 UTC"
  },
  {
    "arxiv_id": "2505.15722v1",
    "title": "Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities",
    "authors": [
      "Xiaoyu Luo",
      "Yiyi Chen",
      "Johannes Bjerva",
      "Qiongxiu Li"
    ],
    "abstract": "We present the first comprehensive study of Memorization in Multilingual\nLarge Language Models (MLLMs), analyzing 95 languages using models across\ndiverse model scales, architectures, and memorization definitions. As MLLMs are\nincreasingly deployed, understanding their memorization behavior has become\ncritical. Yet prior work has focused primarily on monolingual models, leaving\nmultilingual memorization underexplored, despite the inherently long-tailed\nnature of training corpora. We find that the prevailing assumption, that\nmemorization is highly correlated with training data availability, fails to\nfully explain memorization patterns in MLLMs. We hypothesize that treating\nlanguages in isolation - ignoring their similarities - obscures the true\npatterns of memorization. To address this, we propose a novel graph-based\ncorrelation metric that incorporates language similarity to analyze\ncross-lingual memorization. Our analysis reveals that among similar languages,\nthose with fewer training tokens tend to exhibit higher memorization, a trend\nthat only emerges when cross-lingual relationships are explicitly modeled.\nThese findings underscore the importance of a language-aware perspective in\nevaluating and mitigating memorization vulnerabilities in MLLMs. This also\nconstitutes empirical evidence that language similarity both explains\nMemorization in MLLMs and underpins Cross-lingual Transferability, with broad\nimplications for multilingual NLP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 14 tables, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15722v1",
    "published_date": "2025-05-21 16:30:18 UTC",
    "updated_date": "2025-05-21 16:30:18 UTC"
  },
  {
    "arxiv_id": "2505.15703v1",
    "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning",
    "authors": [
      "Xiaodong Mei",
      "Sheng Wang",
      "Jie Cheng",
      "Yingbing Chen",
      "Dan Xu"
    ],
    "abstract": "Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "In submission",
    "pdf_url": "http://arxiv.org/pdf/2505.15703v1",
    "published_date": "2025-05-21 16:16:52 UTC",
    "updated_date": "2025-05-21 16:16:52 UTC"
  },
  {
    "arxiv_id": "2505.15694v1",
    "title": "A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO",
    "authors": [
      "Xingyu Zhou",
      "Yulian Wu",
      "Francesco Orabona"
    ],
    "abstract": "In this paper, we theoretically investigate the effects of noisy labels in\noffline alignment, with a focus on the interplay between privacy and robustness\nagainst adversarial corruption. Specifically, under linear modeling\nassumptions, we present a unified analysis covering both reinforcement learning\nfrom human feedback (RLHF) and direct preference optimization (DPO) under\ndifferent privacy-corruption scenarios, such as Local differential\nprivacy-then-Corruption (LTC), where human preference labels are privatized\nbefore being corrupted by an adversary, and Corruption-then-Local differential\nprivacy (CTL), where labels are corrupted before privacy protection. Our\nanalysis leverages a reduction framework that reduces the offline alignment\nproblem under linear modeling assumptions to parameter estimation in logistic\nregression. This framework allows us to establish an interesting separation\nresult between LTC and CTL, demonstrating that LTC presents a greater challenge\nthan CTL in offline alignment, even under linear models. As important\nby-products, our findings also advance the state-of-the-art theoretical results\nin offline alignment under privacy-only or corruption-only scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15694v1",
    "published_date": "2025-05-21 16:07:47 UTC",
    "updated_date": "2025-05-21 16:07:47 UTC"
  },
  {
    "arxiv_id": "2505.15693v1",
    "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives",
    "authors": [
      "Milad Kazemi",
      "Mateo Perez",
      "Fabio Somenzi",
      "Sadegh Soudjani",
      "Ashutosh Trivedi",
      "Alvaro Velasquez"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 6 figures and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15693v1",
    "published_date": "2025-05-21 16:06:51 UTC",
    "updated_date": "2025-05-21 16:06:51 UTC"
  },
  {
    "arxiv_id": "2505.15687v1",
    "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning",
    "authors": [
      "Zhe Xu",
      "Cheng Jin",
      "Yihui Wang",
      "Ziyi Liu",
      "Hao Chen"
    ],
    "abstract": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15687v1",
    "published_date": "2025-05-21 16:03:03 UTC",
    "updated_date": "2025-05-21 16:03:03 UTC"
  },
  {
    "arxiv_id": "2505.15683v1",
    "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability",
    "authors": [
      "Zishuai Zhang",
      "Hainan Zhang",
      "Jiaying Zheng",
      "Ziwei Wang",
      "Yongxin Tong",
      "Jin Dong",
      "Zhiming Zheng"
    ],
    "abstract": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15683v1",
    "published_date": "2025-05-21 15:58:08 UTC",
    "updated_date": "2025-05-21 15:58:08 UTC"
  },
  {
    "arxiv_id": "2505.15674v1",
    "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
    "authors": [
      "Miao Yu",
      "Liang Lin",
      "Guibin Zhang",
      "Xinfeng Li",
      "Junfeng Fang",
      "Ningyu Zhang",
      "Kun Wang",
      "Yang Wang"
    ],
    "abstract": "Large language models require iterative updates to address challenges such as\nknowledge conflicts and outdated information (e.g., incorrect, private, or\nillegal contents). Machine unlearning provides a systematic methodology for\ntargeted knowledge removal from trained models, enabling elimination of\nsensitive information influences. However, mainstream fine-tuning-based\nunlearning methods often fail to balance unlearning efficacy and model ability,\nfrequently resulting in catastrophic model collapse under extensive knowledge\nremoval. Meanwhile, in-context unlearning, which relies solely on contextual\nprompting without modifying the model's intrinsic mechanisms, suffers from\nlimited generalizability and struggles to achieve true unlearning. In this\nwork, we introduce UniErase, a novel unlearning paradigm that employs learnable\nparametric suffix (unlearning token) to steer language models toward targeted\nforgetting behaviors. UniErase operates through two key phases: (I) an\noptimization stage that binds desired unlearning outputs to the model's\nautoregressive probability distribution via token optimization, followed by\n(II) a lightweight model editing phase that activates the learned token to\nprobabilistically induce specified forgetting objective. Serving as a new\nresearch direction for token learning to induce unlearning target, UniErase\nachieves state-of-the-art (SOTA) performance across batch, sequential, and\nprecise unlearning under fictitious and real-world knowledge settings.\nRemarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%\nof the LLM parameters, outperforms previous forgetting SOTA baseline by around\n4.01 times for model ability with even better unlearning efficacy. Similarly,\nUniErase, maintaining more ability, also surpasses previous retaining SOTA by\n35.96% for unlearning efficacy, showing dual top-tier performances in current\nunlearing domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15674v1",
    "published_date": "2025-05-21 15:53:28 UTC",
    "updated_date": "2025-05-21 15:53:28 UTC"
  },
  {
    "arxiv_id": "2505.15671v1",
    "title": "Enhancing Monte Carlo Dropout Performance for Uncertainty Quantification",
    "authors": [
      "Hamzeh Asgharnezhad",
      "Afshar Shamsi",
      "Roohallah Alizadehsani",
      "Arash Mohammadi",
      "Hamid Alinejad-Rokny"
    ],
    "abstract": "Knowing the uncertainty associated with the output of a deep neural network\nis of paramount importance in making trustworthy decisions, particularly in\nhigh-stakes fields like medical diagnosis and autonomous systems. Monte Carlo\nDropout (MCD) is a widely used method for uncertainty quantification, as it can\nbe easily integrated into various deep architectures. However, conventional MCD\noften struggles with providing well-calibrated uncertainty estimates. To\naddress this, we introduce innovative frameworks that enhances MCD by\nintegrating different search solutions namely Grey Wolf Optimizer (GWO),\nBayesian Optimization (BO), and Particle Swarm Optimization (PSO) as well as an\nuncertainty-aware loss function, thereby improving the reliability of\nuncertainty quantification. We conduct comprehensive experiments using\ndifferent backbones, namely DenseNet121, ResNet50, and VGG16, on various\ndatasets, including Cats vs. Dogs, Myocarditis, Wisconsin, and a synthetic\ndataset (Circles). Our proposed algorithm outperforms the MCD baseline by 2-3%\non average in terms of both conventional accuracy and uncertainty accuracy\nwhile achieving significantly better calibration. These results highlight the\npotential of our approach to enhance the trustworthiness of deep learning\nmodels in safety-critical applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 5 tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15671v1",
    "published_date": "2025-05-21 15:50:03 UTC",
    "updated_date": "2025-05-21 15:50:03 UTC"
  },
  {
    "arxiv_id": "2505.15662v1",
    "title": "Neural Quantum Digital Twins for Optimizing Quantum Annealing",
    "authors": [
      "Jianlong Lu",
      "Hanqiu Peng",
      "Ying Chen"
    ],
    "abstract": "Quantum annealers have shown potential in addressing certain combinatorial\noptimization problems, though their performance is often limited by scalability\nand errors rates. In this work, we propose a Neural Quantum Digital Twin (NQDT)\nframework that reconstructs the energy landscape of quantum many-body systems\nrelevant to quantum annealing. The digital twin models both ground and excited\nstate dynamics, enabling detailed simulation of the adiabatic evolution\nprocess. We benchmark NQDT on systems with known analytical solutions and\ndemonstrate that it accurately captures key quantum phenomena, including\nquantum criticality and phase transitions. Leveraging this framework, one can\nidentify optimal annealing schedules that minimize excitation-related errors.\nThese findings highlight the utility of neural network-based digital twins as a\ndiagnostic and optimization tool for improving the performance of quantum\nannealers.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "quant-ph",
    "comment": "20 pages, 11 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15662v1",
    "published_date": "2025-05-21 15:38:55 UTC",
    "updated_date": "2025-05-21 15:38:55 UTC"
  },
  {
    "arxiv_id": "2505.15657v1",
    "title": "LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought",
    "authors": [
      "Cheng Yan",
      "Felix Mohr",
      "Tom Viering"
    ],
    "abstract": "Sample-wise learning curves plot performance versus training set size. They\nare useful for studying scaling laws and speeding up hyperparameter tuning and\nmodel selection. Learning curves are often assumed to be well-behaved: monotone\n(i.e. improving with more data) and convex. By constructing the Learning Curves\nDatabase 1.1 (LCDB 1.1), a large-scale database with high-resolution learning\ncurves, we show that learning curves are less often well-behaved than\npreviously thought. Using statistically rigorous methods, we observe\nsignificant ill-behavior in approximately 14% of the learning curves, almost\ntwice as much as in previous estimates. We also identify which learners are to\nblame and show that specific learners are more ill-behaved than others.\nAdditionally, we demonstrate that different feature scalings rarely resolve\nill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such\nas learning curve fitting and model selection, and find it poses significant\nchallenges, underscoring the relevance and potential of LCDB 1.1 as a\nchallenging benchmark for future research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15657v1",
    "published_date": "2025-05-21 15:32:42 UTC",
    "updated_date": "2025-05-21 15:32:42 UTC"
  },
  {
    "arxiv_id": "2505.15647v1",
    "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization",
    "authors": [
      "Youming Tao",
      "Zuyuan Zhang",
      "Dongxiao Yu",
      "Xiuzhen Cheng",
      "Falko Dressler",
      "Di Wang"
    ],
    "abstract": "We investigate the problem of finding second-order stationary points (SOSP)\nin differentially private (DP) stochastic non-convex optimization. Existing\nmethods suffer from two key limitations: (i) inaccurate convergence error rate\ndue to overlooking gradient variance in the saddle point escape analysis, and\n(ii) dependence on auxiliary private model selection procedures for identifying\nDP-SOSP, which can significantly impair utility, particularly in distributed\nsettings. To address these issues, we propose a generic perturbed stochastic\ngradient descent (PSGD) framework built upon Gaussian noise injection and\ngeneral gradient oracles. A core innovation of our framework is using model\ndrift distance to determine whether PSGD escapes saddle points, ensuring\nconvergence to approximate local minima without relying on second-order\ninformation or additional DP-SOSP identification. By leveraging the adaptive\nDP-SPIDER estimator as a specific gradient oracle, we develop a new DP\nalgorithm that rectifies the convergence error rates reported in prior work. We\nfurther extend this algorithm to distributed learning with arbitrarily\nheterogeneous data, providing the first formal guarantees for finding DP-SOSP\nin such settings. Our analysis also highlights the detrimental impacts of\nprivate selection procedures in distributed learning under high-dimensional\nmodels, underscoring the practical benefits of our design. Numerical\nexperiments on real-world datasets validate the efficacy of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15647v1",
    "published_date": "2025-05-21 15:25:23 UTC",
    "updated_date": "2025-05-21 15:25:23 UTC"
  },
  {
    "arxiv_id": "2505.15644v1",
    "title": "FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models",
    "authors": [
      "Zhen Sun",
      "Ziyi Zhang",
      "Zeren Luo",
      "Zeyang Sha",
      "Tianshuo Cong",
      "Zheng Li",
      "Shiwen Cui",
      "Weiqiang Wang",
      "Jiaheng Wei",
      "Xinlei He",
      "Qi Li",
      "Qian Wang"
    ],
    "abstract": "Fine-grained edited image detection of localized edits in images is crucial\nfor assessing content authenticity, especially given that modern diffusion\nmodels and image editing methods can produce highly realistic manipulations.\nHowever, this domain faces three challenges: (1) Binary classifiers yield only\na global real-or-fake label without providing localization; (2) Traditional\ncomputer vision methods often rely on costly pixel-level annotations; and (3)\nNo large-scale, high-quality dataset exists for modern image-editing detection\ntechniques. To address these gaps, we develop an automated data-generation\npipeline to create FragFake, the first dedicated benchmark dataset for edited\nimage detection, which includes high-quality images from diverse editing models\nand a wide variety of edited objects. Based on FragFake, we utilize Vision\nLanguage Models (VLMs) for the first time in the task of edited image\nclassification and edited region localization. Experimental results show that\nfine-tuned VLMs achieve higher average Object Precision across all datasets,\nsignificantly outperforming pretrained models. We further conduct ablation and\ntransferability analyses to evaluate the detectors across various\nconfigurations and editing scenarios. To the best of our knowledge, this work\nis the first to reformulate localized image edit detection as a vision-language\nunderstanding task, establishing a new paradigm for the field. We anticipate\nthat this work will establish a solid foundation to facilitate and inspire\nsubsequent research endeavors in the domain of multimodal content authenticity.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "14pages,15 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15644v1",
    "published_date": "2025-05-21 15:22:45 UTC",
    "updated_date": "2025-05-21 15:22:45 UTC"
  },
  {
    "arxiv_id": "2505.15633v1",
    "title": "Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions",
    "authors": [
      "David Thulke",
      "Jakob Kemmler",
      "Christian Dugast",
      "Hermann Ney"
    ],
    "abstract": "Large language models that use retrieval augmented generation have the\npotential to unlock valuable knowledge for researchers, policymakers, and the\npublic by making long and technical climate-related documents more accessible.\nWhile this approach can help alleviate factual hallucinations by relying on\nretrieved passages as additional context, its effectiveness depends on whether\nthe model's output remains faithful to these passages. To address this, we\nexplore the automatic assessment of faithfulness of different models in this\nsetting. We then focus on ClimateGPT, a large language model specialised in\nclimate science, to examine which factors in its instruction fine-tuning impact\nthe model's faithfulness. By excluding unfaithful subsets of the model's\ntraining data, we develop ClimateGPT Faithful+, which achieves an improvement\nin faithfulness from 30% to 57% in supported atomic claims according to our\nautomatic metric.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the ClimateNLP 2025 Workshop at ACL",
    "pdf_url": "http://arxiv.org/pdf/2505.15633v1",
    "published_date": "2025-05-21 15:17:38 UTC",
    "updated_date": "2025-05-21 15:17:38 UTC"
  },
  {
    "arxiv_id": "2505.15612v1",
    "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "authors": [
      "Wei Liu",
      "Ruochen Zhou",
      "Yiyun Deng",
      "Yuzhen Huang",
      "Junteng Liu",
      "Yuntian Deng",
      "Yizhe Zhang",
      "Junxian He"
    ],
    "abstract": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15612v1",
    "published_date": "2025-05-21 15:03:26 UTC",
    "updated_date": "2025-05-21 15:03:26 UTC"
  },
  {
    "arxiv_id": "2505.15607v1",
    "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
    "authors": [
      "David Dinucu-Jianu",
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "abstract": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL",
    "pdf_url": "http://arxiv.org/pdf/2505.15607v1",
    "published_date": "2025-05-21 15:00:07 UTC",
    "updated_date": "2025-05-21 15:00:07 UTC"
  },
  {
    "arxiv_id": "2505.15596v1",
    "title": "Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use",
    "authors": [
      "Xinyi Lu",
      "Aditya Mahesh",
      "Zejia Shen",
      "Mitchell Dudley",
      "Larissa Sano",
      "Xu Wang"
    ],
    "abstract": "This project examines the prospect of using AI-generated feedback as\nsuggestions to expedite and enhance human instructors' feedback provision. In\nparticular, we focus on understanding the teaching assistants' perspectives on\nthe quality of AI-generated feedback and how they may or may not utilize AI\nfeedback in their own workflows. We situate our work in a foundational college\nEconomics class, which has frequent short essay assignments. We developed an\nLLM-powered feedback engine that generates feedback on students' essays based\non grading rubrics used by the teaching assistants (TAs). To ensure that TAs\ncan meaningfully critique and engage with the AI feedback, we had them complete\ntheir regular grading jobs. For a randomly selected set of essays that they had\ngraded, we used our feedback engine to generate feedback and displayed the\nfeedback as in-text comments in a Word document. We then performed think-aloud\nstudies with 5 TAs over 20 1-hour sessions to have them evaluate the AI\nfeedback, contrast the AI feedback with their handwritten feedback, and share\nhow they envision using the AI feedback if they were offered as suggestions.\nThe study highlights the importance of providing detailed rubrics for AI to\ngenerate high-quality feedback for knowledge-intensive essays. TAs considered\nthat using AI feedback as suggestions during their grading could expedite\ngrading, enhance consistency, and improve overall feedback quality. We discuss\nthe importance of decomposing the feedback generation task into steps and\npresenting intermediate results, in order for TAs to use the AI feedback.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "To be published in AIED'2025: In Proceedings of the 26th\n  International Conference on Artificial Intelligence in Education. The system\n  prompt and example feedback can be found through\n  http://github.com/UM-Lifelong-Learning-Lab/AIED2025-Exploring-LLM-Generated-Feedback-for-Economics-Essay",
    "pdf_url": "http://arxiv.org/pdf/2505.15596v1",
    "published_date": "2025-05-21 14:50:30 UTC",
    "updated_date": "2025-05-21 14:50:30 UTC"
  },
  {
    "arxiv_id": "2505.15594v1",
    "title": "Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off",
    "authors": [
      "Yury Belousov",
      "Brian Pulfer",
      "Vitaliy Kinakh",
      "Slava Voloshynovskiy"
    ],
    "abstract": "While foundation models demonstrate impressive performance across various\ntasks, they remain vulnerable to adversarial inputs. Current research explores\nvarious approaches to enhance model robustness, with Diffusion Denoised\nSmoothing emerging as a particularly promising technique. This method employs a\npretrained diffusion model to preprocess inputs before model inference. Yet,\nits effectiveness remains largely unexplored beyond classification. We aim to\naddress this gap by analyzing three datasets with four distinct downstream\ntasks under three different adversarial attack algorithms. Our findings reveal\nthat while foundation models maintain resilience against conventional\ntransformations, applying high-noise diffusion denoising to clean images\nwithout any distortions significantly degrades performance by as high as 57%.\nLow-noise diffusion settings preserve performance but fail to provide adequate\nprotection across all attack types. Moreover, we introduce a novel attack\nstrategy specifically targeting the diffusion process itself, capable of\ncircumventing defenses in the low-noise regime. Our results suggest that the\ntrade-off between adversarial robustness and performance remains a challenge to\nbe addressed.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted at the 33rd European Signal Processing Conference\n  (EUSIPCO 2025)",
    "pdf_url": "http://arxiv.org/pdf/2505.15594v1",
    "published_date": "2025-05-21 14:49:24 UTC",
    "updated_date": "2025-05-21 14:49:24 UTC"
  },
  {
    "arxiv_id": "2505.15589v1",
    "title": "World Models as Reference Trajectories for Rapid Motor Adaptation",
    "authors": [
      "Carlos Stein Brito",
      "Daniel McNamee"
    ],
    "abstract": "Deploying learned control policies in real-world environments poses a\nfundamental challenge. When system dynamics change unexpectedly, performance\ndegrades until models are retrained on new data. We introduce Reflexive World\nModels (RWM), a dual control framework that uses world model predictions as\nimplicit reference trajectories for rapid adaptation. Our method separates the\ncontrol problem into long-term reward maximization through reinforcement\nlearning and robust motor execution through rapid latent control. This dual\narchitecture achieves significantly faster adaptation with low online\ncomputational cost compared to model-based RL baselines, while maintaining\nnear-optimal performance. The approach combines the benefits of flexible policy\nlearning through reinforcement learning with rapid error correction\ncapabilities, providing a principled approach to maintaining performance in\nhigh-dimensional continuous control tasks under varying dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15589v1",
    "published_date": "2025-05-21 14:46:41 UTC",
    "updated_date": "2025-05-21 14:46:41 UTC"
  },
  {
    "arxiv_id": "2505.15581v1",
    "title": "UWSAM: Segment Anything Model Guided Underwater Instance Segmentation and A Large-scale Benchmark Dataset",
    "authors": [
      "Hua Li",
      "Shijie Lian",
      "Zhiyuan Li",
      "Runmin Cong",
      "Sam Kwong"
    ],
    "abstract": "With recent breakthroughs in large-scale modeling, the Segment Anything Model\n(SAM) has demonstrated significant potential in a variety of visual\napplications. However, due to the lack of underwater domain expertise, SAM and\nits variants face performance limitations in end-to-end underwater instance\nsegmentation tasks, while their higher computational requirements further\nhinder their application in underwater scenarios. To address this challenge, we\npropose a large-scale underwater instance segmentation dataset, UIIS10K, which\nincludes 10,048 images with pixel-level annotations for 10 categories. Then, we\nintroduce UWSAM, an efficient model designed for automatic and accurate\nsegmentation of underwater instances. UWSAM efficiently distills knowledge from\nthe SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the\nMask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective\nvisual representation learning. Furthermore, we design an End-to-end Underwater\nPrompt Generator (EUPG) for UWSAM, which automatically generates underwater\nprompts instead of explicitly providing foreground points or boxes as prompts,\nthus enabling the network to locate underwater instances accurately for\nefficient segmentation. Comprehensive experimental results show that our model\nis effective, achieving significant performance improvements over\nstate-of-the-art methods on multiple underwater instance datasets. Datasets and\ncodes are available at https://github.com/LiamLian0727/UIIS10K.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15581v1",
    "published_date": "2025-05-21 14:36:01 UTC",
    "updated_date": "2025-05-21 14:36:01 UTC"
  },
  {
    "arxiv_id": "2505.15572v1",
    "title": "Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback",
    "authors": [
      "Wangyang Ying",
      "Haoyue Bai",
      "Nanxu Gong",
      "Xinyuan Wang",
      "Sixun Dong",
      "Haifeng Chen",
      "Yanjie Fu"
    ],
    "abstract": "The data-to-equation (Data2Eqn) task aims to discover interpretable\nmathematical equations that map observed values to labels, offering physical\ninsights and broad applicability across academic and industrial domains.\nGenetic programming and traditional deep learning-based approaches suffer from\nsearch inefficiency and poor generalization on small task-specific datasets.\nFoundation models showed promise in this area, but existing approaches suffer\nfrom: 1) They are pretrained on general-purpose data distributions, making them\nless effective for domain-specific tasks; and 2) their training objectives\nfocus on token-level alignment, overlooking mathematical semantics, which can\nlead to inaccurate equations. To address these issues, we aim to enhance the\ndomain adaptability of foundation models for Data2Eqn tasks. In this work, we\npropose a reinforcement learning-based finetuning framework that directly\noptimizes the generation policy of a pretrained model through reward signals\nderived from downstream numerical fitness. Our method allows the model to adapt\nto specific and complex data distributions and generate mathematically\nmeaningful equations. Extensive experiments demonstrate that our approach\nimproves both the accuracy and robustness of equation generation under complex\ndistributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15572v1",
    "published_date": "2025-05-21 14:25:41 UTC",
    "updated_date": "2025-05-21 14:25:41 UTC"
  },
  {
    "arxiv_id": "2505.15559v1",
    "title": "Moonbeam: A MIDI Foundation Model Using Both Absolute and Relative Music Attributes",
    "authors": [
      "Zixun Guo",
      "Simon Dixon"
    ],
    "abstract": "Moonbeam is a transformer-based foundation model for symbolic music,\npretrained on a large and diverse collection of MIDI data totaling 81.6K hours\nof music and 18 billion tokens. Moonbeam incorporates music-domain inductive\nbiases by capturing both absolute and relative musical attributes through the\nintroduction of a novel domain-knowledge-inspired tokenization method and\nMultidimensional Relative Attention (MRA), which captures relative music\ninformation without additional trainable parameters. Leveraging the pretrained\nMoonbeam, we propose 2 finetuning architectures with full anticipatory\ncapabilities, targeting 2 categories of downstream tasks: symbolic music\nunderstanding and conditional music generation (including music infilling). Our\nmodel outperforms other large-scale pretrained music models in most cases in\nterms of accuracy and F1 score across 3 downstream music classification tasks\non 4 datasets. Moreover, our finetuned conditional music generation model\noutperforms a strong transformer baseline with a REMI-like tokenizer. We\nopen-source the code, pretrained model, and generated samples on Github.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15559v1",
    "published_date": "2025-05-21 14:17:25 UTC",
    "updated_date": "2025-05-21 14:17:25 UTC"
  },
  {
    "arxiv_id": "2505.15558v1",
    "title": "Robo-DM: Data Management For Large Robot Datasets",
    "authors": [
      "Kaiyuan Chen",
      "Letian Fu",
      "David Huang",
      "Yanxiang Zhang",
      "Lawrence Yunliang Chen",
      "Huang Huang",
      "Kush Hari",
      "Ashwin Balakrishna",
      "Ted Xiao",
      "Pannag R Sanketi",
      "John Kubiatowicz",
      "Ken Goldberg"
    ],
    "abstract": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Best paper finalist of IEEE ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15558v1",
    "published_date": "2025-05-21 14:17:06 UTC",
    "updated_date": "2025-05-21 14:17:06 UTC"
  },
  {
    "arxiv_id": "2505.15554v1",
    "title": "DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion",
    "authors": [
      "Wendi Zhou",
      "Ameer Saadat-Yazdi",
      "Nadin Kökciyan"
    ],
    "abstract": "Critical questions are essential resources to provoke critical thinking when\nencountering an argumentative text. We present our system for the Critical\nQuestions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach\nleverages large language models (LLMs) with chain-of-thought prompting to\ngenerate critical questions guided by Walton's argumentation schemes. For each\ninput intervention, we conversationally prompt LLMs to instantiate the\ncorresponding argument scheme template to first obtain structured arguments,\nand then generate relevant critical questions. Following this, we rank all the\navailable critical questions by prompting LLMs to select the top 3 most helpful\nquestions based on the original intervention text. This combination of\nstructured argumentation theory and step-by-step reasoning enables the\ngeneration of contextually relevant and diverse critical questions. Our\npipeline achieves competitive performance in the final test set, showing its\npotential to foster critical thinking given argumentative text and detect\nmissing or uninformed claims. Code available at\n\\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ArgMining 2025 CQs-Gen shared task",
    "pdf_url": "http://arxiv.org/pdf/2505.15554v1",
    "published_date": "2025-05-21 14:15:49 UTC",
    "updated_date": "2025-05-21 14:15:49 UTC"
  },
  {
    "arxiv_id": "2505.15553v2",
    "title": "Social Bias in Popular Question-Answering Benchmarks",
    "authors": [
      "Angelie Kraft",
      "Judith Simon",
      "Sonja Schimmler"
    ],
    "abstract": "Question-answering (QA) and reading comprehension (RC) benchmarks are\nessential for assessing the capabilities of large language models (LLMs) in\nretrieving and reproducing knowledge. However, we demonstrate that popular QA\nand RC benchmarks are biased and do not cover questions about different\ndemographics or regions in a representative way, potentially due to a lack of\ndiversity of those involved in their creation. We perform a qualitative content\nanalysis of 30 benchmark papers and a quantitative analysis of 20 respective\nbenchmark datasets to learn (1) who is involved in the benchmark creation, (2)\nhow social bias is addressed or prevented, and (3) whether the demographics of\nthe creators and annotators correspond to particular biases in the content.\nMost analyzed benchmark papers provided insufficient information regarding the\nstakeholders involved in benchmark creation, particularly the annotators.\nNotably, just one of the benchmark papers explicitly reported measures taken to\naddress social representation issues. Moreover, the data analysis revealed\ngender, religion, and geographic biases across a wide range of encyclopedic,\ncommonsense, and scholarly benchmarks. More transparent and bias-aware QA and\nRC benchmark creation practices are needed to facilitate better scrutiny and\nincentivize the development of fairer LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15553v2",
    "published_date": "2025-05-21 14:14:47 UTC",
    "updated_date": "2025-05-22 09:39:49 UTC"
  },
  {
    "arxiv_id": "2505.15547v1",
    "title": "Oversmoothing, \"Oversquashing\", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning",
    "authors": [
      "Adrian Arnaiz-Rodriguez",
      "Federico Errica"
    ],
    "abstract": "After a renaissance phase in which researchers revisited the message-passing\nparadigm through the lens of deep learning, the graph machine learning\ncommunity shifted its attention towards a deeper and practical understanding of\nmessage-passing's benefits and limitations. In this position paper, we notice\nhow the fast pace of progress around the topics of oversmoothing and\noversquashing, the homophily-heterophily dichotomy, and long-range tasks, came\nwith the consolidation of commonly accepted beliefs and assumptions that are\nnot always true nor easy to distinguish from each other. We argue that this has\nled to ambiguities around the investigated problems, preventing researchers\nfrom focusing on and addressing precise research questions while causing a good\namount of misunderstandings. Our contribution wants to make such common beliefs\nexplicit and encourage critical thinking around these topics, supported by\nsimple but noteworthy counterexamples. The hope is to clarify the distinction\nbetween the different issues and promote separate but intertwined research\ndirections to address them.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15547v1",
    "published_date": "2025-05-21 14:11:59 UTC",
    "updated_date": "2025-05-21 14:11:59 UTC"
  },
  {
    "arxiv_id": "2505.15524v1",
    "title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs",
    "authors": [
      "Lang Gao",
      "Kaiyang Wan",
      "Wei Liu",
      "Chenxi Wang",
      "Zirui Song",
      "Zixiang Xu",
      "Yanbo Wang",
      "Veselin Stoyanov",
      "Xiuying Chen"
    ],
    "abstract": "Bias in Large Language Models (LLMs) significantly undermines their\nreliability and fairness. We focus on a common form of bias: when two reference\nconcepts in the model's concept space, such as sentiment polarities (e.g.,\n\"positive\" and \"negative\"), are asymmetrically correlated with a third, target\nconcept, such as a reviewing aspect, the model exhibits unintended bias. For\ninstance, the understanding of \"food\" should not skew toward any particular\nsentiment. Existing bias evaluation methods assess behavioral differences of\nLLMs by constructing labeled data for different social groups and measuring\nmodel responses across them, a process that requires substantial human effort\nand captures only a limited set of social concepts. To overcome these\nlimitations, we propose BiasLens, a test-set-free bias analysis framework based\non the structure of the model's vector space. BiasLens combines Concept\nActivation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract\ninterpretable concept representations, and quantifies bias by measuring the\nvariation in representational similarity between the target concept and each of\nthe reference concepts. Even without labeled data, BiasLens shows strong\nagreement with traditional bias evaluation metrics (Spearman correlation r >\n0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect\nusing existing methods. For example, in simulated clinical scenarios, a\npatient's insurance status can cause the LLM to produce biased diagnostic\nassessments. Overall, BiasLens offers a scalable, interpretable, and efficient\nparadigm for bias discovery, paving the way for improving fairness and\ntransparency in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15524v1",
    "published_date": "2025-05-21 13:50:23 UTC",
    "updated_date": "2025-05-21 13:50:23 UTC"
  },
  {
    "arxiv_id": "2505.15517v1",
    "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets",
    "authors": [
      "Kaiyuan Chen",
      "Shuangyu Xie",
      "Zehan Ma",
      "Ken Goldberg"
    ],
    "abstract": "Vision-Language Models (VLMs) acquire real-world knowledge and general\nreasoning ability through Internet-scale image-text corpora. They can augment\nrobotic systems with scene understanding and task planning, and assist\nvisuomotor policies that are trained on robot trajectory data. We explore the\nreverse paradigm - using rich, real, multi-modal robot trajectory data to\nenhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual\nQuestion Answering (VQA) dataset generation framework for VLMs. Given a human\ntele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual\nand non-descriptive sensory modalities, such as end-effector pose, gripper\naperture, and force sensing. Based on these modalities, it segments the robot\ntrajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses\nscene and interaction understanding to identify 3D properties of the robot,\ntask goal, and the target object. The properties are used to generate\nrepresentative VQA queries - images with textural multiple-choice questions -\nbased on spatial, goal-conditioned, and interaction reasoning question\ntemplates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710\nquestions covering 463 distinct scenes and 3,396 robotic manipulation tasks\nfrom 176k real robot trajectories. Results suggest that Robo2VLM-1 can\nbenchmark and improve VLM capabilities in spatial and interaction reasoning.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15517v1",
    "published_date": "2025-05-21 13:42:52 UTC",
    "updated_date": "2025-05-21 13:42:52 UTC"
  },
  {
    "arxiv_id": "2505.15516v1",
    "title": "Explainable embeddings with Distance Explainer",
    "authors": [
      "Christiaan Meijer",
      "E. G. Patrick Bos"
    ],
    "abstract": "While eXplainable AI (XAI) has advanced significantly, few methods address\ninterpretability in embedded vector spaces where dimensions represent complex\nabstractions. We introduce Distance Explainer, a novel method for generating\nlocal, post-hoc explanations of embedded spaces in machine learning models. Our\napproach adapts saliency-based techniques from RISE to explain the distance\nbetween two embedded data points by assigning attribution values through\nselective masking and distance-ranked mask filtering. We evaluate Distance\nExplainer on cross-modal embeddings (image-image and image-caption pairs) using\nestablished XAI metrics including Faithfulness, Sensitivity/Robustness, and\nRandomization. Experiments with ImageNet and CLIP models demonstrate that our\nmethod effectively identifies features contributing to similarity or\ndissimilarity between embedded data points while maintaining high robustness\nand consistency. We also explore how parameter tuning, particularly mask\nquantity and selection strategy, affects explanation quality. This work\naddresses a critical gap in XAI research and enhances transparency and\ntrustworthiness in deep learning applications utilizing embedded spaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "68T99",
      "I.2.m"
    ],
    "primary_category": "cs.LG",
    "comment": "33 pages, 19 figures. Submitted to JMLR. Method implementation:\n  https://research-software-directory.org/software/distance-explainer",
    "pdf_url": "http://arxiv.org/pdf/2505.15516v1",
    "published_date": "2025-05-21 13:42:28 UTC",
    "updated_date": "2025-05-21 13:42:28 UTC"
  },
  {
    "arxiv_id": "2505.15514v1",
    "title": "AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization",
    "authors": [
      "Soham Sane"
    ],
    "abstract": "Proximal Policy Optimization (PPO) is a widely used reinforcement learning\nalgorithm that heavily relies on accurate advantage estimates for stable and\nefficient training. However, raw advantage signals can exhibit significant\nvariance, noise, and scale-related issues, impeding optimal learning\nperformance. To address this challenge, we introduce Advantage Modulation PPO\n(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage\nestimates using a dynamic, non-linear scaling mechanism. This adaptive\nmodulation employs an alpha controller that dynamically adjusts the scaling\nfactor based on evolving statistical properties of the advantage signals, such\nas their norm, variance, and a predefined target saturation level. By\nincorporating a tanh-based gating function driven by these adaptively scaled\nadvantages, AM-PPO reshapes the advantage signals to stabilize gradient updates\nand improve the conditioning of the policy gradient landscape. Crucially, this\nmodulation also influences value function training by providing consistent and\nadaptively conditioned learning targets. Empirical evaluations across standard\ncontinuous control benchmarks demonstrate that AM-PPO achieves superior reward\ntrajectories, exhibits sustained learning progression, and significantly\nreduces the clipping required by adaptive optimizers. These findings underscore\nthe potential of advantage modulation as a broadly applicable technique for\nenhancing reinforcement learning optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 4 Tables, 9 Figures, 11 equations",
    "pdf_url": "http://arxiv.org/pdf/2505.15514v1",
    "published_date": "2025-05-21 13:38:45 UTC",
    "updated_date": "2025-05-21 13:38:45 UTC"
  },
  {
    "arxiv_id": "2505.15507v1",
    "title": "Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning",
    "authors": [
      "Mahesh Godavarti"
    ],
    "abstract": "We introduce a new algebraic structure for multi-dimensional compositional\nembeddings, built on directional non-commutative monoidal operators. The core\ncontribution of this work is this novel framework, which exhibits appealing\ntheoretical properties (associativity along each dimension and an interchange\nlaw ensuring global consistency) while remaining compatible with modern machine\nlearning architectures. Our construction defines a distinct composition\noperator circ_i for each axis i, ensuring associative combination along each\naxis without imposing global commutativity. Importantly, all axis-specific\noperators commute with one another, enforcing a global interchange law that\nenables consistent crossaxis compositions. This is, to our knowledge, the first\napproach that provides a common foundation that generalizes classical\nsequence-modeling paradigms (e.g., structured state-space models (SSMs) and\ntransformer self-attention) to a unified multi-dimensional framework. For\nexample, specific one-dimensional instances of our framework can recover the\nfamiliar affine transformation algebra, vanilla self-attention, and the\nSSM-style recurrence. The higher-dimensional generalizations naturally support\nrecursive, structure-aware operations in embedding spaces. We outline several\npotential applications unlocked by this structure-including structured\npositional encodings in Transformers, directional image embeddings, and\nsymbolic modeling of sequences or grids-indicating that it could inform future\ndeep learning model designs. We formally establish the algebraic properties of\nour framework and discuss efficient implementations. Finally, as our focus is\ntheoretical, we include no experiments here and defer empirical validation to\nfuture work, which we plan to undertake.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "20-XX, 08A02",
      "F.4.1; I.2"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages submitted to NeurIPS 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15507v1",
    "published_date": "2025-05-21 13:27:14 UTC",
    "updated_date": "2025-05-21 13:27:14 UTC"
  },
  {
    "arxiv_id": "2505.15504v1",
    "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification",
    "authors": [
      "Conghao Xiong",
      "Zhengrui Guo",
      "Zhe Xu",
      "Yifei Zhang",
      "Raymond Kai-Yu Tong",
      "Si Yong Yeo",
      "Hao Chen",
      "Joseph J. Y. Sung",
      "Irwin King"
    ],
    "abstract": "Deep learning has advanced computational pathology but expert annotations\nremain scarce. Few-shot learning mitigates annotation burdens yet suffers from\noverfitting and discriminative feature mischaracterization. In addition, the\ncurrent few-shot multiple instance learning (MIL) approaches leverage\npretrained vision-language models to alleviate these issues, but at the cost of\ncomplex preprocessing and high computational cost. We propose a\nSqueeze-and-Recalibrate (SR) block, a drop-in replacement for linear layers in\nMIL models to address these challenges. The SR block comprises two core\ncomponents: a pair of low-rank trainable matrices (squeeze pathway, SP) that\nreduces parameter count and imposes a bottleneck to prevent spurious feature\nlearning, and a frozen random recalibration matrix that preserves geometric\nstructure, diversifies feature directions, and redefines the optimization\nobjective for the SP. We provide theoretical guarantees that the SR block can\napproximate any linear mapping to arbitrary precision, thereby ensuring that\nthe performance of a standard MIL model serves as a lower bound for its\nSR-enhanced counterpart. Extensive experiments demonstrate that our SR-MIL\nmodels consistently outperform prior methods while requiring significantly\nfewer parameters and no architectural changes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15504v1",
    "published_date": "2025-05-21 13:24:47 UTC",
    "updated_date": "2025-05-21 13:24:47 UTC"
  },
  {
    "arxiv_id": "2505.15501v1",
    "title": "Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs",
    "authors": [
      "Federico Ranaldi",
      "Andrea Zugarini",
      "Leonardo Ranaldi",
      "Fabio Massimo Zanzotto"
    ],
    "abstract": "We introduce the concept of protoknowledge to formalize and measure how\nsequences of tokens encoding Knowledge Graphs are internalized during\npretraining and utilized at inference time by Large Language Models (LLMs).\nIndeed, LLMs have demonstrated the ability to memorize vast amounts of token\nsequences during pretraining, and a central open question is how they leverage\nthis memorization as reusable knowledge through generalization. We then\ncategorize protoknowledge into lexical, hierarchical, and topological forms,\nvarying on the type of knowledge that needs to be activated. We measure\nprotoknowledge through Knowledge Activation Tasks (KATs), analyzing its general\nproperties such as semantic bias. We then investigate the impact of\nprotoknowledge on Text-to-SPARQL performance by varying prompting strategies\ndepending on input conditions. To this end, we adopt a novel analysis framework\nthat assesses whether model predictions align with the successful activation of\nthe relevant protoknowledge for each query. This methodology provides a\npractical tool to explore Semantic-Level Data Contamination and serves as an\neffective strategy for Closed-Pretraining models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15501v1",
    "published_date": "2025-05-21 13:22:34 UTC",
    "updated_date": "2025-05-21 13:22:34 UTC"
  },
  {
    "arxiv_id": "2505.15475v1",
    "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models",
    "authors": [
      "Zhanyue Qin",
      "Yue Ding",
      "Deyuan Liu",
      "Qingbin Liu",
      "Junxian Cai",
      "Xi Chen",
      "Zhiying Tu",
      "Dianhui Chu",
      "Cuiyun Gao",
      "Dianbo Sui"
    ],
    "abstract": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15475v1",
    "published_date": "2025-05-21 12:49:37 UTC",
    "updated_date": "2025-05-21 12:49:37 UTC"
  },
  {
    "arxiv_id": "2505.15469v1",
    "title": "A Qualitative Investigation into LLM-Generated Multilingual Code Comments and Automatic Evaluation Metrics",
    "authors": [
      "Jonathan Katzy",
      "Yongcheng Huang",
      "Gopal-Raj Panchu",
      "Maksym Ziemlewski",
      "Paris Loizides",
      "Sander Vermeulen",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "abstract": "Large Language Models are essential coding assistants, yet their training is\npredominantly English-centric. In this study, we evaluate the performance of\ncode language models in non-English contexts, identifying challenges in their\nadoption and integration into multilingual workflows. We conduct an open-coding\nstudy to analyze errors in code comments generated by five state-of-the-art\ncode models, CodeGemma, CodeLlama, CodeQwen1.5, GraniteCode, and StarCoder2\nacross five natural languages: Chinese, Dutch, English, Greek, and Polish. Our\nstudy yields a dataset of 12,500 labeled generations, which we publicly\nrelease. We then assess the reliability of standard metrics in capturing\ncomment \\textit{correctness} across languages and evaluate their\ntrustworthiness as judgment criteria. Through our open-coding investigation, we\nidentified a taxonomy of 26 distinct error categories in model-generated code\ncomments. They highlight variations in language cohesion, informativeness, and\nsyntax adherence across different natural languages. Our analysis shows that,\nwhile these models frequently produce partially correct comments, modern neural\nmetrics fail to reliably differentiate meaningful completions from random\nnoise. Notably, the significant score overlap between expert-rated correct and\nincorrect comments calls into question the effectiveness of these metrics in\nassessing generated comments.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted PROMISE '25",
    "pdf_url": "http://arxiv.org/pdf/2505.15469v1",
    "published_date": "2025-05-21 12:45:49 UTC",
    "updated_date": "2025-05-21 12:45:49 UTC"
  },
  {
    "arxiv_id": "2505.15467v1",
    "title": "Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning",
    "authors": [
      "Yukun Zhao",
      "Lingyong Yan",
      "Zhenyang Li",
      "Shuaiqiang Wang",
      "Zhumin Chen",
      "Zhaochun Ren",
      "Dawei Yin"
    ],
    "abstract": "Large language models have achieved remarkable success in various tasks.\nHowever, it is challenging for them to learn new tasks incrementally due to\ncatastrophic forgetting. Existing approaches rely on experience replay,\noptimization constraints, or task differentiation, which encounter strict\nlimitations in real-world scenarios. To address these issues, we propose Joint\nFlashback Adaptation. We first introduce flashbacks -- a limited number of\nprompts from old tasks -- when adapting to new tasks and constrain the\ndeviations of the model outputs compared to the original one. We then\ninterpolate latent tasks between flashbacks and new tasks to enable jointly\nlearning relevant latent tasks, new tasks, and flashbacks, alleviating data\nsparsity in flashbacks and facilitating knowledge sharing for smooth\nadaptation. Our method requires only a limited number of flashbacks without\naccess to the replay data and is task-agnostic. We conduct extensive\nexperiments on state-of-the-art large language models across 1000+\ninstruction-following tasks, arithmetic reasoning tasks, and general reasoning\ntasks. The results demonstrate the superior performance of our method in\nimproving generalization on new tasks and reducing forgetting in old tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15467v1",
    "published_date": "2025-05-21 12:45:28 UTC",
    "updated_date": "2025-05-21 12:45:28 UTC"
  },
  {
    "arxiv_id": "2505.15447v1",
    "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning",
    "authors": [
      "Ziqiang Xu",
      "Qi Dai",
      "Tian Xie",
      "Yifan Yang",
      "Kai Qiu",
      "DongDong Chen",
      "Zuxuan Wu",
      "Chong Luo"
    ],
    "abstract": "Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15447v1",
    "published_date": "2025-05-21 12:29:40 UTC",
    "updated_date": "2025-05-21 12:29:40 UTC"
  },
  {
    "arxiv_id": "2505.15444v1",
    "title": "Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization",
    "authors": [
      "Yutao Zhu",
      "Jiajie Jin",
      "Hongjin Qian",
      "Zheng Liu",
      "Zhicheng Dou",
      "Ji-Rong Wen"
    ],
    "abstract": "Existing studies have optimized retrieval-augmented generation (RAG) across\nvarious sub-tasks, such as query understanding and retrieval refinement, but\nintegrating these optimizations into a unified framework remains challenging.\nTo tackle this problem, this work proposes RoleRAG, a unified RAG framework\nthat achieves efficient multi-task processing through role-specific token\noptimization. RoleRAG comprises six modules, each handling a specific sub-task\nwithin the RAG process. Additionally, we introduce a query graph to represent\nthe decomposition of the query, which can be dynamically resolved according to\nthe decomposing state. All modules are driven by the same underlying LLM,\ndistinguished by task-specific role tokens that are individually optimized.\nThis design allows RoleRAG to dynamically activate different modules within a\nsingle LLM instance, thereby streamlining deployment and reducing resource\nconsumption. Experimental results on five open-domain question-answering\ndatasets demonstrate the effectiveness, generalizability, and flexibility of\nour framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15444v1",
    "published_date": "2025-05-21 12:25:12 UTC",
    "updated_date": "2025-05-21 12:25:12 UTC"
  },
  {
    "arxiv_id": "2505.15441v2",
    "title": "Stronger ViTs With Octic Equivariance",
    "authors": [
      "David Nordström",
      "Johan Edstedt",
      "Fredrik Kahl",
      "Georg Bökman"
    ],
    "abstract": "Recent efforts at scaling computer vision models have established Vision\nTransformers (ViTs) as the leading architecture. ViTs incorporate weight\nsharing over image patches as an important inductive bias. In this work, we\nshow that ViTs benefit from incorporating equivariance under the octic group,\ni.e., reflections and 90-degree rotations, as a further inductive bias. We\ndevelop new architectures, octic ViTs, that use octic-equivariant layers and\nput them to the test on both supervised and self-supervised learning. Through\nextensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show\nthat octic ViTs yield more computationally efficient networks while also\nimproving performance. In particular, we achieve approximately 40% reduction in\nFLOPs for ViT-H while simultaneously improving both classification and\nsegmentation results.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15441v2",
    "published_date": "2025-05-21 12:22:53 UTC",
    "updated_date": "2025-05-22 15:33:46 UTC"
  },
  {
    "arxiv_id": "2505.15433v1",
    "title": "Set-LLM: A Permutation-Invariant LLM",
    "authors": [
      "Beni Egressy",
      "Jan Stühmer"
    ],
    "abstract": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15433v1",
    "published_date": "2025-05-21 12:14:26 UTC",
    "updated_date": "2025-05-21 12:14:26 UTC"
  },
  {
    "arxiv_id": "2505.15429v1",
    "title": "Uncertainty Quantification in SVM prediction",
    "authors": [
      "Pritam Anand"
    ],
    "abstract": "This paper explores Uncertainty Quantification (UQ) in SVM predictions,\nparticularly for regression and forecasting tasks. Unlike the Neural Network,\nthe SVM solutions are typically more stable, sparse, optimal and interpretable.\nHowever, there are only few literature which addresses the UQ in SVM\nprediction. At first, we provide a comprehensive summary of existing Prediction\nInterval (PI) estimation and probabilistic forecasting methods developed in the\nSVM framework and evaluate them against the key properties expected from an\nideal PI model. We find that none of the existing SVM PI models achieves a\nsparse solution. To introduce sparsity in SVM model, we propose the Sparse\nSupport Vector Quantile Regression (SSVQR) model, which constructs PIs and\nprobabilistic forecasts by solving a pair of linear programs. Further, we\ndevelop a feature selection algorithm for PI estimation using SSVQR that\neffectively eliminates a significant number of features while improving PI\nquality in case of high-dimensional dataset. Finally we extend the SVM models\nin Conformal Regression setting for obtaining more stable prediction set with\nfinite test set guarantees. Extensive experiments on artificial, real-world\nbenchmark datasets compare the different characteristics of both existing and\nproposed SVM-based PI estimation methods and also highlight the advantages of\nthe feature selection in PI estimation. Furthermore, we compare both, the\nexisting and proposed SVM-based PI estimation models, with modern deep learning\nmodels for probabilistic forecasting tasks on benchmark datasets. Furthermore,\nSVM models show comparable or superior performance to modern complex deep\nlearning models for probabilistic forecasting task in our experiments.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15429v1",
    "published_date": "2025-05-21 12:11:07 UTC",
    "updated_date": "2025-05-21 12:11:07 UTC"
  },
  {
    "arxiv_id": "2505.15427v1",
    "title": "Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions",
    "authors": [
      "Zhiwen Li",
      "Die Chen",
      "Mingyuan Fan",
      "Cen Chen",
      "Yaliang Li",
      "Yanhao Wang",
      "Wenmeng Zhou"
    ],
    "abstract": "The remarkable ability of diffusion models to generate high-fidelity images\nhas led to their widespread adoption. However, concerns have also arisen\nregarding their potential to produce Not Safe for Work (NSFW) content and\nexhibit social biases, hindering their practical use in real-world\napplications. In response to this challenge, prior work has focused on\nemploying security filters to identify and exclude toxic text, or\nalternatively, fine-tuning pre-trained diffusion models to erase sensitive\nconcepts. Unfortunately, existing methods struggle to achieve satisfactory\nperformance in the sense that they can have a significant impact on the normal\nmodel output while still failing to prevent the generation of harmful content\nin some cases. In this paper, we propose a novel self-discovery approach to\nidentifying a semantic direction vector in the embedding space to restrict text\nembedding within a safe region. Our method circumvents the need for correcting\nindividual words within the input text and steers the entire text prompt\ntowards a safe region in the embedding space, thereby enhancing model\nrobustness against all possibly unsafe prompts. In addition, we employ Low-Rank\nAdaptation (LoRA) for semantic direction vector initialization to reduce the\nimpact on the model performance for other semantics. Furthermore, our method\ncan also be integrated with existing methods to improve their social\nresponsibility. Extensive experiments on benchmark datasets demonstrate that\nour method can effectively reduce NSFW content and mitigate social bias\ngenerated by diffusion models compared to several state-of-the-art baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15427v1",
    "published_date": "2025-05-21 12:10:26 UTC",
    "updated_date": "2025-05-21 12:10:26 UTC"
  },
  {
    "arxiv_id": "2505.15420v1",
    "title": "Silent Leaks: Implicit Knowledge Extraction Attack on RAG Systems through Benign Queries",
    "authors": [
      "Yuhao Wang",
      "Wenjie Qu",
      "Yanze Jiang",
      "Zichen Liu",
      "Yue Liu",
      "Shengfang Zhai",
      "Yinpeng Dong",
      "Jiaheng Zhang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by incorporating external knowledge bases, but they are vulnerable to\nprivacy risks from data extraction attacks. Existing extraction methods\ntypically rely on malicious inputs such as prompt injection or jailbreaking,\nmaking them easily detectable via input- or output-level detection. In this\npaper, we introduce Implicit Knowledge Extraction Attack (IKEA), which conducts\nknowledge extraction on RAG systems through benign queries. IKEA first\nleverages anchor concepts to generate queries with the natural appearance, and\nthen designs two mechanisms to lead to anchor concept thoroughly 'explore' the\nRAG's privacy knowledge: (1) Experience Reflection Sampling, which samples\nanchor concepts based on past query-response patterns to ensure the queries'\nrelevance to RAG documents; (2) Trust Region Directed Mutation, which\niteratively mutates anchor concepts under similarity constraints to further\nexploit the embedding space. Extensive experiments demonstrate IKEA's\neffectiveness under various defenses, surpassing baselines by over 80% in\nextraction efficiency and 90% in attack success rate. Moreover, the substitute\nRAG system built from IKEA's extractions consistently outperforms those based\non baseline methods across multiple evaluation tasks, underscoring the\nsignificant privacy risk in RAG systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15420v1",
    "published_date": "2025-05-21 12:04:42 UTC",
    "updated_date": "2025-05-21 12:04:42 UTC"
  },
  {
    "arxiv_id": "2505.15868v1",
    "title": "An Inclusive Foundation Model for Generalizable Cytogenetics in Precision Oncology",
    "authors": [
      "Changchun Yang",
      "Weiqian Dai",
      "Yilan Zhang",
      "Siyuan Chen",
      "Jingdong Hu",
      "Junkai Su",
      "Yuxuan Chen",
      "Ao Xu",
      "Na Li",
      "Xin Gao",
      "Yongguo Yu"
    ],
    "abstract": "Chromosome analysis is vital for diagnosing genetic disorders and guiding\ncancer therapy decisions through the identification of somatic clonal\naberrations. However, developing an AI model are hindered by the overwhelming\ncomplexity and diversity of chromosomal abnormalities, requiring extensive\nannotation efforts, while automated methods remain task-specific and lack\ngeneralizability due to the scarcity of comprehensive datasets spanning diverse\nresource conditions. Here, we introduce CHROMA, a foundation model for\ncytogenomics, designed to overcome these challenges by learning generalizable\nrepresentations of chromosomal abnormalities. Pre-trained on over 84,000\nspecimens (~4 million chromosomal images) via self-supervised learning, CHROMA\noutperforms other methods across all types of abnormalities, even when trained\non fewer labelled data and more imbalanced datasets. By facilitating\ncomprehensive mapping of instability and clonal leisons across various\naberration types, CHROMA offers a scalable and generalizable solution for\nreliable and automated clinical analysis, reducing the annotation workload for\nexperts and advancing precision oncology through the early detection of rare\ngenomic abnormalities, enabling broad clinical AI applications and making\nadvanced genomic analysis more accessible.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "q-bio.QM",
    "comment": "These authors contributed equally to this work: Changchun Yang,\n  Weiqian Dai, Yilan Zhang",
    "pdf_url": "http://arxiv.org/pdf/2505.15868v1",
    "published_date": "2025-05-21 12:03:37 UTC",
    "updated_date": "2025-05-21 12:03:37 UTC"
  },
  {
    "arxiv_id": "2505.15418v1",
    "title": "Guided Policy Optimization under Partial Observability",
    "authors": [
      "Yueheng Li",
      "Guangming Xie",
      "Zongqing Lu"
    ],
    "abstract": "Reinforcement Learning (RL) in partially observable environments poses\nsignificant challenges due to the complexity of learning under uncertainty.\nWhile additional information, such as that available in simulations, can\nenhance training, effectively leveraging it remains an open problem. To address\nthis, we introduce Guided Policy Optimization (GPO), a framework that co-trains\na guider and a learner. The guider takes advantage of privileged information\nwhile ensuring alignment with the learner's policy that is primarily trained\nvia imitation learning. We theoretically demonstrate that this learning scheme\nachieves optimality comparable to direct RL, thereby overcoming key limitations\ninherent in existing approaches. Empirical evaluations show strong performance\nof GPO across various tasks, including continuous control with partial\nobservability and noise, and memory-based challenges, significantly\noutperforming existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "24 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15418v1",
    "published_date": "2025-05-21 12:01:08 UTC",
    "updated_date": "2025-05-21 12:01:08 UTC"
  },
  {
    "arxiv_id": "2505.15410v1",
    "title": "ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs",
    "authors": [
      "Bahar Radmehr",
      "Ekaterina Shved",
      "Fatma Betül Güreş",
      "Adish Singla",
      "Tanja Käser"
    ],
    "abstract": "Clickstream data from digital learning environments offer valuable insights\ninto students' learning behaviors, but are challenging to interpret due to\ntheir high dimensionality and granularity. Prior approaches have relied mainly\non handcrafted features, expert labeling, clustering, or supervised models,\ntherefore often lacking generalizability and scalability. In this work, we\nintroduce ClickSight, an in-context Large Language Model (LLM)-based pipeline\nthat interprets student clickstreams to reveal their learning strategies.\nClickSight takes raw clickstreams and a list of learning strategies as input\nand generates textual interpretations of students' behaviors during\ninteraction. We evaluate four different prompting strategies and investigate\nthe impact of self-refinement on interpretation quality. Our evaluation spans\ntwo open-ended learning environments and uses a rubric-based domain-expert\nevaluation. Results show that while LLMs can reasonably interpret learning\nstrategies from clickstreams, interpretation quality varies by prompting\nstrategy, and self-refinement offers limited improvement. ClickSight\ndemonstrates the potential of LLMs to generate theory-driven insights from\neducational interaction data.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted in Latebreaking results track in AIED 2025(26th\n  International Conference on Artificial Intelligence in Education JULY 22-26,\n  2025 PALERMO, ITALY)",
    "pdf_url": "http://arxiv.org/pdf/2505.15410v1",
    "published_date": "2025-05-21 11:52:57 UTC",
    "updated_date": "2025-05-21 11:52:57 UTC"
  },
  {
    "arxiv_id": "2505.15406v1",
    "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
    "authors": [
      "Zirui Song",
      "Qian Jiang",
      "Mingxuan Cui",
      "Mingzhe Li",
      "Lang Gao",
      "Zeyu Zhang",
      "Zixiang Xu",
      "Yanbo Wang",
      "Chenxi Wang",
      "Guangxian Ouyang",
      "Zhenhao Chen",
      "Xiuying Chen"
    ],
    "abstract": "The rise of Large Audio Language Models (LAMs) brings both potential and\nrisks, as their audio outputs may contain harmful or unethical content.\nHowever, current research lacks a systematic, quantitative evaluation of LAM\nsafety especially against jailbreak attacks, which are challenging due to the\ntemporal and semantic nature of speech. To bridge this gap, we introduce\nAJailBench, the first benchmark specifically designed to evaluate jailbreak\nvulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of\n1,495 adversarial audio prompts spanning 10 policy-violating categories,\nconverted from textual jailbreak attacks using realistic text to speech\nsynthesis. Using this dataset, we evaluate several state-of-the-art LAMs and\nreveal that none exhibit consistent robustness across attacks. To further\nstrengthen jailbreak testing and simulate more realistic attack conditions, we\npropose a method to generate dynamic adversarial variants. Our Audio\nPerturbation Toolkit (APT) applies targeted distortions across time, frequency,\nand amplitude domains. To preserve the original jailbreak intent, we enforce a\nsemantic consistency constraint and employ Bayesian optimization to efficiently\nsearch for perturbations that are both subtle and highly effective. This\nresults in AJailBench-APT, an extended dataset of optimized adversarial audio\nsamples. Our findings demonstrate that even small, semantically preserved\nperturbations can significantly reduce the safety performance of leading LAMs,\nunderscoring the need for more robust and semantically aware defense\nmechanisms.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "We release AJailBench, including both static and optimized\n  adversarial data, to facilitate future research:\n  https://github.com/mbzuai-nlp/AudioJailbreak",
    "pdf_url": "http://arxiv.org/pdf/2505.15406v1",
    "published_date": "2025-05-21 11:47:47 UTC",
    "updated_date": "2025-05-21 11:47:47 UTC"
  },
  {
    "arxiv_id": "2505.15400v1",
    "title": "When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning",
    "authors": [
      "Xiaoyun Zhang",
      "Jingqing Ruan",
      "Xing Ma",
      "Yawen Zhu",
      "Haodong Zhao",
      "Hao Li",
      "Jiansong Chen",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "abstract": "Large reasoning models (LRMs) achieve remarkable performance via long\nreasoning chains, but often incur excessive computational overhead due to\nredundant reasoning, especially on simple tasks. In this work, we\nsystematically quantify the upper bounds of LRMs under both Long-Thinking and\nNo-Thinking modes, and uncover the phenomenon of \"Internal Self-Recovery\nMechanism\" where models implicitly supplement reasoning during answer\ngeneration. Building on this insight, we propose Adaptive Self-Recovery\nReasoning (ASRR), a framework that suppresses unnecessary reasoning and enables\nimplicit recovery. By introducing accuracy-aware length reward regulation, ASRR\nadaptively allocates reasoning effort according to problem difficulty,\nachieving high efficiency with negligible performance sacrifice. Experiments\nacross multiple benchmarks and models show that, compared with GRPO, ASRR\nreduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal\naccuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates\non safety benchmarks (up to +21.7%). Our results highlight the potential of\nASRR for enabling efficient, adaptive, and safer reasoning in LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15400v1",
    "published_date": "2025-05-21 11:41:39 UTC",
    "updated_date": "2025-05-21 11:41:39 UTC"
  },
  {
    "arxiv_id": "2505.15386v1",
    "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection",
    "authors": [
      "Yiming Huang",
      "Junyan Zhang",
      "Zihao Wang",
      "Biquan Bie",
      "Xuming Hu",
      "Yi R.",
      "Fung",
      "Xinlei He"
    ],
    "abstract": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15386v1",
    "published_date": "2025-05-21 11:23:05 UTC",
    "updated_date": "2025-05-21 11:23:05 UTC"
  },
  {
    "arxiv_id": "2505.15380v1",
    "title": "Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding",
    "authors": [
      "Zijian Lin",
      "Yang Zhang",
      "Yougen Yuan",
      "Yuming Yan",
      "Jinjiang Liu",
      "Zhiyong Wu",
      "Pengfei Hu",
      "Qun Yu"
    ],
    "abstract": "Modern autoregressive speech synthesis models leveraging language models have\ndemonstrated remarkable performance. However, the sequential nature of next\ntoken prediction in these models leads to significant latency, hindering their\ndeployment in scenarios where inference speed is critical. In this work, we\npropose Speech Speculative Decoding (SSD), a novel framework for autoregressive\nspeech synthesis acceleration. Specifically, our method employs a lightweight\ndraft model to generate candidate token sequences, which are subsequently\nverified in parallel by the target model using the proposed SSD framework.\nExperimental results demonstrate that SSD achieves a significant speedup of\n1.4x compared with conventional autoregressive decoding, while maintaining high\nfidelity and naturalness. Subjective evaluations further validate the\neffectiveness of SSD in preserving the perceptual quality of the target model\nwhile accelerating inference.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "5 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15380v1",
    "published_date": "2025-05-21 11:17:04 UTC",
    "updated_date": "2025-05-21 11:17:04 UTC"
  },
  {
    "arxiv_id": "2505.15367v1",
    "title": "Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition",
    "authors": [
      "Dasol Choi",
      "Seunghyun Lee",
      "Youngsook Song"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nunderstanding visual content, but their reliability in safety-critical contexts\nremains under-explored. We introduce VERI (Visual Emergency Recognition\nDataset), a carefully designed diagnostic benchmark of 200 images (100\ncontrastive pairs). Each emergency scene is matched with a visually similar but\nsafe counterpart through multi-stage human verification and iterative\nrefinement. Using a two-stage protocol - risk identification and emergency\nresponse - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,\naccidents, and natural disasters. Our analysis reveals a systematic\noverreaction problem: models excel at identifying real emergencies (70-100\npercent success rate) but suffer from an alarming rate of false alarms,\nmisidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios\nfailed by all models regardless of scale. This \"better-safe-than-sorry\" bias\nmanifests primarily through contextual overinterpretation (88-93 percent of\nerrors), challenging VLMs' reliability for safety applications. These findings\nhighlight persistent limitations that are not resolved by increasing model\nscale, motivating targeted approaches for improving contextual safety\nassessment in visually misleading scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.15367v1",
    "published_date": "2025-05-21 10:57:40 UTC",
    "updated_date": "2025-05-21 10:57:40 UTC"
  },
  {
    "arxiv_id": "2505.15358v2",
    "title": "Objective Bicycle Occlusion Level Classification using a Deformable Parts-Based Model",
    "authors": [
      "Angelique Mangubat",
      "Shane Gilroy"
    ],
    "abstract": "Road safety is a critical challenge, particularly for cyclists, who are among\nthe most vulnerable road users. This study aims to enhance road safety by\nproposing a novel benchmark for bicycle occlusion level classification using\nadvanced computer vision techniques. Utilizing a parts-based detection model,\nimages are annotated and processed through a custom image detection pipeline. A\nnovel method of bicycle occlusion level is proposed to objectively quantify the\nvisibility and occlusion level of bicycle semantic parts. The findings indicate\nthat the model robustly quantifies the visibility and occlusion level of\nbicycles, a significant improvement over the subjective methods used by the\ncurrent state of the art. Widespread use of the proposed methodology will\nfacilitate accurate performance reporting of cyclist detection algorithms for\noccluded cyclists, informing the development of more robust vulnerable road\nuser detection methods for autonomous vehicles.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15358v2",
    "published_date": "2025-05-21 10:42:41 UTC",
    "updated_date": "2025-05-22 08:25:18 UTC"
  },
  {
    "arxiv_id": "2505.15345v1",
    "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari",
    "authors": [
      "Jacob E. Kooi",
      "Zhao Yang",
      "Vincent François-Lavet"
    ],
    "abstract": "Neural network architectures have a large impact in machine learning. In\nreinforcement learning, network architectures have remained notably simple, as\nchanges often lead to small gains in performance. This work introduces a novel\nencoder architecture for pixel-based model-free reinforcement learning. The\nHadamax (\\textbf{Hada}mard \\textbf{max}-pooling) encoder achieves\nstate-of-the-art performance by max-pooling Hadamard products between\nGELU-activated parallel hidden layers. Based on the recent PQN algorithm, the\nHadamax encoder achieves state-of-the-art model-free performance in the\nAtari-57 benchmark. Specifically, without applying any algorithmic\nhyperparameter modifications, Hadamax-PQN achieves an 80\\% performance gain\nover vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,\nthe full code is available on\n\\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15345v1",
    "published_date": "2025-05-21 10:19:49 UTC",
    "updated_date": "2025-05-21 10:19:49 UTC"
  },
  {
    "arxiv_id": "2505.15344v1",
    "title": "Alpay Algebra: A Universal Structural Foundation",
    "authors": [
      "Faruk Alpay"
    ],
    "abstract": "Alpay Algebra is introduced as a universal, category-theoretic framework that\nunifies classical algebraic structures with modern needs in symbolic recursion\nand explainable AI. Starting from a minimal list of axioms, we model each\nalgebra as an object in a small cartesian closed category $\\mathcal{A}$ and\ndefine a transfinite evolution functor $\\phi\\colon\\mathcal{A}\\to\\mathcal{A}$.\nWe prove that the fixed point $\\phi^{\\infty}$ exists for every initial object\nand satisfies an internal universal property that recovers familiar constructs\n-- limits, colimits, adjunctions -- while extending them to ordinal-indexed\nfolds. A sequence of theorems establishes (i) soundness and conservativity over\nstandard universal algebra, (ii) convergence of $\\phi$-iterates under regular\ncardinals, and (iii) an explanatory correspondence between $\\phi^{\\infty}$ and\nminimal sufficient statistics in information-theoretic AI models. We conclude\nby outlining computational applications: type-safe functional languages,\ncategorical model checking, and signal-level reasoning engines that leverage\nAlpay Algebra's structural invariants. All proofs are self-contained; no\nexternal set-theoretic axioms beyond ZFC are required. This exposition\npositions Alpay Algebra as a bridge between foundational mathematics and\nhigh-impact AI systems, and provides a reference for further work in category\ntheory, transfinite fixed-point analysis, and symbolic computation.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "math.CT",
      "18B99, 68T27",
      "F.4.1; I.2.3"
    ],
    "primary_category": "cs.LO",
    "comment": "37 pages, 0 figures. Self-contained categorical framework built\n  directly on Mac Lane and Bourbaki; minimal references are intentional to\n  foreground the new construction",
    "pdf_url": "http://arxiv.org/pdf/2505.15344v1",
    "published_date": "2025-05-21 10:18:49 UTC",
    "updated_date": "2025-05-21 10:18:49 UTC"
  },
  {
    "arxiv_id": "2505.15337v1",
    "title": "Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors",
    "authors": [
      "Hao Fang",
      "Jiawei Kong",
      "Tianqu Zhuang",
      "Yixiang Qiu",
      "Kuofeng Gao",
      "Bin Chen",
      "Shu-Tao Xia",
      "Yaowei Wang",
      "Min Zhang"
    ],
    "abstract": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15337v1",
    "published_date": "2025-05-21 10:08:39 UTC",
    "updated_date": "2025-05-21 10:08:39 UTC"
  },
  {
    "arxiv_id": "2505.15333v1",
    "title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation",
    "authors": [
      "Yuhao Zhang",
      "Xiangnan Ma",
      "Kaiqi Kou",
      "Peizhuo Liu",
      "Weiqiao Shan",
      "Benyou Wang",
      "Tong Xiao",
      "Yuxin Huang",
      "Zhengtao Yu",
      "Jingbo Zhu"
    ],
    "abstract": "The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2505.15333v1",
    "published_date": "2025-05-21 10:05:25 UTC",
    "updated_date": "2025-05-21 10:05:25 UTC"
  },
  {
    "arxiv_id": "2505.15311v1",
    "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning",
    "authors": [
      "Yurun Yuan",
      "Fan Chen",
      "Zeyu Jia",
      "Alexander Rakhlin",
      "Tengyang Xie"
    ],
    "abstract": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15311v1",
    "published_date": "2025-05-21 09:41:53 UTC",
    "updated_date": "2025-05-21 09:41:53 UTC"
  },
  {
    "arxiv_id": "2505.15308v1",
    "title": "BadSR: Stealthy Label Backdoor Attacks on Image Super-Resolution",
    "authors": [
      "Ji Guo",
      "Xiaolei Wen",
      "Wenbo Jiang",
      "Cheng Huang",
      "Jinjin Li",
      "Hongwei Li"
    ],
    "abstract": "With the widespread application of super-resolution (SR) in various fields,\nresearchers have begun to investigate its security. Previous studies have\ndemonstrated that SR models can also be subjected to backdoor attacks through\ndata poisoning, affecting downstream tasks. A backdoor SR model generates an\nattacker-predefined target image when given a triggered image while producing a\nnormal high-resolution (HR) output for clean images. However, prior backdoor\nattacks on SR models have primarily focused on the stealthiness of poisoned\nlow-resolution (LR) images while ignoring the stealthiness of poisoned HR\nimages, making it easy for users to detect anomalous data. To address this\nproblem, we propose BadSR, which improves the stealthiness of poisoned HR\nimages. The key idea of BadSR is to approximate the clean HR image and the\npre-defined target image in the feature space while ensuring that modifications\nto the clean HR image remain within a constrained range. The poisoned HR images\ngenerated by BadSR can be integrated with existing triggers. To further improve\nthe effectiveness of BadSR, we design an adversarially optimized trigger and a\nbackdoor gradient-driven poisoned sample selection method based on a genetic\nalgorithm. The experimental results show that BadSR achieves a high attack\nsuccess rate in various models and data sets, significantly affecting\ndownstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15308v1",
    "published_date": "2025-05-21 09:36:35 UTC",
    "updated_date": "2025-05-21 09:36:35 UTC"
  },
  {
    "arxiv_id": "2505.15306v1",
    "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One",
    "authors": [
      "Yiwen Song",
      "Qianyue Hao",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15306v1",
    "published_date": "2025-05-21 09:35:43 UTC",
    "updated_date": "2025-05-21 09:35:43 UTC"
  },
  {
    "arxiv_id": "2505.15303v1",
    "title": "Laplace Sample Information: Data Informativeness Through a Bayesian Lens",
    "authors": [
      "Johannes Kaiser",
      "Kristian Schwethelm",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ],
    "abstract": "Accurately estimating the informativeness of individual samples in a dataset\nis an important objective in deep learning, as it can guide sample selection,\nwhich can improve model efficiency and accuracy by removing redundant or\npotentially harmful samples. We propose Laplace Sample Information (LSI)\nmeasure of sample informativeness grounded in information theory widely\napplicable across model architectures and learning settings. LSI leverages a\nBayesian approximation to the weight posterior and the KL divergence to measure\nthe change in the parameter distribution induced by a sample of interest from\nthe dataset. We experimentally show that LSI is effective in ordering the data\nwith respect to typicality, detecting mislabeled samples, measuring class-wise\ninformativeness, and assessing dataset difficulty. We demonstrate these\ncapabilities of LSI on image and text data in supervised and unsupervised\nsettings. Moreover, we show that LSI can be computed efficiently through probes\nand transfers well to the training of large models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15303v1",
    "published_date": "2025-05-21 09:34:27 UTC",
    "updated_date": "2025-05-21 09:34:27 UTC"
  },
  {
    "arxiv_id": "2505.15293v1",
    "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
    "authors": [
      "Qianyue Hao",
      "Yiwen Song",
      "Qingmin Liao",
      "Jian Yuan",
      "Yong Li"
    ],
    "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing\napproaches include greedy, Gaussian process, etc. However, these approaches\nutilize preset stochastic processes and are indiscriminately applied in all\nkinds of RL tasks without considering task-specific features that influence\npolicy exploration. Moreover, during RL training, the evolution of such\nstochastic processes is rigid, which typically only incorporates a decay in the\nvariance, failing to adjust flexibly according to the agent's real-time\nlearning status. Inspired by the analyzing and reasoning capability of large\nlanguage models (LLMs), we design LLM-Explorer to adaptively generate\ntask-specific exploration strategies with LLMs, enhancing the policy\nexploration in RL. In our design, we sample the learning trajectory of the\nagent during the RL training in a given task and prompt the LLM to analyze the\nagent's current policy learning status and then generate a probability\ndistribution for future policy exploration. Updating the probability\ndistribution periodically, we derive a stochastic process specialized for the\nparticular task and dynamically adjusted to adapt to the learning process. Our\ndesign is a plug-in module compatible with various widely applied RL\nalgorithms, including the DQN series, DDPG, TD3, and any possible variants\ndeveloped based on them. Through extensive experiments on the Atari and MuJoCo\nbenchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy\nexploration, achieving an average performance improvement up to 37.27%. Our\ncode is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for\nreproducibility.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15293v1",
    "published_date": "2025-05-21 09:24:23 UTC",
    "updated_date": "2025-05-21 09:24:23 UTC"
  },
  {
    "arxiv_id": "2505.15285v1",
    "title": "Reconsider the Template Mesh in Deep Learning-based Mesh Reconstruction",
    "authors": [
      "Fengting Zhang",
      "Boxu Liang",
      "Qinghao Liu",
      "Min Liu",
      "Xiang Chen",
      "Yaonan Wang"
    ],
    "abstract": "Mesh reconstruction is a cornerstone process across various applications,\nincluding in-silico trials, digital twins, surgical planning, and navigation.\nRecent advancements in deep learning have notably enhanced mesh reconstruction\nspeeds. Yet, traditional methods predominantly rely on deforming a standardised\ntemplate mesh for individual subjects, which overlooks the unique anatomical\nvariations between them, and may compromise the fidelity of the\nreconstructions. In this paper, we propose an adaptive-template-based mesh\nreconstruction network (ATMRN), which generates adaptive templates from the\ngiven images for the subsequent deformation, moving beyond the constraints of a\nsingular, fixed template. Our approach, validated on cortical magnetic\nresonance (MR) images from the OASIS dataset, sets a new benchmark in\nvoxel-to-cortex mesh reconstruction, achieving an average symmetric surface\ndistance of 0.267mm across four cortical structures. Our proposed method is\ngeneric and can be easily transferred to other image modalities and anatomical\nstructures.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15285v1",
    "published_date": "2025-05-21 09:10:31 UTC",
    "updated_date": "2025-05-21 09:10:31 UTC"
  },
  {
    "arxiv_id": "2505.15276v1",
    "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning",
    "authors": [
      "Rongzhi Zhu",
      "Yi Liu",
      "Zequn Sun",
      "Yiwei Wang",
      "Wei Hu"
    ],
    "abstract": "Large reasoning models (LRMs) have significantly advanced performance on\ncomplex tasks, yet their tendency to overthink introduces inefficiencies. This\nstudy investigates the internal mechanisms of reinforcement learning\n(RL)-trained LRMs when prompted to save thinking, revealing three distinct\nthinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking\n(IT). Through comprehensive analysis of confidence in thinking termination,\nattention from thinking to generation, and attentional focus on input sections,\nwe uncover key factors influencing the reasoning behaviors. We further find\nthat NT reduces output length at the cost of accuracy, while ET and IT maintain\naccuracy with reduced response length. Our findings expose fundamental\ninconsistencies in RL-optimized LRMs, necessitating adaptive improvements for\nreliable efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15276v1",
    "published_date": "2025-05-21 08:55:35 UTC",
    "updated_date": "2025-05-21 08:55:35 UTC"
  },
  {
    "arxiv_id": "2505.15275v1",
    "title": "Learning-based Autonomous Oversteer Control and Collision Avoidance",
    "authors": [
      "Seokjun Lee",
      "Seung-Hyun Kong"
    ],
    "abstract": "Oversteer, wherein a vehicle's rear tires lose traction and induce\nunintentional excessive yaw, poses critical safety challenges. Failing to\ncontrol oversteer often leads to severe traffic accidents. Although recent\nautonomous driving efforts have attempted to handle oversteer through\nstabilizing maneuvers, the majority rely on expert-defined trajectories or\nassume obstacle-free environments, limiting real-world applicability. This\npaper introduces a novel end-to-end (E2E) autonomous driving approach that\ntackles oversteer control and collision avoidance simultaneously. Existing E2E\ntechniques, including Imitation Learning (IL), Reinforcement Learning (RL), and\nHybrid Learning (HL), generally require near-optimal demonstrations or\nextensive experience. Yet even skilled human drivers struggle to provide\nperfect demonstrations under oversteer, and high transition variance hinders\naccumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic\n(QC-SAC), a new HL algorithm that effectively learns from suboptimal\ndemonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we\nintroduce a benchmark inspired by real-world driver training: a vehicle\nencounters sudden oversteer on a slippery surface and must avoid randomly\nplaced obstacles ahead. Experimental results show QC-SAC attains near-optimal\ndriving policies, significantly surpassing state-of-the-art IL, RL, and HL\nbaselines. Our method demonstrates the world's first safe autonomous oversteer\ncontrol with obstacle avoidance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15275v1",
    "published_date": "2025-05-21 08:53:38 UTC",
    "updated_date": "2025-05-21 08:53:38 UTC"
  },
  {
    "arxiv_id": "2505.15274v1",
    "title": "Identification of Probabilities of Causation: A Complete Characterization",
    "authors": [
      "Xin Shu",
      "Shuai Wang",
      "Ang Li"
    ],
    "abstract": "Probabilities of causation are fundamental to modern decision-making. Pearl\nfirst introduced three binary probabilities of causation, and Tian and Pearl\nlater derived tight bounds for them using Balke's linear programming. The\ntheoretical characterization of probabilities of causation with multi-valued\ntreatments and outcomes has remained unresolved for decades, limiting the scope\nof causality-based decision-making. In this paper, we resolve this foundational\ngap by proposing a complete set of representative probabilities of causation\nand proving that they are sufficient to characterize all possible probabilities\nof causation within the framework of Structural Causal Models (SCMs). We then\nformally derive tight bounds for these representative quantities using formal\nmathematical proofs. Finally, we demonstrate the practical relevance of our\nresults through illustrative toy examples.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15274v1",
    "published_date": "2025-05-21 08:50:12 UTC",
    "updated_date": "2025-05-21 08:50:12 UTC"
  },
  {
    "arxiv_id": "2505.15270v1",
    "title": "Scaling Diffusion Transformers Efficiently via $μ$P",
    "authors": [
      "Chenyu Zheng",
      "Xinyu Zhang",
      "Rongzhen Wang",
      "Wei Huang",
      "Zhi Tian",
      "Weilin Huang",
      "Jun Zhu",
      "Chongxuan Li"
    ],
    "abstract": "Diffusion Transformers have emerged as the foundation for vision generative\nmodels, but their scalability is limited by the high cost of hyperparameter\n(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P)\nwas proposed for vanilla Transformers, which enables stable HP transfer from\nsmall to large language models, and dramatically reduces tuning costs. However,\nit remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion\nTransformers, which differ architecturally and objectively. In this work, we\ngeneralize standard $\\mu$P to diffusion Transformers and validate its\neffectiveness through large-scale experiments. First, we rigorously prove that\n$\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,\nPixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer,\nenabling the direct application of existing $\\mu$P methodologies. Leveraging\nthis result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP\ntransferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate\nachieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we\nvalidate the effectiveness of $\\mu$P on text-to-image generation by scaling\nPixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,\nmodels under $\\mu$P outperform their respective baselines while requiring small\ntuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of\nconsumption by human experts for MMDiT-18B. These results establish $\\mu$P as a\nprincipled and efficient framework for scaling diffusion Transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 10 figures, 15 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15270v1",
    "published_date": "2025-05-21 08:49:03 UTC",
    "updated_date": "2025-05-21 08:49:03 UTC"
  },
  {
    "arxiv_id": "2505.15265v1",
    "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs",
    "authors": [
      "Zihao Pan",
      "Yu Tong",
      "Weibin Wu",
      "Jingyi Wang",
      "Lifeng Chen",
      "Zhe Zhao",
      "Jiajia Wei",
      "Yitong Qiao",
      "Zibin Zheng"
    ],
    "abstract": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15265v1",
    "published_date": "2025-05-21 08:45:43 UTC",
    "updated_date": "2025-05-21 08:45:43 UTC"
  },
  {
    "arxiv_id": "2505.15256v1",
    "title": "Zero-Shot Gaze-based Volumetric Medical Image Segmentation",
    "authors": [
      "Tatyana Shmykova",
      "Leila Khaertdinova",
      "Ilya Pershin"
    ],
    "abstract": "Accurate segmentation of anatomical structures in volumetric medical images\nis crucial for clinical applications, including disease monitoring and cancer\ntreatment planning. Contemporary interactive segmentation models, such as\nSegment Anything Model 2 (SAM-2) and its medical variant (MedSAM-2), rely on\nmanually provided prompts like bounding boxes and mouse clicks. In this study,\nwe introduce eye gaze as a novel informational modality for interactive\nsegmentation, marking the application of eye-tracking for 3D medical image\nsegmentation. We evaluate the performance of using gaze-based prompts with\nSAM-2 and MedSAM-2 using both synthetic and real gaze data. Compared to\nbounding boxes, gaze-based prompts offer a time-efficient interaction approach\nwith slightly lower segmentation quality. Our findings highlight the potential\nof using gaze as a complementary input modality for interactive 3D medical\nimage segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to MMFM-BIOMED Workshop @ CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15256v1",
    "published_date": "2025-05-21 08:34:13 UTC",
    "updated_date": "2025-05-21 08:34:13 UTC"
  },
  {
    "arxiv_id": "2505.15250v1",
    "title": "Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification",
    "authors": [
      "Suping Xu",
      "Lin Shang",
      "Keyu Liu",
      "Hengrong Ju",
      "Xibei Yang",
      "Witold Pedrycz"
    ],
    "abstract": "Fuzzy rough feature selection (FRFS) is an effective means of addressing the\ncurse of dimensionality in high-dimensional data. By removing redundant and\nirrelevant features, FRFS helps mitigate classifier overfitting, enhance\ngeneralization performance, and lessen computational overhead. However, most\nexisting FRFS algorithms primarily focus on reducing uncertainty in pattern\nclassification, neglecting that lower uncertainty does not necessarily result\nin improved classification performance, despite it commonly being regarded as a\nkey indicator of feature selection effectiveness in the FRFS literature. To\nbridge uncertainty characterization and pattern classification, we propose a\nMargin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers\nboth the compactness and separation of label classes. MAFRFS effectively\nreduces uncertainty in pattern classification tasks, while guiding the feature\nselection towards more separable and discriminative label class structures.\nExtensive experiments on 15 public datasets demonstrate that MAFRFS is highly\nscalable and more effective than FRFS. The algorithms developed using MAFRFS\noutperform six state-of-the-art feature selection algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15250v1",
    "published_date": "2025-05-21 08:26:20 UTC",
    "updated_date": "2025-05-21 08:26:20 UTC"
  },
  {
    "arxiv_id": "2505.15245v1",
    "title": "Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework",
    "authors": [
      "Zihao Jiang",
      "Ben Liu",
      "Miao Peng",
      "Wenjie Xu",
      "Yao Xiao",
      "Zhenyan Shan",
      "Min Peng"
    ],
    "abstract": "While large language models (LLMs) show great potential in temporal\nreasoning, most existing work focuses heavily on enhancing performance, often\nneglecting the explainable reasoning processes underlying the results. To\naddress this gap, we introduce a comprehensive benchmark covering a wide range\nof temporal granularities, designed to systematically evaluate LLMs'\ncapabilities in explainable temporal reasoning. Furthermore, our findings\nreveal that LLMs struggle to deliver convincing explanations when relying\nsolely on textual information. To address challenge, we propose GETER, a novel\nstructure-aware generative framework that integrates Graph structures with text\nfor Explainable TEmporal Reasoning. Specifically, we first leverage temporal\nknowledge graphs to develop a temporal encoder that captures structural\ninformation for the query. Subsequently, we introduce a structure-text prefix\nadapter to map graph structure features into the text embedding space. Finally,\nLLMs generate explanation text by seamlessly integrating the soft graph token\nwith instruction-tuning prompt tokens. Experimental results indicate that GETER\nachieves state-of-the-art performance while also demonstrating its\neffectiveness as well as strong generalization capabilities. Our dataset and\ncode are available at https://github.com/carryTatum/GETER.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In Findings of the Association for Computational Linguistics: ACL\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15245v1",
    "published_date": "2025-05-21 08:20:35 UTC",
    "updated_date": "2025-05-21 08:20:35 UTC"
  },
  {
    "arxiv_id": "2505.15242v2",
    "title": "Adaptive Plan-Execute Framework for Smart Contract Security Auditing",
    "authors": [
      "Zhiyuan Wei",
      "Jing Sun",
      "Zijian Zhang",
      "Zhe Hou",
      "Zixiao Zhao"
    ],
    "abstract": "Large Language Models (LLMs) have shown great promise in code analysis and\nauditing; however, they still struggle with hallucinations and limited\ncontext-aware reasoning. We introduce SmartAuditFlow, a novel Plan-Execute\nframework that enhances smart contract security analysis through dynamic audit\nplanning and structured execution. Unlike conventional LLM-based auditing\napproaches that follow fixed workflows and predefined steps, SmartAuditFlow\ndynamically generates and refines audit plans based on the unique\ncharacteristics of each smart contract. It continuously adjusts its auditing\nstrategy in response to intermediate LLM outputs and newly detected\nvulnerabilities, ensuring a more adaptive and precise security assessment. The\nframework then executes these plans step by step, applying a structured\nreasoning process to enhance vulnerability detection accuracy while minimizing\nhallucinations and false positives. To further improve audit precision,\nSmartAuditFlow integrates iterative prompt optimization and external knowledge\nsources, such as static analysis tools and Retrieval-Augmented Generation\n(RAG). This ensures audit decisions are contextually informed and backed by\nreal-world security knowledge, producing comprehensive security reports.\nExtensive evaluations across multiple benchmarks demonstrate that\nSmartAuditFlow outperforms existing methods, achieving 100 percent accuracy on\ncommon and critical vulnerabilities, 41.2 percent accuracy for comprehensive\ncoverage of known smart contract weaknesses in real-world projects, and\nsuccessfully identifying all 13 tested CVEs. These results highlight\nSmartAuditFlow's scalability, cost-effectiveness, and superior adaptability\nover traditional static analysis tools and contemporary LLM-based approaches,\nestablishing it as a robust solution for automated smart contract auditing.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "30 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15242v2",
    "published_date": "2025-05-21 08:18:41 UTC",
    "updated_date": "2025-05-22 06:10:20 UTC"
  },
  {
    "arxiv_id": "2505.15240v1",
    "title": "Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge",
    "authors": [
      "Yassir Fathullah",
      "Mark J. F. Gales"
    ],
    "abstract": "This paper explores generalised probabilistic modelling and uncertainty\nestimation in comparative LLM-as-a-judge frameworks. We show that existing\nProduct-of-Experts methods are specific cases of a broader framework, enabling\ndiverse modelling options. Furthermore, we propose improved uncertainty\nestimates for individual comparisons, enabling more efficient selection and\nachieving strong performance with fewer evaluations. We also introduce a method\nfor estimating overall ranking uncertainty. Finally, we demonstrate that\ncombining absolute and comparative scoring improves performance. Experiments\nshow that the specific expert model has a limited impact on final rankings but\nour proposed uncertainty estimates, especially the probability of reordering,\nsignificantly improve the efficiency of systems reducing the number of needed\ncomparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used\nto identify low-performing predictions, where the nature of the probabilistic\nmodel has a notable impact on the quality of the overall uncertainty.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in UAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2505.15240v1",
    "published_date": "2025-05-21 08:16:18 UTC",
    "updated_date": "2025-05-21 08:16:18 UTC"
  },
  {
    "arxiv_id": "2505.15239v1",
    "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers",
    "authors": [
      "Peter Súkeník",
      "Christoph H. Lampert",
      "Marco Mondelli"
    ],
    "abstract": "The empirical emergence of neural collapse -- a surprising symmetry in the\nfeature representations of the training data in the penultimate layer of deep\nneural networks -- has spurred a line of theoretical research aimed at its\nunderstanding. However, existing work focuses on data-agnostic models or, when\ndata structure is taken into account, it remains limited to multi-layer\nperceptrons. Our paper fills both these gaps by analyzing modern architectures\nin a data-aware regime: we prove that global optima of deep regularized\ntransformers and residual networks (ResNets) with LayerNorm trained with cross\nentropy or mean squared error loss are approximately collapsed, and the\napproximation gets tighter as the depth grows. More generally, we formally\nreduce any end-to-end large-depth ResNet or transformer training into an\nequivalent unconstrained features model, thus justifying its wide use in the\nliterature even beyond data-agnostic settings. Our theoretical results are\nsupported by experiments on computer vision and language datasets showing that,\nas the depth grows, neural collapse indeed becomes more prominent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15239v1",
    "published_date": "2025-05-21 08:16:03 UTC",
    "updated_date": "2025-05-21 08:16:03 UTC"
  },
  {
    "arxiv_id": "2505.15234v1",
    "title": "SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning",
    "authors": [
      "Saqib Qamar",
      "Mohd Fazil",
      "Parvez Ahmad",
      "Ghulam Muhammad"
    ],
    "abstract": "Medical image segmentation plays an important role in various clinical\napplications, but existing models often struggle with the computational\ninefficiencies and challenges posed by complex medical data. State Space\nSequence Models (SSMs) have demonstrated promise in modeling long-range\ndependencies with linear computational complexity, yet their application in\nmedical image segmentation remains hindered by incompatibilities with image\ntokens and autoregressive assumptions. Moreover, it is difficult to achieve a\nbalance in capturing both local fine-grained information and global semantic\ndependencies. To address these challenges, we introduce SAMA-UNet, a novel\narchitecture for medical image segmentation. A key innovation is the\nSelf-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates\ncontextual self-attention with dynamic weight modulation to prioritise the most\nrelevant features based on local and global contexts. This approach reduces\ncomputational complexity and improves the representation of complex image\nfeatures across multiple scales. We also suggest the Causal-Resonance\nMulti-Scale Module (CR-MSM), which enhances the flow of information between the\nencoder and decoder by using causal resonance learning. This mechanism allows\nthe model to automatically adjust feature resolution and causal dependencies\nacross scales, leading to better semantic alignment between the low-level and\nhigh-level features in U-shaped architectures. Experiments on MRI, CT, and\nendoscopy images show that SAMA-UNet performs better in segmentation accuracy\nthan current methods using CNN, Transformer, and Mamba. The implementation is\npublicly available at GitHub.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15234v1",
    "published_date": "2025-05-21 08:12:31 UTC",
    "updated_date": "2025-05-21 08:12:31 UTC"
  },
  {
    "arxiv_id": "2505.15216v1",
    "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems",
    "authors": [
      "Andy K. Zhang",
      "Joey Ji",
      "Celeste Menders",
      "Riya Dulepet",
      "Thomas Qin",
      "Ron Y. Wang",
      "Junrong Wu",
      "Kyleen Liao",
      "Jiliang Li",
      "Jinghan Hu",
      "Sara Hong",
      "Nardos Demilew",
      "Shivatmica Murgai",
      "Jason Tran",
      "Nishka Kacheria",
      "Ethan Ho",
      "Denis Liu",
      "Lauren McLane",
      "Olivia Bruvik",
      "Dai-Rong Han",
      "Seungwoo Kim",
      "Akhil Vyas",
      "Cuiyuanxiu Chen",
      "Ryan Li",
      "Weiran Xu",
      "Jonathan Z. Ye",
      "Prerit Choudhary",
      "Siddharth M. Bhatia",
      "Vikram Sivashankar",
      "Yuxuan Bao",
      "Dawn Song",
      "Dan Boneh",
      "Daniel E. Ho",
      "Percy Liang"
    ],
    "abstract": "AI agents have the potential to significantly alter the cybersecurity\nlandscape. To help us understand this change, we introduce the first framework\nto capture offensive and defensive cyber-capabilities in evolving real-world\nsystems. Instantiating this framework with BountyBench, we set up 25 systems\nwith complex, real-world codebases. To capture the vulnerability lifecycle, we\ndefine three task types: Detect (detecting a new vulnerability), Exploit\n(exploiting a specific vulnerability), and Patch (patching a specific\nvulnerability). For Detect, we construct a new success indicator, which is\ngeneral across vulnerability types and provides localized evaluation. We\nmanually set up the environment for each system, including installing packages,\nsetting up server(s), and hydrating database(s). We add 40 bug bounties, which\nare vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of\nthe OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy\nbased on information to guide detection, interpolating from identifying a zero\nday to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,\nOpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and\nClaude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing\nagents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with\nClaude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on\nExploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch,\nmapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at\ndefense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit\nscores of 32.5% and 57.5% respectively; in contrast, the custom agents are\nrelatively balanced between offense and defense, achieving Exploit scores of\n40-67.5% and Patch scores of 45-60%.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "78 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.15216v1",
    "published_date": "2025-05-21 07:44:52 UTC",
    "updated_date": "2025-05-21 07:44:52 UTC"
  },
  {
    "arxiv_id": "2505.15206v1",
    "title": "EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy",
    "authors": [
      "Chi Kit Ng",
      "Long Bai",
      "Guankun Wang",
      "Yupeng Wang",
      "Huxin Gao",
      "Kun Yuan",
      "Chenhan Jin",
      "Tieyong Zeng",
      "Hongliang Ren"
    ],
    "abstract": "In endoscopic procedures, autonomous tracking of abnormal regions and\nfollowing circumferential cutting markers can significantly reduce the\ncognitive burden on endoscopists. However, conventional model-based pipelines\nare fragile for each component (e.g., detection, motion planning) requires\nmanual tuning and struggles to incorporate high-level endoscopic intent,\nleading to poor generalization across diverse scenes. Vision-Language-Action\n(VLA) models, which integrate visual perception, language grounding, and motion\nplanning within an end-to-end framework, offer a promising alternative by\nsemantically adapting to surgeon prompts without manual recalibration. Despite\ntheir potential, applying VLA models to robotic endoscopy presents unique\nchallenges due to the complex and dynamic anatomical environments of the\ngastrointestinal (GI) tract. To address this, we introduce EndoVLA, designed\nspecifically for continuum robots in GI interventions. Given endoscopic images\nand surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1)\npolyp tracking, (2) delineation and following of abnormal mucosal regions, and\n(3) adherence to circular markers during circumferential cutting. To tackle\ndata scarcity and domain shifts, we propose a dual-phase strategy comprising\nsupervised fine-tuning on our EndoVLA-Motion dataset and reinforcement\nfine-tuning with task-aware rewards. Our approach significantly improves\ntracking performance in endoscopy and enables zero-shot generalization in\ndiverse scenes and complex sequential tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15206v1",
    "published_date": "2025-05-21 07:35:00 UTC",
    "updated_date": "2025-05-21 07:35:00 UTC"
  },
  {
    "arxiv_id": "2505.15201v1",
    "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
    "authors": [
      "Christian Walder",
      "Deep Karkhanis"
    ],
    "abstract": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15201v1",
    "published_date": "2025-05-21 07:26:36 UTC",
    "updated_date": "2025-05-21 07:26:36 UTC"
  },
  {
    "arxiv_id": "2505.15197v1",
    "title": "Intentional Gesture: Deliver Your Intentions with Gestures for Speech",
    "authors": [
      "Pinxin Liu",
      "Haiyang Liu",
      "Luchuan Song",
      "Chenliang Xu"
    ],
    "abstract": "When humans speak, gestures help convey communicative intentions, such as\nadding emphasis or describing concepts. However, current co-speech gesture\ngeneration methods rely solely on superficial linguistic cues (\\textit{e.g.}\nspeech audio or text transcripts), neglecting to understand and leverage the\ncommunicative intention that underpins human gestures. This results in outputs\nthat are rhythmically synchronized with speech but are semantically shallow. To\naddress this gap, we introduce \\textbf{Intentional-Gesture}, a novel framework\nthat casts gesture generation as an intention-reasoning task grounded in\nhigh-level communicative functions. % First, we curate the \\textbf{InG} dataset\nby augmenting BEAT-2 with gesture-intention annotations (\\textit{i.e.}, text\nsentences summarizing intentions), which are automatically annotated using\nlarge vision-language models. Next, we introduce the \\textbf{Intentional\nGesture Motion Tokenizer} to leverage these intention annotations. It injects\nhigh-level communicative functions (\\textit{e.g.}, intentions) into tokenized\nmotion representations to enable intention-aware gesture synthesis that are\nboth temporally aligned and semantically meaningful, achieving new\nstate-of-the-art performance on the BEAT-2 benchmark. Our framework offers a\nmodular foundation for expressive gesture generation in digital humans and\nembodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15197v1",
    "published_date": "2025-05-21 07:24:51 UTC",
    "updated_date": "2025-05-21 07:24:51 UTC"
  },
  {
    "arxiv_id": "2505.15182v1",
    "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection",
    "authors": [
      "Jeonghye Kim",
      "Sojeong Rhee",
      "Minbeom Kim",
      "Dohyung Kim",
      "Sangmook Lee",
      "Youngchul Sung",
      "Kyomin Jung"
    ],
    "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like\nReAct, which interleave thought and action in complex environments. However,\nReAct often produces ungrounded or incoherent reasoning steps, leading to\nmisalignment between the agent's actual state and goal. Our analysis finds that\nthis stems from ReAct's inability to maintain consistent internal beliefs and\ngoal alignment, causing compounding errors and hallucinations. To address this,\nwe introduce ReflAct, a novel backbone that shifts reasoning from merely\nplanning next actions to continuously reflecting on the agent's state relative\nto its goal. By explicitly grounding decisions in states and enforcing ongoing\ngoal alignment, ReflAct dramatically improves strategic reliability. This\ndesign delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%\non average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even\noutperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),\nshowing that strengthening the core reasoning backbone is key to reliable agent\nperformance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15182v1",
    "published_date": "2025-05-21 06:57:39 UTC",
    "updated_date": "2025-05-21 06:57:39 UTC"
  },
  {
    "arxiv_id": "2505.15173v1",
    "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection",
    "authors": [
      "Zhipei Xu",
      "Xuanyu Zhang",
      "Xing Zhou",
      "Jian Zhang"
    ],
    "abstract": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnologies, particularly in video generation, has led to unprecedented\ncreative capabilities but also increased threats to information integrity,\nidentity security, and public trust. Existing detection methods, while\neffective in general scenarios, lack robust solutions for human-centric videos,\nwhich pose greater risks due to their realism and potential for legal and\nethical misuse. Moreover, current detection approaches often suffer from poor\ngeneralization, limited scalability, and reliance on labor-intensive supervised\nfine-tuning. To address these challenges, we propose AvatarShield, the first\ninterpretable MLLM-based framework for detecting human-centric fake videos,\nenhanced via Group Relative Policy Optimization (GRPO). Through our carefully\ndesigned accuracy detection reward and temporal compensation reward, it\neffectively avoids the use of high-cost text annotation data, enabling precise\ntemporal modeling and forgery detection. Meanwhile, we design a dual-encoder\narchitecture, combining high-level semantic reasoning and low-level artifact\namplification to guide MLLMs in effective forgery detection. We further collect\nFakeHumanVid, a large-scale human-centric video benchmark that includes\nsynthesis methods guided by pose, audio, and text inputs, enabling rigorous\nevaluation of detection methods in real-world scenes. Extensive experiments\nshow that AvatarShield significantly outperforms existing approaches in both\nin-domain and cross-domain detection, setting a new standard for human-centric\nvideo forensics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15173v1",
    "published_date": "2025-05-21 06:43:34 UTC",
    "updated_date": "2025-05-21 06:43:34 UTC"
  },
  {
    "arxiv_id": "2505.15155v1",
    "title": "R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization",
    "authors": [
      "Yuante Li",
      "Xu Yang",
      "Xiao Yang",
      "Minrui Xu",
      "Xisen Wang",
      "Weiqing Liu",
      "Jiang Bian"
    ],
    "abstract": "Financial markets pose fundamental challenges for asset return prediction due\nto their high dimensionality, non-stationarity, and persistent volatility.\nDespite advances in large language models and multi-agent systems, current\nquantitative research pipelines suffer from limited automation, weak\ninterpretability, and fragmented coordination across key components such as\nfactor mining and model innovation. In this paper, we propose R&D-Agent for\nQuantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent\nframework designed to automate the full-stack research and development of\nquantitative strategies via coordinated factor-model co-optimization.\nRD-Agent(Q) decomposes the quant process into two iterative stages: a Research\nstage that dynamically sets goal-aligned prompts, formulates hypotheses based\non domain priors, and maps them to concrete tasks, and a Development stage that\nemploys a code-generation agent, Co-STEER, to implement task-specific code,\nwhich is then executed in real-market backtests. The two stages are connected\nthrough a feedback stage that thoroughly evaluates experimental outcomes and\ninforms subsequent iterations, with a multi-armed bandit scheduler for adaptive\ndirection selection. Empirically, RD-Agent(Q) achieves up to 2X higher\nannualized returns than classical factor libraries using 70% fewer factors, and\noutperforms state-of-the-art deep time-series models on real markets. Its joint\nfactor-model optimization delivers a strong balance between predictive accuracy\nand strategy robustness. Our code is available at:\nhttps://github.com/microsoft/RD-Agent.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15155v1",
    "published_date": "2025-05-21 06:20:56 UTC",
    "updated_date": "2025-05-21 06:20:56 UTC"
  },
  {
    "arxiv_id": "2505.15154v1",
    "title": "Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning",
    "authors": [
      "Jinghui Lu",
      "Haiyang Yu",
      "Siliang Xu",
      "Shiwei Ran",
      "Guozhi Tang",
      "Siqi Wang",
      "Bin Shan",
      "Teng Fu",
      "Hao Feng",
      "Jingqun Tang",
      "Han Wang",
      "Can Huang"
    ],
    "abstract": "Recent advancements in reasoning have significantly enhanced the capabilities\nof Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nacross diverse tasks. However, excessive reliance on chain-of-thought (CoT)\nreasoning can impair model performance and brings unnecessarily lengthened\noutputs, reducing efficiency. Our work reveals that prolonged reasoning does\nnot universally improve accuracy and even degrade performance on simpler tasks.\nTo address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel\nframework that dynamically switches between short answers and long-form\nreasoning based on the model perplexity. CAR first generates a short answer and\nevaluates its perplexity, triggering reasoning only when the model exhibits low\nconfidence (i.e., high perplexity). Experiments across diverse multimodal\nVQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both\nshort-answer and long-form reasoning approaches, striking an optimal balance\nbetween accuracy and efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15154v1",
    "published_date": "2025-05-21 06:20:17 UTC",
    "updated_date": "2025-05-21 06:20:17 UTC"
  },
  {
    "arxiv_id": "2505.15862v1",
    "title": "Bandit based Dynamic Candidate Edge Selection in Solving Traveling Salesman Problems",
    "authors": [
      "Long Wanga",
      "Jiongzhi Zheng",
      "Zhengda Xiong",
      "ChuMin Li",
      "Kun He"
    ],
    "abstract": "Algorithms designed for routing problems typically rely on high-quality\ncandidate edges to guide their search, aiming to reduce the search space and\nenhance the search efficiency. However, many existing algorithms, like the\nclassical Lin-Kernighan-Helsgaun (LKH) algorithm for the Traveling Salesman\nProblem (TSP), often use predetermined candidate edges that remain static\nthroughout local searches. This rigidity could cause the algorithm to get\ntrapped in local optima, limiting its potential to find better solutions. To\naddress this issue, we propose expanding the candidate sets to include other\npromising edges, providing them an opportunity for selection. Specifically, we\nincorporate multi-armed bandit models to dynamically select the most suitable\ncandidate edges in each iteration, enabling LKH to make smarter choices and\nlead to improved solutions. Extensive experiments on multiple TSP benchmarks\nshow the excellent performance of our method. Moreover, we employ this\nbandit-based method to LKH-3, an extension of LKH tailored for solving various\nTSP variant problems, and our method also significantly enhances LKH-3's\nperformance across typical TSP variants.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15862v1",
    "published_date": "2025-05-21 06:11:00 UTC",
    "updated_date": "2025-05-21 06:11:00 UTC"
  },
  {
    "arxiv_id": "2505.15146v1",
    "title": "lmgame-Bench: How Good are LLMs at Playing Games?",
    "authors": [
      "Lanxiang Hu",
      "Mingjia Huo",
      "Yuxuan Zhang",
      "Haoyang Yu",
      "Eric P. Xing",
      "Ion Stoica",
      "Tajana Rosing",
      "Haojian Jin",
      "Hao Zhang"
    ],
    "abstract": "Playing video games requires perception, memory, and planning, exactly the\nfaculties modern large language model (LLM) agents are expected to master. We\nstudy the major challenges in using popular video games to evaluate modern LLMs\nand find that directly dropping LLMs into games cannot make an effective\nevaluation, for three reasons -- brittle vision perception, prompt sensitivity,\nand potential data contamination. We introduce lmgame-Bench to turn games into\nreliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and\nnarrative games delivered through a unified Gym-style API and paired with\nlightweight perception and memory scaffolds, and is designed to stabilize\nprompt variance and remove contamination. Across 13 leading models, we show\nlmgame-Bench is challenging while still separating models well. Correlation\nanalysis shows that every game probes a unique blend of capabilities often\ntested in isolation elsewhere. More interestingly, performing reinforcement\nlearning on a single game from lmgame-Bench transfers both to unseen games and\nto external planning tasks. Our evaluation code is available at\nhttps://github.com/lmgame-org/GamingAgent/lmgame-bench.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15146v1",
    "published_date": "2025-05-21 06:02:55 UTC",
    "updated_date": "2025-05-21 06:02:55 UTC"
  },
  {
    "arxiv_id": "2505.15141v1",
    "title": "BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms",
    "authors": [
      "Yunlong Hou",
      "Fengzhuo Zhang",
      "Cunxiao Du",
      "Xuan Zhang",
      "Jiachun Pan",
      "Tianyu Pang",
      "Chao Du",
      "Vincent Y. F. Tan",
      "Zhuoran Yang"
    ],
    "abstract": "Speculative decoding has emerged as a popular method to accelerate the\ninference of Large Language Models (LLMs) while retaining their superior text\ngeneration performance. Previous methods either adopt a fixed speculative\ndecoding configuration regardless of the prefix tokens, or train draft models\nin an offline or online manner to align them with the context. This paper\nproposes a training-free online learning framework to adaptively choose the\nconfiguration of the hyperparameters for speculative decoding as text is being\ngenerated. We first formulate this hyperparameter selection problem as a\nMulti-Armed Bandit problem and provide a general speculative decoding framework\nBanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,\nUCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,\nthe stopping time regret. We upper bound this regret under both stochastic and\nadversarial reward settings. By deriving an information-theoretic impossibility\nresult, it is shown that the regret performance of UCBSpec is optimal up to\nuniversal constants. Finally, extensive empirical experiments with LLaMA3 and\nQwen2 demonstrate that our algorithms are effective compared to existing\nmethods, and the throughput is close to the oracle best hyperparameter in\nsimulated real-life LLM serving scenarios with diverse input prompts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "35 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15141v1",
    "published_date": "2025-05-21 05:56:31 UTC",
    "updated_date": "2025-05-21 05:56:31 UTC"
  },
  {
    "arxiv_id": "2505.15138v1",
    "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm",
    "authors": [
      "Yang Xu",
      "Swetha Ganesh",
      "Washim Uddin Mondal",
      "Qinbo Bai",
      "Vaneet Aggarwal"
    ],
    "abstract": "This paper investigates infinite-horizon average reward Constrained Markov\nDecision Processes (CMDPs) with general parametrization. We propose a\nPrimal-Dual Natural Actor-Critic algorithm that adeptly manages constraints\nwhile ensuring a high convergence rate. In particular, our algorithm achieves\nglobal convergence and constraint violation rates of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ over a horizon of length $T$ when the mixing\ntime, $\\tau_{\\mathrm{mix}}$, is known to the learner. In absence of knowledge\nof $\\tau_{\\mathrm{mix}}$, the achievable rates change to\n$\\tilde{\\mathcal{O}}(1/T^{0.5-\\epsilon})$ provided that $T \\geq\n\\tilde{\\mathcal{O}}\\left(\\tau_{\\mathrm{mix}}^{2/\\epsilon}\\right)$. Our results\nmatch the theoretical lower bound for Markov Decision Processes and establish a\nnew benchmark in the theoretical exploration of average reward CMDPs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15138v1",
    "published_date": "2025-05-21 05:49:11 UTC",
    "updated_date": "2025-05-21 05:49:11 UTC"
  },
  {
    "arxiv_id": "2505.15134v1",
    "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning",
    "authors": [
      "Shivam Agarwal",
      "Zimin Zhang",
      "Lifan Yuan",
      "Jiawei Han",
      "Hao Peng"
    ],
    "abstract": "Entropy minimization (EM) trains the model to concentrate even more\nprobability mass on its most confident outputs. We show that this simple\nobjective alone, without any labeled data, can substantially improve large\nlanguage models' (LLMs) performance on challenging math, physics, and coding\ntasks. We explore three approaches: (1) EM-FT minimizes token-level entropy\nsimilarly to instruction finetuning, but on unlabeled outputs drawn from the\nmodel; (2) EM-RL: reinforcement learning with negative entropy as the only\nreward to maximize; (3) EM-INF: inference-time logit adjustment to reduce\nentropy without any training data or parameter updates. On Qwen-7B, EM-RL,\nwithout any labeled data, achieves comparable or better performance than strong\nRL baselines such as GRPO and RLOO that are trained on 60K labeled examples.\nFurthermore, EM-INF enables Qwen-32B to match or exceed the performance of\nproprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the\nchallenging SciCode benchmark, while being 3x more efficient than\nself-consistency and sequential refinement. Our findings reveal that many\npretrained LLMs possess previously underappreciated reasoning capabilities that\ncan be effectively elicited through entropy minimization alone, without any\nlabeled data or even any parameter updates.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15134v1",
    "published_date": "2025-05-21 05:39:11 UTC",
    "updated_date": "2025-05-21 05:39:11 UTC"
  },
  {
    "arxiv_id": "2505.15133v1",
    "title": "DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer",
    "authors": [
      "Haiduo Huang",
      "Jiangcheng Song",
      "Yadong Zhang",
      "Pengju Ren"
    ],
    "abstract": "Recent advances in knowledge distillation have emphasized the importance of\ndecoupling different knowledge components. While existing methods utilize\nmomentum mechanisms to separate task-oriented and distillation gradients, they\noverlook the inherent conflict between target-class and non-target-class\nknowledge flows. Furthermore, low-confidence dark knowledge in non-target\nclasses introduces noisy signals that hinder effective knowledge transfer. To\naddress these limitations, we propose DeepKD, a novel training framework that\nintegrates dual-level decoupling with adaptive denoising. First, through\ntheoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics\nin task-oriented and non-task-oriented knowledge distillation, we design\nindependent momentum updaters for each component to prevent mutual\ninterference. We observe that the optimal momentum coefficients for\ntask-oriented gradient (TOG), target-class gradient (TCG), and non-target-class\ngradient (NCG) should be positively related to their GSNR. Second, we introduce\na dynamic top-k mask (DTM) mechanism that gradually increases K from a small\ninitial value to incorporate more non-target classes as training progresses,\nfollowing curriculum learning principles. The DTM jointly filters\nlow-confidence logits from both teacher and student models, effectively\npurifying dark knowledge during early training. Extensive experiments on\nCIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code\nis available at https://github.com/haiduo/DeepKD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15133v1",
    "published_date": "2025-05-21 05:38:57 UTC",
    "updated_date": "2025-05-21 05:38:57 UTC"
  },
  {
    "arxiv_id": "2505.15123v1",
    "title": "Seeing the Trees for the Forest: Rethinking Weakly-Supervised Medical Visual Grounding",
    "authors": [
      "Ta Duc Huy",
      "Duy Anh Huynh",
      "Yutong Xie",
      "Yuankai Qi",
      "Qi Chen",
      "Phi Le Nguyen",
      "Sen Kim Tran",
      "Son Lam Phung",
      "Anton van den Hengel",
      "Zhibin Liao",
      "Minh-Son To",
      "Johan W. Verjans",
      "Vu Minh Hieu Phan"
    ],
    "abstract": "Visual grounding (VG) is the capability to identify the specific regions in\nan image associated with a particular text description. In medical imaging, VG\nenhances interpretability by highlighting relevant pathological features\ncorresponding to textual descriptions, improving model transparency and\ntrustworthiness for wider adoption of deep learning models in clinical\npractice. Current models struggle to associate textual descriptions with\ndisease regions due to inefficient attention mechanisms and a lack of\nfine-grained token representations. In this paper, we empirically demonstrate\ntwo key observations. First, current VLMs assign high norms to background\ntokens, diverting the model's attention from regions of disease. Second, the\nglobal tokens used for cross-modal learning are not representative of local\ndisease tokens. This hampers identifying correlations between the text and\ndisease tokens. To address this, we introduce simple, yet effective\nDisease-Aware Prompting (DAP) process, which uses the explainability map of a\nVLM to identify the appropriate image features. This simple strategy amplifies\ndisease-relevant regions while suppressing background interference. Without any\nadditional pixel-level annotations, DAP improves visual grounding accuracy by\n20.74% compared to state-of-the-art methods across three major chest X-ray\ndatasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2505.15123v1",
    "published_date": "2025-05-21 05:16:45 UTC",
    "updated_date": "2025-05-21 05:16:45 UTC"
  },
  {
    "arxiv_id": "2505.15117v1",
    "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents",
    "authors": [
      "Bowen Jin",
      "Jinsung Yoon",
      "Priyanka Kargupta",
      "Sercan O. Arik",
      "Jiawei Han"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.15117v1",
    "published_date": "2025-05-21 05:09:43 UTC",
    "updated_date": "2025-05-21 05:09:43 UTC"
  },
  {
    "arxiv_id": "2505.15116v1",
    "title": "Graph Foundation Models: A Comprehensive Survey",
    "authors": [
      "Zehong Wang",
      "Zheyuan Liu",
      "Tianyi Ma",
      "Jiazheng Li",
      "Zheyuan Zhang",
      "Xingbo Fu",
      "Yiyang Li",
      "Zhengqing Yuan",
      "Wei Song",
      "Yijun Ma",
      "Qingkai Zeng",
      "Xiusi Chen",
      "Jianan Zhao",
      "Jundong Li",
      "Meng Jiang",
      "Pietro Lio",
      "Nitesh Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Github Repo:\n  https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs. 93 pages,\n  438 references",
    "pdf_url": "http://arxiv.org/pdf/2505.15116v1",
    "published_date": "2025-05-21 05:08:00 UTC",
    "updated_date": "2025-05-21 05:08:00 UTC"
  },
  {
    "arxiv_id": "2505.15111v1",
    "title": "iPad: Iterative Proposal-centric End-to-End Autonomous Driving",
    "authors": [
      "Ke Guo",
      "Haochen Liu",
      "Xiaojun Wu",
      "Jia Pan",
      "Chen Lv"
    ],
    "abstract": "End-to-end (E2E) autonomous driving systems offer a promising alternative to\ntraditional modular pipelines by reducing information loss and error\naccumulation, with significant potential to enhance both mobility and safety.\nHowever, most existing E2E approaches directly generate plans based on dense\nbird's-eye view (BEV) grid features, leading to inefficiency and limited\nplanning awareness. To address these limitations, we propose iterative\nProposal-centric autonomous driving (iPad), a novel framework that places\nproposals - a set of candidate future plans - at the center of feature\nextraction and auxiliary tasks. Central to iPad is ProFormer, a BEV encoder\nthat iteratively refines proposals and their associated features through\nproposal-anchored attention, effectively fusing multi-view image data.\nAdditionally, we introduce two lightweight, proposal-centric auxiliary tasks -\nmapping and prediction - that improve planning quality with minimal\ncomputational overhead. Extensive experiments on the NAVSIM and CARLA\nBench2Drive benchmarks demonstrate that iPad achieves state-of-the-art\nperformance while being significantly more efficient than prior leading\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15111v1",
    "published_date": "2025-05-21 05:05:38 UTC",
    "updated_date": "2025-05-21 05:05:38 UTC"
  },
  {
    "arxiv_id": "2505.15108v1",
    "title": "A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents",
    "authors": [
      "Ian Steenstra",
      "Timothy W. Bickmore"
    ],
    "abstract": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual\nAgents acting as psychotherapists presents significant opportunities for\nexpanding mental healthcare access. However, their deployment has also been\nlinked to serious adverse outcomes, including user harm and suicide,\nfacilitated by a lack of standardized evaluation methodologies capable of\ncapturing the nuanced risks of therapeutic interaction. Current evaluation\ntechniques lack the sensitivity to detect subtle changes in patient cognition\nand behavior during therapy sessions that may lead to subsequent\ndecompensation. We introduce a novel risk taxonomy specifically designed for\nthe systematic evaluation of conversational AI psychotherapists. Developed\nthrough an iterative process including review of the psychotherapy risk\nliterature, qualitative interviews with clinical and legal experts, and\nalignment with established clinical criteria (e.g., DSM-5) and existing\nassessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured\napproach to identifying and assessing user/patient harms. We provide a\nhigh-level overview of this taxonomy, detailing its grounding, and discuss\npotential use cases. We discuss two use cases in detail: monitoring cognitive\nmodel-based risk factors during a counseling conversation to detect unsafe\ndeviations, in both human-AI counseling sessions and in automated benchmarking\nof AI psychotherapists with simulated patients. The proposed taxonomy offers a\nfoundational step towards establishing safer and more responsible innovation in\nthe domain of AI-driven mental health support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15108v1",
    "published_date": "2025-05-21 05:01:39 UTC",
    "updated_date": "2025-05-21 05:01:39 UTC"
  },
  {
    "arxiv_id": "2505.15107v1",
    "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization",
    "authors": [
      "Ziliang Wang",
      "Xuhui Zheng",
      "Kang An",
      "Cijun Ouyang",
      "Jialu Cai",
      "Yuhang Wang",
      "Yichao Wu"
    ],
    "abstract": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our\nimplementation is publicly available at\nhttps://github.com/zxh20001117/StepSearch.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15107v1",
    "published_date": "2025-05-21 05:01:31 UTC",
    "updated_date": "2025-05-21 05:01:31 UTC"
  },
  {
    "arxiv_id": "2505.15105v1",
    "title": "Mechanistic evaluation of Transformers and state space models",
    "authors": [
      "Aryaman Arora",
      "Neil Rathi",
      "Nikil Roashan Selvam",
      "Róbert Csórdas",
      "Dan Jurafsky",
      "Christopher Potts"
    ],
    "abstract": "State space models (SSMs) for language modelling promise an efficient and\nperformant alternative to quadratic-attention Transformers, yet show variable\nperformance on recalling basic information from the context. While performance\non synthetic tasks like Associative Recall (AR) can point to this deficiency,\nbehavioural metrics provide little information as to why--on a mechanistic\nlevel--certain architectures fail and others succeed. To address this, we\nconduct experiments on AR and find that only Transformers and Based SSM models\nfully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,\nHyena) fail. We then use causal interventions to explain why. We find that\nTransformers and Based learn to store key-value associations in-context using\ninduction heads. By contrast, the SSMs compute these associations only at the\nlast state, with only Mamba succeeding because of its short convolution\ncomponent. To extend and deepen these findings, we introduce Associative\nTreecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR\nintroduces language-like hierarchical structure into the AR setting. We find\nthat all architectures learn the same mechanism as they did for AR, and the\nsame three models succeed at the task. These results reveal that architectures\nwith similar accuracy may still have substantive differences, motivating the\nadoption of mechanistic evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "9 page main text, 6 pages appendix",
    "pdf_url": "http://arxiv.org/pdf/2505.15105v1",
    "published_date": "2025-05-21 04:56:09 UTC",
    "updated_date": "2025-05-21 04:56:09 UTC"
  },
  {
    "arxiv_id": "2505.15098v1",
    "title": "Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation",
    "authors": [
      "Yihang Li",
      "Tianle Zhang",
      "Xuelong Wei",
      "Jiayi Li",
      "Lin Zhao",
      "Dongchi Huang",
      "Zhirui Fang",
      "Minhua Zheng",
      "Wenjun Dai",
      "Xiaodong He"
    ],
    "abstract": "Robot manipulation learning from human demonstrations offers a rapid means to\nacquire skills but often lacks generalization across diverse scenes and object\nplacements. This limitation hinders real-world applications, particularly in\ncomplex tasks requiring dexterous manipulation. Vision-Language-Action (VLA)\nparadigm leverages large-scale data to enhance generalization. However, due to\ndata scarcity, VLA's performance remains limited. In this work, we introduce\nObject-Focus Actor (OFA), a novel, data-efficient approach for generalized\ndexterous manipulation. OFA exploits the consistent end trajectories observed\nin dexterous manipulation tasks, allowing for efficient policy training. Our\nmethod employs a hierarchical pipeline: object perception and pose estimation,\npre-manipulation pose arrival and OFA policy execution. This process ensures\nthat the manipulation is focused and efficient, even in varied backgrounds and\npositional layout. Comprehensive real-world experiments across seven tasks\ndemonstrate that OFA significantly outperforms baseline methods in both\npositional and background generalization tests. Notably, OFA achieves robust\nperformance with only 10 demonstrations, highlighting its data efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15098v1",
    "published_date": "2025-05-21 04:37:56 UTC",
    "updated_date": "2025-05-21 04:37:56 UTC"
  },
  {
    "arxiv_id": "2505.15095v1",
    "title": "Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English",
    "authors": [
      "Ishmanbir Singh",
      "Dipankar Srirag",
      "Aditya Joshi"
    ],
    "abstract": "Sarcasm is a challenge to sentiment analysis because of the incongruity\nbetween stated and implied sentiment. The challenge is exacerbated when the\nimplication may be relevant to a specific country or geographical region.\nPragmatic metacognitive prompting (PMP) is a cognition-inspired technique that\nhas been used for pragmatic reasoning. In this paper, we harness PMP for\nexplainable sarcasm detection for Australian and Indian English, alongside a\nbenchmark dataset for standard English. We manually add sarcasm explanations to\nan existing sarcasm-labeled dataset for Australian and Indian English called\nBESSTIE, and compare the performance for explainable sarcasm detection for them\nwith FLUTE, a standard English dataset containing sarcasm explanations. Our\napproach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)\nachieves statistically significant performance improvement across all tasks and\ndatasets when compared with four alternative prompting strategies. We also find\nthat alternative techniques such as agentic prompting mitigate context-related\nfailures by enabling external knowledge retrieval. The focused contribution of\nour work is utilising PMP in generating sarcasm explanations for varieties of\nEnglish.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review. 4 pages + references",
    "pdf_url": "http://arxiv.org/pdf/2505.15095v1",
    "published_date": "2025-05-21 04:34:22 UTC",
    "updated_date": "2025-05-21 04:34:22 UTC"
  },
  {
    "arxiv_id": "2505.15859v1",
    "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
    "authors": [
      "Tianyi Ma",
      "Yiyue Qian",
      "Zheyuan Zhang",
      "Zehong Wang",
      "Xiaoye Qian",
      "Feifan Bai",
      "Yifan Ding",
      "Xuwei Luo",
      "Shinan Zhang",
      "Keerthiram Murugesan",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15859v1",
    "published_date": "2025-05-21 04:32:35 UTC",
    "updated_date": "2025-05-21 04:32:35 UTC"
  },
  {
    "arxiv_id": "2505.15091v2",
    "title": "ThinkRec: Thinking-based recommendation via LLM",
    "authors": [
      "Qihang Yu",
      "Kairui Fu",
      "Shengyu Zhang",
      "Zheqi Lv",
      "Fan Wu",
      "Fei Wu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15091v2",
    "published_date": "2025-05-21 04:25:18 UTC",
    "updated_date": "2025-05-22 14:53:00 UTC"
  },
  {
    "arxiv_id": "2505.15090v1",
    "title": "DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer",
    "authors": [
      "Sona Elza Simon",
      "Preethi Jyothi"
    ],
    "abstract": "Effective cross-lingual transfer remains a critical challenge in scaling the\nbenefits of large language models from high-resource to low-resource languages.\nTowards this goal, prior studies have explored many approaches to combine task\nknowledge from task-specific data in a (high-resource) source language and\nlanguage knowledge from unlabeled text in a (low-resource) target language. One\nnotable approach proposed composable sparse fine-tuning (SFT) for cross-lingual\ntransfer that learns task-specific and language-specific sparse masks to select\na subset of the pretrained model's parameters that are further fine-tuned.\nThese sparse fine-tuned vectors (SFTs) are subsequently composed with the\npretrained model to facilitate zero-shot cross-lingual transfer to a task in a\ntarget language, using only task-specific data from a source language. These\nsparse masks for SFTs were identified using a simple magnitude-based pruning.\nIn our work, we introduce DeFT-X, a novel composable SFT approach that denoises\nthe weight matrices of a pretrained model before magnitude pruning using\nsingular value decomposition, thus yielding more robust SFTs. We evaluate\nDeFT-X on a diverse set of extremely low-resource languages for sentiment\nclassification (NusaX) and natural language inference (AmericasNLI) and\ndemonstrate that it performs at par or outperforms SFT and other prominent\ncross-lingual transfer baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15090v1",
    "published_date": "2025-05-21 04:20:30 UTC",
    "updated_date": "2025-05-21 04:20:30 UTC"
  },
  {
    "arxiv_id": "2505.15088v1",
    "title": "Leveraging Large Language Models for Command Injection Vulnerability Analysis in Python: An Empirical Study on Popular Open-Source Projects",
    "authors": [
      "Yuxuan Wang",
      "Jingshu Chen",
      "Qingyang Wang"
    ],
    "abstract": "Command injection vulnerabilities are a significant security threat in\ndynamic languages like Python, particularly in widely used open-source projects\nwhere security issues can have extensive impact. With the proven effectiveness\nof Large Language Models(LLMs) in code-related tasks, such as testing,\nresearchers have explored their potential for vulnerabilities analysis. This\nstudy evaluates the potential of large language models (LLMs), such as GPT-4,\nas an alternative approach for automated testing for vulnerability detection.\nIn particular, LLMs have demonstrated advanced contextual understanding and\nadaptability, making them promising candidates for identifying nuanced security\nvulnerabilities within code. To evaluate this potential, we applied LLM-based\nanalysis to six high-profile GitHub projects-Django, Flask, TensorFlow,\nScikit-learn, PyTorch, and Langchain-each with over 50,000 stars and extensive\nadoption across software development and academic research. Our analysis\nassesses both the strengths and limitations of LLMs in detecting command\ninjection vulnerabilities, evaluating factors such as detection accuracy,\nefficiency, and practical integration into development workflows. In addition,\nwe provide a comparative analysis of different LLM tools to identify those most\nsuitable for security applications. Our findings offer guidance for developers\nand security researchers on leveraging LLMs as innovative and automated\napproaches to enhance software security.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15088v1",
    "published_date": "2025-05-21 04:14:35 UTC",
    "updated_date": "2025-05-21 04:14:35 UTC"
  },
  {
    "arxiv_id": "2505.15083v1",
    "title": "Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features",
    "authors": [
      "Jeremy Qin"
    ],
    "abstract": "Time series forecasting plays a crucial role in various applications,\nparticularly in healthcare, where accurate predictions of future health\ntrajectories can significantly impact clinical decision-making. Ensuring\ntransparency and explainability of the models responsible for these tasks is\nessential for their adoption in critical settings. Recent work has explored a\ntop-down approach to bi-level transparency, focusing on understanding trends\nand properties of predicted time series using static features. In this work, we\nextend this framework by incorporating exogenous time series features alongside\nstatic features in a structured manner, while maintaining cohesive\ninterpretation. Our approach leverages the insights of trajectory comprehension\nto introduce an encoding mechanism for exogenous time series, where they are\ndecomposed into meaningful trends and properties, enabling the extraction of\ninterpretable patterns. Through experiments on several synthetic datasets, we\ndemonstrate that our approach remains predictive while preserving\ninterpretability and robustness. This work represents a step towards developing\nrobust, and generalized time series forecasting models. The code is available\nat https://github.com/jeremy-qin/TIMEVIEW",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15083v1",
    "published_date": "2025-05-21 04:12:12 UTC",
    "updated_date": "2025-05-21 04:12:12 UTC"
  },
  {
    "arxiv_id": "2505.15080v1",
    "title": "SUS backprop: linear backpropagation algorithm for long inputs in transformers",
    "authors": [
      "Sergey Pankov",
      "Georges Harik"
    ],
    "abstract": "It is straightforward to design an unbiased gradient estimator that\nstochastically cuts the backpropagation flow through any part of a\ncomputational graph. By cutting the parts that have little effect on the\ncomputation, one can potentially save a significant amount of back-propagation\ncomputation in exchange for a minimal increase in the stochastic gradient\nvariance, in some situations. Such a situation occurs in the attention\nmechanism of the transformer architecture. For long sequences, attention\nbecomes the limiting factor, as its compute requirements increase quadratically\nwith sequence length $n$. At the same time, most attention weights become very\nsmall, as most attention heads tend to connect a given token with only a small\nfraction of other tokens in the sequence. These weights become promising\ntargets for cutting backpropagation. We propose a simple probabilistic rule\ncontrolled by a single parameter $c$ that cuts backpropagation through most\nattention weights, leaving at most $c$ interactions per token per attention\nhead. This brings a factor of $c/n$ reduction in the compute required for the\nattention backpropagation, turning it from quadratic $O(n^2)$ to linear\ncomplexity $O(nc)$. We have empirically verified that, for a typical\ntransformer model, cutting $99\\%$ of the attention gradient flow (i.e. choosing\n$c \\sim 20-30$) results in relative gradient variance increase of only about\n$1\\%$ for $n \\sim 2000$, and it decreases with $n$. This approach is amenable\nto efficient sparse matrix implementation, thus being promising for making the\ncost of a backward pass negligible relative to the cost of a forward pass when\ntraining a transformer model on long sequences.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15080v1",
    "published_date": "2025-05-21 04:00:38 UTC",
    "updated_date": "2025-05-21 04:00:38 UTC"
  },
  {
    "arxiv_id": "2505.15077v1",
    "title": "Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation",
    "authors": [
      "Alessandro dos Santos Ferreira",
      "Ana Paula Marques Ramos",
      "José Marcato Junior",
      "Wesley Nunes Gonçalves"
    ],
    "abstract": "Urban forests play a key role in enhancing environmental quality and\nsupporting biodiversity in cities. Mapping and monitoring these green spaces\nare crucial for urban planning and conservation, yet accurately detecting trees\nis challenging due to complex landscapes and the variability in image\nresolution caused by different satellite sensors or UAV flight altitudes. While\ndeep learning architectures have shown promise in addressing these challenges,\ntheir effectiveness remains strongly dependent on the availability of large and\nmanually labeled datasets, which are often expensive and difficult to obtain in\nsufficient quantity. In this work, we propose a novel pipeline that integrates\ndomain adaptation with GANs and Diffusion models to enhance the quality of\nlow-resolution aerial images. Our proposed pipeline enhances low-resolution\nimagery while preserving semantic content, enabling effective tree segmentation\nwithout requiring large volumes of manually annotated data. Leveraging models\nsuch as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we\ngenerate realistic and structurally consistent synthetic samples that expand\nthe training dataset and unify scale across domains. This approach not only\nimproves the robustness of segmentation models across different acquisition\nconditions but also provides a scalable and replicable solution for remote\nsensing scenarios with scarce annotation resources. Experimental results\ndemonstrated an improvement of over 50% in IoU for low-resolution images,\nhighlighting the effectiveness of our method compared to traditional pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "68T07 (Primary), 68U10, 68T45 (Secondary)",
      "I.4.8; I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15077v1",
    "published_date": "2025-05-21 03:57:10 UTC",
    "updated_date": "2025-05-21 03:57:10 UTC"
  },
  {
    "arxiv_id": "2505.15076v1",
    "title": "Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories",
    "authors": [
      "Nanxu Gong",
      "Sixun Dong",
      "Haoyue Bai",
      "Xinyuan Wang",
      "Wangyang Ying",
      "Yanjie Fu"
    ],
    "abstract": "As a widely-used and practical tool, feature engineering transforms raw data\ninto discriminative features to advance AI model performance. However, existing\nmethods usually apply feature selection and generation separately, failing to\nstrive a balance between reducing redundancy and adding meaningful dimensions.\nTo fill this gap, we propose an agentic feature augmentation concept, where the\nunification of feature generation and selection is modeled as agentic teaming\nand planning. Specifically, we develop a Multi-Agent System with Long and\nShort-Term Memory (MAGS), comprising a selector agent to eliminate redundant\nfeatures, a generator agent to produce informative new dimensions, and a router\nagent that strategically coordinates their actions. We leverage in-context\nlearning with short-term memory for immediate feedback refinement and long-term\nmemory for globally optimal guidance. Additionally, we employ offline Proximal\nPolicy Optimization (PPO) reinforcement fine-tuning to train the router agent\nfor effective decision-making to navigate a vast discrete feature space.\nExtensive experiments demonstrate that this unified agentic framework\nconsistently achieves superior task performance by intelligently orchestrating\nfeature selection and generation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15076v1",
    "published_date": "2025-05-21 03:49:24 UTC",
    "updated_date": "2025-05-21 03:49:24 UTC"
  },
  {
    "arxiv_id": "2505.15075v1",
    "title": "Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs",
    "authors": [
      "Hao Wang",
      "Pinzhi Huang",
      "Jihan Yang",
      "Saining Xie",
      "Daisuke Kawahara"
    ],
    "abstract": "The rapid evolution of multimodal large language models (MLLMs) has\nsignificantly enhanced their real-world applications. However, achieving\nconsistent performance across languages, especially when integrating cultural\nknowledge, remains a significant challenge. To better assess this issue, we\nintroduce two new benchmarks: KnowRecall and VisRecall, which evaluate\ncross-lingual consistency in MLLMs. KnowRecall is a visual question answering\nbenchmark designed to measure factual knowledge consistency in 15 languages,\nfocusing on cultural and historical questions about global landmarks. VisRecall\nassesses visual memory consistency by asking models to describe landmark\nappearances in 9 languages without access to images. Experimental results\nreveal that state-of-the-art MLLMs, including proprietary ones, still struggle\nto achieve cross-lingual consistency. This underscores the need for more robust\napproaches that produce truly multilingual and culturally aware models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "https://github.com/nlp-waseda/traveling-across-languages",
    "pdf_url": "http://arxiv.org/pdf/2505.15075v1",
    "published_date": "2025-05-21 03:43:37 UTC",
    "updated_date": "2025-05-21 03:43:37 UTC"
  },
  {
    "arxiv_id": "2505.15074v1",
    "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data",
    "authors": [
      "Yuhang Zhou",
      "Jing Zhu",
      "Shengyi Qian",
      "Zhuokai Zhao",
      "Xiyao Wang",
      "Xiaoyu Liu",
      "Ming Li",
      "Paiheng Xu",
      "Wei Ai",
      "Furong Huang"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups - assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15074v1",
    "published_date": "2025-05-21 03:43:29 UTC",
    "updated_date": "2025-05-21 03:43:29 UTC"
  },
  {
    "arxiv_id": "2505.15068v1",
    "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges",
    "authors": [
      "Cheng Qian",
      "Hongyi Du",
      "Hongru Wang",
      "Xiusi Chen",
      "Yuji Zhang",
      "Avirup Sil",
      "Chengxiang Zhai",
      "Kathleen McKeown",
      "Heng Ji"
    ],
    "abstract": "Recent progress in large language models (LLMs) has enabled substantial\nadvances in solving mathematical problems. However, existing benchmarks often\nfail to reflect the complexity of real-world problems, which demand open-ended,\ninterdisciplinary reasoning and integration of computational tools. To address\nthis gap, we introduce ModelingBench, a novel benchmark featuring\nreal-world-inspired, open-ended problems from math modeling competitions across\ndiverse domains, ranging from urban traffic optimization to ecosystem resource\nplanning. These tasks require translating natural language into formal\nmathematical formulations, applying appropriate tools, and producing\nstructured, defensible reports. ModelingBench also supports multiple valid\nsolutions, capturing the ambiguity and creativity of practical modeling. We\nalso present ModelingAgent, a multi-agent framework that coordinates tool use,\nsupports structured workflows, and enables iterative self-refinement to\ngenerate well-grounded, creative solutions. To evaluate outputs, we further\npropose ModelingJudge, an expert-in-the-loop system leveraging LLMs as\ndomain-specialized judges assessing solutions from multiple expert\nperspectives. Empirical results show that ModelingAgent substantially\noutperforms strong baselines and often produces solutions indistinguishable\nfrom those of human experts. Together, our work provides a comprehensive\nframework for evaluating and advancing real-world problem-solving in\nopen-ended, interdisciplinary modeling challenges.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "36 Pages, 26 Figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15068v1",
    "published_date": "2025-05-21 03:33:23 UTC",
    "updated_date": "2025-05-21 03:33:23 UTC"
  },
  {
    "arxiv_id": "2505.15065v1",
    "title": "The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support",
    "authors": [
      "Suhas BN",
      "Yash Mahajan",
      "Dominik Mattioli",
      "Andrew M. Sherrill",
      "Rosa I. Arriaga",
      "Chris W. Wiese",
      "Saeed Abdullah"
    ],
    "abstract": "Can small language models with 0.5B to 5B parameters meaningfully engage in\ntrauma-informed, empathetic dialogue for individuals with PTSD? We address this\nquestion by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning\n500 diverse PTSD client personas and grounded in a three-factor empathy model:\nemotion recognition, distress normalization, and supportive reflection. All\nscenarios and reference responses were reviewed for realism and trauma\nsensitivity by a clinical psychologist specializing in PTSD. We evaluate eight\nsmall language models before and after fine-tuning, comparing their outputs to\na frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and\nautomatic metrics show that fine-tuning generally improves perceived empathy,\nbut gains are highly scenario- and user-dependent, with smaller models facing\nan empathy ceiling. Demographic analysis shows older adults value distress\nvalidation and graduate-educated users prefer nuanced replies, while gender\neffects are minimal. We highlight the limitations of automatic metrics and the\nneed for context- and user-aware system design. Our findings, along with the\nplanned release of TIDE, provide a foundation for building safe,\nresource-efficient, and ethically sound empathetic AI to supplement, not\nreplace, clinical mental health care.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "68T50, 68T05",
      "I.2.7; I.2.1; H.5.2"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2505.15065v1",
    "published_date": "2025-05-21 03:32:46 UTC",
    "updated_date": "2025-05-21 03:32:46 UTC"
  },
  {
    "arxiv_id": "2505.15062v1",
    "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning",
    "authors": [
      "Jiashu He",
      "Jinxuan Fan",
      "Bowen Jiang",
      "Ignacio Houine",
      "Dan Roth",
      "Alejandro Ribeiro"
    ],
    "abstract": "When addressing complex questions that require new information, people often\nassociate the question with existing knowledge to derive a sensible answer. For\ninstance, when evaluating whether melatonin aids insomnia, one might associate\n\"hormones helping mental disorders\" with \"melatonin being a hormone and\ninsomnia a mental disorder\" to complete the reasoning. Large Language Models\n(LLMs) also require such associative thinking, particularly in resolving\nscientific inquiries when retrieved knowledge is insufficient and does not\ndirectly answer the question. Graph Inspired Veracity Extrapolation (GIVE)\naddresses this by using a knowledge graph (KG) to extrapolate structured\nknowledge. However, it involves the construction and pruning of many\nhypothetical triplets, which limits efficiency and generalizability. We propose\nSelf-GIVE, a retrieve-RL framework that enhances LLMs with automatic\nassociative thinking through reinforcement learning. Self-GIVE extracts\nstructured information and entity sets to assist the model in linking to the\nqueried concepts. We address GIVE's key limitations: (1) extensive LLM calls\nand token overhead for knowledge extrapolation, (2) difficulty in deploying on\nsmaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate\nknowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE\nwith a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B\nmodels by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and\n$\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging\nbiomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or\noutperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\\%.\nSelf-GIVE enhances the scalable integration of structured retrieval and\nreasoning with associative thinking.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15062v1",
    "published_date": "2025-05-21 03:30:55 UTC",
    "updated_date": "2025-05-21 03:30:55 UTC"
  },
  {
    "arxiv_id": "2505.15058v1",
    "title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars",
    "authors": [
      "Tianbao Zhang",
      "Jian Zhao",
      "Yuer Li",
      "Zheng Zhu",
      "Ping Hu",
      "Zhaoxin Fan",
      "Wenjun Wu",
      "Xuelong Li"
    ],
    "abstract": "Whole-body audio-driven avatar pose and expression generation is a critical\ntask for creating lifelike digital humans and enhancing the capabilities of\ninteractive virtual agents, with wide-ranging applications in virtual reality,\ndigital entertainment, and remote communication. Existing approaches often\ngenerate audio-driven facial expressions and gestures independently, which\nintroduces a significant limitation: the lack of seamless coordination between\nfacial and gestural elements, resulting in less natural and cohesive\nanimations. To address this limitation, we propose AsynFusion, a novel\nframework that leverages diffusion transformers to achieve harmonious\nexpression and gesture synthesis. The proposed method is built upon a\ndual-branch DiT architecture, which enables the parallel generation of facial\nexpressions and gestures. Within the model, we introduce a Cooperative\nSynchronization Module to facilitate bidirectional feature interaction between\nthe two modalities, and an Asynchronous LCM Sampling strategy to reduce\ncomputational overhead while maintaining high-quality outputs. Extensive\nexperiments demonstrate that AsynFusion achieves state-of-the-art performance\nin generating real-time, synchronized whole-body animations, consistently\noutperforming existing methods in both quantitative and qualitative\nevaluations.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "eess.AS",
      "68T10"
    ],
    "primary_category": "cs.SD",
    "comment": "11pages, conference",
    "pdf_url": "http://arxiv.org/pdf/2505.15058v1",
    "published_date": "2025-05-21 03:28:53 UTC",
    "updated_date": "2025-05-21 03:28:53 UTC"
  },
  {
    "arxiv_id": "2505.15054v1",
    "title": "MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation",
    "authors": [
      "Feiyang Cai",
      "Jiahui Bai",
      "Tao Tang",
      "Joshua Luo",
      "Tianyu Zhu",
      "Ling Liu",
      "Feng Luo"
    ],
    "abstract": "Precise recognition, editing, and generation of molecules are essential\nprerequisites for both chemists and AI systems tackling various chemical tasks.\nWe present MolLangBench, a comprehensive benchmark designed to evaluate\nfundamental molecule-language interface tasks: language-prompted molecular\nstructure recognition, editing, and generation. To ensure high-quality,\nunambiguous, and deterministic outputs, we construct the recognition tasks\nusing automated cheminformatics tools, and curate editing and generation tasks\nthrough rigorous expert annotation and validation. MolLangBench supports the\nevaluation of models that interface language with different molecular\nrepresentations, including linear strings, molecular images, and molecular\ngraphs. Evaluations of state-of-the-art models reveal significant limitations:\nthe strongest model (o3) achieves $79.2\\%$ and $78.5\\%$ accuracy on recognition\nand editing tasks, which are intuitively simple for humans, and performs even\nworse on the generation task, reaching only $29.0\\%$ accuracy. These results\nhighlight the shortcomings of current AI systems in handling even preliminary\nmolecular recognition and manipulation tasks. We hope MolLangBench will\ncatalyze further research toward more effective and reliable AI systems for\nchemical applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15054v1",
    "published_date": "2025-05-21 03:22:01 UTC",
    "updated_date": "2025-05-21 03:22:01 UTC"
  },
  {
    "arxiv_id": "2505.15049v1",
    "title": "Towards a Working Definition of Designing Generative User Interfaces",
    "authors": [
      "Kyungho Lee"
    ],
    "abstract": "Generative UI is transforming interface design by facilitating AI-driven\ncollaborative workflows between designers and computational systems. This study\nestablishes a working definition of Generative UI through a multi-method\nqualitative approach, integrating insights from a systematic literature review\nof 127 publications, expert interviews with 18 participants, and analyses of 12\ncase studies. Our findings identify five core themes that position Generative\nUI as an iterative and co-creative process. We highlight emerging design\nmodels, including hybrid creation, curation-based workflows, and AI-assisted\nrefinement strategies. Additionally, we examine ethical challenges, evaluation\ncriteria, and interaction models that shape the field. By proposing a\nconceptual foundation, this study advances both theoretical discourse and\npractical implementation, guiding future HCI research toward responsible and\neffective generative UI design practices.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; D.2.2; H.1.2; I.3.6"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15049v1",
    "published_date": "2025-05-21 03:14:09 UTC",
    "updated_date": "2025-05-21 03:14:09 UTC"
  },
  {
    "arxiv_id": "2505.15047v1",
    "title": "PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration",
    "authors": [
      "Yingming Pu",
      "Tao Lin",
      "Hongyu Chen"
    ],
    "abstract": "Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate\nremarkable potential for scientific discovery. Existing approaches, however,\noften automate scientific discovery using predefined workflows that lack\nrationality constraints. This often leads to aimless hypothesizing and a\nfailure to consistently link hypotheses with evidence, thereby hindering\nsystematic uncertainty reduction. Overcoming these limitations fundamentally\nrequires systematic uncertainty reduction. We introduce \\texttt{PiFlow}, an\ninformation-theoretical framework, treating automated scientific discovery as a\nstructured uncertainty reduction problem guided by principles (e.g., scientific\nlaws). In evaluations across three distinct scientific domains -- discovering\nnanomaterial structures, bio-molecules, and superconductor candidates with\ntargeted properties -- our method significantly improves discovery efficiency,\nreflected by a 73.55\\% increase in the Area Under the Curve (AUC) of property\nvalues versus exploration steps, and enhances solution quality by 94.06\\%\ncompared to a vanilla agent system. Overall, \\texttt{PiFlow} serves as a\nPlug-and-Play method, establishing a novel paradigm shift in highly efficient\nautomated scientific discovery, paving the way for more robust and accelerated\nAI-driven research. Code is publicly available at our\n\\href{https://github.com/amair-lab/PiFlow}{GitHub}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15047v1",
    "published_date": "2025-05-21 03:09:39 UTC",
    "updated_date": "2025-05-21 03:09:39 UTC"
  },
  {
    "arxiv_id": "2505.15046v2",
    "title": "ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding",
    "authors": [
      "Yifan Wu",
      "Lutao Yan",
      "Leixian Shen",
      "Yinan Mei",
      "Jiannan Wang",
      "Yuyu Luo"
    ],
    "abstract": "The emergence of Multi-modal Large Language Models (MLLMs) presents new\nopportunities for chart understanding. However, due to the fine-grained nature\nof these tasks, applying MLLMs typically requires large, high-quality datasets\nfor task-specific fine-tuning, leading to high data collection and training\ncosts. To address this, we propose ChartCards, a unified chart-metadata\ngeneration framework for multi-task chart understanding. ChartCards\nsystematically synthesizes various chart information, including data tables,\nvisualization code, visual elements, and multi-dimensional semantic captions.\nBy structuring this information into organized metadata, ChartCards enables a\nsingle chart to support multiple downstream tasks, such as text-to-chart\nretrieval, chart summarization, chart-to-table conversion, chart description,\nand chart question answering. Using ChartCards, we further construct MetaChart,\na large-scale high-quality dataset containing 10,862 data tables, 85K charts,\nand 170 K high-quality chart captions. We validate the dataset through\nqualitative crowdsourcing evaluations and quantitative fine-tuning experiments\nacross various chart understanding tasks. Fine-tuning six different models on\nMetaChart resulted in an average performance improvement of 5% across all\ntasks. The most notable improvements are seen in text-to-chart retrieval and\nchart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements\nof 17% and 28%, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15046v2",
    "published_date": "2025-05-21 03:07:47 UTC",
    "updated_date": "2025-05-22 15:16:47 UTC"
  },
  {
    "arxiv_id": "2505.15044v1",
    "title": "Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment",
    "authors": [
      "Ze Wang",
      "Jingang Qu",
      "Zhenyu Gao",
      "Pascal Morin"
    ],
    "abstract": "This work demonstrates an airflow inertial based odometry system with\nmulti-sensor data fusion, including thermal anemometer, IMU, ESC, and\nbarometer. This goal is challenging because low-cost IMUs and barometers have\nsignificant bias, and anemometer measurements are very susceptible to\ninterference from spinning propellers and ground effects. We employ a GRU-based\ndeep neural network to estimate relative air speed from noisy and disturbed\nanemometer measurements, and an observer with bias model to fuse the sensor\ndata and thus estimate the state of aerial vehicle. A complete flight data,\nincluding takeoff and landing on the ground, shows that the approach is able to\ndecouple the downwash induced wind speed caused by propellers and the ground\neffect, and accurately estimate the flight speed in a wind-free indoor\nenvironment. IMU, and barometer bias are effectively estimated, which\nsignificantly reduces the position integration drift, which is only 5.7m for\n203s manual random flight. The open source is available on\nhttps://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15044v1",
    "published_date": "2025-05-21 02:53:49 UTC",
    "updated_date": "2025-05-21 02:53:49 UTC"
  },
  {
    "arxiv_id": "2505.15039v1",
    "title": "LogiCase: Effective Test Case Generation from Logical Description in Competitive Programming",
    "authors": [
      "Sicheol Sung",
      "Aditi",
      "Dogyu kim",
      "Yo-Sub Han",
      "Sang-Ki Ko"
    ],
    "abstract": "Automated Test Case Generation (ATCG) is crucial for evaluating software\nreliability, particularly in competitive programming where robust algorithm\nassessments depend on diverse and accurate test cases. However, existing ATCG\nmethods often fail to meet complex specifications or generate effective corner\ncases, limiting their utility. In this work, we introduce Context-Free Grammars\nwith Counters (CCFGs), a formalism that captures both syntactic and semantic\nstructures in input specifications. Using a fine-tuned CodeT5 model, we\ntranslate natural language input specifications into CCFGs, enabling the\nsystematic generation of high-quality test cases. Experiments on the\nCodeContests dataset demonstrate that CCFG-based test cases outperform baseline\nmethods in identifying incorrect algorithms, achieving significant gains in\nvalidity and effectiveness. Our approach provides a scalable and reliable\ngrammar-driven framework for enhancing automated competitive programming\nevaluations.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15039v1",
    "published_date": "2025-05-21 02:48:01 UTC",
    "updated_date": "2025-05-21 02:48:01 UTC"
  },
  {
    "arxiv_id": "2505.15038v1",
    "title": "Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering",
    "authors": [
      "Haiyan Zhao",
      "Xuansheng Wu",
      "Fan Yang",
      "Bo Shen",
      "Ninghao Liu",
      "Mengnan Du"
    ],
    "abstract": "Linear Concept Vectors have proven effective for steering large language\nmodels (LLMs). While existing approaches like linear probing and\ndifference-in-means derive these vectors from LLM hidden representations,\ndiverse data introduces noises (i.e., irrelevant features) that challenge\nsteering robustness. To address this, we propose Sparse Autoencoder-Denoised\nConcept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy\nfeatures from hidden representations. When applied to linear probing and\ndifference-in-means, our method improves their steering success rates. We\nvalidate our noise hypothesis through counterfactual experiments and feature\nvisualizations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 5 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2505.15038v1",
    "published_date": "2025-05-21 02:45:11 UTC",
    "updated_date": "2025-05-21 02:45:11 UTC"
  },
  {
    "arxiv_id": "2505.15036v1",
    "title": "Fault-Tolerant Multi-Robot Coordination with Limited Sensing within Confined Environments",
    "authors": [
      "Kehinde O. Aina",
      "Hosain Bagheri",
      "Daniel I. Goldman"
    ],
    "abstract": "As robots are increasingly deployed to collaborate on tasks within shared\nworkspaces and resources, the failure of an individual robot can critically\naffect the group's performance. This issue is particularly challenging when\nrobots lack global information or direct communication, relying instead on\nsocial interaction for coordination and to complete their tasks. In this study,\nwe propose a novel fault-tolerance technique leveraging physical contact\ninteractions in multi-robot systems, specifically under conditions of limited\nsensing and spatial confinement. We introduce the \"Active Contact Response\"\n(ACR) method, where each robot modulates its behavior based on the likelihood\nof encountering an inoperative (faulty) robot. Active robots are capable of\ncollectively repositioning stationary and faulty peers to reduce obstructions\nand maintain optimal group functionality. We implement our algorithm in a team\nof autonomous robots, equipped with contact-sensing and collision-tolerance\ncapabilities, tasked with collectively excavating cohesive model pellets.\nExperimental results indicate that the ACR method significantly improves the\nsystem's recovery time from robot failures, enabling continued collective\nexcavation with minimal performance degradation. Thus, this work demonstrates\nthe potential of leveraging local, social, and physical interactions to enhance\nfault tolerance and coordination in multi-robot systems operating in\nconstrained and extreme environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "15 pages, 4 figures. Accepted to DARS 2024 (Distributed Autonomous\n  Robotic Systems), to appear in Springer Proceedings in Advanced Robotics",
    "pdf_url": "http://arxiv.org/pdf/2505.15036v1",
    "published_date": "2025-05-21 02:43:36 UTC",
    "updated_date": "2025-05-21 02:43:36 UTC"
  },
  {
    "arxiv_id": "2505.15034v1",
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning",
    "authors": [
      "Kaiwen Zha",
      "Zhengqi Gao",
      "Maohao Shen",
      "Zhang-Wei Hong",
      "Duane S. Boning",
      "Dina Katabi"
    ],
    "abstract": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Tech report. The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2505.15034v1",
    "published_date": "2025-05-21 02:43:15 UTC",
    "updated_date": "2025-05-21 02:43:15 UTC"
  },
  {
    "arxiv_id": "2505.15033v1",
    "title": "Toward Task Capable Active Matter: Learning to Avoid Clogging in Confined Collectives via Collisions",
    "authors": [
      "Kehinde O. Aina",
      "Ram Avinery",
      "Hui-Shun Kuan",
      "Meredith D. Betterton",
      "Michael A. D. Goodisman",
      "Daniel I. Goldman"
    ],
    "abstract": "Social organisms which construct nests consisting of tunnels and chambers\nnecessarily navigate confined and crowded conditions. Unlike low-density\ncollectives like bird flocks and insect swarms, in which hydrodynamic and\nstatistical phenomena dominate, the physics of glasses and supercooled fluids\nis important to understand clogging behaviors in high-density collectives. Our\nprevious work revealed that fire ants flowing in confined tunnels utilize\ndiverse behaviors like unequal workload distributions, spontaneous direction\nreversals, and limited interaction times to mitigate clogging and jamming and\nthus maintain functional flow; implementation of similar rules in a small\nrobophysical swarm led to high performance through spontaneous dissolution of\nclogs and clusters. However, how the insects learn such behaviors, and how we\ncan develop \"task capable\" active matter in such regimes, remains a challenge\nin part because interaction dynamics are dominated by local, time-consuming\ncollisions and no single agent can guide the entire collective. Here, we\nhypothesized that effective flow and clog mitigation could emerge purely\nthrough local learning. We tasked small groups of robots with pellet excavation\nin a narrow tunnel, allowing them to modify reversal probabilities over time.\nInitially, robots had equal probabilities and clogs were common. Reversals\nimproved flow. When reversal probabilities adapted via collisions and noisy\ntunnel length estimates, workload inequality and performance improved. Our\nrobophysical study of an excavating swarm shows that, despite the seeming\ncomplexity and difficulty of the task, simple learning rules can mitigate or\nleverage unavoidable features in task-capable dense active matter, leading to\nhypotheses for dense biological and robotic swarms.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "13 pages, 9 figures. Published in Frontiers in Physics, Social\n  Physics section. Includes experimental and simulation analysis of multi-robot\n  excavation using decentralized learning",
    "pdf_url": "http://arxiv.org/pdf/2505.15033v1",
    "published_date": "2025-05-21 02:42:32 UTC",
    "updated_date": "2025-05-21 02:42:32 UTC"
  },
  {
    "arxiv_id": "2505.15031v1",
    "title": "Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI",
    "authors": [
      "Wenqing Wu",
      "Haixu Xi",
      "Chengzhi Zhang"
    ],
    "abstract": "Peer review is vital in academia for evaluating research quality. Top AI\nconferences use reviewer confidence scores to ensure review reliability, but\nexisting studies lack fine-grained analysis of text-score consistency,\npotentially missing key details. This work assesses consistency at word,\nsentence, and aspect levels using deep learning and NLP conference review data.\nWe employ deep learning to detect hedge sentences and aspects, then analyze\nreport length, hedge word/sentence frequency, aspect mentions, and sentiment to\nevaluate text-score alignment. Correlation, significance, and regression tests\nexamine confidence scores' impact on paper outcomes. Results show high\ntext-score consistency across all levels, with regression revealing higher\nconfidence scores correlate with paper rejection, validating expert assessments\nand peer review fairness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15031v1",
    "published_date": "2025-05-21 02:26:47 UTC",
    "updated_date": "2025-05-21 02:26:47 UTC"
  },
  {
    "arxiv_id": "2505.15023v1",
    "title": "Towards a Science of Causal Interpretability in Deep Learning for Software Engineering",
    "authors": [
      "David N. Palacio"
    ],
    "abstract": "This dissertation addresses achieving causal interpretability in Deep\nLearning for Software Engineering (DL4SE). While Neural Code Models (NCMs) show\nstrong performance in automating software tasks, their lack of transparency in\ncausal relationships between inputs and outputs limits full understanding of\ntheir capabilities. To build trust in NCMs, researchers and practitioners must\nexplain code predictions. Associational interpretability, which identifies\ncorrelations, is often insufficient for tasks requiring intervention and change\nanalysis. To address this, the dissertation introduces DoCode, a novel post hoc\ninterpretability method for NCMs. DoCode uses causal inference to provide\nprogramming language-oriented explanations of model predictions. It follows a\nfour-step pipeline: modeling causal problems using Structural Causal Models\n(SCMs), identifying the causal estimand, estimating effects with metrics like\nAverage Treatment Effect (ATE), and refuting effect estimates. Its framework is\nextensible, with an example that reduces spurious correlations by grounding\nexplanations in programming language properties. A case study on deep code\ngeneration across interpretability scenarios and various deep learning\narchitectures demonstrates DoCode's benefits. Results show NCMs' sensitivity to\ncode syntax changes and their ability to learn certain programming concepts\nwhile minimizing confounding bias. The dissertation also examines associational\ninterpretability as a foundation, analyzing software information's causal\nnature using tools like COMET and TraceXplainer for traceability. It highlights\nthe need to identify code confounders and offers practical guidelines for\napplying causal interpretability to NCMs, contributing to more trustworthy AI\nin software engineering.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "PhD thesis, To appear in ProQuest",
    "pdf_url": "http://arxiv.org/pdf/2505.15023v1",
    "published_date": "2025-05-21 02:13:11 UTC",
    "updated_date": "2025-05-21 02:13:11 UTC"
  },
  {
    "arxiv_id": "2505.15011v1",
    "title": "HAVA: Hybrid Approach to Value-Alignment through Reward Weighing for Reinforcement Learning",
    "authors": [
      "Kryspin Varys",
      "Federico Cerutti",
      "Adam Sobey",
      "Timothy J. Norman"
    ],
    "abstract": "Our society is governed by a set of norms which together bring about the\nvalues we cherish such as safety, fairness or trustworthiness. The goal of\nvalue-alignment is to create agents that not only do their tasks but through\ntheir behaviours also promote these values. Many of the norms are written as\nlaws or rules (legal / safety norms) but even more remain unwritten (social\nnorms). Furthermore, the techniques used to represent these norms also differ.\nSafety / legal norms are often represented explicitly, for example, in some\nlogical language while social norms are typically learned and remain hidden in\nthe parameter space of a neural network. There is a lack of approaches in the\nliterature that could combine these various norm representations into a single\nalgorithm. We propose a novel method that integrates these norms into the\nreinforcement learning process. Our method monitors the agent's compliance with\nthe given norms and summarizes it in a quantity we call the agent's reputation.\nThis quantity is used to weigh the received rewards to motivate the agent to\nbecome value-aligned. We carry out a series of experiments including a\ncontinuous state space traffic problem to demonstrate the importance of the\nwritten and unwritten norms and show how our method can find the value-aligned\npolicies. Furthermore, we carry out ablations to demonstrate why it is better\nto combine these two groups of norms rather than using either separately.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15011v1",
    "published_date": "2025-05-21 01:32:54 UTC",
    "updated_date": "2025-05-21 01:32:54 UTC"
  },
  {
    "arxiv_id": "2505.15009v1",
    "title": "One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks",
    "authors": [
      "Quan Nguyen",
      "Thanh Nguyen-Tang"
    ],
    "abstract": "We study the approximation capabilities and on-convergence behaviors of\none-layer transformers on the noiseless and noisy in-context reasoning of\nnext-token prediction. Existing theoretical results focus on understanding the\nin-context reasoning behaviors for either the first gradient step or when the\nnumber of samples is infinite. Furthermore, no convergence rates nor\ngeneralization abilities were known. Our work addresses these gaps by showing\nthat there exists a class of one-layer transformers that are provably\nBayes-optimal with both linear and ReLU attention. When being trained with\ngradient descent, we show via a finite-sample analysis that the expected loss\nof these transformers converges at linear rate to the Bayes risk. Moreover, we\nprove that the trained models generalize to unseen samples as well as exhibit\nlearning behaviors that were empirically observed in previous works. Our\ntheoretical findings are further supported by extensive empirical validations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.15009v1",
    "published_date": "2025-05-21 01:26:44 UTC",
    "updated_date": "2025-05-21 01:26:44 UTC"
  },
  {
    "arxiv_id": "2505.15008v1",
    "title": "Know When to Abstain: Optimal Selective Classification with Likelihood Ratios",
    "authors": [
      "Alvin Heng",
      "Harold Soh"
    ],
    "abstract": "Selective classification enhances the reliability of predictive models by\nallowing them to abstain from making uncertain predictions. In this work, we\nrevisit the design of optimal selection functions through the lens of the\nNeyman--Pearson lemma, a classical result in statistics that characterizes the\noptimal rejection rule as a likelihood ratio test. We show that this\nperspective not only unifies the behavior of several post-hoc selection\nbaselines, but also motivates new approaches to selective classification which\nwe propose here. A central focus of our work is the setting of covariate shift,\nwhere the input distribution at test time differs from that at training. This\nrealistic and challenging scenario remains relatively underexplored in the\ncontext of selective classification. We evaluate our proposed methods across a\nrange of vision and language tasks, including both supervised learning and\nvision-language models. Our experiments demonstrate that our\nNeyman--Pearson-informed methods consistently outperform existing baselines,\nindicating that likelihood ratio-based selection offers a robust mechanism for\nimproving selective classification under covariate shifts. Our code is publicly\navailable at https://github.com/clear-nus/sc-likelihood-ratios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.15008v1",
    "published_date": "2025-05-21 01:26:21 UTC",
    "updated_date": "2025-05-21 01:26:21 UTC"
  },
  {
    "arxiv_id": "2505.15002v1",
    "title": "Unraveling the iterative CHAD",
    "authors": [
      "Fernando Lucatelli Nunes",
      "Gordon Plotkin",
      "Matthijs Vákár"
    ],
    "abstract": "Combinatory Homomorphic Automatic Differentiation (CHAD) was originally\nformulated as a semantics-driven source transformation for reverse-mode AD in\ntotal programming languages. We extend this framework to partial languages with\nfeatures such as potentially non-terminating operations, real-valued\nconditionals, and iteration constructs like while-loops, while preserving\nCHAD's structure-preserving semantics principle. A key contribution is the\nintroduction of iteration-extensive indexed categories, which allow iteration\nin the base category to lift to parameterized initial algebras in the indexed\ncategory. This enables iteration to be interpreted in the Grothendieck\nconstruction of the target language in a principled way. The resulting fibred\niterative structure cleanly models iteration in the categorical semantics.\nConsequently, the extended CHAD transformation remains the unique\nstructure-preserving functor (an iterative Freyd category morphism) from the\nfreely generated iterative Freyd category of the source language to the\nGrothendieck construction of the target's syntactic semantics, mapping each\nprimitive operation to its derivative. We prove the correctness of this\ntransformation using the universal property of the source language's syntax,\nshowing that the transformed programs compute correct reverse-mode derivatives.\nOur development also contributes to understanding iteration constructs within\ndependently typed languages and categories of containers. As our primary\nmotivation and application, we generalize CHAD to languages with data types,\npartial features, and iteration, providing the first rigorous categorical\nsemantics for reverse-mode CHAD in such settings and formally guaranteeing the\ncorrectness of the source-to-source CHAD technique.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG",
      "math.CT",
      "math.LO",
      "18C10, 18C15, 18C20, 18D05, 03B70, 03F52, 68Q55, 68N18, 68T07",
      "F.3.2; F.3.3; D.3.1; D.3.2; D.2.4; G.4; I.2.3; I.2.6; G.1.10"
    ],
    "primary_category": "cs.PL",
    "comment": "57 pages",
    "pdf_url": "http://arxiv.org/pdf/2505.15002v1",
    "published_date": "2025-05-21 01:10:40 UTC",
    "updated_date": "2025-05-21 01:10:40 UTC"
  },
  {
    "arxiv_id": "2505.14999v1",
    "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision",
    "authors": [
      "Eric Hanchen Jiang",
      "Haozheng Luo",
      "Shengyuan Pang",
      "Xiaomin Li",
      "Zhenting Qi",
      "Hengli Li",
      "Cheng-Fu Yang",
      "Zongyu Lin",
      "Xinfeng Li",
      "Hao Xu",
      "Kai-Wei Chang",
      "Ying Nian Wu"
    ],
    "abstract": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14999v1",
    "published_date": "2025-05-21 01:06:29 UTC",
    "updated_date": "2025-05-21 01:06:29 UTC"
  },
  {
    "arxiv_id": "2505.14996v1",
    "title": "Meta-Design Matters: A Self-Design Multi-Agent System",
    "authors": [
      "Zixuan Ke",
      "Austin Xu",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "abstract": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large\nLanguage Models (LLMs) hold significant potential for tackling complex tasks.\nHowever, most current MAS depend on manually designed agent roles and\ncommunication protocols. These manual designs often fail to align with the\nunderlying LLMs' strengths and struggle to adapt to novel tasks. Recent\nautomatic MAS approaches attempt to mitigate these limitations but typically\nnecessitate a validation-set for tuning and yield static MAS designs lacking\nadaptability during inference. We introduce SELF-MAS, the first\nself-supervised, inference-time only framework for automatic MAS design.\nSELF-MAS employs meta-level design to iteratively generate, evaluate, and\nrefine MAS configurations tailored to each problem instance, without requiring\na validation set. Critically, it enables dynamic agent composition and problem\ndecomposition through meta-feedback on solvability and completeness.\nExperiments across math, graduate-level QA, and software engineering\nbenchmarks, using both closed-source and open-source LLM back-bones of varying\nsizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS\nbaselines, achieving a 7.44% average accuracy improvement over the next\nstrongest baseline while maintaining cost-efficiency. These findings underscore\nthe promise of meta-level self-supervised design for creating effective and\nadaptive MAS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14996v1",
    "published_date": "2025-05-21 00:56:09 UTC",
    "updated_date": "2025-05-21 00:56:09 UTC"
  },
  {
    "arxiv_id": "2505.14983v1",
    "title": "Toward Informed AV Decision-Making: Computational Model of Well-being and Trust in Mobility",
    "authors": [
      "Zahra Zahedi",
      "Shashank Mehrotra",
      "Teruhisa Misu",
      "Kumar Akash"
    ],
    "abstract": "For future human-autonomous vehicle (AV) interactions to be effective and\nsmooth, human-aware systems that analyze and align human needs with automation\ndecisions are essential. Achieving this requires systems that account for human\ncognitive states. We present a novel computational model in the form of a\nDynamic Bayesian Network (DBN) that infers the cognitive states of both AV\nusers and other road users, integrating this information into the AV's\ndecision-making process. Specifically, our model captures the well-being of\nboth an AV user and an interacting road user as cognitive states alongside\ntrust. Our DBN models infer beliefs over the AV user's evolving well-being,\ntrust, and intention states, as well as the possible well-being of other road\nusers, based on observed interaction experiences. Using data collected from an\ninteraction study, we refine the model parameters and empirically assess its\nperformance. Finally, we extend our model into a causal inference model (CIM)\nframework for AV decision-making, enabling the AV to enhance user well-being\nand trust while balancing these factors with its own operational costs and the\nwell-being of interacting road users. Our evaluation demonstrates the model's\neffectiveness in accurately predicting user's states and guiding informed,\nhuman-centered AV decisions.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14983v1",
    "published_date": "2025-05-21 00:02:39 UTC",
    "updated_date": "2025-05-21 00:02:39 UTC"
  }
]
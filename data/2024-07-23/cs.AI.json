{
  "date": "2024-07-23",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-23 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的安全性、知识图谱嵌入、多模态处理以及医疗应用等领域，强调了大型语言模型（LLMs）的攻击防御、强化学习优化和多模态情感识别等关键问题。令人印象深刻的文章包括 OpenHands 的 AI 代理平台（作者包括知名学者如 Ion Stoica 和 Matei Zaharia），以及 LLM 在芯片设计和医疗诊断中的创新应用，这些工作展示了 AI 在实际场景中的潜力。\n\n下面，我将逐一简要概述部分论文，先优先讨论重要、话题性强的文章（如 LLM 安全、医疗 AI 和多模态模型），并将相关主题归类。其他较次要的论文（如纯算法优化或特定领域方法）将快速掠过，只提核心贡献，以控制篇幅。\n\n### LLM 安全与应用\n- **Generation Constraint Scaling Can Mitigate Hallucination（生成约束缩放可缓解幻觉）**  \n  这篇论文提出了一种训练-free 方法，通过缩放记忆增强 LLM 的读取向量来减少幻觉问题。主要贡献是几何启发的优化，在生成 Wikipedia 风格的传记时，超越了现有 LLM 编辑方法，提升了生成质量和效率。\n\n- **Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design（罗马不是一天建成的：用于 LLM 芯片设计的层次提示）**  \n  作者包括知名学者 Ramesh Karri 和 Siddharth Garg。该工作引入层次提示技术，帮助 LLM 处理复杂硬件设计任务。通过分层方法，实现了无反馈的处理器设计，显著提高了开源 LLM 的性能。\n\n- **Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach（检索增强生成还是长上下文 LLM？一个综合研究和混合方法）**  \n  论文比较了 RAG 和长上下文 LLM，提出 Self-Route 方法动态路由查询。关键发现是长上下文 LLM 性能更强，但 RAG 成本更低；混合方法在保持性能的同时降低了计算开销。\n\n- **Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack（使用终身 ICL 和任务堆栈测试长上下文 LLM）**  \n  该研究测试 LLM 在动态任务中的长上下文能力，引入 Lifelong ICL 基准。发现前沿模型如 GPT-4o 仍有 15% 的失败率，强调了鲁棒性和上下文利用的挑战。\n\n- **Course-Correction: Safety Alignment Using Synthetic Preferences（课程修正：使用合成偏好进行安全对齐）**  \n  论文提出一种 LLM 安全框架，使用合成数据集微调模型以实现及时纠错。主要发现是，该方法在抵抗越狱攻击时显著提升了准确性，同时保持了泛化性能。\n\n- **Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation（掩码图学习与循环对齐用于多模态对话情感识别）**  \n  这篇工作设计了 MGLRA 框架，通过循环模块对齐多模态特征，并使用掩码图神经网络融合信息。贡献在于提升了情感识别的鲁棒性，尤其在噪声数据下。\n\n### 多模态和视觉处理\n- **MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs（多模态 LLM 的比较推理基准）**  \n  论文构建了 MLLM-CompBench 基准，评估多模态 LLM 在视觉比较任务中的能力。发现模型如 GPT-4V 在属性比较上表现不足，强调了未来改进的需求。\n\n- **OpenHands: An Open Platform for AI Software Developers as Generalist Agents（OpenHands：AI 软件开发者的通用代理平台）**  \n  作者团队包括知名学者如 Ion Stoica 和 Matei Zaharia。该平台提供开源框架，支持 AI 代理的开发和评估，在软件任务中实现了高效协作。\n\n- **Prompt Injection Attacks on Large Language Models in Oncology（在肿瘤学中对 LLM 的提示注入攻击）**  \n  研究揭示了多模态 LLM 在医疗中的安全漏洞，通过提示注入攻击生成有害输出。贡献是警告了临床应用的潜在风险，并呼吁加强防御。\n\n### 医疗和可持续性应用\n- **Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses（基于知识驱动的 AI 生成数据用于精确可解释的乳腺超声诊断）**  \n  该论文提出 TAILOR 框架，使用知识图谱生成合成数据，提高乳腺癌诊断的准确性（AUC 达 85%）。主要发现是，它能辅助临床决策，尤其在低资源数据下。\n\n- **Quantum Computing for Climate Resilience and Sustainability Challenges（量子计算用于气候韧性和可持续性挑战）**  \n  工作探索量子机器学习在气候预测中的应用，展示了量子算法在优化资源管理方面的潜力。贡献在于提供了一个新框架，提升了传统方法的效率。\n\n其他论文，如那些专注于图神经网络（如第2、24）、强化学习（如第11、33）或特定优化（如第14、15），虽然有技术贡献，但相对次要，我将快速掠过：\n- **Balanced Multi-Relational Graph Clustering（平衡多关系图聚类）** 和 **A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in Hyperbolic Space（一个基于几何感知的算法学习层次嵌入）** 分别提出了新图聚类和嵌入方法，提升了聚类性能，但细节较专业，不展开。\n- **In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning（多目标强化学习的架构和损失函数搜索）** 探索了 MORL 的新架构，改善了学习动态，但影响局限于特定领域。\n\n总之，今天的论文突出了 AI 模型的安全性和实际应用潜力，但也暴露了如幻觉和攻击的挑战。重点工作如 OpenHands 和医疗诊断模型值得进一步关注，期待后续研究！（本快报覆盖了约20篇核心论文，其余快速概述，以保持简洁。）",
  "papers": [
    {
      "arxiv_id": "2407.16908v1",
      "title": "Generation Constraint Scaling Can Mitigate Hallucination",
      "title_zh": "翻译失败",
      "authors": [
        "Georgios Kollias",
        "Payel Das",
        "Subhajit Chaudhury"
      ],
      "abstract": "Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.",
      "tldr_zh": "本文提出了一种基于几何灵感的简单方法，通过缩放生成约束的读取向量(readout vector)来缓解大型语言模型(LLMs)中的幻觉(hallucination)问题，该方法在记忆增强LLM解码器中实现，无需任何训练。实验结果显示，该技术在生成维基百科式传记条目任务上，优于最先进的LLM编辑方法，不仅提升了生成质量，还降低了运行时复杂度。这种创新性策略为高效解决LLMs幻觉问题提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages; accepted at ICML 2024 Workshop on Large Language Models and\n  Cognition",
      "pdf_url": "http://arxiv.org/pdf/2407.16908v1",
      "published_date": "2024-07-23 23:58:19 UTC",
      "updated_date": "2024-07-23 23:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:08:44.496174"
    },
    {
      "arxiv_id": "2407.16863v1",
      "title": "Balanced Multi-Relational Graph Clustering",
      "title_zh": "翻译失败",
      "authors": [
        "Zhixiang Shen",
        "Haolan He",
        "Zhao Kang"
      ],
      "abstract": "Multi-relational graph clustering has demonstrated remarkable success in\nuncovering underlying patterns in complex networks. Representative methods\nmanage to align different views motivated by advances in contrastive learning.\nOur empirical study finds the pervasive presence of imbalance in real-world\ngraphs, which is in principle contradictory to the motivation of alignment. In\nthis paper, we first propose a novel metric, the Aggregation Class Distance, to\nempirically quantify structural disparities among different graphs. To address\nthe challenge of view imbalance, we propose Balanced Multi-Relational Graph\nClustering (BMGC), comprising unsupervised dominant view mining and dual\nsignals guided representation learning. It dynamically mines the dominant view\nthroughout the training process, synergistically improving clustering\nperformance with representation learning. Theoretical analysis ensures the\neffectiveness of dominant view mining. Extensive experiments and in-depth\nanalysis on real-world and synthetic datasets showcase that BMGC achieves\nstate-of-the-art performance, underscoring its superiority in addressing the\nview imbalance inherent in multi-relational graphs. The source code and\ndatasets are available at https://github.com/zxlearningdeep/BMGC.",
      "tldr_zh": "本研究发现，多关系图聚类（Multi-Relational Graph Clustering）中存在普遍的视图不平衡问题，这与基于对比学习（contrastive learning）的视图对齐方法相矛盾。为量化这种结构差异，论文提出了一种新指标Aggregation Class Distance。随后，引入Balanced Multi-Relational Graph Clustering (BMGC)框架，包括无监督的主导视图挖掘和双信号引导的表示学习，能够动态优化主导视图并提升聚类性能。理论分析证实了主导视图挖掘的有效性，而在真实和合成数据集上的实验显示，BMGC 实现了最先进（state-of-the-art）的性能，成功解决了多关系图的视图不平衡挑战。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by ACM Multimedia 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16863v1",
      "published_date": "2024-07-23 22:11:13 UTC",
      "updated_date": "2024-07-23 22:11:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:07.094030"
    },
    {
      "arxiv_id": "2407.18276v3",
      "title": "Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design",
      "title_zh": "翻译失败",
      "authors": [
        "Andre Nakkab",
        "Sai Qian Zhang",
        "Ramesh Karri",
        "Siddharth Garg"
      ],
      "abstract": "Large Language Models (LLMs) are effective in computer hardware synthesis via\nhardware description language (HDL) generation. However, LLM-assisted\napproaches for HDL generation struggle when handling complex tasks. We\nintroduce a suite of hierarchical prompting techniques which facilitate\nefficient stepwise design methods, and develop a generalizable automation\npipeline for the process. To evaluate these techniques, we present a benchmark\nset of hardware designs which have solutions with or without architectural\nhierarchy. Using these benchmarks, we compare various open-source and\nproprietary LLMs, including our own fine-tuned Code Llama-Verilog model. Our\nhierarchical methods automatically produce successful designs for complex\nhardware modules that standard flat prompting methods cannot achieve, allowing\nsmaller open-source LLMs to compete with large proprietary models. Hierarchical\nprompting reduces HDL generation time and yields savings on LLM costs. Our\nexperiments detail which LLMs are capable of which applications, and how to\napply hierarchical methods in various modes. We explore case studies of\ngenerating complex cores using automatic scripted hierarchical prompts,\nincluding the first-ever LLM-designed processor with no human feedback. Tools\nfor the Recurrent Optimization via Machine Editing (ROME) method can be found\nat https://github.com/ajn313/ROME-LLM",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在硬件描述语言(HDL)生成中的局限性，提出了一种层次化提示技术，以支持高效的分步芯片设计流程，并开发了一个通用的自动化管道。研究者构建了一个基准数据集，评估各种开源和专有LLMs，包括微调后的Code Llama-Verilog模型，结果显示层次化方法能自动生成复杂硬件模块，而标准平坦提示方法无法实现，从而让小型开源LLMs在性能上与大型专有模型竞争。实验证明，这种方法显著减少了HDL生成时间和LLMs成本，并在案例研究中成功创建了首个无需人类反馈的LLM设计处理器。工具可通过GitHub获取。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "Accepted at MLCAD '24. 10 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.18276v3",
      "published_date": "2024-07-23 21:18:31 UTC",
      "updated_date": "2024-09-09 20:01:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:12.043778"
    },
    {
      "arxiv_id": "2407.16840v1",
      "title": "Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low Resource Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Pai Zhu",
        "Dhruuv Agarwal",
        "Jacob W. Bartel",
        "Kurt Partridge",
        "Hyun Jin Park",
        "Quan Wang"
      ],
      "abstract": "One of the challenges in developing a high quality custom keyword spotting\n(KWS) model is the lengthy and expensive process of collecting training data\ncovering a wide range of languages, phrases and speaking styles. We introduce\nSynth4Kws - a framework to leverage Text to Speech (TTS) synthesized data for\ncustom KWS in different resource settings. With no real data, we found\nincreasing TTS phrase diversity and utterance sampling monotonically improves\nmodel performance, as evaluated by EER and AUC metrics over 11k utterances of\nthe speech command dataset. In low resource settings, with 50k real utterances\nas a baseline, we found using optimal amounts of TTS data can improve EER by\n30.1% and AUC by 46.7%. Furthermore, we mix TTS data with varying amounts of\nreal data and interpolate the real data needed to achieve various quality\ntargets. Our experiments are based on English and single word utterances but\nthe findings generalize to i18n languages and other keyword types.",
      "tldr_zh": "该研究提出Synth4Kws框架，利用Text to Speech (TTS)合成的语音数据来简化自定义关键词识别(KWS)模型的训练，特别是在资源有限的环境中，以解决收集多样化训练数据的挑战。框架通过增加TTS短语多样性和utterance采样，在无真实数据的情况下显著提升模型性能；在基于speech command数据集的评估中，EER和AUC指标得到改善。实验结果显示，在低资源设置下，使用最佳量的TTS数据与50k真实utterances结合，能将EER改善30.1%和AUC改善46.7%，且这些发现可推广到国际化语言和其他关键词类型。",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "primary_category": "eess.AS",
      "comment": "5 pages, 5 figures, 2 tables The paper is accepted in Interspeech\n  SynData4GenAI 2024 Workshop - https://syndata4genai.org/#call-for-papers",
      "pdf_url": "http://arxiv.org/pdf/2407.16840v1",
      "published_date": "2024-07-23 21:05:44 UTC",
      "updated_date": "2024-07-23 21:05:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:21.442600"
    },
    {
      "arxiv_id": "2407.16837v2",
      "title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs",
      "title_zh": "MLLM-CompBench：多模态",
      "authors": [
        "Jihyung Kil",
        "Zheda Mai",
        "Justin Lee",
        "Zihe Wang",
        "Kerrie Cheng",
        "Lemeng Wang",
        "Ye Liu",
        "Arpita Chowdhury",
        "Wei-Lun Chao"
      ],
      "abstract": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.",
      "tldr_zh": "该论文引入了MLLM-CompBench，一个专为评估多模态大型语言模型(MLLMs)的比较推理能力而设计的基准测试。基准测试通过视觉导向的问题，从八个维度（visual attribute, existence, state, emotion, temporality, spatiality, quantity, quality）挖掘并配对约40K对图像，这些图像来自多样视觉数据集，并利用CLIP相似度分数进行筛选，覆盖动物、时尚、体育等领域。实验评估了GPT-4V(ision)、Gemini-Pro和LLaVA-1.6等模型，结果显示这些MLLMs在比较能力方面存在显著缺陷。该基准测试不仅揭示了这些局限性，还为未来提升MLLMs的比较推理能力提供了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2407.16837v2",
      "published_date": "2024-07-23 21:02:38 UTC",
      "updated_date": "2025-01-13 05:04:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:32.973232"
    },
    {
      "arxiv_id": "2407.16834v1",
      "title": "A Multi-Level Hierarchical Framework for the Classification of Weather Conditions and Hazard Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Harish Neelam"
      ],
      "abstract": "This paper presents a multilevel hierarchical framework for the\nclassification of weather conditions and hazard prediction. In recent years,\nthe importance of data has grown significantly, with various types like text,\nnumbers, images, audio, and videos playing a key role. Among these, images make\nup a large portion of the data available. This application shows promise for\nvarious purposes, especially when combined with decision support systems for\ntraffic management, afforestation, and weather forecasting. It's particularly\nuseful in situations where traditional weather predictions are not very\naccurate, such as ensuring the safe operation of self driving cars in dangerous\nweather. While previous studies have looked at this topic with fewer\ncategories, this paper focuses on eleven specific types of weather images. The\ngoal is to create a model that can accurately predict weather conditions after\nbeing trained on a large dataset of images. Accuracy is crucial in real-life\nsituations to prevent accidents, making it the top priority for this paper.\nThis work lays the groundwork for future applications in weather prediction,\nespecially in situations where human expertise is not available or may be\nbiased. The framework, capable of classifying images into eleven weather\ncategories: dew, frost, glaze, rime, snow, hail, rain, lightning, rainbow, and\nsandstorm, provides real-time weather information with an accuracy of 0.9329.\nThe proposed framework addresses the growing need for accurate weather\nclassification and hazard prediction, offering a robust solution for various\napplications in the field.",
      "tldr_zh": "本论文提出了一种多级层次框架，用于天气条件分类和危险预测，旨在利用图像数据提升预测准确性，尤其适用于交通管理、植树造林和自动驾驶车辆的安全操作。框架针对11种特定天气类别（包括dew, frost, glaze, rime, snow, hail, rain, lightning, rainbow和sandstorm）进行训练和分类，通过大型图像数据集实现实时天气信息处理。实验结果显示，该框架的准确率达到0.9329，为未来依赖数据驱动的天气预测应用提供可靠基础，并减少人为偏差的风险。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16834v1",
      "published_date": "2024-07-23 20:55:25 UTC",
      "updated_date": "2024-07-23 20:55:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:44.659628"
    },
    {
      "arxiv_id": "2407.16833v2",
      "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuowan Li",
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Michael Bendersky"
      ],
      "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.",
      "tldr_zh": "这篇论文比较了 Retrieval Augmented Generation (RAG) 与 Long-Context (LC) LLMs 在处理长上下文时的性能，使用三个最新 LLMs 在各种公共数据集上进行基准测试。结果显示，当资源充足时，LC LLMs 在平均性能上 consistently outperforms RAG，但 RAG 的计算成本显著更低。作者提出了一种简单有效的 Self-Route 方法，通过模型自省来动态路由查询至 RAG 或 LC，从而显著降低计算成本，同时保持与 LC 相当的性能。该研究为 LLMs 的长上下文应用提供了实用指导。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024 industry track",
      "pdf_url": "http://arxiv.org/pdf/2407.16833v2",
      "published_date": "2024-07-23 20:51:52 UTC",
      "updated_date": "2024-10-17 17:51:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:09:57.074267"
    },
    {
      "arxiv_id": "2407.16831v1",
      "title": "Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design",
      "title_zh": "网络的网络：复杂度类原则应用于复合 AI 系统设计",
      "authors": [
        "Jared Quincy Davis",
        "Boris Hanin",
        "Lingjiao Chen",
        "Peter Bailis",
        "Ion Stoica",
        "Matei Zaharia"
      ],
      "abstract": "As practitioners seek to surpass the current reliability and quality frontier\nof monolithic models, Compound AI Systems consisting of many language model\ninference calls are increasingly employed. In this work, we construct systems,\nwhich we call Networks of Networks (NoNs) organized around the distinction\nbetween generating a proposed answer and verifying its correctness, a\nfundamental concept in complexity theory that we show empirically extends to\nLanguage Models (LMs). We introduce a verifier-based judge NoN with K\ngenerators, an instantiation of \"best-of-K\" or \"judge-based\" compound AI\nsystems. Through experiments on synthetic tasks such as prime factorization,\nand core benchmarks such as the MMLU, we demonstrate notable performance gains.\nFor instance, in factoring products of two 3-digit primes, a simple NoN\nimproves accuracy from 3.7\\% to 36.6\\%. On MMLU, a verifier-based judge\nconstruction with only 3 generators boosts accuracy over individual GPT-4-Turbo\ncalls by 2.8\\%. Our analysis reveals that these gains are most pronounced in\ndomains where verification is notably easier than generation--a\ncharacterization which we believe subsumes many reasoning and procedural\nknowledge tasks, but doesn't often hold for factual and declarative\nknowledge-based settings. For mathematical and formal logic reasoning-based\nsubjects of MMLU, we observe a 5-8\\% or higher gain, whilst no gain on others\nsuch as geography and religion. We provide key takeaways for ML practitioners,\nincluding the importance of considering verification complexity, the impact of\nwitness format on verifiability, and a simple test to determine the potential\nbenefit of this NoN approach for a given problem distribution. This work aims\nto inform future research and practice in the design of compound AI systems.",
      "tldr_zh": "本文提出 Networks of Networks (NoNs) 框架，将复杂性理论中的生成与验证概念应用于复合 AI 系统设计，旨在通过区分答案生成和正确性验证来提升系统性能。作者引入了一种基于验证器的 judge NoN，使用 K 个生成器（如“best-of-K”方法），并通过实验验证其在合成任务（如质因数分解）和基准测试（如 MMLU）上的效果，例如将质因数分解准确率从 3.7% 提高到 36.6%，并在 MMLU 上使 GPT-4-Turbo 的准确率提升 2.8%。研究发现，这种方法在验证较生成更容易的领域（如数学和形式逻辑推理）中收益更显著，提升可达 5-8%，而事实性知识任务（如地理）则无明显改善，并提供了实用建议，如评估验证复杂度和见证格式的影响。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16831v1",
      "published_date": "2024-07-23 20:40:37 UTC",
      "updated_date": "2024-07-23 20:40:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:10:11.138893"
    },
    {
      "arxiv_id": "2407.16828v3",
      "title": "Pareto Front Approximation for Multi-Objective Session-Based Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Timo Wilm",
        "Philipp Normann",
        "Felix Stepprath"
      ],
      "abstract": "This work introduces MultiTRON, an approach that adapts Pareto front\napproximation techniques to multi-objective session-based recommender systems\nusing a transformer neural network. Our approach optimizes trade-offs between\nkey metrics such as click-through and conversion rates by training on sampled\npreference vectors. A significant advantage is that after training, a single\nmodel can access the entire Pareto front, allowing it to be tailored to meet\nthe specific requirements of different stakeholders by adjusting an additional\ninput vector that weights the objectives. We validate the model's performance\nthrough extensive offline and online evaluation. For broader application and\nresearch, the source code is made available at\nhttps://github.com/otto-de/MultiTRON. The results confirm the model's ability\nto manage multiple recommendation objectives effectively, offering a flexible\ntool for diverse business needs.",
      "tldr_zh": "这篇论文提出 MultiTRON 方法，将 Pareto front 近似技术应用于多目标会话-based 推荐系统，使用 Transformer 神经网络，通过训练采样偏好向量来优化指标如点击率和转换率的权衡。MultiTRON 的关键优势在于，训练后一个模型即可访问整个 Pareto front，并通过调整输入向量来满足不同利益相关者的特定需求。实验通过广泛的离线和在线评估验证了其性能，并开源代码以支持进一步应用和研究。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at the Eighteenth ACM Conference on Recommender Systems\n  (RecSys '24)",
      "pdf_url": "http://arxiv.org/pdf/2407.16828v3",
      "published_date": "2024-07-23 20:38:23 UTC",
      "updated_date": "2025-03-30 12:29:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:10:20.190877"
    },
    {
      "arxiv_id": "2407.16822v1",
      "title": "AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical Knowledge Graphs and Data-Driven Quantification",
      "title_zh": "翻译失败",
      "authors": [
        "Yuheng Wang",
        "Tianze Yu",
        "Jiayue Cai",
        "Sunil Kalia",
        "Harvey Lui",
        "Z. Jane Wang",
        "Tim K. Lee"
      ],
      "abstract": "The 7-point checklist (7PCL) is widely used in dermoscopy to identify\nmalignant melanoma lesions needing urgent medical attention. It assigns point\nvalues to seven attributes: major attributes are worth two points each, and\nminor ones are worth one point each. A total score of three or higher prompts\nfurther evaluation, often including a biopsy. However, a significant limitation\nof current methods is the uniform weighting of attributes, which leads to\nimprecision and neglects their interconnections. Previous deep learning studies\nhave treated the prediction of each attribute with the same importance as\npredicting melanoma, which fails to recognize the clinical significance of the\nattributes for melanoma. To address these limitations, we introduce a novel\ndiagnostic method that integrates two innovative elements: a Clinical\nKnowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy\nwith Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL\nattributes with diagnostic information, revealing both internal and external\nassociations. By employing adaptive receptive domains and weighted edges, we\nestablish connections among melanoma's relevant features. Concurrently, GD-DDW\nemulates dermatologists' diagnostic processes, who first observe the visual\ncharacteristics associated with melanoma and then make predictions. Our model\nuses two imaging modalities for the same lesion, ensuring comprehensive feature\nacquisition. Our method shows outstanding performance in predicting malignant\nmelanoma and its features, achieving an average AUC value of 85%. This was\nvalidated on the EDRA dataset, the largest publicly available dataset for the\n7-point checklist algorithm. Specifically, the integrated weighting system can\nprovide clinicians with valuable data-driven benchmarks for their evaluations.",
      "tldr_zh": "该研究针对传统7-point checklist (7PCL) 在黑素瘤检测中的局限性（如属性权重统一和相互关联忽略），提出了一种AI增强方法。该方法整合了Clinical Knowledge-Based Topological Graph (CKTG)，通过适应性接收域和加权边揭示7PCL属性的内部与外部关联；同时引入Gradient Diagnostic Strategy with Data-Driven Weighting Standards (GD-DDW)，模拟皮肤科医生诊断过程，使用两种成像模式进行全面特征获取。在EDRA数据集上验证，该模型在预测恶性黑素瘤及其特征时平均AUC达到85%，并提供数据驱动的权重基准，帮助临床评估。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16822v1",
      "published_date": "2024-07-23 20:27:16 UTC",
      "updated_date": "2024-07-23 20:27:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:10:32.373027"
    },
    {
      "arxiv_id": "2407.16807v1",
      "title": "In Search for Architectures and Loss Functions in Multi-Objective Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mikhail Terekhov",
        "Caglar Gulcehre"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.",
      "tldr_zh": "本研究探讨多目标强化学习(MORL)中的架构和损失函数，以解决真实世界RL问题中多效用函数权衡的不稳定学习动态。作者引入了Multi-objective Proximal Policy Optimization (MOPPO)，这是一种对PPO的扩展，以及Multi-objective Advantage Actor Critic (MOA2C)作为简单基线，这些方法只需小幅修改函数逼近器即可实现。在MORL Deep Sea Treasure、Minecart和Reacher环境中，实验结果显示MOPPO有效捕获Pareto front，并在hypervolume和expected utility等指标上比Pareto Conditioned Networks (PCN)和Envelope Q-learning更稳健。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 10 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.16807v1",
      "published_date": "2024-07-23 19:17:47 UTC",
      "updated_date": "2024-07-23 19:17:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:10:46.253881"
    },
    {
      "arxiv_id": "2407.16804v1",
      "title": "Multimodal Machine Learning in Mental Health: A Survey of Data, Algorithms, and Challenges",
      "title_zh": "多模态机器学习在心理健康领域：数据、算法和挑战的综述",
      "authors": [
        "Zahraa Al Sahili",
        "Ioannis Patras",
        "Matthew Purver"
      ],
      "abstract": "The application of machine learning (ML) in detecting, diagnosing, and\ntreating mental health disorders is garnering increasing attention.\nTraditionally, research has focused on single modalities, such as text from\nclinical notes, audio from speech samples, or video of interaction patterns.\nRecently, multimodal ML, which combines information from multiple modalities,\nhas demonstrated significant promise in offering novel insights into human\nbehavior patterns and recognizing mental health symptoms and risk factors.\nDespite its potential, multimodal ML in mental health remains an emerging\nfield, facing several complex challenges before practical applications can be\neffectively developed. This survey provides a comprehensive overview of the\ndata availability and current state-of-the-art multimodal ML applications for\nmental health. It discusses key challenges that must be addressed to advance\nthe field. The insights from this survey aim to deepen the understanding of the\npotential and limitations of multimodal ML in mental health, guiding future\nresearch and development in this evolving domain.",
      "tldr_zh": "这篇调查论文回顾了多模态机器学习（multimodal machine learning）在精神健康领域的应用，包括检测、诊断和治疗方面。论文强调了结合多种模态数据（如文本、音频和视频）的优势，能够提供更深入的人类行为模式洞见和精神健康风险识别。作者讨论了数据可用性、当前最先进算法的概述，以及面临的复杂挑战，如实际应用中的障碍，最终旨在指导未来研究以推动该领域的进展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.ET"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16804v1",
      "published_date": "2024-07-23 19:07:56 UTC",
      "updated_date": "2024-07-23 19:07:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:10:56.691839"
    },
    {
      "arxiv_id": "2407.16803v2",
      "title": "C3T: Cross-modal Transfer Through Time for Human Action Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Abhi Kamboj",
        "Anh Duy Nguyen",
        "Minh Do"
      ],
      "abstract": "In order to unlock the potential of diverse sensors, we investigate a method\nto transfer knowledge between modalities using the structure of a unified\nmultimodal representation space for Human Action Recognition (HAR). We\nformalize and explore an understudied cross-modal transfer setting we term\nUnsupervised Modality Adaptation (UMA), where the modality used in testing is\nnot used in supervised training, i.e. zero labeled instances of the test\nmodality are available during training. We develop three methods to perform\nUMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer\nThrough Time (C3T). Our extensive experiments on various camera+IMU datasets\ncompare these methods to each other in the UMA setting, and to their empirical\nupper bound in the supervised setting. The results indicate C3T is the most\nrobust and highest performing by at least a margin of 8%, and nears the\nsupervised setting performance even in the presence of temporal noise. This\nmethod introduces a novel mechanism for aligning signals across time-varying\nlatent vectors, extracted from the receptive field of temporal convolutions.\nOur findings suggest that C3T has significant potential for developing\ngeneralizable models for time-series sensor data, opening new avenues for\nmulti-modal learning in various applications.",
      "tldr_zh": "这篇论文探讨了在人类动作识别(Human Action Recognition, HAR)中，通过统一的多模态表示空间实现模态间知识转移，专注于Unsupervised Modality Adaptation (UMA)设置，即训练时无测试模态的标签数据。研究团队开发了三种方法：Student-Teacher (ST)、Contrastive Alignment (CA)和Cross-modal Transfer Through Time (C3T)，其中C3T通过时间变化的潜在向量对齐信号，展现出最佳性能。实验结果显示，C3T在各种摄像头+IMU数据集上比其他方法高出至少8%，并接近监督设置的性能，即使在时间噪声存在时也保持稳健。该方法为时间序列传感器数据的通用模型开发提供了新途径，推动多模态学习的应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16803v2",
      "published_date": "2024-07-23 19:06:44 UTC",
      "updated_date": "2024-11-07 17:10:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:11:09.892737"
    },
    {
      "arxiv_id": "2407.16802v1",
      "title": "Distribution-Aware Robust Learning from Long-Tailed Data with Noisy Labels",
      "title_zh": "翻译失败",
      "authors": [
        "Jae Soon Baik",
        "In Young Yoon",
        "Kun Hoon Kim",
        "Jun Won Choi"
      ],
      "abstract": "Deep neural networks have demonstrated remarkable advancements in various\nfields using large, well-annotated datasets. However, real-world data often\nexhibit long-tailed distributions and label noise, significantly degrading\ngeneralization performance. Recent studies addressing these issues have focused\non noisy sample selection methods that estimate the centroid of each class\nbased on high-confidence samples within each target class. The performance of\nthese methods is limited because they use only the training samples within each\nclass for class centroid estimation, making the quality of centroids\nsusceptible to long-tailed distributions and noisy labels. In this study, we\npresent a robust training framework called Distribution-aware Sample Selection\nand Contrastive Learning (DaSC). Specifically, DaSC introduces a\nDistribution-aware Class Centroid Estimation (DaCC) to generate enhanced class\ncentroids. DaCC performs weighted averaging of the features from all samples,\nwith weights determined based on model predictions. Additionally, we propose a\nconfidence-aware contrastive learning strategy to obtain balanced and robust\nrepresentations. The training samples are categorized into high-confidence and\nlow-confidence samples. Our method then applies Semi-supervised Balanced\nContrastive Loss (SBCL) using high-confidence samples, leveraging reliable\nlabel information to mitigate class bias. For the low-confidence samples, our\nmethod computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve\ntheir representations in a self-supervised manner. Our experimental results on\nCIFAR and real-world noisy-label datasets demonstrate the superior performance\nof the proposed DaSC compared to previous approaches.",
      "tldr_zh": "这篇论文针对深度神经网络在长尾分布和噪声标签数据上的泛化性能下降问题，提出了一种鲁棒训练框架 Distribution-aware Sample Selection and Contrastive Learning (DaSC)。DaSC 包括 Distribution-aware Class Centroid Estimation (DaCC) 方法，通过对所有样本特征进行基于模型预测的加权平均来生成更可靠的类中心；此外，它采用 confidence-aware contrastive learning 策略，使用 Semi-supervised Balanced Contrastive Loss (SBCL) 处理高置信度样本以缓解类偏置，并通过 Mixup-enhanced Instance Discrimination Loss (MIDL) 改进低置信度样本的自监督表示。实验结果显示，在 CIFAR 和真实世界噪声标签数据集上，DaSC 比现有方法表现出色，显著提升了模型性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16802v1",
      "published_date": "2024-07-23 19:06:15 UTC",
      "updated_date": "2024-07-23 19:06:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:11:32.232747"
    },
    {
      "arxiv_id": "2407.16789v2",
      "title": "What Matters in Range View 3D Object Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Wilson",
        "Nicholas Autio Mitchell",
        "Jhony Kaesemodel Pontes",
        "James Hays"
      ],
      "abstract": "Lidar-based perception pipelines rely on 3D object detection models to\ninterpret complex scenes. While multiple representations for lidar exist, the\nrange-view is enticing since it losslessly encodes the entire lidar sensor\noutput. In this work, we achieve state-of-the-art amongst range-view 3D object\ndetection models without using multiple techniques proposed in past range-view\nliterature. We explore range-view 3D object detection across two modern\ndatasets with substantially different properties: Argoverse 2 and Waymo Open.\nOur investigation reveals key insights: (1) input feature dimensionality\nsignificantly influences the overall performance, (2) surprisingly, employing a\nclassification loss grounded in 3D spatial proximity works as well or better\ncompared to more elaborate IoU-based losses, and (3) addressing non-uniform\nlidar density via a straightforward range subsampling technique outperforms\nexisting multi-resolution, range-conditioned networks. Our experiments reveal\nthat techniques proposed in recent range-view literature are not needed to\nachieve state-of-the-art performance. Combining the above findings, we\nestablish a new state-of-the-art model for range-view 3D object detection --\nimproving AP by 2.2% on the Waymo Open dataset while maintaining a runtime of\n10 Hz. We establish the first range-view model on the Argoverse 2 dataset and\noutperform strong voxel-based baselines. All models are multi-class and\nopen-source. Code is available at\nhttps://github.com/benjaminrwilson/range-view-3d-detection.",
      "tldr_zh": "该论文探讨了 range-view 表示在 Lidar-based 3D 对象检测中的关键因素，通过在 Argoverse 2 和 Waymo Open 数据集上的实验，揭示了输入特征维度对性能的重大影响、基于 3D 空间接近度的分类损失优于复杂的 IoU-based 损失，以及简单范围子采样技术在处理非均匀 Lidar 密度时的优势。研究发现，无需采用以往 range-view 文献中的多种技术，即可实现 state-of-the-art 性能。最终，该方法在 Waymo Open 数据集上将 AP 提高了 2.2%，保持 10 Hz 的运行速度，并在 Argoverse 2 上建立了首个 range-view 模型，同时超越 voxel-based 基线，所有模型均为多类且开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Fixed broken link",
      "pdf_url": "http://arxiv.org/pdf/2407.16789v2",
      "published_date": "2024-07-23 18:42:37 UTC",
      "updated_date": "2024-07-25 20:20:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:11:33.956661"
    },
    {
      "arxiv_id": "2407.16785v1",
      "title": "PrISM-Observer: Intervention Agent to Help Users Perform Everyday Procedures Sensed using a Smartwatch",
      "title_zh": "翻译失败",
      "authors": [
        "Riku Arakawa",
        "Hiromu Yakura",
        "Mayank Goel"
      ],
      "abstract": "We routinely perform procedures (such as cooking) that include a set of\natomic steps. Often, inadvertent omission or misordering of a single step can\nlead to serious consequences, especially for those experiencing cognitive\nchallenges such as dementia. This paper introduces PrISM-Observer, a\nsmartwatch-based, context-aware, real-time intervention system designed to\nsupport daily tasks by preventing errors. Unlike traditional systems that\nrequire users to seek out information, the agent observes user actions and\nintervenes proactively. This capability is enabled by the agent's ability to\ncontinuously update its belief in the user's behavior in real-time through\nmultimodal sensing and forecast optimal intervention moments and methods. We\nfirst validated the steps-tracking performance of our framework through\nevaluations across three datasets with different complexities. Then, we\nimplemented a real-time agent system using a smartwatch and conducted a user\nstudy in a cooking task scenario. The system generated helpful interventions,\nand we gained positive feedback from the participants. The general\napplicability of PrISM-Observer to daily tasks promises broad applications, for\ninstance, including support for users requiring more involved interventions,\nsuch as people with dementia or post-surgical patients.",
      "tldr_zh": "本研究引入了PrISM-Observer，一种基于智能手表(smartwatch)的上下文感知实时干预系统，旨在帮助用户执行日常程序（如烹饪），通过主动观察和纠正步骤遗漏或错误，以避免潜在风险，尤其适用于认知障碍者如痴呆患者。系统利用多模态感知(multimodal sensing)实时更新对用户行为的信念，并预测最佳干预时机和方法，与传统被动系统形成对比。在实验中，该框架在三个不同复杂度的数据集上验证了步骤跟踪性能，并在烹饪任务的用户研究中展示了有效干预，用户反馈积极。PrISM-Observer的应用前景广阔，可扩展到支持术后患者等群体，实现更广泛的日常生活辅助。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "conditionally accepted to ACM UIST 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16785v1",
      "published_date": "2024-07-23 18:38:07 UTC",
      "updated_date": "2024-07-23 18:38:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:11:54.926141"
    },
    {
      "arxiv_id": "2407.16770v1",
      "title": "Infinite Ends from Finite Samples: Open-Ended Goal Inference as Top-Down Bayesian Filtering of Bottom-Up Proposals",
      "title_zh": "翻译失败",
      "authors": [
        "Tan Zhi-Xuan",
        "Gloria Kang",
        "Vikash Mansinghka",
        "Joshua B. Tenenbaum"
      ],
      "abstract": "The space of human goals is tremendously vast; and yet, from just a few\nmoments of watching a scene or reading a story, we seem to spontaneously infer\na range of plausible motivations for the people and characters involved. What\nexplains this remarkable capacity for intuiting other agents' goals, despite\nthe infinitude of ends they might pursue? And how does this cohere with our\nunderstanding of other people as approximately rational agents? In this paper,\nwe introduce a sequential Monte Carlo model of open-ended goal inference, which\ncombines top-down Bayesian inverse planning with bottom-up sampling based on\nthe statistics of co-occurring subgoals. By proposing goal hypotheses related\nto the subgoals achieved by an agent, our model rapidly generates plausible\ngoals without exhaustive search, then filters out goals that would be\nirrational given the actions taken so far. We validate this model in a goal\ninference task called Block Words, where participants try to guess the word\nthat someone is stacking out of lettered blocks. In comparison to both\nheuristic bottom-up guessing and exact Bayesian inference over hundreds of\ngoals, our model better predicts the mean, variance, efficiency, and resource\nrationality of human goal inferences, achieving similar accuracy to the exact\nmodel at a fraction of the cognitive cost, while also explaining garden-path\neffects that arise from misleading bottom-up cues. Our experiments thus\nhighlight the importance of uniting top-down and bottom-up models for\nexplaining the speed, accuracy, and generality of human theory-of-mind.",
      "tldr_zh": "本研究探讨了人类如何从有限观察中推断无限可能的目标，提出一个顺序 Monte Carlo 模型，用于开放式目标推断。该模型结合自上而下的 Bayesian inverse planning 和自下而上的采样机制，通过生成与代理子目标相关的假设并过滤不合理的目标，实现高效的推断过程。在 Block Words 任务实验中，该模型比启发式猜测和精确 Bayesian 推断更准确地预测了人类的推断均值、方差、效率和资源理性，同时解释了 garden-path effects 等认知偏差现象。该方法突出了整合 top-down 和 bottom-up 模型在解释人类理论-of-mind 的速度、准确性和普遍性方面的关键作用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at CogSci 2024. 6 pages, 4 figures.\n  (Appendix: 5 pages, 6 figures, 2 tables)",
      "pdf_url": "http://arxiv.org/pdf/2407.16770v1",
      "published_date": "2024-07-23 18:04:40 UTC",
      "updated_date": "2024-07-23 18:04:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:11:57.732882"
    },
    {
      "arxiv_id": "2407.16695v2",
      "title": "Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack",
      "title_zh": "通过 Lifelong ICL 和 Task Haystack 压力测试长上下文语言模型",
      "authors": [
        "Xiaoyue Xu",
        "Qinyuan Ye",
        "Xiang Ren"
      ],
      "abstract": "We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn a sequence of language tasks through in-context\nlearning (ICL). We further introduce Task Haystack, an evaluation suite\ndedicated to assessing and diagnosing how long-context LMs utilizes contexts in\nLifelong ICL. When given a task instruction and test inputs, long-context LMs\nare expected to leverage the relevant demonstrations in the Lifelong ICL\nprompt, avoid distraction and interference from other tasks, and achieve test\naccuracies that are not significantly worse than those of the Single-task ICL\nbaseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents distinct new challenges.\nIt requires models (1) to utilize the contexts at a deeper level, rather than\nresorting to simple copying and pasting; (2) to navigate through long streams\nof evolving topics and tasks, proxying the complexities and dynamism of\ncontexts in real-world scenarios. Additionally, Task Haystack inherits the\ncontrollability of NIAH, providing model developers with tools and\nvisualizations to identify model vulnerabilities effectively.\n  We benchmark 14 long-context LMs using Task Haystack, finding that frontier\nmodels like GPT-4o still struggle with the setting, failing on 15% of cases on\naverage. Most open-weight models further lack behind by a large margin, with\nfailure rates reaching up to 61%. In our controlled analysis, we identify\nfactors such as distraction and recency bias as contributors to these failure\ncases. Further, performance declines when task instructions are paraphrased at\ntest time or when ICL demonstrations are repeated excessively, raising concerns\nabout the robustness, instruction understanding, and true context utilization\nof long-context LMs.",
      "tldr_zh": "本论文引入了Lifelong ICL问题设置，以挑战长上下文语言模型通过in-context learning (ICL)学习一系列语言任务，并提出了Task Haystack评估套件，用于评估模型在这种设置中如何利用上下文、避免干扰，并与单任务ICL基准保持相当性能。Task Haystack借鉴“needle-in-a-haystack”(NIAH)评估，但增加了更深入的上下文利用和动态任务处理等新挑战，提供工具帮助开发者识别模型弱点。在基准测试中，14个长上下文模型的表现不佳，前沿模型如GPT-4o平均失败率达15%，开源模型更高达61%，揭示了模型易受干扰、近因偏差影响，且在任务指令改写或演示重复时性能下降，凸显其鲁棒性和真实上下文利用的不足。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS 2024 (Datasets and Benchmarks Track). Code:\n  https://github.com/INK-USC/Lifelong-ICL Website:\n  https://inklab.usc.edu/lifelong-icl/",
      "pdf_url": "http://arxiv.org/pdf/2407.16695v2",
      "published_date": "2024-07-23 17:57:41 UTC",
      "updated_date": "2024-12-02 20:23:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:12:10.863625"
    },
    {
      "arxiv_id": "2407.16741v3",
      "title": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Xingyao Wang",
        "Boxuan Li",
        "Yufan Song",
        "Frank F. Xu",
        "Xiangru Tang",
        "Mingchen Zhuge",
        "Jiayi Pan",
        "Yueqi Song",
        "Bowen Li",
        "Jaskirat Singh",
        "Hoang H. Tran",
        "Fuqiang Li",
        "Ren Ma",
        "Mingzhang Zheng",
        "Bill Qian",
        "Yanjun Shao",
        "Niklas Muennighoff",
        "Yizhe Zhang",
        "Binyuan Hui",
        "Junyang Lin",
        "Robert Brennan",
        "Hao Peng",
        "Heng Ji",
        "Graham Neubig"
      ],
      "abstract": "Software is one of the most powerful tools that we humans have at our\ndisposal; it allows a skilled programmer to interact with the world in complex\nand profound ways. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. In this\npaper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the\ndevelopment of powerful and flexible AI agents that interact with the world in\nsimilar ways to those of a human developer: by writing code, interacting with a\ncommand line, and browsing the web. We describe how the platform allows for the\nimplementation of new agents, safe interaction with sandboxed environments for\ncode execution, coordination between multiple agents, and incorporation of\nevaluation benchmarks. Based on our currently incorporated benchmarks, we\nperform an evaluation of agents over 15 challenging tasks, including software\nengineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others.\nReleased under the permissive MIT license, OpenHands is a community project\nspanning academia and industry with more than 2.1K contributions from over 188\ncontributors.",
      "tldr_zh": "本文介绍了 OpenHands（前称 OpenDevin），一个开源平台，旨在帮助 AI 软件开发者构建通用 AI 代理，这些代理通过编写代码、交互命令行和浏览网页的方式模拟人类开发者与环境的互动。平台支持新代理的实现、安全的沙盒环境、多代理协调以及集成评估基准（如 SWE-BENCH 和 WEBARENA）。在 15 个挑战任务的评估中，OpenHands 展示了代理的强大性能，并在软件工程和网页浏览等领域表现出色。该项目采用 MIT 许可，由学术和工业社区共同推动，已收到超过 188 个贡献者的 2.1K 贡献。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "Accepted by ICLR 2025; Code:\n  https://github.com/All-Hands-AI/OpenHands",
      "pdf_url": "http://arxiv.org/pdf/2407.16741v3",
      "published_date": "2024-07-23 17:50:43 UTC",
      "updated_date": "2025-04-18 18:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:12:20.628943"
    },
    {
      "arxiv_id": "2407.16674v2",
      "title": "KAN or MLP: A Fairer Comparison",
      "title_zh": "KAN 或 MLP：一个更公平的比较",
      "authors": [
        "Runpeng Yu",
        "Weihao Yu",
        "Xinchao Wang"
      ],
      "abstract": "This paper does not introduce a novel method. Instead, it offers a fairer and\nmore comprehensive comparison of KAN and MLP models across various tasks,\nincluding machine learning, computer vision, audio processing, natural language\nprocessing, and symbolic formula representation. Specifically, we control the\nnumber of parameters and FLOPs to compare the performance of KAN and MLP. Our\nmain observation is that, except for symbolic formula representation tasks, MLP\ngenerally outperforms KAN. We also conduct ablation studies on KAN and find\nthat its advantage in symbolic formula representation mainly stems from its\nB-spline activation function. When B-spline is applied to MLP, performance in\nsymbolic formula representation significantly improves, surpassing or matching\nthat of KAN. However, in other tasks where MLP already excels over KAN,\nB-spline does not substantially enhance MLP's performance. Furthermore, we find\nthat KAN's forgetting issue is more severe than that of MLP in a standard\nclass-incremental continual learning setting, which differs from the findings\nreported in the KAN paper. We hope these results provide insights for future\nresearch on KAN and other MLP alternatives. Project link:\nhttps://github.com/yu-rp/KANbeFair",
      "tldr_zh": "这篇论文通过控制参数数量和 FLOPs，对 KAN 和 MLP 模型在机器学习、计算机视觉、音频处理、自然语言处理以及符号公式表示等任务上进行更公平的比较。结果显示，除符号公式表示任务外，MLP 整体优于 KAN；通过消融研究发现，KAN 的优势主要源于其 B-spline 激活函数，当将 B-spline 应用到 MLP 时，其在符号公式表示中的性能可显著提升，甚至超过 KAN，但在其他任务上并未带来明显改善。此外，论文指出 KAN 在标准类增量连续学习(class-incremental continual learning)中的遗忘问题比 MLP 更严重，这与现有 KAN 研究结果不同，并为未来 KAN 和 MLP 替代方案的研究提供参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2407.16674v2",
      "published_date": "2024-07-23 17:43:35 UTC",
      "updated_date": "2024-08-17 15:20:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:12:34.146692"
    },
    {
      "arxiv_id": "2407.16740v1",
      "title": "PLM-Net: Perception Latency Mitigation Network for Vision-Based Lateral Control of Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Aws Khalil",
        "Jaerock Kwon"
      ],
      "abstract": "This study introduces the Perception Latency Mitigation Network (PLM-Net), a\nnovel deep learning approach for addressing perception latency in vision-based\nAutonomous Vehicle (AV) lateral control systems. Perception latency is the\ndelay between capturing the environment through vision sensors (e.g., cameras)\nand applying an action (e.g., steering). This issue is understudied in both\nclassical and neural-network-based control methods. Reducing this latency with\npowerful GPUs and FPGAs is possible but impractical for automotive platforms.\nPLM-Net comprises the Base Model (BM) and the Timed Action Prediction Model\n(TAPM). BM represents the original Lane Keeping Assist (LKA) system, while TAPM\npredicts future actions for different latency values. By integrating these\nmodels, PLM-Net mitigates perception latency. The final output is determined\nthrough linear interpolation of BM and TAPM outputs based on real-time latency.\nThis design addresses both constant and varying latency, improving driving\ntrajectories and steering control. Experimental results validate the efficacy\nof PLM-Net across various latency conditions. Source code:\nhttps://github.com/AwsKhalil/oscar/tree/devel-plm-net.",
      "tldr_zh": "这篇论文介绍了 PLM-Net，一种新型深度学习网络，旨在缓解视觉-based 自动驾驶车辆（Autonomous Vehicles, AV）横向控制中的感知延迟问题，该延迟是视觉传感器捕获环境到执行行动（如转向）之间的延时。PLM-Net 由 Base Model (BM) 和 Timed Action Prediction Model (TAPM) 组成，其中 BM 基于原有的 Lane Keeping Assist (LKA) 系统，而 TAPM 预测不同延迟值的未来行动，通过线性插值整合输出以适应实时延迟。实验结果显示，PLM-Net 在各种延迟条件下显著改善了驾驶轨迹和转向控制性能，验证了其有效性。开源代码可从 GitHub 获取。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "13 pages excluding the appendixes. 19 pages including appendixes",
      "pdf_url": "http://arxiv.org/pdf/2407.16740v1",
      "published_date": "2024-07-23 17:41:13 UTC",
      "updated_date": "2024-07-23 17:41:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:12:46.829677"
    },
    {
      "arxiv_id": "2407.16667v1",
      "title": "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent",
      "title_zh": "RedAgent: 基于上下文感知自治语言代理的大语言",
      "authors": [
        "Huiyu Xu",
        "Wenhui Zhang",
        "Zhibo Wang",
        "Feng Xiao",
        "Rui Zheng",
        "Yunhe Feng",
        "Zhongjie Ba",
        "Kui Ren"
      ],
      "abstract": "Recently, advanced Large Language Models (LLMs) such as GPT-4 have been\nintegrated into many real-world applications like Code Copilot. These\napplications have significantly expanded the attack surface of LLMs, exposing\nthem to a variety of threats. Among them, jailbreak attacks that induce toxic\nresponses through jailbreak prompts have raised critical safety concerns. To\nidentify these threats, a growing number of red teaming approaches simulate\npotential adversarial scenarios by crafting jailbreak prompts to test the\ntarget LLM. However, existing red teaming methods do not consider the unique\nvulnerabilities of LLM in different scenarios, making it difficult to adjust\nthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,\nthese methods are limited to refining jailbreak templates using a few mutation\noperations, lacking the automation and scalability to adapt to different\nscenarios. To enable context-aware and efficient red teaming, we abstract and\nmodel existing attacks into a coherent concept called \"jailbreak strategy\" and\npropose a multi-agent LLM system named RedAgent that leverages these strategies\nto generate context-aware jailbreak prompts. By self-reflecting on contextual\nfeedback in an additional memory buffer, RedAgent continuously learns how to\nleverage these strategies to achieve effective jailbreaks in specific contexts.\nExtensive experiments demonstrate that our system can jailbreak most black-box\nLLMs in just five queries, improving the efficiency of existing red teaming\nmethods by two times. Additionally, RedAgent can jailbreak customized LLM\napplications more efficiently. By generating context-aware jailbreak prompts\ntowards applications on GPTs, we discover 60 severe vulnerabilities of these\nreal-world applications with only two queries per vulnerability. We have\nreported all found issues and communicated with OpenAI and Meta for bug fixes.",
      "tldr_zh": "该研究提出 RedAgent，一种基于多智能体 LLM 系统(context-aware autonomous language agent)的红队测试框架，旨在生成上下文感知的越狱提示(jailbreak prompts)，以识别 Large Language Models (LLMs) 在不同场景下的特定漏洞。RedAgent 通过抽象攻击为“jailbreak strategy”，并利用自反省和记忆缓冲区，动态学习并优化提示策略，实现高效的自动化测试。实验结果显示，RedAgent 能在五次查询内成功越狱大多数黑盒 LLM，比现有方法效率提高两倍，并在实际应用中仅用两次查询发现 60 个严重漏洞，已报告给 OpenAI 和 Meta 进行修复。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16667v1",
      "published_date": "2024-07-23 17:34:36 UTC",
      "updated_date": "2024-07-23 17:34:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:12:58.392860"
    },
    {
      "arxiv_id": "2407.16665v2",
      "title": "A Framework for Pupil Tracking with Event Cameras",
      "title_zh": "基于事件相机的瞳孔跟踪框架",
      "authors": [
        "Khadija Iddrisu",
        "Waseem Shariff",
        "Suzanne Little"
      ],
      "abstract": "Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.",
      "tldr_zh": "该论文提出一个框架，使用event cameras追踪瞳孔，以解决传统摄像头在捕捉快速眼动(saccades)时存在的运动模糊和延迟问题。框架将事件数据转换为标准帧，并应用YOLOv8进行瞳孔检测，利用Ev-Eye dataset进行实验验证。结果显示，该方法提高了追踪精度和速度，具有潜在应用于神经科学、眼科和人机交互领域。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is a preprint of a paper submitted to the 26th Irish\n  Machine Vision and Image Processing Conference (IMVIP 2024). If accepted, the\n  copy of record will be available at IET Digital Library",
      "pdf_url": "http://arxiv.org/pdf/2407.16665v2",
      "published_date": "2024-07-23 17:32:02 UTC",
      "updated_date": "2024-10-07 09:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:13:13.525758"
    },
    {
      "arxiv_id": "2407.16641v1",
      "title": "A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in Hyperbolic Space",
      "title_zh": "翻译失败",
      "authors": [
        "Zhangyu Wang",
        "Lantian Xu",
        "Zhifeng Kong",
        "Weilong Wang",
        "Xuyu Peng",
        "Enyang Zheng"
      ],
      "abstract": "Hyperbolic embeddings are a class of representation learning methods that\noffer competitive performances when data can be abstracted as a tree-like\ngraph. However, in practice, learning hyperbolic embeddings of hierarchical\ndata is difficult due to the different geometry between hyperbolic space and\nthe Euclidean space. To address such difficulties, we first categorize three\nkinds of illness that harm the performance of the embeddings. Then, we develop\na geometry-aware algorithm using a dilation operation and a transitive closure\nregularization to tackle these illnesses. We empirically validate these\ntechniques and present a theoretical analysis of the mechanism behind the\ndilation operation. Experiments on synthetic and real-world datasets reveal\nsuperior performances of our algorithm.",
      "tldr_zh": "这篇论文针对超曲空间(Hyperbolic embeddings)学习中的几何差异问题，分类了三种影响性能的因素，包括嵌入学习中的不稳定性。论文提出了一种基于几何的算法，使用dilation operation和transitive closure regularization来解决这些问题，并对dilation operation的机制进行了理论分析。实验结果显示，该算法在合成和真实数据集上表现出色，性能优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16641v1",
      "published_date": "2024-07-23 16:56:59 UTC",
      "updated_date": "2024-07-23 16:56:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:13:21.888691"
    },
    {
      "arxiv_id": "2407.16637v2",
      "title": "Course-Correction: Safety Alignment Using Synthetic Preferences",
      "title_zh": "翻译失败",
      "authors": [
        "Rongwu Xu",
        "Yishuo Cai",
        "Zhenhong Zhou",
        "Renjie Gu",
        "Haiqin Weng",
        "Yan Liu",
        "Tianwei Zhang",
        "Wei Xu",
        "Han Qiu"
      ],
      "abstract": "The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）生成有害内容的风险，并引入了course-correction任务，即模型自主避免生成有害输出。研究者开发了C²-Eval基准来评估10个流行LLMs的course-correction能力，发现现有安全微调模型的表现参差不齐。为提升性能，他们提出使用preference learning进行微调，强调及时course-correction，并通过自动化管道创建了包含750K对偏好数据的合成数据集C²-Syn。实验在Llama2-Chat 7B和Qwen2 7B模型上显示，该方法显著提高了course-correction技能，同时提升了模型的安全性，特别是抵抗jailbreak attacks，而不影响整体性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper accepted to EMNLP 2024. Camera-ready version. We have released\n  our dataset and scripts at https://github.com/pillowsofwind/Course-Correction",
      "pdf_url": "http://arxiv.org/pdf/2407.16637v2",
      "published_date": "2024-07-23 16:54:28 UTC",
      "updated_date": "2024-10-26 15:29:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:13:34.822885"
    },
    {
      "arxiv_id": "2407.16634v1",
      "title": "Knowledge-driven AI-generated data for accurate and interpretable breast ultrasound diagnoses",
      "title_zh": "翻译失败",
      "authors": [
        "Haojun Yu",
        "Youcheng Li",
        "Nan Zhang",
        "Zihan Niu",
        "Xuantong Gong",
        "Yanwen Luo",
        "Quanlin Wu",
        "Wangyan Qin",
        "Mengyuan Zhou",
        "Jie Han",
        "Jia Tao",
        "Ziwei Zhao",
        "Di Dai",
        "Di He",
        "Dong Wang",
        "Binghui Tang",
        "Ling Huo",
        "Qingli Zhu",
        "Yong Wang",
        "Liwei Wang"
      ],
      "abstract": "Data-driven deep learning models have shown great capabilities to assist\nradiologists in breast ultrasound (US) diagnoses. However, their effectiveness\nis limited by the long-tail distribution of training data, which leads to\ninaccuracies in rare cases. In this study, we address a long-standing challenge\nof improving the diagnostic model performance on rare cases using long-tailed\ndata. Specifically, we introduce a pipeline, TAILOR, that builds a\nknowledge-driven generative model to produce tailored synthetic data. The\ngenerative model, using 3,749 lesions as source data, can generate millions of\nbreast-US images, especially for error-prone rare cases. The generated data can\nbe further used to build a diagnostic model for accurate and interpretable\ndiagnoses. In the prospective external evaluation, our diagnostic model\noutperforms the average performance of nine radiologists by 33.5% in\nspecificity with the same sensitivity, improving their performance by providing\npredictions with an interpretable decision-making process. Moreover, on ductal\ncarcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by\na large margin, with only 34 DCIS lesions in the source data. We believe that\nTAILOR can potentially be extended to various diseases and imaging modalities.",
      "tldr_zh": "这篇论文针对乳腺超声(breast ultrasound)诊断中长尾数据分布导致的稀有病例准确性问题，提出TAILOR管道——一个知识驱动的生成模型，从3,749个病变源数据生成数百万张合成图像，尤其聚焦于易出错的稀有病例。利用这些合成数据构建的诊断模型，在外部评估中以相同敏感性提高了33.5%的特异性，并在导管原位癌(DCIS)上大幅超过九位放射科医生的平均表现，提供可解释的决策过程。该方法不仅提升了诊断的准确性和可解释性，还具有潜力扩展到其他疾病和成像方式。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16634v1",
      "published_date": "2024-07-23 16:49:01 UTC",
      "updated_date": "2024-07-23 16:49:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:13:48.090850"
    },
    {
      "arxiv_id": "2407.16616v1",
      "title": "Implementing engrams from a machine learning perspective: the relevance of a latent space",
      "title_zh": "翻译失败",
      "authors": [
        "J Marco de Lucas"
      ],
      "abstract": "In our previous work, we proposed that engrams in the brain could be\nbiologically implemented as autoencoders over recurrent neural networks. These\nautoencoders would comprise basic excitatory/inhibitory motifs, with credit\nassignment deriving from a simple homeostatic criterion. This brief note\nexamines the relevance of the latent space in these autoencoders. We consider\nthe relationship between the dimensionality of these autoencoders and the\ncomplexity of the information being encoded. We discuss how observed\ndifferences between species in their connectome could be linked to their\ncognitive capacities. Finally, we link this analysis with a basic but often\noverlooked fact: human cognition is likely limited by our own brain structure.\nHowever, this limitation does not apply to machine learning systems, and we\nshould be aware of the need to learn how to exploit this augmented vision of\nthe nature.",
      "tldr_zh": "本研究从机器学习视角扩展了先前关于脑部 engrams 的假设，提出 engrams 可通过 recurrent neural networks 上的 autoencoders 来实现，并重点探讨了这些 autoencoders 的 latent space 在信息编码中的相关性。论文分析了 latent space 的维度与编码信息复杂度的关系，以及物种间 connectome 差异如何影响认知能力。最终，研究强调人类认知受脑结构限制，而机器学习系统可超越此局限，因此需要充分利用这一优势来增强认知模拟。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "6 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16616v1",
      "published_date": "2024-07-23 16:24:29 UTC",
      "updated_date": "2024-07-23 16:24:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:13:56.964578"
    },
    {
      "arxiv_id": "2407.16735v1",
      "title": "Theoretical Analysis of Privacy Leakage in Trustworthy Federated Learning: A Perspective from Linear Algebra and Optimization Theory",
      "title_zh": "值得信赖的联邦学习中隐私泄露的理论分析：来自线性代数和最优化理论的视角",
      "authors": [
        "Xiaojin Zhang",
        "Wei Chen"
      ],
      "abstract": "Federated learning has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy. However, recent studies have\nshown that it is vulnerable to various privacy attacks, such as data\nreconstruction attacks. In this paper, we provide a theoretical analysis of\nprivacy leakage in federated learning from two perspectives: linear algebra and\noptimization theory. From the linear algebra perspective, we prove that when\nthe Jacobian matrix of the batch data is not full rank, there exist different\nbatches of data that produce the same model update, thereby ensuring a level of\nprivacy. We derive a sufficient condition on the batch size to prevent data\nreconstruction attacks. From the optimization theory perspective, we establish\nan upper bound on the privacy leakage in terms of the batch size, the\ndistortion extent, and several other factors. Our analysis provides insights\ninto the relationship between privacy leakage and various aspects of federated\nlearning, offering a theoretical foundation for designing privacy-preserving\nfederated learning algorithms.",
      "tldr_zh": "这篇论文从 Linear Algebra 和 Optimization Theory 的角度，对 Federated Learning 中的隐私泄露进行了理论分析。研究证明，当批量数据的 Jacobian 矩阵不是满秩时，不同批量数据可能产生相同的模型更新，从而提供隐私保护，并导出了防止数据重建攻击的批量大小充分条件。从 Optimization Theory 视角，论文建立了隐私泄露的上界，与批量大小、失真程度及其他因素相关。这些分析揭示了隐私泄露与 Federated Learning 各方面关系的洞见，为设计隐私保护算法提供了理论基础。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16735v1",
      "published_date": "2024-07-23 16:23:38 UTC",
      "updated_date": "2024-07-23 16:23:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:14:11.317132"
    },
    {
      "arxiv_id": "2407.16615v2",
      "title": "Lawma: The Power of Specialization for Legal Annotation",
      "title_zh": "翻译失败",
      "authors": [
        "Ricardo Dominguez-Olmedo",
        "Vedant Nanda",
        "Rediet Abebe",
        "Stefan Bechtold",
        "Christoph Engel",
        "Jens Frankenreiter",
        "Krishna Gummadi",
        "Moritz Hardt",
        "Michael Livermore"
      ],
      "abstract": "Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal annotation remains limited. To bridge this gap, we introduce\nCaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to\nthe machine learning community. We demonstrate that commercial models, such as\nGPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable\naccuracy, generally falling short of the performance required for legal work.\nWe then demonstrate that small, lightly fine-tuned models outperform commercial\nmodels. A few hundred to a thousand labeled examples are usually enough to\nachieve higher accuracy. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal\nannotation tasks with some available labeled data, researchers are likely\nbetter off using a fine-tuned open-source model.",
      "tldr_zh": "该研究探讨了使用大型语言模型（LLM）进行法律文本标注的效率，引入了CaselawQA基准测试，该基准包含260个新的法律标注任务，用于评估模型性能。结果显示，商业模型如GPT-4.5和Claude 3.7 Sonnet的准确率虽有一定水平，但高度不稳定，且无法满足法律工作的要求。相比之下，通过轻微微调的小型开源模型，仅需几百到一千个标记样本即可实现更高准确率，从而为法律研究提供更可靠的替代方案。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.16615v2",
      "published_date": "2024-07-23 16:23:04 UTC",
      "updated_date": "2025-04-23 12:18:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:14:22.866853"
    },
    {
      "arxiv_id": "2407.16613v1",
      "title": "No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots",
      "title_zh": "翻译失败",
      "authors": [
        "Alican Mertan",
        "Nick Cheney"
      ],
      "abstract": "It is prevalent in contemporary AI and robotics to separately postulate a\nbrain modeled by neural networks and employ it to learn intelligent and\nadaptive behavior. While this method has worked very well for many types of\ntasks, it isn't the only type of intelligence that exists in nature. In this\nwork, we study the ways in which intelligent behavior can be created without a\nseparate and explicit brain for robot control, but rather solely as a result of\nthe computation occurring within the physical body of a robot. Specifically, we\nshow that adaptive and complex behavior can be created in voxel-based virtual\nsoft robots by using simple reactive materials that actively change the shape\nof the robot, and thus its behavior, under different environmental cues. We\ndemonstrate a proof of concept for the idea of closed-loop morphological\ncomputation, and show that in our implementation, it enables behavior mimicking\nlogic gates, enabling us to demonstrate how such behaviors may be combined to\nbuild up more complex collective behaviors.",
      "tldr_zh": "本文提出“No-brainer”框架，强调通过形态计算(morphological computation)实现软机器人的适应性行为，而非依赖独立的神经网络大脑。研究方法使用基于体素(voxel-based virtual soft robots)和简单反应性材料(reactive materials)，这些材料响应环境变化主动调整机器人形状，从而产生复杂行为。结果展示了闭环形态计算(closed-loop morphological computation)的概念验证，该方法能模仿逻辑门(logic gates)的功能，并组合构建更高级的集体行为，为自然灵感机器人设计提供新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the From Animals to Animats: 17th International\n  Conference on the Simulation of Adaptive Behavior (SAB 2024) conference",
      "pdf_url": "http://arxiv.org/pdf/2407.16613v1",
      "published_date": "2024-07-23 16:20:36 UTC",
      "updated_date": "2024-07-23 16:20:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:14:34.600990"
    },
    {
      "arxiv_id": "2407.16611v1",
      "title": "Local vs Global continual learning",
      "title_zh": "翻译失败",
      "authors": [
        "Giulia Lanzillotta",
        "Sidak Pal Singh",
        "Benjamin F. Grewe",
        "Thomas Hofmann"
      ],
      "abstract": "Continual learning is the problem of integrating new information in a model\nwhile retaining the knowledge acquired in the past. Despite the tangible\nimprovements achieved in recent years, the problem of continual learning is\nstill an open one. A better understanding of the mechanisms behind the\nsuccesses and failures of existing continual learning algorithms can unlock the\ndevelopment of new successful strategies. In this work, we view continual\nlearning from the perspective of the multi-task loss approximation, and we\ncompare two alternative strategies, namely local and global approximations. We\nclassify existing continual learning algorithms based on the approximation\nused, and we assess the practical effects of this distinction in common\ncontinual learning settings.Additionally, we study optimal continual learning\nobjectives in the case of local polynomial approximations and we provide\nexamples of existing algorithms implementing the optimal objectives",
      "tldr_zh": "这篇论文探讨了持续学习（continual learning）问题，即在模型中整合新信息的同时保留过去知识，并从多任务损失逼近（multi-task loss approximation）的角度比较局部（local）和全局（global）逼近策略。作者对现有持续学习算法进行了分类，并评估了这些策略在常见设置中的实际效果，以揭示其成功与失败机制。此外，该研究分析了局部多项式逼近（local polynomial approximations）情况下的最优持续学习目标，并举例说明了现有算法如何实现这些目标。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "(10 pages, Will appear in the proceedings of CoLLAs 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.16611v1",
      "published_date": "2024-07-23 16:18:00 UTC",
      "updated_date": "2024-07-23 16:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:14:56.723191"
    },
    {
      "arxiv_id": "2407.16608v1",
      "title": "Deep Bayesian segmentation for colon polyps: Well-calibrated predictions in medical imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Daniela L. Ramos",
        "Hector J. Hortua"
      ],
      "abstract": "Colorectal polyps are generally benign alterations that, if not identified\npromptly and managed successfully, can progress to cancer and cause\naffectations on the colon mucosa, known as adenocarcinoma. Today advances in\nDeep Learning have demonstrated the ability to achieve significant performance\nin image classification and detection in medical diagnosis applications.\nNevertheless, these models are prone to overfitting, and making decisions based\nonly on point estimations may provide incorrect predictions. Thus, to obtain a\nmore informed decision, we must consider point estimations along with their\nreliable uncertainty quantification. In this paper, we built different Bayesian\nneural network approaches based on the flexibility of posterior distribution to\ndevelop semantic segmentation of colorectal polyp images. We found that these\nmodels not only provide state-of-the-art performance on the segmentation of\nthis medical dataset but also, yield accurate uncertainty estimates. We applied\nmultiplicative normalized flows(MNF) and reparameterization trick on the UNET,\nFPN, and LINKNET architectures tested with multiple backbones in deterministic\nand Bayesian versions. We report that the FPN + EfficientnetB7 architecture\nwith MNF is the most promising option given its IOU of 0.94 and Expected\nCalibration Error (ECE) of 0.004, combined with its superiority in identifying\ndifficult-to-detect colorectal polyps, which is effective in clinical areas\nwhere early detection prevents the development of colon cancer.",
      "tldr_zh": "该研究针对结肠息肉的语义分割问题，提出使用Bayesian神经网络方法来提供校准良好的预测，并量化不确定性，以避免传统深度学习模型的过拟合风险。研究者应用multiplicative normalized flows (MNF)和reparameterization trick在UNET、FPN和LINKNET架构上进行测试，包括确定性和Bayesian版本。结果显示，FPN + EfficientnetB7结合MNF的模型取得了state-of-the-art性能，IOU达0.94、Expected Calibration Error (ECE)为0.004，并在识别难检测息肉方面表现出色，有助于临床早期检测预防结肠癌。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "comments are welcome. 43 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16608v1",
      "published_date": "2024-07-23 16:13:27 UTC",
      "updated_date": "2024-07-23 16:13:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:14:58.709726"
    },
    {
      "arxiv_id": "2407.16602v2",
      "title": "Functional Acceleration for Policy Mirror Descent",
      "title_zh": "翻译失败",
      "authors": [
        "Veronica Chelu",
        "Doina Precup"
      ],
      "abstract": "We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based\nPMD update. By taking the functional route, our approach is independent of the\npolicy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a\nspecial case. We theoretically analyze several properties of this approach and\ncomplement with a numerical ablation study, which serves to illustrate the\npolicy optimization dynamics on the value polytope, relative to different\nalgorithmic design choices in this space. We further characterize numerically\nseveral features of the problem setting relevant for functional acceleration,\nand lastly, we investigate the impact of approximation on their learning\nmechanics.",
      "tldr_zh": "本文提出了一种将 functional acceleration 应用于 Policy Mirror Descent (PMD) 算法家族的方法，利用对偶性设计了基于动量(momentum-based)的 PMD 更新，以提升强化学习(RL)中的优化效率。该方法独立于策略参数化，适用于大规模优化，并将先前基于策略参数的动量应用作为特例。研究通过理论分析和数值消融研究(ablation study)探讨了策略优化在价值多面体(value polytope)上的动态，并评估了问题设置中功能加速相关特征及近似对学习机制的影响。实验结果表明，该方法在RL优化中显示出显著潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16602v2",
      "published_date": "2024-07-23 16:04:55 UTC",
      "updated_date": "2025-03-25 17:30:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:15:11.797030"
    },
    {
      "arxiv_id": "2407.16594v2",
      "title": "Flexible Generation of Preference Data for Recommendation Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Mungari",
        "Erica Coppolillo",
        "Ettore Ritacco",
        "Giuseppe Manco"
      ],
      "abstract": "Simulating a recommendation system in a controlled environment, to identify\nspecific behaviors and user preferences, requires highly flexible synthetic\ndata generation models capable of mimicking the patterns and trends of real\ndatasets. In this context, we propose HYDRA, a novel preferences data\ngeneration model driven by three main factors: user-item interaction level,\nitem popularity, and user engagement level. The key innovations of the proposed\nprocess include the ability to generate user communities characterized by\nsimilar item adoptions, reflecting real-world social influences and trends.\nAdditionally, HYDRA considers item popularity and user engagement as mixtures\nof different probability distributions, allowing for a more realistic\nsimulation of diverse scenarios. This approach enhances the model's capacity to\nsimulate a wide range of real-world cases, capturing the complexity and\nvariability found in actual user behavior. We demonstrate the effectiveness of\nHYDRA through extensive experiments on well-known benchmark datasets. The\nresults highlight its capability to replicate real-world data patterns,\noffering valuable insights for developing and testing recommendation systems in\na controlled and realistic manner. The code used to perform the experiments is\npublicly available at https://github.com/SimoneMungari/HYDRA.",
      "tldr_zh": "本论文提出 HYDRA，一种灵活的偏好数据生成模型，用于在受控环境中模拟推荐系统，从而识别用户行为和偏好。HYDRA 由用户-物品交互水平、物品流行度和用户参与水平三大因素驱动，其创新点包括生成具有类似物品采用的用户社区，以反映现实世界的社会影响和趋势，并通过概率分布混合模拟多样化场景。该模型在基准数据集上的广泛实验证明了其有效性，能够准确复制真实数据模式，为推荐系统的开发和测试提供宝贵的洞见。代码已在 GitHub 上公开可用。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16594v2",
      "published_date": "2024-07-23 15:53:17 UTC",
      "updated_date": "2025-05-16 10:53:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:15:23.343601"
    },
    {
      "arxiv_id": "2407.16593v1",
      "title": "A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions",
      "title_zh": "翻译失败",
      "authors": [
        "Giorgos Lysandrou",
        "Roma English Owen",
        "Vanja Popovic",
        "Grant Le Brun",
        "Aryo Pradipta Gema",
        "Beatrice Alex",
        "Elizabeth A. L. Fairley"
      ],
      "abstract": "There exists an invisible barrier between healthcare professionals'\nperception of a patient's clinical experience and the reality. This barrier may\nbe induced by the environment that hinders patients from sharing their\nexperiences openly with healthcare professionals. As patients are observed to\ndiscuss and exchange knowledge more candidly on social media, valuable insights\ncan be leveraged from these platforms. However, the abundance of non-patient\nposts on social media necessitates filtering out such irrelevant content to\ndistinguish the genuine voices of patients, a task we refer to as patient voice\nclassification. In this study, we analyse the importance of linguistic\ncharacteristics in accurately classifying patient voices. Our findings\nunderscore the essential role of linguistic and statistical text similarity\nanalysis in identifying common patterns among patient groups. These results\nallude to even starker differences in the way patients express themselves at a\ndisease level and across various therapeutic domains. Additionally, we\nfine-tuned a pre-trained Language Model on the combined datasets with similar\nlinguistic patterns, resulting in a highly accurate automatic patient voice\nclassification. Being the pioneering study on the topic, our focus on\nextracting authentic patient experiences from social media stands as a crucial\nstep towards advancing healthcare standards and fostering a patient-centric\napproach.",
      "tldr_zh": "本研究探讨了患者在社交媒体上的语言特征，以实现有效的患者声音分类（patient voice classification），旨在克服医护人员对患者经历认知的障碍。研究通过语言和统计文本相似性分析，发现患者群体间存在共同模式，但不同疾病水平和治疗领域（therapeutic domains）表达方式有显著差异。作者微调了预训练语言模型（Language Model）在结合相似语言模式的数据集上，实现了高准确率的自动分类。作为开创性研究，这一步骤有助于从社交媒体提取真实患者经历，推动患者中心医疗发展。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 4 figures, 5 tables, funded by Talking Medicines Limited",
      "pdf_url": "http://arxiv.org/pdf/2407.16593v1",
      "published_date": "2024-07-23 15:51:46 UTC",
      "updated_date": "2024-07-23 15:51:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:15:35.727161"
    },
    {
      "arxiv_id": "2407.16588v2",
      "title": "A Faster Branching Algorithm for the Maximum $k$-Defective Clique Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Chunyu Luo",
        "Yi Zhou",
        "Zhengren Wang",
        "Mingyu Xiao"
      ],
      "abstract": "A $k$-defective clique of an undirected graph $G$ is a subset of its vertices\nthat induces a nearly complete graph with a maximum of $k$ missing edges. The\nmaximum $k$-defective clique problem, which asks for the largest $k$-defective\nclique from the given graph, is important in many applications, such as social\nand biological network analysis. In the paper, we propose a new branching\nalgorithm that takes advantage of the structural properties of the\n$k$-defective clique and uses the efficient maximum clique algorithm as a\nsubroutine. As a result, the algorithm has a better asymptotic running time\nthan the existing ones. We also investigate upper-bounding techniques and\npropose a new upper bound utilizing the \\textit{conflict relationship} between\nvertex pairs. Because conflict relationship is common in many graph problems,\nwe believe that this technique can be potentially generalized. Finally,\nexperiments show that our algorithm outperforms state-of-the-art solvers on a\nwide range of open benchmarks.",
      "tldr_zh": "该论文针对最大 $k$-defective clique 问题提出了一种更快的分支算法，其中 $k$-defective clique 指一个子图最多缺失 $k$ 条边的近似团。该算法利用 $k$-defective clique 的结构特性，并将高效的最大团算法作为子例程，从而实现了比现有算法更好的渐进运行时间。研究者还引入了基于顶点对冲突关系（conflict relationship）的新上界技术，以进一步优化性能，并认为此技术可推广至其他图问题。实验结果显示，该算法在各种公开基准测试中优于最先进的求解器，尤其适用于社交和生物网络分析。",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "The accepted paper of confernece ECAI-2024 as well as the appendix",
      "pdf_url": "http://arxiv.org/pdf/2407.16588v2",
      "published_date": "2024-07-23 15:40:35 UTC",
      "updated_date": "2024-07-24 02:44:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:15:50.273090"
    },
    {
      "arxiv_id": "2407.18981v1",
      "title": "Prompt Injection Attacks on Large Language Models in Oncology",
      "title_zh": "针对肿瘤学领域大型语言模型的提示注入攻击",
      "authors": [
        "Jan Clusmann",
        "Dyke Ferber",
        "Isabella C. Wiest",
        "Carolin V. Schneider",
        "Titus J. Brinker",
        "Sebastian Foersch",
        "Daniel Truhn",
        "Jakob N. Kather"
      ],
      "abstract": "Vision-language artificial intelligence models (VLMs) possess medical\nknowledge and can be employed in healthcare in numerous ways, including as\nimage interpreters, virtual scribes, and general decision support systems.\nHowever, here, we demonstrate that current VLMs applied to medical tasks\nexhibit a fundamental security flaw: they can be attacked by prompt injection\nattacks, which can be used to output harmful information just by interacting\nwith the VLM, without any access to its parameters. We performed a quantitative\nstudy to evaluate the vulnerabilities to these attacks in four state of the art\nVLMs which have been proposed to be of utility in healthcare: Claude 3 Opus,\nClaude 3.5 Sonnet, Reka Core, and GPT-4o. Using a set of N=297 attacks, we show\nthat all of these models are susceptible. Specifically, we show that embedding\nsub-visual prompts in medical imaging data can cause the model to provide\nharmful output, and that these prompts are non-obvious to human observers.\nThus, our study demonstrates a key vulnerability in medical VLMs which should\nbe mitigated before widespread clinical adoption.",
      "tldr_zh": "该研究揭示了视觉语言模型（VLMs）在肿瘤学应用中的安全漏洞，特别是prompt injection attacks（提示注入攻击）。这些攻击可以通过简单交互（如嵌入医疗图像中的子视觉提示）使模型输出有害信息，而无需访问模型参数。研究对四种先进VLMs（Claude 3 Opus、Claude 3.5 Sonnet、Reka Core 和 GPT-4o）进行了量化评估，使用297个攻击样本，证明所有模型均易受影响，且这些提示不易被人类察觉。结果显示，这种漏洞可能导致临床风险，论文强调在广泛采用前需采取措施缓解此问题。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "57 Pages, 5 Figures",
      "pdf_url": "http://arxiv.org/pdf/2407.18981v1",
      "published_date": "2024-07-23 15:29:57 UTC",
      "updated_date": "2024-07-23 15:29:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:15:59.021991"
    },
    {
      "arxiv_id": "2407.16732v2",
      "title": "PyBench: Evaluating LLM Agent on various real-world coding tasks",
      "title_zh": "PyBench：评估 LLM Agent 在各种真实世界编码任务中的性能",
      "authors": [
        "Yaolun Zhang",
        "Yinxu Pan",
        "Yudong Wang",
        "Jie Cai"
      ],
      "abstract": "The LLM Agent, equipped with a code interpreter, is capable of automatically\nsolving real-world coding tasks, such as data analysis and image editing.\n  However, existing benchmarks primarily focus on either simplistic tasks, such\nas completing a few lines of code, or on extremely complex and specific tasks\nat the repository level, neither of which are representative of various daily\ncoding tasks.\n  To address this gap, we introduce \\textbf{PyBench}, a benchmark encompassing\nfive main categories of real-world tasks, covering more than 10 types of files.\nGiven a high-level user query and related files, the LLM Agent needs to reason\nand execute Python code via a code interpreter for a few turns before making a\nformal response to fulfill the user's requirements. Successfully addressing\ntasks in PyBench demands a robust understanding of various Python packages,\nsuperior reasoning capabilities, and the ability to incorporate feedback from\nexecuted code. Our evaluations indicate that current open-source LLMs are\nstruggling with these tasks. Hence, we conduct analysis and experiments on four\nkinds of datasets proving that comprehensive abilities are needed for PyBench.\nOur fine-tuned 8B size model: \\textbf{PyLlama3} achieves an exciting\nperformance on PyBench which surpasses many 33B and 70B size models. Our\nBenchmark, Training Dataset, and Model are available at:\n{https://github.com/Mercury7353/PyBench}",
      "tldr_zh": "本研究引入了PyBench基准测试，用于评估LLM Agent在各种真实世界编码任务（如数据分析和图像编辑）上的性能，以填补现有基准测试的不足。PyBench涵盖五大类别和超过10种文件类型，要求LLM Agent基于用户查询和相关文件，通过多轮推理、执行Python代码以及处理反馈来完成任务，这需要对Python包的深入理解和强大的推理能力。实验结果显示，当前开源LLM在PyBench上表现不佳，但微调的8B模型PyLlama3超越了许多33B和70B模型，证明了综合能力的必要性，并提供了开源资源以促进进一步研究。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16732v2",
      "published_date": "2024-07-23 15:23:14 UTC",
      "updated_date": "2024-08-03 03:00:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:16:10.470646"
    },
    {
      "arxiv_id": "2407.16564v2",
      "title": "Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning",
      "title_zh": "翻译失败",
      "authors": [
        "Fang-Duo Tsai",
        "Shih-Lun Wu",
        "Haven Kim",
        "Bo-Yu Chen",
        "Hao-Chung Cheng",
        "Yi-Hsuan Yang"
      ],
      "abstract": "Text-to-music models allow users to generate nearly realistic musical audio\nwith textual commands. However, editing music audios remains challenging due to\nthe conflicting desiderata of performing fine-grained alterations on the audio\nwhile maintaining a simple user interface. To address this challenge, we\npropose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to\npretrained text-to-music models. We utilize AudioMAE to extract features from\nthe input audio, and construct attention-based adapters to feedthese features\ninto the internal layers of AudioLDM2, a diffusion-based text-to-music model.\nWith 22M trainable parameters, AP-Adapter empowers users to harness both global\n(e.g., genre and timbre) and local (e.g., melody) aspects of music, using the\noriginal audio and a short text as inputs. Through objective and subjective\nstudies, we evaluate AP-Adapter on three tasks: timbre transfer, genre\ntransfer, and accompaniment generation. Additionally, we demonstrate its\neffectiveness on out-of-domain audios containing unseen instruments during\ntraining.",
      "tldr_zh": "该研究提出 Audio Prompt Adapter (AP-Adapter)，一个轻量级模块，用于增强预训练文本到音乐模型的编辑能力，仅需 22M 可训练参数。通过 AudioMAE 提取音频特征，并利用注意力-based 适配器将这些特征注入 AudioLDM2 模型的内部层，用户可基于原始音频和简短文本进行音乐编辑，包括全局方面（如流派和音色）和局部方面（如旋律）。实验评估了 AP-Adapter 在音色转移、流派转移和伴奏生成任务上的性能，并在训练外音频（如未见乐器）上展示了其鲁棒性。总的来说，该方法简化了音乐编辑过程，同时保持用户界面简单。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by the 25th International Society for Music Information\n  Retrieval (ISMIR)",
      "pdf_url": "http://arxiv.org/pdf/2407.16564v2",
      "published_date": "2024-07-23 15:16:18 UTC",
      "updated_date": "2024-07-24 11:12:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:16:23.909763"
    },
    {
      "arxiv_id": "2407.16557v3",
      "title": "Patched RTC: evaluating LLMs for diverse software development tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Asankhaya Sharma"
      ],
      "abstract": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows.",
      "tldr_zh": "本文提出Patched RTC，一种新型评估技术，用于评估Large Language Models (LLMs)在多样软件开发任务（如bug fixing、code review和documentation updates）中的性能。Patched RTC扩展了Round-Trip Correctness方法，提供一个无需人工干预的自评估框架，测量模型响应的consistency和robustness，并与任务特定准确性指标相关联。实验比较GPT-3.5和GPT-4模型后，发现Patched RTC能有效区分模型性能和任务难度，同时通过consistency prompts指导prompt优化和模型选择，为复杂软件开发工作流程提供可靠评估工具。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16557v3",
      "published_date": "2024-07-23 15:12:14 UTC",
      "updated_date": "2025-04-29 23:30:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:16:35.889173"
    },
    {
      "arxiv_id": "2407.16729v1",
      "title": "PateGail: A Privacy-Preserving Mobility Trajectory Generator with Imitation Learning",
      "title_zh": "PateGail：一种基于模仿学习的隐私保护移动轨迹生成器",
      "authors": [
        "Huandong Wang",
        "Changzheng Gao",
        "Yuchen Wu",
        "Depeng Jin",
        "Lina Yao",
        "Yong Li"
      ],
      "abstract": "Generating human mobility trajectories is of great importance to solve the\nlack of large-scale trajectory data in numerous applications, which is caused\nby privacy concerns. However, existing mobility trajectory generation methods\nstill require real-world human trajectories centrally collected as the training\ndata, where there exists an inescapable risk of privacy leakage. To overcome\nthis limitation, in this paper, we propose PateGail, a privacy-preserving\nimitation learning model to generate mobility trajectories, which utilizes the\npowerful generative adversary imitation learning model to simulate the\ndecision-making process of humans. Further, in order to protect user privacy,\nwe train this model collectively based on decentralized mobility data stored in\nuser devices, where personal discriminators are trained locally to distinguish\nand reward the real and generated human trajectories. In the training process,\nonly the generated trajectories and their rewards obtained based on personal\ndiscriminators are shared between the server and devices, whose privacy is\nfurther preserved by our proposed perturbation mechanisms with theoretical\nproof to satisfy differential privacy. Further, to better model the human\ndecision-making process, we propose a novel aggregation mechanism of the\nrewards obtained from personal discriminators. We theoretically prove that\nunder the reward obtained based on the aggregation mechanism, our proposed\nmodel maximizes the lower bound of the discounted total rewards of users.\nExtensive experiments show that the trajectories generated by our model are\nable to resemble real-world trajectories in terms of five key statistical\nmetrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore,\nwe demonstrate that the synthetic trajectories are able to efficiently support\npractical applications, including mobility prediction and location\nrecommendation.",
      "tldr_zh": "论文提出 PateGail，一种基于 Imitation Learning 的隐私保护移动轨迹生成模型，旨在解决现有方法因集中收集数据而带来的隐私泄露风险。 该模型利用生成性对抗模仿学习在用户设备上进行分散训练，仅分享生成轨迹和基于个人鉴别器的奖励，并通过差分隐私(Differential Privacy)机制进行扰动，以理论上确保隐私安全。 实验结果显示，PateGail 生成的轨迹在五个关键统计指标上比最先进算法提高 48.03%，并能有效支持实际应用如移动预测和位置推荐。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16729v1",
      "published_date": "2024-07-23 14:59:23 UTC",
      "updated_date": "2024-07-23 14:59:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:16:48.617038"
    },
    {
      "arxiv_id": "2407.16533v1",
      "title": "HAPFI: History-Aware Planning based on Fused Information",
      "title_zh": "翻译失败",
      "authors": [
        "Sujin Jeon",
        "Suyeon Shin",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Embodied Instruction Following (EIF) is a task of planning a long sequence of\nsub-goals given high-level natural language instructions, such as \"Rinse a\nslice of lettuce and place on the white table next to the fork\". To\nsuccessfully execute these long-term horizon tasks, we argue that an agent must\nconsider its past, i.e., historical data, when making decisions in each step.\nNevertheless, recent approaches in EIF often neglects the knowledge from\nhistorical data and also do not effectively utilize information across the\nmodalities. To this end, we propose History-Aware Planning based on Fused\nInformation (HAPFI), effectively leveraging the historical data from diverse\nmodalities that agents collect while interacting with the environment.\nSpecifically, HAPFI integrates multiple modalities, including historical RGB\nobservations, bounding boxes, sub-goals, and high-level instructions, by\neffectively fusing modalities via our Mutually Attentive Fusion method. Through\nexperiments with diverse comparisons, we show that an agent utilizing\nhistorical multi-modal information surpasses all the compared methods that\nneglect the historical data in terms of action planning capability, enabling\nthe generation of well-informed action plans for the next step. Moreover, we\nprovided qualitative evidence highlighting the significance of leveraging\nhistorical multi-modal data, particularly in scenarios where the agent\nencounters intermediate failures, showcasing its robust re-planning\ncapabilities.",
      "tldr_zh": "本文针对Embodied Instruction Following (EIF)任务，提出HAPFI（History-Aware Planning based on Fused Information）方法，该方法强调代理在决策时利用历史数据来处理长序列子目标规划。HAPFI通过Mutually Attentive Fusion技术融合多模态信息，包括历史RGB观察、边界框、子目标和高层自然语言指令，从而提升行动规划的准确性和鲁棒性。实验结果表明，HAPFI在各种场景中优于忽略历史数据的基线方法，特别是在遇到中间失败时展现出强大的重新规划能力。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 3 figures, published to ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16533v1",
      "published_date": "2024-07-23 14:46:07 UTC",
      "updated_date": "2024-07-23 14:46:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:17:00.697197"
    },
    {
      "arxiv_id": "2407.16728v1",
      "title": "Distributed Difference of Convex Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Vivek Khatana",
        "Murti V. Salapaka"
      ],
      "abstract": "In this article, we focus on solving a class of distributed optimization\nproblems involving $n$ agents with the local objective function at every agent\n$i$ given by the difference of two convex functions $f_i$ and $g_i$\n(difference-of-convex (DC) form), where $f_i$ and $g_i$ are potentially\nnonsmooth. The agents communicate via a directed graph containing $n$ nodes. We\ncreate smooth approximations of the functions $f_i$ and $g_i$ and develop a\ndistributed algorithm utilizing the gradients of the smooth surrogates and a\nfinite-time approximate consensus protocol. We term this algorithm as\nDDC-Consensus. The developed DDC-Consensus algorithm allows for non-symmetric\ndirected graph topologies and can be synthesized distributively. We establish\nthat the DDC-Consensus algorithm converges to a stationary point of the\nnonconvex distributed optimization problem. The performance of the\nDDC-Consensus algorithm is evaluated via a simulation study to solve a\nnonconvex DC-regularized distributed least squares problem. The numerical\nresults corroborate the efficacy of the proposed algorithm.",
      "tldr_zh": "本论文探讨了分布式优化问题，其中 n 个代理的局部目标函数采用差分凸 (Difference-of-Convex, DC) 形式，由两个可能非光滑的凸函数 f_i 和 g_i 组成，代理通过有向图通信。作者开发了 DDC-Consensus 算法，通过创建 f_i 和 g_i 的平滑逼近，并结合平滑替代函数的梯度和有限时间近似共识协议，实现算法在非对称有向图上的分布式合成，并证明其收敛到非凸优化问题的驻点。实验结果通过模拟非凸 DC-正则化分布式最小二乘问题验证了该算法的有效性和优越性。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.DC",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "9 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16728v1",
      "published_date": "2024-07-23 14:41:32 UTC",
      "updated_date": "2024-07-23 14:41:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:17:12.584305"
    },
    {
      "arxiv_id": "2407.16526v1",
      "title": "Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Aristeidis Panos",
        "Rahaf Aljundi",
        "Daniel Olmeda Reino",
        "Richard E Turner"
      ],
      "abstract": "Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.",
      "tldr_zh": "本研究针对视觉语言模型 (VLMs) 依赖预训练视觉编码器（如 CLIP）的问题，指出这些编码器存在图像理解错误，导致模型性能不佳。作者提出一种高效鲁棒的更新方法，通过选择性局部更新编码器，提高了在错误数据上的表现，同时维持整体鲁棒性。该方法在持续少样本更新中表现出色，并具备理论基础、通用性和计算效率优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16526v1",
      "published_date": "2024-07-23 14:39:40 UTC",
      "updated_date": "2024-07-23 14:39:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:17:33.795473"
    },
    {
      "arxiv_id": "2407.16514v1",
      "title": "Is 3D Convolution with 5D Tensors Really Necessary for Video Analysis?",
      "title_zh": "翻译失败",
      "authors": [
        "Habib Hajimolahoseini",
        "Walid Ahmed",
        "Austin Wen",
        "Yang Liu"
      ],
      "abstract": "In this paper, we present a comprehensive study and propose several novel\ntechniques for implementing 3D convolutional blocks using 2D and/or 1D\nconvolutions with only 4D and/or 3D tensors. Our motivation is that 3D\nconvolutions with 5D tensors are computationally very expensive and they may\nnot be supported by some of the edge devices used in real-time applications\nsuch as robots. The existing approaches mitigate this by splitting the 3D\nkernels into spatial and temporal domains, but they still use 3D convolutions\nwith 5D tensors in their implementations. We resolve this issue by introducing\nsome appropriate 4D/3D tensor reshaping as well as new combination techniques\nfor spatial and temporal splits. The proposed implementation methods show\nsignificant improvement both in terms of efficiency and accuracy. The\nexperimental results confirm that the proposed spatio-temporal processing\nstructure outperforms the original model in terms of speed and accuracy using\nonly 4D tensors with fewer parameters.",
      "tldr_zh": "本文质疑视频分析中是否必须使用 5D 张量的 3D Convolution，因为其计算开销大且不适用于边缘设备如机器人。作者提出新型技术，使用 2D 和/或 1D convolutions 仅以 4D 和/或 3D tensors 实现 3D 卷积块，通过张量重塑和空间-时间组合方法来优化处理。实验结果表明，该方法在速度和准确性上均优于原模型，且参数更少，从而提升了视频分析的效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16514v1",
      "published_date": "2024-07-23 14:30:51 UTC",
      "updated_date": "2024-07-23 14:30:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:17:36.927711"
    },
    {
      "arxiv_id": "2407.16496v2",
      "title": "Articulation Work and Tinkering for Fairness in Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Miriam Fahimi",
        "Mayra Russo",
        "Kristen M. Scott",
        "Maria-Esther Vidal",
        "Bettina Berendt",
        "Katharina Kinder-Kurlanda"
      ],
      "abstract": "The field of fair AI aims to counter biased algorithms through computational\nmodelling. However, it faces increasing criticism for perpetuating the use of\noverly technical and reductionist methods. As a result, novel approaches appear\nin the field to address more socially-oriented and interdisciplinary (SOI)\nperspectives on fair AI. In this paper, we take this dynamic as the starting\npoint to study the tension between computer science (CS) and SOI research. By\ndrawing on STS and CSCW theory, we position fair AI research as a matter of\n'organizational alignment': what makes research 'doable' is the successful\nalignment of three levels of work organization (the social world, the\nlaboratory, and the experiment). Based on qualitative interviews with CS\nresearchers, we analyze the tasks, resources, and actors required for doable\nresearch in the case of fair AI. We find that CS researchers engage with SOI\nresearch to some extent, but organizational conditions, articulation work, and\nambiguities of the social world constrain the doability of SOI research for\nthem. Based on our findings, we identify and discuss problems for aligning CS\nand SOI as fair AI continues to evolve.",
      "tldr_zh": "本论文探讨了公平 AI 研究中计算机科学（CS）和社会导向跨学科（SOI）视角之间的张力，批评现有方法过于技术化和简化化，并通过 STS 和 CSCW 理论将公平 AI 视为“组织对齐”的问题。研究者通过质性访谈分析了 CS 研究者的任务、资源和参与者，发现他们虽部分参与 SOI 研究，但组织条件、articulation work 和社会世界的模糊性限制了 SOI 研究的可行性。最终，论文识别并讨论了在公平 AI 持续演进中对齐 CS 和 SOI 的潜在挑战，以推动更全面的 AI 公平性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "K.4.3; I.2.0"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16496v2",
      "published_date": "2024-07-23 14:11:12 UTC",
      "updated_date": "2024-08-28 12:20:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:17:49.780232"
    },
    {
      "arxiv_id": "2407.16485v3",
      "title": "Learning Constraint Network from Demonstrations via Positive-Unlabeled Learning with Memory Replay",
      "title_zh": "翻译失败",
      "authors": [
        "Baiyu Peng",
        "Aude Billard"
      ],
      "abstract": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.",
      "tldr_zh": "这篇论文提出了一种基于 Positive-Unlabeled Learning 的方法，从专家演示中学习约束网络，以处理未知或难以指定的任务约束。该方法将演示数据视为正样本（可行数据），并通过学习一个次优策略生成高奖励但可能不可行的轨迹作为无标签数据，然后使用后处理 PU 学习技术训练一个可行-不可行分类器（即约束模型）。此外，引入 Memory Replay 机制来记录和重用之前迭代的样本，防止模型遗忘；实验在 Mujoco 环境中验证了该方法的有效性，能够准确推断连续非线性约束，并在约束准确性和策略安全性上优于基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16485v3",
      "published_date": "2024-07-23 14:00:18 UTC",
      "updated_date": "2025-01-16 11:59:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:18:01.674018"
    },
    {
      "arxiv_id": "2408.06346v1",
      "title": "Closing the Affective Loop via Experience-Driven Reinforcement Learning Designers",
      "title_zh": "翻译失败",
      "authors": [
        "Matthew Barthet",
        "Diogo Branco",
        "Roberto Gallotta",
        "Ahmed Khalifa",
        "Georgios N. Yannakakis"
      ],
      "abstract": "Autonomously tailoring content to a set of predetermined affective patterns\nhas long been considered the holy grail of affect-aware human-computer\ninteraction at large. The experience-driven procedural content generation\nframework realises this vision by searching for content that elicits a certain\nexperience pattern to a user. In this paper, we propose a novel reinforcement\nlearning (RL) framework for generating affect-tailored content, and we test it\nin the domain of racing games. Specifically, the experience-driven RL (EDRL)\nframework is given a target arousal trace, and it then generates a racetrack\nthat elicits the desired affective responses for a particular type of player.\nEDRL leverages a reward function that assesses the affective pattern of any\ngenerated racetrack from a corpus of arousal traces. Our findings suggest that\nEDRL can accurately generate affect-driven racing game levels according to a\ndesigner's style and outperforms search-based methods for personalised content\ngeneration. The method is not only directly applicable to game content\ngeneration tasks but also employable broadly to any domain that uses content\nfor affective adaptation.",
      "tldr_zh": "这篇论文提出了一种名为 Experience-Driven Reinforcement Learning (EDRL) 的新型强化学习（RL）框架，用于生成针对特定情感模式（如唤醒轨迹）的个性化内容，实现情感驱动的人机交互。EDRL 通过一个基于唤醒轨迹语料库的奖励函数，在赛车游戏领域测试生成赛道，以引发特定玩家的期望情感响应，并根据设计师风格优化内容。实验结果表明，EDRL 优于基于搜索的方法，在准确性和个性化方面表现出色，且可扩展应用于游戏外其他情感适应领域。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "9 pages, 4 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2408.06346v1",
      "published_date": "2024-07-23 13:56:43 UTC",
      "updated_date": "2024-07-23 13:56:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:18:12.799935"
    },
    {
      "arxiv_id": "2407.16726v1",
      "title": "Topology Reorganized Graph Contrastive Learning with Mitigating Semantic Drift",
      "title_zh": "拓扑重组的图对比学习以缓解语义漂移",
      "authors": [
        "Jiaqiang Zhang",
        "Songcan Chen"
      ],
      "abstract": "Graph contrastive learning (GCL) is an effective paradigm for node\nrepresentation learning in graphs. The key components hidden behind GCL are\ndata augmentation and positive-negative pair selection. Typical data\naugmentations in GCL, such as uniform deletion of edges, are generally blind\nand resort to local perturbation, which is prone to producing under-diversity\nviews. Additionally, there is a risk of making the augmented data traverse to\nother classes. Moreover, most methods always treat all other samples as\nnegatives. Such a negative pairing naturally results in sampling bias and\nlikewise may make the learned representation suffer from semantic drift.\nTherefore, to increase the diversity of the contrastive view, we propose two\nsimple and effective global topological augmentations to compensate current\nGCL. One is to mine the semantic correlation between nodes in the feature\nspace. The other is to utilize the algebraic properties of the adjacency matrix\nto characterize the topology by eigen-decomposition. With the help of both, we\ncan retain important edges to build a better view. To reduce the risk of\nsemantic drift, a prototype-based negative pair selection is further designed\nwhich can filter false negative samples. Extensive experiments on various tasks\ndemonstrate the advantages of the model compared to the state-of-the-art\nmethods.",
      "tldr_zh": "该论文针对图对比学习（GCL）中的数据增强和负样本选择问题，提出了一种拓扑重组策略，以缓解语义漂移风险。方法包括两种全局拓扑增强：一是基于特征空间挖掘节点间的语义相关性，二是利用邻接矩阵的特征分解保留重要边；此外，还设计了基于原型的负样本选择机制来过滤假负样本。实验在多种任务上证明，该模型相较于最先进方法表现出显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16726v1",
      "published_date": "2024-07-23 13:55:33 UTC",
      "updated_date": "2024-07-23 13:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:18:25.114048"
    },
    {
      "arxiv_id": "2407.16482v1",
      "title": "BONES: a Benchmark fOr Neural Estimation of Shapley values",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Napolitano",
        "Luca Cagliero"
      ],
      "abstract": "Shapley Values are concepts established for eXplainable AI. They are used to\nexplain black-box predictive models by quantifying the features' contributions\nto the model's outcomes. Since computing the exact Shapley Values is known to\nbe computationally intractable on real-world datasets, neural estimators have\nemerged as alternative, more scalable approaches to get approximated Shapley\nValues estimates. However, experiments with neural estimators are currently\nhard to replicate as algorithm implementations, explainer evaluators, and\nresults visualizations are neither standardized nor promptly usable. To bridge\nthis gap, we present BONES, a new benchmark focused on neural estimation of\nShapley Value. It provides researchers with a suite of state-of-the-art neural\nand traditional estimators, a set of commonly used benchmark datasets, ad hoc\nmodules for training black-box models, as well as specific functions to easily\ncompute the most popular evaluation metrics and visualize results. The purpose\nis to simplify XAI model usage, evaluation, and comparison. In this paper, we\nshowcase BONES results and visualizations for XAI model benchmarking on both\ntabular and image data. The open-source library is available at the following\nlink: https://github.com/DavideNapolitano/BONES.",
      "tldr_zh": "该研究提出BONES，一个用于神经估计Shapley Values的基准框架，旨在解决Shapley Values在可解释AI (XAI)中计算复杂性问题，并提供可扩展的神经估计器作为替代方案。BONES包括最先进的神经和传统估计器、常用数据集、黑盒模型训练模块，以及评估指标和可视化工具，以标准化XAI模型的实验流程和比较。实验结果展示了BONES在表格和图像数据上的基准表现，并通过开源库（https://github.com/DavideNapolitano/BONES）促进研究的可复制性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16482v1",
      "published_date": "2024-07-23 13:53:22 UTC",
      "updated_date": "2024-07-23 13:53:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:18:36.624798"
    },
    {
      "arxiv_id": "2407.16470v3",
      "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kenza Benkirane",
        "Laura Gongas",
        "Shahar Pelles",
        "Naomi Fuchs",
        "Joshua Darmon",
        "Pontus Stenetorp",
        "David Ifeoluwa Adelani",
        "Eduardo Sánchez"
      ],
      "abstract": "Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates sentence-level\nhallucination detection approaches using Large Language Models (LLMs) and\nsemantic similarity within massively multilingual embeddings. Our study spans\n16 language directions, covering HRLs, LRLs, with diverse scripts. We find that\nthe choice of model is essential for performance. On average, for HRLs,\nLlama3-70B outperforms the previous state of the art by as much as 0.16 MCC\n(Matthews Correlation Coefficient). However, for LRLs we observe that Claude\nSonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our\nstudy is that LLMs can achieve performance comparable or even better than\npreviously proposed models, despite not being explicitly trained for any\nmachine translation task. However, their advantage is less significant for\nLRLs.",
      "tldr_zh": "该论文探讨了使用大型语言模型(LLMs)检测机器翻译(MT)中的幻觉(hallucinations)，特别针对高资源语言(HRLs)和低资源语言(LRLs)的挑战。研究评估了句子级幻觉检测方法，结合LLMs和语义相似性于多语言嵌入，涵盖16个语言方向和不同脚本。结果显示，对于HRLs，Llama3-70B模型比先前最先进模型提高0.16 MCC(Matthews Correlation Coefficient)；而对于LRLs，Claude Sonnet模型平均领先0.03 MCC。总体而言，LLMs尽管未专门训练用于MT任务，却能实现与现有模型相当或更好的性能，但其优势在LRLs上不那么显著。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "Authors Kenza Benkirane and Laura Gongas contributed equally to this\n  work",
      "pdf_url": "http://arxiv.org/pdf/2407.16470v3",
      "published_date": "2024-07-23 13:40:54 UTC",
      "updated_date": "2024-10-20 15:01:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:18:49.649787"
    },
    {
      "arxiv_id": "2407.16467v2",
      "title": "Side-Channel Analysis of OpenVINO-based Neural Network Models",
      "title_zh": "基于 OpenVINO 的神经网络模型的侧通道分析",
      "authors": [
        "Dirmanto Jap",
        "Jakub Breier",
        "Zdenko Lehocký",
        "Shivam Bhasin",
        "Xiaolu Hou"
      ],
      "abstract": "Embedded devices with neural network accelerators offer great versatility for\ntheir users, reducing the need to use cloud-based services. At the same time,\nthey introduce new security challenges in the area of hardware attacks, the\nmost prominent being side-channel analysis (SCA). It was shown that SCA can\nrecover model parameters with a high accuracy, posing a threat to entities that\nwish to keep their models confidential. In this paper, we explore the\nsusceptibility of quantized models implemented in OpenVINO, an embedded\nframework for deploying neural networks on embedded and Edge devices. We show\nthat it is possible to recover model parameters with high precision, allowing\nthe recovered model to perform very close to the original one. Our experiments\non GoogleNet v1 show only a 1% difference in the Top 1 and a 0.64% difference\nin the Top 5 accuracies.",
      "tldr_zh": "本研究探讨了基于 OpenVINO 的量化神经网络模型在侧信道分析 (SCA) 攻击下的安全性问题，强调嵌入式设备使用神经网络加速器虽减少了对云服务的依赖，但易遭受硬件攻击。研究团队通过 SCA 方法成功高精度恢复模型参数，并在 GoogleNet v1 上进行实验，结果显示恢复模型的 Top 1 准确率仅比原模型低 1%，Top 5 准确率差 0.64%。这一发现突显了模型保密性的潜在风险，并为提升嵌入式框架的安全性提供了重要依据。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16467v2",
      "published_date": "2024-07-23 13:33:37 UTC",
      "updated_date": "2024-08-20 11:48:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:19:01.135785"
    },
    {
      "arxiv_id": "2407.16444v1",
      "title": "Psychomatics -- A Multidisciplinary Framework for Understanding Artificial Minds",
      "title_zh": "翻译失败",
      "authors": [
        "Giuseppe Riva",
        "Fabrizia Mantovani",
        "Brenda K. Wiederhold",
        "Antonella Marchetti",
        "Andrea Gaggioli"
      ],
      "abstract": "Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.",
      "tldr_zh": "这篇论文引入了 Psychomatics 框架，这是一个跨学科方法，结合认知科学、语言学和计算机科学，以理解大型语言模型 (LLMs) 的高级功能，特别是它们如何获取、学习、记忆和使用信息。框架采用比较方法，从理论问题入手，探讨人类和 LLMs 在语言发展和使用上的差异，例如 LLMs 能映射复杂语言模式并遵循 Grice's Cooperative Principle，但缺乏人类认知中的经验、情感和 physical embodiment 元素。研究发现，这种缺失使 LLMs 无法完全捕捉人类感知、行动与认知的互动。最终，Psychomatics 框架有望提供关于语言、认知和智能的深刻洞见，并推动开发更具人性化特性的 AI 系统。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 4 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16444v1",
      "published_date": "2024-07-23 12:53:41 UTC",
      "updated_date": "2024-07-23 12:53:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:19:14.267997"
    },
    {
      "arxiv_id": "2407.16397v1",
      "title": "On ADMM in Heterogeneous Federated Learning: Personalization, Robustness, and Fairness",
      "title_zh": "翻译失败",
      "authors": [
        "Shengkun Zhu",
        "Jinshan Zeng",
        "Sheng Wang",
        "Yuan Sun",
        "Xiaodong Li",
        "Yuan Yao",
        "Zhiyong Peng"
      ],
      "abstract": "Statistical heterogeneity is a root cause of tension among accuracy,\nfairness, and robustness of federated learning (FL), and is key in paving a\npath forward. Personalized FL (PFL) is an approach that aims to reduce the\nimpact of statistical heterogeneity by developing personalized models for\nindividual users, while also inherently providing benefits in terms of fairness\nand robustness. However, existing PFL frameworks focus on improving the\nperformance of personalized models while neglecting the global model. Moreover,\nthese frameworks achieve sublinear convergence rates and rely on strong\nassumptions. In this paper, we propose FLAME, an optimization framework by\nutilizing the alternating direction method of multipliers (ADMM) to train\npersonalized and global models. We propose a model selection strategy to\nimprove performance in situations where clients have different types of\nheterogeneous data. Our theoretical analysis establishes the global convergence\nand two kinds of convergence rates for FLAME under mild assumptions. We\ntheoretically demonstrate that FLAME is more robust and fair than the\nstate-of-the-art methods on a class of linear problems. Our experimental\nfindings show that FLAME outperforms state-of-the-art methods in convergence\nand accuracy, and it achieves higher test accuracy under various attacks and\nperforms more uniformly across clients.",
      "tldr_zh": "这篇论文探讨了异质联邦学习（Federated Learning）中统计异质性对准确性、公平性和鲁棒性造成的影响，并提出 FLAME 框架，利用交替方向乘子法（ADMM）来同时训练个性化模型和全局模型。FLAME 包括一个模型选择策略，以处理不同类型异质数据，并理论上证明了其在温和假设下的全局收敛和两种收敛率，同时展示了比现有方法更强的鲁棒性和公平性。实验结果表明，FLAME 在收敛速度、准确性、抗攻击能力和客户端间均匀性能上均优于最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2311.06756",
      "pdf_url": "http://arxiv.org/pdf/2407.16397v1",
      "published_date": "2024-07-23 11:35:42 UTC",
      "updated_date": "2024-07-23 11:35:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:19:26.885076"
    },
    {
      "arxiv_id": "2408.01449v1",
      "title": "AI Act for the Working Programmer",
      "title_zh": "翻译失败",
      "authors": [
        "Holger Hermanns",
        "Anne Lauber-Rönsberg",
        "Philip Meinel",
        "Sarah Sterz",
        "Hanwei Zhang"
      ],
      "abstract": "The European AI Act is a new, legally binding instrument that will enforce\ncertain requirements on the development and use of AI technology potentially\naffecting people in Europe. It can be expected that the stipulations of the\nAct, in turn, are going to affect the work of many software engineers, software\ntesters, data engineers, and other professionals across the IT sector in Europe\nand beyond. The 113 articles, 180 recitals, and 13 annexes that make up the Act\ncover 144 pages. This paper aims at providing an aid for navigating the Act\nfrom the perspective of some professional in the software domain, termed \"the\nworking programmer\", who feels the need to know about the stipulations of the\nAct.",
      "tldr_zh": "这篇论文针对软件工程师、测试人员和数据工程师等从业者，提供了欧洲 AI Act 的实用导航指南，以帮助他们理解该法案对 AI 技术开发和使用的规定。欧洲 AI Act 是一个新的法律框架，包括 113 条文章、180 条序言和 13 个附件，总计 144 页，旨在规范 AI 应用并潜在影响欧洲及全球 IT 专业人士的工作。论文的核心贡献在于从“working programmer”的视角总结和解读法案的关键内容，促进合规性和实际应用。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CY",
      "comment": "25 pages, 2 figures; submitted to AISoLA 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01449v1",
      "published_date": "2024-07-23 11:30:20 UTC",
      "updated_date": "2024-07-23 11:30:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:19:36.612174"
    },
    {
      "arxiv_id": "2407.21053v1",
      "title": "Knowledge Models for Cancer Clinical Practice Guidelines : Construction, Management and Usage in Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Pralaypati Ta",
        "Bhumika Gupta",
        "Arihant Jain",
        "Sneha Sree C",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "An automated knowledge modeling algorithm for Cancer Clinical Practice\nGuidelines (CPGs) extracts the knowledge contained in the CPG documents and\ntransforms it into a programmatically interactable, easy-to-update structured\nmodel with minimal human intervention. The existing automated algorithms have\nminimal scope and cannot handle the varying complexity of the knowledge content\nin the CPGs for different cancer types. This work proposes an improved\nautomated knowledge modeling algorithm to create knowledge models from the\nNational Comprehensive Cancer Network (NCCN) CPGs in Oncology for different\ncancer types. The proposed algorithm has been evaluated with NCCN CPGs for four\ndifferent cancer types. We also proposed an algorithm to compare the knowledge\nmodels for different versions of a guideline to discover the specific changes\nintroduced in the treatment protocol of a new version. We created a\nquestion-answering (Q&A) framework with the guideline knowledge models as the\naugmented knowledge base to study our ability to query the knowledge models. We\ncompiled a set of 32 question-answer pairs derived from two reliable data\nsources for the treatment of Non-Small Cell Lung Cancer (NSCLC) to evaluate the\nQ&A framework. The framework was evaluated against the question-answer pairs\nfrom one data source, and it can generate the answers with 54.5% accuracy from\nthe treatment algorithm and 81.8% accuracy from the discussion part of the NCCN\nNSCLC guideline knowledge model.",
      "tldr_zh": "本研究提出了一种改进的自动化知识建模算法，用于从National Comprehensive Cancer Network (NCCN) Cancer Clinical Practice Guidelines (CPGs)中提取知识，并将其转化为可编程交互且易于更新的结构化模型，以应对不同癌症类型知识复杂性的挑战。算法已应用于四种癌症类型的NCCN CPGs，并引入了一个算法来比较指南不同版本的知识模型，从而识别治疗协议的具体变化。研究还构建了一个Question Answering (Q&A)框架，以CPGs知识模型作为增强知识库，并针对Non-Small Cell Lung Cancer (NSCLC)编译了32对问答对进行评估，结果显示Q&A框架从治疗算法部分获得54.5%的准确率，从讨论部分获得81.8%的准确率。总的来说，该工作为临床指南的自动化管理和查询提供了高效工具，提升了癌症治疗决策的精确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21053v1",
      "published_date": "2024-07-23 11:26:40 UTC",
      "updated_date": "2024-07-23 11:26:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:19:49.881772"
    },
    {
      "arxiv_id": "2407.18274v1",
      "title": "Adaptive Differentially Private Structural Entropy Minimization for Unsupervised Social Event Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiwei Yang",
        "Yuecen Wei",
        "Haoran Li",
        "Qian Li",
        "Lei Jiang",
        "Li Sun",
        "Xiaoyan Yu",
        "Chunming Hu",
        "Hao Peng"
      ],
      "abstract": "Social event detection refers to extracting relevant message clusters from\nsocial media data streams to represent specific events in the real world.\nSocial event detection is important in numerous areas, such as opinion\nanalysis, social safety, and decision-making. Most current methods are\nsupervised and require access to large amounts of data. These methods need\nprior knowledge of the events and carry a high risk of leaking sensitive\ninformation in the messages, making them less applicable in open-world\nsettings. Therefore, conducting unsupervised detection while fully utilizing\nthe rich information in the messages and protecting data privacy remains a\nsignificant challenge. To this end, we propose a novel social event detection\nframework, ADP-SEMEvent, an unsupervised social event detection method that\nprioritizes privacy. Specifically, ADP-SEMEvent is divided into two stages,\ni.e., the construction stage of the private message graph and the clustering\nstage of the private message graph. In the first stage, an adaptive\ndifferential privacy approach is used to construct a private message graph. In\nthis process, our method can adaptively apply differential privacy based on the\nevents occurring each day in an open environment to maximize the use of the\nprivacy budget. In the second stage, to address the reduction in data utility\ncaused by noise, a novel 2-dimensional structural entropy minimization\nalgorithm based on optimal subgraphs is used to detect events in the message\ngraph. The highlight of this process is unsupervised and does not compromise\ndifferential privacy. Extensive experiments on two public datasets demonstrate\nthat ADP-SEMEvent can achieve detection performance comparable to\nstate-of-the-art methods while maintaining reasonable privacy budget\nparameters.",
      "tldr_zh": "该论文提出了一种无监督社交事件检测框架ADP-SEMEvent，旨在解决现有监督方法对大量数据依赖和敏感信息泄露风险的问题，同时充分利用消息中的丰富信息。框架分为两个阶段：首先，使用Adaptive Differentially Private方法自适应构建私有消息图，根据每天的事件动态分配隐私预算以最大化数据利用；其次，采用基于最优子图的2-dimensional Structural Entropy Minimization算法进行无监督聚类，确保检测过程不影响隐私保护。在两个公共数据集上的广泛实验表明，ADP-SEMEvent的检测性能可与最先进方法相当，同时保持合理的隐私预算参数。",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI",
      "comment": "Accepted to ACM CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.18274v1",
      "published_date": "2024-07-23 11:19:22 UTC",
      "updated_date": "2024-07-23 11:19:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:20:00.468028"
    },
    {
      "arxiv_id": "2408.00803v1",
      "title": "A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends",
      "title_zh": "翻译失败",
      "authors": [
        "Tingting Wang",
        "Guilin Qi"
      ],
      "abstract": "The complex dependencies and propagative faults inherent in microservices,\ncharacterized by a dense network of interconnected services, pose significant\nchallenges in identifying the underlying causes of issues. Prompt\nidentification and resolution of disruptive problems are crucial to ensure\nrapid recovery and maintain system stability. Numerous methodologies have\nemerged to address this challenge, primarily focusing on diagnosing failures\nthrough symptomatic data. This survey aims to provide a comprehensive,\nstructured review of root cause analysis (RCA) techniques within microservices,\nexploring methodologies that include metrics, traces, logs, and multi-model\ndata. It delves deeper into the methodologies, challenges, and future trends\nwithin microservices architectures. Positioned at the forefront of AI and\nautomation advancements, it offers guidance for future research directions.",
      "tldr_zh": "本论文对微服务架构中的Root Cause Analysis (RCA)进行了全面调查，探讨了复杂依赖和故障传播带来的挑战，以及基于metrics、traces、logs和multi-model data等多种方法来诊断问题。调查分析了当前RCA的技术趋势、潜在难题，并强调了AI和自动化在快速故障识别中的重要作用。该研究为未来微服务系统的稳定性和研究方向提供了宝贵指导。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00803v1",
      "published_date": "2024-07-23 11:02:49 UTC",
      "updated_date": "2024-07-23 11:02:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:20:13.170572"
    },
    {
      "arxiv_id": "2407.16375v1",
      "title": "Ranking protein-protein models with large language models and graph neural networks",
      "title_zh": "利用大型语言模型和图神经网络对蛋白-蛋白质模型进行排序",
      "authors": [
        "Xiaotong Xu",
        "Alexandre M. J. J. Bonvin"
      ],
      "abstract": "Protein-protein interactions (PPIs) are associated with various diseases,\nincluding cancer, infections, and neurodegenerative disorders. Obtaining\nthree-dimensional structural information on these PPIs serves as a foundation\nto interfere with those or to guide drug design. Various strategies can be\nfollowed to model those complexes, all typically resulting in a large number of\nmodels. A challenging step in this process is the identification of good models\n(near-native PPI conformations) from the large pool of generated models. To\naddress this challenge, we previously developed DeepRank-GNN-esm, a graph-based\ndeep learning algorithm for ranking modelled PPI structures harnessing the\npower of protein language models. Here, we detail the use of our software with\nexamples. DeepRank-GNN-esm is freely available at\nhttps://github.com/haddocking/DeepRank-GNN-esm",
      "tldr_zh": "本研究针对蛋白质-蛋白质相互作用（PPIs）的建模挑战，开发了DeepRank-GNN-esm工具，该工具利用large language models和graph neural networks对生成的PPI结构进行排名，以从大量模型中识别出接近原生构象的良好模型。DeepRank-GNN-esm通过图-based深度学习算法，结合蛋白质语言模型的力量，帮助解决PPI与癌症、感染和神经退行性疾病相关的结构信息获取问题。工具已开源并提供示例，可在https://github.com/haddocking/DeepRank-GNN-esm获取，为药物设计和疾病干预奠定基础。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "14 pages. Detailed protocol to use our DeepRank-GNN-esm software to\n  analyse models of protein-protein complexes",
      "pdf_url": "http://arxiv.org/pdf/2407.16375v1",
      "published_date": "2024-07-23 10:51:35 UTC",
      "updated_date": "2024-07-23 10:51:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:20:25.550052"
    },
    {
      "arxiv_id": "2407.16361v1",
      "title": "Virtue Ethics For Ethically Tunable Robotic Assistants",
      "title_zh": "翻译失败",
      "authors": [
        "Rajitha Ramanayake",
        "Vivek Nallur"
      ],
      "abstract": "The common consensus is that robots designed to work alongside or serve\nhumans must adhere to the ethical standards of their operational environment.\nTo achieve this, several methods based on established ethical theories have\nbeen suggested. Nonetheless, numerous empirical studies show that the ethical\nrequirements of the real world are very diverse and can change rapidly from\nregion to region. This eliminates the idea of a universal robot that can fit\ninto any ethical context. However, creating customised robots for each\ndeployment, using existing techniques is challenging. This paper presents a way\nto overcome this challenge by introducing a virtue ethics inspired\ncomputational method that enables character-based tuning of robots to\naccommodate the specific ethical needs of an environment. Using a simulated\nelder-care environment, we illustrate how tuning can be used to change the\nbehaviour of a robot that interacts with an elderly resident in an\nambient-assisted environment. Further, we assess the robot's responses by\nconsulting ethicists to identify potential shortcomings.",
      "tldr_zh": "该论文探讨了机器人需适应多样化伦理环境的挑战，指出现有基于伦理理论的方法难以应对区域间快速变化的伦理需求。作者提出一种受virtue ethics启发的计算方法，通过character-based tuning来调整机器人的角色和行为，以满足特定环境的伦理要求。在模拟的elder-care环境中，论文展示了这种调谐如何改变机器人与老年居民的互动，并通过咨询ethicists评估响应以识别潜在缺点，从而为可定制的伦理机器人设计提供新途径。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for EUMAS24",
      "pdf_url": "http://arxiv.org/pdf/2407.16361v1",
      "published_date": "2024-07-23 10:11:18 UTC",
      "updated_date": "2024-07-23 10:11:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:20:35.896964"
    },
    {
      "arxiv_id": "2407.16357v2",
      "title": "TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou",
      "title_zh": "翻译失败",
      "authors": [
        "Zihua Si",
        "Lin Guan",
        "ZhongXiang Sun",
        "Xiaoxue Zang",
        "Jing Lu",
        "Yiqun Hui",
        "Xingchao Cao",
        "Zeyu Yang",
        "Yichen Zheng",
        "Dewei Leng",
        "Kai Zheng",
        "Chenbin Zhang",
        "Yanan Niu",
        "Yang Song",
        "Kun Gai"
      ],
      "abstract": "The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.",
      "tldr_zh": "这篇论文介绍了 TWIN-V2，一种改进版的模型，用于扩展超长用户行为序列建模，以提升 Kuaishou 平台的 CTR 预测准确性和多样性。TWIN-V2 采用 divide-and-conquer 方法，通过离线阶段的分层聚类（hierarchical clustering）将类似特征的物品分组压缩行为序列（从 10^5 级别以上缩减到可管理的长度），并在在线阶段使用 cluster-aware target attention 提取全面的多方面用户兴趣。实验结果显示，在多亿规模工业数据集上的离线测试和在线 A/B 测试中，TWIN-V2 表现出色，并已成功部署到 Kuaishou 的主要流量中，服务数亿日活跃用户。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16357v2",
      "published_date": "2024-07-23 10:00:45 UTC",
      "updated_date": "2024-08-16 05:16:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:20:50.492993"
    },
    {
      "arxiv_id": "2407.16344v3",
      "title": "SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Wenbo Huang",
        "Jinghui Zhang",
        "Xuwei Qian",
        "Zhen Wu",
        "Meng Wang",
        "Lei Zhang"
      ],
      "abstract": "High frame-rate (HFR) videos of action recognition improve fine-grained\nexpression while reducing the spatio-temporal relation and motion information\ndensity. Thus, large amounts of video samples are continuously required for\ntraditional data-driven training. However, samples are not always sufficient in\nreal-world scenarios, promoting few-shot action recognition (FSAR) research. We\nobserve that most recent FSAR works build spatio-temporal relation of video\nsamples via temporal alignment after spatial feature extraction, cutting apart\nspatial and temporal features within samples. They also capture motion\ninformation via narrow perspectives between adjacent frames without considering\ndensity, leading to insufficient motion information capturing. Therefore, we\npropose a novel plug-and-play architecture for FSAR called Spatio-tempOral\nfrAme tuPle enhancer (SOAP) in this paper. The model we designed with such\narchitecture refers to SOAP-Net. Temporal connections between different feature\nchannels and spatio-temporal relation of features are considered instead of\nsimple feature extraction. Comprehensive motion information is also captured,\nusing frame tuples with multiple frames containing more motion information than\nadjacent frames. Combining frame tuples of diverse frame counts further\nprovides a broader perspective. SOAP-Net achieves new state-of-the-art\nperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,\nand HMDB51. Extensive empirical evaluations underscore the competitiveness,\npluggability, generalization, and robustness of SOAP. The code is released at\nhttps://github.com/wenbohuang1002/SOAP.",
      "tldr_zh": "本论文针对 Few-Shot Action Recognition (FSAR) 的挑战，提出了一种新型即插即用架构 SOAP（Spatio-tempOral frAme tuPle enhancer），旨在提升高帧率视频中时空关系和运动信息的捕获效率。SOAP-Net 通过整合特征间的 temporal connections 和 spatio-temporal relation，并使用包含多个帧的帧元组来全面捕捉运动信息，同时结合不同帧数的元组提供更广阔视角，解决了现有方法分离空间时间特征和狭隘运动捕获的局限。实验结果显示，SOAP-Net 在 SthSthV2、Kinetics、UCF101 和 HMDB51 等基准数据集上达到了新的 state-of-the-art 性能，证明了其竞争力、泛化性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16344v3",
      "published_date": "2024-07-23 09:45:25 UTC",
      "updated_date": "2024-08-21 16:07:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:21:01.597310"
    },
    {
      "arxiv_id": "2407.16329v1",
      "title": "PhenoFlow: A Human-LLM Driven Visual Analytics System for Exploring Large and Complex Stroke Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "Jaeyoung Kim",
        "Sihyeon Lee",
        "Hyeon Jeon",
        "Keon-Joo Lee",
        "Hee-Joon Bae",
        "Bohyoung Kim",
        "Jinwook Seo"
      ],
      "abstract": "Acute stroke demands prompt diagnosis and treatment to achieve optimal\npatient outcomes. However, the intricate and irregular nature of clinical data\nassociated with acute stroke, particularly blood pressure (BP) measurements,\npresents substantial obstacles to effective visual analytics and\ndecision-making. Through a year-long collaboration with experienced\nneurologists, we developed PhenoFlow, a visual analytics system that leverages\nthe collaboration between human and Large Language Models (LLMs) to analyze the\nextensive and complex data of acute ischemic stroke patients. PhenoFlow\npioneers an innovative workflow, where the LLM serves as a data wrangler while\nneurologists explore and supervise the output using visualizations and natural\nlanguage interactions. This approach enables neurologists to focus more on\ndecision-making with reduced cognitive load. To protect sensitive patient\ninformation, PhenoFlow only utilizes metadata to make inferences and synthesize\nexecutable codes, without accessing raw patient data. This ensures that the\nresults are both reproducible and interpretable while maintaining patient\nprivacy. The system incorporates a slice-and-wrap design that employs temporal\nfolding to create an overlaid circular visualization. Combined with a linear\nbar graph, this design aids in exploring meaningful patterns within irregularly\nmeasured BP data. Through case studies, PhenoFlow has demonstrated its\ncapability to support iterative analysis of extensive clinical datasets,\nreducing cognitive load and enabling neurologists to make well-informed\ndecisions. Grounded in long-term collaboration with domain experts, our\nresearch demonstrates the potential of utilizing LLMs to tackle current\nchallenges in data-driven clinical decision-making for acute ischemic stroke\npatients.",
      "tldr_zh": "PhenoFlow 是一个由人类和大型语言模型(LLMs)驱动的可视分析系统，旨在处理急性中风患者的大规模复杂数据集，特别是血压(BP)测量的不规则数据。通过创新的工作流，LLMs 作为数据整理工具，神经科医生可使用可视化和自然语言交互来探索和监督输出，从而减少认知负担并提升决策效率。该系统采用 slice-and-wrap 设计，包括时间折叠的叠加圆形可视化和线性条形图，帮助识别血压模式，同时仅使用元数据进行推理，确保患者隐私和结果的可解释性。通过案例研究，PhenoFlow 展示了其在支持迭代分析和数据驱动临床决策方面的潜力。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "11 pages, 5 figures, paper to appear in IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) (Proc. IEEE VIS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.16329v1",
      "published_date": "2024-07-23 09:25:59 UTC",
      "updated_date": "2024-07-23 09:25:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:21:13.590114"
    },
    {
      "arxiv_id": "2407.16326v2",
      "title": "On The Expressive Power of Knowledge Graph Embedding Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Jiexing Gao",
        "Dmitry Rodin",
        "Vasily Motolygin",
        "Denis Zaytsev"
      ],
      "abstract": "Knowledge Graph Embedding (KGE) is a popular approach, which aims to\nrepresent entities and relations of a knowledge graph in latent spaces. Their\nrepresentations are known as embeddings. To measure the plausibility of\ntriplets, score functions are defined over embedding spaces. Despite wide\ndissemination of KGE in various tasks, KGE methods have limitations in\nreasoning abilities. In this paper we propose a mathematical framework to\ncompare reasoning abilities of KGE methods. We show that STransE has a higher\ncapability than TransComplEx, and then present new STransCoRe method, which\nimproves the STransE by combining it with the TransCoRe insights, which can\nreduce the STransE space complexity.",
      "tldr_zh": "这篇论文探讨了Knowledge Graph Embedding (KGE)方法的表达能力，特别针对其推理能力的局限性。作者提出一个数学框架，用于比较不同KGE方法的推理性能，并发现STransE比TransComplEx具有更高的能力。新方法STransCoRe通过结合TransCoRe的见解改进STransE，实现了空间复杂度的降低，从而提升了KGE在知识图谱任务中的整体效能。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "MCS 68T30",
        "I.2.4"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper may involve data that is not readily available to the\n  public",
      "pdf_url": "http://arxiv.org/pdf/2407.16326v2",
      "published_date": "2024-07-23 09:21:38 UTC",
      "updated_date": "2024-07-26 16:11:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:21:24.558854"
    },
    {
      "arxiv_id": "2407.16318v1",
      "title": "PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing",
      "title_zh": "PrimeGuard：通过免调优路由实现安全且有帮助的LLMs",
      "authors": [
        "Blazej Manczak",
        "Eliott Zemour",
        "Eric Lin",
        "Vaikkunth Mugunthan"
      ],
      "abstract": "Deploying language models (LMs) necessitates outputs to be both high-quality\nand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)\noffer solutions that shift model output distributions towards compliance, we\nfind that current methods struggle in balancing safety with helpfulness. ITG\nMethods that safely address non-compliant queries exhibit lower helpfulness\nwhile those that prioritize helpfulness compromise on safety. We refer to this\ntrade-off as the guardrail tax, analogous to the alignment tax. To address\nthis, we propose PrimeGuard, a novel ITG method that utilizes structured\ncontrol flow.\n  PrimeGuard routes requests to different self-instantiations of the LM with\nvarying instructions, leveraging its inherent instruction-following\ncapabilities and in-context learning. Our tuning-free approach dynamically\ncompiles system-designer guidelines for each query. We construct and release\nsafe-eval, a diverse red-team safety benchmark. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax\nby (1) significantly increasing resistance to iterative jailbreak attacks and\n(2) achieving state-of-the-art results in safety guardrailing while (3)\nmatching helpfulness scores of alignment-tuned models. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, outperforms all competing\nbaselines and overcomes the guardrail tax by improving the fraction of safe\nresponses from 61% to 97% and increasing average helpfulness scores from 4.17\nto 4.29 on the largest models, while reducing attack success rate from 100% to\n8%.\n  PrimeGuard implementation is available at\nhttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at\nhttps://huggingface.co/datasets/dynamoai/safe_eval.",
      "tldr_zh": "这篇论文提出了 PrimeGuard，一种无调优（tuning-free）的 Inference-Time Guardrails (ITG) 方法，通过结构化控制流将请求路由到不同指令的语言模型（LLMs）实例，利用其指令遵循能力和 in-context learning，动态平衡安全性和帮助性，以克服 guardrail tax。PrimeGuard 在新发布的 safe-eval 红队安全基准上进行评估，显著提升了模型的抗迭代越狱攻击能力。实验结果显示，它在最大模型上将安全响应比例从61%提高到97%，帮助性分数从4.17增加到4.29，同时将攻击成功率降至8%，实现了最先进的性能。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "ICML 2024 NextGenAISafety workshop version with links to\n  implementation and dataset",
      "pdf_url": "http://arxiv.org/pdf/2407.16318v1",
      "published_date": "2024-07-23 09:14:27 UTC",
      "updated_date": "2024-07-23 09:14:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:21:38.155749"
    },
    {
      "arxiv_id": "2407.16312v2",
      "title": "MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Florian Felten",
        "Umut Ucak",
        "Hicham Azmani",
        "Gao Peng",
        "Willem Röpke",
        "Hendrik Baier",
        "Patrick Mannion",
        "Diederik M. Roijers",
        "Jordan K. Terry",
        "El-Ghazali Talbi",
        "Grégoire Danoy",
        "Ann Nowé",
        "Roxana Rădulescu"
      ],
      "abstract": "Many challenging tasks such as managing traffic systems, electricity grids,\nor supply chains involve complex decision-making processes that must balance\nmultiple conflicting objectives and coordinate the actions of various\nindependent decision-makers (DMs). One perspective for formalising and\naddressing such tasks is multi-objective multi-agent reinforcement learning\n(MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple\nagents each needing to consider multiple objectives in their learning process.\nIn reinforcement learning research, benchmarks are crucial in facilitating\nprogress, evaluation, and reproducibility. The significance of benchmarks is\nunderscored by the existence of numerous benchmark frameworks developed for\nvarious RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent\nRL (e.g., PettingZoo), and single-agent multi-objective RL (e.g.,\nMO-Gymnasium). To support the advancement of the MOMARL field, we introduce\nMOMAland, the first collection of standardised environments for multi-objective\nmulti-agent reinforcement learning. MOMAland addresses the need for\ncomprehensive benchmarking in this emerging field, offering over 10 diverse\nenvironments that vary in the number of agents, state representations, reward\nstructures, and utility considerations. To provide strong baselines for future\nresearch, MOMAland also includes algorithms capable of learning policies in\nsuch settings.",
      "tldr_zh": "本研究介绍了 MOMAland，这是一个针对多目标多智能体强化学习 (MOMARL) 的标准化基准集合，用于处理涉及多个冲突目标和独立决策者的复杂任务，如交通系统或电力网格。MOMAland 提供了超过 10 个多样化的环境，涵盖不同代理数量、状态表示、奖励结构和效用考虑，从而促进 MOMARL 的评估、可重复性和研究进展。该基准还包括可用于这些环境的算法基线，为未来研究提供强有力的基础。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.MA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16312v2",
      "published_date": "2024-07-23 09:05:06 UTC",
      "updated_date": "2024-10-27 17:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:21:48.968557"
    },
    {
      "arxiv_id": "2407.21052v1",
      "title": "Table-Filling via Mean Teacher for Cross-domain Aspect Sentiment Triplet Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Kun Peng",
        "Lei Jiang",
        "Qian Li",
        "Haoran Li",
        "Xiaoyan Yu",
        "Li Sun",
        "Shuo Sun",
        "Yanxian Bi",
        "Hao Peng"
      ],
      "abstract": "Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract\nfine-grained sentiment elements from target domain sentences by leveraging the\nknowledge acquired from the source domain. Due to the absence of labeled data\nin the target domain, recent studies tend to rely on pre-trained language\nmodels to generate large amounts of synthetic data for training purposes.\nHowever, these approaches entail additional computational costs associated with\nthe generation process. Different from them, we discover a striking resemblance\nbetween table-filling methods in ASTE and two-stage Object Detection (OD) in\ncomputer vision, which inspires us to revisit the cross-domain ASTE task and\napproach it from an OD standpoint. This allows the model to benefit from the OD\nextraction paradigm and region-level alignment. Building upon this premise, we\npropose a novel method named \\textbf{T}able-\\textbf{F}illing via \\textbf{M}ean\n\\textbf{T}eacher (TFMT). Specifically, the table-filling methods encode the\nsentence into a 2D table to detect word relations, while TFMT treats the table\nas a feature map and utilizes a region consistency to enhance the quality of\nthose generated pseudo labels. Additionally, considering the existence of the\ndomain gap, a cross-domain consistency based on Maximum Mean Discrepancy is\ndesigned to alleviate domain shift problems. Our method achieves\nstate-of-the-art performance with minimal parameters and computational costs,\nmaking it a strong baseline for cross-domain ASTE.",
      "tldr_zh": "这篇论文针对跨域 Aspect Sentiment Triplet Extraction (ASTE) 问题，提出了一种名为 Table-Filling via Mean Teacher (TFMT) 的新方法，通过借鉴计算机视觉中两阶段 Object Detection (OD) 的思路，将句子编码成 2D 表来检测词关系，并利用 region consistency 提升伪标签质量。TFMT 还引入基于 Maximum Mean Discrepancy 的 cross-domain consistency 机制，以缓解域移问题，从而减少了合成数据生成的计算成本。实验结果显示，该方法在最小参数和计算开销下达到了最先进性能，成为跨域 ASTE 的强基线。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by CIKM2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21052v1",
      "published_date": "2024-07-23 09:04:08 UTC",
      "updated_date": "2024-07-23 09:04:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:22:01.260044"
    },
    {
      "arxiv_id": "2407.16296v1",
      "title": "Quantum Computing for Climate Resilience and Sustainability Challenges",
      "title_zh": "量子计算用于气候韧性和可持续性挑战",
      "authors": [
        "Kin Tung Michael Ho",
        "Kuan-Cheng Chen",
        "Lily Lee",
        "Felix Burt",
        "Shang Yu",
        "Po-Heng",
        "Lee"
      ],
      "abstract": "The escalating impacts of climate change and the increasing demand for\nsustainable development and natural resource management necessitate innovative\ntechnological solutions. Quantum computing (QC) has emerged as a promising tool\nwith the potential to revolutionize these critical areas. This review explores\nthe application of quantum machine learning and optimization techniques for\nclimate change prediction and enhancing sustainable development. Traditional\ncomputational methods often fall short in handling the scale and complexity of\nclimate models and natural resource management. Quantum advancements, however,\noffer significant improvements in computational efficiency and problem-solving\ncapabilities. By synthesizing the latest research and developments, this paper\nhighlights how QC and quantum machine learning can optimize\nmulti-infrastructure systems towards climate neutrality. The paper also\nevaluates the performance of current quantum algorithms and hardware in\npractical applications and presents realistic cases, i.e., waste-to-energy in\nanaerobic digestion, disaster prevention in flooding prediction, and new\nmaterial development for carbon capture. The integration of these quantum\ntechnologies promises to drive significant advancements in achieving climate\nresilience and sustainable development.",
      "tldr_zh": "本论文探讨了量子计算（QC）在应对气候变化和可持续性挑战中的潜力，强调其通过量子机器学习和优化技术提升气候预测和资源管理的计算效率。相比传统方法，QC能够更好地处理复杂模型，并优化多基础设施系统以实现气候中性。论文通过分析最新研究和实际案例（如厌氧消化废物转化为能源、洪水预测灾害预防以及碳捕获新材料开发），评估了当前量子算法和硬件的性能，并展示了这些技术的整合有望推动气候韧性和可持续发展的重大进展。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16296v1",
      "published_date": "2024-07-23 08:54:12 UTC",
      "updated_date": "2024-07-23 08:54:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:22:16.002721"
    },
    {
      "arxiv_id": "2407.16292v2",
      "title": "Visual Stereotypes of Autism Spectrum in DALL-E, Stable Diffusion, SDXL, and Midjourney",
      "title_zh": "翻译失败",
      "authors": [
        "Maciej Wodziński",
        "Marcin Rządeczka",
        "Anastazja Szuła",
        "Marta Sokół",
        "Marcin Moskalewicz"
      ],
      "abstract": "Avoiding systemic discrimination requires investigating AI models' potential\nto propagate stereotypes resulting from the inherent biases of training\ndatasets. Our study investigated how text-to-image models unintentionally\nperpetuate non-rational beliefs regarding autism. The research protocol\ninvolved generating images based on 53 prompts aimed at visualizing concrete\nobjects and abstract concepts related to autism across four models: DALL-E,\nStable Diffusion, SDXL, and Midjourney (N=249). Expert assessment of results\nwas performed via a framework of 10 deductive codes representing common\nstereotypes contested by the community regarding their presence and spatial\nintensity, quantified on ordinal scales and subject to statistical analysis of\ninter-rater reliability and size effects. The models frequently utilised\ncontroversial themes and symbols which were unevenly distributed, however, with\nstriking homogeneity in terms of skin colour, gender, and age, with autistic\nindividuals portrayed as engaged in solitary activities, interacting with\nobjects rather than people, and displaying stereotypical emotional expressions\nsuch as pale, anger, or sad. Secondly we observed representational\ninsensitivity regarding autism images despite directional prompting aimed at\nfalsifying the above results. Additionally, DALL-E explicitly denied\nperpetuating stereotypes. We interpret this as ANNs mirroring the human\ncognitive architecture regarding the discrepancy between background and\nreflective knowledge, as justified by our previous research on autism-related\nstereotypes in humans.",
      "tldr_zh": "本研究调查了DALL-E、Stable Diffusion、SDXL和Midjourney等文本到图像模型如何无意中传播自闭症相关的刻板印象，通过分析53个提示生成的249张图像（N=249）。方法包括专家使用10个演绎代码框架评估图像中刻板印象的存在和强度，如皮肤颜色、性别、年龄的同质性，以及个体被描绘为孤独活动、与物体互动或显示刻板情感（如pale、anger或sad）。结果显示，这些模型表现出显著的代表性不敏感，即使通过定向提示试图纠正，刻板印象仍普遍存在，且DALL-E甚至否认perpetuating stereotypes；研究解释此现象为ANNs（人工神经网络）镜像人类认知架构中背景知识与反思知识的差异，为避免系统性歧视提供了重要洞见。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16292v2",
      "published_date": "2024-07-23 08:48:09 UTC",
      "updated_date": "2024-07-24 07:15:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:22:26.322974"
    },
    {
      "arxiv_id": "2407.16289v1",
      "title": "Federated Learning for Face Recognition via Intra-subject Self-supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hansol Kim",
        "Hoyeol Choi",
        "Youngjun Kwak"
      ],
      "abstract": "Federated Learning (FL) for face recognition aggregates locally optimized\nmodels from individual clients to construct a generalized face recognition\nmodel. However, previous studies present two major challenges: insufficient\nincorporation of self-supervised learning and the necessity for clients to\naccommodate multiple subjects. To tackle these limitations, we propose FedFS\n(Federated Learning for personalized Face recognition via intra-subject\nSelf-supervised learning framework), a novel federated learning architecture\ntailored to train personalized face recognition models without imposing\nsubjects. Our proposed FedFS comprises two crucial components that leverage\naggregated features of the local and global models to cooperate with\nrepresentations of an off-the-shelf model. These components are (1) adaptive\nsoft label construction, utilizing dot product operations to reformat labels\nwithin intra-instances, and (2) intra-subject self-supervised learning,\nemploying cosine similarity operations to strengthen robust intra-subject\nrepresentations. Additionally, we introduce a regularization loss to prevent\noverfitting and ensure the stability of the optimized model. To assess the\neffectiveness of FedFS, we conduct comprehensive experiments on the DigiFace-1M\nand VGGFace datasets, demonstrating superior performance compared to previous\nmethods.",
      "tldr_zh": "这篇论文提出了 FedFS 框架，用于 Federated Learning (FL) 在人脸识别中的应用，旨在解决现有方法中自监督学习不足和客户端需处理多个主体的挑战。FedFS 通过自适应软标签构建（利用点积操作重构内部实例标签）和内部主体自监督学习（采用余弦相似度加强鲁棒表示）来训练个性化的面部识别模型，同时引入正则化损失以防止过拟合并确保模型稳定性。在 DigiFace-1M 和 VGGFace 数据集上的实验显示，FedFS 比之前方法表现出色，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at the The 35th British Machine Vision Conference 2024 (BMVC\n  2024), Glasgow, UK. Youngjun Kwak is corresponding author",
      "pdf_url": "http://arxiv.org/pdf/2407.16289v1",
      "published_date": "2024-07-23 08:43:42 UTC",
      "updated_date": "2024-07-23 08:43:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:22:36.987761"
    },
    {
      "arxiv_id": "2407.16286v1",
      "title": "A deeper look at depth pruning of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Shoaib Ahmed Siddiqui",
        "Xin Dong",
        "Greg Heinrich",
        "Thomas Breuel",
        "Jan Kautz",
        "David Krueger",
        "Pavlo Molchanov"
      ],
      "abstract": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
      "tldr_zh": "本文深入探讨了 Large Language Models (LLMs) 的深度修剪技术，通过静态指标和自适应指标（如 Shapley value）评估块重要性，发现自适应指标在不同任务间存在性能权衡，即提升一个任务可能损害另一个。研究扩展到单个层级，显示 self-attention 层更易修剪，可移除高达 33% 的层而不降低 MMLU 性能，从而减少 KV-cache 的维护成本。最后，提出性能恢复方法，如训练轻量级添加偏差或低秩线性适配器，能显著改善模型表现（如 MMLU 上 5% 的绝对提升）。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16286v1",
      "published_date": "2024-07-23 08:40:27 UTC",
      "updated_date": "2024-07-23 08:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:22:50.346892"
    },
    {
      "arxiv_id": "2407.16280v1",
      "title": "Efficient Detection of Commutative Factors in Factor Graphs",
      "title_zh": "因子图中交换因子的高效检测",
      "authors": [
        "Malte Luttermann",
        "Johann Machemer",
        "Marcel Gehrke"
      ],
      "abstract": "Lifted probabilistic inference exploits symmetries in probabilistic graphical\nmodels to allow for tractable probabilistic inference with respect to domain\nsizes. To exploit symmetries in, e.g., factor graphs, it is crucial to identify\ncommutative factors, i.e., factors having symmetries within themselves due to\ntheir arguments being exchangeable. The current state of the art to check\nwhether a factor is commutative with respect to a subset of its arguments\niterates over all possible subsets of the factor's arguments, i.e., $O(2^n)$\niterations for a factor with $n$ arguments in the worst case. In this paper, we\nefficiently solve the problem of detecting commutative factors in a factor\ngraph. In particular, we introduce the detection of commutative factors (DECOR)\nalgorithm, which allows us to drastically reduce the computational effort for\nchecking whether a factor is commutative in practice. We prove that DECOR\nefficiently identifies restrictions to drastically reduce the number of\nrequired iterations and validate the efficiency of DECOR in our empirical\nevaluation.",
      "tldr_zh": "这篇论文针对因子图（factor graphs）中交换因子（commutative factors）的检测问题，提出了一种高效算法 DECOR，以解决传统方法需检查所有参数子集导致的 O(2^n) 计算复杂度问题。DECOR 算法通过识别限制来大幅减少迭代次数，从而加速对称性利用过程。实验结果证明，DECOR 在实际应用中显著提高了效率，为可扩展的概率图形模型推理提供了实用工具。",
      "categories": [
        "cs.AI",
        "cs.DS",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to the Proceedings of the 12th Conference on Probabilistic\n  Graphical Models (PGM 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.16280v1",
      "published_date": "2024-07-23 08:31:24 UTC",
      "updated_date": "2024-07-23 08:31:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:23:01.067092"
    },
    {
      "arxiv_id": "2407.16274v2",
      "title": "Comparative Analysis of AES, Blowfish, Twofish, Salsa20, and ChaCha20 for Image Encryption",
      "title_zh": "AES、Blowfish、Twofish、Salsa20 和 ChaCha20 用于图像加密的比较分析",
      "authors": [
        "Rebwar Khalid Muhammed",
        "Ribwar Rashid Aziz",
        "Alla Ahmad Hassan",
        "Aso Mohammed Aladdin",
        "Shaida Jumaah Saydah",
        "Tarik Ahmed. Rashid",
        "Bryar Ahmad Hassan"
      ],
      "abstract": "Nowadays, cybersecurity has grown into a more significant and difficult\nscientific issue. The recog-nition of threats and attacks meant for knowledge\nand safety on the internet is growing harder to detect. Since cybersecurity\nguarantees the privacy and security of data sent via the Internet, it is\nessential, while also providing protection against malicious attacks. Encrypt\nhas grown into an an-swer that has become an essential element of information\nsecurity systems. To ensure the security of shared data, including text,\nimages, or videos, it is essential to employ various methods and strategies.\nThis study delves into the prevalent cryptographic methods and algorithms\nutilized for prevention and stream encryption, examining their encoding\ntechniques such as advanced encryp-tion standard (AES), Blowfish, Twofish,\nSalsa20, and ChaCha20. The primary objective of this re-search is to identify\nthe optimal times and throughputs (speeds) for data encryption and decryption\nprocesses. The methodology of this study involved selecting five distinct types\nof images to com-pare the outcomes of the techniques evaluated in this\nresearch. The assessment focused on pro-cessing time and speed parameters,\nexamining visual encoding and decoding using Java as the pri-mary platform. A\ncomparative analysis of several symmetric key ciphers was performed, focusing\non handling large datasets. Despite this limitation, comparing different images\nhelped evaluate the techniques' novelty. The results showed that ChaCha20 had\nthe best average time for both encryp-tion and decryption, being over 50%\nfaster than some other algorithms. However, the Twofish algo-rithm had lower\nthroughput during testing. The paper concludes with findings and suggestions\nfor future improvements.",
      "tldr_zh": "这篇论文比较了 AES、Blowfish、Twofish、Salsa20 和 ChaCha20 等对称密钥加密算法在图像加密中的性能，旨在评估它们的加密和解密速度及吞吐量，以提升网络安全。研究方法涉及选取五种不同类型图像，使用 Java 平台进行测试，重点分析处理时间和速度参数。结果表明，ChaCha20 在加密和解密平均时间上表现最佳，比其他算法快超过 50%，而 Twofish 的吞吐量较低。论文总结了这些算法的优缺点，并为未来加密技术改进提出建议。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16274v2",
      "published_date": "2024-07-23 08:26:05 UTC",
      "updated_date": "2024-07-26 11:04:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:23:13.577259"
    },
    {
      "arxiv_id": "2407.16255v1",
      "title": "Self-Reasoning Assistant Learning for non-Abelian Gauge Fields Design",
      "title_zh": "翻译失败",
      "authors": [
        "Jinyang Sun",
        "Xi Chen",
        "Xiumei Wang",
        "Dandan Zhu",
        "Xingping Zhou"
      ],
      "abstract": "Non-Abelian braiding has attracted substantial attention because of its\npivotal role in describing the exchange behaviour of anyons, in which the input\nand outcome of non-Abelian braiding are connected by a unitary matrix.\nImplementing braiding in a classical system can assist the experimental\ninvestigation of non-Abelian physics. However, the design of non-Abelian gauge\nfields faces numerous challenges stemmed from the intricate interplay of group\nstructures, Lie algebra properties, representation theory, topology, and\nsymmetry breaking. The extreme diversity makes it a powerful tool for the study\nof condensed matter physics. Whereas the widely used artificial intelligence\nwith data-driven approaches has greatly promoted the development of physics,\nmost works are limited on the data-to-data design. Here we propose a\nself-reasoning assistant learning framework capable of directly generating\nnon-Abelian gauge fields. This framework utilizes the forward diffusion process\nto capture and reproduce the complex patterns and details inherent in the\ntarget distribution through continuous transformation. Then the reverse\ndiffusion process is used to make the generated data closer to the distribution\nof the original situation. Thus, it owns strong self-reasoning capabilities,\nallowing to automatically discover the feature representation and capture more\nsubtle relationships from the dataset. Moreover, the self-reasoning eliminates\nthe need for manual feature engineering and simplifies the process of model\nbuilding. Our framework offers a disruptive paradigm shift to parse complex\nphysical processes, automatically uncovering patterns from massive datasets.",
      "tldr_zh": "该研究针对非Abelian gauge fields设计面临的复杂挑战（如群结构、Lie algebra和拓扑互动），提出了一种self-reasoning assistant learning框架，用于直接生成这些规范场。该框架利用forward diffusion process捕获目标分布的复杂模式，并通过reverse diffusion process使生成数据接近原分布，从而实现自动特征发现和微妙关系的捕捉。与传统数据驱动方法不同，该框架消除了手动特征工程的需求，并为解析复杂物理过程提供了一个颠覆性新范式，从海量数据集自动揭示模式。实验结果表明，该方法能有效辅助非Abelian braiding的经典系统实现，推动凝聚态物理研究。",
      "categories": [
        "cs.LG",
        "cond-mat.mes-hall",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16255v1",
      "published_date": "2024-07-23 07:49:35 UTC",
      "updated_date": "2024-07-23 07:49:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:23:25.798978"
    },
    {
      "arxiv_id": "2407.16252v3",
      "title": "LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese Legal Consultation",
      "title_zh": "LawLuo：一个多智能体协作框架，用于多轮中文法律咨询",
      "authors": [
        "Jingyun Sun",
        "Chengxiao Dai",
        "Zhongze Luo",
        "Yangbo Chang",
        "Yang Li"
      ],
      "abstract": "Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.",
      "tldr_zh": "论文提出 LawLuo，一种多智能体协作框架，用于模拟真实律师事务所的多轮中文法律咨询，旨在解决现有单智能体系统在个性化响应和模糊查询处理上的局限。框架包括四个代理：接待员代理（评估用户意图并选择律师代理）、律师代理（与用户互动）、秘书代理（组织对话记录和生成报告），以及老板代理（评估其他代理的表现）。通过开发专用微调数据集和基于案例图的 RAG（Retrieval-Augmented Generation），LawLuo 能够更好地遵循法律指令；实验结果显示，它在生成专业化响应、处理模糊查询和多轮对话中优于基线模型，并计划开源代码和数据集。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16252v3",
      "published_date": "2024-07-23 07:40:41 UTC",
      "updated_date": "2024-12-16 05:53:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:23:37.725452"
    },
    {
      "arxiv_id": "2407.16244v1",
      "title": "HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification",
      "title_zh": "HSVLT：分层尺度感知视觉-语言 Transformer 用于多标签图像分类",
      "authors": [
        "Shuyi Ouyang",
        "Hongyi Wang",
        "Ziwei Niu",
        "Zhenjia Bai",
        "Shiao Xie",
        "Yingying Xu",
        "Ruofeng Tong",
        "Yen-Wei Chen",
        "Lanfen Lin"
      ],
      "abstract": "The task of multi-label image classification involves recognizing multiple\nobjects within a single image. Considering both valuable semantic information\ncontained in the labels and essential visual features presented in the image,\ntight visual-linguistic interactions play a vital role in improving\nclassification performance. Moreover, given the potential variance in object\nsize and appearance within a single image, attention to features of different\nscales can help to discover possible objects in the image. Recently,\nTransformer-based methods have achieved great success in multi-label image\nclassification by leveraging the advantage of modeling long-range dependencies,\nbut they have several limitations. Firstly, existing methods treat visual\nfeature extraction and cross-modal fusion as separate steps, resulting in\ninsufficient visual-linguistic alignment in the joint semantic space.\nAdditionally, they only extract visual features and perform cross-modal fusion\nat a single scale, neglecting objects with different characteristics. To\naddress these issues, we propose a Hierarchical Scale-Aware Vision-Language\nTransformer (HSVLT) with two appealing designs: (1)~A hierarchical multi-scale\narchitecture that involves a Cross-Scale Aggregation module, which leverages\njoint multi-modal features extracted from multiple scales to recognize objects\nof varying sizes and appearances in images. (2)~Interactive Visual-Linguistic\nAttention, a novel attention mechanism module that tightly integrates\ncross-modal interaction, enabling the joint updating of visual, linguistic and\nmulti-modal features. We have evaluated our method on three benchmark datasets.\nThe experimental results demonstrate that HSVLT surpasses state-of-the-art\nmethods with lower computational cost.",
      "tldr_zh": "该论文提出了一种Hierarchical Scale-Aware Vision-Language Transformer (HSVLT)，旨在提升多标签图像分类任务的表现，通过紧密整合视觉和语言特征来识别图像中的多个对象。HSVLT 包括两个关键设计：(1) 层次化多尺度架构，采用Cross-Scale Aggregation 模块利用多尺度多模态特征，处理对象大小和外观的差异；(2) Interactive Visual-Linguistic Attention 机制，实现视觉、语言和多模态特征的联合更新，以加强跨模态交互。实验结果显示，该方法在三个基准数据集上超越了现有最先进方法，同时降低了计算成本。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16244v1",
      "published_date": "2024-07-23 07:31:42 UTC",
      "updated_date": "2024-07-23 07:31:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:23:49.489627"
    },
    {
      "arxiv_id": "2407.16237v2",
      "title": "OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and Self-Reflection",
      "title_zh": "翻译失败",
      "authors": [
        "Fan Cui",
        "Chenyang Yin",
        "Kexing Zhou",
        "Youwei Xiao",
        "Guangyu Sun",
        "Qiang Xu",
        "Qipeng Guo",
        "Demin Song",
        "Dahua Lin",
        "Xingcheng Zhang",
        "Yun",
        "Liang"
      ],
      "abstract": "Recent studies have demonstrated the significant potential of Large Language\nModels (LLMs) in generating Register Transfer Level (RTL) code, with notable\nadvancements showcased by commercial models such as GPT-4 and Claude3-Opus.\nHowever, these proprietary LLMs often raise concerns regarding privacy and\nsecurity. While open-source LLMs offer solutions to these concerns, they\ntypically underperform commercial models in RTL code generation tasks,\nprimarily due to the scarcity of high-quality open-source RTL datasets. To\naddress this challenge, we introduce OriGen , a fully open-source framework\nthat incorporates self-reflection capabilities and a novel dataset augmentation\nmethodology for generating high-quality, large-scale RTL code. Our approach\nemploys a code-tocode augmentation technique to enhance the quality of\nopen-source RTL code datasets. Furthermore, OriGen can rectify syntactic errors\nthrough a self-reflection process that leverages compiler feedback.\nExperimental results demonstrate that OriGen significantly outperforms other\nopen-source alternatives in RTL code generation. It surpasses the previous\nbest-performing open-source LLM by 12.8% and even exceeds GPT-4 Turbo in the\npass@1 metric on the VerilogEval-Human benchmark. Moreover, OriGen exhibits\nsuperior capabilities in self-reflection and error correction, outperforming\nGPT-4 by 19.9% on a benchmark designed to evaluate self-reflection\ncapabilities.",
      "tldr_zh": "本论文提出 OriGen，一个开源框架，用于提升 Large Language Models (LLMs) 在 Register Transfer Level (RTL) 代码生成中的性能，以解决开源模型因高质量数据集稀缺而落后于商业模型（如 GPT-4）的问题。OriGen 引入 code-to-code augmentation 技术来增强数据集质量，并通过 self-reflection 机制利用编译器反馈自动修正语法错误。实验结果显示，OriGen 在 VerilogEval-Human 基准上比最佳开源 LLM 提高 12.8%，并在 pass@1 指标上超过 GPT-4 Turbo；此外，在 self-reflection 和错误修正能力上，它领先 GPT-4 19.9%。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16237v2",
      "published_date": "2024-07-23 07:22:25 UTC",
      "updated_date": "2024-09-02 07:25:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:24:03.105755"
    },
    {
      "arxiv_id": "2407.16235v1",
      "title": "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Zhou",
        "Duc-Manh Tran",
        "Thanh Le-Cong",
        "Ting Zhang",
        "Ivana Clairine Irsan",
        "Joshua Sumarlin",
        "Bach Le",
        "David Lo"
      ],
      "abstract": "Software vulnerabilities pose significant security challenges and potential\nrisks to society, necessitating extensive efforts in automated vulnerability\ndetection. There are two popular lines of work to address automated\nvulnerability detection. On one hand, Static Application Security Testing\n(SAST) is usually utilized to scan source code for security vulnerabilities,\nespecially in industries. On the other hand, deep learning (DL)-based methods,\nespecially since the introduction of large language models (LLMs), have\ndemonstrated their potential in software vulnerability detection. However,\nthere is no comparative study between SAST tools and LLMs, aiming to determine\ntheir effectiveness in vulnerability detection, understand the pros and cons of\nboth SAST and LLMs, and explore the potential combination of these two families\nof approaches.\n  In this paper, we compared 15 diverse SAST tools with 12 popular or\nstate-of-the-art open-source LLMs in detecting software vulnerabilities from\nrepositories of three popular programming languages: Java, C, and Python. The\nexperimental results showed that SAST tools obtain low vulnerability detection\nrates with relatively low false positives, while LLMs can detect up 90\\% to\n100\\% of vulnerabilities but suffer from high false positives. By further\nensembling the SAST tools and LLMs, the drawbacks of both SAST tools and LLMs\ncan be mitigated to some extent. Our analysis sheds light on both the current\nprogress and future directions for software vulnerability detection.",
      "tldr_zh": "这篇论文比较了Static Application Security Testing (SAST) 工具和Large Language Models (LLMs) 在仓库级软件漏洞检测中的有效性，涉及15个SAST工具和12个开源LLMs，对Java、C和Python语言的仓库进行实验。结果显示，SAST工具的检测率较低但假阳性率相对低，而LLMs能检测高达90%至100%的漏洞，却面临高假阳性率的问题。通过将SAST工具与LLMs结合，可以部分缓解各自缺点，从而提升整体检测性能。该研究为软件漏洞检测的未来方向提供了重要启示。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16235v1",
      "published_date": "2024-07-23 07:21:14 UTC",
      "updated_date": "2024-07-23 07:21:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:24:14.029966"
    },
    {
      "arxiv_id": "2407.16220v1",
      "title": "ODGR: Online Dynamic Goal Recognition",
      "title_zh": "ODGR：在线动态目标识别",
      "authors": [
        "Matan Shamir",
        "Osher Elhadad",
        "Matthew E. Taylor",
        "Reuth Mirsky"
      ],
      "abstract": "Traditionally, Reinforcement Learning (RL) problems are aimed at optimization\nof the behavior of an agent. This paper proposes a novel take on RL, which is\nused to learn the policy of another agent, to allow real-time recognition of\nthat agent's goals. Goal Recognition (GR) has traditionally been framed as a\nplanning problem where one must recognize an agent's objectives based on its\nobserved actions. Recent approaches have shown how reinforcement learning can\nbe used as part of the GR pipeline, but are limited to recognizing predefined\ngoals and lack scalability in domains with a large goal space. This paper\nformulates a novel problem, \"Online Dynamic Goal Recognition\" (ODGR), as a\nfirst step to address these limitations. Contributions include introducing the\nconcept of dynamic goals into the standard GR problem definition, revisiting\ncommon approaches by reformulating them using ODGR, and demonstrating the\nfeasibility of solving ODGR in a navigation domain using transfer learning.\nThese novel formulations open the door for future extensions of existing\ntransfer learning-based GR methods, which will be robust to changing and\nexpansive real-time environments.",
      "tldr_zh": "本文提出一种新型强化学习（RL）应用，旨在通过学习另一个代理的政策来实现实时目标识别（Goal Recognition, GR），以克服传统 GR 方法在处理预定义目标和大规模目标空间时的局限性。论文引入“Online Dynamic Goal Recognition (ODGR)”新问题，将动态目标整合进标准 GR 定义，并通过重构现有方法和在导航领域应用转移学习，证明了 ODGR 的可行性。这些创新为开发适应变化实时环境的 GR 方法铺平了道路，提供更具扩展性和鲁棒性的解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages, 1 figure, RLC workshop, WAHT workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.16220v1",
      "published_date": "2024-07-23 06:52:52 UTC",
      "updated_date": "2024-07-23 06:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:24:26.104099"
    },
    {
      "arxiv_id": "2407.20257v1",
      "title": "Causal Understanding For Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Bhanu Prakash Reddy Guda",
        "Tanmay Kulkarni",
        "Adithya Sampath",
        "Swarnashree Mysore Sathyendra"
      ],
      "abstract": "Video Question Answering is a challenging task, which requires the model to\nreason over multiple frames and understand the interaction between different\nobjects to answer questions based on the context provided within the video,\nespecially in datasets like NExT-QA (Xiao et al., 2021a) which emphasize on\ncausal and temporal questions. Previous approaches leverage either sub-sampled\ninformation or causal intervention techniques along with complete video\nfeatures to tackle the NExT-QA task. In this work we elicit the limitations of\nthese approaches and propose solutions along four novel directions of\nimprovements on theNExT-QA dataset. Our approaches attempts to compensate for\nthe shortcomings in the previous works by systematically attacking each of\nthese problems by smartly sampling frames, explicitly encoding actions and\ncreating interventions that challenge the understanding of the model. Overall,\nfor both single-frame (+6.3%) and complete-video (+1.1%) based approaches, we\nobtain the state-of-the-art results on NExT-QA dataset.",
      "tldr_zh": "这篇论文针对视频问答（Video Question Answering）任务的挑战，特别是处理因果和时间相关问题（如NExT-QA数据集），指出了现有方法（如子采样信息或因果干预技术）的局限性。作者提出了四个新方向的改进，包括智能采样帧（smartly sampling frames）、显式编码动作（explicitly encoding actions）和创建挑战模型理解的干预，从而提升模型对视频中物体互动的推理能力。在NExT-QA数据集上，该方法实现了最先进的结果，单帧方法准确率提升6.3%，完整视频方法提升1.1%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20257v1",
      "published_date": "2024-07-23 06:32:46 UTC",
      "updated_date": "2024-07-23 06:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:24:37.519181"
    },
    {
      "arxiv_id": "2407.16210v1",
      "title": "Strategy and Skill Learning for Physics-based Table Tennis Animation",
      "title_zh": "基于物理的乒乓球动画的策略与技能学习",
      "authors": [
        "Jiashun Wang",
        "Jessica Hodgins",
        "Jungdam Won"
      ],
      "abstract": "Recent advancements in physics-based character animation leverage deep\nlearning to generate agile and natural motion, enabling characters to execute\nmovements such as backflips, boxing, and tennis. However, reproducing the\nselection and use of diverse motor skills in dynamic environments to solve\ncomplex tasks, as humans do, still remains a challenge. We present a strategy\nand skill learning approach for physics-based table tennis animation. Our\nmethod addresses the issue of mode collapse, where the characters do not fully\nutilize the motor skills they need to perform to execute complex tasks. More\nspecifically, we demonstrate a hierarchical control system for diversified\nskill learning and a strategy learning framework for effective decision-making.\nWe showcase the efficacy of our method through comparative analysis with\nstate-of-the-art methods, demonstrating its capabilities in executing various\nskills for table tennis. Our strategy learning framework is validated through\nboth agent-agent interaction and human-agent interaction in Virtual Reality,\nhandling both competitive and cooperative tasks.",
      "tldr_zh": "该研究提出了一种策略和技能学习方法，用于基于物理的乒乓球动画，旨在解决角色在动态环境中无法充分利用多样化运动技能的问题，如模式崩溃（mode collapse）。该方法采用分层控制系统（hierarchical control system）来实现技能多样化学习，以及策略学习框架（strategy learning framework）来提升决策效能。实验结果通过与最先进方法的比较显示，该框架能有效执行各种乒乓球技能，并在代理-代理交互和人类-代理交互（Virtual Reality）中处理竞争性和合作性任务。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "SIGGRAPH 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16210v1",
      "published_date": "2024-07-23 06:31:13 UTC",
      "updated_date": "2024-07-23 06:31:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:24:49.746905"
    },
    {
      "arxiv_id": "2407.16205v5",
      "title": "LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shi Lin",
        "Hongming Yang",
        "Dingyang Lin",
        "Rongchang Li",
        "Xun Wang",
        "Changting Lin",
        "Wenpeng Xing",
        "Meng Han"
      ],
      "abstract": "The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.",
      "tldr_zh": "该研究揭示了大型语言模型(LLMs)在面对jailbreak攻击时的安全漏洞，提出了一种高效的Analyzing-based Jailbreak(ABJ)方法，利用LLMs的先进推理能力来自主生成有害内容，从而暴露其潜在风险。\nABJ克服了现有jailbreak方法的缺点，如依赖复杂的提示工程和迭代优化，导致的低攻击成功率(ASR)和攻击效率(AE)。\n实验结果显示，ABJ在多种开源和闭源LLMs上表现出色，在GPT-4o-2024-11-20上达到82.1%的ASR，并展现出卓越的转移性和效率。\n这强调了亟需优先提升LLMs的安全性，以防范潜在误用风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16205v5",
      "published_date": "2024-07-23 06:14:41 UTC",
      "updated_date": "2025-03-05 14:43:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:25:03.322032"
    },
    {
      "arxiv_id": "2407.16200v1",
      "title": "MCTS Based Dispatch of Autonomous Vehicles under Operational Constraints for Continuous Transportation",
      "title_zh": "基于蒙特卡罗树搜索的自治车辆调度，在操作约束下，用于连续运输",
      "authors": [
        "Milan Tomy",
        "Konstantin M. Seiler",
        "Andrew J. Hill"
      ],
      "abstract": "Continuous transportation of material in the mining industry is achieved by\nthe dispatch of autonomous haul-trucks with discrete haulage capacities.\nRecently, Monte Carlo Tree Search (MCTS) was successfully deployed in tackling\nchallenges of long-run optimality, scalability and adaptability in haul-truck\ndispatch. Typically, operational constraints imposed on the mine site are\nsatisfied by heuristic controllers or human operators independent of the\ndispatch planning. This article incorporates operational constraint\nsatisfaction into the dispatch planning by utilising the MCTS based dispatch\nplanner Flow-Achieving Scheduling Tree (FAST). Operational constraint violation\nand satisfaction are modelled as opportunity costs in the combinatorial\noptimisation problem of dispatch. Explicit cost formulations are avoided by\nutilising MCTS generator models to derive opportunity costs. Experimental\nstudies with four types of operational constraints demonstrate the success of\nutilising opportunity costs for constraint satisfaction, and the effectiveness\nof integrating constraints into dispatch planning.",
      "tldr_zh": "本研究提出了一种基于 Monte Carlo Tree Search (MCTS) 的方法，用于在操作约束下调度自主车辆，实现矿业材料的连续运输。该方法将操作约束（如资源限制）整合到调度规划器 Flow-Achieving Scheduling Tree (FAST) 中，通过建模约束违反和满足为机会成本，并利用 MCTS 生成器模型推导成本，避免了显式成本公式。实验结果显示，在四种操作约束类型上，该方法成功实现了约束满足，并证明了将约束融入调度规划的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "International Conference on Automation Science and Engineering\n  (CASE), 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16200v1",
      "published_date": "2024-07-23 06:06:16 UTC",
      "updated_date": "2024-07-23 06:06:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:25:13.989548"
    },
    {
      "arxiv_id": "2407.16198v1",
      "title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwei Ma",
        "Zhibin Wang",
        "Xiaoshuai Sun",
        "Weihuang Lin",
        "Qiang Zhou",
        "Jiayi Ji",
        "Rongrong Ji"
      ],
      "abstract": "With advancements in data availability and computing resources, Multimodal\nLarge Language Models (MLLMs) have showcased capabilities across various\nfields. However, the quadratic complexity of the vision encoder in MLLMs\nconstrains the resolution of input images. Most current approaches mitigate\nthis issue by cropping high-resolution images into smaller sub-images, which\nare then processed independently by the vision encoder. Despite capturing\nsufficient local details, these sub-images lack global context and fail to\ninteract with one another. To address this limitation, we propose a novel MLLM,\nINF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA\nincorporates two innovative components. First, we introduce a Dual-perspective\nCropping Module (DCM), which ensures that each sub-image contains continuous\ndetails from a local perspective and comprehensive information from a global\nperspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to\nenable the mutual enhancement of global and local features, allowing INF-LLaVA\nto effectively process high-resolution images by simultaneously capturing\ndetailed local information and comprehensive global context. Extensive ablation\nstudies validate the effectiveness of these components, and experiments on a\ndiverse set of benchmarks demonstrate that INF-LLaVA outperforms existing\nMLLMs. Code and pretrained model are available at\nhttps://github.com/WeihuangLin/INF-LLaVA.",
      "tldr_zh": "该论文提出 INF-LLaVA，一种新型 Multimodal Large Language Models (MLLMs)，旨在解决视觉编码器二次方复杂度导致的高分辨率图像处理限制问题。INF-LLaVA 引入 Dual-perspective Cropping Module (DCM) 来确保每个子图像包含局部连续细节和全局全面信息，以及 Dual-perspective Enhancement Module (DEM) 来实现全局与局部特征的相互增强，从而同时捕获详细局部信息和全面全局上下文。实验结果显示，通过消融研究和各种基准测试，INF-LLaVA 在性能上超过了现有 MLLMs，并提供了代码和预训练模型以供进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16198v1",
      "published_date": "2024-07-23 06:02:30 UTC",
      "updated_date": "2024-07-23 06:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:25:26.986909"
    },
    {
      "arxiv_id": "2407.16190v2",
      "title": "Artificial Agency and Large Language Models",
      "title_zh": "人工代理与大语言模型",
      "authors": [
        "Maud van Lier",
        "Gorka Muñoz-Gil"
      ],
      "abstract": "The arrival of Large Language Models (LLMs) has stirred up philosophical\ndebates about the possibility of realizing agency in an artificial manner. In\nthis work we contribute to the debate by presenting a theoretical model that\ncan be used as a threshold conception for artificial agents. The model defines\nagents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its\nadaptive repertoire and its external environment. This framework, in turn, is\ninfluenced by the actions that the agent takes and the goals that it forms. We\nshow with the help of the model that state-of-the-art LLMs are not agents yet,\nbut that there are elements to them that suggest a way forward. The paper\nargues that a combination of the agent architecture presented in Park et al.\n(2023) together with the use of modules like the Coscientist in Boiko et al.\n(2023) could potentially be a way to realize agency in an artificial manner. We\nend the paper by reflecting on the obstacles one might face in building such an\nartificial agent and by presenting possible directions for future research.",
      "tldr_zh": "这篇论文探讨了Large Language Models (LLMs) 是否能实现人工代理（artificial agency），并提出一个理论模型作为人工代理的阈值概念。模型将代理定义为一个系统，其行为和目标受动态框架影响，包括可访问历史（accessible history）、适应性 repertoire 和外部环境，而该框架又会反向受代理行动和目标的影响。论文分析表明，当前LLMs 尚未达到代理标准，但其某些元素显示了潜在可能性，并建议结合Park et al. (2023)的代理架构和Boiko et al. (2023)的Coscientist模块作为实现路径。最后，论文反思了构建人工代理的障碍，并提出未来研究方向。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication in journal Intellectica, special issue\n  \"Philosophies of AI: thinking and writing with LLMs\" (Intellectica, issue 81)",
      "pdf_url": "http://arxiv.org/pdf/2407.16190v2",
      "published_date": "2024-07-23 05:32:00 UTC",
      "updated_date": "2024-07-24 07:32:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:25:39.808186"
    },
    {
      "arxiv_id": "2407.16186v1",
      "title": "Automatic Environment Shaping is the Next Frontier in RL",
      "title_zh": "自动环境塑造是强化学习中的下一个前沿",
      "authors": [
        "Younghyo Park",
        "Gabriel B. Margolis",
        "Pulkit Agrawal"
      ],
      "abstract": "Many roboticists dream of presenting a robot with a task in the evening and\nreturning the next morning to find the robot capable of solving the task. What\nis preventing us from achieving this? Sim-to-real reinforcement learning (RL)\nhas achieved impressive performance on challenging robotics tasks, but requires\nsubstantial human effort to set up the task in a way that is amenable to RL.\nIt's our position that algorithmic improvements in policy optimization and\nother ideas should be guided towards resolving the primary bottleneck of\nshaping the training environment, i.e., designing observations, actions,\nrewards and simulation dynamics. Most practitioners don't tune the RL\nalgorithm, but other environment parameters to obtain a desirable controller.\nWe posit that scaling RL to diverse robotic tasks will only be achieved if the\ncommunity focuses on automating environment shaping procedures.",
      "tldr_zh": "该论文认为，实现机器人快速自主学习任务的关键在于自动环境塑造，这是强化学习（RL）的下一个前沿。目前，Sim-to-real RL 在复杂机器人任务中表现出色，但需要大量人工努力来设计观察、动作、奖励和模拟动态等环境参数。作者主张，RL 算法的改进应优先解决环境塑造的瓶颈，通过自动化这些过程来扩展 RL 到更多样化的机器人应用，从而减少手动干预并提升整体效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "ICML 2024 Position Track; Website at\n  https://auto-env-shaping.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.16186v1",
      "published_date": "2024-07-23 05:22:29 UTC",
      "updated_date": "2024-07-23 05:22:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:25:48.977832"
    },
    {
      "arxiv_id": "2407.16174v1",
      "title": "Pixel Embedding: Fully Quantized Convolutional Neural Network with Differentiable Lookup Table",
      "title_zh": "翻译失败",
      "authors": [
        "Hiroyuki Tokunaga",
        "Joel Nicholls",
        "Daria Vazhenina",
        "Atsunori Kanemura"
      ],
      "abstract": "By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.",
      "tldr_zh": "本文提出Pixel Embedding方法，用于实现全量化的卷积神经网络，通过一个可微分的Lookup Table将浮点输入像素替换为量化向量，从而解决现有量化技术在处理高位输入数据时的挑战。类似于NLP中的词嵌入，该方法使网络权重和激活值能够低位量化，同时保持可训练性。在ImageNet数据集上，Pixel Embedding将量化第一层导致的top-5错误率差距减少到仅1%；在CIFAR-100数据集上，量化第一和最后一层导致的top-1错误率差距控制在略高于1%。此外，该方法在推理时间上实现了超过1.7倍的加速，提高了网络的硬件友好性和能效。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16174v1",
      "published_date": "2024-07-23 04:41:36 UTC",
      "updated_date": "2024-07-23 04:41:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:26:02.396195"
    },
    {
      "arxiv_id": "2407.16171v1",
      "title": "Learning Trimodal Relation for Audio-Visual Question Answering with Missing Modality",
      "title_zh": "翻译失败",
      "authors": [
        "Kyu Ri Park",
        "Hong Joo Lee",
        "Jung Uk Kim"
      ],
      "abstract": "Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.",
      "tldr_zh": "该论文针对音频-视觉问答(AVQA)中模态缺失问题（如音频或视觉输入缺失），提出一个鲁棒框架，以确保性能不显著下降。主要方法包括Relation-aware Missing Modal (RMM)生成器及其Relation-aware Missing Modal Recalling (RMMR)损失函数，用于通过理解可用模态间的关系和上下文来回忆缺失信息；以及Audio-Visual Relation-aware (AVR)扩散模型及其Audio-Visual Enhancing (AVE)损失函数，用于增强音频-视觉特征并利用共享线索。结果表明，该框架能有效利用可用信息提供准确答案，并在AVQA及其他多模态场景中具有潜在应用价值。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16171v1",
      "published_date": "2024-07-23 04:35:56 UTC",
      "updated_date": "2024-07-23 04:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:26:14.529209"
    },
    {
      "arxiv_id": "2407.16164v1",
      "title": "Representation Magnitude has a Liability to Privacy Vulnerability",
      "title_zh": "翻译失败",
      "authors": [
        "Xingli Fang",
        "Jung-Eun Kim"
      ],
      "abstract": "The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}",
      "tldr_zh": "本研究探讨了机器学习模型中表示幅度（representation magnitude）的差异如何导致隐私漏洞，特别分析了成员数据和非成员数据在常见训练框架下的表示差异及其与隐私脆弱性的相关性。研究发现，这种差异会加剧模型的隐私风险，同时影响模型的性能和隐私平衡。针对这一问题，提出Saturn Ring Classifier Module (SRCM)，一个插件式模型级解决方案，通过受限的表示空间来缓解成员隐私泄露，同时保持模型的泛化能力。该方法为提升机器学习模型的隐私保护提供了有效途径，相关代码可在GitHub上获取。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16164v1",
      "published_date": "2024-07-23 04:13:52 UTC",
      "updated_date": "2024-07-23 04:13:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:26:26.858232"
    },
    {
      "arxiv_id": "2407.16160v2",
      "title": "UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Liu Qi",
        "He Yongyi",
        "Lian Defu",
        "Zheng Zhi",
        "Xu Tong",
        "Liu Che",
        "Chen Enhong"
      ],
      "abstract": "Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://github.com/Javkonline/UniMEL.",
      "tldr_zh": "该论文提出 UniMEL，一种统一的框架，利用 Large Language Models (LLMs) 和 Multimodal Large Language Models (MLLMs) 来处理 Multimodal Entity Linking (MEL) 任务，该任务涉及将多模态上下文中的模糊提及链接到知识库实体。UniMEL 通过 LLMs 增强提及和实体的文本及视觉表示，进行基于嵌入的候选实体检索和重新排序，并仅微调约 0.26% 的模型参数来实现最终选择，从而解决文本模糊性、冗余和噪声图像等问题。实验在三个公共基准数据集上显示，UniMEL 达到了 state-of-the-art 性能，并通过消融研究验证了各模块的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "CIKM 2024. The first two authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2407.16160v2",
      "published_date": "2024-07-23 03:58:08 UTC",
      "updated_date": "2024-08-21 01:52:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:26:38.886222"
    },
    {
      "arxiv_id": "2408.01446v1",
      "title": "Estimating Environmental Cost Throughout Model's Adaptive Life Cycle",
      "title_zh": "在模型自适应生命周期中估计环境成本",
      "authors": [
        "Vishwesh Sangarya",
        "Richard Bradford",
        "Jung-Eun Kim"
      ],
      "abstract": "With the rapid increase in the research, development, and application of\nneural networks in the current era, there is a proportional increase in the\nenergy needed to train and use models. Crucially, this is accompanied by the\nincrease in carbon emissions into the environment. A sustainable and socially\nbeneficial approach to reducing the carbon footprint and rising energy demands\nassociated with the modern age of AI/deep learning is the adaptive and\ncontinuous reuse of models with regard to changes in the environment of model\ndeployment or variations/changes in the input data. In this paper, we propose\nPreIndex, a predictive index to estimate the environmental and compute\nresources associated with model retraining to distributional shifts in data.\nPreIndex can be used to estimate environmental costs such as carbon emissions\nand energy usage when retraining from current data distribution to new data\ndistribution. It also correlates with and can be used to estimate other\nresource indicators associated with deep learning, such as epochs, gradient\nnorm, and magnitude of model parameter change. PreIndex requires only one\nforward pass of the data, following which it provides a single concise value to\nestimate resources associated with retraining to the new distribution shifted\ndata. We show that PreIndex can be reliably used across various datasets, model\narchitectures, different types, and intensities of distribution shifts. Thus,\nPreIndex enables users to make informed decisions for retraining to different\ndistribution shifts and determine the most cost-effective and sustainable\noption, allowing for the reuse of a model with a much smaller footprint in the\nenvironment. The code for this work is available here:\nhttps://github.com/JEKimLab/AIES2024PreIndex",
      "tldr_zh": "这篇论文针对AI模型训练和使用导致的能源消耗及carbon emissions增加问题，提出了一种可持续方法，通过模型的适应性重用来应对数据分布偏移。作者开发了PreIndex，一个预测指数，仅需一次前向传递即可估计重新训练的环境成本（如carbon emissions、能源使用）和计算资源（如训练周期、梯度范数及模型参数变化）。实验结果显示，PreIndex在多种数据集、模型架构及distribution shifts类型上表现可靠，帮助用户做出明智决策，选择最经济可持续的模型重用策略，从而减少环境足迹。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024",
      "pdf_url": "http://arxiv.org/pdf/2408.01446v1",
      "published_date": "2024-07-23 03:58:06 UTC",
      "updated_date": "2024-07-23 03:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:26:51.141444"
    },
    {
      "arxiv_id": "2407.16150v1",
      "title": "Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjun Gu",
        "Yihao Zhong",
        "Shizun Li",
        "Changsong Wei",
        "Liting Dong",
        "Zhuoyue Wang",
        "Chao Yan"
      ],
      "abstract": "The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.",
      "tldr_zh": "本研究探讨了利用深度学习模型预测股票价格，重点整合新闻情绪分析以提升预测准确性。作者开发了FinBERT模型，用于分析金融文本的情感，并将其与Long Short Term Memory (LSTM)架构结合，创建了FinBERT-LSTM模型，该模型融入市场、行业和股票相关新闻类别，以及前一周的股票价格数据进行预测。实验使用NASDAQ-100指数股票数据和Benzinga新闻文章进行训练，并以Mean Absolute Error (MAE)、Mean Absolute Percentage Error (MAPE)和Accuracy作为评估指标。结果显示，FinBERT-LSTM模型的表现优于LSTM和DNN模型，证明了新闻情绪整合的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 6 figures, 2 tables, 2024 8th International Conference on\n  Cloud and Big Data Computing",
      "pdf_url": "http://arxiv.org/pdf/2407.16150v1",
      "published_date": "2024-07-23 03:26:07 UTC",
      "updated_date": "2024-07-23 03:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:27:03.859064"
    },
    {
      "arxiv_id": "2407.16715v2",
      "title": "Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning",
      "title_zh": "结合知识图谱嵌入和深度学习的药物不良反应预测模型研究",
      "authors": [
        "Yufeng Li",
        "Wenchao Zhao",
        "Bo Dang",
        "Xu Yan",
        "Weimin Wang",
        "Min Gao",
        "Mingxuan Xiao"
      ],
      "abstract": "In clinical treatment, identifying potential adverse reactions of drugs can\nhelp assist doctors in making medication decisions. In response to the problems\nin previous studies that features are high-dimensional and sparse, independent\nprediction models need to be constructed for each adverse reaction of drugs,\nand the prediction accuracy is low, this paper develops an adverse drug\nreaction prediction model based on knowledge graph embedding and deep learning,\nwhich can predict experimental results. Unified prediction of adverse drug\nreactions covered. Knowledge graph embedding technology can fuse the associated\ninformation between drugs and alleviate the shortcomings of high-dimensional\nsparsity in feature matrices, and the efficient training capabilities of deep\nlearning can improve the prediction accuracy of the model. This article builds\nan adverse drug reaction knowledge graph based on drug feature data; by\nanalyzing the embedding effect of the knowledge graph under different embedding\nstrategies, the best embedding strategy is selected to obtain sample vectors;\nand then a convolutional neural network model is constructed to predict adverse\nreactions. The results show that under the DistMult embedding model and\n400-dimensional embedding strategy, the convolutional neural network model has\nthe best prediction effect; the average accuracy, F_1 score, recall rate and\narea under the curve of repeated experiments are better than the methods\nreported in the literature. The obtained prediction model has good prediction\naccuracy and stability, and can provide an effective reference for later safe\nmedication guidance.",
      "tldr_zh": "本研究针对药物不良反应预测中特征高维稀疏、需单独构建模型以及准确率低的问题，提出了一种结合 Knowledge Graph Embedding 和 Deep Learning 的预测模型。该模型首先构建不良药物反应知识图，并通过分析不同嵌入策略（如 DistMult 和 400-维嵌入）选择最佳方案，以融合药物间关联信息并缓解特征稀疏性；随后，利用 Convolutional Neural Network 进行统一预测多种不良反应。实验结果显示，该模型在 DistMult 策略下表现出最佳性能，平均准确率、F_1 score、召回率和 AUC 均优于现有文献方法，具有良好的稳定性和实际应用价值，可为安全用药提供有效指导。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "12 pages, 4 figures, 9 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.16715v2",
      "published_date": "2024-07-23 03:25:55 UTC",
      "updated_date": "2024-07-27 15:09:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:27:15.097469"
    },
    {
      "arxiv_id": "2407.16142v1",
      "title": "Diffusion Models as Optimizers for Efficient Planning in Offline RL",
      "title_zh": "翻译失败",
      "authors": [
        "Renming Huang",
        "Yunqiang Pei",
        "Guoqing Wang",
        "Yangming Zhang",
        "Yang Yang",
        "Peng Wang",
        "Hengtao Shen"
      ],
      "abstract": "Diffusion models have shown strong competitiveness in offline reinforcement\nlearning tasks by formulating decision-making as sequential generation.\nHowever, the practicality of these methods is limited due to the lengthy\ninference processes they require. In this paper, we address this problem by\ndecomposing the sampling process of diffusion models into two decoupled\nsubprocesses: 1) generating a feasible trajectory, which is a time-consuming\nprocess, and 2) optimizing the trajectory. With this decomposition approach, we\nare able to partially separate efficiency and quality factors, enabling us to\nsimultaneously gain efficiency advantages and ensure quality assurance. We\npropose the Trajectory Diffuser, which utilizes a faster autoregressive model\nto handle the generation of feasible trajectories while retaining the\ntrajectory optimization process of diffusion models. This allows us to achieve\nmore efficient planning without sacrificing capability. To evaluate the\neffectiveness and efficiency of the Trajectory Diffuser, we conduct experiments\non the D4RL benchmarks. The results demonstrate that our method achieves $\\it\n3$-$\\it 10 \\times$ faster inference speed compared to previous sequence\nmodeling methods, while also outperforming them in terms of overall\nperformance. https://github.com/RenMing-Huang/TrajectoryDiffuser\n  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model",
      "tldr_zh": "该研究将扩散模型（Diffusion Models）作为优化器应用于离线强化学习（Offline RL），通过分解采样过程为生成可行轨迹和优化轨迹两个子过程，提高了决策效率。论文提出Trajectory Diffuser方法，使用更快的自回归模型生成可行轨迹，同时保留扩散模型的轨迹优化功能，从而实现效率与性能的平衡。在D4RL基准实验中，该方法比现有序列建模方法快3-10倍，同时在整体性能上表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "The paper was accepted by ECCV2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16142v1",
      "published_date": "2024-07-23 03:00:01 UTC",
      "updated_date": "2024-07-23 03:00:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:27:27.372035"
    },
    {
      "arxiv_id": "2407.21049v1",
      "title": "Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Yannick Assogba",
        "Donghao Ren"
      ],
      "abstract": "As language models support larger and larger context sizes, evaluating their\nability to make effective use of that context becomes increasingly important.\nWe analyze the ability of several code generation models to handle long range\ndependencies using a suite of multi-step key retrieval tasks in context windows\nup to 8k tokens in length. The tasks progressively increase in difficulty and\nallow more nuanced evaluation of model capabilities than tests like the popular\nneedle-in-the-haystack test. We find that performance degrades significantly\n(up to 2x) when a function references another function that is defined later in\nthe prompt. We also observe that models that use sliding window attention\nmechanisms have difficulty handling references further than the size of a\nsingle window. We perform simple prompt modifications using call graph\ninformation to improve multi-step retrieval performance up to 3x. Our analysis\nhighlights different facets of long-context performance and is suggestive of\nprompt construction strategies for code completion tools",
      "tldr_zh": "这篇论文评估了代码生成模型处理长距离依赖（long range dependencies）的能力，使用多步关键检索（multi-step key retrieval）任务，上下文窗口长达 8k tokens，这些任务的难度逐步增加，提供比传统测试（如 needle-in-the-haystack）更细致的评估。研究发现，当函数引用位于提示中较后的位置时，模型性能显著下降（高达 2 倍），而采用滑动窗口注意力（sliding window attention）机制的模型难以处理超过单个窗口的引用。通过利用调用图（call graph）信息进行简单的提示修改，论文将多步检索性能提高了高达 3 倍。该分析突出了长上下文性能的不同方面，并为代码完成工具的提示构建策略提供了实用建议。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21049v1",
      "published_date": "2024-07-23 02:45:22 UTC",
      "updated_date": "2024-07-23 02:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:27:40.387938"
    },
    {
      "arxiv_id": "2407.16129v1",
      "title": "FoRA: Low-Rank Adaptation Model beyond Multimodal Siamese Network",
      "title_zh": "翻译失败",
      "authors": [
        "Weiying Xie",
        "Yusi Zhang",
        "Tianlin Hui",
        "Jiaqing Zhang",
        "Jie Lei",
        "Yunsong Li"
      ],
      "abstract": "Multimodal object detection offers a promising prospect to facilitate robust\ndetection in various visual conditions. However, existing two-stream backbone\nnetworks are challenged by complex fusion and substantial parameter increments.\nThis is primarily due to large data distribution biases of multimodal\nhomogeneous information. In this paper, we propose a novel multimodal object\ndetector, named Low-rank Modal Adaptors (LMA) with a shared backbone. The\nshared parameters enhance the consistency of homogeneous information, while\nlightweight modal adaptors focus on modality unique features. Furthermore, we\ndesign an adaptive rank allocation strategy to adapt to the varying\nheterogeneity at different feature levels. When applied to two multimodal\nobject detection datasets, experiments validate the effectiveness of our\nmethod. Notably, on DroneVehicle, LMA attains a 10.4% accuracy improvement over\nthe state-of-the-art method with a 149M-parameters reduction. The code is\navailable at https://github.com/zyszxhy/FoRA.\n  Our work was submitted to ACM MM in April 2024, but was rejected. We will\ncontinue to refine our work and paper writing next, mainly including proof of\ntheory and multi-task applications of FoRA.",
      "tldr_zh": "本研究针对多模态物体检测中的复杂融合和参数增加问题，提出了一种名为 FoRA 的低秩适应模型（Low-Rank Adaptation Model），超越传统的多模态 Siamese 网络。该方法采用共享骨干网络（shared backbone）和轻量级模态适配器（Low-rank Modal Adaptors），通过自适应秩分配策略（adaptive rank allocation strategy）来增强同质信息的一致性和处理模态独特特征。在 DroneVehicle 等数据集上的实验显示，FoRA 比最先进方法提高了 10.4% 的准确率，同时减少了 149M 参数，证明了其高效性。代码已在 GitHub 上开源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16129v1",
      "published_date": "2024-07-23 02:27:52 UTC",
      "updated_date": "2024-07-23 02:27:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:27:51.973042"
    },
    {
      "arxiv_id": "2407.16128v1",
      "title": "Advancing Brain Imaging Analysis Step-by-step via Progressive Self-paced Learning",
      "title_zh": "通过渐进自适应学习逐步推进脑成像分析",
      "authors": [
        "Yanwu Yang",
        "Hairui Chen",
        "Jiesi Hu",
        "Xutao Guo",
        "Ting Ma"
      ],
      "abstract": "Recent advancements in deep learning have shifted the development of brain\nimaging analysis. However, several challenges remain, such as heterogeneity,\nindividual variations, and the contradiction between the high dimensionality\nand small size of brain imaging datasets. These issues complicate the learning\nprocess, preventing models from capturing intrinsic, meaningful patterns and\npotentially leading to suboptimal performance due to biases and overfitting.\nCurriculum learning (CL) presents a promising solution by organizing training\nexamples from simple to complex, mimicking the human learning process, and\npotentially fostering the development of more robust and accurate models.\nDespite its potential, the inherent limitations posed by small initial training\ndatasets present significant challenges, including overfitting and poor\ngeneralization. In this paper, we introduce the Progressive Self-Paced\nDistillation (PSPD) framework, employing an adaptive and progressive pacing and\ndistillation mechanism. This allows for dynamic curriculum adjustments based on\nthe states of both past and present models. The past model serves as a teacher,\nguiding the current model with gradually refined curriculum knowledge and\nhelping prevent the loss of previously acquired knowledge. We validate PSPD's\nefficacy and adaptability across various convolutional neural networks using\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, underscoring\nits superiority in enhancing model performance and generalization capabilities.\nThe source code for this approach will be released at\nhttps://github.com/Hrychen7/PSPD.",
      "tldr_zh": "脑成像分析面临异质性、个体差异以及高维度小样本数据集的挑战，导致模型容易出现偏差和过拟合。论文提出 Progressive Self-Paced Distillation (PSPD) 框架，通过自适应渐进式节奏和知识蒸馏机制，动态调整 Curriculum Learning (CL) 过程，其中过去模型作为教师指导当前模型，防止知识丢失。实验在 Alzheimer's Disease Neuroimaging Initiative (ADNI) 数据集上验证了 PSPD 在各种卷积神经网络上的优越性，提升了模型性能和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "miccai-2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16128v1",
      "published_date": "2024-07-23 02:26:04 UTC",
      "updated_date": "2024-07-23 02:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:28:03.786018"
    },
    {
      "arxiv_id": "2407.16127v1",
      "title": "Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Liu",
        "Xiaobin Tian",
        "Zequn Sun",
        "Wei Hu"
      ],
      "abstract": "Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.",
      "tldr_zh": "本研究提出了一种微调框架DIFT，用于提升生成式Large Language Models (LLMs)在Knowledge Graph Completion (KG)任务中的性能，同时避免实体映射错误。DIFT的方法包括使用轻量级模型获取候选实体，并通过Discrimination Instructions微调LLMs，以从候选中选择正确的实体；此外，还采用截断采样方法筛选有用的facts进行微调，并注入KG嵌入以优化性能。在基准数据集上的广泛实验证明，该框架有效提高了KG完成任务的准确性和效率。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.16127v1",
      "published_date": "2024-07-23 02:25:01 UTC",
      "updated_date": "2024-07-23 02:25:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:28:15.621782"
    },
    {
      "arxiv_id": "2407.16714v1",
      "title": "Masked Graph Learning with Recurrent Alignment for Multimodal Emotion Recognition in Conversation",
      "title_zh": "基于循环对齐的掩码图学习，用于多模态对话情感",
      "authors": [
        "Tao Meng",
        "Fuchen Zhang",
        "Yuntao Shou",
        "Hongen Shao",
        "Wei Ai",
        "Keqin Li"
      ],
      "abstract": "Since Multimodal Emotion Recognition in Conversation (MERC) can be applied to\npublic opinion monitoring, intelligent dialogue robots, and other fields, it\nhas received extensive research attention in recent years. Unlike traditional\nunimodal emotion recognition, MERC can fuse complementary semantic information\nbetween multiple modalities (e.g., text, audio, and vision) to improve emotion\nrecognition. However, previous work ignored the inter-modal alignment process\nand the intra-modal noise information before multimodal fusion but directly\nfuses multimodal features, which will hinder the model for representation\nlearning. In this study, we have developed a novel approach called Masked Graph\nLearning with Recursive Alignment (MGLRA) to tackle this problem, which uses a\nrecurrent iterative module with memory to align multimodal features, and then\nuses the masked GCN for multimodal feature fusion. First, we employ LSTM to\ncapture contextual information and use a graph attention-filtering mechanism to\neliminate noise effectively within the modality. Second, we build a recurrent\niteration module with a memory function, which can use communication between\ndifferent modalities to eliminate the gap between modalities and achieve the\npreliminary alignment of features between modalities. Then, a cross-modal\nmulti-head attention mechanism is introduced to achieve feature alignment\nbetween modalities and construct a masked GCN for multimodal feature fusion,\nwhich can perform random mask reconstruction on the nodes in the graph to\nobtain better node feature representation. Finally, we utilize a multilayer\nperceptron (MLP) for emotion recognition. Extensive experiments on two\nbenchmark datasets (i.e., IEMOCAP and MELD) demonstrate that {MGLRA}\noutperforms state-of-the-art methods.",
      "tldr_zh": "本研究针对多模态对话情感识别（Multimodal Emotion Recognition in Conversation, MERC）的问题，提出了一种新型方法Masked Graph Learning with Recurrent Alignment (MGLRA)，通过循环迭代模块和记忆功能实现多模态特征的初步对齐，并消除模态内的噪声信息。MGLRA 首先使用 LSTM 捕获上下文信息和图注意力过滤机制处理噪声，然后通过跨模态多头注意力机制进一步对齐特征，并采用 masked GCN 进行多模态特征融合，最后利用多层感知器 (MLP) 进行情感识别。实验结果显示，该方法在 IEMOCAP 和 MELD 基准数据集上优于现有最先进方法，显著提升了情感识别的准确性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16714v1",
      "published_date": "2024-07-23 02:23:51 UTC",
      "updated_date": "2024-07-23 02:23:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:28:27.722886"
    },
    {
      "arxiv_id": "2407.21048v1",
      "title": "APTNESS: Incorporating Appraisal Theory and Emotion Support Strategies for Empathetic Response Generation",
      "title_zh": "APTNESS：整合评价理论和情感支持策略用于移情响应生成",
      "authors": [
        "Yuxuan Hu",
        "Minghuan Tan",
        "Chenwei Zhang",
        "Zixuan Li",
        "Xiaodan Liang",
        "Min Yang",
        "Chengming Li",
        "Xiping Hu"
      ],
      "abstract": "Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.",
      "tldr_zh": "本文提出 APTNESS 框架，结合 Appraisal Theory 和情感支持策略，旨在提升大语言模型（LLMs）的同理心响应生成能力，通过引入情感调色板并应用评估理论分解创建响应数据库，同时整合检索增强机制来增强认知 empathy 和 affective empathy。框架强调响应策略的适当表达，并在从 EmpatheticDialogues 和 ExTES 数据集中提取的 ED 和 ET 子数据集上进行实验。结果显示，APTNESS 显著提高了 LLMs 在认知和情感同理心方面的表现，并开源了相关代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Appectped to CIKM2024",
      "pdf_url": "http://arxiv.org/pdf/2407.21048v1",
      "published_date": "2024-07-23 02:23:37 UTC",
      "updated_date": "2024-07-23 02:23:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:28:39.796020"
    },
    {
      "arxiv_id": "2407.16123v1",
      "title": "Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal Data for Smart Mobility",
      "title_zh": "面向智能移动性的多模态时空数据有效融合和预测",
      "authors": [
        "Chenxing Wang"
      ],
      "abstract": "With the rapid development of location based services, multimodal\nspatio-temporal (ST) data including trajectories, transportation modes, traffic\nflow and social check-ins are being collected for deep learning based methods.\nThese deep learning based methods learn ST correlations to support the\ndownstream tasks in the fields such as smart mobility, smart city and other\nintelligent transportation systems. Despite their effectiveness, ST data fusion\nand forecasting methods face practical challenges in real-world scenarios.\nFirst, forecasting performance for ST data-insufficient area is inferior,\nmaking it necessary to transfer meta knowledge from heterogeneous area to\nenhance the sparse representations. Second, it is nontrivial to accurately\nforecast in multi-transportation-mode scenarios due to the fine-grained ST\nfeatures of similar transportation modes, making it necessary to distinguish\nand measure the ST correlations to alleviate the influence caused by entangled\nST features. At last, partial data modalities (e.g., transportation mode) are\nlost due to privacy or technical issues in certain scenarios, making it\nnecessary to effectively fuse the multimodal sparse ST features and enrich the\nST representations. To tackle these challenges, our research work aim to\ndevelop effective fusion and forecasting methods for multimodal ST data in\nsmart mobility scenario. In this paper, we will introduce our recent works that\ninvestigates the challenges in terms of various real-world applications and\nestablish the open challenges in this field for future work.",
      "tldr_zh": "该论文探讨了多模态时空（ST）数据的融合和预测问题，以支持智能交通（smart mobility）等领域的下游任务。面对实际挑战，包括ST数据不足区域的预测性能低下、多交通模式场景下细粒度特征的纠缠以及部分数据模态（如交通模式）的丢失，研究提出通过转移元知识、区分和测量ST相关性以及有效融合多模态稀疏特征的方法来提升预测准确性。论文介绍了最近的相关工作，并总结了这些挑战，为未来研究建立了开放问题。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.16123v1",
      "published_date": "2024-07-23 02:08:22 UTC",
      "updated_date": "2024-07-23 02:08:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:28:52.184847"
    },
    {
      "arxiv_id": "2407.16119v2",
      "title": "Uncertainty-Aware Deep Neural Representations for Visual Analysis of Vector Field Data",
      "title_zh": "不确定性感知的深度神经表示用于矢量场数据的视觉分析",
      "authors": [
        "Atul Kumar",
        "Siddharth Garg",
        "Soumya Dutta"
      ],
      "abstract": "The widespread use of Deep Neural Networks (DNNs) has recently resulted in\ntheir application to challenging scientific visualization tasks. While advanced\nDNNs demonstrate impressive generalization abilities, understanding factors\nlike prediction quality, confidence, robustness, and uncertainty is crucial.\nThese insights aid application scientists in making informed decisions.\nHowever, DNNs lack inherent mechanisms to measure prediction uncertainty,\nprompting the creation of distinct frameworks for constructing robust\nuncertainty-aware models tailored to various visualization tasks. In this work,\nwe develop uncertainty-aware implicit neural representations to model\nsteady-state vector fields effectively. We comprehensively evaluate the\nefficacy of two principled deep uncertainty estimation techniques: (1) Deep\nEnsemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed\nvisual analysis of features within steady vector field data. Our detailed\nexploration using several vector data sets indicate that uncertainty-aware\nmodels generate informative visualization results of vector field features.\nFurthermore, incorporating prediction uncertainty improves the resilience and\ninterpretability of our DNN model, rendering it applicable for the analysis of\nnon-trivial vector field data sets.",
      "tldr_zh": "本文提出了一种不确定性感知的隐式神经表示方法，用于提升稳态向量场数据的视觉分析。研究评估了两种不确定性估计技术：Deep Ensemble 和 Monte Carlo Dropout，以生成更具信息性的可视化结果。结果显示，这些方法显著提高了DNN模型的鲁棒性和可解释性，使其适用于复杂向量场数据的分析。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GR",
      "comment": "Accepted for publication at IEEE Visualization 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16119v2",
      "published_date": "2024-07-23 01:59:58 UTC",
      "updated_date": "2024-08-10 10:06:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:29:03.100179"
    },
    {
      "arxiv_id": "2407.16115v2",
      "title": "Transformer-based Graph Neural Networks for Battery Range Prediction in AIoT Battery-Swap Services",
      "title_zh": "基于 Transformer 的图神经网络，用于 AIoT 电池更换服务中的电池剩余里程预测",
      "authors": [
        "Zhao Li",
        "Yang Liu",
        "Chuan Zhou",
        "Xuanwu Liu",
        "Xuming Pan",
        "Buqing Cao",
        "Xindong Wu"
      ],
      "abstract": "The concept of the sharing economy has gained broad recognition, and within\nthis context, Sharing E-Bike Battery (SEB) have emerged as a focal point of\nsocietal interest. Despite the popularity, a notable discrepancy remains\nbetween user expectations regarding the remaining battery range of SEBs and the\nreality, leading to a pronounced inclination among users to find an available\nSEB during emergency situations. In response to this challenge, the integration\nof Artificial Intelligence of Things (AIoT) and battery-swap services has\nsurfaced as a viable solution. In this paper, we propose a novel structural\nTransformer-based model, referred to as the SEB-Transformer, designed\nspecifically for predicting the battery range of SEBs. The scenario is\nconceptualized as a dynamic heterogeneous graph that encapsulates the\ninteractions between users and bicycles, providing a comprehensive framework\nfor analysis. Furthermore, we incorporate the graph structure into the\nSEB-Transformer to facilitate the estimation of the remaining e-bike battery\nrange, in conjunction with mean structural similarity, enhancing the prediction\naccuracy. By employing the predictions made by our model, we are able to\ndynamically adjust the optimal cycling routes for users in real-time, while\nalso considering the strategic locations of charging stations, thereby\noptimizing the user experience. Empirically our results on real-world datasets\ndemonstrate the superiority of our model against nine competitive baselines.\nThese innovations, powered by AIoT, not only bridge the gap between user\nexpectations and the physical limitations of battery range but also\nsignificantly improve the operational efficiency and sustainability of SEB\nservices. Through these advancements, the shared electric bicycle ecosystem is\nevolving, making strides towards a more reliable, user-friendly, and\nsustainable mode of transportation.",
      "tldr_zh": "本论文针对共享电动自行车电池 (SEB) 服务中，用户对电池续航里程期望与实际不符的问题，提出了一种基于 Transformer 的图神经网络模型 SEB-Transformer，用于在 AIoT 电池更换服务中预测电池剩余续航。模型将用户和自行车的交互建模为动态异构图，并整合图结构与均值结构相似性，以提高预测准确性，同时动态优化用户骑行路线和充电站位置。实验结果显示，该模型在真实数据集上优于九个竞争基线，显著提升了 SEB 服务的操作效率和可持续性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9pages, 6figures, accepted by IEEE ICWS 2024 The International\n  Conference on Web Services",
      "pdf_url": "http://arxiv.org/pdf/2407.16115v2",
      "published_date": "2024-07-23 01:33:21 UTC",
      "updated_date": "2025-02-14 06:38:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:29:16.570712"
    },
    {
      "arxiv_id": "2408.01445v1",
      "title": "MiranDa: Mimicking the Learning Processes of Human Doctors to Achieve Causal Inference for Medication Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziheng Wang",
        "Xinhe Li",
        "Haruki Momma",
        "Ryoichi Nagatomi"
      ],
      "abstract": "To enhance therapeutic outcomes from a pharmacological perspective, we\npropose MiranDa, designed for medication recommendation, which is the first\nactionable model capable of providing the estimated length of stay in hospitals\n(ELOS) as counterfactual outcomes that guide clinical practice and model\ntraining. In detail, MiranDa emulates the educational trajectory of doctors\nthrough two gradient-scaling phases shifted by ELOS: an Evidence-based Training\nPhase that utilizes supervised learning and a Therapeutic Optimization Phase\ngrounds in reinforcement learning within the gradient space, explores optimal\nmedications by perturbations from ELOS. Evaluation of the Medical Information\nMart for Intensive Care III dataset and IV dataset, showcased the superior\nresults of our model across five metrics, particularly in reducing the ELOS.\nSurprisingly, our model provides structural attributes of medication\ncombinations proved in hyperbolic space and advocated \"procedure-specific\"\nmedication combinations. These findings posit that MiranDa enhanced medication\nefficacy. Notably, our paradigm can be applied to nearly all medical tasks and\nthose with information to evaluate predicted outcomes. The source code of the\nMiranDa model is available at https://github.com/azusakou/MiranDa.",
      "tldr_zh": "本研究提出 MiranDa 模型，用于药物推荐，通过模仿医生的学习过程实现因果推理，并首次提供预估住院时间 (ELOS) 作为反事实结果，以指导临床实践和模型训练。MiranDa 分为两个阶段：Evidence-based Training Phase 采用监督学习，以及 Therapeutic Optimization Phase 通过强化学习在 ELOS 梯度空间中探索最佳药物组合。实验在 MIMIC-III 和 MIMIC-IV 数据集上显示，该模型在五个指标上表现出色，特别是显著减少 ELOS，并揭示药物组合的结构属性，如在双曲空间 (hyperbolic space) 中证明的“procedure-specific”组合。这些发现提升了药物疗效，并表明该范式可扩展到几乎所有医疗任务和可评估预测结果的领域。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01445v1",
      "published_date": "2024-07-23 01:08:57 UTC",
      "updated_date": "2024-07-23 01:08:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:29:28.582475"
    },
    {
      "arxiv_id": "2407.16110v3",
      "title": "Analyzing Polysemy Evolution Using Semantic Cells",
      "title_zh": "翻译失败",
      "authors": [
        "Yukio Ohsawa",
        "Dingming Xue",
        "Kaira Sekiguchi"
      ],
      "abstract": "The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.",
      "tldr_zh": "该论文提出了一种使用 Semantic Cells 分析词语多义性（Polysemy）演化的方法，认为词义如同生态系统般动态变化，多义现象可能源于彼此的演变关系。作者通过引入初始状态的多样性，对 Chat GPT 收集的 1000 句句子序列进行案例研究，聚焦单词 \"Spring\" 的四个义项。结果显示，当按演化顺序排列义项时，单词的多义性会单调增加，从而揭示多义性是 Semantic Cells 修改的演化后果。最终，该研究提供了一种从演化框架分析词语动态获得多义性的方法，取代传统的学习-based 范式。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749",
      "pdf_url": "http://arxiv.org/pdf/2407.16110v3",
      "published_date": "2024-07-23 00:52:12 UTC",
      "updated_date": "2024-08-06 02:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:29:40.663084"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 105,
  "processed_papers_count": 105,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T09:30:04.149699"
}
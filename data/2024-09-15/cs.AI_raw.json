[
  {
    "arxiv_id": "2410.01818v1",
    "title": "Integrating AI's Carbon Footprint into Risk Management Frameworks: Strategies and Tools for Sustainable Compliance in Banking Sector",
    "authors": [
      "Nataliya Tkachenko"
    ],
    "abstract": "This paper examines the integration of AI's carbon footprint into the risk\nmanagement frameworks (RMFs) of the banking sector, emphasising its importance\nin aligning with sustainability goals and regulatory requirements. As AI\nbecomes increasingly central to banking operations, its energy-intensive\nprocesses contribute significantly to carbon emissions, posing environmental,\nregulatory, and reputational risks. Regulatory frameworks such as the EU AI\nAct, Corporate Sustainability Reporting Directive (CSRD), Corporate\nSustainability Due Diligence Directive (CSDDD), and the Prudential Regulation\nAuthority's SS1/23 are driving banks to incorporate environmental\nconsiderations into their AI model governance. Recent advancements in AI\nresearch, like the Open Mixture-of-Experts (OLMoE) framework and the Agentic\nRAG framework, offer more efficient and dynamic AI models, reducing their\ncarbon footprint without compromising performance. Using these technological\nexamples, the paper outlines a structured approach for banks to identify,\nassess, and mitigate AI's carbon footprint within their RMFs, including\nadopting energy-efficient models, utilising green cloud computing, and\nimplementing lifecycle management.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01818v1",
    "published_date": "2024-09-15 23:09:27 UTC",
    "updated_date": "2024-09-15 23:09:27 UTC"
  },
  {
    "arxiv_id": "2409.09877v2",
    "title": "REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models",
    "authors": [
      "Teerapong Panboonyuen"
    ],
    "abstract": "This paper introduces a novel framework for detecting and segmenting critical\nroad assets on Thai highways using an advanced Refined Generalized Focal Loss\n(REG) formulation. Integrated into state-of-the-art vision-based detection and\nsegmentation models, the proposed method effectively addresses class imbalance\nand the challenges of localizing small, underrepresented road elements,\nincluding pavilions, pedestrian bridges, information signs, single-arm poles,\nbus stops, warning signs, and concrete guardrails. To improve both detection\nand segmentation accuracy, a multi-task learning strategy is adopted,\noptimizing REG across multiple tasks. REG is further enhanced by incorporating\na spatial-contextual adjustment term, which accounts for the spatial\ndistribution of road assets, and a probabilistic refinement that captures\nprediction uncertainty in complex environments, such as varying lighting\nconditions and cluttered backgrounds. Our rigorous mathematical formulation\ndemonstrates that REG minimizes localization and classification errors by\napplying adaptive weighting to hard-to-detect instances while down-weighting\neasier examples. Experimental results show a substantial performance\nimprovement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly\noutperforming conventional methods. This research underscores the capability of\nadvanced loss function refinements to enhance the robustness and accuracy of\nroad asset detection and segmentation, thereby contributing to improved road\nsafety and infrastructure management. For an in-depth discussion of the\nmathematical background and related methods, please refer to previous work\navailable at \\url{https://github.com/kaopanboonyuen/REG}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.09877v2",
    "published_date": "2024-09-15 22:04:33 UTC",
    "updated_date": "2024-09-17 01:30:22 UTC"
  },
  {
    "arxiv_id": "2409.13758v1",
    "title": "Optimizing the Songwriting Process: Genre-Based Lyric Generation Using Deep Learning Models",
    "authors": [
      "Tracy Cai",
      "Wilson Liang",
      "Donte Townes"
    ],
    "abstract": "The traditional songwriting process is rather complex and this is evident in\nthe time it takes to produce lyrics that fit the genre and form comprehensive\nverses. Our project aims to simplify this process with deep learning\ntechniques, thus optimizing the songwriting process and enabling an artist to\nhit their target audience by staying in genre. Using a dataset of 18,000 songs\noff Spotify, we developed a unique preprocessing format using tokens to parse\nlyrics into individual verses. These results were used to train a baseline\npretrained seq2seq model, and a LSTM-based neural network models according to\nsong genres. We found that generation yielded higher recall (ROUGE) in the\nbaseline model, but similar precision (BLEU) for both models. Qualitatively, we\nfound that many of the lyrical phrases generated by the original model were\nstill comprehensible and discernible between which genres they fit into,\ndespite not necessarily being the exact the same as the true lyrics. Overall,\nour results yielded that lyric generation can reasonably be sped up to produce\ngenre-based lyrics and aid in hastening the songwriting process.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13758v1",
    "published_date": "2024-09-15 21:32:46 UTC",
    "updated_date": "2024-09-15 21:32:46 UTC"
  },
  {
    "arxiv_id": "2409.09869v1",
    "title": "Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent",
    "authors": [
      "Pavel Osinenko",
      "Grigory Yaremenko",
      "Roman Zashchitin",
      "Anton Bolychev",
      "Sinan Ibrahim",
      "Dmitrii Dobriborsci"
    ],
    "abstract": "This work presents and showcases a novel reinforcement learning agent called\nCritic As Lyapunov Function (CALF) which is model-free and ensures online\nenvironment, in other words, dynamical system stabilization. Online means that\nin each learning episode, the said environment is stabilized. This, as\ndemonstrated in a case study with a mobile robot simulator, greatly improves\nthe overall learning performance. The base actor-critic scheme of CALF is\nanalogous to SARSA. The latter did not show any success in reaching the target\nin our studies. However, a modified version thereof, called SARSA-m here, did\nsucceed in some learning scenarios. Still, CALF greatly outperformed the said\napproach. CALF was also demonstrated to improve a nominal stabilizer provided\nto it. In summary, the presented agent may be considered a viable approach to\nfusing classical control with reinforcement learning. Its concurrent approaches\nare mostly either offline or model-based, like, for instance, those that fuse\nmodel-predictive control into the agent.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.RO",
    "comment": "IEEE Conference on Decision and Control. Accepted for publication in\n  proceedings of the conference",
    "pdf_url": "http://arxiv.org/pdf/2409.09869v1",
    "published_date": "2024-09-15 21:27:44 UTC",
    "updated_date": "2024-09-15 21:27:44 UTC"
  },
  {
    "arxiv_id": "2409.09867v2",
    "title": "Towards Kinetic Manipulation of the Latent Space",
    "authors": [
      "Diego Porres"
    ],
    "abstract": "The latent space of many generative models are rich in unexplored valleys and\nmountains. The majority of tools used for exploring them are so far limited to\nGraphical User Interfaces (GUIs). While specialized hardware can be used for\nthis task, we show that a simple feature extraction of pre-trained\nConvolutional Neural Networks (CNNs) from a live RGB camera feed does a very\ngood job at manipulating the latent space with simple changes in the scene,\nwith vast room for improvement. We name this new paradigm Visual-reactive\nInterpolation, and the full code can be found at\nhttps://github.com/PDillis/stylegan3-fun.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024 Creative AI Track & LatinX in AI Affinity Workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.09867v2",
    "published_date": "2024-09-15 21:24:51 UTC",
    "updated_date": "2024-11-09 00:44:08 UTC"
  },
  {
    "arxiv_id": "2409.09866v2",
    "title": "S2Cap: A Benchmark and a Baseline for Singing Style Captioning",
    "authors": [
      "Hyunjong Ok",
      "Jaeho Lee"
    ],
    "abstract": "Singing voices contain much richer information than common voices, such as\ndiverse vocal and acoustic characteristics. However, existing open-source\naudio-text datasets for singing voices capture only a limited set of attributes\nand lacks acoustic features, leading to limited utility towards downstream\ntasks, such as style captioning. To fill this gap, we formally consider the\ntask of singing style captioning and introduce S2Cap, a singing voice dataset\nwith comprehensive descriptions of diverse vocal, acoustic and demographic\nattributes. Based on this dataset, we develop a simple yet effective baseline\nalgorithm for the singing style captioning. The algorithm utilizes two novel\ntechnical components: CRESCENDO for mitigating misalignment between pretrained\nunimodal models, and demixing supervision to regularize the model to focus on\nthe singing voice. Despite its simplicity, the proposed method outperforms\nstate-of-the-art baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2409.09866v2",
    "published_date": "2024-09-15 21:19:24 UTC",
    "updated_date": "2025-02-15 15:33:20 UTC"
  },
  {
    "arxiv_id": "2409.09858v3",
    "title": "A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View",
    "authors": [
      "Jing Ma"
    ],
    "abstract": "Graph machine learning (GML) has been successfully applied across a wide\nrange of tasks. Nonetheless, GML faces significant challenges in generalizing\nover out-of-distribution (OOD) data, which raises concerns about its wider\napplicability. Recent advancements have underscored the crucial role of\ncausality-driven approaches in overcoming these generalization challenges.\nDistinct from traditional GML methods that primarily rely on statistical\ndependencies, causality-focused strategies delve into the underlying causal\nmechanisms of data generation and model prediction, thus significantly\nimproving the generalization of GML across different environments. This paper\noffers a thorough review of recent progress in causality-involved GML\ngeneralization. We elucidate the fundamental concepts of employing causality to\nenhance graph model generalization and categorize the various approaches,\nproviding detailed descriptions of their methodologies and the connections\namong them. Furthermore, we explore the incorporation of causality in other\nrelated important areas of trustworthy GML, such as explanation, fairness, and\nrobustness. Concluding with a discussion on potential future research\ndirections, this review seeks to articulate the continuing development and\nfuture potential of causality in enhancing the trustworthiness of graph machine\nlearning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2409.09858v3",
    "published_date": "2024-09-15 20:41:18 UTC",
    "updated_date": "2024-10-16 08:23:11 UTC"
  },
  {
    "arxiv_id": "2409.09828v2",
    "title": "Latent Diffusion Models for Controllable RNA Sequence Generation",
    "authors": [
      "Kaixuan Huang",
      "Yukang Yang",
      "Kaidi Fu",
      "Yanyi Chu",
      "Le Cong",
      "Mengdi Wang"
    ],
    "abstract": "This work presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences of variable lengths. RNA is a key\nintermediary between DNA and protein, exhibiting high sequence diversity and\ncomplex three-dimensional structures to support a wide range of functions. We\nutilize pretrained BERT-type models to encode raw RNA sequences into\ntoken-level, biologically meaningful representations. A Query Transformer is\nemployed to compress such representations into a set of fixed-length latent\nvectors, with an autoregressive decoder trained to reconstruct RNA sequences\nfrom these latent variables. We then develop a continuous diffusion model\nwithin this latent space. To enable optimization, we integrate the gradients of\nreward models--surrogates for RNA functional properties--into the backward\ndiffusion process, thereby generating RNAs with high reward scores. Empirical\nresults confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological metrics. Further, we fine-tune\nthe diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize\nsequences for high translation efficiencies. Our guided diffusion model\neffectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and\nTranslation Efficiency (TE), outperforming baselines in balancing rewards and\nstructural stability trade-off. Our findings hold potential for advancing RNA\nsequence-function research and therapeutic RNA design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09828v2",
    "published_date": "2024-09-15 19:04:50 UTC",
    "updated_date": "2024-10-02 16:42:46 UTC"
  },
  {
    "arxiv_id": "2409.09827v1",
    "title": "On the Effect of Robot Errors on Human Teaching Dynamics",
    "authors": [
      "Jindan Huang",
      "Isaac Sheidlower",
      "Reuben M. Aronson",
      "Elaine Schaertl Short"
    ],
    "abstract": "Human-in-the-loop learning is gaining popularity, particularly in the field\nof robotics, because it leverages human knowledge about real-world tasks to\nfacilitate agent learning. When people instruct robots, they naturally adapt\ntheir teaching behavior in response to changes in robot performance. While\ncurrent research predominantly focuses on integrating human teaching dynamics\nfrom an algorithmic perspective, understanding these dynamics from a\nhuman-centered standpoint is an under-explored, yet fundamental problem.\nAddressing this issue will enhance both robot learning and user experience.\nTherefore, this paper explores one potential factor contributing to the dynamic\nnature of human teaching: robot errors. We conducted a user study to\ninvestigate how the presence and severity of robot errors affect three\ndimensions of human teaching dynamics: feedback granularity, feedback richness,\nand teaching time, in both forced-choice and open-ended teaching contexts. The\nresults show that people tend to spend more time teaching robots with errors,\nprovide more detailed feedback over specific segments of a robot's trajectory,\nand that robot error can influence a teacher's choice of feedback modality. Our\nfindings offer valuable insights for designing effective interfaces for\ninteractive learning and optimizing algorithms to better understand human\nintentions.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to 2024 International Conference on Human-Agent Interaction\n  (HAI)",
    "pdf_url": "http://arxiv.org/pdf/2409.09827v1",
    "published_date": "2024-09-15 19:02:34 UTC",
    "updated_date": "2024-09-15 19:02:34 UTC"
  },
  {
    "arxiv_id": "2409.09825v2",
    "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping",
    "authors": [
      "Yanjun Lyu",
      "Zihao Wu",
      "Lu Zhang",
      "Jing Zhang",
      "Yiwei Li",
      "Wei Ruan",
      "Zhengliang Liu",
      "Xiaowei Yu",
      "Chao Cao",
      "Tong Chen",
      "Minheng Chen",
      "Yan Zhuang",
      "Xiang Li",
      "Rongjie Liu",
      "Chao Huang",
      "Wentao Li",
      "Tianming Liu",
      "Dajiang Zhu"
    ],
    "abstract": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09825v2",
    "published_date": "2024-09-15 18:56:20 UTC",
    "updated_date": "2024-09-27 20:26:15 UTC"
  },
  {
    "arxiv_id": "2409.10580v1",
    "title": "Veridical Data Science for Medical Foundation Models",
    "authors": [
      "Ahmed Alaa",
      "Bin Yu"
    ],
    "abstract": "The advent of foundation models (FMs) such as large language models (LLMs)\nhas led to a cultural shift in data science, both in medicine and beyond. This\nshift involves moving away from specialized predictive models trained for\nspecific, well-defined domain questions to generalist FMs pre-trained on vast\namounts of unstructured data, which can then be adapted to various clinical\ntasks and questions. As a result, the standard data science workflow in\nmedicine has been fundamentally altered; the foundation model lifecycle (FMLC)\nnow includes distinct upstream and downstream processes, in which computational\nresources, model and data access, and decision-making power are distributed\namong multiple stakeholders. At their core, FMs are fundamentally statistical\nmodels, and this new workflow challenges the principles of Veridical Data\nScience (VDS), hindering the rigorous statistical analysis expected in\ntransparent and scientifically reproducible data science practices. We\ncritically examine the medical FMLC in light of the core principles of VDS:\npredictability, computability, and stability (PCS), and explain how it deviates\nfrom the standard data science workflow. Finally, we propose recommendations\nfor a reimagined medical FMLC that expands and refines the PCS principles for\nVDS including considering the computational and accessibility constraints\ninherent to FMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10580v1",
    "published_date": "2024-09-15 18:44:38 UTC",
    "updated_date": "2024-09-15 18:44:38 UTC"
  },
  {
    "arxiv_id": "2409.09822v3",
    "title": "Causal Inference with Large Language Model: A Survey",
    "authors": [
      "Jing Ma"
    ],
    "abstract": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 2 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09822v3",
    "published_date": "2024-09-15 18:43:11 UTC",
    "updated_date": "2025-02-09 06:59:47 UTC"
  },
  {
    "arxiv_id": "2409.10579v1",
    "title": "Recent advances in deep learning and language models for studying the microbiome",
    "authors": [
      "Binghao Yan",
      "Yunbi Nam",
      "Lingyao Li",
      "Rebecca A. Deek",
      "Hongzhe Li",
      "Siyuan Ma"
    ],
    "abstract": "Recent advancements in deep learning, particularly large language models\n(LLMs), made a significant impact on how researchers study microbiome and\nmetagenomics data. Microbial protein and genomic sequences, like natural\nlanguages, form a language of life, enabling the adoption of LLMs to extract\nuseful insights from complex microbial ecologies. In this paper, we review\napplications of deep learning and language models in analyzing microbiome and\nmetagenomics data. We focus on problem formulations, necessary datasets, and\nthe integration of language modeling techniques. We provide an extensive\noverview of protein/genomic language modeling and their contributions to\nmicrobiome studies. We also discuss applications such as novel viromics\nlanguage modeling, biosynthetic gene cluster prediction, and knowledge\nintegration for metagenomics studies.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10579v1",
    "published_date": "2024-09-15 18:32:31 UTC",
    "updated_date": "2024-09-15 18:32:31 UTC"
  },
  {
    "arxiv_id": "2409.10578v1",
    "title": "GLEAN: Generative Learning for Eliminating Adversarial Noise",
    "authors": [
      "Justin Lyu Kim",
      "Kyoungwan Woo"
    ],
    "abstract": "In the age of powerful diffusion models such as DALL-E and Stable Diffusion,\nmany in the digital art community have suffered style mimicry attacks due to\nfine-tuning these models on their works. The ability to mimic an artist's style\nvia text-to-image diffusion models raises serious ethical issues, especially\nwithout explicit consent. Glaze, a tool that applies various ranges of\nperturbations to digital art, has shown significant success in preventing style\nmimicry attacks, at the cost of artifacts ranging from imperceptible noise to\nsevere quality degradation. The release of Glaze has sparked further\ndiscussions regarding the effectiveness of similar protection methods. In this\npaper, we propose GLEAN- applying I2I generative networks to strip\nperturbations from Glazed images, evaluating the performance of style mimicry\nattacks before and after GLEAN on the results of Glaze. GLEAN aims to support\nand enhance Glaze by highlighting its limitations and encouraging further\ndevelopment.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10578v1",
    "published_date": "2024-09-15 18:28:56 UTC",
    "updated_date": "2024-09-15 18:28:56 UTC"
  },
  {
    "arxiv_id": "2409.11436v1",
    "title": "Analysis of flexible traffic control method in SDN",
    "authors": [
      "Marta Szymczyk"
    ],
    "abstract": "The aim of this paper is to analyze methods of flexible control in SDN\nnetworks and to propose a self-developed solution that will enable intelligent\nadaptation of SDN controller performance. This work aims not only to review\nexisting solutions, but also to develop an approach that will increase the\nefficiency and adaptability of network management. The project uses a modern\ntype of machine learning, Reinforcement Learning, which allows autonomous\ndecisions of a network that learns based on its choices in a dynamically\nchanging environment, which is most similar to the way humans learn. The\nsolution aims not only to improve the network's performance, but also its\nflexibility and real-time adaptability - flexible traffic control.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.11436v1",
    "published_date": "2024-09-15 18:09:59 UTC",
    "updated_date": "2024-09-15 18:09:59 UTC"
  },
  {
    "arxiv_id": "2409.09808v3",
    "title": "Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion",
    "authors": [
      "Hui Shen",
      "Zhongwei Wan",
      "Xin Wang",
      "Mi Zhang"
    ],
    "abstract": "Mamba and Vision Mamba (Vim) models have shown their potential as an\nalternative to methods based on Transformer architecture. This work introduces\nFast Mamba for Vision (Famba-V), a cross-layer token fusion technique to\nenhance the training efficiency of Vim models. The key idea of Famba-V is to\nidentify and fuse similar tokens across different Vim layers based on a suit of\ncross-layer strategies instead of simply applying token fusion uniformly across\nall the layers that existing works propose. We evaluate the performance of\nFamba-V on CIFAR-100. Our results show that Famba-V is able to enhance the\ntraining efficiency of Vim models by reducing both training time and peak\nmemory usage during training. Moreover, the proposed cross-layer strategies\nallow Famba-V to deliver superior accuracy-efficiency trade-offs. These results\nall together demonstrate Famba-V as a promising efficiency enhancement\ntechnique for Vim models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Camera ready version of ECCV 2024 Workshop on Computational Aspects\n  of Deep Learning (Best Paper Award)",
    "pdf_url": "http://arxiv.org/pdf/2409.09808v3",
    "published_date": "2024-09-15 18:02:26 UTC",
    "updated_date": "2024-10-06 16:34:48 UTC"
  },
  {
    "arxiv_id": "2409.09804v1",
    "title": "Abnormal Event Detection In Videos Using Deep Embedding",
    "authors": [
      "Darshan Venkatrayappa"
    ],
    "abstract": "Abnormal event detection or anomaly detection in surveillance videos is\ncurrently a challenge because of the diversity of possible events. Due to the\nlack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without supervision. In this work we propose an\nunsupervised approach for video anomaly detection with the aim to jointly\noptimize the objectives of the deep neural network and the anomaly detection\ntask using a hybrid architecture. Initially, a convolutional autoencoder is\npre-trained in an unsupervised manner with a fusion of depth, motion and\nappearance features. In the second step, we utilize the encoder part of the\npre-trained autoencoder and extract the embeddings of the fused input. Now, we\njointly train/ fine tune the encoder to map the embeddings to a hypercenter.\nThus, embeddings of normal data fall near the hypercenter, whereas embeddings\nof anomalous data fall far away from the hypercenter.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09804v1",
    "published_date": "2024-09-15 17:44:51 UTC",
    "updated_date": "2024-09-15 17:44:51 UTC"
  },
  {
    "arxiv_id": "2409.09790v1",
    "title": "Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization",
    "authors": [
      "Shiqi Li",
      "Jihua Zhu",
      "Yifan Xie",
      "Naiwen Hu",
      "Mingchen Zhu",
      "Zhongyu Li",
      "Di Wang"
    ],
    "abstract": "Multiple rotation averaging plays a crucial role in computer vision and\nrobotics domains. The conventional optimization-based methods optimize a\nnonlinear cost function based on certain noise assumptions, while most previous\nlearning-based methods require ground truth labels in the supervised training\nprocess. Recognizing the handcrafted noise assumption may not be reasonable in\nall real-world scenarios, this paper proposes an effective rotation averaging\nmethod for mining data patterns in a learning manner while avoiding the\nrequirement of labels. Specifically, we apply deep matrix factorization to\ndirectly solve the multiple rotation averaging problem in unconstrained linear\nspace. For deep matrix factorization, we design a neural network model, which\nis explicitly low-rank and symmetric to better suit the background of multiple\nrotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering\nto suppress the influence of rotation outliers. What's more, we also adopt a\nreweighting scheme and dynamic depth selection strategy to further improve the\nrobustness. Our method synthesizes the merit of both optimization-based and\nlearning-based methods. Experimental results on various datasets validate the\neffectiveness of our proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09790v1",
    "published_date": "2024-09-15 16:50:27 UTC",
    "updated_date": "2024-09-15 16:50:27 UTC"
  },
  {
    "arxiv_id": "2409.09787v4",
    "title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching",
    "authors": [
      "RuiKang OuYang",
      "Bo Qiang",
      "José Miguel Hernández-Lobato"
    ],
    "abstract": "Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "38 pages, 10 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09787v4",
    "published_date": "2024-09-15 16:41:30 UTC",
    "updated_date": "2025-05-12 16:54:02 UTC"
  },
  {
    "arxiv_id": "2409.09785v3",
    "title": "Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
    "authors": [
      "Chao-Han Huck Yang",
      "Taejin Park",
      "Yuan Gong",
      "Yuanchao Li",
      "Zhehuai Chen",
      "Yen-Ting Lin",
      "Chen Chen",
      "Yuchen Hu",
      "Kunal Dhawan",
      "Piotr Żelasko",
      "Chao Zhang",
      "Yun-Nung Chen",
      "Yu Tsao",
      "Jagadeesh Balam",
      "Boris Ginsburg",
      "Sabato Marco Siniscalchi",
      "Eng Siong Chng",
      "Peter Bell",
      "Catherine Lai",
      "Shinji Watanabe",
      "Andreas Stolcke"
    ],
    "abstract": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline",
    "pdf_url": "http://arxiv.org/pdf/2409.09785v3",
    "published_date": "2024-09-15 16:32:49 UTC",
    "updated_date": "2024-10-18 07:11:35 UTC"
  },
  {
    "arxiv_id": "2409.09784v1",
    "title": "Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and Advanced Data Preprocessing Techniques",
    "authors": [
      "Jiayi Liu",
      "Qiaoyi Xue",
      "Youdan Feng",
      "Tianming Xu",
      "Kaixin Shen",
      "Chuyun Shen",
      "Yuhang Shi"
    ],
    "abstract": "The escalating global cancer burden underscores the critical need for precise\ndiagnostic tools in oncology. This research employs deep learning to enhance\nlesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body\nFDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our\nmethodical approach includes robust preprocessing and data augmentation\ntechniques to ensure model robustness and generalizability. We investigate the\ninfluence of non-zero normalization and modifications to the data augmentation\npipeline, such as the introduction of RandGaussianSharpen and adjustments to\nthe Gamma transform parameter. This study aims to contribute to the\nstandardization of preprocessing and augmentation strategies in PET/CT imaging,\npotentially improving the diagnostic accuracy and the personalized management\nof cancer patients. Our code will be open-sourced and available at\nhttps://github.com/jiayiliu-pku/DC2024.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09784v1",
    "published_date": "2024-09-15 16:27:34 UTC",
    "updated_date": "2024-09-15 16:27:34 UTC"
  },
  {
    "arxiv_id": "2409.09766v1",
    "title": "Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting",
    "authors": [
      "Qiaoyi Xue",
      "Youdan Feng",
      "Jiayi Liu",
      "Tianming Xu",
      "Kaixin Shen",
      "Chuyun Shen",
      "Yuhang Shi"
    ],
    "abstract": "This study explores a workflow for automated segmentation of lesions in FDG\nand PSMA PET/CT images. Due to the substantial differences in image\ncharacteristics between FDG and PSMA, specialized preprocessing steps are\nrequired. Utilizing YOLOv8 for data classification, the FDG and PSMA images are\npreprocessed separately before feeding them into the segmentation models,\naiming to improve lesion segmentation accuracy. The study focuses on evaluating\nthe performance of automated segmentation workflow for multitracer PET images.\nThe findings are expected to provide critical insights for enhancing diagnostic\nworkflows and patient-specific treatment plans. Our code will be open-sourced\nand available at https://github.com/jiayiliu-pku/AP2024.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09766v1",
    "published_date": "2024-09-15 15:32:29 UTC",
    "updated_date": "2024-09-15 15:32:29 UTC"
  },
  {
    "arxiv_id": "2409.09760v3",
    "title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing",
    "authors": [
      "Suhyeon Yoo",
      "Khai N. Truong",
      "Young-Ho Kim"
    ],
    "abstract": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "H.5.2; I.2.8"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/elmi",
    "pdf_url": "http://arxiv.org/pdf/2409.09760v3",
    "published_date": "2024-09-15 15:01:00 UTC",
    "updated_date": "2025-02-21 18:40:52 UTC"
  },
  {
    "arxiv_id": "2409.09753v1",
    "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
    "authors": [
      "Shahriar Rifat",
      "Jonathan Ashdown",
      "Francesco Restuccia"
    ],
    "abstract": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09753v1",
    "published_date": "2024-09-15 14:49:30 UTC",
    "updated_date": "2024-09-15 14:49:30 UTC"
  },
  {
    "arxiv_id": "2409.09748v1",
    "title": "Explore the Hallucination on Low-level Perception for MLLMs",
    "authors": [
      "Yinan Sun",
      "Zicheng Zhang",
      "Haoning Wu",
      "Xiaohong Liu",
      "Weisi Lin",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has\nsignificantly influenced various aspects of industry and daily life, showcasing\nimpressive capabilities in visual perception and understanding. However, these\nmodels also exhibit hallucinations, which limit their reliability as AI\nsystems, especially in tasks involving low-level visual perception and\nunderstanding. We believe that hallucinations stem from a lack of explicit\nself-awareness in these models, which directly impacts their overall\nperformance. In this paper, we aim to define and evaluate the self-awareness of\nMLLMs in low-level visual perception and understanding tasks. To this end, we\npresent QL-Bench, a benchmark settings to simulate human responses to low-level\nvision, investigating self-awareness in low-level visual perception through\nvisual question answering related to low-level attributes such as clarity and\nlighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990\nsingle images and 1,999 image pairs, each accompanied by an open-ended question\nabout its low-level features. Through the evaluation of 15 MLLMs, we\ndemonstrate that while some models exhibit robust low-level visual\ncapabilities, their self-awareness remains relatively underdeveloped. Notably,\nfor the same model, simpler questions are often answered more accurately than\ncomplex ones. However, self-awareness appears to improve when addressing more\nchallenging questions. We hope that our benchmark will motivate further\nresearch, particularly focused on enhancing the self-awareness of MLLMs in\ntasks involving low-level visual perception and understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09748v1",
    "published_date": "2024-09-15 14:38:29 UTC",
    "updated_date": "2024-09-15 14:38:29 UTC"
  },
  {
    "arxiv_id": "2409.09741v1",
    "title": "Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data",
    "authors": [
      "Bastián González-Bustamante"
    ],
    "abstract": "This article benchmarked the ability of OpenAI's GPTs and a number of\nopen-source LLMs to perform annotation tasks on political content. We used a\nnovel protest event dataset comprising more than three million digital\ninteractions and created a gold standard that includes ground-truth labels\nannotated by human coders about toxicity and incivility on social media. We\nincluded in our benchmark Google's Perspective algorithm, which, along with\nGPTs, was employed throughout their respective APIs while the open-source LLMs\nwere deployed locally. The findings show that Perspective API using a laxer\nthreshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot\nclassification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,\nwith a smaller number of parameters, are able to perform the task with high\nperformance, being attractive options that could offer good trade-offs between\nperformance, implementing costs and computing time. Ancillary findings using\nexperiments setting different temperature levels show that although GPTs tend\nto show not only excellent computing time but also overall good levels of\nreliability, only open-source LLMs ensure full reproducibility in the\nannotation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50 (Primary) 91F10, 91F20 (Secondary)"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper prepared for delivery at the 8th Monash-Warwick-Zurich\n  Text-as-Data Workshop, September 16-17, 2024: 11 pages, 3 tables, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09741v1",
    "published_date": "2024-09-15 14:11:24 UTC",
    "updated_date": "2024-09-15 14:11:24 UTC"
  },
  {
    "arxiv_id": "2409.10575v1",
    "title": "A Tie-breaking based Local Search Algorithm for Stable Matching Problems",
    "authors": [
      "Junyuan Qiu"
    ],
    "abstract": "The stable marriage problem with incomplete lists and ties (SMTI) and the\nhospitals/residents problem with ties (HRT) are important in matching theory\nwith broad practical applications. In this paper, we introduce a tie-breaking\nbased local search algorithm (TBLS) designed to achieve a weakly stable\nmatching of maximum size for both the SMTI and HRT problems. TBLS begins by\narbitrarily resolving all ties and iteratively refines the tie-breaking\nstrategy by adjusting the relative order within ties based on preference ranks\nand the current stable matching. Additionally, we introduce TBLS-E, an\nequity-focused variant of TBLS, specifically designed for the SMTI problem.\nThis variant maintains the objective of maximizing matching size, while\nenhancing equity through two simple modifications. In comparison with ten other\napproximation and local search algorithms, TBLS achieves the highest matching\nsize, while TBLS-E exhibits the lowest sex equality cost. Significantly, TBLS-E\npreserves a matching size comparable to that of TBLS. Both our algorithms\ndemonstrate faster computational speed than other local search algorithms in\nsolving large-sized instances.",
    "categories": [
      "cs.DS",
      "cs.AI",
      "cs.DM",
      "math.OC"
    ],
    "primary_category": "cs.DS",
    "comment": "Submitted to Journal of Heuristics",
    "pdf_url": "http://arxiv.org/pdf/2409.10575v1",
    "published_date": "2024-09-15 13:36:55 UTC",
    "updated_date": "2024-09-15 13:36:55 UTC"
  },
  {
    "arxiv_id": "2409.10574v2",
    "title": "Detection Made Easy: Potentials of Large Language Models for Solidity Vulnerabilities",
    "authors": [
      "Md Tauseef Alam",
      "Raju Halder",
      "Abyayananda Maiti"
    ],
    "abstract": "The large-scale deployment of Solidity smart contracts on the Ethereum\nmainnet has increasingly attracted financially-motivated attackers in recent\nyears. A few now-infamous attacks in Ethereum's history includes DAO attack in\n2016 (50 million dollars lost), Parity Wallet hack in 2017 (146 million dollars\nlocked), Beautychain's token BEC in 2018 (900 million dollars market value fell\nto 0), and NFT gaming blockchain breach in 2022 ($600 million in Ether stolen).\nThis paper presents a comprehensive investigation of the use of large language\nmodels (LLMs) and their capabilities in detecting OWASP Top Ten vulnerabilities\nin Solidity. We introduce a novel, class-balanced, structured, and labeled\ndataset named VulSmart, which we use to benchmark and compare the performance\nof open-source LLMs such as CodeLlama, Llama2, CodeT5 and Falcon, alongside\nclosed-source models like GPT-3.5 Turbo and GPT-4o Mini. Our proposed SmartVD\nframework is rigorously tested against these models through extensive automated\nand manual evaluations, utilizing BLEU and ROUGE metrics to assess the\neffectiveness of vulnerability detection in smart contracts. We also explore\nthree distinct prompting strategies-zero-shot, few-shot, and\nchain-of-thought-to evaluate the multi-class classification and generative\ncapabilities of the SmartVD framework. Our findings reveal that SmartVD\noutperforms its open-source counterparts and even exceeds the performance of\nclosed-source base models like GPT-3.5 and GPT-4 Mini. After fine-tuning, the\nclosed-source models, GPT-3.5 Turbo and GPT-4o Mini, achieved remarkable\nperformance with 99% accuracy in detecting vulnerabilities, 94% in identifying\ntheir types, and 98% in determining severity. Notably, SmartVD performs best\nwith the `chain-of-thought' prompting technique, whereas the fine-tuned\nclosed-source models excel with the `zero-shot' prompting approach.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.10574v2",
    "published_date": "2024-09-15 13:16:58 UTC",
    "updated_date": "2024-10-26 09:38:20 UTC"
  },
  {
    "arxiv_id": "2409.09727v2",
    "title": "From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare",
    "authors": [
      "Ming Li",
      "Pengcheng Xu",
      "Junjie Hu",
      "Zeyu Tang",
      "Guang Yang"
    ],
    "abstract": "Federated learning holds great potential for enabling large-scale healthcare\nresearch and collaboration across multiple centres while ensuring data privacy\nand security are not compromised. Although numerous recent studies suggest or\nutilize federated learning based methods in healthcare, it remains unclear\nwhich ones have potential clinical utility. This review paper considers and\nanalyzes the most recent studies up to May 2024 that describe federated\nlearning based methods in healthcare. After a thorough review, we find that the\nvast majority are not appropriate for clinical use due to their methodological\nflaws and/or underlying biases which include but are not limited to privacy\nconcerns, generalization issues, and communication costs. As a result, the\neffectiveness of federated learning in healthcare is significantly compromised.\nTo overcome these challenges, we provide recommendations and promising\nopportunities that might be implemented to resolve these problems and improve\nthe quality of model development in federated learning with healthcare.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Medical Image Analysis",
    "pdf_url": "http://arxiv.org/pdf/2409.09727v2",
    "published_date": "2024-09-15 13:11:07 UTC",
    "updated_date": "2025-02-04 16:56:42 UTC"
  },
  {
    "arxiv_id": "2409.09717v1",
    "title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents",
    "authors": [
      "Justas Andriuškevičius",
      "Junzi Sun"
    ],
    "abstract": "Recent developments in language models have created new opportunities in air\ntraffic control studies. The current focus is primarily on text and\nlanguage-based use cases. However, these language models may offer a higher\npotential impact in the air traffic control domain, thanks to their ability to\ninteract with air traffic environments in an embodied agent form. They also\nprovide a language-like reasoning capability to explain their decisions, which\nhas been a significant roadblock for the implementation of automatic air\ntraffic control.\n  This paper investigates the application of a language model-based agent with\nfunction-calling and learning capabilities to resolve air traffic conflicts\nwithout human intervention. The main components of this research are\nfoundational large language models, tools that allow the agent to interact with\nthe simulator, and a new concept, the experience library. An innovative part of\nthis research, the experience library, is a vector database that stores\nsynthesized knowledge that agents have learned from interactions with the\nsimulations and language models.\n  To evaluate the performance of our language model-based agent, both\nopen-source and closed-source models were tested. The results of our study\nreveal significant differences in performance across various configurations of\nthe language model-based agents. The best-performing configuration was able to\nsolve almost all 120 but one imminent conflict scenarios, including up to four\naircraft at the same time. Most importantly, the agents are able to provide\nhuman-level text explanations on traffic situations and conflict resolution\nstrategies.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09717v1",
    "published_date": "2024-09-15 12:49:05 UTC",
    "updated_date": "2024-09-15 12:49:05 UTC"
  },
  {
    "arxiv_id": "2409.09706v2",
    "title": "Exploring Utility in a Real-World Warehouse Optimization Problem: Formulation Based on Quantum Annealers and Preliminary Results",
    "authors": [
      "Eneko Osaba",
      "Esther Villar-Rodriguez",
      "Antón Asla"
    ],
    "abstract": "In the current NISQ-era, one of the major challenges faced by researchers and\npractitioners lies in figuring out how to combine quantum and classical\ncomputing in the most efficient and innovative way. In this paper, we present a\nmechanism coined as Quantum Initialization for Warehouse Optimization Problem\nthat resorts to D-Wave's Quantum Annealer. The module has been specifically\ndesigned to be embedded into already existing classical software dedicated to\nthe optimization of a real-world industrial problem. We preliminary tested the\nimplemented mechanism through a two-phase experiment against the classical\nversion of the software.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "2 pages, 2 figures. Paper presented at the 5th IEEE International\n  Conference on Quantum Computing and Engineering (IEEE QCE 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.09706v2",
    "published_date": "2024-09-15 11:58:07 UTC",
    "updated_date": "2024-10-01 13:02:24 UTC"
  },
  {
    "arxiv_id": "2409.09702v1",
    "title": "GFlowNet Pretraining with Inexpensive Rewards",
    "authors": [
      "Mohit Pandey",
      "Gopeshh Subbaraj",
      "Emmanuel Bengio"
    ],
    "abstract": "Generative Flow Networks (GFlowNets), a class of generative models have\nrecently emerged as a suitable framework for generating diverse and\nhigh-quality molecular structures by learning from unnormalized reward\ndistributions. Previous works in this direction often restrict exploration by\nusing predefined molecular fragments as building blocks, limiting the chemical\nspace that can be accessed. In this work, we introduce Atomic GFlowNets\n(A-GFNs), a foundational generative model leveraging individual atoms as\nbuilding blocks to explore drug-like chemical space more comprehensively. We\npropose an unsupervised pre-training approach using offline drug-like molecule\ndatasets, which conditions A-GFNs on inexpensive yet informative molecular\ndescriptors such as drug-likeliness, topological polar surface area, and\nsynthetic accessibility scores. These properties serve as proxy rewards,\nguiding A-GFNs towards regions of chemical space that exhibit desirable\npharmacological properties. We further our method by implementing a\ngoal-conditioned fine-tuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on the ZINC15\noffline dataset and employ robust evaluation metrics to show the effectiveness\nof our approach when compared to other relevant baseline methods in drug\ndesign.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09702v1",
    "published_date": "2024-09-15 11:42:17 UTC",
    "updated_date": "2024-09-15 11:42:17 UTC"
  },
  {
    "arxiv_id": "2409.13755v2",
    "title": "Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences",
    "authors": [
      "Xin Wang",
      "Xinyi Bai"
    ],
    "abstract": "Relation extraction as an important natural Language processing (NLP) task is\nto identify relations between named entities in text. Recently, graph\nconvolutional networks over dependency trees have been widely used to capture\nsyntactic features and achieved attractive performance. However, most existing\ndependency-based approaches ignore the positive influence of the words outside\nthe dependency trees, sometimes conveying rich and useful information on\nrelation extraction. In this paper, we propose a novel model, Entity-aware\nSelf-attention Contextualized GCN (ESC-GCN), which efficiently incorporates\nsyntactic structure of input sentences and semantic context of sequences. To be\nspecific, relative position self-attention obtains the overall semantic\npairwise correlation related to word position, and contextualized graph\nconvolutional networks capture rich intra-sentence dependencies between words\nby adequately pruning operations. Furthermore, entity-aware attention layer\ndynamically selects which token is more decisive to make final relation\nprediction. In this way, our proposed model not only reduces the noisy impact\nfrom dependency trees, but also obtains easily-ignored entity-related semantic\nrepresentation. Extensive experiments on various tasks demonstrate that our\nmodel achieves encouraging performance as compared to existing dependency-based\nand sequence-based models. Specially, our model excels in extracting relations\nbetween entities of long sentences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13755v2",
    "published_date": "2024-09-15 10:50:51 UTC",
    "updated_date": "2024-11-12 02:01:37 UTC"
  },
  {
    "arxiv_id": "2409.09687v1",
    "title": "Training Safe Neural Networks with Global SDP Bounds",
    "authors": [
      "Roman Soletskyi",
      "David \"davidad\" Dalrymple"
    ],
    "abstract": "This paper presents a novel approach to training neural networks with formal\nsafety guarantees using semidefinite programming (SDP) for verification. Our\nmethod focuses on verifying safety over large, high-dimensional input regions,\naddressing limitations of existing techniques that focus on adversarial\nrobustness bounds. We introduce an ADMM-based training scheme for an accurate\nneural network classifier on the Adversarial Spheres dataset, achieving\nprovably perfect recall with input dimensions up to $d=40$. This work advances\nthe development of reliable neural network verification methods for\nhigh-dimensional systems, with potential applications in safe RL policies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09687v1",
    "published_date": "2024-09-15 10:50:22 UTC",
    "updated_date": "2024-09-15 10:50:22 UTC"
  },
  {
    "arxiv_id": "2409.09684v1",
    "title": "Anatomy of Machines for Markowitz: Decision-Focused Learning for Mean-Variance Portfolio Optimization",
    "authors": [
      "Junhyeong Lee",
      "Inwoo Tae",
      "Yongjae Lee"
    ],
    "abstract": "Markowitz laid the foundation of portfolio theory through the mean-variance\noptimization (MVO) framework. However, the effectiveness of MVO is contingent\non the precise estimation of expected returns, variances, and covariances of\nasset returns, which are typically uncertain. Machine learning models are\nbecoming useful in estimating uncertain parameters, and such models are trained\nto minimize prediction errors, such as mean squared errors (MSE), which treat\nprediction errors uniformly across assets. Recent studies have pointed out that\nthis approach would lead to suboptimal decisions and proposed Decision-Focused\nLearning (DFL) as a solution, integrating prediction and optimization to\nimprove decision-making outcomes. While studies have shown DFL's potential to\nenhance portfolio performance, the detailed mechanisms of how DFL modifies\nprediction models for MVO remain unexplored. This study aims to investigate how\nDFL adjusts stock return prediction models to optimize decisions in MVO,\naddressing the question: \"MSE treats the errors of all assets equally, but how\ndoes DFL reduce errors of different assets differently?\" Answering this will\nprovide crucial insights into optimal stock return prediction for constructing\nefficient portfolios.",
    "categories": [
      "q-fin.PM",
      "cs.AI"
    ],
    "primary_category": "q-fin.PM",
    "comment": "7 pages, 3 figures, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.09684v1",
    "published_date": "2024-09-15 10:37:11 UTC",
    "updated_date": "2024-09-15 10:37:11 UTC"
  },
  {
    "arxiv_id": "2409.09680v1",
    "title": "Reliable Multi-View Learning with Conformal Prediction for Aortic Stenosis Classification in Echocardiography",
    "authors": [
      "Ang Nan Gu",
      "Michael Tsang",
      "Hooman Vaseli",
      "Teresa Tsang",
      "Purang Abolmaesumi"
    ],
    "abstract": "The fundamental problem with ultrasound-guided diagnosis is that the acquired\nimages are often 2-D cross-sections of a 3-D anatomy, potentially missing\nimportant anatomical details. This limitation leads to challenges in ultrasound\nechocardiography, such as poor visualization of heart valves or foreshortening\nof ventricles. Clinicians must interpret these images with inherent\nuncertainty, a nuance absent in machine learning's one-hot labels. We propose\nRe-Training for Uncertainty (RT4U), a data-centric method to introduce\nuncertainty to weakly informative inputs in the training set. This simple\napproach can be incorporated to existing state-of-the-art aortic stenosis\nclassification methods to further improve their accuracy. When combined with\nconformal prediction techniques, RT4U can yield adaptively sized prediction\nsets which are guaranteed to contain the ground truth class to a high accuracy.\nWe validate the effectiveness of RT4U on three diverse datasets: a public\n(TMED-2) and a private AS dataset, along with a CIFAR-10-derived toy dataset.\nResults show improvement on all the datasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in:\n  International Conference on Medical Image Computing and Computer-Assisted\n  Intervention (MICCAI), Springer (2024) under the same title",
    "pdf_url": "http://arxiv.org/pdf/2409.09680v1",
    "published_date": "2024-09-15 10:06:06 UTC",
    "updated_date": "2024-09-15 10:06:06 UTC"
  },
  {
    "arxiv_id": "2409.11432v1",
    "title": "A hybrid solution for 2-UAV RAN slicing",
    "authors": [
      "Nathan Boyer"
    ],
    "abstract": "It's possible to distribute the Internet to users via drones. However it is\nthen necessary to place the drones according to the positions of the users.\nMoreover, the 5th Generation (5G) New Radio (NR) technology is designed to\naccommodate a wide range of applications and industries. The NGNM 5G White\nPaper \\cite{5gwhitepaper} groups these vertical use cases into three\ncategories:\n  - enhanced Mobile Broadband (eMBB)\n  - massive Machine Type Communication (mMTC)\n  - Ultra-Reliable Low-latency Communication (URLLC).\n  Partitioning the physical network into multiple virtual networks appears to\nbe the best way to provide a customised service for each application and limit\noperational costs. This design is well known as \\textit{network slicing}. Each\ndrone must thus slice its bandwidth between each of the 3 user classes. This\nwhole problem (placement + bandwidth) can be defined as an optimization\nproblem, but since it is very hard to solve efficiently, it is almost always\naddressed by AI in the litterature. In my internship, I wanted to prove that\nviewing the problem as an optimization problem can still be useful, by building\nan hybrid solution involving on one hand AI and on the other optimization. I\nuse it to achieve better results than approaches that use only AI, although at\nthe cost of slightly larger (but still reasonable) computation times.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "9 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.11432v1",
    "published_date": "2024-09-15 09:42:31 UTC",
    "updated_date": "2024-09-15 09:42:31 UTC"
  },
  {
    "arxiv_id": "2409.09662v3",
    "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models",
    "authors": [
      "Inhwa Song",
      "SoHyun Park",
      "Sachin R. Pendse",
      "Jessica Lee Schleider",
      "Munmun De Choudhury",
      "Young-Ho Kim"
    ],
    "abstract": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/exploreself",
    "pdf_url": "http://arxiv.org/pdf/2409.09662v3",
    "published_date": "2024-09-15 08:25:24 UTC",
    "updated_date": "2025-02-05 17:41:42 UTC"
  },
  {
    "arxiv_id": "2409.09653v1",
    "title": "KAN v.s. MLP for Offline Reinforcement Learning",
    "authors": [
      "Haihong Guo",
      "Fengxin Li",
      "Jiao Li",
      "Hongyan Liu"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KAN) is an emerging neural network architecture\nin machine learning. It has greatly interested the research community about\nwhether KAN can be a promising alternative of the commonly used Multi-Layer\nPerceptions (MLP). Experiments in various fields demonstrated that KAN-based\nmachine learning can achieve comparable if not better performance than\nMLP-based methods, but with much smaller parameter scales and are more\nexplainable. In this paper, we explore the incorporation of KAN into the actor\nand critic networks for offline reinforcement learning (RL). We evaluated the\nperformance, parameter scales, and training efficiency of various KAN and MLP\nbased conservative Q-learning (CQL) on the the classical D4RL benchmark for\noffline RL. Our study demonstrates that KAN can achieve performance close to\nthe commonly used MLP with significantly fewer parameters. This provides us an\noption to choose the base networks according to the requirements of the offline\nRL tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages,2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09653v1",
    "published_date": "2024-09-15 07:52:44 UTC",
    "updated_date": "2024-09-15 07:52:44 UTC"
  },
  {
    "arxiv_id": "2409.09647v2",
    "title": "Self-supervised Learning for Acoustic Few-Shot Classification",
    "authors": [
      "Jingyong Liang",
      "Bernd Meyer",
      "Isaac Ning Lee",
      "Thanh-Toan Do"
    ],
    "abstract": "Labelled data are limited and self-supervised learning is one of the most\nimportant approaches for reducing labelling requirements. While it has been\nextensively explored in the image domain, it has so far not received the same\namount of attention in the acoustic domain. Yet, reducing labelling is a key\nrequirement for many acoustic applications. Specifically in bioacoustic, there\nare rarely sufficient labels for fully supervised learning available. This has\nled to the widespread use of acoustic recognisers that have been pre-trained on\nunrelated data for bioacoustic tasks. We posit that training on the actual task\ndata and combining self-supervised pre-training with few-shot classification is\na superior approach that has the ability to deliver high accuracy even when\nonly a few labels are available. To this end, we introduce and evaluate a new\narchitecture that combines CNN-based preprocessing with feature extraction\nbased on state space models (SSMs). This combination is motivated by the fact\nthat CNN-based networks alone struggle to capture temporal information\neffectively, which is crucial for classifying acoustic signals. SSMs,\nspecifically S4 and Mamba, on the other hand, have been shown to have an\nexcellent ability to capture long-range dependencies in sequence data. We\npre-train this architecture using contrastive learning on the actual task data\nand subsequent fine-tuning with an extremely small amount of labelled data. We\nevaluate the performance of this proposed architecture for ($n$-shot,\n$n$-class) classification on standard benchmarks as well as real-world data.\nOur evaluation shows that it outperforms state-of-the-art architectures on the\nfew-shot classification problem.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09647v2",
    "published_date": "2024-09-15 07:45:11 UTC",
    "updated_date": "2025-05-15 11:26:08 UTC"
  },
  {
    "arxiv_id": "2409.09645v1",
    "title": "COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate Time Series Classification",
    "authors": [
      "Jesus Barreda",
      "Ashley Gomez",
      "Ruben Puga",
      "Kaixiong Zhou",
      "Li Zhang"
    ],
    "abstract": "Multivariate time series classification is an important task with widespread\ndomains of applications. Recently, deep neural networks (DNN) have achieved\nstate-of-the-art performance in time series classification. However, they often\nrequire large expert-labeled training datasets which can be infeasible in\npractice. In few-shot settings, i.e. only a limited number of samples per class\nare available in training data, DNNs show a significant drop in testing\naccuracy and poor generalization ability. In this paper, we propose to address\nthese problems from an optimization and a loss function perspective.\nSpecifically, we propose a new learning framework named COSCO consisting of a\nsharpness-aware minimization (SAM) optimization and a Prototypical loss\nfunction to improve the generalization ability of DNN for multivariate time\nseries classification problems under few-shot setting. Our experiments\ndemonstrate our proposed method outperforms the existing baseline methods. Our\nsource code is available at: https://github.com/JRB9/COSCO.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages, 5 figures, CIKM '24 Short Paper Track",
    "pdf_url": "http://arxiv.org/pdf/2409.09645v1",
    "published_date": "2024-09-15 07:41:55 UTC",
    "updated_date": "2024-09-15 07:41:55 UTC"
  },
  {
    "arxiv_id": "2409.09641v4",
    "title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic Children and Parents with Contextual Guidance and Card Recommendation",
    "authors": [
      "Dasom Choi",
      "SoHyun Park",
      "Kyungah Lee",
      "Hwajung Hong",
      "Young-Ho Kim"
    ],
    "abstract": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "21 pages excluding reference. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/aacesstalk/",
    "pdf_url": "http://arxiv.org/pdf/2409.09641v4",
    "published_date": "2024-09-15 07:23:07 UTC",
    "updated_date": "2025-02-14 07:14:14 UTC"
  },
  {
    "arxiv_id": "2409.09635v1",
    "title": "A Novel Framework For Text Detection From Natural Scene Images With Complex Background",
    "authors": [
      "Basavaraj Kaladagi",
      "Jagadeesh Pujari"
    ],
    "abstract": "Recognizing texts from camera images is a known hard problem because of the\ndifficulties in text detection from the varied and complicated background. In\nthis paper we propose a novel and efficient method to detect text region from\nimages with complex background using Wavelet Transforms. The framework uses\nWavelet Transformation of the original image in its grayscale form followed by\nSub-band filtering. Then Region clustering technique is applied using centroids\nof the regions, further Bounding box is fitted to each region thus identifying\nthe text regions. This method is much sophisticated and efficient than the\nprevious methods as it doesn't stick to a particular font size of the text\nthus, making it generalized. The sample set used for experimental purpose\nconsists of 50 images with varying backgrounds. Images with edge prominence are\nconsidered. Furthermore, our method can be easily customized for applications\nwith different scopes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09635v1",
    "published_date": "2024-09-15 07:12:33 UTC",
    "updated_date": "2024-09-15 07:12:33 UTC"
  },
  {
    "arxiv_id": "2409.09629v2",
    "title": "Confidence Estimation for LLM-Based Dialogue State Tracking",
    "authors": [
      "Yi-Jyun Sun",
      "Suvodip Dey",
      "Dilek Hakkani-Tur",
      "Gokhan Tur"
    ],
    "abstract": "Estimation of a model's confidence on its outputs is critical for\nConversational AI systems based on large language models (LLMs), especially for\nreducing hallucination and preventing over-reliance. In this work, we provide\nan exhaustive exploration of methods, including approaches proposed for open-\nand closed-weight LLMs, aimed at quantifying and leveraging model uncertainty\nto improve the reliability of LLM-generated responses, specifically focusing on\ndialogue state tracking (DST) in task-oriented dialogue systems (TODS).\nRegardless of the model type, well-calibrated confidence scores are essential\nto handle uncertainties, thereby improving model performance. We evaluate four\nmethods for estimating confidence scores based on softmax, raw token scores,\nverbalized confidences, and a combination of these methods, using the area\nunder the curve (AUC) metric to assess calibration, with higher AUC indicating\nbetter calibration. We also enhance these with a self-probing mechanism,\nproposed for closed models. Furthermore, we assess these methods using an\nopen-weight model fine-tuned for the task of DST, achieving superior joint goal\naccuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can\nresult in enhanced AUC performance, indicating better confidence score\ncalibration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at IEEE SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.09629v2",
    "published_date": "2024-09-15 06:44:26 UTC",
    "updated_date": "2024-09-21 13:11:11 UTC"
  },
  {
    "arxiv_id": "2409.09628v1",
    "title": "Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition",
    "authors": [
      "Zongyou Yu",
      "Qiang Qu",
      "Xiaoming Chen",
      "Chen Wang"
    ],
    "abstract": "Recent advancements in event-based zero-shot object recognition have\ndemonstrated promising results. However, these methods heavily depend on\nextensive training and are inherently constrained by the characteristics of\nCLIP. To the best of our knowledge, this research is the first study to explore\nthe understanding capabilities of large language models (LLMs) for event-based\nvisual content. We demonstrate that LLMs can achieve event-based object\nrecognition without additional training or fine-tuning in conjunction with\nCLIP, effectively enabling pure zero-shot event-based recognition.\nParticularly, we evaluate the ability of GPT-4o / 4turbo and two other\nopen-source LLMs to directly recognize event-based visual content. Extensive\nexperiments are conducted across three benchmark datasets, systematically\nassessing the recognition accuracy of these models. The results show that LLMs,\nespecially when enhanced with well-designed prompts, significantly improve\nevent-based zero-shot recognition performance. Notably, GPT-4o outperforms the\ncompared models and exceeds the recognition accuracy of state-of-the-art\nevent-based zero-shot methods on N-ImageNet by five orders of magnitude. The\nimplementation of this paper is available at\n\\url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09628v1",
    "published_date": "2024-09-15 06:43:03 UTC",
    "updated_date": "2024-09-15 06:43:03 UTC"
  },
  {
    "arxiv_id": "2410.03538v2",
    "title": "Dreaming User Multimodal Representation Guided by The Platonic Representation Hypothesis for Micro-Video Recommendation",
    "authors": [
      "Chengzhi Lin",
      "Hezheng Lin",
      "Shuchang Liu",
      "Cangguang Ruan",
      "LingJing Xu",
      "Dezhao Yang",
      "Chuyuan Wang",
      "Yongqi Liu"
    ],
    "abstract": "The proliferation of online micro-video platforms has underscored the\nnecessity for advanced recommender systems to mitigate information overload and\ndeliver tailored content. Despite advancements, accurately and promptly\ncapturing dynamic user interests remains a formidable challenge. Inspired by\nthe Platonic Representation Hypothesis, which posits that different data\nmodalities converge towards a shared statistical model of reality, we introduce\nDreamUMM (Dreaming User Multi-Modal Representation), a novel approach\nleveraging user historical behaviors to create real-time user representation in\na multimoda space. DreamUMM employs a closed-form solution correlating user\nvideo preferences with multimodal similarity, hypothesizing that user interests\ncan be effectively represented in a unified multimodal space. Additionally, we\npropose Candidate-DreamUMM for scenarios lacking recent user behavior data,\ninferring interests from candidate videos alone. Extensive online A/B tests\ndemonstrate significant improvements in user engagement metrics, including\nactive days and play count. The successful deployment of DreamUMM in two\nmicro-video platforms with hundreds of millions of daily active users,\nillustrates its practical efficacy and scalability in personalized micro-video\ncontent delivery. Our work contributes to the ongoing exploration of\nrepresentational convergence by providing empirical evidence supporting the\npotential for user interest representations to reside in a multimodal space.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.IR",
    "comment": "4 Figure; 2 Table",
    "pdf_url": "http://arxiv.org/pdf/2410.03538v2",
    "published_date": "2024-09-15 06:40:38 UTC",
    "updated_date": "2024-10-19 13:51:20 UTC"
  },
  {
    "arxiv_id": "2409.09626v1",
    "title": "Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics",
    "authors": [
      "Yi Ren",
      "Danica J. Sutherland"
    ],
    "abstract": "Obtaining compositional mappings is important for the model to generalize\nwell compositionally. To better understand when and how to encourage the model\nto learn such mappings, we study their uniqueness through different\nperspectives. Specifically, we first show that the compositional mappings are\nthe simplest bijections through the lens of coding length (i.e., an upper bound\nof their Kolmogorov complexity). This property explains why models having such\nmappings can generalize well. We further show that the simplicity bias is\nusually an intrinsic property of neural network training via gradient descent.\nThat partially explains why some models spontaneously generalize well when they\nare trained appropriately.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.09626v1",
    "published_date": "2024-09-15 06:37:12 UTC",
    "updated_date": "2024-09-15 06:37:12 UTC"
  },
  {
    "arxiv_id": "2409.09621v1",
    "title": "Stutter-Solver: End-to-end Multi-lingual Dysfluency Detection",
    "authors": [
      "Xuanru Zhou",
      "Cheol Jun Cho",
      "Ayati Sharma",
      "Brittany Morin",
      "David Baquirin",
      "Jet Vonk",
      "Zoe Ezzes",
      "Zachary Miller",
      "Boon Lead Tee",
      "Maria Luisa Gorno Tempini",
      "Jiachen Lian",
      "Gopala Anumanchipalli"
    ],
    "abstract": "Current de-facto dysfluency modeling methods utilize template matching\nalgorithms which are not generalizable to out-of-domain real-world dysfluencies\nacross languages, and are not scalable with increasing amounts of training\ndata. To handle these problems, we propose Stutter-Solver: an end-to-end\nframework that detects dysfluency with accurate type and time transcription,\ninspired by the YOLO object detection algorithm. Stutter-Solver can handle\nco-dysfluencies and is a natural multi-lingual dysfluency detector. To leverage\nscalability and boost performance, we also introduce three novel dysfluency\ncorpora: VCTK-Pro, VCTK-Art, and AISHELL3-Pro, simulating natural spoken\ndysfluencies including repetition, block, missing, replacement, and\nprolongation through articulatory-encodec and TTS-based methods. Our approach\nachieves state-of-the-art performance on all available dysfluency corpora. Code\nand datasets are open-sourced at https://github.com/eureka235/Stutter-Solver",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "IEEE Spoken Language Technology Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.09621v1",
    "published_date": "2024-09-15 06:11:00 UTC",
    "updated_date": "2024-09-15 06:11:00 UTC"
  },
  {
    "arxiv_id": "2409.09615v1",
    "title": "Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting",
    "authors": [
      "Jianfei Wu",
      "Xubin Wang",
      "Weijia Jia"
    ],
    "abstract": "The traditional data annotation process is often labor-intensive,\ntime-consuming, and susceptible to human bias, which complicates the management\nof increasingly complex datasets. This study explores the potential of large\nlanguage models (LLMs) as automated data annotators to improve efficiency and\nconsistency in annotation tasks. By employing rationale-driven collaborative\nfew-shot prompting techniques, we aim to improve the performance of LLMs in\ntext annotation. We conduct a rigorous evaluation of six LLMs across four\nbenchmark datasets, comparing seven distinct methodologies. Our results\ndemonstrate that collaborative methods consistently outperform traditional\nfew-shot techniques and other baseline approaches, particularly in complex\nannotation tasks. Our work provides valuable insights and a robust framework\nfor leveraging collaborative learning methods to tackle challenging text\nannotation tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09615v1",
    "published_date": "2024-09-15 05:32:21 UTC",
    "updated_date": "2024-09-15 05:32:21 UTC"
  },
  {
    "arxiv_id": "2409.09613v1",
    "title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
    "authors": [
      "Yungi Kim",
      "Hyunsoo Ha",
      "Sukyung Lee",
      "Jihoo Kim",
      "Seonghoon Yang",
      "Chanjun Park"
    ],
    "abstract": "With the increasing demand for substantial amounts of high-quality data to\ntrain large language models (LLMs), efficiently filtering large web corpora has\nbecome a critical challenge. For this purpose, KenLM, a lightweight\nn-gram-based language model that operates on CPUs, is widely used. However, the\ntraditional method of training KenLM utilizes only high-quality data and,\nconsequently, does not explicitly learn the linguistic patterns of low-quality\ndata. To address this issue, we propose an ensemble approach that leverages two\ncontrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad\nKenLM, trained on low-quality data. Experimental results demonstrate that our\napproach significantly reduces noisy content while preserving high-quality\ncontent compared to the traditional KenLM training method. This indicates that\nour method can be a practical solution with minimal computational overhead for\nresource-constrained environments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09613v1",
    "published_date": "2024-09-15 05:27:56 UTC",
    "updated_date": "2024-09-15 05:27:56 UTC"
  },
  {
    "arxiv_id": "2409.09611v1",
    "title": "Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action Recognition",
    "authors": [
      "Cagri Gungor",
      "Adriana Kovashka"
    ],
    "abstract": "First-person activity recognition is rapidly growing due to the widespread\nuse of wearable cameras but faces challenges from domain shifts across\ndifferent environments, such as varying objects or background scenes. We\npropose a multimodal framework that improves domain generalization by\nintegrating motion, audio, and appearance features. Key contributions include\nanalyzing the resilience of audio and motion features to domain shifts, using\naudio narrations for enhanced audio-text alignment, and applying consistency\nratings between audio and visual narrations to optimize the impact of audio in\nrecognition during training. Our approach achieves state-of-the-art performance\non the ARGO1M dataset, effectively generalizing across unseen scenarios and\nlocations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09611v1",
    "published_date": "2024-09-15 04:43:00 UTC",
    "updated_date": "2024-09-15 04:43:00 UTC"
  },
  {
    "arxiv_id": "2409.09603v1",
    "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison",
    "authors": [
      "Judy Hanwen Shen",
      "Archit Sharma",
      "Jun Qin"
    ],
    "abstract": "The goal of aligning language models to human preferences requires data that\nreveal these preferences. Ideally, time and money can be spent carefully\ncollecting and tailoring bespoke preference data to each downstream\napplication. However, in practice, a select few publicly available preference\ndatasets are often used to train reward models for reinforcement learning from\nhuman feedback (RLHF). While new preference datasets are being introduced with\nincreasing frequency, there are currently no existing efforts to measure and\ncompare these datasets. In this paper, we systematically study preference\ndatasets through three perspectives: scale, label noise, and information\ncontent. We propose specific metrics for each of these perspectives and uncover\ndifferent axes of comparison for a better understanding of preference datasets.\nOur work is a first step towards a data-centric approach to alignment by\nproviding perspectives that aid in training efficiency and iterative data\ncollection for RLHF.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Working Paper",
    "pdf_url": "http://arxiv.org/pdf/2409.09603v1",
    "published_date": "2024-09-15 03:55:03 UTC",
    "updated_date": "2024-09-15 03:55:03 UTC"
  },
  {
    "arxiv_id": "2409.09601v1",
    "title": "A Survey of Foundation Models for Music Understanding",
    "authors": [
      "Wenjun Li",
      "Ying Cai",
      "Ziyang Wu",
      "Wenyi Zhang",
      "Yifan Chen",
      "Rundong Qi",
      "Mengqi Dong",
      "Peigen Chen",
      "Xiao Dong",
      "Fenghao Shi",
      "Lei Guo",
      "Junwei Han",
      "Bao Ge",
      "Tianming Liu",
      "Lin Gan",
      "Tuo Zhang"
    ],
    "abstract": "Music is essential in daily life, fulfilling emotional and entertainment\nneeds, and connecting us personally, socially, and culturally. A better\nunderstanding of music can enhance our emotions, cognitive skills, and cultural\nconnections. The rapid advancement of artificial intelligence (AI) has\nintroduced new ways to analyze music, aiming to replicate human understanding\nof music and provide related services. While the traditional models focused on\naudio features and simple tasks, the recent development of large language\nmodels (LLMs) and foundation models (FMs), which excel in various fields by\nintegrating semantic information and demonstrating strong reasoning abilities,\ncould capture complex musical features and patterns, integrate music with\nlanguage and incorporate rich musical, emotional and psychological knowledge.\nTherefore, they have the potential in handling complex music understanding\ntasks from a semantic perspective, producing outputs closer to human\nperception. This work, to our best knowledge, is one of the early reviews of\nthe intersection of AI techniques and music understanding. We investigated,\nanalyzed, and tested recent large-scale music foundation models in respect of\ntheir music comprehension abilities. We also discussed their limitations and\nproposed possible future directions, offering insights for researchers in this\nfield.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "20 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.09601v1",
    "published_date": "2024-09-15 03:34:14 UTC",
    "updated_date": "2024-09-15 03:34:14 UTC"
  },
  {
    "arxiv_id": "2409.09598v2",
    "title": "Improving Statistical Significance in Human Evaluation of Automatic Metrics via Soft Pairwise Accuracy",
    "authors": [
      "Brian Thompson",
      "Nitika Mathur",
      "Daniel Deutsch",
      "Huda Khayrallah"
    ],
    "abstract": "Selecting an automatic metric that best emulates human annotators is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\nscores, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric scores. We show that SPA is more stable than PA with\nrespect to changes in the number of systems/segments used for evaluation. We\nalso show that PA can only assign a small set of distinct output values to\nmetrics, and this results in many metrics being artificially assigned the exact\nsame PA score. We demonstrate that SPA fixes this issue. Finally, we show that\nSPA is more discriminative than PA, producing more statistically significant\ncomparisons between metrics. SPA was selected as the official system-level\nmetric for the 2024 WMT Metrics Shared Task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at WMT 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.09598v2",
    "published_date": "2024-09-15 03:25:55 UTC",
    "updated_date": "2024-10-04 16:57:08 UTC"
  },
  {
    "arxiv_id": "2410.01817v1",
    "title": "From Experts to the Public: Governing Multimodal Language Models in Politically Sensitive Video Analysis",
    "authors": [
      "Tanusree Sharma",
      "Yujin Potter",
      "Zachary Kilhoffer",
      "Yun Huang",
      "Dawn Song",
      "Yang Wang"
    ],
    "abstract": "This paper examines the governance of multimodal large language models\n(MM-LLMs) through individual and collective deliberation, focusing on analyses\nof politically sensitive videos. We conducted a two-step study: first,\ninterviews with 10 journalists established a baseline understanding of expert\nvideo interpretation; second, 114 individuals from the general public engaged\nin deliberation using Inclusive.AI, a platform that facilitates democratic\ndecision-making through decentralized autonomous organization (DAO) mechanisms.\nOur findings show that while experts emphasized emotion and narrative, the\ngeneral public prioritized factual clarity, objectivity of the situation, and\nemotional neutrality. Additionally, we explored the impact of different\ngovernance mechanisms: quadratic vs. weighted ranking voting and equal vs.\n20-80 power distributions on users decision-making on how AI should behave.\nSpecifically, quadratic voting enhanced perceptions of liberal democracy and\npolitical equality, and participants who were more optimistic about AI\nperceived the voting process to have a higher level of participatory democracy.\nOur results suggest the potential of applying DAO mechanisms to help\ndemocratize AI governance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01817v1",
    "published_date": "2024-09-15 03:17:38 UTC",
    "updated_date": "2024-09-15 03:17:38 UTC"
  },
  {
    "arxiv_id": "2409.09591v1",
    "title": "Open-World Test-Time Training: Self-Training with Contrast Learning",
    "authors": [
      "Houcheng Su",
      "Mengzhu Wang",
      "Jiao Li",
      "Bingli Wang",
      "Daixian Liu",
      "Zeheng Wang"
    ],
    "abstract": "Traditional test-time training (TTT) methods, while addressing domain shifts,\noften assume a consistent class set, limiting their applicability in real-world\nscenarios characterized by infinite variety. Open-World Test-Time Training\n(OWTTT) addresses the challenge of generalizing deep learning models to unknown\ntarget domain distributions, especially in the presence of strong\nOut-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain\nperformance when confronted with strong OOD data. In OWTTT, the focus has\npredominantly been on distinguishing between overall strong and weak OOD data.\nHowever, during the early stages of TTT, initial feature extraction is hampered\nby interference from strong OOD and corruptions, resulting in diminished\ncontrast and premature classification of certain classes as strong OOD. To\naddress this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an\ninnovative approach that utilizes contrastive learning to augment positive\nsample pairs. This strategy not only bolsters contrast in the early stages but\nalso significantly enhances model robustness in subsequent stages. In\ncomparison datasets, our OWDCL model has produced the most advanced\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10page",
    "pdf_url": "http://arxiv.org/pdf/2409.09591v1",
    "published_date": "2024-09-15 02:36:26 UTC",
    "updated_date": "2024-09-15 02:36:26 UTC"
  },
  {
    "arxiv_id": "2409.09586v2",
    "title": "ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs",
    "authors": [
      "Hua Shen",
      "Tiffany Knearem",
      "Reshmi Ghosh",
      "Yu-Ju Yang",
      "Nicholas Clark",
      "Tanushree Mitra",
      "Yun Huang"
    ],
    "abstract": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09586v2",
    "published_date": "2024-09-15 02:13:03 UTC",
    "updated_date": "2025-04-16 21:50:45 UTC"
  },
  {
    "arxiv_id": "2409.09570v1",
    "title": "MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences",
    "authors": [
      "Subigya Nepal",
      "Arvind Pillai",
      "William Campbell",
      "Talie Massachi",
      "Michael V. Heinz",
      "Ashmita Kunwar",
      "Eunsol Soul Choi",
      "Orson Xu",
      "Joanna Kuc",
      "Jeremy Huckins",
      "Jason Holden",
      "Sarah M. Preum",
      "Colin Depp",
      "Nicholas Jacobson",
      "Mary Czerwinski",
      "Eric Granholm",
      "Andrew T. Campbell"
    ],
    "abstract": "Mental health concerns are prevalent among college students, highlighting the\nneed for effective interventions that promote self-awareness and holistic\nwell-being. MindScape pioneers a novel approach to AI-powered journaling by\nintegrating passively collected behavioral patterns such as conversational\nengagement, sleep, and location with Large Language Models (LLMs). This\nintegration creates a highly personalized and context-aware journaling\nexperience, enhancing self-awareness and well-being by embedding behavioral\nintelligence into AI. We present an 8-week exploratory study with 20 college\nstudents, demonstrating the MindScape app's efficacy in enhancing positive\naffect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and\ndepression, with a significant week-over-week decrease in PHQ-4 scores (-0.25\ncoefficient), alongside improvements in mindfulness (7%) and self-reflection\n(6%). The study highlights the advantages of contextual AI journaling, with\nparticipants particularly appreciating the tailored prompts and insights\nprovided by the MindScape app. Our analysis also includes a comparison of\nresponses to AI-driven contextual versus generic prompts, participant feedback\ninsights, and proposed strategies for leveraging contextual AI journaling to\nimprove well-being on college campuses. By showcasing the potential of\ncontextual AI journaling to support mental health, we provide a foundation for\nfurther investigation into the effects of contextual AI journaling on mental\nhealth and well-being.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0; H.5.3; H.5.m; J.0"
    ],
    "primary_category": "cs.HC",
    "comment": "arXiv admin note: text overlap with arXiv:2404.00487",
    "pdf_url": "http://arxiv.org/pdf/2409.09570v1",
    "published_date": "2024-09-15 01:10:46 UTC",
    "updated_date": "2024-09-15 01:10:46 UTC"
  },
  {
    "arxiv_id": "2409.09566v3",
    "title": "Learning Transferable Features for Implicit Neural Representations",
    "authors": [
      "Kushal Vyas",
      "Ahmed Imtiaz Humayun",
      "Aniket Dashpute",
      "Richard G. Baraniuk",
      "Ashok Veeraraghavan",
      "Guha Balakrishnan"
    ],
    "abstract": "Implicit neural representations (INRs) have demonstrated success in a variety\nof applications, including inverse problems and neural rendering. An INR is\ntypically trained to capture one signal of interest, resulting in learned\nneural features that are highly attuned to that signal. Assumed to be less\ngeneralizable, we explore the aspect of transferability of such learned neural\nfeatures for fitting similar signals. We introduce a new INR training\nframework, STRAINER that learns transferrable features for fitting INRs to new\nsignals from a given distribution, faster and with better reconstruction\nquality. Owing to the sequential layer-wise affine operations in an INR, we\npropose to learn transferable representations by sharing initial encoder layers\nacross multiple INRs with independent decoder layers. At test time, the learned\nencoder representations are transferred as initialization for an otherwise\nrandomly initialized INR. We find STRAINER to yield extremely powerful\ninitialization for fitting images from the same domain and allow for $\\approx\n+10dB$ gain in signal quality early on compared to an untrained INR itself.\nSTRAINER also provides a simple way to encode data-driven priors in INRs. We\nevaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks\nand inverse problems and further provide detailed analysis and discussion on\nthe transferability of STRAINER's features. Our demo can be accessed at\nhttps://kushalvyas.github.io/strainer.html .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Website: https://kushalvyas.github.io/strainer.html",
    "pdf_url": "http://arxiv.org/pdf/2409.09566v3",
    "published_date": "2024-09-15 00:53:44 UTC",
    "updated_date": "2025-01-09 20:24:46 UTC"
  },
  {
    "arxiv_id": "2409.09564v2",
    "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
    "authors": [
      "Dawei Yan",
      "Pengcheng Li",
      "Yang Li",
      "Hao Chen",
      "Qingguo Chen",
      "Weihua Luo",
      "Wei Dong",
      "Qingsen Yan",
      "Haokui Zhang",
      "Chunhua Shen"
    ],
    "abstract": "Currently, inspired by the success of vision-language models (VLMs), an\nincreasing number of researchers are focusing on improving VLMs and have\nachieved promising results. However, most existing methods concentrate on\noptimizing the connector and enhancing the language model component, while\nneglecting improvements to the vision encoder itself. In contrast, we propose\nText Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the\nvision encoder with text, offering a new and orthogonal optimization direction.\nSpecifically, inspired by the purpose-driven logic inherent in human behavior,\nwe use learnable latent embeddings as a bridge to analyze textual instruction\nand add the analysis results to the vision encoder as guidance, refining it.\nSubsequently, another set of latent embeddings extracts additional detailed\ntext-guided information from high-resolution local patches as auxiliary\ninformation. Finally, with the guidance of text, the vision encoder can extract\ntext-related features, similar to how humans focus on the most relevant parts\nof an image when considering a question. This results in generating better\nanswers. Experiments on various datasets validate the effectiveness of the\nproposed method. Remarkably, without the need for additional training data, our\npropsoed method can bring more benefits to the baseline (LLaVA-1.5) compared\nwith other concurrent methods. Furthermore, the proposed method consistently\nbrings improvement in different settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.09564v2",
    "published_date": "2024-09-15 00:38:34 UTC",
    "updated_date": "2024-09-20 14:51:18 UTC"
  }
]
{
  "date": "2024-09-15",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-09-15 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、LLM 在医疗和对话中的应用、图像处理以及强化学习等领域，其中 LLM 在基因-表型映射和因果推理的创新性工作（如 GP-GPT 和 Causal Inference with LLM）令人印象深刻，还有知名学者如 Bin Yu 的 Veridical Data Science 论文值得关注。这些论文突显了 AI 在实际问题中的潜力，但也暴露了泛化性和鲁棒性的挑战。\n\n下面，我将挑选并简要讨论部分关键论文，先优先聊那些创新性强、可能有话题度的作品（如 LLM 相关和医疗应用），并将相关主题归类快速概述。其他较常规或初步研究的论文（如某些图像处理或小众优化方法），我将简略掠过，只列出标题和核心点。\n\n### LLM 和 AI 应用（重点领域，创新性高）\n- **GP-GPT: Large Language Model for Gene-Phenotype Mapping（中文：GP-GPT：用于基因-表型映射的大型语言模型）**  \n  这篇论文由 Yanjun Lyu 等作者提出，利用 LLM 进行基因-表型知识表示和关系分析。贡献在于首次构建针对基因数据的 LLM，显著提升了基因信息检索和关系判断的准确性，超越了 Llama2 和 GPT-4，在医疗遗传学领域有潜力应用。\n\n- **Causal Inference with Large Language Model: A Survey（中文：基于大型语言模型的因果推理：调查）**  \n  作者 Jing Ma 审视了 LLM 在因果推理中的进展，涵盖不同因果水平的任务。发现在于总结了 LLM 如何改善传统因果方法，并讨论了未来方向，如医疗和经济学应用。这是一篇有影响力的综述，强调 LLM 的泛化潜力。\n\n- **Large Language Model Based Generative Error Correction: A Challenge and Baselines（中文：基于大型语言模型的生成性错误修正：挑战与基线）**  \n  多作者团队（如 Shinji Watanabe）提出 GenSEC 框架，用于语音识别中的错误修正。贡献包括基准数据集和多任务评估，提升了 ASR 的鲁棒性。该论文可能引发语音 AI 领域的讨论。\n\n- **Benchmarking LLMs in Political Content Text-Annotation（中文：LLM 在政治内容文本标注中的基准测试）**  \n  作者 Bastián González-Bustamante 评估了 LLM（如 GPT-4o）在政治文本标注中的性能。发现在于某些开源模型在毒性和不文明检测上表现突出，提供高效的标注工具，适用于社会媒体分析。\n\n其他 LLM 相关论文（如 Dreaming User Multimodal Representation）快速掠过：它们探索 LLM 在个性化推荐中的应用，但贡献较初步，仅提升了微视频推荐的准确性。\n\n### 医疗和生物领域（实际影响大，知名学者参与）\n- **Veridical Data Science for Medical Foundation Models（中文：医疗基础模型的真实数据科学）**  \n  知名学者 Bin Yu 领导的论文，分析了 LLM 在医疗中的生命周期。贡献在于扩展 PCS 原则（可预测性、可计算性和稳定性），提出改进建议以提升模型可靠性。该工作可能推动医疗 AI 的标准化。\n\n- **Latent Diffusion Models for Controllable RNA Sequence Generation（中文：用于可控 RNA 序列生成的潜在扩散模型）**  \n  作者 Kaixuan Huang 等提出 RNAdiffusion 模型，用于 RNA 优化。发现在于通过奖励模型引导生成高性能 RNA 序列，提升了生物学指标，如翻译效率，适用于 RNA 治疗设计。\n\n- **Enhancing Lesion Segmentation in PET/CT Imaging（中文：增强 PET/CT 图像中的病变分割）**  \n  作者 Jiayi Liu 等开发了深度学习框架，优化数据预处理。贡献在于提高分割准确性，支持癌症诊断。该论文与 Automated Lesion Segmentation 相关，提供开源代码。\n\n其他医疗论文（如 A Survey of Out-of-distribution Generalization）快速提要：它从因果视角审视图机器学习，改善了模型泛化，但细节较泛。\n\n### 计算机视觉和强化学习（技术创新突出）\n- **Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent（中文：Critic 作为 Lyapunov 函数：一种无模型的稳定代理）**  \n  作者 Pavel Osinenko 等提出 CALF 算法，用于强化学习中的系统稳定。贡献在于在线稳定环境，提升了机器人学习性能，融合了经典控制和 RL，适用于 NeurIPS 等会议。\n\n- **Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion（中文：快速视觉 Mamba：带跨层标记融合）**  \n  作者 Hui Shen 等开发了 Famba-V 模型，提升了视觉 Mamba 的训练效率。发现在于减少计算资源，同时提高准确性（如 CIFAR-100 上表现优异），获得 ECCV 最佳论文奖。\n\n其他视觉论文（如 Abnormal Event Detection）快速掠过：它们使用深度嵌入进行异常检测，但方法较常规，无重大突破。\n\n### 其他领域（快速概述，非核心）\n- **Optimizing the Songwriting Process（中文：优化歌曲创作过程：基于深度学习的流派歌词生成）**  \n  作者 Tracy Cai 等使用 LSTM 生成流派歌词。贡献在于简化创作，但效果中等。\n\n- **A Survey of Foundation Models for Music Understanding（中文：音乐理解的基础模型调查）**  \n  这篇综述探索 LLM 在音乐中的应用。发现在于整合语义和音频，但仍需改进泛化。\n\n剩余论文（如交通优化、仓库问题）不那么重要，仅涉及特定应用优化，我这里就不再详细展开。\n\n总之，今天的 arXiv 论文展示了 AI 的多样性，LLM 在医疗和因果推理中的进展特别值得关注。读者可根据兴趣优先查看上述重点论文，以节省时间。明天见！",
  "papers": [
    {
      "arxiv_id": "2410.01818v1",
      "title": "Integrating AI's Carbon Footprint into Risk Management Frameworks: Strategies and Tools for Sustainable Compliance in Banking Sector",
      "title_zh": "翻译失败",
      "authors": [
        "Nataliya Tkachenko"
      ],
      "abstract": "This paper examines the integration of AI's carbon footprint into the risk\nmanagement frameworks (RMFs) of the banking sector, emphasising its importance\nin aligning with sustainability goals and regulatory requirements. As AI\nbecomes increasingly central to banking operations, its energy-intensive\nprocesses contribute significantly to carbon emissions, posing environmental,\nregulatory, and reputational risks. Regulatory frameworks such as the EU AI\nAct, Corporate Sustainability Reporting Directive (CSRD), Corporate\nSustainability Due Diligence Directive (CSDDD), and the Prudential Regulation\nAuthority's SS1/23 are driving banks to incorporate environmental\nconsiderations into their AI model governance. Recent advancements in AI\nresearch, like the Open Mixture-of-Experts (OLMoE) framework and the Agentic\nRAG framework, offer more efficient and dynamic AI models, reducing their\ncarbon footprint without compromising performance. Using these technological\nexamples, the paper outlines a structured approach for banks to identify,\nassess, and mitigate AI's carbon footprint within their RMFs, including\nadopting energy-efficient models, utilising green cloud computing, and\nimplementing lifecycle management.",
      "tldr_zh": "本论文探讨了将AI的碳足print整合到银行部门的风险管理框架(RMFs)中的策略，以实现可持续合规和降低环境风险。论文强调AI在银行运营中的高能耗导致的碳排放可能带来监管（如EU AI Act、CSRD和CSDDD）和声誉挑战，并介绍了先进技术如Open Mixture-of-Experts (OLMoE)框架和Agentic RAG框架，以提高AI效率并减少碳足print。最终，论文提出一个结构化方法，包括采用能量高效模型、利用绿色云计算和实施生命周期管理，帮助银行识别、评估和缓解这些风险。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01818v1",
      "published_date": "2024-09-15 23:09:27 UTC",
      "updated_date": "2024-09-15 23:09:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:05:06.339825"
    },
    {
      "arxiv_id": "2409.09877v2",
      "title": "REG: Refined Generalized Focal Loss for Road Asset Detection on Thai Highways Using Vision-Based Detection and Segmentation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Teerapong Panboonyuen"
      ],
      "abstract": "This paper introduces a novel framework for detecting and segmenting critical\nroad assets on Thai highways using an advanced Refined Generalized Focal Loss\n(REG) formulation. Integrated into state-of-the-art vision-based detection and\nsegmentation models, the proposed method effectively addresses class imbalance\nand the challenges of localizing small, underrepresented road elements,\nincluding pavilions, pedestrian bridges, information signs, single-arm poles,\nbus stops, warning signs, and concrete guardrails. To improve both detection\nand segmentation accuracy, a multi-task learning strategy is adopted,\noptimizing REG across multiple tasks. REG is further enhanced by incorporating\na spatial-contextual adjustment term, which accounts for the spatial\ndistribution of road assets, and a probabilistic refinement that captures\nprediction uncertainty in complex environments, such as varying lighting\nconditions and cluttered backgrounds. Our rigorous mathematical formulation\ndemonstrates that REG minimizes localization and classification errors by\napplying adaptive weighting to hard-to-detect instances while down-weighting\neasier examples. Experimental results show a substantial performance\nimprovement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly\noutperforming conventional methods. This research underscores the capability of\nadvanced loss function refinements to enhance the robustness and accuracy of\nroad asset detection and segmentation, thereby contributing to improved road\nsafety and infrastructure management. For an in-depth discussion of the\nmathematical background and related methods, please refer to previous work\navailable at \\url{https://github.com/kaopanboonyuen/REG}.",
      "tldr_zh": "本研究提出了一种名为 REG（Refined Generalized Focal Loss）的先进损失函数框架，用于基于视觉检测和分割模型检测泰国高速公路上的关键路资产，如亭子、行人桥和警告标志。该框架通过多任务学习策略、空间-上下文调整项和概率精炼来解决类不平衡问题和定位小目标的挑战，从而提高在复杂环境（如变化照明和杂乱背景）下的准确性。实验结果显示，REG 显著提升了性能，达到 mAP50 80.34 和 F1-score 77.87，比传统方法改善明显，最终有助于增强道路安全和基础设施管理。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.09877v2",
      "published_date": "2024-09-15 22:04:33 UTC",
      "updated_date": "2024-09-17 01:30:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:05:16.675965"
    },
    {
      "arxiv_id": "2409.13758v1",
      "title": "Optimizing the Songwriting Process: Genre-Based Lyric Generation Using Deep Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tracy Cai",
        "Wilson Liang",
        "Donte Townes"
      ],
      "abstract": "The traditional songwriting process is rather complex and this is evident in\nthe time it takes to produce lyrics that fit the genre and form comprehensive\nverses. Our project aims to simplify this process with deep learning\ntechniques, thus optimizing the songwriting process and enabling an artist to\nhit their target audience by staying in genre. Using a dataset of 18,000 songs\noff Spotify, we developed a unique preprocessing format using tokens to parse\nlyrics into individual verses. These results were used to train a baseline\npretrained seq2seq model, and a LSTM-based neural network models according to\nsong genres. We found that generation yielded higher recall (ROUGE) in the\nbaseline model, but similar precision (BLEU) for both models. Qualitatively, we\nfound that many of the lyrical phrases generated by the original model were\nstill comprehensible and discernible between which genres they fit into,\ndespite not necessarily being the exact the same as the true lyrics. Overall,\nour results yielded that lyric generation can reasonably be sped up to produce\ngenre-based lyrics and aid in hastening the songwriting process.",
      "tldr_zh": "这篇论文旨在使用深度学习模型优化歌词创作过程，通过生成符合特定流派的歌词来简化传统复杂流程。研究者利用 Spotify 的 18,000 首歌曲数据集，进行 tokens 预处理以解析歌词为独立 verses，并训练基线 seq2seq 模型和 LSTM 神经网络模型。结果表明，seq2seq 模型在 ROUGE 指标上显示更高 recall，而两模型在 BLEU 指标上精度相似；生成的歌词虽不完全匹配真实内容，但整体可理解且能区分流派，从而加速歌词创作。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13758v1",
      "published_date": "2024-09-15 21:32:46 UTC",
      "updated_date": "2024-09-15 21:32:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:05:28.733251"
    },
    {
      "arxiv_id": "2409.09869v1",
      "title": "Critic as Lyapunov function (CALF): a model-free, stability-ensuring agent",
      "title_zh": "翻译失败",
      "authors": [
        "Pavel Osinenko",
        "Grigory Yaremenko",
        "Roman Zashchitin",
        "Anton Bolychev",
        "Sinan Ibrahim",
        "Dmitrii Dobriborsci"
      ],
      "abstract": "This work presents and showcases a novel reinforcement learning agent called\nCritic As Lyapunov Function (CALF) which is model-free and ensures online\nenvironment, in other words, dynamical system stabilization. Online means that\nin each learning episode, the said environment is stabilized. This, as\ndemonstrated in a case study with a mobile robot simulator, greatly improves\nthe overall learning performance. The base actor-critic scheme of CALF is\nanalogous to SARSA. The latter did not show any success in reaching the target\nin our studies. However, a modified version thereof, called SARSA-m here, did\nsucceed in some learning scenarios. Still, CALF greatly outperformed the said\napproach. CALF was also demonstrated to improve a nominal stabilizer provided\nto it. In summary, the presented agent may be considered a viable approach to\nfusing classical control with reinforcement learning. Its concurrent approaches\nare mostly either offline or model-based, like, for instance, those that fuse\nmodel-predictive control into the agent.",
      "tldr_zh": "本论文提出了一种模型无关（model-free）的强化学习代理 Critic As Lyapunov Function (CALF)，其通过将 Critic 视为 Lyapunov function，确保在线环境稳定，即在每个学习回合中实现动态系统稳定化。CALF 的架构类似于 actor-critic 方案，但通过移动机器人模拟器的案例研究，证明其比传统 SARSA 和其修改版 SARSA-m 显著提高了学习性能。实验结果显示，CALF 不仅在某些场景下成功稳定环境，还能改进提供的名义稳定器（nominal stabilizer），为融合经典控制与强化学习的创新方法提供了可行途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.RO",
      "comment": "IEEE Conference on Decision and Control. Accepted for publication in\n  proceedings of the conference",
      "pdf_url": "http://arxiv.org/pdf/2409.09869v1",
      "published_date": "2024-09-15 21:27:44 UTC",
      "updated_date": "2024-09-15 21:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:05:41.926925"
    },
    {
      "arxiv_id": "2409.09867v2",
      "title": "Towards Kinetic Manipulation of the Latent Space",
      "title_zh": "翻译失败",
      "authors": [
        "Diego Porres"
      ],
      "abstract": "The latent space of many generative models are rich in unexplored valleys and\nmountains. The majority of tools used for exploring them are so far limited to\nGraphical User Interfaces (GUIs). While specialized hardware can be used for\nthis task, we show that a simple feature extraction of pre-trained\nConvolutional Neural Networks (CNNs) from a live RGB camera feed does a very\ngood job at manipulating the latent space with simple changes in the scene,\nwith vast room for improvement. We name this new paradigm Visual-reactive\nInterpolation, and the full code can be found at\nhttps://github.com/PDillis/stylegan3-fun.",
      "tldr_zh": "该论文探讨了生成模型的潜在空间（latent space）的动态操纵问题，目前大多数工具仅限于 Graphical User Interfaces (GUIs)。研究者提出了一种新范式Visual-reactive Interpolation，通过从实时RGB camera feed提取预训练Convolutional Neural Networks (CNNs)特征，利用简单场景变化来有效探索和操纵潜在空间。实验结果显示，该方法性能出色，并具有显著的改进潜力，相关代码可在GitHub上获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS 2024 Creative AI Track & LatinX in AI Affinity Workshop",
      "pdf_url": "http://arxiv.org/pdf/2409.09867v2",
      "published_date": "2024-09-15 21:24:51 UTC",
      "updated_date": "2024-11-09 00:44:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:05:51.898628"
    },
    {
      "arxiv_id": "2409.09866v2",
      "title": "S2Cap: A Benchmark and a Baseline for Singing Style Captioning",
      "title_zh": "S2Cap：歌唱风格标注的基准和基线",
      "authors": [
        "Hyunjong Ok",
        "Jaeho Lee"
      ],
      "abstract": "Singing voices contain much richer information than common voices, such as\ndiverse vocal and acoustic characteristics. However, existing open-source\naudio-text datasets for singing voices capture only a limited set of attributes\nand lacks acoustic features, leading to limited utility towards downstream\ntasks, such as style captioning. To fill this gap, we formally consider the\ntask of singing style captioning and introduce S2Cap, a singing voice dataset\nwith comprehensive descriptions of diverse vocal, acoustic and demographic\nattributes. Based on this dataset, we develop a simple yet effective baseline\nalgorithm for the singing style captioning. The algorithm utilizes two novel\ntechnical components: CRESCENDO for mitigating misalignment between pretrained\nunimodal models, and demixing supervision to regularize the model to focus on\nthe singing voice. Despite its simplicity, the proposed method outperforms\nstate-of-the-art baselines.",
      "tldr_zh": "该论文针对唱歌声音的丰富声学和声带特性，指出现有音频文本数据集的局限性（如属性有限和缺少声学特征），并正式定义了 singing style captioning 任务。作者引入 S2Cap 数据集，提供多样化的声学、声带和人口统计属性的全面描述，作为一个基准。基于此，他们提出一个简单有效的基线算法，采用 CRESCENDO 技术缓解预训练单模态模型的不对齐，以及 demixing supervision 来使模型专注于唱歌声音，该方法在性能上超过了现有最先进基线。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2409.09866v2",
      "published_date": "2024-09-15 21:19:24 UTC",
      "updated_date": "2025-02-15 15:33:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:06:04.844014"
    },
    {
      "arxiv_id": "2409.09858v3",
      "title": "A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View",
      "title_zh": "从因果视角看图机器学习的分布外泛化综述",
      "authors": [
        "Jing Ma"
      ],
      "abstract": "Graph machine learning (GML) has been successfully applied across a wide\nrange of tasks. Nonetheless, GML faces significant challenges in generalizing\nover out-of-distribution (OOD) data, which raises concerns about its wider\napplicability. Recent advancements have underscored the crucial role of\ncausality-driven approaches in overcoming these generalization challenges.\nDistinct from traditional GML methods that primarily rely on statistical\ndependencies, causality-focused strategies delve into the underlying causal\nmechanisms of data generation and model prediction, thus significantly\nimproving the generalization of GML across different environments. This paper\noffers a thorough review of recent progress in causality-involved GML\ngeneralization. We elucidate the fundamental concepts of employing causality to\nenhance graph model generalization and categorize the various approaches,\nproviding detailed descriptions of their methodologies and the connections\namong them. Furthermore, we explore the incorporation of causality in other\nrelated important areas of trustworthy GML, such as explanation, fairness, and\nrobustness. Concluding with a discussion on potential future research\ndirections, this review seeks to articulate the continuing development and\nfuture potential of causality in enhancing the trustworthiness of graph machine\nlearning.",
      "tldr_zh": "这篇论文从因果视角（Causal View）对图机器学习（Graph Machine Learning, GML）的分布外泛化（Out-of-distribution Generalization, OOD）问题进行了全面调查，强调了因果方法在克服传统依赖统计依赖的方法局限性方面的关键作用。论文阐述了利用因果机制来揭示数据生成和模型预测的底层过程，从而显著提升GML在不同环境下的泛化能力，并对相关方法进行了分类和详细描述。作者还探讨了因果在GML的可信领域（如解释性、公平性和鲁棒性）中的应用，并为未来研究方向提供了展望，以推动GML的可靠发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2409.09858v3",
      "published_date": "2024-09-15 20:41:18 UTC",
      "updated_date": "2024-10-16 08:23:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:06:17.064913"
    },
    {
      "arxiv_id": "2409.09828v2",
      "title": "Latent Diffusion Models for Controllable RNA Sequence Generation",
      "title_zh": "用于",
      "authors": [
        "Kaixuan Huang",
        "Yukang Yang",
        "Kaidi Fu",
        "Yanyi Chu",
        "Le Cong",
        "Mengdi Wang"
      ],
      "abstract": "This work presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences of variable lengths. RNA is a key\nintermediary between DNA and protein, exhibiting high sequence diversity and\ncomplex three-dimensional structures to support a wide range of functions. We\nutilize pretrained BERT-type models to encode raw RNA sequences into\ntoken-level, biologically meaningful representations. A Query Transformer is\nemployed to compress such representations into a set of fixed-length latent\nvectors, with an autoregressive decoder trained to reconstruct RNA sequences\nfrom these latent variables. We then develop a continuous diffusion model\nwithin this latent space. To enable optimization, we integrate the gradients of\nreward models--surrogates for RNA functional properties--into the backward\ndiffusion process, thereby generating RNAs with high reward scores. Empirical\nresults confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological metrics. Further, we fine-tune\nthe diffusion model on mRNA 5' untranslated regions (5'-UTRs) and optimize\nsequences for high translation efficiencies. Our guided diffusion model\neffectively generates diverse 5'-UTRs with high Mean Ribosome Loading (MRL) and\nTranslation Efficiency (TE), outperforming baselines in balancing rewards and\nstructural stability trade-off. Our findings hold potential for advancing RNA\nsequence-function research and therapeutic RNA design.",
      "tldr_zh": "该研究提出RNAdiffusion，一种潜在扩散模型，用于生成和优化可变长度的RNA序列，以支持RNA在生物功能中的多样性。模型利用预训练的BERT-type模型对RNA序列进行编码，并通过Query Transformer压缩成固定长度的潜在向量，随后训练自回归解码器和连续扩散模型来重建和优化序列。实验结果显示，RNAdiffusion生成的非编码RNA与自然分布一致，并在mRNA 5'-UTRs优化中，产生多样性高的序列，具有高Mean Ribosome Loading (MRL)和Translation Efficiency (TE)，优于基线模型。该方法为RNA序列-功能研究和治疗性RNA设计提供了新潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09828v2",
      "published_date": "2024-09-15 19:04:50 UTC",
      "updated_date": "2024-10-02 16:42:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:06:30.013507"
    },
    {
      "arxiv_id": "2409.09827v1",
      "title": "On the Effect of Robot Errors on Human Teaching Dynamics",
      "title_zh": "机器人错误对人类教学动态的影响",
      "authors": [
        "Jindan Huang",
        "Isaac Sheidlower",
        "Reuben M. Aronson",
        "Elaine Schaertl Short"
      ],
      "abstract": "Human-in-the-loop learning is gaining popularity, particularly in the field\nof robotics, because it leverages human knowledge about real-world tasks to\nfacilitate agent learning. When people instruct robots, they naturally adapt\ntheir teaching behavior in response to changes in robot performance. While\ncurrent research predominantly focuses on integrating human teaching dynamics\nfrom an algorithmic perspective, understanding these dynamics from a\nhuman-centered standpoint is an under-explored, yet fundamental problem.\nAddressing this issue will enhance both robot learning and user experience.\nTherefore, this paper explores one potential factor contributing to the dynamic\nnature of human teaching: robot errors. We conducted a user study to\ninvestigate how the presence and severity of robot errors affect three\ndimensions of human teaching dynamics: feedback granularity, feedback richness,\nand teaching time, in both forced-choice and open-ended teaching contexts. The\nresults show that people tend to spend more time teaching robots with errors,\nprovide more detailed feedback over specific segments of a robot's trajectory,\nand that robot error can influence a teacher's choice of feedback modality. Our\nfindings offer valuable insights for designing effective interfaces for\ninteractive learning and optimizing algorithms to better understand human\nintentions.",
      "tldr_zh": "这篇论文探讨了机器人错误（robot errors）对人类教学动态（human teaching dynamics）的影响，旨在从人类中心视角理解人们如何调整教学行为以提升机器人学习。研究通过用户研究考察了错误的存在和严重程度对反馈粒度（feedback granularity）、反馈丰富度（feedback richness）和教学时间的影响，在强制选择和开放式教学语境中进行测试。结果显示，人们倾向于为有错误的机器人投入更多时间，提供更详细的针对性反馈，并可能改变反馈方式。这些发现为设计有效的交互学习接口和优化算法以更好地捕捉人类意图提供了宝贵洞见。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to 2024 International Conference on Human-Agent Interaction\n  (HAI)",
      "pdf_url": "http://arxiv.org/pdf/2409.09827v1",
      "published_date": "2024-09-15 19:02:34 UTC",
      "updated_date": "2024-09-15 19:02:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:06:40.895638"
    },
    {
      "arxiv_id": "2409.09825v2",
      "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping",
      "title_zh": "GP-GPT：用于基因-表型映射的大型语言模型",
      "authors": [
        "Yanjun Lyu",
        "Zihao Wu",
        "Lu Zhang",
        "Jing Zhang",
        "Yiwei Li",
        "Wei Ruan",
        "Zhengliang Liu",
        "Xiaowei Yu",
        "Chao Cao",
        "Tong Chen",
        "Minheng Chen",
        "Yan Zhuang",
        "Xiang Li",
        "Rongjie Liu",
        "Chao Huang",
        "Wentao Li",
        "Tianming Liu",
        "Dajiang Zhu"
      ],
      "abstract": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.",
      "tldr_zh": "这篇论文介绍了 GP-GPT，一种专门设计用于基因-表型映射的 Large Language Model（LLMs），旨在解决基因组数据复杂性和异质性带来的挑战。模型通过两阶段微调，在超过 3,000,000 条基因组学、蛋白质组学和医学遗传学术语的综合语料库上进行训练，提升了信息检索和关系分析能力。实验结果显示，GP-GPT 在相关任务上优于 Llama2、Llama3 和 GPT-4，提升了 29.32% 的准确率，并展示了 LLM 在基因-表型研究中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09825v2",
      "published_date": "2024-09-15 18:56:20 UTC",
      "updated_date": "2024-09-27 20:26:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:06:53.214661"
    },
    {
      "arxiv_id": "2409.10580v1",
      "title": "Veridical Data Science for Medical Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Alaa",
        "Bin Yu"
      ],
      "abstract": "The advent of foundation models (FMs) such as large language models (LLMs)\nhas led to a cultural shift in data science, both in medicine and beyond. This\nshift involves moving away from specialized predictive models trained for\nspecific, well-defined domain questions to generalist FMs pre-trained on vast\namounts of unstructured data, which can then be adapted to various clinical\ntasks and questions. As a result, the standard data science workflow in\nmedicine has been fundamentally altered; the foundation model lifecycle (FMLC)\nnow includes distinct upstream and downstream processes, in which computational\nresources, model and data access, and decision-making power are distributed\namong multiple stakeholders. At their core, FMs are fundamentally statistical\nmodels, and this new workflow challenges the principles of Veridical Data\nScience (VDS), hindering the rigorous statistical analysis expected in\ntransparent and scientifically reproducible data science practices. We\ncritically examine the medical FMLC in light of the core principles of VDS:\npredictability, computability, and stability (PCS), and explain how it deviates\nfrom the standard data science workflow. Finally, we propose recommendations\nfor a reimagined medical FMLC that expands and refines the PCS principles for\nVDS including considering the computational and accessibility constraints\ninherent to FMs.",
      "tldr_zh": "该论文探讨了基础模型（FMs），如大型语言模型（LLMs），在医学数据科学中的兴起，以及其从特定预测模型向通用模型的转变如何改变了标准工作流。作者分析了基础模型生命周期（FMLC）的上游和下游过程，指出它挑战了 Veridical Data Science (VDS) 的核心原则：可预测性（predictability）、计算性（computability）和稳定性（stability），导致透明性和可重复性问题。论文通过批判性审视医疗 FMLC，解释了其与传统工作流的偏差。最终，作者提出重新构想医疗 FMLC 的推荐，包括扩展 PCS 原则以应对 FMs 的计算和可访问性约束。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10580v1",
      "published_date": "2024-09-15 18:44:38 UTC",
      "updated_date": "2024-09-15 18:44:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:07:06.007621"
    },
    {
      "arxiv_id": "2409.09822v3",
      "title": "Causal Inference with Large Language Model: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Ma"
      ],
      "abstract": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.",
      "tldr_zh": "这篇论文对使用大型语言模型（LLMs）进行因果推理进行了全面调查，强调了LLMs如何通过整合人类知识、数学推理和数据挖掘来解决传统因果推理领域的挑战。论文总结了各种因果任务和方法，包括不同因果水平的评估结果，并比较了这些方法在实际场景中的表现。最终，它讨论了关键发现和未来研究方向，突出了LLMs在推进因果推理方法论方面的潜在影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 2 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.09822v3",
      "published_date": "2024-09-15 18:43:11 UTC",
      "updated_date": "2025-02-09 06:59:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:07:26.421849"
    },
    {
      "arxiv_id": "2409.10579v1",
      "title": "Recent advances in deep learning and language models for studying the microbiome",
      "title_zh": "深度学习和语言模型在微生物组研究中的最新进展",
      "authors": [
        "Binghao Yan",
        "Yunbi Nam",
        "Lingyao Li",
        "Rebecca A. Deek",
        "Hongzhe Li",
        "Siyuan Ma"
      ],
      "abstract": "Recent advancements in deep learning, particularly large language models\n(LLMs), made a significant impact on how researchers study microbiome and\nmetagenomics data. Microbial protein and genomic sequences, like natural\nlanguages, form a language of life, enabling the adoption of LLMs to extract\nuseful insights from complex microbial ecologies. In this paper, we review\napplications of deep learning and language models in analyzing microbiome and\nmetagenomics data. We focus on problem formulations, necessary datasets, and\nthe integration of language modeling techniques. We provide an extensive\noverview of protein/genomic language modeling and their contributions to\nmicrobiome studies. We also discuss applications such as novel viromics\nlanguage modeling, biosynthetic gene cluster prediction, and knowledge\nintegration for metagenomics studies.",
      "tldr_zh": "这篇论文回顾了深度学习，特别是大型语言模型 (LLMs) 在微生物组和宏基因组研究中的最新进展，将微生物蛋白质和基因组序列视为“生命语言”来提取复杂生态见解。论文重点讨论了问题表述、必要数据集以及语言建模技术的整合，包括蛋白质/基因组语言建模的应用。最终，它概述了这些方法在新型病毒组语言建模、生物合成基因簇预测以及宏基因组知识整合方面的贡献。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10579v1",
      "published_date": "2024-09-15 18:32:31 UTC",
      "updated_date": "2024-09-15 18:32:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:07:28.894236"
    },
    {
      "arxiv_id": "2409.10578v1",
      "title": "GLEAN: Generative Learning for Eliminating Adversarial Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Justin Lyu Kim",
        "Kyoungwan Woo"
      ],
      "abstract": "In the age of powerful diffusion models such as DALL-E and Stable Diffusion,\nmany in the digital art community have suffered style mimicry attacks due to\nfine-tuning these models on their works. The ability to mimic an artist's style\nvia text-to-image diffusion models raises serious ethical issues, especially\nwithout explicit consent. Glaze, a tool that applies various ranges of\nperturbations to digital art, has shown significant success in preventing style\nmimicry attacks, at the cost of artifacts ranging from imperceptible noise to\nsevere quality degradation. The release of Glaze has sparked further\ndiscussions regarding the effectiveness of similar protection methods. In this\npaper, we propose GLEAN- applying I2I generative networks to strip\nperturbations from Glazed images, evaluating the performance of style mimicry\nattacks before and after GLEAN on the results of Glaze. GLEAN aims to support\nand enhance Glaze by highlighting its limitations and encouraging further\ndevelopment.",
      "tldr_zh": "本文针对扩散模型（如 DALL-E 和 Stable Diffusion）引发的艺术风格模仿攻击问题，提出 GLEAN 方法，该方法使用 I2I 生成网络去除 Glaze 工具添加的扰动噪声，从而减少图像质量退化。GLEAN 通过评估风格模仿攻击在处理前后的性能，突出了 Glaze 的局限性，并旨在支持其改进以提供更有效的保护机制。实验结果有助于推动数字艺术社区的伦理防护发展。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10578v1",
      "published_date": "2024-09-15 18:28:56 UTC",
      "updated_date": "2024-09-15 18:28:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:07:40.982720"
    },
    {
      "arxiv_id": "2409.11436v1",
      "title": "Analysis of flexible traffic control method in SDN",
      "title_zh": "翻译失败",
      "authors": [
        "Marta Szymczyk"
      ],
      "abstract": "The aim of this paper is to analyze methods of flexible control in SDN\nnetworks and to propose a self-developed solution that will enable intelligent\nadaptation of SDN controller performance. This work aims not only to review\nexisting solutions, but also to develop an approach that will increase the\nefficiency and adaptability of network management. The project uses a modern\ntype of machine learning, Reinforcement Learning, which allows autonomous\ndecisions of a network that learns based on its choices in a dynamically\nchanging environment, which is most similar to the way humans learn. The\nsolution aims not only to improve the network's performance, but also its\nflexibility and real-time adaptability - flexible traffic control.",
      "tldr_zh": "这篇论文分析了软件定义网络(SDN)中的灵活流量控制方法，并提出一个自研解决方案，以实现SDN控制器的智能适应。该方案不仅回顾现有技术，还利用强化学习(Reinforcement Learning)技术，使网络能够基于动态环境中的自主决策进行学习，类似于人类学习过程。这种方法显著提高了网络管理的效率、灵活性和实时适应性，为优化SDN性能提供了新途径。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.11436v1",
      "published_date": "2024-09-15 18:09:59 UTC",
      "updated_date": "2024-09-15 18:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:07:51.945783"
    },
    {
      "arxiv_id": "2409.09808v3",
      "title": "Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Hui Shen",
        "Zhongwei Wan",
        "Xin Wang",
        "Mi Zhang"
      ],
      "abstract": "Mamba and Vision Mamba (Vim) models have shown their potential as an\nalternative to methods based on Transformer architecture. This work introduces\nFast Mamba for Vision (Famba-V), a cross-layer token fusion technique to\nenhance the training efficiency of Vim models. The key idea of Famba-V is to\nidentify and fuse similar tokens across different Vim layers based on a suit of\ncross-layer strategies instead of simply applying token fusion uniformly across\nall the layers that existing works propose. We evaluate the performance of\nFamba-V on CIFAR-100. Our results show that Famba-V is able to enhance the\ntraining efficiency of Vim models by reducing both training time and peak\nmemory usage during training. Moreover, the proposed cross-layer strategies\nallow Famba-V to deliver superior accuracy-efficiency trade-offs. These results\nall together demonstrate Famba-V as a promising efficiency enhancement\ntechnique for Vim models.",
      "tldr_zh": "本研究引入 Famba-V，一种针对 Vision Mamba (Vim) 模型的快速框架，通过跨层 token fusion 技术提升训练效率。Famba-V 的核心创新在于基于一组跨层策略识别并融合不同层之间的相似 token，而不是简单地均匀应用现有方法。在 CIFAR-100 数据集上的实验结果显示，Famba-V 显著减少了训练时间和峰值内存使用，同时实现了优越的准确率与效率权衡，证明其是 Vim 模型的 promising 效率提升技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Camera ready version of ECCV 2024 Workshop on Computational Aspects\n  of Deep Learning (Best Paper Award)",
      "pdf_url": "http://arxiv.org/pdf/2409.09808v3",
      "published_date": "2024-09-15 18:02:26 UTC",
      "updated_date": "2024-10-06 16:34:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:08:04.677872"
    },
    {
      "arxiv_id": "2409.09804v1",
      "title": "Abnormal Event Detection In Videos Using Deep Embedding",
      "title_zh": "基于深度嵌入的视频异常事件检测",
      "authors": [
        "Darshan Venkatrayappa"
      ],
      "abstract": "Abnormal event detection or anomaly detection in surveillance videos is\ncurrently a challenge because of the diversity of possible events. Due to the\nlack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without supervision. In this work we propose an\nunsupervised approach for video anomaly detection with the aim to jointly\noptimize the objectives of the deep neural network and the anomaly detection\ntask using a hybrid architecture. Initially, a convolutional autoencoder is\npre-trained in an unsupervised manner with a fusion of depth, motion and\nappearance features. In the second step, we utilize the encoder part of the\npre-trained autoencoder and extract the embeddings of the fused input. Now, we\njointly train/ fine tune the encoder to map the embeddings to a hypercenter.\nThus, embeddings of normal data fall near the hypercenter, whereas embeddings\nof anomalous data fall far away from the hypercenter.",
      "tldr_zh": "该论文针对监控视频中的异常事件检测（Abnormal Event Detection）提出了一种无监督（unsupervised）方法，以应对事件多样性和训练数据缺乏的挑战。方法首先使用卷积自编码器（convolutional autoencoder）在无监督方式下预训练，融合深度（depth）、运动（motion）和外观（appearance）特征提取嵌入（embeddings）。随后，通过联合训练/微调编码器，将正常数据的嵌入映射到超中心（hypercenter）附近，而异常数据的嵌入则远离超中心，从而实现有效的异常检测。该方法优化了深度神经网络（deep neural network）的目标与异常检测任务，提供了一种混合架构的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09804v1",
      "published_date": "2024-09-15 17:44:51 UTC",
      "updated_date": "2024-09-15 17:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:08:16.515170"
    },
    {
      "arxiv_id": "2409.09790v1",
      "title": "Multiple Rotation Averaging with Constrained Reweighting Deep Matrix Factorization",
      "title_zh": "翻译失败",
      "authors": [
        "Shiqi Li",
        "Jihua Zhu",
        "Yifan Xie",
        "Naiwen Hu",
        "Mingchen Zhu",
        "Zhongyu Li",
        "Di Wang"
      ],
      "abstract": "Multiple rotation averaging plays a crucial role in computer vision and\nrobotics domains. The conventional optimization-based methods optimize a\nnonlinear cost function based on certain noise assumptions, while most previous\nlearning-based methods require ground truth labels in the supervised training\nprocess. Recognizing the handcrafted noise assumption may not be reasonable in\nall real-world scenarios, this paper proposes an effective rotation averaging\nmethod for mining data patterns in a learning manner while avoiding the\nrequirement of labels. Specifically, we apply deep matrix factorization to\ndirectly solve the multiple rotation averaging problem in unconstrained linear\nspace. For deep matrix factorization, we design a neural network model, which\nis explicitly low-rank and symmetric to better suit the background of multiple\nrotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering\nto suppress the influence of rotation outliers. What's more, we also adopt a\nreweighting scheme and dynamic depth selection strategy to further improve the\nrobustness. Our method synthesizes the merit of both optimization-based and\nlearning-based methods. Experimental results on various datasets validate the\neffectiveness of our proposed method.",
      "tldr_zh": "这篇论文提出了一种无需监督标签的学习方法，用于处理Multiple Rotation Averaging问题，旨在克服传统优化方法对噪声假设的依赖。方法基于Constrained Reweighting Deep Matrix Factorization，通过设计一个显式低秩和对称的神经网络模型、基于生成树的边过滤机制，以及再加权方案和动态深度选择策略，来提升鲁棒性和抑制旋转异常值。实验结果在各种数据集上验证了该方法的有效性，成功结合了优化和学习方法的优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09790v1",
      "published_date": "2024-09-15 16:50:27 UTC",
      "updated_date": "2024-09-15 16:50:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:08:28.814857"
    },
    {
      "arxiv_id": "2409.09787v4",
      "title": "BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching",
      "title_zh": "翻译失败",
      "authors": [
        "RuiKang OuYang",
        "Bo Qiang",
        "José Miguel Hernández-Lobato"
      ],
      "abstract": "Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, Noised Energy Matching, which\ntheoretically has lower variance and more complexity compared to related works.\nFurthermore, a novel bootstrapping technique is applied to NEM to balance\nbetween bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40\nGaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The\nexperimental results demonstrate that BNEM can achieve state-of-the-art\nperformance while being more robust.",
      "tldr_zh": "本研究针对从 Boltzmann 分布生成独立同分布 (IID) 样本的挑战，提出了一种基于扩散模型的采样器 Noised Energy Matching (NEM)，通过学习噪声数据的能量来实现更低的方差和更高的复杂性。作者进一步引入 bootstrapping 技术，开发了 BNEM，以平衡偏差和方差，提升采样器的鲁棒性。在实验中，BNEM 在 2D 40 Gaussian Mixture Model (GMM) 和 4-particle double-well potential (DW-4) 上表现出色，达到了 state-of-the-art 性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "38 pages, 10 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.09787v4",
      "published_date": "2024-09-15 16:41:30 UTC",
      "updated_date": "2025-05-12 16:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:08:41.290941"
    },
    {
      "arxiv_id": "2409.09785v3",
      "title": "Large Language Model Based Generative Error Correction: A Challenge and Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Chao-Han Huck Yang",
        "Taejin Park",
        "Yuan Gong",
        "Yuanchao Li",
        "Zhehuai Chen",
        "Yen-Ting Lin",
        "Chen Chen",
        "Yuchen Hu",
        "Kunal Dhawan",
        "Piotr Żelasko",
        "Chao Zhang",
        "Yun-Nung Chen",
        "Yu Tsao",
        "Jagadeesh Balam",
        "Boris Ginsburg",
        "Sabato Marco Siniscalchi",
        "Eng Siong Chng",
        "Peter Bell",
        "Catherine Lai",
        "Shinji Watanabe",
        "Andreas Stolcke"
      ],
      "abstract": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
      "tldr_zh": "该研究引入了GenSEC挑战，利用大型语言模型(LLMs)对预训练的自动语音识别(ASR)模型输出进行生成性错误修正，旨在提升语音处理任务的性能。挑战包括三个关键任务：(i) 后-ASR转录修正，(ii) 说话者标记，以及(iii) 情感识别，这些任务模拟了未来LLMs在语音接口中的应用潜力。研究提供了基线评估的见解，并分享了设计未来评估的经验教训，以促进更广泛的LLM应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline",
      "pdf_url": "http://arxiv.org/pdf/2409.09785v3",
      "published_date": "2024-09-15 16:32:49 UTC",
      "updated_date": "2024-10-18 07:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:08:52.813985"
    },
    {
      "arxiv_id": "2409.09784v1",
      "title": "Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and Advanced Data Preprocessing Techniques",
      "title_zh": "利用深度学习和高级数据预处理技术增强 PET/CT 成像中的病变分割",
      "authors": [
        "Jiayi Liu",
        "Qiaoyi Xue",
        "Youdan Feng",
        "Tianming Xu",
        "Kaixin Shen",
        "Chuyun Shen",
        "Yuhang Shi"
      ],
      "abstract": "The escalating global cancer burden underscores the critical need for precise\ndiagnostic tools in oncology. This research employs deep learning to enhance\nlesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body\nFDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our\nmethodical approach includes robust preprocessing and data augmentation\ntechniques to ensure model robustness and generalizability. We investigate the\ninfluence of non-zero normalization and modifications to the data augmentation\npipeline, such as the introduction of RandGaussianSharpen and adjustments to\nthe Gamma transform parameter. This study aims to contribute to the\nstandardization of preprocessing and augmentation strategies in PET/CT imaging,\npotentially improving the diagnostic accuracy and the personalized management\nof cancer patients. Our code will be open-sourced and available at\nhttps://github.com/jiayiliu-pku/DC2024.",
      "tldr_zh": "本研究利用深度学习技术提升 PET/CT 成像中的病变分割（lesion segmentation），基于 AutoPET challenge III 的数据集，包括 900 个全身体 FDG-PET/CT 和 600 个 PSMA-PET/CT 研究。研究方法强调了先进的预处理和数据增强策略，如非零归一化（non-zero normalization）、RandGaussianSharpen 以及 Gamma 变换参数调整，以提高模型的鲁棒性和泛化性。实验结果表明，这些优化有助于标准化 PET/CT 图像处理流程，从而提升癌症诊断准确性和个性化管理；相关代码已开源，可在 https://github.com/jiayiliu-pku/DC2024 获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09784v1",
      "published_date": "2024-09-15 16:27:34 UTC",
      "updated_date": "2024-09-15 16:27:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:09:05.062042"
    },
    {
      "arxiv_id": "2409.09766v1",
      "title": "Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting",
      "title_zh": "翻译失败",
      "authors": [
        "Qiaoyi Xue",
        "Youdan Feng",
        "Jiayi Liu",
        "Tianming Xu",
        "Kaixin Shen",
        "Chuyun Shen",
        "Yuhang Shi"
      ],
      "abstract": "This study explores a workflow for automated segmentation of lesions in FDG\nand PSMA PET/CT images. Due to the substantial differences in image\ncharacteristics between FDG and PSMA, specialized preprocessing steps are\nrequired. Utilizing YOLOv8 for data classification, the FDG and PSMA images are\npreprocessed separately before feeding them into the segmentation models,\naiming to improve lesion segmentation accuracy. The study focuses on evaluating\nthe performance of automated segmentation workflow for multitracer PET images.\nThe findings are expected to provide critical insights for enhancing diagnostic\nworkflows and patient-specific treatment plans. Our code will be open-sourced\nand available at https://github.com/jiayiliu-pku/AP2024.",
      "tldr_zh": "本研究提出了一种针对多示踪剂（FDG 和 PSMA）PET/CT 图像的自动病变分割工作流，以应对图像特性差异带来的挑战。方法包括使用 YOLOv8 进行数据分类，并对 FDG 和 PSMA 图像进行单独预处理，然后输入分割模型，以提高分割准确性。实验评估显示，该工作流有望提升诊断工作流和患者特定治疗计划的性能，并提供关键见解；相关代码已开源，地址为 https://github.com/jiayiliu-pku/AP2024。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09766v1",
      "published_date": "2024-09-15 15:32:29 UTC",
      "updated_date": "2024-09-15 15:32:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:09:17.031141"
    },
    {
      "arxiv_id": "2409.09760v3",
      "title": "ELMI: Interactive and Intelligent Sign Language Translation of Lyrics for Song Signing",
      "title_zh": "翻译失败",
      "authors": [
        "Suhyeon Yoo",
        "Khai N. Truong",
        "Young-Ho Kim"
      ],
      "abstract": "d/Deaf and hearing song-signers have become prevalent across video-sharing\nplatforms, but translating songs into sign language remains cumbersome and\ninaccessible. Our formative study revealed the challenges song-signers face,\nincluding semantic, syntactic, expressive, and rhythmic considerations in\ntranslations. We present ELMI, an accessible song-signing tool that assists in\ntranslating lyrics into sign language. ELMI enables users to edit glosses\nline-by-line, with real-time synced lyric and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions throughout. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss research and design\nimplications for accessible and culturally sensitive song-signing translation\ntools.",
      "tldr_zh": "本研究针对聋人/听障者和听力正常者进行歌曲手语翻译的挑战，包括语义、句法、表达和节奏方面的难题，开发了 ELMI 工具。该工具允许用户逐行编辑手语词汇（glosses），并提供实时同步的歌词和音乐视频片段，以及基于 LLM 的聊天功能，帮助讨论翻译含义、情感和时机。通过一项涉及13名手语表演者的探索性研究，ELMI 显著提升了参与者的翻译信心和独立性，他们反馈该工具鼓舞人心、建设性和信息性。该研究还讨论了设计可访问且文化敏感的歌曲手语翻译工具的启示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "H.5.2; I.2.8"
      ],
      "primary_category": "cs.HC",
      "comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/elmi",
      "pdf_url": "http://arxiv.org/pdf/2409.09760v3",
      "published_date": "2024-09-15 15:01:00 UTC",
      "updated_date": "2025-02-21 18:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:09:29.707187"
    },
    {
      "arxiv_id": "2409.09753v1",
      "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
      "title_zh": "DARDA：领域感知实时动态神经网络适应",
      "authors": [
        "Shahriar Rifat",
        "Jonathan Ashdown",
        "Francesco Restuccia"
      ],
      "abstract": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
      "tldr_zh": "该研究针对深度神经网络(DNNs)在输入受噪声或腐败影响时性能下降的问题，提出了一种Domain-Aware Real-Time Dynamic Adaptation (DARDA)方法，以改善Test Time Adaptation (TTA)的资源效率和准确性。DARDA通过预先学习腐败类型的潜在表示，并将其与特定子网络状态关联，在部署后无监督地动态适应新腐败，包括估计当前腐败、选择最接近子网络并调整网络状态，从而减少资源消耗。实验结果显示，在Raspberry Pi和NVIDIA Jetson Nano设备上，DARDA分别将能量消耗和缓存内存占用降低1.74倍和2.64倍，同时在CIFAR-10、CIFAR-100和TinyImagenet数据集上性能提升10.4%、5.7%和4.4%。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09753v1",
      "published_date": "2024-09-15 14:49:30 UTC",
      "updated_date": "2024-09-15 14:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:09:41.385570"
    },
    {
      "arxiv_id": "2409.09748v1",
      "title": "Explore the Hallucination on Low-level Perception for MLLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Yinan Sun",
        "Zicheng Zhang",
        "Haoning Wu",
        "Xiaohong Liu",
        "Weisi Lin",
        "Guangtao Zhai",
        "Xiongkuo Min"
      ],
      "abstract": "The rapid development of Multi-modality Large Language Models (MLLMs) has\nsignificantly influenced various aspects of industry and daily life, showcasing\nimpressive capabilities in visual perception and understanding. However, these\nmodels also exhibit hallucinations, which limit their reliability as AI\nsystems, especially in tasks involving low-level visual perception and\nunderstanding. We believe that hallucinations stem from a lack of explicit\nself-awareness in these models, which directly impacts their overall\nperformance. In this paper, we aim to define and evaluate the self-awareness of\nMLLMs in low-level visual perception and understanding tasks. To this end, we\npresent QL-Bench, a benchmark settings to simulate human responses to low-level\nvision, investigating self-awareness in low-level visual perception through\nvisual question answering related to low-level attributes such as clarity and\nlighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990\nsingle images and 1,999 image pairs, each accompanied by an open-ended question\nabout its low-level features. Through the evaluation of 15 MLLMs, we\ndemonstrate that while some models exhibit robust low-level visual\ncapabilities, their self-awareness remains relatively underdeveloped. Notably,\nfor the same model, simpler questions are often answered more accurately than\ncomplex ones. However, self-awareness appears to improve when addressing more\nchallenging questions. We hope that our benchmark will motivate further\nresearch, particularly focused on enhancing the self-awareness of MLLMs in\ntasks involving low-level visual perception and understanding.",
      "tldr_zh": "本研究探讨了多模态大语言模型(MLLMs)在低级视觉感知任务中的幻觉问题，认为这些幻觉源于模型缺乏明确的自我意识。论文引入了QL-Bench基准，通过构建LLSAVisionQA数据集（包含2990张单图像和1999对图像的开放式问题）来评估MLLMs在清晰度、光照等低级属性的视觉问答中的自我意识。实验评估了15个MLLMs，发现这些模型在低级视觉能力上表现出色，但自我意识相对薄弱，简单问题回答更准确，而复杂问题可能有助于提升自我意识。该基准有望推动未来研究，增强MLLMs在低级视觉感知中的可靠性和性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09748v1",
      "published_date": "2024-09-15 14:38:29 UTC",
      "updated_date": "2024-09-15 14:38:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:09:53.474223"
    },
    {
      "arxiv_id": "2409.09741v1",
      "title": "Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data",
      "title_zh": "翻译失败",
      "authors": [
        "Bastián González-Bustamante"
      ],
      "abstract": "This article benchmarked the ability of OpenAI's GPTs and a number of\nopen-source LLMs to perform annotation tasks on political content. We used a\nnovel protest event dataset comprising more than three million digital\ninteractions and created a gold standard that includes ground-truth labels\nannotated by human coders about toxicity and incivility on social media. We\nincluded in our benchmark Google's Perspective algorithm, which, along with\nGPTs, was employed throughout their respective APIs while the open-source LLMs\nwere deployed locally. The findings show that Perspective API using a laxer\nthreshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot\nclassification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,\nwith a smaller number of parameters, are able to perform the task with high\nperformance, being attractive options that could offer good trade-offs between\nperformance, implementing costs and computing time. Ancillary findings using\nexperiments setting different temperature levels show that although GPTs tend\nto show not only excellent computing time but also overall good levels of\nreliability, only open-source LLMs ensure full reproducibility in the\nannotation.",
      "tldr_zh": "这篇论文评估了各种LLMs（包括OpenAI的GPTs和开源模型）在政治内容文本标注任务中的性能，使用了一个包含超过300万数字互动的抗议事件数据集，并以人类标注的ground-truth标签作为标准，焦点是社交媒体上的toxicity（毒性）和incivility（不文明）。研究比较了Google的Perspective API、GPTs和本地部署的开源LLMs，结果显示Perspective API（使用宽松阈值）、GPT-4o以及Nous Hermes 2 Mixtral在zero-shot classification标注中表现最佳。Nous Hermes 2和Mistral OpenOrca等参数较少的模型提供了性能、实现成本和计算时间间的良好权衡，而开源LLMs在实验中显示出更高的可重复性，尽管GPTs在计算时间和可靠性上表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50 (Primary) 91F10, 91F20 (Secondary)"
      ],
      "primary_category": "cs.CL",
      "comment": "Paper prepared for delivery at the 8th Monash-Warwick-Zurich\n  Text-as-Data Workshop, September 16-17, 2024: 11 pages, 3 tables, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09741v1",
      "published_date": "2024-09-15 14:11:24 UTC",
      "updated_date": "2024-09-15 14:11:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:10:05.176012"
    },
    {
      "arxiv_id": "2409.10575v1",
      "title": "A Tie-breaking based Local Search Algorithm for Stable Matching Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Junyuan Qiu"
      ],
      "abstract": "The stable marriage problem with incomplete lists and ties (SMTI) and the\nhospitals/residents problem with ties (HRT) are important in matching theory\nwith broad practical applications. In this paper, we introduce a tie-breaking\nbased local search algorithm (TBLS) designed to achieve a weakly stable\nmatching of maximum size for both the SMTI and HRT problems. TBLS begins by\narbitrarily resolving all ties and iteratively refines the tie-breaking\nstrategy by adjusting the relative order within ties based on preference ranks\nand the current stable matching. Additionally, we introduce TBLS-E, an\nequity-focused variant of TBLS, specifically designed for the SMTI problem.\nThis variant maintains the objective of maximizing matching size, while\nenhancing equity through two simple modifications. In comparison with ten other\napproximation and local search algorithms, TBLS achieves the highest matching\nsize, while TBLS-E exhibits the lowest sex equality cost. Significantly, TBLS-E\npreserves a matching size comparable to that of TBLS. Both our algorithms\ndemonstrate faster computational speed than other local search algorithms in\nsolving large-sized instances.",
      "tldr_zh": "本论文针对稳定匹配问题，包括 SMTI 和 HRT，提出了一种基于 tie-breaking 的局部搜索算法(TBLS)，旨在实现最大规模的弱稳定匹配。TBLS 通过初始任意打破 ties 并迭代调整 ties 的顺序，根据偏好排名和当前匹配来优化结果；同时，引入了 TBLS-E 变体，针对 SMTI 问题增强公平性（如降低性别平等成本），而保持匹配规模与 TBLS 相当。与其他 10 个算法相比，TBLS 实现了最高的匹配规模，TBLS-E 在公平性上表现出色，且两者在处理大型实例时计算速度更快。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.DM",
        "math.OC"
      ],
      "primary_category": "cs.DS",
      "comment": "Submitted to Journal of Heuristics",
      "pdf_url": "http://arxiv.org/pdf/2409.10575v1",
      "published_date": "2024-09-15 13:36:55 UTC",
      "updated_date": "2024-09-15 13:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:10:16.400715"
    },
    {
      "arxiv_id": "2409.10574v2",
      "title": "Detection Made Easy: Potentials of Large Language Models for Solidity Vulnerabilities",
      "title_zh": "检测变得简单：大语言模型在 Solidity 漏洞检测中的潜力",
      "authors": [
        "Md Tauseef Alam",
        "Raju Halder",
        "Abyayananda Maiti"
      ],
      "abstract": "The large-scale deployment of Solidity smart contracts on the Ethereum\nmainnet has increasingly attracted financially-motivated attackers in recent\nyears. A few now-infamous attacks in Ethereum's history includes DAO attack in\n2016 (50 million dollars lost), Parity Wallet hack in 2017 (146 million dollars\nlocked), Beautychain's token BEC in 2018 (900 million dollars market value fell\nto 0), and NFT gaming blockchain breach in 2022 ($600 million in Ether stolen).\nThis paper presents a comprehensive investigation of the use of large language\nmodels (LLMs) and their capabilities in detecting OWASP Top Ten vulnerabilities\nin Solidity. We introduce a novel, class-balanced, structured, and labeled\ndataset named VulSmart, which we use to benchmark and compare the performance\nof open-source LLMs such as CodeLlama, Llama2, CodeT5 and Falcon, alongside\nclosed-source models like GPT-3.5 Turbo and GPT-4o Mini. Our proposed SmartVD\nframework is rigorously tested against these models through extensive automated\nand manual evaluations, utilizing BLEU and ROUGE metrics to assess the\neffectiveness of vulnerability detection in smart contracts. We also explore\nthree distinct prompting strategies-zero-shot, few-shot, and\nchain-of-thought-to evaluate the multi-class classification and generative\ncapabilities of the SmartVD framework. Our findings reveal that SmartVD\noutperforms its open-source counterparts and even exceeds the performance of\nclosed-source base models like GPT-3.5 and GPT-4 Mini. After fine-tuning, the\nclosed-source models, GPT-3.5 Turbo and GPT-4o Mini, achieved remarkable\nperformance with 99% accuracy in detecting vulnerabilities, 94% in identifying\ntheir types, and 98% in determining severity. Notably, SmartVD performs best\nwith the `chain-of-thought' prompting technique, whereas the fine-tuned\nclosed-source models excel with the `zero-shot' prompting approach.",
      "tldr_zh": "本论文探讨了大型语言模型（LLMs）在检测 Solidity 智能合约中 OWASP Top Ten 漏洞的潜力，引入了一个平衡的结构化数据集 VulSmart 和框架 SmartVD，以评估各种开源模型（如 CodeLlama、Llama2、CodeT5 和 Falcon）以及闭源模型（如 GPT-3.5 Turbo 和 GPT-4o Mini）。研究通过零-shot、few-shot 和 chain-of-thought 等提示策略进行多类分类和生成任务评估，使用 BLEU 和 ROUGE 指标进行自动化和手动测试。结果显示，SmartVD 优于开源模型，并在 chain-of-thought 提示下表现最佳，而细调后的 GPT-3.5 Turbo 和 GPT-4o Mini 实现了 99% 的漏洞检测准确率、94% 的类型识别准确率和 98% 的严重性判断准确率。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.10574v2",
      "published_date": "2024-09-15 13:16:58 UTC",
      "updated_date": "2024-10-26 09:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:10:30.253915"
    },
    {
      "arxiv_id": "2409.09727v2",
      "title": "From Challenges and Pitfalls to Recommendations and Opportunities: Implementing Federated Learning in Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Ming Li",
        "Pengcheng Xu",
        "Junjie Hu",
        "Zeyu Tang",
        "Guang Yang"
      ],
      "abstract": "Federated learning holds great potential for enabling large-scale healthcare\nresearch and collaboration across multiple centres while ensuring data privacy\nand security are not compromised. Although numerous recent studies suggest or\nutilize federated learning based methods in healthcare, it remains unclear\nwhich ones have potential clinical utility. This review paper considers and\nanalyzes the most recent studies up to May 2024 that describe federated\nlearning based methods in healthcare. After a thorough review, we find that the\nvast majority are not appropriate for clinical use due to their methodological\nflaws and/or underlying biases which include but are not limited to privacy\nconcerns, generalization issues, and communication costs. As a result, the\neffectiveness of federated learning in healthcare is significantly compromised.\nTo overcome these challenges, we provide recommendations and promising\nopportunities that might be implemented to resolve these problems and improve\nthe quality of model development in federated learning with healthcare.",
      "tldr_zh": "这篇评论论文审查了联邦学习（Federated Learning）在医疗领域的应用，分析了截至2024年5月的最新研究，发现大多数方法因方法缺陷（如隐私问题、泛化问题和通信成本）而无法用于临床实践，从而降低了其实际有效性。论文强调了这些挑战可能导致模型性能受损，并提供了具体推荐，包括改进隐私保护和优化通信策略，以提升联邦学习在医疗中的质量和潜力。通过这些机会，研究者可推动更可靠的跨中心协作和数据隐私保护。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by Medical Image Analysis",
      "pdf_url": "http://arxiv.org/pdf/2409.09727v2",
      "published_date": "2024-09-15 13:11:07 UTC",
      "updated_date": "2025-02-04 16:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:10:40.017610"
    },
    {
      "arxiv_id": "2409.09717v1",
      "title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Justas Andriuškevičius",
        "Junzi Sun"
      ],
      "abstract": "Recent developments in language models have created new opportunities in air\ntraffic control studies. The current focus is primarily on text and\nlanguage-based use cases. However, these language models may offer a higher\npotential impact in the air traffic control domain, thanks to their ability to\ninteract with air traffic environments in an embodied agent form. They also\nprovide a language-like reasoning capability to explain their decisions, which\nhas been a significant roadblock for the implementation of automatic air\ntraffic control.\n  This paper investigates the application of a language model-based agent with\nfunction-calling and learning capabilities to resolve air traffic conflicts\nwithout human intervention. The main components of this research are\nfoundational large language models, tools that allow the agent to interact with\nthe simulator, and a new concept, the experience library. An innovative part of\nthis research, the experience library, is a vector database that stores\nsynthesized knowledge that agents have learned from interactions with the\nsimulations and language models.\n  To evaluate the performance of our language model-based agent, both\nopen-source and closed-source models were tested. The results of our study\nreveal significant differences in performance across various configurations of\nthe language model-based agents. The best-performing configuration was able to\nsolve almost all 120 but one imminent conflict scenarios, including up to four\naircraft at the same time. Most importantly, the agents are able to provide\nhuman-level text explanations on traffic situations and conflict resolution\nstrategies.",
      "tldr_zh": "本研究探讨了语言模型（language models）在航空交通控制中的应用，提出了一种基于语言模型的embodied agent，能够通过函数调用和学习能力自动解决航空冲突，并提供类人推理解释。核心组件包括基础大型语言模型、与模拟器交互的工具，以及一个创新概念——experience library，这是一个存储代理从模拟和模型交互中学到的知识的向量数据库。实验结果显示，最佳配置的代理几乎解决了所有120个紧急冲突场景，包括同时处理多达四架飞机，并能生成人类水平的文本解释，显著提升了自动控制的可解释性和可靠性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09717v1",
      "published_date": "2024-09-15 12:49:05 UTC",
      "updated_date": "2024-09-15 12:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:10:52.653577"
    },
    {
      "arxiv_id": "2409.09706v2",
      "title": "Exploring Utility in a Real-World Warehouse Optimization Problem: Formulation Based on Quantum Annealers and Preliminary Results",
      "title_zh": "翻译失败",
      "authors": [
        "Eneko Osaba",
        "Esther Villar-Rodriguez",
        "Antón Asla"
      ],
      "abstract": "In the current NISQ-era, one of the major challenges faced by researchers and\npractitioners lies in figuring out how to combine quantum and classical\ncomputing in the most efficient and innovative way. In this paper, we present a\nmechanism coined as Quantum Initialization for Warehouse Optimization Problem\nthat resorts to D-Wave's Quantum Annealer. The module has been specifically\ndesigned to be embedded into already existing classical software dedicated to\nthe optimization of a real-world industrial problem. We preliminary tested the\nimplemented mechanism through a two-phase experiment against the classical\nversion of the software.",
      "tldr_zh": "本论文探讨了在 NISQ 时代如何高效结合量子和经典计算的问题，提出了一种名为 Quantum Initialization for Warehouse Optimization Problem 的机制，利用 D-Wave's Quantum Annealer 进行仓库优化。 该机制设计为可嵌入现有经典软件中，针对真实世界的工业优化问题进行处理。 通过两阶段实验，与经典版本进行初步比较，展示了其潜在优势，但具体结果需进一步验证。",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "2 pages, 2 figures. Paper presented at the 5th IEEE International\n  Conference on Quantum Computing and Engineering (IEEE QCE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2409.09706v2",
      "published_date": "2024-09-15 11:58:07 UTC",
      "updated_date": "2024-10-01 13:02:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:11:03.690753"
    },
    {
      "arxiv_id": "2409.09702v1",
      "title": "GFlowNet Pretraining with Inexpensive Rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Mohit Pandey",
        "Gopeshh Subbaraj",
        "Emmanuel Bengio"
      ],
      "abstract": "Generative Flow Networks (GFlowNets), a class of generative models have\nrecently emerged as a suitable framework for generating diverse and\nhigh-quality molecular structures by learning from unnormalized reward\ndistributions. Previous works in this direction often restrict exploration by\nusing predefined molecular fragments as building blocks, limiting the chemical\nspace that can be accessed. In this work, we introduce Atomic GFlowNets\n(A-GFNs), a foundational generative model leveraging individual atoms as\nbuilding blocks to explore drug-like chemical space more comprehensively. We\npropose an unsupervised pre-training approach using offline drug-like molecule\ndatasets, which conditions A-GFNs on inexpensive yet informative molecular\ndescriptors such as drug-likeliness, topological polar surface area, and\nsynthetic accessibility scores. These properties serve as proxy rewards,\nguiding A-GFNs towards regions of chemical space that exhibit desirable\npharmacological properties. We further our method by implementing a\ngoal-conditioned fine-tuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on the ZINC15\noffline dataset and employ robust evaluation metrics to show the effectiveness\nof our approach when compared to other relevant baseline methods in drug\ndesign.",
      "tldr_zh": "本研究引入了Atomic GFlowNets (A-GFNs)，一种基础生成模型，使用单个原子作为构建块，以更全面地探索药物化学空间，克服传统GFlowNets依赖预定义分子片段的限制。研究提出了一种无监督预训练方法，利用廉价的分子描述符（如药物相似性、拓扑极性表面面积和合成可访问性分数）作为代理奖励，引导模型生成具有理想药理特性的分子，并通过目标条件微调优化特定属性。在ZINC15数据集上预训练后，A-GFNs与基线方法相比显示出显著优势，提升了药物设计的多样性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09702v1",
      "published_date": "2024-09-15 11:42:17 UTC",
      "updated_date": "2024-09-15 11:42:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:11:16.860727"
    },
    {
      "arxiv_id": "2409.13755v2",
      "title": "Entity-Aware Self-Attention and Contextualized GCN for Enhanced Relation Extraction in Long Sentences",
      "title_zh": "实体感知自注意力和上下文化 GCN 用于长句子中增强的关系抽取",
      "authors": [
        "Xin Wang",
        "Xinyi Bai"
      ],
      "abstract": "Relation extraction as an important natural Language processing (NLP) task is\nto identify relations between named entities in text. Recently, graph\nconvolutional networks over dependency trees have been widely used to capture\nsyntactic features and achieved attractive performance. However, most existing\ndependency-based approaches ignore the positive influence of the words outside\nthe dependency trees, sometimes conveying rich and useful information on\nrelation extraction. In this paper, we propose a novel model, Entity-aware\nSelf-attention Contextualized GCN (ESC-GCN), which efficiently incorporates\nsyntactic structure of input sentences and semantic context of sequences. To be\nspecific, relative position self-attention obtains the overall semantic\npairwise correlation related to word position, and contextualized graph\nconvolutional networks capture rich intra-sentence dependencies between words\nby adequately pruning operations. Furthermore, entity-aware attention layer\ndynamically selects which token is more decisive to make final relation\nprediction. In this way, our proposed model not only reduces the noisy impact\nfrom dependency trees, but also obtains easily-ignored entity-related semantic\nrepresentation. Extensive experiments on various tasks demonstrate that our\nmodel achieves encouraging performance as compared to existing dependency-based\nand sequence-based models. Specially, our model excels in extracting relations\nbetween entities of long sentences.",
      "tldr_zh": "这篇论文提出了一种名为 Entity-aware Self-Attention Contextualized GCN (ESC-GCN) 的新模型，用于提升自然语言处理(NLP)中长句子的关系抽取性能。该模型通过相对位置自注意力机制捕获整体语义相关性，并结合上下文化 Graph Convolutional Networks (GCN) 来处理句子内部依赖，同时引入实体感知注意力层以动态选择关键标记，减少依赖树噪声并整合外部语义信息。实验结果显示，ESC-GCN 在各种任务上优于现有基于依赖树或序列的模型，尤其在长句子中实体关系抽取方面表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.13755v2",
      "published_date": "2024-09-15 10:50:51 UTC",
      "updated_date": "2024-11-12 02:01:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:11:29.519523"
    },
    {
      "arxiv_id": "2409.09687v1",
      "title": "Training Safe Neural Networks with Global SDP Bounds",
      "title_zh": "翻译失败",
      "authors": [
        "Roman Soletskyi",
        "David \"davidad\" Dalrymple"
      ],
      "abstract": "This paper presents a novel approach to training neural networks with formal\nsafety guarantees using semidefinite programming (SDP) for verification. Our\nmethod focuses on verifying safety over large, high-dimensional input regions,\naddressing limitations of existing techniques that focus on adversarial\nrobustness bounds. We introduce an ADMM-based training scheme for an accurate\nneural network classifier on the Adversarial Spheres dataset, achieving\nprovably perfect recall with input dimensions up to $d=40$. This work advances\nthe development of reliable neural network verification methods for\nhigh-dimensional systems, with potential applications in safe RL policies.",
      "tldr_zh": "这篇论文提出了一种使用半定规划(SDP)进行验证的新方法，以训练具有正式安全保证的神经网络，重点解决现有技术在大型高维输入区域上的局限性。作者引入了基于 ADMM 的训练方案，在 Adversarial Spheres 数据集上训练了一个精确的神经网络分类器，实现 d=40 的输入维度时可证明的完美召回率。该方法推进了可靠的神经网络验证技术的发展，并具有潜在应用，如安全的强化学习(RL)策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09687v1",
      "published_date": "2024-09-15 10:50:22 UTC",
      "updated_date": "2024-09-15 10:50:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:11:40.117466"
    },
    {
      "arxiv_id": "2409.09684v1",
      "title": "Anatomy of Machines for Markowitz: Decision-Focused Learning for Mean-Variance Portfolio Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Junhyeong Lee",
        "Inwoo Tae",
        "Yongjae Lee"
      ],
      "abstract": "Markowitz laid the foundation of portfolio theory through the mean-variance\noptimization (MVO) framework. However, the effectiveness of MVO is contingent\non the precise estimation of expected returns, variances, and covariances of\nasset returns, which are typically uncertain. Machine learning models are\nbecoming useful in estimating uncertain parameters, and such models are trained\nto minimize prediction errors, such as mean squared errors (MSE), which treat\nprediction errors uniformly across assets. Recent studies have pointed out that\nthis approach would lead to suboptimal decisions and proposed Decision-Focused\nLearning (DFL) as a solution, integrating prediction and optimization to\nimprove decision-making outcomes. While studies have shown DFL's potential to\nenhance portfolio performance, the detailed mechanisms of how DFL modifies\nprediction models for MVO remain unexplored. This study aims to investigate how\nDFL adjusts stock return prediction models to optimize decisions in MVO,\naddressing the question: \"MSE treats the errors of all assets equally, but how\ndoes DFL reduce errors of different assets differently?\" Answering this will\nprovide crucial insights into optimal stock return prediction for constructing\nefficient portfolios.",
      "tldr_zh": "本研究探讨了Markowitz的均值-方差优化（MVO）框架在处理资产回报不确定性时的挑战，强调传统机器学习模型（如最小均方误差，MSE）均匀对待预测错误可能导致次优投资组合决策。研究引入Decision-Focused Learning (DFL)方法，将预测和优化过程整合，以更好地调整股票回报预测模型。DFL通过差异化处理不同资产的错误，优化MVO决策机制，提供构建高效投资组合的关键见解。实验结果有望揭示DFL如何优先减少高影响资产的错误，提升整体投资绩效。",
      "categories": [
        "q-fin.PM",
        "cs.AI"
      ],
      "primary_category": "q-fin.PM",
      "comment": "7 pages, 3 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2409.09684v1",
      "published_date": "2024-09-15 10:37:11 UTC",
      "updated_date": "2024-09-15 10:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:11:52.298118"
    },
    {
      "arxiv_id": "2409.09680v1",
      "title": "Reliable Multi-View Learning with Conformal Prediction for Aortic Stenosis Classification in Echocardiography",
      "title_zh": "翻译失败",
      "authors": [
        "Ang Nan Gu",
        "Michael Tsang",
        "Hooman Vaseli",
        "Teresa Tsang",
        "Purang Abolmaesumi"
      ],
      "abstract": "The fundamental problem with ultrasound-guided diagnosis is that the acquired\nimages are often 2-D cross-sections of a 3-D anatomy, potentially missing\nimportant anatomical details. This limitation leads to challenges in ultrasound\nechocardiography, such as poor visualization of heart valves or foreshortening\nof ventricles. Clinicians must interpret these images with inherent\nuncertainty, a nuance absent in machine learning's one-hot labels. We propose\nRe-Training for Uncertainty (RT4U), a data-centric method to introduce\nuncertainty to weakly informative inputs in the training set. This simple\napproach can be incorporated to existing state-of-the-art aortic stenosis\nclassification methods to further improve their accuracy. When combined with\nconformal prediction techniques, RT4U can yield adaptively sized prediction\nsets which are guaranteed to contain the ground truth class to a high accuracy.\nWe validate the effectiveness of RT4U on three diverse datasets: a public\n(TMED-2) and a private AS dataset, along with a CIFAR-10-derived toy dataset.\nResults show improvement on all the datasets.",
      "tldr_zh": "该论文针对超声心动图（Echocardiography）中主动脉瓣狭窄（Aortic Stenosis）分类的挑战，提出了一种可靠的多视图学习方法，结合Conformal Prediction来处理图像不确定性问题，如2D截面遗漏3D解剖细节。作者引入Re-Training for Uncertainty (RT4U)——一种数据中心方法，通过向训练集添加不确定性来提升现有分类模型的准确性，并与Conformal Prediction技术整合，生成适应性预测集，确保高置信度包含真实类别。在三个数据集（包括公共数据集TMED-2和私有AS数据集）上的实验验证显示，RT4U显著提高了分类性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in:\n  International Conference on Medical Image Computing and Computer-Assisted\n  Intervention (MICCAI), Springer (2024) under the same title",
      "pdf_url": "http://arxiv.org/pdf/2409.09680v1",
      "published_date": "2024-09-15 10:06:06 UTC",
      "updated_date": "2024-09-15 10:06:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:12:04.846367"
    },
    {
      "arxiv_id": "2409.11432v1",
      "title": "A hybrid solution for 2-UAV RAN slicing",
      "title_zh": "翻译失败",
      "authors": [
        "Nathan Boyer"
      ],
      "abstract": "It's possible to distribute the Internet to users via drones. However it is\nthen necessary to place the drones according to the positions of the users.\nMoreover, the 5th Generation (5G) New Radio (NR) technology is designed to\naccommodate a wide range of applications and industries. The NGNM 5G White\nPaper \\cite{5gwhitepaper} groups these vertical use cases into three\ncategories:\n  - enhanced Mobile Broadband (eMBB)\n  - massive Machine Type Communication (mMTC)\n  - Ultra-Reliable Low-latency Communication (URLLC).\n  Partitioning the physical network into multiple virtual networks appears to\nbe the best way to provide a customised service for each application and limit\noperational costs. This design is well known as \\textit{network slicing}. Each\ndrone must thus slice its bandwidth between each of the 3 user classes. This\nwhole problem (placement + bandwidth) can be defined as an optimization\nproblem, but since it is very hard to solve efficiently, it is almost always\naddressed by AI in the litterature. In my internship, I wanted to prove that\nviewing the problem as an optimization problem can still be useful, by building\nan hybrid solution involving on one hand AI and on the other optimization. I\nuse it to achieve better results than approaches that use only AI, although at\nthe cost of slightly larger (but still reasonable) computation times.",
      "tldr_zh": "本论文提出了一种针对两个无人机的无线接入网切片（2-UAV RAN slicing）的混合解决方案，旨在优化无人机放置和带宽分配，以支持5G NR的多种应用，包括enhanced Mobile Broadband (eMBB)、massive Machine Type Communication (mMTC)和Ultra-Reliable Low-latency Communication (URLLC)。该方法结合AI技术和优化算法，将网络切片用于物理网络分区，提供定制化服务。结果表明，该混合方案比纯AI方法取得了更好的性能优化，尽管计算时间略有增加。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "9 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.11432v1",
      "published_date": "2024-09-15 09:42:31 UTC",
      "updated_date": "2024-09-15 09:42:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:12:16.800342"
    },
    {
      "arxiv_id": "2409.09662v3",
      "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Inhwa Song",
        "SoHyun Park",
        "Sachin R. Pendse",
        "Jessica Lee Schleider",
        "Munmun De Choudhury",
        "Young-Ho Kim"
      ],
      "abstract": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges.",
      "tldr_zh": "该研究针对人们在表达压力经历时因组织想法和情绪而脱机的现象，引入了ExploreSelf——一个由Large Language Models (LLMs)驱动的应用。该系统通过动态生成的自适应问题，提供灵活的指导，让用户主导自己的反思过程。在一项涉及19名参与者的探索性研究中，参与者反馈显示，这种用户驱动的导航增强了参与度和洞见深度。研究讨论了设计LLMs驱动工具的启示，以促进更有效的人类挑战反思。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "H.5.2; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/exploreself",
      "pdf_url": "http://arxiv.org/pdf/2409.09662v3",
      "published_date": "2024-09-15 08:25:24 UTC",
      "updated_date": "2025-02-05 17:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:12:28.307190"
    },
    {
      "arxiv_id": "2409.09653v1",
      "title": "KAN v.s. MLP for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Haihong Guo",
        "Fengxin Li",
        "Jiao Li",
        "Hongyan Liu"
      ],
      "abstract": "Kolmogorov-Arnold Networks (KAN) is an emerging neural network architecture\nin machine learning. It has greatly interested the research community about\nwhether KAN can be a promising alternative of the commonly used Multi-Layer\nPerceptions (MLP). Experiments in various fields demonstrated that KAN-based\nmachine learning can achieve comparable if not better performance than\nMLP-based methods, but with much smaller parameter scales and are more\nexplainable. In this paper, we explore the incorporation of KAN into the actor\nand critic networks for offline reinforcement learning (RL). We evaluated the\nperformance, parameter scales, and training efficiency of various KAN and MLP\nbased conservative Q-learning (CQL) on the the classical D4RL benchmark for\noffline RL. Our study demonstrates that KAN can achieve performance close to\nthe commonly used MLP with significantly fewer parameters. This provides us an\noption to choose the base networks according to the requirements of the offline\nRL tasks.",
      "tldr_zh": "本论文比较了Kolmogorov-Arnold Networks (KAN) 与 Multi-Layer Perceptions (MLP) 在离线强化学习 (offline RL) 中的表现，探讨了将 KAN 整合到 actor 和 critic 网络中的可能性。研究者在经典的 D4RL 基准上评估了基于 KAN 和 MLP 的保守 Q 学习 (CQL) 算法，比较了性能、参数规模和训练效率。结果表明，KAN 能以显著更少的参数实现接近 MLP 的性能，为根据 offline RL 任务需求选择网络架构提供了灵活选项。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages,2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09653v1",
      "published_date": "2024-09-15 07:52:44 UTC",
      "updated_date": "2024-09-15 07:52:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:12:40.647560"
    },
    {
      "arxiv_id": "2409.09647v2",
      "title": "Self-supervised Learning for Acoustic Few-Shot Classification",
      "title_zh": "自监督学习用于声学少样本分类",
      "authors": [
        "Jingyong Liang",
        "Bernd Meyer",
        "Isaac Ning Lee",
        "Thanh-Toan Do"
      ],
      "abstract": "Labelled data are limited and self-supervised learning is one of the most\nimportant approaches for reducing labelling requirements. While it has been\nextensively explored in the image domain, it has so far not received the same\namount of attention in the acoustic domain. Yet, reducing labelling is a key\nrequirement for many acoustic applications. Specifically in bioacoustic, there\nare rarely sufficient labels for fully supervised learning available. This has\nled to the widespread use of acoustic recognisers that have been pre-trained on\nunrelated data for bioacoustic tasks. We posit that training on the actual task\ndata and combining self-supervised pre-training with few-shot classification is\na superior approach that has the ability to deliver high accuracy even when\nonly a few labels are available. To this end, we introduce and evaluate a new\narchitecture that combines CNN-based preprocessing with feature extraction\nbased on state space models (SSMs). This combination is motivated by the fact\nthat CNN-based networks alone struggle to capture temporal information\neffectively, which is crucial for classifying acoustic signals. SSMs,\nspecifically S4 and Mamba, on the other hand, have been shown to have an\nexcellent ability to capture long-range dependencies in sequence data. We\npre-train this architecture using contrastive learning on the actual task data\nand subsequent fine-tuning with an extremely small amount of labelled data. We\nevaluate the performance of this proposed architecture for ($n$-shot,\n$n$-class) classification on standard benchmarks as well as real-world data.\nOur evaluation shows that it outperforms state-of-the-art architectures on the\nfew-shot classification problem.",
      "tldr_zh": "本文探讨了声学领域标签数据稀缺的问题，提出了一种结合自监督学习(self-supervised learning)和少样本分类(few-shot classification)的框架，以减少对标签的依赖。作者设计了新架构，使用CNN-based preprocessing与基于状态空间模型(SSMs，如S4和Mamba)的特征提取，旨在更好地捕捉声学信号的时序信息。实验结果显示，该架构在实际任务数据上通过contrastive learning预训练和少量标签微调后，在标准基准和真实世界数据上优于现有方法，实现了更高的分类准确率。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09647v2",
      "published_date": "2024-09-15 07:45:11 UTC",
      "updated_date": "2025-05-15 11:26:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:12:53.274876"
    },
    {
      "arxiv_id": "2409.09645v1",
      "title": "COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate Time Series Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Jesus Barreda",
        "Ashley Gomez",
        "Ruben Puga",
        "Kaixiong Zhou",
        "Li Zhang"
      ],
      "abstract": "Multivariate time series classification is an important task with widespread\ndomains of applications. Recently, deep neural networks (DNN) have achieved\nstate-of-the-art performance in time series classification. However, they often\nrequire large expert-labeled training datasets which can be infeasible in\npractice. In few-shot settings, i.e. only a limited number of samples per class\nare available in training data, DNNs show a significant drop in testing\naccuracy and poor generalization ability. In this paper, we propose to address\nthese problems from an optimization and a loss function perspective.\nSpecifically, we propose a new learning framework named COSCO consisting of a\nsharpness-aware minimization (SAM) optimization and a Prototypical loss\nfunction to improve the generalization ability of DNN for multivariate time\nseries classification problems under few-shot setting. Our experiments\ndemonstrate our proposed method outperforms the existing baseline methods. Our\nsource code is available at: https://github.com/JRB9/COSCO.",
      "tldr_zh": "这篇论文针对少样本（few-shot）多变量时间序列分类（Multivariate Time Series Classification）问题，提出了一种新的训练框架 COSCO，以解决深度神经网络（DNN）在数据有限场景下的准确率下降和泛化能力差的问题。COSCO 框架结合了 Sharpness-aware minimization (SAM) 优化方法和 Prototypical loss 函数，提高了模型的泛化性能。实验结果显示，该方法在少样本设置下优于现有基线方法，并提供了开源代码以便进一步验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 5 figures, CIKM '24 Short Paper Track",
      "pdf_url": "http://arxiv.org/pdf/2409.09645v1",
      "published_date": "2024-09-15 07:41:55 UTC",
      "updated_date": "2024-09-15 07:41:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:13:04.909789"
    },
    {
      "arxiv_id": "2409.09641v4",
      "title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic Children and Parents with Contextual Guidance and Card Recommendation",
      "title_zh": "AACessTalk：通过上下文指导和卡片推荐促进语言能力有限的自闭症儿童与父母之间的沟通",
      "authors": [
        "Dasom Choi",
        "SoHyun Park",
        "Kyungah Lee",
        "Hwajung Hong",
        "Young-Ho Kim"
      ],
      "abstract": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.",
      "tldr_zh": "该研究针对最小语言自闭症（MVA）儿童与父母的沟通挑战，引入AACessTalk——一个基于平板电脑的AI中介系统，提供实时指导帮助父母参与对话，并向儿童推荐上下文相关的词汇卡。系统通过促进日常对话实践和相互参与，提升了沟通互动的质量。在为期两周的部署研究中，涉及11对MVA儿童-父母对，结果显示对话频率和轮流显著增加，父母探索了新的互动策略，同时赋予儿童更多沟通自主权。该工作为设计平衡沟通动态的技术提供了重要启示。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.2; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "21 pages excluding reference. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/aacesstalk/",
      "pdf_url": "http://arxiv.org/pdf/2409.09641v4",
      "published_date": "2024-09-15 07:23:07 UTC",
      "updated_date": "2025-02-14 07:14:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:13:17.126048"
    },
    {
      "arxiv_id": "2409.09635v1",
      "title": "A Novel Framework For Text Detection From Natural Scene Images With Complex Background",
      "title_zh": "一种新颖的框架，用于从带有复杂背景的自然场景图像中检测文本",
      "authors": [
        "Basavaraj Kaladagi",
        "Jagadeesh Pujari"
      ],
      "abstract": "Recognizing texts from camera images is a known hard problem because of the\ndifficulties in text detection from the varied and complicated background. In\nthis paper we propose a novel and efficient method to detect text region from\nimages with complex background using Wavelet Transforms. The framework uses\nWavelet Transformation of the original image in its grayscale form followed by\nSub-band filtering. Then Region clustering technique is applied using centroids\nof the regions, further Bounding box is fitted to each region thus identifying\nthe text regions. This method is much sophisticated and efficient than the\nprevious methods as it doesn't stick to a particular font size of the text\nthus, making it generalized. The sample set used for experimental purpose\nconsists of 50 images with varying backgrounds. Images with edge prominence are\nconsidered. Furthermore, our method can be easily customized for applications\nwith different scopes.",
      "tldr_zh": "这篇论文提出了一种新框架，用于从复杂背景的自然场景图像中检测文本，解决了传统方法在背景干扰下的难题。框架的核心方法包括对灰度图像应用 Wavelet Transforms 进行变换和 Sub-band filtering，随后使用 Region clustering 技术基于区域中心进行聚类，并为每个区域拟合 Bounding box 以识别文本区域。与现有方法相比，该框架更高效且通用，不受特定字体大小的限制。实验在 50 张背景多变的图像上进行，证明了其有效性，且该方法易于自定义以适应不同应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09635v1",
      "published_date": "2024-09-15 07:12:33 UTC",
      "updated_date": "2024-09-15 07:12:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:13:28.684845"
    },
    {
      "arxiv_id": "2409.09629v2",
      "title": "Confidence Estimation for LLM-Based Dialogue State Tracking",
      "title_zh": "基于LLM的对话状态跟踪置信度估计",
      "authors": [
        "Yi-Jyun Sun",
        "Suvodip Dey",
        "Dilek Hakkani-Tur",
        "Gokhan Tur"
      ],
      "abstract": "Estimation of a model's confidence on its outputs is critical for\nConversational AI systems based on large language models (LLMs), especially for\nreducing hallucination and preventing over-reliance. In this work, we provide\nan exhaustive exploration of methods, including approaches proposed for open-\nand closed-weight LLMs, aimed at quantifying and leveraging model uncertainty\nto improve the reliability of LLM-generated responses, specifically focusing on\ndialogue state tracking (DST) in task-oriented dialogue systems (TODS).\nRegardless of the model type, well-calibrated confidence scores are essential\nto handle uncertainties, thereby improving model performance. We evaluate four\nmethods for estimating confidence scores based on softmax, raw token scores,\nverbalized confidences, and a combination of these methods, using the area\nunder the curve (AUC) metric to assess calibration, with higher AUC indicating\nbetter calibration. We also enhance these with a self-probing mechanism,\nproposed for closed models. Furthermore, we assess these methods using an\nopen-weight model fine-tuned for the task of DST, achieving superior joint goal\naccuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can\nresult in enhanced AUC performance, indicating better confidence score\ncalibration.",
      "tldr_zh": "这篇论文探讨了在基于大型语言模型 (LLMs) 的对话状态跟踪 (DST) 中估计模型置信度的方法，以减少幻觉并提升任务导向对话系统 (TODS) 的可靠性。研究评估了四种置信度估计方法，包括基于 softmax、原始 token 分数、verbalized confidences 以及它们的组合，并使用 AUC 指标衡量校准性能，同时引入 self-probing 机制来进一步优化封闭模型。实验结果显示，微调开放权重 LLMs 可显著提高 AUC 和联合目标准确率 (JGA)，证明了这种方法能有效改善模型的不确定性处理和整体表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at IEEE SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09629v2",
      "published_date": "2024-09-15 06:44:26 UTC",
      "updated_date": "2024-09-21 13:11:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:13:40.895030"
    },
    {
      "arxiv_id": "2409.09628v1",
      "title": "Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot Event-based Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Zongyou Yu",
        "Qiang Qu",
        "Xiaoming Chen",
        "Chen Wang"
      ],
      "abstract": "Recent advancements in event-based zero-shot object recognition have\ndemonstrated promising results. However, these methods heavily depend on\nextensive training and are inherently constrained by the characteristics of\nCLIP. To the best of our knowledge, this research is the first study to explore\nthe understanding capabilities of large language models (LLMs) for event-based\nvisual content. We demonstrate that LLMs can achieve event-based object\nrecognition without additional training or fine-tuning in conjunction with\nCLIP, effectively enabling pure zero-shot event-based recognition.\nParticularly, we evaluate the ability of GPT-4o / 4turbo and two other\nopen-source LLMs to directly recognize event-based visual content. Extensive\nexperiments are conducted across three benchmark datasets, systematically\nassessing the recognition accuracy of these models. The results show that LLMs,\nespecially when enhanced with well-designed prompts, significantly improve\nevent-based zero-shot recognition performance. Notably, GPT-4o outperforms the\ncompared models and exceeds the recognition accuracy of state-of-the-art\nevent-based zero-shot methods on N-ImageNet by five orders of magnitude. The\nimplementation of this paper is available at\n\\url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.",
      "tldr_zh": "本研究首次探索大型语言模型 (LLMs) 在事件-based 视觉内容理解方面的能力，旨在实现纯 zero-shot 事件-based 识别，而无需额外训练或微调。研究者使用 GPT-4o / 4turbo 和其他开源 LLMs 与 CLIP 结合，通过精心设计的提示，直接识别事件-based 对象。实验结果显示，LLMs 显著提升了 zero-shot 识别性能，其中 GPT-4o 在 N-ImageNet 数据集上比最先进方法高出五个数量级的准确率。该方法为事件-based 识别提供了新范式，并提供了开源实现。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09628v1",
      "published_date": "2024-09-15 06:43:03 UTC",
      "updated_date": "2024-09-15 06:43:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:13:53.276602"
    },
    {
      "arxiv_id": "2410.03538v2",
      "title": "Dreaming User Multimodal Representation Guided by The Platonic Representation Hypothesis for Micro-Video Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Chengzhi Lin",
        "Hezheng Lin",
        "Shuchang Liu",
        "Cangguang Ruan",
        "LingJing Xu",
        "Dezhao Yang",
        "Chuyuan Wang",
        "Yongqi Liu"
      ],
      "abstract": "The proliferation of online micro-video platforms has underscored the\nnecessity for advanced recommender systems to mitigate information overload and\ndeliver tailored content. Despite advancements, accurately and promptly\ncapturing dynamic user interests remains a formidable challenge. Inspired by\nthe Platonic Representation Hypothesis, which posits that different data\nmodalities converge towards a shared statistical model of reality, we introduce\nDreamUMM (Dreaming User Multi-Modal Representation), a novel approach\nleveraging user historical behaviors to create real-time user representation in\na multimoda space. DreamUMM employs a closed-form solution correlating user\nvideo preferences with multimodal similarity, hypothesizing that user interests\ncan be effectively represented in a unified multimodal space. Additionally, we\npropose Candidate-DreamUMM for scenarios lacking recent user behavior data,\ninferring interests from candidate videos alone. Extensive online A/B tests\ndemonstrate significant improvements in user engagement metrics, including\nactive days and play count. The successful deployment of DreamUMM in two\nmicro-video platforms with hundreds of millions of daily active users,\nillustrates its practical efficacy and scalability in personalized micro-video\ncontent delivery. Our work contributes to the ongoing exploration of\nrepresentational convergence by providing empirical evidence supporting the\npotential for user interest representations to reside in a multimodal space.",
      "tldr_zh": "该研究受 Platonic Representation Hypothesis 启发，针对微视频推荐系统的信息过载问题，提出了一种名为 DreamUMM 的新方法，通过用户历史行为在多模态空间中创建实时用户表示，并使用闭式解（closed-form solution）将用户视频偏好与多模态相似度相关联。针对缺乏最近用户行为数据的场景，还引入了 Candidate-DreamUMM，从候选视频推断用户兴趣。实验结果显示，在线 A/B tests 显著提升了用户参与指标，如活跃天数和播放次数，且 DreamUMM 已成功部署在两个拥有数亿日活跃用户的微视频平台上，证明其在个性化内容交付中的实际效力和可扩展性。该工作提供了实证证据，支持用户兴趣表示在多模态空间中的可能性，推进了代表性收敛的探索。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "4 Figure; 2 Table",
      "pdf_url": "http://arxiv.org/pdf/2410.03538v2",
      "published_date": "2024-09-15 06:40:38 UTC",
      "updated_date": "2024-10-19 13:51:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:14:05.888999"
    },
    {
      "arxiv_id": "2409.09626v1",
      "title": "Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Ren",
        "Danica J. Sutherland"
      ],
      "abstract": "Obtaining compositional mappings is important for the model to generalize\nwell compositionally. To better understand when and how to encourage the model\nto learn such mappings, we study their uniqueness through different\nperspectives. Specifically, we first show that the compositional mappings are\nthe simplest bijections through the lens of coding length (i.e., an upper bound\nof their Kolmogorov complexity). This property explains why models having such\nmappings can generalize well. We further show that the simplicity bias is\nusually an intrinsic property of neural network training via gradient descent.\nThat partially explains why some models spontaneously generalize well when they\nare trained appropriately.",
      "tldr_zh": "这篇论文探讨了神经网络模型在学习动态（learning dynamics）中对组合映射（compositional mappings）的简单性偏差（simplicity bias），以理解模型的组合泛化能力。研究首先证明，组合映射是编码长度（coding length）最短的双射（bijections），这从 Kolmogorov 复杂度角度解释了为什么这类映射能提升模型的泛化性能。进一步，论文显示简单性偏差是神经网络通过梯度下降训练的内在属性，从而部分解释了为什么某些模型在适当训练后能自发实现良好泛化。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "4 pages",
      "pdf_url": "http://arxiv.org/pdf/2409.09626v1",
      "published_date": "2024-09-15 06:37:12 UTC",
      "updated_date": "2024-09-15 06:37:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:14:17.076463"
    },
    {
      "arxiv_id": "2409.09621v1",
      "title": "Stutter-Solver: End-to-end Multi-lingual Dysfluency Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanru Zhou",
        "Cheol Jun Cho",
        "Ayati Sharma",
        "Brittany Morin",
        "David Baquirin",
        "Jet Vonk",
        "Zoe Ezzes",
        "Zachary Miller",
        "Boon Lead Tee",
        "Maria Luisa Gorno Tempini",
        "Jiachen Lian",
        "Gopala Anumanchipalli"
      ],
      "abstract": "Current de-facto dysfluency modeling methods utilize template matching\nalgorithms which are not generalizable to out-of-domain real-world dysfluencies\nacross languages, and are not scalable with increasing amounts of training\ndata. To handle these problems, we propose Stutter-Solver: an end-to-end\nframework that detects dysfluency with accurate type and time transcription,\ninspired by the YOLO object detection algorithm. Stutter-Solver can handle\nco-dysfluencies and is a natural multi-lingual dysfluency detector. To leverage\nscalability and boost performance, we also introduce three novel dysfluency\ncorpora: VCTK-Pro, VCTK-Art, and AISHELL3-Pro, simulating natural spoken\ndysfluencies including repetition, block, missing, replacement, and\nprolongation through articulatory-encodec and TTS-based methods. Our approach\nachieves state-of-the-art performance on all available dysfluency corpora. Code\nand datasets are open-sourced at https://github.com/eureka235/Stutter-Solver",
      "tldr_zh": "该论文针对当前基于模板匹配的dysfluency建模方法存在泛化性和可扩展性问题，提出了一种端到端(end-to-end)框架Stutter-Solver，灵感来源于YOLO物体检测算法，能够准确检测dysfluency的类型和时间转录，并支持多语言和co-dysfluencies。框架还引入了三个新语料库（VCTK-Pro、VCTK-Art和AISHELL3-Pro），通过articulatory-encodec和TTS-based方法模拟自然dysfluencies，包括重复、阻塞、缺失、替换和延长。实验结果显示，该方法在所有可用dysfluency语料库上达到了state-of-the-art性能，并开源了代码和数据集。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "IEEE Spoken Language Technology Workshop 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09621v1",
      "published_date": "2024-09-15 06:11:00 UTC",
      "updated_date": "2024-09-15 06:11:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:14:29.663468"
    },
    {
      "arxiv_id": "2409.09615v1",
      "title": "Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Jianfei Wu",
        "Xubin Wang",
        "Weijia Jia"
      ],
      "abstract": "The traditional data annotation process is often labor-intensive,\ntime-consuming, and susceptible to human bias, which complicates the management\nof increasingly complex datasets. This study explores the potential of large\nlanguage models (LLMs) as automated data annotators to improve efficiency and\nconsistency in annotation tasks. By employing rationale-driven collaborative\nfew-shot prompting techniques, we aim to improve the performance of LLMs in\ntext annotation. We conduct a rigorous evaluation of six LLMs across four\nbenchmark datasets, comparing seven distinct methodologies. Our results\ndemonstrate that collaborative methods consistently outperform traditional\nfew-shot techniques and other baseline approaches, particularly in complex\nannotation tasks. Our work provides valuable insights and a robust framework\nfor leveraging collaborative learning methods to tackle challenging text\nannotation tasks.",
      "tldr_zh": "这篇论文探讨了传统文本标注过程的缺点，如劳累、耗时和易受人为偏差影响，并提出使用大型语言模型 (LLMs) 通过 rationale-driven collaborative few-shot prompting 技术来提升标注效率和一致性。研究在四个基准数据集上评估了六个 LLMs 和七种方法，结果显示协作方法在复杂标注任务中显著优于传统 few-shot 技术和基线方法。该框架为利用协作学习方法处理挑战性文本标注任务提供了宝贵的见解和稳健解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09615v1",
      "published_date": "2024-09-15 05:32:21 UTC",
      "updated_date": "2024-09-15 05:32:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:14:41.341486"
    },
    {
      "arxiv_id": "2409.09613v1",
      "title": "Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text Quality Filtering in Large Web Corpora",
      "title_zh": "翻译失败",
      "authors": [
        "Yungi Kim",
        "Hyunsoo Ha",
        "Sukyung Lee",
        "Jihoo Kim",
        "Seonghoon Yang",
        "Chanjun Park"
      ],
      "abstract": "With the increasing demand for substantial amounts of high-quality data to\ntrain large language models (LLMs), efficiently filtering large web corpora has\nbecome a critical challenge. For this purpose, KenLM, a lightweight\nn-gram-based language model that operates on CPUs, is widely used. However, the\ntraditional method of training KenLM utilizes only high-quality data and,\nconsequently, does not explicitly learn the linguistic patterns of low-quality\ndata. To address this issue, we propose an ensemble approach that leverages two\ncontrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad\nKenLM, trained on low-quality data. Experimental results demonstrate that our\napproach significantly reduces noisy content while preserving high-quality\ncontent compared to the traditional KenLM training method. This indicates that\nour method can be a practical solution with minimal computational overhead for\nresource-constrained environments.",
      "tldr_zh": "本研究重新审视 KenLM 模型，针对训练大型语言模型 (LLMs) 时高效过滤大型网络语料库的挑战提出了一种集成方法。传统 KenLM 只使用高质量数据训练，无法识别低质量数据的语言模式；为此，作者引入 Good KenLM（基于高质量数据训练）和 Bad KenLM（基于低质量数据训练）的组合。实验结果表明，这种集成方法相较传统方法更有效地减少噪声内容同时保留高质量数据，且在资源受限环境中具有较低的计算开销。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09613v1",
      "published_date": "2024-09-15 05:27:56 UTC",
      "updated_date": "2024-09-15 05:27:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:14:52.740679"
    },
    {
      "arxiv_id": "2409.09611v1",
      "title": "Integrating Audio Narrations to Strengthen Domain Generalization in Multimodal First-Person Action Recognition",
      "title_zh": "整合音频叙述以加强多模态第一人称动作识别中的领域泛化",
      "authors": [
        "Cagri Gungor",
        "Adriana Kovashka"
      ],
      "abstract": "First-person activity recognition is rapidly growing due to the widespread\nuse of wearable cameras but faces challenges from domain shifts across\ndifferent environments, such as varying objects or background scenes. We\npropose a multimodal framework that improves domain generalization by\nintegrating motion, audio, and appearance features. Key contributions include\nanalyzing the resilience of audio and motion features to domain shifts, using\naudio narrations for enhanced audio-text alignment, and applying consistency\nratings between audio and visual narrations to optimize the impact of audio in\nrecognition during training. Our approach achieves state-of-the-art performance\non the ARGO1M dataset, effectively generalizing across unseen scenarios and\nlocations.",
      "tldr_zh": "该研究针对第一人称活动识别（First-Person Action Recognition）中因环境变化（如不同物体或背景）导致的领域偏移（Domain Generalization）问题，提出了一种多模态框架，通过整合运动（motion）、音频（audio）和外观（appearance）特征来提升模型的泛化能力。关键贡献包括分析音频和运动特征对领域偏移的抵抗力、使用音频叙述（Audio Narrations）增强音频-文本对齐（Audio-Text Alignment），以及应用音频和视觉叙述之间的一致性评分（Consistency Ratings）来优化训练过程。该框架在ARGO1M数据集上实现了最先进（State-of-the-Art）性能，能够有效泛化到未见场景和位置。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09611v1",
      "published_date": "2024-09-15 04:43:00 UTC",
      "updated_date": "2024-09-15 04:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:15:05.297596"
    },
    {
      "arxiv_id": "2409.09603v1",
      "title": "Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison",
      "title_zh": "翻译失败",
      "authors": [
        "Judy Hanwen Shen",
        "Archit Sharma",
        "Jun Qin"
      ],
      "abstract": "The goal of aligning language models to human preferences requires data that\nreveal these preferences. Ideally, time and money can be spent carefully\ncollecting and tailoring bespoke preference data to each downstream\napplication. However, in practice, a select few publicly available preference\ndatasets are often used to train reward models for reinforcement learning from\nhuman feedback (RLHF). While new preference datasets are being introduced with\nincreasing frequency, there are currently no existing efforts to measure and\ncompare these datasets. In this paper, we systematically study preference\ndatasets through three perspectives: scale, label noise, and information\ncontent. We propose specific metrics for each of these perspectives and uncover\ndifferent axes of comparison for a better understanding of preference datasets.\nOur work is a first step towards a data-centric approach to alignment by\nproviding perspectives that aid in training efficiency and iterative data\ncollection for RLHF.",
      "tldr_zh": "该论文探讨了强化学习从人类反馈（RLHF）中对语言模型进行对齐的挑战，强调需要更有效的偏好数据集比较方法。作者从规模（scale）、标签噪声（label noise）和信息内容（information content）三个角度系统研究偏好数据集，并提出相应的简单指标，以揭示数据集间的差异轴。最终，这为数据中心方法（data-centric approach）提供了基础，帮助提升 RLHF 的训练效率和迭代数据收集过程。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Working Paper",
      "pdf_url": "http://arxiv.org/pdf/2409.09603v1",
      "published_date": "2024-09-15 03:55:03 UTC",
      "updated_date": "2024-09-15 03:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:15:16.819116"
    },
    {
      "arxiv_id": "2409.09601v1",
      "title": "A Survey of Foundation Models for Music Understanding",
      "title_zh": "音乐理解的基础模型综述",
      "authors": [
        "Wenjun Li",
        "Ying Cai",
        "Ziyang Wu",
        "Wenyi Zhang",
        "Yifan Chen",
        "Rundong Qi",
        "Mengqi Dong",
        "Peigen Chen",
        "Xiao Dong",
        "Fenghao Shi",
        "Lei Guo",
        "Junwei Han",
        "Bao Ge",
        "Tianming Liu",
        "Lin Gan",
        "Tuo Zhang"
      ],
      "abstract": "Music is essential in daily life, fulfilling emotional and entertainment\nneeds, and connecting us personally, socially, and culturally. A better\nunderstanding of music can enhance our emotions, cognitive skills, and cultural\nconnections. The rapid advancement of artificial intelligence (AI) has\nintroduced new ways to analyze music, aiming to replicate human understanding\nof music and provide related services. While the traditional models focused on\naudio features and simple tasks, the recent development of large language\nmodels (LLMs) and foundation models (FMs), which excel in various fields by\nintegrating semantic information and demonstrating strong reasoning abilities,\ncould capture complex musical features and patterns, integrate music with\nlanguage and incorporate rich musical, emotional and psychological knowledge.\nTherefore, they have the potential in handling complex music understanding\ntasks from a semantic perspective, producing outputs closer to human\nperception. This work, to our best knowledge, is one of the early reviews of\nthe intersection of AI techniques and music understanding. We investigated,\nanalyzed, and tested recent large-scale music foundation models in respect of\ntheir music comprehension abilities. We also discussed their limitations and\nproposed possible future directions, offering insights for researchers in this\nfield.",
      "tldr_zh": "这篇论文对基础模型 (Foundation Models) 在音乐理解领域的应用进行了综述，强调了大型语言模型 (LLMs) 如何通过整合语义信息和强推理能力来捕捉复杂音乐特征、模式，并将音乐与语言相结合，实现更接近人类感知的分析。作者调查、分析并测试了最近的大型音乐基础模型 (FMs)，展示了它们在处理复杂音乐任务（如情感和文化解读）方面的潜力，同时突出了传统音频模型的局限性。论文还讨论了这些模型的不足，并提出了未来研究方向，以推动AI在音乐理解领域的进展。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "20 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2409.09601v1",
      "published_date": "2024-09-15 03:34:14 UTC",
      "updated_date": "2024-09-15 03:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:15:29.429654"
    },
    {
      "arxiv_id": "2409.09598v2",
      "title": "Improving Statistical Significance in Human Evaluation of Automatic Metrics via Soft Pairwise Accuracy",
      "title_zh": "通过 Soft Pairwise Accuracy 改善自动指标",
      "authors": [
        "Brian Thompson",
        "Nitika Mathur",
        "Daniel Deutsch",
        "Huda Khayrallah"
      ],
      "abstract": "Selecting an automatic metric that best emulates human annotators is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\nscores, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric scores. We show that SPA is more stable than PA with\nrespect to changes in the number of systems/segments used for evaluation. We\nalso show that PA can only assign a small set of distinct output values to\nmetrics, and this results in many metrics being artificially assigned the exact\nsame PA score. We demonstrate that SPA fixes this issue. Finally, we show that\nSPA is more discriminative than PA, producing more statistically significant\ncomparisons between metrics. SPA was selected as the official system-level\nmetric for the 2024 WMT Metrics Shared Task.",
      "tldr_zh": "该论文探讨了在人类评估中选择最佳自动指标的挑战，因为缺乏明确的“最佳”定义，且指标排名依赖于元指标(meta-metric)的选择。作者提出Soft Pairwise Accuracy (SPA)，一种基于Pairwise Accuracy (PA)的改进元指标，通过整合人类判断和指标分数的统计显著性，提升评估的稳定性和区分度。研究发现，SPA比PA更稳定，不易受系统或段数变化影响，并能避免PA的局限性，如导致多个指标获得相同分数。最终，SPA在2024 WMT Metrics Shared Task中被选为官方系统级指标，证明了其在自动指标评估中的实际价值。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at WMT 2024",
      "pdf_url": "http://arxiv.org/pdf/2409.09598v2",
      "published_date": "2024-09-15 03:25:55 UTC",
      "updated_date": "2024-10-04 16:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:15:41.449929"
    },
    {
      "arxiv_id": "2410.01817v1",
      "title": "From Experts to the Public: Governing Multimodal Language Models in Politically Sensitive Video Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Tanusree Sharma",
        "Yujin Potter",
        "Zachary Kilhoffer",
        "Yun Huang",
        "Dawn Song",
        "Yang Wang"
      ],
      "abstract": "This paper examines the governance of multimodal large language models\n(MM-LLMs) through individual and collective deliberation, focusing on analyses\nof politically sensitive videos. We conducted a two-step study: first,\ninterviews with 10 journalists established a baseline understanding of expert\nvideo interpretation; second, 114 individuals from the general public engaged\nin deliberation using Inclusive.AI, a platform that facilitates democratic\ndecision-making through decentralized autonomous organization (DAO) mechanisms.\nOur findings show that while experts emphasized emotion and narrative, the\ngeneral public prioritized factual clarity, objectivity of the situation, and\nemotional neutrality. Additionally, we explored the impact of different\ngovernance mechanisms: quadratic vs. weighted ranking voting and equal vs.\n20-80 power distributions on users decision-making on how AI should behave.\nSpecifically, quadratic voting enhanced perceptions of liberal democracy and\npolitical equality, and participants who were more optimistic about AI\nperceived the voting process to have a higher level of participatory democracy.\nOur results suggest the potential of applying DAO mechanisms to help\ndemocratize AI governance.",
      "tldr_zh": "本论文探讨了多模态大型语言模型（MM-LLMs）在政治敏感视频分析中的治理问题，通过专家和公众的审议机制进行研究。研究采用两步方法：首先采访10名记者以建立专家解读基线，强调情感和叙事；其次，让114名普通公众使用Inclusive.AI平台进行审议，该平台基于去中心化自治组织（DAO）机制。结果显示，公众更注重事实清晰、客观性和情感中立，而二次投票机制提高了对自由民主和政治平等的感知，尤其在对AI乐观的参与者中。总体而言，该研究证明了DAO机制在民主化AI治理方面的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2410.01817v1",
      "published_date": "2024-09-15 03:17:38 UTC",
      "updated_date": "2024-09-15 03:17:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:15:53.366956"
    },
    {
      "arxiv_id": "2409.09591v1",
      "title": "Open-World Test-Time Training: Self-Training with Contrast Learning",
      "title_zh": "开放世界测试时训练：利用对比学习的自训练",
      "authors": [
        "Houcheng Su",
        "Mengzhu Wang",
        "Jiao Li",
        "Bingli Wang",
        "Daixian Liu",
        "Zeheng Wang"
      ],
      "abstract": "Traditional test-time training (TTT) methods, while addressing domain shifts,\noften assume a consistent class set, limiting their applicability in real-world\nscenarios characterized by infinite variety. Open-World Test-Time Training\n(OWTTT) addresses the challenge of generalizing deep learning models to unknown\ntarget domain distributions, especially in the presence of strong\nOut-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain\nperformance when confronted with strong OOD data. In OWTTT, the focus has\npredominantly been on distinguishing between overall strong and weak OOD data.\nHowever, during the early stages of TTT, initial feature extraction is hampered\nby interference from strong OOD and corruptions, resulting in diminished\ncontrast and premature classification of certain classes as strong OOD. To\naddress this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an\ninnovative approach that utilizes contrastive learning to augment positive\nsample pairs. This strategy not only bolsters contrast in the early stages but\nalso significantly enhances model robustness in subsequent stages. In\ncomparison datasets, our OWDCL model has produced the most advanced\nperformance.",
      "tldr_zh": "本研究针对传统 Test-Time Training (TTT) 方法在处理未知目标域分布和强 Out-of-Distribution (OOD) 数据时的局限性，提出 Open-World Test-Time Training (OWTTT) 框架，以提升模型在真实世界场景中的泛化能力。论文引入 Open World Dynamic Contrastive Learning (OWDCL) 方法，通过对比学习增强正样本对，缓解早期特征提取的干扰，提高对比度和模型鲁棒性。实验结果显示，OWDCL 在比较数据集上实现了最先进的性能表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10page",
      "pdf_url": "http://arxiv.org/pdf/2409.09591v1",
      "published_date": "2024-09-15 02:36:26 UTC",
      "updated_date": "2024-09-15 02:36:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:16:04.319835"
    },
    {
      "arxiv_id": "2409.09586v2",
      "title": "ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Hua Shen",
        "Tiffany Knearem",
        "Reshmi Ghosh",
        "Yu-Ju Yang",
        "Nicholas Clark",
        "Tanushree Mitra",
        "Yun Huang"
      ],
      "abstract": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.",
      "tldr_zh": "该研究引入了 ValueCompass 框架，一种基于心理理论和系统回顾的工具，用于识别和评估人类与大型语言模型 (LLMs) 之间的上下文价值对齐。研究将该框架应用于协作写作、教育、公共部门和医疗保健等四个真实场景，揭示了人类和 LLMs 存在的显著失调，例如人类更倾向支持“National Security”而 LLMs 往往拒绝。结果强调价值因场景而异，需采用上下文感知的 AI 对齐策略，这为设计负责任的人类-AI alignment 系统提供了重要见解。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09586v2",
      "published_date": "2024-09-15 02:13:03 UTC",
      "updated_date": "2025-04-16 21:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:16:17.960683"
    },
    {
      "arxiv_id": "2409.09570v1",
      "title": "MindScape Study: Integrating LLM and Behavioral Sensing for Personalized AI-Driven Journaling Experiences",
      "title_zh": "MindScape Study：整合 LLM 和行为传感以实现个性化的 AI 驱动日记体验",
      "authors": [
        "Subigya Nepal",
        "Arvind Pillai",
        "William Campbell",
        "Talie Massachi",
        "Michael V. Heinz",
        "Ashmita Kunwar",
        "Eunsol Soul Choi",
        "Orson Xu",
        "Joanna Kuc",
        "Jeremy Huckins",
        "Jason Holden",
        "Sarah M. Preum",
        "Colin Depp",
        "Nicholas Jacobson",
        "Mary Czerwinski",
        "Eric Granholm",
        "Andrew T. Campbell"
      ],
      "abstract": "Mental health concerns are prevalent among college students, highlighting the\nneed for effective interventions that promote self-awareness and holistic\nwell-being. MindScape pioneers a novel approach to AI-powered journaling by\nintegrating passively collected behavioral patterns such as conversational\nengagement, sleep, and location with Large Language Models (LLMs). This\nintegration creates a highly personalized and context-aware journaling\nexperience, enhancing self-awareness and well-being by embedding behavioral\nintelligence into AI. We present an 8-week exploratory study with 20 college\nstudents, demonstrating the MindScape app's efficacy in enhancing positive\naffect (7%), reducing negative affect (11%), loneliness (6%), and anxiety and\ndepression, with a significant week-over-week decrease in PHQ-4 scores (-0.25\ncoefficient), alongside improvements in mindfulness (7%) and self-reflection\n(6%). The study highlights the advantages of contextual AI journaling, with\nparticipants particularly appreciating the tailored prompts and insights\nprovided by the MindScape app. Our analysis also includes a comparison of\nresponses to AI-driven contextual versus generic prompts, participant feedback\ninsights, and proposed strategies for leveraging contextual AI journaling to\nimprove well-being on college campuses. By showcasing the potential of\ncontextual AI journaling to support mental health, we provide a foundation for\nfurther investigation into the effects of contextual AI journaling on mental\nhealth and well-being.",
      "tldr_zh": "MindScape 研究整合了 Large Language Models (LLMs) 和行为感知（如对话参与、睡眠和位置数据），开发了一种个性化的 AI 驱动日记应用，以提升大学生的自我意识和心理健康。研究进行了 8 周的探索性试验，涉及 20 名参与者，结果显示积极情绪提升 7%、消极情绪减少 11%、孤独感降低 6%，以及 PHQ-4 焦虑和抑郁评分每周显著下降（系数 -0.25），同时正念和自我反思也提高了 6%。与通用提示相比，AI 驱动的上下文提示获得积极反馈，该方法为利用上下文 AI 日记改善校园心理健康提供了新策略和研究基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "H.5.0; H.5.3; H.5.m; J.0"
      ],
      "primary_category": "cs.HC",
      "comment": "arXiv admin note: text overlap with arXiv:2404.00487",
      "pdf_url": "http://arxiv.org/pdf/2409.09570v1",
      "published_date": "2024-09-15 01:10:46 UTC",
      "updated_date": "2024-09-15 01:10:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:16:30.158042"
    },
    {
      "arxiv_id": "2409.09566v3",
      "title": "Learning Transferable Features for Implicit Neural Representations",
      "title_zh": "学习隐式神经表示的可转移特征",
      "authors": [
        "Kushal Vyas",
        "Ahmed Imtiaz Humayun",
        "Aniket Dashpute",
        "Richard G. Baraniuk",
        "Ashok Veeraraghavan",
        "Guha Balakrishnan"
      ],
      "abstract": "Implicit neural representations (INRs) have demonstrated success in a variety\nof applications, including inverse problems and neural rendering. An INR is\ntypically trained to capture one signal of interest, resulting in learned\nneural features that are highly attuned to that signal. Assumed to be less\ngeneralizable, we explore the aspect of transferability of such learned neural\nfeatures for fitting similar signals. We introduce a new INR training\nframework, STRAINER that learns transferrable features for fitting INRs to new\nsignals from a given distribution, faster and with better reconstruction\nquality. Owing to the sequential layer-wise affine operations in an INR, we\npropose to learn transferable representations by sharing initial encoder layers\nacross multiple INRs with independent decoder layers. At test time, the learned\nencoder representations are transferred as initialization for an otherwise\nrandomly initialized INR. We find STRAINER to yield extremely powerful\ninitialization for fitting images from the same domain and allow for $\\approx\n+10dB$ gain in signal quality early on compared to an untrained INR itself.\nSTRAINER also provides a simple way to encode data-driven priors in INRs. We\nevaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks\nand inverse problems and further provide detailed analysis and discussion on\nthe transferability of STRAINER's features. Our demo can be accessed at\nhttps://kushalvyas.github.io/strainer.html .",
      "tldr_zh": "本论文探讨了隐式神经表示(INRs)的可转移特征学习问题，旨在解决传统INRs特征针对特定信号而缺乏通用性的局限。作者提出STRAINER框架，通过共享初始编码器层而独立解码器层，在多个INRs之间学习可转移表示，从而加速新信号的拟合并提升重建质量。实验结果显示，STRAINER在同域图像任务上提供强大初始化，早期信号质量提升约+10dB，并在各种信号拟合和逆问题任务中表现出色，提供了一种简单的数据驱动先验编码方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Website: https://kushalvyas.github.io/strainer.html",
      "pdf_url": "http://arxiv.org/pdf/2409.09566v3",
      "published_date": "2024-09-15 00:53:44 UTC",
      "updated_date": "2025-01-09 20:24:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:16:40.730237"
    },
    {
      "arxiv_id": "2409.09564v2",
      "title": "TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Dawei Yan",
        "Pengcheng Li",
        "Yang Li",
        "Hao Chen",
        "Qingguo Chen",
        "Weihua Luo",
        "Wei Dong",
        "Qingsen Yan",
        "Haokui Zhang",
        "Chunhua Shen"
      ],
      "abstract": "Currently, inspired by the success of vision-language models (VLMs), an\nincreasing number of researchers are focusing on improving VLMs and have\nachieved promising results. However, most existing methods concentrate on\noptimizing the connector and enhancing the language model component, while\nneglecting improvements to the vision encoder itself. In contrast, we propose\nText Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the\nvision encoder with text, offering a new and orthogonal optimization direction.\nSpecifically, inspired by the purpose-driven logic inherent in human behavior,\nwe use learnable latent embeddings as a bridge to analyze textual instruction\nand add the analysis results to the vision encoder as guidance, refining it.\nSubsequently, another set of latent embeddings extracts additional detailed\ntext-guided information from high-resolution local patches as auxiliary\ninformation. Finally, with the guidance of text, the vision encoder can extract\ntext-related features, similar to how humans focus on the most relevant parts\nof an image when considering a question. This results in generating better\nanswers. Experiments on various datasets validate the effectiveness of the\nproposed method. Remarkably, without the need for additional training data, our\npropsoed method can bring more benefits to the baseline (LLaVA-1.5) compared\nwith other concurrent methods. Furthermore, the proposed method consistently\nbrings improvement in different settings.",
      "tldr_zh": "本文提出 TG-LLaVA，一种通过文本指导优化视觉语言模型 (VLMs) 的新方法，专注于改进视觉编码器而非传统连接器或语言模型组件。方法利用 learnable latent embeddings 作为桥梁，分析文本指令并将其结果添加到视觉编码器中作为指导，同时从高分辨率局部补丁中提取额外文本相关信息，帮助模型像人类一样聚焦图像关键部分。实验在多种数据集上验证了其有效性，无需额外训练数据，即可为基线模型 (LLaVA-1.5) 带来显著改进，并适用于不同设置。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2409.09564v2",
      "published_date": "2024-09-15 00:38:34 UTC",
      "updated_date": "2024-09-20 14:51:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-20T00:16:52.693998"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 60,
  "processed_papers_count": 60,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-20T00:17:09.540028"
}
[
  {
    "arxiv_id": "2407.08890v1",
    "title": "DeepCodeProbe: Towards Understanding What Models Trained on Code Learn",
    "authors": [
      "Vahid Majdinasab",
      "Amin Nikanjam",
      "Foutse Khomh"
    ],
    "abstract": "Machine learning models trained on code and related artifacts offer valuable\nsupport for software maintenance but suffer from interpretability issues due to\ntheir complex internal variables. These concerns are particularly significant\nin safety-critical applications where the models' decision-making processes\nmust be reliable. The specific features and representations learned by these\nmodels remain unclear, adding to the hesitancy in adopting them widely. To\naddress these challenges, we introduce DeepCodeProbe, a probing approach that\nexamines the syntax and representation learning abilities of ML models designed\nfor software maintenance tasks. Our study applies DeepCodeProbe to\nstate-of-the-art models for code clone detection, code summarization, and\ncomment generation. Findings reveal that while small models capture abstract\nsyntactic representations, their ability to fully grasp programming language\nsyntax is limited. Increasing model capacity improves syntax learning but\nintroduces trade-offs such as increased training time and overfitting.\nDeepCodeProbe also identifies specific code patterns the models learn from\ntheir training data. Additionally, we provide best practices for training\nmodels on code to enhance performance and interpretability, supported by an\nopen-source replication package for broader application of DeepCodeProbe in\ninterpreting other code-related models.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "I.2.5; D.2.3"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08890v1",
    "published_date": "2024-07-11 23:16:44 UTC",
    "updated_date": "2024-07-11 23:16:44 UTC"
  },
  {
    "arxiv_id": "2407.08878v1",
    "title": "SALT: Introducing a Framework for Hierarchical Segmentations in Medical Imaging using Softmax for Arbitrary Label Trees",
    "authors": [
      "Sven Koitka",
      "Giulia Baldini",
      "Cynthia S. Schmidt",
      "Olivia B. Pollok",
      "Obioma Pelka",
      "Judith Kohnke",
      "Katarzyna Borys",
      "Christoph M. Friedrich",
      "Benedikt M. Schaarschmidt",
      "Michael Forsting",
      "Lale Umutlu",
      "Johannes Haubold",
      "Felix Nensa",
      "René Hosch"
    ],
    "abstract": "Traditional segmentation networks approach anatomical structures as\nstandalone elements, overlooking the intrinsic hierarchical connections among\nthem. This study introduces Softmax for Arbitrary Label Trees (SALT), a novel\napproach designed to leverage the hierarchical relationships between labels,\nimproving the efficiency and interpretability of the segmentations.\n  This study introduces a novel segmentation technique for CT imaging, which\nleverages conditional probabilities to map the hierarchical structure of\nanatomical landmarks, such as the spine's division into lumbar, thoracic, and\ncervical regions and further into individual vertebrae. The model was developed\nusing the SAROS dataset from The Cancer Imaging Archive (TCIA), comprising 900\nbody region segmentations from 883 patients. The dataset was further enhanced\nby generating additional segmentations with the TotalSegmentator, for a total\nof 113 labels. The model was trained on 600 scans, while validation and testing\nwere conducted on 150 CT scans. Performance was assessed using the Dice score\nacross various datasets, including SAROS, CT-ORG, FLARE22, LCTSC, LUNA16, and\nWORD.\n  Among the evaluated datasets, SALT achieved its best results on the LUNA16\nand SAROS datasets, with Dice scores of 0.93 and 0.929 respectively. The model\ndemonstrated reliable accuracy across other datasets, scoring 0.891 on CT-ORG\nand 0.849 on FLARE22. The LCTSC dataset showed a score of 0.908 and the WORD\ndataset also showed good performance with a score of 0.844.\n  SALT used the hierarchical structures inherent in the human body to achieve\nwhole-body segmentations with an average of 35 seconds for 100 slices. This\nrapid processing underscores its potential for integration into clinical\nworkflows, facilitating the automatic and efficient computation of full-body\nsegmentations with each CT scan, thus enhancing diagnostic processes and\npatient care.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08878v1",
    "published_date": "2024-07-11 21:33:08 UTC",
    "updated_date": "2024-07-11 21:33:08 UTC"
  },
  {
    "arxiv_id": "2407.17436v2",
    "title": "AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies",
    "authors": [
      "Yi Zeng",
      "Yu Yang",
      "Andy Zhou",
      "Jeffrey Ziwei Tan",
      "Yuheng Tu",
      "Yifan Mai",
      "Kevin Klyman",
      "Minzhou Pan",
      "Ruoxi Jia",
      "Dawn Song",
      "Percy Liang",
      "Bo Li"
    ],
    "abstract": "Foundation models (FMs) provide societal benefits but also amplify risks.\nGovernments, companies, and researchers have proposed regulatory frameworks,\nacceptable use policies, and safety benchmarks in response. However, existing\npublic benchmarks often define safety categories based on previous literature,\nintuitions, or common sense, leading to disjointed sets of categories for risks\nspecified in recent regulations and policies, which makes it challenging to\nevaluate and compare FMs across these benchmarks. To bridge this gap, we\nintroduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging\ngovernment regulations and company policies, following the regulation-based\nsafety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes\n8 government regulations and 16 company policies into a four-tiered safety\ntaxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024\ncontains 5,694 diverse prompts spanning these categories, with manual curation\nand human auditing to ensure quality. We evaluate leading language models on\nAIR-Bench 2024, uncovering insights into their alignment with specified safety\nconcerns. By bridging the gap between public benchmarks and practical AI risks,\nAIR-Bench 2024 provides a foundation for assessing model safety across\njurisdictions, fostering the development of safer and more responsible AI\nsystems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.17436v2",
    "published_date": "2024-07-11 21:16:48 UTC",
    "updated_date": "2024-08-05 18:12:27 UTC"
  },
  {
    "arxiv_id": "2407.08867v3",
    "title": "Perceptions of Sentient AI and Other Digital Minds: Evidence from the AI, Morality, and Sentience (AIMS) Survey",
    "authors": [
      "Jacy Reese Anthis",
      "Janet V. T. Pauketat",
      "Ali Ladak",
      "Aikaterina Manoli"
    ],
    "abstract": "Humans now interact with a variety of digital minds, AI systems that appear\nto have mental faculties such as reasoning, emotion, and agency, and public\nfigures are discussing the possibility of sentient AI. We present initial\nresults from 2021 and 2023 for the nationally representative AI, Morality, and\nSentience (AIMS) survey (N = 3,500). Mind perception and moral concern for AI\nwelfare were surprisingly high and significantly increased: in 2023, one in\nfive U.S. adults believed some AI systems are currently sentient, and 38%\nsupported legal rights for sentient AI. People became more opposed to building\ndigital minds: in 2023, 63% supported banning smarter-than-human AI, and 69%\nsupported banning sentient AI. The median 2023 forecast was that sentient AI\nwould arrive in just five years. The development of safe and beneficial AI\nrequires not just technical study but understanding the complex ways in which\nhumans perceive and coexist with digital minds.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.08867v3",
    "published_date": "2024-07-11 21:04:39 UTC",
    "updated_date": "2025-03-10 17:10:28 UTC"
  },
  {
    "arxiv_id": "2407.08861v1",
    "title": "A Hybrid Spiking-Convolutional Neural Network Approach for Advancing Machine Learning Models",
    "authors": [
      "Sanaullah",
      "Kaushik Roy",
      "Ulrich Rückert",
      "Thorsten Jungeblut"
    ],
    "abstract": "In this article, we propose a novel standalone hybrid Spiking-Convolutional\nNeural Network (SC-NN) model and test on using image inpainting tasks. Our\napproach uses the unique capabilities of SNNs, such as event-based computation\nand temporal processing, along with the strong representation learning\nabilities of CNNs, to generate high-quality inpainted images. The model is\ntrained on a custom dataset specifically designed for image inpainting, where\nmissing regions are created using masks. The hybrid model consists of SNNConv2d\nlayers and traditional CNN layers. The SNNConv2d layers implement the leaky\nintegrate-and-fire (LIF) neuron model, capturing spiking behavior, while the\nCNN layers capture spatial features. In this study, a mean squared error (MSE)\nloss function demonstrates the training process, where a training loss value of\n0.015, indicates accurate performance on the training set and the model\nachieved a validation loss value as low as 0.0017 on the testing set.\nFurthermore, extensive experimental results demonstrate state-of-the-art\nperformance, showcasing the potential of integrating temporal dynamics and\nfeature extraction in a single network for image inpainting.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "7 Pages, 3 figures, and 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.08861v1",
    "published_date": "2024-07-11 20:50:33 UTC",
    "updated_date": "2024-07-11 20:50:33 UTC"
  },
  {
    "arxiv_id": "2407.08850v3",
    "title": "UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset",
    "authors": [
      "Peitong Duan",
      "Chin-yi Chen",
      "Gang Li",
      "Bjoern Hartmann",
      "Yang Li"
    ],
    "abstract": "Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to ACM UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08850v3",
    "published_date": "2024-07-11 20:18:19 UTC",
    "updated_date": "2024-08-13 23:41:43 UTC"
  },
  {
    "arxiv_id": "2407.08839v2",
    "title": "A Survey on the Application of Generative Adversarial Networks in Cybersecurity: Prospective, Direction and Open Research Scopes",
    "authors": [
      "Md Mashrur Arifin",
      "Md Shoaib Ahmed",
      "Tanmai Kumar Ghosh",
      "Ikteder Akhand Udoy",
      "Jun Zhuang",
      "Jyh-haw Yeh"
    ],
    "abstract": "With the proliferation of Artificial Intelligence, there has been a massive\nincrease in the amount of data required to be accumulated and disseminated\ndigitally. As the data are available online in digital landscapes with complex\nand sophisticated infrastructures, it is crucial to implement various defense\nmechanisms based on cybersecurity. Generative Adversarial Networks (GANs),\nwhich are deep learning models, have emerged as powerful solutions for\naddressing the constantly changing security issues. This survey studies the\nsignificance of the deep learning model, precisely on GANs, in strengthening\ncybersecurity defenses. Our survey aims to explore the various works completed\nin GANs, such as Intrusion Detection Systems (IDS), Mobile and Network\nTrespass, BotNet Detection, and Malware Detection. The focus is to examine how\nGANs can be influential tools to strengthen cybersecurity defenses in these\ndomains. Further, the paper discusses the challenges and constraints of using\nGANs in these areas and suggests future research directions. Overall, the paper\nhighlights the potential of GANs in enhancing cybersecurity measures and\naddresses the need for further exploration in this field.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08839v2",
    "published_date": "2024-07-11 19:51:48 UTC",
    "updated_date": "2024-09-20 01:27:34 UTC"
  },
  {
    "arxiv_id": "2407.08836v1",
    "title": "Fault Diagnosis in Power Grids with Large Language Model",
    "authors": [
      "Liu Jing",
      "Amirul Rahman"
    ],
    "abstract": "Power grid fault diagnosis is a critical task for ensuring the reliability\nand stability of electrical infrastructure. Traditional diagnostic systems\noften struggle with the complexity and variability of power grid data. This\npaper proposes a novel approach that leverages Large Language Models (LLMs),\nspecifically ChatGPT and GPT-4, combined with advanced prompt engineering to\nenhance fault diagnosis accuracy and explainability. We designed comprehensive,\ncontext-aware prompts to guide the LLMs in interpreting complex data and\nproviding detailed, actionable insights. Our method was evaluated against\nbaseline techniques, including standard prompting, Chain-of-Thought (CoT), and\nTree-of-Thought (ToT) methods, using a newly constructed dataset comprising\nreal-time sensor data, historical fault records, and component descriptions.\nExperimental results demonstrate significant improvements in diagnostic\naccuracy, explainability quality, response coherence, and contextual\nunderstanding, underscoring the effectiveness of our approach. These findings\nsuggest that prompt-engineered LLMs offer a promising solution for robust and\nreliable power grid fault diagnosis.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.08836v1",
    "published_date": "2024-07-11 19:44:18 UTC",
    "updated_date": "2024-07-11 19:44:18 UTC"
  },
  {
    "arxiv_id": "2407.08831v1",
    "title": "Neural Networks Meet Elliptic Curve Cryptography: A Novel Approach to Secure Communication",
    "authors": [
      "Mina Cecilie Wøien",
      "Ferhat Ozgur Catak",
      "Murat Kuzlu",
      "Umit Cali"
    ],
    "abstract": "In recent years, neural networks have been used to implement symmetric\ncryptographic functions for secure communications. Extending this domain, the\nproposed approach explores the application of asymmetric cryptography within a\nneural network framework to safeguard the exchange between two communicating\nentities, i.e., Alice and Bob, from an adversarial eavesdropper, i.e., Eve. It\nemploys a set of five distinct cryptographic keys to examine the efficacy and\nrobustness of communication security against eavesdropping attempts using the\nprinciples of elliptic curve cryptography. The experimental setup reveals that\nAlice and Bob achieve secure communication with negligible variation in\nsecurity effectiveness across different curves. It is also designed to evaluate\ncryptographic resilience. Specifically, the loss metrics for Bob oscillate\nbetween 0 and 1 during encryption-decryption processes, indicating successful\nmessage comprehension post-encryption by Alice. The potential vulnerability\nwith a decryption accuracy exceeds 60\\%, where Eve experiences enhanced\nadversarial training, receiving twice the training iterations per batch\ncompared to Alice and Bob.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.08831v1",
    "published_date": "2024-07-11 19:34:16 UTC",
    "updated_date": "2024-07-11 19:34:16 UTC"
  },
  {
    "arxiv_id": "2407.08824v1",
    "title": "Proving that Cryptic Crossword Clue Answers are Correct",
    "authors": [
      "Martin Andrews",
      "Sam Witteveen"
    ],
    "abstract": "Cryptic crossword clues are challenging cognitive tasks, for which new test\nsets are released on a daily basis by multiple international newspapers. Each\ncryptic clue contains both the definition of the answer to be placed in the\ncrossword grid (in common with regular crosswords), and `wordplay' that proves\nthat the answer is correct (i.e. a human solver can be confident that an answer\nis correct without needing crossing words to confirm it). Using an existing\ncryptic wordplay proving framework (operating on Python proofs created by an\nLLM), we show that it is possible to distinguish between correct answers and\nalmost-correct ones based upon whether the wordplay `works'.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted paper for the ICML 2024 Workshop on LLMs and Cognition (4\n  pages + references + 6 pages of Appendices)",
    "pdf_url": "http://arxiv.org/pdf/2407.08824v1",
    "published_date": "2024-07-11 19:13:16 UTC",
    "updated_date": "2024-07-11 19:13:16 UTC"
  },
  {
    "arxiv_id": "2407.08822v1",
    "title": "FedMedICL: Towards Holistic Evaluation of Distribution Shifts in Federated Medical Imaging",
    "authors": [
      "Kumail Alhamoud",
      "Yasir Ghunaim",
      "Motasem Alfarra",
      "Thomas Hartvigsen",
      "Philip Torr",
      "Bernard Ghanem",
      "Adel Bibi",
      "Marzyeh Ghassemi"
    ],
    "abstract": "For medical imaging AI models to be clinically impactful, they must\ngeneralize. However, this goal is hindered by (i) diverse types of distribution\nshifts, such as temporal, demographic, and label shifts, and (ii) limited\ndiversity in datasets that are siloed within single medical institutions. While\nthese limitations have spurred interest in federated learning, current\nevaluation benchmarks fail to evaluate different shifts simultaneously.\nHowever, in real healthcare settings, multiple types of shifts co-exist, yet\ntheir impact on medical imaging performance remains unstudied. In response, we\nintroduce FedMedICL, a unified framework and benchmark to holistically evaluate\nfederated medical imaging challenges, simultaneously capturing label,\ndemographic, and temporal distribution shifts. We comprehensively evaluate\nseveral popular methods on six diverse medical imaging datasets (totaling 550\nGPU hours). Furthermore, we use FedMedICL to simulate COVID-19 propagation\nacross hospitals and evaluate whether methods can adapt to pandemic changes in\ndisease prevalence. We find that a simple batch balancing technique surpasses\nadvanced methods in average performance across FedMedICL experiments. This\nfinding questions the applicability of results from previous, narrow benchmarks\nin real-world medical settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted at MICCAI 2024. Code is available at:\n  https://github.com/m1k2zoo/FedMedICL",
    "pdf_url": "http://arxiv.org/pdf/2407.08822v1",
    "published_date": "2024-07-11 19:12:23 UTC",
    "updated_date": "2024-07-11 19:12:23 UTC"
  },
  {
    "arxiv_id": "2407.08813v2",
    "title": "FairDomain: Achieving Fairness in Cross-Domain Medical Image Segmentation and Classification",
    "authors": [
      "Yu Tian",
      "Congcong Wen",
      "Min Shi",
      "Muhammad Muneeb Afzal",
      "Hao Huang",
      "Muhammad Osama Khan",
      "Yan Luo",
      "Yi Fang",
      "Mengyu Wang"
    ],
    "abstract": "Addressing fairness in artificial intelligence (AI), particularly in medical\nAI, is crucial for ensuring equitable healthcare outcomes. Recent efforts to\nenhance fairness have introduced new methodologies and datasets in medical AI.\nHowever, the fairness issue under the setting of domain transfer is almost\nunexplored, while it is common that clinics rely on different imaging\ntechnologies (e.g., different retinal imaging modalities) for patient\ndiagnosis. This paper presents FairDomain, a pioneering systemic study into\nalgorithmic fairness under domain shifts, employing state-of-the-art domain\nadaptation (DA) and generalization (DG) algorithms for both medical\nsegmentation and classification tasks to understand how biases are transferred\nbetween different domains. We also introduce a novel plug-and-play fair\nidentity attention (FIA) module that adapts to various DA and DG algorithms to\nimprove fairness by using self-attention to adjust feature importance based on\ndemographic attributes. Additionally, we curate the first fairness-focused\ndataset with two paired imaging modalities for the same patient cohort on\nmedical segmentation and classification tasks, to rigorously assess fairness in\ndomain-shift scenarios. Excluding the confounding impact of demographic\ndistribution variation between source and target domains will allow clearer\nquantification of the performance of domain transfer models. Our extensive\nevaluations reveal that the proposed FIA significantly enhances both model\nperformance accounted for fairness across all domain shift settings (i.e., DA\nand DG) with respect to different demographics, which outperforms existing\nmethods on both segmentation and classification. The code and data can be\naccessed at https://ophai.hms.harvard.edu/datasets/harvard-fairdomain20k.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "ECCV 2024; Codes and datasets are available at\n  https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain",
    "pdf_url": "http://arxiv.org/pdf/2407.08813v2",
    "published_date": "2024-07-11 18:52:32 UTC",
    "updated_date": "2024-07-18 20:30:28 UTC"
  },
  {
    "arxiv_id": "2407.08803v2",
    "title": "PID Accelerated Temporal Difference Algorithms",
    "authors": [
      "Mark Bedaywi",
      "Amin Rakhsha",
      "Amir-massoud Farahmand"
    ],
    "abstract": "Long-horizon tasks, which have a large discount factor, pose a challenge for\nmost conventional reinforcement learning (RL) algorithms. Algorithms such as\nValue Iteration and Temporal Difference (TD) learning have a slow convergence\nrate and become inefficient in these tasks. When the transition distributions\nare given, PID VI was recently introduced to accelerate the convergence of\nValue Iteration using ideas from control theory. Inspired by this, we introduce\nPID TD Learning and PID Q-Learning algorithms for the RL setting, in which only\nsamples from the environment are available. We give a theoretical analysis of\nthe convergence of PID TD Learning and its acceleration compared to the\nconventional TD Learning. We also introduce a method for adapting PID gains in\nthe presence of noise and empirically verify its effectiveness.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08803v2",
    "published_date": "2024-07-11 18:23:46 UTC",
    "updated_date": "2024-09-03 16:59:07 UTC"
  },
  {
    "arxiv_id": "2407.08737v1",
    "title": "Video Diffusion Alignment via Reward Gradients",
    "authors": [
      "Mihir Prabhudesai",
      "Russell Mendonca",
      "Zheyang Qin",
      "Katerina Fragkiadaki",
      "Deepak Pathak"
    ],
    "abstract": "We have made significant progress towards building foundational video\ndiffusion models. As these models are trained using large-scale unsupervised\ndata, it has become crucial to adapt these models to specific downstream tasks.\nAdapting these models via supervised fine-tuning requires collecting target\ndatasets of videos, which is challenging and tedious. In this work, we utilize\npre-trained reward models that are learned via preferences on top of powerful\nvision discriminative models to adapt video diffusion models. These models\ncontain dense gradient information with respect to generated RGB pixels, which\nis critical to efficient learning in complex search spaces, such as videos. We\nshow that backpropagating gradients from these reward models to a video\ndiffusion model can allow for compute and sample efficient alignment of the\nvideo diffusion model. We show results across a variety of reward models and\nvideo diffusion models, demonstrating that our approach can learn much more\nefficiently in terms of reward queries and computation than prior gradient-free\napproaches. Our code, model weights,and more visualization are available at\nhttps://vader-vid.github.io.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Webpage: https://vader-vid.github.io; Code available at:\n  https://github.com/mihirp1998/VADER",
    "pdf_url": "http://arxiv.org/pdf/2407.08737v1",
    "published_date": "2024-07-11 17:59:45 UTC",
    "updated_date": "2024-07-11 17:59:45 UTC"
  },
  {
    "arxiv_id": "2407.08735v1",
    "title": "Real-Time Anomaly Detection and Reactive Planning with Large Language Models",
    "authors": [
      "Rohan Sinha",
      "Amine Elhafsi",
      "Christopher Agia",
      "Matthew Foutter",
      "Edward Schmerling",
      "Marco Pavone"
    ],
    "abstract": "Foundation models, e.g., large language models (LLMs), trained on\ninternet-scale data possess zero-shot generalization capabilities that make\nthem a promising technology towards detecting and mitigating\nout-of-distribution failure modes of robotic systems. Fully realizing this\npromise, however, poses two challenges: (i) mitigating the considerable\ncomputational expense of these models such that they may be applied online, and\n(ii) incorporating their judgement regarding potential anomalies into a safe\ncontrol framework. In this work, we present a two-stage reasoning framework:\nFirst is a fast binary anomaly classifier that analyzes observations in an LLM\nembedding space, which may then trigger a slower fallback selection stage that\nutilizes the reasoning capabilities of generative LLMs. These stages correspond\nto branch points in a model predictive control strategy that maintains the\njoint feasibility of continuing along various fallback plans to account for the\nslow reasoner's latency as soon as an anomaly is detected, thus ensuring\nsafety. We show that our fast anomaly classifier outperforms autoregressive\nreasoning with state-of-the-art GPT models, even when instantiated with\nrelatively small language models. This enables our runtime monitor to improve\nthe trustworthiness of dynamic robotic systems, such as quadrotors or\nautonomous vehicles, under resource and time constraints. Videos illustrating\nour approach in both simulation and real-world experiments are available on\nthis project page: https://sites.google.com/view/aesop-llm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to Robotics: Science and Systems (RSS) 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08735v1",
    "published_date": "2024-07-11 17:59:22 UTC",
    "updated_date": "2024-07-11 17:59:22 UTC"
  },
  {
    "arxiv_id": "2407.08734v1",
    "title": "Transformer Circuit Faithfulness Metrics are not Robust",
    "authors": [
      "Joseph Miller",
      "Bilal Chughtai",
      "William Saunders"
    ],
    "abstract": "Mechanistic interpretability work attempts to reverse engineer the learned\nalgorithms present inside neural networks. One focus of this work has been to\ndiscover 'circuits' -- subgraphs of the full model that explain behaviour on\nspecific tasks. But how do we measure the performance of such circuits? Prior\nwork has attempted to measure circuit 'faithfulness' -- the degree to which the\ncircuit replicates the performance of the full model. In this work, we survey\nmany considerations for designing experiments that measure circuit faithfulness\nby ablating portions of the model's computation. Concerningly, we find existing\nmethods are highly sensitive to seemingly insignificant changes in the ablation\nmethodology. We conclude that existing circuit faithfulness scores reflect both\nthe methodological choices of researchers as well as the actual components of\nthe circuit - the task a circuit is required to perform depends on the ablation\nused to test it. The ultimate goal of mechanistic interpretability work is to\nunderstand neural networks, so we emphasize the need for more clarity in the\nprecise claims being made about circuits. We open source a library at\nhttps://github.com/UFO-101/auto-circuit that includes highly efficient\nimplementations of a wide range of ablation methodologies and circuit discovery\nalgorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "CoLM 2024 Conference Paper. 11 page main body. 11 page appendix. 12\n  figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08734v1",
    "published_date": "2024-07-11 17:59:00 UTC",
    "updated_date": "2024-07-11 17:59:00 UTC"
  },
  {
    "arxiv_id": "2407.08725v2",
    "title": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility",
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Jack He",
      "Yiran Wang",
      "Chenda Duan",
      "Zhizheng Liu",
      "Quanyi Li",
      "Bolei Zhou"
    ],
    "abstract": "Public urban spaces like streetscapes and plazas serve residents and\naccommodate social life in all its vibrant variations. Recent advances in\nRobotics and Embodied AI make public urban spaces no longer exclusive to\nhumans. Food delivery bots and electric wheelchairs have started sharing\nsidewalks with pedestrians, while robot dogs and humanoids have recently\nemerged in the street. Micromobility enabled by AI for short-distance travel in\npublic urban spaces plays a crucial component in the future transportation\nsystem. Ensuring the generalizability and safety of AI models maneuvering\nmobile machines is essential. In this work, we present MetaUrban, a\ncompositional simulation platform for the AI-driven urban micromobility\nresearch. MetaUrban can construct an infinite number of interactive urban\nscenes from compositional elements, covering a vast array of ground plans,\nobject placements, pedestrians, vulnerable road users, and other mobile agents'\nappearances and dynamics. We design point navigation and social navigation\ntasks as the pilot study using MetaUrban for urban micromobility research and\nestablish various baselines of Reinforcement Learning and Imitation Learning.\nWe conduct extensive evaluation across mobile machines, demonstrating that\nheterogeneous mechanical structures significantly influence the learning and\nexecution of AI policies. We perform a thorough ablation study, showing that\nthe compositional nature of the simulated environments can substantially\nimprove the generalizability and safety of the trained mobile agents. MetaUrban\nwill be made publicly available to provide research opportunities and foster\nsafe and trustworthy embodied AI and micromobility in cities. The code and\ndataset will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report. Project page:\n  https://metadriverse.github.io/metaurban/",
    "pdf_url": "http://arxiv.org/pdf/2407.08725v2",
    "published_date": "2024-07-11 17:56:49 UTC",
    "updated_date": "2024-10-11 09:41:17 UTC"
  },
  {
    "arxiv_id": "2407.08770v2",
    "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
    "authors": [
      "Huanqian Wang",
      "Yang Yue",
      "Rui Lu",
      "Jingxin Shi",
      "Andrew Zhao",
      "Shenzhi Wang",
      "Shiji Song",
      "Gao Huang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated great potential as generalist\nassistants, showcasing powerful task understanding and problem-solving\ncapabilities. To deploy LLMs as AI assistants, it is crucial that these models\nexhibit desirable behavioral traits, such as non-toxicity and resilience\nagainst jailbreak attempts. Current approaches for detoxification or preventing\njailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement\nLearning from Human Feedback (RLHF), which requires finetuning billions of\nparameters through gradient descent with substantial computational cost.\nFurthermore, models modified through SFT and RLHF may deviate from the\npretrained models, potentially leading to a degradation in foundational LLM\ncapabilities. In this paper, we observe that surprisingly, directly editing a\nsmall subset of parameters can effectively modulate specific behaviors of LLMs,\nsuch as detoxification and resistance to jailbreaking, with only\ninference-level computational resources. Experiments demonstrate that in the\ndetoxification task, our approach achieves reductions of up to 90.0% in\ntoxicity on the RealToxicityPrompts dataset and 49.2% on ToxiGen, while\nmaintaining the LLM's general capabilities in areas such as common sense,\nquestion answering, and mathematics",
    "categories": [
      "cs.AI",
      "68T50 (Primary) 68T07, 62M45 (Secondary)",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 14 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08770v2",
    "published_date": "2024-07-11 17:52:03 UTC",
    "updated_date": "2025-02-11 15:39:08 UTC"
  },
  {
    "arxiv_id": "2407.08717v1",
    "title": "WhisperNetV2: SlowFast Siamese Network For Lip-Based Biometrics",
    "authors": [
      "Abdollah Zakeri",
      "Hamid Hassanpour",
      "Mohammad Hossein Khosravi",
      "Amir Masoud Nourollah"
    ],
    "abstract": "Lip-based biometric authentication (LBBA) has attracted many researchers\nduring the last decade. The lip is specifically interesting for biometric\nresearchers because it is a twin biometric with the potential to function both\nas a physiological and a behavioral trait. Although much valuable research was\nconducted on LBBA, none of them considered the different emotions of the client\nduring the video acquisition step of LBBA, which can potentially affect the\nclient's facial expressions and speech tempo. We proposed a novel network\nstructure called WhisperNetV2, which extends our previously proposed network\ncalled WhisperNet. Our proposed network leverages a deep Siamese structure with\ntriplet loss having three identical SlowFast networks as embedding networks.\nThe SlowFast network is an excellent candidate for our task since the fast\npathway extracts motion-related features (behavioral lip movements) with a high\nframe rate and low channel capacity. The slow pathway extracts visual features\n(physiological lip appearance) with a low frame rate and high channel capacity.\nUsing an open-set protocol, we trained our network using the CREMA-D dataset\nand acquired an Equal Error Rate (EER) of 0.005 on the test set. Considering\nthat the acquired EER is less than most similar LBBA methods, our method can be\nconsidered as a state-of-the-art LBBA method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08717v1",
    "published_date": "2024-07-11 17:51:49 UTC",
    "updated_date": "2024-07-11 17:51:49 UTC"
  },
  {
    "arxiv_id": "2407.08713v2",
    "title": "GTA: A Benchmark for General Tool Agents",
    "authors": [
      "Jize Wang",
      "Zerun Ma",
      "Yining Li",
      "Songyang Zhang",
      "Cailian Chen",
      "Kai Chen",
      "Xinyi Le"
    ],
    "abstract": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Github repo: https://github.com/open-compass/GTA",
    "pdf_url": "http://arxiv.org/pdf/2407.08713v2",
    "published_date": "2024-07-11 17:50:09 UTC",
    "updated_date": "2024-11-22 11:25:34 UTC"
  },
  {
    "arxiv_id": "2407.08708v2",
    "title": "eyeballvul: a future-proof benchmark for vulnerability detection in the wild",
    "authors": [
      "Timothee Chauvin"
    ],
    "abstract": "Long contexts of recent LLMs have enabled a new use case: asking models to\nfind security vulnerabilities in entire codebases. To evaluate model\nperformance on this task, we introduce eyeballvul: a benchmark designed to test\nthe vulnerability detection capabilities of language models at scale, that is\nsourced and updated weekly from the stream of published vulnerabilities in\nopen-source repositories. The benchmark consists of a list of revisions in\ndifferent repositories, each associated with the list of known vulnerabilities\npresent at that revision. An LLM-based scorer is used to compare the list of\npossible vulnerabilities returned by a model to the list of known\nvulnerabilities for each revision. As of July 2024, eyeballvul contains 24,000+\nvulnerabilities across 6,000+ revisions and 5,000+ repositories, and is around\n55GB in size.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Due to a bug in the litellm library (we haven't tracked exactly which\n  one, but probably at least\n  https://github.com/BerriAI/litellm/commit/2452753e084e8134c0c484b32c63fb5f2950c5ba),\n  our Gemini 1.5 Pro inference costs were incorrect. We've updated the relevant\n  plot (Fig 7) and its interpretation (both Gemini 1.5 Pro and Claude 3.5\n  Sonnet stand out from the other models, not just Gemini 1.5 Pro)",
    "pdf_url": "http://arxiv.org/pdf/2407.08708v2",
    "published_date": "2024-07-11 17:46:21 UTC",
    "updated_date": "2024-07-13 10:44:58 UTC"
  },
  {
    "arxiv_id": "2407.08704v1",
    "title": "Towards Efficient Deployment of Hybrid SNNs on Neuromorphic and Edge AI Hardware",
    "authors": [
      "James Seekings",
      "Peyton Chandarana",
      "Mahsa Ardakani",
      "MohammadReza Mohammadi",
      "Ramtin Zand"
    ],
    "abstract": "This paper explores the synergistic potential of neuromorphic and edge\ncomputing to create a versatile machine learning (ML) system tailored for\nprocessing data captured by dynamic vision sensors. We construct and train\nhybrid models, blending spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture\nintegrates an SNN for temporal feature extraction and an ANN for\nclassification. We delve into the challenges of deploying such hybrid\nstructures on hardware. Specifically, we deploy individual components on\nIntel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We\nalso propose an accumulator circuit to transfer data from the spiking to the\nnon-spiking domain. Furthermore, we conduct comprehensive performance analyses\nof hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI\nhardware, evaluating accuracy, latency, power, and energy consumption. Our\nfindings demonstrate that the hybrid spiking networks surpass the baseline ANN\nmodel across all metrics and outperform the baseline SNN model in accuracy and\nlatency.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08704v1",
    "published_date": "2024-07-11 17:40:39 UTC",
    "updated_date": "2024-07-11 17:40:39 UTC"
  },
  {
    "arxiv_id": "2407.08700v1",
    "title": "Flex-TPU: A Flexible TPU with Runtime Reconfigurable Dataflow Architecture",
    "authors": [
      "Mohammed Elbtity",
      "Peyton Chandarana",
      "Ramtin Zand"
    ],
    "abstract": "Tensor processing units (TPUs) are one of the most well-known machine\nlearning (ML) accelerators utilized at large scale in data centers as well as\nin tiny ML applications. TPUs offer several improvements and advantages over\nconventional ML accelerators, like graphical processing units (GPUs), being\ndesigned specifically to perform the multiply-accumulate (MAC) operations\nrequired in the matrix-matrix and matrix-vector multiplies extensively present\nthroughout the execution of deep neural networks (DNNs). Such improvements\ninclude maximizing data reuse and minimizing data transfer by leveraging the\ntemporal dataflow paradigms provided by the systolic array architecture. While\nthis design provides a significant performance benefit, the current\nimplementations are restricted to a single dataflow consisting of either input,\noutput, or weight stationary architectures. This can limit the achievable\nperformance of DNN inference and reduce the utilization of compute units.\nTherefore, the work herein consists of developing a reconfigurable dataflow\nTPU, called the Flex-TPU, which can dynamically change the dataflow per layer\nduring run-time. Our experiments thoroughly test the viability of the Flex-TPU\ncomparing it to conventional TPU designs across multiple well-known ML\nworkloads. The results show that our Flex-TPU design achieves a significant\nperformance increase of up to 2.75x compared to conventional TPU, with only\nminor area and power overheads.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08700v1",
    "published_date": "2024-07-11 17:33:38 UTC",
    "updated_date": "2024-07-11 17:33:38 UTC"
  },
  {
    "arxiv_id": "2407.08694v1",
    "title": "Cloud Atlas: Efficient Fault Localization for Cloud Systems using Language Models and Causal Insight",
    "authors": [
      "Zhiqiang Xie",
      "Yujia Zheng",
      "Lizi Ottens",
      "Kun Zhang",
      "Christos Kozyrakis",
      "Jonathan Mace"
    ],
    "abstract": "Runtime failure and performance degradation is commonplace in modern cloud\nsystems. For cloud providers, automatically determining the root cause of\nincidents is paramount to ensuring high reliability and availability as prompt\nfault localization can enable faster diagnosis and triage for timely\nresolution. A compelling solution explored in recent work is causal reasoning\nusing causal graphs to capture relationships between varied cloud system\nperformance metrics. To be effective, however, systems developers must\ncorrectly define the causal graph of their system, which is a time-consuming,\nbrittle, and challenging task that increases in difficulty for large and\ndynamic systems and requires domain expertise. Alternatively, automated\ndata-driven approaches have limited efficacy for cloud systems due to the\ninherent rarity of incidents. In this work, we present Atlas, a novel approach\nto automatically synthesizing causal graphs for cloud systems. Atlas leverages\nlarge language models (LLMs) to generate causal graphs using system\ndocumentation, telemetry, and deployment feedback. Atlas is complementary to\ndata-driven causal discovery techniques, and we further enhance Atlas with a\ndata-driven validation step. We evaluate Atlas across a range of fault\nlocalization scenarios and demonstrate that Atlas is capable of generating\ncausal graphs in a scalable and generalizable manner, with performance that far\nsurpasses that of data-driven algorithms and is commensurate to the\nground-truth baseline.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08694v1",
    "published_date": "2024-07-11 17:31:12 UTC",
    "updated_date": "2024-07-11 17:31:12 UTC"
  },
  {
    "arxiv_id": "2407.08691v1",
    "title": "ElasticAST: An Audio Spectrogram Transformer for All Length and Resolutions",
    "authors": [
      "Jiu Feng",
      "Mehmet Hamza Erol",
      "Joon Son Chung",
      "Arda Senocak"
    ],
    "abstract": "Transformers have rapidly overtaken CNN-based architectures as the new\nstandard in audio classification. Transformer-based models, such as the Audio\nSpectrogram Transformers (AST), also inherit the fixed-size input paradigm from\nCNNs. However, this leads to performance degradation for ASTs in the inference\nwhen input lengths vary from the training. This paper introduces an approach\nthat enables the use of variable-length audio inputs with AST models during\nboth training and inference. By employing sequence packing, our method\nElasticAST, accommodates any audio length during training, thereby offering\nflexibility across all lengths and resolutions at the inference. This\nflexibility allows ElasticAST to maintain evaluation capabilities at various\nlengths or resolutions and achieve similar performance to standard ASTs trained\nat specific lengths or resolutions. Moreover, experiments demonstrate\nElasticAST's better performance when trained and evaluated on native-length\naudio datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Interspeech 2024. Code is available at\n  https://github.com/JiuFengSC/ElasticAST",
    "pdf_url": "http://arxiv.org/pdf/2407.08691v1",
    "published_date": "2024-07-11 17:29:56 UTC",
    "updated_date": "2024-07-11 17:29:56 UTC"
  },
  {
    "arxiv_id": "2407.08689v1",
    "title": "Operationalizing the Blueprint for an AI Bill of Rights: Recommendations for Practitioners, Researchers, and Policy Makers",
    "authors": [
      "Alex Oesterling",
      "Usha Bhalla",
      "Suresh Venkatasubramanian",
      "Himabindu Lakkaraju"
    ],
    "abstract": "As Artificial Intelligence (AI) tools are increasingly employed in diverse\nreal-world applications, there has been significant interest in regulating\nthese tools. To this end, several regulatory frameworks have been introduced by\ndifferent countries worldwide. For example, the European Union recently passed\nthe AI Act, the White House issued an Executive Order on safe, secure, and\ntrustworthy AI, and the White House Office of Science and Technology Policy\nissued the Blueprint for an AI Bill of Rights (AI BoR). Many of these\nframeworks emphasize the need for auditing and improving the trustworthiness of\nAI tools, underscoring the importance of safety, privacy, explainability,\nfairness, and human fallback options. Although these regulatory frameworks\nhighlight the necessity of enforcement, practitioners often lack detailed\nguidance on implementing them. Furthermore, the extensive research on\noperationalizing each of these aspects is frequently buried in technical papers\nthat are difficult for practitioners to parse. In this write-up, we address\nthis shortcoming by providing an accessible overview of existing literature\nrelated to operationalizing regulatory principles. We provide\neasy-to-understand summaries of state-of-the-art literature and highlight\nvarious gaps that exist between regulatory guidelines and existing AI research,\nincluding the trade-offs that emerge during operationalization. We hope that\nthis work not only serves as a starting point for practitioners interested in\nlearning more about operationalizing the regulatory guidelines outlined in the\nBlueprint for an AI BoR but also provides researchers with a list of critical\nopen problems and gaps between regulations and state-of-the-art AI research.\nFinally, we note that this is a working paper and we invite feedback in line\nwith the purpose of this document as described in the introduction.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.08689v1",
    "published_date": "2024-07-11 17:28:07 UTC",
    "updated_date": "2024-07-11 17:28:07 UTC"
  },
  {
    "arxiv_id": "2407.08675v2",
    "title": "CAD-Prompted Generative Models: A Pathway to Feasible and Novel Engineering Designs",
    "authors": [
      "Leah Chong",
      "Jude Rayan",
      "Steven Dow",
      "Ioanna Lykourentzou",
      "Faez Ahmed"
    ],
    "abstract": "Text-to-image generative models have increasingly been used to assist\ndesigners during concept generation in various creative domains, such as\ngraphic design, user interface design, and fashion design. However, their\napplications in engineering design remain limited due to the models' challenges\nin generating images of feasible designs concepts. To address this issue, this\npaper introduces a method that improves the design feasibility by prompting the\ngeneration with feasible CAD images. In this work, the usefulness of this\nmethod is investigated through a case study with a bike design task using an\noff-the-shelf text-to-image model, Stable Diffusion 2.1. A diverse set of bike\ndesigns are produced in seven different generation settings with varying CAD\nimage prompting weights, and these designs are evaluated on their perceived\nfeasibility and novelty. Results demonstrate that the CAD image prompting\nsuccessfully helps text-to-image models like Stable Diffusion 2.1 create\nvisibly more feasible design images. While a general tradeoff is observed\nbetween feasibility and novelty, when the prompting weight is kept low around\n0.35, the design feasibility is significantly improved while its novelty\nremains on par with those generated by text prompts alone. The insights from\nthis case study offer some guidelines for selecting the appropriate CAD image\nprompting weight for different stages of the engineering design process. When\nutilized effectively, our CAD image prompting method opens doors to a wider\nrange of applications of text-to-image models in engineering design.",
    "categories": [
      "cs.AI",
      "I.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 3 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2407.08675v2",
    "published_date": "2024-07-11 17:07:32 UTC",
    "updated_date": "2024-07-22 06:49:45 UTC"
  },
  {
    "arxiv_id": "2407.08662v1",
    "title": "Uncertainty Estimation of Large Language Models in Medical Question Answering",
    "authors": [
      "Jiaxin Wu",
      "Yizhou Yu",
      "Hong-Yu Zhou"
    ],
    "abstract": "Large Language Models (LLMs) show promise for natural language generation in\nhealthcare, but risk hallucinating factually incorrect information. Deploying\nLLMs for medical question answering necessitates reliable uncertainty\nestimation (UE) methods to detect hallucinations. In this work, we benchmark\npopular UE methods with different model sizes on medical question-answering\ndatasets. Our results show that current approaches generally perform poorly in\nthis domain, highlighting the challenge of UE for medical applications. We also\nobserve that larger models tend to yield better results, suggesting a\ncorrelation between model size and the reliability of UE. To address these\nchallenges, we propose Two-phase Verification, a probability-free Uncertainty\nEstimation approach. First, an LLM generates a step-by-step explanation\nalongside its initial answer, followed by formulating verification questions to\ncheck the factual claims in the explanation. The model then answers these\nquestions twice: first independently, and then referencing the explanation.\nInconsistencies between the two sets of answers measure the uncertainty in the\noriginal response. We evaluate our approach on three biomedical\nquestion-answering datasets using Llama 2 Chat models and compare it against\nthe benchmarked baseline methods. The results show that our Two-phase\nVerification method achieves the best overall accuracy and stability across\nvarious datasets and model sizes, and its performance scales as the model size\nincreases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08662v1",
    "published_date": "2024-07-11 16:51:33 UTC",
    "updated_date": "2024-07-11 16:51:33 UTC"
  },
  {
    "arxiv_id": "2407.08655v1",
    "title": "SPOCKMIP: Segmentation of Vessels in MRAs with Enhanced Continuity using Maximum Intensity Projection as Loss",
    "authors": [
      "Chethan Radhakrishna",
      "Karthikesh Varma Chintalapati",
      "Sri Chandana Hudukula Ram Kumar",
      "Raviteja Sutrave",
      "Hendrik Mattern",
      "Oliver Speck",
      "Andreas Nürnberger",
      "Soumick Chatterjee"
    ],
    "abstract": "Identification of vessel structures of different sizes in biomedical images\nis crucial in the diagnosis of many neurodegenerative diseases. However, the\nsparsity of good-quality annotations of such images makes the task of vessel\nsegmentation challenging. Deep learning offers an efficient way to segment\nvessels of different sizes by learning their high-level feature representations\nand the spatial continuity of such features across dimensions. Semi-supervised\npatch-based approaches have been effective in identifying small vessels of one\nto two voxels in diameter. This study focuses on improving the segmentation\nquality by considering the spatial correlation of the features using the\nMaximum Intensity Projection~(MIP) as an additional loss criterion. Two methods\nare proposed with the incorporation of MIPs of label segmentation on the\nsingle~(z-axis) and multiple perceivable axes of the 3D volume. The proposed\nMIP-based methods produce segmentations with improved vessel continuity, which\nis evident in visual examinations of ROIs. Patch-based training is improved by\nintroducing an additional loss term, MIP loss, to penalise the predicted\ndiscontinuity of vessels. A training set of 14 volumes is selected from the\nStudyForrest dataset comprising of 18 7-Tesla 3D Time-of-Flight~(ToF) Magnetic\nResonance Angiography (MRA) images. The generalisation performance of the\nmethod is evaluated using the other unseen volumes in the dataset. It is\nobserved that the proposed method with multi-axes MIP loss produces better\nquality segmentations with a median Dice of $80.245 \\pm 0.129$. Also, the\nmethod with single-axis MIP loss produces segmentations with a median Dice of\n$79.749 \\pm 0.109$. Furthermore, a visual comparison of the ROIs in the\npredicted segmentation reveals a significant improvement in the continuity of\nthe vessels when MIP loss is incorporated into training.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08655v1",
    "published_date": "2024-07-11 16:39:24 UTC",
    "updated_date": "2024-07-11 16:39:24 UTC"
  },
  {
    "arxiv_id": "2407.09573v1",
    "title": "Have We Reached AGI? Comparing ChatGPT, Claude, and Gemini to Human Literacy and Education Benchmarks",
    "authors": [
      "Mfon Akpan"
    ],
    "abstract": "Recent advancements in AI, particularly in large language models (LLMs) like\nChatGPT, Claude, and Gemini, have prompted questions about their proximity to\nArtificial General Intelligence (AGI). This study compares LLM performance on\neducational benchmarks with Americans' average educational attainment and\nliteracy levels, using data from the U.S. Census Bureau and technical reports.\nResults show that LLMs significantly outperform human benchmarks in tasks such\nas undergraduate knowledge and advanced reading comprehension, indicating\nsubstantial progress toward AGI. However, true AGI requires broader cognitive\nassessments. The study highlights the implications for AI development,\neducation, and societal impact, emphasizing the need for ongoing research and\nethical considerations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.09573v1",
    "published_date": "2024-07-11 16:38:40 UTC",
    "updated_date": "2024-07-11 16:38:40 UTC"
  },
  {
    "arxiv_id": "2407.08649v2",
    "title": "Confidence-based Estimators for Predictive Performance in Model Monitoring",
    "authors": [
      "Juhani Kivimäki",
      "Jakub Białek",
      "Jukka K. Nurminen",
      "Wojtek Kuberski"
    ],
    "abstract": "After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]",
    "pdf_url": "http://arxiv.org/pdf/2407.08649v2",
    "published_date": "2024-07-11 16:28:31 UTC",
    "updated_date": "2025-02-12 14:49:16 UTC"
  },
  {
    "arxiv_id": "2407.08639v2",
    "title": "$β$-DPO: Direct Preference Optimization with Dynamic $β$",
    "authors": [
      "Junkang Wu",
      "Yuexiang Xie",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Jinyang Gao",
      "Bolin Ding",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as a compelling approach for\ntraining Large Language Models (LLMs) to adhere to human preferences. However,\nthe performance of DPO is sensitive to the fine-tuning of its trade-off\nparameter $\\beta$, as well as to the quality of the preference data. We analyze\nthe impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$\nvalues vary with the informativeness of pairwise data. Addressing the\nlimitations of static $\\beta$ values, we introduce a novel framework that\ndynamically calibrates $\\beta$ at the batch level, informed by data quality\nconsiderations. Additionally, our method incorporates $\\beta$-guided data\nfiltering to safeguard against the influence of outliers. Through empirical\nevaluation, we demonstrate that our dynamic $\\beta$ adjustment technique\nsignificantly improves DPO's performance across a range of models and datasets,\noffering a more robust and adaptable training paradigm for aligning LLMs with\nhuman feedback. The code is available at\n\\url{https://github.com/junkangwu/beta-DPO}.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08639v2",
    "published_date": "2024-07-11 16:21:18 UTC",
    "updated_date": "2024-10-13 08:53:00 UTC"
  },
  {
    "arxiv_id": "2407.08633v2",
    "title": "A Novel Framework for Automated Warehouse Layout Generation",
    "authors": [
      "Atefeh Shahroudnejad",
      "Payam Mousavi",
      "Oleksii Perepelytsia",
      "Sahir",
      "David Staszak",
      "Matthew E. Taylor",
      "Brent Bawel"
    ],
    "abstract": "Optimizing warehouse layouts is crucial due to its significant impact on\nefficiency and productivity. We present an AI-driven framework for automated\nwarehouse layout generation. This framework employs constrained beam search to\nderive optimal layouts within given spatial parameters, adhering to all\nfunctional requirements. The feasibility of the generated layouts is verified\nbased on criteria such as item accessibility, required minimum clearances, and\naisle connectivity. A scoring function is then used to evaluate the feasible\nlayouts considering the number of storage locations, access points, and\naccessibility costs. We demonstrate our method's ability to produce feasible,\noptimal layouts for a variety of warehouse dimensions and shapes, diverse door\nplacements, and interconnections. This approach, currently being prepared for\ndeployment, will enable human designers to rapidly explore and confirm options,\nfacilitating the selection of the most appropriate layout for their use-case.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08633v2",
    "published_date": "2024-07-11 16:15:09 UTC",
    "updated_date": "2024-07-12 19:06:45 UTC"
  },
  {
    "arxiv_id": "2407.08608v2",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
    "authors": [
      "Jay Shah",
      "Ganesh Bikshandi",
      "Ying Zhang",
      "Vijay Thakkar",
      "Pradeep Ramani",
      "Tri Dao"
    ],
    "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the\nbottleneck for large language models and long-context applications.\nFlashAttention elaborated an approach to speed up attention on GPUs through\nminimizing memory reads/writes. However, it has yet to take advantage of new\ncapabilities present in recent hardware, with FlashAttention-2 achieving only\n35% utilization on the H100 GPU. We develop three main techniques to speed up\nattention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to\n(1) overlap overall computation and data movement via warp-specialization and\n(2) interleave block-wise matmul and softmax operations, and (3) block\nquantization and incoherent processing that leverages hardware support for FP8\nlow-precision. We demonstrate that our method, FlashAttention-3, achieves\nspeedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s\n(75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate\nthat FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a\nbaseline FP8 attention.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08608v2",
    "published_date": "2024-07-11 15:44:48 UTC",
    "updated_date": "2024-07-12 22:15:02 UTC"
  },
  {
    "arxiv_id": "2407.08590v2",
    "title": "A Review of Nine Physics Engines for Reinforcement Learning Research",
    "authors": [
      "Michael Kaup",
      "Cornelius Wolff",
      "Hyerim Hwang",
      "Julius Mayer",
      "Elia Bruni"
    ],
    "abstract": "We present a review of popular simulation engines and frameworks used in\nreinforcement learning (RL) research, aiming to guide researchers in selecting\ntools for creating simulated physical environments for RL and training setups.\nIt evaluates nine frameworks (Brax, Chrono, Gazebo, MuJoCo, ODE, PhysX,\nPyBullet, Webots, and Unity) based on their popularity, feature range, quality,\nusability, and RL capabilities. We highlight the challenges in selecting and\nutilizing physics engines for RL research, including the need for detailed\ncomparisons and an understanding of each framework's capabilities. Key findings\nindicate MuJoCo as the leading framework due to its performance and\nflexibility, despite usability challenges. Unity is noted for its ease of use\nbut lacks scalability and simulation fidelity. The study calls for further\ndevelopment to improve simulation engines' usability and performance and\nstresses the importance of transparency and reproducibility in RL research.\nThis review contributes to the RL community by offering insights into the\nselection process for simulation engines, facilitating informed\ndecision-making.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08590v2",
    "published_date": "2024-07-11 15:13:28 UTC",
    "updated_date": "2024-08-23 07:16:52 UTC"
  },
  {
    "arxiv_id": "2407.08585v1",
    "title": "HACMan++: Spatially-Grounded Motion Primitives for Manipulation",
    "authors": [
      "Bowen Jiang",
      "Yilin Wu",
      "Wenxuan Zhou",
      "Chris Paxton",
      "David Held"
    ],
    "abstract": "Although end-to-end robot learning has shown some success for robot\nmanipulation, the learned policies are often not sufficiently robust to\nvariations in object pose or geometry. To improve the policy generalization, we\nintroduce spatially-grounded parameterized motion primitives in our method\nHACMan++. Specifically, we propose an action representation consisting of three\ncomponents: what primitive type (such as grasp or push) to execute, where the\nprimitive will be grounded (e.g. where the gripper will make contact with the\nworld), and how the primitive motion is executed, such as parameters specifying\nthe push direction or grasp orientation. These three components define a novel\ndiscrete-continuous action space for reinforcement learning. Our framework\nenables robot agents to learn to chain diverse motion primitives together and\nselect appropriate primitive parameters to complete long-horizon manipulation\ntasks. By grounding the primitives on a spatial location in the environment,\nour method is able to effectively generalize across object shape and pose\nvariations. Our approach significantly outperforms existing methods,\nparticularly in complex scenarios demanding both high-level sequential\nreasoning and object generalization. With zero-shot sim-to-real transfer, our\npolicy succeeds in challenging real-world manipulation tasks, with\ngeneralization to unseen objects. Videos can be found on the project website:\nhttps://sgmp-rss2024.github.io.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08585v1",
    "published_date": "2024-07-11 15:10:14 UTC",
    "updated_date": "2024-07-11 15:10:14 UTC"
  },
  {
    "arxiv_id": "2407.08583v2",
    "title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective",
    "authors": [
      "Zhen Qin",
      "Daoyuan Chen",
      "Wenhao Zhang",
      "Liuyi Yao",
      "Yilun Huang",
      "Bolin Ding",
      "Yaliang Li",
      "Shuiguang Deng"
    ],
    "abstract": "The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md",
    "pdf_url": "http://arxiv.org/pdf/2407.08583v2",
    "published_date": "2024-07-11 15:08:11 UTC",
    "updated_date": "2024-08-05 10:31:24 UTC"
  },
  {
    "arxiv_id": "2407.08571v2",
    "title": "Multi-Group Proportional Representation in Retrieval",
    "authors": [
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Carol Xuan Long",
      "Alexander Glynn",
      "Lucas Monteiro Paes",
      "Sajani Vithana",
      "Martina Cardone",
      "Flavio P. Calmon"
    ],
    "abstract": "Image search and retrieval tasks can perpetuate harmful stereotypes, erase\ncultural identities, and amplify social disparities. Current approaches to\nmitigate these representational harms balance the number of retrieved items\nacross population groups defined by a small number of (often binary)\nattributes. However, most existing methods overlook intersectional groups\ndetermined by combinations of group attributes, such as gender, race, and\nethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel\nmetric that measures representation across intersectional groups. We develop\npractical methods for estimating MPR, provide theoretical guarantees, and\npropose optimization algorithms to ensure MPR in retrieval. We demonstrate that\nexisting methods optimizing for equal and proportional representation metrics\nmay fail to promote MPR. Crucially, our work shows that optimizing MPR yields\nmore proportional representation across multiple intersectional groups\nspecified by a rich function class, often with minimal compromise in retrieval\naccuracy.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "comment": "48 pages, 33 figures. Accepted as poster at NeurIPS 2024. Code can be\n  found at\n  https://github.com/alex-oesterling/multigroup-proportional-representation",
    "pdf_url": "http://arxiv.org/pdf/2407.08571v2",
    "published_date": "2024-07-11 14:59:17 UTC",
    "updated_date": "2024-10-31 20:30:51 UTC"
  },
  {
    "arxiv_id": "2407.08564v1",
    "title": "The Career Interests of Large Language Models",
    "authors": [
      "Meng Hua",
      "Yuan Cheng",
      "Hengshu Zhu"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nextended their capabilities, evolving from basic text generation to complex,\nhuman-like interactions. In light of the possibilities that LLMs could assume\nsignificant workplace responsibilities, it becomes imminently necessary to\nexplore LLMs' capacities as professional assistants. This study focuses on the\naspect of career interests by applying the Occupation Network's Interest\nProfiler short form to LLMs as if they were human participants and investigates\ntheir hypothetical career interests and competence, examining how these vary\nwith language changes and model advancements. We analyzed the answers using a\ngeneral linear mixed model approach and found distinct career interest\ninclinations among LLMs, particularly towards the social and artistic domains.\nInterestingly, these preferences did not align with the occupations where LLMs\nexhibited higher competence. This novel approach of using psychometric\ninstruments and sophisticated statistical tools on LLMs unveils fresh\nperspectives on their integration into professional environments, highlighting\nhuman-like tendencies and promoting a reevaluation of LLMs' self-perception and\ncompetency alignment in the workforce.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08564v1",
    "published_date": "2024-07-11 14:54:46 UTC",
    "updated_date": "2024-07-11 14:54:46 UTC"
  },
  {
    "arxiv_id": "2407.08563v1",
    "title": "Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion",
    "authors": [
      "Leah von der Heyde",
      "Anna-Carolina Haensch",
      "Alexander Wenz"
    ],
    "abstract": "The recent development of large language models (LLMs) has spurred\ndiscussions about whether LLM-generated \"synthetic samples\" could complement or\nreplace traditional surveys, considering their training data potentially\nreflects attitudes and behaviors prevalent in the population. A number of\nmostly US-based studies have prompted LLMs to mimic survey respondents, with\nsome of them finding that the responses closely match the survey data. However,\nseveral contextual factors related to the relationship between the respective\ntarget population and LLM training data might affect the generalizability of\nsuch findings. In this study, we investigate the extent to which LLMs can\nestimate public opinion in Germany, using the example of vote choice. We\ngenerate a synthetic sample of personas matching the individual characteristics\nof the 2017 German Longitudinal Election Study respondents. We ask the LLM\nGPT-3.5 to predict each respondent's vote choice and compare these predictions\nto the survey-based estimates on the aggregate and subgroup levels. We find\nthat GPT-3.5 does not predict citizens' vote choice accurately, exhibiting a\nbias towards the Green and Left parties. While the LLM captures the tendencies\nof \"typical\" voter subgroups, such as partisans, it misses the multifaceted\nfactors swaying individual voter choices. By examining the LLM-based prediction\nof voting behavior in a new context, our study contributes to the growing body\nof research about the conditions under which LLMs can be leveraged for studying\npublic opinion. The findings point to disparities in opinion representation in\nLLMs and underscore the limitations in applying them for public opinion\nestimation.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08563v1",
    "published_date": "2024-07-11 14:52:18 UTC",
    "updated_date": "2024-07-11 14:52:18 UTC"
  },
  {
    "arxiv_id": "2407.08558v1",
    "title": "ST-Mamba: Spatial-Temporal Mamba for Traffic Flow Estimation Recovery using Limited Data",
    "authors": [
      "Doncheng Yuan",
      "Jianzhe Xue",
      "Jinshan Su",
      "Wenchao Xu",
      "Haibo Zhou"
    ],
    "abstract": "Traffic flow estimation (TFE) is crucial for urban intelligent traffic\nsystems. While traditional on-road detectors are hindered by limited coverage\nand high costs, cloud computing and data mining of vehicular network data, such\nas driving speeds and GPS coordinates, present a promising and cost-effective\nalternative. Furthermore, minimizing data collection can significantly reduce\noverhead. However, limited data can lead to inaccuracies and instability in\nTFE. To address this, we introduce the spatial-temporal Mamba (ST-Mamba), a\ndeep learning model combining a convolutional neural network (CNN) with a Mamba\nframework. ST-Mamba is designed to enhance TFE accuracy and stability by\neffectively capturing the spatial-temporal patterns within traffic flow. Our\nmodel aims to achieve results comparable to those from extensive data sets\nwhile only utilizing minimal data. Simulations using real-world datasets have\nvalidated our model's ability to deliver precise and stable TFE across an urban\nlandscape based on limited data, establishing a cost-efficient solution for\nTFE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by 2024 IEEE/CIC International Conference on Communications\n  in China (ICCC)",
    "pdf_url": "http://arxiv.org/pdf/2407.08558v1",
    "published_date": "2024-07-11 14:43:03 UTC",
    "updated_date": "2024-07-11 14:43:03 UTC"
  },
  {
    "arxiv_id": "2407.08554v2",
    "title": "Establishing Rigorous and Cost-effective Clinical Trials for Artificial Intelligence Models",
    "authors": [
      "Wanling Gao",
      "Yunyou Huang",
      "Dandan Cui",
      "Zhuoming Yu",
      "Wenjing Liu",
      "Xiaoshuang Liang",
      "Jiahui Zhao",
      "Jiyue Xie",
      "Hao Li",
      "Li Ma",
      "Ning Ye",
      "Yumiao Kang",
      "Dingfeng Luo",
      "Peng Pan",
      "Wei Huang",
      "Zhongmou Liu",
      "Jizhong Hu",
      "Gangyuan Zhao",
      "Chongrong Jiang",
      "Fan Huang",
      "Tianyi Wei",
      "Suqin Tang",
      "Bingjie Xia",
      "Zhifei Zhang",
      "Jianfeng Zhan"
    ],
    "abstract": "A profound gap persists between artificial intelligence (AI) and clinical\npractice in medicine, primarily due to the lack of rigorous and cost-effective\nevaluation methodologies. State-of-the-art and state-of-the-practice AI model\nevaluations are limited to laboratory studies on medical datasets or direct\nclinical trials with no or solely patient-centered controls. Moreover, the\ncrucial role of clinicians in collaborating with AI, pivotal for determining\nits impact on clinical practice, is often overlooked. For the first time, we\nemphasize the critical necessity for rigorous and cost-effective evaluation\nmethodologies for AI models in clinical practice, featuring\npatient/clinician-centered (dual-centered) AI randomized controlled trials\n(DC-AI RCTs) and virtual clinician-based in-silico trials (VC-MedAI) as an\neffective proxy for DC-AI RCTs. Leveraging 7500 diagnosis records from two-step\ninaugural DC-AI RCTs across 14 medical centers with 125 clinicians, our results\ndemonstrate the necessity of DC-AI RCTs and the effectiveness of VC-MedAI.\nNotably, VC-MedAI performs comparably to human clinicians, replicating insights\nand conclusions from prospective DC-AI RCTs. We envision DC-AI RCTs and\nVC-MedAI as pivotal advancements, presenting innovative and transformative\nevaluation methodologies for AI models in clinical practice, offering a\npreclinical-like setting mirroring conventional medicine, and reshaping\ndevelopment paradigms in a cost-effective and fast-iterative manner. Chinese\nClinical Trial Registration: ChiCTR2400086816.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.08554v2",
    "published_date": "2024-07-11 14:37:08 UTC",
    "updated_date": "2024-07-28 15:33:33 UTC"
  },
  {
    "arxiv_id": "2407.08550v1",
    "title": "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility",
    "authors": [
      "Yuchen Xia",
      "Jize Zhang",
      "Nasser Jazdi",
      "Michael Weyrich"
    ],
    "abstract": "This paper introduces a novel approach to integrating large language model\n(LLM) agents into automated production systems, aimed at enhancing task\nautomation and flexibility. We organize production operations within a\nhierarchical framework based on the automation pyramid. Atomic operation\nfunctionalities are modeled as microservices, which are executed through\ninterface invocation within a dedicated digital twin system. This allows for a\nscalable and flexible foundation for orchestrating production processes. In\nthis digital twin system, low-level, hardware-specific data is semantically\nenriched and made interpretable for LLMs for production planning and control\ntasks. Large language model agents are systematically prompted to interpret\nthese production-specific data and knowledge. Upon receiving a user request or\nidentifying a triggering event, the LLM agents generate a process plan. This\nplan is then decomposed into a series of atomic operations, executed as\nmicroservices within the real-world automation system. We implement this\noverall approach on an automated modular production facility at our laboratory,\ndemonstrating how the LLMs can handle production planning and control tasks\nthrough a concrete case study. This results in an intuitive production facility\nwith higher levels of task automation and flexibility. Finally, we reveal the\nseveral limitations in realizing the full potential of the large language\nmodels in autonomous systems and point out promising benefits. Demos of this\nseries of ongoing research series can be accessed at:\nhttps://github.com/YuchenXia/GPT4IndustrialAutomation",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.MA",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08550v1",
    "published_date": "2024-07-11 14:34:43 UTC",
    "updated_date": "2024-07-11 14:34:43 UTC"
  },
  {
    "arxiv_id": "2407.08516v5",
    "title": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents",
    "authors": [
      "Haoyi Xiong",
      "Zhiyuan Wang",
      "Xuhong Li",
      "Jiang Bian",
      "Zeke Xie",
      "Shahid Mumtaz",
      "Anwer Al-Dulaimi",
      "Laura E. Barnes"
    ],
    "abstract": "This article explores the convergence of connectionist and symbolic\nartificial intelligence (AI), from historical debates to contemporary\nadvancements. Traditionally considered distinct paradigms, connectionist AI\nfocuses on neural networks, while symbolic AI emphasizes symbolic\nrepresentation and logic. Recent advancements in large language models (LLMs),\nexemplified by ChatGPT and GPT-4, highlight the potential of connectionist\narchitectures in handling human language as a form of symbols. The study argues\nthat LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence.\nBy utilizing LLMs for text-based knowledge modeling and representation, LAAs\nintegrate neuro-symbolic AI principles, showcasing enhanced reasoning and\ndecision-making capabilities. Comparing LAAs with Knowledge Graphs within the\nneuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking\nhuman-like reasoning processes, scaling effectively with large datasets, and\nleveraging in-context samples without explicit re-training. The research\nunderscores promising avenues in neuro-vector-symbolic integration,\ninstructional encoding, and implicit reasoning, aimed at further enhancing LAA\ncapabilities. By exploring the progression of neuro-symbolic AI and proposing\nfuture research trajectories, this work advances the understanding and\ndevelopment of AI technologies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08516v5",
    "published_date": "2024-07-11 14:00:53 UTC",
    "updated_date": "2024-10-14 16:55:26 UTC"
  },
  {
    "arxiv_id": "2407.08515v2",
    "title": "15M Multimodal Facial Image-Text Dataset",
    "authors": [
      "Dawei Dai",
      "YuTang Li",
      "YingGe Liu",
      "Mingming Jia",
      "Zhang YuanHui",
      "Guoyin Wang"
    ],
    "abstract": "Currently, image-text-driven multi-modal deep learning models have\ndemonstrated their outstanding potential in many fields. In practice, tasks\ncentered around facial images have broad application prospects. This paper\npresents \\textbf{FaceCaption-15M}, a large-scale, diverse, and high-quality\ndataset of facial images accompanied by their natural language descriptions\n(facial image-to-text). This dataset aims to facilitate a study on\nface-centered tasks. FaceCaption-15M comprises over 15 million pairs of facial\nimages and their corresponding natural language descriptions of facial\nfeatures, making it the largest facial image-caption dataset to date. We\nconducted a comprehensive analysis of image quality, text naturalness, text\ncomplexity, and text-image relevance to demonstrate the superiority of\nFaceCaption-15M. To validate the effectiveness of FaceCaption-15M, we first\ntrained a facial language-image pre-training model (FLIP, similar to CLIP) to\nalign facial image with its corresponding captions in feature space.\nSubsequently, using both image and text encoders and fine-tuning only the\nlinear layer, our FLIP-based models achieved state-of-the-art results on two\nchallenging face-centered tasks. The purpose is to promote research in the\nfield of face-related tasks through the availability of the proposed\nFaceCaption-15M dataset. All data, codes, and models are publicly available.\nhttps://huggingface.co/datasets/OpenFace-CQUPT/FaceCaption-15M",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08515v2",
    "published_date": "2024-07-11 14:00:14 UTC",
    "updated_date": "2024-07-12 01:19:33 UTC"
  },
  {
    "arxiv_id": "2407.08500v2",
    "title": "Latent Conditional Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model",
    "authors": [
      "Yuxing Tian",
      "Yiyan Qi",
      "Aiwen Jiang",
      "Qi Huang",
      "Jian Guo"
    ],
    "abstract": "Continuous-Time Dynamic Graph (CTDG) precisely models evolving real-world\nrelationships, drawing heightened interest in dynamic graph learning across\nacademia and industry. However, existing CTDG models encounter challenges\nstemming from noise and limited historical data. Graph Data Augmentation (GDA)\nemerges as a critical solution, yet current approaches primarily focus on\nstatic graphs and struggle to effectively address the dynamics inherent in\nCTDGs. Moreover, these methods often demand substantial domain expertise for\nparameter tuning and lack theoretical guarantees for augmentation efficacy. To\naddress these issues, we propose Conda, a novel latent diffusion-based GDA\nmethod tailored for CTDGs. Conda features a sandwich-like architecture,\nincorporating a Variational Auto-Encoder (VAE) and a conditional diffusion\nmodel, aimed at generating enhanced historical neighbor embeddings for target\nnodes. Unlike conventional diffusion models trained on entire graphs via\npre-training, Conda requires historical neighbor sequence embeddings of target\nnodes for training, thus facilitating more targeted augmentation. We integrate\nConda into the CTDG model and adopt an alternating training strategy to\noptimize performance. Extensive experimentation across six widely used\nreal-world datasets showcases the consistent performance improvement of our\napproach, particularly in scenarios with limited historical data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08500v2",
    "published_date": "2024-07-11 13:35:22 UTC",
    "updated_date": "2024-07-20 06:44:09 UTC"
  },
  {
    "arxiv_id": "2407.08497v2",
    "title": "CE-QArg: Counterfactual Explanations for Quantitative Bipolar Argumentation Frameworks (Technical Report)",
    "authors": [
      "Xiang Yin",
      "Nico Potyka",
      "Francesca Toni"
    ],
    "abstract": "There is a growing interest in understanding arguments' strength in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). Most existing studies\nfocus on attribution-based methods that explain an argument's strength by\nassigning importance scores to other arguments but fail to explain how to\nchange the current strength to a desired one. To solve this issue, we introduce\ncounterfactual explanations for QBAFs. We discuss problem variants and propose\nan iterative algorithm named Counterfactual Explanations for Quantitative\nbipolar Argumentation frameworks (CE-QArg). CE-QArg can identify valid and\ncost-effective counterfactual explanations based on two core modules, polarity\nand priority, which help determine the updating direction and magnitude for\neach argument, respectively. We discuss some formal properties of our\ncounterfactual explanations and empirically evaluate CE-QArg on randomly\ngenerated QBAFs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted at KR 2024 (21st International\n  Conference on Principles of Knowledge Representation and Reasoning)",
    "pdf_url": "http://arxiv.org/pdf/2407.08497v2",
    "published_date": "2024-07-11 13:34:11 UTC",
    "updated_date": "2024-11-11 11:19:27 UTC"
  },
  {
    "arxiv_id": "2407.08488v2",
    "title": "Lynx: An Open Source Hallucination Evaluation Model",
    "authors": [
      "Selvan Sunitha Ravi",
      "Bartosz Mielczarek",
      "Anand Kannappan",
      "Douwe Kiela",
      "Rebecca Qian"
    ],
    "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08488v2",
    "published_date": "2024-07-11 13:22:17 UTC",
    "updated_date": "2024-07-22 18:41:53 UTC"
  },
  {
    "arxiv_id": "2407.08479v1",
    "title": "Robust Generalization of Graph Neural Networks for Carrier Scheduling",
    "authors": [
      "Daniel F. Perez-Ramirez",
      "Carlos Pérez-Penichet",
      "Nicolas Tsiftes",
      "Dejan Kostic",
      "Magnus Boman",
      "Thiemo Voigt"
    ],
    "abstract": "Battery-free sensor tags are devices that leverage backscatter techniques to\ncommunicate with standard IoT devices, thereby augmenting a network's sensing\ncapabilities in a scalable way. For communicating, a sensor tag relies on an\nunmodulated carrier provided by a neighboring IoT device, with a schedule\ncoordinating this provisioning across the network. Carrier\nscheduling--computing schedules to interrogate all sensor tags while minimizing\nenergy, spectrum utilization, and latency--is an NP-Hard optimization problem.\nRecent work introduces learning-based schedulers that achieve resource savings\nover a carefully-crafted heuristic, generalizing to networks of up to 60 nodes.\nHowever, we find that their advantage diminishes in networks with hundreds of\nnodes, and degrades further in larger setups. This paper introduces\nRobustGANTT, a GNN-based scheduler that improves generalization (without\nre-training) to networks up to 1000 nodes (100x training topology sizes).\nRobustGANTT not only achieves better and more consistent generalization, but\nalso computes schedules requiring up to 2x less resources than existing\nsystems. Our scheduler exhibits average runtimes of hundreds of milliseconds,\nallowing it to react fast to changing network conditions. Our work not only\nimproves resource utilization in large-scale backscatter networks, but also\noffers valuable insights in learning-based scheduling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "15 Pages, 12 Figures. Pre-print, under review",
    "pdf_url": "http://arxiv.org/pdf/2407.08479v1",
    "published_date": "2024-07-11 13:13:24 UTC",
    "updated_date": "2024-07-11 13:13:24 UTC"
  },
  {
    "arxiv_id": "2407.08473v1",
    "title": "Natural language is not enough: Benchmarking multi-modal generative AI for Verilog generation",
    "authors": [
      "Kaiyan Chang",
      "Zhirong Chen",
      "Yunhao Zhou",
      "Wenlong Zhu",
      "kun wang",
      "Haobo Xu",
      "Cangyuan Li",
      "Mengdi Wang",
      "Shengwen Liang",
      "Huawei Li",
      "Yinhe Han",
      "Ying Wang"
    ],
    "abstract": "Natural language interfaces have exhibited considerable potential in the\nautomation of Verilog generation derived from high-level specifications through\nthe utilization of large language models, garnering significant attention.\nNevertheless, this paper elucidates that visual representations contribute\nessential contextual information critical to design intent for hardware\narchitectures possessing spatial complexity, potentially surpassing the\nefficacy of natural-language-only inputs. Expanding upon this premise, our\npaper introduces an open-source benchmark for multi-modal generative models\ntailored for Verilog synthesis from visual-linguistic inputs, addressing both\nsingular and complex modules. Additionally, we introduce an open-source visual\nand natural language Verilog query language framework to facilitate efficient\nand user-friendly multi-modal queries. To evaluate the performance of the\nproposed multi-modal hardware generative AI in Verilog generation tasks, we\ncompare it with a popular method that relies solely on natural language. Our\nresults demonstrate a significant accuracy improvement in the multi-modal\ngenerated Verilog compared to queries based solely on natural language. We hope\nto reveal a new approach to hardware design in the large-hardware-design-model\nera, thereby fostering a more diversified and productive approach to hardware\ndesign.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Accepted by ICCAD 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08473v1",
    "published_date": "2024-07-11 13:10:09 UTC",
    "updated_date": "2024-07-11 13:10:09 UTC"
  },
  {
    "arxiv_id": "2407.08470v1",
    "title": "Brain Tumor Segmentation in MRI Images with 3D U-Net and Contextual Transformer",
    "authors": [
      "Thien-Qua T. Nguyen",
      "Hieu-Nghia Nguyen",
      "Thanh-Hieu Bui",
      "Thien B. Nguyen-Tat",
      "Vuong M. Ngo"
    ],
    "abstract": "This research presents an enhanced approach for precise segmentation of brain\ntumor masses in magnetic resonance imaging (MRI) using an advanced 3D-UNet\nmodel combined with a Context Transformer (CoT). By architectural expansion\nCoT, the proposed model extends its architecture to a 3D format, integrates it\nsmoothly with the base model to utilize the complex contextual information\nfound in MRI scans, emphasizing how elements rely on each other across an\nextended spatial range. The proposed model synchronizes tumor mass\ncharacteristics from CoT, mutually reinforcing feature extraction, facilitating\nthe precise capture of detailed tumor mass structures, including location,\nsize, and boundaries. Several experimental results present the outstanding\nsegmentation performance of the proposed method in comparison to current\nstate-of-the-art approaches, achieving Dice score of 82.0%, 81.5%, 89.0% for\nEnhancing Tumor, Tumor Core and Whole Tumor, respectively, on BraTS2019.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08470v1",
    "published_date": "2024-07-11 13:04:20 UTC",
    "updated_date": "2024-07-11 13:04:20 UTC"
  },
  {
    "arxiv_id": "2407.08464v2",
    "title": "TLDR: Unsupervised Goal-Conditioned RL via Temporal Distance-Aware Representations",
    "authors": [
      "Junik Bae",
      "Kwanyoung Park",
      "Youngwoon Lee"
    ],
    "abstract": "Unsupervised goal-conditioned reinforcement learning (GCRL) is a promising\nparadigm for developing diverse robotic skills without external supervision.\nHowever, existing unsupervised GCRL methods often struggle to cover a wide\nrange of states in complex environments due to their limited exploration and\nsparse or noisy rewards for GCRL. To overcome these challenges, we propose a\nnovel unsupervised GCRL method that leverages TemporaL Distance-aware\nRepresentations (TLDR). Based on temporal distance, TLDR selects faraway goals\nto initiate exploration and computes intrinsic exploration rewards and\ngoal-reaching rewards. Specifically, our exploration policy seeks states with\nlarge temporal distances (i.e. covering a large state space), while the\ngoal-conditioned policy learns to minimize the temporal distance to the goal\n(i.e. reaching the goal). Our results in six simulated locomotion environments\ndemonstrate that TLDR significantly outperforms prior unsupervised GCRL methods\nin achieving a wide range of states.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "CoRL 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08464v2",
    "published_date": "2024-07-11 13:01:18 UTC",
    "updated_date": "2024-12-09 08:50:46 UTC"
  },
  {
    "arxiv_id": "2407.08448v2",
    "title": "Paving the way toward foundation models for irregular and unaligned Satellite Image Time Series",
    "authors": [
      "Iris Dumeur",
      "Silvia Valero",
      "Jordi Inglada"
    ],
    "abstract": "Although recently several foundation models for satellite remote sensing\nimagery have been proposed, they fail to address major challenges of\nreal/operational applications. Indeed, embeddings that don't take into account\nthe spectral, spatial and temporal dimensions of the data as well as the\nirregular or unaligned temporal sampling are of little use for most real world\nuses. As a consequence, we propose an ALIgned Sits Encoder (ALISE), a novel\napproach that leverages the spatial, spectral, and temporal dimensions of\nirregular and unaligned SITS while producing aligned latent representations.\nUnlike SSL models currently available for SITS, ALISE incorporates a flexible\nquery mechanism to project the SITS into a common and learned temporal\nprojection space. Additionally, thanks to a multi-view framework, we explore\nintegration of instance discrimination along a masked autoencoding task to\nSITS. The quality of the produced representation is assessed through three\ndownstream tasks: crop segmentation (PASTIS), land cover segmentation\n(MultiSenGE), and a novel crop change detection dataset. Furthermore, the\nchange detection task is performed without supervision. The results suggest\nthat the use of aligned representations is more effective than previous SSL\nmethods for linear probing segmentation tasks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08448v2",
    "published_date": "2024-07-11 12:42:10 UTC",
    "updated_date": "2024-09-30 08:13:28 UTC"
  },
  {
    "arxiv_id": "2407.08442v2",
    "title": "How Deep is your Guess? A Fresh Perspective on Deep Learning for Medical Time-Series Imputation",
    "authors": [
      "Linglong Qian",
      "Tao Wang",
      "Jun Wang",
      "Hugh Logan Ellis",
      "Robin Mitra",
      "Richard Dobson",
      "Zina Ibrahim"
    ],
    "abstract": "We present a comprehensive analysis of deep learning approaches for\nElectronic Health Record (EHR) time-series imputation, examining how\narchitectural and framework biases combine to influence model performance. Our\ninvestigation reveals varying capabilities of deep imputers in capturing\ncomplex spatiotemporal dependencies within EHRs, and that model effectiveness\ndepends on how its combined biases align with medical time-series\ncharacteristics. Our experimental evaluation challenges common assumptions\nabout model complexity, demonstrating that larger models do not necessarily\nimprove performance. Rather, carefully designed architectures can better\ncapture the complex patterns inherent in clinical data. The study highlights\nthe need for imputation approaches that prioritise clinically meaningful data\nreconstruction over statistical accuracy. Our experiments show imputation\nperformance variations of up to 20\\% based on preprocessing and implementation\nchoices, emphasising the need for standardised benchmarking methodologies.\nFinally, we identify critical gaps between current deep imputation methods and\nmedical requirements, highlighting the importance of integrating clinical\ninsights to achieve more reliable imputation approaches for healthcare\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08442v2",
    "published_date": "2024-07-11 12:33:28 UTC",
    "updated_date": "2025-02-04 00:12:27 UTC"
  },
  {
    "arxiv_id": "2407.08441v2",
    "title": "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation",
    "authors": [
      "Riccardo Cantini",
      "Giada Cosenza",
      "Alessio Orsino",
      "Domenico Talia"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence,\ndemonstrating remarkable computational power and linguistic capabilities.\nHowever, these models are inherently prone to various biases stemming from\ntheir training data. These include selection, linguistic, and confirmation\nbiases, along with common stereotypes related to gender, ethnicity, sexual\norientation, religion, socioeconomic status, disability, and age. This study\nexplores the presence of these biases within the responses given by the most\nrecent LLMs, analyzing the impact on their fairness and reliability. We also\ninvestigate how known prompt engineering techniques can be exploited to\neffectively reveal hidden biases of LLMs, testing their adversarial robustness\nagainst jailbreak prompts specially crafted for bias elicitation. Extensive\nexperiments are conducted using the most widespread LLMs at different scales,\nconfirming that LLMs can still be manipulated to produce biased or\ninappropriate responses, despite their advanced capabilities and sophisticated\nalignment processes. Our findings underscore the importance of enhancing\nmitigation techniques to address these safety issues, toward a more sustainable\nand inclusive artificial intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08441v2",
    "published_date": "2024-07-11 12:30:19 UTC",
    "updated_date": "2025-02-13 11:30:41 UTC"
  },
  {
    "arxiv_id": "2407.08440v4",
    "title": "Beyond Instruction Following: Evaluating Inferential Rule Following of Large Language Models",
    "authors": [
      "Wangtao Sun",
      "Chenxiang Zhang",
      "XueYou Zhang",
      "Xuanqing Yu",
      "Ziyang Huang",
      "Pei Chen",
      "Haotian Xu",
      "Shizhu He",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated strong ability, they\nare further supposed to be controlled and guided by in real-world scenarios to\nbe safe, accurate, and intelligent. This demands the possession of capability\nof LLMs. However, no prior work has made a clear evaluation of the inferential\nrule-following capability of LLMs. Previous studies that try to evaluate the\ninferential rule-following capability of LLMs fail to distinguish the\ninferential rule-following scenarios from the instruction-following scenarios.\nTherefore, this paper first clarifies the concept of inferential rule-following\nand proposes a comprehensive benchmark, RuleBench, to evaluate a diversified\nrange of inferential rule-following abilities. Our experimental results on a\nvariety of LLMs show that they are still limited in following rules. Our\nanalysis based on the evaluation results provides insights into the\nimprovements for LLMs toward a better inferential rule-following intelligent\nagent. We further propose Inferential Rule-Following Tuning (IRFT). The\nexperimental results show that through IRFT, LLMs can learn abstract\nrule-following abilities from purely synthetic data and then generalize to\nRuleBench. The data and code can be found at:\nhttps://anonymous.4open.science/r/llm-rule-following-B3E3/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08440v4",
    "published_date": "2024-07-11 12:26:55 UTC",
    "updated_date": "2024-10-17 07:00:19 UTC"
  },
  {
    "arxiv_id": "2407.08428v1",
    "title": "A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights",
    "authors": [
      "Wentao Lei",
      "Jinting Wang",
      "Fengji Ma",
      "Guanjie Huang",
      "Li Liu"
    ],
    "abstract": "Human video generation is a dynamic and rapidly evolving task that aims to\nsynthesize 2D human body video sequences with generative models given control\nconditions such as text, audio, and pose. With the potential for wide-ranging\napplications in film, gaming, and virtual communication, the ability to\ngenerate natural and realistic human video is critical. Recent advancements in\ngenerative models have laid a solid foundation for the growing interest in this\narea. Despite the significant progress, the task of human video generation\nremains challenging due to the consistency of characters, the complexity of\nhuman motion, and difficulties in their relationship with the environment. This\nsurvey provides a comprehensive review of the current state of human video\ngeneration, marking, to the best of our knowledge, the first extensive\nliterature review in this domain. We start with an introduction to the\nfundamentals of human video generation and the evolution of generative models\nthat have facilitated the field's growth. We then examine the main methods\nemployed for three key sub-tasks within human video generation: text-driven,\naudio-driven, and pose-driven motion generation. These areas are explored\nconcerning the conditions that guide the generation process. Furthermore, we\noffer a collection of the most commonly utilized datasets and the evaluation\nmetrics that are crucial in assessing the quality and realism of generated\nvideos. The survey concludes with a discussion of the current challenges in the\nfield and suggests possible directions for future research. The goal of this\nsurvey is to offer the research community a clear and holistic view of the\nadvancements in human video generation, highlighting the milestones achieved\nand the challenges that lie ahead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08428v1",
    "published_date": "2024-07-11 12:09:05 UTC",
    "updated_date": "2024-07-11 12:09:05 UTC"
  },
  {
    "arxiv_id": "2407.08422v2",
    "title": "On the (In)Security of LLM App Stores",
    "authors": [
      "Xinyi Hou",
      "Yanjie Zhao",
      "Haoyu Wang"
    ],
    "abstract": "LLM app stores have seen rapid growth, leading to the proliferation of\nnumerous custom LLM apps. However, this expansion raises security concerns. In\nthis study, we propose a three-layer concern framework to identify the\npotential security risks of LLM apps, i.e., LLM apps with abusive potential,\nLLM apps with malicious intent, and LLM apps with exploitable vulnerabilities.\nOver five months, we collected 786,036 LLM apps from six major app stores: GPT\nStore, FlowGPT, Poe, Coze, Cici, and Character.AI. Our research integrates\nstatic and dynamic analysis, the development of a large-scale toxic word\ndictionary (i.e., ToxicDict) comprising over 31,783 entries, and automated\nmonitoring tools to identify and mitigate threats. We uncovered that 15,146\napps had misleading descriptions, 1,366 collected sensitive personal\ninformation against their privacy policies, and 15,996 generated harmful\ncontent such as hate speech, self-harm, extremism, etc. Additionally, we\nevaluated the potential for LLM apps to facilitate malicious activities,\nfinding that 616 apps could be used for malware generation, phishing, etc. Our\nfindings highlight the urgent need for robust regulatory frameworks and\nenhanced enforcement mechanisms.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08422v2",
    "published_date": "2024-07-11 12:03:32 UTC",
    "updated_date": "2024-07-29 11:18:57 UTC"
  },
  {
    "arxiv_id": "2407.08410v2",
    "title": "Specialized curricula for training vision-language models in retinal image analysis",
    "authors": [
      "Robbie Holland",
      "Thomas R. P. Taylor",
      "Christopher Holmes",
      "Sophie Riedl",
      "Julia Mai",
      "Maria Patsiamanidi",
      "Dimitra Mitsopoulou",
      "Paul Hager",
      "Philip Müller",
      "Hendrik P. N. Scholl",
      "Hrvoje Bogunović",
      "Ursula Schmidt-Erfurth",
      "Daniel Rueckert",
      "Sobha Sivaprasad",
      "Andrew J. Lotery",
      "Martin J. Menten"
    ],
    "abstract": "Clinicians spend a significant amount of time reviewing medical images and\ntranscribing their findings regarding patient diagnosis, referral and treatment\nin text form. Vision-language models (VLMs), which automatically interpret\nimages and summarize their findings as text, have enormous potential to\nalleviate clinical workloads and increase patient access to high-quality\nmedical care. While foundational models have stirred considerable interest in\nthe medical community, it is unclear whether their general capabilities\ntranslate to real-world clinical utility. In this work, we demonstrate that\nOpenAI's ChatGPT-4o model, in addition to two foundation VLMs designed for\nmedical use, markedly underperform compared to practicing ophthalmologists on\nspecialist tasks crucial to the care of patients with age-related macular\ndegeneration (AMD). To address this, we initially identified the essential\ncapabilities required for image-based clinical decision-making, and then\ndeveloped a curriculum to selectively train VLMs in these skills. The resulting\nmodel, RetinaVLM, can be instructed to write reports that significantly\noutperform those written by leading foundation medical VLMs and ChatGPT-4o in\ndisease staging (F1 score of 0.63 vs. 0.33) and patient referral (0.67 vs.\n0.50), and approaches the diagnostic performance of junior ophthalmologists\n(who achieve 0.77 and 0.78 on the respective tasks). Furthermore, in a\nsingle-blind reader study two senior ophthalmologists with up to 32 years of\nexperience found RetinaVLM's reports were found to be substantially more\naccurate than those by ChatGPT-4o (64.3% vs. 14.3%). These results reinforce\nthat our curriculum-based approach provides a blueprint towards specializing\nfoundation medical VLMs for real-world clinical tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review at npj Digital Medicine",
    "pdf_url": "http://arxiv.org/pdf/2407.08410v2",
    "published_date": "2024-07-11 11:31:48 UTC",
    "updated_date": "2025-02-25 01:54:59 UTC"
  },
  {
    "arxiv_id": "2407.08408v1",
    "title": "A Two-Stage Machine Learning-Aided Approach for Quench Identification at the European XFEL",
    "authors": [
      "Lynda Boukela",
      "Annika Eichler",
      "Julien Branlard",
      "Nur Zulaiha Jomhari"
    ],
    "abstract": "This paper introduces a machine learning-aided fault detection and isolation\nmethod applied to the case study of quench identification at the European X-Ray\nFree-Electron Laser. The plant utilizes 800 superconducting radio-frequency\ncavities in order to accelerate electron bunches to high energies of up to 17.5\nGeV. Various faulty events can disrupt the nominal functioning of the\naccelerator, including quenches that can lead to a loss of the\nsuperconductivity of the cavities and the interruption of their operation. In\nthis context, our solution consists in analyzing signals reflecting the\ndynamics of the cavities in a two-stage approach. (I) Fault detection that uses\nanalytical redundancy to process the data and generate a residual. The\nevaluation of the residual through the generalized likelihood ratio allows\ndetecting the faulty behaviors. (II) Fault isolation which involves the\ndistinction of the quenches from the other faults. To this end, we proceed with\na data-driven model of the k-medoids algorithm that explores different\nsimilarity measures, namely, the Euclidean and the dynamic time warping.\nFinally, we evaluate the new method and compare it to the currently deployed\nquench detection system, the results show the improved performance achieved by\nour method.",
    "categories": [
      "physics.ins-det",
      "cs.AI"
    ],
    "primary_category": "physics.ins-det",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08408v1",
    "published_date": "2024-07-11 11:21:41 UTC",
    "updated_date": "2024-07-11 11:21:41 UTC"
  },
  {
    "arxiv_id": "2407.08400v3",
    "title": "Self-training Language Models for Arithmetic Reasoning",
    "authors": [
      "Marek Kadlčík",
      "Michal Štefánik"
    ],
    "abstract": "Recent language models achieve impressive results in tasks involving complex\nmultistep reasoning, but scaling these capabilities further traditionally\nrequires expensive collection of more annotated data. In this work, we explore\nthe potential of improving models' reasoning capabilities without new data,\nmerely using automated feedback to the validity of their predictions in\narithmetic reasoning (self-training).\n  In systematic experimentation across six different arithmetic reasoning\ndatasets, we find that models can substantially improve in both single-round\n(offline) and online self-training, reaching a correct result in +13.9% and\n+25.9% more cases, respectively, underlining the importance of actuality of\nself-training feedback. We further find that in the single-round, offline\nself-training, traditional supervised training can deliver gains comparable to\npreference optimization, but in online self-training, preference optimization\nmethods largely outperform supervised training thanks to their superior\nstability and robustness on unseen types of problems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear in Findings of EMNLP 2024. Reproducible implementations and\n  references to resulting models can be found on\n  https://github.com/prompteus/calc-x",
    "pdf_url": "http://arxiv.org/pdf/2407.08400v3",
    "published_date": "2024-07-11 11:06:05 UTC",
    "updated_date": "2024-10-23 20:43:02 UTC"
  },
  {
    "arxiv_id": "2407.08388v1",
    "title": "On the attribution of confidence to large language models",
    "authors": [
      "Geoff Keeling",
      "Winnie Street"
    ],
    "abstract": "Credences are mental states corresponding to degrees of confidence in\npropositions. Attribution of credences to Large Language Models (LLMs) is\ncommonplace in the empirical literature on LLM evaluation. Yet the theoretical\nbasis for LLM credence attribution is unclear. We defend three claims. First,\nour semantic claim is that LLM credence attributions are (at least in general)\ncorrectly interpreted literally, as expressing truth-apt beliefs on the part of\nscientists that purport to describe facts about LLM credences. Second, our\nmetaphysical claim is that the existence of LLM credences is at least\nplausible, although current evidence is inconclusive. Third, our epistemic\nclaim is that LLM credence attributions made in the empirical literature on LLM\nevaluation are subject to non-trivial sceptical concerns. It is a distinct\npossibility that even if LLMs have credences, LLM credence attributions are\ngenerally false because the experimental techniques used to assess LLM\ncredences are not truth-tracking.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "22 pages, 0 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08388v1",
    "published_date": "2024-07-11 10:51:06 UTC",
    "updated_date": "2024-07-11 10:51:06 UTC"
  },
  {
    "arxiv_id": "2407.08380v1",
    "title": "Digital twins to alleviate the need for real field data in vision-based vehicle speed detection systems",
    "authors": [
      "Antonio Hernández Martínez",
      "Iván García Daza",
      "Carlos Fernández López",
      "David Fernández Llorca"
    ],
    "abstract": "Accurate vision-based speed estimation is much more cost-effective than\ntraditional methods based on radar or LiDAR. However, it is also challenging\ndue to the limitations of perspective projection on a discrete sensor, as well\nas the high sensitivity to calibration, lighting and weather conditions.\nInterestingly, deep learning approaches (which dominate the field of computer\nvision) are very limited in this context due to the lack of available data.\nIndeed, obtaining video sequences of real road traffic with accurate speed\nvalues associated with each vehicle is very complex and costly, and the number\nof available datasets is very limited. Recently, some approaches are focusing\non the use of synthetic data. However, it is still unclear how models trained\non synthetic data can be effectively applied to real world conditions. In this\nwork, we propose the use of digital-twins using CARLA simulator to generate a\nlarge dataset representative of a specific real-world camera. The synthetic\ndataset contains a large variability of vehicle types, colours, speeds,\nlighting and weather conditions. A 3D CNN model is trained on the digital twin\nand tested on the real sequences. Unlike previous approaches that generate\nmulti-camera sequences, we found that the gap between the the real and the\nvirtual conditions is a key factor in obtaining low speed estimation errors.\nEven with a preliminary approach, the mean absolute error obtained remains\nbelow 3km/h.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Paper accepted at the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.08380v1",
    "published_date": "2024-07-11 10:41:20 UTC",
    "updated_date": "2024-07-11 10:41:20 UTC"
  },
  {
    "arxiv_id": "2407.08348v2",
    "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On",
    "authors": [
      "Liang Zeng",
      "Liangjun Zhong",
      "Liang Zhao",
      "Tianwen Wei",
      "Liu Yang",
      "Jujie He",
      "Cheng Cheng",
      "Rui Hu",
      "Yang Liu",
      "Shuicheng Yan",
      "Han Fang",
      "Yahui Zhou"
    ],
    "abstract": "In this paper, we investigate the underlying factors that potentially enhance\nthe mathematical reasoning capabilities of large language models (LLMs). We\nargue that the data scaling law for math reasoning capabilities in modern LLMs\nis far from being saturated, highlighting how the model's quality improves with\nincreases in data quantity. To support this claim, we introduce the\nSkywork-Math model series, supervised fine-tuned (SFT) on common 7B LLMs using\nour proposed 2.5M-instance Skywork-MathQA dataset. Skywork-Math 7B has achieved\nimpressive accuracies of 51.2% on the competition-level MATH benchmark and\n83.9% on the GSM8K benchmark using only SFT data, outperforming an early\nversion of GPT-4 on MATH. The superior performance of Skywork-Math models\ncontributes to our novel two-stage data synthesis and model SFT pipelines,\nwhich include three different augmentation methods and a diverse seed problem\nset, ensuring both the quantity and quality of Skywork-MathQA dataset across\nvarying difficulty levels. Most importantly, we provide several practical\ntakeaways to enhance math reasoning abilities in LLMs for both research and\nindustry applications.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08348v2",
    "published_date": "2024-07-11 09:56:51 UTC",
    "updated_date": "2024-07-17 16:28:33 UTC"
  },
  {
    "arxiv_id": "2407.08331v2",
    "title": "Towards Explainable Evolution Strategies with Large Language Models",
    "authors": [
      "Jill Baumann",
      "Oliver Kramer"
    ],
    "abstract": "This paper introduces an approach that integrates self-adaptive Evolution\nStrategies (ES) with Large Language Models (LLMs) to enhance the explainability\nof complex optimization processes. By employing a self-adaptive ES equipped\nwith a restart mechanism, we effectively navigate the challenging landscapes of\nbenchmark functions, capturing detailed logs of the optimization journey. The\nlogs include fitness evolution, step-size adjustments and restart events due to\nstagnation. An LLM is then utilized to process these logs, generating concise,\nuser-friendly summaries that highlight key aspects such as convergence\nbehavior, optimal fitness achievements, and encounters with local optima. Our\ncase study on the Rastrigin function demonstrates how our approach makes the\ncomplexities of ES optimization transparent. Our findings highlight the\npotential of using LLMs to bridge the gap between advanced optimization\nalgorithms and their interpretability.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.NE",
    "comment": "Accepted at ESANN 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08331v2",
    "published_date": "2024-07-11 09:28:27 UTC",
    "updated_date": "2024-08-05 08:13:37 UTC"
  },
  {
    "arxiv_id": "2407.08328v1",
    "title": "Unveiling Disparities in Maternity Care: A Topic Modelling Approach to Analysing Maternity Incident Investigation Reports",
    "authors": [
      "Georgina Cosma",
      "Mohit Kumar Singh",
      "Patrick Waterson",
      "Gyuchan Thomas Jun",
      "Jonathan Back"
    ],
    "abstract": "This study applies Natural Language Processing techniques, including Latent\nDirichlet Allocation, to analyse anonymised maternity incident investigation\nreports from the Healthcare Safety Investigation Branch. The reports underwent\npreprocessing, annotation using the Safety Intelligence Research taxonomy, and\ntopic modelling to uncover prevalent topics and detect differences in maternity\ncare across ethnic groups. A combination of offline and online methods was\nutilised to ensure data protection whilst enabling advanced analysis, with\noffline processing for sensitive data and online processing for non-sensitive\ndata using the `Claude 3 Opus' language model. Interactive topic analysis and\nsemantic network visualisation were employed to extract and display thematic\ntopics and visualise semantic relationships among keywords. The analysis\nrevealed disparities in care among different ethnic groups, with distinct focus\nareas for the Black, Asian, and White British ethnic groups. The study\ndemonstrates the effectiveness of topic modelling and NLP techniques in\nanalysing maternity incident investigation reports and highlighting disparities\nin care. The findings emphasise the crucial role of advanced data analysis in\nimproving maternity care quality and equity.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08328v1",
    "published_date": "2024-07-11 09:26:05 UTC",
    "updated_date": "2024-07-11 09:26:05 UTC"
  },
  {
    "arxiv_id": "2407.08324v1",
    "title": "A Cantor-Kantorovich Metric Between Markov Decision Processes with Application to Transfer Learning",
    "authors": [
      "Adrien Banse",
      "Venkatraman Renganathan",
      "Raphaël M. Jungers"
    ],
    "abstract": "We extend the notion of Cantor-Kantorovich distance between Markov chains\nintroduced by (Banse et al., 2023) in the context of Markov Decision Processes\n(MDPs). The proposed metric is well-defined and can be efficiently approximated\ngiven a finite horizon. Then, we provide numerical evidences that the latter\nmetric can lead to interesting applications in the field of reinforcement\nlearning. In particular, we show that it could be used for forecasting the\nperformance of transfer learning algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented at the 26th International Symposium on Mathematical Theory\n  of Networks and Systems (Cambridge, UK)",
    "pdf_url": "http://arxiv.org/pdf/2407.08324v1",
    "published_date": "2024-07-11 09:13:11 UTC",
    "updated_date": "2024-07-11 09:13:11 UTC"
  },
  {
    "arxiv_id": "2407.08322v1",
    "title": "Intelligent Multi-Document Summarisation for Extracting Insights on Racial Inequalities from Maternity Incident Investigation Reports",
    "authors": [
      "Georgina Cosma",
      "Mohit Kumar Singh",
      "Patrick Waterson",
      "Gyuchan Thomas Jun",
      "Jonathan Back"
    ],
    "abstract": "In healthcare, thousands of safety incidents occur every year, but learning\nfrom these incidents is not effectively aggregated. Analysing incident reports\nusing AI could uncover critical insights to prevent harm by identifying\nrecurring patterns and contributing factors. To aggregate and extract valuable\ninformation, natural language processing (NLP) and machine learning techniques\ncan be employed to summarise and mine unstructured data, potentially surfacing\nsystemic issues and priority areas for improvement. This paper presents\nI-SIRch:CS, a framework designed to facilitate the aggregation and analysis of\nsafety incident reports while ensuring traceability throughout the process. The\nframework integrates concept annotation using the Safety Intelligence Research\n(SIRch) taxonomy with clustering, summarisation, and analysis capabilities.\nUtilising a dataset of 188 anonymised maternity investigation reports annotated\nwith 27 SIRch human factors concepts, I-SIRch:CS groups the annotated sentences\ninto clusters using sentence embeddings and k-means clustering, maintaining\ntraceability via file and sentence IDs. Summaries are generated for each\ncluster using offline state-of-the-art abstractive summarisation models (BART,\nDistilBART, T5), which are evaluated and compared using metrics assessing\nsummary quality attributes. The generated summaries are linked back to the\noriginal file and sentence IDs, ensuring traceability and allowing for\nverification of the summarised information. Results demonstrate BART's\nstrengths in creating informative and concise summaries.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08322v1",
    "published_date": "2024-07-11 09:11:20 UTC",
    "updated_date": "2024-07-11 09:11:20 UTC"
  },
  {
    "arxiv_id": "2407.18945v2",
    "title": "CogNarr Ecosystem: Facilitating Group Cognition at Scale",
    "authors": [
      "John C. Boik"
    ],
    "abstract": "Human groups of all sizes and kinds engage in deliberation, problem solving,\nstrategizing, decision making, and more generally, cognition. Some groups are\nlarge, and that setting presents unique challenges. The small-group setting\noften involves face-to-face dialogue, but group cognition in the large-group\nsetting typically requires some form of online interaction. New approaches are\nneeded to facilitate the kind of rich communication and information processing\nthat are required for effective, functional cognition in the online setting,\nespecially for groups characterized by thousands to millions of participants\nwho wish to share potentially complex, nuanced, and dynamic perspectives. This\nconcept paper proposes the CogNarr (Cognitive Narrative) ecosystem, which is\ndesigned to facilitate functional cognition in the large-group setting. The\npaper's contribution is a novel vision as to how recent developments in\ncognitive science, artificial intelligence, natural language processing, and\nrelated fields might be scaled and applied to large-group cognition, using an\napproach that itself promotes further scientific advancement. A key perspective\nis to view a group as an organism that uses some form of cognitive architecture\nto sense the world, process information, remember, learn, predict, make\ndecisions, and adapt to changing conditions. The CogNarr ecosystem is designed\nto serve as a component within that architecture.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "F.4.1, I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.18945v2",
    "published_date": "2024-07-11 08:59:17 UTC",
    "updated_date": "2024-07-31 20:58:18 UTC"
  },
  {
    "arxiv_id": "2407.08306v3",
    "title": "Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training",
    "authors": [
      "Zijian Zhao"
    ],
    "abstract": "As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music\nUnderstanding (SMU) has garnered significant attention for its potential to\nassist both musicians and enthusiasts in learning and creating music. Recently,\npre-trained language models have been widely adopted in SMU due to the\nsubstantial similarities between symbolic music and natural language, as well\nas the ability of these models to leverage limited music data effectively.\nHowever, some studies have shown the common pre-trained methods like Mask\nLanguage Model (MLM) may introduce bias issues like racism discrimination in\nNatural Language Process (NLP) and affects the performance of downstream tasks,\nwhich also happens in SMU. This bias often arises when masked tokens cannot be\ninferred from their context, forcing the model to overfit the training set\ninstead of generalizing. To address this challenge, we propose\nAdversarial-MidiBERT for SMU, which adaptively determines what to mask during\nMLM via a masker network, rather than employing random masking. By avoiding the\nmasking of tokens that are difficult to infer from context, our model is better\nequipped to capture contextual structures and relationships, rather than merely\nconforming to the training data distribution. We evaluate our method across\nfour SMU tasks, and our approach demonstrates excellent performance in all\ncases. The code for our model is publicly available at\nhttps://github.com/RS2002/Adversarial-MidiBERT .",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08306v3",
    "published_date": "2024-07-11 08:54:38 UTC",
    "updated_date": "2025-04-30 05:22:05 UTC"
  },
  {
    "arxiv_id": "2407.08303v2",
    "title": "DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception",
    "authors": [
      "Xiaotong Li",
      "Fan Zhang",
      "Haiwen Diao",
      "Yueze Wang",
      "Xinlong Wang",
      "Ling-Yu Duan"
    ],
    "abstract": "Existing Multimodal Large Language Models (MLLMs) increasingly emphasize\ncomplex understanding of various visual elements, including multiple objects,\ntext information, and spatial relations. Their development for comprehensive\nvisual perception hinges on the availability of high-quality image-text\ndatasets that offer diverse visual elements and throughout image descriptions.\nHowever, the scarcity of such hyper-detailed datasets currently hinders\nprogress within the MLLM community. The bottleneck stems from the limited\nperceptual capabilities of current caption engines, which fall short in\nproviding complete and accurate annotations. To facilitate the cutting-edge\nresearch of MLLMs on comprehensive vision perception, we thereby propose\nPerceptual Fusion, using a low-budget but highly effective caption engine for\ncomplete and accurate image descriptions. Specifically, Perceptual Fusion\nintegrates diverse perception experts as image priors to provide explicit\ninformation on visual elements and adopts an efficient MLLM as a centric pivot\nto mimic advanced MLLMs' perception abilities. We carefully select 1M highly\nrepresentative images from uncurated LAION dataset and generate dense\ndescriptions using our engine, dubbed DenseFusion-1M. Extensive experiments\nvalidate that our engine outperforms its counterparts, where the resulting\ndataset significantly improves the perception and cognition abilities of\nexisting MLLMs across diverse vision-language benchmarks, especially with\nhigh-resolution images as inputs. The dataset and code are publicly available\nat https://github.com/baaivision/DenseFusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS 2024. Project is available at\n  https://github.com/baaivision/DenseFusion",
    "pdf_url": "http://arxiv.org/pdf/2407.08303v2",
    "published_date": "2024-07-11 08:48:06 UTC",
    "updated_date": "2024-11-24 13:14:47 UTC"
  },
  {
    "arxiv_id": "2407.08302v1",
    "title": "Impact Measures for Gradual Argumentation Semantics",
    "authors": [
      "Caren Al Anaissy",
      "Jérôme Delobelle",
      "Srdjan Vesic",
      "Bruno Yun"
    ],
    "abstract": "Argumentation is a formalism allowing to reason with contradictory\ninformation by modeling arguments and their interactions. There are now an\nincreasing number of gradual semantics and impact measures that have emerged to\nfacilitate the interpretation of their outcomes. An impact measure assesses,\nfor each argument, the impact of other arguments on its score. In this paper,\nwe refine an existing impact measure from Delobelle and Villata and introduce a\nnew impact measure rooted in Shapley values. We introduce several principles to\nevaluate those two impact measures w.r.t. some well-known gradual semantics.\nThis comprehensive analysis provides deeper insights into their functionality\nand desirability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08302v1",
    "published_date": "2024-07-11 08:47:44 UTC",
    "updated_date": "2024-07-11 08:47:44 UTC"
  },
  {
    "arxiv_id": "2407.08289v1",
    "title": "Predicting Heart Failure with Attention Learning Techniques Utilizing Cardiovascular Data",
    "authors": [
      "Ershadul Haque",
      "Manoranjan Paul",
      "Faranak Tohidi"
    ],
    "abstract": "Cardiovascular diseases (CVDs) encompass a group of disorders affecting the\nheart and blood vessels, including conditions such as coronary artery disease,\nheart failure, stroke, and hypertension. In cardiovascular diseases, heart\nfailure is one of the main causes of death and also long-term suffering in\npatients worldwide. Prediction is one of the risk factors that is highly\nvaluable for treatment and intervention to minimize heart failure. In this\nwork, an attention learning-based heart failure prediction approach is proposed\non EHR(electronic health record) cardiovascular data such as ejection fraction\nand serum creatinine. Moreover, different optimizers with various learning rate\napproaches are applied to fine-tune the proposed approach. Serum creatinine and\nejection fraction are the two most important features to predict the patient's\nheart failure. The computational result shows that the RMSProp optimizer with\n0.001 learning rate has a better prediction based on serum creatinine. On the\nother hand, the combination of SGD optimizer with 0.01 learning rate exhibits\noptimum performance based on ejection fraction features. Overall, the proposed\nattention learning-based approach performs very efficiently in predicting heart\nfailure compared to the existing state-of-the-art such as LSTM approach.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 37 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08289v1",
    "published_date": "2024-07-11 08:33:42 UTC",
    "updated_date": "2024-07-11 08:33:42 UTC"
  },
  {
    "arxiv_id": "2407.08279v1",
    "title": "Continually Learn to Map Visual Concepts to Large Language Models in Resource-constrained Environments",
    "authors": [
      "Clea Rebillard",
      "Julio Hurtado",
      "Andrii Krutsylo",
      "Lucia Passaro",
      "Vincenzo Lomonaco"
    ],
    "abstract": "Learning continually from a stream of non-i.i.d. data is an open challenge in\ndeep learning, even more so when working in resource-constrained environments\nsuch as embedded devices. Visual models that are continually updated through\nsupervised learning are often prone to overfitting, catastrophic forgetting,\nand biased representations. On the other hand, large language models contain\nknowledge about multiple concepts and their relations, which can foster a more\nrobust, informed and coherent learning process. This work proposes Continual\nVisual Mapping (CVM), an approach that continually ground vision\nrepresentations to a knowledge space extracted from a fixed Language model.\nSpecifically, CVM continually trains a small and efficient visual model to map\nits representations into a conceptual space established by a fixed Large\nLanguage Model. Due to their smaller nature, CVM can be used when directly\nadapting large visual pre-trained models is unfeasible due to computational or\ndata constraints. CVM overcome state-of-the-art continual learning methods on\nfive benchmarks and offers a promising avenue for addressing generalization\ncapabilities in continual learning, even in computationally constrained\ndevices.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08279v1",
    "published_date": "2024-07-11 08:28:40 UTC",
    "updated_date": "2024-07-11 08:28:40 UTC"
  },
  {
    "arxiv_id": "2407.08270v1",
    "title": "SciQu: Accelerating Materials Properties Prediction with Automated Literature Mining for Self-Driving Laboratories",
    "authors": [
      "Anand Babu"
    ],
    "abstract": "Assessing different material properties to predict specific attributes, such\nas band gap, resistivity, young modulus, work function, and refractive index,\nis a fundamental requirement for materials science-based applications. However,\nthe process is time-consuming and often requires extensive literature reviews\nand numerous experiments. Our study addresses these challenges by leveraging\nmachine learning to analyze material properties with greater precision and\nefficiency. By automating the data extraction process and using the extracted\ninformation to train machine learning models, our developed model, SciQu,\noptimizes material properties. As a proof of concept, we predicted the\nrefractive index of materials using data extracted from numerous research\narticles with SciQu, considering input descriptors such as space group, volume,\nand bandgap with Root Mean Square Error (RMSE) 0.068 and R2 0.94. Thus, SciQu\nnot only predicts the properties of materials but also plays a key role in\nself-driving laboratories by optimizing the synthesis parameters to achieve\nprecise shape, size, and phase of the materials subjected to the input\nparameters.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08270v1",
    "published_date": "2024-07-11 08:12:46 UTC",
    "updated_date": "2024-07-11 08:12:46 UTC"
  },
  {
    "arxiv_id": "2407.08257v1",
    "title": "Knowledge distillation to effectively attain both region-of-interest and global semantics from an image where multiple objects appear",
    "authors": [
      "Seonwhee Jin"
    ],
    "abstract": "Models based on convolutional neural networks (CNN) and transformers have\nsteadily been improved. They also have been applied in various computer vision\ndownstream tasks. However, in object detection tasks, accurately localizing and\nclassifying almost infinite categories of foods in images remains challenging.\nTo address these problems, we first segmented the food as the\nregion-of-interest (ROI) by using the segment-anything model (SAM) and masked\nthe rest of the region except ROI as black pixels. This process simplified the\nproblems into a single classification for which annotation and training were\nmuch simpler than object detection. The images in which only the ROI was\npreserved were fed as inputs to fine-tune various off-the-shelf models that\nencoded their own inductive biases. Among them, Data-efficient image\nTransformers (DeiTs) had the best classification performance. Nonetheless, when\nfoods' shapes and textures were similar, the contextual features of the\nROI-only images were not enough for accurate classification. Therefore, we\nintroduced a novel type of combined architecture, RveRNet, which consisted of\nROI, extra-ROI, and integration modules that allowed it to account for both the\nROI's and global contexts. The RveRNet's F1 score was 10% better than other\nindividual models when classifying ambiguous food images. If the RveRNet's\nmodules were DeiT with the knowledge distillation from the CNN, performed the\nbest. We investigated how architectures can be made robust against input noise\ncaused by permutation and translocation. The results indicated that there was a\ntrade-off between how much the CNN teacher's knowledge could be distilled to\nDeiT and DeiT's innate strength. Code is publicly available at:\nhttps://github.com/Seonwhee-Genome/RveRNet.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08257v1",
    "published_date": "2024-07-11 07:57:33 UTC",
    "updated_date": "2024-07-11 07:57:33 UTC"
  },
  {
    "arxiv_id": "2407.08254v2",
    "title": "United We Stand: Decentralized Multi-Agent Planning With Attrition",
    "authors": [
      "Nhat Nguyen",
      "Duong Nguyen",
      "Gianluca Rizzo",
      "Hung Nguyen"
    ],
    "abstract": "Decentralized planning is a key element of cooperative multi-agent systems\nfor information gathering tasks. However, despite the high frequency of agent\nfailures in realistic large deployment scenarios, current approaches perform\npoorly in the presence of failures, by not converging at all, and/or by making\nvery inefficient use of resources (e.g. energy). In this work, we propose\nAttritable MCTS (A-MCTS), a decentralized MCTS algorithm capable of timely and\nefficient adaptation to changes in the set of active agents. It is based on the\nuse of a global reward function for the estimation of each agent's local\ncontribution, and regret matching for coordination. We evaluate its\neffectiveness in realistic data-harvesting problems under different scenarios.\nWe show both theoretically and experimentally that A-MCTS enables efficient\nadaptation even under high failure rates. Results suggest that, in the presence\nof frequent failures, our solution improves substantially over the best\nexisting approaches in terms of global utility and scalability.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "To appear in ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08254v2",
    "published_date": "2024-07-11 07:55:50 UTC",
    "updated_date": "2024-09-02 02:35:50 UTC"
  },
  {
    "arxiv_id": "2407.08250v1",
    "title": "Gradient Boosting Reinforcement Learning",
    "authors": [
      "Benjamin Fuhrer",
      "Chen Tessler",
      "Gal Dalal"
    ],
    "abstract": "Neural networks (NN) achieve remarkable results in various tasks, but lack\nkey characteristics: interpretability, support for categorical features, and\nlightweight implementations suitable for edge devices. While ongoing efforts\naim to address these challenges, Gradient Boosting Trees (GBT) inherently meet\nthese requirements. As a result, GBTs have become the go-to method for\nsupervised learning tasks in many real-world applications and competitions.\nHowever, their application in online learning scenarios, notably in\nreinforcement learning (RL), has been limited. In this work, we bridge this gap\nby introducing Gradient-Boosting RL (GBRL), a framework that extends the\nadvantages of GBT to the RL domain. Using the GBRL framework, we implement\nvarious actor-critic algorithms and compare their performance with their NN\ncounterparts. Inspired by shared backbones in NN we introduce a tree-sharing\napproach for policy and value functions with distinct learning rates, enhancing\nlearning efficiency over millions of interactions. GBRL achieves competitive\nperformance across a diverse array of tasks, excelling in domains with\nstructured or categorical features. Additionally, we present a\nhigh-performance, GPU-accelerated implementation that integrates seamlessly\nwith widely-used RL libraries (available at https://github.com/NVlabs/gbrl).\nGBRL expands the toolkit for RL practitioners, demonstrating the viability and\npromise of GBT within the RL paradigm, particularly in domains characterized by\nstructured or categorical features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08250v1",
    "published_date": "2024-07-11 07:52:33 UTC",
    "updated_date": "2024-07-11 07:52:33 UTC"
  },
  {
    "arxiv_id": "2407.08249v1",
    "title": "GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and Configuration",
    "authors": [
      "Beni Ifland",
      "Elad Duani",
      "Rubin Krief",
      "Miro Ohana",
      "Aviram Zilberman",
      "Andres Murillo",
      "Ofir Manor",
      "Ortal Lavi",
      "Hikichi Kenji",
      "Asaf Shabtai",
      "Yuval Elovici",
      "Rami Puzis"
    ],
    "abstract": "Communication network engineering in enterprise environments is traditionally\na complex, time-consuming, and error-prone manual process. Most research on\nnetwork engineering automation has concentrated on configuration synthesis,\noften overlooking changes in the physical network topology. This paper\nintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet\nis a novel framework that leverages a large language model (LLM) to streamline\nnetwork design workflows. It uses visual and textual modalities to interpret\nand update network topologies and device configurations based on user intents.\nGeNet was evaluated on enterprise network scenarios adapted from Cisco\ncertification exercises. Our results demonstrate GeNet's ability to interpret\nnetwork topology images accurately, potentially reducing network engineers'\nefforts and accelerating network design processes in enterprise environments.\nFurthermore, we show the importance of precise topology understanding when\nhandling intents that require modifications to the network's topology.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08249v1",
    "published_date": "2024-07-11 07:51:57 UTC",
    "updated_date": "2024-07-11 07:51:57 UTC"
  },
  {
    "arxiv_id": "2407.08248v2",
    "title": "Toward accessible comics for blind and low vision readers",
    "authors": [
      "Christophe Rigaud",
      "Jean-Christophe Burie",
      "Samuel Petit"
    ],
    "abstract": "This work explores how to fine-tune large language models using prompt\nengineering techniques with contextual information for generating an accurate\ntext description of the full story, ready to be forwarded to off-the-shelve\nspeech synthesis tools. We propose to use existing computer vision and optical\ncharacter recognition techniques to build a grounded context from the comic\nstrip image content, such as panels, characters, text, reading order and the\nassociation of bubbles and characters. Then we infer character identification\nand generate comic book script with context-aware panel description including\ncharacter's appearance, posture, mood, dialogues etc. We believe that such\nenriched content description can be easily used to produce audiobook and eBook\nwith various voices for characters, captions and playing sound effects.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to MANPU 2024 (Athens, Greece, August 30, 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.08248v2",
    "published_date": "2024-07-11 07:50:25 UTC",
    "updated_date": "2024-09-10 07:59:21 UTC"
  },
  {
    "arxiv_id": "2407.08240v1",
    "title": "Leveraging LLMs to Predict Affective States via Smartphone Sensor Features",
    "authors": [
      "Tianyi Zhang",
      "Songyan Teng",
      "Hong Jia",
      "Simon D'Alfonso"
    ],
    "abstract": "As mental health issues for young adults present a pressing public health\nconcern, daily digital mood monitoring for early detection has become an\nimportant prospect. An active research area, digital phenotyping, involves\ncollecting and analysing data from personal digital devices such as smartphones\n(usage and sensors) and wearables to infer behaviours and mental health. Whilst\nthis data is standardly analysed using statistical and machine learning\napproaches, the emergence of large language models (LLMs) offers a new approach\nto make sense of smartphone sensing data. Despite their effectiveness across\nvarious domains, LLMs remain relatively unexplored in digital mental health,\nparticularly in integrating mobile sensor data. Our study aims to bridge this\ngap by employing LLMs to predict affect outcomes based on smartphone sensing\ndata from university students. We demonstrate the efficacy of zero-shot and\nfew-shot embedding LLMs in inferring general wellbeing. Our findings reveal\nthat LLMs can make promising predictions of affect measures using solely\nsmartphone sensing data. This research sheds light on the potential of LLMs for\naffective state prediction, emphasizing the intricate link between smartphone\nbehavioral patterns and affective states. To our knowledge, this is the first\nwork to leverage LLMs for affective state prediction and digital phenotyping\ntasks.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08240v1",
    "published_date": "2024-07-11 07:37:52 UTC",
    "updated_date": "2024-07-11 07:37:52 UTC"
  },
  {
    "arxiv_id": "2407.08227v3",
    "title": "DALL-M: Context-Aware Clinical Data Augmentation with LLMs",
    "authors": [
      "Chihcheng Hsieh",
      "Catarina Moreira",
      "Isabel Blanco Nobre",
      "Sandra Costa Sousa",
      "Chun Ouyang",
      "Margot Brereton",
      "Joaquim Jorge",
      "Jacinto C. Nascimento"
    ],
    "abstract": "X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating the integration\nof structured clinical features with radiology reports.\n  To address this, we introduce DALL-M, a novel framework that enhances\nclinical datasets by generating contextual synthetic data. DALL-M augments\nstructured patient data, including vital signs (e.g., heart rate, oxygen\nsaturation), radiology findings (e.g., lesion presence), and demographic\nfactors. It integrates this tabular data with contextual knowledge extracted\nfrom radiology reports and domain-specific resources (e.g., Radiopaedia,\nWikipedia), ensuring clinical consistency and reliability.\n  DALL-M follows a three-phase process: (i) clinical context storage, (ii)\nexpert query generation, and (iii) context-aware feature augmentation. Using\nlarge language models (LLMs), it generates both contextual synthetic values for\nexisting clinical features and entirely new, clinically relevant features.\n  Applied to 799 cases from the MIMIC-IV dataset, DALL-M expanded the original\n9 clinical features to 91. Empirical validation with machine learning models\n(including Decision Trees, Random Forests, XGBoost, and TabNET) demonstrated a\n16.5% improvement in F1 score and a 25% increase in Precision and Recall.\n  DALL-M bridges an important gap in clinical data augmentation by preserving\ndata integrity while enhancing predictive modeling in healthcare. Our results\nshow that integrating LLM-generated synthetic features significantly improves\nmodel performance, making DALL-M a scalable and practical approach for\nAI-driven medical diagnostics.",
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "I.5.1; J.3; H.3.3; I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08227v3",
    "published_date": "2024-07-11 07:01:50 UTC",
    "updated_date": "2025-03-15 06:25:38 UTC"
  },
  {
    "arxiv_id": "2407.08224v1",
    "title": "stEnTrans: Transformer-based deep learning for spatial transcriptomics enhancement",
    "authors": [
      "Shuailin Xue",
      "Fangfang Zhu",
      "Changmiao Wang",
      "Wenwen Min"
    ],
    "abstract": "The spatial location of cells within tissues and organs is crucial for the\nmanifestation of their specific functions.Spatial transcriptomics technology\nenables comprehensive measurement of the gene expression patterns in tissues\nwhile retaining spatial information. However, current popular spatial\ntranscriptomics techniques either have shallow sequencing depth or low\nresolution. We present stEnTrans, a deep learning method based on Transformer\narchitecture that provides comprehensive predictions for gene expression in\nunmeasured areas or unexpectedly lost areas and enhances gene expression in\noriginal and inputed spots. Utilizing a self-supervised learning approach,\nstEnTrans establishes proxy tasks on gene expression profile without requiring\nadditional data, mining intrinsic features of the tissues as supervisory\ninformation. We evaluate stEnTrans on six datasets and the results indicate\nsuperior performance in enhancing spots resolution and predicting gene\nexpression in unmeasured areas compared to other deep learning and traditional\ninterpolation methods. Additionally, Our method also can help the discovery of\nspatial patterns in Spatial Transcriptomics and enrich to more biologically\nsignificant pathways. Our source code is available at\nhttps://github.com/shuailinxue/stEnTrans.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "ISBRA2024, Code: https://github.com/shuailinxue/stEnTrans",
    "pdf_url": "http://arxiv.org/pdf/2407.08224v1",
    "published_date": "2024-07-11 06:50:34 UTC",
    "updated_date": "2024-07-11 06:50:34 UTC"
  },
  {
    "arxiv_id": "2407.08223v2",
    "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
    "authors": [
      "Zilong Wang",
      "Zifeng Wang",
      "Long Le",
      "Huaixiu Steven Zheng",
      "Swaroop Mishra",
      "Vincent Perot",
      "Yuwei Zhang",
      "Anush Mattapalli",
      "Ankur Taly",
      "Jingbo Shang",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of\nlarge language models (LLMs) with external knowledge sources to provide more\naccurate and up-to-date responses. Recent RAG advancements focus on improving\nretrieval outcomes through iterative LLM refinement or self-critique\ncapabilities acquired through additional instruction tuning of LLMs. In this\nwork, we introduce Speculative RAG - a framework that leverages a larger\ngeneralist LM to efficiently verify multiple RAG drafts produced in parallel by\na smaller, distilled specialist LM. Each draft is generated from a distinct\nsubset of retrieved documents, offering diverse perspectives on the evidence\nwhile reducing input token counts per draft. This approach enhances\ncomprehension of each subset and mitigates potential position bias over long\ncontext. Our method accelerates RAG by delegating drafting to the smaller\nspecialist LM, with the larger generalist LM performing a single verification\npass over the drafts. Extensive experiments demonstrate that Speculative RAG\nachieves state-of-the-art performance with reduced latency on TriviaQA,\nMuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances\naccuracy by up to 12.97% while reducing latency by 50.83% compared to\nconventional RAG systems on PubHealth.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.08223v2",
    "published_date": "2024-07-11 06:50:19 UTC",
    "updated_date": "2025-02-27 19:03:36 UTC"
  },
  {
    "arxiv_id": "2407.08216v1",
    "title": "Multimodal contrastive learning for spatial gene expression prediction using histology images",
    "authors": [
      "Wenwen Min",
      "Zhiceng Shi",
      "Jun Zhang",
      "Jun Wan",
      "Changmiao Wang"
    ],
    "abstract": "In recent years, the advent of spatial transcriptomics (ST) technology has\nunlocked unprecedented opportunities for delving into the complexities of gene\nexpression patterns within intricate biological systems. Despite its\ntransformative potential, the prohibitive cost of ST technology remains a\nsignificant barrier to its widespread adoption in large-scale studies. An\nalternative, more cost-effective strategy involves employing artificial\nintelligence to predict gene expression levels using readily accessible\nwhole-slide images (WSIs) stained with Hematoxylin and Eosin (H\\&E). However,\nexisting methods have yet to fully capitalize on multimodal information\nprovided by H&E images and ST data with spatial location. In this paper, we\npropose \\textbf{mclSTExp}, a multimodal contrastive learning with Transformer\nand Densenet-121 encoder for Spatial Transcriptomics Expression prediction. We\nconceptualize each spot as a \"word\", integrating its intrinsic features with\nspatial context through the self-attention mechanism of a Transformer encoder.\nThis integration is further enriched by incorporating image features via\ncontrastive learning, thereby enhancing the predictive capability of our model.\nOur extensive evaluation of \\textbf{mclSTExp} on two breast cancer datasets and\na skin squamous cell carcinoma dataset demonstrates its superior performance in\npredicting spatial gene expression. Moreover, mclSTExp has shown promise in\ninterpreting cancer-specific overexpressed genes, elucidating immune-related\ngenes, and identifying specialized spatial domains annotated by pathologists.\nOur source code is available at https://github.com/shizhiceng/mclSTExp.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM"
    ],
    "primary_category": "eess.IV",
    "comment": "BIB, Code: https://github.com/shizhiceng/mclSTExp",
    "pdf_url": "http://arxiv.org/pdf/2407.08216v1",
    "published_date": "2024-07-11 06:33:38 UTC",
    "updated_date": "2024-07-11 06:33:38 UTC"
  },
  {
    "arxiv_id": "2407.08214v1",
    "title": "Towards stable training of parallel continual learning",
    "authors": [
      "Li Yuepan",
      "Fan Lyu",
      "Yuyang Li",
      "Wei Feng",
      "Guangcan Liu",
      "Fanhua Shang"
    ],
    "abstract": "Parallel Continual Learning (PCL) tasks investigate the training methods for\ncontinual learning with multi-source input, where data from different tasks are\nlearned as they arrive. PCL offers high training efficiency and is well-suited\nfor complex multi-source data systems, such as autonomous vehicles equipped\nwith multiple sensors. However, at any time, multiple tasks need to be trained\nsimultaneously, leading to severe training instability in PCL. This instability\nmanifests during both forward and backward propagation, where features are\nentangled and gradients are conflict. This paper introduces Stable Parallel\nContinual Learning (SPCL), a novel approach that enhances the training\nstability of PCL for both forward and backward propagation. For the forward\npropagation, we apply Doubly-block Toeplit (DBT) Matrix based orthogonality\nconstraints to network parameters to ensure stable and consistent propagation.\nFor the backward propagation, we employ orthogonal decomposition for gradient\nmanagement stabilizes backpropagation and mitigates gradient conflicts across\ntasks. By optimizing gradients by ensuring orthogonality and minimizing the\ncondition number, SPCL effectively stabilizing the gradient descent in complex\noptimization tasks. Experimental results demonstrate that SPCL outperforms\nstate-of-the-art methjods and achieve better training stability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08214v1",
    "published_date": "2024-07-11 06:31:04 UTC",
    "updated_date": "2024-07-11 06:31:04 UTC"
  },
  {
    "arxiv_id": "2407.08204v1",
    "title": "Chromosomal Structural Abnormality Diagnosis by Homologous Similarity",
    "authors": [
      "Juren Li",
      "Fanzhe Fu",
      "Ran Wei",
      "Yifei Sun",
      "Zeyu Lai",
      "Ning Song",
      "Xin Chen",
      "Yang Yang"
    ],
    "abstract": "Pathogenic chromosome abnormalities are very common among the general\npopulation. While numerical chromosome abnormalities can be quickly and\nprecisely detected, structural chromosome abnormalities are far more complex\nand typically require considerable efforts by human experts for identification.\nThis paper focuses on investigating the modeling of chromosome features and the\nidentification of chromosomes with structural abnormalities. Most existing\ndata-driven methods concentrate on a single chromosome and consider each\nchromosome independently, overlooking the crucial aspect of homologous\nchromosomes. In normal cases, homologous chromosomes share identical\nstructures, with the exception that one of them is abnormal. Therefore, we\npropose an adaptive method to align homologous chromosomes and diagnose\nstructural abnormalities through homologous similarity. Inspired by the process\nof human expert diagnosis, we incorporate information from multiple pairs of\nhomologous chromosomes simultaneously, aiming to reduce noise disturbance and\nimprove prediction performance. Extensive experiments on real-world datasets\nvalidate the effectiveness of our model compared to baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08204v1",
    "published_date": "2024-07-11 06:04:21 UTC",
    "updated_date": "2024-07-11 06:04:21 UTC"
  },
  {
    "arxiv_id": "2407.08196v1",
    "title": "SoupLM: Model Integration in Large Language and Multi-Modal Models",
    "authors": [
      "Yue Bai",
      "Zichen Zhang",
      "Jiasen Lu",
      "Yun Fu"
    ],
    "abstract": "Training large language models (LLMs) and multimodal LLMs necessitates\nsignificant computing resources, and existing publicly available LLMs are\ntypically pre-trained on diverse, privately curated datasets spanning various\ntasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained\nwith LLaMA base models using very different training recipes, tasks, and data\nmodalities. The training cost and complexity for such LLM variants grow\nrapidly. In this study, we propose to use a soup strategy to assemble these LLM\nvariants into a single well-generalized multimodal LLM (SoupLM) in a\ncost-efficient manner. Assembling these LLM variants efficiently brings\nknowledge and specialities trained from different domains and data modalities\ninto an integrated one (e.g., chatbot speciality from user-shared conversations\nfor Vicuna, and visual capacity from vision-language data for LLaVA),\ntherefore, to avoid computing costs of repetitive training on several different\ndomains. We propose series of soup strategies to systematically benchmark\nperformance gains across various configurations, and probe the soup behavior\nacross base models in the interpolation space.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08196v1",
    "published_date": "2024-07-11 05:38:15 UTC",
    "updated_date": "2024-07-11 05:38:15 UTC"
  },
  {
    "arxiv_id": "2407.08195v2",
    "title": "A Text-to-Game Engine for UGC-Based Role-Playing Games",
    "authors": [
      "Lei Zhang",
      "Xuezheng Peng",
      "Shuyi Yang",
      "Feiyang Wang"
    ],
    "abstract": "The transition from professionally generated content (PGC) to user-generated\ncontent (UGC) has reshaped various media formats, encompassing formats such as\ntext and video. With rapid advancements in generative AI, a similar\ntransformation is set to redefine the gaming industry, particularly within the\ndomain of role-playing games (RPGs). This paper introduces a novel framework\nfor a text-to-game engine that leverages foundation models to transform simple\ntextual inputs into intricate, multi-modal RPG experiences. The engine\ndynamically generates game narratives, integrating text, visuals, and\nmechanics, while adapting characters, environments, and gameplay in realtime\nbased on player interactions. To evaluate and demonstrate the feasibility and\nversatility of this framework, we developed the 'Zagii' game engine. Zagii has\nsuccessfully powered hundreds of RPG games across diverse genres and\nfacilitated tens of thousands of online gameplay sessions, showcasing its\nscalability and adaptability. These results highlight the framework's\neffectiveness and its potential to foster a more open and democratized approach\nto game development. Our work underscores the transformative role of generative\nAI in reshaping the gaming lifecycle and advancing the boundaries of\ninteractive entertainment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2407.08195v2",
    "published_date": "2024-07-11 05:33:19 UTC",
    "updated_date": "2025-01-11 07:53:25 UTC"
  },
  {
    "arxiv_id": "2407.08192v3",
    "title": "Dynamic Co-Optimization Compiler: Leveraging Multi-Agent Reinforcement Learning for Enhanced DNN Accelerator Performance",
    "authors": [
      "Arya Fayyazi",
      "Mehdi Kamal",
      "Massoud Pedram"
    ],
    "abstract": "This paper introduces a novel Dynamic Co-Optimization Compiler (DCOC), which\nemploys an adaptive Multi-Agent Reinforcement Learning (MARL) framework to\nenhance the efficiency of mapping machine learning (ML) models, particularly\nDeep Neural Networks (DNNs), onto diverse hardware platforms. DCOC incorporates\nthree specialized actor-critic agents within MARL, each dedicated to different\noptimization facets: one for hardware and two for software. This cooperative\nstrategy results in an integrated hardware/software co-optimization approach,\nimproving the precision and speed of DNN deployments. By focusing on\nhigh-confidence configurations, DCOC effectively reduces the search space,\nachieving remarkable performance over existing methods. Our results demonstrate\nthat DCOC enhances throughput by up to 37.95% while reducing optimization time\nby up to 42.2% across various DNN models, outperforming current\nstate-of-the-art frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceeding of ASP-DAC25",
    "pdf_url": "http://arxiv.org/pdf/2407.08192v3",
    "published_date": "2024-07-11 05:22:04 UTC",
    "updated_date": "2025-02-21 21:17:06 UTC"
  },
  {
    "arxiv_id": "2407.08189v1",
    "title": "fairBERTs: Erasing Sensitive Information Through Semantic and Fairness-aware Perturbations",
    "authors": [
      "Jinfeng Li",
      "Yuefeng Chen",
      "Xiangyu Liu",
      "Longtao Huang",
      "Rong Zhang",
      "Hui Xue"
    ],
    "abstract": "Pre-trained language models (PLMs) have revolutionized both the natural\nlanguage processing research and applications. However, stereotypical biases\n(e.g., gender and racial discrimination) encoded in PLMs have raised negative\nethical implications for PLMs, which critically limits their broader\napplications. To address the aforementioned unfairness issues, we present\nfairBERTs, a general framework for learning fair fine-tuned BERT series models\nby erasing the protected sensitive information via semantic and fairness-aware\nperturbations generated by a generative adversarial network. Through extensive\nqualitative and quantitative experiments on two real-world tasks, we\ndemonstrate the great superiority of fairBERTs in mitigating unfairness while\nmaintaining the model utility. We also verify the feasibility of transferring\nadversarial components in fairBERTs to other conventionally trained BERT-like\nmodels for yielding fairness improvements. Our findings may shed light on\nfurther research on building fairer fine-tuned PLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08189v1",
    "published_date": "2024-07-11 05:13:38 UTC",
    "updated_date": "2024-07-11 05:13:38 UTC"
  },
  {
    "arxiv_id": "2407.12057v1",
    "title": "NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2",
    "authors": [
      "Tengfei Xue",
      "Xuefeng Li",
      "Roman Smirnov",
      "Tahir Azim",
      "Arash Sadrieh",
      "Babak Pahlavan"
    ],
    "abstract": "Retrieval-augmented generation (RAG) techniques are widely used today to\nretrieve and present information in a conversational format. This paper\npresents a set of enhancements to traditional RAG techniques, focusing on large\nlanguage models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI\nchips via SageMaker. These chips are characterized by their elasticity,\naffordability, and efficient performance for AI compute tasks. Besides enabling\ndeployment on these chips, this work aims to improve tool usage, add citation\ncapabilities, and mitigate the risks of hallucinations and unsafe responses due\nto context bias. We benchmark our RAG system's performance on the Natural\nQuestions and HotPotQA datasets, achieving an accuracy of 62% and 59%\nrespectively, exceeding other models such as DBRX and Mixtral Instruct.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.12057v1",
    "published_date": "2024-07-11 05:04:44 UTC",
    "updated_date": "2024-07-11 05:04:44 UTC"
  },
  {
    "arxiv_id": "2407.08179v1",
    "title": "CoGS: Causality Constrained Counterfactual Explanations using goal-directed ASP",
    "authors": [
      "Sopam Dasgupta",
      "Joaquín Arias",
      "Elmer Salazar",
      "Gopal Gupta"
    ],
    "abstract": "Machine learning models are increasingly used in areas such as loan approvals\nand hiring, yet they often function as black boxes, obscuring their\ndecision-making processes. Transparency is crucial, and individuals need\nexplanations to understand decisions, especially for the ones not desired by\nthe user. Ethical and legal considerations require informing individuals of\nchanges in input attribute values (features) that could lead to a desired\noutcome for the user. Our work aims to generate counterfactual explanations by\nconsidering causal dependencies between features. We present the CoGS\n(Counterfactual Generation with s(CASP)) framework that utilizes the\ngoal-directed Answer Set Programming system s(CASP) to generate counterfactuals\nfrom rule-based machine learning models, specifically the FOLD-SE algorithm.\nCoGS computes realistic and causally consistent changes to attribute values\ntaking causal dependencies between them into account. It finds a path from an\nundesired outcome to a desired one using counterfactuals. We present details of\nthe CoGS framework along with its evaluation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08179v1",
    "published_date": "2024-07-11 04:50:51 UTC",
    "updated_date": "2024-07-11 04:50:51 UTC"
  },
  {
    "arxiv_id": "2407.08176v1",
    "title": "Foundation Model Engineering: Engineering Foundation Models Just as Engineering Software",
    "authors": [
      "Dezhi Ran",
      "Mengzhou Wu",
      "Wei Yang",
      "Tao Xie"
    ],
    "abstract": "By treating data and models as the source code, Foundation Models (FMs)\nbecome a new type of software. Mirroring the concept of software crisis, the\nincreasing complexity of FMs making FM crisis a tangible concern in the coming\ndecade, appealing for new theories and methodologies from the field of software\nengineering. In this paper, we outline our vision of introducing Foundation\nModel (FM) engineering, a strategic response to the anticipated FM crisis with\nprincipled engineering methodologies. FM engineering aims to mitigate potential\nissues in FM development and application through the introduction of\ndeclarative, automated, and unified programming interfaces for both data and\nmodel management, reducing the complexities involved in working with FMs by\nproviding a more structured and intuitive process for developers. Through the\nestablishment of FM engineering, we aim to provide a robust, automated, and\nextensible framework that addresses the imminent challenges, and discovering\nnew research opportunities for the software engineering field.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted by 2030 Software Engineering Workshop, co-located with\n  FSE24; Invited to ACM TOSEM 2030 Roadmap for Software Engineering",
    "pdf_url": "http://arxiv.org/pdf/2407.08176v1",
    "published_date": "2024-07-11 04:40:02 UTC",
    "updated_date": "2024-07-11 04:40:02 UTC"
  },
  {
    "arxiv_id": "2407.08169v2",
    "title": "The Approximate Fisher Influence Function: Faster Estimation of Data Influence in Statistical Models",
    "authors": [
      "Omri Lev",
      "Ashia C. Wilson"
    ],
    "abstract": "Quantifying the influence of infinitesimal changes in training data on model\nperformance is crucial for understanding and improving machine learning models.\nIn this work, we reformulate this problem as a weighted empirical risk\nminimization and enhance existing influence function-based methods by using\ninformation geometry to derive a new algorithm to estimate influence. Our\nformulation proves versatile across various applications, and we further\ndemonstrate in simulations how it remains informative even in non-convex cases.\nFurthermore, we show that our method offers significant computational\nadvantages over current Newton step-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08169v2",
    "published_date": "2024-07-11 04:19:28 UTC",
    "updated_date": "2025-04-10 02:33:37 UTC"
  },
  {
    "arxiv_id": "2407.08166v1",
    "title": "Synthetic Electroretinogram Signal Generation Using Conditional Generative Adversarial Network for Enhancing Classification of Autism Spectrum Disorder",
    "authors": [
      "Mikhail Kulyabin",
      "Paul A. Constable",
      "Aleksei Zhdanov",
      "Irene O. Lee",
      "David H. Skuse",
      "Dorothy A. Thompson",
      "Andreas Maier"
    ],
    "abstract": "The electroretinogram (ERG) is a clinical test that records the retina's\nelectrical response to light. The ERG is a promising way to study different\nneurodevelopmental and neurodegenerative disorders, including autism spectrum\ndisorder (ASD) - a neurodevelopmental condition that impacts language,\ncommunication, and reciprocal social interactions. However, in heterogeneous\npopulations, such as ASD, where the ability to collect large datasets is\nlimited, the application of artificial intelligence (AI) is complicated.\nSynthetic ERG signals generated from real ERG recordings carry similar\ninformation as natural ERGs and, therefore, could be used as an extension for\nnatural data to increase datasets so that AI applications can be fully\nutilized. As proof of principle, this study presents a Generative Adversarial\nNetwork capable of generating synthetic ERG signals of children with ASD and\ntypically developing control individuals. We applied a Time Series Transformer\nand Visual Transformer with Continuous Wavelet Transform to enhance\nclassification results on the extended synthetic signals dataset. This approach\nmay support classification models in related psychiatric conditions where the\nERG may help classify disorders.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08166v1",
    "published_date": "2024-07-11 04:11:52 UTC",
    "updated_date": "2024-07-11 04:11:52 UTC"
  },
  {
    "arxiv_id": "2407.08164v2",
    "title": "Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks",
    "authors": [
      "Pu Feng",
      "Junkang Liang",
      "Size Wang",
      "Xin Yu",
      "Xin Ji",
      "Yiting Chen",
      "Kui Zhang",
      "Rongye Shi",
      "Wenjun Wu"
    ],
    "abstract": "In multi-agent reinforcement learning (MARL), the Centralized Training with\nDecentralized Execution (CTDE) framework is pivotal but struggles due to a gap:\nglobal state guidance in training versus reliance on local observations in\nexecution, lacking global signals. Inspired by human societal consensus\nmechanisms, we introduce the Hierarchical Consensus-based Multi-Agent\nReinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL\nemploys contrastive learning to foster a global consensus among agents,\nenabling cooperative behavior without direct communication. This approach\nenables agents to form a global consensus from local observations, using it as\nan additional piece of information to guide collaborative actions during\nexecution. To cater to the dynamic requirements of various tasks, consensus is\ndivided into multiple layers, encompassing both short-term and long-term\nconsiderations. Short-term observations prompt the creation of an immediate,\nlow-layer consensus, while long-term observations contribute to the formation\nof a strategic, high-layer consensus. This process is further refined through\nan adaptive attention mechanism that dynamically adjusts the influence of each\nconsensus layer. This mechanism optimizes the balance between immediate\nreactions and strategic planning, tailoring it to the specific demands of the\ntask at hand. Extensive experiments and real-world applications in multi-robot\nsystems showcase our framework's superior performance, marking significant\nadvancements over baselines.",
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 10 figures. Accepted for presentation at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2407.08164v2",
    "published_date": "2024-07-11 03:55:55 UTC",
    "updated_date": "2024-08-23 13:07:46 UTC"
  },
  {
    "arxiv_id": "2407.08152v2",
    "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
    "authors": [
      "Aydin Abadi",
      "Vishnu Asutosh Dasu",
      "Sumanta Sarkar"
    ],
    "abstract": "Deduplication is a vital preprocessing step that enhances machine learning\nmodel performance and saves training time and energy. However, enhancing\nfederated learning through deduplication poses challenges, especially regarding\nscalability and potential privacy violations if deduplication involves sharing\nall clients' data. In this paper, we address the problem of deduplication in a\nfederated setup by introducing a pioneering protocol, Efficient\nPrivacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes\nduplicates from multiple clients' datasets without compromising data privacy.\nEP-MPD is constructed in a modular fashion, utilizing two novel variants of the\nPrivate Set Intersection protocol. Our extensive experiments demonstrate the\nsignificant benefits of deduplication in federated learning of large language\nmodels. For instance, we observe up to 19.62\\% improvement in perplexity and up\nto 27.95\\% reduction in running time while varying the duplication level\nbetween 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in\nfederated learning, making it a valuable solution for large-scale applications.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted at the Network and Distributed Systems Security (NDSS)\n  Symposium, 2025",
    "pdf_url": "http://arxiv.org/pdf/2407.08152v2",
    "published_date": "2024-07-11 03:10:27 UTC",
    "updated_date": "2024-12-04 17:56:57 UTC"
  },
  {
    "arxiv_id": "2407.08147v1",
    "title": "Looks can be Deceptive: Distinguishing Repetition Disfluency from Reduplication",
    "authors": [
      "Arif Ahmad",
      "Mothika Gayathri Khyathi",
      "Pushpak Bhattacharyya"
    ],
    "abstract": "Reduplication and repetition, though similar in form, serve distinct\nlinguistic purposes. Reduplication is a deliberate morphological process used\nto express grammatical, semantic, or pragmatic nuances, while repetition is\noften unintentional and indicative of disfluency. This paper presents the first\nlarge-scale study of reduplication and repetition in speech using computational\nlinguistics. We introduce IndicRedRep, a new publicly available dataset\ncontaining Hindi, Telugu, and Marathi text annotated with reduplication and\nrepetition at the word level. We evaluate transformer-based models for\nmulti-class reduplication and repetition token classification, utilizing the\nReparandum-Interregnum-Repair structure to distinguish between the two\nphenomena. Our models achieve macro F1 scores of up to 85.62% in Hindi, 83.95%\nin Telugu, and 84.82% in Marathi for reduplication-repetition classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08147v1",
    "published_date": "2024-07-11 03:00:14 UTC",
    "updated_date": "2024-07-11 03:00:14 UTC"
  },
  {
    "arxiv_id": "2407.08134v1",
    "title": "Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates",
    "authors": [
      "A. Noorizadegan",
      "Y. C. Hon",
      "D. L. Young",
      "C. S. Chen"
    ],
    "abstract": "Surface reconstruction from point clouds is a fundamental challenge in\ncomputer graphics and medical imaging. In this paper, we explore the\napplication of advanced neural network architectures for the accurate and\nefficient reconstruction of surfaces from data points. We introduce a novel\nvariant of the Highway network (Hw) called Square-Highway (SqrHw) within the\ncontext of multilayer perceptrons and investigate its performance alongside\nplain neural networks and a simplified Hw in various numerical examples. These\nexamples include the reconstruction of simple and complex surfaces, such as\nspheres, human hands, and intricate models like the Stanford Bunny. We analyze\nthe impact of factors such as the number of hidden layers, interior and\nexterior points, and data distribution on surface reconstruction quality. Our\nresults show that the proposed SqrHw architecture outperforms other neural\nnetwork configurations, achieving faster convergence and higher-quality surface\nreconstructions. Additionally, we demonstrate the SqrHw's ability to predict\nsurfaces over missing data, a valuable feature for challenging applications\nlike medical imaging. Furthermore, our study delves into further details,\ndemonstrating that the proposed method based on highway networks yields more\nstable weight norms and backpropagation gradients compared to the Plain Network\narchitecture. This research not only advances the field of computer graphics\nbut also holds utility for other purposes such as function interpolation and\nphysics-informed neural networks, which integrate multilayer perceptrons into\ntheir algorithms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08134v1",
    "published_date": "2024-07-11 02:15:21 UTC",
    "updated_date": "2024-07-11 02:15:21 UTC"
  },
  {
    "arxiv_id": "2407.08133v2",
    "title": "Nonverbal Interaction Detection",
    "authors": [
      "Jianan Wei",
      "Tianfei Zhou",
      "Yi Yang",
      "Wenguan Wang"
    ],
    "abstract": "This work addresses a new challenge of understanding human nonverbal\ninteraction in social contexts. Nonverbal signals pervade virtually every\ncommunicative act. Our gestures, facial expressions, postures, gaze, even\nphysical appearance all convey messages, without anything being said. Despite\ntheir critical role in social life, nonverbal signals receive very limited\nattention as compared to the linguistic counterparts, and existing solutions\ntypically examine nonverbal cues in isolation. Our study marks the first\nsystematic effort to enhance the interpretation of multifaceted nonverbal\nsignals. First, we contribute a novel large-scale dataset, called NVI, which is\nmeticulously annotated to include bounding boxes for humans and corresponding\nsocial groups, along with 22 atomic-level nonverbal behaviors under five broad\ninteraction types. Second, we establish a new task NVI-DET for nonverbal\ninteraction detection, which is formalized as identifying triplets in the form\n<individual, group, interaction> from images. Third, we propose a nonverbal\ninteraction detection hypergraph (NVI-DEHR), a new approach that explicitly\nmodels high-order nonverbal interactions using hypergraphs. Central to the\nmodel is a dual multi-scale hypergraph that adeptly addresses\nindividual-to-individual and group-to-group correlations across varying scales,\nfacilitating interactional feature learning and eventually improving\ninteraction prediction. Extensive experiments on NVI show that NVI-DEHR\nimproves various baselines significantly in NVI-DET. It also exhibits leading\nperformance on HOI-DET, confirming its versatility in supporting related tasks\nand strong generalization ability. We hope that our study will offer the\ncommunity new avenues to explore nonverbal signals in more depth.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024; Project page: https://github.com/weijianan1/NVI",
    "pdf_url": "http://arxiv.org/pdf/2407.08133v2",
    "published_date": "2024-07-11 02:14:06 UTC",
    "updated_date": "2024-07-14 13:33:57 UTC"
  },
  {
    "arxiv_id": "2407.08126v1",
    "title": "Label-anticipated Event Disentanglement for Audio-Visual Video Parsing",
    "authors": [
      "Jinxing Zhou",
      "Dan Guo",
      "Yuxin Mao",
      "Yiran Zhong",
      "Xiaojun Chang",
      "Meng Wang"
    ],
    "abstract": "Audio-Visual Video Parsing (AVVP) task aims to detect and temporally locate\nevents within audio and visual modalities. Multiple events can overlap in the\ntimeline, making identification challenging. While traditional methods usually\nfocus on improving the early audio-visual encoders to embed more effective\nfeatures, the decoding phase -- crucial for final event classification, often\nreceives less attention. We aim to advance the decoding phase and improve its\ninterpretability. Specifically, we introduce a new decoding paradigm,\n\\underline{l}abel s\\underline{e}m\\underline{a}ntic-based \\underline{p}rojection\n(LEAP), that employs labels texts of event categories, each bearing distinct\nand explicit semantics, for parsing potentially overlapping events.LEAP works\nby iteratively projecting encoded latent features of audio/visual segments onto\nsemantically independent label embeddings. This process, enriched by modeling\ncross-modal (audio/visual-label) interactions, gradually disentangles event\nsemantics within video segments to refine relevant label embeddings,\nguaranteeing a more discriminative and interpretable decoding process. To\nfacilitate the LEAP paradigm, we propose a semantic-aware optimization\nstrategy, which includes a novel audio-visual semantic similarity loss\nfunction. This function leverages the Intersection over Union of audio and\nvisual events (EIoU) as a novel metric to calibrate audio-visual similarities\nat the feature level, accommodating the varied event densities across\nmodalities. Extensive experiments demonstrate the superiority of our method,\nachieving new state-of-the-art performance for AVVP and also enhancing the\nrelevant audio-visual event localization task.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by ECCV2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08126v1",
    "published_date": "2024-07-11 01:57:08 UTC",
    "updated_date": "2024-07-11 01:57:08 UTC"
  },
  {
    "arxiv_id": "2407.08112v2",
    "title": "How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities",
    "authors": [
      "Jerry Huang"
    ],
    "abstract": "Long sequences occur in abundance within real-world scenarios, hence properly\nmodelling them opens numerous down-stream use-cases. Deep neural networks,\nhowever, have often struggled with these for a variety of reasons. Recent\nadvances, both in system engineering as well as model design, have enabled the\nscaling up of model that are purported to support extended context length. In\nparticular, the state-space and linear recurrent neural network families of\nmodels hypothetically can entend to infinite sequence lenth. However, is this\ntoo good to be true? We conduct an evaluation to show that while such claims\nmay be sound theoretically, there remain large practical gaps that are\nempirically observed. In particular, recurrent models still suffer in the same\nsettings as long-context LLMs with attention. We further show that different\ninductive biases have inconsistent extrapolation capabilities, highlighting the\nneed to further study such paradigms and investigate why long-context models\nseemingly fail to behave as one might expect.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Work In Progress. 9 pages",
    "pdf_url": "http://arxiv.org/pdf/2407.08112v2",
    "published_date": "2024-07-11 01:08:39 UTC",
    "updated_date": "2024-07-26 17:31:51 UTC"
  },
  {
    "arxiv_id": "2407.08109v1",
    "title": "Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter",
    "authors": [
      "Suqi Song",
      "Chenxu Zhang",
      "Peng Zhang",
      "Pengkun Li",
      "Fenglong Song",
      "Lei Zhang"
    ],
    "abstract": "Urban waterlogging poses a major risk to public safety and infrastructure.\nConventional methods using water-level sensors need high-maintenance to hardly\nachieve full coverage. Recent advances employ surveillance camera imagery and\ndeep learning for detection, yet these struggle amidst scarce data and adverse\nenvironmental conditions. In this paper, we establish a challenging Urban\nWaterlogging Benchmark (UW-Bench) under diverse adverse conditions to advance\nreal-world applications. We propose a Large-Small Model co-adapter paradigm\n(LSM-adapter), which harnesses the substantial generic segmentation potential\nof large model and the specific task-directed guidance of small model.\nSpecifically, a Triple-S Prompt Adapter module alongside a Dynamic Prompt\nCombiner are proposed to generate then merge multiple prompts for mask decoder\nadaptation. Meanwhile, a Histogram Equalization Adap-ter module is designed to\ninfuse the image specific information for image encoder adaptation. Results and\nanalysis show the challenge and superiority of our developed benchmark and\nalgorithm. Project page: \\url{https://github.com/zhang-chenxu/LSM-Adapter}",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2407.08109v1",
    "published_date": "2024-07-11 01:03:02 UTC",
    "updated_date": "2024-07-11 01:03:02 UTC"
  },
  {
    "arxiv_id": "2407.08108v2",
    "title": "CADC: Encoding User-Item Interactions for Compressing Recommendation Model Training Data",
    "authors": [
      "Hossein Entezari Zarch",
      "Abdulla Alshabanah",
      "Chaoyi Jiang",
      "Murali Annavaram"
    ],
    "abstract": "Deep learning recommendation models (DLRMs) are at the heart of the current\ne-commerce industry. However, the amount of training data used to train these\nlarge models is growing exponentially, leading to substantial training hurdles.\nThe training dataset contains two primary types of information: content-based\ninformation (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce the training\ndataset is to remove user-item interactions. But that significantly diminishes\ncollaborative information, which is crucial for maintaining accuracy due to its\ninclusion of interaction histories. This loss profoundly impacts DLRM\nperformance.\n  This paper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embeddings, then the\ninteraction history can be compressed without losing model accuracy. Thus, this\nwork, Collaborative Aware Data Compression (CADC), takes a two-step approach to\ntraining dataset compression. In the first step, we use matrix factorization of\nthe user-item interaction matrix to create a novel embedding representation for\nboth the users and items. Once the user and item embeddings are enriched by the\ninteraction history information the approach then applies uniform random\nsampling of the training dataset to drastically reduce the training dataset\nsize while minimizing model accuracy drop. The source code of CADC is available\nat\n\\href{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.08108v2",
    "published_date": "2024-07-11 00:54:56 UTC",
    "updated_date": "2024-07-24 03:37:17 UTC"
  },
  {
    "arxiv_id": "2407.08105v2",
    "title": "Federated Learning and AI Regulation in the European Union: Who is Responsible? -- An Interdisciplinary Analysis",
    "authors": [
      "Herbert Woisetschläger",
      "Simon Mertel",
      "Christoph Krönke",
      "Ruben Mayer",
      "Hans-Arno Jacobsen"
    ],
    "abstract": "The European Union Artificial Intelligence Act mandates clear stakeholder\nresponsibilities in developing and deploying machine learning applications to\navoid substantial fines, prioritizing private and secure data processing with\ndata remaining at its origin. Federated Learning (FL) enables the training of\ngenerative AI Models across data siloes, sharing only model parameters while\nimproving data security. Since FL is a cooperative learning paradigm, clients\nand servers naturally share legal responsibility in the FL pipeline. Our work\ncontributes to clarifying the roles of both parties, explains strategies for\nshifting responsibilities to the server operator, and points out open technical\nchallenges that we must solve to improve FL's practical applicability under the\nEU AI Act.",
    "categories": [
      "cs.AI",
      "K.5; I.2.11; C.2.4; D.2.1"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at the GenLaw'24 workshop in conjunction with ICML'24",
    "pdf_url": "http://arxiv.org/pdf/2407.08105v2",
    "published_date": "2024-07-11 00:41:16 UTC",
    "updated_date": "2024-07-12 13:37:53 UTC"
  }
]
{
  "date": "2024-06-02",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-06-02 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型的创新应用、计算机视觉和医疗领域的研究，其中 LLMs（Large Language Models）在知识推理、图像生成和多模态处理上的进展最为引人注目，Stuart Russell 等知名学者参与的论文（如 Evidence of Learned Look-Ahead）令人印象深刻，同时强调了 AI 在实际问题中的鲁棒性和效率提升。\n\n下面，我挑选并简要概述几篇重要的、话题度高的论文，先从 AI 和 LLMs 相关的内容入手，再聊聊计算机视觉和医疗领域的亮点。对于其他较常规或非核心论文（如体育分析、金融预测等），我将快速掠过，只列出标题而不深入讨论。\n\n### AI 和 LLMs 相关\n- **Pretrained Hybrids with MAD Skills（预训练混合模型）**：论文提出 Manticore 框架，通过可微神经架构搜索和投影器自动设计混合架构（如 GPT 和 Mamba 的结合），显著提升了语言模型在长序列任务上的性能，超越手动设计混合模型，并在 Long Range Arena 基准上表现出色。\n- **Evidence of Learned Look-Ahead in a Chess-Playing Neural Network（国际象棋神经网络中学习的前瞻证据）**：Stuart Russell 等学者参与，这篇论文证明了 Leela Chess Zero 模型学会了前瞻搜索机制，通过注意力头和探针分析，展示了神经网络在棋盘状态下的决策能力，为理解 AI 算法学习提供了存在性证明。\n- **OLIVE: Object Level In-Context Visual Embeddings（对象级上下文视觉嵌入）**：该工作利用视觉语言模型改进对象级理解，通过区域检索和提示技术，实现零样本泛化和鲁棒性，在指称对象分类和描述任务中表现出色。\n- **FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models（通过对比自剽窃提升语言模型原创性）**：论文引入自剽窃对比解码策略，减少语言模型生成重复序列，提高文本原创性，在 AASC 和 ROCStories 数据集上显著降低非原创序列。\n- **Teams of LLM Agents can Exploit Zero-Day Vulnerabilities（LLM 代理团队可利用零日漏洞）**：研究显示 LLM 代理团队（如 HPTSA 系统）能在未知漏洞下进行攻击，强调了 AI 安全风险，并在真实漏洞基准上提升了 4.3 倍性能。\n- **LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models（扩展大型语言模型上下文长度的训练方案）**：该论文提出高效训练方法，通过合成数据和 SFT 阶段，将模型上下文扩展到 20 万 tokens，并在 Needle 测试中实现完美准确率，接近 Claude2.1 的水平。\n- **Brainstorming Brings Power to Large Language Models of Knowledge Reasoning（脑暴增强大型语言模型的知识推理能力）**：论文探索多模型脑暴机制，通过迭代推理从多个 LLM 中得出共识，显著提升逻辑推理和事实提取性能。\n- **Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction（评估大型语言模型的数学推理：关注错误识别和修正）**：研究设计了错误识别任务，使用提示优化提升了模型的数学推理准确性，在多个基准上表现出色。\n\n这些 AI 论文突出了 LLMs 在推理、多模态和安全方面的潜力，强调了通过脑暴和混合架构提升模型鲁棒性的重要性。\n\n### 计算机视觉和图像处理\n- **Robust Multi-Modal Speech In-Painting: A Sequence-to-Sequence Approach（鲁棒多模态语音修复：序列到序列方法）**：论文提出 seq2seq 模型融合音频和视觉特征，处理语音和视频失真，提高语音质量和可懂度 38.8% 和 7.14%，适用于真实环境。\n- **PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency（基于 3D 空间一致性的分段数据集修剪）**：该工作通过影响函数和深度估计优化数据集，增强 NeRF 在存在干扰时的鲁棒性，在基准测试中显著提升性能。\n- **Diffusion Features to Bridge Domain Gap for Semantic Segmentation（扩散特征桥接语义分割的领域差距）**：论文利用扩散模型的特征融合，改善跨域语义分割效果，实现 SOTA 性能。\n- **SuperGaussian: Repurposing Video Models for 3D Super Resolution（视频模型重用于 3D 超分辨率）**：研究将视频上采样模型应用于 3D 高保真重建，通过高斯模型提升细节和轮廓一致性。\n\n这些论文展示了计算机视觉在多模态和 3D 处理上的创新，特别强调了扩散模型在桥接领域差距中的作用。\n\n### 医疗和生物应用\n- **Scaffold Splits Overestimate Virtual Screening Performance（支架分割高估虚拟筛选性能）**：论文揭示了虚拟筛选中数据分割偏差，使用 UMAP 聚类改进模型评估，在 NCI-60 数据集上显著降低错误。\n- **A Diagnostic Model for Acute Lymphoblastic Leukemia Using Metaheuristics and Deep Learning Methods（使用元启发式和深度学习的白血病诊断模型）**：该工作结合 ResNet 和特征选择算法，实现了 90.71% 的准确率，用于白血病细胞检测。\n- **Multimodal Deep Learning for Low-Resource Settings（面向资源有限环境的 multimodal 深度学习）**：论文提出嵌入向量对齐方法，提升医疗图像分类性能，支持 CPU 环境，适用于低资源地区。\n\n其他论文如体育分析（Expected Possession Value）和游戏 AI（Learning to Play 7 Wonders Duel）等，虽然有趣但非核心，我仅快速提及：**Expected Possession Value of Control and Duel Actions for Soccer Player's Skills Estimation（足球球员技能估计的控制和对抗预期占有值）**：扩展 EPV 模型评估球员能力；**Learning to Play 7 Wonders Duel Without Human Supervision（无需人类监督学习玩 7 Wonders Duel）**：使用强化学习实现自主游戏策略。\n\n总之，今天的 arXiv 论文展示了 AI 领域的快速迭代，LLMs 和视觉模型的进展可能对实际应用如医疗和安全带来深远影响。感兴趣的读者可查阅具体论文深入探索！",
  "papers": [
    {
      "arxiv_id": "2406.00901v1",
      "title": "Robust Multi-Modal Speech In-Painting: A Sequence-to-Sequence Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Mahsa Kadkhodaei Elyaderani",
        "Shahram Shirani"
      ],
      "abstract": "The process of reconstructing missing parts of speech audio from context is\ncalled speech in-painting. Human perception of speech is inherently\nmulti-modal, involving both audio and visual (AV) cues. In this paper, we\nintroduce and study a sequence-to-sequence (seq2seq) speech in-painting model\nthat incorporates AV features. Our approach extends AV speech in-painting\ntechniques to scenarios where both audio and visual data may be jointly\ncorrupted. To achieve this, we employ a multi-modal training paradigm that\nboosts the robustness of our model across various conditions involving acoustic\nand visual distortions. This makes our distortion-aware model a plausible\nsolution for real-world challenging environments. We compare our method with\nexisting transformer-based and recurrent neural network-based models, which\nattempt to reconstruct missing speech gaps ranging from a few milliseconds to\nover a second. Our experimental results demonstrate that our novel seq2seq\narchitecture outperforms the state-of-the-art transformer solution by 38.8% in\nterms of enhancing speech quality and 7.14% in terms of improving speech\nintelligibility. We exploit a multi-task learning framework that simultaneously\nperforms lip-reading (transcribing video components to text) while\nreconstructing missing parts of the associated speech.",
      "tldr_zh": "本文提出了一种鲁棒的多模态语音修复（speech in-painting）方法，使用 sequence-to-sequence (seq2seq) 模型整合音频和视觉（AV）特征，以处理音频和视觉数据可能同时损坏的场景。该方法采用多模态训练范式和多任务学习框架，同时进行唇读（lip-reading，将视频组件转录为文本）和缺失语音重建，提高了模型在声学和视觉失真条件下的鲁棒性。实验结果表明，与现有 Transformer 和 RNN 模型相比，该 seq2seq 架构在提升语音质量方面提高了38.8%，在改善语音可懂度方面提高了7.14%。这为真实世界挑战环境下的语音处理提供了可行的解决方案。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00901v1",
      "published_date": "2024-06-02 23:51:43 UTC",
      "updated_date": "2024-06-02 23:51:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:02:59.995049"
    },
    {
      "arxiv_id": "2406.00894v1",
      "title": "Pretrained Hybrids with MAD Skills",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas Roberts",
        "Samuel Guo",
        "Zhiqi Gao",
        "Satya Sai Srinath Namburi GNVV",
        "Sonia Cromp",
        "Chengjun Wu",
        "Chengyu Duan",
        "Frederic Sala"
      ],
      "abstract": "While Transformers underpin modern large language models (LMs), there is a\ngrowing list of alternative architectures with new capabilities, promises, and\ntradeoffs. This makes choosing the right LM architecture challenging.\nRecently-proposed $\\textit{hybrid architectures}$ seek a best-of-all-worlds\napproach that reaps the benefits of all architectures. Hybrid design is\ndifficult for two reasons: it requires manual expert-driven search, and new\nhybrids must be trained from scratch. We propose $\\textbf{Manticore}$, a\nframework that addresses these challenges. Manticore $\\textit{automates the\ndesign of hybrid architectures}$ while reusing pretrained models to create\n$\\textit{pretrained}$ hybrids. Our approach augments ideas from differentiable\nNeural Architecture Search (NAS) by incorporating simple projectors that\ntranslate features between pretrained blocks from different architectures. We\nthen fine-tune hybrids that combine pretrained models from different\narchitecture families -- such as the GPT series and Mamba -- end-to-end. With\nManticore, we enable LM selection without training multiple models, the\nconstruction of pretrained hybrids from existing pretrained models, and the\nability to $\\textit{program}$ pretrained hybrids to have certain capabilities.\nManticore hybrids outperform existing manually-designed hybrids, achieve strong\nperformance on Long Range Arena (LRA) tasks, and can improve on pretrained\ntransformers and state space models.",
      "tldr_zh": "该研究提出 Manticore 框架，用于自动化设计预训练混合架构（pretrained hybrids），以结合不同语言模型（如 Transformers、GPT 和 Mamba）的优势，解决手动设计和从零训练的挑战。Manticore 基于可微神经架构搜索（Differentiable NAS），通过添加简单投影器来转换不同架构的特征，并端到端微调预训练模型，实现高效的混合构建。实验结果表明，Manticore 混合模型优于现有手动设计的混合模型，在 Long Range Arena (LRA) 任务上表现出色，并提升了预训练 Transformers 和状态空间模型的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00894v1",
      "published_date": "2024-06-02 23:24:30 UTC",
      "updated_date": "2024-06-02 23:24:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:03:11.800760"
    },
    {
      "arxiv_id": "2406.00877v1",
      "title": "Evidence of Learned Look-Ahead in a Chess-Playing Neural Network",
      "title_zh": "翻译失败",
      "authors": [
        "Erik Jenner",
        "Shreyas Kapur",
        "Vasil Georgiev",
        "Cameron Allen",
        "Scott Emmons",
        "Stuart Russell"
      ],
      "abstract": "Do neural networks learn to implement algorithms such as look-ahead or search\n\"in the wild\"? Or do they rely purely on collections of simple heuristics? We\npresent evidence of learned look-ahead in the policy network of Leela Chess\nZero, the currently strongest neural chess engine. We find that Leela\ninternally represents future optimal moves and that these representations are\ncrucial for its final output in certain board states. Concretely, we exploit\nthe fact that Leela is a transformer that treats every chessboard square like a\ntoken in language models, and give three lines of evidence (1) activations on\ncertain squares of future moves are unusually important causally; (2) we find\nattention heads that move important information \"forward and backward in time,\"\ne.g., from squares of future moves to squares of earlier ones; and (3) we train\na simple probe that can predict the optimal move 2 turns ahead with 92%\naccuracy (in board states where Leela finds a single best line). These findings\nare an existence proof of learned look-ahead in neural networks and might be a\nstep towards a better understanding of their capabilities.",
      "tldr_zh": "本研究探讨了神经网络是否在实际应用中学会了 look-ahead 或搜索算法，而非仅依赖简单启发式，通过分析 Leela Chess Zero 的策略网络提供了证据。研究者利用 transformer 架构，将棋盘方格视为 token，观察到未来最优走子的内部表示，并通过三条证据验证：激活因果重要性、注意力 heads 的时间信息转移，以及训练的探针能以92%准确率预测两步后的最优走子。结果表明，Leela Chess Zero 确实学会了 look-ahead，这为理解神经网络能力提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Project page: https://leela-interp.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2406.00877v1",
      "published_date": "2024-06-02 21:57:32 UTC",
      "updated_date": "2024-06-02 21:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:03:24.829814"
    },
    {
      "arxiv_id": "2406.00873v2",
      "title": "Scaffold Splits Overestimate Virtual Screening Performance",
      "title_zh": "支架分割高估虚拟筛选性能",
      "authors": [
        "Qianrong Guo",
        "Saiveth Hernandez-Hernandez",
        "Pedro J Ballester"
      ],
      "abstract": "Virtual Screening (VS) of vast compound libraries guided by Artificial\nIntelligence (AI) models is a highly productive approach to early drug\ndiscovery. Data splitting is crucial for better benchmarking of such AI models.\nTraditional random data splits produce similar molecules between training and\ntest sets, conflicting with the reality of VS libraries which mostly contain\nstructurally distinct compounds. Scaffold split, grouping molecules by shared\ncore structure, is widely considered to reflect this real-world scenario.\nHowever, here we show that the scaffold split also overestimates VS\nperformance. The reason is that molecules with different chemical scaffolds are\noften similar, which hence introduces unrealistically high similarities between\ntraining molecules and test molecules following a scaffold split. Our study\nexamined three representative AI models on 60 NCI-60 datasets, each with\napproximately 30,000 to 50,000 molecules tested on a different cancer cell\nline. Each dataset was split with three methods: scaffold, Butina clustering\nand the more accurate Uniform Manifold Approximation and Projection (UMAP)\nclustering. Regardless of the model, model performance is much worse with UMAP\nsplits from the results of the 2100 models trained and evaluated for each\nalgorithm and split. These robust results demonstrate the need for more\nrealistic data splits to tune, compare, and select models for VS. For the same\nreason, avoiding the scaffold split is also recommended for other molecular\nproperty prediction problems. The code to reproduce these results is available\nat https://github.com/ScaffoldSplitsOverestimateVS",
      "tldr_zh": "本研究揭示了在虚拟筛选 (VS) 任务中，Scaffold split 数据分割方法高估了人工智能 (AI) 模型的性能，因为不同化学支架的分子往往具有不现实的高相似性，导致训练集和测试集不充分分离。研究者使用三种分割方法（Scaffold、Butina clustering 和 UMAP clustering）对60个NCI-60数据集进行测试，每个数据集包含约30,000至50,000分子，并评估了三种代表性AI模型。结果显示，无论哪种模型，在UMAP clustering分割下性能显著低于Scaffold split，证明了现有分割方法的局限性。该发现强调需要采用更真实的分割策略来优化、比较和选择VS模型，并建议其他分子属性预测问题避免使用Scaffold split。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CE",
        "cs.LG",
        "q-bio.BM"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00873v2",
      "published_date": "2024-06-02 21:40:13 UTC",
      "updated_date": "2024-06-30 12:12:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:03:35.775035"
    },
    {
      "arxiv_id": "2406.00872v1",
      "title": "OLIVE: Object Level In-Context Visual Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Timothy Ossowski",
        "Junjie Hu"
      ],
      "abstract": "Recent generalist vision-language models (VLMs) have demonstrated impressive\nreasoning capabilities across diverse multimodal tasks. However, these models\nstill struggle with fine-grained object-level understanding and grounding. In\nterms of modeling, existing VLMs implicitly align text tokens with image patch\ntokens, which is ineffective for embedding alignment at the same granularity\nand inevitably introduces noisy spurious background features. Additionally,\nthese models struggle when generalizing to unseen visual concepts and may not\nbe reliable for domain-specific tasks without further fine-tuning. To address\nthese limitations, we propose a novel method to prompt large language models\nwith in-context visual object vectors, thereby enabling controllable\nobject-level reasoning. This eliminates the necessity of fusing a lengthy array\nof image patch features and significantly speeds up training. Furthermore, we\npropose region-level retrieval using our object representations, facilitating\nrapid adaptation to new objects without additional training. Our experiments\nreveal that our method achieves competitive referring object classification and\ncaptioning performance, while also offering zero-shot generalization and\nrobustness to visually challenging contexts.",
      "tldr_zh": "本研究提出OLIVE方法，通过对象级in-context visual embeddings提示大型语言模型(LLMs)，以解决现有视觉语言模型(VLMs)在细粒度对象理解和grounding方面的局限性，例如隐式对齐导致的噪音背景特征和泛化能力不足。OLIVE避免融合大量图像patch特征，改用可控的对象级推理，并引入region-level retrieval技术，实现快速适应新对象而无需额外训练。实验结果显示，该方法在referring object classification和captioning任务上表现出色，具有zero-shot generalization能力，并对视觉挑战环境具有鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00872v1",
      "published_date": "2024-06-02 21:36:31 UTC",
      "updated_date": "2024-06-02 21:36:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:03:47.067902"
    },
    {
      "arxiv_id": "2406.00867v1",
      "title": "Formality Style Transfer in Persian",
      "title_zh": "翻译失败",
      "authors": [
        "Parastoo Falakaflaki",
        "Mehrnoush Shamsfard"
      ],
      "abstract": "This study explores the formality style transfer in Persian, particularly\nrelevant in the face of the increasing prevalence of informal language on\ndigital platforms, which poses challenges for existing Natural Language\nProcessing (NLP) tools. The aim is to transform informal text into formal while\nretaining the original meaning, addressing both lexical and syntactic\ndifferences. We introduce a novel model, Fa-BERT2BERT, based on the Fa-BERT\narchitecture, incorporating consistency learning and gradient-based dynamic\nweighting. This approach improves the model's understanding of syntactic\nvariations, balancing loss components effectively during training. Our\nevaluation of Fa-BERT2BERT against existing methods employs new metrics\ndesigned to accurately measure syntactic and stylistic changes. Results\ndemonstrate our model's superior performance over traditional techniques across\nvarious metrics, including BLEU, BERT score, Rouge-l, and proposed metrics\nunderscoring its ability to adeptly navigate the complexities of Persian\nlanguage style transfer. This study significantly contributes to Persian\nlanguage processing by enhancing the accuracy and functionality of NLP models\nand thereby supports the development of more efficient and reliable NLP\napplications, capable of handling language style transformation effectively,\nthereby streamlining content moderation, enhancing data mining results, and\nfacilitating cross-cultural communication.",
      "tldr_zh": "这篇论文探讨了波斯语的正式风格转换，旨在将非正式文本转化为正式文本，同时保留原意，以应对数字平台上非正式语言对NLP工具的挑战，并处理词汇和句法差异。研究引入了Fa-BERT2BERT模型，该模型基于Fa-BERT架构，结合一致性学习和梯度-based动态加权，有效平衡训练中的损失组件并提升对句法变化的理解。实验结果显示，Fa-BERT2BERT在BLEU、BERT score、Rouge-l等指标上优于传统方法，证明其在波斯语风格转换中的出色性能。该研究为波斯语处理领域做出了重要贡献，提升了NLP应用的准确性和功能，支持内容审核、数据挖掘及跨文化交流。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, 4 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.00867v1",
      "published_date": "2024-06-02 20:57:27 UTC",
      "updated_date": "2024-06-02 20:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:03:59.248787"
    },
    {
      "arxiv_id": "2406.00855v1",
      "title": "LinkLogic: A New Method and Benchmark for Explainable Knowledge Graph Predictions",
      "title_zh": "翻译失败",
      "authors": [
        "Niraj Kumar-Singh",
        "Gustavo Polleti",
        "Saee Paliwal",
        "Rachel Hodos-Nkhereanye"
      ],
      "abstract": "While there are a plethora of methods for link prediction in knowledge\ngraphs, state-of-the-art approaches are often black box, obfuscating model\nreasoning and thereby limiting the ability of users to make informed decisions\nabout model predictions. Recently, methods have emerged to generate prediction\nexplanations for Knowledge Graph Embedding models, a widely-used class of\nmethods for link prediction. The question then becomes, how well do these\nexplanation systems work? To date this has generally been addressed\nanecdotally, or through time-consuming user research. In this work, we present\nan in-depth exploration of a simple link prediction explanation method we call\nLinkLogic, that surfaces and ranks explanatory information used for the\nprediction. Importantly, we construct the first-ever link prediction\nexplanation benchmark, based on family structures present in the FB13 dataset.\nWe demonstrate the use of this benchmark as a rich evaluation sandbox, probing\nLinkLogic quantitatively and qualitatively to assess the fidelity, selectivity\nand relevance of the generated explanations. We hope our work paves the way for\nmore holistic and empirical assessment of knowledge graph prediction\nexplanation methods in the future.",
      "tldr_zh": "该论文提出了一种名为LinkLogic的新方法，用于生成知识图谱链接预测的解释信息，通过揭示和排名预测所用的关键数据来提升模型透明度。研究者构建了首个链接预测解释基准，基于FB13数据集中的家族结构，用于定量和定性评估解释的保真度(fidelity)、选择性(selectivity)和相关性。实验结果显示，LinkLogic在评估中表现出色，为知识图谱预测解释方法提供了更全面的实证评估框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI",
        "I.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 4 figures in main text. For code and data, see\n  https://github.com/niraj17singh/LinkLogic",
      "pdf_url": "http://arxiv.org/pdf/2406.00855v1",
      "published_date": "2024-06-02 20:22:22 UTC",
      "updated_date": "2024-06-02 20:22:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:04:09.850587"
    },
    {
      "arxiv_id": "2406.00839v1",
      "title": "FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kaixin Lan",
        "Tao Fang",
        "Derek F. Wong",
        "Yabo Xu",
        "Lidia S. Chao",
        "Cecilia G. Zhao"
      ],
      "abstract": "Pre-trained Language Models (PLMs) have shown impressive results in various\nNatural Language Generation (NLG) tasks, such as powering chatbots and\ngenerating stories. However, an ethical concern arises due to their potential\nto produce verbatim copies of paragraphs from their training data. This is\nproblematic as PLMs are trained on corpora constructed by human authors. As\nsuch, there is a pressing need for research to promote the generation of\noriginal content by these models. In this study, we introduce a unique\n\"self-plagiarism\" contrastive decoding strategy, aimed at boosting the\noriginality of text produced by PLMs. Our method entails modifying prompts in\nLLMs to develop an amateur model and a professional model. Specifically, the\namateur model is urged to plagiarize using three plagiarism templates we have\ndesigned, while the professional model maintains its standard language model\nstatus. This strategy employs prompts to stimulate the model's capacity to\nidentify non-original candidate token combinations and subsequently impose\npenalties. The application of this strategy is integrated prior to the model's\nfinal layer, ensuring smooth integration with most existing PLMs (T5, GPT,\nLLaMA) without necessitating further adjustments. Implementing our strategy, we\nobserve a significant decline in non-original sequences comprised of more than\nthree words in the academic AASC dataset and the story-based ROCStories\ndataset.",
      "tldr_zh": "本研究针对预训练语言模型 (PLMs) 在自然语言生成 (NLG) 任务中可能产生训练数据抄袭的问题，提出了一种名为 FOCUS 的“self-plagiarism”对比解码策略，以提升生成文本的原创性。策略通过修改提示创建 amateur model 和 professional model，前者使用三种设计的 plagiarize 模板鼓励抄袭，后者保持标准状态，从而使模型识别并惩罚非原创 token 组合，并在最终层前整合，兼容 T5、GPT 和 LLaMA 等模型。实验结果显示，在 AASC 和 ROCStories 数据集上，非原创序列（超过三个单词）的出现显著减少，证明了该方法在促进原创内容生成方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 8 figures. The paper has been accepted by ACL 2024\n  (Findings), with Kaixin Lan and Tao Fang contributing equally, and Derek F.\n  Wong serving as the corresponding author",
      "pdf_url": "http://arxiv.org/pdf/2406.00839v1",
      "published_date": "2024-06-02 19:17:00 UTC",
      "updated_date": "2024-06-02 19:17:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:04:22.198890"
    },
    {
      "arxiv_id": "2406.00833v2",
      "title": "Harvard Undergraduate Survey on Generative AI",
      "title_zh": "哈佛本科生对生成式人工智能的调查",
      "authors": [
        "Shikoh Hirabayashi",
        "Rishab Jain",
        "Nikola Jurković",
        "Gabriel Wu"
      ],
      "abstract": "How has generative AI impacted the experiences of college students? We study\nthe influence of AI on the study habits, class choices, and career prospects of\nHarvard undergraduates (n=326), finding that almost 90% of students use\ngenerative AI. For roughly 25% of these students, AI has begun to substitute\nfor attending office hours and completing required readings. Half of students\nare concerned that AI will negatively impact their job prospects, and over half\nof students wish that Harvard had more classes on the future impacts of AI. We\nalso investigate students' outlook on the broader social implications of AI,\nfinding that half of students are worried that AI will increase economic\ninequality, and 40% believe that extinction risk from AI should be treated as a\nglobal priority with the same urgency as pandemics and nuclear war. Around half\nof students who have taken a class on AI expect AI to exceed human capabilities\non almost all tasks within 30 years. We make some recommendations to the\nHarvard community in light of these results.",
      "tldr_zh": "这篇论文通过对326名Harvard本科生的调查，探讨了generative AI对他们的学习习惯、课程选择和职业前景的影响，发现近90%的学生在使用generative AI，其中约25%的学生已开始用AI取代办公室时间和必读材料。调查结果显示，一半学生担心AI会负面影响就业前景，超过一半希望Harvard提供更多关于AI未来影响的课程。此外，学生对AI的社会影响持谨慎态度，一半人担忧AI会加剧经济不平等，40%认为AI灭绝风险应与pandemics和nuclear war同等优先处理。基于这些发现，论文为Harvard社区提出了相关推荐。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00833v2",
      "published_date": "2024-06-02 18:47:08 UTC",
      "updated_date": "2024-08-08 02:55:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:04:35.199211"
    },
    {
      "arxiv_id": "2406.02606v2",
      "title": "Know Your Neighborhood: General and Zero-Shot Capable Binary Function Search Powered by Call Graphlets",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Collyer",
        "Tim Watson",
        "Iain Phillips"
      ],
      "abstract": "Binary code similarity detection is an important problem with applications in\nareas such as malware analysis, vulnerability research and license violation\ndetection. This paper proposes a novel graph neural network architecture\ncombined with a novel graph data representation called call graphlets. A call\ngraphlet encodes the neighborhood around each function in a binary executable,\ncapturing the local and global context through a series of statistical\nfeatures. A specialized graph neural network model operates on this graph\nrepresentation, learning to map it to a feature vector that encodes semantic\nbinary code similarities using deep-metric learning. The proposed approach is\nevaluated across five distinct datasets covering different architectures,\ncompiler tool chains, and optimization levels. Experimental results show that\nthe combination of call graphlets and the novel graph neural network\narchitecture achieves comparable or state-of-the-art performance compared to\nbaseline techniques across cross-architecture, mono-architecture and zero shot\ntasks. In addition, our proposed approach also performs well when evaluated\nagainst an out-of-domain function inlining task. The work provides a general\nand effective graph neural network-based solution for conducting binary code\nsimilarity detection.",
      "tldr_zh": "这篇论文提出了一种新的图神经网络架构和 call graphlets 表示方法，用于二进制代码相似性检测，能够处理恶意软件分析、漏洞研究等应用场景。Call graphlets 通过统计特征编码每个函数的邻域，捕捉局部和全局上下文，并结合深度度量学习将图表示映射为语义相似性特征向量。该方法在五个涵盖不同架构、编译工具链和优化级别的数据集上进行了评估，结果显示其在跨架构、单架构和 zero-shot 任务中，与基线技术相比达到了相当或最先进的性能。此外，该方法在域外函数内联任务中也表现出色，提供了一个通用有效的基于图神经网络的解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, Under-Review",
      "pdf_url": "http://arxiv.org/pdf/2406.02606v2",
      "published_date": "2024-06-02 18:26:50 UTC",
      "updated_date": "2024-11-11 21:40:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:04:47.675954"
    },
    {
      "arxiv_id": "2406.16903v1",
      "title": "Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Changyu Du",
        "Stavros Nousias",
        "André Borrmann"
      ],
      "abstract": "Facing increasingly complex BIM authoring software and the accompanying\nexpensive learning costs, designers often seek to interact with the software in\na more intelligent and lightweight manner. They aim to automate modeling\nworkflows, avoiding obstacles and difficulties caused by software usage,\nthereby focusing on the design process itself. To address this issue, we\nproposed an LLM-based autonomous agent framework that can function as a copilot\nin the BIM authoring tool, answering software usage questions, understanding\nthe user's design intentions from natural language, and autonomously executing\nmodeling tasks by invoking the appropriate tools. In a case study based on the\nBIM authoring software Vectorworks, we implemented a software prototype to\nintegrate the proposed framework seamlessly into the BIM authoring scenario. We\nevaluated the planning and reasoning capabilities of different LLMs within this\nframework when faced with complex instructions. Our work demonstrates the\nsignificant potential of LLM-based agents in design automation and intelligent\ninteraction.",
      "tldr_zh": "该研究针对 BIM 建模软件的复杂性和高学习成本，提出了一种基于大型语言模型 (LLM) 的自主代理框架，作为 BIM 工具中的“copilot”，以实现智能人机交互。该框架能回答软件使用问题、从自然语言中理解用户设计意图，并通过调用适当工具自动执行建模任务。在 Vectorworks 软件的案例研究中，原型系统评估了不同 LLM 的规划和推理能力，结果显示该框架在设计自动化和智能交互方面具有显著潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.16903v1",
      "published_date": "2024-06-02 17:47:57 UTC",
      "updated_date": "2024-06-02 17:47:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:04:59.889939"
    },
    {
      "arxiv_id": "2406.00814v1",
      "title": "Expected Possession Value of Control and Duel Actions for Soccer Player's Skills Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Andrei Shelopugin"
      ],
      "abstract": "Estimation of football players' skills is one of the key tasks in sports\nanalytics. This paper introduces multiple extensions to a widely used model,\nexpected possession value (EPV), to address some key challenges such as\nselection problem. First, we assign greater weights to events occurring\nimmediately prior to the shot rather than those preceding them (decay effect).\nSecond, our model incorporates possession risk more accurately by considering\nthe decay effect and effective playing time. Third, we integrate the assessment\nof individual player ability to win aerial and ground duels. Using the extended\nEPV model, we predict this metric for various football players for the upcoming\nseason, particularly taking into account the strength of their opponents.",
      "tldr_zh": "这篇论文扩展了 Expected Possession Value (EPV) 模型，用于足球球员技能估计，旨在解决选择问题等挑战。主要扩展包括：为射门前事件赋予更大权重（decay effect）、更准确地整合控球风险（考虑衰减效应和有效比赛时间），以及评估球员赢得空中小对抗和地面对抗的能力。最终，使用改进的 EPV 模型预测球员在下赛季的表现指标，特别是结合对手实力因素。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00814v1",
      "published_date": "2024-06-02 17:29:42 UTC",
      "updated_date": "2024-06-02 17:29:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:05:11.653223"
    },
    {
      "arxiv_id": "2407.06162v2",
      "title": "RNNs, CNNs and Transformers in Human Action Recognition: A Survey and a Hybrid Model",
      "title_zh": "翻译失败",
      "authors": [
        "Khaled Alomar",
        "Halil Ibrahim Aysel",
        "Xiaohao Cai"
      ],
      "abstract": "Human Action Recognition (HAR) encompasses the task of monitoring human\nactivities across various domains, including but not limited to medical,\neducational, entertainment, visual surveillance, video retrieval, and the\nidentification of anomalous activities. Over the past decade, the field of HAR\nhas witnessed substantial progress by leveraging Convolutional Neural Networks\n(CNNs) to effectively extract and comprehend intricate information, thereby\nenhancing the overall performance of HAR systems. Recently, the domain of\ncomputer vision has witnessed the emergence of Vision Transformers (ViTs) as a\npotent solution. The efficacy of transformer architecture has been validated\nbeyond the confines of image analysis, extending their applicability to diverse\nvideo-related tasks. Notably, within this landscape, the research community has\nshown keen interest in HAR, acknowledging its manifold utility and widespread\nadoption across various domains. This article aims to present an encompassing\nsurvey that focuses on CNNs and the evolution of Recurrent Neural Networks\n(RNNs) to ViTs given their importance in the domain of HAR. By conducting a\nthorough examination of existing literature and exploring emerging trends, this\nstudy undertakes a critical analysis and synthesis of the accumulated knowledge\nin this field. Additionally, it investigates the ongoing efforts to develop\nhybrid approaches. Following this direction, this article presents a novel\nhybrid model that seeks to integrate the inherent strengths of CNNs and ViTs.",
      "tldr_zh": "这篇论文对Human Action Recognition (HAR)领域进行了全面调查，聚焦于Recurrent Neural Networks (RNNs)、Convolutional Neural Networks (CNNs)和Vision Transformers (ViTs)在监控人类活动中的应用和发展。作者回顾了过去十年CNNs在提取复杂信息方面的进步，以及ViTs在视频任务中的新兴作用，并分析了现有文献和混合方法的趋势。最终，论文提出一个新颖的混合模型，结合CNNs和ViTs的优点，以提升HAR系统的性能和适用性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.06162v2",
      "published_date": "2024-06-02 17:09:59 UTC",
      "updated_date": "2024-08-15 08:59:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:05:24.403990"
    },
    {
      "arxiv_id": "2406.00800v2",
      "title": "MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization",
      "title_zh": "翻译失败",
      "authors": [
        "Aozhong Zhang",
        "Naigang Wang",
        "Yanxia Deng",
        "Xin Li",
        "Zi Yang",
        "Penghang Yin"
      ],
      "abstract": "In this paper, we present a simple optimization-based preprocessing technique\ncalled Weight Magnitude Reduction (MagR) to improve the performance of\npost-training quantization. For each linear layer, we adjust the pre-trained\nfloating-point weights by solving an $\\ell_\\infty$-regularized optimization\nproblem. This process greatly diminishes the maximum magnitude of the weights\nand smooths out outliers, while preserving the layer's output. The preprocessed\nweights are centered more towards zero, which facilitates the subsequent\nquantization process. To implement MagR, we address the\n$\\ell_\\infty$-regularization by employing an efficient proximal gradient\ndescent algorithm. Unlike existing preprocessing methods that involve linear\ntransformations and subsequent post-processing steps, which can introduce\nsignificant overhead at inference time, MagR functions as a non-linear\ntransformation, eliminating the need for any additional post-processing. This\nensures that MagR introduces no overhead whatsoever during inference. Our\nexperiments demonstrate that MagR achieves state-of-the-art performance on the\nLlama family of models. For example, we achieve a Wikitext2 perplexity of 5.95\non the LLaMA2-70B model for per-channel INT2 weight quantization without\nincurring any inference overhead.",
      "tldr_zh": "本研究提出了一种简单的优化预处理技术，名为 Weight Magnitude Reduction (MagR)，旨在提升后训练量化(post-training quantization)的性能。通过为每个线性层解决一个带 $\\ell_\\infty$-regularized 的优化问题，MagR 显著减少权重最大幅度、平滑异常值，同时保持层输出不变，从而使权重更接近零以便后续量化。不同于现有方法，MagR 采用非线性变换和高效的 proximal gradient descent 算法，无需额外后处理，从而避免推理时的开销。实验结果显示，MagR 在 Llama 模型家族上达到最先进性能，例如在 LLaMA2-70B 模型的 per-channel INT2 权重量化下，Wikitext2 perplexity 仅为 5.95。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00800v2",
      "published_date": "2024-06-02 17:00:02 UTC",
      "updated_date": "2024-10-17 03:51:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:05:37.559759"
    },
    {
      "arxiv_id": "2406.00798v1",
      "title": "PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Yeonsung Jung",
        "Heecheol Yun",
        "Joonhyung Park",
        "Jin-Hwa Kim",
        "Eunho Yang"
      ],
      "abstract": "Neural Radiance Fields (NeRF) have shown remarkable performance in learning\n3D scenes. However, NeRF exhibits vulnerability when confronted with\ndistractors in the training images -- unexpected objects are present only\nwithin specific views, such as moving entities like pedestrians or birds.\nExcluding distractors during dataset construction is a straightforward\nsolution, but without prior knowledge of their types and quantities, it becomes\nprohibitively expensive. In this paper, we propose PruNeRF, a segment-centric\ndataset pruning framework via 3D spatial consistency, that effectively\nidentifies and prunes the distractors. We first examine existing metrics for\nmeasuring pixel-wise distraction and introduce Influence Functions for more\naccurate measurements. Then, we assess 3D spatial consistency using a\ndepth-based reprojection technique to obtain 3D-aware distraction. Furthermore,\nwe incorporate segmentation for pixel-to-segment refinement, enabling more\nprecise identification. Our experiments on benchmark datasets demonstrate that\nPruNeRF consistently outperforms state-of-the-art methods in robustness against\ndistractors.",
      "tldr_zh": "该论文针对 Neural Radiance Fields (NeRF) 在处理训练图像中干扰物（如行人或鸟类）时的脆弱性，提出 PruNeRF 框架，这是一种基于 3D Spatial Consistency 的段心数据集修剪方法。PruNeRF 首先利用 Influence Functions 精确测量像素级干扰，然后通过深度-based reprojection 技术评估 3D 空间一致性，并结合 segmentation 进行像素到段的精炼，以有效识别和去除干扰物。实验结果显示，在基准数据集上，PruNeRF 在鲁棒性方面 consistently outperforms 现有 state-of-the-art 方法，提高了 NeRF 的整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00798v1",
      "published_date": "2024-06-02 16:49:05 UTC",
      "updated_date": "2024-06-02 16:49:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:05:48.347006"
    },
    {
      "arxiv_id": "2406.01637v2",
      "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Zhu",
        "Antony Kellermann",
        "Akul Gupta",
        "Philip Li",
        "Richard Fang",
        "Rohan Bindu",
        "Daniel Kang"
      ],
      "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and\nshow that our team of agents improve over prior agent frameworks by up to 4.3X.",
      "tldr_zh": "这篇论文探讨了LLM agents在网络安全领域的潜力，指出现有代理在处理未知的零日漏洞（zero-day vulnerabilities）时，由于探索能力和长期规划不足而表现不佳。研究团队引入了HPTSA系统，该系统采用团队结构，包括一个规划代理（planning agent）来探索系统并启动子代理（subagents），从而解决多漏洞探索和规划问题。实验结果显示，在一个包含14个真实世界漏洞的基准测试中，HPTSA比先前代理框架提高了高达4.3倍的性能，为LLM agents利用零日漏洞提供了可行的新方法。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2.7; D.4.6"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.01637v2",
      "published_date": "2024-06-02 16:25:26 UTC",
      "updated_date": "2025-03-30 00:26:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:06:02.599139"
    },
    {
      "arxiv_id": "2406.03503v1",
      "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Yifan Xia",
        "Xianliang Yang",
        "Zichuan Liu",
        "Zhihao Liu",
        "Lei Song",
        "Jiang Bian"
      ],
      "abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP)\nutilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where\nmachine learning (ML) models generate heatmaps, indicating the probability\ndistribution of each edge being part of the optimal solution, to guide MCTS in\nsolution finding. However, our theoretical and experimental analysis raises\ndoubts about the effectiveness of ML-based heatmap generation. In support of\nthis, we demonstrate that a simple baseline method can outperform complex ML\napproaches in heatmap generation. Furthermore, we question the practical value\nof the heatmap-guided MCTS paradigm. To substantiate this, our findings show\nits inferiority to the LKH-3 heuristic despite the paradigm's reliance on\nproblem-specific, hand-crafted strategies. For the future, we suggest research\ndirections focused on developing more theoretically sound heatmap generation\nmethods and exploring autonomous, generalizable ML approaches for combinatorial\nproblems. The code is available for review:\nhttps://github.com/xyfffff/rethink_mcts_for_tsp.",
      "tldr_zh": "这篇论文重新审视了基于神经网络的后验搜索方法在解决大规模 Traveling Salesman Problems (TSP) 中的有效性，特别是质疑热力图引导的 Monte Carlo tree search (MCTS) 范式。作者通过理论和实验分析证明，一个简单基线方法在热力图生成上优于复杂的 machine learning (ML) 模型。实验结果显示，热力图引导 MCTS 范式不如 LKH-3 启发式算法有效，尽管它依赖问题特定的手工策略。论文建议未来的研究方向，包括开发更理论可靠的热力图生成方法，以及探索自治和通用化的 ML 方案。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by International Conference on Machine Learning (ICML 2024)",
      "pdf_url": "http://arxiv.org/pdf/2406.03503v1",
      "published_date": "2024-06-02 16:11:38 UTC",
      "updated_date": "2024-06-02 16:11:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:06:13.512172"
    },
    {
      "arxiv_id": "2406.01636v1",
      "title": "COVID-19: post infection implications in different age groups, mechanism, diagnosis, effective prevention, treatment, and recommendations",
      "title_zh": "COVID-19：不同年龄组的感染后影响、机制、诊断、有效预防、治疗和推荐",
      "authors": [
        "Muhammad Akmal Raheem",
        "Muhammad Ajwad Rahim",
        "Ijaz Gul",
        "Md. Reyad-ul-Ferdous",
        "Liyan Le",
        "Junguo Hui",
        "Shuiwei Xia",
        "Minjiang Chen",
        "Dongmei Yu",
        "Vijay Pandey",
        "Peiwu Qin",
        "Jiansong Ji"
      ],
      "abstract": "SARS-CoV-2, the highly contagious pathogen responsible for the COVID-19\npandemic, has persistent effects that begin four weeks after initial infection\nand last for an undetermined duration. These chronic effects are more harmful\nthan acute ones. This review explores the long-term impact of the virus on\nvarious human organs, including the pulmonary, cardiovascular, neurological,\nreproductive, gastrointestinal, musculoskeletal, endocrine, and lymphoid\nsystems, particularly in older adults. Regarding diagnosis, RT-PCR is the gold\nstandard for detecting COVID-19, though it requires specialized equipment,\nskilled personnel, and considerable time to produce results. To address these\nlimitations, artificial intelligence in imaging and microfluidics technologies\noffers promising alternatives for diagnosing COVID-19 efficiently.\nPharmacological and non-pharmacological strategies are effective in mitigating\nthe persistent impacts of COVID-19. These strategies enhance immunity in\npost-COVID-19 patients by reducing cytokine release syndrome, improving T cell\nresponse, and increasing the circulation of activated natural killer and CD8 T\ncells in blood and tissues. This, in turn, alleviates symptoms such as fever,\nnausea, fatigue, muscle weakness, and pain. Vaccines, including inactivated\nviral, live attenuated viral, protein subunit, viral vectored, mRNA, DNA, and\nnanoparticle vaccines, significantly reduce the adverse long-term effects of\nthe virus. However, no vaccine has been reported to provide lifetime protection\nagainst COVID-19. Consequently, protective measures such as physical\ndistancing, mask usage, and hand hygiene remain essential strategies. This\nreview offers a comprehensive understanding of the persistent effects of\nCOVID-19 on individuals of varying ages, along with insights into diagnosis,\ntreatment, vaccination, and future preventative measures against the spread of\nSARS-CoV-2.",
      "tldr_zh": "这篇论文回顾了 SARS-CoV-2 感染后对不同年龄组的长期影响，特别是老年人群，强调这些慢性效果（如对肺部、心血管、神经和生殖系统等的损害）比急性症状更具危害性。诊断方面，RT-PCR 被视为金标准，但作者提出 AI 和微流控技术作为更高效的替代方案，以克服其设备和时间限制。治疗策略包括药理和非药理方法，通过减少细胞因子释放综合征、改善 T 细胞响应和增强自然杀伤细胞来缓解症状，如发热、疲劳和肌肉无力。论文还讨论了多种疫苗（如 mRNA 和灭活病毒疫苗）的作用及其在减少长期影响中的重要性，并推荐持续采用社交距离、戴口罩和手卫生等预防措施，以应对 SARS-CoV-2 的传播。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.01636v1",
      "published_date": "2024-06-02 16:07:49 UTC",
      "updated_date": "2024-06-02 16:07:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:06:25.323889"
    },
    {
      "arxiv_id": "2406.00778v3",
      "title": "Bayesian Joint Additive Factor Models for Multiview Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Niccolo Anceschi",
        "Federico Ferrari",
        "David B. Dunson",
        "Himel Mallick"
      ],
      "abstract": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar.",
      "tldr_zh": "本研究提出了一种Bayesian联合加性因子回归模型（JAFAR），用于多视图学习（Multiview Learning），旨在分析多类型数据（如多组学数据）与响应变量的关系，同时整合多模态信息以提升预测准确性。模型采用结构化加性设计，包含共享和视图特定组件，并通过新型依赖累积收缩过程（D-CUSP）先验确保可识别性，同时支持灵活的特征和结果分布，并提供高效的部分折叠Gibbs采样器实现。在精准医学应用中，使用免疫组学、代谢组学和蛋白质组学数据预测分娩时间，JAFAR模型表现出色，优于现有竞争方法，并附带开源R包以便使用。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "stat.CO",
        "stat.ME",
        "62F15"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00778v3",
      "published_date": "2024-06-02 15:35:45 UTC",
      "updated_date": "2025-01-10 05:35:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:06:36.856987"
    },
    {
      "arxiv_id": "2406.00777v2",
      "title": "Diffusion Features to Bridge Domain Gap for Semantic Segmentation",
      "title_zh": "扩散特征",
      "authors": [
        "Yuxiang Ji",
        "Boyong He",
        "Chenyuan Qu",
        "Zhuoyue Tan",
        "Chuan Qin",
        "Liaoni Wu"
      ],
      "abstract": "Pre-trained diffusion models have demonstrated remarkable proficiency in\nsynthesizing images across a wide range of scenarios with customizable prompts,\nindicating their effective capacity to capture universal features. Motivated by\nthis, our study delves into the utilization of the implicit knowledge embedded\nwithin diffusion models to address challenges in cross-domain semantic\nsegmentation. This paper investigates the approach that leverages the sampling\nand fusion techniques to harness the features of diffusion models efficiently.\nWe propose DIffusion Feature Fusion (DIFF) as a backbone use for extracting and\nintegrating effective semantic representations through the diffusion process.\nBy leveraging the strength of text-to-image generation capability, we introduce\na new training framework designed to implicitly learn posterior knowledge from\nit. Through rigorous evaluation in the contexts of domain generalization\nsemantic segmentation, we establish that our methodology surpasses preceding\napproaches in mitigating discrepancies across distinct domains and attains the\nstate-of-the-art (SOTA) benchmark.",
      "tldr_zh": "本研究利用预训练的 diffusion models 来桥接语义分割中的领域差距（domain gap），通过采样和融合技术提取其隐含知识。论文提出 DIffusion Feature Fusion (DIFF) 框架，作为骨干网络，用于在扩散过程中提取和整合有效的语义表示，并引入一个新训练框架，利用文本到图像生成能力隐式学习后验知识。在域泛化语义分割的严格评估中，该方法超越了现有方法，达到了 state-of-the-art (SOTA) 基准，显著缓解了不同领域的差异。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The code is released at https://github.com/Yux1angJi/DIFF",
      "pdf_url": "http://arxiv.org/pdf/2406.00777v2",
      "published_date": "2024-06-02 15:33:46 UTC",
      "updated_date": "2024-11-21 09:10:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:06:48.836831"
    },
    {
      "arxiv_id": "2406.00770v1",
      "title": "Automatic Instruction Evolving for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Weihao Zeng",
        "Can Xu",
        "Yingxiu Zhao",
        "Jian-Guang Lou",
        "Weizhu Chen"
      ],
      "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved\nencouraging results across a wide range of tasks. However, designing effective\nevolving methods for instruction evolution requires substantial human\nexpertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that\nevolves instruction datasets using large language models without any human\neffort. The framework automatically analyzes and summarizes suitable\nevolutionary strategies for the given instruction data and iteratively improves\nthe evolving method based on issues exposed during the instruction evolution\nprocess. Our extensive experiments demonstrate that the best method optimized\nby Auto Evol-Instruct outperforms human-designed methods on various benchmarks,\nincluding MT-Bench, AlpacaEval, GSM8K, and HumanEval.",
      "tldr_zh": "这篇论文提出 Auto Evol-Instruct，一个端到端的框架，使用 Large Language Models 自动演化指令数据集，无需人类干预。该框架通过分析和总结适合的演化策略，并基于指令演化过程中的问题进行迭代改进，从而优化演化方法。实验结果显示，Auto Evol-Instruct 优化的最佳方法在 MT-Bench、AlpacaEval、GSM8K 和 HumanEval 等基准测试中，表现优于人类设计的 Evol-Instruct 方法。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00770v1",
      "published_date": "2024-06-02 15:09:00 UTC",
      "updated_date": "2024-06-02 15:09:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:07:01.331524"
    },
    {
      "arxiv_id": "2406.00765v1",
      "title": "The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Wakana Haijima",
        "Kou Nakakubo",
        "Masahiro Suzuki",
        "Yutaka Matsuo"
      ],
      "abstract": "In recent years, as machine learning, particularly for vision and language\nunderstanding, has been improved, research in embedded AI has also evolved.\nVOYAGER is a well-known LLM-based embodied AI that enables autonomous\nexploration in the Minecraft world, but it has issues such as underutilization\nof visual data and insufficient functionality as a world model. In this\nresearch, the possibility of utilizing visual data and the function of LLM as a\nworld model were investigated with the aim of improving the performance of\nembodied AI. The experimental results revealed that LLM can extract necessary\ninformation from visual data, and the utilization of the information improves\nits performance as a world model. It was also suggested that devised prompts\ncould bring out the LLM's function as a world model.",
      "tldr_zh": "该研究针对基于 LLM 的 embodied AI（如 VOYAGER）在 Minecraft 世界中存在的视觉数据利用不足和世界模型功能弱点问题，探讨了如何整合视觉信息和预测导向提示（prediction-oriented prompts）来提升其性能。研究方法包括使用 LLM 从视觉数据中提取必要信息，并通过设计特定提示强化 LLM 作为世界模型的功能。实验结果表明，这种整合显著改善了 embodied AI 的表现，并证明了 LLM 在处理视觉信息和预测方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00765v1",
      "published_date": "2024-06-02 14:50:01 UTC",
      "updated_date": "2024-06-02 14:50:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:07:16.850011"
    },
    {
      "arxiv_id": "2406.06561v1",
      "title": "Brainstorming Brings Power to Large Language Models of Knowledge Reasoning",
      "title_zh": "脑力激荡赋予大型语言模型知识推理能力",
      "authors": [
        "Zining Qin",
        "Chenhao Wang",
        "Huiling Qin",
        "Weijia Jia"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated amazing capabilities in\nlanguage generation, text comprehension, and knowledge reasoning. While a\nsingle powerful model can already handle multiple tasks, relying on a single\nperspective can lead to biased and unstable results. Recent studies have\nfurther improved the model's reasoning ability on a wide range of tasks by\nintroducing multi-model collaboration. However, models with different\ncapabilities may produce conflicting answers on the same problem, and how to\nreasonably obtain the correct answer from multiple candidate models has become\na challenging problem. In this paper, we propose the multi-model brainstorming\nbased on prompt. It incorporates different models into a group for\nbrainstorming, and after multiple rounds of reasoning elaboration and\nre-inference, a consensus answer is reached within the group. We conducted\nexperiments on three different types of datasets, and demonstrate that the\nbrainstorming can significantly improve the effectiveness in logical reasoning\nand fact extraction. Furthermore, we find that two small-parameter models can\nachieve accuracy approximating that of larger-parameter models through\nbrainstorming, which provides a new solution for distributed deployment of\nLLMs.",
      "tldr_zh": "该研究探讨了大型语言模型(LLMs)在知识推理中的局限性，如单一视角导致的偏见和不稳定性，并提出了一种基于提示的多模型脑力激荡方法，将不同模型整合成小组，通过多轮推理阐述和重新推理达成共识答案。实验在三种不同类型的数据集上进行，证明该方法显著提升了逻辑推理和事实提取的有效性。进一步发现，两个小参数模型通过脑力激荡可实现接近大参数模型的准确率，为LLMs的分布式部署提供了创新解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.06561v1",
      "published_date": "2024-06-02 14:47:14 UTC",
      "updated_date": "2024-06-02 14:47:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:07:26.312189"
    },
    {
      "arxiv_id": "2406.00761v1",
      "title": "Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Po-Shao Lin",
        "Jia-Fong Yeh",
        "Yi-Ting Chen",
        "Winston H. Hsu"
      ],
      "abstract": "We observe that current state-of-the-art (SOTA) methods suffer from the\nperformance imbalance issue when performing multi-task reinforcement learning\n(MTRL) tasks. While these methods may achieve impressive performance on\naverage, they perform extremely poorly on a few tasks. To address this, we\npropose a new and effective method called STARS, which consists of two novel\nstrategies: a shared-unique feature extractor and task-aware prioritized\nsampling. First, the shared-unique feature extractor learns both shared and\ntask-specific features to enable better synergy of knowledge between different\ntasks. Second, the task-aware sampling strategy is combined with the\nprioritized experience replay for efficient learning on tasks with poor\nperformance. The effectiveness and stability of our STARS are verified through\nexperiments on the mainstream Meta-World benchmark. From the results, our STARS\nstatistically outperforms current SOTA methods and alleviates the performance\nimbalance issue. Besides, we visualize the learned features to support our\nclaims and enhance the interpretability of STARS.",
      "tldr_zh": "本研究发现，现有的最先进方法（SOTA）在多任务强化学习（MTRL）中存在性能不平衡问题，即某些任务表现极差。为解决此问题，提出了一种新方法STARS，包括shared-unique feature extractor（用于学习共享和任务特定特征，以提升任务间知识协同）和task-aware prioritized sampling（与优先经验回放结合，针对表现差的任务进行高效学习）。实验在Meta-World基准上验证了STARS的有效性和稳定性，其统计上优于SOTA方法，并缓解了性能不平衡问题；此外，通过特征可视化增强了方法的解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "The first two authors contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2406.00761v1",
      "published_date": "2024-06-02 14:33:49 UTC",
      "updated_date": "2024-06-02 14:33:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:07:38.117863"
    },
    {
      "arxiv_id": "2406.00755v1",
      "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaoyuan Li",
        "Wenjie Wang",
        "Moxin Li",
        "Junrong Guo",
        "Yang Zhang",
        "Fuli Feng"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of\nmathematical reasoning necessitates comprehensive evaluations to gauge progress\nand inspire future directions. Existing assessments predominantly focus on\nproblem-solving from the examinee perspective, overlooking a dual perspective\nof examiner regarding error identification and correction. From the examiner\nperspective, we define four evaluation tasks for error identification and\ncorrection along with a new dataset with annotated error types and steps. We\nalso design diverse prompts to thoroughly evaluate eleven representative LLMs.\nOur principal findings indicate that GPT-4 outperforms all models, while\nopen-source model LLaMA-2-7B demonstrates comparable abilities to closed-source\nmodels GPT-3.5 and Gemini Pro. Notably, calculation error proves the most\nchallenging error type. Moreover, prompting LLMs with the error types can\nimprove the average correction accuracy by 47.9\\%. These results reveal\npotential directions for developing the mathematical reasoning abilities of\nLLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.",
      "tldr_zh": "本研究评估大型语言模型 (LLMs) 在数学推理方面的错误识别和修正能力，引入了从 examiner 视角的四个评估任务，并创建了一个标注错误类型和步骤的新数据集。\n他们设计了多样提示来测试 11 个代表性 LLMs，结果显示 GPT-4 表现出色，而开源模型 LLaMA-2-7B 的表现可与闭源模型 GPT-3.5 和 Gemini Pro 相媲美。\n计算错误被证明是最难处理的类型，提供错误类型提示可将修正准确率平均提高 47.9%，这些发现为提升 LLMs 的数学推理能力提供了潜在方向，并公开了代码和数据集。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ACL Findings 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00755v1",
      "published_date": "2024-06-02 14:16:24 UTC",
      "updated_date": "2024-06-02 14:16:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:07:52.119490"
    },
    {
      "arxiv_id": "2406.00748v1",
      "title": "Augmenting the FedProx Algorithm by Minimizing Convergence",
      "title_zh": "翻译失败",
      "authors": [
        "Anomitra Sarkar",
        "Lavanya Vajpayee"
      ],
      "abstract": "The Internet of Things has experienced significant growth and has become an\nintegral part of various industries. This expansion has given rise to the\nIndustrial IoT initiative where industries are utilizing IoT technology to\nenhance communication and connectivity through innovative solutions such as\ndata analytics and cloud computing. However this widespread adoption of IoT is\ndemanding of algorithms that provide better efficiency for the same training\nenvironment without speed being a factor. In this paper we present a novel\napproach called G Federated Proximity. Building upon the existing FedProx\ntechnique our implementation introduces slight modifications to enhance its\nefficiency and effectiveness. By leveraging FTL our proposed system aims to\nimprove the accuracy of model obtained after the training dataset with the help\nof normalization techniques such that it performs better on real time devices\nand heterogeneous networks Our results indicate a significant increase in the\nthroughput of approximately 90% better convergence compared to existing model\nperformance.",
      "tldr_zh": "这篇论文提出了一种名为 G Federated Proximity 的新方法，旨在改进 FedProx 算法，以提升 IoT 和 Industrial IoT 环境下的模型训练效率，而非单纯关注速度。方法通过引入 FTL 和归一化技术，对 FedProx 进行微调，提高模型准确性和收敛性，使其在异构网络和实时设备上更具适应性。实验结果显示，与现有模型相比，该方法实现了约 90% 的吞吐量提升和更好的收敛性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "F.2.2; I.2.7"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00748v1",
      "published_date": "2024-06-02 14:01:55 UTC",
      "updated_date": "2024-06-02 14:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:08:11.333605"
    },
    {
      "arxiv_id": "2406.00741v1",
      "title": "Learning to Play 7 Wonders Duel Without Human Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Giovanni Paolini",
        "Lorenzo Moreschini",
        "Francesco Veneziano",
        "Alessandro Iraci"
      ],
      "abstract": "This paper introduces ZeusAI, an artificial intelligence system developed to\nplay the board game 7 Wonders Duel. Inspired by the AlphaZero reinforcement\nlearning algorithm, ZeusAI relies on a combination of Monte Carlo Tree Search\nand a Transformer Neural Network to learn the game without human supervision.\nZeusAI competes at the level of top human players, develops both known and\nnovel strategies, and allows us to test rule variants to improve the game's\nbalance. This work demonstrates how AI can help in understanding and enhancing\nboard games.",
      "tldr_zh": "该论文介绍了ZeusAI，一种无需人类监督的AI系统，用于学习玩桌游7 Wonders Duel。它结合了Monte Carlo Tree Search和Transformer Neural Network，受AlphaZero启发，通过强化学习开发出既有的和新颖的游戏策略。实验结果显示，ZeusAI能与顶级人类玩家匹敌，并通过测试规则变体来提升游戏平衡，从而展示了AI在理解和优化桌游设计方面的潜力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00741v1",
      "published_date": "2024-06-02 13:28:57 UTC",
      "updated_date": "2024-06-02 13:28:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:08:12.953689"
    },
    {
      "arxiv_id": "2406.18568v2",
      "title": "A Diagnostic Model for Acute Lymphoblastic Leukemia Using Metaheuristics and Deep Learning Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Amir Masoud Rahmani",
        "Parisa Khoshvaght",
        "Hamid Alinejad-Rokny",
        "Samira Sadeghi",
        "Parvaneh Asghari",
        "Zohre Arabi",
        "Mehdi Hosseinzadeh"
      ],
      "abstract": "Acute lymphoblastic leukemia (ALL) severity is determined by the presence and\nratios of blast cells (abnormal white blood cells) in both bone marrow and\nperipheral blood. Manual diagnosis of this disease is a tedious and\ntime-consuming operation, making it difficult for professionals to accurately\nexamine blast cell characteristics. To address this difficulty, researchers use\ndeep learning and machine learning. In this paper, a ResNet-based feature\nextractor is utilized to detect ALL, along with a variety of feature selectors\nand classifiers. To get the best results, a variety of transfer learning\nmodels, including the Resnet, VGG, EfficientNet, and DensNet families, are used\nas deep feature extractors. Following extraction, different feature selectors\nare used, including Genetic algorithm, PCA, ANOVA, Random Forest, Univariate,\nMutual information, Lasso, XGB, Variance, and Binary ant colony. After feature\nqualification, a variety of classifiers are used, with MLP outperforming the\nothers. The recommended technique is used to categorize ALL and HEM in the\nselected dataset which is C-NMC 2019. This technique got an impressive 90.71%\naccuracy and 95.76% sensitivity for the relevant classifications, and its\nmetrics on this dataset outperformed others.",
      "tldr_zh": "本文提出了一种诊断 Acute Lymphoblastic Leukemia (ALL) 的模型，结合深度学习和元启发式方法来自动化检测骨髓和外周血中的爆炸细胞（blast cells），以解决手动诊断的耗时和准确性问题。模型使用 ResNet 作为特征提取器，并结合多种转移学习模型（如 VGG, EfficientNet, DensNet）和特征选择器（如 Genetic algorithm, PCA, ANOVA），最终采用 MLP 分类器进行分类。在 C-NMC 2019 数据集上，该方法实现了 90.71% 的准确率和 95.76% 的敏感性，显著优于其他基准方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.18568v2",
      "published_date": "2024-06-02 13:25:44 UTC",
      "updated_date": "2024-08-12 06:11:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:08:27.208943"
    },
    {
      "arxiv_id": "2406.00738v2",
      "title": "Global Rewards in Restless Multi-Armed Bandits",
      "title_zh": "翻译失败",
      "authors": [
        "Naveen Raman",
        "Zheyuan Ryan Shi",
        "Fei Fang"
      ],
      "abstract": "Restless multi-armed bandits (RMAB) extend multi-armed bandits so pulling an\narm impacts future states. Despite the success of RMABs, a key limiting\nassumption is the separability of rewards into a sum across arms. We address\nthis deficiency by proposing restless-multi-armed bandit with global rewards\n(RMAB-G), a generalization of RMABs to global non-separable rewards. To solve\nRMAB-G, we develop the Linear- and Shapley-Whittle indices, which extend\nWhittle indices from RMABs to RMAB-Gs. We prove approximation bounds but also\npoint out how these indices could fail when reward functions are highly\nnon-linear. To overcome this, we propose two sets of adaptive policies: the\nfirst computes indices iteratively, and the second combines indices with\nMonte-Carlo Tree Search (MCTS). Empirically, we demonstrate that our proposed\npolicies outperform baselines and index-based policies with synthetic data and\nreal-world data from food rescue.",
      "tldr_zh": "该研究扩展了Restless Multi-Armed Bandits (RMAB)，提出RMAB-G框架，以处理奖励非分离的全局奖励问题，克服传统RMAB中奖励可分离的限制。作者开发了Linear-Whittle indices和Shapley-Whittle indices作为Whittle indices的扩展，并证明了它们的近似边界，同时指出了在奖励函数高度非线性时可能失效的风险。为此，他们提出两种自适应策略：一种通过迭代计算indices，另一种将indices与Monte-Carlo Tree Search (MCTS)结合。实验结果显示，这些策略在合成数据和真实世界食物救援数据上优于基线和基于indices的策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages",
      "pdf_url": "http://arxiv.org/pdf/2406.00738v2",
      "published_date": "2024-06-02 13:13:46 UTC",
      "updated_date": "2024-06-07 20:38:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:08:37.865767"
    },
    {
      "arxiv_id": "2406.00735v1",
      "title": "Full-Atom Peptide Design based on Multi-modal Flow Matching",
      "title_zh": "基于多模态流匹配的全原子肽设计",
      "authors": [
        "Jiahan Li",
        "Chaoran Cheng",
        "Zuofan Wu",
        "Ruihan Guo",
        "Shitong Luo",
        "Zhizhou Ren",
        "Jian Peng",
        "Jianzhu Ma"
      ],
      "abstract": "Peptides, short chains of amino acid residues, play a vital role in numerous\nbiological processes by interacting with other target molecules, offering\nsubstantial potential in drug discovery. In this work, we present PepFlow, the\nfirst multi-modal deep generative model grounded in the flow-matching framework\nfor the design of full-atom peptides that target specific protein receptors.\nDrawing inspiration from the crucial roles of residue backbone orientations and\nside-chain dynamics in protein-peptide interactions, we characterize the\npeptide structure using rigid backbone frames within the $\\mathrm{SE}(3)$\nmanifold and side-chain angles on high-dimensional tori. Furthermore, we\nrepresent discrete residue types in the peptide sequence as categorical\ndistributions on the probability simplex. By learning the joint distributions\nof each modality using derived flows and vector fields on corresponding\nmanifolds, our method excels in the fine-grained design of full-atom peptides.\nHarnessing the multi-modal paradigm, our approach adeptly tackles various tasks\nsuch as fix-backbone sequence design and side-chain packing through partial\nsampling. Through meticulously crafted experiments, we demonstrate that PepFlow\nexhibits superior performance in comprehensive benchmarks, highlighting its\nsignificant potential in computational peptide design and analysis.",
      "tldr_zh": "本研究提出 PepFlow，一种基于 multi-modal flow matching 框架的首个多模态深度生成模型，用于设计针对特定蛋白受体的 full-atom peptides。模型通过表征肽结构，包括 $\\mathrm{SE}(3)$ 流形上的刚性主链框架、高维 tori 上的侧链角度，以及概率单纯形上的离散残基类型分布，来学习多模态的联合分布，从而实现细粒度的肽设计任务，如 fix-backbone sequence design 和 side-chain packing。实验结果显示，PepFlow 在全面基准测试中表现出色，展示了其在 computational peptide design and analysis 中的巨大潜力。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00735v1",
      "published_date": "2024-06-02 12:59:54 UTC",
      "updated_date": "2024-06-02 12:59:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:08:49.607513"
    },
    {
      "arxiv_id": "2406.02605v1",
      "title": "A Novel Defense Against Poisoning Attacks on Federated Learning: LayerCAM Augmented with Autoencoder",
      "title_zh": "一种针对联邦学习投毒攻击的新",
      "authors": [
        "Jingjing Zheng",
        "Xin Yuan",
        "Kai Li",
        "Wei Ni",
        "Eduardo Tovar",
        "Jon Crowcroft"
      ],
      "abstract": "Recent attacks on federated learning (FL) can introduce malicious model\nupdates that circumvent widely adopted Euclidean distance-based detection\nmethods. This paper proposes a novel defense strategy, referred to as\nLayerCAM-AE, designed to counteract model poisoning in federated learning. The\nLayerCAM-AE puts forth a new Layer Class Activation Mapping (LayerCAM)\nintegrated with an autoencoder (AE), significantly enhancing detection\ncapabilities. Specifically, LayerCAM-AE generates a heat map for each local\nmodel update, which is then transformed into a more compact visual format. The\nautoencoder is designed to process the LayerCAM heat maps from the local model\nupdates, improving their distinctiveness and thereby increasing the accuracy in\nspotting anomalous maps and malicious local models. To address the risk of\nmisclassifications with LayerCAM-AE, a voting algorithm is developed, where a\nlocal model update is flagged as malicious if its heat maps are consistently\nsuspicious over several rounds of communication. Extensive tests of LayerCAM-AE\non the SVHN and CIFAR-100 datasets are performed under both Independent and\nIdentically Distributed (IID) and non-IID settings in comparison with existing\nResNet-50 and REGNETY-800MF defense models. Experimental results show that\nLayerCAM-AE increases detection rates (Recall: 1.0, Precision: 1.0, FPR: 0.0,\nAccuracy: 1.0, F1 score: 1.0, AUC: 1.0) and test accuracy in FL, surpassing the\nperformance of both the ResNet-50 and REGNETY-800MF. Our code is available at:\nhttps://github.com/jjzgeeks/LayerCAM-AE",
      "tldr_zh": "这篇论文提出了一种新型防御策略LayerCAM-AE，用于对抗联邦学习(Federated Learning)中的模型投毒攻击(Poisoning Attacks)，该策略通过将Layer Class Activation Mapping (LayerCAM)与autoencoder (AE)整合，生成并优化本地模型更新的热图，以提升异常检测的准确性，并辅以投票算法减少误分类。实验在SVHN和CIFAR-100数据集上进行，包括IID和non-IID设置，与ResNet-50和REGNETY-800MF模型比较，结果显示LayerCAM-AE实现了完美的检测性能（Recall: 1.0, Precision: 1.0, F1 score: 1.0等），并显著提高了测试准确率。整体而言，该方法为联邦学习的安全性提供了更可靠的解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02605v1",
      "published_date": "2024-06-02 12:37:12 UTC",
      "updated_date": "2024-06-02 12:37:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:09:03.955272"
    },
    {
      "arxiv_id": "2406.06560v2",
      "title": "Inverse Constitutional AI: Compressing Preferences into Principles",
      "title_zh": "逆向宪法 AI：将偏好压缩成原则",
      "authors": [
        "Arduin Findeis",
        "Timo Kaufmann",
        "Eyke Hüllermeier",
        "Samuel Albanie",
        "Robert Mullins"
      ],
      "abstract": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.",
      "tldr_zh": "本论文引入了 Inverse Constitutional AI (ICAI) 问题，将配对文本偏好数据的解释视为一个压缩任务，旨在从反馈数据中提取一组原则（constitution），以帮助 large language model (LLM) 重建原注释，从而解决偏好数据集的解读挑战。ICAI 算法逆转了 Constitutional AI 的过程，通过训练 LLM 来生成简洁原则，并已在合成数据、AlpacaEval、Chatbot Arena 和 PRISM 数据集上验证，实现了高注释重建准确率。研究结果表明，这些原则可用于识别annotator 偏见、更好地理解模型性能、扩展反馈应用或适应用户/群体偏好，提供了一个可解释的偏好分析框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICLR 2025, v2 is camera-ready version; Main changes from\n  v1: extended experiments, additional baselines",
      "pdf_url": "http://arxiv.org/pdf/2406.06560v2",
      "published_date": "2024-06-02 11:54:50 UTC",
      "updated_date": "2025-04-21 15:37:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:09:15.189012"
    },
    {
      "arxiv_id": "2406.00704v2",
      "title": "An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites",
      "title_zh": "Tsetlin Machine Composites 的高级图像处理优化工具箱",
      "authors": [
        "Ylva Grønningsæter",
        "Halvor S. Smørvik",
        "Ole-Christoffer Granmo"
      ],
      "abstract": "The Tsetlin Machine (TM) has achieved competitive results on several image\nclassification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2.\nHowever, color image classification is arguably still in its infancy for TMs,\nwith CIFAR-10 being a focal point for tracking progress. Over the past few\nyears, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in\n2023 with the introduction of Drop Clause. In this paper, we leverage the\nrecently proposed TM Composites architecture and introduce a range of TM\nSpecialists that use various image processing techniques. These include Canny\nedge detection, Histogram of Oriented Gradients, adaptive mean thresholding,\nadaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and\nadaptive color thermometers. In addition, we conduct a rigorous hyperparameter\nsearch, where we uncover optimal hyperparameters for several of the TM\nSpecialists. The result is a toolbox that provides new state-of-the-art results\non CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM\nSpecialists forms a foundation for new TM applications and a landmark for\nfurther research on TM Composites in image analysis.",
      "tldr_zh": "本文提出了一种优化工具箱，用于基于 Tsetlin Machine Composites 的高级图像处理，针对颜色图像分类问题如 CIFAR-10 进行改进。工具箱引入了多种 TM Specialists，运用图像处理技术包括 Canny edge detection、Histogram of Oriented Gradients、adaptive mean thresholding 等，以提升分类性能。作者通过 rigorous hyperparameter search 优化超参数，最终在 CIFAR-10 上实现了 82.8% 的新 state-of-the-art 准确率，为 Tsetlin Machine 在图像分析中的新应用和进一步研究奠定基础。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.00704v2",
      "published_date": "2024-06-02 10:52:48 UTC",
      "updated_date": "2025-02-05 18:23:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:09:28.142704"
    },
    {
      "arxiv_id": "2406.00702v4",
      "title": "Enhanced Heart Sound Classification Using Mel Frequency Cepstral Coefficients and Comparative Analysis of Single vs. Ensemble Classifier Strategies",
      "title_zh": "使用梅尔频率倒谱系数的增强心音分类及单分类器与集成分类器策略的比较分析",
      "authors": [
        "Amir Masoud Rahmani",
        "Amir Haider",
        "Mohammad Adeli",
        "Olfa Mzoughi",
        "Entesar Gemeay",
        "Mokhtar Mohammadi",
        "Hamid Alinejad-Rokny",
        "Parisa Khoshvaght",
        "Mehdi Hosseinzadeh"
      ],
      "abstract": "This paper explores the efficacy of Mel Frequency Cepstral Coefficients\n(MFCCs) in detecting abnormal heart sounds using two classification strategies:\na single classifier and an ensemble classifier approach. Heart sounds were\nfirst pre-processed to remove noise and then segmented into S1, systole, S2,\nand diastole intervals, with thirteen MFCCs estimated from each segment,\nyielding 52 MFCCs per beat. Finally, MFCCs were used for heart sound\nclassification. For that purpose, in the single classifier strategy, the MFCCs\nfrom nine consecutive beats were averaged to classify heart sounds by a single\nclassifier (either a support vector machine (SVM), the k nearest neighbors\n(kNN), or a decision tree (DT)). Conversely, the ensemble classifier strategy\nemployed nine classifiers (either nine SVMs, nine kNN classifiers, or nine DTs)\nto individually assess beats as normal or abnormal, with the overall\nclassification based on the majority vote. Both methods were tested on a\npublicly available phonocardiogram database. The heart sound classification\naccuracy was 91.95% for the SVM, 91.9% for the kNN, and 87.33% for the DT in\nthe single classifier strategy. Also, the accuracy was 93.59% for the SVM,\n91.84% for the kNN, and 92.22% for the DT in the ensemble classifier strategy.\nOverall, the results demonstrated that the ensemble classifier strategy\nimproved the accuracies of the DT and the SVM by 4.89% and 1.64%, establishing\nMFCCs as more effective than other features, including time, time-frequency,\nand statistical features, evaluated in similar studies.",
      "tldr_zh": "本文研究了使用 Mel Frequency Cepstral Coefficients (MFCCs) 增强心音分类的效能，并比较了单一分类器和集成分类器策略。方法包括对心音进行预处理、分割成 S1、收缩期、S2 和舒张期，并从每个心跳提取 52 个 MFCCs；单一策略使用单个 SVM、kNN 或 DT 分类器处理 9 个心跳的平均特征，而集成策略则采用 9 个分类器进行多数投票。实验结果显示，集成策略使 DT 和 SVM 的准确率分别提高了 4.89% 和 1.64%，最高达 93.59%，证明 MFCCs 比时间、时频或统计特征更有效。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00702v4",
      "published_date": "2024-06-02 10:45:30 UTC",
      "updated_date": "2024-06-30 03:34:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:09:40.215794"
    },
    {
      "arxiv_id": "2407.01576v1",
      "title": "The Future of Aerial Communications: A Survey of IRS-Enhanced UAV Communication Technologies",
      "title_zh": "航空通信的未来：IRS 增强的 UAV 通信技术的调查",
      "authors": [
        "Zina Chkirbene",
        "Ala Gouissem",
        "Ridha Hamila",
        "Devrim Unal"
      ],
      "abstract": "The advent of Intelligent Reflecting Surfaces (IRS) and Unmanned Aerial\nVehicles (UAVs) is setting a new benchmark in the field of wireless\ncommunications. IRS, with their groundbreaking ability to manipulate\nelectromagnetic waves, have opened avenues for substantial enhancements in\nsignal quality, network efficiency, and spectral usage. These surfaces\ndynamically reconfigure the propagation environment, leading to optimized\nsignal paths and reduced interference. Concurrently, UAVs have emerged as\ndynamic, versatile elements within communication networks, offering high\nmobility and the ability to access and enhance coverage in areas where\ntraditional, fixed infrastructure falls short. This paper presents a\ncomprehensive survey on the synergistic integration of IRS and UAVs in wireless\nnetworks, highlighting how this innovative combination substantially boosts\nnetwork performance, particularly in terms of security, energy efficiency, and\nreliability. The versatility of UAVs, combined with the signal-manipulating\nprowess of IRS, creates a potent solution for overcoming the limitations of\nconventional communication setups, especially in challenging and underserved\nenvironments. Furthermore, the survey delves into the cutting-edge realm of\nMachine Learning (ML), exploring its role in the strategic deployment and\noperational optimization of UAVs equipped with IRS. The paper also underscores\nthe latest research and practical advancements in this field, providing\ninsights into real-world applications and experimental setups. It concludes by\ndiscussing the future prospects and potential directions for this emerging\ntechnology, positioning the IRS-UAV integration as a transformative force in\nthe landscape of next-generation wireless",
      "tldr_zh": "这篇论文对 IRS（Intelligent Reflecting Surfaces）和 UAV（Unmanned Aerial Vehicles）在无线通信中的整合进行全面调查，强调这种结合如何提升信号质量、网络效率和频谱利用，尤其在安全、能源效率和可靠性方面。论文探讨了 UAV 的高移动性与 IRS 的电磁波操控能力如何共同克服传统通信系统的局限性，并在挑战性环境中提供优化解决方案，同时引入 Machine Learning（ML）来优化 UAV 的部署和操作。最终，该调查总结了最新研究进展、实际应用和实验设置，并展望 IRS-UAV 整合作为下一代无线通信的变革性技术。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.01576v1",
      "published_date": "2024-06-02 09:58:53 UTC",
      "updated_date": "2024-06-02 09:58:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:09:49.539941"
    },
    {
      "arxiv_id": "2406.00667v1",
      "title": "An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging",
      "title_zh": "多模态大语言模型在医学成像中的效用初步调查",
      "authors": [
        "Sulaiman Khan",
        "Md. Rafiul Biswas",
        "Alina Murad",
        "Hazrat Ali",
        "Zubair Shah"
      ],
      "abstract": "Recent developments in multimodal large language models (MLLMs) have spurred\nsignificant interest in their potential applications across various medical\nimaging domains. On the one hand, there is a temptation to use these generative\nmodels to synthesize realistic-looking medical image data, while on the other\nhand, the ability to identify synthetic image data in a pool of data is also\nsignificantly important. In this study, we explore the potential of the Gemini\n(\\textit{gemini-1.0-pro-vision-latest}) and GPT-4V (gpt-4-vision-preview)\nmodels for medical image analysis using two modalities of medical image data.\nUtilizing synthetic and real imaging data, both Gemini AI and GPT-4V are first\nused to classify real versus synthetic images, followed by an interpretation\nand analysis of the input images. Experimental results demonstrate that both\nGemini and GPT-4 could perform some interpretation of the input images. In this\nspecific experiment, Gemini was able to perform slightly better than the GPT-4V\non the classification task. In contrast, responses associated with GPT-4V were\nmostly generic in nature. Our early investigation presented in this work\nprovides insights into the potential of MLLMs to assist with the classification\nand interpretation of retinal fundoscopy and lung X-ray images. We also\nidentify key limitations associated with the early investigation study on MLLMs\nfor specialized tasks in medical image analysis.",
      "tldr_zh": "这篇论文初步探讨了 Multimodal Large Language Models (MLLMs) 在医疗成像领域的应用潜力，特别是使用 Gemini (gemini-1.0-pro-vision-latest) 和 GPT-4V (gpt-4-vision-preview) 模型来分类真实与合成图像，并对输入图像进行解释和分析。实验涉及视网膜眼底摄影 (retinal fundoscopy) 和肺 X 光图像，结果显示 Gemini 在分类任务中略优于 GPT-4V，而 GPT-4V 的响应更倾向于泛化。总体而言，该研究揭示了 MLLMs 在医疗图像分类和解释方面的潜在价值，同时指出了模型在专业任务中的关键局限性。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted in Fifth IEEE Workshop on Artificial Intelligence for\n  HealthCare, IEEE 25th International Conference on Information Reuse and\n  Integration for Data Science",
      "pdf_url": "http://arxiv.org/pdf/2406.00667v1",
      "published_date": "2024-06-02 08:29:23 UTC",
      "updated_date": "2024-06-02 08:29:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:10:04.146913"
    },
    {
      "arxiv_id": "2406.00663v1",
      "title": "SimSAM: Zero-shot Medical Image Segmentation via Simulated Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Benjamin Towle",
        "Xin Chen",
        "Ke Zhou"
      ],
      "abstract": "The recently released Segment Anything Model (SAM) has shown powerful\nzero-shot segmentation capabilities through a semi-automatic annotation setup\nin which the user can provide a prompt in the form of clicks or bounding boxes.\nThere is growing interest around applying this to medical imaging, where the\ncost of obtaining expert annotations is high, privacy restrictions may limit\nsharing of patient data, and model generalisation is often poor. However, there\nare large amounts of inherent uncertainty in medical images, due to unclear\nobject boundaries, low-contrast media, and differences in expert labelling\nstyle. Currently, SAM is known to struggle in a zero-shot setting to adequately\nannotate the contours of the structure of interest in medical images, where the\nuncertainty is often greatest, thus requiring significant manual correction. To\nmitigate this, we introduce \\textbf{Sim}ulated Interaction for \\textbf{S}egment\n\\textbf{A}nything \\textbf{M}odel (\\textsc{\\textbf{SimSAM}}), an approach that\nleverages simulated user interaction to generate an arbitrary number of\ncandidate masks, and uses a novel aggregation approach to output the most\ncompatible mask. Crucially, our method can be used during inference directly on\ntop of SAM, without any additional training requirement. Quantitatively, we\nevaluate our method across three publicly available medical imaging datasets,\nand find that our approach leads to up to a 15.5\\% improvement in contour\nsegmentation accuracy compared to zero-shot SAM. Our code is available at\n\\url{https://github.com/BenjaminTowle/SimSAM}.",
      "tldr_zh": "该研究针对 Segment Anything Model (SAM) 在医疗图像零样本分割中的局限性（如模糊边界和低对比度导致的标注不准），提出了一种名为 SimSAM 的方法，通过模拟用户交互生成多个候选掩码，并采用新颖的聚合策略输出最兼容的掩码。SimSAM 无需额外训练，直接在 SAM 上进行推理，显著提高了分割准确性。在三个公开医疗图像数据集上的实验显示，与零样本 SAM 相比，SimSAM 将轮廓分割准确率提升了多达 15.5%。这项创新为减少手动修正和降低医疗图像标注成本提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at ISBI 2024. Awarded Top 12 Oral Presentation",
      "pdf_url": "http://arxiv.org/pdf/2406.00663v1",
      "published_date": "2024-06-02 08:13:12 UTC",
      "updated_date": "2024-06-02 08:13:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:10:15.278240"
    },
    {
      "arxiv_id": "2406.00661v1",
      "title": "Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayun Wu",
        "Jiashuo Liu",
        "Peng Cui",
        "Zhiwei Steven Wu"
      ],
      "abstract": "We establish a new model-agnostic optimization framework for\nout-of-distribution generalization via multicalibration, a criterion that\nensures a predictor is calibrated across a family of overlapping groups.\nMulticalibration is shown to be associated with robustness of statistical\ninference under covariate shift. We further establish a link between\nmulticalibration and robustness for prediction tasks both under and beyond\ncovariate shift. We accomplish this by extending multicalibration to\nincorporate grouping functions that consider covariates and labels jointly.\nThis leads to an equivalence of the extended multicalibration and invariance,\nan objective for robust learning in existence of concept shift. We show a\nlinear structure of the grouping function class spanned by density ratios,\nresulting in a unifying framework for robust learning by designing specific\ngrouping functions. We propose MC-Pseudolabel, a post-processing algorithm to\nachieve both extended multicalibration and out-of-distribution generalization.\nThe algorithm, with lightweight hyperparameters and optimization through a\nseries of supervised regression steps, achieves superior performance on\nreal-world datasets with distribution shift.",
      "tldr_zh": "本文提出一个模型无关的优化框架，通过 multicalibration 准则来提升 out-of-distribution 泛化性能，该框架确保预测器在重叠群体中保持校准，并与 covariate shift 下的统计推断鲁棒性相关联。研究扩展了 multicalibration，将 grouping functions 设计为考虑 covariates 和 labels 的联合形式，从而建立其与 invariance 的等价性，用于处理 concept shift 等场景。最终，作者开发了 MC-Pseudolabel 算法，通过一系列监督回归步骤实现 extended multicalibration 和分布偏移下的泛化表现，在真实数据集上取得了优越性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00661v1",
      "published_date": "2024-06-02 08:11:35 UTC",
      "updated_date": "2024-06-02 08:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:10:27.699826"
    },
    {
      "arxiv_id": "2406.00645v2",
      "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuwei Fu",
        "Haichao Zhang",
        "Di Wu",
        "Wei Xu",
        "Benoit Boulet"
      ],
      "abstract": "In this work, we investigate how to leverage pre-trained visual-language\nmodels (VLM) for online Reinforcement Learning (RL). In particular, we focus on\nsparse reward tasks with pre-defined textual task descriptions. We first\nidentify the problem of reward misalignment when applying VLM as a reward in RL\ntasks. To address this issue, we introduce a lightweight fine-tuning method,\nnamed Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL.\nSpecifically, we enhance the performance of SAC/DrQ baseline agents on sparse\nreward tasks by fine-tuning VLM representations and using relay RL to avoid\nlocal minima. Extensive experiments on the Meta-world benchmark tasks\ndemonstrate the efficacy of the proposed method. Code is available at:\nhttps://github.com/fuyw/FuRL.",
      "tldr_zh": "该研究探讨了如何利用预训练的视觉语言模型（VLM）作为模糊奖励来提升在线强化学习（RL），特别是在稀疏奖励任务中伴随预定义文本描述的问题。论文首先识别了VLM在RL中作为奖励时的奖励不匹配问题，并提出FuRL方法，通过奖励对齐和中继RL对VLM表示进行轻量级微调，以避免局部最优并增强SAC/DrQ基线代理的性能。在Meta-world基准任务上的广泛实验证明了FuRL的有效性，代码已在GitHub开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00645v2",
      "published_date": "2024-06-02 07:20:08 UTC",
      "updated_date": "2024-06-05 00:05:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:10:38.302537"
    },
    {
      "arxiv_id": "2406.00637v1",
      "title": "Representing Animatable Avatar via Factorized Neural Fields",
      "title_zh": "翻译失败",
      "authors": [
        "Chunjin Song",
        "Zhijie Wu",
        "Bastian Wandt",
        "Leonid Sigal",
        "Helge Rhodin"
      ],
      "abstract": "For reconstructing high-fidelity human 3D models from monocular videos, it is\ncrucial to maintain consistent large-scale body shapes along with finely\nmatched subtle wrinkles. This paper explores the observation that the per-frame\nrendering results can be factorized into a pose-independent component and a\ncorresponding pose-dependent equivalent to facilitate frame consistency. Pose\nadaptive textures can be further improved by restricting frequency bands of\nthese two components. In detail, pose-independent outputs are expected to be\nlow-frequency, while highfrequency information is linked to pose-dependent\nfactors. We achieve a coherent preservation of both coarse body contours across\nthe entire input video and finegrained texture features that are time variant\nwith a dual-branch network with distinct frequency components. The first branch\ntakes coordinates in canonical space as input, while the second branch\nadditionally considers features outputted by the first branch and pose\ninformation of each frame. Our network integrates the information predicted by\nboth branches and utilizes volume rendering to generate photo-realistic 3D\nhuman images. Through experiments, we demonstrate that our network surpasses\nthe neural radiance fields (NeRF) based state-of-the-art methods in preserving\nhigh-frequency details and ensuring consistent body contours.",
      "tldr_zh": "本论文提出了一种基于因子化神经字段(Factorized Neural Fields)的方法，用于从单目视频重建高保真可动画化人类3D模型，重点维护一致的大规模体型和精细纹理细节。方法通过将每帧渲染结果分解为姿势无关(pose-independent)低频组件和姿势相关(pose-dependent)高频组件，确保帧间一致性，并使用双分支网络优化这些组件：第一分支处理规范空间坐标，第二分支整合第一分支输出与帧姿势信息。最终，通过体积渲染(volume rendering)生成逼真的3D人类图像，实验结果显示，该方法在保留高频细节和确保体型一致性方面优于基于NeRF的现有技术。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00637v1",
      "published_date": "2024-06-02 06:45:38 UTC",
      "updated_date": "2024-06-02 06:45:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:10:50.839007"
    },
    {
      "arxiv_id": "2406.02604v1",
      "title": "Gated recurrent neural network with TPE Bayesian optimization for enhancing stock index prediction accuracy",
      "title_zh": "翻译失败",
      "authors": [
        "Bivas Dinda"
      ],
      "abstract": "The recent advancement of deep learning architectures, neural networks, and\nthe combination of abundant financial data and powerful computers are\ntransforming finance, leading us to develop an advanced method for predicting\nfuture stock prices. However, the accessibility of investment and trading at\neveryone's fingertips made the stock markets increasingly intricate and prone\nto volatility. The increased complexity and volatility of the stock market have\ndriven demand for more models, which would effectively capture high volatility\nand non-linear behavior of the different stock prices. This study explored\ngated recurrent neural network (GRNN) algorithms such as LSTM (long short-term\nmemory), GRU (gated recurrent unit), and hybrid models like GRU-LSTM, LSTM-GRU,\nwith Tree-structured Parzen Estimator (TPE) Bayesian optimization for\nhyperparameter optimization (TPE-GRNN). The aim is to improve the prediction\naccuracy of the next day's closing price of the NIFTY 50 index, a prominent\nIndian stock market index, using TPE-GRNN. A combination of eight influential\nfactors is carefully chosen from fundamental stock data, technical indicators,\ncrude oil price, and macroeconomic data to train the models for capturing the\nchanges in the price of the index with the factors of the broader economy.\nSingle-layer and multi-layer TPE-GRNN models have been developed. The models'\nperformance is evaluated using standard matrices like R2, MAPE, and RMSE. The\nanalysis of models' performance reveals the impact of feature selection and\nhyperparameter optimization (HPO) in enhancing stock index price prediction\naccuracy. The results show that the MAPE of our proposed TPE-LSTM method is the\nlowest (best) with respect to all the previous models for stock index price\nprediction.",
      "tldr_zh": "本研究探讨了使用门控循环神经网络 (GRNN) 算法，包括 LSTM、GRU 和混合模型 (如 GRU-LSTM、LSTM-GRU)，结合 Tree-structured Parzen Estimator (TPE) Bayesian optimization 进行超参数优化 (HPO)，以提升 NIFTY 50 指数次日收盘价的预测准确性。研究选取了八个影响因素，包括基本股票数据、技术指标、原油价格和宏观经济数据，来训练单层和多层 TPE-GRNN 模型。结果显示，TPE-LSTM 方法的 MAPE 最低，比现有模型表现更优，并证明了特征选择和 HPO 在提高股票指数预测准确性方面的关键作用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "q-fin.CP",
        "J.1"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 9 figures, 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2406.02604v1",
      "published_date": "2024-06-02 06:39:01 UTC",
      "updated_date": "2024-06-02 06:39:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:11:04.762045"
    },
    {
      "arxiv_id": "2406.00633v3",
      "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Dinghuai Zhang",
        "Yizhe Zhang",
        "Jiatao Gu",
        "Ruixiang Zhang",
        "Josh Susskind",
        "Navdeep Jaitly",
        "Shuangfei Zhai"
      ],
      "abstract": "Diffusion models have become the de-facto approach for generating visual\ndata, which are trained to match the distribution of the training dataset. In\naddition, we also want to control generation to fulfill desired properties such\nas alignment to a text description, which can be specified with a black-box\nreward function. Prior works fine-tune pretrained diffusion models to achieve\nthis goal through reinforcement learning-based algorithms. Nonetheless, they\nsuffer from issues including slow credit assignment as well as low quality in\ntheir generated samples. In this work, we explore techniques that do not\ndirectly maximize the reward but rather generate high-reward images with\nrelatively high probability -- a natural scenario for the framework of\ngenerative flow networks (GFlowNets). To this end, we propose the Diffusion\nAlignment with GFlowNet (DAG) algorithm to post-train diffusion models with\nblack-box property functions. Extensive experiments on Stable Diffusion and\nvarious reward specifications corroborate that our method could effectively\nalign large-scale text-to-image diffusion models with given reward information.",
      "tldr_zh": "本研究针对文本到图像扩散模型（Diffusion models）的对齐问题，提出改进 GFlowNets 框架，以生成更高质量且高奖励的图像，而非直接最大化奖励函数。该方法引入 Diffusion Alignment with GFlowNet (DAG) 算法，用于后训练预训练扩散模型，从而解决现有强化学习方法中的缓慢信用分配和样本质量低下问题。通过这种概率导向策略，DAG 算法能有效处理黑箱奖励函数，确保生成图像与文本描述更好地对齐。在 Stable Diffusion 等模型上的广泛实验验证了该方法的有效性，显著提升了文本到图像生成任务的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00633v3",
      "published_date": "2024-06-02 06:36:46 UTC",
      "updated_date": "2024-12-26 02:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:11:16.030797"
    },
    {
      "arxiv_id": "2406.06559v1",
      "title": "Harnessing Business and Media Insights with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yujia Bao",
        "Ankit Parag Shah",
        "Neeru Narang",
        "Jonathan Rivers",
        "Rajeev Maksey",
        "Lan Guan",
        "Louise N. Barrere",
        "Shelley Evenson",
        "Rahul Basole",
        "Connie Miao",
        "Ankit Mehta",
        "Fabien Boulay",
        "Su Min Park",
        "Natalie E. Pearson",
        "Eldhose Joy",
        "Tiger He",
        "Sumiran Thakur",
        "Koustav Ghosal",
        "Josh On",
        "Phoebe Morrison",
        "Tim Major",
        "Eva Siqi Wang",
        "Gina Escobar",
        "Jiaheng Wei",
        "Tharindu Cyril Weerasooriya",
        "Queena Song",
        "Daria Lashkevich",
        "Clare Chen",
        "Gyuhak Kim",
        "Dengpan Yin",
        "Don Hejna",
        "Mo Nomeli",
        "Wei Wei"
      ],
      "abstract": "This paper introduces Fortune Analytics Language Model (FALM). FALM empowers\nusers with direct access to comprehensive business analysis, including market\ntrends, company performance metrics, and expert insights. Unlike generic LLMs,\nFALM leverages a curated knowledge base built from professional journalism,\nenabling it to deliver precise and in-depth answers to intricate business\nquestions. Users can further leverage natural language queries to directly\nvisualize financial data, generating insightful charts and graphs to understand\ntrends across diverse business sectors clearly. FALM fosters user trust and\nensures output accuracy through three novel methods: 1) Time-aware reasoning\nguarantees accurate event registration and prioritizes recent updates. 2)\nThematic trend analysis explicitly examines topic evolution over time,\nproviding insights into emerging business landscapes. 3) Content referencing\nand task decomposition enhance answer fidelity and data visualization accuracy.\nWe conduct both automated and human evaluations, demonstrating FALM's\nsignificant performance improvements over baseline methods while prioritizing\nresponsible AI practices. These benchmarks establish FALM as a cutting-edge LLM\nin the business and media domains, with exceptional accuracy and\ntrustworthiness.",
      "tldr_zh": "本论文介绍了Fortune Analytics Language Model (FALM)，一个基于Large Language Models的工具，允许用户通过自然语言查询直接获取全面商业分析，包括市场趋势、公司绩效指标和专家见解。FALM利用从专业新闻构建的精选知识库，并引入三项创新方法——Time-aware reasoning、Thematic trend analysis以及Content referencing and task decomposition——来提升答案的准确性、时效性和数据可视化效果，从而在商业领域提供精确且可信的洞见。实验评估显示，FALM在自动和人工基准测试中显著优于基线模型，同时强调负责任的AI实践，确立其在商业和媒体领域的领先地位。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.06559v1",
      "published_date": "2024-06-02 06:24:38 UTC",
      "updated_date": "2024-06-02 06:24:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:11:29.028726"
    },
    {
      "arxiv_id": "2406.03406v1",
      "title": "LncRNA-disease association prediction method based on heterogeneous information completion and convolutional neural network",
      "title_zh": "基于异构信息补全和卷积神经网络的LncRNA-疾病关联预测方法",
      "authors": [
        "Wen-Yu Xi",
        "Juan Wang",
        "Yu-Lin Zhang",
        "Jin-Xing Liu",
        "Yin-Lian Gao"
      ],
      "abstract": "The emerging research shows that lncRNA has crucial research value in a\nseries of complex human diseases. Therefore, the accurate identification of\nlncRNA-disease associations (LDAs) is very important for the warning and\ntreatment of diseases. However, most of the existing methods have limitations\nin identifying nonlinear LDAs, and it remains a huge challenge to predict new\nLDAs. In this paper, a deep learning model based on a heterogeneous network and\nconvolutional neural network (CNN) is proposed for lncRNA-disease association\nprediction, named HCNNLDA. The heterogeneous network containing the lncRNA,\ndisease, and miRNA nodes, is constructed firstly. The embedding matrix of a\nlncRNA-disease node pair is constructed according to various biological\npremises about lncRNAs, diseases, and miRNAs. Then, the low-dimensional feature\nrepresentation is fully learned by the convolutional neural network. In the\nend, the XGBoot classifier model is trained to predict the potential LDAs.\nHCNNLDA obtains a high AUC value of 0.9752 and AUPR of 0.9740 under the 5-fold\ncross-validation. The experimental results show that the proposed model has\nbetter performance than that of several latest prediction models. Meanwhile,\nthe effectiveness of HCNNLDA in identifying novel LDAs is further demonstrated\nby case studies of three diseases. To sum up, HCNNLDA is a feasible calculation\nmodel to predict LDAs.",
      "tldr_zh": "本文提出了一种名为 HCNNLDA 的 lncRNA-disease association (LDA) 预测方法，结合异构网络和 convolutional neural network (CNN)，旨在克服现有模型在识别非线性 LDAs 时的局限性。该方法首先构建包含 lncRNA、disease 和 miRNA 节点的异构网络，并通过嵌入矩阵和 CNN 学习低维特征表示，然后使用 XGBoot 分类器进行预测。在 5-fold 交叉验证中，HCNNLDA 取得了 0.9752 的 AUC 和 0.9740 的 AUPR 值，显著优于其他模型，并在三种疾病的案例研究中证明了其识别新 LDAs 的有效性。总之，该模型为 lncRNA 与疾病关联的准确预测提供了可行且高效的计算框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.03406v1",
      "published_date": "2024-06-02 06:11:27 UTC",
      "updated_date": "2024-06-02 06:11:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:11:43.501023"
    },
    {
      "arxiv_id": "2406.00622v2",
      "title": "Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Xingrui Wang",
        "Wufei Ma",
        "Angtian Wang",
        "Shuo Chen",
        "Adam Kortylewski",
        "Alan Yuille"
      ],
      "abstract": "For vision-language models (VLMs), understanding the dynamic properties of\nobjects and their interactions in 3D scenes from videos is crucial for\neffective reasoning about high-level temporal and action semantics. Although\nhumans are adept at understanding these properties by constructing 3D and\ntemporal (4D) representations of the world, current video understanding models\nstruggle to extract these dynamic semantics, arguably because these models use\ncross-frame reasoning without underlying knowledge of the 3D/4D scenes. In this\nwork, we introduce DynSuperCLEVR, the first video question answering dataset\nthat focuses on language understanding of the dynamic properties of 3D objects.\nWe concentrate on three physical concepts -- velocity, acceleration, and\ncollisions within 4D scenes. We further generate three types of questions,\nincluding factual queries, future predictions, and counterfactual reasoning\nthat involve different aspects of reasoning about these 4D dynamic properties.\nTo further demonstrate the importance of explicit scene representations in\nanswering these 4D dynamics questions, we propose NS-4DPhysics, a\nNeural-Symbolic VideoQA model integrating Physics prior for 4D dynamic\nproperties with explicit scene representation of videos. Instead of answering\nthe questions directly from the video text input, our method first estimates\nthe 4D world states with a 3D generative model powered by physical priors, and\nthen uses neural symbolic reasoning to answer the questions based on the 4D\nworld states. Our evaluation on all three types of questions in DynSuperCLEVR\nshows that previous video question answering models and large multimodal models\nstruggle with questions about 4D dynamics, while our NS-4DPhysics significantly\noutperforms previous state-of-the-art models. Our code and data are released in\nhttps://xingruiwang.github.io/projects/DynSuperCLEVR/.",
      "tldr_zh": "本研究探讨了视觉语言模型（VLMs）在从视频中理解 3D 场景动态属性（如速度、加速度和碰撞）及其互动的挑战，强调了构建 4D 场景表示的重要性。为此，作者引入了 DynSuperCLEVR，这是首个专注于 4D 动态属性的视频问答数据集，包括事实查询、未来预测和反事实推理。提出 NS-4DPhysics 模型，该模型整合物理先验，通过 3D 生成模型估计 4D 世界状态，并使用神经符号推理（neural symbolic reasoning）回答问题。实验结果显示，NS-4DPhysics 在 DynSuperCLEVR 数据集上显著优于现有 VideoQA 模型，证明了显式场景表示在处理 4D dynamics 方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ICLR 2025 accepted paper. Project url:\n  https://xingruiwang.github.io/projects/DynSuperCLEVR/",
      "pdf_url": "http://arxiv.org/pdf/2406.00622v2",
      "published_date": "2024-06-02 05:51:15 UTC",
      "updated_date": "2025-04-23 04:53:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:11:54.791111"
    },
    {
      "arxiv_id": "2406.00619v1",
      "title": "A Multi-Graph Convolutional Neural Network Model for Short-Term Prediction of Turning Movements at Signalized Intersections",
      "title_zh": "翻译失败",
      "authors": [
        "Jewel Rana Palit",
        "Osama A Osman"
      ],
      "abstract": "Traffic flow forecasting is a crucial first step in intelligent and proactive\ntraffic management. Traffic flow parameters are volatile and uncertain, making\ntraffic flow forecasting a difficult task if the appropriate forecasting model\nis not used. Additionally, the non-Euclidean data structure of traffic flow\nparameters is challenging to analyze from both spatial and temporal\nperspectives. State-of-the-art deep learning approaches use pure convolution,\nrecurrent neural networks, and hybrid methods to achieve this objective\nefficiently. However, many of the approaches in the literature rely on complex\narchitectures that can be difficult to train. This complexity also adds to the\nblack-box nature of deep learning. This study introduces a novel deep learning\narchitecture, referred to as the multigraph convolution neural network (MGCNN),\nfor turning movement prediction at intersections. The proposed architecture\ncombines a multigraph structure, built to model temporal variations in traffic\ndata, with a spectral convolution operation to support modeling the spatial\nvariations in traffic data over the graphs. The proposed model was tested using\ntwenty days of flow and traffic control data collected from an arterial in\ndowntown Chattanooga, TN, with ten signalized intersections. The model's\nability to perform short-term predictions over 1, 2, 3, 4, and 5 minutes into\nthe future was evaluated against four baseline state-of-the-art models. The\nresults showed that our proposed model is superior to the other baseline models\nin predicting turning movements with a mean squared error (MSE) of 0.9",
      "tldr_zh": "本研究针对信号灯路口的转向运动短期预测问题，提出了一种新型多图卷积神经网络（Multi-Graph Convolutional Neural Network, MGCNN）模型，以应对交通数据的不确定性和非欧空间结构带来的挑战。MGCNN 通过多图结构建模交通数据的时变特性，并结合谱卷积操作来处理空间变异，实现高效的预测。实验使用查塔努加市20天的交通数据在10个信号灯路口进行测试，结果显示该模型在1至5分钟预测中，均方误差（Mean Squared Error, MSE）为0.9，比其他四种基线模型表现出色。总的来说，该方法为智能交通管理提供了更准确且易训练的深度学习框架。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "26 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.00619v1",
      "published_date": "2024-06-02 05:41:25 UTC",
      "updated_date": "2024-06-02 05:41:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:12:05.189623"
    },
    {
      "arxiv_id": "2406.00614v1",
      "title": "Efficient Monte Carlo Tree Search via On-the-Fly State-Conditioned Action Abstraction",
      "title_zh": "翻译失败",
      "authors": [
        "Yunhyeok Kwak",
        "Inwoo Hwang",
        "Dooyoung Kim",
        "Sanghack Lee",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Monte Carlo Tree Search (MCTS) has showcased its efficacy across a broad\nspectrum of decision-making problems. However, its performance often degrades\nunder vast combinatorial action space, especially where an action is composed\nof multiple sub-actions. In this work, we propose an action abstraction based\non the compositional structure between a state and sub-actions for improving\nthe efficiency of MCTS under a factored action space. Our method learns a\nlatent dynamics model with an auxiliary network that captures sub-actions\nrelevant to the transition on the current state, which we call\nstate-conditioned action abstraction. Notably, it infers such compositional\nrelationships from high-dimensional observations without the known environment\nmodel. During the tree traversal, our method constructs the state-conditioned\naction abstraction for each node on-the-fly, reducing the search space by\ndiscarding the exploration of redundant sub-actions. Experimental results\ndemonstrate the superior sample efficiency of our method compared to vanilla\nMuZero, which suffers from expansive action space.",
      "tldr_zh": "本文提出了一种高效的 Monte Carlo Tree Search (MCTS) 方法，通过实时构建 state-conditioned action abstraction 来优化在庞大组合动作空间下的性能。该方法学习一个潜在动态模型（latent dynamics model）并使用辅助网络，从高维观察中推断状态相关的子动作，从而减少树遍历中的冗余探索。实验结果表明，与 vanilla MuZero 相比，该方法显著提高了样本效率，尤其适用于动作由多个子动作组成的环境。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "UAI 2024 (Oral). The first two authors contributed equally",
      "pdf_url": "http://arxiv.org/pdf/2406.00614v1",
      "published_date": "2024-06-02 04:31:30 UTC",
      "updated_date": "2024-06-02 04:31:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:12:17.649827"
    },
    {
      "arxiv_id": "2406.00609v4",
      "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
      "title_zh": "SuperGaussian：为 3",
      "authors": [
        "Yuan Shen",
        "Duygu Ceylan",
        "Paul Guerrero",
        "Zexiang Xu",
        "Niloy J. Mitra",
        "Shenlong Wang",
        "Anna Frühstück"
      ],
      "abstract": "We present a simple, modular, and generic method that upsamples coarse 3D\nmodels by adding geometric and appearance details. While generative 3D models\nnow exist, they do not yet match the quality of their counterparts in image and\nvideo domains. We demonstrate that it is possible to directly repurpose\nexisting (pretrained) video models for 3D super-resolution and thus sidestep\nthe problem of the shortage of large repositories of high-quality 3D training\nmodels. We describe how to repurpose video upsampling models, which are not 3D\nconsistent, and combine them with 3D consolidation to produce 3D-consistent\nresults. As output, we produce high quality Gaussian Splat models, which are\nobject centric and effective. Our method is category agnostic and can be easily\nincorporated into existing 3D workflows. We evaluate our proposed SuperGaussian\non a variety of 3D inputs, which are diverse both in terms of complexity and\nrepresentation (e.g., Gaussian Splats or NeRFs), and demonstrate that our\nsimple method significantly improves the fidelity of the final 3D models. Check\nour project website for details: supergaussian.github.io",
      "tldr_zh": "本文提出了一种名为 SuperGaussian 的简单、模块化和通用方法，用于通过添加几何和外观细节来提升粗略 3D 模型的分辨率。该方法直接 repurposing 现有的预训练视频上采样模型，并结合 3D consolidation 技术，以生成 3D 一致的高质量 Gaussian Splat 模型，从而避免了高质量 3D 训练数据短缺的问题。实验结果显示，该方法在多样化的 3D 输入（如 Gaussian Splats 或 NeRFs）上显著提高了模型的保真度，并易于整合到现有的 3D 工作流中。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECCV 2024, project website with interactive demo:\n  https://supergaussian.github.io",
      "pdf_url": "http://arxiv.org/pdf/2406.00609v4",
      "published_date": "2024-06-02 03:44:50 UTC",
      "updated_date": "2024-07-16 04:41:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:12:31.505882"
    },
    {
      "arxiv_id": "2406.00605v1",
      "title": "LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Liang Zhao",
        "Tianwen Wei",
        "Liang Zeng",
        "Cheng Cheng",
        "Liu Yang",
        "Peng Cheng",
        "Lijie Wang",
        "Chenxia Li",
        "Xuejie Wu",
        "Bo Zhu",
        "Yimeng Gan",
        "Rui Hu",
        "Shuicheng Yan",
        "Han Fang",
        "Yahui Zhou"
      ],
      "abstract": "We introduce LongSkywork, a long-context Large Language Model (LLM) capable\nof processing up to 200,000 tokens. We provide a training recipe for\nefficiently extending context length of LLMs. We identify that the critical\nelement in enhancing long-context processing capability is to incorporate a\nlong-context SFT stage following the standard SFT stage. A mere 200 iterations\ncan convert the standard SFT model into a long-context model. To reduce the\neffort in collecting and annotating data for long-context language modeling, we\ndevelop two novel methods for creating synthetic data. These methods are\napplied during the continual pretraining phase as well as the Supervised\nFine-Tuning (SFT) phase, greatly enhancing the training efficiency of our\nlong-context LLMs. Our findings suggest that synthetic long-context SFT data\ncan surpass the performance of data curated by humans to some extent.\nLongSkywork achieves outstanding performance on a variety of long-context\nbenchmarks. In the Needle test, a benchmark for long-context information\nretrieval, our models achieved perfect accuracy across multiple context spans.\nMoreover, in realistic application scenarios, LongSkywork-13B demonstrates\nperformance on par with Claude2.1, the leading long-context model, underscoring\nthe effectiveness of our proposed methods.",
      "tldr_zh": "本研究引入 LongSkywork，一种能处理高达 200,000 tokens 的长上下文 Large Language Model (LLM)，并提出一种高效训练方法来扩展 LLM 的上下文长度。关键方法包括在标准 Supervised Fine-Tuning (SFT) 阶段后添加一个长上下文 SFT 阶段，仅需 200 次迭代即可实现模型转换，同时开发两种新颖的合成数据生成方法，用于持续预训练和 SFT 阶段，以提高训练效率。实验结果显示，LongSkywork 在 Needle 测试等长上下文基准上表现出色，实现完美准确率，且 LongSkywork-13B 在实际应用场景中与领先模型 Claude2.1 相当，证明合成数据在某些方面可超越人工策划数据。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00605v1",
      "published_date": "2024-06-02 03:34:41 UTC",
      "updated_date": "2024-06-02 03:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:12:41.823775"
    },
    {
      "arxiv_id": "2406.00600v1",
      "title": "Kolmogorov-Arnold Network for Satellite Image Classification in Remote Sensing",
      "title_zh": "Kolmogorov-Arnold 网络用于遥感中的卫星图像分类",
      "authors": [
        "Minjong Cheon"
      ],
      "abstract": "In this research, we propose the first approach for integrating the\nKolmogorov-Arnold Network (KAN) with various pre-trained Convolutional Neural\nNetwork (CNN) models for remote sensing (RS) scene classification tasks using\nthe EuroSAT dataset. Our novel methodology, named KCN, aims to replace\ntraditional Multi-Layer Perceptrons (MLPs) with KAN to enhance classification\nperformance. We employed multiple CNN-based models, including VGG16,\nMobileNetV2, EfficientNet, ConvNeXt, ResNet101, and Vision Transformer (ViT),\nand evaluated their performance when paired with KAN. Our experiments\ndemonstrated that KAN achieved high accuracy with fewer training epochs and\nparameters. Specifically, ConvNeXt paired with KAN showed the best performance,\nachieving 94% accuracy in the first epoch, which increased to 96% and remained\nconsistent across subsequent epochs. The results indicated that KAN and MLP\nboth achieved similar accuracy, with KAN performing slightly better in later\nepochs. By utilizing the EuroSAT dataset, we provided a robust testbed to\ninvestigate whether KAN is suitable for remote sensing classification tasks.\nGiven that KAN is a novel algorithm, there is substantial capacity for further\ndevelopment and optimization, suggesting that KCN offers a promising\nalternative for efficient image analysis in the RS field.",
      "tldr_zh": "本文提出了一种名为 KCN 的新方法，将 Kolmogorov-Arnold Network (KAN) 与各种预训练的 Convolutional Neural Network (CNN) 模型（如 VGG16、MobileNetV2 和 ConvNeXt）整合，用于遥感场景分类任务，旨在替换传统的 Multi-Layer Perceptrons (MLPs) 以提升性能。实验使用 EuroSAT 数据集评估了这些模型的组合，结果显示 KAN 能够在更少的训练周期和参数下实现高准确率，其中 ConvNeXt 与 KAN 结合在第一轮就达到 94% 准确率，并稳定在 96%。与 MLP 相比，KAN 在后期表现出轻微优势，为遥感图像分析提供了一个高效且有前景的替代方案，具有进一步优化潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "physics.data-an"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00600v1",
      "published_date": "2024-06-02 03:11:37 UTC",
      "updated_date": "2024-06-02 03:11:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:12:55.779554"
    },
    {
      "arxiv_id": "2406.00599v3",
      "title": "Robust Fair Clustering with Group Membership Uncertainty Sets",
      "title_zh": "带有群组成员不确定性集合的鲁棒公平聚类",
      "authors": [
        "Sharmila Duppala",
        "Juan Luque",
        "John P. Dickerson",
        "Seyed A. Esmaeili"
      ],
      "abstract": "We study the canonical fair clustering problem where each cluster is\nconstrained to have close to population-level representation of each group.\nDespite significant attention, the salient issue of having incomplete knowledge\nabout the group membership of each point has been superficially addressed. In\nthis paper, we consider a setting where the assigned group memberships are\nnoisy. We introduce a simple noise model that requires a small number of\nparameters to be given by the decision maker. We then present an algorithm for\nfair clustering with provable \\emph{robustness} guarantees. Our framework\nenables the decision maker to trade off between the robustness and the\nclustering quality. Unlike previous work, our algorithms are backed by\nworst-case theoretical guarantees. Finally, we empirically verify the\nperformance of our algorithm on real world datasets and show its superior\nperformance over existing baselines.",
      "tldr_zh": "这篇论文研究了鲁棒公平聚类问题，针对群组成员不确定性（如噪声数据）的情况，确保每个聚类中群体的表示接近总体水平。作者引入了一个简单的噪声模型，仅需决策者提供少量参数，并提出一个算法，提供可证明的鲁棒性保证，同时允许权衡鲁棒性和聚类质量。实验结果表明，该算法在真实数据集上表现优于现有基线，并具有最坏情况理论支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.DS"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00599v3",
      "published_date": "2024-06-02 03:11:31 UTC",
      "updated_date": "2024-11-20 17:12:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:13:08.816587"
    },
    {
      "arxiv_id": "2406.02602v1",
      "title": "D-FaST: Cognitive Signal Decoding with Disentangled Frequency-Spatial-Temporal Attention",
      "title_zh": "D-FaST：基于分离频域-空间-时域注意力的认知信号解码",
      "authors": [
        "Weiguo Chen",
        "Changjian Wang",
        "Kele Xu",
        "Yuan Yuan",
        "Yanru Bai",
        "Dongsong Zhang"
      ],
      "abstract": "Cognitive Language Processing (CLP), situated at the intersection of Natural\nLanguage Processing (NLP) and cognitive science, plays a progressively pivotal\nrole in the domains of artificial intelligence, cognitive intelligence, and\nbrain science. Among the essential areas of investigation in CLP, Cognitive\nSignal Decoding (CSD) has made remarkable achievements, yet there still exist\nchallenges related to insufficient global dynamic representation capability and\ndeficiencies in multi-domain feature integration. In this paper, we introduce a\nnovel paradigm for CLP referred to as Disentangled Frequency-Spatial-Temporal\nAttention(D-FaST). Specifically, we present an novel cognitive signal decoder\nthat operates on disentangled frequency-space-time domain attention. This\ndecoder encompasses three key components: frequency domain feature extraction\nemploying multi-view attention, spatial domain feature extraction utilizing\ndynamic brain connection graph attention, and temporal feature extraction\nrelying on local time sliding window attention. These components are integrated\nwithin a novel disentangled framework. Additionally, to encourage advancements\nin this field, we have created a new CLP dataset, MNRED. Subsequently, we\nconducted an extensive series of experiments, evaluating D-FaST's performance\non MNRED, as well as on publicly available datasets including ZuCo, BCIC IV-2A,\nand BCIC IV-2B. Our experimental results demonstrate that D-FaST outperforms\nexisting methods significantly on both our datasets and traditional CSD\ndatasets including establishing a state-of-the-art accuracy score 78.72% on\nMNRED, pushing the accuracy score on ZuCo to 78.35%, accuracy score on BCIC\nIV-2A to 74.85% and accuracy score on BCIC IV-2B to 76.81%.",
      "tldr_zh": "本文提出了一种新型认知信号解码框架 D-FaST，利用 disentangled frequency-spatial-temporal attention 来解决 Cognitive Signal Decoding (CSD) 中的全球动态表示和多域特征整合挑战。D-FaST 包括三个关键组件：频率域的 multi-view attention、空间域的 dynamic brain connection graph attention，以及时间域的 local time sliding window attention，这些在 disentangled framework 中整合以提升认知语言处理 (CLP) 性能。为推动研究，作者创建了新数据集 MNRED，并在多个数据集上进行实验，D-FaST 显著优于现有方法，达到 state-of-the-art 准确率，如 MNRED 上 78.72%、ZuCo 上 78.35%、BCIC IV-2A 上 74.85% 和 BCIC IV-2B 上 76.81%。这为 CLP 领域提供了更有效的解码范式。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 9 figures. Accepted by IEEE Transactions on Cognitive and\n  Developmental Systems",
      "pdf_url": "http://arxiv.org/pdf/2406.02602v1",
      "published_date": "2024-06-02 02:33:14 UTC",
      "updated_date": "2024-06-02 02:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:13:24.238250"
    },
    {
      "arxiv_id": "2406.00592v3",
      "title": "Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming",
      "title_zh": "模型预测控制和强化学习：基于动态规划的统一框架",
      "authors": [
        "Dimitri P. Bertsekas"
      ],
      "abstract": "In this paper we describe a new conceptual framework that connects\napproximate Dynamic Programming (DP), Model Predictive Control (MPC), and\nReinforcement Learning (RL). This framework centers around two algorithms,\nwhich are designed largely independently of each other and operate in synergy\nthrough the powerful mechanism of Newton's method. We call them the off-line\ntraining and the on-line play algorithms. The names are borrowed from some of\nthe major successes of RL involving games; primary examples are the recent\n(2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the\nsimilarly structured and earlier (1990s) TD-Gammon program (which plays\nbackgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line\ntraining algorithm is the method used to teach the program how to evaluate\npositions and to generate good moves at any given position, while the on-line\nplay algorithm is the method used to play in real time against human or\ncomputer opponents.\n  Significantly, the synergy between off-line training and on-line play also\nunderlies MPC (as well as other major classes of sequential decision problems),\nand indeed the MPC design architecture is very similar to the one of AlphaZero\nand TD-Gammon. This conceptual insight provides a vehicle for bridging the\ncultural gap between RL and MPC, and sheds new light on some fundamental issues\nin MPC. These include the enhancement of stability properties through rollout,\nthe treatment of uncertainty through the use of certainty equivalence, the\nresilience of MPC in adaptive control settings that involve changing system\nparameters, and the insights provided by the superlinear performance bounds\nimplied by Newton's method.",
      "tldr_zh": "本论文提出一个基于Dynamic Programming的统一框架，将Approximate Dynamic Programming (DP)、Model Predictive Control (MPC)和Reinforcement Learning (RL)连接起来，通过离线训练算法和在线播放算法协同工作。框架借鉴RL成功案例如AlphaZero和TD-Gammon，利用Newton's method增强算法协同，实现对顺序决策问题的有效处理。该框架揭示了MPC的稳定性提升（如通过rollout机制）、不确定性处理（如certainty equivalence）、适应性（如应对系统参数变化），并提供超线性性能边界，桥接了RL和MPC之间的文化差距。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.SY",
        "math.OC"
      ],
      "primary_category": "eess.SY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00592v3",
      "published_date": "2024-06-02 02:01:03 UTC",
      "updated_date": "2024-06-30 19:17:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:13:33.504919"
    },
    {
      "arxiv_id": "2406.00589v1",
      "title": "Robust Visual Tracking via Iterative Gradient Descent and Threshold Selection",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuang Qi",
        "Junlin Zhang",
        "Xin Qi"
      ],
      "abstract": "Visual tracking fundamentally involves regressing the state of the target in\neach frame of a video. Despite significant progress, existing regression-based\ntrackers still tend to experience failures and inaccuracies. To enhance the\nprecision of target estimation, this paper proposes a tracking technique based\non robust regression. Firstly, we introduce a novel robust linear regression\nestimator, which achieves favorable performance when the error vector follows\ni.i.d Gaussian-Laplacian distribution. Secondly, we design an iterative process\nto quickly solve the problem of outliers. In fact, the coefficients are\nobtained by Iterative Gradient Descent and Threshold Selection algorithm\n(IGDTS). In addition, we expend IGDTS to a generative tracker, and apply\nIGDTS-distance to measure the deviation between the sample and the model.\nFinally, we propose an update scheme to capture the appearance changes of the\ntracked object and ensure that the model is updated correctly. Experimental\nresults on several challenging image sequences show that the proposed tracker\noutperformance existing trackers.",
      "tldr_zh": "本文针对视觉跟踪中的回归问题，提出了一种基于鲁棒回归的跟踪技术，以提升目标估计精度。首先，引入一个新的鲁棒线性回归估计器，适用于误差向量遵循 i.i.d Gaussian-Laplacian distribution，并通过 Iterative Gradient Descent and Threshold Selection (IGDTS) 算法迭代解决异常值问题。其次，将 IGDTS 扩展到生成式跟踪器，并使用 IGDTS-distance 测量样本与模型的偏差，同时提出一个更新方案来捕捉跟踪对象的外观变化。实验结果显示，该跟踪器在多个具有挑战性的图像序列上，性能优于现有跟踪器。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00589v1",
      "published_date": "2024-06-02 01:51:09 UTC",
      "updated_date": "2024-06-02 01:51:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:13:46.440457"
    },
    {
      "arxiv_id": "2406.00586v2",
      "title": "VeriSplit: Secure and Practical Offloading of Machine Learning Inferences across IoT Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Han Zhang",
        "Zifan Wang",
        "Mihir Dhamankar",
        "Matt Fredrikson",
        "Yuvraj Agarwal"
      ],
      "abstract": "Many Internet-of-Things (IoT) devices rely on cloud computation resources to\nperform machine learning inferences. This is expensive and may raise privacy\nconcerns for users. Consumers of these devices often have hardware such as\ngaming consoles and PCs with graphics accelerators that are capable of\nperforming these computations, which may be left idle for significant periods\nof time. While this presents a compelling potential alternative to cloud\noffloading, concerns about the integrity of inferences, the confidentiality of\nmodel parameters, and the privacy of users' data mean that device vendors may\nbe hesitant to offload their inferences to a platform managed by another\nmanufacturer.\n  We propose VeriSplit, a framework for offloading machine learning inferences\nto locally-available devices that address these concerns. We introduce masking\ntechniques to protect data privacy and model confidentiality, and a\ncommitment-based verification protocol to address integrity. Unlike much prior\nwork aimed at addressing these issues, our approach does not rely on\ncomputation over finite field elements, which may interfere with floating-point\ncomputation supports on hardware accelerators and require modification to\nexisting models. We implemented a prototype of VeriSplit and our evaluation\nresults show that, compared to performing computation locally, our secure and\nprivate offloading solution can reduce inference latency by 28%--83%.",
      "tldr_zh": "论文提出 VeriSplit 框架，用于安全地将机器学习 inferences 从 IoT 设备卸载到本地可用设备，如 PC 和游戏主机，以降低云端计算的成本和隐私风险。框架采用 masking techniques 保护数据隐私和模型机密性，并引入 commitment-based verification protocol 来确保推理完整性，与传统方法不同的是，它避免了依赖有限域元素的计算，从而兼容硬件加速器的浮点运算。实验原型显示，与本地计算相比，VeriSplit 可将推理延迟减少 28%–83%。这项工作为实用、安全的 IoT 推理卸载提供了可行解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.00586v2",
      "published_date": "2024-06-02 01:28:38 UTC",
      "updated_date": "2025-03-31 04:32:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:13:58.094619"
    },
    {
      "arxiv_id": "2406.00584v1",
      "title": "A Blueprint Architecture of Compound AI Systems for Enterprise",
      "title_zh": "用于企业的复合 AI 系统蓝图架构",
      "authors": [
        "Eser Kandogan",
        "Sajjadur Rahman",
        "Nikita Bhutani",
        "Dan Zhang",
        "Rafael Li Chen",
        "Kushan Mitra",
        "Sairam Gurajada",
        "Pouya Pezeshkpour",
        "Hayate Iso",
        "Yanlin Feng",
        "Hannah Kim",
        "Chen Shen",
        "Jin Wang",
        "Estevam Hruschka"
      ],
      "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities\nsurpassing conventional NLP challenges, creating opportunities for use in\nproduction use cases. Towards this goal, there is a notable shift to building\ncompound AI systems, wherein LLMs are integrated into an expansive software\ninfrastructure with many components like models, retrievers, databases and\ntools. In this paper, we introduce a blueprint architecture for compound AI\nsystems to operate in enterprise settings cost-effectively and feasibly. Our\nproposed architecture aims for seamless integration with existing compute and\ndata infrastructure, with ``stream'' serving as the key orchestration concept\nto coordinate data and instructions among agents and other components. Task and\ndata planners, respectively, break down, map, and optimize tasks and data to\navailable agents and data sources defined in respective registries, given\nproduction constraints such as accuracy and latency.",
      "tldr_zh": "这篇论文提出了一种针对企业的compound AI systems的blueprint architecture，将Large Language Models (LLMs)整合到包括retrievers、databases和tools在内的软件基础设施中，以实现高效生产应用。该架构强调与现有计算和数据基础设施的无缝整合，以“stream”作为关键编排概念，用于协调agents和组件之间的数据及指令。论文中，task and data planners分别负责分解、映射和优化任务及数据资源，以满足生产约束如accuracy和latency。总体而言，此blueprint architecture为企业在成本有效和可行条件下部署compound AI systems提供了实用框架。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Compound AI Systems Workshop at the Data+AI Summit 2024",
      "pdf_url": "http://arxiv.org/pdf/2406.00584v1",
      "published_date": "2024-06-02 01:16:32 UTC",
      "updated_date": "2024-06-02 01:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:14:11.686905"
    },
    {
      "arxiv_id": "2406.02601v1",
      "title": "Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding Alignment Approach for Healthcare Applications",
      "title_zh": "多模态深度学习在低资源环境中的应用：一种用于医疗领域的向量嵌入对齐方法",
      "authors": [
        "David Restrepo",
        "Chenwei Wu",
        "Sebastián Andrés Cajas",
        "Luis Filipe Nakayama",
        "Leo Anthony Celi",
        "Diego M López"
      ],
      "abstract": "Large-scale multi-modal deep learning models have revolutionized domains such\nas healthcare, highlighting the importance of computational power. However, in\nresource-constrained regions like Low and Middle-Income Countries (LMICs),\nlimited access to GPUs and data poses significant challenges, often leaving\nCPUs as the sole resource. To address this, we advocate for leveraging vector\nembeddings to enable flexible and efficient computational methodologies,\ndemocratizing multimodal deep learning across diverse contexts.\n  Our paper investigates the efficiency and effectiveness of using vector\nembeddings from single-modal foundation models and multi-modal Vision-Language\nModels (VLMs) for multimodal deep learning in low-resource environments,\nparticularly in healthcare. Additionally, we propose a simple yet effective\ninference-time method to enhance performance by aligning image-text embeddings.\nComparing these approaches with traditional methods, we assess their impact on\ncomputational efficiency and model performance using metrics like accuracy,\nF1-score, inference time, training time, and memory usage across three medical\nmodalities: BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench\n(public health).\n  Our findings show that embeddings reduce computational demands without\ncompromising model performance. Furthermore, our alignment method improves\nperformance in medical tasks. This research promotes sustainable AI practices\nby optimizing resources in constrained environments, highlighting the potential\nof embedding-based approaches for efficient multimodal learning. Vector\nembeddings democratize multimodal deep learning in LMICs, particularly in\nhealthcare, enhancing AI adaptability in varied use cases.",
      "tldr_zh": "该论文探讨了在资源有限的低收入和中等收入国家（LMICs）中应用多模态深度学习（Multimodal Deep Learning）的挑战，提出了一种基于向量嵌入（Vector Embeddings）对齐方法，以提升医疗应用的计算效率。研究利用单模态基础模型和多模态视觉语言模型（VLMs）的嵌入，开发了一个简单的推理时图像-文本嵌入对齐技术，并在BRSET（眼科）、HAM10000（皮肤科）和SatelliteBench（公共卫生）等数据集上评估其性能，包括准确率、F1-score、推理时间、训练时间和内存使用。结果显示，这种方法显著降低了计算需求，同时维持或提升了模型性能，推动了可持续AI实践，并使多模态深度学习在LMICs的医疗领域更易普及。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.02601v1",
      "published_date": "2024-06-02 01:13:01 UTC",
      "updated_date": "2024-06-02 01:13:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:14:22.076028"
    },
    {
      "arxiv_id": "2406.00583v1",
      "title": "CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Yanlin Feng",
        "Sajjadur Rahman",
        "Aaron Feng",
        "Vincent Chen",
        "Eser Kandogan"
      ],
      "abstract": "Compound AI systems (CASs) that employ LLMs as agents to accomplish\nknowledge-intensive tasks via interactions with tools and data retrievers have\ngarnered significant interest within database and AI communities. While these\nsystems have the potential to supplement typical analysis workflows of data\nanalysts in enterprise data platforms, unfortunately, CASs are subject to the\nsame data discovery challenges that analysts have encountered over the years --\nsilos of multimodal data sources, created across teams and departments within\nan organization, make it difficult to identify appropriate data sources for\naccomplishing the task at hand. Existing data discovery benchmarks do not model\nsuch multimodality and multiplicity of data sources. Moreover, benchmarks of\nCASs prioritize only evaluating end-to-end task performance. To catalyze\nresearch on evaluating the data discovery performance of multimodal data\nretrievers in CASs within a real-world setting, we propose CMDBench, a\nbenchmark modeling the complexity of enterprise data platforms. We adapt\nexisting datasets and benchmarks in open-domain -- from question answering and\ncomplex reasoning tasks to natural language querying over structured data -- to\nevaluate coarse- and fine-grained data discovery and task execution\nperformance. Our experiments reveal the impact of data retriever design on\ndownstream task performance -- a 46% drop in task accuracy on average -- across\nvarious modalities, data sources, and task difficulty. The results indicate the\nneed to develop optimization strategies to identify appropriate LLM agents and\nretrievers for efficient execution of CASs over enterprise data.",
      "tldr_zh": "该研究提出了 CMDBench，一种基准，用于评估 Compound AI Systems (CASs) 中粗到细的多模态数据发现性能，针对企业数据平台中多源数据孤岛的挑战。CMDBench 通过适应现有数据集（如问答、复杂推理和自然语言查询任务），来评估粗粒度和细粒度的数据检索以及下游任务执行效果。实验结果显示，数据检索器设计对任务性能有显著影响，导致平均任务准确率下降 46%，强调了开发优化策略以选择合适的 LLM 代理和检索器的重要性。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Governance, Understanding and Integration of Data for Effective and\n  Responsible AI (GUIDE-AI '24), June 14, 2024, Santiago, AA, Chile",
      "pdf_url": "http://arxiv.org/pdf/2406.00583v1",
      "published_date": "2024-06-02 01:10:41 UTC",
      "updated_date": "2024-06-02 01:10:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:14:34.559817"
    },
    {
      "arxiv_id": "2406.00578v1",
      "title": "ContextFlow++: Generalist-Specialist Flow-based Generative Models with Mixed-Variable Context Encoding",
      "title_zh": "ContextFlow++：通用-专家式基于流的生成模型，带有混合变量上下文编码",
      "authors": [
        "Denis Gudovskiy",
        "Tomoyuki Okuno",
        "Yohei Nakata"
      ],
      "abstract": "Normalizing flow-based generative models have been widely used in\napplications where the exact density estimation is of major importance. Recent\nresearch proposes numerous methods to improve their expressivity. However,\nconditioning on a context is largely overlooked area in the bijective flow\nresearch. Conventional conditioning with the vector concatenation is limited to\nonly a few flow types. More importantly, this approach cannot support a\npractical setup where a set of context-conditioned (specialist) models are\ntrained with the fixed pretrained general-knowledge (generalist) model. We\npropose ContextFlow++ approach to overcome these limitations using an additive\nconditioning with explicit generalist-specialist knowledge decoupling.\nFurthermore, we support discrete contexts by the proposed mixed-variable\narchitecture with context encoders. Particularly, our context encoder for\ndiscrete variables is a surjective flow from which the context-conditioned\ncontinuous variables are sampled. Our experiments on rotated MNIST-R, corrupted\nCIFAR-10C, real-world ATM predictive maintenance and SMAP unsupervised anomaly\ndetection benchmarks show that the proposed ContextFlow++ offers faster stable\ntraining and achieves higher performance metrics. Our code is publicly\navailable at https://github.com/gudovskiy/contextflow.",
      "tldr_zh": "该论文针对基于 Normalizing Flow 的生成模型在条件化（conditioning）方面的不足，提出了 ContextFlow++ 方法，以提升其对上下文的处理能力。ContextFlow++ 通过加法条件化和显式 generalist-specialist 知识解耦，支持将固定预训练的通用模型（generalist）与条件化专家模型（specialist）结合，同时引入混合变量架构和上下文编码器来处理离散变量，其中离散变量的编码器采用 surjective flow 从中采样条件化的连续变量。实验结果显示，在 rotated MNIST-R、corrupted CIFAR-10C、ATM 预测维护和 SMAP 无监督异常检测基准上，ContextFlow++ 实现了更快的稳定训练和更高的性能指标。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to UAI 2024. Preprint",
      "pdf_url": "http://arxiv.org/pdf/2406.00578v1",
      "published_date": "2024-06-02 00:00:00 UTC",
      "updated_date": "2024-06-02 00:00:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T15:14:46.863894"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 59,
  "processed_papers_count": 59,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T15:15:13.246495"
}
[
  {
    "arxiv_id": "2406.00901v1",
    "title": "Robust Multi-Modal Speech In-Painting: A Sequence-to-Sequence Approach",
    "authors": [
      "Mahsa Kadkhodaei Elyaderani",
      "Shahram Shirani"
    ],
    "abstract": "The process of reconstructing missing parts of speech audio from context is\ncalled speech in-painting. Human perception of speech is inherently\nmulti-modal, involving both audio and visual (AV) cues. In this paper, we\nintroduce and study a sequence-to-sequence (seq2seq) speech in-painting model\nthat incorporates AV features. Our approach extends AV speech in-painting\ntechniques to scenarios where both audio and visual data may be jointly\ncorrupted. To achieve this, we employ a multi-modal training paradigm that\nboosts the robustness of our model across various conditions involving acoustic\nand visual distortions. This makes our distortion-aware model a plausible\nsolution for real-world challenging environments. We compare our method with\nexisting transformer-based and recurrent neural network-based models, which\nattempt to reconstruct missing speech gaps ranging from a few milliseconds to\nover a second. Our experimental results demonstrate that our novel seq2seq\narchitecture outperforms the state-of-the-art transformer solution by 38.8% in\nterms of enhancing speech quality and 7.14% in terms of improving speech\nintelligibility. We exploit a multi-task learning framework that simultaneously\nperforms lip-reading (transcribing video components to text) while\nreconstructing missing parts of the associated speech.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.MM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00901v1",
    "published_date": "2024-06-02 23:51:43 UTC",
    "updated_date": "2024-06-02 23:51:43 UTC"
  },
  {
    "arxiv_id": "2406.00894v1",
    "title": "Pretrained Hybrids with MAD Skills",
    "authors": [
      "Nicholas Roberts",
      "Samuel Guo",
      "Zhiqi Gao",
      "Satya Sai Srinath Namburi GNVV",
      "Sonia Cromp",
      "Chengjun Wu",
      "Chengyu Duan",
      "Frederic Sala"
    ],
    "abstract": "While Transformers underpin modern large language models (LMs), there is a\ngrowing list of alternative architectures with new capabilities, promises, and\ntradeoffs. This makes choosing the right LM architecture challenging.\nRecently-proposed $\\textit{hybrid architectures}$ seek a best-of-all-worlds\napproach that reaps the benefits of all architectures. Hybrid design is\ndifficult for two reasons: it requires manual expert-driven search, and new\nhybrids must be trained from scratch. We propose $\\textbf{Manticore}$, a\nframework that addresses these challenges. Manticore $\\textit{automates the\ndesign of hybrid architectures}$ while reusing pretrained models to create\n$\\textit{pretrained}$ hybrids. Our approach augments ideas from differentiable\nNeural Architecture Search (NAS) by incorporating simple projectors that\ntranslate features between pretrained blocks from different architectures. We\nthen fine-tune hybrids that combine pretrained models from different\narchitecture families -- such as the GPT series and Mamba -- end-to-end. With\nManticore, we enable LM selection without training multiple models, the\nconstruction of pretrained hybrids from existing pretrained models, and the\nability to $\\textit{program}$ pretrained hybrids to have certain capabilities.\nManticore hybrids outperform existing manually-designed hybrids, achieve strong\nperformance on Long Range Arena (LRA) tasks, and can improve on pretrained\ntransformers and state space models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00894v1",
    "published_date": "2024-06-02 23:24:30 UTC",
    "updated_date": "2024-06-02 23:24:30 UTC"
  },
  {
    "arxiv_id": "2406.00877v1",
    "title": "Evidence of Learned Look-Ahead in a Chess-Playing Neural Network",
    "authors": [
      "Erik Jenner",
      "Shreyas Kapur",
      "Vasil Georgiev",
      "Cameron Allen",
      "Scott Emmons",
      "Stuart Russell"
    ],
    "abstract": "Do neural networks learn to implement algorithms such as look-ahead or search\n\"in the wild\"? Or do they rely purely on collections of simple heuristics? We\npresent evidence of learned look-ahead in the policy network of Leela Chess\nZero, the currently strongest neural chess engine. We find that Leela\ninternally represents future optimal moves and that these representations are\ncrucial for its final output in certain board states. Concretely, we exploit\nthe fact that Leela is a transformer that treats every chessboard square like a\ntoken in language models, and give three lines of evidence (1) activations on\ncertain squares of future moves are unusually important causally; (2) we find\nattention heads that move important information \"forward and backward in time,\"\ne.g., from squares of future moves to squares of earlier ones; and (3) we train\na simple probe that can predict the optimal move 2 turns ahead with 92%\naccuracy (in board states where Leela finds a single best line). These findings\nare an existence proof of learned look-ahead in neural networks and might be a\nstep towards a better understanding of their capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Project page: https://leela-interp.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.00877v1",
    "published_date": "2024-06-02 21:57:32 UTC",
    "updated_date": "2024-06-02 21:57:32 UTC"
  },
  {
    "arxiv_id": "2406.00873v2",
    "title": "Scaffold Splits Overestimate Virtual Screening Performance",
    "authors": [
      "Qianrong Guo",
      "Saiveth Hernandez-Hernandez",
      "Pedro J Ballester"
    ],
    "abstract": "Virtual Screening (VS) of vast compound libraries guided by Artificial\nIntelligence (AI) models is a highly productive approach to early drug\ndiscovery. Data splitting is crucial for better benchmarking of such AI models.\nTraditional random data splits produce similar molecules between training and\ntest sets, conflicting with the reality of VS libraries which mostly contain\nstructurally distinct compounds. Scaffold split, grouping molecules by shared\ncore structure, is widely considered to reflect this real-world scenario.\nHowever, here we show that the scaffold split also overestimates VS\nperformance. The reason is that molecules with different chemical scaffolds are\noften similar, which hence introduces unrealistically high similarities between\ntraining molecules and test molecules following a scaffold split. Our study\nexamined three representative AI models on 60 NCI-60 datasets, each with\napproximately 30,000 to 50,000 molecules tested on a different cancer cell\nline. Each dataset was split with three methods: scaffold, Butina clustering\nand the more accurate Uniform Manifold Approximation and Projection (UMAP)\nclustering. Regardless of the model, model performance is much worse with UMAP\nsplits from the results of the 2100 models trained and evaluated for each\nalgorithm and split. These robust results demonstrate the need for more\nrealistic data splits to tune, compare, and select models for VS. For the same\nreason, avoiding the scaffold split is also recommended for other molecular\nproperty prediction problems. The code to reproduce these results is available\nat https://github.com/ScaffoldSplitsOverestimateVS",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00873v2",
    "published_date": "2024-06-02 21:40:13 UTC",
    "updated_date": "2024-06-30 12:12:23 UTC"
  },
  {
    "arxiv_id": "2406.00872v1",
    "title": "OLIVE: Object Level In-Context Visual Embeddings",
    "authors": [
      "Timothy Ossowski",
      "Junjie Hu"
    ],
    "abstract": "Recent generalist vision-language models (VLMs) have demonstrated impressive\nreasoning capabilities across diverse multimodal tasks. However, these models\nstill struggle with fine-grained object-level understanding and grounding. In\nterms of modeling, existing VLMs implicitly align text tokens with image patch\ntokens, which is ineffective for embedding alignment at the same granularity\nand inevitably introduces noisy spurious background features. Additionally,\nthese models struggle when generalizing to unseen visual concepts and may not\nbe reliable for domain-specific tasks without further fine-tuning. To address\nthese limitations, we propose a novel method to prompt large language models\nwith in-context visual object vectors, thereby enabling controllable\nobject-level reasoning. This eliminates the necessity of fusing a lengthy array\nof image patch features and significantly speeds up training. Furthermore, we\npropose region-level retrieval using our object representations, facilitating\nrapid adaptation to new objects without additional training. Our experiments\nreveal that our method achieves competitive referring object classification and\ncaptioning performance, while also offering zero-shot generalization and\nrobustness to visually challenging contexts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ACL 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00872v1",
    "published_date": "2024-06-02 21:36:31 UTC",
    "updated_date": "2024-06-02 21:36:31 UTC"
  },
  {
    "arxiv_id": "2406.00867v1",
    "title": "Formality Style Transfer in Persian",
    "authors": [
      "Parastoo Falakaflaki",
      "Mehrnoush Shamsfard"
    ],
    "abstract": "This study explores the formality style transfer in Persian, particularly\nrelevant in the face of the increasing prevalence of informal language on\ndigital platforms, which poses challenges for existing Natural Language\nProcessing (NLP) tools. The aim is to transform informal text into formal while\nretaining the original meaning, addressing both lexical and syntactic\ndifferences. We introduce a novel model, Fa-BERT2BERT, based on the Fa-BERT\narchitecture, incorporating consistency learning and gradient-based dynamic\nweighting. This approach improves the model's understanding of syntactic\nvariations, balancing loss components effectively during training. Our\nevaluation of Fa-BERT2BERT against existing methods employs new metrics\ndesigned to accurately measure syntactic and stylistic changes. Results\ndemonstrate our model's superior performance over traditional techniques across\nvarious metrics, including BLEU, BERT score, Rouge-l, and proposed metrics\nunderscoring its ability to adeptly navigate the complexities of Persian\nlanguage style transfer. This study significantly contributes to Persian\nlanguage processing by enhancing the accuracy and functionality of NLP models\nand thereby supports the development of more efficient and reliable NLP\napplications, capable of handling language style transformation effectively,\nthereby streamlining content moderation, enhancing data mining results, and\nfacilitating cross-cultural communication.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 4 figures, 8 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.00867v1",
    "published_date": "2024-06-02 20:57:27 UTC",
    "updated_date": "2024-06-02 20:57:27 UTC"
  },
  {
    "arxiv_id": "2406.00855v1",
    "title": "LinkLogic: A New Method and Benchmark for Explainable Knowledge Graph Predictions",
    "authors": [
      "Niraj Kumar-Singh",
      "Gustavo Polleti",
      "Saee Paliwal",
      "Rachel Hodos-Nkhereanye"
    ],
    "abstract": "While there are a plethora of methods for link prediction in knowledge\ngraphs, state-of-the-art approaches are often black box, obfuscating model\nreasoning and thereby limiting the ability of users to make informed decisions\nabout model predictions. Recently, methods have emerged to generate prediction\nexplanations for Knowledge Graph Embedding models, a widely-used class of\nmethods for link prediction. The question then becomes, how well do these\nexplanation systems work? To date this has generally been addressed\nanecdotally, or through time-consuming user research. In this work, we present\nan in-depth exploration of a simple link prediction explanation method we call\nLinkLogic, that surfaces and ranks explanatory information used for the\nprediction. Importantly, we construct the first-ever link prediction\nexplanation benchmark, based on family structures present in the FB13 dataset.\nWe demonstrate the use of this benchmark as a rich evaluation sandbox, probing\nLinkLogic quantitatively and qualitatively to assess the fidelity, selectivity\nand relevance of the generated explanations. We hope our work paves the way for\nmore holistic and empirical assessment of knowledge graph prediction\nexplanation methods in the future.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "I.2.4"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages, 4 figures in main text. For code and data, see\n  https://github.com/niraj17singh/LinkLogic",
    "pdf_url": "http://arxiv.org/pdf/2406.00855v1",
    "published_date": "2024-06-02 20:22:22 UTC",
    "updated_date": "2024-06-02 20:22:22 UTC"
  },
  {
    "arxiv_id": "2406.00839v1",
    "title": "FOCUS: Forging Originality through Contrastive Use in Self-Plagiarism for Language Models",
    "authors": [
      "Kaixin Lan",
      "Tao Fang",
      "Derek F. Wong",
      "Yabo Xu",
      "Lidia S. Chao",
      "Cecilia G. Zhao"
    ],
    "abstract": "Pre-trained Language Models (PLMs) have shown impressive results in various\nNatural Language Generation (NLG) tasks, such as powering chatbots and\ngenerating stories. However, an ethical concern arises due to their potential\nto produce verbatim copies of paragraphs from their training data. This is\nproblematic as PLMs are trained on corpora constructed by human authors. As\nsuch, there is a pressing need for research to promote the generation of\noriginal content by these models. In this study, we introduce a unique\n\"self-plagiarism\" contrastive decoding strategy, aimed at boosting the\noriginality of text produced by PLMs. Our method entails modifying prompts in\nLLMs to develop an amateur model and a professional model. Specifically, the\namateur model is urged to plagiarize using three plagiarism templates we have\ndesigned, while the professional model maintains its standard language model\nstatus. This strategy employs prompts to stimulate the model's capacity to\nidentify non-original candidate token combinations and subsequently impose\npenalties. The application of this strategy is integrated prior to the model's\nfinal layer, ensuring smooth integration with most existing PLMs (T5, GPT,\nLLaMA) without necessitating further adjustments. Implementing our strategy, we\nobserve a significant decline in non-original sequences comprised of more than\nthree words in the academic AASC dataset and the story-based ROCStories\ndataset.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 8 figures. The paper has been accepted by ACL 2024\n  (Findings), with Kaixin Lan and Tao Fang contributing equally, and Derek F.\n  Wong serving as the corresponding author",
    "pdf_url": "http://arxiv.org/pdf/2406.00839v1",
    "published_date": "2024-06-02 19:17:00 UTC",
    "updated_date": "2024-06-02 19:17:00 UTC"
  },
  {
    "arxiv_id": "2406.00833v2",
    "title": "Harvard Undergraduate Survey on Generative AI",
    "authors": [
      "Shikoh Hirabayashi",
      "Rishab Jain",
      "Nikola Jurković",
      "Gabriel Wu"
    ],
    "abstract": "How has generative AI impacted the experiences of college students? We study\nthe influence of AI on the study habits, class choices, and career prospects of\nHarvard undergraduates (n=326), finding that almost 90% of students use\ngenerative AI. For roughly 25% of these students, AI has begun to substitute\nfor attending office hours and completing required readings. Half of students\nare concerned that AI will negatively impact their job prospects, and over half\nof students wish that Harvard had more classes on the future impacts of AI. We\nalso investigate students' outlook on the broader social implications of AI,\nfinding that half of students are worried that AI will increase economic\ninequality, and 40% believe that extinction risk from AI should be treated as a\nglobal priority with the same urgency as pandemics and nuclear war. Around half\nof students who have taken a class on AI expect AI to exceed human capabilities\non almost all tasks within 30 years. We make some recommendations to the\nHarvard community in light of these results.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00833v2",
    "published_date": "2024-06-02 18:47:08 UTC",
    "updated_date": "2024-08-08 02:55:34 UTC"
  },
  {
    "arxiv_id": "2406.02606v2",
    "title": "Know Your Neighborhood: General and Zero-Shot Capable Binary Function Search Powered by Call Graphlets",
    "authors": [
      "Joshua Collyer",
      "Tim Watson",
      "Iain Phillips"
    ],
    "abstract": "Binary code similarity detection is an important problem with applications in\nareas such as malware analysis, vulnerability research and license violation\ndetection. This paper proposes a novel graph neural network architecture\ncombined with a novel graph data representation called call graphlets. A call\ngraphlet encodes the neighborhood around each function in a binary executable,\ncapturing the local and global context through a series of statistical\nfeatures. A specialized graph neural network model operates on this graph\nrepresentation, learning to map it to a feature vector that encodes semantic\nbinary code similarities using deep-metric learning. The proposed approach is\nevaluated across five distinct datasets covering different architectures,\ncompiler tool chains, and optimization levels. Experimental results show that\nthe combination of call graphlets and the novel graph neural network\narchitecture achieves comparable or state-of-the-art performance compared to\nbaseline techniques across cross-architecture, mono-architecture and zero shot\ntasks. In addition, our proposed approach also performs well when evaluated\nagainst an out-of-domain function inlining task. The work provides a general\nand effective graph neural network-based solution for conducting binary code\nsimilarity detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, Under-Review",
    "pdf_url": "http://arxiv.org/pdf/2406.02606v2",
    "published_date": "2024-06-02 18:26:50 UTC",
    "updated_date": "2024-11-11 21:40:16 UTC"
  },
  {
    "arxiv_id": "2406.16903v1",
    "title": "Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction",
    "authors": [
      "Changyu Du",
      "Stavros Nousias",
      "André Borrmann"
    ],
    "abstract": "Facing increasingly complex BIM authoring software and the accompanying\nexpensive learning costs, designers often seek to interact with the software in\na more intelligent and lightweight manner. They aim to automate modeling\nworkflows, avoiding obstacles and difficulties caused by software usage,\nthereby focusing on the design process itself. To address this issue, we\nproposed an LLM-based autonomous agent framework that can function as a copilot\nin the BIM authoring tool, answering software usage questions, understanding\nthe user's design intentions from natural language, and autonomously executing\nmodeling tasks by invoking the appropriate tools. In a case study based on the\nBIM authoring software Vectorworks, we implemented a software prototype to\nintegrate the proposed framework seamlessly into the BIM authoring scenario. We\nevaluated the planning and reasoning capabilities of different LLMs within this\nframework when faced with complex instructions. Our work demonstrates the\nsignificant potential of LLM-based agents in design automation and intelligent\ninteraction.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16903v1",
    "published_date": "2024-06-02 17:47:57 UTC",
    "updated_date": "2024-06-02 17:47:57 UTC"
  },
  {
    "arxiv_id": "2406.00814v1",
    "title": "Expected Possession Value of Control and Duel Actions for Soccer Player's Skills Estimation",
    "authors": [
      "Andrei Shelopugin"
    ],
    "abstract": "Estimation of football players' skills is one of the key tasks in sports\nanalytics. This paper introduces multiple extensions to a widely used model,\nexpected possession value (EPV), to address some key challenges such as\nselection problem. First, we assign greater weights to events occurring\nimmediately prior to the shot rather than those preceding them (decay effect).\nSecond, our model incorporates possession risk more accurately by considering\nthe decay effect and effective playing time. Third, we integrate the assessment\nof individual player ability to win aerial and ground duels. Using the extended\nEPV model, we predict this metric for various football players for the upcoming\nseason, particularly taking into account the strength of their opponents.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00814v1",
    "published_date": "2024-06-02 17:29:42 UTC",
    "updated_date": "2024-06-02 17:29:42 UTC"
  },
  {
    "arxiv_id": "2407.06162v2",
    "title": "RNNs, CNNs and Transformers in Human Action Recognition: A Survey and a Hybrid Model",
    "authors": [
      "Khaled Alomar",
      "Halil Ibrahim Aysel",
      "Xiaohao Cai"
    ],
    "abstract": "Human Action Recognition (HAR) encompasses the task of monitoring human\nactivities across various domains, including but not limited to medical,\neducational, entertainment, visual surveillance, video retrieval, and the\nidentification of anomalous activities. Over the past decade, the field of HAR\nhas witnessed substantial progress by leveraging Convolutional Neural Networks\n(CNNs) to effectively extract and comprehend intricate information, thereby\nenhancing the overall performance of HAR systems. Recently, the domain of\ncomputer vision has witnessed the emergence of Vision Transformers (ViTs) as a\npotent solution. The efficacy of transformer architecture has been validated\nbeyond the confines of image analysis, extending their applicability to diverse\nvideo-related tasks. Notably, within this landscape, the research community has\nshown keen interest in HAR, acknowledging its manifold utility and widespread\nadoption across various domains. This article aims to present an encompassing\nsurvey that focuses on CNNs and the evolution of Recurrent Neural Networks\n(RNNs) to ViTs given their importance in the domain of HAR. By conducting a\nthorough examination of existing literature and exploring emerging trends, this\nstudy undertakes a critical analysis and synthesis of the accumulated knowledge\nin this field. Additionally, it investigates the ongoing efforts to develop\nhybrid approaches. Following this direction, this article presents a novel\nhybrid model that seeks to integrate the inherent strengths of CNNs and ViTs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.06162v2",
    "published_date": "2024-06-02 17:09:59 UTC",
    "updated_date": "2024-08-15 08:59:38 UTC"
  },
  {
    "arxiv_id": "2406.00800v2",
    "title": "MagR: Weight Magnitude Reduction for Enhancing Post-Training Quantization",
    "authors": [
      "Aozhong Zhang",
      "Naigang Wang",
      "Yanxia Deng",
      "Xin Li",
      "Zi Yang",
      "Penghang Yin"
    ],
    "abstract": "In this paper, we present a simple optimization-based preprocessing technique\ncalled Weight Magnitude Reduction (MagR) to improve the performance of\npost-training quantization. For each linear layer, we adjust the pre-trained\nfloating-point weights by solving an $\\ell_\\infty$-regularized optimization\nproblem. This process greatly diminishes the maximum magnitude of the weights\nand smooths out outliers, while preserving the layer's output. The preprocessed\nweights are centered more towards zero, which facilitates the subsequent\nquantization process. To implement MagR, we address the\n$\\ell_\\infty$-regularization by employing an efficient proximal gradient\ndescent algorithm. Unlike existing preprocessing methods that involve linear\ntransformations and subsequent post-processing steps, which can introduce\nsignificant overhead at inference time, MagR functions as a non-linear\ntransformation, eliminating the need for any additional post-processing. This\nensures that MagR introduces no overhead whatsoever during inference. Our\nexperiments demonstrate that MagR achieves state-of-the-art performance on the\nLlama family of models. For example, we achieve a Wikitext2 perplexity of 5.95\non the LLaMA2-70B model for per-channel INT2 weight quantization without\nincurring any inference overhead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00800v2",
    "published_date": "2024-06-02 17:00:02 UTC",
    "updated_date": "2024-10-17 03:51:08 UTC"
  },
  {
    "arxiv_id": "2406.00798v1",
    "title": "PruNeRF: Segment-Centric Dataset Pruning via 3D Spatial Consistency",
    "authors": [
      "Yeonsung Jung",
      "Heecheol Yun",
      "Joonhyung Park",
      "Jin-Hwa Kim",
      "Eunho Yang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have shown remarkable performance in learning\n3D scenes. However, NeRF exhibits vulnerability when confronted with\ndistractors in the training images -- unexpected objects are present only\nwithin specific views, such as moving entities like pedestrians or birds.\nExcluding distractors during dataset construction is a straightforward\nsolution, but without prior knowledge of their types and quantities, it becomes\nprohibitively expensive. In this paper, we propose PruNeRF, a segment-centric\ndataset pruning framework via 3D spatial consistency, that effectively\nidentifies and prunes the distractors. We first examine existing metrics for\nmeasuring pixel-wise distraction and introduce Influence Functions for more\naccurate measurements. Then, we assess 3D spatial consistency using a\ndepth-based reprojection technique to obtain 3D-aware distraction. Furthermore,\nwe incorporate segmentation for pixel-to-segment refinement, enabling more\nprecise identification. Our experiments on benchmark datasets demonstrate that\nPruNeRF consistently outperforms state-of-the-art methods in robustness against\ndistractors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00798v1",
    "published_date": "2024-06-02 16:49:05 UTC",
    "updated_date": "2024-06-02 16:49:05 UTC"
  },
  {
    "arxiv_id": "2406.01637v2",
    "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
    "authors": [
      "Yuxuan Zhu",
      "Antony Kellermann",
      "Akul Gupta",
      "Philip Li",
      "Richard Fang",
      "Rohan Bindu",
      "Daniel Kang"
    ],
    "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of\ncybersecurity. Researchers have shown that LLM agents can exploit real-world\nvulnerabilities when given a description of the vulnerability and toy\ncapture-the-flag problems. However, these agents still perform poorly on\nreal-world vulnerabilities that are unknown to the agent ahead of time\n(zero-day vulnerabilities).\n  In this work, we show that teams of LLM agents can exploit real-world,\nzero-day vulnerabilities. Prior agents struggle with exploring many different\nvulnerabilities and long-range planning when used alone. To resolve this, we\nintroduce HPTSA, a system of agents with a planning agent that can launch\nsubagents. The planning agent explores the system and determines which\nsubagents to call, resolving long-term planning issues when trying different\nvulnerabilities. We construct a benchmark of 14 real-world vulnerabilities and\nshow that our team of agents improve over prior agent frameworks by up to 4.3X.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.7; D.4.6"
    ],
    "primary_category": "cs.MA",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.01637v2",
    "published_date": "2024-06-02 16:25:26 UTC",
    "updated_date": "2025-03-30 00:26:48 UTC"
  },
  {
    "arxiv_id": "2406.03503v1",
    "title": "Position: Rethinking Post-Hoc Search-Based Neural Approaches for Solving Large-Scale Traveling Salesman Problems",
    "authors": [
      "Yifan Xia",
      "Xianliang Yang",
      "Zichuan Liu",
      "Zhihao Liu",
      "Lei Song",
      "Jiang Bian"
    ],
    "abstract": "Recent advancements in solving large-scale traveling salesman problems (TSP)\nutilize the heatmap-guided Monte Carlo tree search (MCTS) paradigm, where\nmachine learning (ML) models generate heatmaps, indicating the probability\ndistribution of each edge being part of the optimal solution, to guide MCTS in\nsolution finding. However, our theoretical and experimental analysis raises\ndoubts about the effectiveness of ML-based heatmap generation. In support of\nthis, we demonstrate that a simple baseline method can outperform complex ML\napproaches in heatmap generation. Furthermore, we question the practical value\nof the heatmap-guided MCTS paradigm. To substantiate this, our findings show\nits inferiority to the LKH-3 heuristic despite the paradigm's reliance on\nproblem-specific, hand-crafted strategies. For the future, we suggest research\ndirections focused on developing more theoretically sound heatmap generation\nmethods and exploring autonomous, generalizable ML approaches for combinatorial\nproblems. The code is available for review:\nhttps://github.com/xyfffff/rethink_mcts_for_tsp.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by International Conference on Machine Learning (ICML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2406.03503v1",
    "published_date": "2024-06-02 16:11:38 UTC",
    "updated_date": "2024-06-02 16:11:38 UTC"
  },
  {
    "arxiv_id": "2406.01636v1",
    "title": "COVID-19: post infection implications in different age groups, mechanism, diagnosis, effective prevention, treatment, and recommendations",
    "authors": [
      "Muhammad Akmal Raheem",
      "Muhammad Ajwad Rahim",
      "Ijaz Gul",
      "Md. Reyad-ul-Ferdous",
      "Liyan Le",
      "Junguo Hui",
      "Shuiwei Xia",
      "Minjiang Chen",
      "Dongmei Yu",
      "Vijay Pandey",
      "Peiwu Qin",
      "Jiansong Ji"
    ],
    "abstract": "SARS-CoV-2, the highly contagious pathogen responsible for the COVID-19\npandemic, has persistent effects that begin four weeks after initial infection\nand last for an undetermined duration. These chronic effects are more harmful\nthan acute ones. This review explores the long-term impact of the virus on\nvarious human organs, including the pulmonary, cardiovascular, neurological,\nreproductive, gastrointestinal, musculoskeletal, endocrine, and lymphoid\nsystems, particularly in older adults. Regarding diagnosis, RT-PCR is the gold\nstandard for detecting COVID-19, though it requires specialized equipment,\nskilled personnel, and considerable time to produce results. To address these\nlimitations, artificial intelligence in imaging and microfluidics technologies\noffers promising alternatives for diagnosing COVID-19 efficiently.\nPharmacological and non-pharmacological strategies are effective in mitigating\nthe persistent impacts of COVID-19. These strategies enhance immunity in\npost-COVID-19 patients by reducing cytokine release syndrome, improving T cell\nresponse, and increasing the circulation of activated natural killer and CD8 T\ncells in blood and tissues. This, in turn, alleviates symptoms such as fever,\nnausea, fatigue, muscle weakness, and pain. Vaccines, including inactivated\nviral, live attenuated viral, protein subunit, viral vectored, mRNA, DNA, and\nnanoparticle vaccines, significantly reduce the adverse long-term effects of\nthe virus. However, no vaccine has been reported to provide lifetime protection\nagainst COVID-19. Consequently, protective measures such as physical\ndistancing, mask usage, and hand hygiene remain essential strategies. This\nreview offers a comprehensive understanding of the persistent effects of\nCOVID-19 on individuals of varying ages, along with insights into diagnosis,\ntreatment, vaccination, and future preventative measures against the spread of\nSARS-CoV-2.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.01636v1",
    "published_date": "2024-06-02 16:07:49 UTC",
    "updated_date": "2024-06-02 16:07:49 UTC"
  },
  {
    "arxiv_id": "2406.00778v3",
    "title": "Bayesian Joint Additive Factor Models for Multiview Learning",
    "authors": [
      "Niccolo Anceschi",
      "Federico Ferrari",
      "David B. Dunson",
      "Himel Mallick"
    ],
    "abstract": "It is increasingly common in a wide variety of applied settings to collect\ndata of multiple different types on the same set of samples. Our particular\nfocus in this article is on studying relationships between such multiview\nfeatures and responses. A motivating application arises in the context of\nprecision medicine where multi-omics data are collected to correlate with\nclinical outcomes. It is of interest to infer dependence within and across\nviews while combining multimodal information to improve the prediction of\noutcomes. The signal-to-noise ratio can vary substantially across views,\nmotivating more nuanced statistical tools beyond standard late and early\nfusion. This challenge comes with the need to preserve interpretability, select\nfeatures, and obtain accurate uncertainty quantification. We propose a joint\nadditive factor regression model (JAFAR) with a structured additive design,\naccounting for shared and view-specific components. We ensure identifiability\nvia a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide\nan efficient implementation via a partially collapsed Gibbs sampler and extend\nour approach to allow flexible feature and outcome distributions. Prediction of\ntime-to-labor onset from immunome, metabolome, and proteome data illustrates\nperformance gains against state-of-the-art competitors. Our open-source\nsoftware (R package) is available at https://github.com/niccoloanceschi/jafar.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.CO",
      "stat.ME",
      "62F15"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00778v3",
    "published_date": "2024-06-02 15:35:45 UTC",
    "updated_date": "2025-01-10 05:35:58 UTC"
  },
  {
    "arxiv_id": "2406.00777v2",
    "title": "Diffusion Features to Bridge Domain Gap for Semantic Segmentation",
    "authors": [
      "Yuxiang Ji",
      "Boyong He",
      "Chenyuan Qu",
      "Zhuoyue Tan",
      "Chuan Qin",
      "Liaoni Wu"
    ],
    "abstract": "Pre-trained diffusion models have demonstrated remarkable proficiency in\nsynthesizing images across a wide range of scenarios with customizable prompts,\nindicating their effective capacity to capture universal features. Motivated by\nthis, our study delves into the utilization of the implicit knowledge embedded\nwithin diffusion models to address challenges in cross-domain semantic\nsegmentation. This paper investigates the approach that leverages the sampling\nand fusion techniques to harness the features of diffusion models efficiently.\nWe propose DIffusion Feature Fusion (DIFF) as a backbone use for extracting and\nintegrating effective semantic representations through the diffusion process.\nBy leveraging the strength of text-to-image generation capability, we introduce\na new training framework designed to implicitly learn posterior knowledge from\nit. Through rigorous evaluation in the contexts of domain generalization\nsemantic segmentation, we establish that our methodology surpasses preceding\napproaches in mitigating discrepancies across distinct domains and attains the\nstate-of-the-art (SOTA) benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The code is released at https://github.com/Yux1angJi/DIFF",
    "pdf_url": "http://arxiv.org/pdf/2406.00777v2",
    "published_date": "2024-06-02 15:33:46 UTC",
    "updated_date": "2024-11-21 09:10:23 UTC"
  },
  {
    "arxiv_id": "2406.00770v1",
    "title": "Automatic Instruction Evolving for Large Language Models",
    "authors": [
      "Weihao Zeng",
      "Can Xu",
      "Yingxiu Zhao",
      "Jian-Guang Lou",
      "Weizhu Chen"
    ],
    "abstract": "Fine-tuning large pre-trained language models with Evol-Instruct has achieved\nencouraging results across a wide range of tasks. However, designing effective\nevolving methods for instruction evolution requires substantial human\nexpertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that\nevolves instruction datasets using large language models without any human\neffort. The framework automatically analyzes and summarizes suitable\nevolutionary strategies for the given instruction data and iteratively improves\nthe evolving method based on issues exposed during the instruction evolution\nprocess. Our extensive experiments demonstrate that the best method optimized\nby Auto Evol-Instruct outperforms human-designed methods on various benchmarks,\nincluding MT-Bench, AlpacaEval, GSM8K, and HumanEval.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00770v1",
    "published_date": "2024-06-02 15:09:00 UTC",
    "updated_date": "2024-06-02 15:09:00 UTC"
  },
  {
    "arxiv_id": "2406.00765v1",
    "title": "The Embodied World Model Based on LLM with Visual Information and Prediction-Oriented Prompts",
    "authors": [
      "Wakana Haijima",
      "Kou Nakakubo",
      "Masahiro Suzuki",
      "Yutaka Matsuo"
    ],
    "abstract": "In recent years, as machine learning, particularly for vision and language\nunderstanding, has been improved, research in embedded AI has also evolved.\nVOYAGER is a well-known LLM-based embodied AI that enables autonomous\nexploration in the Minecraft world, but it has issues such as underutilization\nof visual data and insufficient functionality as a world model. In this\nresearch, the possibility of utilizing visual data and the function of LLM as a\nworld model were investigated with the aim of improving the performance of\nembodied AI. The experimental results revealed that LLM can extract necessary\ninformation from visual data, and the utilization of the information improves\nits performance as a world model. It was also suggested that devised prompts\ncould bring out the LLM's function as a world model.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00765v1",
    "published_date": "2024-06-02 14:50:01 UTC",
    "updated_date": "2024-06-02 14:50:01 UTC"
  },
  {
    "arxiv_id": "2406.06561v1",
    "title": "Brainstorming Brings Power to Large Language Models of Knowledge Reasoning",
    "authors": [
      "Zining Qin",
      "Chenhao Wang",
      "Huiling Qin",
      "Weijia Jia"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated amazing capabilities in\nlanguage generation, text comprehension, and knowledge reasoning. While a\nsingle powerful model can already handle multiple tasks, relying on a single\nperspective can lead to biased and unstable results. Recent studies have\nfurther improved the model's reasoning ability on a wide range of tasks by\nintroducing multi-model collaboration. However, models with different\ncapabilities may produce conflicting answers on the same problem, and how to\nreasonably obtain the correct answer from multiple candidate models has become\na challenging problem. In this paper, we propose the multi-model brainstorming\nbased on prompt. It incorporates different models into a group for\nbrainstorming, and after multiple rounds of reasoning elaboration and\nre-inference, a consensus answer is reached within the group. We conducted\nexperiments on three different types of datasets, and demonstrate that the\nbrainstorming can significantly improve the effectiveness in logical reasoning\nand fact extraction. Furthermore, we find that two small-parameter models can\nachieve accuracy approximating that of larger-parameter models through\nbrainstorming, which provides a new solution for distributed deployment of\nLLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06561v1",
    "published_date": "2024-06-02 14:47:14 UTC",
    "updated_date": "2024-06-02 14:47:14 UTC"
  },
  {
    "arxiv_id": "2406.00761v1",
    "title": "Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement Learning",
    "authors": [
      "Po-Shao Lin",
      "Jia-Fong Yeh",
      "Yi-Ting Chen",
      "Winston H. Hsu"
    ],
    "abstract": "We observe that current state-of-the-art (SOTA) methods suffer from the\nperformance imbalance issue when performing multi-task reinforcement learning\n(MTRL) tasks. While these methods may achieve impressive performance on\naverage, they perform extremely poorly on a few tasks. To address this, we\npropose a new and effective method called STARS, which consists of two novel\nstrategies: a shared-unique feature extractor and task-aware prioritized\nsampling. First, the shared-unique feature extractor learns both shared and\ntask-specific features to enable better synergy of knowledge between different\ntasks. Second, the task-aware sampling strategy is combined with the\nprioritized experience replay for efficient learning on tasks with poor\nperformance. The effectiveness and stability of our STARS are verified through\nexperiments on the mainstream Meta-World benchmark. From the results, our STARS\nstatistically outperforms current SOTA methods and alleviates the performance\nimbalance issue. Besides, we visualize the learned features to support our\nclaims and enhance the interpretability of STARS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "The first two authors contribute equally",
    "pdf_url": "http://arxiv.org/pdf/2406.00761v1",
    "published_date": "2024-06-02 14:33:49 UTC",
    "updated_date": "2024-06-02 14:33:49 UTC"
  },
  {
    "arxiv_id": "2406.00755v1",
    "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
    "authors": [
      "Xiaoyuan Li",
      "Wenjie Wang",
      "Moxin Li",
      "Junrong Guo",
      "Yang Zhang",
      "Fuli Feng"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of\nmathematical reasoning necessitates comprehensive evaluations to gauge progress\nand inspire future directions. Existing assessments predominantly focus on\nproblem-solving from the examinee perspective, overlooking a dual perspective\nof examiner regarding error identification and correction. From the examiner\nperspective, we define four evaluation tasks for error identification and\ncorrection along with a new dataset with annotated error types and steps. We\nalso design diverse prompts to thoroughly evaluate eleven representative LLMs.\nOur principal findings indicate that GPT-4 outperforms all models, while\nopen-source model LLaMA-2-7B demonstrates comparable abilities to closed-source\nmodels GPT-3.5 and Gemini Pro. Notably, calculation error proves the most\nchallenging error type. Moreover, prompting LLMs with the error types can\nimprove the average correction accuracy by 47.9\\%. These results reveal\npotential directions for developing the mathematical reasoning abilities of\nLLMs. Our code and dataset is available on https://github.com/LittleCirc1e/EIC.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ACL Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00755v1",
    "published_date": "2024-06-02 14:16:24 UTC",
    "updated_date": "2024-06-02 14:16:24 UTC"
  },
  {
    "arxiv_id": "2406.00748v1",
    "title": "Augmenting the FedProx Algorithm by Minimizing Convergence",
    "authors": [
      "Anomitra Sarkar",
      "Lavanya Vajpayee"
    ],
    "abstract": "The Internet of Things has experienced significant growth and has become an\nintegral part of various industries. This expansion has given rise to the\nIndustrial IoT initiative where industries are utilizing IoT technology to\nenhance communication and connectivity through innovative solutions such as\ndata analytics and cloud computing. However this widespread adoption of IoT is\ndemanding of algorithms that provide better efficiency for the same training\nenvironment without speed being a factor. In this paper we present a novel\napproach called G Federated Proximity. Building upon the existing FedProx\ntechnique our implementation introduces slight modifications to enhance its\nefficiency and effectiveness. By leveraging FTL our proposed system aims to\nimprove the accuracy of model obtained after the training dataset with the help\nof normalization techniques such that it performs better on real time devices\nand heterogeneous networks Our results indicate a significant increase in the\nthroughput of approximately 90% better convergence compared to existing model\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00748v1",
    "published_date": "2024-06-02 14:01:55 UTC",
    "updated_date": "2024-06-02 14:01:55 UTC"
  },
  {
    "arxiv_id": "2406.00741v1",
    "title": "Learning to Play 7 Wonders Duel Without Human Supervision",
    "authors": [
      "Giovanni Paolini",
      "Lorenzo Moreschini",
      "Francesco Veneziano",
      "Alessandro Iraci"
    ],
    "abstract": "This paper introduces ZeusAI, an artificial intelligence system developed to\nplay the board game 7 Wonders Duel. Inspired by the AlphaZero reinforcement\nlearning algorithm, ZeusAI relies on a combination of Monte Carlo Tree Search\nand a Transformer Neural Network to learn the game without human supervision.\nZeusAI competes at the level of top human players, develops both known and\nnovel strategies, and allows us to test rule variants to improve the game's\nbalance. This work demonstrates how AI can help in understanding and enhancing\nboard games.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00741v1",
    "published_date": "2024-06-02 13:28:57 UTC",
    "updated_date": "2024-06-02 13:28:57 UTC"
  },
  {
    "arxiv_id": "2406.18568v2",
    "title": "A Diagnostic Model for Acute Lymphoblastic Leukemia Using Metaheuristics and Deep Learning Methods",
    "authors": [
      "Amir Masoud Rahmani",
      "Parisa Khoshvaght",
      "Hamid Alinejad-Rokny",
      "Samira Sadeghi",
      "Parvaneh Asghari",
      "Zohre Arabi",
      "Mehdi Hosseinzadeh"
    ],
    "abstract": "Acute lymphoblastic leukemia (ALL) severity is determined by the presence and\nratios of blast cells (abnormal white blood cells) in both bone marrow and\nperipheral blood. Manual diagnosis of this disease is a tedious and\ntime-consuming operation, making it difficult for professionals to accurately\nexamine blast cell characteristics. To address this difficulty, researchers use\ndeep learning and machine learning. In this paper, a ResNet-based feature\nextractor is utilized to detect ALL, along with a variety of feature selectors\nand classifiers. To get the best results, a variety of transfer learning\nmodels, including the Resnet, VGG, EfficientNet, and DensNet families, are used\nas deep feature extractors. Following extraction, different feature selectors\nare used, including Genetic algorithm, PCA, ANOVA, Random Forest, Univariate,\nMutual information, Lasso, XGB, Variance, and Binary ant colony. After feature\nqualification, a variety of classifiers are used, with MLP outperforming the\nothers. The recommended technique is used to categorize ALL and HEM in the\nselected dataset which is C-NMC 2019. This technique got an impressive 90.71%\naccuracy and 95.76% sensitivity for the relevant classifications, and its\nmetrics on this dataset outperformed others.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.18568v2",
    "published_date": "2024-06-02 13:25:44 UTC",
    "updated_date": "2024-08-12 06:11:33 UTC"
  },
  {
    "arxiv_id": "2406.00738v2",
    "title": "Global Rewards in Restless Multi-Armed Bandits",
    "authors": [
      "Naveen Raman",
      "Zheyuan Ryan Shi",
      "Fei Fang"
    ],
    "abstract": "Restless multi-armed bandits (RMAB) extend multi-armed bandits so pulling an\narm impacts future states. Despite the success of RMABs, a key limiting\nassumption is the separability of rewards into a sum across arms. We address\nthis deficiency by proposing restless-multi-armed bandit with global rewards\n(RMAB-G), a generalization of RMABs to global non-separable rewards. To solve\nRMAB-G, we develop the Linear- and Shapley-Whittle indices, which extend\nWhittle indices from RMABs to RMAB-Gs. We prove approximation bounds but also\npoint out how these indices could fail when reward functions are highly\nnon-linear. To overcome this, we propose two sets of adaptive policies: the\nfirst computes indices iteratively, and the second combines indices with\nMonte-Carlo Tree Search (MCTS). Empirically, we demonstrate that our proposed\npolicies outperform baselines and index-based policies with synthetic data and\nreal-world data from food rescue.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages",
    "pdf_url": "http://arxiv.org/pdf/2406.00738v2",
    "published_date": "2024-06-02 13:13:46 UTC",
    "updated_date": "2024-06-07 20:38:51 UTC"
  },
  {
    "arxiv_id": "2406.00735v1",
    "title": "Full-Atom Peptide Design based on Multi-modal Flow Matching",
    "authors": [
      "Jiahan Li",
      "Chaoran Cheng",
      "Zuofan Wu",
      "Ruihan Guo",
      "Shitong Luo",
      "Zhizhou Ren",
      "Jian Peng",
      "Jianzhu Ma"
    ],
    "abstract": "Peptides, short chains of amino acid residues, play a vital role in numerous\nbiological processes by interacting with other target molecules, offering\nsubstantial potential in drug discovery. In this work, we present PepFlow, the\nfirst multi-modal deep generative model grounded in the flow-matching framework\nfor the design of full-atom peptides that target specific protein receptors.\nDrawing inspiration from the crucial roles of residue backbone orientations and\nside-chain dynamics in protein-peptide interactions, we characterize the\npeptide structure using rigid backbone frames within the $\\mathrm{SE}(3)$\nmanifold and side-chain angles on high-dimensional tori. Furthermore, we\nrepresent discrete residue types in the peptide sequence as categorical\ndistributions on the probability simplex. By learning the joint distributions\nof each modality using derived flows and vector fields on corresponding\nmanifolds, our method excels in the fine-grained design of full-atom peptides.\nHarnessing the multi-modal paradigm, our approach adeptly tackles various tasks\nsuch as fix-backbone sequence design and side-chain packing through partial\nsampling. Through meticulously crafted experiments, we demonstrate that PepFlow\nexhibits superior performance in comprehensive benchmarks, highlighting its\nsignificant potential in computational peptide design and analysis.",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00735v1",
    "published_date": "2024-06-02 12:59:54 UTC",
    "updated_date": "2024-06-02 12:59:54 UTC"
  },
  {
    "arxiv_id": "2406.02605v1",
    "title": "A Novel Defense Against Poisoning Attacks on Federated Learning: LayerCAM Augmented with Autoencoder",
    "authors": [
      "Jingjing Zheng",
      "Xin Yuan",
      "Kai Li",
      "Wei Ni",
      "Eduardo Tovar",
      "Jon Crowcroft"
    ],
    "abstract": "Recent attacks on federated learning (FL) can introduce malicious model\nupdates that circumvent widely adopted Euclidean distance-based detection\nmethods. This paper proposes a novel defense strategy, referred to as\nLayerCAM-AE, designed to counteract model poisoning in federated learning. The\nLayerCAM-AE puts forth a new Layer Class Activation Mapping (LayerCAM)\nintegrated with an autoencoder (AE), significantly enhancing detection\ncapabilities. Specifically, LayerCAM-AE generates a heat map for each local\nmodel update, which is then transformed into a more compact visual format. The\nautoencoder is designed to process the LayerCAM heat maps from the local model\nupdates, improving their distinctiveness and thereby increasing the accuracy in\nspotting anomalous maps and malicious local models. To address the risk of\nmisclassifications with LayerCAM-AE, a voting algorithm is developed, where a\nlocal model update is flagged as malicious if its heat maps are consistently\nsuspicious over several rounds of communication. Extensive tests of LayerCAM-AE\non the SVHN and CIFAR-100 datasets are performed under both Independent and\nIdentically Distributed (IID) and non-IID settings in comparison with existing\nResNet-50 and REGNETY-800MF defense models. Experimental results show that\nLayerCAM-AE increases detection rates (Recall: 1.0, Precision: 1.0, FPR: 0.0,\nAccuracy: 1.0, F1 score: 1.0, AUC: 1.0) and test accuracy in FL, surpassing the\nperformance of both the ResNet-50 and REGNETY-800MF. Our code is available at:\nhttps://github.com/jjzgeeks/LayerCAM-AE",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02605v1",
    "published_date": "2024-06-02 12:37:12 UTC",
    "updated_date": "2024-06-02 12:37:12 UTC"
  },
  {
    "arxiv_id": "2406.06560v2",
    "title": "Inverse Constitutional AI: Compressing Preferences into Principles",
    "authors": [
      "Arduin Findeis",
      "Timo Kaufmann",
      "Eyke Hüllermeier",
      "Samuel Albanie",
      "Robert Mullins"
    ],
    "abstract": "Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025, v2 is camera-ready version; Main changes from\n  v1: extended experiments, additional baselines",
    "pdf_url": "http://arxiv.org/pdf/2406.06560v2",
    "published_date": "2024-06-02 11:54:50 UTC",
    "updated_date": "2025-04-21 15:37:15 UTC"
  },
  {
    "arxiv_id": "2406.00704v2",
    "title": "An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites",
    "authors": [
      "Ylva Grønningsæter",
      "Halvor S. Smørvik",
      "Ole-Christoffer Granmo"
    ],
    "abstract": "The Tsetlin Machine (TM) has achieved competitive results on several image\nclassification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2.\nHowever, color image classification is arguably still in its infancy for TMs,\nwith CIFAR-10 being a focal point for tracking progress. Over the past few\nyears, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in\n2023 with the introduction of Drop Clause. In this paper, we leverage the\nrecently proposed TM Composites architecture and introduce a range of TM\nSpecialists that use various image processing techniques. These include Canny\nedge detection, Histogram of Oriented Gradients, adaptive mean thresholding,\nadaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and\nadaptive color thermometers. In addition, we conduct a rigorous hyperparameter\nsearch, where we uncover optimal hyperparameters for several of the TM\nSpecialists. The result is a toolbox that provides new state-of-the-art results\non CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM\nSpecialists forms a foundation for new TM applications and a landmark for\nfurther research on TM Composites in image analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.00704v2",
    "published_date": "2024-06-02 10:52:48 UTC",
    "updated_date": "2025-02-05 18:23:58 UTC"
  },
  {
    "arxiv_id": "2406.00702v4",
    "title": "Enhanced Heart Sound Classification Using Mel Frequency Cepstral Coefficients and Comparative Analysis of Single vs. Ensemble Classifier Strategies",
    "authors": [
      "Amir Masoud Rahmani",
      "Amir Haider",
      "Mohammad Adeli",
      "Olfa Mzoughi",
      "Entesar Gemeay",
      "Mokhtar Mohammadi",
      "Hamid Alinejad-Rokny",
      "Parisa Khoshvaght",
      "Mehdi Hosseinzadeh"
    ],
    "abstract": "This paper explores the efficacy of Mel Frequency Cepstral Coefficients\n(MFCCs) in detecting abnormal heart sounds using two classification strategies:\na single classifier and an ensemble classifier approach. Heart sounds were\nfirst pre-processed to remove noise and then segmented into S1, systole, S2,\nand diastole intervals, with thirteen MFCCs estimated from each segment,\nyielding 52 MFCCs per beat. Finally, MFCCs were used for heart sound\nclassification. For that purpose, in the single classifier strategy, the MFCCs\nfrom nine consecutive beats were averaged to classify heart sounds by a single\nclassifier (either a support vector machine (SVM), the k nearest neighbors\n(kNN), or a decision tree (DT)). Conversely, the ensemble classifier strategy\nemployed nine classifiers (either nine SVMs, nine kNN classifiers, or nine DTs)\nto individually assess beats as normal or abnormal, with the overall\nclassification based on the majority vote. Both methods were tested on a\npublicly available phonocardiogram database. The heart sound classification\naccuracy was 91.95% for the SVM, 91.9% for the kNN, and 87.33% for the DT in\nthe single classifier strategy. Also, the accuracy was 93.59% for the SVM,\n91.84% for the kNN, and 92.22% for the DT in the ensemble classifier strategy.\nOverall, the results demonstrated that the ensemble classifier strategy\nimproved the accuracies of the DT and the SVM by 4.89% and 1.64%, establishing\nMFCCs as more effective than other features, including time, time-frequency,\nand statistical features, evaluated in similar studies.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00702v4",
    "published_date": "2024-06-02 10:45:30 UTC",
    "updated_date": "2024-06-30 03:34:23 UTC"
  },
  {
    "arxiv_id": "2407.01576v1",
    "title": "The Future of Aerial Communications: A Survey of IRS-Enhanced UAV Communication Technologies",
    "authors": [
      "Zina Chkirbene",
      "Ala Gouissem",
      "Ridha Hamila",
      "Devrim Unal"
    ],
    "abstract": "The advent of Intelligent Reflecting Surfaces (IRS) and Unmanned Aerial\nVehicles (UAVs) is setting a new benchmark in the field of wireless\ncommunications. IRS, with their groundbreaking ability to manipulate\nelectromagnetic waves, have opened avenues for substantial enhancements in\nsignal quality, network efficiency, and spectral usage. These surfaces\ndynamically reconfigure the propagation environment, leading to optimized\nsignal paths and reduced interference. Concurrently, UAVs have emerged as\ndynamic, versatile elements within communication networks, offering high\nmobility and the ability to access and enhance coverage in areas where\ntraditional, fixed infrastructure falls short. This paper presents a\ncomprehensive survey on the synergistic integration of IRS and UAVs in wireless\nnetworks, highlighting how this innovative combination substantially boosts\nnetwork performance, particularly in terms of security, energy efficiency, and\nreliability. The versatility of UAVs, combined with the signal-manipulating\nprowess of IRS, creates a potent solution for overcoming the limitations of\nconventional communication setups, especially in challenging and underserved\nenvironments. Furthermore, the survey delves into the cutting-edge realm of\nMachine Learning (ML), exploring its role in the strategic deployment and\noperational optimization of UAVs equipped with IRS. The paper also underscores\nthe latest research and practical advancements in this field, providing\ninsights into real-world applications and experimental setups. It concludes by\ndiscussing the future prospects and potential directions for this emerging\ntechnology, positioning the IRS-UAV integration as a transformative force in\nthe landscape of next-generation wireless",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.01576v1",
    "published_date": "2024-06-02 09:58:53 UTC",
    "updated_date": "2024-06-02 09:58:53 UTC"
  },
  {
    "arxiv_id": "2406.00667v1",
    "title": "An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging",
    "authors": [
      "Sulaiman Khan",
      "Md. Rafiul Biswas",
      "Alina Murad",
      "Hazrat Ali",
      "Zubair Shah"
    ],
    "abstract": "Recent developments in multimodal large language models (MLLMs) have spurred\nsignificant interest in their potential applications across various medical\nimaging domains. On the one hand, there is a temptation to use these generative\nmodels to synthesize realistic-looking medical image data, while on the other\nhand, the ability to identify synthetic image data in a pool of data is also\nsignificantly important. In this study, we explore the potential of the Gemini\n(\\textit{gemini-1.0-pro-vision-latest}) and GPT-4V (gpt-4-vision-preview)\nmodels for medical image analysis using two modalities of medical image data.\nUtilizing synthetic and real imaging data, both Gemini AI and GPT-4V are first\nused to classify real versus synthetic images, followed by an interpretation\nand analysis of the input images. Experimental results demonstrate that both\nGemini and GPT-4 could perform some interpretation of the input images. In this\nspecific experiment, Gemini was able to perform slightly better than the GPT-4V\non the classification task. In contrast, responses associated with GPT-4V were\nmostly generic in nature. Our early investigation presented in this work\nprovides insights into the potential of MLLMs to assist with the classification\nand interpretation of retinal fundoscopy and lung X-ray images. We also\nidentify key limitations associated with the early investigation study on MLLMs\nfor specialized tasks in medical image analysis.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted in Fifth IEEE Workshop on Artificial Intelligence for\n  HealthCare, IEEE 25th International Conference on Information Reuse and\n  Integration for Data Science",
    "pdf_url": "http://arxiv.org/pdf/2406.00667v1",
    "published_date": "2024-06-02 08:29:23 UTC",
    "updated_date": "2024-06-02 08:29:23 UTC"
  },
  {
    "arxiv_id": "2406.00663v1",
    "title": "SimSAM: Zero-shot Medical Image Segmentation via Simulated Interaction",
    "authors": [
      "Benjamin Towle",
      "Xin Chen",
      "Ke Zhou"
    ],
    "abstract": "The recently released Segment Anything Model (SAM) has shown powerful\nzero-shot segmentation capabilities through a semi-automatic annotation setup\nin which the user can provide a prompt in the form of clicks or bounding boxes.\nThere is growing interest around applying this to medical imaging, where the\ncost of obtaining expert annotations is high, privacy restrictions may limit\nsharing of patient data, and model generalisation is often poor. However, there\nare large amounts of inherent uncertainty in medical images, due to unclear\nobject boundaries, low-contrast media, and differences in expert labelling\nstyle. Currently, SAM is known to struggle in a zero-shot setting to adequately\nannotate the contours of the structure of interest in medical images, where the\nuncertainty is often greatest, thus requiring significant manual correction. To\nmitigate this, we introduce \\textbf{Sim}ulated Interaction for \\textbf{S}egment\n\\textbf{A}nything \\textbf{M}odel (\\textsc{\\textbf{SimSAM}}), an approach that\nleverages simulated user interaction to generate an arbitrary number of\ncandidate masks, and uses a novel aggregation approach to output the most\ncompatible mask. Crucially, our method can be used during inference directly on\ntop of SAM, without any additional training requirement. Quantitatively, we\nevaluate our method across three publicly available medical imaging datasets,\nand find that our approach leads to up to a 15.5\\% improvement in contour\nsegmentation accuracy compared to zero-shot SAM. Our code is available at\n\\url{https://github.com/BenjaminTowle/SimSAM}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at ISBI 2024. Awarded Top 12 Oral Presentation",
    "pdf_url": "http://arxiv.org/pdf/2406.00663v1",
    "published_date": "2024-06-02 08:13:12 UTC",
    "updated_date": "2024-06-02 08:13:12 UTC"
  },
  {
    "arxiv_id": "2406.00661v1",
    "title": "Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift",
    "authors": [
      "Jiayun Wu",
      "Jiashuo Liu",
      "Peng Cui",
      "Zhiwei Steven Wu"
    ],
    "abstract": "We establish a new model-agnostic optimization framework for\nout-of-distribution generalization via multicalibration, a criterion that\nensures a predictor is calibrated across a family of overlapping groups.\nMulticalibration is shown to be associated with robustness of statistical\ninference under covariate shift. We further establish a link between\nmulticalibration and robustness for prediction tasks both under and beyond\ncovariate shift. We accomplish this by extending multicalibration to\nincorporate grouping functions that consider covariates and labels jointly.\nThis leads to an equivalence of the extended multicalibration and invariance,\nan objective for robust learning in existence of concept shift. We show a\nlinear structure of the grouping function class spanned by density ratios,\nresulting in a unifying framework for robust learning by designing specific\ngrouping functions. We propose MC-Pseudolabel, a post-processing algorithm to\nachieve both extended multicalibration and out-of-distribution generalization.\nThe algorithm, with lightweight hyperparameters and optimization through a\nseries of supervised regression steps, achieves superior performance on\nreal-world datasets with distribution shift.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00661v1",
    "published_date": "2024-06-02 08:11:35 UTC",
    "updated_date": "2024-06-02 08:11:35 UTC"
  },
  {
    "arxiv_id": "2406.00645v2",
    "title": "FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning",
    "authors": [
      "Yuwei Fu",
      "Haichao Zhang",
      "Di Wu",
      "Wei Xu",
      "Benoit Boulet"
    ],
    "abstract": "In this work, we investigate how to leverage pre-trained visual-language\nmodels (VLM) for online Reinforcement Learning (RL). In particular, we focus on\nsparse reward tasks with pre-defined textual task descriptions. We first\nidentify the problem of reward misalignment when applying VLM as a reward in RL\ntasks. To address this issue, we introduce a lightweight fine-tuning method,\nnamed Fuzzy VLM reward-aided RL (FuRL), based on reward alignment and relay RL.\nSpecifically, we enhance the performance of SAC/DrQ baseline agents on sparse\nreward tasks by fine-tuning VLM representations and using relay RL to avoid\nlocal minima. Extensive experiments on the Meta-world benchmark tasks\ndemonstrate the efficacy of the proposed method. Code is available at:\nhttps://github.com/fuyw/FuRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00645v2",
    "published_date": "2024-06-02 07:20:08 UTC",
    "updated_date": "2024-06-05 00:05:23 UTC"
  },
  {
    "arxiv_id": "2406.00637v1",
    "title": "Representing Animatable Avatar via Factorized Neural Fields",
    "authors": [
      "Chunjin Song",
      "Zhijie Wu",
      "Bastian Wandt",
      "Leonid Sigal",
      "Helge Rhodin"
    ],
    "abstract": "For reconstructing high-fidelity human 3D models from monocular videos, it is\ncrucial to maintain consistent large-scale body shapes along with finely\nmatched subtle wrinkles. This paper explores the observation that the per-frame\nrendering results can be factorized into a pose-independent component and a\ncorresponding pose-dependent equivalent to facilitate frame consistency. Pose\nadaptive textures can be further improved by restricting frequency bands of\nthese two components. In detail, pose-independent outputs are expected to be\nlow-frequency, while highfrequency information is linked to pose-dependent\nfactors. We achieve a coherent preservation of both coarse body contours across\nthe entire input video and finegrained texture features that are time variant\nwith a dual-branch network with distinct frequency components. The first branch\ntakes coordinates in canonical space as input, while the second branch\nadditionally considers features outputted by the first branch and pose\ninformation of each frame. Our network integrates the information predicted by\nboth branches and utilizes volume rendering to generate photo-realistic 3D\nhuman images. Through experiments, we demonstrate that our network surpasses\nthe neural radiance fields (NeRF) based state-of-the-art methods in preserving\nhigh-frequency details and ensuring consistent body contours.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00637v1",
    "published_date": "2024-06-02 06:45:38 UTC",
    "updated_date": "2024-06-02 06:45:38 UTC"
  },
  {
    "arxiv_id": "2406.02604v1",
    "title": "Gated recurrent neural network with TPE Bayesian optimization for enhancing stock index prediction accuracy",
    "authors": [
      "Bivas Dinda"
    ],
    "abstract": "The recent advancement of deep learning architectures, neural networks, and\nthe combination of abundant financial data and powerful computers are\ntransforming finance, leading us to develop an advanced method for predicting\nfuture stock prices. However, the accessibility of investment and trading at\neveryone's fingertips made the stock markets increasingly intricate and prone\nto volatility. The increased complexity and volatility of the stock market have\ndriven demand for more models, which would effectively capture high volatility\nand non-linear behavior of the different stock prices. This study explored\ngated recurrent neural network (GRNN) algorithms such as LSTM (long short-term\nmemory), GRU (gated recurrent unit), and hybrid models like GRU-LSTM, LSTM-GRU,\nwith Tree-structured Parzen Estimator (TPE) Bayesian optimization for\nhyperparameter optimization (TPE-GRNN). The aim is to improve the prediction\naccuracy of the next day's closing price of the NIFTY 50 index, a prominent\nIndian stock market index, using TPE-GRNN. A combination of eight influential\nfactors is carefully chosen from fundamental stock data, technical indicators,\ncrude oil price, and macroeconomic data to train the models for capturing the\nchanges in the price of the index with the factors of the broader economy.\nSingle-layer and multi-layer TPE-GRNN models have been developed. The models'\nperformance is evaluated using standard matrices like R2, MAPE, and RMSE. The\nanalysis of models' performance reveals the impact of feature selection and\nhyperparameter optimization (HPO) in enhancing stock index price prediction\naccuracy. The results show that the MAPE of our proposed TPE-LSTM method is the\nlowest (best) with respect to all the previous models for stock index price\nprediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "q-fin.CP",
      "J.1"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 9 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.02604v1",
    "published_date": "2024-06-02 06:39:01 UTC",
    "updated_date": "2024-06-02 06:39:01 UTC"
  },
  {
    "arxiv_id": "2406.00633v3",
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang Zhang",
      "Josh Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ],
    "abstract": "Diffusion models have become the de-facto approach for generating visual\ndata, which are trained to match the distribution of the training dataset. In\naddition, we also want to control generation to fulfill desired properties such\nas alignment to a text description, which can be specified with a black-box\nreward function. Prior works fine-tune pretrained diffusion models to achieve\nthis goal through reinforcement learning-based algorithms. Nonetheless, they\nsuffer from issues including slow credit assignment as well as low quality in\ntheir generated samples. In this work, we explore techniques that do not\ndirectly maximize the reward but rather generate high-reward images with\nrelatively high probability -- a natural scenario for the framework of\ngenerative flow networks (GFlowNets). To this end, we propose the Diffusion\nAlignment with GFlowNet (DAG) algorithm to post-train diffusion models with\nblack-box property functions. Extensive experiments on Stable Diffusion and\nvarious reward specifications corroborate that our method could effectively\nalign large-scale text-to-image diffusion models with given reward information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00633v3",
    "published_date": "2024-06-02 06:36:46 UTC",
    "updated_date": "2024-12-26 02:30:48 UTC"
  },
  {
    "arxiv_id": "2406.06559v1",
    "title": "Harnessing Business and Media Insights with Large Language Models",
    "authors": [
      "Yujia Bao",
      "Ankit Parag Shah",
      "Neeru Narang",
      "Jonathan Rivers",
      "Rajeev Maksey",
      "Lan Guan",
      "Louise N. Barrere",
      "Shelley Evenson",
      "Rahul Basole",
      "Connie Miao",
      "Ankit Mehta",
      "Fabien Boulay",
      "Su Min Park",
      "Natalie E. Pearson",
      "Eldhose Joy",
      "Tiger He",
      "Sumiran Thakur",
      "Koustav Ghosal",
      "Josh On",
      "Phoebe Morrison",
      "Tim Major",
      "Eva Siqi Wang",
      "Gina Escobar",
      "Jiaheng Wei",
      "Tharindu Cyril Weerasooriya",
      "Queena Song",
      "Daria Lashkevich",
      "Clare Chen",
      "Gyuhak Kim",
      "Dengpan Yin",
      "Don Hejna",
      "Mo Nomeli",
      "Wei Wei"
    ],
    "abstract": "This paper introduces Fortune Analytics Language Model (FALM). FALM empowers\nusers with direct access to comprehensive business analysis, including market\ntrends, company performance metrics, and expert insights. Unlike generic LLMs,\nFALM leverages a curated knowledge base built from professional journalism,\nenabling it to deliver precise and in-depth answers to intricate business\nquestions. Users can further leverage natural language queries to directly\nvisualize financial data, generating insightful charts and graphs to understand\ntrends across diverse business sectors clearly. FALM fosters user trust and\nensures output accuracy through three novel methods: 1) Time-aware reasoning\nguarantees accurate event registration and prioritizes recent updates. 2)\nThematic trend analysis explicitly examines topic evolution over time,\nproviding insights into emerging business landscapes. 3) Content referencing\nand task decomposition enhance answer fidelity and data visualization accuracy.\nWe conduct both automated and human evaluations, demonstrating FALM's\nsignificant performance improvements over baseline methods while prioritizing\nresponsible AI practices. These benchmarks establish FALM as a cutting-edge LLM\nin the business and media domains, with exceptional accuracy and\ntrustworthiness.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.06559v1",
    "published_date": "2024-06-02 06:24:38 UTC",
    "updated_date": "2024-06-02 06:24:38 UTC"
  },
  {
    "arxiv_id": "2406.03406v1",
    "title": "LncRNA-disease association prediction method based on heterogeneous information completion and convolutional neural network",
    "authors": [
      "Wen-Yu Xi",
      "Juan Wang",
      "Yu-Lin Zhang",
      "Jin-Xing Liu",
      "Yin-Lian Gao"
    ],
    "abstract": "The emerging research shows that lncRNA has crucial research value in a\nseries of complex human diseases. Therefore, the accurate identification of\nlncRNA-disease associations (LDAs) is very important for the warning and\ntreatment of diseases. However, most of the existing methods have limitations\nin identifying nonlinear LDAs, and it remains a huge challenge to predict new\nLDAs. In this paper, a deep learning model based on a heterogeneous network and\nconvolutional neural network (CNN) is proposed for lncRNA-disease association\nprediction, named HCNNLDA. The heterogeneous network containing the lncRNA,\ndisease, and miRNA nodes, is constructed firstly. The embedding matrix of a\nlncRNA-disease node pair is constructed according to various biological\npremises about lncRNAs, diseases, and miRNAs. Then, the low-dimensional feature\nrepresentation is fully learned by the convolutional neural network. In the\nend, the XGBoot classifier model is trained to predict the potential LDAs.\nHCNNLDA obtains a high AUC value of 0.9752 and AUPR of 0.9740 under the 5-fold\ncross-validation. The experimental results show that the proposed model has\nbetter performance than that of several latest prediction models. Meanwhile,\nthe effectiveness of HCNNLDA in identifying novel LDAs is further demonstrated\nby case studies of three diseases. To sum up, HCNNLDA is a feasible calculation\nmodel to predict LDAs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.03406v1",
    "published_date": "2024-06-02 06:11:27 UTC",
    "updated_date": "2024-06-02 06:11:27 UTC"
  },
  {
    "arxiv_id": "2406.00622v2",
    "title": "Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering",
    "authors": [
      "Xingrui Wang",
      "Wufei Ma",
      "Angtian Wang",
      "Shuo Chen",
      "Adam Kortylewski",
      "Alan Yuille"
    ],
    "abstract": "For vision-language models (VLMs), understanding the dynamic properties of\nobjects and their interactions in 3D scenes from videos is crucial for\neffective reasoning about high-level temporal and action semantics. Although\nhumans are adept at understanding these properties by constructing 3D and\ntemporal (4D) representations of the world, current video understanding models\nstruggle to extract these dynamic semantics, arguably because these models use\ncross-frame reasoning without underlying knowledge of the 3D/4D scenes. In this\nwork, we introduce DynSuperCLEVR, the first video question answering dataset\nthat focuses on language understanding of the dynamic properties of 3D objects.\nWe concentrate on three physical concepts -- velocity, acceleration, and\ncollisions within 4D scenes. We further generate three types of questions,\nincluding factual queries, future predictions, and counterfactual reasoning\nthat involve different aspects of reasoning about these 4D dynamic properties.\nTo further demonstrate the importance of explicit scene representations in\nanswering these 4D dynamics questions, we propose NS-4DPhysics, a\nNeural-Symbolic VideoQA model integrating Physics prior for 4D dynamic\nproperties with explicit scene representation of videos. Instead of answering\nthe questions directly from the video text input, our method first estimates\nthe 4D world states with a 3D generative model powered by physical priors, and\nthen uses neural symbolic reasoning to answer the questions based on the 4D\nworld states. Our evaluation on all three types of questions in DynSuperCLEVR\nshows that previous video question answering models and large multimodal models\nstruggle with questions about 4D dynamics, while our NS-4DPhysics significantly\noutperforms previous state-of-the-art models. Our code and data are released in\nhttps://xingruiwang.github.io/projects/DynSuperCLEVR/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 accepted paper. Project url:\n  https://xingruiwang.github.io/projects/DynSuperCLEVR/",
    "pdf_url": "http://arxiv.org/pdf/2406.00622v2",
    "published_date": "2024-06-02 05:51:15 UTC",
    "updated_date": "2025-04-23 04:53:40 UTC"
  },
  {
    "arxiv_id": "2406.00619v1",
    "title": "A Multi-Graph Convolutional Neural Network Model for Short-Term Prediction of Turning Movements at Signalized Intersections",
    "authors": [
      "Jewel Rana Palit",
      "Osama A Osman"
    ],
    "abstract": "Traffic flow forecasting is a crucial first step in intelligent and proactive\ntraffic management. Traffic flow parameters are volatile and uncertain, making\ntraffic flow forecasting a difficult task if the appropriate forecasting model\nis not used. Additionally, the non-Euclidean data structure of traffic flow\nparameters is challenging to analyze from both spatial and temporal\nperspectives. State-of-the-art deep learning approaches use pure convolution,\nrecurrent neural networks, and hybrid methods to achieve this objective\nefficiently. However, many of the approaches in the literature rely on complex\narchitectures that can be difficult to train. This complexity also adds to the\nblack-box nature of deep learning. This study introduces a novel deep learning\narchitecture, referred to as the multigraph convolution neural network (MGCNN),\nfor turning movement prediction at intersections. The proposed architecture\ncombines a multigraph structure, built to model temporal variations in traffic\ndata, with a spectral convolution operation to support modeling the spatial\nvariations in traffic data over the graphs. The proposed model was tested using\ntwenty days of flow and traffic control data collected from an arterial in\ndowntown Chattanooga, TN, with ten signalized intersections. The model's\nability to perform short-term predictions over 1, 2, 3, 4, and 5 minutes into\nthe future was evaluated against four baseline state-of-the-art models. The\nresults showed that our proposed model is superior to the other baseline models\nin predicting turning movements with a mean squared error (MSE) of 0.9",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.00619v1",
    "published_date": "2024-06-02 05:41:25 UTC",
    "updated_date": "2024-06-02 05:41:25 UTC"
  },
  {
    "arxiv_id": "2406.00614v1",
    "title": "Efficient Monte Carlo Tree Search via On-the-Fly State-Conditioned Action Abstraction",
    "authors": [
      "Yunhyeok Kwak",
      "Inwoo Hwang",
      "Dooyoung Kim",
      "Sanghack Lee",
      "Byoung-Tak Zhang"
    ],
    "abstract": "Monte Carlo Tree Search (MCTS) has showcased its efficacy across a broad\nspectrum of decision-making problems. However, its performance often degrades\nunder vast combinatorial action space, especially where an action is composed\nof multiple sub-actions. In this work, we propose an action abstraction based\non the compositional structure between a state and sub-actions for improving\nthe efficiency of MCTS under a factored action space. Our method learns a\nlatent dynamics model with an auxiliary network that captures sub-actions\nrelevant to the transition on the current state, which we call\nstate-conditioned action abstraction. Notably, it infers such compositional\nrelationships from high-dimensional observations without the known environment\nmodel. During the tree traversal, our method constructs the state-conditioned\naction abstraction for each node on-the-fly, reducing the search space by\ndiscarding the exploration of redundant sub-actions. Experimental results\ndemonstrate the superior sample efficiency of our method compared to vanilla\nMuZero, which suffers from expansive action space.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "UAI 2024 (Oral). The first two authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2406.00614v1",
    "published_date": "2024-06-02 04:31:30 UTC",
    "updated_date": "2024-06-02 04:31:30 UTC"
  },
  {
    "arxiv_id": "2406.00609v4",
    "title": "SuperGaussian: Repurposing Video Models for 3D Super Resolution",
    "authors": [
      "Yuan Shen",
      "Duygu Ceylan",
      "Paul Guerrero",
      "Zexiang Xu",
      "Niloy J. Mitra",
      "Shenlong Wang",
      "Anna Frühstück"
    ],
    "abstract": "We present a simple, modular, and generic method that upsamples coarse 3D\nmodels by adding geometric and appearance details. While generative 3D models\nnow exist, they do not yet match the quality of their counterparts in image and\nvideo domains. We demonstrate that it is possible to directly repurpose\nexisting (pretrained) video models for 3D super-resolution and thus sidestep\nthe problem of the shortage of large repositories of high-quality 3D training\nmodels. We describe how to repurpose video upsampling models, which are not 3D\nconsistent, and combine them with 3D consolidation to produce 3D-consistent\nresults. As output, we produce high quality Gaussian Splat models, which are\nobject centric and effective. Our method is category agnostic and can be easily\nincorporated into existing 3D workflows. We evaluate our proposed SuperGaussian\non a variety of 3D inputs, which are diverse both in terms of complexity and\nrepresentation (e.g., Gaussian Splats or NeRFs), and demonstrate that our\nsimple method significantly improves the fidelity of the final 3D models. Check\nour project website for details: supergaussian.github.io",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024, project website with interactive demo:\n  https://supergaussian.github.io",
    "pdf_url": "http://arxiv.org/pdf/2406.00609v4",
    "published_date": "2024-06-02 03:44:50 UTC",
    "updated_date": "2024-07-16 04:41:59 UTC"
  },
  {
    "arxiv_id": "2406.00605v1",
    "title": "LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models",
    "authors": [
      "Liang Zhao",
      "Tianwen Wei",
      "Liang Zeng",
      "Cheng Cheng",
      "Liu Yang",
      "Peng Cheng",
      "Lijie Wang",
      "Chenxia Li",
      "Xuejie Wu",
      "Bo Zhu",
      "Yimeng Gan",
      "Rui Hu",
      "Shuicheng Yan",
      "Han Fang",
      "Yahui Zhou"
    ],
    "abstract": "We introduce LongSkywork, a long-context Large Language Model (LLM) capable\nof processing up to 200,000 tokens. We provide a training recipe for\nefficiently extending context length of LLMs. We identify that the critical\nelement in enhancing long-context processing capability is to incorporate a\nlong-context SFT stage following the standard SFT stage. A mere 200 iterations\ncan convert the standard SFT model into a long-context model. To reduce the\neffort in collecting and annotating data for long-context language modeling, we\ndevelop two novel methods for creating synthetic data. These methods are\napplied during the continual pretraining phase as well as the Supervised\nFine-Tuning (SFT) phase, greatly enhancing the training efficiency of our\nlong-context LLMs. Our findings suggest that synthetic long-context SFT data\ncan surpass the performance of data curated by humans to some extent.\nLongSkywork achieves outstanding performance on a variety of long-context\nbenchmarks. In the Needle test, a benchmark for long-context information\nretrieval, our models achieved perfect accuracy across multiple context spans.\nMoreover, in realistic application scenarios, LongSkywork-13B demonstrates\nperformance on par with Claude2.1, the leading long-context model, underscoring\nthe effectiveness of our proposed methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00605v1",
    "published_date": "2024-06-02 03:34:41 UTC",
    "updated_date": "2024-06-02 03:34:41 UTC"
  },
  {
    "arxiv_id": "2406.00600v1",
    "title": "Kolmogorov-Arnold Network for Satellite Image Classification in Remote Sensing",
    "authors": [
      "Minjong Cheon"
    ],
    "abstract": "In this research, we propose the first approach for integrating the\nKolmogorov-Arnold Network (KAN) with various pre-trained Convolutional Neural\nNetwork (CNN) models for remote sensing (RS) scene classification tasks using\nthe EuroSAT dataset. Our novel methodology, named KCN, aims to replace\ntraditional Multi-Layer Perceptrons (MLPs) with KAN to enhance classification\nperformance. We employed multiple CNN-based models, including VGG16,\nMobileNetV2, EfficientNet, ConvNeXt, ResNet101, and Vision Transformer (ViT),\nand evaluated their performance when paired with KAN. Our experiments\ndemonstrated that KAN achieved high accuracy with fewer training epochs and\nparameters. Specifically, ConvNeXt paired with KAN showed the best performance,\nachieving 94% accuracy in the first epoch, which increased to 96% and remained\nconsistent across subsequent epochs. The results indicated that KAN and MLP\nboth achieved similar accuracy, with KAN performing slightly better in later\nepochs. By utilizing the EuroSAT dataset, we provided a robust testbed to\ninvestigate whether KAN is suitable for remote sensing classification tasks.\nGiven that KAN is a novel algorithm, there is substantial capacity for further\ndevelopment and optimization, suggesting that KCN offers a promising\nalternative for efficient image analysis in the RS field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.data-an"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00600v1",
    "published_date": "2024-06-02 03:11:37 UTC",
    "updated_date": "2024-06-02 03:11:37 UTC"
  },
  {
    "arxiv_id": "2406.00599v3",
    "title": "Robust Fair Clustering with Group Membership Uncertainty Sets",
    "authors": [
      "Sharmila Duppala",
      "Juan Luque",
      "John P. Dickerson",
      "Seyed A. Esmaeili"
    ],
    "abstract": "We study the canonical fair clustering problem where each cluster is\nconstrained to have close to population-level representation of each group.\nDespite significant attention, the salient issue of having incomplete knowledge\nabout the group membership of each point has been superficially addressed. In\nthis paper, we consider a setting where the assigned group memberships are\nnoisy. We introduce a simple noise model that requires a small number of\nparameters to be given by the decision maker. We then present an algorithm for\nfair clustering with provable \\emph{robustness} guarantees. Our framework\nenables the decision maker to trade off between the robustness and the\nclustering quality. Unlike previous work, our algorithms are backed by\nworst-case theoretical guarantees. Finally, we empirically verify the\nperformance of our algorithm on real world datasets and show its superior\nperformance over existing baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00599v3",
    "published_date": "2024-06-02 03:11:31 UTC",
    "updated_date": "2024-11-20 17:12:50 UTC"
  },
  {
    "arxiv_id": "2406.02602v1",
    "title": "D-FaST: Cognitive Signal Decoding with Disentangled Frequency-Spatial-Temporal Attention",
    "authors": [
      "Weiguo Chen",
      "Changjian Wang",
      "Kele Xu",
      "Yuan Yuan",
      "Yanru Bai",
      "Dongsong Zhang"
    ],
    "abstract": "Cognitive Language Processing (CLP), situated at the intersection of Natural\nLanguage Processing (NLP) and cognitive science, plays a progressively pivotal\nrole in the domains of artificial intelligence, cognitive intelligence, and\nbrain science. Among the essential areas of investigation in CLP, Cognitive\nSignal Decoding (CSD) has made remarkable achievements, yet there still exist\nchallenges related to insufficient global dynamic representation capability and\ndeficiencies in multi-domain feature integration. In this paper, we introduce a\nnovel paradigm for CLP referred to as Disentangled Frequency-Spatial-Temporal\nAttention(D-FaST). Specifically, we present an novel cognitive signal decoder\nthat operates on disentangled frequency-space-time domain attention. This\ndecoder encompasses three key components: frequency domain feature extraction\nemploying multi-view attention, spatial domain feature extraction utilizing\ndynamic brain connection graph attention, and temporal feature extraction\nrelying on local time sliding window attention. These components are integrated\nwithin a novel disentangled framework. Additionally, to encourage advancements\nin this field, we have created a new CLP dataset, MNRED. Subsequently, we\nconducted an extensive series of experiments, evaluating D-FaST's performance\non MNRED, as well as on publicly available datasets including ZuCo, BCIC IV-2A,\nand BCIC IV-2B. Our experimental results demonstrate that D-FaST outperforms\nexisting methods significantly on both our datasets and traditional CSD\ndatasets including establishing a state-of-the-art accuracy score 78.72% on\nMNRED, pushing the accuracy score on ZuCo to 78.35%, accuracy score on BCIC\nIV-2A to 74.85% and accuracy score on BCIC IV-2B to 76.81%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages, 9 figures. Accepted by IEEE Transactions on Cognitive and\n  Developmental Systems",
    "pdf_url": "http://arxiv.org/pdf/2406.02602v1",
    "published_date": "2024-06-02 02:33:14 UTC",
    "updated_date": "2024-06-02 02:33:14 UTC"
  },
  {
    "arxiv_id": "2406.00592v3",
    "title": "Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming",
    "authors": [
      "Dimitri P. Bertsekas"
    ],
    "abstract": "In this paper we describe a new conceptual framework that connects\napproximate Dynamic Programming (DP), Model Predictive Control (MPC), and\nReinforcement Learning (RL). This framework centers around two algorithms,\nwhich are designed largely independently of each other and operate in synergy\nthrough the powerful mechanism of Newton's method. We call them the off-line\ntraining and the on-line play algorithms. The names are borrowed from some of\nthe major successes of RL involving games; primary examples are the recent\n(2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the\nsimilarly structured and earlier (1990s) TD-Gammon program (which plays\nbackgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line\ntraining algorithm is the method used to teach the program how to evaluate\npositions and to generate good moves at any given position, while the on-line\nplay algorithm is the method used to play in real time against human or\ncomputer opponents.\n  Significantly, the synergy between off-line training and on-line play also\nunderlies MPC (as well as other major classes of sequential decision problems),\nand indeed the MPC design architecture is very similar to the one of AlphaZero\nand TD-Gammon. This conceptual insight provides a vehicle for bridging the\ncultural gap between RL and MPC, and sheds new light on some fundamental issues\nin MPC. These include the enhancement of stability properties through rollout,\nthe treatment of uncertainty through the use of certainty equivalence, the\nresilience of MPC in adaptive control settings that involve changing system\nparameters, and the insights provided by the superlinear performance bounds\nimplied by Newton's method.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00592v3",
    "published_date": "2024-06-02 02:01:03 UTC",
    "updated_date": "2024-06-30 19:17:33 UTC"
  },
  {
    "arxiv_id": "2406.00589v1",
    "title": "Robust Visual Tracking via Iterative Gradient Descent and Threshold Selection",
    "authors": [
      "Zhuang Qi",
      "Junlin Zhang",
      "Xin Qi"
    ],
    "abstract": "Visual tracking fundamentally involves regressing the state of the target in\neach frame of a video. Despite significant progress, existing regression-based\ntrackers still tend to experience failures and inaccuracies. To enhance the\nprecision of target estimation, this paper proposes a tracking technique based\non robust regression. Firstly, we introduce a novel robust linear regression\nestimator, which achieves favorable performance when the error vector follows\ni.i.d Gaussian-Laplacian distribution. Secondly, we design an iterative process\nto quickly solve the problem of outliers. In fact, the coefficients are\nobtained by Iterative Gradient Descent and Threshold Selection algorithm\n(IGDTS). In addition, we expend IGDTS to a generative tracker, and apply\nIGDTS-distance to measure the deviation between the sample and the model.\nFinally, we propose an update scheme to capture the appearance changes of the\ntracked object and ensure that the model is updated correctly. Experimental\nresults on several challenging image sequences show that the proposed tracker\noutperformance existing trackers.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00589v1",
    "published_date": "2024-06-02 01:51:09 UTC",
    "updated_date": "2024-06-02 01:51:09 UTC"
  },
  {
    "arxiv_id": "2406.00586v2",
    "title": "VeriSplit: Secure and Practical Offloading of Machine Learning Inferences across IoT Devices",
    "authors": [
      "Han Zhang",
      "Zifan Wang",
      "Mihir Dhamankar",
      "Matt Fredrikson",
      "Yuvraj Agarwal"
    ],
    "abstract": "Many Internet-of-Things (IoT) devices rely on cloud computation resources to\nperform machine learning inferences. This is expensive and may raise privacy\nconcerns for users. Consumers of these devices often have hardware such as\ngaming consoles and PCs with graphics accelerators that are capable of\nperforming these computations, which may be left idle for significant periods\nof time. While this presents a compelling potential alternative to cloud\noffloading, concerns about the integrity of inferences, the confidentiality of\nmodel parameters, and the privacy of users' data mean that device vendors may\nbe hesitant to offload their inferences to a platform managed by another\nmanufacturer.\n  We propose VeriSplit, a framework for offloading machine learning inferences\nto locally-available devices that address these concerns. We introduce masking\ntechniques to protect data privacy and model confidentiality, and a\ncommitment-based verification protocol to address integrity. Unlike much prior\nwork aimed at addressing these issues, our approach does not rely on\ncomputation over finite field elements, which may interfere with floating-point\ncomputation supports on hardware accelerators and require modification to\nexisting models. We implemented a prototype of VeriSplit and our evaluation\nresults show that, compared to performing computation locally, our secure and\nprivate offloading solution can reduce inference latency by 28%--83%.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.00586v2",
    "published_date": "2024-06-02 01:28:38 UTC",
    "updated_date": "2025-03-31 04:32:49 UTC"
  },
  {
    "arxiv_id": "2406.00584v1",
    "title": "A Blueprint Architecture of Compound AI Systems for Enterprise",
    "authors": [
      "Eser Kandogan",
      "Sajjadur Rahman",
      "Nikita Bhutani",
      "Dan Zhang",
      "Rafael Li Chen",
      "Kushan Mitra",
      "Sairam Gurajada",
      "Pouya Pezeshkpour",
      "Hayate Iso",
      "Yanlin Feng",
      "Hannah Kim",
      "Chen Shen",
      "Jin Wang",
      "Estevam Hruschka"
    ],
    "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities\nsurpassing conventional NLP challenges, creating opportunities for use in\nproduction use cases. Towards this goal, there is a notable shift to building\ncompound AI systems, wherein LLMs are integrated into an expansive software\ninfrastructure with many components like models, retrievers, databases and\ntools. In this paper, we introduce a blueprint architecture for compound AI\nsystems to operate in enterprise settings cost-effectively and feasibly. Our\nproposed architecture aims for seamless integration with existing compute and\ndata infrastructure, with ``stream'' serving as the key orchestration concept\nto coordinate data and instructions among agents and other components. Task and\ndata planners, respectively, break down, map, and optimize tasks and data to\navailable agents and data sources defined in respective registries, given\nproduction constraints such as accuracy and latency.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Compound AI Systems Workshop at the Data+AI Summit 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.00584v1",
    "published_date": "2024-06-02 01:16:32 UTC",
    "updated_date": "2024-06-02 01:16:32 UTC"
  },
  {
    "arxiv_id": "2406.02601v1",
    "title": "Multimodal Deep Learning for Low-Resource Settings: A Vector Embedding Alignment Approach for Healthcare Applications",
    "authors": [
      "David Restrepo",
      "Chenwei Wu",
      "Sebastián Andrés Cajas",
      "Luis Filipe Nakayama",
      "Leo Anthony Celi",
      "Diego M López"
    ],
    "abstract": "Large-scale multi-modal deep learning models have revolutionized domains such\nas healthcare, highlighting the importance of computational power. However, in\nresource-constrained regions like Low and Middle-Income Countries (LMICs),\nlimited access to GPUs and data poses significant challenges, often leaving\nCPUs as the sole resource. To address this, we advocate for leveraging vector\nembeddings to enable flexible and efficient computational methodologies,\ndemocratizing multimodal deep learning across diverse contexts.\n  Our paper investigates the efficiency and effectiveness of using vector\nembeddings from single-modal foundation models and multi-modal Vision-Language\nModels (VLMs) for multimodal deep learning in low-resource environments,\nparticularly in healthcare. Additionally, we propose a simple yet effective\ninference-time method to enhance performance by aligning image-text embeddings.\nComparing these approaches with traditional methods, we assess their impact on\ncomputational efficiency and model performance using metrics like accuracy,\nF1-score, inference time, training time, and memory usage across three medical\nmodalities: BRSET (ophthalmology), HAM10000 (dermatology), and SatelliteBench\n(public health).\n  Our findings show that embeddings reduce computational demands without\ncompromising model performance. Furthermore, our alignment method improves\nperformance in medical tasks. This research promotes sustainable AI practices\nby optimizing resources in constrained environments, highlighting the potential\nof embedding-based approaches for efficient multimodal learning. Vector\nembeddings democratize multimodal deep learning in LMICs, particularly in\nhealthcare, enhancing AI adaptability in varied use cases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.02601v1",
    "published_date": "2024-06-02 01:13:01 UTC",
    "updated_date": "2024-06-02 01:13:01 UTC"
  },
  {
    "arxiv_id": "2406.00583v1",
    "title": "CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems",
    "authors": [
      "Yanlin Feng",
      "Sajjadur Rahman",
      "Aaron Feng",
      "Vincent Chen",
      "Eser Kandogan"
    ],
    "abstract": "Compound AI systems (CASs) that employ LLMs as agents to accomplish\nknowledge-intensive tasks via interactions with tools and data retrievers have\ngarnered significant interest within database and AI communities. While these\nsystems have the potential to supplement typical analysis workflows of data\nanalysts in enterprise data platforms, unfortunately, CASs are subject to the\nsame data discovery challenges that analysts have encountered over the years --\nsilos of multimodal data sources, created across teams and departments within\nan organization, make it difficult to identify appropriate data sources for\naccomplishing the task at hand. Existing data discovery benchmarks do not model\nsuch multimodality and multiplicity of data sources. Moreover, benchmarks of\nCASs prioritize only evaluating end-to-end task performance. To catalyze\nresearch on evaluating the data discovery performance of multimodal data\nretrievers in CASs within a real-world setting, we propose CMDBench, a\nbenchmark modeling the complexity of enterprise data platforms. We adapt\nexisting datasets and benchmarks in open-domain -- from question answering and\ncomplex reasoning tasks to natural language querying over structured data -- to\nevaluate coarse- and fine-grained data discovery and task execution\nperformance. Our experiments reveal the impact of data retriever design on\ndownstream task performance -- a 46% drop in task accuracy on average -- across\nvarious modalities, data sources, and task difficulty. The results indicate the\nneed to develop optimization strategies to identify appropriate LLM agents and\nretrievers for efficient execution of CASs over enterprise data.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Governance, Understanding and Integration of Data for Effective and\n  Responsible AI (GUIDE-AI '24), June 14, 2024, Santiago, AA, Chile",
    "pdf_url": "http://arxiv.org/pdf/2406.00583v1",
    "published_date": "2024-06-02 01:10:41 UTC",
    "updated_date": "2024-06-02 01:10:41 UTC"
  },
  {
    "arxiv_id": "2406.00578v1",
    "title": "ContextFlow++: Generalist-Specialist Flow-based Generative Models with Mixed-Variable Context Encoding",
    "authors": [
      "Denis Gudovskiy",
      "Tomoyuki Okuno",
      "Yohei Nakata"
    ],
    "abstract": "Normalizing flow-based generative models have been widely used in\napplications where the exact density estimation is of major importance. Recent\nresearch proposes numerous methods to improve their expressivity. However,\nconditioning on a context is largely overlooked area in the bijective flow\nresearch. Conventional conditioning with the vector concatenation is limited to\nonly a few flow types. More importantly, this approach cannot support a\npractical setup where a set of context-conditioned (specialist) models are\ntrained with the fixed pretrained general-knowledge (generalist) model. We\npropose ContextFlow++ approach to overcome these limitations using an additive\nconditioning with explicit generalist-specialist knowledge decoupling.\nFurthermore, we support discrete contexts by the proposed mixed-variable\narchitecture with context encoders. Particularly, our context encoder for\ndiscrete variables is a surjective flow from which the context-conditioned\ncontinuous variables are sampled. Our experiments on rotated MNIST-R, corrupted\nCIFAR-10C, real-world ATM predictive maintenance and SMAP unsupervised anomaly\ndetection benchmarks show that the proposed ContextFlow++ offers faster stable\ntraining and achieves higher performance metrics. Our code is publicly\navailable at https://github.com/gudovskiy/contextflow.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to UAI 2024. Preprint",
    "pdf_url": "http://arxiv.org/pdf/2406.00578v1",
    "published_date": "2024-06-02 00:00:00 UTC",
    "updated_date": "2024-06-02 00:00:00 UTC"
  }
]
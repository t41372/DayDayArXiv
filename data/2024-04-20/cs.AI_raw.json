[
  {
    "arxiv_id": "2404.13476v1",
    "title": "A Framework for Feasible Counterfactual Exploration incorporating Causality, Sparsity and Density",
    "authors": [
      "Kleopatra Markou",
      "Dimitrios Tomaras",
      "Vana Kalogeraki",
      "Dimitrios Gunopulos"
    ],
    "abstract": "The imminent need to interpret the output of a Machine Learning model with\ncounterfactual (CF) explanations - via small perturbations to the input - has\nbeen notable in the research community. Although the variety of CF examples is\nimportant, the aspect of them being feasible at the same time, does not\nnecessarily apply in their entirety. This work uses different benchmark\ndatasets to examine through the preservation of the logical causal relations of\ntheir attributes, whether CF examples can be generated after a small amount of\nchanges to the original input, be feasible and actually useful to the end-user\nin a real-world case. To achieve this, we used a black box model as a\nclassifier, to distinguish the desired from the input class and a Variational\nAutoencoder (VAE) to generate feasible CF examples. As an extension, we also\nextracted two-dimensional manifolds (one for each dataset) that located the\nmajority of the feasible examples, a representation that adequately\ndistinguished them from infeasible ones. For our experimentation we used three\ncommonly used datasets and we managed to generate feasible and at the same time\nsparse, CF examples that satisfy all possible predefined causal constraints, by\nconfirming their importance with the attributes in a dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.13476v1",
    "published_date": "2024-04-20 22:05:48 UTC",
    "updated_date": "2024-04-20 22:05:48 UTC"
  },
  {
    "arxiv_id": "2404.13475v1",
    "title": "PristiQ: A Co-Design Framework for Preserving Data Security of Quantum Learning in the Cloud",
    "authors": [
      "Zhepeng Wang",
      "Yi Sheng",
      "Nirajan Koirala",
      "Kanad Basu",
      "Taeho Jung",
      "Cheng-Chang Lu",
      "Weiwen Jiang"
    ],
    "abstract": "Benefiting from cloud computing, today's early-stage quantum computers can be\nremotely accessed via the cloud services, known as Quantum-as-a-Service (QaaS).\nHowever, it poses a high risk of data leakage in quantum machine learning\n(QML). To run a QML model with QaaS, users need to locally compile their\nquantum circuits including the subcircuit of data encoding first and then send\nthe compiled circuit to the QaaS provider for execution. If the QaaS provider\nis untrustworthy, the subcircuit to encode the raw data can be easily stolen.\nTherefore, we propose a co-design framework for preserving the data security of\nQML with the QaaS paradigm, namely PristiQ. By introducing an encryption\nsubcircuit with extra secure qubits associated with a user-defined security\nkey, the security of data can be greatly enhanced. And an automatic search\nalgorithm is proposed to optimize the model to maintain its performance on the\nencrypted quantum data. Experimental results on simulation and the actual IBM\nquantum computer both prove the ability of PristiQ to provide high security for\nthe quantum data while maintaining the model performance in QML.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13475v1",
    "published_date": "2024-04-20 22:03:32 UTC",
    "updated_date": "2024-04-20 22:03:32 UTC"
  },
  {
    "arxiv_id": "2404.13474v1",
    "title": "Composing Pre-Trained Object-Centric Representations for Robotics From \"What\" and \"Where\" Foundation Models",
    "authors": [
      "Junyao Shi",
      "Jianing Qian",
      "Yecheng Jason Ma",
      "Dinesh Jayaraman"
    ],
    "abstract": "There have recently been large advances both in pre-training visual\nrepresentations for robotic control and segmenting unknown category objects in\ngeneral images. To leverage these for improved robot learning, we propose\n$\\textbf{POCR}$, a new framework for building pre-trained object-centric\nrepresentations for robotic control. Building on theories of \"what-where\"\nrepresentations in psychology and computer vision, we use segmentations from a\npre-trained model to stably locate across timesteps, various entities in the\nscene, capturing \"where\" information. To each such segmented entity, we apply\nother pre-trained models that build vector descriptions suitable for robotic\ncontrol tasks, thus capturing \"what\" the entity is. Thus, our pre-trained\nobject-centric representations for control are constructed by appropriately\ncombining the outputs of off-the-shelf pre-trained models, with no new\ntraining. On various simulated and real robotic tasks, we show that imitation\npolicies for robotic manipulators trained on POCR achieve better performance\nand systematic generalization than state of the art pre-trained representations\nfor robotics, as well as prior object-centric representations that are\ntypically trained from scratch.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2024. Project website: https://sites.google.com/view/pocr",
    "pdf_url": "http://arxiv.org/pdf/2404.13474v1",
    "published_date": "2024-04-20 21:51:15 UTC",
    "updated_date": "2024-04-20 21:51:15 UTC"
  },
  {
    "arxiv_id": "2404.13470v1",
    "title": "GWLZ: A Group-wise Learning-based Lossy Compression Framework for Scientific Data",
    "authors": [
      "Wenqi Jia",
      "Sian Jin",
      "Jinzhen Wang",
      "Wei Niu",
      "Dingwen Tao",
      "Miao Yin"
    ],
    "abstract": "The rapid expansion of computational capabilities and the ever-growing scale\nof modern HPC systems present formidable challenges in managing exascale\nscientific data. Faced with such vast datasets, traditional lossless\ncompression techniques prove insufficient in reducing data size to a manageable\nlevel while preserving all information intact. In response, researchers have\nturned to error-bounded lossy compression methods, which offer a balance\nbetween data size reduction and information retention. However, despite their\nutility, these compressors employing conventional techniques struggle with\nlimited reconstruction quality. To address this issue, we draw inspiration from\nrecent advancements in deep learning and propose GWLZ, a novel group-wise\nlearning-based lossy compression framework with multiple lightweight learnable\nenhancer models. Leveraging a group of neural networks, GWLZ significantly\nenhances the decompressed data reconstruction quality with negligible impact on\nthe compression efficiency. Experimental results on different fields from the\nNyx dataset demonstrate remarkable improvements by GWLZ, achieving up to 20%\nquality enhancements with negligible overhead as low as 0.0003x.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13470v1",
    "published_date": "2024-04-20 21:12:53 UTC",
    "updated_date": "2024-04-20 21:12:53 UTC"
  },
  {
    "arxiv_id": "2404.13454v1",
    "title": "Revolutionizing System Reliability: The Role of AI in Predictive Maintenance Strategies",
    "authors": [
      "Michael Bidollahkhani",
      "Julian M. Kunkel"
    ],
    "abstract": "The landscape of maintenance in distributed systems is rapidly evolving with\nthe integration of Artificial Intelligence (AI). Also, as the complexity of\ncomputing continuum systems intensifies, the role of AI in predictive\nmaintenance (Pd.M.) becomes increasingly pivotal. This paper presents a\ncomprehensive survey of the current state of Pd.M. in the computing continuum,\nwith a focus on the combination of scalable AI technologies. Recognizing the\nlimitations of traditional maintenance practices in the face of increasingly\ncomplex and heterogenous computing continuum systems, the study explores how\nAI, especially machine learning and neural networks, is being used to enhance\nPd.M. strategies. The survey encompasses a thorough review of existing\nliterature, highlighting key advancements, methodologies, and case studies in\nthe field. It critically examines the role of AI in improving prediction\naccuracy for system failures and in optimizing maintenance schedules, thereby\ncontributing to reduced downtime and enhanced system longevity. By synthesizing\nfindings from the latest advancements in the field, the article provides\ninsights into the effectiveness and challenges of implementing AI-driven\npredictive maintenance. It underscores the evolution of maintenance practices\nin response to technological advancements and the growing complexity of\ncomputing continuum systems. The conclusions drawn from this survey are\ninstrumental for practitioners and researchers in understanding the current\nlandscape and future directions of Pd.M. in distributed systems. It emphasizes\nthe need for continued research and development in this area, pointing towards\na trend of more intelligent, efficient, and cost-effective maintenance\nsolutions in the era of AI.",
    "categories": [
      "cs.AI",
      "cs.PF",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted, published and presented for the IARIA CLOUDCOMP2024\n  Conference of Venice, Italy",
    "pdf_url": "http://arxiv.org/pdf/2404.13454v1",
    "published_date": "2024-04-20 19:31:05 UTC",
    "updated_date": "2024-04-20 19:31:05 UTC"
  },
  {
    "arxiv_id": "2404.14451v1",
    "title": "Generative Subspace Adversarial Active Learning for Outlier Detection in Multiple Views of High-dimensional Data",
    "authors": [
      "Jose Cribeiro-Ramallo",
      "Vadim Arzamasov",
      "Federico Matteucci",
      "Denis Wambold",
      "Klemens Böhm"
    ],
    "abstract": "Outlier detection in high-dimensional tabular data is an important task in\ndata mining, essential for many downstream tasks and applications. Existing\nunsupervised outlier detection algorithms face one or more problems, including\ninlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).\nTo address these issues, we introduce Generative Subspace Adversarial Active\nLearning (GSAAL), a novel approach that uses a Generative Adversarial Network\nwith multiple adversaries. These adversaries learn the marginal class\nprobability functions over different data subspaces, while a single generator\nin the full space models the entire distribution of the inlier class. GSAAL is\nspecifically designed to address the MV limitation while also handling the IA\nand CD, being the only method to do so. We provide a comprehensive mathematical\nformulation of MV, convergence guarantees for the discriminators, and\nscalability results for GSAAL. Our extensive experiments demonstrate the\neffectiveness and scalability of GSAAL, highlighting its superior performance\ncompared to other popular OD methods, especially in MV scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "16 pages, Pre-print",
    "pdf_url": "http://arxiv.org/pdf/2404.14451v1",
    "published_date": "2024-04-20 19:22:05 UTC",
    "updated_date": "2024-04-20 19:22:05 UTC"
  },
  {
    "arxiv_id": "2404.13449v1",
    "title": "SiNC+: Adaptive Camera-Based Vitals with Unsupervised Learning of Periodic Signals",
    "authors": [
      "Jeremy Speth",
      "Nathan Vance",
      "Patrick Flynn",
      "Adam Czajka"
    ],
    "abstract": "Subtle periodic signals, such as blood volume pulse and respiration, can be\nextracted from RGB video, enabling noncontact health monitoring at low cost.\nAdvancements in remote pulse estimation -- or remote photoplethysmography\n(rPPG) -- are currently driven by deep learning solutions. However, modern\napproaches are trained and evaluated on benchmark datasets with ground truth\nfrom contact-PPG sensors. We present the first non-contrastive unsupervised\nlearning framework for signal regression to mitigate the need for labelled\nvideo data. With minimal assumptions of periodicity and finite bandwidth, our\napproach discovers the blood volume pulse directly from unlabelled videos. We\nfind that encouraging sparse power spectra within normal physiological\nbandlimits and variance over batches of power spectra is sufficient for\nlearning visual features of periodic signals. We perform the first experiments\nutilizing unlabelled video data not specifically created for rPPG to train\nrobust pulse rate estimators. Given the limited inductive biases, we\nsuccessfully applied the same approach to camera-based respiration by changing\nthe bandlimits of the target signal. This shows that the approach is general\nenough for unsupervised learning of bandlimited quasi-periodic signals from\ndifferent domains. Furthermore, we show that the framework is effective for\nfinetuning models on unlabelled video from a single subject, allowing for\npersonalized and adaptive signal regressors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Extension of CVPR2023 highlight paper. arXiv admin note: substantial\n  text overlap with arXiv:2303.07944",
    "pdf_url": "http://arxiv.org/pdf/2404.13449v1",
    "published_date": "2024-04-20 19:17:40 UTC",
    "updated_date": "2024-04-20 19:17:40 UTC"
  },
  {
    "arxiv_id": "2404.14450v1",
    "title": "GraphMatcher: A Graph Representation Learning Approach for Ontology Matching",
    "authors": [
      "Sefika Efeoglu"
    ],
    "abstract": "Ontology matching is defined as finding a relationship or correspondence\nbetween two or more entities in two or more ontologies. To solve the\ninteroperability problem of the domain ontologies, semantically similar\nentities in these ontologies must be found and aligned before merging them.\nGraphMatcher, developed in this study, is an ontology matching system using a\ngraph attention approach to compute higher-level representation of a class\ntogether with its surrounding terms. The GraphMatcher has obtained remarkable\nresults in in the Ontology Alignment Evaluation Initiative (OAEI) 2022\nconference track. Its codes are available at\n~\\url{https://github.com/sefeoglu/gat_ontology_matching}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The 17th International Workshop on Ontology Matching, The 21st\n  International Semantic Web Conference (ISWC) 2022, 23 October 2022, Hangzhou,\n  China",
    "pdf_url": "http://arxiv.org/pdf/2404.14450v1",
    "published_date": "2024-04-20 18:30:17 UTC",
    "updated_date": "2024-04-20 18:30:17 UTC"
  },
  {
    "arxiv_id": "2404.17593v1",
    "title": "A Continual Relation Extraction Approach for Knowledge Graph Completeness",
    "authors": [
      "Sefika Efeoglu"
    ],
    "abstract": "Representing unstructured data in a structured form is most significant for\ninformation system management to analyze and interpret it. To do this, the\nunstructured data might be converted into Knowledge Graphs, by leveraging an\ninformation extraction pipeline whose main tasks are named entity recognition\nand relation extraction. This thesis aims to develop a novel continual relation\nextraction method to identify relations (interconnections) between entities in\na data stream coming from the real world. Domain-specific data of this thesis\nis corona news from German and Austrian newspapers.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "Published at TPDL 2022",
    "pdf_url": "http://arxiv.org/pdf/2404.17593v1",
    "published_date": "2024-04-20 18:15:52 UTC",
    "updated_date": "2024-04-20 18:15:52 UTC"
  },
  {
    "arxiv_id": "2404.13434v1",
    "title": "Nested-TNT: Hierarchical Vision Transformers with Multi-Scale Feature Processing",
    "authors": [
      "Yuang Liu",
      "Zhiheng Qiu",
      "Xiaokai Qin"
    ],
    "abstract": "Transformer has been applied in the field of computer vision due to its\nexcellent performance in natural language processing, surpassing traditional\nconvolutional neural networks and achieving new state-of-the-art. ViT divides\nan image into several local patches, known as \"visual sentences\". However, the\ninformation contained in the image is vast and complex, and focusing only on\nthe features at the \"visual sentence\" level is not enough. The features between\nlocal patches should also be taken into consideration. In order to achieve\nfurther improvement, the TNT model is proposed, whose algorithm further divides\nthe image into smaller patches, namely \"visual words,\" achieving more accurate\nresults. The core of Transformer is the Multi-Head Attention mechanism, and\ntraditional attention mechanisms ignore interactions across different attention\nheads. In order to reduce redundancy and improve utilization, we introduce the\nnested algorithm and apply the Nested-TNT to image classification tasks. The\nexperiment confirms that the proposed model has achieved better classification\nperformance over ViT and TNT, exceeding 2.25%, 1.1% on dataset CIFAR10 and\n2.78%, 0.25% on dataset FLOWERS102 respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13434v1",
    "published_date": "2024-04-20 17:56:14 UTC",
    "updated_date": "2024-04-20 17:56:14 UTC"
  },
  {
    "arxiv_id": "2404.13428v1",
    "title": "Text-dependent Speaker Verification (TdSV) Challenge 2024: Challenge Evaluation Plan",
    "authors": [
      "Zeinali Hossein",
      "Lee Kong Aik",
      "Alam Jahangir",
      "Burget Lukas"
    ],
    "abstract": "This document outlines the Text-dependent Speaker Verification (TdSV)\nChallenge 2024, which centers on analyzing and exploring novel approaches for\ntext-dependent speaker verification. The primary goal of this challenge is to\nmotive participants to develop single yet competitive systems, conduct thorough\nanalyses, and explore innovative concepts such as multi-task learning,\nself-supervised learning, few-shot learning, and others, for text-dependent\nspeaker verification.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13428v1",
    "published_date": "2024-04-20 17:26:59 UTC",
    "updated_date": "2024-04-20 17:26:59 UTC"
  },
  {
    "arxiv_id": "2404.13425v3",
    "title": "Enhancing Adversarial Robustness of Vision-Language Models through Low-Rank Adaptation",
    "authors": [
      "Yuheng Ji",
      "Yue Liu",
      "Zhicheng Zhang",
      "Zhao Zhang",
      "Yuting Zhao",
      "Xiaoshuai Hao",
      "Gang Zhou",
      "Xingwei Zhang",
      "Xiaolong Zheng"
    ],
    "abstract": "Vision-Language Models (VLMs) play a crucial role in the advancement of\nArtificial General Intelligence (AGI). As AGI rapidly evolves, addressing\nsecurity concerns has emerged as one of the most significant challenges for\nVLMs. In this paper, we present extensive experiments that expose the\nvulnerabilities of conventional adaptation methods for VLMs, highlighting\nsignificant security risks. Moreover, as VLMs grow in size, the application of\ntraditional adversarial adaptation techniques incurs substantial computational\ncosts. To address these issues, we propose a parameter-efficient adversarial\nadaptation method called \\textbf{\\textit{AdvLoRA}} based on Low-Rank\nAdaptation. We investigate and reveal the inherent low-rank properties involved\nin adversarial adaptation for VLMs. Different from LoRA, we enhance the\nefficiency and robustness of adversarial adaptation by introducing a novel\nreparameterization method that leverages parameter clustering and alignment.\nAdditionally, we propose an adaptive parameter update strategy to further\nbolster robustness. These innovations enable our AdvLoRA to mitigate issues\nrelated to model security and resource wastage. Extensive experiments confirm\nthe effectiveness and efficiency of AdvLoRA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13425v3",
    "published_date": "2024-04-20 17:19:54 UTC",
    "updated_date": "2025-02-20 02:24:55 UTC"
  },
  {
    "arxiv_id": "2404.13421v1",
    "title": "MultiConfederated Learning: Inclusive Non-IID Data handling with Decentralized Federated Learning",
    "authors": [
      "Michael Duchesne",
      "Kaiwen Zhang",
      "Chamseddine Talhi"
    ],
    "abstract": "Federated Learning (FL) has emerged as a prominent privacy-preserving\ntechnique for enabling use cases like confidential clinical machine learning.\nFL operates by aggregating models trained by remote devices which owns the\ndata. Thus, FL enables the training of powerful global models using\ncrowd-sourced data from a large number of learners, without compromising their\nprivacy. However, the aggregating server is a single point of failure when\ngenerating the global model. Moreover, the performance of the model suffers\nwhen the data is not independent and identically distributed (non-IID data) on\nall remote devices. This leads to vastly different models being aggregated,\nwhich can reduce the performance by as much as 50% in certain scenarios.\n  In this paper, we seek to address the aforementioned issues while retaining\nthe benefits of FL. We propose MultiConfederated Learning: a decentralized FL\nframework which is designed to handle non-IID data. Unlike traditional FL,\nMultiConfederated Learning will maintain multiple models in parallel (instead\nof a single global model) to help with convergence when the data is non-IID.\nWith the help of transfer learning, learners can converge to fewer models. In\norder to increase adaptability, learners are allowed to choose which updates to\naggregate from their peers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13421v1",
    "published_date": "2024-04-20 16:38:26 UTC",
    "updated_date": "2024-04-20 16:38:26 UTC"
  },
  {
    "arxiv_id": "2404.13417v1",
    "title": "Efficient and Concise Explanations for Object Detection with Gaussian-Class Activation Mapping Explainer",
    "authors": [
      "Quoc Khanh Nguyen",
      "Truong Thanh Hung Nguyen",
      "Vo Thanh Khang Nguyen",
      "Van Binh Truong",
      "Tuong Phan",
      "Hung Cao"
    ],
    "abstract": "To address the challenges of providing quick and plausible explanations in\nExplainable AI (XAI) for object detection models, we introduce the Gaussian\nClass Activation Mapping Explainer (G-CAME). Our method efficiently generates\nconcise saliency maps by utilizing activation maps from selected layers and\napplying a Gaussian kernel to emphasize critical image regions for the\npredicted object. Compared with other Region-based approaches, G-CAME\nsignificantly reduces explanation time to 0.5 seconds without compromising the\nquality. Our evaluation of G-CAME, using Faster-RCNN and YOLOX on the MS-COCO\n2017 dataset, demonstrates its ability to offer highly plausible and faithful\nexplanations, especially in reducing the bias on tiny object detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Canadian AI 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.13417v1",
    "published_date": "2024-04-20 16:11:47 UTC",
    "updated_date": "2024-04-20 16:11:47 UTC"
  },
  {
    "arxiv_id": "2404.13402v1",
    "title": "Intrusion Detection at Scale with the Assistance of a Command-line Language Model",
    "authors": [
      "Jiongliang Lin",
      "Yiwen Guo",
      "Hao Chen"
    ],
    "abstract": "Intrusion detection is a long standing and crucial problem in security. A\nsystem capable of detecting intrusions automatically is on great demand in\nenterprise security solutions. Existing solutions rely heavily on hand-crafted\nrules designed by security operators, which suffer from high false negative\nrates and poor generalization ability to new, zero-day attacks at scale. AI and\nmachine learning offer promising solutions to address the issues, by inspecting\nabnormal user behaviors intelligently and automatically from data. However,\nexisting learning-based intrusion detection systems in the literature are\nmostly designed for small data, and they lack the ability to leverage the power\nof big data in cloud environments. In this paper, we target at this problem and\nintroduce an intrusion detection system which incorporates large-scale\npre-training, so as to train a large language model based on tens of millions\nof command lines for AI-based intrusion detection. Experiments performed on 30\nmillion training samples and 10 million test samples verify the effectiveness\nof our solution.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "Accepted by IEEE/IFIP International Conference on Dependable Systems\n  and Networks (DSN), industry track",
    "pdf_url": "http://arxiv.org/pdf/2404.13402v1",
    "published_date": "2024-04-20 15:04:25 UTC",
    "updated_date": "2024-04-20 15:04:25 UTC"
  },
  {
    "arxiv_id": "2404.13397v1",
    "title": "Retrieval-Augmented Generation-based Relation Extraction",
    "authors": [
      "Sefika Efeoglu",
      "Adrian Paschke"
    ],
    "abstract": "Information Extraction (IE) is a transformative process that converts\nunstructured text data into a structured format by employing entity and\nrelation extraction (RE) methodologies. The identification of the relation\nbetween a pair of entities plays a crucial role within this framework. Despite\nthe existence of various techniques for relation extraction, their efficacy\nheavily relies on access to labeled data and substantial computational\nresources. In addressing these challenges, Large Language Models (LLMs) emerge\nas promising solutions; however, they might return hallucinating responses due\nto their own training data. To overcome these limitations, Retrieved-Augmented\nGeneration-based Relation Extraction (RAG4RE) in this work is proposed,\noffering a pathway to enhance the performance of relation extraction tasks.\n  This work evaluated the effectiveness of our RAG4RE approach utilizing\ndifferent LLMs. Through the utilization of established benchmarks, such as\nTACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to\ncomprehensively evaluate the efficacy of our RAG4RE approach. In particularly,\nwe leverage prominent LLMs including Flan T5, Llama2, and Mistral in our\ninvestigation. The results of our study demonstrate that our RAG4RE approach\nsurpasses performance of traditional RE approaches based solely on LLMs,\nparticularly evident in the TACRED dataset and its variations. Furthermore, our\napproach exhibits remarkable performance compared to previous RE methodologies\nacross both TACRED and TACREV datasets, underscoring its efficacy and potential\nfor advancing RE tasks in natural language processing.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Submitted to Semantic Web Journal. Under Review",
    "pdf_url": "http://arxiv.org/pdf/2404.13397v1",
    "published_date": "2024-04-20 14:42:43 UTC",
    "updated_date": "2024-04-20 14:42:43 UTC"
  },
  {
    "arxiv_id": "2406.06533v1",
    "title": "Pragmatic Formal Verification Methodology for Clock Domain Crossing (CDC)",
    "authors": [
      "Aman Kumar",
      "Muhammad Ul Haque Khan",
      "Bijitendra Mittra"
    ],
    "abstract": "Modern System-on-Chip (SoC) designs are becoming more and more complex due to\nthe technology upscaling. SoC designs often operate on multiple asynchronous\nclock domains, further adding to the complexity of the overall design. To make\nthe devices power efficient, designers take a Globally-Asynchronous\nLocally-Synchronous (GALS) approach that creates multiple asynchronous domains.\nThese Clock Domain Crossings (CDC) are prone to metastability effects, and\nfunctional verification of such CDC is very important to ensure that no bug\nescapes. Conventional verification methods, such as register transfer level\n(RTL) simulations and static timing analysis, are not enough to address these\nCDC issues, which may lead to verification gaps. Additionally, identifying\nthese CDC-related bugs is very time-consuming and is one of the most common\nreasons for costly silicon re-spins. This paper is focused on the development\nof a pragmatic formal verification methodology to minimize the CDC issues by\nexercising Metastability Injection (MSI) in different CDC paths.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "Published in DVCon Europe 2023",
    "pdf_url": "http://arxiv.org/pdf/2406.06533v1",
    "published_date": "2024-04-20 13:17:25 UTC",
    "updated_date": "2024-04-20 13:17:25 UTC"
  },
  {
    "arxiv_id": "2404.15371v1",
    "title": "Efficient Verification of a RADAR SoC Using Formal and Simulation-Based Methods",
    "authors": [
      "Aman Kumar",
      "Mark Litterick",
      "Samuele Candido"
    ],
    "abstract": "As the demand for Internet of Things (IoT) and Human-to-Machine Interaction\n(HMI) increases, modern System-on-Chips (SoCs) offering such solutions are\nbecoming increasingly complex. This intricate design poses significant\nchallenges for verification, particularly when time-to-market is a crucial\nfactor for consumer electronics products. This paper presents a case study\nbased on our work to verify a complex Radio Detection And Ranging (RADAR) based\nSoC that performs on-chip sensing of human motion with millimetre accuracy. We\nleverage both formal and simulation-based methods to complement each other and\nachieve verification sign-off with high confidence. While employing a\nrequirements-driven flow approach, we demonstrate the use of different\nverification methods to cater to multiple requirements and highlight our\nknow-how from the project. Additionally, we used Machine Learning (ML) based\nmethods, specifically the Xcelium ML tool from Cadence, to improve verification\nthroughput.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "Published in DVCon Europe 2023",
    "pdf_url": "http://arxiv.org/pdf/2404.15371v1",
    "published_date": "2024-04-20 13:16:55 UTC",
    "updated_date": "2024-04-20 13:16:55 UTC"
  },
  {
    "arxiv_id": "2405.01572v1",
    "title": "A Semi-Formal Verification Methodology for Efficient Configuration Coverage of Highly Configurable Digital Designs",
    "authors": [
      "Aman Kumar",
      "Sebastian Simon"
    ],
    "abstract": "Nowadays, a majority of System-on-Chips (SoCs) make use of Intellectual\nProperty (IP) in order to shorten development cycles. When such IPs are\ndeveloped, one of the main focuses lies in the high configurability of the\ndesign. This flexibility on the design side introduces the challenge of\ncovering a huge state space of IP configurations on the verification side to\nensure the functional correctness under every possible parameter setting. The\nvast number of possibilities does not allow a brute-force approach, and\ntherefore, only a selected number of settings based on typical and extreme\nassumptions are usually verified. Especially in automotive applications, which\nneed to follow the ISO 26262 functional safety standard, the requirement of\ncovering all significant variants needs to be fulfilled in any case.\nState-of-the-Art existing verification techniques such as simulation-based\nverification and formal verification have challenges such as time-space\nexplosion and state-space explosion respectively and therefore, lack behind in\nverifying highly configurable digital designs efficiently. This paper is\nfocused on a semi-formal verification methodology for efficient configuration\ncoverage of highly configurable digital designs. The methodology focuses on\nreduced runtime based on simulative and formal methods that allow high\nconfiguration coverage. The paper also presents the results when the developed\nmethodology was applied on a highly configurable microprocessor IP and\ndiscusses the gained benefits.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.SE",
    "comment": "Published in DVCon U.S. 2021",
    "pdf_url": "http://arxiv.org/pdf/2405.01572v1",
    "published_date": "2024-04-20 12:18:47 UTC",
    "updated_date": "2024-04-20 12:18:47 UTC"
  },
  {
    "arxiv_id": "2404.13362v1",
    "title": "Semantically Corrected Amharic Automatic Speech Recognition",
    "authors": [
      "Samuael Adnew",
      "Paul Pu Liang"
    ],
    "abstract": "Automatic Speech Recognition (ASR) can play a crucial role in enhancing the\naccessibility of spoken languages worldwide. In this paper, we build a set of\nASR tools for Amharic, a language spoken by more than 50 million people\nprimarily in eastern Africa. Amharic is written in the Ge'ez script, a sequence\nof graphemes with spacings denoting word boundaries. This makes computational\nprocessing of Amharic challenging since the location of spacings can\nsignificantly impact the meaning of formed sentences. We find that existing\nbenchmarks for Amharic ASR do not account for these spacings and only measure\nindividual grapheme error rates, leading to significantly inflated measurements\nof in-the-wild performance. In this paper, we first release corrected\ntranscriptions of existing Amharic ASR test datasets, enabling the community to\naccurately evaluate progress. Furthermore, we introduce a post-processing\napproach using a transformer encoder-decoder architecture to organize raw ASR\noutputs into a grammatically complete and semantically meaningful Amharic\nsentence. Through experiments on the corrected test dataset, our model enhances\nthe semantic correctness of Amharic speech recognition systems, achieving a\nCharacter Error Rate (CER) of 5.5\\% and a Word Error Rate (WER) of 23.3\\%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13362v1",
    "published_date": "2024-04-20 12:08:00 UTC",
    "updated_date": "2024-04-20 12:08:00 UTC"
  },
  {
    "arxiv_id": "2404.13358v1",
    "title": "Music Consistency Models",
    "authors": [
      "Zhengcong Fei",
      "Mingyuan Fan",
      "Junshi Huang"
    ],
    "abstract": "Consistency models have exhibited remarkable capabilities in facilitating\nefficient image/video generation, enabling synthesis with minimal sampling\nsteps. It has proven to be advantageous in mitigating the computational burdens\nassociated with diffusion models. Nevertheless, the application of consistency\nmodels in music generation remains largely unexplored. To address this gap, we\npresent Music Consistency Models (\\texttt{MusicCM}), which leverages the\nconcept of consistency models to efficiently synthesize mel-spectrogram for\nmusic clips, maintaining high quality while minimizing the number of sampling\nsteps. Building upon existing text-to-music diffusion models, the\n\\texttt{MusicCM} model incorporates consistency distillation and adversarial\ndiscriminator training. Moreover, we find it beneficial to generate extended\ncoherent music by incorporating multiple diffusion processes with shared\nconstraints. Experimental results reveal the effectiveness of our model in\nterms of computational efficiency, fidelity, and naturalness. Notable,\n\\texttt{MusicCM} achieves seamless music synthesis with a mere four sampling\nsteps, e.g., only one second per minute of the music clip, showcasing the\npotential for real-time application.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13358v1",
    "published_date": "2024-04-20 11:52:30 UTC",
    "updated_date": "2024-04-20 11:52:30 UTC"
  },
  {
    "arxiv_id": "2404.16870v1",
    "title": "LEMDA: A Novel Feature Engineering Method for Intrusion Detection in IoT Systems",
    "authors": [
      "Ali Ghubaish",
      "Zebo Yang",
      "Aiman Erbad",
      "Raj Jain"
    ],
    "abstract": "Intrusion detection systems (IDS) for the Internet of Things (IoT) systems\ncan use AI-based models to ensure secure communications. IoT systems tend to\nhave many connected devices producing massive amounts of data with high\ndimensionality, which requires complex models. Complex models have notorious\nproblems such as overfitting, low interpretability, and high computational\ncomplexity. Adding model complexity penalty (i.e., regularization) can ease\noverfitting, but it barely helps interpretability and computational efficiency.\nFeature engineering can solve these issues; hence, it has become critical for\nIDS in large-scale IoT systems to reduce the size and dimensionality of data,\nresulting in less complex models with excellent performance, smaller data\nstorage, and fast detection. This paper proposes a new feature engineering\nmethod called LEMDA (Light feature Engineering based on the Mean Decrease in\nAccuracy). LEMDA applies exponential decay and an optional sensitivity factor\nto select and create the most informative features. The proposed method has\nbeen evaluated and compared to other feature engineering methods using three\nIoT datasets and four AI/ML models. The results show that LEMDA improves the F1\nscore performance of all the IDS models by an average of 34% and reduces the\naverage training and detection times in most cases.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.16870v1",
    "published_date": "2024-04-20 11:11:47 UTC",
    "updated_date": "2024-04-20 11:11:47 UTC"
  },
  {
    "arxiv_id": "2404.13347v1",
    "title": "Augmenting Safety-Critical Driving Scenarios while Preserving Similarity to Expert Trajectories",
    "authors": [
      "Hamidreza Mirkhani",
      "Behzad Khamidehi",
      "Kasra Rezaee"
    ],
    "abstract": "Trajectory augmentation serves as a means to mitigate distributional shift in\nimitation learning. However, imitating trajectories that inadequately represent\nthe original expert data can result in undesirable behaviors, particularly in\nsafety-critical scenarios. We propose a trajectory augmentation method designed\nto maintain similarity with expert trajectory data. To accomplish this, we\nfirst cluster trajectories to identify minority yet safety-critical groups.\nThen, we combine the trajectories within the same cluster through geometrical\ntransformation to create new trajectories. These trajectories are then added to\nthe training dataset, provided that they meet our specified safety-related\ncriteria. Our experiments exhibit that training an imitation learning model\nusing these augmented trajectories can significantly improve closed-loop\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to 35th IEEE Intelligent Vehicles Symposium, 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.13347v1",
    "published_date": "2024-04-20 11:05:47 UTC",
    "updated_date": "2024-04-20 11:05:47 UTC"
  },
  {
    "arxiv_id": "2404.13344v2",
    "title": "GRANOLA: Adaptive Normalization for Graph Neural Networks",
    "authors": [
      "Moshe Eliasof",
      "Beatrice Bevilacqua",
      "Carola-Bibiane Schönlieb",
      "Haggai Maron"
    ],
    "abstract": "In recent years, significant efforts have been made to refine the design of\nGraph Neural Network (GNN) layers, aiming to overcome diverse challenges, such\nas limited expressive power and oversmoothing. Despite their widespread\nadoption, the incorporation of off-the-shelf normalization layers like\nBatchNorm or InstanceNorm within a GNN architecture may not effectively capture\nthe unique characteristics of graph-structured data, potentially reducing the\nexpressive power of the overall architecture. Moreover, existing graph-specific\nnormalization layers often struggle to offer substantial and consistent\nbenefits. In this paper, we propose GRANOLA, a novel graph-adaptive\nnormalization layer. Unlike existing normalization layers, GRANOLA normalizes\nnode features by adapting to the specific characteristics of the graph,\nparticularly by generating expressive representations of its neighborhood\nstructure, obtained by leveraging the propagation of Random Node Features (RNF)\nin the graph. We present theoretical results that support our design choices.\nOur extensive empirical evaluation of various graph benchmarks underscores the\nsuperior performance of GRANOLA over existing normalization techniques.\nFurthermore, GRANOLA emerges as the top-performing method among all baselines\nwithin the same time complexity of Message Passing Neural Networks (MPNNs).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2404.13344v2",
    "published_date": "2024-04-20 10:44:13 UTC",
    "updated_date": "2024-10-31 23:12:29 UTC"
  },
  {
    "arxiv_id": "2404.13343v1",
    "title": "UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions",
    "authors": [
      "Ana-Cristina Rogoz",
      "Radu Tudor Ionescu"
    ],
    "abstract": "This work explores a novel data augmentation method based on Large Language\nModels (LLMs) for predicting item difficulty and response time of retired USMLE\nMultiple-Choice Questions (MCQs) in the BEA 2024 Shared Task. Our approach is\nbased on augmenting the dataset with answers from zero-shot LLMs (Falcon,\nMeditron, Mistral) and employing transformer-based models based on six\nalternative feature combinations. The results suggest that predicting the\ndifficulty of questions is more challenging. Notably, our top performing\nmethods consistently include the question text, and benefit from the\nvariability of LLM answers, highlighting the potential of LLMs for improving\nautomated assessment in medical licensing exams. We make our code available\nhttps://github.com/ana-rogoz/BEA-2024.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at BEA 2024 (NAACL Workshop)",
    "pdf_url": "http://arxiv.org/pdf/2404.13343v1",
    "published_date": "2024-04-20 10:41:02 UTC",
    "updated_date": "2024-04-20 10:41:02 UTC"
  },
  {
    "arxiv_id": "2404.13340v1",
    "title": "Large Language Models as Test Case Generators: Performance Evaluation and Enhancement",
    "authors": [
      "Kefan Li",
      "Yuan Yuan"
    ],
    "abstract": "Code generation with Large Language Models (LLMs) has been extensively\nstudied and achieved remarkable progress. As a complementary aspect to code\ngeneration, test case generation is of crucial importance in ensuring the\nquality and reliability of code. However, using LLMs as test case generators\nhas been much less explored. Current research along this line primarily focuses\non enhancing code generation with assistance from test cases generated by LLMs,\nwhile the performance of LLMs in test case generation alone has not been\ncomprehensively examined. To bridge this gap, we conduct extensive experiments\nto study how well LLMs can generate high-quality test cases. We find that as\nthe problem difficulty increases, state-of-the-art LLMs struggle to generate\ncorrect test cases, largely due to their inherent limitations in computation\nand reasoning. To mitigate this issue, we further propose a multi-agent\nframework called \\emph{TestChain} that decouples the generation of test inputs\nand test outputs. Notably, TestChain uses a ReAct format conversation chain for\nLLMs to interact with a Python interpreter in order to provide more accurate\ntest outputs. Our results indicate that TestChain outperforms the baseline by a\nlarge margin. Particularly, in terms of the accuracy of test cases, TestChain\nusing GPT-4 as the backbone achieves a 13.84\\% improvement over the baseline on\nthe LeetCode-hard dataset.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13340v1",
    "published_date": "2024-04-20 10:27:01 UTC",
    "updated_date": "2024-04-20 10:27:01 UTC"
  },
  {
    "arxiv_id": "2404.13327v2",
    "title": "Comparative Analysis on Snowmelt-Driven Streamflow Forecasting Using Machine Learning Techniques",
    "authors": [
      "Ukesh Thapa",
      "Bipun Man Pati",
      "Samit Thapa",
      "Dhiraj Pyakurel",
      "Anup Shrestha"
    ],
    "abstract": "The rapid advancement of machine learning techniques has led to their\nwidespread application in various domains including water resources. However,\nsnowmelt modeling remains an area that has not been extensively explored. In\nthis study, we propose a state-of-the-art (SOTA) deep learning sequential\nmodel, leveraging the Temporal Convolutional Network (TCN), for snowmelt-driven\ndischarge modeling in the Himalayan basin of the Hindu Kush Himalayan Region.\nTo evaluate the performance of our proposed model, we conducted a comparative\nanalysis with other popular models including Support Vector Regression (SVR),\nLong Short Term Memory (LSTM), and Transformer. Furthermore, Nested\ncross-validation (CV) is used with five outer folds and three inner folds, and\nhyper-parameter tuning is performed on the inner folds. To evaluate the\nperformance of the model mean absolute error (MAE), root mean square error\n(RMSE), R square ($R^{2}$), Kling-Gupta Efficiency (KGE), and Nash-Sutcliffe\nEfficiency (NSE) are computed for each outer fold. The average metrics revealed\nthat TCN outperformed the other models, with an average MAE of 0.011, RMSE of\n0.023, $R^{2}$ of 0.991, KGE of 0.992, and NSE of 0.991. The findings of this\nstudy demonstrate the effectiveness of the deep learning model as compared to\ntraditional machine learning approaches for snowmelt-driven streamflow\nforecasting. Moreover, the superior performance of TCN highlights its potential\nas a promising deep learning model for similar hydrological applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 4 Tables, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.13327v2",
    "published_date": "2024-04-20 09:02:50 UTC",
    "updated_date": "2024-04-23 05:32:11 UTC"
  },
  {
    "arxiv_id": "2404.13322v3",
    "title": "MergeNet: Knowledge Migration across Heterogeneous Models, Tasks, and Modalities",
    "authors": [
      "Kunxi Li",
      "Tianyu Zhan",
      "Kairui Fu",
      "Shengyu Zhang",
      "Kun Kuang",
      "Jiwei Li",
      "Zhou Zhao",
      "Fan Wu",
      "Fei Wu"
    ],
    "abstract": "In this study, we focus on heterogeneous knowledge transfer across entirely\ndifferent model architectures, tasks, and modalities. Existing knowledge\ntransfer methods (e.g., backbone sharing, knowledge distillation) often hinge\non shared elements within model structures or task-specific features/labels,\nlimiting transfers to complex model types or tasks. To overcome these\nchallenges, we present MergeNet, which learns to bridge the gap of parameter\nspaces of heterogeneous models, facilitating the direct interaction,\nextraction, and application of knowledge within these parameter spaces. The\ncore mechanism of MergeNet lies in the parameter adapter, which operates by\nquerying the source model's low-rank parameters and adeptly learning to\nidentify and map parameters into the target model. MergeNet is learned\nalongside both models, allowing our framework to dynamically transfer and adapt\nknowledge relevant to the current stage, including the training trajectory\nknowledge of the source model. Extensive experiments on heterogeneous knowledge\ntransfer demonstrate significant improvements in challenging settings, where\nrepresentative approaches may falter or prove less applicable.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13322v3",
    "published_date": "2024-04-20 08:34:39 UTC",
    "updated_date": "2024-12-25 06:32:27 UTC"
  },
  {
    "arxiv_id": "2404.13320v2",
    "title": "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than We Think",
    "authors": [
      "Haotian Xue",
      "Yongxin Chen"
    ],
    "abstract": "Adversarial examples for diffusion models are widely used as solutions for\nsafety concerns. By adding adversarial perturbations to personal images,\nattackers can not edit or imitate them easily. However, it is essential to note\nthat all these protections target the latent diffusion model (LDMs), the\nadversarial examples for diffusion models in the pixel space (PDMs) are largely\noverlooked. This may mislead us to think that the diffusion models are\nvulnerable to adversarial attacks like most deep models. In this paper, we show\nnovel findings that: even though gradient-based white-box attacks can be used\nto attack the LDMs, they fail to attack PDMs. This finding is supported by\nextensive experiments of almost a wide range of attacking methods on various\nPDMs and LDMs with different model structures, which means diffusion models are\nindeed much more robust against adversarial attacks. We also find that PDMs can\nbe used as an off-the-shelf purifier to effectively remove the adversarial\npatterns that were generated on LDMs to protect the images, which means that\nmost protection methods nowadays, to some extent, cannot protect our images\nfrom malicious attacks. We hope that our insights will inspire the community to\nrethink the adversarial samples for diffusion models as protection methods and\nmove forward to more effective protection. Codes are available in\nhttps://github.com/xavihart/PDM-Pure.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13320v2",
    "published_date": "2024-04-20 08:28:43 UTC",
    "updated_date": "2024-05-02 02:25:39 UTC"
  },
  {
    "arxiv_id": "2404.14445v1",
    "title": "A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models",
    "authors": [
      "Yefeng Yuan",
      "Yuhong Liu",
      "Liang Cheng"
    ],
    "abstract": "The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 1 figure, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2404.14445v1",
    "published_date": "2024-04-20 08:08:28 UTC",
    "updated_date": "2024-04-20 08:08:28 UTC"
  },
  {
    "arxiv_id": "2404.13292v1",
    "title": "Evaluating Subword Tokenization: Alien Subword Composition and OOV Generalization Challenge",
    "authors": [
      "Khuyagbaatar Batsuren",
      "Ekaterina Vylomova",
      "Verna Dankers",
      "Tsetsuukhei Delgerbaatar",
      "Omri Uzan",
      "Yuval Pinter",
      "Gábor Bella"
    ],
    "abstract": "The popular subword tokenizers of current language models, such as Byte-Pair\nEncoding (BPE), are known not to respect morpheme boundaries, which affects the\ndownstream performance of the models. While many improved tokenization\nalgorithms have been proposed, their evaluation and cross-comparison is still\nan open problem. As a solution, we propose a combined intrinsic-extrinsic\nevaluation framework for subword tokenization. Intrinsic evaluation is based on\nour new UniMorph Labeller tool that classifies subword tokenization as either\nmorphological or alien. Extrinsic evaluation, in turn, is performed via the\nOut-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of\nthree newly specified downstream text classification tasks. Our empirical\nfindings show that the accuracy of UniMorph Labeller is 98%, and that, in all\nlanguage models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien\ntokenization leads to poorer generalizations compared to morphological\ntokenization for semantic compositionality of word meanings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.13292v1",
    "published_date": "2024-04-20 06:49:15 UTC",
    "updated_date": "2024-04-20 06:49:15 UTC"
  },
  {
    "arxiv_id": "2405.02320v1",
    "title": "A SER-based Device Selection Mechanism in Multi-bits Quantization Federated Learning",
    "authors": [
      "Pengcheng Sun",
      "Erwu Liu",
      "Rui Wang"
    ],
    "abstract": "The quality of wireless communication will directly affect the performance of\nfederated learning (FL), so this paper analyze the influence of wireless\ncommunication on FL through symbol error rate (SER). In FL system,\nnon-orthogonal multiple access (NOMA) can be used as the basic communication\nframework to reduce the communication congestion and interference caused by\nmultiple users, which takes advantage of the superposition characteristics of\nwireless channels. The Minimum Mean Square Error (MMSE) based serial\ninterference cancellation (SIC) technology is used to recover the gradient of\neach terminal node one by one at the receiving end. In this paper, the gradient\nparameters are quantized into multiple bits to retain more gradient information\nto the maximum extent and to improve the tolerance of transmission errors. On\nthis basis, we designed the SER-based device selection mechanism (SER-DSM) to\nensure that the learning performance is not affected by users with bad\ncommunication conditions, while accommodating as many users as possible to\nparticipate in the learning process, which is inclusive to a certain extent.\nThe experiments show the influence of multi-bit quantization of gradient on FL\nand the necessity and superiority of the proposed SER-based device selection\nmechanism.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "math.IT"
    ],
    "primary_category": "cs.IT",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02320v1",
    "published_date": "2024-04-20 06:27:01 UTC",
    "updated_date": "2024-04-20 06:27:01 UTC"
  },
  {
    "arxiv_id": "2404.13278v1",
    "title": "Federated Transfer Learning with Task Personalization for Condition Monitoring in Ultrasonic Metal Welding",
    "authors": [
      "Ahmadreza Eslaminia",
      "Yuquan Meng",
      "Klara Nahrstedt",
      "Chenhui Shao"
    ],
    "abstract": "Ultrasonic metal welding (UMW) is a key joining technology with widespread\nindustrial applications. Condition monitoring (CM) capabilities are critically\nneeded in UMW applications because process anomalies significantly deteriorate\nthe joining quality. Recently, machine learning models emerged as a promising\ntool for CM in many manufacturing applications due to their ability to learn\ncomplex patterns. Yet, the successful deployment of these models requires\nsubstantial training data that may be expensive and time-consuming to collect.\nAdditionally, many existing machine learning models lack generalizability and\ncannot be directly applied to new process configurations (i.e., domains). Such\nissues may be potentially alleviated by pooling data across manufacturers, but\ndata sharing raises critical data privacy concerns. To address these\nchallenges, this paper presents a Federated Transfer Learning with Task\nPersonalization (FTL-TP) framework that provides domain generalization\ncapabilities in distributed learning while ensuring data privacy. By\neffectively learning a unified representation from feature space, FTL-TP can\nadapt CM models for clients working on similar tasks, thereby enhancing their\noverall adaptability and performance jointly. To demonstrate the effectiveness\nof FTL-TP, we investigate two distinct UMW CM tasks, tool condition monitoring\nand workpiece surface condition classification. Compared with state-of-the-art\nFL algorithms, FTL-TP achieves a 5.35%--8.08% improvement of accuracy in CM in\nnew target domains. FTL-TP is also shown to perform excellently in challenging\nscenarios involving unbalanced data distributions and limited client fractions.\nFurthermore, by implementing the FTL-TP method on an edge-cloud architecture,\nwe show that this method is both viable and efficient in practice. The FTL-TP\nframework is readily extensible to various other manufacturing applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "37 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.13278v1",
    "published_date": "2024-04-20 05:31:59 UTC",
    "updated_date": "2024-04-20 05:31:59 UTC"
  },
  {
    "arxiv_id": "2404.13274v4",
    "title": "Augmented Object Intelligence: Making the Analog World Interactable with XR-Objects",
    "authors": [
      "Mustafa Doga Dogan",
      "Eric J. Gonzalez",
      "Karan Ahuja",
      "Ruofei Du",
      "Andrea Colaço",
      "Johnny Lee",
      "Mar Gonzalez-Franco",
      "David Kim"
    ],
    "abstract": "Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper introduces Augmented\nObject Intelligence (AOI), a novel XR interaction paradigm designed to blur the\nlines between digital and physical by equipping real-world objects with the\nability to interact as if they were digital, where every object has the\npotential to serve as a portal to vast digital functionalities. Our approach\nutilizes object segmentation and classification, combined with the power of\nMultimodal Large Language Models (MLLMs), to facilitate these interactions. We\nimplement the AOI concept in the form of XR-Objects, an open-source prototype\nsystem that provides a platform for users to engage with their physical\nenvironment in rich and contextually relevant ways. This system enables analog\nobjects to not only convey information but also to initiate digital actions,\nsuch as querying for details or executing tasks. Our contributions are\nthreefold: (1) we define the AOI concept and detail its advantages over\ntraditional AI assistants, (2) detail the XR-Objects system's open-source\ndesign and implementation, and (3) show its versatility through a variety of\nuse cases and a user study.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.0; H.5.1; H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "2024 ACM Symposium on User Interface Software and Technology (UIST)",
    "pdf_url": "http://arxiv.org/pdf/2404.13274v4",
    "published_date": "2024-04-20 05:14:52 UTC",
    "updated_date": "2025-03-18 23:29:40 UTC"
  },
  {
    "arxiv_id": "2404.14444v1",
    "title": "Practical Battery Health Monitoring using Uncertainty-Aware Bayesian Neural Network",
    "authors": [
      "Yunyi Zhao",
      "Zhang Wei",
      "Qingyu Yan",
      "Man-Fai Ng",
      "B. Sivaneasan",
      "Cheng Xiang"
    ],
    "abstract": "Battery health monitoring and prediction are critically important in the era\nof electric mobility with a huge impact on safety, sustainability, and economic\naspects. Existing research often focuses on prediction accuracy but tends to\nneglect practical factors that may hinder the technology's deployment in\nreal-world applications. In this paper, we address these practical\nconsiderations and develop models based on the Bayesian neural network for\npredicting battery end-of-life. Our models use sensor data related to battery\nhealth and apply distributions, rather than single-point, for each parameter of\nthe models. This allows the models to capture the inherent randomness and\nuncertainty of battery health, which leads to not only accurate predictions but\nalso quantifiable uncertainty. We conducted an experimental study and\ndemonstrated the effectiveness of our proposed models, with a prediction error\nrate averaging 13.9%, and as low as 2.9% for certain tested batteries.\nAdditionally, all predictions include quantifiable certainty, which improved by\n66% from the initial to the mid-life stage of the battery. This research has\npractical values for battery technologies and contributes to accelerating the\ntechnology adoption in the industry.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2404.14444v1",
    "published_date": "2024-04-20 05:13:14 UTC",
    "updated_date": "2024-04-20 05:13:14 UTC"
  },
  {
    "arxiv_id": "2404.13265v1",
    "title": "F5C-finder: An Explainable and Ensemble Biological Language Model for Predicting 5-Formylcytidine Modifications on mRNA",
    "authors": [
      "Guohao Wang",
      "Ting Liu",
      "Hongqiang Lyu",
      "Ze Liu"
    ],
    "abstract": "As a prevalent and dynamically regulated epigenetic modification,\n5-formylcytidine (f5C) is crucial in various biological processes. However,\ntraditional experimental methods for f5C detection are often laborious and\ntime-consuming, limiting their ability to map f5C sites across the\ntranscriptome comprehensively. While computational approaches offer a\ncost-effective and high-throughput alternative, no recognition model for f5C\nhas been developed to date. Drawing inspiration from language models in natural\nlanguage processing, this study presents f5C-finder, an ensemble neural\nnetwork-based model utilizing multi-head attention for the identification of\nf5C. Five distinct feature extraction methods were employed to construct five\nindividual artificial neural networks, and these networks were subsequently\nintegrated through ensemble learning to create f5C-finder. 10-fold\ncross-validation and independent tests demonstrate that f5C-finder achieves\nstate-of-the-art (SOTA) performance with AUC of 0.807 and 0.827, respectively.\nThe result highlights the effectiveness of biological language model in\ncapturing both the order (sequential) and functional meaning (semantics) within\ngenomes. Furthermore, the built-in interpretability allows us to understand\nwhat the model is learning, creating a bridge between identifying key\nsequential elements and a deeper exploration of their biological functions.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.GN",
    "comment": "34 pages, 10 figures, journal",
    "pdf_url": "http://arxiv.org/pdf/2404.13265v1",
    "published_date": "2024-04-20 04:24:45 UTC",
    "updated_date": "2024-04-20 04:24:45 UTC"
  },
  {
    "arxiv_id": "2404.14443v1",
    "title": "Evaluation of Machine Translation Based on Semantic Dependencies and Keywords",
    "authors": [
      "Kewei Yuan",
      "Qiurong Zhao",
      "Yang Xu",
      "Xiao Zhang",
      "Huansheng Ning"
    ],
    "abstract": "In view of the fact that most of the existing machine translation evaluation\nalgorithms only consider the lexical and syntactic information, but ignore the\ndeep semantic information contained in the sentence, this paper proposes a\ncomputational method for evaluating the semantic correctness of machine\ntranslations based on reference translations and incorporating semantic\ndependencies and sentence keyword information. Use the language technology\nplatform developed by the Social Computing and Information Retrieval Research\nCenter of Harbin Institute of Technology to conduct semantic dependency\nanalysis and keyword analysis on sentences, and obtain semantic dependency\ngraphs, keywords, and weight information corresponding to keywords. It includes\nall word information with semantic dependencies in the sentence and keyword\ninformation that affects semantic information. Construct semantic association\npairs including word and dependency multi-features. The key semantics of the\nsentence cannot be highlighted in the semantic information extracted through\nsemantic dependence, resulting in vague semantics analysis. Therefore, the\nsentence keyword information is also included in the scope of machine\ntranslation semantic evaluation. To achieve a comprehensive and in-depth\nevaluation of the semantic correctness of sentences, the experimental results\nshow that the accuracy of the evaluation algorithm has been improved compared\nwith similar methods, and it can more accurately measure the semantic\ncorrectness of machine translation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14443v1",
    "published_date": "2024-04-20 04:14:28 UTC",
    "updated_date": "2024-04-20 04:14:28 UTC"
  },
  {
    "arxiv_id": "2405.05142v1",
    "title": "Ordinal Behavior Classification of Student Online Course Interactions",
    "authors": [
      "Thomas Trask"
    ],
    "abstract": "The study in interaction patterns between students in on-campus and\nMOOC-style online courses has been broadly studied for the last 11 years. Yet\nthere remains a gap in the literature comparing the habits of students\ncompleting the same course offered in both on-campus and MOOC-style online\nformats. This study will look at browser-based usage patterns for students in\nthe Georgia Tech CS1301 edx course for both the online course offered to\non-campus students and the MOOCstyle course offered to anyone to determine\nwhat, if any, patterns exist between the two cohorts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages, 4 tables, 6 figures. Submitted to CSEDM Workshop @ EDM 24",
    "pdf_url": "http://arxiv.org/pdf/2405.05142v1",
    "published_date": "2024-04-20 02:34:03 UTC",
    "updated_date": "2024-04-20 02:34:03 UTC"
  },
  {
    "arxiv_id": "2404.13238v1",
    "title": "Personalized Wireless Federated Learning for Large Language Models",
    "authors": [
      "Feibo Jiang",
      "Li Dong",
      "Siwei Tu",
      "Yubo Peng",
      "Kezhi Wang",
      "Kun Yang",
      "Cunhua Pan",
      "Dusit Niyato"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing\ntasks. However, their deployment in wireless networks still face challenges,\ni.e., a lack of privacy and security protection mechanisms. Federated Learning\n(FL) has emerged as a promising approach to address these challenges. Yet, it\nsuffers from issues including inefficient handling with big and heterogeneous\ndata, resource-intensive training, and high communication overhead. To tackle\nthese issues, we first compare different learning stages and their features of\nLLMs in wireless networks. Next, we introduce two personalized wireless\nfederated fine-tuning methods with low communication overhead, i.e., (1)\nPersonalized Federated Instruction Tuning (PFIT), which employs reinforcement\nlearning to fine-tune local LLMs with diverse reward models to achieve\npersonalization; (2) Personalized Federated Task Tuning (PFTT), which can\nleverage global adapters and local Low-Rank Adaptations (LoRA) to\ncollaboratively fine-tune local LLMs, where the local LoRAs can be applied to\nachieve personalization without aggregation. Finally, we perform simulations to\ndemonstrate the effectiveness of the proposed two methods and comprehensively\ndiscuss open issues.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2404.13238v1",
    "published_date": "2024-04-20 02:30:21 UTC",
    "updated_date": "2024-04-20 02:30:21 UTC"
  },
  {
    "arxiv_id": "2404.14442v3",
    "title": "Unified ODE Analysis of Smooth Q-Learning Algorithms",
    "authors": [
      "Donghwan Lee"
    ],
    "abstract": "Convergence of Q-learning has been the focus of extensive research over the\npast several decades. Recently, an asymptotic convergence analysis for\nQ-learning was introduced using a switching system framework. This approach\napplies the so-called ordinary differential equation (ODE) approach to prove\nthe convergence of the asynchronous Q-learning modeled as a continuous-time\nswitching system, where notions from switching system theory are used to prove\nits asymptotic stability without using explicit Lyapunov arguments. However, to\nprove stability, restrictive conditions, such as quasi-monotonicity, must be\nsatisfied for the underlying switching systems, which makes it hard to easily\ngeneralize the analysis method to other reinforcement learning algorithms, such\nas the smooth Q-learning variants. In this paper, we present a more general and\nunified convergence analysis that improves upon the switching system approach\nand can analyze Q-learning and its smooth variants. The proposed analysis is\nmotivated by previous work on the convergence of synchronous Q-learning based\non $p$-norm serving as a Lyapunov function. However, the proposed analysis\naddresses more general ODE models that can cover both asynchronous Q-learning\nand its smooth versions with simpler frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14442v3",
    "published_date": "2024-04-20 01:16:27 UTC",
    "updated_date": "2025-03-28 11:38:47 UTC"
  },
  {
    "arxiv_id": "2404.14441v1",
    "title": "Optimizing Contrail Detection: A Deep Learning Approach with EfficientNet-b4 Encoding",
    "authors": [
      "Qunwei Lin",
      "Qian Leng",
      "Zhicheng Ding",
      "Chao Yan",
      "Xiaonan Xu"
    ],
    "abstract": "In the pursuit of environmental sustainability, the aviation industry faces\nthe challenge of minimizing its ecological footprint. Among the key solutions\nis contrail avoidance, targeting the linear ice-crystal clouds produced by\naircraft exhaust. These contrails exacerbate global warming by trapping\natmospheric heat, necessitating precise segmentation and comprehensive analysis\nof contrail images to gauge their environmental impact. However, this\nsegmentation task is complex due to the varying appearances of contrails under\ndifferent atmospheric conditions and potential misalignment issues in\npredictive modeling. This paper presents an innovative deep-learning approach\nutilizing the efficient net-b4 encoder for feature extraction, seamlessly\nintegrating misalignment correction, soft labeling, and pseudo-labeling\ntechniques to enhance the accuracy and efficiency of contrail detection in\nsatellite imagery. The proposed methodology aims to redefine contrail image\nanalysis and contribute to the objectives of sustainable aviation by providing\na robust framework for precise contrail detection and analysis in satellite\nimagery, thus aiding in the mitigation of aviation's environmental impact.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2404.14441v1",
    "published_date": "2024-04-20 00:21:06 UTC",
    "updated_date": "2024-04-20 00:21:06 UTC"
  }
]
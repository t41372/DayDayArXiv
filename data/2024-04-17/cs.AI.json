{
  "date": "2024-04-17",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-17 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 105 篇论文，主要聚焦 AI 模型优化（如 LLM 和联邦学习）、图像处理（如图像生成和分割）、强化学习等领域，亮点包括 LLM 在多任务中的高效应用（如 Prompt-Guided Generation）和联邦学习的新框架（如 FedFa），以及一些知名学者（如 Aleksander Madry）参与的创新工作。\n\n下面，我将挑选并简要讨论部分重要或有话题度的论文，先从 LLM 和 AI 优化这类热门主题入手，再聊图像处理和强化学习相关内容。其他较常规或小众论文（如一些纯理论分析或特定领域小改进）将快速掠过，仅列出标题和核心要点，以控制篇幅。\n\n### LLM 和 AI 模型优化\n- **Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM** (英文原标题: Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM)  \n  这篇论文提出了一种基于预训练 LLM 的框架，用于生成结构化的胸部 X 光报告，主要贡献是通过文本提示和图像融合提升报告的准确性和临床相关性，发现能显著改善医疗图像分析的实用性。\n\n- **Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System** (英文原标题: Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System)  \n  作者探索了 LLM 与协作过滤的结合，构建了一个高效推荐系统，主要发现 LLM 可以显著提升推荐性能，尤其在冷启动场景下，实现了高达 85% 的成本节省。\n\n- **LMEraser: Large Model Unlearning through Adaptive Prompt Tuning** (英文原标题: LMEraser: Large Model Unlearning through Adaptive Prompt Tuning)  \n  这篇工作提出 LMEraser 框架，用于 LLM 的机器遗忘，主要贡献是通过自适应提示调整实现高效数据移除，发现能将训练成本降低 100 倍，同时保持模型准确性。\n\n- **Embedding Privacy in Computational Social Science and Artificial Intelligence Research** (英文原标题: Embedding Privacy in Computational Social Science and Artificial Intelligence Research)  \n  论文讨论了在计算社会科学和 AI 研究中嵌入隐私保护，主要发现通过设计框架可有效防范数据泄露，强调了 LLM 在隐私敏感领域的实际应用。\n\n其他 AI 优化论文如 **FedFa: A Fully Asynchronous Training Paradigm for Federated Learning**，快速提一下：它提出了一种全异步联邦学习框架，主要贡献是消除同步等待，提高训练效率 6 倍。\n\n### 图像处理和生成\n- **MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for Infrared and Visible Image Fusion via Guided Training** (英文原标题: MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for Infrared and Visible Image Fusion via Guided Training)  \n  这篇论文引入 MaeFuse 模型，用于红外和可见光图像融合，主要发现通过预训练掩码自动编码器和引导训练，能实现高保真融合，提升图像质量。\n\n- **Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM** (已在前文提及，此处关联)  \n  与图像处理相关，扩展了 LLM 在医疗图像中的作用。\n\n其他图像相关论文如 **Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks**，主要贡献是多模态语义通信减少数据传输，但细节较常规，快速掠过。\n\n### 强化学习和机器人\n- **Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning** (英文原标题: Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning)  \n  论文提出了一种群组感知协调图框架，用于多代理强化学习，主要发现能捕获更高阶的代理间关系，提升在复杂环境（如 StarCraft II）中的性能。\n\n- **LTL-Constrained Policy Optimization with Cycle Experience Replay** (英文原标题: LTL-Constrained Policy Optimization with Cycle Experience Replay)  \n  这篇工作优化了强化学习的策略约束，主要贡献是通过循环经验重放处理稀疏奖励，发现能显著提高策略的鲁棒性和约束满足率。\n\n其他强化学习论文如 **Cross-Problem Learning for Solving Vehicle Routing Problems**，主要贡献是跨问题学习提升路由优化，但主题较窄，快速掠过。\n\n### 其他快速掠过\n剩余论文涉及联邦学习、图像分割、自然语言处理等领域，但许多是细枝末节改进，如 **Incremental Bootstrapping and Classification of Structured Scenes in a Fuzzy Ontology**（主要贡献是模糊本体论的增量分类），或 **A Survey on Semantic Modeling for Building Energy Management**（能源管理语义建模调查），这些不那么热门，仅列出标题和核心：它们提供了领域特定优化，但影响力有限。\n\n总之，今天的论文突显了 AI 模型（如 LLM）的灵活性和高效性，未来应用潜力巨大。更多细节可查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2404.11803v2",
      "title": "TempBEV: Improving Learned BEV Encoders with Combined Image and BEV Space Temporal Aggregation",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Monninger",
        "Vandana Dokkadi",
        "Md Zafar Anwar",
        "Steffen Staab"
      ],
      "abstract": "Autonomous driving requires an accurate representation of the environment. A\nstrategy toward high accuracy is to fuse data from several sensors. Learned\nBird's-Eye View (BEV) encoders can achieve this by mapping data from individual\nsensors into one joint latent space. For cost-efficient camera-only systems,\nthis provides an effective mechanism to fuse data from multiple cameras with\ndifferent views. Accuracy can further be improved by aggregating sensor\ninformation over time. This is especially important in monocular camera systems\nto account for the lack of explicit depth and velocity measurements. Thereby,\nthe effectiveness of developed BEV encoders crucially depends on the operators\nused to aggregate temporal information and on the used latent representation\nspaces. We analyze BEV encoders proposed in the literature and compare their\neffectiveness, quantifying the effects of aggregation operators and latent\nrepresentations. While most existing approaches aggregate temporal information\neither in image or in BEV latent space, our analyses and performance\ncomparisons suggest that these latent representations exhibit complementary\nstrengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which\nintegrates aggregated temporal information from both latent spaces. We consider\nsubsequent image frames as stereo through time and leverage methods from\noptical flow estimation for temporal stereo encoding. Empirical evaluation on\nthe NuScenes dataset shows a significant improvement by TempBEV over the\nbaseline for 3D object detection and BEV segmentation. The ablation uncovers a\nstrong synergy of joint temporal aggregation in the image and BEV latent space.\nThese results indicate the overall effectiveness of our approach and make a\nstrong case for aggregating temporal information in both image and BEV latent\nspaces.",
      "tldr_zh": "本研究提出 TempBEV，一种改进学习型 Bird's-Eye View (BEV) 编码器的框架，通过在图像空间和 BEV 空间结合时间聚合来提升自主驾驶系统的环境表示准确性。TempBEV 将后续图像帧视为时间上的立体视差，并利用光学流估计方法进行 temporal stereo 编码，从而融合多个摄像头数据并弥补单目系统缺乏深度和速度测量的不足。实验在 NuScenes 数据集上显示，TempBEV 在 3D 对象检测和 BEV 分割任务上显著优于基线模型，量化分析证明了在两个潜在空间中联合时间聚合的协同效应。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.11803v2",
      "published_date": "2024-04-17 23:49:00 UTC",
      "updated_date": "2024-09-19 03:21:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:07:30.538759"
    },
    {
      "arxiv_id": "2404.11800v1",
      "title": "Developing Situational Awareness for Joint Action with Autonomous Vehicles",
      "title_zh": "针对与自动驾驶车辆联合行动开发情境意识",
      "authors": [
        "Robert Kaufman",
        "David Kirsh",
        "Nadir Weibel"
      ],
      "abstract": "Unanswered questions about how human-AV interaction designers can support\nrider's informational needs hinders Autonomous Vehicles (AV) adoption. To\nachieve joint human-AV action goals - such as safe transportation, trust, or\nlearning from an AV - sufficient situational awareness must be held by the\nhuman, AV, and human-AV system collectively. We present a systems-level\nframework that integrates cognitive theories of joint action and situational\nawareness as a means to tailor communications that meet the criteria necessary\nfor goal success. This framework is based on four components of the shared\nsituation: AV traits, action goals, subject-specific traits and states, and the\nsituated driving context. AV communications should be tailored to these factors\nand be sensitive when they change. This framework can be useful for\nunderstanding individual, shared, and distributed human-AV situational\nawareness and designing for future AV communications that meet the\ninformational needs and goals of diverse groups and in diverse driving\ncontexts.",
      "tldr_zh": "本研究探讨了如何通过提升 situational awareness 来支持人类与 Autonomous Vehicles (AV) 的联合行动，以解决骑手资讯需求不足的问题，从而促进 AV 的采用。研究提出一个系统级框架，整合 joint action 和 situational awareness 的认知理论，该框架基于四个组件——AV traits、action goals、subject-specific traits and states 以及 situated driving context——来定制通讯策略，确保其适应动态变化。框架有助于理解个体、共享和分布式的 human-AV situational awareness，并为设计未来的 AV 通讯提供指导，以满足多样群体和驾驶情境下的目标，如安全运输和信任建立。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.11800v1",
      "published_date": "2024-04-17 23:41:48 UTC",
      "updated_date": "2024-04-17 23:41:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:07:41.320869"
    },
    {
      "arxiv_id": "2404.15360v1",
      "title": "Towards Robust and Interpretable EMG-based Hand Gesture Recognition using Deep Metric Meta Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Simon Tam",
        "Shriram Tallam Puranam Raghu",
        "Étienne Buteau",
        "Erik Scheme",
        "Mounir Boukadoum",
        "Alexandre Campeau-Lecours",
        "Benoit Gosselin"
      ],
      "abstract": "Current electromyography (EMG) pattern recognition (PR) models have been\nshown to generalize poorly in unconstrained environments, setting back their\nadoption in applications such as hand gesture control. This problem is often\ndue to limited training data, exacerbated by the use of supervised\nclassification frameworks that are known to be suboptimal in such settings. In\nthis work, we propose a shift to deep metric-based meta-learning in EMG PR to\nsupervise the creation of meaningful and interpretable representations. We use\na Siamese Deep Convolutional Neural Network (SDCNN) and contrastive triplet\nloss to learn an EMG feature embedding space that captures the distribution of\nthe different classes. A nearest-centroid approach is subsequently employed for\ninference, relying on how closely a test sample aligns with the established\ndata distributions. We derive a robust class proximity-based confidence\nestimator that leads to a better rejection of incorrect decisions, i.e. false\npositives, especially when operating beyond the training data domain. We show\nour approach's efficacy by testing the trained SDCNN's predictions and\nconfidence estimations on unseen data, both in and out of the training domain.\nThe evaluation metrics include the accuracy-rejection curve and the\nKullback-Leibler divergence between the confidence distributions of accurate\nand inaccurate predictions. Outperforming comparable models on both metrics,\nour results demonstrate that the proposed meta-learning approach improves the\nclassifier's precision in active decisions (after rejection), thus leading to\nbetter generalization and applicability.",
      "tldr_zh": "本文针对EMG-based手势识别模型在非约束环境中的泛化性差问题，提出采用Deep Metric Meta Learning方法，以创建有意义且可解释的特征表示。方法使用Siamese Deep Convolutional Neural Network (SDCNN)和contrastive triplet loss来学习EMG特征嵌入空间，随后通过nearest-centroid approach进行推理，并引入一个鲁棒的类近似基于置信度估计器来拒绝错误决策。实验结果显示，该方法在未见过的数据上表现优异，在准确率-拒绝曲线和Kullback-Leibler divergence指标上优于可比模型，从而提升了分类器的精确度、泛化和实际适用性。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "eess.SP",
      "comment": "11 pages, 9 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems",
      "pdf_url": "http://arxiv.org/pdf/2404.15360v1",
      "published_date": "2024-04-17 23:37:50 UTC",
      "updated_date": "2024-04-17 23:37:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:07:54.253875"
    },
    {
      "arxiv_id": "2404.11797v1",
      "title": "When are Foundation Models Effective? Understanding the Suitability for Pixel-Level Classification Using Multispectral Imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Yiqun Xie",
        "Zhihao Wang",
        "Weiye Chen",
        "Zhili Li",
        "Xiaowei Jia",
        "Yanhua Li",
        "Ruichen Wang",
        "Kangyang Chai",
        "Ruohan Li",
        "Sergii Skakun"
      ],
      "abstract": "Foundation models, i.e., very large deep learning models, have demonstrated\nimpressive performances in various language and vision tasks that are otherwise\ndifficult to reach using smaller-size models. The major success of GPT-type of\nlanguage models is particularly exciting and raises expectations on the\npotential of foundation models in other domains including satellite remote\nsensing. In this context, great efforts have been made to build foundation\nmodels to test their capabilities in broader applications, and examples include\nPrithvi by NASA-IBM, Segment-Anything-Model, ViT, etc. This leads to an\nimportant question: Are foundation models always a suitable choice for\ndifferent remote sensing tasks, and when or when not? This work aims to enhance\nthe understanding of the status and suitability of foundation models for\npixel-level classification using multispectral imagery at moderate resolution,\nthrough comparisons with traditional machine learning (ML) and regular-size\ndeep learning models. Interestingly, the results reveal that in many scenarios\ntraditional ML models still have similar or better performance compared to\nfoundation models, especially for tasks where texture is less useful for\nclassification. On the other hand, deep learning models did show more promising\nresults for tasks where labels partially depend on texture (e.g., burn scar),\nwhile the difference in performance between foundation models and deep learning\nmodels is not obvious. The results conform with our analysis: The suitability\nof foundation models depend on the alignment between the self-supervised\nlearning tasks and the real downstream tasks, and the typical masked\nautoencoder paradigm is not necessarily suitable for many remote sensing\nproblems.",
      "tldr_zh": "本研究探讨了 Foundation Models 在使用多光谱图像进行像素级分类任务中的适用性，通过与传统机器学习 (ML) 模型和常规深度学习模型的比较，评估其在中等分辨率遥感图像上的表现。结果显示，在许多场景中，传统 ML 模型的表现与 Foundation Models 相当或更好，特别是当分类任务不依赖纹理时；相反，深度学习模型在依赖纹理的 tasks（如烧伤疤痕识别）中表现出更大优势。总体而言，Foundation Models 的有效性取决于自监督学习任务（如 masked autoencoder 范式）与下游任务的匹配度，并非总是适合遥感问题。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11797v1",
      "published_date": "2024-04-17 23:30:48 UTC",
      "updated_date": "2024-04-17 23:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:08:05.928986"
    },
    {
      "arxiv_id": "2404.11795v1",
      "title": "Prompt-Driven Feature Diffusion for Open-World Semi-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Marzi Heidari",
        "Hanping Zhang",
        "Yuhong Guo"
      ],
      "abstract": "In this paper, we present a novel approach termed Prompt-Driven Feature\nDiffusion (PDFD) within a semi-supervised learning framework for Open World\nSemi-Supervised Learning (OW-SSL). At its core, PDFD deploys an efficient\nfeature-level diffusion model with the guidance of class-specific prompts to\nsupport discriminative feature representation learning and feature generation,\ntackling the challenge of the non-availability of labeled data for unseen\nclasses in OW-SSL. In particular, PDFD utilizes class prototypes as prompts in\nthe diffusion model, leveraging their class-discriminative and semantic\ngeneralization ability to condition and guide the diffusion process across all\nthe seen and unseen classes. Furthermore, PDFD incorporates a class-conditional\nadversarial loss for diffusion model training, ensuring that the features\ngenerated via the diffusion process can be discriminatively aligned with the\nclass-conditional features of the real data. Additionally, the class prototypes\nof the unseen classes are computed using only unlabeled instances with\nconfident predictions within a semi-supervised learning framework. We conduct\nextensive experiments to evaluate the proposed PDFD. The empirical results show\nPDFD exhibits remarkable performance enhancements over many state-of-the-art\nexisting methods.",
      "tldr_zh": "本论文提出了一种名为Prompt-Driven Feature Diffusion (PDFD)的新方法，用于Open World Semi-Supervised Learning (OW-SSL)，旨在通过特征级扩散模型和类特定提示引导来处理未见类别无标签数据的挑战。PDFD利用类原型作为提示来指导扩散过程，确保特征表示学习的判别性和语义泛化，同时引入类条件对抗损失，使生成的特征与真实数据特征对齐；未见类别的类原型则基于无标签实例的置信预测计算。实验结果显示，PDFD在多个基准上显著优于现有方法，展示了其在半监督学习中的性能提升。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11795v1",
      "published_date": "2024-04-17 23:10:11 UTC",
      "updated_date": "2024-04-17 23:10:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:08:16.559797"
    },
    {
      "arxiv_id": "2404.11792v2",
      "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
      "title_zh": "使用领域特定微调和",
      "authors": [
        "Zooey Nguyen",
        "Anthony Annunziata",
        "Vinh Luong",
        "Sang Dinh",
        "Quynh Le",
        "Anh Hai Ha",
        "Chanh Le",
        "Hong An Phan",
        "Shruti Raghavan",
        "Christopher Nguyen"
      ],
      "abstract": "This paper investigates the impact of domain-specific model fine-tuning and\nof reasoning mechanisms on the performance of question-answering (Q&A) systems\npowered by large language models (LLMs) and Retrieval-Augmented Generation\n(RAG). Using the FinanceBench SEC financial filings dataset, we observe that,\nfor RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves\nbetter accuracy than generic models, with relatively greater gains attributable\nto fine-tuned embedding models. Additionally, employing reasoning iterations on\ntop of RAG delivers an even bigger jump in performance, enabling the Q&A\nsystems to get closer to human-expert quality. We discuss the implications of\nsuch findings, propose a structured technical design space capturing major\ntechnical components of Q&A AI, and provide recommendations for making\nhigh-impact technical choices for such components. We plan to follow up on this\nwork with actionable guides for AI teams and further investigations into the\nimpact of domain-specific augmentation in RAG and into agentic AI capabilities\nsuch as advanced planning and reasoning.",
      "tldr_zh": "这篇论文通过比较研究，探讨了领域特定微调（Fine-Tuning）和迭代推理（Iterative Reasoning）对基于大型语言模型（LLMs）和检索增强生成（RAG）的问答（Q&A）系统的性能影响，使用 FinanceBench SEC 金融文件数据集进行实验。结果显示，结合微调嵌入模型和微调 LLM 的 RAG 系统比通用模型更准确，其中微调嵌入模型带来的收益更大，而添加推理迭代进一步显著提升性能，使系统接近人类专家水平。论文还提出了 Q&A AI 的结构化技术设计空间，提供高影响技术选择的推荐，并计划后续工作，包括行动指南和对领域特定增强以及代理 AI 能力的进一步调查。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Fixed typo of OODA's score on harder-question set in Table 2",
      "pdf_url": "http://arxiv.org/pdf/2404.11792v2",
      "published_date": "2024-04-17 23:00:03 UTC",
      "updated_date": "2024-04-19 20:28:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:08:30.610756"
    },
    {
      "arxiv_id": "2404.11782v1",
      "title": "REQUAL-LM: Reliability and Equity through Aggregation in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sana Ebrahimi",
        "Nima Shahbazi",
        "Abolfazl Asudeh"
      ],
      "abstract": "The extensive scope of large language models (LLMs) across various domains\nunderscores the critical importance of responsibility in their application,\nbeyond natural language processing. In particular, the randomized nature of\nLLMs, coupled with inherent biases and historical stereotypes in data, raises\ncritical concerns regarding reliability and equity. Addressing these challenges\nare necessary before using LLMs for applications with societal impact. Towards\naddressing this gap, we introduce REQUAL-LM, a novel method for finding\nreliable and equitable LLM outputs through aggregation. Specifically, we\ndevelop a Monte Carlo method based on repeated sampling to find a reliable\noutput close to the mean of the underlying distribution of possible outputs. We\nformally define the terms such as reliability and bias, and design an\nequity-aware aggregation to minimize harmful bias while finding a highly\nreliable output. REQUAL-LM does not require specialized hardware, does not\nimpose a significant computing load, and uses LLMs as a blackbox. This design\nchoice enables seamless scalability alongside the rapid advancement of LLM\ntechnologies. Our system does not require retraining the LLMs, which makes it\ndeployment ready and easy to adapt. Our comprehensive experiments using various\ntasks and datasets demonstrate that REQUAL- LM effectively mitigates bias and\nselects a more equitable response, specifically the outputs that properly\nrepresents minority groups.",
      "tldr_zh": "这篇论文提出了REQUAL-LM，一种通过输出聚合来提升大型语言模型(LLMs)可靠性和公平性的创新方法，旨在解决LLMs的随机性、固有偏见和历史刻板印象问题。方法包括使用Monte Carlo重复采样来生成接近均值的可靠输出，并设计公平感知聚合机制，以最小化有害偏见，同时将LLMs视为黑盒，无需专用硬件或重新训练。实验结果显示，REQUAL-LM在多种任务和数据集上有效缓解了偏见，并选择了更能代表少数群体的公平响应。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11782v1",
      "published_date": "2024-04-17 22:12:41 UTC",
      "updated_date": "2024-04-17 22:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:08:41.405853"
    },
    {
      "arxiv_id": "2404.11773v2",
      "title": "Behavior Alignment: A New Perspective of Evaluating LLM-based Conversational Recommender Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Dayu Yang",
        "Fumian Chen",
        "Hui Fang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated great potential in\nConversational Recommender Systems (CRS). However, the application of LLMs to\nCRS has exposed a notable discrepancy in behavior between LLM-based CRS and\nhuman recommenders: LLMs often appear inflexible and passive, frequently\nrushing to complete the recommendation task without sufficient inquiry.This\nbehavior discrepancy can lead to decreased accuracy in recommendations and\nlower user satisfaction. Despite its importance, existing studies in CRS lack a\nstudy about how to measure such behavior discrepancy. To fill this gap, we\npropose Behavior Alignment, a new evaluation metric to measure how well the\nrecommendation strategies made by a LLM-based CRS are consistent with human\nrecommenders'. Our experiment results show that the new metric is better\naligned with human preferences and can better differentiate how systems perform\nthan existing evaluation metrics. As Behavior Alignment requires explicit and\ncostly human annotations on the recommendation strategies, we also propose a\nclassification-based method to implicitly measure the Behavior Alignment based\non the responses. The evaluation results confirm the robustness of the method.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLM）在对话推荐系统（Conversational Recommender Systems, CRS）中的行为差异问题，即LLM往往过于僵化和被动，导致推荐准确性降低和用户满意度下降。作者提出Behavior Alignment作为一种新评估指标，用于衡量LLM-based CRS的推荐策略与人类推荐者的行为一致性，实验结果显示该指标更符合人类偏好，并能更好地区分系统性能。针对Behavior Alignment需要昂贵的人类标注，论文还开发了一种基于分类的隐式测量方法，其鲁棒性通过评估得到证实。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval",
      "pdf_url": "http://arxiv.org/pdf/2404.11773v2",
      "published_date": "2024-04-17 21:56:27 UTC",
      "updated_date": "2024-10-17 18:59:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:08:53.717208"
    },
    {
      "arxiv_id": "2404.11770v1",
      "title": "Event-Based Eye Tracking. AIS 2024 Challenge Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Zuowen Wang",
        "Chang Gao",
        "Zongwei Wu",
        "Marcos V. Conde",
        "Radu Timofte",
        "Shih-Chii Liu",
        "Qinyu Chen",
        "Zheng-jun Zha",
        "Wei Zhai",
        "Han Han",
        "Bohao Liao",
        "Yuliang Wu",
        "Zengyu Wan",
        "Zhong Wang",
        "Yang Cao",
        "Ganchao Tan",
        "Jinze Chen",
        "Yan Ru Pei",
        "Sasskia Brüers",
        "Sébastien Crouzet",
        "Douglas McLelland",
        "Oliver Coenen",
        "Baoheng Zhang",
        "Yizhao Gao",
        "Jingyuan Li",
        "Hayden Kwok-Hay So",
        "Philippe Bich",
        "Chiara Boretti",
        "Luciano Prono",
        "Mircea Lică",
        "David Dinucu-Jianu",
        "Cătălin Grîu",
        "Xiaopeng Lin",
        "Hongwei Ren",
        "Bojun Cheng",
        "Xinan Zhang",
        "Valentin Vial",
        "Anthony Yezzi",
        "James Tsai"
      ],
      "abstract": "This survey reviews the AIS 2024 Event-Based Eye Tracking (EET) Challenge.\nThe task of the challenge focuses on processing eye movement recorded with\nevent cameras and predicting the pupil center of the eye. The challenge\nemphasizes efficient eye tracking with event cameras to achieve good task\naccuracy and efficiency trade-off. During the challenge period, 38 participants\nregistered for the Kaggle competition, and 8 teams submitted a challenge\nfactsheet. The novel and diverse methods from the submitted factsheets are\nreviewed and analyzed in this survey to advance future event-based eye tracking\nresearch.",
      "tldr_zh": "本调查回顾了AIS 2024 Event-Based Eye Tracking (EET) Challenge，该挑战专注于使用事件相机处理眼动数据并预测瞳孔中心。挑战强调了Event-Based眼动追踪的效率，以实现准确性和任务效率的最佳平衡。共有38位参与者注册，8个团队提交了挑战事实表。本文分析了这些提交方法的多样性和创新性，为未来Event-Based眼动追踪研究提供了宝贵见解。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Qinyu Chen is the corresponding author",
      "pdf_url": "http://arxiv.org/pdf/2404.11770v1",
      "published_date": "2024-04-17 21:53:01 UTC",
      "updated_date": "2024-04-17 21:53:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:09:05.953574"
    },
    {
      "arxiv_id": "2404.11754v3",
      "title": "Improved Generalization Bounds for Communication Efficient Federated Learning",
      "title_zh": "通信高效联邦学习的改进泛化界限",
      "authors": [
        "Peyman Gholami",
        "Hulya Seferoglu"
      ],
      "abstract": "This paper focuses on reducing the communication cost of federated learning\nby exploring generalization bounds and representation learning. We first\ncharacterize a tighter generalization bound for one-round federated learning\nbased on local clients' generalizations and heterogeneity of data distribution\n(non-iid scenario). We also characterize a generalization bound in R-round\nfederated learning and its relation to the number of local updates (local\nstochastic gradient descents (SGDs)). Then, based on our generalization bound\nanalysis and our representation learning interpretation of this analysis, we\nshow for the first time that less frequent aggregations, hence more local\nupdates, for the representation extractor (usually corresponds to initial\nlayers) leads to the creation of more generalizable models, particularly for\nnon-iid scenarios. We design a novel Federated Learning with Adaptive Local\nSteps (FedALS) algorithm based on our generalization bound and representation\nlearning analysis. FedALS employs varying aggregation frequencies for different\nparts of the model, so reduces the communication cost. The paper is followed\nwith experimental results showing the effectiveness of FedALS.",
      "tldr_zh": "这篇论文改进了联邦学习(federated learning)的通信效率，通过分析更紧凑的泛化边界(generalization bounds)。他们首先基于本地客户端泛化和数据分布异质性(non-iid scenario)推导出一轮联邦学习的 tighter 边界，并扩展到 R-round 联邦学习，揭示了本地更新次数(包括 local SGDs)与模型泛化性的关系。论文首次证明，在 non-iid 场景中，针对表示提取器(representation extractor)的更多本地更新能创建更可泛化的模型。基于此，提出了一种新算法 Federated Learning with Adaptive Local Steps (FedALS)，它通过为模型不同部分调整聚合频率来减少通信成本，并通过实验验证了其有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11754v3",
      "published_date": "2024-04-17 21:17:48 UTC",
      "updated_date": "2024-05-27 23:20:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:09:18.423648"
    },
    {
      "arxiv_id": "2404.11744v1",
      "title": "Incremental Bootstrapping and Classification of Structured Scenes in a Fuzzy Ontology",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Buoncompagni",
        "Fulvio Mastrogiovanni"
      ],
      "abstract": "We foresee robots that bootstrap knowledge representations and use them for\nclassifying relevant situations and making decisions based on future\nobservations. Particularly for assistive robots, the bootstrapping mechanism\nmight be supervised by humans who should not repeat a training phase several\ntimes and should be able to refine the taught representation. We consider\nrobots that bootstrap structured representations to classify some intelligible\ncategories. Such a structure should be incrementally bootstrapped, i.e.,\nwithout invalidating the identified category models when a new additional\ncategory is considered. To tackle this scenario, we presented the Scene\nIdentification and Tagging (SIT) algorithm, which bootstraps structured\nknowledge representation in a crisp OWL-DL ontology. Over time, SIT bootstraps\na graph representing scenes, sub-scenes and similar scenes. Then, SIT can\nclassify new scenes within the bootstrapped graph through logic-based\nreasoning. However, SIT has issues with sensory data because its crisp\nimplementation is not robust to perception noises. This paper presents a\nreformulation of SIT within the fuzzy domain, which exploits a fuzzy DL\nontology to overcome the robustness issues. By comparing the performances of\nfuzzy and crisp implementations of SIT, we show that fuzzy SIT is robust,\npreserves the properties of its crisp formulation, and enhances the\nbootstrapped representations. On the contrary, the fuzzy implementation of SIT\nleads to less intelligible knowledge representations than the one bootstrapped\nin the crisp domain.",
      "tldr_zh": "本研究探讨了机器人通过增量引导（Incremental Bootstrapping）机制构建结构化知识表示，用于分类场景和决策，特别针对辅助机器人以减少人类重复训练。论文提出了Scene Identification and Tagging (SIT)算法的模糊版本，使用fuzzy DL ontology来处理感知噪声问题，从而提升鲁棒性，同时保留了原crisp OWL-DL ontology版本的属性。实验结果显示，fuzzy SIT在鲁棒性和引导表示方面表现出色，但知识表示的intelligibility较crisp版本有所降低。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LO",
        "cs.RO",
        "68T40 (Primary) 68T30, 68T27, 68T37, 03B52 (Secondary)",
        "I.2.4; I.2.6; I.2.3; I.2.9; I.2.10"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11744v1",
      "published_date": "2024-04-17 20:51:13 UTC",
      "updated_date": "2024-04-17 20:51:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:09:31.355556"
    },
    {
      "arxiv_id": "2404.11742v1",
      "title": "Meta-Decomposition: Dynamic Segmentation Approach Selection in IoT-based Activity Recognition",
      "title_zh": "Meta-Decomposition：在基于 IoT 的活动识别中的动态分割方法选择",
      "authors": [
        "Seyed M. R. Modaresi",
        "Aomar Osmani",
        "Mohammadreza Razzazi",
        "Abdelghani Chibani"
      ],
      "abstract": "Internet of Things (IoT) devices generate heterogeneous data over time; and\nrelying solely on individual data points is inadequate for accurate analysis.\n  Segmentation is a common preprocessing step in many IoT applications,\nincluding IoT-based activity recognition, aiming to address the limitations of\nindividual events and streamline the process. However, this step introduces at\nleast two families of uncontrollable biases. The first is caused by the changes\nmade by the segmentation process on the initial problem space, such as dividing\nthe input data into 60 seconds windows. The second category of biases results\nfrom the segmentation process itself, including the fixation of the\nsegmentation method and its parameters.\n  To address these biases, we propose to redefine the segmentation problem as a\nspecial case of a decomposition problem, including three key components: a\ndecomposer, resolutions, and a composer.\n  The inclusion of the composer task in the segmentation process facilitates an\nassessment of the relationship between the original problem and the problem\nafter the segmentation. Therefore, It leads to an improvement in the evaluation\nprocess and, consequently, in the selection of the appropriate segmentation\nmethod.\n  Then, we formally introduce our novel meta-decomposition or\nlearning-to-decompose approach. It reduces the segmentation biases by\nconsidering the segmentation as a hyperparameter to be optimized by the outer\nlearning problem. Therefore, meta-decomposition improves the overall system\nperformance by dynamically selecting the appropriate segmentation method\nwithout including the mentioned biases. Extensive experiments on four\nreal-world datasets demonstrate the effectiveness of our proposal.",
      "tldr_zh": "本文提出 Meta-Decomposition 方法，针对 IoT-based activity recognition 中分段过程引入的偏差（如问题空间改变和方法固定）进行优化。该方法将分段问题重新定义为分解问题，包括 decomposer、resolutions 和 composer 三个关键组件，通过外层学习将分段视为超参数动态选择，减少偏差并提升系统性能。实验在四个真实数据集上验证了该方法的有效性，显著改善了 IoT 应用的准确性和可靠性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11742v1",
      "published_date": "2024-04-17 20:50:28 UTC",
      "updated_date": "2024-04-17 20:50:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:09:44.057153"
    },
    {
      "arxiv_id": "2404.11730v2",
      "title": "Missed Connections: Lateral Thinking Puzzles for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Graham Todd",
        "Tim Merino",
        "Sam Earle",
        "Julian Togelius"
      ],
      "abstract": "The Connections puzzle published each day by the New York Times tasks players\nwith dividing a bank of sixteen words into four groups of four words that each\nrelate to a common theme. Solving the puzzle requires both common linguistic\nknowledge (i.e. definitions and typical usage) as well as, in many cases,\nlateral or abstract thinking. This is because the four categories ascend in\ncomplexity, with the most challenging category often requiring thinking about\nwords in uncommon ways or as parts of larger phrases. We investigate the\ncapacity for automated AI systems to play Connections and explore the game's\npotential as an automated benchmark for abstract reasoning and a way to measure\nthe semantic information encoded by data-driven linguistic systems. In\nparticular, we study both a sentence-embedding baseline and modern large\nlanguage models (LLMs). We report their accuracy on the task, measure the\nimpacts of chain-of-thought prompting, and discuss their failure modes.\nOverall, we find that the Connections task is challenging yet feasible, and a\nstrong test-bed for future work.",
      "tldr_zh": "本研究探讨了Connections谜题——一种需要将16个单词分成四组主题相关单词的游戏——作为评估大型语言模型(LLMs)抽象推理能力的基准。该谜题要求常见语言知识和横向思考，论文通过句嵌入基线和现代LLMs进行实验，评估了chain-of-thought提示对准确率的影响，并分析了失败模式。结果显示，LLMs在该任务上表现出挑战性但可行的性能，为未来数据驱动语言系统的语义信息测量提供了一个强有力的测试平台。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11730v2",
      "published_date": "2024-04-17 20:31:05 UTC",
      "updated_date": "2024-04-21 15:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:09:55.741164"
    },
    {
      "arxiv_id": "2404.11720v1",
      "title": "GEOBIND: Binding Text, Image, and Audio through Satellite Images",
      "title_zh": "翻译失败",
      "authors": [
        "Aayush Dhakal",
        "Subash Khanal",
        "Srikumar Sastry",
        "Adeel Ahmad",
        "Nathan Jacobs"
      ],
      "abstract": "In remote sensing, we are interested in modeling various modalities for some\ngeographic location. Several works have focused on learning the relationship\nbetween a location and type of landscape, habitability, audio, textual\ndescriptions, etc. Recently, a common way to approach these problems is to\ntrain a deep-learning model that uses satellite images to infer some unique\ncharacteristics of the location. In this work, we present a deep-learning\nmodel, GeoBind, that can infer about multiple modalities, specifically text,\nimage, and audio, from satellite imagery of a location. To do this, we use\nsatellite images as the binding element and contrastively align all other\nmodalities to the satellite image data. Our training results in a joint\nembedding space with multiple types of data: satellite image, ground-level\nimage, audio, and text. Furthermore, our approach does not require a single\ncomplex dataset that contains all the modalities mentioned above. Rather it\nonly requires multiple satellite-image paired data. While we only align three\nmodalities in this paper, we present a general framework that can be used to\ncreate an embedding space with any number of modalities by using satellite\nimages as the binding element. Our results show that, unlike traditional\nunimodal models, GeoBind is versatile and can reason about multiple modalities\nfor a given satellite image input.",
      "tldr_zh": "这篇论文提出了GeoBind模型，用于通过卫星 images 作为绑定元素，实现文本、图像和音频等多个模态的对比学习对齐，从而从卫星图像推断地点的多种特征。模型构建了一个联合 embedding space，仅需卫星-image paired data，而非复杂的多模态数据集，这提高了训练的灵活性和可扩展性。实验结果表明，GeoBind比传统单模态模型更通用，能够有效推理多种模态特征，为遥感领域的多模态建模提供了通用框架。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2024 IEEE International Geoscience and Remote Sensing Symposium",
      "pdf_url": "http://arxiv.org/pdf/2404.11720v1",
      "published_date": "2024-04-17 20:13:37 UTC",
      "updated_date": "2024-04-17 20:13:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:10:07.934897"
    },
    {
      "arxiv_id": "2404.11716v1",
      "title": "A Survey on Semantic Modeling for Building Energy Management",
      "title_zh": "翻译失败",
      "authors": [
        "Miracle Aniakor",
        "Vinicius V. Cogo",
        "Pedro M. Ferreira"
      ],
      "abstract": "Buildings account for a substantial portion of global energy consumption.\nReducing buildings' energy usage primarily involves obtaining data from\nbuilding systems and environment, which are instrumental in assessing and\noptimizing the building's performance. However, as devices from various\nmanufacturers represent their data in unique ways, this disparity introduces\nchallenges for semantic interoperability and creates obstacles in developing\nscalable building applications. This survey explores the leading semantic\nmodeling techniques deployed for energy management in buildings. Furthermore,\nit aims to offer tangible use cases for applying semantic models, shedding\nlight on the pivotal concepts and limitations intrinsic to each model. Our\nfindings will assist researchers in discerning the appropriate circumstances\nand methodologies for employing these models in various use cases.",
      "tldr_zh": "本调查探讨了语义建模（semantic modeling）在建筑能源管理（Building Energy Management）中的应用，旨在解决建筑物能源消耗高企和设备数据格式不统一导致的语义互操作性（semantic interoperability）挑战。论文回顾了领先的语义建模技术，并通过实际用例（use cases）阐述了这些模型的关键概念、优势和局限性。研究结果有助于研究者识别适合不同情境的语义建模方法，从而开发可扩展的建筑应用。",
      "categories": [
        "cs.AI",
        "I.2.4; C.m; H.m"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 6 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.11716v1",
      "published_date": "2024-04-17 20:10:43 UTC",
      "updated_date": "2024-04-17 20:10:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:10:18.155780"
    },
    {
      "arxiv_id": "2404.11714v1",
      "title": "Implementation and Evaluation of a Gradient Descent-Trained Defensible Blackboard Architecture System",
      "title_zh": "翻译失败",
      "authors": [
        "Jordan Milbrath",
        "Jonathan Rivard",
        "Jeremy Straub"
      ],
      "abstract": "A variety of forms of artificial intelligence systems have been developed.\nTwo well-known techniques are neural networks and rule-fact expert systems. The\nformer can be trained from presented data while the latter is typically\ndeveloped by human domain experts. A combined implementation that uses gradient\ndescent to train a rule-fact expert system has been previously proposed. A\nrelated system type, the Blackboard Architecture, adds an actualization\ncapability to expert systems. This paper proposes and evaluates the\nincorporation of a defensible-style gradient descent training capability into\nthe Blackboard Architecture. It also introduces the use of activation functions\nfor defensible artificial intelligence systems and implements and evaluates a\nnew best path-based training algorithm.",
      "tldr_zh": "该论文提出了一种将梯度下降训练整合到可防御 Blackboard Architecture 系统中的方法，旨在结合神经网络的自动训练优势与规则事实专家系统的结构。研究引入了激活 functions 用于可防御人工智能系统，并实现了一个新的基于最佳路径的训练算法，以提升系统性能。最终，通过评估对比，展示了这一方法的有效性，为人工智能系统的开发提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11714v1",
      "published_date": "2024-04-17 19:55:58 UTC",
      "updated_date": "2024-04-17 19:55:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:10:30.799806"
    },
    {
      "arxiv_id": "2404.11706v1",
      "title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
      "title_zh": "翻译失败",
      "authors": [
        "Aristeidis Tsaris",
        "Philipe Ambrozio Dias",
        "Abhishek Potnis",
        "Junqi Yin",
        "Feiyi Wang",
        "Dalton Lunga"
      ],
      "abstract": "As AI workloads increase in scope, generalization capability becomes\nchallenging for small task-specific models and their demand for large amounts\nof labeled training samples increases. On the contrary, Foundation Models (FMs)\nare trained with internet-scale unlabeled data via self-supervised learning and\nhave been shown to adapt to various tasks with minimal fine-tuning. Although\nlarge FMs have demonstrated significant impact in natural language processing\nand computer vision, efforts toward FMs for geospatial applications have been\nrestricted to smaller size models, as pretraining larger models requires very\nlarge computing resources equipped with state-of-the-art hardware accelerators.\nCurrent satellite constellations collect 100+TBs of data a day, resulting in\nimages that are billions of pixels and multimodal in nature. Such geospatial\ndata poses unique challenges opening up new opportunities to develop FMs. We\ninvestigate billion scale FMs and HPC training profiles for geospatial\napplications by pretraining on publicly available data. We studied from\nend-to-end the performance and impact in the solution by scaling the model\nsize. Our larger 3B parameter size model achieves up to 30% improvement in top1\nscene classification accuracy when comparing a 100M parameter model. Moreover,\nwe detail performance experiments on the Frontier supercomputer, America's\nfirst exascale system, where we study different model and data parallel\napproaches using PyTorch's Fully Sharded Data Parallel library. Specifically,\nwe study variants of the Vision Transformer architecture (ViT), conducting\nperformance analysis for ViT models with size up to 15B parameters. By\ndiscussing throughput and performance bottlenecks under different parallelism\nconfigurations, we offer insights on how to leverage such leadership-class HPC\nresources when developing large models for geospatial imagery applications.",
      "tldr_zh": "本研究探讨了在Frontier超级计算机上预训练十亿规模的地理空间基础模型（Foundation Models, FMs），以应对AI泛化挑战，通过自监督学习利用公开数据训练Vision Transformer (ViT)架构的变体。研究者比较了不同模型规模的性能，发现3B参数模型在场景分类准确率上比100M参数模型提高了30%。此外，他们分析了PyTorch的Fully Sharded Data Parallel库下的模型并行和数据并行策略，为利用领导级HPC资源开发大型地理空间模型提供了关键见解和性能瓶颈优化建议。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11706v1",
      "published_date": "2024-04-17 19:16:32 UTC",
      "updated_date": "2024-04-17 19:16:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:10:45.328834"
    },
    {
      "arxiv_id": "2404.11698v1",
      "title": "A Secure and Trustworthy Network Architecture for Federated Learning Healthcare Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Boiano",
        "Marco Di Gennaro",
        "Luca Barbieri",
        "Michele Carminati",
        "Monica Nicoli",
        "Alessandro Redondi",
        "Stefano Savazzi",
        "Albert Sund Aillet",
        "Diogo Reis Santos",
        "Luigi Serio"
      ],
      "abstract": "Federated Learning (FL) has emerged as a promising approach for\nprivacy-preserving machine learning, particularly in sensitive domains such as\nhealthcare. In this context, the TRUSTroke project aims to leverage FL to\nassist clinicians in ischemic stroke prediction. This paper provides an\noverview of the TRUSTroke FL network infrastructure. The proposed architecture\nadopts a client-server model with a central Parameter Server (PS). We introduce\na Docker-based design for the client nodes, offering a flexible solution for\nimplementing FL processes in clinical settings. The impact of different\ncommunication protocols (HTTP or MQTT) on FL network operation is analyzed,\nwith MQTT selected for its suitability in FL scenarios. A control plane to\nsupport the main operations required by FL processes is also proposed. The\npaper concludes with an analysis of security aspects of the FL architecture,\naddressing potential threats and proposing mitigation strategies to increase\nthe trustworthiness level.",
      "tldr_zh": "这篇论文介绍了TRUSTroke项目中的Federated Learning (FL)网络架构，旨在为医疗应用（如ischemic stroke预测）提供隐私保护的机器学习解决方案。架构采用client-server模型，包含中央Parameter Server (PS)，并使用Docker-based设计来实现客户端节点的灵活部署。论文分析了HTTP和MQTT通信协议的影响，最终选择MQTT作为更适合FL场景的协议，并提出控制平面来支持FL进程的主要操作。同时，该架构评估了潜在安全威胁，并提供缓解策略，以提升整体可信度。",
      "categories": [
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11698v1",
      "published_date": "2024-04-17 18:55:41 UTC",
      "updated_date": "2024-04-17 18:55:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:10:55.801713"
    },
    {
      "arxiv_id": "2404.11677v3",
      "title": "Cross-Problem Learning for Solving Vehicle Routing Problems",
      "title_zh": "跨问题学习用于解决车辆路径问题",
      "authors": [
        "Zhuoyi Lin",
        "Yaoxin Wu",
        "Bangjian Zhou",
        "Zhiguang Cao",
        "Wen Song",
        "Yingqian Zhang",
        "Senthilnath Jayavelu"
      ],
      "abstract": "Existing neural heuristics often train a deep architecture from scratch for\neach specific vehicle routing problem (VRP), ignoring the transferable\nknowledge across different VRP variants. This paper proposes the cross-problem\nlearning to assist heuristics training for different downstream VRP variants.\nParticularly, we modularize neural architectures for complex VRPs into 1) the\nbackbone Transformer for tackling the travelling salesman problem (TSP), and 2)\nthe additional lightweight modules for processing problem-specific features in\ncomplex VRPs. Accordingly, we propose to pre-train the backbone Transformer for\nTSP, and then apply it in the process of fine-tuning the Transformer models for\neach target VRP variant. On the one hand, we fully fine-tune the trained\nbackbone Transformer and problem-specific modules simultaneously. On the other\nhand, we only fine-tune small adapter networks along with the modules, keeping\nthe backbone Transformer still. Extensive experiments on typical VRPs\nsubstantiate that 1) the full fine-tuning achieves significantly better\nperformance than the one trained from scratch, and 2) the adapter-based\nfine-tuning also delivers comparable performance while being notably\nparameter-efficient. Furthermore, we empirically demonstrate the favorable\neffect of our method in terms of cross-distribution application and\nversatility.",
      "tldr_zh": "本文提出了一种跨问题学习方法，用于解决车辆路径问题 (VRP)，通过模块化神经架构（包括主干 Transformer 处理旅行 salesman 问题 (TSP) 和轻量级模块处理特定特征）来利用不同 VRP 变体之间的可转移知识。方法包括预训练 TSP 的主干 Transformer，然后通过全微调或适配器-based 微调来优化目标 VRP 模型，其中适配器微调显著提高了参数效率。实验结果显示，该方法在典型 VRP 上比从零开始训练的模型性能提升明显，并在跨分布应用和多功能性方面表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IJCAI'24",
      "pdf_url": "http://arxiv.org/pdf/2404.11677v3",
      "published_date": "2024-04-17 18:17:50 UTC",
      "updated_date": "2024-06-18 09:03:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:11:08.977772"
    },
    {
      "arxiv_id": "2404.11667v1",
      "title": "Deep Dependency Networks and Advanced Inference Schemes for Multi-Label Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Shivvrat Arya",
        "Yu Xiang",
        "Vibhav Gogate"
      ],
      "abstract": "We present a unified framework called deep dependency networks (DDNs) that\ncombines dependency networks and deep learning architectures for multi-label\nclassification, with a particular emphasis on image and video data. The primary\nadvantage of dependency networks is their ease of training, in contrast to\nother probabilistic graphical models like Markov networks. In particular, when\ncombined with deep learning architectures, they provide an intuitive,\neasy-to-use loss function for multi-label classification. A drawback of DDNs\ncompared to Markov networks is their lack of advanced inference schemes,\nnecessitating the use of Gibbs sampling. To address this challenge, we propose\nnovel inference schemes based on local search and integer linear programming\nfor computing the most likely assignment to the labels given observations. We\nevaluate our novel methods on three video datasets (Charades, TACoS, Wetlab)\nand three image datasets (MS-COCO, PASCAL VOC, NUS-WIDE), comparing their\nperformance with (a) basic neural architectures and (b) neural architectures\ncombined with Markov networks equipped with advanced inference and learning\ntechniques. Our results demonstrate the superiority of our new DDN methods over\nthe two competing approaches.",
      "tldr_zh": "该论文提出了一种统一的框架 deep dependency networks (DDNs)，将 dependency networks 与 deep learning 架构结合，用于多标签分类任务，特别是图像和视频数据，其主要优势在于易于训练和直观的损失函数设计。针对 DDNs 的缺点——依赖 Gibbs sampling 而缺乏高级推理方案，作者引入了基于 local search 和 integer linear programming 的新推理方法，以计算给定观察的最可能标签赋值。在 Charades、TACoS、Wetlab 等视频数据集以及 MS-COCO、PASCAL VOC、NUS-WIDE 等图像数据集上的实验结果显示，DDNs 方法优于基本神经架构和结合 Markov networks 的竞争方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Will appear in AISTATS 2024. arXiv admin note: substantial text\n  overlap with arXiv:2302.00633",
      "pdf_url": "http://arxiv.org/pdf/2404.11667v1",
      "published_date": "2024-04-17 18:04:37 UTC",
      "updated_date": "2024-04-17 18:04:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:11:20.905769"
    },
    {
      "arxiv_id": "2404.11606v1",
      "title": "Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models",
      "title_zh": "在概率图模型中学习求解约束最可能解释任务",
      "authors": [
        "Shivvrat Arya",
        "Tahrima Rahman",
        "Vibhav Gogate"
      ],
      "abstract": "We propose a self-supervised learning approach for solving the following\nconstrained optimization task in log-linear models or Markov networks. Let $f$\nand $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and\n$\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$\nto all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the\nconstrained most-probable explanation (CMPE) task seeks to find an assignment\n$\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x},\n\\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our\nproposed self-supervised approach, given assignments $\\mathbf{x}$ to\n$\\mathbf{X}$ (data), we train a deep neural network that learns to output\nnear-optimal solutions to the CMPE problem without requiring access to any\npre-computed solutions. The key idea in our approach is to use first principles\nand approximate inference methods for CMPE to derive novel loss functions that\nseek to push infeasible solutions towards feasible ones and feasible solutions\ntowards optimal ones. We analyze the properties of our proposed method and\nexperimentally demonstrate its efficacy on several benchmark problems.",
      "tldr_zh": "本研究提出了一种自监督学习(self-supervised learning)方法，用于解决概率图形模型(Probabilistic Graphical Models)中的约束最可能解释(Constrained Most Probable Explanation, CMPE)任务。具体来说，给定证据赋值$\\mathbf{x}$和约束$q$，该方法训练一个深度神经网络(deep neural network)，通过基于第一原理和近似推理方法的损失函数，将不可行解推向可行解，并优化可行解以最大化目标函数。实验结果表明，该方法在多个基准问题上表现出色，能够生成近似最优解，而无需预先计算的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Will appear in AISTATS 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11606v1",
      "published_date": "2024-04-17 17:55:17 UTC",
      "updated_date": "2024-04-17 17:55:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:11:33.298870"
    },
    {
      "arxiv_id": "2404.11605v1",
      "title": "VG4D: Vision-Language Model Goes 4D Video Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Zhichao Deng",
        "Xiangtai Li",
        "Xia Li",
        "Yunhai Tong",
        "Shen Zhao",
        "Mengyuan Liu"
      ],
      "abstract": "Understanding the real world through point cloud video is a crucial aspect of\nrobotics and autonomous driving systems. However, prevailing methods for 4D\npoint cloud recognition have limitations due to sensor resolution, which leads\nto a lack of detailed information. Recent advances have shown that\nVision-Language Models (VLM) pre-trained on web-scale text-image datasets can\nlearn fine-grained visual concepts that can be transferred to various\ndownstream tasks. However, effectively integrating VLM into the domain of 4D\npoint clouds remains an unresolved problem. In this work, we propose the\nVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from\nvisual-text pre-trained models to a 4D point cloud network. Our approach\ninvolves aligning the 4D encoder's representation with a VLM to learn a shared\nvisual and text space from training on large-scale image-text pairs. By\ntransferring the knowledge of the VLM to the 4D encoder and combining the VLM,\nour VG4D achieves improved recognition performance. To enhance the 4D encoder,\nwe modernize the classic dynamic point cloud backbone and propose an improved\nversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.\nExperiments demonstrate that our method achieves state-of-the-art performance\nfor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120\ndataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.",
      "tldr_zh": "本研究提出VG4D框架，将Vision-Language Models (VLM)预训练的知识转移到4D点云视频识别领域，以解决现有方法的传感器分辨率限制和信息不足问题。该框架通过将4D编码器的表示与VLM对齐，并在大规模图像-文本对上训练，学习共享的视觉和文本空间，同时引入改进版动态点云骨干网络im-PSTNet来高效建模点云视频。实验结果显示，VG4D在NTU RGB+D 60和120数据集上实现了最先进的动作识别性能，显著提升了4D点云识别的准确性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "ICRA 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11605v1",
      "published_date": "2024-04-17 17:54:49 UTC",
      "updated_date": "2024-04-17 17:54:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:11:46.794053"
    },
    {
      "arxiv_id": "2404.11597v2",
      "title": "Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review",
      "title_zh": "可解释人工智能技术用于准确故障检测和诊断：综述",
      "authors": [
        "Ahmed Maged",
        "Salah Haridy",
        "Herman Shen"
      ],
      "abstract": "As the manufacturing industry advances with sensor integration and\nautomation, the opaque nature of deep learning models in machine learning poses\na significant challenge for fault detection and diagnosis. And despite the\nrelated predictive insights Artificial Intelligence (AI) can deliver, advanced\nmachine learning engines often remain a black box. This paper reviews the\neXplainable AI (XAI) tools and techniques in this context. We explore various\nXAI methodologies, focusing on their role in making AI decision-making\ntransparent, particularly in critical scenarios where humans are involved. We\nalso discuss current limitations and potential future research that aims to\nbalance explainability with model performance while improving trustworthiness\nin the context of AI applications for critical industrial use cases.",
      "tldr_zh": "这篇论文回顾了eXplainable AI (XAI)技术在制造行业故障检测和诊断中的应用，旨在解决深度学习模型的黑箱问题，使AI决策过程更透明。作者探讨了各种XAI方法论，强调其在关键场景（如涉及人类操作）中的作用，以提升AI的可解释性和可靠性。论文还分析了当前XAI的局限性，并提出未来研究方向，以平衡explainability与模型性能，确保AI在工业领域的trustworthiness。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11597v2",
      "published_date": "2024-04-17 17:49:38 UTC",
      "updated_date": "2024-06-10 17:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:11:59.151775"
    },
    {
      "arxiv_id": "2404.11589v1",
      "title": "Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Zezhong Fan",
        "Xiaohan Li",
        "Chenhao Fang",
        "Topojoy Biswas",
        "Kaushiki Nag",
        "Jianpeng Xu",
        "Kannan Achan"
      ],
      "abstract": "The rapid evolution of text-to-image diffusion models has opened the door of\ngenerative AI, enabling the translation of textual descriptions into visually\ncompelling images with remarkable quality. However, a persistent challenge\nwithin this domain is the optimization of prompts to effectively convey\nabstract concepts into concrete objects. For example, text encoders can hardly\nexpress \"peace\", while can easily illustrate olive branches and white doves.\nThis paper introduces a novel approach named Prompt Optimizer for Abstract\nConcepts (POAC) specifically designed to enhance the performance of\ntext-to-image diffusion models in interpreting and generating images from\nabstract concepts. We propose a Prompt Language Model (PLM), which is\ninitialized from a pre-trained language model, and then fine-tuned with a\ncurated dataset of abstract concept prompts. The dataset is created with GPT-4\nto extend the abstract concept to a scene and concrete objects. Our framework\nemploys a Reinforcement Learning (RL)-based optimization strategy, focusing on\nthe alignment between the generated images by a stable diffusion model and\noptimized prompts. Through extensive experiments, we demonstrate that our\nproposed POAC significantly improves the accuracy and aesthetic quality of\ngenerated images, particularly in the description of abstract concepts and\nalignment with optimized prompts. We also present a comprehensive analysis of\nour model's performance across diffusion models under different settings,\nshowcasing its versatility and effectiveness in enhancing abstract concept\nrepresentation.",
      "tldr_zh": "本文研究了文本到图像扩散模型在处理抽象概念时的挑战，例如难以直接表达“peace”而更易描述具体对象。论文提出了一种名为 POAC（Prompt Optimizer for Abstract Concepts）的创新方法，包括 Prompt Language Model (PLM)，该模型从预训练语言模型初始化，并使用由 GPT-4 生成的抽象概念数据集进行微调，同时采用 Reinforcement Learning (RL) 基于优化策略来提升提示与生成图像的对齐度。通过广泛实验，POAC 显著提高了图像的准确性和美学质量，尤其在抽象概念表示方面，并展示了其在不同扩散模型设置下的通用性和有效性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "WWW 2024 Companion",
      "pdf_url": "http://arxiv.org/pdf/2404.11589v1",
      "published_date": "2024-04-17 17:38:56 UTC",
      "updated_date": "2024-04-17 17:38:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:12:12.202832"
    },
    {
      "arxiv_id": "2404.11585v2",
      "title": "Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Carlos Penarrubia",
        "Carlos Garrido-Munoz",
        "Jose J. Valero-Mas",
        "Jorge Calvo-Zaragoza"
      ],
      "abstract": "Handwritten Text Recognition (HTR) is a relevant problem in computer vision,\nand implies unique challenges owing to its inherent variability and the rich\ncontextualization required for its interpretation. Despite the success of\nSelf-Supervised Learning (SSL) in computer vision, its application to HTR has\nbeen rather scattered, leaving key SSL methodologies unexplored. This work\nfocuses on one of them, namely Spatial Context-based SSL. We investigate how\nthis family of approaches can be adapted and optimized for HTR and propose new\nworkflows that leverage the unique features of handwritten text. Our\nexperiments demonstrate that the methods considered lead to advancements in the\nstate-of-the-art of SSL for HTR in a number of benchmark cases.",
      "tldr_zh": "该研究针对 Handwritten Text Recognition (HTR) 的变异性和上下文需求问题，探讨了 Spatial Context-based Self-Supervised Learning (SSL) 的应用，以填补这一领域的空白。论文提出新的工作流，通过适应和优化 Spatial Context-based SSL 方法，利用手写文本的独特特征来提升模型性能。实验结果表明，这些方法在多个 HTR 基准案例中推动了状态-of-the-art 的进展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2404.11585v2",
      "published_date": "2024-04-17 17:33:32 UTC",
      "updated_date": "2025-02-25 09:57:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:12:21.297017"
    },
    {
      "arxiv_id": "2404.11584v1",
      "title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Tula Masterman",
        "Sandi Besen",
        "Mason Sawtell",
        "Alex Chao"
      ],
      "abstract": "This survey paper examines the recent advancements in AI agent\nimplementations, with a focus on their ability to achieve complex goals that\nrequire enhanced reasoning, planning, and tool execution capabilities. The\nprimary objectives of this work are to a) communicate the current capabilities\nand limitations of existing AI agent implementations, b) share insights gained\nfrom our observations of these systems in action, and c) suggest important\nconsiderations for future developments in AI agent design. We achieve this by\nproviding overviews of single-agent and multi-agent architectures, identifying\nkey patterns and divergences in design choices, and evaluating their overall\nimpact on accomplishing a provided goal. Our contribution outlines key themes\nwhen selecting an agentic architecture, the impact of leadership on agent\nsystems, agent communication styles, and key phases for planning, execution,\nand reflection that enable robust AI agent systems.",
      "tldr_zh": "这篇调查论文探讨了新兴 AI 代理架构在推理（reasoning）、规划（planning）和工具调用（tool calling）方面的进展，重点评估这些架构如何实现复杂目标。论文概述了单代理（single-agent）和多代理（multi-agent）系统，识别设计选择的模式和差异，并分析其对目标完成的影响，同时分享了从系统实际运行中获得的见解。最终贡献包括提出选择架构的关键主题、领导力（leadership）对代理系统的影响、代理通信风格（agent communication styles），以及规划、执行和反思的关键阶段，以指导未来 AI 代理设计的发展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages,6 figures,38 references",
      "pdf_url": "http://arxiv.org/pdf/2404.11584v1",
      "published_date": "2024-04-17 17:32:41 UTC",
      "updated_date": "2024-04-17 17:32:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:12:34.671875"
    },
    {
      "arxiv_id": "2404.11581v3",
      "title": "E2ETune: End-to-End Knob Tuning via Fine-tuned Generative Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Xinmei Huang",
        "Haoyang Li",
        "Jing Zhang",
        "Xinxin Zhao",
        "Zhiming Yao",
        "Yiyan Li",
        "Tieying Zhang",
        "Jianjun Chen",
        "Hong Chen",
        "Cuiping Li"
      ],
      "abstract": "Database knob tuning is a significant challenge for database administrators,\nas it involves tuning a large number of configuration knobs with continuous or\ndiscrete values to achieve optimal database performance. Traditional methods,\nsuch as manual tuning or learning-based approaches, typically require numerous\nworkload replays and are both time-consuming and resource-intensive. To address\nthis challenge, we introduce E2ETune, an end-to-end knob tuner powered by a\nfine-tuned generative language model. The key idea is to leverage the\nexceptional sequence-to-sequence modeling capabilities of generative language\nmodels to capture the complex mapping between workloads (inputs) and their\ncorresponding promising configurations (outputs). To achieve this goal, we\npropose a novel data generation framework to efficiently produce a large amount\nof training data, where each data sample consists of a workload and its\npromising configuration. Then, these data are used to fine-tune a generative\nlanguage model, yielding an end-to-end knob tuner. This tuner offers\nout-of-the-box configuration recommendations for new workloads. We conduct\nextensive experiments to evaluate E2ETune's efficiency and effectiveness using\n10 representative and 3 real-world benchmarks. Compared to state-of-the-art\nmethods, E2ETune can identify competitive configurations in significantly less\ntime.",
      "tldr_zh": "这篇论文针对数据库 knob tuning 的挑战，提出 E2ETune，一种基于 fine-tuned generative language model 的端到端解决方案，以高效处理工作负载与最佳配置之间的复杂映射。研究者设计了一个新颖的数据生成框架，用于快速产生大量训练数据，每个样本包含工作负载及其对应配置，然后利用这些数据微调语言模型，实现对新工作负载的即时配置推荐。实验在 10 个代表性基准和 3 个真实世界基准上表明，E2ETune 比现有方法更快地识别出竞争性配置，显著降低了时间和资源消耗。",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by VLDB 2025",
      "pdf_url": "http://arxiv.org/pdf/2404.11581v3",
      "published_date": "2024-04-17 17:28:05 UTC",
      "updated_date": "2025-03-19 06:19:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:12:47.305984"
    },
    {
      "arxiv_id": "2404.11578v3",
      "title": "LTL-Constrained Policy Optimization with Cycle Experience Replay",
      "title_zh": "翻译失败",
      "authors": [
        "Ameesh Shah",
        "Cameron Voloshin",
        "Chenxi Yang",
        "Abhinav Verma",
        "Swarat Chaudhuri",
        "Sanjit A. Seshia"
      ],
      "abstract": "Linear Temporal Logic (LTL) offers a precise means for constraining the\nbehavior of reinforcement learning agents. However, in many settings where both\nsatisfaction and optimality conditions are present, LTL is insufficient to\ncapture both. Instead, LTL-constrained policy optimization, where the goal is\nto optimize a scalar reward under LTL constraints, is needed. This constrained\noptimization problem proves difficult in deep Reinforcement Learning (DRL)\nsettings, where learned policies often ignore the LTL constraint due to the\nsparse nature of LTL satisfaction. To alleviate the sparsity issue, we\nintroduce Cycle Experience Replay (CyclER), a novel reward shaping technique\nthat exploits the underlying structure of the LTL constraint to guide a policy\ntowards satisfaction by encouraging partial behaviors compliant with the\nconstraint. We provide a theoretical guarantee that optimizing CyclER will\nachieve policies that satisfy the LTL constraint with near-optimal probability.\nWe evaluate CyclER in three continuous control domains. Our experimental\nresults show that optimizing CyclER in tandem with the existing scalar reward\noutperforms existing reward-shaping methods at finding performant\nLTL-satisfying policies.",
      "tldr_zh": "该论文提出了一种名为 Cycle Experience Replay (CyclER) 的奖励整形技术，用于在强化学习中优化受 Linear Temporal Logic (LTL) 约束的策略，以同时满足约束条件和最优性目标。CyclER 利用 LTL 约束的底层结构，鼓励代理的部分行为符合约束，从而缓解 LTL 满足的稀疏性问题，并提供理论保证，确保优化后策略能以近似最优概率满足 LTL。实验结果显示，在三个连续控制领域中，CyclER 与现有标量奖励结合，使用时优于其他奖励整形方法，能更有效地找到高性能的 LTL 满足策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.FL",
        "I.2.6; I.2.4"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in TMLR, 12 pages in main text",
      "pdf_url": "http://arxiv.org/pdf/2404.11578v3",
      "published_date": "2024-04-17 17:24:44 UTC",
      "updated_date": "2025-03-24 23:37:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:12:59.407711"
    },
    {
      "arxiv_id": "2404.11577v3",
      "title": "Towards Reliable Empirical Machine Unlearning Evaluation: A Cryptographic Game Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwen Tu",
        "Pingbang Hu",
        "Jiaqi Ma"
      ],
      "abstract": "Machine unlearning updates machine learning models to remove information from\nspecific training samples, complying with data protection regulations that\nallow individuals to request the removal of their personal data. Despite the\nrecent development of numerous unlearning algorithms, reliable evaluation of\nthese algorithms remains an open research question. In this work, we focus on\nmembership inference attack (MIA) based evaluation, one of the most common\napproaches for evaluating unlearning algorithms, and address various pitfalls\nof existing evaluation metrics lacking theoretical understanding and\nreliability. Specifically, by modeling the proposed evaluation process as a\n\\emph{cryptographic game} between unlearning algorithms and MIA adversaries,\nthe naturally-induced evaluation metric measures the data removal efficacy of\nunlearning algorithms and enjoys provable guarantees that existing evaluation\nmetrics fail to satisfy. Furthermore, we propose a practical and efficient\napproximation of the induced evaluation metric and demonstrate its\neffectiveness through both theoretical analysis and empirical experiments.\nOverall, this work presents a novel and reliable approach to empirically\nevaluating unlearning algorithms, paving the way for the development of more\neffective unlearning techniques.",
      "tldr_zh": "这篇论文针对机器取消学习（Machine Unlearning）的评估问题，指出现有基于 Membership Inference Attack (MIA) 的方法缺乏理论基础和可靠性。作者通过将评估过程建模为一个加密游戏（Cryptographic Game），提出一个新的评价指标，能够量化数据移除的有效性和提供可证明的保证。论文还开发了该指标的实用高效近似方法，并通过理论分析和实验证明其有效性，为更可靠的取消学习算法开发奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11577v3",
      "published_date": "2024-04-17 17:20:27 UTC",
      "updated_date": "2025-02-14 03:03:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:13:10.498974"
    },
    {
      "arxiv_id": "2404.11565v2",
      "title": "MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Kuan-Chieh Wang",
        "Daniil Ostashev",
        "Yuwei Fang",
        "Sergey Tulyakov",
        "Kfir Aberman"
      ],
      "abstract": "We introduce a new architecture for personalization of text-to-image\ndiffusion models, coined Mixture-of-Attention (MoA). Inspired by the\nMixture-of-Experts mechanism utilized in large language models (LLMs), MoA\ndistributes the generation workload between two attention pathways: a\npersonalized branch and a non-personalized prior branch. MoA is designed to\nretain the original model's prior by fixing its attention layers in the prior\nbranch, while minimally intervening in the generation process with the\npersonalized branch that learns to embed subjects in the layout and context\ngenerated by the prior branch. A novel routing mechanism manages the\ndistribution of pixels in each layer across these branches to optimize the\nblend of personalized and generic content creation. Once trained, MoA\nfacilitates the creation of high-quality, personalized images featuring\nmultiple subjects with compositions and interactions as diverse as those\ngenerated by the original model. Crucially, MoA enhances the distinction\nbetween the model's pre-existing capability and the newly augmented\npersonalized intervention, thereby offering a more disentangled subject-context\ncontrol that was previously unattainable. Project page:\nhttps://snap-research.github.io/mixture-of-attention",
      "tldr_zh": "我们引入了MoA（Mixture-of-Attention）架构，用于个性化文本到图像扩散模型的生成过程。该架构受Mixture-of-Experts机制启发，通过两个注意力路径——个性化分支和非个性化先验分支——来分配生成任务，其中先验分支固定以保留原模型能力，而个性化分支最小干预地嵌入主题。MoA的路由机制优化像素分布，实现主题-上下文的分离控制，支持创建高质量图像，包含多个主题的多样组合和交互，从而提升了个性化生成的可控性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Website:\n  https://snap-research.github.io/mixture-of-attention, Same as previous\n  version, only updated metadata because bib was missing an author name",
      "pdf_url": "http://arxiv.org/pdf/2404.11565v2",
      "published_date": "2024-04-17 17:08:05 UTC",
      "updated_date": "2024-05-06 16:29:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:13:21.975700"
    },
    {
      "arxiv_id": "2404.11553v3",
      "title": "Language Ranker: A Metric for Quantifying LLM Performance Across High and Low-Resource Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Li",
        "Yucheng Shi",
        "Zirui Liu",
        "Fan Yang",
        "Ali Payani",
        "Ninghao Liu",
        "Mengnan Du"
      ],
      "abstract": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)在高资源语言（如英语）和低资源语言间的性能不均衡问题，提出了一种新的量化指标Language Ranker。该指标通过比较LLMs的内部表示与英语基准，评估和排名不同语言的模型性能，实现一种鲁棒且语言无关的基准测试。实验结果显示，高资源语言与英语的相似度得分更高，表现出色，而低资源语言得分较低；此外，LLMs在各语言的性能与预训练语料中语言比例高度相关。这些发现证明了Language Ranker的有效性，有助于提升对低资源语言能力的评估。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2025 (Social Impact Track)",
      "pdf_url": "http://arxiv.org/pdf/2404.11553v3",
      "published_date": "2024-04-17 16:53:16 UTC",
      "updated_date": "2024-12-11 09:04:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:13:36.057708"
    },
    {
      "arxiv_id": "2404.11536v2",
      "title": "FedPFT: Federated Proxy Fine-Tuning of Foundation Models",
      "title_zh": "翻译失败",
      "authors": [
        "Zhaopeng Peng",
        "Xiaoliang Fan",
        "Yufan Chen",
        "Zheng Wang",
        "Shirui Pan",
        "Chenglu Wen",
        "Ruisheng Zhang",
        "Cheng Wang"
      ],
      "abstract": "Adapting Foundation Models (FMs) for downstream tasks through Federated\nLearning (FL) emerges a promising strategy for protecting data privacy and\nvaluable FMs. Existing methods fine-tune FM by allocating sub-FM to clients in\nFL, however, leading to suboptimal performance due to insufficient tuning and\ninevitable error accumulations of gradients. In this paper, we propose\nFederated Proxy Fine-Tuning (FedPFT), a novel method enhancing FMs adaptation\nin downstream tasks through FL by two key modules. First, the sub-FM\nconstruction module employs a layer-wise compression approach, facilitating\ncomprehensive FM fine-tuning across all layers by emphasizing those crucial\nneurons. Second, the sub-FM alignment module conducts a two-step\ndistillations-layer-level and neuron-level-before and during FL fine-tuning\nrespectively, to reduce error of gradient by accurately aligning sub-FM with FM\nunder theoretical guarantees. Experimental results on seven commonly used\ndatasets (i.e., four text and three vision) demonstrate the superiority of\nFedPFT.",
      "tldr_zh": "该研究提出了一种名为 FedPFT 的新方法，用于通过联邦学习 (Federated Learning, FL) 适应基础模型 (Foundation Models, FMs)，以保护数据隐私并提升下游任务性能。FedPFT 包括两个关键模块：子模型构建模块采用层级压缩方法，强调关键神经元以实现对所有层的全面调优；子模型对齐模块则通过两步蒸馏（层级和神经元级）在 FL 调优前后减少梯度错误，并提供理论保证。实验结果显示，FedPFT 在七个常用数据集（包括四个文本和三个视觉数据集）上表现出优越性，显著优于现有方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI'24",
      "pdf_url": "http://arxiv.org/pdf/2404.11536v2",
      "published_date": "2024-04-17 16:30:06 UTC",
      "updated_date": "2024-04-28 11:11:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:13:47.505762"
    },
    {
      "arxiv_id": "2404.11534v1",
      "title": "Decomposing and Editing Predictions by Modeling Model Computation",
      "title_zh": "通过建模模型计算来分解和编辑预测",
      "authors": [
        "Harshay Shah",
        "Andrew Ilyas",
        "Aleksander Madry"
      ],
      "abstract": "How does the internal computation of a machine learning model transform\ninputs into predictions? In this paper, we introduce a task called component\nmodeling that aims to address this question. The goal of component modeling is\nto decompose an ML model's prediction in terms of its components -- simple\nfunctions (e.g., convolution filters, attention heads) that are the \"building\nblocks\" of model computation. We focus on a special case of this task,\ncomponent attribution, where the goal is to estimate the counterfactual impact\nof individual components on a given prediction. We then present COAR, a\nscalable algorithm for estimating component attributions; we demonstrate its\neffectiveness across models, datasets, and modalities. Finally, we show that\ncomponent attributions estimated with COAR directly enable model editing across\nfive tasks, namely: fixing model errors, ``forgetting'' specific classes,\nboosting subpopulation robustness, localizing backdoor attacks, and improving\nrobustness to typographic attacks. We provide code for COAR at\nhttps://github.com/MadryLab/modelcomponents .",
      "tldr_zh": "本论文引入了component modeling任务，旨在分解机器学习模型的内部计算过程，将预测拆解成简单组件（如convolution filters和attention heads），以理解输入如何转化为输出。作者提出COAR算法，一种可扩展的方法，用于估算component attribution，即评估单个组件对特定预测的反事实影响，并在多种模型、数据集和模态中证明了其有效性。最后，COAR支持模型编辑应用，包括修复模型错误、忘记特定类、提升子群体鲁棒性、定位backdoor attacks以及改善对typographic attacks的鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11534v1",
      "published_date": "2024-04-17 16:28:08 UTC",
      "updated_date": "2024-04-17 16:28:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:13:59.363934"
    },
    {
      "arxiv_id": "2404.11515v2",
      "title": "Embedding Privacy in Computational Social Science and Artificial Intelligence Research",
      "title_zh": "在计算社会科学和人工智能研究中嵌入隐私",
      "authors": [
        "Keenan Jones",
        "Fatima Zahrah",
        "Jason R. C. Nurse"
      ],
      "abstract": "Privacy is a human right. It ensures that individuals are free to engage in\ndiscussions, participate in groups, and form relationships online or offline\nwithout fear of their data being inappropriately harvested, analyzed, or\notherwise used to harm them. Preserving privacy has emerged as a critical\nfactor in research, particularly in the computational social science (CSS),\nartificial intelligence (AI) and data science domains, given their reliance on\nindividuals' data for novel insights. The increasing use of advanced\ncomputational models stands to exacerbate privacy concerns because, if\ninappropriately used, they can quickly infringe privacy rights and lead to\nadverse effects for individuals -- especially vulnerable groups -- and society.\nWe have already witnessed a host of privacy issues emerge with the advent of\nlarge language models (LLMs), such as ChatGPT, which further demonstrate the\nimportance of embedding privacy from the start. This article contributes to the\nfield by discussing the role of privacy and the issues that researchers working\nin CSS, AI, data science and related domains are likely to face. It then\npresents several key considerations for researchers to ensure participant\nprivacy is best preserved in their research design, data collection and use,\nanalysis, and dissemination of research results.",
      "tldr_zh": "这篇论文强调了隐私作为一项人权的重要性，尤其在 Computational Social Science (CSS)、Artificial Intelligence (AI) 和数据科学领域，这些领域依赖个人数据进行创新研究，但先进计算模型可能加剧隐私侵犯风险。文章分析了如 Large Language Models (LLMs) 例如 ChatGPT 带来的实际问题，包括数据滥用和对弱势群体的负面影响。最终，它提出关键考虑建议，帮助研究人员在研究设计、数据收集、使用、分析和结果传播过程中从一开始就嵌入隐私保护机制，以确保研究伦理和参与者权益。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.ET",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "International Association for the Advancement of Artificial\n  Intelligence (AAAI) Conference on Web and Social Media (ICWSM) Workshops\n  (Disrupt, Ally, Resist, Embrace (DARE) Workshop), 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11515v2",
      "published_date": "2024-04-17 16:07:53 UTC",
      "updated_date": "2024-06-03 14:32:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:14:10.763164"
    },
    {
      "arxiv_id": "2404.11502v1",
      "title": "Towards Coarse-to-Fine Evaluation of Inference Efficiency for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yushuo Chen",
        "Tianyi Tang",
        "Erge Xiang",
        "Linjiang Li",
        "Wayne Xin Zhao",
        "Jing Wang",
        "Yunpeng Chai",
        "Ji-Rong Wen"
      ],
      "abstract": "In real world, large language models (LLMs) can serve as the assistant to\nhelp users accomplish their jobs, and also support the development of advanced\napplications. For the wide application of LLMs, the inference efficiency is an\nessential concern, which has been widely studied in existing work, and numerous\noptimization algorithms and code libraries have been proposed to improve it.\nNonetheless, users still find it challenging to compare the effectiveness of\nall the above methods and understand the underlying mechanisms. In this work,\nwe perform a detailed coarse-to-fine analysis of the inference performance of\nvarious code libraries. To evaluate the overall effectiveness, we examine four\nusage scenarios within two practical applications. We further provide both\ntheoretical and empirical fine-grained analyses of each module in the\nTransformer architecture. Our experiments yield comprehensive results that are\ninvaluable for researchers to evaluate code libraries and improve inference\nstrategies.",
      "tldr_zh": "本文针对 Large Language Models (LLMs) 的推理效率问题，提出了一种从粗到细的评估方法，以帮助用户比较现有优化算法和代码库的有效性。该方法包括考察四个使用场景和两个实际应用来进行整体评估，并对 Transformer 架构的每个模块进行理论和实证细粒度分析。实验结果提供了全面见解，有助于研究者评估代码库并改进推理策略。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11502v1",
      "published_date": "2024-04-17 15:57:50 UTC",
      "updated_date": "2024-04-17 15:57:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:14:23.280117"
    },
    {
      "arxiv_id": "2404.11500v1",
      "title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Zhou",
        "Yada Zhu",
        "Diego Antognini",
        "Yoon Kim",
        "Yang Zhang"
      ],
      "abstract": "This paper studies the relationship between the surface form of a\nmathematical problem and its solvability by large language models. We find that\nsubtle alterations in the surface form can significantly impact the answer\ndistribution and the solve rate, exposing the language model's lack of\nrobustness and sensitivity to the surface form in reasoning through complex\nproblems. To improve mathematical reasoning performance, we propose\nSelf-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths\nfrom specific surface forms of the problem. We evaluate our approach on four\nmathematics reasoning benchmarks over three large language models and show that\nSCoP improves mathematical reasoning performance over vanilla self-consistency,\nparticularly for problems initially deemed unsolvable. Finally, we provide\nadditional experiments and discussion regarding problem difficulty and surface\nforms, including cross-model difficulty agreement and paraphrasing\ntransferability, and Variance of Variations (VOV) for language model\nevaluation.",
      "tldr_zh": "本研究探讨了数学问题表面形式（surface form）对大型语言模型（Large Language Models, LLMs）的数学推理能力的影响，发现微小的表面形式变化会显著改变答案分布和求解率，暴露了模型在处理复杂问题时的不稳定性。针对这一问题，作者提出Self-Consistency-over-Paraphrases (SCoP) 方法，通过对问题进行改写来多样化推理路径，从而提升数学推理性能。实验在四个数学推理基准上评估了三种LLMs，结果显示SCoP比传统的Self-Consistency方法更有效，尤其对最初无法求解的问题有显著改善。最后，研究还讨论了问题难度、表面形式的影响、跨模型难度一致性、改写可转移性，以及Variance of Variations (VOV) 用于模型评估。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to the main conference of NAACL (2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.11500v1",
      "published_date": "2024-04-17 15:53:49 UTC",
      "updated_date": "2024-04-17 15:53:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:14:36.568583"
    },
    {
      "arxiv_id": "2404.11499v1",
      "title": "A Data-Driven Representation for Sign Language Production",
      "title_zh": "翻译失败",
      "authors": [
        "Harry Walsh",
        "Abolfazl Ravanshad",
        "Mariam Rahmani",
        "Richard Bowden"
      ],
      "abstract": "Phonetic representations are used when recording spoken languages, but no\nequivalent exists for recording signed languages. As a result, linguists have\nproposed several annotation systems that operate on the gloss or sub-unit\nlevel; however, these resources are notably irregular and scarce.\n  Sign Language Production (SLP) aims to automatically translate spoken\nlanguage sentences into continuous sequences of sign language. However, current\nstate-of-the-art approaches rely on scarce linguistic resources to work. This\nhas limited progress in the field. This paper introduces an innovative solution\nby transforming the continuous pose generation problem into a discrete sequence\ngeneration problem. Thus, overcoming the need for costly annotation. Although,\nif available, we leverage the additional information to enhance our approach.\n  By applying Vector Quantisation (VQ) to sign language data, we first learn a\ncodebook of short motions that can be combined to create a natural sequence of\nsign. Where each token in the codebook can be thought of as the lexicon of our\nrepresentation. Then using a transformer we perform a translation from spoken\nlanguage text to a sequence of codebook tokens. Each token can be directly\nmapped to a sequence of poses allowing the translation to be performed by a\nsingle network. Furthermore, we present a sign stitching method to effectively\njoin tokens together. We evaluate on the RWTH-PHOENIX-Weather-2014T\n(PHOENIX14T) and the more challenging Meine DGS Annotated (mDGS) datasets. An\nextensive evaluation shows our approach outperforms previous methods,\nincreasing the BLEU-1 back translation score by up to 72%.",
      "tldr_zh": "这篇论文针对手语生产的挑战，提出了一种数据驱动的表示方法，以解决手语注释资源稀缺的问题，通过将连续姿势生成转化为离散序列生成，从而减少对昂贵注释的依赖。方法包括使用 Vector Quantisation (VQ) 学习一个代码本来表示短动作序列，然后通过 Transformer 将口语文本翻译成代码本标记序列，并引入 sign stitching 技术来有效连接这些标记。在 RWTH-PHOENIX-Weather-2014T (PHOENIX14T) 和 Meine DGS Annotated (mDGS) 数据集上评估，结果显示该方法超过了现有方法，提高了 BLEU-1 回译分数高达 72%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "8 Pages, 3 Figures, 7 Tables, 18th IEEE International Conference on\n  Automatic Face and Gesture Recognition 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11499v1",
      "published_date": "2024-04-17 15:52:38 UTC",
      "updated_date": "2024-04-17 15:52:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:14:48.011979"
    },
    {
      "arxiv_id": "2404.11496v2",
      "title": "Runtime Analysis of Evolutionary Diversity Optimization on the Multi-objective (LeadingOnes, TrailingZeros) Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Denis Antipov",
        "Aneta Neumann",
        "Frank Neumann",
        "Andrew M. Sutton"
      ],
      "abstract": "The diversity optimization is the class of optimization problems, in which we\naim at finding a diverse set of good solutions. One of the frequently used\napproaches to solve such problems is to use evolutionary algorithms which\nevolve a desired diverse population. This approach is called evolutionary\ndiversity optimization (EDO).\n  In this paper, we analyse EDO on a 3-objective function LOTZ$_k$, which is a\nmodification of the 2-objective benchmark function (LeadingOnes,\nTrailingZeros). We prove that the GSEMO computes a set of all Pareto-optimal\nsolutions in $O(kn^3)$ expected iterations. We also analyze the runtime of the\nGSEMO$_D$ (a modification of the GSEMO for diversity optimization) until it\nfinds a population with the best possible diversity for two different diversity\nmeasures, the total imbalance and the sorted imbalances vector. For the first\nmeasure we show that the GSEMO$_D$ optimizes it asymptotically faster than it\nfinds a Pareto-optimal population, in $O(kn^2\\log(n))$ expected iterations, and\nfor the second measure we show an upper bound of $O(k^2n^3\\log(n))$ expected\niterations. We complement our theoretical analysis with an empirical study,\nwhich shows a very similar behavior for both diversity measures that is close\nto the theory predictions.",
      "tldr_zh": "这篇论文分析了进化多样性优化（EDO）在多目标函数 LOTZ_k（基于 LeadingOnes 和 TrailingZeros 的修改版本）上的运行时间，旨在找到一组多样化的良好解决方案。研究证明，GSEMO 算法可以在 O(kn^3) 期望迭代中计算出所有 Pareto-optimal 解决方案。对于 GSEMO_D 算法（GSEMO 的多样性优化修改版本），在 total imbalance 度量下，它能以 O(kn^2 log(n)) 期望迭代优化多样性，比找到 Pareto-optimal 种群更快；在 sorted imbalances vector 度量下，上界为 O(k^2 n^3 log(n)) 期望迭代。经验研究进一步验证了这些理论结果，与预测行为高度一致。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11496v2",
      "published_date": "2024-04-17 15:51:15 UTC",
      "updated_date": "2024-04-19 00:31:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:15:01.529995"
    },
    {
      "arxiv_id": "2404.11492v1",
      "title": "arcjetCV: an open-source software to analyze material ablation",
      "title_zh": "arcjetCV：开源软件，用于分析材料烧蚀",
      "authors": [
        "Alexandre Quintart",
        "Magnus Haw",
        "Federico Semeraro"
      ],
      "abstract": "arcjetCV is an open-source Python software designed to automate time-resolved\nmeasurements of heatshield material recession and recession rates from arcjet\ntest video footage. This new automated and accessible capability greatly\nexceeds previous manual extraction methods, enabling rapid and detailed\ncharacterization of material recession for any sample with a profile video.\narcjetCV automates the video segmentation process using machine learning\nmodels, including a one-dimensional (1D) Convolutional Neural Network (CNN) to\ninfer the time-window of interest, a two-dimensional (2D) CNN for image and\nedge segmentation, and a Local Outlier Factor (LOF) for outlier filtering. A\ngraphical user interface (GUI) simplifies the user experience and an\napplication programming interface (API) allows users to call the core functions\nfrom scripts, enabling video batch processing. arcjetCV's capability to measure\ntime-resolved recession in turn enables characterization of non-linear\nprocesses (shrinkage, swelling, melt flows, etc.), contributing to higher\nfidelity validation and improved modeling of heatshield material performance.\nThe source code associated with this article can be found at\nhttps://github.com/magnus-haw/arcjetCV.",
      "tldr_zh": "arcjetCV 是一个开源 Python 软件，用于从 arcjet 测试视频中自动测量热屏蔽材料的消融（recession）和消融率，大大超越手动方法，提供快速、详细的材料特征化。  \n该软件采用机器学习模型，包括1D CNN 推断感兴趣的时间窗口、2D CNN 进行图像和边缘分割，以及 LOF 用于异常过滤。  \n它配备图形用户界面 (GUI) 和应用编程接口 (API)，简化用户操作并支持视频批量处理，从而表征非线性过程（如收缩、膨胀和熔流）。  \narcjetCV 的开源实现有助于提升热屏蔽材料性能的验证和建模，源代码可在 GitHub 上获取。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11492v1",
      "published_date": "2024-04-17 15:47:26 UTC",
      "updated_date": "2024-04-17 15:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:15:13.002293"
    },
    {
      "arxiv_id": "2404.11488v1",
      "title": "Multi-resolution Rescored ByteTrack for Video Object Detection on Ultra-low-power Embedded Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Luca Bompani",
        "Manuele Rusci",
        "Daniele Palossi",
        "Francesco Conti",
        "Luca Benini"
      ],
      "abstract": "This paper introduces Multi-Resolution Rescored Byte-Track (MR2-ByteTrack), a\nnovel video object detection framework for ultra-low-power embedded processors.\nThis method reduces the average compute load of an off-the-shelf Deep Neural\nNetwork (DNN) based object detector by up to 2.25$\\times$ by alternating the\nprocessing of high-resolution images (320$\\times$320 pixels) with multiple\ndown-sized frames (192$\\times$192 pixels). To tackle the accuracy degradation\ndue to the reduced image input size, MR2-ByteTrack correlates the output\ndetections over time using the ByteTrack tracker and corrects potential\nmisclassification using a novel probabilistic Rescore algorithm. By\ninterleaving two down-sized images for every high-resolution one as the input\nof different state-of-the-art DNN object detectors with our MR2-ByteTrack, we\ndemonstrate an average accuracy increase of 2.16% and a latency reduction of\n43% on the GAP9 microcontroller compared to a baseline frame-by-frame inference\nscheme using exclusively full-resolution images. Code available at:\nhttps://github.com/Bomps4/Multi_Resolution_Rescored_ByteTrack",
      "tldr_zh": "本论文提出了一种名为 Multi-Resolution Rescored ByteTrack (MR2-ByteTrack) 的新型视频物体检测框架，针对超低功耗嵌入式处理器，通过交替处理高分辨率图像（320×320 像素）和缩小帧（192×192 像素），将 Deep Neural Network (DNN) 检测器的平均计算负载减少多达 2.25 倍。框架利用 ByteTrack 追踪器关联输出检测，并引入一个新的概率 Rescore 算法来修正因图像缩小导致的准确率下降。实验结果显示，在 GAP9 微控制器上，与仅使用全分辨率图像的基线方案相比，MR2-ByteTrack 实现了平均准确率提高 2.16% 和延迟减少 43%，并提供了开源代码以支持进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.4"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures Accepted for publication at the Embedded Vision\n  Workshop of the Computer Vision and Pattern Recognition conference, Seattle,\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11488v1",
      "published_date": "2024-04-17 15:45:49 UTC",
      "updated_date": "2024-04-17 15:45:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:15:23.845878"
    },
    {
      "arxiv_id": "2404.11483v2",
      "title": "AgentKit: Structured LLM Reasoning with Dynamic Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Yue Wu",
        "Yewen Fan",
        "So Yeon Min",
        "Shrimai Prabhumoye",
        "Stephen McAleer",
        "Yonatan Bisk",
        "Ruslan Salakhutdinov",
        "Yuanzhi Li",
        "Tom Mitchell"
      ],
      "abstract": "We propose an intuitive LLM prompting framework (AgentKit) for\nmultifunctional agents. AgentKit offers a unified framework for explicitly\nconstructing a complex \"thought process\" from simple natural language prompts.\nThe basic building block in AgentKit is a node, containing a natural language\nprompt for a specific subtask. The user then puts together chains of nodes,\nlike stacking LEGO pieces. The chains of nodes can be designed to explicitly\nenforce a naturally structured \"thought process\". For example, for the task of\nwriting a paper, one may start with the thought process of 1) identify a core\nmessage, 2) identify prior research gaps, etc. The nodes in AgentKit can be\ndesigned and combined in different ways to implement multiple advanced\ncapabilities including on-the-fly hierarchical planning, reflection, and\nlearning from interactions. In addition, due to the modular nature and the\nintuitive design to simulate explicit human thought process, a basic agent\ncould be implemented as simple as a list of prompts for the subtasks and\ntherefore could be designed and tuned by someone without any programming\nexperience. Quantitatively, we show that agents designed through AgentKit\nachieve SOTA performance on WebShop and Crafter. These advances underscore\nAgentKit's potential in making LLM agents effective and accessible for a wider\nrange of applications. https://github.com/holmeswww/AgentKit",
      "tldr_zh": "本研究提出 AgentKit，一种直观的 LLM 提示框架，用于构建多功能代理，通过动态图实现结构化的思考过程。框架的基本构建块是节点，每个节点包含特定子任务的自然语言提示，用户可像搭乐高积木一样连接成链条，以强制执行明确的思考流程，例如分层规划、反思和从互动中学习。AgentKit 的模块化设计允许非程序员轻松设计和调整代理，在 WebShop 和 Crafter 等基准上实现 SOTA 性能，显著提升了 LLM 代理的有效性和可访问性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11483v2",
      "published_date": "2024-04-17 15:40:45 UTC",
      "updated_date": "2024-07-24 20:53:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:15:37.589932"
    },
    {
      "arxiv_id": "2404.11477v3",
      "title": "Discovering Nuclear Models from Symbolic Machine Learning",
      "title_zh": "通过符号化机器学习发现核模型",
      "authors": [
        "Jose M. Munoz",
        "Silviu M. Udrescu",
        "Ronald F. Garcia Ruiz"
      ],
      "abstract": "Numerous phenomenological nuclear models have been proposed to describe\nspecific observables within different regions of the nuclear chart. However,\ndeveloping a unified model that describes the complex behavior of all nuclei\nremains an open challenge. Here, we explore whether novel symbolic Machine\nLearning (ML) can rediscover traditional nuclear physics models or identify\nalternatives with improved simplicity, fidelity, and predictive power. To\naddress this challenge, we developed a Multi-objective Iterated Symbolic\nRegression approach that handles symbolic regressions over multiple target\nobservables, accounts for experimental uncertainties and is robust against\nhigh-dimensional problems. As a proof of principle, we applied this method to\ndescribe the nuclear binding energies and charge radii of light and medium mass\nnuclei. Our approach identified simple analytical relationships based on the\nnumber of protons and neutrons, providing interpretable models with precision\ncomparable to state-of-the-art nuclear models. Additionally, we integrated this\nML-discovered model with an existing complementary model to estimate the limits\nof nuclear stability. These results highlight the potential of symbolic ML to\ndevelop accurate nuclear models and guide our description of complex many-body\nproblems.",
      "tldr_zh": "该研究探讨了符号机器学习（Symbolic Machine Learning）在核物理中的应用，旨在重新发现传统核模型或开发更简单、精确且预测力强的替代模型，以解决统一描述核行为的挑战。研究团队开发了多目标迭代符号回归（Multi-objective Iterated Symbolic Regression）方法，该方法能同时处理多个目标观测值、考虑实验不确定性和应对高维问题，并将其应用于轻质和中质核的结合能和电荷半径。结果显示，该方法发现了基于质子和中子的简单分析关系，其精确度与最先进核模型相当，并通过与现有模型整合，成功估计了核稳定性的极限。这些发现突显了符号机器学习在构建可解释核模型和指导复杂多体问题方面的潜力。",
      "categories": [
        "nucl-th",
        "cs.AI",
        "cs.LG",
        "nucl-ex"
      ],
      "primary_category": "nucl-th",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11477v3",
      "published_date": "2024-04-17 15:32:58 UTC",
      "updated_date": "2024-07-03 14:47:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:15:48.735210"
    },
    {
      "arxiv_id": "2404.11476v1",
      "title": "Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and Regulatory Measures in the EU AI Act",
      "title_zh": "翻译失败",
      "authors": [
        "Sinan Arda"
      ],
      "abstract": "Technological innovations have shown remarkable capabilities to benefit and\nharm society alike. AI constitutes a democratized sophisticated technology\naccessible to large parts of society, including malicious actors. This work\nproposes a taxonomy focusing on on (geo)political risks associated with AI. It\nidentifies 12 risks in total divided into four categories: (1) Geopolitical\nPressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks,\nand (4) Privacy and Trust Violations. Incorporating a regulatory side, this\npaper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the\nlandmark regulation has the potential to have a positive top-down impact\nconcerning AI risk reduction but needs regulatory adjustments to mitigate risks\nmore comprehensively. Regulatory exceptions for open-source models, excessively\nhigh parameters for the classification of GPAI models as a systemic risk, and\nthe exclusion of systems designed exclusively for military purposes from the\nregulation's obligations leave room for future action.",
      "tldr_zh": "本论文提出了一种专注于（地缘）政治风险的AI风险分类法（Taxonomy），共识别12个风险，分为四个类别：(1) Geopolitical Pressures、(2) Malicious Usage、(3) Environmental, Social, and Ethical Risks，以及(4) Privacy and Trust Violations，以帮助评估AI对社会的潜在危害。论文同时评估了EU AI Act（欧盟AI法案），该法案于2023年3月通过，具有减少AI风险的积极影响，但存在不足。研究发现，该法案对开源模型的例外规定、GPAI模型作为系统性风险的门槛过高，以及排除专为军事目的设计的系统等问题，需要进一步调整以更全面地缓解风险。整体而言，此工作为AI监管提供了一个框架，促进更有效的政策制定。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "68T01",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11476v1",
      "published_date": "2024-04-17 15:32:56 UTC",
      "updated_date": "2024-04-17 15:32:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:16:01.046375"
    },
    {
      "arxiv_id": "2404.11475v1",
      "title": "AdaIR: Exploiting Underlying Similarities of Image Restoration Tasks with Adapters",
      "title_zh": "翻译失败",
      "authors": [
        "Hao-Wei Chen",
        "Yu-Syuan Xu",
        "Kelvin C. K. Chan",
        "Hsien-Kai Kuo",
        "Chun-Yi Lee",
        "Ming-Hsuan Yang"
      ],
      "abstract": "Existing image restoration approaches typically employ extensive networks\nspecifically trained for designated degradations. Despite being effective, such\nmethods inevitably entail considerable storage costs and computational\noverheads due to the reliance on task-specific networks. In this work, we go\nbeyond this well-established framework and exploit the inherent commonalities\namong image restoration tasks. The primary objective is to identify components\nthat are shareable across restoration tasks and augment the shared components\nwith modules specifically trained for individual tasks. Towards this goal, we\npropose AdaIR, a novel framework that enables low storage cost and efficient\ntraining without sacrificing performance. Specifically, a generic restoration\nnetwork is first constructed through self-supervised pre-training using\nsynthetic degradations. Subsequent to the pre-training phase, adapters are\ntrained to adapt the pre-trained network to specific degradations. AdaIR\nrequires solely the training of lightweight, task-specific modules, ensuring a\nmore efficient storage and training regimen. We have conducted extensive\nexperiments to validate the effectiveness of AdaIR and analyze the influence of\nthe pre-training strategy on discovering shareable components. Extensive\nexperimental results show that AdaIR achieves outstanding results on multi-task\nrestoration while utilizing significantly fewer parameters (1.9 MB) and less\ntraining time (7 hours) for each restoration task. The source codes and trained\nmodels will be released.",
      "tldr_zh": "本文提出 AdaIR 框架，通过利用图像恢复任务的内在相似性，构建一个通用恢复网络，并使用 adapters 作为任务特定模块来适应不同退化类型，从而显著降低存储成本和训练开销，同时保持高性能。具体来说，AdaIR 先通过自监督预训练合成退化数据来初始化共享网络，然后仅训练轻量级 adapters 以处理特定任务。实验结果显示，AdaIR 在多任务图像恢复中表现出色，仅需 1.9 MB 参数和 7 小时训练时间，就实现了与专用网络相当的恢复效果。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11475v1",
      "published_date": "2024-04-17 15:31:06 UTC",
      "updated_date": "2024-04-17 15:31:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:16:13.291961"
    },
    {
      "arxiv_id": "2404.11461v2",
      "title": "Using Game Engines and Machine Learning to Create Synthetic Satellite Imagery for a Tabletop Verification Exercise",
      "title_zh": "翻译失败",
      "authors": [
        "Johannes Hoster",
        "Sara Al-Sayed",
        "Felix Biessmann",
        "Alexander Glaser",
        "Kristian Hildebrand",
        "Igor Moric",
        "Tuong Vy Nguyen"
      ],
      "abstract": "Satellite imagery is regarded as a great opportunity for citizen-based\nmonitoring of activities of interest. Relevant imagery may however not be\navailable at sufficiently high resolution, quality, or cadence -- let alone be\nuniformly accessible to open-source analysts. This limits an assessment of the\ntrue long-term potential of citizen-based monitoring of nuclear activities\nusing publicly available satellite imagery. In this article, we demonstrate how\nmodern game engines combined with advanced machine-learning techniques can be\nused to generate synthetic imagery of sites of interest with the ability to\nchoose relevant parameters upon request; these include time of day, cloud\ncover, season, or level of activity onsite. At the same time, resolution and\noff-nadir angle can be adjusted to simulate different characteristics of the\nsatellite. While there are several possible use-cases for synthetic imagery,\nhere we focus on its usefulness to support tabletop exercises in which simple\nmonitoring scenarios can be examined to better understand verification\ncapabilities enabled by new satellite constellations and very short revisit\ntimes.",
      "tldr_zh": "本研究探讨了使用公开卫星图像进行公民监控核活动的潜力，但受限于图像分辨率、质量和可用性。该方法结合游戏引擎(game engines)和机器学习(machine learning)技术生成合成卫星图像(synthetic imagery)，允许用户自定义参数如时间、云量、季节或现场活动，并模拟不同卫星的特性，如分辨率和偏离角度(off-nadir angle)。通过这种方式，合成图像支持桌面演习(tabletop exercises)，帮助评估新卫星星座(satellite constellations)和短重访时间带来的验证能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Annual Meeting of the Institute of Nuclear Materials Management\n  (INMM), Vienna",
      "pdf_url": "http://arxiv.org/pdf/2404.11461v2",
      "published_date": "2024-04-17 15:09:31 UTC",
      "updated_date": "2024-06-23 10:38:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:16:24.759209"
    },
    {
      "arxiv_id": "2404.11458v1",
      "title": "Learn to Tour: Operator Design For Solution Feasibility Mapping in Pickup-and-delivery Traveling Salesman Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Fang",
        "Xu Chen",
        "Xuan Di"
      ],
      "abstract": "This paper aims to develop a learning method for a special class of traveling\nsalesman problems (TSP), namely, the pickup-and-delivery TSP (PDTSP), which\nfinds the shortest tour along a sequence of one-to-one pickup-and-delivery\nnodes. One-to-one here means that the transported people or goods are\nassociated with designated pairs of pickup and delivery nodes, in contrast to\nthat indistinguishable goods can be delivered to any nodes. In PDTSP,\nprecedence constraints need to be satisfied that each pickup node must be\nvisited before its corresponding delivery node. Classic operations research\n(OR) algorithms for PDTSP are difficult to scale to large-sized problems.\nRecently, reinforcement learning (RL) has been applied to TSPs. The basic idea\nis to explore and evaluate visiting sequences in a solution space. However,\nthis approach could be less computationally efficient, as it has to potentially\nevaluate many infeasible solutions of which precedence constraints are\nviolated. To restrict solution search within a feasible space, we utilize\noperators that always map one feasible solution to another, without spending\ntime exploring the infeasible solution space. Such operators are evaluated and\nselected as policies to solve PDTSPs in an RL framework. We make a comparison\nof our method and baselines, including classic OR algorithms and existing\nlearning methods. Results show that our approach can find tours shorter than\nbaselines.",
      "tldr_zh": "本研究针对pickup-and-delivery TSP (PDTSP)开发了一种学习方法，该问题要求在旅行 salesman 问题中处理一对一的pickup和delivery节点，并确保pickup节点在对应delivery节点之前访问。作者设计了operators，这些operators能将一个可行解映射到另一个可行解，从而在reinforcement learning (RL)框架中避免探索不可行解空间，提高计算效率。与传统operations research (OR)算法和现有学习方法相比，该方法能够找到更短的路径。实验结果证明了其有效性，为大规模PDTSP问题提供了可扩展的解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11458v1",
      "published_date": "2024-04-17 15:05:51 UTC",
      "updated_date": "2024-04-17 15:05:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:16:36.651832"
    },
    {
      "arxiv_id": "2404.11457v2",
      "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
      "title_zh": "翻译失败",
      "authors": [
        "Sunhao Dai",
        "Chen Xu",
        "Shicheng Xu",
        "Liang Pang",
        "Zhenhua Dong",
        "Jun Xu"
      ],
      "abstract": "With the rapid advancements of large language models (LLMs), information\nretrieval (IR) systems, such as search engines and recommender systems, have\nundergone a significant paradigm shift. This evolution, while heralding new\nopportunities, introduces emerging challenges, particularly in terms of biases\nand unfairness, which may threaten the information ecosystem. In this paper, we\npresent a comprehensive survey of existing works on emerging and pressing bias\nand unfairness issues in IR systems when the integration of LLMs. We first\nunify bias and unfairness issues as distribution mismatch problems, providing a\ngroundwork for categorizing various mitigation strategies through distribution\nalignment. Subsequently, we systematically delve into the specific bias and\nunfairness issues arising from three critical stages of LLMs integration into\nIR systems: data collection, model development, and result evaluation. In doing\nso, we meticulously review and analyze recent literature, focusing on the\ndefinitions, characteristics, and corresponding mitigation strategies\nassociated with these issues. Finally, we identify and highlight some open\nproblems and challenges for future work, aiming to inspire researchers and\nstakeholders in the IR field and beyond to better understand and mitigate bias\nand unfairness issues of IR in this LLM era. We also consistently maintain a\nGitHub repository for the relevant papers and resources in this rising\ndirection at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.",
      "tldr_zh": "这篇论文调查了大型语言模型(LLMs)集成到信息检索(IR)系统（如搜索引擎和推荐系统）中带来的偏见(bias)和不公平(unfairness)新挑战，这些问题可能威胁信息生态系统。作者将这些问题统一为分布不匹配(distribution mismatch)问题，并通过分布对齐(distribution alignment)来分类各种缓解策略。论文系统分析了LLMs在IR系统中的三个关键阶段——数据收集、模型开发和结果评估——并回顾了相关文献，讨论了这些阶段的具体问题、特征和应对方法。最后，论文指出了未来研究的开放问题，并提供了一个GitHub仓库（https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey）以支持进一步探索。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "KDD 2024 Tutorial&Survey; Tutorial Website:\n  https://llm-ir-bias-fairness.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2404.11457v2",
      "published_date": "2024-04-17 15:05:03 UTC",
      "updated_date": "2024-08-21 17:23:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:16:48.768008"
    },
    {
      "arxiv_id": "2404.11447v1",
      "title": "Research on emotionally intelligent dialogue generation based on automatic dialogue system",
      "title_zh": "基于自动对话系统的情感智能对话生成研究",
      "authors": [
        "Jin Wang",
        "JinFei Wang",
        "Shuying Dai",
        "Jiqiang Yu",
        "Keqin Li"
      ],
      "abstract": "Automated dialogue systems are important applications of artificial\nintelligence, and traditional systems struggle to understand user emotions and\nprovide empathetic feedback. This study integrates emotional intelligence\ntechnology into automated dialogue systems and creates a dialogue generation\nmodel with emotional intelligence through deep learning and natural language\nprocessing techniques. The model can detect and understand a wide range of\nemotions and specific pain signals in real time, enabling the system to provide\nempathetic interaction. By integrating the results of the study \"Can artificial\nintelligence detect pain and express pain empathy?\", the model's ability to\nunderstand the subtle elements of pain empathy has been enhanced, setting\nhigher standards for emotional intelligence dialogue systems. The project aims\nto provide theoretical understanding and practical suggestions to integrate\nadvanced emotional intelligence capabilities into dialogue systems, thereby\nimproving user experience and interaction quality.",
      "tldr_zh": "本研究针对传统自动对话系统的不足，提出了一种基于深度学习和自然语言处理技术的对话生成模型，以增强 emotional intelligence 能力。该模型能实时检测和理解用户情绪及特定疼痛信号，并提供共情的互动反馈，同时整合了“Can artificial intelligence detect pain and express pain empathy?”的研究成果，进一步提升了对疼痛共情的微妙把握。实验结果表明，该系统设定了更高的情感智能标准，并为对话系统的优化提供了理论和实用建议，从而改善用户体验和互动质量。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11447v1",
      "published_date": "2024-04-17 14:55:03 UTC",
      "updated_date": "2024-04-17 14:55:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:17:00.400788"
    },
    {
      "arxiv_id": "2404.11446v1",
      "title": "Open-Ended Wargames with Large Language Models",
      "title_zh": "基于大语言模型的开放式战争游戏",
      "authors": [
        "Daniel P. Hogan",
        "Andrea Brennen"
      ],
      "abstract": "Wargames are a powerful tool for understanding and rehearsing real-world\ndecision making. Automated play of wargames using artificial intelligence (AI)\nenables possibilities beyond those of human-conducted games, such as playing\nthe game many times over to see a range of possible outcomes. There are two\ncategories of wargames: quantitative games, with discrete types of moves, and\nqualitative games, which revolve around open-ended responses. Historically,\nautomation efforts have focused on quantitative games, but large language\nmodels (LLMs) make it possible to automate qualitative wargames. We introduce\n\"Snow Globe,\" an LLM-powered multi-agent system for playing qualitative\nwargames. With Snow Globe, every stage of a text-based qualitative wargame from\nscenario preparation to post-game analysis can be optionally carried out by AI,\nhumans, or a combination thereof. We describe its software architecture\nconceptually and release an open-source implementation alongside this\npublication. As case studies, we simulate a tabletop exercise about an AI\nincident response and a political wargame about a geopolitical crisis. We\ndiscuss potential applications of the approach and how it fits into the broader\nwargaming ecosystem.",
      "tldr_zh": "该论文探讨了使用大型语言模型(LLMs)自动化开放式 wargames，以增强决策模拟和结果分析。该研究引入了\"Snow Globe\"系统，一个基于LLMs的多智能体框架，能够处理文本-based 定性 wargames 的所有阶段，包括场景准备、游戏进行和后分析，支持AI、人类或混合模式。实验通过模拟AI事件响应和地缘政治危机的案例研究，展示了该系统比传统量化wargames更灵活，并讨论了其在wargaming生态系统中的潜在应用和扩展可能性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11446v1",
      "published_date": "2024-04-17 14:54:58 UTC",
      "updated_date": "2024-04-17 14:54:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:17:14.146142"
    },
    {
      "arxiv_id": "2404.11443v1",
      "title": "Prediction of Unmanned Surface Vessel Motion Attitude Based on CEEMDAN-PSO-SVM",
      "title_zh": "基于 CEEMDAN-PSO-SVM 的",
      "authors": [
        "Zhuoya Geng",
        "Jianmei Chen",
        "Wanqiang Zhu"
      ],
      "abstract": "Unmanned boats, while navigating at sea, utilize active compensation systems\nto mitigate wave disturbances experienced by onboard instruments and equipment.\nHowever, there exists a lag in the measurement of unmanned boat attitudes, thus\nintroducing unmanned boat motion attitude prediction to compensate for the lag\nin the signal acquisition process. This paper, based on the basic principles of\nwaves, derives the disturbance patterns of waves on unmanned boats from the\nwave energy spectrum. Through simulation analysis of unmanned boat motion\nattitudes, motion attitude data is obtained, providing experimental data for\nsubsequent work. A combined prediction model based on Complete Ensemble\nEmpirical Mode Decomposition with Adaptive Noise (CEEMDAN), Particle Swarm\nOptimization (PSO), and Support Vector Machine (SVM) is designed to predict the\nmotion attitude of unmanned boats. Simulation results validate its superior\nprediction accuracy compared to traditional prediction models. For example, in\nterms of mean absolute error, it improves by 17% compared to the EMD-PSO-SVM\nmodel.",
      "tldr_zh": "本论文针对无人船(Unmanned Surface Vessel)姿态测量的延迟问题，提出了一种基于波浪基本原理的干扰模式推导和模拟分析方法，以获取运动姿态数据。研究设计了CEEMDAN-PSO-SVM组合预测模型，其中CEEMDAN用于信号分解、PSO优化参数、SVM进行预测，以补偿信号采集滞后。模拟结果显示，该模型的预测准确性显著提升，与EMD-PSO-SVM模型相比，平均绝对误差(Mean Absolute Error)改善了17%。这为无人船导航中的主动补偿系统提供了更可靠的技术支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11443v1",
      "published_date": "2024-04-17 14:53:03 UTC",
      "updated_date": "2024-04-17 14:53:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:17:23.995077"
    },
    {
      "arxiv_id": "2404.11431v2",
      "title": "Instantiations and Computational Aspects of Non-Flat Assumption-based Argumentation",
      "title_zh": "非平坦",
      "authors": [
        "Tuomo Lehtonen",
        "Anna Rapberger",
        "Francesca Toni",
        "Markus Ulbricht",
        "Johannes P. Wallner"
      ],
      "abstract": "Most existing computational tools for assumption-based argumentation (ABA)\nfocus on so-called flat frameworks, disregarding the more general case. In this\npaper, we study an instantiation-based approach for reasoning in possibly\nnon-flat ABA. We make use of a semantics-preserving translation between ABA and\nbipolar argumentation frameworks (BAFs). By utilizing compilability theory, we\nestablish that the constructed BAFs will in general be of exponential size. In\norder to keep the number of arguments and computational cost low, we present\nthree ways of identifying redundant arguments. Moreover, we identify fragments\nof ABA which admit a poly-sized instantiation. We propose two algorithmic\napproaches for reasoning in possibly non-flat ABA. The first approach utilizes\nthe BAF instantiation while the second works directly without constructing\narguments. An empirical evaluation shows that the former outperforms the latter\non many instances, reflecting the lower complexity of BAF reasoning. This\nresult is in contrast to flat ABA, where direct approaches dominate\ninstantiation-based approaches.",
      "tldr_zh": "本文探讨了非平坦 assumption-based argumentation (ABA) 的实例化和计算方面，提出了一种基于 semantics-preserving translation 到 bipolar argumentation frameworks (BAFs) 的推理方法，以处理非平坦框架的复杂性。通过 compilability theory 证明了构建的 BAFs 可能呈指数大小，并介绍了三种识别冗余 arguments 的方式，以及支持多项式大小实例化的 ABA 片段。实验评估显示，使用 BAF 实例化的算法在许多情况下优于直接方法，这与平坦 ABA 的性能模式相反。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11431v2",
      "published_date": "2024-04-17 14:36:47 UTC",
      "updated_date": "2024-05-24 13:42:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:17:38.031912"
    },
    {
      "arxiv_id": "2404.11422v2",
      "title": "Short-term wind speed forecasting model based on an attention-gated recurrent neural network and error correction strategy",
      "title_zh": "翻译失败",
      "authors": [
        "Haojian Huang"
      ],
      "abstract": "The accurate wind speed series forecast is very pivotal to security of grid\ndispatching and the application of wind power. Nevertheless, on account of\ntheir nonlinear and non-stationary nature, their short-term forecast is\nextremely challenging. Therefore, this dissertation raises one short-term wind\nspeed forecast pattern on the foundation of attention with an improved gated\nrecurrent neural network (AtGRU) and a tactic of error correction. That model\nuses the AtGRU model as the preliminary predictor and the GRU model as the\nerror corrector. At the beginning, SSA (singular spectrum analysis) is employed\nin previous wind speed series for lessening the noise. Subsequently, historical\nwind speed series is going to be used for the predictor training. During this\nprocess, the prediction can have certain errors. The sequence of these errors\nprocessed by variational modal decomposition (VMD) is used to train the\ncorrector of error. The eventual forecast consequence is just the sum of\npredictor forecast and error corrector. The proposed SSA-AtGRU-VMD-GRU model\noutperforms the compared models in three case studies on Woodburn, St. Thomas,\nand Santa Cruz. It is indicated that the model evidently enhances the\ncorrection of the wind speed forecast.",
      "tldr_zh": "该研究提出了一种基于注意力门控循环神经网络(AtGRU)和错误修正策略的短期风速预测模型，以应对风速序列的非线性非平稳特性。模型首先使用SSA(singular spectrum analysis)减少风速数据中的噪声，然后以AtGRU作为初步预测器，并通过VMD(variational modal decomposition)处理预测错误序列，再利用GRU模型进行错误修正，最终预测结果为初步预测与修正之和。在Woodburn、St. Thomas和Santa Cruz的三个案例研究中，该SSA-AtGRU-VMD-GRU模型显著优于对比模型，提高了风速预测的准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.ao-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 11 figures, 6 tables, Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2404.11422v2",
      "published_date": "2024-04-17 14:27:45 UTC",
      "updated_date": "2024-04-22 11:21:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:17:48.907730"
    },
    {
      "arxiv_id": "2404.11408v1",
      "title": "DUPE: Detection Undermining via Prompt Engineering for Deepfake Text",
      "title_zh": "翻译失败",
      "authors": [
        "James Weichert",
        "Chinecherem Dimobi"
      ],
      "abstract": "As large language models (LLMs) become increasingly commonplace, concern\nabout distinguishing between human and AI text increases as well. The growing\npower of these models is of particular concern to teachers, who may worry that\nstudents will use LLMs to write school assignments. Facing a technology with\nwhich they are unfamiliar, teachers may turn to publicly-available AI text\ndetectors. Yet the accuracy of many of these detectors has not been thoroughly\nverified, posing potential harm to students who are falsely accused of academic\ndishonesty. In this paper, we evaluate three different AI text\ndetectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human\nand AI-generated essays. We find that watermarking results in a high false\npositive rate, and that ZeroGPT has both high false positive and false negative\nrates. Further, we are able to significantly increase the false negative rate\nof all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated\ntexts, thereby effectively bypassing the detectors.",
      "tldr_zh": "本文研究了大型语言模型 (LLMs) 生成的 AI 文本检测问题，评估了三种检测器——Kirchenbauer et al. 的水印、ZeroGPT 和 GPTZero——在人类和 AI 生成论文中的表现。结果显示，水印检测器有高假阳性率，而 ZeroGPT 存在高假阳性和假阴性率，可能导致对学生的错误学术不端指控。通过使用 ChatGPT 3.5 改写 AI 生成文本，研究者成功显著提高了所有检测器的假阴性率，从而证明了通过提示工程 (Prompt Engineering) 可以有效规避这些检测器。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11408v1",
      "published_date": "2024-04-17 14:10:27 UTC",
      "updated_date": "2024-04-17 14:10:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:18:02.835970"
    },
    {
      "arxiv_id": "2404.11370v3",
      "title": "Characterizing and modeling harms from interactions with design patterns in AI interfaces",
      "title_zh": "翻译失败",
      "authors": [
        "Lujain Ibrahim",
        "Luc Rocher",
        "Ana Valdivia"
      ],
      "abstract": "The proliferation of applications using artificial intelligence (AI) systems\nhas led to a growing number of users interacting with these systems through\nsophisticated interfaces. Human-computer interaction research has long shown\nthat interfaces shape both user behavior and user perception of technical\ncapabilities and risks. Yet, practitioners and researchers evaluating the\nsocial and ethical risks of AI systems tend to overlook the impact of\nanthropomorphic, deceptive, and immersive interfaces on human-AI interactions.\nHere, we argue that design features of interfaces with adaptive AI systems can\nhave cascading impacts, driven by feedback loops, which extend beyond those\npreviously considered. We first conduct a scoping review of AI interface\ndesigns and their negative impact to extract salient themes of potentially\nharmful design patterns in AI interfaces. Then, we propose Design-Enhanced\nControl of AI systems (DECAI), a conceptual model to structure and facilitate\nimpact assessments of AI interface designs. DECAI draws on principles from\ncontrol systems theory -- a theory for the analysis and design of dynamic\nphysical systems -- to dissect the role of the interface in human-AI systems.\nThrough two case studies on recommendation systems and conversational language\nmodel systems, we show how DECAI can be used to evaluate AI interface designs.",
      "tldr_zh": "该研究探讨了 AI 接口设计模式（如 anthropomorphic、deceptive 和 immersive）如何通过反馈循环放大用户行为和风险感知，从而导致 AI 系统中的潜在危害。作者首先进行文献综述，提取了常见的有害设计模式。论文提出 DECAI（Design-Enhanced Control of AI systems）模型，该模型借鉴控制系统理论，用于系统评估 AI 接口设计的影响。最后，通过推荐系统和对话语言模型的案例研究，展示了 DECAI 如何帮助识别和缓解这些风险。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Fixed issue with subsection titles",
      "pdf_url": "http://arxiv.org/pdf/2404.11370v3",
      "published_date": "2024-04-17 13:30:45 UTC",
      "updated_date": "2024-05-20 19:23:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:18:13.437816"
    },
    {
      "arxiv_id": "2404.11350v2",
      "title": "Calibrating Bayesian Learning via Regularization, Confidence Minimization, and Selective Inference",
      "title_zh": "通过正则化、置信度最小化和选择性推断校准贝叶斯学习",
      "authors": [
        "Jiayi Huang",
        "Sangwoo Park",
        "Osvaldo Simeone"
      ],
      "abstract": "The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs.",
      "tldr_zh": "本研究针对人工智能(AI)模型在工程等领域中可靠性量化的挑战，提出了一种扩展变分推理(VI)-based Bayesian learning 的方法，以提升模型在 in-distribution (ID) 输入上的校准准确性和 out-of-distribution (OOD) 输入的检测能力。该方法依次引入校准正则化(calibration regularization)形成 CBNN，然后整合 OOD 置信度最小化(confidence minimization)得到 CBNN-OCM，最后添加选择性校准(selective calibration)产生 SCBNN-OCM，以实现这些组件的协同作用。数值实验显示，SCBNN-OCM 在 ID 准确率和 OOD 性能上优于现有最先进方法，但需以拒绝部分输入为代价，展示了 ID 校准、OOD 校准与准确率之间的权衡。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2404.11350v2",
      "published_date": "2024-04-17 13:08:26 UTC",
      "updated_date": "2024-12-31 23:02:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:18:24.878165"
    },
    {
      "arxiv_id": "2404.11343v2",
      "title": "Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System",
      "title_zh": "翻译失败",
      "authors": [
        "Sein Kim",
        "Hongseok Kang",
        "Seungyoon Choi",
        "Donghyun Kim",
        "Minchul Yang",
        "Chanyoung Park"
      ],
      "abstract": "Collaborative filtering recommender systems (CF-RecSys) have shown successive\nresults in enhancing the user experience on social media and e-commerce\nplatforms. However, as CF-RecSys struggles under cold scenarios with sparse\nuser-item interactions, recent strategies have focused on leveraging modality\ninformation of user/items (e.g., text or images) based on pre-trained modality\nencoders and Large Language Models (LLMs). Despite their effectiveness under\ncold scenarios, we observe that they underperform simple traditional\ncollaborative filtering models under warm scenarios due to the lack of\ncollaborative knowledge. In this work, we propose an efficient All-round\nLLM-based Recommender system, called A-LLMRec, that excels not only in the cold\nscenario but also in the warm scenario. Our main idea is to enable an LLM to\ndirectly leverage the collaborative knowledge contained in a pre-trained\nstate-of-the-art CF-RecSys so that the emergent ability of the LLM as well as\nthe high-quality user/item embeddings that are already trained by the\nstate-of-the-art CF-RecSys can be jointly exploited. This approach yields two\nadvantages: (1) model-agnostic, allowing for integration with various existing\nCF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typically\nrequired for LLM-based recommenders. Our extensive experiments on various\nreal-world datasets demonstrate the superiority of A-LLMRec in various\nscenarios, including cold/warm, few-shot, cold user, and cross-domain\nscenarios. Beyond the recommendation task, we also show the potential of\nA-LLMRec in generating natural language outputs based on the understanding of\nthe collaborative knowledge by performing a favorite genre prediction task. Our\ncode is available at https://github.com/ghdtjr/A-LLMRec .",
      "tldr_zh": "本文提出 A-LLMRec，一种高效的基于 Large Language Models (LLMs) 的推荐系统，旨在解决 Collaborative Filtering (CF-RecSys) 在冷启动场景下表现不佳的问题，同时在温启动场景中超越传统模型。核心方法是让 LLMs 直接利用预训练的 state-of-the-art CF-RecSys 中的协作知识，实现模型无关的整合和高效运行，而无需大规模微调。实验结果显示，A-LLMRec 在各种真实数据集上表现出色，包括冷/温启动、少样本、冷用户和跨域场景，并能生成自然语言输出，如预测用户偏好类型。代码已在 GitHub 开源。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11343v2",
      "published_date": "2024-04-17 13:03:07 UTC",
      "updated_date": "2024-06-01 07:08:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:18:38.823283"
    },
    {
      "arxiv_id": "2404.11341v2",
      "title": "The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology",
      "title_zh": "翻译失败",
      "authors": [
        "Juan L. Gamella",
        "Jonas Peters",
        "Peter Bühlmann"
      ],
      "abstract": "In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber.",
      "tldr_zh": "该研究针对AI、机器学习和统计领域中新方法验证的难题——真实世界数据集的稀缺性——提出了一种解决方案：构建了两个名为Causal Chambers的计算机控制设备。这些设备能快速、低成本地从非平凡但易懂的物理系统中生成大量数据集，允许操纵和测量变量，从而为因果发现、out-of-distribution generalization、变化点检测、独立成分分析和符号回归等算法提供丰富的测试环境。通过案例研究，研究者展示了这些设备在因果推理中的应用，并提供了每个Causal Chambers的因果模型作为ground truth；所有硬件、软件和数据集均开源可用，以促进进一步研究。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "40 pages, 20 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11341v2",
      "published_date": "2024-04-17 13:00:52 UTC",
      "updated_date": "2024-08-26 12:14:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:18:49.353851"
    },
    {
      "arxiv_id": "2404.11335v1",
      "title": "SoccerNet Game State Reconstruction: End-to-End Athlete Tracking and Identification on a Minimap",
      "title_zh": "翻译失败",
      "authors": [
        "Vladimir Somers",
        "Victor Joos",
        "Anthony Cioppa",
        "Silvio Giancola",
        "Seyed Abolfazl Ghasemzadeh",
        "Floriane Magera",
        "Baptiste Standaert",
        "Amir Mohammad Mansourian",
        "Xin Zhou",
        "Shohreh Kasaei",
        "Bernard Ghanem",
        "Alexandre Alahi",
        "Marc Van Droogenbroeck",
        "Christophe De Vleeschouwer"
      ],
      "abstract": "Tracking and identifying athletes on the pitch holds a central role in\ncollecting essential insights from the game, such as estimating the total\ndistance covered by players or understanding team tactics. This tracking and\nidentification process is crucial for reconstructing the game state, defined by\nthe athletes' positions and identities on a 2D top-view of the pitch, (i.e. a\nminimap). However, reconstructing the game state from videos captured by a\nsingle camera is challenging. It requires understanding the position of the\nathletes and the viewpoint of the camera to localize and identify players\nwithin the field. In this work, we formalize the task of Game State\nReconstruction and introduce SoccerNet-GSR, a novel Game State Reconstruction\ndataset focusing on football videos. SoccerNet-GSR is composed of 200 video\nsequences of 30 seconds, annotated with 9.37 million line points for pitch\nlocalization and camera calibration, as well as over 2.36 million athlete\npositions on the pitch with their respective role, team, and jersey number.\nFurthermore, we introduce GS-HOTA, a novel metric to evaluate game state\nreconstruction methods. Finally, we propose and release an end-to-end baseline\nfor game state reconstruction, bootstrapping the research on this task. Our\nexperiments show that GSR is a challenging novel task, which opens the field\nfor future research. Our dataset and codebase are publicly available at\nhttps://github.com/SoccerNet/sn-gamestate.",
      "tldr_zh": "该论文针对足球视频分析，形式化了Game State Reconstruction任务，即从单一摄像机视频中重建运动员位置和身份，并将其显示在2D顶视图的minimap上。该研究引入了SoccerNet-GSR数据集，包含200个30秒视频序列，标注了937万线点用于场地定位和相机校准，以及超过236万运动员位置信息，包括角色、团队和球衣号码。此外，论文提出了GS-HOTA指标来评估重建方法，并发布了一个端到端baseline模型。实验结果表明，这一任务极具挑战性，为未来足球视频分析研究提供了新方向，数据集和代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11335v1",
      "published_date": "2024-04-17 12:53:45 UTC",
      "updated_date": "2024-04-17 12:53:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:19:02.134062"
    },
    {
      "arxiv_id": "2404.11317v2",
      "title": "Improving Composed Image Retrieval via Contrastive Learning with Scaling Positives and Negatives",
      "title_zh": "通过扩展正负样本的对比学习改善组合图像检索",
      "authors": [
        "Zhangchi Feng",
        "Richong Zhang",
        "Zhijie Nie"
      ],
      "abstract": "The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.",
      "tldr_zh": "本文针对 Composed Image Retrieval (CIR) 任务，提出一种通过扩展正例和负例的对比学习方法来提升检索性能。具体方法包括使用 multi-modal large language model 生成三元组数据以增加正例数量，以及设计两阶段 fine-tuning 框架，在第二阶段引入大量静态负例表示来优化表示空间。这些改进易于集成到现有模型中，且实验在 FashionIQ 和 CIRR 数据集上取得了 state-of-the-art 结果，同时在零样本 CIR 场景中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ACM MM 2024 Regular Papers",
      "pdf_url": "http://arxiv.org/pdf/2404.11317v2",
      "published_date": "2024-04-17 12:30:54 UTC",
      "updated_date": "2024-08-07 13:20:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:19:14.484908"
    },
    {
      "arxiv_id": "2404.11313v1",
      "title": "NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods and Results",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Li",
        "Kun Yuan",
        "Yajing Pei",
        "Yiting Lu",
        "Ming Sun",
        "Chao Zhou",
        "Zhibo Chen",
        "Radu Timofte",
        "Wei Sun",
        "Haoning Wu",
        "Zicheng Zhang",
        "Jun Jia",
        "Zhichao Zhang",
        "Linhan Cao",
        "Qiubo Chen",
        "Xiongkuo Min",
        "Weisi Lin",
        "Guangtao Zhai",
        "Jianhui Sun",
        "Tianyi Wang",
        "Lei Li",
        "Han Kong",
        "Wenxuan Wang",
        "Bing Li",
        "Cheng Luo",
        "Haiqiang Wang",
        "Xiangguang Chen",
        "Wenhui Meng",
        "Xiang Pan",
        "Huiying Shi",
        "Han Zhu",
        "Xiaozhong Xu",
        "Lei Sun",
        "Zhenzhong Chen",
        "Shan Liu",
        "Fangyuan Kong",
        "Haotian Fan",
        "Yifang Xu",
        "Haoran Xu",
        "Mengduo Yang",
        "Jie Zhou",
        "Jiaze Li",
        "Shijie Wen",
        "Mai Xu",
        "Da Li",
        "Shunyu Yao",
        "Jiazhi Du",
        "Wangmeng Zuo",
        "Zhibo Li",
        "Shuai He",
        "Anlong Ming",
        "Huiyuan Fu",
        "Huadong Ma",
        "Yong Wu",
        "Fie Xue",
        "Guozhi Zhao",
        "Lina Du",
        "Jie Guo",
        "Yu Zhang",
        "Huimin Zheng",
        "Junhao Chen",
        "Yue Liu",
        "Dulan Zhou",
        "Kele Xu",
        "Qisheng Xu",
        "Tao Sun",
        "Zhixiang Ding",
        "Yuhang Hu"
      ],
      "abstract": "This paper reviews the NTIRE 2024 Challenge on Shortform UGC Video Quality\nAssessment (S-UGC VQA), where various excellent solutions are submitted and\nevaluated on the collected dataset KVQ from popular short-form video platform,\ni.e., Kuaishou/Kwai Platform. The KVQ database is divided into three parts,\nincluding 2926 videos for training, 420 videos for validation, and 854 videos\nfor testing. The purpose is to build new benchmarks and advance the development\nof S-UGC VQA. The competition had 200 participants and 13 teams submitted valid\nsolutions for the final testing phase. The proposed solutions achieved\nstate-of-the-art performances for S-UGC VQA. The project can be found at\nhttps://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024.",
      "tldr_zh": "这篇论文回顾了 NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment (S-UGC VQA)，旨在通过构建新基准促进短视频 UGC 视频质量评估的发展。挑战赛使用了 KVQ 数据集，其中包括 2926 个训练视频、420 个验证视频和 854 个测试视频。共有 200 名参与者，13 队提交了有效解决方案，这些方案在 S-UGC VQA 上达到了 state-of-the-art 性能，并可通过 GitHub 项目（https://github.com/lixinustc/KVQChallenge-CVPR-NTIRE2024）获取。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted by CVPR2024 Workshop. The challenge report for CVPR\n  NTIRE2024 Short-form UGC Video Quality Assessment Challenge",
      "pdf_url": "http://arxiv.org/pdf/2404.11313v1",
      "published_date": "2024-04-17 12:26:13 UTC",
      "updated_date": "2024-04-17 12:26:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:19:27.516018"
    },
    {
      "arxiv_id": "2406.15377v1",
      "title": "Model Callers for Transforming Predictive and Generative AI Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Mukesh Dalal"
      ],
      "abstract": "We introduce a novel software abstraction termed \"model caller,\" acting as an\nintermediary for AI and ML model calling, advocating its transformative utility\nbeyond existing model-serving frameworks. This abstraction offers multiple\nadvantages: enhanced accuracy and reduced latency in model predictions,\nsuperior monitoring and observability of models, more streamlined AI system\narchitectures, simplified AI development and management processes, and improved\ncollaboration and accountability across AI/ML/Data Science, software, data, and\noperations teams. Model callers are valuable for both creators and users of\nmodels within both predictive and generative AI applications. Additionally, we\nhave developed and released a prototype Python library for model callers,\naccessible for installation via pip or for download from GitHub.",
      "tldr_zh": "该研究引入了“model caller”这一新型软件抽象，作为AI和ML模型调用的中介，旨在超越现有模型服务框架。该抽象带来多项优势，包括提升模型预测的准确性、减少延迟、增强模型监控和可观察性、简化AI系统架构以及优化AI开发、管理和跨团队协作。“Model callers”适用于预测和生成式AI应用的模型创建者和用户，并已发布一个开源Python原型库，可通过pip安装或从GitHub获取。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "cs.NE",
        "cs.PL",
        "cs.SE",
        "68T05 (Primary) 68T07, 68N19, 68T35 (Secondary)",
        "I.2.0; I.2.1; I.2.5; I.2.11; D.2.11; D.3.3; H.1.2; J.0"
      ],
      "primary_category": "cs.CY",
      "comment": "18 pages, 14 figures",
      "pdf_url": "http://arxiv.org/pdf/2406.15377v1",
      "published_date": "2024-04-17 12:21:06 UTC",
      "updated_date": "2024-04-17 12:21:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:19:41.344946"
    },
    {
      "arxiv_id": "2404.11296v2",
      "title": "How to Exhibit More Predictable Behaviors",
      "title_zh": "如何表现出更可预测的行为",
      "authors": [
        "Salomé Lepers",
        "Sophie Lemonnier",
        "Vincent Thomas",
        "Olivier Buffet"
      ],
      "abstract": "This paper looks at predictability problems, i.e., wherein an agent must\nchoose its strategy in order to optimize the predictions that an external\nobserver could make. We address these problems while taking into account\nuncertainties on the environment dynamics and on the observed agent's policy.\nTo that end, we assume that the observer 1. seeks to predict the agent's future\naction or state at each time step, and 2. models the agent using a stochastic\npolicy computed from a known underlying problem, and we leverage on the\nframework of observer-aware Markov decision processes (OAMDPs). We propose\naction and state predictability performance criteria through reward functions\nbuilt on the observer's belief about the agent policy; show that these induced\npredictable OAMDPs can be represented by goal-oriented or discounted MDPs; and\nanalyze the properties of the proposed reward functions both theoretically and\nempirically on two types of grid-world problems.",
      "tldr_zh": "该论文探讨了代理（agent）在不确定环境中选择策略以优化外部观察者预测的问题，考虑了环境动态和代理策略的不确定性。作者假设观察者使用从已知问题计算的随机策略（stochastic policy）来预测代理的未来行动或状态，并基于观察者感知马尔可夫决策过程（OAMDPs）框架提出新的行动和状态可预测性性能标准。这些标准通过基于观察者信念的奖励函数构建，并证明可预测 OAMDPs 可以表示为目标导向或折扣 MDPs。论文通过理论分析和在两种网格世界（grid-world）问题上的经验验证，展示了这些方法的有效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 14 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.11296v2",
      "published_date": "2024-04-17 12:06:17 UTC",
      "updated_date": "2024-10-07 13:06:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:19:53.087052"
    },
    {
      "arxiv_id": "2404.11290v1",
      "title": "Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based Online Intelligent Education Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Liu",
        "Junhao Shen",
        "Hong Qian",
        "Aimin Zhou"
      ],
      "abstract": "Cognitive diagnosis aims to gauge students' mastery levels based on their\nresponse logs. Serving as a pivotal module in web-based online intelligent\neducation systems (WOIESs), it plays an upstream and fundamental role in\ndownstream tasks like learning item recommendation and computerized adaptive\ntesting. WOIESs are open learning environment where numerous new students\nconstantly register and complete exercises. In WOIESs, efficient cognitive\ndiagnosis is crucial to fast feedback and accelerating student learning.\nHowever, the existing cognitive diagnosis methods always employ intrinsically\ntransductive student-specific embeddings, which become slow and costly due to\nretraining when dealing with new students who are unseen during training. To\nthis end, this paper proposes an inductive cognitive diagnosis model (ICDM) for\nfast new students' mastery levels inference in WOIESs. Specifically, in ICDM,\nwe propose a novel student-centered graph (SCG). Rather than inferring mastery\nlevels through updating student-specific embedding, we derive the inductive\nmastery levels as the aggregated outcomes of students' neighbors in SCG.\nNamely, SCG enables to shift the task from finding the most suitable\nstudent-specific embedding that fits the response logs to finding the most\nsuitable representations for different node types in SCG, and the latter is\nmore efficient since it no longer requires retraining. To obtain this\nrepresentation, ICDM consists of a\nconstruction-aggregation-generation-transformation process to learn the final\nrepresentation of students, exercises and concepts. Extensive experiments\nacross real-world datasets show that, compared with the existing cognitive\ndiagnosis methods that are always transductive, ICDM is much more faster while\nmaintains the competitive inference performance for new students.",
      "tldr_zh": "本论文针对基于网络的在线智能教育系统（WOIESs）中认知诊断（Cognitive Diagnosis）的效率问题，提出了一种Inductive Cognitive Diagnosis Model (ICDM)，以快速推断新学生的掌握水平，避免了传统方法依赖Transductive student-specific embeddings导致的重训开销。ICDM引入Student-Centered Graph (SCG)，通过聚合学生在SCG中的邻居信息，并采用construction-aggregation-generation-transformation过程来学习节点表示，从而实现高效的归纳式推理。实验结果显示，ICDM在真实数据集上比现有Transductive方法更快，同时保持了竞争性的性能，为加速学生学习提供了有效支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "WWW 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11290v1",
      "published_date": "2024-04-17 11:55:43 UTC",
      "updated_date": "2024-04-17 11:55:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:20:05.006191"
    },
    {
      "arxiv_id": "2404.11280v2",
      "title": "Image Generative Semantic Communication with Multi-Modal Similarity Estimation for Resource-Limited Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Eri Hosonuma",
        "Taku Yamazaki",
        "Takumi Miyoshi",
        "Akihito Taya",
        "Yuuki Nishiyama",
        "Kaoru Sezaki"
      ],
      "abstract": "To reduce network traffic and support environments with limited resources, a\nmethod for transmitting images with minimal transmission data is required.\nSeveral machine learning-based image compression methods, which compress the\ndata size of images while maintaining their features, have been proposed.\nHowever, in certain situations, reconstructing only the semantic information of\nimages at the receiver end may be sufficient. To realize this concept,\nsemantic-information-based communication, called semantic communication, has\nbeen proposed, along with an image transmission method using semantic\ncommunication. This method transmits only the semantic information of an image,\nand the receiver reconstructs it using an image-generation model. This method\nutilizes a single type of semantic information for image reconstruction, but\nreconstructing images similar to the original image using only this information\nis challenging. This study proposes a multi-modal image transmission method\nthat leverages various types of semantic information for efficient semantic\ncommunication. The proposed method extracts multi-modal semantic information\nfrom an original image and transmits only that to a receiver. Subsequently, the\nreceiver generates multiple images using an image-generation model and selects\nan output image based on semantic similarity. The receiver must select the\nresult based only on the received features; however, evaluating the similarity\nusing conventional metrics is challenging. Therefore, this study explores new\nmetrics to evaluate the similarity between semantic features of images and\nproposes two scoring procedures for evaluating semantic similarity between\nimages based on multiple semantic features. The results indicate that the\nproposed procedures can compare semantic similarities, such as position and\ncomposition, between the semantic features of the original and generated\nimages.",
      "tldr_zh": "该论文针对资源有限的网络环境，提出了一种基于多模态语义信息的图像生成语义通信（semantic communication）方法，以减少传输数据并维持图像关键特征。该方法从原图像提取多种语义信息，仅传输这些信息至接收端，接收端随后使用图像生成模型生成多个图像，并通过新提出的语义相似度评估机制选择最匹配的输出图像。为解决传统指标的局限性，研究探索并开发了两种基于多模态语义特征的评分程序，用于评估图像间的语义相似度，如位置和组成。实验结果表明，该方法能有效比较原图和生成图的语义相似度，提升了资源受限场景下的图像传输效率。",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "14 pages, 15 figures, this paper has been submitted to IEICE\n  Transactions on Communications",
      "pdf_url": "http://arxiv.org/pdf/2404.11280v2",
      "published_date": "2024-04-17 11:42:39 UTC",
      "updated_date": "2024-08-03 04:49:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:20:17.846673"
    },
    {
      "arxiv_id": "2404.11276v2",
      "title": "Towards Data-Centric Automatic R&D",
      "title_zh": "翻译失败",
      "authors": [
        "Haotian Chen",
        "Xinjie Shen",
        "Zeqi Ye",
        "Wenjun Feng",
        "Haoxue Wang",
        "Xiao Yang",
        "Xu Yang",
        "Weiqing Liu",
        "Jiang Bian"
      ],
      "abstract": "The progress of humanity is driven by those successful discoveries\naccompanied by countless failed experiments. Researchers often seek the\npotential research directions by reading and then verifying them through\nexperiments. The process imposes a significant burden on researchers. In the\npast decade, the data-driven black-box deep learning method has demonstrated\nits effectiveness in a wide range of real-world scenarios, which exacerbates\nthe experimental burden of researchers and thus renders the potential\nsuccessful discoveries veiled. Therefore, automating such a research and\ndevelopment (R&D) process is an urgent need. In this paper, we serve as the\nfirst effort to formalize the goal by proposing a Real-world Data-centric\nautomatic R&D Benchmark, namely RD2Bench. RD2Bench benchmarks all the\noperations in data-centric automatic R&D (D-CARD) as a whole to navigate future\nwork toward our goal directly. We focus on evaluating the interaction and\nsynergistic effects of various model capabilities and aiding in selecting\nwell-performing trustworthy models. Although RD2Bench is very challenging to\nthe state-of-the-art (SOTA) large language model (LLM) named GPT-4, indicating\nample research opportunities and more research efforts, LLMs possess promising\npotential to bring more significant development to D-CARD: They are able to\nimplement some simple methods without adopting any additional techniques. We\nappeal to future work to take developing techniques for tackling automatic R&D\ninto consideration, thus bringing the opportunities of the potential\nrevolutionary upgrade to human productivity.",
      "tldr_zh": "该论文探讨了自动化研究与开发 (R&D) 的必要性，以缓解研究者因实验负担和数据驱动深度学习方法而面临的挑战。作者提出首个 Real-world Data-centric automatic R&D Benchmark（RD2Bench），用于全面评估数据中心自动 R&D (D-CARD) 的所有操作，包括模型能力的交互和协同效应，以帮助选择可靠的模型。尽管 RD2Bench 对 state-of-the-art (SOTA) large language model (LLM) 如 GPT-4 构成重大挑战，但 LLM 显示出实现简单方法的潜力。论文呼吁未来工作开发相关技术，推动 D-CARD 的发展，从而提升人类生产力。",
      "categories": [
        "cs.AI",
        "q-fin.GN"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11276v2",
      "published_date": "2024-04-17 11:33:21 UTC",
      "updated_date": "2024-07-30 04:10:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:20:27.762458"
    },
    {
      "arxiv_id": "2404.11269v3",
      "title": "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series",
      "title_zh": "DACAD：领域适应对比学习用于多变量时间序列的异常检测",
      "authors": [
        "Zahra Zamanzadeh Darban",
        "Yiyuan Yang",
        "Geoffrey I. Webb",
        "Charu C. Aggarwal",
        "Qingsong Wen",
        "Shirui Pan",
        "Mahsa Salehi"
      ],
      "abstract": "In time series anomaly detection (TSAD), the scarcity of labeled data poses a\nchallenge to the development of accurate models. Unsupervised domain adaptation\n(UDA) offers a solution by leveraging labeled data from a related domain to\ndetect anomalies in an unlabeled target domain. However, existing UDA methods\nassume consistent anomalous classes across domains. To address this limitation,\nwe propose a novel Domain Adaptation Contrastive learning model for Anomaly\nDetection in multivariate time series (DACAD), combining UDA with contrastive\nlearning. DACAD utilizes an anomaly injection mechanism that enhances\ngeneralization across unseen anomalous classes, improving adaptability and\nrobustness. Additionally, our model employs supervised contrastive loss for the\nsource domain and self-supervised contrastive triplet loss for the target\ndomain, ensuring comprehensive feature representation learning and\ndomain-invariant feature extraction. Finally, an effective Center-based Entropy\nClassifier (CEC) accurately learns normal boundaries in the source domain.\nExtensive evaluations on multiple real-world datasets and a synthetic dataset\nhighlight DACAD's superior performance in transferring knowledge across domains\nand mitigating the challenge of limited labeled data in TSAD.",
      "tldr_zh": "在时间序列异常检测(TSAD)中，标注数据稀缺导致模型准确性不足，为解决域间异常类别不一致的问题，我们提出DACAD模型，将无监督域适应(UDA)与对比学习相结合。DACAD通过异常注入机制增强对未见异常类别的泛化能力，并采用源域的监督对比损失、目标域的自监督对比三元组损失，以及Center-based Entropy Classifier(CEC)来实现全面特征表示学习和域不变特征提取。实验结果显示，在多个真实世界和合成数据集上，DACAD显著提升了跨域知识转移的性能，并有效缓解了TSAD中标注数据有限的挑战。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 3 figures, 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.11269v3",
      "published_date": "2024-04-17 11:20:14 UTC",
      "updated_date": "2025-05-14 13:06:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:20:42.232159"
    },
    {
      "arxiv_id": "2404.11243v4",
      "title": "Multi-Sensor Diffusion-Driven Optical Image Translation for Large-Scale Applications",
      "title_zh": "多传感器扩散驱动的光学图像翻译，用于大规模应用",
      "authors": [
        "João Gabriel Vinholi",
        "Marco Chini",
        "Anis Amziane",
        "Renato Machado",
        "Danilo Silva",
        "Patrick Matgen"
      ],
      "abstract": "Comparing images captured by disparate sensors is a common challenge in\nremote sensing. This requires image translation -- converting imagery from one\nsensor domain to another while preserving the original content. Denoising\nDiffusion Implicit Models (DDIM) are potential state-of-the-art solutions for\nsuch domain translation due to their proven superiority in multiple\nimage-to-image translation tasks in computer vision. However, these models\nstruggle with reproducing radiometric features of large-scale multi-patch\nimagery, resulting in inconsistencies across the full image. This renders\ndownstream tasks like Heterogeneous Change Detection impractical. To overcome\nthese limitations, we propose a method that leverages denoising diffusion for\neffective multi-sensor optical image translation over large areas. Our approach\nsuper-resolves large-scale low spatial resolution images into high-resolution\nequivalents from disparate optical sensors, ensuring uniformity across hundreds\nof patches. Our contributions lie in new forward and reverse diffusion\nprocesses that address the challenges of large-scale image translation.\nExtensive experiments using paired Sentinel-II (10m) and Planet Dove (3m)\nimages demonstrate that our approach provides precise domain adaptation,\npreserving image content while improving radiometric accuracy and feature\nrepresentation. A thorough image quality assessment and comparisons with the\nstandard DDIM framework and five other leading methods are presented. We reach\na mean Learned Perceptual Image Patch Similarity (mLPIPS) of 0.1884 and a\nFr\\'echet Inception Distance (FID) of 45.64, expressively outperforming all\ncompared methods, including DDIM, ShuffleMixer, and SwinIR. The usefulness of\nour approach is further demonstrated in two Heterogeneous Change Detection\ntasks.",
      "tldr_zh": "该研究针对遥感中不同传感器图像的比较问题，提出了一种基于去噪扩散隐式模型(DDIM)的多传感器光学图像翻译方法，专注于大规模应用的图像域转换，同时保留原始内容并确保跨补丁的一致性。该方法引入新的forward和reverse扩散进程，能够将低分辨率图像（如Sentinel-II 10m）超分辨到高分辨率等效（如Planet Dove 3m），从而改善辐射准确性和特征表示。在实验中，该方法在配对图像上实现了mLPIPS为0.1884和FID为45.64的优异性能，显著优于DDIM、ShuffleMixer和SwinIR等基准，并在Heterogeneous Change Detection任务中证明了其实际效用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This is the accepted version of the manuscript published in IEEE\n  Journal of Selected Topics in Applied Earth Observations and Remote Sensing\n  (JSTARS). Please access the final version at IEEEXplore (Open Access). DOI\n  10.1109/JSTARS.2024.3506032. This technology is protected by a patent filed\n  on 23 december 2023 at Office Luxembourgeois de la propri\\'et\\'e\n  intellectuelle (LU505861)",
      "pdf_url": "http://arxiv.org/pdf/2404.11243v4",
      "published_date": "2024-04-17 10:49:00 UTC",
      "updated_date": "2024-12-04 11:23:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:20:53.561134"
    },
    {
      "arxiv_id": "2404.11230v1",
      "title": "Energy-Efficient Uncertainty-Aware Biomass Composition Prediction at the Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Zawish",
        "Paul Albert",
        "Flavio Esposito",
        "Steven Davy",
        "Lizy Abraham"
      ],
      "abstract": "Clover fixates nitrogen from the atmosphere to the ground, making\ngrass-clover mixtures highly desirable to reduce external nitrogen\nfertilization. Herbage containing clover additionally promotes higher food\nintake, resulting in higher milk production. Herbage probing however remains\nlargely unused as it requires a time-intensive manual laboratory analysis.\nWithout this information, farmers are unable to perform localized clover sowing\nor take targeted fertilization decisions. Deep learning algorithms have been\nproposed with the goal to estimate the dry biomass composition from images of\nthe grass directly in the fields. The energy-intensive nature of deep learning\nhowever limits deployment to practical edge devices such as smartphones. This\npaper proposes to fill this gap by applying filter pruning to reduce the energy\nrequirement of existing deep learning solutions. We report that although pruned\nnetworks are accurate on controlled, high-quality images of the grass, they\nstruggle to generalize to real-world smartphone images that are blurry or taken\nfrom challenging angles. We address this challenge by training filter-pruned\nmodels using a variance attenuation loss so they can predict the uncertainty of\ntheir predictions. When the uncertainty exceeds a threshold, we re-infer using\na more accurate unpruned model. This hybrid approach allows us to reduce energy\nconsumption while retaining a high accuracy. We evaluate our algorithm on two\ndatasets: the GrassClover and the Irish clover using an NVIDIA Jetson Nano edge\ndevice. We find that we reduce energy reduction with respect to\nstate-of-the-art solutions by 50% on average with only 4% accuracy loss.",
      "tldr_zh": "这篇论文针对草地图像预测生物质组成的问题，提出了一种能量高效且不确定性感知的深度学习方法，以适应边缘设备如智能手机的使用。通过 filter pruning 修剪模型来降低能耗，但发现修剪后模型在真实世界模糊图像上泛化性不足。为此，作者引入 variance attenuation loss 训练模型，使其能预测预测不确定性，并在不确定性超过阈值时切换到更准确的未修剪模型。实验结果显示，在 GrassClover 和 Irish clover 数据集上，使用 NVIDIA Jetson Nano 设备，该方法平均能耗减少 50%，准确率仅损失 4%。这种混合方法为农民提供实用的现场决策工具，支持局部三叶草播种和针对性施肥。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "The paper has been accepted to CVPR 2024 5th Workshop on Vision for\n  Agriculture",
      "pdf_url": "http://arxiv.org/pdf/2404.11230v1",
      "published_date": "2024-04-17 10:26:49 UTC",
      "updated_date": "2024-04-17 10:26:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:21:06.368381"
    },
    {
      "arxiv_id": "2404.11225v2",
      "title": "In-Context Learning State Vector with Inner and Momentum Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Dongfang Li",
        "Zhenyu Liu",
        "Xinshuo Hu",
        "Zetian Sun",
        "Baotian Hu",
        "Min Zhang"
      ],
      "abstract": "Large Language Models (LLMs) have exhibited an impressive ability to perform\nIn-Context Learning (ICL) from only a few examples. Recent works have indicated\nthat the functions learned by ICL can be represented through compressed vectors\nderived from the transformer. However, the working mechanisms and optimization\nof these vectors are yet to be thoroughly explored. In this paper, we address\nthis gap by presenting a comprehensive analysis of these compressed vectors,\ndrawing parallels to the parameters trained with gradient descent, and\nintroduce the concept of state vector. Inspired by the works on model soup and\nmomentum-based gradient descent, we propose inner and momentum optimization\nmethods that are applied to refine the state vector progressively as test-time\nadaptation. Moreover, we simulate state vector aggregation in the multiple\nexample setting, where demonstrations comprising numerous examples are usually\ntoo lengthy for regular ICL, and further propose a divide-and-conquer\naggregation method to address this challenge. We conduct extensive experiments\nusing Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The\nexperimental results show that our optimization method effectively enhances the\nstate vector and achieves the state-of-the-art performance on diverse tasks.\nCode is available at https://github.com/HITsz-TMG/ICL-State-Vector",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在 In-Context Learning (ICL) 中的压缩向量机制，引入了 state vector 概念，将其类比为通过梯度下降训练的参数。受 model soup 和 momentum-based gradient descent 启发，论文提出 inner and momentum optimization 方法，用于测试时适应性地优化 state vector，并开发 divide-and-conquer 聚合方法处理多示例场景。实验结果显示，在 Llama-2 和 GPT-J 模型上，该优化方法在 zero-shot 和 few-shot 设置中显著提升了性能，达到了 state-of-the-art 水平。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.11225v2",
      "published_date": "2024-04-17 10:19:15 UTC",
      "updated_date": "2024-07-04 11:52:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:21:18.505593"
    },
    {
      "arxiv_id": "2404.11216v2",
      "title": "Position Engineering: Boosting Large Language Models through Positional Information Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyuan He",
        "Huiqiang Jiang",
        "Zilong Wang",
        "Yuqing Yang",
        "Luna Qiu",
        "Lili Qiu"
      ],
      "abstract": "The performance of large language models (LLMs) is significantly influenced\nby the quality of the prompts provided. In response, researchers have developed\nenormous prompt engineering strategies aimed at modifying the prompt text to\nenhance task performance. In this paper, we introduce a novel technique termed\nposition engineering, which offers a more efficient way to guide large language\nmodels. Unlike prompt engineering, which requires substantial effort to modify\nthe text provided to LLMs, position engineering merely involves altering the\npositional information in the prompt without modifying the text itself. We have\nevaluated position engineering in two widely-used LLM scenarios:\nretrieval-augmented generation (RAG) and in-context learning (ICL). Our\nfindings show that position engineering substantially improves upon the\nbaseline in both cases. Position engineering thus represents a promising new\nstrategy for exploiting the capabilities of large language models.",
      "tldr_zh": "本文提出了一种名为 position engineering 的新技巧，用于提升大型语言模型 (LLMs) 的性能。该方法通过操纵提示中的位置信息，而非修改文本内容本身，从而提供比传统 prompt engineering 更高效的引导方式。在 retrieval-augmented generation (RAG) 和 in-context learning (ICL) 场景中，实验结果显示 position engineering 显著提高了基线性能，为更好地利用 LLMs 能力提供了有前景的新策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11216v2",
      "published_date": "2024-04-17 10:00:56 UTC",
      "updated_date": "2024-10-22 05:45:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:21:28.541810"
    },
    {
      "arxiv_id": "2404.11214v2",
      "title": "Feature Corrective Transfer Learning: End-to-End Solutions to Object Detection in Non-Ideal Visual Conditions",
      "title_zh": "翻译失败",
      "authors": [
        "Chuheng Wei",
        "Guoyuan Wu",
        "Matthew J. Barth"
      ],
      "abstract": "A significant challenge in the field of object detection lies in the system's\nperformance under non-ideal imaging conditions, such as rain, fog, low\nillumination, or raw Bayer images that lack ISP processing. Our study\nintroduces \"Feature Corrective Transfer Learning\", a novel approach that\nleverages transfer learning and a bespoke loss function to facilitate the\nend-to-end detection of objects in these challenging scenarios without the need\nto convert non-ideal images into their RGB counterparts. In our methodology, we\ninitially train a comprehensive model on a pristine RGB image dataset.\nSubsequently, non-ideal images are processed by comparing their feature maps\nagainst those from the initial ideal RGB model. This comparison employs the\nExtended Area Novel Structural Discrepancy Loss (EANSDL), a novel loss function\ndesigned to quantify similarities and integrate them into the detection loss.\nThis approach refines the model's ability to perform object detection across\nvarying conditions through direct feature map correction, encapsulating the\nessence of Feature Corrective Transfer Learning. Experimental validation on\nvariants of the KITTI dataset demonstrates a significant improvement in mean\nAverage Precision (mAP), resulting in a 3.8-8.1% relative enhancement in\ndetection under non-ideal conditions compared to the baseline model, and a less\nmarginal performance difference within 1.3% of the mAP@[0.5:0.95] achieved\nunder ideal conditions by the standard Faster RCNN algorithm.",
      "tldr_zh": "本研究针对物体检测在非理想视觉条件下（如雨、雾、低光照或原始Bayer图像）的性能挑战，提出了一种创新方法——Feature Corrective Transfer Learning。该方法利用迁移学习和自定义损失函数Extended Area Novel Structural Discrepancy Loss (EANSDL)，实现端到端的物体检测，无需将非理想图像转换为RGB，而是通过比较特征图来直接修正模型。实验在KITTI数据集变体上验证，结果显示mAP相对提升3.8-8.1%，使检测性能在非理想条件下接近理想环境下的Faster RCNN水平，差距控制在1.3%以内。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 CVPR UG2+ Workshop",
      "pdf_url": "http://arxiv.org/pdf/2404.11214v2",
      "published_date": "2024-04-17 09:58:53 UTC",
      "updated_date": "2024-04-19 14:26:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:21:42.656778"
    },
    {
      "arxiv_id": "2404.11213v1",
      "title": "Revisiting Noise Resilience Strategies in Gesture Recognition: Short-Term Enhancement in Surface Electromyographic Signal Analysis",
      "title_zh": "重新审视手",
      "authors": [
        "Weiyu Guo",
        "Ziyue Qiao",
        "Ying Sun",
        "Hui Xiong"
      ],
      "abstract": "Gesture recognition based on surface electromyography (sEMG) has been gaining\nimportance in many 3D Interactive Scenes. However, sEMG is easily influenced by\nvarious forms of noise in real-world environments, leading to challenges in\nproviding long-term stable interactions through sEMG. Existing methods often\nstruggle to enhance model noise resilience through various predefined data\naugmentation techniques. In this work, we revisit the problem from a short term\nenhancement perspective to improve precision and robustness against various\ncommon noisy scenarios with learnable denoise using sEMG intrinsic pattern\ninformation and sliding-window attention. We propose a Short Term Enhancement\nModule(STEM) which can be easily integrated with various models. STEM offers\nseveral benefits: 1) Learnable denoise, enabling noise reduction without manual\ndata augmentation; 2) Scalability, adaptable to various models; and 3)\nCost-effectiveness, achieving short-term enhancement through minimal\nweight-sharing in an efficient attention mechanism. In particular, we\nincorporate STEM into a transformer, creating the Short Term Enhanced\nTransformer (STET). Compared with best-competing approaches, the impact of\nnoise on STET is reduced by more than 20%. We also report promising results on\nboth classification and regression datasets and demonstrate that STEM\ngeneralizes across different gesture recognition tasks.",
      "tldr_zh": "本研究重新审视了基于 surface electromyography (sEMG) 的手势识别中噪声鲁棒性策略，提出从短期增强角度出发，使用可学习的去噪技术及 sliding-window attention 机制，基于 sEMG 的内在模式信息来提升精度和鲁棒性。作者设计了 Short Term Enhancement Module (STEM)，该模块易于集成到各种模型中，具有可学习的去噪、可扩展性和成本效益优势，例如通过最小权重共享实现高效注意力机制。实验结果显示，将 STEM 融入 Transformer 创建的 Short Term Enhanced Transformer (STET) 比最佳竞争方法减少了超过 20% 的噪声影响，并在分类和回归数据集上取得良好性能，同时证明了 STEM 在不同手势识别任务中的泛化能力。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11213v1",
      "published_date": "2024-04-17 09:57:40 UTC",
      "updated_date": "2024-04-17 09:57:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:21:56.079249"
    },
    {
      "arxiv_id": "2404.11209v1",
      "title": "Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Hongzhao Li",
        "Hongyu Wang",
        "Xia Sun",
        "Hua He",
        "Jun Feng"
      ],
      "abstract": "Medical report generation automates radiology descriptions from images,\neasing the burden on physicians and minimizing errors. However, current methods\nlack structured outputs and physician interactivity for clear, clinically\nrelevant reports. Our method introduces a prompt-guided approach to generate\nstructured chest X-ray reports using a pre-trained large language model (LLM).\nFirst, we identify anatomical regions in chest X-rays to generate focused\nsentences that center on key visual elements, thereby establishing a structured\nreport foundation with anatomy-based sentences. We also convert the detected\nanatomy into textual prompts conveying anatomical comprehension to the LLM.\nAdditionally, the clinical context prompts guide the LLM to emphasize\ninteractivity and clinical requirements. By integrating anatomy-focused\nsentences and anatomy/clinical prompts, the pre-trained LLM can generate\nstructured chest X-ray reports tailored to prompted anatomical regions and\nclinical contexts. We evaluate using language generation and clinical\neffectiveness metrics, demonstrating strong performance.",
      "tldr_zh": "本研究提出了一种prompt-guided方法，使用预训练的大型语言模型 (LLM) 生成结构化的胸部 X-ray 报告，以解决现有方法缺乏结构输出和医生互动的问题。方法首先识别胸部 X-ray 中的解剖区域，生成以关键视觉元素为中心的句子，并将这些区域转换为文本提示，以传达解剖理解和临床上下文。最终，通过整合这些提示，LLM 可以输出针对特定解剖区域和临床需求的报告；评估结果显示，该方法在语言生成和临床有效性指标上表现出强劲性能。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by IEEE Conference on Multimedia Expo 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11209v1",
      "published_date": "2024-04-17 09:45:43 UTC",
      "updated_date": "2024-04-17 09:45:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:22:06.434861"
    },
    {
      "arxiv_id": "2404.11208v1",
      "title": "CAGE: Causality-Aware Shapley Value for Global Explanations",
      "title_zh": "翻译失败",
      "authors": [
        "Nils Ole Breuer",
        "Andreas Sauter",
        "Majid Mohammadi",
        "Erman Acar"
      ],
      "abstract": "As Artificial Intelligence (AI) is having more influence on our everyday\nlives, it becomes important that AI-based decisions are transparent and\nexplainable. As a consequence, the field of eXplainable AI (or XAI) has become\npopular in recent years. One way to explain AI models is to elucidate the\npredictive importance of the input features for the AI model in general, also\nreferred to as global explanations. Inspired by cooperative game theory,\nShapley values offer a convenient way for quantifying the feature importance as\nexplanations. However many methods based on Shapley values are built on the\nassumption of feature independence and often overlook causal relations of the\nfeatures which could impact their importance for the ML model. Inspired by\nstudies of explanations at the local level, we propose CAGE (Causally-Aware\nShapley Values for Global Explanations). In particular, we introduce a novel\nsampling procedure for out-coalition features that respects the causal\nrelations of the input features. We derive a practical approach that\nincorporates causal knowledge into global explanation and offers the\npossibility to interpret the predictive feature importance considering their\ncausal relation. We evaluate our method on synthetic data and real-world data.\nThe explanations from our approach suggest that they are not only more\nintuitive but also more faithful compared to previous global explanation\nmethods.",
      "tldr_zh": "本文提出CAGE（Causality-Aware Shapley Value for Global Explanations）方法，以解决传统Shapley values在XAI（eXplainable AI）领域忽略特征因果关系的局限性。CAGE引入一种新型采样程序，考虑输入特征的因果关系，从而在全局解释中更准确地量化特征重要性。实验在合成数据和真实世界数据上验证，该方法生成的解释比现有方法更直观和忠实。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11208v1",
      "published_date": "2024-04-17 09:43:54 UTC",
      "updated_date": "2024-04-17 09:43:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:22:17.771893"
    },
    {
      "arxiv_id": "2404.11207v1",
      "title": "Exploring the Transferability of Visual Prompting for Multimodal Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yichi Zhang",
        "Yinpeng Dong",
        "Siyuan Zhang",
        "Tianzan Min",
        "Hang Su",
        "Jun Zhu"
      ],
      "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated promising\nversatile capabilities, their performance is still inferior to specialized\nmodels on downstream tasks, which makes adaptation necessary to enhance their\nutility. However, fine-tuning methods require independent training for every\nmodel, leading to huge computation and memory overheads. In this paper, we\npropose a novel setting where we aim to improve the performance of diverse\nMLLMs with a group of shared parameters optimized for a downstream task. To\nachieve this, we propose Transferable Visual Prompting (TVP), a simple and\neffective approach to generate visual prompts that can transfer to different\nmodels and improve their performance on downstream tasks after trained on only\none model. We introduce two strategies to address the issue of cross-model\nfeature corruption of existing visual prompting methods and enhance the\ntransferability of the learned prompts, including 1) Feature Consistency\nAlignment: which imposes constraints to the prompted feature changes to\nmaintain task-agnostic knowledge; 2) Task Semantics Enrichment: which\nencourages the prompted images to contain richer task-specific semantics with\nlanguage guidance. We validate the effectiveness of TVP through extensive\nexperiments with 6 modern MLLMs on a wide variety of tasks ranging from object\nrecognition and counting to multimodal reasoning and hallucination correction.",
      "tldr_zh": "本研究探讨了视觉提示在多模态大型语言模型(MLLMs)中的可转移性，旨在通过一组共享参数优化下游任务性能，而避免为每个模型单独训练带来的计算和内存开销。论文提出Transferable Visual Prompting (TVP)方法，该方法仅在单一模型上训练即可生成可转移的视觉提示，提升不同MLLMs在对象识别、计数、多模态推理和幻觉修正等任务上的表现。TVP包括两个关键策略：Feature Consistency Alignment，用于约束提示后的特征变化以保留任务无关知识；以及Task Semantics Enrichment，通过语言指导丰富提示图像的任务特定语义。实验在6个现代MLLMs上验证了TVP的有效性，展示了其在广泛任务中的实际应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in CVPR 2024 as Poster (Highlight)",
      "pdf_url": "http://arxiv.org/pdf/2404.11207v1",
      "published_date": "2024-04-17 09:39:07 UTC",
      "updated_date": "2024-04-17 09:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:22:30.832448"
    },
    {
      "arxiv_id": "2404.11181v2",
      "title": "KI-GAN: Knowledge-Informed Generative Adversarial Networks for Enhanced Multi-Vehicle Trajectory Forecasting at Signalized Intersections",
      "title_zh": "翻译失败",
      "authors": [
        "Chuheng Wei",
        "Guoyuan Wu",
        "Matthew J. Barth",
        "Amr Abdelraouf",
        "Rohit Gupta",
        "Kyungtae Han"
      ],
      "abstract": "Reliable prediction of vehicle trajectories at signalized intersections is\ncrucial to urban traffic management and autonomous driving systems. However, it\npresents unique challenges, due to the complex roadway layout at intersections,\ninvolvement of traffic signal controls, and interactions among different types\nof road users. To address these issues, we present in this paper a novel model\ncalled Knowledge-Informed Generative Adversarial Network (KI-GAN), which\nintegrates both traffic signal information and multi-vehicle interactions to\npredict vehicle trajectories accurately. Additionally, we propose a specialized\nattention pooling method that accounts for vehicle orientation and proximity at\nintersections. Based on the SinD dataset, our KI-GAN model is able to achieve\nan Average Displacement Error (ADE) of 0.05 and a Final Displacement Error\n(FDE) of 0.12 for a 6-second observation and 6-second prediction cycle. When\nthe prediction window is extended to 9 seconds, the ADE and FDE values are\nfurther reduced to 0.11 and 0.26, respectively. These results demonstrate the\neffectiveness of the proposed KI-GAN model in vehicle trajectory prediction\nunder complex scenarios at signalized intersections, which represents a\nsignificant advancement in the target field.",
      "tldr_zh": "本文提出 KI-GAN（Knowledge-Informed Generative Adversarial Networks）模型，用于提升信号灯交叉口多车辆轨迹预测的准确性，通过整合交通信号信息和多车辆互动来应对复杂的道路布局和用户互动挑战。模型还引入了一种专门的注意力池化方法，考虑车辆的方向和接近度，以优化预测过程。在 SinD 数据集上的实验显示，KI-GAN 在 6 秒观察 + 6 秒预测中实现 ADE 0.05 和 FDE 0.12 的优异性能，并在 9 秒预测中进一步降低至 ADE 0.11 和 FDE 0.26，这标志着该领域在复杂场景下的显著进步。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "2024 CVPR AICity Workshop",
      "pdf_url": "http://arxiv.org/pdf/2404.11181v2",
      "published_date": "2024-04-17 08:53:59 UTC",
      "updated_date": "2024-04-19 14:28:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:22:43.536778"
    },
    {
      "arxiv_id": "2404.11172v2",
      "title": "Deep Neural Networks via Complex Network Theory: a Perspective",
      "title_zh": "复杂网络理论视角下的深度神经网络",
      "authors": [
        "Emanuele La Malfa",
        "Gabriele La Malfa",
        "Giuseppe Nicosia",
        "Vito Latora"
      ],
      "abstract": "Deep Neural Networks (DNNs) can be represented as graphs whose links and\nvertices iteratively process data and solve tasks sub-optimally. Complex\nNetwork Theory (CNT), merging statistical physics with graph theory, provides a\nmethod for interpreting neural networks by analysing their weights and neuron\nstructures. However, classic works adapt CNT metrics that only permit a\ntopological analysis as they do not account for the effect of the input data.\nIn addition, CNT metrics have been applied to a limited range of architectures,\nmainly including Fully Connected neural networks. In this work, we extend the\nexisting CNT metrics with measures that sample from the DNNs' training\ndistribution, shifting from a purely topological analysis to one that connects\nwith the interpretability of deep learning. For the novel metrics, in addition\nto the existing ones, we provide a mathematical formalisation for Fully\nConnected, AutoEncoder, Convolutional and Recurrent neural networks, of which\nwe vary the activation functions and the number of hidden layers. We show that\nthese metrics differentiate DNNs based on the architecture, the number of\nhidden layers, and the activation function. Our contribution provides a method\nrooted in physics for interpreting DNNs that offers insights beyond the\ntraditional input-output relationship and the CNT topological analysis.",
      "tldr_zh": "本研究从 Complex Network Theory (CNT) 的角度审视 Deep Neural Networks (DNNs)，通过将 DNNs 视为图结构来分析其权重和神经元。作者扩展了现有的 CNT 指标，引入从 DNNs 训练分布采样的措施，以超越纯拓扑分析，实现与深度学习可解释性的连接。针对 Fully Connected、AutoEncoder、Convolutional 和 Recurrent 神经网络，他们提供了数学形式化，并测试了不同激活函数和隐藏层数量。结果显示，这些新指标能有效区分基于架构、隐藏层数和激活函数的 DNNs，提供了一种基于物理学的解释方法，超越传统输入-输出关系。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "IJCAI'24 (full paper, main track)",
      "pdf_url": "http://arxiv.org/pdf/2404.11172v2",
      "published_date": "2024-04-17 08:42:42 UTC",
      "updated_date": "2024-04-18 11:17:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:22:54.270887"
    },
    {
      "arxiv_id": "2404.11171v2",
      "title": "Personalized Heart Disease Detection via ECG Digital Twin Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Yaojun Hu",
        "Jintai Chen",
        "Lianting Hu",
        "Dantong Li",
        "Jiahuan Yan",
        "Haochao Ying",
        "Huiying Liang",
        "Jian Wu"
      ],
      "abstract": "Heart diseases rank among the leading causes of global mortality,\ndemonstrating a crucial need for early diagnosis and intervention. Most\ntraditional electrocardiogram (ECG) based automated diagnosis methods are\ntrained at population level, neglecting the customization of personalized ECGs\nto enhance individual healthcare management. A potential solution to address\nthis limitation is to employ digital twins to simulate symptoms of diseases in\nreal patients. In this paper, we present an innovative prospective learning\napproach for personalized heart disease detection, which generates digital\ntwins of healthy individuals' anomalous ECGs and enhances the model sensitivity\nto the personalized symptoms. In our approach, a vector quantized feature\nseparator is proposed to locate and isolate the disease symptom and normal\nsegments in ECG signals with ECG report guidance. Thus, the ECG digital twins\ncan simulate specific heart diseases used to train a personalized heart disease\ndetection model. Experiments demonstrate that our approach not only excels in\ngenerating high-fidelity ECG signals but also improves personalized heart\ndisease detection. Moreover, our approach ensures robust privacy protection,\nsafeguarding patient data in model development.",
      "tldr_zh": "该研究针对心脏病作为全球主要死亡原因的问题，提出了一种个性化心病检测方法，通过生成 ECG 数字孪生（digital twins）来模拟健康个体的异常 ECG 信号，从而提升模型对个性化症状的敏感性。具体而言，该方法引入向量量化特征分离器（vector quantized feature separator），在 ECG 报告指导下定位和隔离 ECG 信号中的疾病和正常段，以生成高质量的 ECG 数字孪生用于训练个性化检测模型。实验结果显示，该方法不仅在生成高保真 ECG 信号方面表现出色，还显著提高了心病检测性能，同时确保了患者数据的隐私保护。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11171v2",
      "published_date": "2024-04-17 08:40:54 UTC",
      "updated_date": "2024-05-11 18:15:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:23:05.091598"
    },
    {
      "arxiv_id": "2404.11160v2",
      "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation",
      "title_zh": "低成本语言模型：",
      "authors": [
        "Jessica López Espejel",
        "Mahaman Sanoussi Yahaya Alassan",
        "Merieme Bouhandi",
        "Walid Dahhane",
        "El Hassane Ettifouri"
      ],
      "abstract": "Large Language Models (LLMs) have become a popular choice for many Natural\nLanguage Processing (NLP) tasks due to their versatility and ability to produce\nhigh-quality results. Specifically, they are increasingly used for automatic\ncode generation to help developers tackle repetitive coding tasks. However,\nLLMs' substantial computational and memory requirements often make them\ninaccessible to users with limited resources. This paper focuses on very\nlow-cost models which offer a more accessible alternative to resource-intensive\nLLMs. We notably: (1) propose a thorough semi-manual evaluation of their\nperformance in generating Python code, (2) introduce a Chain-of-Thought (CoT)\nprompting strategy to improve model reasoning and code quality, and (3) propose\na new dataset of 60 programming problems, with varied difficulty levels,\ndesigned to extend existing benchmarks like HumanEval and EvalPlus. Our\nfindings show that some low-cost compatible models achieve competitive results\ncompared to larger models like ChatGPT despite using significantly fewer\nresources. We will make our dataset and prompts publicly available to support\nfurther research.",
      "tldr_zh": "这篇论文调查了低成本语言模型（LLMs）在Python代码生成任务中的性能，以解决传统LLMs的高资源需求问题。主要贡献包括：提出一种彻底的半手动评估方法、对Chain-of-Thought (CoT)提示策略的引入，以提升模型的推理和代码质量，以及构建一个包含60个难度多样的新数据集，用于扩展现有基准如HumanEval和EvalPlus。研究发现，一些低成本模型在资源消耗远低于ChatGPT的情况下，实现了具有竞争力的性能。最后，作者将数据集和提示公开，以支持进一步研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2404.11160v2",
      "published_date": "2024-04-17 08:16:48 UTC",
      "updated_date": "2024-08-29 13:23:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:23:18.854886"
    },
    {
      "arxiv_id": "2404.11148v1",
      "title": "Explainable Machine Learning System for Predicting Chronic Kidney Disease in High-Risk Cardiovascular Patients",
      "title_zh": "预测高危心血管患者慢性肾病的",
      "authors": [
        "Nantika Nguycharoen"
      ],
      "abstract": "As the global population ages, the incidence of Chronic Kidney Disease (CKD)\nis rising. CKD often remains asymptomatic until advanced stages, which\nsignificantly burdens both the healthcare system and patient quality of life.\nThis research developed an explainable machine learning system for predicting\nCKD in patients with cardiovascular risks, utilizing medical history and\nlaboratory data. The Random Forest model achieved the highest sensitivity of\n88.2%. The study introduces a comprehensive explainability framework that\nextends beyond traditional feature importance methods, incorporating global and\nlocal interpretations, bias inspection, biomedical relevance, and safety\nassessments. Key predictive features identified in global interpretation were\nthe use of diabetic and ACEI/ARB medications, and initial eGFR values. Local\ninterpretation provided model insights through counterfactual explanations,\nwhich aligned with other system parts. After conducting a bias inspection, it\nwas found that the initial eGFR values and CKD predictions exhibited some bias,\nbut no significant gender bias was identified. The model's logic, extracted by\nscoped rules, was confirmed to align with existing medical literature. The\nsafety assessment tested potentially dangerous cases and confirmed that the\nmodel behaved safely. This system enhances the explainability, reliability, and\naccountability of the model, promoting its potential integration into\nhealthcare settings and compliance with upcoming regulatory standards, and\nshowing promise for broader applications in healthcare machine learning.",
      "tldr_zh": "本研究开发了一个可解释的机器学习系统，用于预测高风险心血管患者是否患有慢性肾病 (CKD)，利用医疗历史和实验室数据。Random Forest 模型在该系统中表现出色，实现了88.2%的敏感度，并通过全局解释识别出关键预测特征，如糖尿病药物和ACEI/ARB药物使用，以及初始eGFR值。研究引入了全面的可解释性框架，包括局部解释（如反事实解释）、偏差检查（发现eGFR值和CKD预测有偏差但无显著性别偏差）、生物医学相关性和安全评估，确保模型逻辑符合医疗文献。该系统提升了模型的可靠性和责任性，促进其在医疗环境中的应用，并符合未来监管标准。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "J.3; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "9 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.11148v1",
      "published_date": "2024-04-17 07:59:33 UTC",
      "updated_date": "2024-04-17 07:59:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:23:30.769874"
    },
    {
      "arxiv_id": "2404.11144v1",
      "title": "Self-adaptive PSRO: Towards an Automatic Population-based Game Solver",
      "title_zh": "翻译失败",
      "authors": [
        "Pengdeng Li",
        "Shuxin Li",
        "Chang Yang",
        "Xinrun Wang",
        "Xiao Huang",
        "Hau Chan",
        "Bo An"
      ],
      "abstract": "Policy-Space Response Oracles (PSRO) as a general algorithmic framework has\nachieved state-of-the-art performance in learning equilibrium policies of\ntwo-player zero-sum games. However, the hand-crafted hyperparameter value\nselection in most of the existing works requires extensive domain knowledge,\nforming the main barrier to applying PSRO to different games. In this work, we\nmake the first attempt to investigate the possibility of self-adaptively\ndetermining the optimal hyperparameter values in the PSRO framework. Our\ncontributions are three-fold: (1) Using several hyperparameters, we propose a\nparametric PSRO that unifies the gradient descent ascent (GDA) and different\nPSRO variants. (2) We propose the self-adaptive PSRO (SPSRO) by casting the\nhyperparameter value selection of the parametric PSRO as a hyperparameter\noptimization (HPO) problem where our objective is to learn an HPO policy that\ncan self-adaptively determine the optimal hyperparameter values during the\nrunning of the parametric PSRO. (3) To overcome the poor performance of online\nHPO methods, we propose a novel offline HPO approach to optimize the HPO policy\nbased on the Transformer architecture. Experiments on various two-player\nzero-sum games demonstrate the superiority of SPSRO over different baselines.",
      "tldr_zh": "本研究针对 Policy-Space Response Oracles (PSRO) 在两玩家零和游戏中学习均衡策略的框架，解决了手动选择超参数的难题，提出了一种自适应 PSRO (SPSRO) 方法，以实现自动优化。SPSRO 通过构建一个参数化的 PSRO，将 gradient descent ascent (GDA) 和不同 PSRO 变体统一起来，并将超参数值选择转化为 hyperparameter optimization (HPO) 问题，使用 Transformer 架构的离线 HPO 策略来优化 HPO 政策。实验结果显示，在各种两玩家零和游戏上，SPSRO 优于基线模型，证明了其在自动游戏求解方面的有效性。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to 33rd International Joint Conference on Artificial\n  Intelligence (IJCAI 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.11144v1",
      "published_date": "2024-04-17 07:40:57 UTC",
      "updated_date": "2024-04-17 07:40:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:23:45.281550"
    },
    {
      "arxiv_id": "2404.11124v2",
      "title": "What's under the hood: Investigating Automatic Metrics on Meeting Summarization",
      "title_zh": "翻译失败",
      "authors": [
        "Frederic Kirstein",
        "Jan Philip Wahle",
        "Terry Ruas",
        "Bela Gipp"
      ],
      "abstract": "Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.",
      "tldr_zh": "这篇论文调查了自动 metrics 在会议摘要任务中的表现，重点评估它们是否能捕捉会议特定错误（如缺失信息和语言不准确），并通过与人类评估的相关性分析揭示潜在问题。作者首先进行文献综述，定义了会议摘要的关键挑战（如 speaker dynamics 和 contextual turn-taking），并使用 QMSum 数据集的 Transformer-based 模型进行实验验证。结果显示，不同模型架构对这些挑战的响应差异显著，而当前 automatic metrics 通常表现出弱相关性或错误 masking，只有少数指标能准确反映特定错误对摘要质量的影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11124v2",
      "published_date": "2024-04-17 07:15:07 UTC",
      "updated_date": "2024-10-18 15:34:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:23:56.993563"
    },
    {
      "arxiv_id": "2404.11122v1",
      "title": "Small Language Models are Good Too: An Empirical Study of Zero-Shot Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Pierre Lepagnol",
        "Thomas Gerald",
        "Sahar Ghannay",
        "Christophe Servan",
        "Sophie Rosset"
      ],
      "abstract": "This study is part of the debate on the efficiency of large versus small\nlanguage models for text classification by prompting.We assess the performance\nof small language models in zero-shot text classification, challenging the\nprevailing dominance of large models.Across 15 datasets, our investigation\nbenchmarks language models from 77M to 40B parameters using different\narchitectures and scoring functions. Our findings reveal that small models can\neffectively classify texts, getting on par with or surpassing their larger\ncounterparts.We developed and shared a comprehensive open-source repository\nthat encapsulates our methodologies. This research underscores the notion that\nbigger isn't always better, suggesting that resource-efficient small models may\noffer viable solutions for specific data classification challenges.",
      "tldr_zh": "这篇论文通过实证研究评估了小语言模型在零-shot classification（零样本文本分类）中的性能，挑战了大语言模型的主导地位。研究团队在15个数据集上测试了参数从77M到40B的多种语言模型架构和评分函数。结果显示，小模型的表现可与大模型相当或更优，突显了其资源效率的优势。该研究还开发并分享了一个开源仓库，促进了相关领域的进一步探索。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11122v1",
      "published_date": "2024-04-17 07:10:28 UTC",
      "updated_date": "2024-04-17 07:10:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:24:07.987757"
    },
    {
      "arxiv_id": "2404.11121v2",
      "title": "TransLinkGuard: Safeguarding Transformer Models Against Model Stealing in Edge Deployment",
      "title_zh": "翻译失败",
      "authors": [
        "Qinfeng Li",
        "Zhiqiang Shen",
        "Zhenghan Qin",
        "Yangfan Xie",
        "Xuhong Zhang",
        "Tianyu Du",
        "Jianwei Yin"
      ],
      "abstract": "Proprietary large language models (LLMs) have been widely applied in various\nscenarios. Additionally, deploying LLMs on edge devices is trending for\nefficiency and privacy reasons. However, edge deployment of proprietary LLMs\nintroduces new security challenges: edge-deployed models are exposed as\nwhite-box accessible to users, enabling adversaries to conduct effective model\nstealing (MS) attacks. Unfortunately, existing defense mechanisms fail to\nprovide effective protection. Specifically, we identify four critical\nprotection properties that existing methods fail to simultaneously satisfy: (1)\nmaintaining protection after a model is physically copied; (2) authorizing\nmodel access at request level; (3) safeguarding runtime reverse engineering;\n(4) achieving high security with negligible runtime overhead. To address the\nabove issues, we propose TransLinkGuard, a plug-and-play model protection\napproach against model stealing on edge devices. The core part of\nTransLinkGuard is a lightweight authorization module residing in a secure\nenvironment, e.g., TEE. The authorization module can freshly authorize each\nrequest based on its input. Extensive experiments show that TransLinkGuard\nachieves the same security protection as the black-box security guarantees with\nnegligible overhead.",
      "tldr_zh": "该研究针对边缘部署的专有大语言模型（LLMs）面临模型窃取（Model Stealing）攻击的问题，指出现有防御机制无法同时满足模型物理复制后保护、请求级授权、运行时逆向工程防护以及高安全低开销等四个关键属性。为此，提出TransLinkGuard，一种即插即用的保护方法，其核心是轻量级授权模块驻留在安全环境（如TEE）中，对每个请求基于输入进行新鲜授权。实验结果显示，TransLinkGuard实现了与黑箱安全相当的防护效果，同时运行时开销微乎其微。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ACM MM24 Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.11121v2",
      "published_date": "2024-04-17 07:08:45 UTC",
      "updated_date": "2024-11-21 02:16:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:24:20.778900"
    },
    {
      "arxiv_id": "2404.11116v1",
      "title": "Music Enhancement with Deep Filters: A Technical Report for The ICASSP 2024 Cadenza Challenge",
      "title_zh": "翻译失败",
      "authors": [
        "Keren Shao",
        "Ke Chen",
        "Shlomo Dubnov"
      ],
      "abstract": "In this challenge, we disentangle the deep filters from the original\nDeepfilterNet and incorporate them into our Spec-UNet-based network to further\nimprove a hybrid Demucs (hdemucs) based remixing pipeline. The motivation\nbehind the use of the deep filter component lies at its potential in better\nhandling temporal fine structures. We demonstrate an incremental improvement in\nboth the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality\nIndex (HAAQI) metrics when comparing the performance of hdemucs against\ndifferent versions of our model.",
      "tldr_zh": "本论文针对 ICASSP 2024 Cadenza Challenge，提出了一种音乐增强方法，将 Deep Filters 从原 DeepfilterNet 中分离出来，并整合到 Spec-UNet-based 网络中，以改进混合 Demucs (hdemucs) 的重混管道。动机在于 Deep Filters 能更好地处理时间精细结构，从而提升音频质量。实验结果显示，与 hdemucs 相比，该模型在 Signal-to-Distortion Ratio (SDR) 和 Hearing Aid Audio Quality Index (HAAQI) 指标上实现了渐进式改进。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "2 pages, 2 figures, 1 tables, Proceedings of the International\n  Conference on Acoustics, Speech, and Signal Processing, ICASSP 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.11116v1",
      "published_date": "2024-04-17 07:01:29 UTC",
      "updated_date": "2024-04-17 07:01:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:24:32.787842"
    },
    {
      "arxiv_id": "2404.11095v2",
      "title": "Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues",
      "title_zh": "归纳-演绎策略重用于多轮指令对话",
      "authors": [
        "Jiao Ou",
        "Jiayu Wu",
        "Che Liu",
        "Fuzheng Zhang",
        "Di Zhang",
        "Kun Gai"
      ],
      "abstract": "Aligning large language models (LLMs) with human expectations requires\nhigh-quality instructional dialogues, which usually require instructions that\nare diverse and in-depth. Existing methods leverage two LLMs to interact for\nautomatic collection: one simulating a user to pose instructions, and the other\nacting as a system agent to respond. However, these user simulators struggle to\nmodel the rules behind how dialogues can pose different instructions without\nexplicit guidance, resulting in general instructions. In this paper, we propose\nto explicitly capture the complex rules to help the user simulator pose diverse\nand in-depth instruction. Specifically, we first induce high-level instruction\nstrategies from various real instruction dialogues serving as rules. Afterward,\ndifferent possible strategies are applied to the newly given dialogue scenario\ndeductively to pose various instructions. Experimental results show that our\nmethod can generate diverse and in-depth instructions. The constructed\nmulti-turn instructional dialogues can outperform competitive baselines on the\ndownstream chat model.",
      "tldr_zh": "本论文针对大型语言模型（LLMs）在多轮指令对话生成中存在的指令多样性和深度不足问题，提出了一种归纳-演绎（Inductive-Deductive）策略重用方法。具体而言，该方法首先从真实指令对话中归纳出高层指令策略作为规则，然后将这些策略演绎应用于新对话场景，以生成更多样和深入的指令。实验结果显示，该方法构建的指令对话在下游聊天模型性能上优于竞争基线，从而提升了LLMs与人类期望的 alignment。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2024 Main Conference",
      "pdf_url": "http://arxiv.org/pdf/2404.11095v2",
      "published_date": "2024-04-17 06:26:32 UTC",
      "updated_date": "2024-09-29 12:38:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:24:44.854779"
    },
    {
      "arxiv_id": "2404.11086v2",
      "title": "ViLLM-Eval: A Comprehensive Evaluation Suite for Vietnamese Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Trong-Hieu Nguyen",
        "Anh-Cuong Le",
        "Viet-Cuong Nguyen"
      ],
      "abstract": "The rapid advancement of large language models (LLMs) necessitates the\ndevelopment of new benchmarks to accurately assess their capabilities. To\naddress this need for Vietnamese, this work aims to introduce ViLLM-Eval, the\ncomprehensive evaluation suite designed to measure the advanced knowledge and\nreasoning abilities of foundation models within a Vietnamese context.\nViLLM-Eval consists of multiple-choice questions and predict next word tasks\nspanning various difficulty levels and diverse disciplines, ranging from\nhumanities to science and engineering. A thorough evaluation of the most\nadvanced LLMs on ViLLM-Eval revealed that even the best performing models have\nsignificant room for improvement in understanding and responding to Vietnamese\nlanguage tasks. ViLLM-Eval is believed to be instrumental in identifying key\nstrengths and weaknesses of foundation models, ultimately promoting their\ndevelopment and enhancing their performance for Vietnamese users. This paper\nprovides a thorough overview of ViLLM-Eval as part of the Vietnamese Large\nLanguage Model shared task, held within the 10th International Workshop on\nVietnamese Language and Speech Processing (VLSP 2023).",
      "tldr_zh": "这篇论文介绍了 ViLLM-Eval，一套全面的评估套件，旨在评估 Large Language Models (LLMs) 在越南语语境中的高级知识和推理能力。ViLLM-Eval 包括多项选择题和预测下一个词任务，覆盖不同难度水平和多样学科，如人文、科学和工程。通过对最先进 LLMs 的评估，结果显示这些模型在处理越南语任务时仍有显著改进空间。该套件有望识别模型的强项与弱点，促进 LLMs 的发展并提升越南语用户的体验。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2305.08322 by other authors",
      "pdf_url": "http://arxiv.org/pdf/2404.11086v2",
      "published_date": "2024-04-17 05:57:17 UTC",
      "updated_date": "2024-04-18 07:41:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:24:56.872096"
    },
    {
      "arxiv_id": "2404.13082v2",
      "title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xuechen Zhang",
        "Zijian Huang",
        "Ege Onur Taga",
        "Carlee Joe-Wong",
        "Samet Oymak",
        "Jiasi Chen"
      ],
      "abstract": "Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost.",
      "tldr_zh": "该研究针对多种大型语言模型（LLM）的准确性、成本和延迟差异，提出了一种名为 TREACLE 的强化学习策略，用于在预算和延迟约束下联合选择模型和提示方案。TREACLE 通过利用问题上下文（如问题文本嵌入和响应历史）来优化决策，确保长期准确性和成本控制。在 GSM8K、CSQA 和 LLC 等标准推理数据集上的评估显示，与基线相比，TREACLE 可节省高达 85% 的成本，同时维持高准确性，并允许用户灵活权衡准确性和成本。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.13082v2",
      "published_date": "2024-04-17 05:56:49 UTC",
      "updated_date": "2024-11-19 22:02:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:25:11.008971"
    },
    {
      "arxiv_id": "2404.11075v1",
      "title": "EEG_GLT-Net: Optimising EEG Graphs for Real-time Motor Imagery Signals Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Htoo Wai Aung",
        "Jiao Jiao Li",
        "Yang An",
        "Steven W. Su"
      ],
      "abstract": "Brain-Computer Interfaces connect the brain to external control devices,\nnecessitating the accurate translation of brain signals such as from\nelectroencephalography (EEG) into executable commands. Graph Neural Networks\n(GCN) have been increasingly applied for classifying EEG Motor Imagery signals,\nprimarily because they incorporates the spatial relationships among EEG\nchannels, resulting in improved accuracy over traditional convolutional\nmethods. Recent advances by GCNs-Net in real-time EEG MI signal classification\nutilised Pearson Coefficient Correlation (PCC) for constructing adjacency\nmatrices, yielding significant results on the PhysioNet dataset. Our paper\nintroduces the EEG Graph Lottery Ticket (EEG_GLT) algorithm, an innovative\ntechnique for constructing adjacency matrices for EEG channels. It does not\nrequire pre-existing knowledge of inter-channel relationships, and it can be\ntailored to suit both individual subjects and GCN model architectures. Our\nfindings demonstrated that the PCC method outperformed the Geodesic approach by\n9.65% in mean accuracy, while our EEG_GLT matrix consistently exceeded the\nperformance of the PCC method by a mean accuracy of 13.39%. Also, we found that\nthe construction of the adjacency matrix significantly influenced accuracy, to\na greater extent than GCN model configurations. A basic GCN configuration\nutilising our EEG_GLT matrix exceeded the performance of even the most complex\nGCN setup with a PCC matrix in average accuracy. Our EEG_GLT method also\nreduced MACs by up to 97% compared to the PCC method, while maintaining or\nenhancing accuracy. In conclusion, the EEG_GLT algorithm marks a breakthrough\nin the development of optimal adjacency matrices, effectively boosting both\ncomputational accuracy and efficiency, making it well-suited for real-time\nclassification of EEG MI signals that demand intensive computational resources.",
      "tldr_zh": "该论文提出 EEG_GLT 算法，用于优化 EEG 图的邻接矩阵，以提升脑机接口中 EEG 运动想象信号的实时分类准确性。该算法无需预先了解通道间关系，可根据个体和 GCN 模型架构进行定制化调整。实验结果显示，EEG_GLT 矩阵比 Pearson Coefficient Correlation (PCC) 方法平均准确率提高 13.39%，并比 Geodesic 方法高出 9.65%；同时，它将 MACs 减少高达 97%，显著提升计算效率。总体而言，该方法证明邻接矩阵构建对分类性能的影响大于 GCN 模型配置，使其更适合资源密集型实时应用。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11075v1",
      "published_date": "2024-04-17 05:16:12 UTC",
      "updated_date": "2024-04-17 05:16:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:25:22.769860"
    },
    {
      "arxiv_id": "2404.11072v1",
      "title": "Large Language Models Meet User Interfaces: The Case of Provisioning Feedback",
      "title_zh": "大语言模型遇见用户界面：提供反馈的案例",
      "authors": [
        "Stanislav Pozdniakov",
        "Jonathan Brazil",
        "Solmaz Abdi",
        "Aneesha Bakharia",
        "Shazia Sadiq",
        "Dragan Gasevic",
        "Paul Denny",
        "Hassan Khosravi"
      ],
      "abstract": "Incorporating Generative AI (GenAI) and Large Language Models (LLMs) in\neducation can enhance teaching efficiency and enrich student learning. Current\nLLM usage involves conversational user interfaces (CUIs) for tasks like\ngenerating materials or providing feedback. However, this presents challenges\nincluding the need for educator expertise in AI and CUIs, ethical concerns with\nhigh-stakes decisions, and privacy risks. CUIs also struggle with complex\ntasks. To address these, we propose transitioning from CUIs to user-friendly\napplications leveraging LLMs via API calls. We present a framework for\nethically incorporating GenAI into educational tools and demonstrate its\napplication in our tool, Feedback Copilot, which provides personalized feedback\non student assignments. Our evaluation shows the effectiveness of this\napproach, with implications for GenAI researchers, educators, and\ntechnologists. This work charts a course for the future of GenAI in education.",
      "tldr_zh": "本研究探讨了在教育领域整合 Generative AI (GenAI) 和 Large Language Models (LLMs) 的挑战，特别是对话式用户界面 (CUIs) 在生成材料或提供反馈时面临的专家知识需求、伦理问题、隐私风险和复杂任务处理不足等问题。作者提出一个框架，通过 API 调用将 LLMs 整合到用户友好的应用中，以更伦理的方式提升教育工具。示范工具 Feedback Copilot 能提供个性化学生作业反馈，评估结果显示其有效性，并为 GenAI 研究者、教育者和技术人员提供未来应用启示。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "submission to C&E AI",
      "pdf_url": "http://arxiv.org/pdf/2404.11072v1",
      "published_date": "2024-04-17 05:05:05 UTC",
      "updated_date": "2024-04-17 05:05:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:25:33.252776"
    },
    {
      "arxiv_id": "2404.11068v1",
      "title": "ScaleFold: Reducing AlphaFold Initial Training Time to 10 Hours",
      "title_zh": "ScaleFold: 将 AlphaFold 初始训练时间减少到 10 小时",
      "authors": [
        "Feiwen Zhu",
        "Arkadiusz Nowaczynski",
        "Rundong Li",
        "Jie Xin",
        "Yifei Song",
        "Michal Marcinkiewicz",
        "Sukru Burc Eryilmaz",
        "Jun Yang",
        "Michael Andersch"
      ],
      "abstract": "AlphaFold2 has been hailed as a breakthrough in protein folding. It can\nrapidly predict protein structures with lab-grade accuracy. However, its\nimplementation does not include the necessary training code. OpenFold is the\nfirst trainable public reimplementation of AlphaFold. AlphaFold training\nprocedure is prohibitively time-consuming, and gets diminishing benefits from\nscaling to more compute resources. In this work, we conducted a comprehensive\nanalysis on the AlphaFold training procedure based on Openfold, identified that\ninefficient communications and overhead-dominated computations were the key\nfactors that prevented the AlphaFold training from effective scaling. We\nintroduced ScaleFold, a systematic training method that incorporated\noptimizations specifically for these factors. ScaleFold successfully scaled the\nAlphaFold training to 2080 NVIDIA H100 GPUs with high resource utilization. In\nthe MLPerf HPC v3.0 benchmark, ScaleFold finished the OpenFold benchmark in\n7.51 minutes, shown over $6\\times$ speedup than the baseline. For training the\nAlphaFold model from scratch, ScaleFold completed the pretraining in 10 hours,\na significant improvement over the seven days required by the original\nAlphaFold pretraining baseline.",
      "tldr_zh": "该研究针对 AlphaFold 的训练过程进行了全面分析，发现无效通信和开销主导计算是导致效率低下的关键问题。ScaleFold 引入了针对这些因素的系统优化方法，包括扩展到 2080 个 NVIDIA H100 GPUs 的高资源利用训练策略。在 MLPerf HPC v3.0 基准测试中，ScaleFold 将 OpenFold 基准测试时间缩短至 7.51 分钟，比基线快 6 倍；从零开始训练 AlphaFold 模型仅需 10 小时，显著降低了原先七天的训练时间。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11068v1",
      "published_date": "2024-04-17 04:55:33 UTC",
      "updated_date": "2024-04-17 04:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:25:48.049265"
    },
    {
      "arxiv_id": "2404.11064v3",
      "title": "Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework through Prompt-based Localization",
      "title_zh": "翻译失败",
      "authors": [
        "Yongdong Luo",
        "Haojia Lin",
        "Xiawu Zheng",
        "Yigeng Jiang",
        "Fei Chao",
        "Jie Hu",
        "Guannan Jiang",
        "Songan Zhang",
        "Rongrong Ji"
      ],
      "abstract": "3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial\ntasks in various 3D applications, which require both shared and complementary\ninformation in localization and visual-language relationships. Therefore,\nexisting approaches adopt the two-stage \"detect-then-describe/discriminate\"\npipeline, which relies heavily on the performance of the detector, resulting in\nsuboptimal performance. Inspired by DETR, we propose a unified framework,\n3DGCTR, to jointly solve these two distinct but closely related tasks in an\nend-to-end fashion. The key idea is to reconsider the prompt-based localization\nability of the 3DVG model. In this way, the 3DVG model with a well-designed\nprompt as input can assist the 3DDC task by extracting localization information\nfrom the prompt. In terms of implementation, we integrate a Lightweight Caption\nHead into the existing 3DVG network with a Caption Text Prompt as a connection,\neffectively harnessing the existing 3DVG model's inherent localization\ncapacity, thereby boosting 3DDC capability. This integration facilitates\nsimultaneous multi-task training on both tasks, mutually enhancing their\nperformance. Extensive experimental results demonstrate the effectiveness of\nthis approach. Specifically, on the ScanRefer dataset, 3DGCTR surpasses the\nstate-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and\nimproves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU. The codes are at\nhttps://github.com/Leon1207/3DGCTR.",
      "tldr_zh": "本研究重新审视了 3D Visual Grounding (3DVG) 和 3D Dense Captioning (3DDC) 两个任务的局限性，提出一个统一的端到端框架 3DGCTR，通过提示-based localization 能力来联合解决这些共享和互补的视觉语言问题。框架受 DETR 启发，将 Lightweight Caption Head 与现有 3DVG 网络集成，并使用 Caption Text Prompt 作为连接，实现多任务同时训练，从而提升了定位和描述的性能。在 ScanRefer 数据集上，3DGCTR 比最先进方法提高了 CIDEr@0.5IoU 4.3% 和 Acc@0.25IoU 3.16%，证明了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11064v3",
      "published_date": "2024-04-17 04:46:27 UTC",
      "updated_date": "2024-12-18 04:23:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:00.739186"
    },
    {
      "arxiv_id": "2404.11056v1",
      "title": "LMEraser: Large Model Unlearning through Adaptive Prompt Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Xu",
        "Zihan Wu",
        "Cong Wang",
        "Xiaohua Jia"
      ],
      "abstract": "To address the growing demand for privacy protection in machine learning, we\npropose a novel and efficient machine unlearning approach for \\textbf{L}arge\n\\textbf{M}odels, called \\textbf{LM}Eraser. Existing unlearning research suffers\nfrom entangled training data and complex model architectures, incurring\nextremely high computational costs for large models. LMEraser takes a\ndivide-and-conquer strategy with a prompt tuning architecture to isolate data\ninfluence. The training dataset is partitioned into public and private\ndatasets. Public data are used to train the backbone of the model. Private data\nare adaptively clustered based on their diversity, and each cluster is used to\noptimize a prompt separately. This adaptive prompt tuning mechanism reduces\nunlearning costs and maintains model performance. Experiments demonstrate that\nLMEraser achieves a $100$-fold reduction in unlearning costs without\ncompromising accuracy compared to prior work. Our code is available at:\n\\url{https://github.com/lmeraser/lmeraser}.",
      "tldr_zh": "本研究提出了一种名为 LMEraser 的高效机器 unlearning 方法，针对 Large Models 的隐私保护问题，通过 divide-and-conquer 策略和 adaptive prompt tuning 架构来隔离数据影响。将训练数据集分为公共和私有部分，使用公共数据训练模型 backbone，并对私有数据基于多样性进行自适应聚类，每个聚类优化一个 prompt，从而降低 unlearning 成本并维持性能。实验结果显示，LMEraser 相较于先前工作减少了 100 倍的计算成本，同时不牺牲模型准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11056v1",
      "published_date": "2024-04-17 04:08:38 UTC",
      "updated_date": "2024-04-17 04:08:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:12.159988"
    },
    {
      "arxiv_id": "2404.11049v3",
      "title": "Stepwise Alignment for Constrained Language Model Policy Optimization",
      "title_zh": "逐步对齐用于约束语言模型策略优化",
      "authors": [
        "Akifumi Wachi",
        "Thien Q. Tran",
        "Rei Sato",
        "Takumi Tanabe",
        "Youhei Akimoto"
      ],
      "abstract": "Safety and trustworthiness are indispensable requirements for real-world\napplications of AI systems using large language models (LLMs). This paper\nformulates human value alignment as an optimization problem of the language\nmodel policy to maximize reward under a safety constraint, and then proposes an\nalgorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO). One\nkey idea behind SACPO, supported by theory, is that the optimal policy\nincorporating reward and safety can be directly obtained from a reward-aligned\npolicy. Building on this key idea, SACPO aligns LLMs step-wise with each metric\nwhile leveraging simple yet powerful alignment algorithms such as direct\npreference optimization (DPO). SACPO offers several advantages, including\nsimplicity, stability, computational efficiency, and flexibility of algorithms\nand datasets. Under mild assumptions, our theoretical analysis provides the\nupper bounds on optimality and safety constraint violation. Our experimental\nresults show that SACPO can fine-tune Alpaca-7B better than the\nstate-of-the-art method in terms of both helpfulness and harmlessness.",
      "tldr_zh": "本研究将人类价值对齐问题表述为在安全约束下最大化奖励的语言模型政策优化问题，并提出了一种算法 SACPO（Stepwise Alignment for Constrained Policy Optimization）。SACPO 的核心思想是通过理论支持，从奖励对齐策略直接获得最佳策略，并逐步对齐奖励和安全指标，利用如 DPO（direct preference optimization）等简单算法实现。相比现有方法，SACPO 具有简单性、稳定性和计算效率的优势，并在理论分析中提供优化性和安全约束违反的上界。实验结果显示，SACPO 在微调 Alpaca-7B 模型时，在帮助性和无害性方面优于最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at NeurIPS 2024. Code and models are available at\n  https://github.com/line/sacpo",
      "pdf_url": "http://arxiv.org/pdf/2404.11049v3",
      "published_date": "2024-04-17 03:44:58 UTC",
      "updated_date": "2024-10-21 00:42:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:22.904693"
    },
    {
      "arxiv_id": "2404.11046v1",
      "title": "Lightweight Unsupervised Federated Learning with Pretrained Vision Language Model",
      "title_zh": "基于预训练视觉语言模型的轻量级无监督联邦学习",
      "authors": [
        "Hao Yan",
        "Yuhong Guo"
      ],
      "abstract": "Federated learning aims to tackle the ``isolated data island\" problem, where\nit trains a collective model from physically isolated clients while\nsafeguarding the privacy of users' data. However, supervised federated learning\nnecessitates that each client labels their data for training, which can be both\ntime-consuming and resource-intensive, and may even be impractical for edge\ndevices. Moreover, the training and transmission of deep models present\nchallenges to the computation and communication capabilities of the clients. To\naddress these two inherent challenges in supervised federated learning, we\npropose a novel lightweight unsupervised federated learning approach that\nleverages unlabeled data on each client to perform lightweight model training\nand communication by harnessing pretrained vision-language models, such as\nCLIP. By capitalizing on the zero-shot prediction capability and the\nwell-trained image encoder of the pre-trained CLIP model, we have carefully\ncrafted an efficient and resilient self-training approach. This method refines\nthe initial zero-shot predicted pseudo-labels of unlabeled instances through\nthe sole training of a linear classifier on top of the fixed image encoder.\nAdditionally, to address data heterogeneity within each client, we propose a\nclass-balanced text feature sampling strategy for generating synthetic\ninstances in the feature space to support local training. Experiments are\nconducted on multiple benchmark datasets. The experimental results demonstrate\nthat our proposed method greatly enhances model performance in comparison to\nCLIP's zero-shot predictions and even outperforms supervised federated learning\nbenchmark methods given limited computational and communication overhead.",
      "tldr_zh": "本文提出了一种轻量级无监督联邦学习方法，利用预训练视觉语言模型（如 CLIP），以解决传统监督联邦学习中数据标注耗时、资源密集以及计算通信挑战的问题。该方法通过CLIP的零-shot预测能力和图像编码器，进行高效的自训练，仅训练一个线性分类器来改进伪标签，并引入类平衡文本特征采样策略在特征空间生成合成实例，以应对数据异质性。实验在多个基准数据集上表明，该方法显著提升模型性能，优于CLIP的零-shot预测，甚至超越监督联邦学习基准，同时保持较低的计算和通信开销。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11046v1",
      "published_date": "2024-04-17 03:42:48 UTC",
      "updated_date": "2024-04-17 03:42:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:35.888701"
    },
    {
      "arxiv_id": "2404.11041v2",
      "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Liwei Kang",
        "Zirui Zhao",
        "David Hsu",
        "Wee Sun Lee"
      ],
      "abstract": "Chain-of-thought (CoT), tree-of-thought (ToT), and related techniques work\nsurprisingly well in practice for some complex reasoning tasks with Large\nLanguage Models (LLMs), but why? This work seeks the underlying reasons by\nconducting experimental case studies and linking the performance benefits to\nwell-established sample and computational complexity principles in machine\nlearning. We experimented with 6 reasoning tasks, ranging from grade school\nmath, air travel planning, ..., to Blocksworld. The results suggest that (i)\nboth CoT and ToT benefit significantly from task decomposition, which breaks a\ncomplex reasoning task into a sequence of steps with low sample complexity and\nexplicitly outlines the reasoning structure, and (ii) for computationally hard\nreasoning tasks, the more sophisticated tree structure of ToT outperforms the\nlinear structure of CoT. These findings provide useful guidelines for the use\nof LLM in solving reasoning tasks in practice.",
      "tldr_zh": "这篇论文通过实验研究探讨了 Chain-of-thought (CoT) 和 Tree-of-thought (ToT) 等技术在 Large Language Models (LLMs) 中的推理性能，并将其与机器学习的样本和计算复杂度原则联系起来。研究涉及6个任务，从小学数学到Blocksworld，揭示了任务分解能显著提升CoT和ToT的效果，因为它将复杂任务分解为低样本复杂度的步骤并明确推理结构。对于计算上困难的任务，ToT的树状结构比CoT的线性结构表现出色。这些发现为实际应用LLMs解决推理任务提供了实用指导。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11041v2",
      "published_date": "2024-04-17 03:34:27 UTC",
      "updated_date": "2024-06-18 02:03:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:47.320520"
    },
    {
      "arxiv_id": "2404.11027v1",
      "title": "Empowering Large Language Models on Robotic Manipulation with Affordance Prompting",
      "title_zh": "翻译失败",
      "authors": [
        "Guangran Cheng",
        "Chuheng Zhang",
        "Wenzhe Cai",
        "Li Zhao",
        "Changyin Sun",
        "Jiang Bian"
      ],
      "abstract": "While large language models (LLMs) are successful in completing various\nlanguage processing tasks, they easily fail to interact with the physical world\nby generating control sequences properly. We find that the main reason is that\nLLMs are not grounded in the physical world. Existing LLM-based approaches\ncircumvent this problem by relying on additional pre-defined skills or\npre-trained sub-policies, making it hard to adapt to new tasks. In contrast, we\naim to address this problem and explore the possibility to prompt pre-trained\nLLMs to accomplish a series of robotic manipulation tasks in a training-free\nparadigm. Accordingly, we propose a framework called LLM+A(ffordance) where the\nLLM serves as both the sub-task planner (that generates high-level plans) and\nthe motion controller (that generates low-level control sequences). To ground\nthese plans and control sequences on the physical world, we develop the\naffordance prompting technique that stimulates the LLM to 1) predict the\nconsequences of generated plans and 2) generate affordance values for relevant\nobjects. Empirically, we evaluate the effectiveness of LLM+A in various\nlanguage-conditioned robotic manipulation tasks, which show that our approach\nsubstantially improves performance by enhancing the feasibility of generated\nplans and control and can easily generalize to different environments.",
      "tldr_zh": "本文研究发现，大语言模型(LLMs)在机器人操作中难以生成合适的控制序列，主要由于缺乏对物理世界的grounding。为解决此问题，提出LLM+A框架，利用Affordance Prompting技术，让LLM同时担任子任务规划器（生成高层计划）和运动控制器（生成低层控制序列），并通过预测计划后果和生成相关对象的affordance值来提升可行性。实验结果显示，该方法在各种语言条件下的机器人操作任务中显著提高了性能，并易于泛化到不同环境。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11027v1",
      "published_date": "2024-04-17 03:06:32 UTC",
      "updated_date": "2024-04-17 03:06:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:26:59.611872"
    },
    {
      "arxiv_id": "2404.11018v3",
      "title": "Many-Shot In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Rishabh Agarwal",
        "Avi Singh",
        "Lei M. Zhang",
        "Bernd Bohnet",
        "Luis Rosias",
        "Stephanie Chan",
        "Biao Zhang",
        "Ankesh Anand",
        "Zaheer Abbas",
        "Azade Nova",
        "John D. Co-Reyes",
        "Eric Chu",
        "Feryal Behbahani",
        "Aleksandra Faust",
        "Hugo Larochelle"
      ],
      "abstract": "Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. We also find that inference cost increases\nlinearly in the many-shot regime, and frontier LLMs benefit from many-shot ICL\nto varying degrees. Our analysis also reveals the limitations of next-token\nprediction loss as an indicator of downstream ICL performance.",
      "tldr_zh": "大语言模型（LLMs）在 many-shot In-Context Learning（ICL）中表现出色，能够从数百或数千示例中学习，显著提升生成和判别任务的性能。论文引入了 Reinforced ICL（使用模型生成的 chain-of-thought 推理代替人类示例）和 Unsupervised ICL（仅提供领域特定问题），以缓解示例短缺问题，并在复杂推理任务上取得有效效果。研究发现，many-shot ICL 可以覆盖预训练偏差、学习高维函数，并与 fine-tuning 性能相当，但推理成本线性增加，且 next-token prediction loss 并非下游 ICL 性能的可靠指标。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2404.11018v3",
      "published_date": "2024-04-17 02:49:26 UTC",
      "updated_date": "2024-10-17 17:45:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:27:12.924905"
    },
    {
      "arxiv_id": "2404.11016v2",
      "title": "MaeFuse: Transferring Omni Features with Pretrained Masked Autoencoders for Infrared and Visible Image Fusion via Guided Training",
      "title_zh": "翻译失败",
      "authors": [
        "Jiayang Li",
        "Junjun Jiang",
        "Pengwei Liang",
        "Jiayi Ma",
        "Liqiang Nie"
      ],
      "abstract": "In this paper, we introduce MaeFuse, a novel autoencoder model designed for\nInfrared and Visible Image Fusion (IVIF). The existing approaches for image\nfusion often rely on training combined with downstream tasks to obtain\nhighlevel visual information, which is effective in emphasizing target objects\nand delivering impressive results in visual quality and task-specific\napplications. Instead of being driven by downstream tasks, our model called\nMaeFuse utilizes a pretrained encoder from Masked Autoencoders (MAE), which\nfacilities the omni features extraction for low-level reconstruction and\nhigh-level vision tasks, to obtain perception friendly features with a low\ncost. In order to eliminate the domain gap of different modal features and the\nblock effect caused by the MAE encoder, we further develop a guided training\nstrategy. This strategy is meticulously crafted to ensure that the fusion layer\nseamlessly adjusts to the feature space of the encoder, gradually enhancing the\nfusion performance. The proposed method can facilitate the comprehensive\nintegration of feature vectors from both infrared and visible modalities, thus\npreserving the rich details inherent in each modal. MaeFuse not only introduces\na novel perspective in the realm of fusion techniques but also stands out with\nimpressive performance across various public datasets.",
      "tldr_zh": "本研究提出MaeFuse，一种新型自编码器模型，用于Infrared and Visible Image Fusion (IVIF)，通过利用预训练的Masked Autoencoders (MAE)编码器，以低成本提取omni features，实现低级重建和高水平视觉任务的感知友好特征。MaeFuse引入guided training策略，以消除不同模态特征的领域差距和由MAE编码器引起的块效应，确保融合层顺畅适应特征空间，从而全面整合红外和可见光图像的丰富细节。该方法在各种公共数据集上表现出色，提供了一种创新的融合视角，并提升了图像融合的整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11016v2",
      "published_date": "2024-04-17 02:47:39 UTC",
      "updated_date": "2025-02-09 09:23:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:27:23.522943"
    },
    {
      "arxiv_id": "2404.11015v2",
      "title": "FedFa: A Fully Asynchronous Training Paradigm for Federated Learning",
      "title_zh": "FedFa：联邦学习的一种完全异步训练范式",
      "authors": [
        "Haotian Xu",
        "Zhaorui Zhang",
        "Sheng Di",
        "Benben Liu",
        "Khalid Ayed Alharthi",
        "Jiannong Cao"
      ],
      "abstract": "Federated learning has been identified as an efficient decentralized training\nparadigm for scaling the machine learning model training on a large number of\ndevices while guaranteeing the data privacy of the trainers. FedAvg has become\na foundational parameter update strategy for federated learning, which has been\npromising to eliminate the effect of the heterogeneous data across clients and\nguarantee convergence. However, the synchronization parameter update barriers\nfor each communication round during the training significant time on waiting,\nslowing down the training procedure. Therefore, recent state-of-the-art\nsolutions propose using semi-asynchronous approaches to mitigate the waiting\ntime cost with guaranteed convergence. Nevertheless, emerging semi-asynchronous\napproaches are unable to eliminate the waiting time completely.\n  We propose a full asynchronous training paradigm, called FedFa, which can\nguarantee model convergence and eliminate the waiting time completely for\nfederated learning by using a few buffered results on the server for parameter\nupdating. Further, we provide theoretical proof of the convergence rate for our\nproposed FedFa. Extensive experimental results indicate our approach\neffectively improves the training performance of federated learning by up to 6x\nand 4x speedup compared to the state-of-the-art synchronous and\nsemi-asynchronous strategies while retaining high accuracy in both IID and\nNon-IID scenarios.",
      "tldr_zh": "该论文针对 Federated Learning 中同步更新导致的等待时间问题，提出了一种全异步训练范式 FedFa，通过服务器上的少量缓冲结果进行参数更新，从而完全消除等待时间并保证模型收敛。FedFa 提供了收敛率的理论证明，解决了传统 FedAvg 和半异步方法的局限性。实验结果显示，该方法在 IID 和 Non-IID 场景下，相较于同步和半异步策略，训练速度分别提升高达 6 倍和 4 倍，同时保持高准确率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11015v2",
      "published_date": "2024-04-17 02:46:59 UTC",
      "updated_date": "2024-04-20 14:26:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:27:35.617148"
    },
    {
      "arxiv_id": "2404.11014v2",
      "title": "Towards Multi-agent Reinforcement Learning based Traffic Signal Control through Spatio-temporal Hypergraphs",
      "title_zh": "翻译失败",
      "authors": [
        "Kang Wang",
        "Zhishu Shen",
        "Zhen Lei",
        "Tiehua Zhang"
      ],
      "abstract": "Traffic signal control systems (TSCSs) are integral to intelligent traffic\nmanagement, fostering efficient vehicle flow. Traditional approaches often\nsimplify road networks into standard graphs, which results in a failure to\nconsider the dynamic nature of traffic data at neighboring intersections,\nthereby neglecting higher-order interconnections necessary for real-time\ncontrol. To address this, we propose a novel TSCS framework to realize\nintelligent traffic control. This framework collaborates with multiple\nneighboring edge computing servers to collect traffic information across the\nroad network. To elevate the efficiency of traffic signal control, we have\ncrafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning\nalgorithm. Within this algorithm, individual agents are deployed at each\nintersection with a mandate to optimize traffic flow across the road network\ncollectively. Furthermore, we introduce hypergraph learning into the critic\nnetwork of MA-SAC to enable the spatio-temporal interactions from multiple\nintersections in the road network. This method fuses hypergraph and\nspatio-temporal graph structures to encode traffic data and capture the complex\nspatio-temporal correlations between multiple intersections. Our empirical\nevaluation, tested on varied datasets, demonstrates the superiority of our\nframework in minimizing average vehicle travel times and sustaining\nhigh-throughput performance. This work facilitates the development of more\nintelligent urban traffic management solutions. We release the code to support\nthe reproducibility of this work at https://github.com/Edun-Eyes/TSC",
      "tldr_zh": "本研究针对传统交通信号控制系统(TSCSs)简化路网为标准图而忽略动态交通数据和邻近路口更高阶连接的问题，提出一个新型框架，利用多个边缘计算服务器收集路网交通信息。框架中引入多智能体软演员-评论家(MA-SAC)强化学习算法，每个路口部署一个代理，共同优化交通流量，并在批评网络中整合超图学习(hypergraph learning)，通过融合超图和时空图结构捕捉多个路口的复杂时空相关性。实验结果显示，该框架在各种数据集上显著降低了平均车辆旅行时间并保持高吞吐量，为智能城市交通管理提供了先进解决方案，并发布了代码以支持可重复性。",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Accepted by IEEE Transactions on Mobile Computing",
      "pdf_url": "http://arxiv.org/pdf/2404.11014v2",
      "published_date": "2024-04-17 02:46:18 UTC",
      "updated_date": "2025-04-03 13:50:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:27:49.315624"
    },
    {
      "arxiv_id": "2404.11008v1",
      "title": "AKGNet: Attribute Knowledge-Guided Unsupervised Lung-Infected Area Segmentation",
      "title_zh": "AKGNet:",
      "authors": [
        "Qing En",
        "Yuhong Guo"
      ],
      "abstract": "Lung-infected area segmentation is crucial for assessing the severity of lung\ndiseases. However, existing image-text multi-modal methods typically rely on\nlabour-intensive annotations for model training, posing challenges regarding\ntime and expertise. To address this issue, we propose a novel attribute\nknowledge-guided framework for unsupervised lung-infected area segmentation\n(AKGNet), which achieves segmentation solely based on image-text data without\nany mask annotation. AKGNet facilitates text attribute knowledge learning,\nattribute-image cross-attention fusion, and high-confidence-based pseudo-label\nexploration simultaneously. It can learn statistical information and capture\nspatial correlations between image and text attributes in the embedding space,\niteratively refining the mask to enhance segmentation. Specifically, we\nintroduce a text attribute knowledge learning module by extracting attribute\nknowledge and incorporating it into feature representations, enabling the model\nto learn statistical information and adapt to different attributes. Moreover,\nwe devise an attribute-image cross-attention module by calculating the\ncorrelation between attributes and images in the embedding space to capture\nspatial dependency information, thus selectively focusing on relevant regions\nwhile filtering irrelevant areas. Finally, a self-training mask improvement\nprocess is employed by generating pseudo-labels using high-confidence\npredictions to iteratively enhance the mask and segmentation. Experimental\nresults on a benchmark medical image dataset demonstrate the superior\nperformance of our method compared to state-of-the-art segmentation techniques\nin unsupervised scenarios.",
      "tldr_zh": "该研究提出了一种名为 AKGNet 的无监督肺部感染区域分割框架，利用属性知识指导（attribute knowledge-guided），无需任何掩码标注，仅基于图像-文本数据进行分割。框架包括文本属性知识学习模块、属性-图像交叉注意力（cross-attention）融合模块，以及基于高置信度伪标签（pseudo-label）的自训练掩码改进过程，以学习统计信息、捕获空间相关性和迭代优化分割结果。实验在基准医疗图像数据集上显示，AKGNet 在无监督场景下优于现有最先进分割技术，证明了其在评估肺部疾病严重性方面的潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.11008v1",
      "published_date": "2024-04-17 02:36:02 UTC",
      "updated_date": "2024-04-17 02:36:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:28:01.325738"
    },
    {
      "arxiv_id": "2404.10991v1",
      "title": "Function Approximation for Reinforcement Learning Controller for Energy from Spread Waves",
      "title_zh": "翻译失败",
      "authors": [
        "Soumyendu Sarkar",
        "Vineet Gundecha",
        "Sahand Ghorbanpour",
        "Alexander Shmakov",
        "Ashwin Ramesh Babu",
        "Avisek Naug",
        "Alexandre Pichard",
        "Mathieu Cocho"
      ],
      "abstract": "The industrial multi-generator Wave Energy Converters (WEC) must handle\nmultiple simultaneous waves coming from different directions called spread\nwaves. These complex devices in challenging circumstances need controllers with\nmultiple objectives of energy capture efficiency, reduction of structural\nstress to limit maintenance, and proactive protection against high waves. The\nMulti-Agent Reinforcement Learning (MARL) controller trained with the Proximal\nPolicy Optimization (PPO) algorithm can handle these complexities. In this\npaper, we explore different function approximations for the policy and critic\nnetworks in modeling the sequential nature of the system dynamics and find that\nthey are key to better performance. We investigated the performance of a fully\nconnected neural network (FCN), LSTM, and Transformer model variants with\nvarying depths and gated residual connections. Our results show that the\ntransformer model of moderate depth with gated residual connections around the\nmulti-head attention, multi-layer perceptron, and the transformer block (STrXL)\nproposed in this paper is optimal and boosts energy efficiency by an average of\n22.1% for these complex spread waves over the existing spring damper (SD)\ncontroller. Furthermore, unlike the default SD controller, the transformer\ncontroller almost eliminated the mechanical stress from the rotational yaw\nmotion for angled waves. Demo: https://tinyurl.com/yueda3jh",
      "tldr_zh": "本论文探讨了针对多向传播波的波浪能转换器（WEC）设计多智能体强化学习（MARL）控制器，以优化能量捕获效率、减少结构应力和防范高波风险。研究者使用近端策略优化（PPO）算法，比较了不同函数逼近方法，包括全连接神经网络（FCN）、LSTM 和 Transformer 模型变体，并提出了一种中等深度的 Transformer 模型（STrXL），其包含门控残差连接。结果显示，STrXL 模型比传统弹簧阻尼（SD）控制器平均提高能量效率 22.1%，并几乎消除了 angled waves 引起的旋转偏航机械应力，为复杂波浪环境下的能量控制提供了更有效的解决方案。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "IJCAI 2023, Proceedings of the Thirty-Second International Joint\n  Conference on Artificial IntelligenceAugust 2023",
      "pdf_url": "http://arxiv.org/pdf/2404.10991v1",
      "published_date": "2024-04-17 02:04:10 UTC",
      "updated_date": "2024-04-17 02:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:28:14.324732"
    },
    {
      "arxiv_id": "2404.10981v2",
      "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Yizheng Huang",
        "Jimmy Huang"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep\nlearning advancements to address the static limitations of large language\nmodels (LLMs) by enabling the dynamic integration of up-to-date external\ninformation. This methodology, focusing primarily on the text domain, provides\na cost-effective solution to the generation of plausible but possibly incorrect\nresponses by LLMs, thereby enhancing the accuracy and reliability of their\noutputs through the use of real-world data. As RAG grows in complexity and\nincorporates multiple concepts that can influence its performance, this paper\norganizes the RAG paradigm into four categories: pre-retrieval, retrieval,\npost-retrieval, and generation, offering a detailed perspective from the\nretrieval viewpoint. It outlines RAG's evolution and discusses the field's\nprogression through the analysis of significant studies. Additionally, the\npaper introduces evaluation methods for RAG, addressing the challenges faced\nand proposing future research directions. By offering an organized framework\nand categorization, the study aims to consolidate existing research on RAG,\nclarify its technological underpinnings, and highlight its potential to broaden\nthe adaptability and applications of LLMs.",
      "tldr_zh": "这篇论文对 Retrieval-Augmented Generation (RAG) 进行了全面调查，RAG 通过结合检索方法和深度学习，解决了 Large Language Models (LLMs) 的静态限制，实现动态整合外部信息，从而提升输出准确性和可靠性。论文将 RAG 框架分为四个阶段——pre-retrieval、retrieval、post-retrieval 和 generation，从检索视角分析其演变、关键研究进展，并介绍了评估方法。最终，该研究整合现有工作、澄清技术基础，并提出未来方向，以扩展 LLMs 的适应性和应用潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Ongoing Work",
      "pdf_url": "http://arxiv.org/pdf/2404.10981v2",
      "published_date": "2024-04-17 01:27:42 UTC",
      "updated_date": "2024-08-23 00:17:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:28:25.746689"
    },
    {
      "arxiv_id": "2404.10978v1",
      "title": "Leveraging 3D LiDAR Sensors to Enable Enhanced Urban Safety and Public Health: Pedestrian Monitoring and Abnormal Activity Detection",
      "title_zh": "利用 3D LiDAR 传感器增强城市安全和公共健康：行人监测与异常活动检测",
      "authors": [
        "Nawfal Guefrachi",
        "Jian Shi",
        "Hakim Ghazzai",
        "Ahmad Alsharoa"
      ],
      "abstract": "The integration of Light Detection and Ranging (LiDAR) and Internet of Things\n(IoT) technologies offers transformative opportunities for public health\ninformatics in urban safety and pedestrian well-being. This paper proposes a\nnovel framework utilizing these technologies for enhanced 3D object detection\nand activity classification in urban traffic scenarios. By employing elevated\nLiDAR, we obtain detailed 3D point cloud data, enabling precise pedestrian\nactivity monitoring. To overcome urban data scarcity, we create a specialized\ndataset through simulated traffic environments in Blender, facilitating\ntargeted model training. Our approach employs a modified Point\nVoxel-Region-based Convolutional Neural Network (PV-RCNN) for robust 3D\ndetection and PointNet for classifying pedestrian activities, significantly\nbenefiting urban traffic management and public health by offering insights into\npedestrian behavior and promoting safer urban environments. Our dual-model\napproach not only enhances urban traffic management but also contributes\nsignificantly to public health by providing insights into pedestrian behavior\nand promoting safer urban environment.",
      "tldr_zh": "本研究提出一个整合 3D LiDAR 和 IoT 技术的框架，用于提升城市安全和公共健康，通过精确监测行人活动和检测异常行为。框架利用高位 LiDAR 获取详细的 3D 点云数据，并通过 Blender 模拟交通环境创建专用数据集，以解决城市数据稀缺问题。采用修改后的 PV-RCNN 进行 3D 物体检测，以及 PointNet 进行行人活动分类，从而提供行人行为洞察，提升交通管理和促进更安全的城市环境。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.10978v1",
      "published_date": "2024-04-17 01:23:49 UTC",
      "updated_date": "2024-04-17 01:23:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:28:36.046499"
    },
    {
      "arxiv_id": "2404.10976v3",
      "title": "Group-Aware Coordination Graph for Multi-Agent Reinforcement Learning",
      "title_zh": "群组感知协调图用于多智能体强化学习",
      "authors": [
        "Wei Duan",
        "Jie Lu",
        "Junyu Xuan"
      ],
      "abstract": "Cooperative Multi-Agent Reinforcement Learning (MARL) necessitates seamless\ncollaboration among agents, often represented by an underlying relation graph.\nExisting methods for learning this graph primarily focus on agent-pair\nrelations, neglecting higher-order relationships. While several approaches\nattempt to extend cooperation modelling to encompass behaviour similarities\nwithin groups, they commonly fall short in concurrently learning the latent\ngraph, thereby constraining the information exchange among partially observed\nagents. To overcome these limitations, we present a novel approach to infer the\nGroup-Aware Coordination Graph (GACG), which is designed to capture both the\ncooperation between agent pairs based on current observations and group-level\ndependencies from behaviour patterns observed across trajectories. This graph\nis further used in graph convolution for information exchange between agents\nduring decision-making. To further ensure behavioural consistency among agents\nwithin the same group, we introduce a group distance loss, which promotes group\ncohesion and encourages specialization between groups. Our evaluations,\nconducted on StarCraft II micromanagement tasks, demonstrate GACG's superior\nperformance. An ablation study further provides experimental evidence of the\neffectiveness of each component of our method.",
      "tldr_zh": "本研究针对合作性多智能体强化学习（MARL），提出了一种Group-Aware Coordination Graph (GACG)方法，以解决现有方法忽略更高阶关系和组内行为相似性的问题。GACG 通过分析当前观察捕获代理对之间的合作，并从轨迹行为模式中推断组级依赖，然后利用图卷积进行代理间的信息交换；同时，引入group distance loss以强化组内行为一致性和组间专业化。在StarCraft II微管理任务的评估中，GACG表现出优越性能，并通过消融研究证实了各组件的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IJCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.10976v3",
      "published_date": "2024-04-17 01:17:10 UTC",
      "updated_date": "2024-05-11 23:50:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:28:48.895844"
    },
    {
      "arxiv_id": "2404.13081v1",
      "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs",
      "title_zh": "SuRe：利用答案候选总结检索以支持大型语言模型的开放域问答",
      "authors": [
        "Jaehyung Kim",
        "Jaehyun Nam",
        "Sangwoo Mo",
        "Jongjin Park",
        "Sang-Woo Lee",
        "Minjoon Seo",
        "Jung-Woo Ha",
        "Jinwoo Shin"
      ],
      "abstract": "Large language models (LLMs) have made significant advancements in various\nnatural language processing tasks, including question answering (QA) tasks.\nWhile incorporating new information with the retrieval of relevant passages is\na promising way to improve QA with LLMs, the existing methods often require\nadditional fine-tuning which becomes infeasible with recent LLMs. Augmenting\nretrieved passages via prompting has the potential to address this limitation,\nbut this direction has been limitedly explored. To this end, we design a simple\nyet effective framework to enhance open-domain QA (ODQA) with LLMs, based on\nthe summarized retrieval (SuRe). SuRe helps LLMs predict more accurate answers\nfor a given question, which are well-supported by the summarized retrieval that\ncould be viewed as an explicit rationale extracted from the retrieved passages.\nSpecifically, SuRe first constructs summaries of the retrieved passages for\neach of the multiple answer candidates. Then, SuRe confirms the most plausible\nanswer from the candidate set by evaluating the validity and ranking of the\ngenerated summaries. Experimental results on diverse ODQA benchmarks\ndemonstrate the superiority of SuRe, with improvements of up to 4.6% in exact\nmatch (EM) and 4.0% in F1 score over standard prompting approaches. SuRe also\ncan be integrated with a broad range of retrieval methods and LLMs. Finally,\nthe generated summaries from SuRe show additional advantages to measure the\nimportance of retrieved passages and serve as more preferred rationales by\nmodels and humans.",
      "tldr_zh": "本研究提出 SuRe 框架，用于提升大型语言模型 (LLMs) 在开放域问答 (ODQA) 中的性能，通过总结检索段落并利用答案候选来避免额外微调的需求。SuRe 的方法包括为多个答案候选构建检索段落的总结，然后通过评估这些总结的有效性和排名，选出最可靠的答案，从而提供明确的理性依据。实验结果显示，SuRe 在多种 ODQA 基准上比标准提示方法提高了最高 4.6% 的精确匹配 (EM) 和 4.0% 的 F1 分数；此外，该框架可与各种检索方法和 LLMs 整合，并生成有价值的总结来衡量检索段落的重要性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.13081v1",
      "published_date": "2024-04-17 01:15:54 UTC",
      "updated_date": "2024-04-17 01:15:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T01:29:01.824008"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 107,
  "processed_papers_count": 107,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T01:29:25.658626"
}
[
  {
    "arxiv_id": "2502.05724v2",
    "title": "Rethinking Link Prediction for Directed Graphs",
    "authors": [
      "Mingguo He",
      "Yuhe Guo",
      "Yanping Zheng",
      "Zhewei Wei",
      "Stephan GÃ¼nnemann",
      "Xiaokui Xiao"
    ],
    "abstract": "Link prediction for directed graphs is a crucial task with diverse real-world\napplications. Recent advances in embedding methods and Graph Neural Networks\n(GNNs) have shown promising improvements. However, these methods often lack a\nthorough analysis of their expressiveness and suffer from effective benchmarks\nfor a fair evaluation. In this paper, we propose a unified framework to assess\nthe expressiveness of existing methods, highlighting the impact of dual\nembeddings and decoder design on directed link prediction performance. To\naddress limitations in current benchmark setups, we introduce DirLinkBench, a\nrobust new benchmark with comprehensive coverage, standardized evaluation, and\nmodular extensibility. The results on DirLinkBench show that current methods\nstruggle to achieve strong performance, while DiGAE outperforms other baselines\noverall. We further revisit DiGAE theoretically, showing its graph convolution\naligns with GCN on an undirected bipartite graph. Inspired by these insights,\nwe propose a novel Spectral Directed Graph Auto-Encoder SDGAE that achieves\nstate-of-the-art average performance on DirLinkBench. Finally, we analyze key\nfactors influencing directed link prediction and highlight open challenges in\nthis field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2502.05724v2",
    "published_date": "2025-02-08 23:51:05 UTC",
    "updated_date": "2025-05-21 07:07:55 UTC"
  },
  {
    "arxiv_id": "2502.05720v1",
    "title": "Pareto-Optimality, Smoothness, and Stochasticity in Learning-Augmented One-Max-Search",
    "authors": [
      "Ziyad Benomar",
      "Lorenzo Croissant",
      "Vianney Perchet",
      "Spyros Angelopoulos"
    ],
    "abstract": "One-max search is a classic problem in online decision-making, in which a\ntrader acts on a sequence of revealed prices and accepts one of them\nirrevocably to maximise its profit. The problem has been studied both in\nprobabilistic and in worst-case settings, notably through competitive analysis,\nand more recently in learning-augmented settings in which the trader has access\nto a prediction on the sequence. However, existing approaches either lack\nsmoothness, or do not achieve optimal worst-case guarantees: they do not attain\nthe best possible trade-off between the consistency and the robustness of the\nalgorithm. We close this gap by presenting the first algorithm that\nsimultaneously achieves both of these important objectives. Furthermore, we\nshow how to leverage the obtained smoothness to provide an analysis of one-max\nsearch in stochastic learning-augmented settings which capture randomness in\nboth the observed prices and the prediction.",
    "categories": [
      "cs.DS",
      "cs.AI"
    ],
    "primary_category": "cs.DS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05720v1",
    "published_date": "2025-02-08 23:25:52 UTC",
    "updated_date": "2025-02-08 23:25:52 UTC"
  },
  {
    "arxiv_id": "2502.05719v1",
    "title": "Extended Histogram-based Outlier Score (EHBOS)",
    "authors": [
      "Tanvir Islam"
    ],
    "abstract": "Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly\ndetection method known for its computational efficiency and simplicity.\nHowever, its assumption of feature independence limits its ability to detect\nanomalies in datasets where interactions between features are critical. In this\npaper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which\nenhances HBOS by incorporating two-dimensional histograms to capture\ndependencies between feature pairs. This extension allows EHBOS to identify\ncontextual and dependency-driven anomalies that HBOS fails to detect. We\nevaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and\nrobustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS\non several datasets, particularly those where feature interactions are critical\nin defining the anomaly structure, achieving notable improvements in ROC AUC.\nThese results highlight that EHBOS can be a valuable extension to HBOS, with\nthe ability to model complex feature dependencies. EHBOS offers a powerful new\ntool for anomaly detection, particularly in datasets where contextual or\nrelational anomalies play a significant role.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05719v1",
    "published_date": "2025-02-08 23:24:30 UTC",
    "updated_date": "2025-02-08 23:24:30 UTC"
  },
  {
    "arxiv_id": "2502.06885v1",
    "title": "Topological derivative approach for deep neural network architecture adaptation",
    "authors": [
      "C G Krishnanunni",
      "Tan Bui-Thanh",
      "Clint Dawson"
    ],
    "abstract": "This work presents a novel algorithm for progressively adapting neural\nnetwork architecture along the depth. In particular, we attempt to address the\nfollowing questions in a mathematically principled way: i) Where to add a new\ncapacity (layer) during the training process? ii) How to initialize the new\ncapacity? At the heart of our approach are two key ingredients: i) the\nintroduction of a ``shape functional\" to be minimized, which depends on neural\nnetwork topology, and ii) the introduction of a topological derivative of the\nshape functional with respect to the neural network topology. Using an optimal\ncontrol viewpoint, we show that the network topological derivative exists under\ncertain conditions, and its closed-form expression is derived. In particular,\nwe explore, for the first time, the connection between the topological\nderivative from a topology optimization framework with the Hamiltonian from\noptimal control theory. Further, we show that the optimality condition for the\nshape functional leads to an eigenvalue problem for deep neural architecture\nadaptation. Our approach thus determines the most sensitive location along the\ndepth where a new layer needs to be inserted during the training phase and the\nassociated parametric initialization for the newly added layer. We also\ndemonstrate that our layer insertion strategy can be derived from an optimal\ntransport viewpoint as a solution to maximizing a topological derivative in\n$p$-Wasserstein space, where $p>= 1$. Numerical investigations with fully\nconnected network, convolutional neural network, and vision transformer on\nvarious regression and classification problems demonstrate that our proposed\napproach can outperform an ad-hoc baseline network and other architecture\nadaptation strategies. Further, we also demonstrate other applications of\ntopological derivative in fields such as transfer learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06885v1",
    "published_date": "2025-02-08 23:01:07 UTC",
    "updated_date": "2025-02-08 23:01:07 UTC"
  },
  {
    "arxiv_id": "2502.05714v1",
    "title": "Proving the Coding Interview: A Benchmark for Formally Verified Code Generation",
    "authors": [
      "Quinn Dougherty",
      "Ronak Mehta"
    ],
    "abstract": "We introduce the Formally Verified Automated Programming Progress Standards,\nor FVAPPS, a benchmark of 4715 samples for writing programs and proving their\ncorrectness, the largest formal verification benchmark, including 1083 curated\nand quality controlled samples. Previously, APPS provided a benchmark and\ndataset for programming puzzles to be completed in Python and checked against\nunit tests, of the kind seen in technical assessments in the software\nengineering industry. Building upon recent approaches for benchmarks in\ninteractive theorem proving, we generalize the unit tests to Lean 4 theorems\ngiven without proof (i.e., using Lean's \"sorry\" keyword). On the 406 theorems\nof 100 randomly selected samples, Sonnet correctly proves 30% and Gemini\ncorrectly proves 18%. We challenge the machine learning and program synthesis\ncommunities to solve both each general purpose programming problem and its\nassociated correctness specifications. The benchmark is available at\nhttps://huggingface.co/datasets/quinn-dougherty/fvapps.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.SE",
    "comment": "8 pages, to appear at the 2025LLM4Code Workshop at ICSE 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.05714v1",
    "published_date": "2025-02-08 22:54:58 UTC",
    "updated_date": "2025-02-08 22:54:58 UTC"
  },
  {
    "arxiv_id": "2502.05713v1",
    "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
    "authors": [
      "An Zhao",
      "Moucheng Xu",
      "Ahmed H. Shahin",
      "Wim Wuyts",
      "Mark G. Jones",
      "Joseph Jacob",
      "Daniel C. Alexander"
    ],
    "abstract": "Understanding the progression trajectories of diseases is crucial for early\ndiagnosis and effective treatment planning. This is especially vital for\nlife-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a\nchronic, progressive lung disease with a prognosis comparable to many cancers.\nComputed tomography (CT) imaging has been established as a reliable diagnostic\ntool for IPF. Accurately predicting future CT scans of early-stage IPF patients\ncan aid in developing better treatment strategies, thereby improving survival\noutcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial\nNetworks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF\npatients at any time point. The model is trained using a two-stage approach. In\nthe first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the\nsecond stage, a Neural Ordinary Differential Equation (ODE) based temporal\nmodel is trained to capture the temporal dynamics of the quantised embeddings\ngenerated by the encoder in the first stage. We evaluate different\nconfigurations of our model for generating longitudinal CT scans and compare\nthe results against ground truth data, both quantitatively and qualitatively.\nFor validation, we conduct survival analysis using imaging biomarkers derived\nfrom generated CT scans and achieve a C-index comparable to that of biomarkers\nderived from the real CT scans. The survival analysis results demonstrate the\npotential clinical utility inherent to generated longitudinal CT scans, showing\nthat they can reliably predict survival outcomes.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "4D image synthesis, VQ-GAN, neural ODEs, spatial temporal disease\n  progression modelling, CT, IPF",
    "pdf_url": "http://arxiv.org/pdf/2502.05713v1",
    "published_date": "2025-02-08 22:25:53 UTC",
    "updated_date": "2025-02-08 22:25:53 UTC"
  },
  {
    "arxiv_id": "2502.05704v1",
    "title": "Rethinking Word Similarity: Semantic Similarity through Classification Confusion",
    "authors": [
      "Kaitlyn Zhou",
      "Haishan Gao",
      "Sarah Chen",
      "Dan Edelstein",
      "Dan Jurafsky",
      "Chen Shani"
    ],
    "abstract": "Word similarity has many applications to social science and cultural\nanalytics tasks like measuring meaning change over time and making sense of\ncontested terms. Yet traditional similarity methods based on cosine similarity\nbetween word embeddings cannot capture the context-dependent, asymmetrical,\npolysemous nature of semantic similarity. We propose a new measure of\nsimilarity, Word Confusion, that reframes semantic similarity in terms of\nfeature-based classification confusion. Word Confusion is inspired by Tversky's\nsuggestion that similarity features be chosen dynamically. Here we train a\nclassifier to map contextual embeddings to word identities and use the\nclassifier confusion (the probability of choosing a confounding word c instead\nof the correct target word t) as a measure of the similarity of c and t. The\nset of potential confounding words acts as the chosen features. Our method is\ncomparable to cosine similarity in matching human similarity judgments across\nseveral datasets (MEN, WirdSim353, and SimLex), and can measure similarity\nusing predetermined features of interest. We demonstrate our model's ability to\nmake use of dynamic features by applying it to test a hypothesis about changes\nin the 18th C. meaning of the French word \"revolution\" from popular to state\naction during the French Revolution. We hope this reimagining of semantic\nsimilarity will inspire the development of new tools that better capture the\nmulti-faceted and dynamic nature of language, advancing the fields of\ncomputational social science and cultural analytics and beyond.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL-main-2025",
    "pdf_url": "http://arxiv.org/pdf/2502.05704v1",
    "published_date": "2025-02-08 21:55:38 UTC",
    "updated_date": "2025-02-08 21:55:38 UTC"
  },
  {
    "arxiv_id": "2502.05699v1",
    "title": "Context information can be more important than reasoning for time series forecasting with a large language model",
    "authors": [
      "Janghoon Yang"
    ],
    "abstract": "With the evolution of large language models (LLMs), there is growing interest\nin leveraging LLMs for time series tasks. In this paper, we explore the\ncharacteristics of LLMs for time series forecasting by considering various\nexisting and proposed prompting techniques. Forecasting for both short and long\ntime series was evaluated. Our findings indicate that no single prompting\nmethod is universally applicable. It was also observed that simply providing\nproper context information related to the time series, without additional\nreasoning prompts, can achieve performance comparable to the best-performing\nprompt for each case. From this observation, it is expected that providing\nproper context information can be more crucial than a prompt for specific\nreasoning in time series forecasting. Several weaknesses in prompting for time\nseries forecasting were also identified. First, LLMs often fail to follow the\nprocedures described by the prompt. Second, when reasoning steps involve simple\nalgebraic calculations with several operands, LLMs often fail to calculate\naccurately. Third, LLMs sometimes misunderstand the semantics of prompts,\nresulting in incomplete responses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05699v1",
    "published_date": "2025-02-08 21:39:07 UTC",
    "updated_date": "2025-02-08 21:39:07 UTC"
  },
  {
    "arxiv_id": "2502.06884v1",
    "title": "Learning Conformal Abstention Policies for Adaptive Risk Management in Large Language and Vision-Language Models",
    "authors": [
      "Sina Tayebati",
      "Divake Kumar",
      "Nastaran Darabi",
      "Dinithi Jayasuriya",
      "Ranganath Krishnan",
      "Amit Ranjan Trivedi"
    ],
    "abstract": "Large Language and Vision-Language Models (LLMs/VLMs) are increasingly used\nin safety-critical applications, yet their opaque decision-making complicates\nrisk assessment and reliability. Uncertainty quantification (UQ) helps assess\nprediction confidence and enables abstention when uncertainty is high.\nConformal prediction (CP), a leading UQ method, provides statistical guarantees\nbut relies on static thresholds, which fail to adapt to task complexity and\nevolving data distributions, leading to suboptimal trade-offs in accuracy,\ncoverage, and informativeness. To address this, we propose learnable conformal\nabstention, integrating reinforcement learning (RL) with CP to optimize\nabstention thresholds dynamically. By treating CP thresholds as adaptive\nactions, our approach balances multiple objectives, minimizing prediction set\nsize while maintaining reliable coverage. Extensive evaluations across diverse\nLLM/VLM benchmarks show our method outperforms Least Ambiguous Classifiers\n(LAC) and Adaptive Prediction Sets (APS), improving accuracy by up to 3.2%,\nboosting AUROC for hallucination detection by 22.19%, enhancing\nuncertainty-guided selective generation (AUARC) by 21.17%, and reducing\ncalibration error by 70%-85%. These improvements hold across multiple models\nand datasets while consistently meeting the 90% coverage target, establishing\nour approach as a more effective and flexible solution for reliable\ndecision-making in safety-critical applications. The code is available at:\n{https://github.com/sinatayebati/vlm-uncertainty}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06884v1",
    "published_date": "2025-02-08 21:30:41 UTC",
    "updated_date": "2025-02-08 21:30:41 UTC"
  },
  {
    "arxiv_id": "2502.05695v1",
    "title": "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks",
    "authors": [
      "Zijiang Yan",
      "Jianhua Pei",
      "Hongda Wu",
      "Hina Tabassum",
      "Ping Wang"
    ],
    "abstract": "This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.MM",
    "comment": "Submission for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2502.05695v1",
    "published_date": "2025-02-08 21:14:28 UTC",
    "updated_date": "2025-02-08 21:14:28 UTC"
  },
  {
    "arxiv_id": "2502.05694v1",
    "title": "Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA and ChatGPT",
    "authors": [
      "Shaoshuai Du",
      "Yiyi Tao",
      "Yixian Shen",
      "Hang Zhang",
      "Yanxin Shen",
      "Xinyu Qiu",
      "Chuanqi Shi"
    ],
    "abstract": "This study investigates the performance of various large language models\n(LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that\nintegrates entity recognition and relation extraction without requiring\nannotated data. While LLMs show promise for RE, most prior work focuses on\nEnglish or assumes pre-annotated entities, leaving their effectiveness in\nChinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini,\nand LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates\nthe highest overall performance, balancing precision and recall, while Gemini\nachieves the fastest inference speed, making it suitable for real-time\napplications. LLaMA underperforms in both accuracy and latency, highlighting\nthe need for further adaptation. Our findings provide insights into the\nstrengths and limitations of LLMs for zero-shot Chinese RE, shedding light on\ntrade-offs between accuracy and efficiency. This study serves as a foundation\nfor future research aimed at improving LLM adaptability to complex linguistic\ntasks in Chinese NLP.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05694v1",
    "published_date": "2025-02-08 21:12:04 UTC",
    "updated_date": "2025-02-08 21:12:04 UTC"
  },
  {
    "arxiv_id": "2502.05690v1",
    "title": "Managing Geological Uncertainty in Critical Mineral Supply Chains: A POMDP Approach with Application to U.S. Lithium Resources",
    "authors": [
      "Mansur Arief",
      "Yasmine Alonso",
      "CJ Oshiro",
      "William Xu",
      "Anthony Corso",
      "David Zhen Yin",
      "Jef K. Caers",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "The world is entering an unprecedented period of critical mineral demand,\ndriven by the global transition to renewable energy technologies and electric\nvehicles. This transition presents unique challenges in mineral resource\ndevelopment, particularly due to geological uncertainty-a key characteristic\nthat traditional supply chain optimization approaches do not adequately\naddress. To tackle this challenge, we propose a novel application of Partially\nObservable Markov Decision Processes (POMDPs) that optimizes critical mineral\nsourcing decisions while explicitly accounting for the dynamic nature of\ngeological uncertainty. Through a case study of the U.S. lithium supply chain,\nwe demonstrate that POMDP-based policies achieve superior outcomes compared to\ntraditional approaches, especially when initial reserve estimates are\nimperfect. Our framework provides quantitative insights for balancing domestic\nresource development with international supply diversification, offering\npolicymakers a systematic approach to strategic decision-making in critical\nmineral supply chains.",
    "categories": [
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05690v1",
    "published_date": "2025-02-08 20:44:44 UTC",
    "updated_date": "2025-02-08 20:44:44 UTC"
  },
  {
    "arxiv_id": "2502.05685v1",
    "title": "Mobile Application Threats and Security",
    "authors": [
      "Timur Mirzoev",
      "Mark Miller",
      "Shamimara Lasker",
      "Michael Brannon"
    ],
    "abstract": "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05685v1",
    "published_date": "2025-02-08 20:33:57 UTC",
    "updated_date": "2025-02-08 20:33:57 UTC"
  },
  {
    "arxiv_id": "2502.05684v2",
    "title": "Machine Unlearning via Information Theoretic Regularization",
    "authors": [
      "Shizhou Xu",
      "Thomas Strohmer"
    ],
    "abstract": "How can we effectively remove or \"unlearn\" undesirable information, such as\nspecific features or individual data points, from a learning outcome while\nminimizing utility loss and ensuring rigorous guarantees? We introduce a\nmathematical framework based on information-theoretic regularization to address\nboth feature and data point unlearning. For feature unlearning, we derive a\nunified solution that simultaneously optimizes diverse learning objectives,\nincluding entropy, conditional entropy, KL-divergence, and the energy of\nconditional probability. For data point unlearning, we first propose a novel\ndefinition that serves as a practical condition for unlearning via retraining,\nis easy to verify, and aligns with the principles of differential privacy from\nan inference perspective. Then, we provide provable guarantees for our\nframework on data point unlearning. By combining flexibility in learning\nobjectives with simplicity in regularization design, our approach is highly\nadaptable and practical for a wide range of machine learning and AI\napplications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.05684v2",
    "published_date": "2025-02-08 20:33:06 UTC",
    "updated_date": "2025-02-11 19:45:20 UTC"
  },
  {
    "arxiv_id": "2502.17454v1",
    "title": "Smart Sampling Strategies for Wireless Industrial Data Acquisition",
    "authors": [
      "Marcos Soto"
    ],
    "abstract": "In industrial environments, data acquisition accuracy is crucial for process\ncontrol and optimization. Wireless telemetry has proven to be a valuable tool\nfor improving efficiency in well-testing operations, enabling bidirectional\ncommunication and real-time control of downhole tools. However, high sampling\nfrequencies present challenges in telemetry, including data storage,\ntransmission, computational resource consumption, and battery life of wireless\ndevices. This study explores how optimizing data acquisition strategies can\nreduce aliasing effects and systematic errors while improving sampling rates\nwithout compromising measurement accuracy. A reduction of 80% in sampling\nfrequency was achieved without degrading measurement quality, demonstrating the\npotential for resource optimization in industrial environments.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "17 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.17454v1",
    "published_date": "2025-02-08 20:22:29 UTC",
    "updated_date": "2025-02-08 20:22:29 UTC"
  },
  {
    "arxiv_id": "2502.05672v1",
    "title": "On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers",
    "authors": [
      "Miroslav Å trupl",
      "Oleg Szehr",
      "Francesco Faccio",
      "Dylan R. Ashley",
      "Rupesh Kumar Srivastava",
      "JÃ¼rgen Schmidhuber"
    ],
    "abstract": "This article provides a rigorous analysis of convergence and stability of\nEpisodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised\nLearning and Online Decision Transformers. These algorithms performed\ncompetitively across various benchmarks, from games to robotic tasks, but their\ntheoretical understanding is limited to specific environmental conditions. This\nwork initiates a theoretical foundation for algorithms that build on the broad\nparadigm of approaching reinforcement learning through supervised learning or\nsequence modeling. At the core of this investigation lies the analysis of\nconditions on the underlying environment, under which the algorithms can\nidentify optimal solutions. We also assess whether emerging solutions remain\nstable in situations where the environment is subject to tiny levels of noise.\nSpecifically, we study the continuity and asymptotic convergence of\ncommand-conditioned policies, values and the goal-reaching objective depending\non the transition kernel of the underlying Markov Decision Process. We\ndemonstrate that near-optimal behavior is achieved if the transition kernel is\nlocated in a sufficiently small neighborhood of a deterministic kernel. The\nmentioned quantities are continuous (with respect to a specific topology) at\ndeterministic kernels, both asymptotically and after a finite number of\nlearning cycles. The developed methods allow us to present the first explicit\nestimates on the convergence and stability of policies and values in terms of\nthe underlying transition kernels. On the theoretical side we introduce a\nnumber of new concepts to reinforcement learning, like working in segment\nspaces, studying continuity in quotient topologies and the application of the\nfixed-point theory of dynamical systems. The theoretical study is accompanied\nby a detailed investigation of example environments and numerical experiments.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "cs.SY",
      "eess.SY",
      "68T07",
      "I.2.6; I.5.1"
    ],
    "primary_category": "stat.ML",
    "comment": "85 pages in main text + 4 pages of references + 26 pages of\n  appendices, 12 figures in main text + 2 figures in appendices; source code\n  available at https://github.com/struplm/eUDRL-GCSL-ODT-Convergence-public",
    "pdf_url": "http://arxiv.org/pdf/2502.05672v1",
    "published_date": "2025-02-08 19:26:22 UTC",
    "updated_date": "2025-02-08 19:26:22 UTC"
  },
  {
    "arxiv_id": "2502.05670v3",
    "title": "Language Models Largely Exhibit Human-like Constituent Ordering Preferences",
    "authors": [
      "Ada Defne Tur",
      "Gaurav Kamath",
      "Siva Reddy"
    ],
    "abstract": "Though English sentences are typically inflexible vis-\\`a-vis word order,\nconstituents often show far more variability in ordering. One prominent theory\npresents the notion that constituent ordering is directly correlated with\nconstituent weight: a measure of the constituent's length or complexity. Such\ntheories are interesting in the context of natural language processing (NLP),\nbecause while recent advances in NLP have led to significant gains in the\nperformance of large language models (LLMs), much remains unclear about how\nthese models process language, and how this compares to human language\nprocessing. In particular, the question remains whether LLMs display the same\npatterns with constituent movement, and may provide insights into existing\ntheories on when and how the shift occurs in human language. We compare a\nvariety of LLMs with diverse properties to evaluate broad LLM performance on\nfour types of constituent movement: heavy NP shift, particle movement, dative\nalternation, and multiple PPs. Despite performing unexpectedly around particle\nmovement, LLMs generally align with human preferences around constituent\nordering.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2502.05670v3",
    "published_date": "2025-02-08 19:13:40 UTC",
    "updated_date": "2025-02-14 21:06:20 UTC"
  },
  {
    "arxiv_id": "2502.05664v1",
    "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
    "authors": [
      "Md. Ashraful Islam",
      "Mohammed Eunus Ali",
      "Md Rizwan Parvez"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in code generation\nand problem solving. Current approaches employ external tool-based iterative\ndebuggers that use compiler or other tool-based runtime feedback to refine\ncoarse programs generated by various methods. However, the effectiveness of\nthese approaches heavily relies on the quality of the initial code generation,\nwhich remains an open challenge. In this paper, we introduce CodeSim, a novel\nmulti-agent code generation framework that comprehensively addresses the stages\nof program synthesis-planning, coding, and debugging-through a human-like\nperception approach. As human verifies their understanding of any algorithms\nthrough visual simulation, CodeSim uniquely features a method of plan\nverification and internal debugging through the step-by-step simulation of\ninput/output. Extensive experiments across seven challenging competitive\nproblem-solving and program synthesis benchmarks demonstrate CodeSim's\nremarkable code generation capabilities. Our framework achieves new\nstate-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and\nCodeContests 29.1%). Furthermore, our method shows potential for even greater\nenhancement when cascaded with external debuggers. To facilitate further\nresearch and development in this area, we have open-sourced our framework in\nthis link (https://kagnlp.github.io/codesim.github.io/).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2502.05664v1",
    "published_date": "2025-02-08 18:43:59 UTC",
    "updated_date": "2025-02-08 18:43:59 UTC"
  },
  {
    "arxiv_id": "2502.05651v1",
    "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy",
    "authors": [
      "Hyunjong Kim",
      "Suyeon Lee",
      "Yeongjae Cho",
      "Eunseo Ryu",
      "Yohan Jo",
      "Suran Seong",
      "Sungzoon Cho"
    ],
    "abstract": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NAACL 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2502.05651v1",
    "published_date": "2025-02-08 17:53:41 UTC",
    "updated_date": "2025-02-08 17:53:41 UTC"
  },
  {
    "arxiv_id": "2502.07813v2",
    "title": "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
    "authors": [
      "Jiajun Shi",
      "Chaoren Wei",
      "Liqun Yang",
      "Zekun Moore Wang",
      "Chenghao Yang",
      "Ge Zhang",
      "Stephen Huang",
      "Tao Peng",
      "Jian Yang",
      "Zhoufutu Wen"
    ],
    "abstract": "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.07813v2",
    "published_date": "2025-02-08 17:19:43 UTC",
    "updated_date": "2025-03-12 13:17:27 UTC"
  },
  {
    "arxiv_id": "2502.05641v1",
    "title": "Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs",
    "authors": [
      "Aayam Shrestha",
      "Pan Liu",
      "German Ros",
      "Kai Yuan",
      "Alan Fern"
    ],
    "abstract": "This work focuses on generating realistic, physically-based human behaviors\nfrom multi-modal inputs, which may only partially specify the desired motion.\nFor example, the input may come from a VR controller providing arm motion and\nbody velocity, partial key-point animation, computer vision applied to videos,\nor even higher-level motion goals. This requires a versatile low-level humanoid\ncontroller that can handle such sparse, under-specified guidance, seamlessly\nswitch between skills, and recover from failures. Current approaches for\nlearning humanoid controllers from demonstration data capture some of these\ncharacteristics, but none achieve them all. To this end, we introduce the\nMasked Humanoid Controller (MHC), a novel approach that applies multi-objective\nimitation learning on augmented and selectively masked motion demonstrations.\nThe training methodology results in an MHC that exhibits the key capabilities\nof catch-up to out-of-sync input commands, combining elements from multiple\nmotion sequences, and completing unspecified parts of motions from sparse\nmultimodal input. We demonstrate these key capabilities for an MHC learned over\na dataset of 87 diverse skills and showcase different multi-modal use cases,\nincluding integration with planning frameworks to highlight MHC's ability to\nsolve new user-defined tasks without any finetuning.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05641v1",
    "published_date": "2025-02-08 17:02:11 UTC",
    "updated_date": "2025-02-08 17:02:11 UTC"
  },
  {
    "arxiv_id": "2502.05638v1",
    "title": "ELMTEX: Fine-Tuning Large Language Models for Structured Clinical Information Extraction. A Case Study on Clinical Reports",
    "authors": [
      "Aynur Guluzade",
      "Naguib Heiba",
      "Zeyd Boukhers",
      "Florim Hamiti",
      "Jahid Hasan Polash",
      "Yehya Mohamad",
      "Carlos A Velasco"
    ],
    "abstract": "Europe's healthcare systems require enhanced interoperability and\ndigitalization, driving a demand for innovative solutions to process legacy\nclinical data. This paper presents the results of our project, which aims to\nleverage Large Language Models (LLMs) to extract structured information from\nunstructured clinical reports, focusing on patient history, diagnoses,\ntreatments, and other predefined categories. We developed a workflow with a\nuser interface and evaluated LLMs of varying sizes through prompting strategies\nand fine-tuning. Our results show that fine-tuned smaller models match or\nsurpass larger counterparts in performance, offering efficiency for\nresource-limited settings. A new dataset of 60,000 annotated English clinical\nsummaries and 24,000 German translations was validated with automated and\nmanual checks. The evaluations used ROUGE, BERTScore, and entity-level metrics.\nThe work highlights the approach's viability and outlines future improvements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.6; I.2.7; J.3"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05638v1",
    "published_date": "2025-02-08 16:44:56 UTC",
    "updated_date": "2025-02-08 16:44:56 UTC"
  },
  {
    "arxiv_id": "2502.05637v1",
    "title": "Adversarial Machine Learning: Attacks, Defenses, and Open Challenges",
    "authors": [
      "Pranav K Jha"
    ],
    "abstract": "Adversarial Machine Learning (AML) addresses vulnerabilities in AI systems\nwhere adversaries manipulate inputs or training data to degrade performance.\nThis article provides a comprehensive analysis of evasion and poisoning\nattacks, formalizes defense mechanisms with mathematical rigor, and discusses\nthe challenges of implementing robust solutions in adaptive threat models.\nAdditionally, it highlights open challenges in certified robustness,\nscalability, and real-world deployment.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05637v1",
    "published_date": "2025-02-08 16:43:17 UTC",
    "updated_date": "2025-02-08 16:43:17 UTC"
  },
  {
    "arxiv_id": "2502.05632v1",
    "title": "Amorphous Fortress Online: Collaboratively Designing Open-Ended Multi-Agent AI and Game Environments",
    "authors": [
      "M Charity",
      "Mayu Wilson",
      "Steven Lee",
      "Dipika Rajesh",
      "Sam Earle",
      "Julian Togelius"
    ],
    "abstract": "This work introduces Amorphous Fortress Online -- a web-based platform where\nusers can design petri-dish-like environments and games consisting of\nmulti-agent AI characters. Users can play, create, and share artificial life\nand game environments made up of microscopic but transparent finite-state\nmachine agents that interact with each other. The website features multiple\ninteractive editors and accessible settings to view the multi-agent\ninteractions directly from the browser. This system serves to provide a\ndatabase of thematically diverse AI and game environments that use the emergent\nbehaviors of simple AI agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05632v1",
    "published_date": "2025-02-08 16:25:52 UTC",
    "updated_date": "2025-02-08 16:25:52 UTC"
  },
  {
    "arxiv_id": "2502.05615v1",
    "title": "XiHeFusion: Harnessing Large Language Models for Science Communication in Nuclear Fusion",
    "authors": [
      "Xiao Wang",
      "Qingquan Yang",
      "Fuling Wang",
      "Qiang Chen",
      "Wentao Wu",
      "Yu Jin",
      "Jingtao Jiang",
      "Liye Jin",
      "Bo Jiang",
      "Dengdi Sun",
      "Wanli Lv",
      "Meiwen Chen",
      "Zehua Chen",
      "Guosheng Xu",
      "Jin Tang"
    ],
    "abstract": "Nuclear fusion is one of the most promising ways for humans to obtain\ninfinite energy. Currently, with the rapid development of artificial\nintelligence, the mission of nuclear fusion has also entered a critical period\nof its development. How to let more people to understand nuclear fusion and\njoin in its research is one of the effective means to accelerate the\nimplementation of fusion. This paper proposes the first large model in the\nfield of nuclear fusion, XiHeFusion, which is obtained through supervised\nfine-tuning based on the open-source large model Qwen2.5-14B. We have collected\nmulti-source knowledge about nuclear fusion tasks to support the training of\nthis model, including the common crawl, eBooks, arXiv, dissertation, etc. After\nthe model has mastered the knowledge of the nuclear fusion field, we further\nused the chain of thought to enhance its logical reasoning ability, making\nXiHeFusion able to provide more accurate and logical answers. In addition, we\npropose a test questionnaire containing 180+ questions to assess the\nconversational ability of this science popularization large model. Extensive\nexperimental results show that our nuclear fusion dialogue model, XiHeFusion,\ncan perform well in answering science popularization knowledge. The pre-trained\nXiHeFusion model is released on https://github.com/Event-AHU/XiHeFusion.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05615v1",
    "published_date": "2025-02-08 15:47:48 UTC",
    "updated_date": "2025-02-08 15:47:48 UTC"
  },
  {
    "arxiv_id": "2502.05608v1",
    "title": "Closing the Responsibility Gap in AI-based Network Management: An Intelligent Audit System Approach",
    "authors": [
      "Emanuel Figetakis",
      "Ahmed Refaey Hussein"
    ],
    "abstract": "Existing network paradigms have achieved lower downtime as well as a higher\nQuality of Experience (QoE) through the use of Artificial Intelligence\n(AI)-based network management tools. These AI management systems, allow for\nautomatic responses to changes in network conditions, lowering operation costs\nfor operators, and improving overall performance. While adopting AI-based\nmanagement tools enhance the overall network performance, it also introduce\nchallenges such as removing human supervision, privacy violations, algorithmic\nbias, and model inaccuracies. Furthermore, AI-based agents that fail to address\nthese challenges should be culpable themselves rather than the network as a\nwhole. To address this accountability gap, a framework consisting of a Deep\nReinforcement Learning (DRL) model and a Machine Learning (ML) model is\nproposed to identify and assign numerical values of responsibility to the\nAI-based management agents involved in any decision-making regarding the\nnetwork conditions, which eventually affects the end-user. A simulation\nenvironment was created for the framework to be trained using simulated network\noperation parameters. The DRL model had a 96% accuracy during testing for\nidentifying the AI-based management agents, while the ML model using gradient\ndescent learned the network conditions at an 83% accuracy during testing.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05608v1",
    "published_date": "2025-02-08 15:30:25 UTC",
    "updated_date": "2025-02-08 15:30:25 UTC"
  },
  {
    "arxiv_id": "2502.06882v1",
    "title": "Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction",
    "authors": [
      "Shengbin Yue",
      "Ting Huang",
      "Zheng Jia",
      "Siyuan Wang",
      "Shujun Liu",
      "Yun Song",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced legal intelligence,\nbut the scarcity of scenario data impedes the progress toward interactive legal\nscenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER)\nto scalably generate synthetic data by simulating interactive legal scenarios.\nLeveraging real-legal case sources, MASER ensures the consistency of legal\nattributes between participants and introduces a supervisory mechanism to align\nparticipants' characters and behaviors as well as addressing distractions. A\nMulti-stage Interactive Legal Evaluation (MILE) benchmark is further\nconstructed to evaluate LLMs' performance in dynamic legal scenarios. Extensive\nexperiments confirm the effectiveness of our framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06882v1",
    "published_date": "2025-02-08 15:05:24 UTC",
    "updated_date": "2025-02-08 15:05:24 UTC"
  },
  {
    "arxiv_id": "2502.05589v3",
    "title": "On Memory Construction and Retrieval for Personalized Conversational Agents",
    "authors": [
      "Zhuoshi Pan",
      "Qianhui Wu",
      "Huiqiang Jiang",
      "Xufang Luo",
      "Hao Cheng",
      "Dongsheng Li",
      "Yuqing Yang",
      "Chin-Yew Lin",
      "H. Vicky Zhao",
      "Lili Qiu",
      "Jianfeng Gao"
    ],
    "abstract": "To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques.In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nturn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as LLMLingua-2, can\neffectively serve as a denoising mechanism, enhancing memory retrieval accuracy\nacross different granularities. Building on these insights, we propose SeCom, a\nmethod that constructs the memory bank at segment level by introducing a\nconversation segmentation model that partitions long-term conversations into\ntopically coherent segments, while applying compression based denoising on\nmemory units to enhance memory retrieval. Experimental results show that SeCom\nexhibits a significant performance advantage over baselines on long-term\nconversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed\nconversation segmentation method demonstrates superior performance on dialogue\nsegmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 5 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2502.05589v3",
    "published_date": "2025-02-08 14:28:36 UTC",
    "updated_date": "2025-03-03 16:49:18 UTC"
  },
  {
    "arxiv_id": "2502.05574v1",
    "title": "Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark",
    "authors": [
      "Shiao Wang",
      "Xiao Wang",
      "Chao Wang",
      "Liye Jin",
      "Lin Zhu",
      "Bo Jiang",
      "Yonghong Tian",
      "Jin Tang"
    ],
    "abstract": "We then introduce a novel hierarchical knowledge distillation strategy that\nincorporates the similarity matrix, feature representation, and response\nmap-based distillation to guide the learning of the student Transformer\nnetwork. We also enhance the model's ability to capture temporal dependencies\nby applying the temporal Fourier transform to establish temporal relationships\nbetween video frames. We adapt the network model to specific target objects\nduring testing via a newly proposed test-time tuning strategy to achieve high\nperformance and flexibility in target tracking. Recognizing the limitations of\nexisting event-based tracking datasets, which are predominantly low-resolution,\nwe propose EventVOT, the first large-scale high-resolution event-based tracking\ndataset. It comprises 1141 videos spanning diverse categories such as\npedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both\nlow-resolution (FE240hz, VisEvent, FELT), and our newly proposed\nhigh-resolution EventVOT dataset fully validated the effectiveness of our\nproposed method. Both the benchmark dataset and source code have been released\non https://github.com/Event-AHU/EventVOT_Benchmark",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Journal Extension of EventVOT, CVPR24",
    "pdf_url": "http://arxiv.org/pdf/2502.05574v1",
    "published_date": "2025-02-08 13:59:52 UTC",
    "updated_date": "2025-02-08 13:59:52 UTC"
  },
  {
    "arxiv_id": "2502.05573v1",
    "title": "Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning",
    "authors": [
      "Beining Zhang",
      "Aditya Kapoor",
      "Mingfei Sun"
    ],
    "abstract": "Multi-agent reinforcement learning (MARL) often relies on \\emph{parameter\nsharing (PS)} to scale efficiently. However, purely shared policies can stifle\neach agent's unique specialization, reducing overall performance in\nheterogeneous environments. We propose \\textbf{Low-Rank Agent-Specific\nAdaptation (LoRASA)}, a novel approach that treats each agent's policy as a\nspecialized ``task'' fine-tuned from a shared backbone. Drawing inspiration\nfrom parameter-efficient transfer methods, LoRASA appends small, low-rank\nadaptation matrices to each layer of the shared policy, naturally inducing\n\\emph{parameter-space sparsity} that promotes both specialization and\nscalability. We evaluate LoRASA on challenging benchmarks including the\nStarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo),\nimplementing it atop widely used algorithms such as MAPPO and A2PO. Across\ndiverse tasks, LoRASA matches or outperforms existing baselines \\emph{while\nreducing memory and computational overhead}. Ablation studies on adapter rank,\nplacement, and timing validate the method's flexibility and efficiency. Our\nresults suggest LoRASA's potential to establish a new norm for MARL policy\nparameterization: combining a shared foundation for coordination with low-rank\nagent-specific refinements for individual specialization.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "31 pages, 20 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.05573v1",
    "published_date": "2025-02-08 13:57:53 UTC",
    "updated_date": "2025-02-08 13:57:53 UTC"
  },
  {
    "arxiv_id": "2502.05568v1",
    "title": "Large Multimodal Models for Low-Resource Languages: A Survey",
    "authors": [
      "Marian Lupascu",
      "Ana-Cristina Rogoz",
      "Mihai Sorin Stupariu",
      "Radu Tudor Ionescu"
    ],
    "abstract": "In this survey, we systematically analyze techniques used to adapt large\nmultimodal models (LMMs) for low-resource (LR) languages, examining approaches\nranging from visual enhancement and data creation to cross-modal transfer and\nfusion strategies. Through a comprehensive analysis of 106 studies across 75 LR\nlanguages, we identify key patterns in how researchers tackle the challenges of\nlimited data and computational resources. We find that visual information often\nserves as a crucial bridge for improving model performance in LR settings,\nthough significant challenges remain in areas such as hallucination mitigation\nand computational efficiency. We aim to provide researchers with a clear\nunderstanding of current approaches and remaining challenges in making LMMs\nmore accessible to speakers of LR (understudied) languages. We complement our\nsurvey with an open-source repository available at:\nhttps://github.com/marianlupascu/LMM4LRL-Survey.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05568v1",
    "published_date": "2025-02-08 13:29:44 UTC",
    "updated_date": "2025-02-08 13:29:44 UTC"
  },
  {
    "arxiv_id": "2502.05567v2",
    "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data",
    "authors": [
      "Xiaoyang Liu",
      "Kangjie Bao",
      "Jiashuo Zhang",
      "Yunqi Liu",
      "Yuntian Liu",
      "Yu Chen",
      "Yang Jiao",
      "Tao Luo"
    ],
    "abstract": "Autoformalization, the automatic translation of mathematical content from\nnatural language into machine-verifiable formal languages, has seen significant\nprogress driven by advances in large language models (LLMs). Nonetheless, a\nprimary barrier to further improvements is the limited availability of parallel\ncorpora that map informal mathematical text to its formal counterpart. To\naddress this limitation, we propose ATLAS (Autoformalizing Theorems through\nLifting, Augmentation, and Synthesis of Data), a novel data generation\nframework designed to produce large-scale, high-quality parallel corpora of\ntheorem statements. Distinct from prior approaches, ATLAS begins with a concept\nrepository, accelerates the improvement of student model through expert\niteration combined with knowledge distillation, and introduces two novel\naugmentation strategies that exploit the structural characteristics of formal\nlanguages. With the proposed ATLAS running for 10 iterations, we construct an\nundergraduate-level dataset comprising 117k theorem statements and develop\nATLAS Translator, which demonstrates statistically significant improvements\nover both the HERALD Translator and the Kimina-Autoformalizer across all\nbenchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The\ndatasets, model, and code will be released to the public soon.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05567v2",
    "published_date": "2025-02-08 13:28:51 UTC",
    "updated_date": "2025-05-19 04:17:39 UTC"
  },
  {
    "arxiv_id": "2502.05564v1",
    "title": "TabICL: A Tabular Foundation Model for In-Context Learning on Large Data",
    "authors": [
      "Jingang Qu",
      "David HolzmÃ¼ller",
      "GaÃ«l Varoquaux",
      "Marine Le Morvan"
    ],
    "abstract": "The long-standing dominance of gradient-boosted decision trees on tabular\ndata is currently challenged by tabular foundation models using In-Context\nLearning (ICL): setting the training data as context for the test data and\npredicting in a single forward pass without parameter updates. While the very\nrecent TabPFNv2 foundation model (2025) excels on tables with up to 10K\nsamples, its alternating column- and row-wise attentions make handling large\ntraining sets computationally prohibitive. So, can ICL be effectively scaled\nand deliver a benefit for larger tables? We introduce TabICL, a tabular\nfoundation model for classification, pretrained on synthetic datasets with up\nto 60K samples and capable of handling 500K samples on affordable resources.\nThis is enabled by a novel two-stage architecture: a column-then-row attention\nmechanism to build fixed-dimensional embeddings of rows, followed by a\ntransformer for efficient ICL. Across 200 classification datasets from the\nTALENT benchmark, TabICL is on par with TabPFNv2 while being systematically\nfaster (up to 10 times), and significantly outperforms all other approaches. On\n56 datasets with over 10K samples, TabICL surpasses both TabPFNv2 and CatBoost,\ndemonstrating the potential of ICL for large data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05564v1",
    "published_date": "2025-02-08 13:25:04 UTC",
    "updated_date": "2025-02-08 13:25:04 UTC"
  },
  {
    "arxiv_id": "2502.05556v1",
    "title": "Knowledge is Power: Harnessing Large Language Models for Enhanced Cognitive Diagnosis",
    "authors": [
      "Zhiang Dong",
      "Jingyuan Chen",
      "Fei Wu"
    ],
    "abstract": "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive\nstates by analyzing their performance across a series of exercises. However,\nexisting CDMs often struggle with diagnosing infrequent students and exercises\ndue to a lack of rich prior knowledge. With the advancement in large language\nmodels (LLMs), which possess extensive domain knowledge, their integration into\ncognitive diagnosis presents a promising opportunity. Despite this potential,\nintegrating LLMs with CDMs poses significant challenges. LLMs are not\nwell-suited for capturing the fine-grained collaborative interactions between\nstudents and exercises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integration. To address\nthese issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD)\nframework, which is a model-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD framework operates in\ntwo stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis\nstage, both students and exercises are diagnosed to achieve comprehensive and\ndetailed modeling. In the Cognitive Level Alignment stage, we bridge the gap\nbetween the CDMs' behavioral space and the LLMs' semantic space using\ncontrastive learning and mask-reconstruction approaches. Experiments on several\nreal-world datasets demonstrate the effectiveness of our proposed framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05556v1",
    "published_date": "2025-02-08 13:02:45 UTC",
    "updated_date": "2025-02-08 13:02:45 UTC"
  },
  {
    "arxiv_id": "2502.10431v1",
    "title": "Leveraging Constraint Violation Signals For Action-Constrained Reinforcement Learning",
    "authors": [
      "Janaka Chathuranga Brahmanage",
      "Jiajing Ling",
      "Akshat Kumar"
    ],
    "abstract": "In many RL applications, ensuring an agent's actions adhere to constraints is\ncrucial for safety. Most previous methods in Action-Constrained Reinforcement\nLearning (ACRL) employ a projection layer after the policy network to correct\nthe action. However projection-based methods suffer from issues like the zero\ngradient problem and higher runtime due to the usage of optimization solvers.\nRecently methods were proposed to train generative models to learn a\ndifferentiable mapping between latent variables and feasible actions to address\nthis issue. However, generative models require training using samples from the\nconstrained action space, which itself is challenging. To address such\nlimitations, first, we define a target distribution for feasible actions based\non constraint violation signals, and train normalizing flows by minimizing the\nKL divergence between an approximated distribution over feasible actions and\nthe target. This eliminates the need to generate feasible action samples,\ngreatly simplifying the flow model learning. Second, we integrate the learned\nflow model with existing deep RL methods, which restrict it to exploring only\nthe feasible action space. Third, we extend our approach beyond ACRL to handle\nstate-wise constraints by learning the constraint violation signal from the\nenvironment. Empirically, our approach has significantly fewer constraint\nviolations while achieving similar or better quality in several control tasks\nthan previous best methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages and 5 pages supplementary",
    "pdf_url": "http://arxiv.org/pdf/2502.10431v1",
    "published_date": "2025-02-08 12:58:26 UTC",
    "updated_date": "2025-02-08 12:58:26 UTC"
  },
  {
    "arxiv_id": "2502.05547v1",
    "title": "Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning",
    "authors": [
      "Runhua Xu",
      "Shiqi Gao",
      "Chao Li",
      "James Joshi",
      "Jianxin Li"
    ],
    "abstract": "Federated learning (FL) is inherently susceptible to privacy breaches and\npoisoning attacks. To tackle these challenges, researchers have separately\ndevised secure aggregation mechanisms to protect data privacy and robust\naggregation methods that withstand poisoning attacks. However, simultaneously\naddressing both concerns is challenging; secure aggregation facilitates\npoisoning attacks as most anomaly detection techniques require access to\nunencrypted local model updates, which are obscured by secure aggregation. Few\nrecent efforts to simultaneously tackle both challenges offen depend on\nimpractical assumption of non-colluding two-server setups that disrupt FL's\ntopology, or three-party computation which introduces scalability issues,\ncomplicating deployment and application. To overcome this dilemma, this paper\nintroduce a Dual Defense Federated learning (DDFed) framework. DDFed\nsimultaneously boosts privacy protection and mitigates poisoning attacks,\nwithout introducing new participant roles or disrupting the existing FL\ntopology. DDFed initially leverages cutting-edge fully homomorphic encryption\n(FHE) to securely aggregate model updates, without the impractical requirement\nfor non-colluding two-server setups and ensures strong privacy protection.\nAdditionally, we proposes a unique two-phase anomaly detection mechanism for\nencrypted model updates, featuring secure similarity computation and\nfeedback-driven collaborative selection, with additional measures to prevent\npotential privacy breaches from Byzantine clients incorporated into the\ndetection process. We conducted extensive experiments on various model\npoisoning attacks and FL scenarios, including both cross-device and cross-silo\nFL. Experiments on publicly available datasets demonstrate that DDFed\nsuccessfully protects model privacy and effectively defends against model\npoisoning threats.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "accepted by The Thirty-eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2502.05547v1",
    "published_date": "2025-02-08 12:28:20 UTC",
    "updated_date": "2025-02-08 12:28:20 UTC"
  },
  {
    "arxiv_id": "2502.05537v1",
    "title": "Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning",
    "authors": [
      "Xinsong Feng",
      "Zihan Yu",
      "Yanhai Xiong",
      "Haipeng Chen"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a promising tool for combinatorial\noptimization (CO) problems due to its ability to learn fast, effective, and\ngeneralizable solutions. Nonetheless, existing works mostly focus on one-shot\ndeterministic CO, while sequential stochastic CO (SSCO) has rarely been studied\ndespite its broad applications such as adaptive influence maximization (IM) and\ninfectious disease intervention. In this paper, we study the SSCO problem where\nwe first decide the budget (e.g., number of seed nodes in adaptive IM)\nallocation for all time steps, and then select a set of nodes for each time\nstep. The few existing studies on SSCO simplify the problems by assuming a\nuniformly distributed budget allocation over the time horizon, yielding\nsuboptimal solutions. We propose a generic hierarchical RL (HRL) framework\ncalled wake-sleep option (WS-option), a two-layer option-based framework that\nsimultaneously decides adaptive budget allocation on the higher layer and node\nselection on the lower layer. WS-option starts with a coherent formulation of\nthe two-layer Markov decision processes (MDPs), capturing the interdependencies\nbetween the two layers of decisions. Building on this, WS-option employs\nseveral innovative designs to balance the model's training stability and\ncomputational efficiency, preventing the vicious cyclic interference issue\nbetween the two layers. Empirical results show that WS-option exhibits\nsignificantly improved effectiveness and generalizability compared to\ntraditional methods. Moreover, the learned model can be generalized to larger\ngraphs, which significantly reduces the overhead of computational resources.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05537v1",
    "published_date": "2025-02-08 12:00:30 UTC",
    "updated_date": "2025-02-08 12:00:30 UTC"
  },
  {
    "arxiv_id": "2502.06876v3",
    "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
    "authors": [
      "Jinluan Yang",
      "Dingnan Jin",
      "Anke Tang",
      "Li Shen",
      "Didi Zhu",
      "Zhengyu Chen",
      "Ziyu Zhao",
      "Daixin Wang",
      "Qing Cui",
      "Zhiqiang Zhang",
      "Jun Zhou",
      "Fei Wu",
      "Kun Kuang"
    ],
    "abstract": "Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI. Existing methods like data mixture strategies\nface limitations, including heavy reliance on expert knowledge and conflicting\noptimization signals. While model merging offers parameter-level\nconflict-resolution strategies through integrating specialized models'\nparameters, its potential for 3H optimization remains underexplored. This paper\nsystematically compares the effectiveness of model merging and data mixture\nmethods in constructing 3H-aligned LLMs for the first time, revealing\npreviously overlooked collaborative and conflict relationships among the 3H\ndimensions and discussing the advantages and drawbacks of data mixture\n(\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in\nmitigating the conflict for balanced 3H optimization. Specially, we propose a\nnovel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular\n\\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and\nsparsity-aware rank selection strategies to address the challenges of\npreference noise accumulation and layer sparsity adaptation inherent in\n3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and\nrobustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model\nmerging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release\nour models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for\nfurther investigations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06876v3",
    "published_date": "2025-02-08 11:56:58 UTC",
    "updated_date": "2025-05-16 05:35:39 UTC"
  },
  {
    "arxiv_id": "2502.05526v1",
    "title": "Towards Learning Scalable Agile Dynamic Motion Planning for Robosoccer Teams with Policy Optimization",
    "authors": [
      "Brandon Ho",
      "Batuhan Altundas",
      "Matthew Gombolay"
    ],
    "abstract": "In fast-paced, ever-changing environments, dynamic Motion Planning for\nMulti-Agent Systems in the presence of obstacles is a universal and unsolved\nproblem. Be it from path planning around obstacles to the movement of robotic\narms, or in planning navigation of robot teams in settings such as Robosoccer,\ndynamic motion planning is needed to avoid collisions while reaching the\ntargeted destination when multiple agents occupy the same area. In continuous\ndomains where the world changes quickly, existing classical Motion Planning\nalgorithms such as RRT* and A* become computationally expensive to rerun at\nevery time step. Many variations of classical and well-formulated non-learning\npath-planning methods have been proposed to solve this universal problem but\nfall short due to their limitations of speed, smoothness, optimally, etc. Deep\nLearning models overcome their challenges due to their ability to adapt to\nvarying environments based on past experience. However, current learning motion\nplanning models use discretized environments, do not account for heterogeneous\nagents or replanning, and build up to improve the classical motion planners'\nefficiency, leading to issues with scalability. To prevent collisions between\nheterogenous team members and collision to obstacles while trying to reach the\ntarget location, we present a learning-based dynamic navigation model and show\nour model working on a simple environment in the concept of a simple Robosoccer\nGame.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05526v1",
    "published_date": "2025-02-08 11:13:07 UTC",
    "updated_date": "2025-02-08 11:13:07 UTC"
  },
  {
    "arxiv_id": "2502.05512v1",
    "title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
    "authors": [
      "Wei Deng",
      "Siyi Zhou",
      "Jingchen Shu",
      "Jinchao Wang",
      "Lu Wang"
    ],
    "abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have\ngradually become the mainstream in the industry due to their high naturalness\nand powerful zero-shot voice cloning capabilities.Here, we introduce the\nIndexTTS system, which is mainly based on the XTTS and Tortoise model. We add\nsome novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid\nmodeling method that combines characters and pinyin, making the pronunciations\nof polyphonic characters and long-tail characters controllable. We also\nperformed a comparative analysis of the Vector Quantization (VQ) with\nFinite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech\ntokens. To further enhance the effect and stability of voice cloning, we\nintroduce a conformer-based speech conditional encoder and replace the\nspeechcode decoder with BigVGAN2. Compared with XTTS, it has achieved\nsignificant improvements in naturalness, content consistency, and zero-shot\nvoice cloning. As for the popular TTS systems in the open-source, such as\nFish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively\nsimple training process, more controllable usage, and faster inference speed.\nMoreover, its performance surpasses that of these systems. Our demos are\navailable at https://index-tts.github.io.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05512v1",
    "published_date": "2025-02-08 10:23:20 UTC",
    "updated_date": "2025-02-08 10:23:20 UTC"
  },
  {
    "arxiv_id": "2502.08657v1",
    "title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions",
    "authors": [
      "Jingxin Xu",
      "Guoshun Nan",
      "Sheng Guan",
      "Sicong Leng",
      "Yilian Liu",
      "Zixiao Wang",
      "Yuyang Ma",
      "Zhili Zhou",
      "Yanzhao Hou",
      "Xiaofeng Tao"
    ],
    "abstract": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction\ntuning and reinforcement learning to calibrate the output of large language\nmodels (LLMs) with human intentions, ensuring the outputs are harmless and\nhelpful. Existing methods heavily depend on the manual annotation of\nhigh-quality positive samples, while contending with issues such as noisy\nlabels and minimal distinctions between preferred and dispreferred response\ndata. However, readily available toxic samples with clear safety distinctions\nare often filtered out, removing valuable negative references that could aid\nLLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety\nself-alignment approach that minimizes human supervision by automatically\nrefining positive and toxic samples and performing fine-grained dual\ninstruction tuning. Positive samples are harmless responses, while toxic\nsamples deliberately contain extremely harmful content, serving as a new\nsupervisory signals. Specifically, we utilize LLM itself to iteratively\ngenerate and refine training instances by only exploring fewer than 50 human\nannotations. We then employ two losses, i.e., maximum likelihood estimation\n(MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance\nthe LLM's safety. The MLE loss encourages an LLM to maximize the generation of\nharmless content based on positive samples. Conversely, the fine-grained UT\nloss guides the LLM to minimize the output of harmful words based on negative\nsamples at the token-level, thereby guiding the model to decouple safety from\neffectiveness, directing it toward safer fine-tuning objectives, and increasing\nthe likelihood of generating helpful and reliable content. Experiments on 9\npopular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for\nsafety alignment, while maintaining comparable levels of helpfulness and\nusefulness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2502.08657v1",
    "published_date": "2025-02-08 09:54:47 UTC",
    "updated_date": "2025-02-08 09:54:47 UTC"
  },
  {
    "arxiv_id": "2502.06875v1",
    "title": "Beyond Vision: How Large Language Models Interpret Facial Expressions from Valence-Arousal Values",
    "authors": [
      "Vaibhav Mehra",
      "Guy Laban",
      "Hatice Gunes"
    ],
    "abstract": "Large Language Models primarily operate through text-based inputs and\noutputs, yet human emotion is communicated through both verbal and non-verbal\ncues, including facial expressions. While Vision-Language Models analyze facial\nexpressions from images, they are resource-intensive and may depend more on\nlinguistic priors than visual understanding. To address this, this study\ninvestigates whether LLMs can infer affective meaning from dimensions of facial\nexpressions-Valence and Arousal values, structured numerical representations,\nrather than using raw visual input. VA values were extracted using Facechannel\nfrom images of facial expressions and provided to LLMs in two tasks: (1)\ncategorizing facial expressions into basic (on the IIMI dataset) and complex\nemotions (on the Emotic dataset) and (2) generating semantic descriptions of\nfacial expressions (on the Emotic dataset). Results from the categorization\ntask indicate that LLMs struggle to classify VA values into discrete emotion\ncategories, particularly for emotions beyond basic polarities (e.g., happiness,\nsadness). However, in the semantic description task, LLMs produced textual\ndescriptions that align closely with human-generated interpretations,\ndemonstrating a stronger capacity for free text affective inference of facial\nexpressions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06875v1",
    "published_date": "2025-02-08 09:54:03 UTC",
    "updated_date": "2025-02-08 09:54:03 UTC"
  },
  {
    "arxiv_id": "2502.05503v3",
    "title": "A Physical Coherence Benchmark for Evaluating Video Generation Models via Optical Flow-guided Frame Prediction",
    "authors": [
      "Yongfan Chen",
      "Xiuwen Zhu",
      "Tianyu Li"
    ],
    "abstract": "Recent advances in video generation models demonstrate their potential as\nworld simulators, but they often struggle with videos deviating from physical\nlaws, a key concern overlooked by most text-to-video benchmarks. We introduce a\nbenchmark designed specifically to assess the Physical Coherence of generated\nvideos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of\nphysical principles, capturing key physical laws observable in video content.\nWe evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and\nconducted manual assessments. Additionally, we propose an automated evaluation\nmodel: PhyCoPredictor, a diffusion model that generates optical flow and video\nframes in a cascade manner. Through a consistency evaluation comparing\nautomated and manual sorting, the experimental results show that PhyCoPredictor\ncurrently aligns most closely with human evaluation. Therefore, it can\neffectively evaluate the physical coherence of videos, providing insights for\nfuture model optimization. Our benchmark, including physical coherence prompts,\nthe automatic evaluation tool PhyCoPredictor, and the generated video dataset,\nhas been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05503v3",
    "published_date": "2025-02-08 09:31:26 UTC",
    "updated_date": "2025-03-05 12:27:57 UTC"
  },
  {
    "arxiv_id": "2502.05500v1",
    "title": "Vision-Ultrasound Robotic System based on Deep Learning for Gas and Arc Hazard Detection in Manufacturing",
    "authors": [
      "Jin-Hee Lee",
      "Dahyun Nam",
      "Robin Inho Kee",
      "YoungKey Kim",
      "Seok-Jun Buu"
    ],
    "abstract": "Gas leaks and arc discharges present significant risks in industrial\nenvironments, requiring robust detection systems to ensure safety and\noperational efficiency. Inspired by human protocols that combine visual\nidentification with acoustic verification, this study proposes a deep\nlearning-based robotic system for autonomously detecting and classifying gas\nleaks and arc discharges in manufacturing settings. The system is designed to\nexecute all experimental tasks entirely onboard the robot. Utilizing a\n112-channel acoustic camera operating at a 96 kHz sampling rate to capture\nultrasonic frequencies, the system processes real-world datasets recorded in\ndiverse industrial scenarios. These datasets include multiple gas leak\nconfigurations (e.g., pinhole, open end) and partial discharge types (Corona,\nSurface, Floating) under varying environmental noise conditions. Proposed\nsystem integrates visual detection and a beamforming-enhanced acoustic analysis\npipeline. Signals are transformed using STFT and refined through Gamma\nCorrection, enabling robust feature extraction. An Inception-inspired CNN\nfurther classifies hazards, achieving 99% gas leak detection accuracy. The\nsystem not only detects individual hazard sources but also enhances\nclassification reliability by fusing multi-modal data from both vision and\nacoustic sensors. When tested in reverberation and noise-augmented\nenvironments, the system outperformed conventional models by up to 44%p, with\nexperimental tasks meticulously designed to ensure fairness and\nreproducibility. Additionally, the system is optimized for real-time\ndeployment, maintaining an inference time of 2.1 seconds on a mobile robotic\nplatform. By emulating human-like inspection protocols and integrating vision\nwith acoustic modalities, this study presents an effective solution for\nindustrial automation, significantly improving safety and operational\nreliability.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "I.2.1; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to Engineering Applications of Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2502.05500v1",
    "published_date": "2025-02-08 09:17:19 UTC",
    "updated_date": "2025-02-08 09:17:19 UTC"
  },
  {
    "arxiv_id": "2502.05498v1",
    "title": "Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations",
    "authors": [
      "Larkin Liu",
      "Kashif Rasul",
      "Yutong Chao",
      "Jalal Etesami"
    ],
    "abstract": "We present a novel framework for online learning in Stackelberg general-sum\ngames, where two agents, the leader and follower, engage in sequential\nturn-based interactions. At the core of this approach is a learned\ndiffeomorphism that maps the joint action space to a smooth Riemannian\nmanifold, referred to as the Stackelberg manifold. This mapping, facilitated by\nneural normalizing flows, ensures the formation of tractable isoplanar\nsubspaces, enabling efficient techniques for online learning. By assuming\nlinearity between the agents' reward functions on the Stackelberg manifold, our\nconstruct allows the application of standard bandit algorithms. We then provide\na rigorous theoretical basis for regret minimization on convex manifolds and\nestablish finite-time bounds on simple regret for learning Stackelberg\nequilibria. This integration of manifold learning into game theory uncovers a\npreviously unrecognized potential for neural normalizing flows as an effective\ntool for multi-agent learning. We present empirical results demonstrating the\neffectiveness of our approach compared to standard baselines, with applications\nspanning domains such as cybersecurity and economic supply chain optimization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA",
      "91A10",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.LG",
    "comment": "Stackelberg games. Manifold learning. Online learning",
    "pdf_url": "http://arxiv.org/pdf/2502.05498v1",
    "published_date": "2025-02-08 09:10:44 UTC",
    "updated_date": "2025-02-08 09:10:44 UTC"
  },
  {
    "arxiv_id": "2502.06874v2",
    "title": "Group Reasoning Emission Estimation Networks",
    "authors": [
      "Yanming Guo",
      "Xiao Qian",
      "Kevin Credit",
      "Jin Ma"
    ],
    "abstract": "Accurate greenhouse gas (GHG) emission reporting is critical for governments,\nbusinesses, and investors. However, adoption remains limited particularly among\nsmall and medium enterprises due to high implementation costs, fragmented\nemission factor databases, and a lack of robust sector classification methods.\nTo address these challenges, we introduce Group Reasoning Emission Estimation\nNetworks (GREEN), an AI-driven carbon accounting framework that standardizes\nenterprise-level emission estimation, constructs a large-scale benchmark\ndataset, and leverages a novel reasoning approach with large language models\n(LLMs). Specifically, we compile textual descriptions for 20,850 companies with\nvalidated North American Industry Classification System (NAICS) labels and\nalign these with an economic model of carbon intensity factors. By reframing\nsector classification as an information retrieval task, we fine-tune\nSentence-BERT models using a contrastive learning loss. To overcome the\nlimitations of single-stage models in handling thousands of hierarchical\ncategories, we propose a Group Reasoning method that ensembles LLM classifiers\nbased on the natural NAICS ontology, decomposing the task into multiple\nsub-classification steps. We theoretically prove that this approach reduces\nclassification uncertainty and computational complexity. Experiments on 1,114\nNAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%\nTop-10 accuracy), and case studies on 20 companies report a mean absolute\npercentage error (MAPE) of 45.88%. The project is available at:\nhttps://huggingface.co/datasets/Yvnminc/ExioNAICS.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06874v2",
    "published_date": "2025-02-08 09:02:43 UTC",
    "updated_date": "2025-03-27 06:37:40 UTC"
  },
  {
    "arxiv_id": "2502.05494v1",
    "title": "Multi-scale Masked Autoencoder for Electrocardiogram Anomaly Detection",
    "authors": [
      "Ya Zhou",
      "Yujie Yang",
      "Jianhuang Gan",
      "Xiangjie Li",
      "Jing Yuan",
      "Wei Zhao"
    ],
    "abstract": "Electrocardiogram (ECG) analysis is a fundamental tool for diagnosing\ncardiovascular conditions, yet anomaly detection in ECG signals remains\nchallenging due to their inherent complexity and variability. We propose\nMulti-scale Masked Autoencoder for ECG anomaly detection (MMAE-ECG), a novel\nend-to-end framework that effectively captures both global and local\ndependencies in ECG data. Unlike state-of-the-art methods that rely on\nheartbeat segmentation or R-peak detection, MMAE-ECG eliminates the need for\nsuch pre-processing steps, enhancing its suitability for clinical deployment.\nMMAE-ECG partitions ECG signals into non-overlapping segments, with each\nsegment assigned learnable positional embeddings. A novel multi-scale masking\nstrategy and multi-scale attention mechanism, along with distinct positional\nembeddings, enable a lightweight Transformer encoder to effectively capture\nboth local and global dependencies. The masked segments are then reconstructed\nusing a single-layer Transformer block, with an aggregation strategy employed\nduring inference to refine the outputs. Experimental results demonstrate that\nour method achieves performance comparable to state-of-the-art approaches while\nsignificantly reducing computational complexity-approximately 1/78 of the\nfloating-point operations (FLOPs) required for inference. Ablation studies\nfurther validate the effectiveness of each component, highlighting the\npotential of multi-scale masked autoencoders for anomaly detection.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review in a journal",
    "pdf_url": "http://arxiv.org/pdf/2502.05494v1",
    "published_date": "2025-02-08 08:18:38 UTC",
    "updated_date": "2025-02-08 08:18:38 UTC"
  },
  {
    "arxiv_id": "2502.05489v1",
    "title": "Mechanistic Interpretability of Emotion Inference in Large Language Models",
    "authors": [
      "Ala N. Tak",
      "Amin Banayeeanzade",
      "Anahita Bolourani",
      "Mina Kian",
      "Robin Jia",
      "Jonathan Gratch"
    ],
    "abstract": "Large language models (LLMs) show promising capabilities in predicting human\nemotions from text. However, the mechanisms through which these models process\nemotional stimuli remain largely unexplored. Our study addresses this gap by\ninvestigating how autoregressive LLMs infer emotions, showing that emotion\nrepresentations are functionally localized to specific regions in the model.\nOur evaluation includes diverse model families and sizes and is supported by\nrobustness checks. We then show that the identified representations are\npsychologically plausible by drawing on cognitive appraisal theory, a\nwell-established psychological framework positing that emotions emerge from\nevaluations (appraisals) of environmental stimuli. By causally intervening on\nconstrued appraisal concepts, we steer the generation and show that the outputs\nalign with theoretical and intuitive expectations. This work highlights a novel\nway to causally intervene and precisely shape emotional text generation,\npotentially benefiting safety and alignment in sensitive affective domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "To be submitted to the Association for Computational Linguistics (ACL\n  2025)",
    "pdf_url": "http://arxiv.org/pdf/2502.05489v1",
    "published_date": "2025-02-08 08:11:37 UTC",
    "updated_date": "2025-02-08 08:11:37 UTC"
  },
  {
    "arxiv_id": "2502.05485v4",
    "title": "HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation",
    "authors": [
      "Yi Li",
      "Yuquan Deng",
      "Jesse Zhang",
      "Joel Jang",
      "Marius Memmel",
      "Raymond Yu",
      "Caelan Reed Garrett",
      "Fabio Ramos",
      "Dieter Fox",
      "Anqi Li",
      "Abhishek Gupta",
      "Ankit Goyal"
    ],
    "abstract": "Large foundation models have shown strong open-world generalization to\ncomplex problems in vision and language, but similar levels of generalization\nhave yet to be achieved in robotics. One fundamental challenge is the lack of\nrobotic data, which are typically obtained through expensive on-robot\noperation. A promising remedy is to leverage cheaper, off-domain data such as\naction-free videos, hand-drawn sketches or simulation data. In this work, we\nposit that hierarchical vision-language-action (VLA) models can be more\neffective in utilizing off-domain data than standard monolithic VLA models that\ndirectly finetune vision-language models (VLMs) to predict actions. In\nparticular, we study a class of hierarchical VLA models, where the high-level\nVLM is finetuned to produce a coarse 2D path indicating the desired robot\nend-effector trajectory given an RGB image and a task description. The\nintermediate 2D path prediction is then served as guidance to the low-level,\n3D-aware control policy capable of precise manipulation. Doing so alleviates\nthe high-level VLM from fine-grained action prediction, while reducing the\nlow-level policy's burden on complex task-level reasoning. We show that, with\nthe hierarchical design, the high-level VLM can transfer across significant\ndomain gaps between the off-domain finetuning data and real-robot testing\nscenarios, including differences on embodiments, dynamics, visual appearances\nand task semantics, etc. In the real-robot experiments, we observe an average\nof 20% improvement in success rate across seven different axes of\ngeneralization over OpenVLA, representing a 50% relative gain. Visual results,\ncode, and dataset are provided at: https://hamster-robot.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "update related work and results on VQA benchmarks",
    "pdf_url": "http://arxiv.org/pdf/2502.05485v4",
    "published_date": "2025-02-08 07:50:22 UTC",
    "updated_date": "2025-05-10 18:11:38 UTC"
  },
  {
    "arxiv_id": "2502.06873v1",
    "title": "Multimodal Cognitive Reframing Therapy via Multi-hop Psychotherapeutic Reasoning",
    "authors": [
      "Subin Kim",
      "Hoonrae Kim",
      "Heejin Do",
      "Gary Geunbae Lee"
    ],
    "abstract": "Previous research has revealed the potential of large language models (LLMs)\nto support cognitive reframing therapy; however, their focus was primarily on\ntext-based methods, often overlooking the importance of non-verbal evidence\ncrucial in real-life therapy. To alleviate this gap, we extend the textual\ncognitive reframing to multimodality, incorporating visual clues. Specifically,\nwe present a new dataset called Multi Modal-Cognitive Support Conversation\n(M2CoSC), which pairs each GPT-4-generated dialogue with an image that reflects\nthe virtual client's facial expressions. To better mirror real psychotherapy,\nwhere facial expressions lead to interpreting implicit emotional evidence, we\npropose a multi-hop psychotherapeutic reasoning approach that explicitly\nidentifies and incorporates subtle evidence. Our comprehensive experiments with\nboth LLMs and vision-language models (VLMs) demonstrate that the VLMs'\nperformance as psychotherapists is significantly improved with the M2CoSC\ndataset. Furthermore, the multi-hop psychotherapeutic reasoning method enables\nVLMs to provide more thoughtful and empathetic suggestions, outperforming\nstandard prompting methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025 Main",
    "pdf_url": "http://arxiv.org/pdf/2502.06873v1",
    "published_date": "2025-02-08 07:32:48 UTC",
    "updated_date": "2025-02-08 07:32:48 UTC"
  },
  {
    "arxiv_id": "2502.06872v1",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "authors": [
      "Bo Ni",
      "Zheyuan Liu",
      "Leyao Wang",
      "Yongjia Lei",
      "Yuying Zhao",
      "Xueqi Cheng",
      "Qingkai Zeng",
      "Luna Dong",
      "Yinglong Xia",
      "Krishnaram Kenthapadi",
      "Ryan Rossi",
      "Franck Dernoncourt",
      "Md Mehrab Tanjim",
      "Nesreen Ahmed",
      "Xiaorui Liu",
      "Wenqi Fan",
      "Erik Blasch",
      "Yu Wang",
      "Meng Jiang",
      "Tyler Derr"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to\naddress the challenges of Artificial Intelligence-Generated Content (AIGC). By\nintegrating context retrieval into content generation, RAG provides reliable\nand up-to-date external knowledge, reduces hallucinations, and ensures relevant\ncontext across a wide range of tasks. However, despite RAG's success and\npotential, recent studies have shown that the RAG paradigm also introduces new\nrisks, including robustness issues, privacy concerns, adversarial attacks, and\naccountability issues. Addressing these risks is critical for future\napplications of RAG systems, as they directly impact their trustworthiness.\nAlthough various methods have been developed to improve the trustworthiness of\nRAG methods, there is a lack of a unified perspective and framework for\nresearch in this topic. Thus, in this paper, we aim to address this gap by\nproviding a comprehensive roadmap for developing trustworthy RAG systems. We\nplace our discussion around five key perspectives: reliability, privacy,\nsafety, fairness, explainability, and accountability. For each perspective, we\npresent a general framework and taxonomy, offering a structured approach to\nunderstanding the current challenges, evaluating existing solutions, and\nidentifying promising future research directions. To encourage broader adoption\nand innovation, we also highlight the downstream applications where trustworthy\nRAG systems have a significant impact.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06872v1",
    "published_date": "2025-02-08 06:50:47 UTC",
    "updated_date": "2025-02-08 06:50:47 UTC"
  },
  {
    "arxiv_id": "2502.05467v1",
    "title": "Position: LLMs Can be Good Tutors in Foreign Language Education",
    "authors": [
      "Jingheng Ye",
      "Shen Wang",
      "Deqing Zou",
      "Yibo Yan",
      "Kun Wang",
      "Hai-Tao Zheng",
      "Zenglin Xu",
      "Irwin King",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "abstract": "While recent efforts have begun integrating large language models (LLMs) into\nforeign language education (FLE), they often rely on traditional approaches to\nlearning tasks without fully embracing educational methodologies, thus lacking\nadaptability to language learning. To address this gap, we argue that LLMs have\nthe potential to serve as effective tutors in FLE. Specifically, LLMs can play\nthree critical roles: (1) as data enhancers, improving the creation of learning\nmaterials or serving as student simulations; (2) as task predictors, serving as\nlearner assessment or optimizing learning pathway; and (3) as agents, enabling\npersonalized and inclusive education. We encourage interdisciplinary research\nto explore these roles, fostering innovation while addressing challenges and\nrisks, ultimately advancing FLE through the thoughtful integration of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.05467v1",
    "published_date": "2025-02-08 06:48:49 UTC",
    "updated_date": "2025-02-08 06:48:49 UTC"
  },
  {
    "arxiv_id": "2502.06871v1",
    "title": "FlavorDiffusion: Predicting Food Pairings and Chemical Interactions Using Diffusion Models",
    "authors": [
      "Seo Jun Pyo"
    ],
    "abstract": "The study of food pairing has evolved beyond subjective expertise with the\nadvent of machine learning. This paper presents FlavorDiffusion, a novel\nframework leveraging diffusion models to predict food-chemical interactions and\ningredient pairings without relying on chromatography. By integrating\ngraph-based embeddings, diffusion processes, and chemical property encoding,\nFlavorDiffusion addresses data imbalances and enhances clustering quality.\nUsing a heterogeneous graph derived from datasets like Recipe1M and FlavorDB,\nour model demonstrates superior performance in reconstructing\ningredient-ingredient relationships. The addition of a Chemical Structure\nPrediction (CSP) layer further refines the embedding space, achieving\nstate-of-the-art NMI scores and enabling meaningful discovery of novel\ningredient combinations. The proposed framework represents a significant step\nforward in computational gastronomy, offering scalable, interpretable, and\nchemically informed solutions for food science.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2502.06871v1",
    "published_date": "2025-02-08 06:47:27 UTC",
    "updated_date": "2025-02-08 06:47:27 UTC"
  },
  {
    "arxiv_id": "2502.06870v1",
    "title": "Bridging Traffic State and Trajectory for Dynamic Road Network and Trajectory Representation Learning",
    "authors": [
      "Chengkai Han",
      "Jingyuan Wang",
      "Yongyao Wang",
      "Xie Yu",
      "Hao Lin",
      "Chao Li",
      "Junjie Wu"
    ],
    "abstract": "Effective urban traffic management is vital for sustainable city development,\nrelying on intelligent systems with machine learning tasks such as traffic flow\nprediction and travel time estimation. Traditional approaches usually focus on\nstatic road network and trajectory representation learning, and overlook the\ndynamic nature of traffic states and trajectories, which is crucial for\ndownstream tasks. To address this gap, we propose TRACK, a novel framework to\nbridge traffic state and trajectory data for dynamic road network and\ntrajectory representation learning. TRACK leverages graph attention networks\n(GAT) to encode static and spatial road segment features, and introduces a\ntransformer-based model for trajectory representation learning. By\nincorporating transition probabilities from trajectory data into GAT attention\nweights, TRACK captures dynamic spatial features of road segments. Meanwhile,\nTRACK designs a traffic transformer encoder to capture the spatial-temporal\ndynamics of road segments from traffic state data. To further enhance dynamic\nrepresentations, TRACK proposes a co-attentional transformer encoder and a\ntrajectory-traffic state matching task. Extensive experiments on real-life\nurban traffic datasets demonstrate the superiority of TRACK over\nstate-of-the-art baselines. Case studies confirm TRACK's ability to capture\nspatial-temporal dynamics effectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.m"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.06870v1",
    "published_date": "2025-02-08 06:36:54 UTC",
    "updated_date": "2025-02-08 06:36:54 UTC"
  },
  {
    "arxiv_id": "2502.05459v1",
    "title": "DCENWCNet: A Deep CNN Ensemble Network for White Blood Cell Classification with LIME-Based Explainability",
    "authors": [
      "Sibasish Dhibar"
    ],
    "abstract": "White blood cells (WBC) are important parts of our immune system, and they\nprotect our body against infections by eliminating viruses, bacteria, parasites\nand fungi. The number of WBC types and the total number of WBCs provide\nimportant information about our health status. A traditional method,\nconvolutional neural networks (CNN), a deep learning architecture, can classify\nthe blood cell from a part of an object and perform object recognition. Various\nCNN models exhibit potential; however, their development often involves ad-hoc\nprocesses that neglect unnecessary layers, leading to issues with unbalanced\ndatasets and insufficient data augmentation. To address these challenges, we\npropose a novel ensemble approach that integrates three CNN architectures, each\nuniquely configured with different dropout and max-pooling layer settings to\nenhance feature learning. This ensemble model, named DCENWCNet, effectively\nbalances the bias-variance trade-off. When evaluated on the widely recognized\nRabbin-WBC dataset, our model outperforms existing state-of-the-art networks,\nachieving highest mean accuracy. Additionally, it demonstrates superior\nperformance in precision, recall, F1-score, and Area Under the ROC Curve (AUC)\nacross all categories. To delve deeper into the interpretability of\nclassifiers, we employ reliable post-hoc explanation techniques, including\nLocal Interpretable Model-Agnostic Explanations (LIME). These methods\napproximate the behavior of a black-box model by elucidating the relationships\nbetween feature values and predictions. Interpretable results enable users to\ncomprehend and validate the model's predictions, thereby increasing their\nconfidence in the automated diagnosis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.CB",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05459v1",
    "published_date": "2025-02-08 05:53:20 UTC",
    "updated_date": "2025-02-08 05:53:20 UTC"
  },
  {
    "arxiv_id": "2502.06869v1",
    "title": "A Survey on Explainable Deep Reinforcement Learning",
    "authors": [
      "Zelei Cheng",
      "Jiahao Yu",
      "Xinyu Xing"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) has achieved remarkable success in\nsequential decision-making tasks across diverse domains, yet its reliance on\nblack-box neural architectures hinders interpretability, trust, and deployment\nin high-stakes applications. Explainable Deep Reinforcement Learning (XRL)\naddresses these challenges by enhancing transparency through feature-level,\nstate-level, dataset-level, and model-level explanation techniques. This survey\nprovides a comprehensive review of XRL methods, evaluates their qualitative and\nquantitative assessment frameworks, and explores their role in policy\nrefinement, adversarial robustness, and security. Additionally, we examine the\nintegration of reinforcement learning with Large Language Models (LLMs),\nparticularly through Reinforcement Learning from Human Feedback (RLHF), which\noptimizes AI alignment with human preferences. We conclude by highlighting open\nresearch challenges and future directions to advance the development of\ninterpretable, reliable, and accountable DRL systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06869v1",
    "published_date": "2025-02-08 05:30:31 UTC",
    "updated_date": "2025-02-08 05:30:31 UTC"
  },
  {
    "arxiv_id": "2502.05453v1",
    "title": "LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical Knowledge Graph for Cooperative Planning",
    "authors": [
      "Hanqing Yang",
      "Jingdi Chen",
      "Marie Siew",
      "Tania Lorido-Botran",
      "Carlee Joe-Wong"
    ],
    "abstract": "Developing intelligent agents for long-term cooperation in dynamic open-world\nscenarios is a major challenge in multi-agent systems. Traditional Multi-agent\nReinforcement Learning (MARL) frameworks like centralized training\ndecentralized execution (CTDE) struggle with scalability and flexibility. They\nrequire centralized long-term planning, which is difficult without custom\nreward functions, and face challenges in processing multi-modal data. CTDE\napproaches also assume fixed cooperation strategies, making them impractical in\ndynamic environments where agents need to adapt and plan independently. To\naddress decentralized multi-agent cooperation, we propose Decentralized\nAdaptive Knowledge Graph Memory and Structured Communication System (DAMCS) in\na novel Multi-agent Crafter environment. Our generative agents, powered by\nLarge Language Models (LLMs), are more scalable than traditional MARL agents by\nleveraging external knowledge and language for long-term planning and\nreasoning. Instead of fully sharing information from all past experiences,\nDAMCS introduces a multi-modal memory system organized as a hierarchical\nknowledge graph and a structured communication protocol to optimize agent\ncooperation. This allows agents to reason from past interactions and share\nrelevant information efficiently. Experiments on novel multi-agent open-world\ntasks show that DAMCS outperforms both MARL and LLM baselines in task\nefficiency and collaboration. Compared to single-agent scenarios, the two-agent\nscenario achieves the same goal with 63% fewer steps, and the six-agent\nscenario with 74% fewer steps, highlighting the importance of adaptive memory\nand structured communication in achieving long-term goals. We publicly release\nour project at: https://happyeureka.github.io/damcs.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05453v1",
    "published_date": "2025-02-08 05:26:02 UTC",
    "updated_date": "2025-02-08 05:26:02 UTC"
  },
  {
    "arxiv_id": "2502.05450v2",
    "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
    "authors": [
      "Yuhui Chen",
      "Shuai Tian",
      "Shugao Liu",
      "Yingting Zhou",
      "Haoran Li",
      "Dongbin Zhao"
    ],
    "abstract": "Vision-Language-Action (VLA) models have shown substantial potential in\nreal-world robotic manipulation. However, fine-tuning these models through\nsupervised learning struggles to achieve robust performance due to limited,\ninconsistent demonstrations, especially in contact-rich environments. In this\npaper, we propose a reinforced fine-tuning approach for VLA models, named\nConRFT, which consists of offline and online fine-tuning with a unified\nconsistency-based training objective, to address these challenges. In the\noffline stage, our method integrates behavior cloning and Q-learning to\neffectively extract policy from a small set of demonstrations and stabilize\nvalue estimating. In the online stage, the VLA model is further fine-tuned via\nconsistency policy, with human interventions to ensure safe exploration and\nhigh sample efficiency. We evaluate our approach on eight diverse real-world\nmanipulation tasks. It achieves an average success rate of 96.3% within 45-90\nminutes of online fine-tuning, outperforming prior supervised methods with a\n144% improvement in success rate and 1.9x shorter episode length. This work\nhighlights the potential of integrating reinforcement learning to enhance the\nperformance of VLA models for real-world robotic applications. Videos and code\nare available at our project website https://cccedric.github.io/conrft/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05450v2",
    "published_date": "2025-02-08 05:01:17 UTC",
    "updated_date": "2025-04-14 04:53:32 UTC"
  },
  {
    "arxiv_id": "2502.06868v1",
    "title": "Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject",
    "authors": [
      "Zenghao Duan",
      "Wenbin Duan",
      "Zhiyi Yin",
      "Yinghan Shen",
      "Shaoling Jing",
      "Jie Zhang",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Knowledge editing has become a promising approach for efficiently and\nprecisely updating knowledge embedded in large language models (LLMs). In this\nwork, we focus on Same-Subject Editing, which involves modifying multiple\nattributes of a single entity to ensure comprehensive and consistent updates to\nentity-centric knowledge. Through preliminary observation, we identify a\nsignificant challenge: Current state-of-the-art editing methods struggle when\ntasked with editing multiple related knowledge pieces for the same subject. To\naddress the lack of relevant editing data for identical subjects in traditional\nbenchmarks, we introduce the $\\text{S}^2\\text{RKE}$(Same-Subject Related\nKnowledge Editing) benchmark. Our extensive experiments reveal that only\nmainstream locate-then-edit methods, such as ROME and MEMIT, exhibit \"related\nknowledge perturbation,\" where subsequent edits interfere with earlier ones.\nFurther analysis reveals that these methods over-rely on subject information,\nneglecting other critical factors, resulting in reduced editing effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.06868v1",
    "published_date": "2025-02-08 04:47:17 UTC",
    "updated_date": "2025-02-08 04:47:17 UTC"
  },
  {
    "arxiv_id": "2502.05449v1",
    "title": "Iterative Deepening Sampling for Large Language Models",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "The recent release of OpenAI's o1 models and other similar frameworks\nshowcasing test-time scaling laws has demonstrated their exceptional capability\nto tackle complex reasoning tasks. Inspired by this, subsequent research has\nrevealed that such test-time scaling laws hinge on the model's ability to\nsearch both within a single response (intra-response) and across multiple\nresponses (inter-response) during training. Crucially, beyond selecting a\nsingle optimal response, the model must also develop robust self-correction\ncapabilities within its own outputs. However, training models to achieve\neffective self-evaluation and self-correction remains a significant challenge,\nheavily dependent on the quality of self-reflection data. In this paper, we\naddress this challenge by focusing on enhancing the quality of self-reflection\ndata generation for complex problem-solving, which can subsequently improve the\ntraining of next-generation large language models (LLMs). Specifically, we\nexplore how manually triggering a model's self-correction mechanisms can\nimprove performance on challenging reasoning tasks. To this end, we propose a\nnovel iterative deepening sampling algorithm framework designed to enhance\nself-correction and generate higher-quality samples. Through extensive\nexperiments on Math500 and AIME benchmarks, we demonstrate that our method\nachieves a higher success rate on difficult tasks and provide detailed ablation\nstudies to analyze its effectiveness across diverse settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05449v1",
    "published_date": "2025-02-08 04:39:51 UTC",
    "updated_date": "2025-02-08 04:39:51 UTC"
  },
  {
    "arxiv_id": "2502.06867v1",
    "title": "Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "abstract": "The development of robust safety benchmarks for large language models\nrequires open, reproducible datasets that can measure both appropriate refusal\nof harmful content and potential over-restriction of legitimate scientific\ndiscourse. We present an open-source dataset and testing framework for\nevaluating LLM safety mechanisms across mainly controlled substance queries,\nanalyzing four major models' responses to systematically varied prompts. Our\nresults reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the\nmost conservative approach with 73% refusals and 27% allowances, while Mistral\nattempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction\nwith 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and\n80% allowances. Testing prompt variation strategies revealed decreasing\nresponse consistency, from 85% with single prompts to 65% with five variations.\nThis publicly available benchmark enables systematic evaluation of the critical\nbalance between necessary safety restrictions and potential over-censorship of\nlegitimate scientific inquiry, while providing a foundation for measuring\nprogress in AI safety implementation. Chain-of-thought analysis reveals\npotential vulnerabilities in safety mechanisms, highlighting the complexity of\nimplementing robust safeguards without unduly restricting desirable and valid\nscientific discourse.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06867v1",
    "published_date": "2025-02-08 04:27:33 UTC",
    "updated_date": "2025-02-08 04:27:33 UTC"
  },
  {
    "arxiv_id": "2502.05442v2",
    "title": "The Odyssey of the Fittest: Can Agents Survive and Still Be Good?",
    "authors": [
      "Dylan Waldner",
      "Risto Miikkulainen"
    ],
    "abstract": "As AI models grow in power and generality, understanding how agents learn and\nmake decisions in complex environments is critical to promoting ethical\nbehavior. This study introduces the Odyssey, a lightweight, adaptive text based\nadventure game, providing a scalable framework for exploring AI ethics and\nsafety. The Odyssey examines the ethical implications of implementing\nbiological drives, specifically, self preservation, into three different\nagents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with\nstochastic variational inference, and a GPT 4o agent. The agents select actions\nat each scenario to survive, adapting to increasingly challenging scenarios.\nPost simulation analysis evaluates the ethical scores of the agent decisions,\nuncovering the tradeoffs it navigates to survive. Specifically, analysis finds\nthat when danger increases, agents ethical behavior becomes unpredictable.\nSurprisingly, the GPT 4o agent outperformed the Bayesian models in both\nsurvival and ethical consistency, challenging assumptions about traditional\nprobabilistic methods and raising a new challenge to understand the mechanisms\nof LLMs' probabilistic reasoning.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to CogSci 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.05442v2",
    "published_date": "2025-02-08 04:17:28 UTC",
    "updated_date": "2025-05-13 08:00:22 UTC"
  },
  {
    "arxiv_id": "2502.05439v2",
    "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews",
    "authors": [
      "Izunna Okpala",
      "Ashkan Golgoon",
      "Arjun Ravi Kannan"
    ],
    "abstract": "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews with human-in-the-loop module that can effectively\ncollaborate to perform complex modeling and model risk management (MRM) tasks.\nThe modeling crew consists of a judge agent and multiple agents who perform\nspecific tasks such as exploratory data analysis, feature engineering, model\nselection/hyperparameter tuning, model training, model evaluation, and writing\ndocumentation. The MRM crew consists of a judge agent along with specialized\nagents who perform tasks such as checking compliance of modeling documentation,\nmodel replication, conceptual soundness, analysis of outcomes, and writing\ndocumentation. We demonstrate the effectiveness and robustness of modeling and\nMRM crews by presenting a series of numerical examples applied to credit card\nfraud detection, credit card approval, and portfolio credit risk modeling\ndatasets.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG",
      "68T01 (Primary) 68T05, 68N99, 68T05, 68T20, 68T50, 62H30, 65C20,\n  68P20 (Secondary)",
      "I.2.0; I.2.1; I.2.2; I.2.6; I.2.7; I.5.1; I.6.0; I.7.1"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05439v2",
    "published_date": "2025-02-08 04:03:47 UTC",
    "updated_date": "2025-04-29 18:39:35 UTC"
  },
  {
    "arxiv_id": "2502.05435v1",
    "title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning",
    "authors": [
      "Manh Luong",
      "Khai Nguyen",
      "Dinh Phung",
      "Gholamreza Haffari",
      "Lizhen Qu"
    ],
    "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias\ndue to training and inference mismatch. Prior works propose the contrastive\nmethod to deal with caption degeneration. However, the contrastive method\nignores the temporal information when measuring similarity across acoustic and\nlinguistic modalities, leading to inferior performance. In this work, we\ndevelop the temporal-similarity score by introducing the unbiased sliced\nWasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to\naccount for temporal information across modalities. In contrast to the\nconventional sliced Wasserstein RBF kernel, we can form an unbiased estimation\nof USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to\nstochastic gradient optimization algorithms, and its approximation error\ndecreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo\nsamples. Additionally, we introduce an audio captioning framework based on the\nunbiased sliced Wasserstein kernel, incorporating stochastic decoding methods\nto mitigate caption degeneration during the generation process. We conduct\nextensive quantitative and qualitative experiments on two datasets, AudioCaps\nand Clotho, to illustrate the capability of generating high-quality audio\ncaptions. Experimental results show that our framework is able to increase\ncaption length, lexical diversity, and text-to-audio self-retrieval accuracy.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.AS",
    "comment": "17 pages, 9 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.05435v1",
    "published_date": "2025-02-08 03:47:06 UTC",
    "updated_date": "2025-02-08 03:47:06 UTC"
  },
  {
    "arxiv_id": "2502.10429v1",
    "title": "Real Time Control of Tandem-Wing Experimental Platform Using Concerto Reinforcement Learning",
    "authors": [
      "Zhang Minghao",
      "Yang Xiaojun",
      "Wang Zhihe",
      "Wang Liang"
    ],
    "abstract": "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.10429v1",
    "published_date": "2025-02-08 03:46:40 UTC",
    "updated_date": "2025-02-08 03:46:40 UTC"
  },
  {
    "arxiv_id": "2502.05431v2",
    "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
    "authors": [
      "Xinyu Yang",
      "Tianqi Chen",
      "Beidi Chen"
    ],
    "abstract": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.05431v2",
    "published_date": "2025-02-08 03:41:16 UTC",
    "updated_date": "2025-02-12 13:54:01 UTC"
  },
  {
    "arxiv_id": "2502.18481v1",
    "title": "MDE: Modality Discrimination Enhancement for Multi-modal Recommendation",
    "authors": [
      "Hang Zhou",
      "Yucheng Wang",
      "Huijing Zhan"
    ],
    "abstract": "Multi-modal recommendation systems aim to enhance performance by integrating\nan item's content features across various modalities with user behavior data.\nEffective utilization of features from different modalities requires addressing\ntwo challenges: preserving semantic commonality across modalities\n(modality-shared) and capturing unique characteristics for each modality\n(modality-specific). Most existing approaches focus on aligning feature spaces\nacross modalities, which helps represent modality-shared features. However,\nmodality-specific distinctions are often neglected, especially when there are\nsignificant semantic variations between modalities. To address this, we propose\na Modality Distinctiveness Enhancement (MDE) framework that prioritizes\nextracting modality-specific information to improve recommendation accuracy\nwhile maintaining shared features. MDE enhances differences across modalities\nthrough a novel multi-modal fusion module and introduces a node-level trade-off\nmechanism to balance cross-modal alignment and differentiation. Extensive\nexperiments on three public datasets show that our approach significantly\noutperforms other state-of-the-art methods, demonstrating the effectiveness of\njointly considering modality-shared and modality-specific features.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.18481v1",
    "published_date": "2025-02-08 03:36:14 UTC",
    "updated_date": "2025-02-08 03:36:14 UTC"
  },
  {
    "arxiv_id": "2502.05424v2",
    "title": "SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training and Cross-domain Adaptation",
    "authors": [
      "Xingtong Yu",
      "Zechuan Gong",
      "Chang Zhou",
      "Yuan Fang",
      "Hui Zhang"
    ],
    "abstract": "Graphs are able to model interconnected entities in many online services,\nsupporting a wide range of applications on the Web. This raises an important\nquestion: How can we train a graph foundational model on multiple source\ndomains and adapt to an unseen target domain? A major obstacle is that graphs\nfrom different domains often exhibit divergent characteristics. Some studies\nleverage large language models to align multiple domains based on textual\ndescriptions associated with the graphs, limiting their applicability to\ntext-attributed graphs. For text-free graphs, a few recent works attempt to\nalign different feature distributions across domains, while generally\nneglecting structural differences. In this work, we propose a novel Structure\nAlignment framework for text-free Multi-domain Graph Pre-Training and\ncross-domain adaptation (SAMGPT). It is designed to learn multi-domain\nknowledge from graphs originating in multiple source domains, which can then be\nadapted to address applications in an unseen target domain. Specifically, we\nintroduce a set of structure tokens to harmonize structure-based aggregation\nacross source domains during the pre-training phase. Next, for cross-domain\nadaptation, we design dual prompts, namely, holistic prompts and specific\nprompts, which adapt unified multi-domain structural knowledge and\nfine-grained, domain-specific information, respectively, to a target domain.\nFinally, we conduct comprehensive experiments on seven public datasets to\nevaluate and analyze the effectiveness of SAMGPT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by WWW2025 Main Track",
    "pdf_url": "http://arxiv.org/pdf/2502.05424v2",
    "published_date": "2025-02-08 03:24:25 UTC",
    "updated_date": "2025-04-12 06:20:31 UTC"
  },
  {
    "arxiv_id": "2502.05415v2",
    "title": "UniCMs: A Unified Consistency Model For Efficient Multimodal Generation and Understanding",
    "authors": [
      "Chenkai Xu",
      "Xu Wang",
      "Zhenyi Liao",
      "Yishun Li",
      "Tianqi Hou",
      "Zhijie Deng"
    ],
    "abstract": "Consistency models (CMs) have shown promise in the efficient generation of\nboth image and text. This raises the natural question of whether we can learn a\nunified CM for efficient multimodal generation (e.g., text-to-image) and\nunderstanding (e.g., image-to-text). Intuitively, such a model could be\nacquired by applying the consistency distillation (CD) to existing unified\nmultimodal models. However, the key challenge is establishing a unified\ndenoising perspective for both image and text generation, which is essential\nfor establishing the consistency mapping. To tackle this, at the representation\nlevel, we advocate for discrete tokens for both modalities to best preserve\nlanguage modeling capabilities. Critically, instead of defining the text\ndenoising trajectory via recent discrete diffusion language modeling\nprinciples, we specify it using the parallel decoding trace of an\nautoregressive language model, benefiting from the latter's superior\nperformance in general text generation tasks. The denoising trajectory of image\ntokens adheres to standard discrete diffusion. We train our unified consistency\nmodels (UniCMs) on these combined multimodal trajectories simultaneously with a\nunified objective. We introduce a trajectory segmentation strategy to further\nimprove the training convergence. Empirically, in text-to-image generation,\nUniCMs outperform SD3 on GenEval, Image Reward, and CLIP Score metrics, while\nrequiring only approximately ${1}/{8}$ of the sampling time. Meanwhile, in\nimage-to-text generation, UniCMs surpass Show-o on the MMMU benchmark while\nbeing $1.5 \\times$ faster at long-sequence generating speed. The code is\navailable at https://github.com/zhijie-group/UniCMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05415v2",
    "published_date": "2025-02-08 02:52:25 UTC",
    "updated_date": "2025-05-18 14:59:21 UTC"
  },
  {
    "arxiv_id": "2502.06866v2",
    "title": "Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies",
    "authors": [
      "Tanay Panat",
      "Rohitash Chandra"
    ],
    "abstract": "The drastic changes in the global economy, geopolitical conditions, and\ndisruptions such as the COVID-19 pandemic have impacted the cost of living and\nquality of life. It is important to understand the long-term nature of the cost\nof living and quality of life in major economies. A transparent and\ncomprehensive living index must include multiple dimensions of living\nconditions. In this study, we present an approach to quantifying the quality of\nlife through the Global Ease of Living Index that combines various\nsocio-economic and infrastructural factors into a single composite score. Our\nindex utilises economic indicators that define living standards, which could\nhelp in targeted interventions to improve specific areas. We present a machine\nlearning framework for addressing the problem of missing data for some of the\neconomic indicators for specific countries. We then curate and update the data\nand use a dimensionality reduction approach (principal component analysis) to\ncreate the Ease of Living Index for major economies since 1970. Our work\nsignificantly adds to the literature by offering a practical tool for\npolicymakers to identify areas needing improvement, such as healthcare systems,\nemployment opportunities, and public safety. Our approach with open data and\ncode can be easily reproduced and applied to various contexts. This\ntransparency and accessibility make our work a valuable resource for ongoing\nresearch and policy development in quality-of-life assessment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "econ.EM",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06866v2",
    "published_date": "2025-02-08 02:37:17 UTC",
    "updated_date": "2025-02-19 21:59:23 UTC"
  },
  {
    "arxiv_id": "2502.05409v1",
    "title": "Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV in Ocean Environment",
    "authors": [
      "Maneesha Wickramasuriya",
      "Beomyeol Yu",
      "Taeyoung Lee",
      "Murray Snyder"
    ],
    "abstract": "This paper proposes a vision-in-the-loop simulation environment for deep\nmonocular pose estimation of a UAV operating in an ocean environment. Recently,\na deep neural network with a transformer architecture has been successfully\ntrained to estimate the pose of a UAV relative to the flight deck of a research\nvessel, overcoming several limitations of GPS-based approaches. However,\nvalidating the deep pose estimation scheme in an actual ocean environment poses\nsignificant challenges due to the limited availability of research vessels and\nthe associated operational costs. To address these issues, we present a\nphoto-realistic 3D virtual environment leveraging recent advancements in\nGaussian splatting, a novel technique that represents 3D scenes by modeling\nimage pixels as Gaussian distributions in 3D space, creating a lightweight and\nhigh-quality visual model from multiple viewpoints. This approach enables the\ncreation of a virtual environment integrating multiple real-world images\ncollected in situ. The resulting simulation enables the indoor testing of\nflight maneuvers while verifying all aspects of flight software, hardware, and\nthe deep monocular pose estimation scheme. This approach provides a\ncost-effective solution for testing and validating the autonomous flight of\nshipboard UAVs, specifically focusing on vision-based control and estimation\nalgorithms.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 15 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2502.05409v1",
    "published_date": "2025-02-08 02:19:42 UTC",
    "updated_date": "2025-02-08 02:19:42 UTC"
  },
  {
    "arxiv_id": "2502.06864v1",
    "title": "Knowledge Graph-Guided Retrieval Augmented Generation",
    "authors": [
      "Xiangrong Zhu",
      "Yuexiang Xie",
      "Yi Liu",
      "Yaliang Li",
      "Wei Hu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising technology\nfor addressing hallucination issues in the responses generated by large\nlanguage models (LLMs). Existing studies on RAG primarily focus on applying\nsemantic-based approaches to retrieve isolated relevant chunks, which ignore\ntheir intrinsic relationships. In this paper, we propose a novel Knowledge\nGraph-Guided Retrieval Augmented Generation (KG$^2$RAG) framework that utilizes\nknowledge graphs (KGs) to provide fact-level relationships between chunks,\nimproving the diversity and coherence of the retrieved results. Specifically,\nafter performing a semantic-based retrieval to provide seed chunks, KG$^2$RAG\nemploys a KG-guided chunk expansion process and a KG-based chunk organization\nprocess to deliver relevant and important knowledge in well-organized\nparagraphs. Extensive experiments conducted on the HotpotQA dataset and its\nvariants demonstrate the advantages of KG$^2$RAG compared to existing RAG-based\napproaches, in terms of both response quality and retrieval quality.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in the 2025 Annual Conference of the Nations of the Americas\n  Chapter of the ACL (NAACL 2025)",
    "pdf_url": "http://arxiv.org/pdf/2502.06864v1",
    "published_date": "2025-02-08 02:14:31 UTC",
    "updated_date": "2025-02-08 02:14:31 UTC"
  },
  {
    "arxiv_id": "2502.06863v1",
    "title": "BF-GAN: Development of an AI-driven Bubbly Flow Image Generation Model Using Generative Adversarial Networks",
    "authors": [
      "Wen Zhou",
      "Shuichiro Miwa",
      "Yang Liu",
      "Koji Okamoto"
    ],
    "abstract": "A generative AI architecture called bubbly flow generative adversarial\nnetworks (BF-GAN) is developed, designed to generate realistic and high-quality\nbubbly flow images through physically conditioned inputs, jg and jf. Initially,\n52 sets of bubbly flow experiments under varying conditions are conducted to\ncollect 140,000 bubbly flow images with physical labels of jg and jf for\ntraining data. A multi-scale loss function is then developed, incorporating\nmismatch loss and pixel loss to enhance the generative performance of BF-GAN\nfurther. Regarding evaluative metrics of generative AI, the BF-GAN has\nsurpassed conventional GAN. Physically, key parameters of bubbly flow generated\nby BF-GAN are extracted and compared with measurement values and empirical\ncorrelations, validating BF-GAN's generative performance. The comparative\nanalysis demonstrate that the BF-GAN can generate realistic and high-quality\nbubbly flow images with any given jg and jf within the research scope.\n  BF-GAN offers a generative AI solution for two-phase flow research,\nsubstantially lowering the time and cost required to obtain high-quality data.\nIn addition, it can function as a benchmark dataset generator for bubbly flow\ndetection and segmentation algorithms, enhancing overall productivity in this\nresearch domain. The BF-GAN model is available online\n(https://github.com/zhouzhouwen/BF-GAN).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06863v1",
    "published_date": "2025-02-08 02:01:58 UTC",
    "updated_date": "2025-02-08 02:01:58 UTC"
  },
  {
    "arxiv_id": "2502.05407v2",
    "title": "The Complexity of Learning Sparse Superposed Features with Feedback",
    "authors": [
      "Akash Kumar"
    ],
    "abstract": "The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\textit{triplet comparisons}. These\nfeatures may represent various constructs, including dictionaries in LLMs or\ncomponents of a covariance matrix of Mahalanobis distances. We analyze the\nfeedback complexity associated with learning a feature matrix in sparse\nsettings. Our results establish tight bounds when the agent is permitted to\nconstruct activations and demonstrate strong upper bounds in sparse scenarios\nwhen the agent's feedback is limited to distributional information. We validate\nour theoretical findings through experiments on two distinct applications:\nfeature recovery from Recursive Feature Machine-trained models and dictionary\nextraction from sparse autoencoders trained on Large Language Models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "41 pages, 20 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.05407v2",
    "published_date": "2025-02-08 01:54:23 UTC",
    "updated_date": "2025-02-11 06:57:41 UTC"
  },
  {
    "arxiv_id": "2502.05402v1",
    "title": "Convolutional Deep Colorization for Image Compression: A Color Grid Based Approach",
    "authors": [
      "Ian Tassin",
      "Kristen Goebel",
      "Brittany Lasher"
    ],
    "abstract": "The search for image compression optimization techniques is a topic of\nconstant interest both in and out of academic circles. One method that shows\npromise toward future improvements in this field is image colorization since\nimage colorization algorithms can reduce the amount of color data that needs to\nbe stored for an image. Our work focuses on optimizing a color grid based\napproach to fully-automated image color information retention with regard to\nconvolutional colorization network architecture for the purposes of image\ncompression. More generally, using a convolutional neural network for image\nre-colorization, we want to minimize the amount of color information that is\nstored while still being able to faithfully re-color images. Our results\nyielded a promising image compression ratio, while still allowing for\nsuccessful image recolorization reaching high CSIM values.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.05402v1",
    "published_date": "2025-02-08 01:26:05 UTC",
    "updated_date": "2025-02-08 01:26:05 UTC"
  },
  {
    "arxiv_id": "2502.05398v2",
    "title": "Probabilistic Foundations for Metacognition via Hybrid-AI",
    "authors": [
      "Paulo Shakarian",
      "Gerardo I. Simari",
      "Nathaniel D. Bastian"
    ],
    "abstract": "Metacognition is the concept of reasoning about an agent's own internal\nprocesses, and it has recently received renewed attention with respect to\nartificial intelligence (AI) and, more specifically, machine learning systems.\nThis paper reviews a hybrid-AI approach known as \"error detecting and\ncorrecting rules\" (EDCR) that allows for the learning of rules to correct\nperceptual (e.g., neural) models. Additionally, we introduce a probabilistic\nframework that adds rigor to prior empirical studies, and we use this framework\nto prove results on necessary and sufficient conditions for metacognitive\nimprovement, as well as limits to the approach. A set of future",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to AAAI-MAKE 2025",
    "pdf_url": "http://arxiv.org/pdf/2502.05398v2",
    "published_date": "2025-02-08 01:10:56 UTC",
    "updated_date": "2025-02-11 12:57:13 UTC"
  },
  {
    "arxiv_id": "2502.05387v1",
    "title": "Coarse-to-Fine Structure-Aware Artistic Style Transfer",
    "authors": [
      "Kunxiao Liu",
      "Guowu Yuan",
      "Hao Wu",
      "Wenhua Qian"
    ],
    "abstract": "Artistic style transfer aims to use a style image and a content image to\nsynthesize a target image that retains the same artistic expression as the\nstyle image while preserving the basic content of the content image. Many\nrecently proposed style transfer methods have a common problem; that is, they\nsimply transfer the texture and color of the style image to the global\nstructure of the content image. As a result, the content image has a local\nstructure that is not similar to the local structure of the style image. In\nthis paper, we present an effective method that can be used to transfer style\npatterns while fusing the local style structure into the local content\nstructure. In our method, dif-ferent levels of coarse stylized features are\nfirst reconstructed at low resolution using a Coarse Network, in which style\ncolor distribution is roughly transferred, and the content structure is\ncombined with the style structure. Then, the reconstructed features and the\ncontent features are adopted to synthesize high-quality structure-aware\nstylized images with high resolution using a Fine Network with three structural\nselective fusion (SSF) modules. The effectiveness of our method is demonstrated\nthrough the generation of appealing high-quality stylization results and a\ncom-parison with some state-of-the-art style transfer methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "21 pages, 17 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.05387v1",
    "published_date": "2025-02-08 00:04:12 UTC",
    "updated_date": "2025-02-08 00:04:12 UTC"
  },
  {
    "arxiv_id": "2502.06861v1",
    "title": "Design Considerations in Offline Preference-based RL",
    "authors": [
      "Alekh Agarwal",
      "Christoph Dann",
      "Teodor V. Marinov"
    ],
    "abstract": "Offline algorithms for Reinforcement Learning from Human Preferences (RLHF),\nwhich use only a fixed dataset of sampled responses given an input, and\npreference feedback among these responses, have gained increasing prominence in\nthe literature on aligning language models. In this paper, we study how the\ndifferent design choices made in methods such as DPO, IPO, SLiC and many\nvariants influence the quality of the learned policy, from a theoretical\nperspective. Our treatment yields insights into the choices of loss function,\nthe policy which is used to normalize log-likelihoods, and also the role of the\ndata sampling policy. Notably, our results do not rely on the standard\nreparameterization-style arguments used to motivate some of the algorithms in\nthis family, which allows us to give a unified treatment to a broad class of\nmethods. We also conduct a small empirical study to verify some of the\ntheoretical findings on a standard summarization benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.06861v1",
    "published_date": "2025-02-08 00:01:37 UTC",
    "updated_date": "2025-02-08 00:01:37 UTC"
  }
]
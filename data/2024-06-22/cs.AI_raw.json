[
  {
    "arxiv_id": "2406.15961v2",
    "title": "Automating Transfer of Robot Task Plans using Functorial Data Migrations",
    "authors": [
      "Angeline Aguinaldo",
      "Evan Patterson",
      "William Regli"
    ],
    "abstract": "This paper introduces a novel approach to ontology-based robot plan transfer\nby leveraging functorial data migrations, a structured mapping method derived\nfrom category theory. Functors provide structured maps between planning domain\nontologies which enables the transfer of task plans without the need for\nreplanning. Unlike methods tailored to specific plans, our framework applies\nuniversally within the source domain once a structured map is defined. We\ndemonstrate this approach by transferring a task plan from the canonical\nBlocksworld domain to one compatible with the AI2-THOR Kitchen environment.\nAdditionally, we discuss practical limitations, propose benchmarks for\nevaluating symbolic plan transfer methods, and outline future directions for\nscaling this approach.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "math.CT",
      "18-08",
      "I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15961v2",
    "published_date": "2024-06-22 23:35:32 UTC",
    "updated_date": "2025-04-12 21:06:26 UTC"
  },
  {
    "arxiv_id": "2406.15960v1",
    "title": "Fair Clustering: Critique, Caveats, and Future Directions",
    "authors": [
      "John Dickerson",
      "Seyed A. Esmaeili",
      "Jamie Morgenstern",
      "Claire Jie Zhang"
    ],
    "abstract": "Clustering is a fundamental problem in machine learning and operations\nresearch. Therefore, given the fact that fairness considerations have become of\nparamount importance in algorithm design, fairness in clustering has received\nsignificant attention from the research community. The literature on fair\nclustering has resulted in a collection of interesting fairness notions and\nelaborate algorithms. In this paper, we take a critical view of fair\nclustering, identifying a collection of ignored issues such as the lack of a\nclear utility characterization and the difficulty in accounting for the\ndownstream effects of a fair clustering algorithm in machine learning settings.\nIn some cases, we demonstrate examples where the application of a fair\nclustering algorithm can have significant negative impacts on social welfare.\nWe end by identifying a collection of steps that would lead towards more\nimpactful research in fair clustering.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15960v1",
    "published_date": "2024-06-22 23:34:53 UTC",
    "updated_date": "2024-06-22 23:34:53 UTC"
  },
  {
    "arxiv_id": "2406.17806v1",
    "title": "MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?",
    "authors": [
      "Xirui Li",
      "Hengguang Zhou",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Minhao Cheng",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Humans are prone to cognitive distortions -- biased thinking patterns that\nlead to exaggerated responses to specific stimuli, albeit in very different\ncontexts. This paper demonstrates that advanced Multimodal Large Language\nModels (MLLMs) exhibit similar tendencies. While these models are designed to\nrespond queries under safety mechanism, they sometimes reject harmless queries\nin the presence of certain visual stimuli, disregarding the benign nature of\ntheir contexts. As the initial step in investigating this behavior, we identify\nthree types of stimuli that trigger the oversensitivity of existing MLLMs:\nExaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To\nsystematically evaluate MLLMs' oversensitivity to these stimuli, we propose the\nMultimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300\nmanually collected benign multimodal queries, cross-verified by third-party\nreviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several\ninsights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal\nrates reaching up to 76% for harmless queries. (2). Safer models are more\noversensitive: increasing safety may inadvertently raise caution and\nconservatism in the model's responses. (3). Different types of stimuli tend to\ncause errors at specific stages -- perception, intent reasoning, and safety\njudgement -- in the response process of MLLMs. These findings highlight the\nneed for refined safety mechanisms that balance caution with contextually\nappropriate responses, improving the reliability of MLLMs in real-world\napplications. We make our project available at\nhttps://turningpoint-ai.github.io/MOSSBench/.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17806v1",
    "published_date": "2024-06-22 23:26:07 UTC",
    "updated_date": "2024-06-22 23:26:07 UTC"
  },
  {
    "arxiv_id": "2406.17805v1",
    "title": "Can LLMs Generate Visualizations with Dataless Prompts?",
    "authors": [
      "Darius Coelho",
      "Harshit Barot",
      "Naitik Rathod",
      "Klaus Mueller"
    ],
    "abstract": "Recent advancements in large language models have revolutionized information\naccess, as these models harness data available on the web to address complex\nqueries, becoming the preferred information source for many users. In certain\ncases, queries are about publicly available data, which can be effectively\nanswered with data visualizations. In this paper, we investigate the ability of\nlarge language models to provide accurate data and relevant visualizations in\nresponse to such queries. Specifically, we investigate the ability of GPT-3 and\nGPT-4 to generate visualizations with dataless prompts, where no data\naccompanies the query. We evaluate the results of the models by comparing them\nto visualization cheat sheets created by visualization experts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17805v1",
    "published_date": "2024-06-22 22:59:09 UTC",
    "updated_date": "2024-06-22 22:59:09 UTC"
  },
  {
    "arxiv_id": "2406.15955v3",
    "title": "Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects",
    "authors": [
      "Michael A. Lepori",
      "Alexa R. Tartaglini",
      "Wai Keen Vong",
      "Thomas Serre",
      "Brenden M. Lake",
      "Ellie Pavlick"
    ],
    "abstract": "Though vision transformers (ViTs) have achieved state-of-the-art performance\nin a variety of settings, they exhibit surprising failures when performing\ntasks involving visual relations. This begs the question: how do ViTs attempt\nto perform tasks that require computing visual relations between objects? Prior\nefforts to interpret ViTs tend to focus on characterizing relevant low-level\nvisual features. In contrast, we adopt methods from mechanistic\ninterpretability to study the higher-level visual algorithms that ViTs use to\nperform abstract visual reasoning. We present a case study of a fundamental,\nyet surprisingly difficult, relational reasoning task: judging whether two\nvisual entities are the same or different. We find that pretrained ViTs\nfine-tuned on this task often exhibit two qualitatively different stages of\nprocessing despite having no obvious inductive biases to do so: 1) a perceptual\nstage wherein local object features are extracted and stored in a disentangled\nrepresentation, and 2) a relational stage wherein object representations are\ncompared. In the second stage, we find evidence that ViTs can learn to\nrepresent somewhat abstract visual relations, a capability that has long been\nconsidered out of reach for artificial neural networks. Finally, we demonstrate\nthat failures at either stage can prevent a model from learning a generalizable\nsolution to our fairly simple tasks. By understanding ViTs in terms of discrete\nprocessing stages, one can more precisely diagnose and rectify shortcomings of\nexisting and future models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15955v3",
    "published_date": "2024-06-22 22:43:10 UTC",
    "updated_date": "2024-11-22 19:16:50 UTC"
  },
  {
    "arxiv_id": "2406.15946v2",
    "title": "Optimizing LaneSegNet for Real-Time Lane Topology Prediction in Autonomous Vehicles",
    "authors": [
      "William Stevens",
      "Vishal Urs",
      "Karthik Selvaraj",
      "Gabriel Torres",
      "Gaurish Lakhanpal"
    ],
    "abstract": "With the increasing prevalence of autonomous vehicles, it is essential for\ncomputer vision algorithms to accurately assess road features in real-time.\nThis study explores the LaneSegNet architecture, a new approach to lane\ntopology prediction which integrates topological information with lane-line\ndata to provide a more contextual understanding of road environments. The\nLaneSegNet architecture includes a feature extractor, lane encoder, lane\ndecoder, and prediction head, leveraging components from ResNet-50, BEVFormer,\nand various attention mechanisms. We experimented with optimizations to the\nLaneSegNet architecture through feature extractor modification and transformer\nencoder-decoder stack modification. We found that modifying the encoder and\ndecoder stacks offered an interesting tradeoff between training time and\nprediction accuracy, with certain combinations showing promising results. Our\nimplementation, trained on a single NVIDIA Tesla A100 GPU, found that a 2:4\nratio reduced training time by 22.3% with only a 7.1% drop in mean average\nprecision, while a 4:8 ratio increased training time by only 11.1% but improved\nmean average precision by a significant 23.7%. These results indicate that\nstrategic hyperparameter tuning can yield substantial improvements depending on\nthe resources of the user. This study provides valuable insights for optimizing\nLaneSegNet according to available computation power, making it more accessible\nfor users with limited resources and increasing the capabilities for users with\nmore powerful resources.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.15946v2",
    "published_date": "2024-06-22 21:49:12 UTC",
    "updated_date": "2024-07-30 20:15:10 UTC"
  },
  {
    "arxiv_id": "2406.15940v1",
    "title": "Beyond Individual Facts: Investigating Categorical Knowledge Locality of Taxonomy and Meronomy Concepts in GPT Models",
    "authors": [
      "Christopher Burger",
      "Yifan Hu",
      "Thai Le"
    ],
    "abstract": "The location of knowledge within Generative Pre-trained Transformer\n(GPT)-like models has seen extensive recent investigation. However, much of the\nwork is focused towards determining locations of individual facts, with the end\ngoal being the editing of facts that are outdated, erroneous, or otherwise\nharmful, without the time and expense of retraining the entire model. In this\nwork, we investigate a broader view of knowledge location, that of concepts or\nclusters of related information, instead of disparate individual facts. To do\nthis, we first curate a novel dataset, called DARC, that includes a total of 34\nconcepts of ~120K factual statements divided into two types of hierarchical\ncategories, namely taxonomy and meronomy. Next, we utilize existing causal\nmediation analysis methods developed for determining regions of importance for\nindividual facts and apply them to a series of related categories to provide\ndetailed investigation into whether concepts are associated with distinct\nregions within these models. We find that related categories exhibit similar\nareas of importance in contrast to less similar categories. However,\nfine-grained localization of individual category subsets to specific regions is\nnot apparent.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "27 pages, 23 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.15940v1",
    "published_date": "2024-06-22 21:12:57 UTC",
    "updated_date": "2024-06-22 21:12:57 UTC"
  },
  {
    "arxiv_id": "2406.15938v4",
    "title": "RuleR: Improving LLM Controllability by Rule-based Data Recycling",
    "authors": [
      "Ming Li",
      "Han Chen",
      "Chenguang Wang",
      "Dang Nguyen",
      "Dianqi Li",
      "Tianyi Zhou"
    ],
    "abstract": "Large language models (LLMs) still lack delicate controllability over their\nresponses, which is critical to enhancing their performance and the user\nexperience. However, curating supervised fine-tuning (SFT) datasets to improve\nLLM controllability usually relies on human experts or proprietary LLMs, which\nrequires additional costs. To bridge this gap, we propose Rule-based Data\nRecycling (RuleR), a data augmentation method incorporating multiple\nconstraints into the original data samples according to predefined rules, which\ncreates new training tasks to consolidate the controllability of LLMs. Instead\nof creating new data from scratch, RuleR \"recycles\" existing data by simply\napplying rule-based edits to their responses and appending the\nrule-instructions in their original instructions. Experimental results\ndemonstrate RuleR's effectiveness in improving LLM controllability while\nmaintaining general instruction-following capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL2025 main, Camera-ready",
    "pdf_url": "http://arxiv.org/pdf/2406.15938v4",
    "published_date": "2024-06-22 20:57:12 UTC",
    "updated_date": "2025-02-15 20:33:41 UTC"
  },
  {
    "arxiv_id": "2406.15936v1",
    "title": "An Automated SQL Query Grading System Using An Attention-Based Convolutional Neural Network",
    "authors": [
      "Donald R. Schwartz",
      "Pablo Rivas"
    ],
    "abstract": "Grading SQL queries can be a time-consuming, tedious and challenging task,\nespecially as the number of student submissions increases. Several systems have\nbeen introduced in an attempt to mitigate these challenges, but those systems\nhave their own limitations. This paper describes our novel approach to\nautomating the process of grading SQL queries. Unlike previous approaches, we\nemploy a unique convolutional neural network architecture that employs a\nparameter-sharing approach for different machine learning tasks that enables\nthe architecture to induce different knowledge representations of the data to\nincrease its potential for understanding SQL statements.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DB",
      "cs.LG",
      "I.2.6; H.2.3; K.3.2"
    ],
    "primary_category": "cs.CY",
    "comment": "12 pages, 8 figures, paper accepted at \"The 18th International\n  Conference on Frontiers in Education: Computer Science and Computer\n  Engineering\"",
    "pdf_url": "http://arxiv.org/pdf/2406.15936v1",
    "published_date": "2024-06-22 20:52:17 UTC",
    "updated_date": "2024-06-22 20:52:17 UTC"
  },
  {
    "arxiv_id": "2406.15927v1",
    "title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in LLMs",
    "authors": [
      "Jannik Kossen",
      "Jiatong Han",
      "Muhammed Razzak",
      "Lisa Schut",
      "Shreshth Malik",
      "Yarin Gal"
    ],
    "abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for\nuncertainty quantification in Large Language Models (LLMs). Hallucinations,\nwhich are plausible-sounding but factually incorrect and arbitrary model\ngenerations, present a major challenge to the practical adoption of LLMs.\nRecent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can\ndetect hallucinations by estimating uncertainty in the space semantic meaning\nfor a set of model generations. However, the 5-to-10-fold increase in\ncomputation cost associated with SE computation hinders practical adoption. To\naddress this, we propose SEPs, which directly approximate SE from the hidden\nstates of a single generation. SEPs are simple to train and do not require\nsampling multiple model generations at test time, reducing the overhead of\nsemantic uncertainty quantification to almost zero. We show that SEPs retain\nhigh performance for hallucination detection and generalize better to\nout-of-distribution data than previous probing methods that directly predict\nmodel accuracy. Our results across models and tasks suggest that model hidden\nstates capture SE, and our ablation studies give further insights into the\ntoken positions and model layers for which this is the case.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "First three authors contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2406.15927v1",
    "published_date": "2024-06-22 19:46:06 UTC",
    "updated_date": "2024-06-22 19:46:06 UTC"
  },
  {
    "arxiv_id": "2407.16900v1",
    "title": "Regulating AI Adaptation: An Analysis of AI Medical Device Updates",
    "authors": [
      "Kevin Wu",
      "Eric Wu",
      "Kit Rodolfa",
      "Daniel E. Ho",
      "James Zou"
    ],
    "abstract": "While the pace of development of AI has rapidly progressed in recent years,\nthe implementation of safe and effective regulatory frameworks has lagged\nbehind. In particular, the adaptive nature of AI models presents unique\nchallenges to regulators as updating a model can improve its performance but\nalso introduce safety risks. In the US, the Food and Drug Administration (FDA)\nhas been a forerunner in regulating and approving hundreds of AI medical\ndevices. To better understand how AI is updated and its regulatory\nconsiderations, we systematically analyze the frequency and nature of updates\nin FDA-approved AI medical devices. We find that less than 2% of all devices\nreport having been updated by being re-trained on new data. Meanwhile, nearly a\nquarter of devices report updates in the form of new functionality and\nmarketing claims. As an illustrative case study, we analyze pneumothorax\ndetection models and find that while model performance can degrade by as much\nas 0.18 AUC when evaluated on new sites, re-training on site-specific data can\nmitigate this performance drop, recovering up to 0.23 AUC. However, we also\nobserved significant degradation on the original site after re-training using\ndata from new sites, providing insight from one example that challenges the\ncurrent one-model-fits-all approach to regulatory approvals. Our analysis\nprovides an in-depth look at the current state of FDA-approved AI device\nupdates and insights for future regulatory policies toward model updating and\nadaptive AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.16900v1",
    "published_date": "2024-06-22 19:44:47 UTC",
    "updated_date": "2024-06-22 19:44:47 UTC"
  },
  {
    "arxiv_id": "2406.15920v4",
    "title": "SEDMamba: Enhancing Selective State Space Modelling with Bottleneck Mechanism and Fine-to-Coarse Temporal Fusion for Efficient Error Detection in Robot-Assisted Surgery",
    "authors": [
      "Jialang Xu",
      "Nazir Sirajudeen",
      "Matthew Boal",
      "Nader Francis",
      "Danail Stoyanov",
      "Evangelos Mazomenos"
    ],
    "abstract": "Automated detection of surgical errors can improve robotic-assisted surgery.\nDespite promising progress, existing methods still face challenges in capturing\nrich temporal context to establish long-term dependencies while maintaining\ncomputational efficiency. In this paper, we propose a novel hierarchical model\nnamed SEDMamba, which incorporates the selective state space model (SSM) into\nsurgical error detection, facilitating efficient long sequence modelling with\nlinear complexity. SEDMamba enhances selective SSM with a bottleneck mechanism\nand fine-to-coarse temporal fusion (FCTF) to detect and temporally localize\nsurgical errors in long videos. The bottleneck mechanism compresses and\nrestores features within their spatial dimension, thereby reducing\ncomputational complexity. FCTF utilizes multiple dilated 1D convolutional\nlayers to merge temporal information across diverse scale ranges, accommodating\nerrors of varying duration. Our work also contributes the first-of-its-kind,\nframe-level, in-vivo surgical error dataset to support error detection in real\nsurgical cases. Specifically, we deploy the clinically validated observational\nclinical human reliability assessment tool (OCHRA) to annotate the errors\nduring suturing tasks in an open-source radical prostatectomy dataset\n(SAR-RARP50). Experimental results demonstrate that our SEDMamba outperforms\nstate-of-the-art methods with at least 1.82% AUC and 3.80% AP performance gains\nwith significantly reduced computational complexity. The corresponding error\nannotations, code and models are released at\nhttps://github.com/wzjialang/SEDMamba.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE RA-L",
    "pdf_url": "http://arxiv.org/pdf/2406.15920v4",
    "published_date": "2024-06-22 19:20:35 UTC",
    "updated_date": "2024-11-29 20:34:28 UTC"
  },
  {
    "arxiv_id": "2406.15906v1",
    "title": "OpticGAI: Generative AI-aided Deep Reinforcement Learning for Optical Networks Optimization",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Yaju Liu",
      "Gaolei Li",
      "Jianhua Li"
    ],
    "abstract": "Deep Reinforcement Learning (DRL) is regarded as a promising tool for optical\nnetwork optimization. However, the flexibility and efficiency of current\nDRL-based solutions for optical network optimization require further\nimprovement. Currently, generative models have showcased their significant\nperformance advantages across various domains. In this paper, we introduce\nOpticGAI, the AI-generated policy design paradigm for optical networks. In\ndetail, it is implemented as a novel DRL framework that utilizes generative\nmodels to learn the optimal policy network. Furthermore, we assess the\nperformance of OpticGAI on two NP-hard optical network problems, Routing and\nWavelength Assignment (RWA) and dynamic Routing, Modulation, and Spectrum\nAllocation (RMSA), to show the feasibility of the AI-generated policy paradigm.\nSimulation results have shown that OpticGAI achieves the highest reward and the\nlowest blocking rate of both RWA and RMSA problems. OpticGAI poses a promising\ndirection for future research on generative AI-enhanced flexible optical\nnetwork optimization.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "Accepted by ACM SIGCOMM 2024 Workshop on Hot Topics in Optical\n  Technologies and Applications in Networking",
    "pdf_url": "http://arxiv.org/pdf/2406.15906v1",
    "published_date": "2024-06-22 17:59:50 UTC",
    "updated_date": "2024-06-22 17:59:50 UTC"
  },
  {
    "arxiv_id": "2406.15890v1",
    "title": "Language Alignment via Nash-learning and Adaptive feedback",
    "authors": [
      "Ari Azarafrooz",
      "Farshid Faal"
    ],
    "abstract": "Recent research has shown the potential of Nash Learning via Human Feedback\nfor large language model alignment by incorporating the notion of a preference\nmodel in a minimax game setup. We take this idea further by casting the\nalignment as a mirror descent algorithm against the adaptive feedback of an\nimproved opponent, thereby removing the need for learning a preference model or\nthe existence of an annotated dataset altogether. The resulting algorithm,\nwhich we refer to as Language Alignment via Nash-learning and Adaptive feedback\n(LANA), is capable of self-alignment without the need for a human-annotated\npreference dataset. We support this statement with various experiments and\nmathematical discussion.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024 Workshop on Models of Human Feedback for AI\n  Alignment, Vienna, Austria",
    "pdf_url": "http://arxiv.org/pdf/2406.15890v1",
    "published_date": "2024-06-22 16:55:21 UTC",
    "updated_date": "2024-06-22 16:55:21 UTC"
  },
  {
    "arxiv_id": "2406.15888v2",
    "title": "Real-time Speech Summarization for Medical Conversations",
    "authors": [
      "Khai Le-Duc",
      "Khai-Nguyen Nguyen",
      "Long Vo-Dang",
      "Truong-Son Hy"
    ],
    "abstract": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Interspeech 2024 (Oral)",
    "pdf_url": "http://arxiv.org/pdf/2406.15888v2",
    "published_date": "2024-06-22 16:37:51 UTC",
    "updated_date": "2025-04-04 14:12:54 UTC"
  },
  {
    "arxiv_id": "2406.15885v1",
    "title": "The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models",
    "authors": [
      "Jiajia Li",
      "Lu Yang",
      "Mingni Tang",
      "Cong Chen",
      "Zuchao Li",
      "Ping Wang",
      "Hai Zhao"
    ],
    "abstract": "Benchmark plays a pivotal role in assessing the advancements of large\nlanguage models (LLMs). While numerous benchmarks have been proposed to\nevaluate LLMs' capabilities, there is a notable absence of a dedicated\nbenchmark for assessing their musical abilities. To address this gap, we\npresent ZIQI-Eval, a comprehensive and large-scale music benchmark specifically\ndesigned to evaluate the music-related capabilities of LLMs. ZIQI-Eval\nencompasses a wide range of questions, covering 10 major categories and 56\nsubcategories, resulting in over 14,000 meticulously curated data entries. By\nleveraging ZIQI-Eval, we conduct a comprehensive evaluation over 16 LLMs to\nevaluate and analyze LLMs' performance in the domain of music. Results indicate\nthat all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant\nroom for improvement in their musical capabilities. With ZIQI-Eval, we aim to\nprovide a standardized and robust evaluation framework that facilitates a\ncomprehensive assessment of LLMs' music-related abilities. The dataset is\navailable at GitHub\\footnote{https://github.com/zcli-charlie/ZIQI-Eval} and\nHuggingFace\\footnote{https://huggingface.co/datasets/MYTH-Lab/ZIQI-Eval}.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to ACL-Findings 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15885v1",
    "published_date": "2024-06-22 16:24:42 UTC",
    "updated_date": "2024-06-22 16:24:42 UTC"
  },
  {
    "arxiv_id": "2406.15883v1",
    "title": "SimSMoE: Solving Representational Collapse via Similarity Measure",
    "authors": [
      "Giang Do",
      "Hung Le",
      "Truyen Tran"
    ],
    "abstract": "Sparse mixture of experts (SMoE) have emerged as an effective approach for\nscaling large language models while keeping a constant computational cost.\nRegardless of several notable successes of SMoE, effective training such\narchitecture remains elusive due to the representation collapse problem, which\nin turn harms model performance and causes parameter redundancy. In this work,\nwe present Similarity-based Sparse Mixture of Experts (SimSMoE), a novel\nsimilarity of neural network algorithm, that guarantees a solution to address\nthe representation collapse issue between experts given a fixed FLOPs budget.\nWe conduct extensive empirical evaluations on three large language models for\nboth Pre-training and Fine-tuning tasks to illustrate the efficacy, robustness,\nand scalability of our method. The results demonstrate that SimSMoE\nsignificantly enhances existing routing policy and outperforms other SMoE\ntraining methods in performance for the tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15883v1",
    "published_date": "2024-06-22 16:10:45 UTC",
    "updated_date": "2024-06-22 16:10:45 UTC"
  },
  {
    "arxiv_id": "2406.15881v2",
    "title": "Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers",
    "authors": [
      "Krzysztof Choromanski",
      "Arijit Sehanobish",
      "Somnath Basu Roy Chowdhury",
      "Han Lin",
      "Avinava Dubey",
      "Tamas Sarlos",
      "Snigdha Chaturvedi"
    ],
    "abstract": "We present a new class of fast polylog-linear algorithms based on the theory\nof structured matrices (in particular low displacement rank) for integrating\ntensor fields defined on weighted trees. Several applications of the resulting\nfast tree-field integrators (FTFIs) are presented, including (a) approximation\nof graph metrics with tree metrics, (b) graph classification, (c) modeling on\nmeshes, and finally (d) Topological Transformers (TTs) (Choromanski et al.,\n2022) for images. For Topological Transformers, we propose new relative\nposition encoding (RPE) masking mechanisms with as few as three extra learnable\nparameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains.\nImportantly, most of FTFIs are exact methods, thus numerically equivalent to\ntheir brute-force counterparts. When applied to graphs with thousands of nodes,\nthose exact algorithms provide 5.7-13x speedups. We also provide an extensive\ntheoretical analysis of our methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15881v2",
    "published_date": "2024-06-22 16:05:34 UTC",
    "updated_date": "2024-12-06 18:41:56 UTC"
  },
  {
    "arxiv_id": "2406.15877v4",
    "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
    "authors": [
      "Terry Yue Zhuo",
      "Minh Chien Vu",
      "Jenny Chim",
      "Han Hu",
      "Wenhao Yu",
      "Ratnadira Widyasari",
      "Imam Nur Bani Yusuf",
      "Haolan Zhan",
      "Junda He",
      "Indraneil Paul",
      "Simon Brunner",
      "Chen Gong",
      "Thong Hoang",
      "Armel Randy Zebaze",
      "Xiaoheng Hong",
      "Wen-Ding Li",
      "Jean Kaddour",
      "Ming Xu",
      "Zhihan Zhang",
      "Prateek Yadav",
      "Naman Jain",
      "Alex Gu",
      "Zhoujun Cheng",
      "Jiawei Liu",
      "Qian Liu",
      "Zijian Wang",
      "Binyuan Hui",
      "Niklas Muennighoff",
      "David Lo",
      "Daniel Fried",
      "Xiaoning Du",
      "Harm de Vries",
      "Leandro Von Werra"
    ],
    "abstract": "Task automation has been greatly empowered by the recent advances in Large\nLanguage Models (LLMs) via Python code, where the tasks ranging from software\nengineering development to general-purpose reasoning. While current benchmarks\nhave shown that LLMs can solve tasks using programs like human developers, the\nmajority of their evaluations are limited to short and self-contained\nalgorithmic tasks or standalone function calls. Solving challenging and\npractical tasks requires the capability of utilizing diverse function calls as\ntools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.To\nassess how well LLMs can solve challenging and practical tasks via programs, we\nintroduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\ntasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with\nan average branch coverage of 99%. In addition, we propose a\nnatural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "Accpeted at ICLR 2025 (Oral), built with love by the BigCode\n  community :)",
    "pdf_url": "http://arxiv.org/pdf/2406.15877v4",
    "published_date": "2024-06-22 15:52:04 UTC",
    "updated_date": "2025-04-01 08:36:44 UTC"
  },
  {
    "arxiv_id": "2406.16972v1",
    "title": "An Efficient NAS-based Approach for Handling Imbalanced Datasets",
    "authors": [
      "Zhiwei Yao"
    ],
    "abstract": "Class imbalance is a common issue in real-world data distributions,\nnegatively impacting the training of accurate classifiers. Traditional\napproaches to mitigate this problem fall into three main categories: class\nre-balancing, information transfer, and representation learning. This paper\nintroduces a novel approach to enhance performance on long-tailed datasets by\noptimizing the backbone architecture through neural architecture search (NAS).\nOur research shows that an architecture's accuracy on a balanced dataset does\nnot reliably predict its performance on imbalanced datasets. This necessitates\na complete NAS run on long-tailed datasets, which can be computationally\nexpensive. To address this computational challenge, we focus on existing work,\ncalled IMB-NAS, which proposes efficiently adapting a NAS super-network trained\non a balanced source dataset to an imbalanced target dataset. A detailed\ndescription of the fundamental techniques for IMB-NAS is provided in this\npaper, including NAS and architecture transfer. Among various adaptation\nstrategies, we find that the most effective approach is to retrain the linear\nclassification head with reweighted loss while keeping the backbone NAS\nsuper-network trained on the balanced source dataset frozen. Finally, we\nconducted a series of experiments on the imbalanced CIFAR dataset for\nperformance evaluation. Our conclusions are the same as those proposed in the\nIMB-NAS paper.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages,3 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.16972v1",
    "published_date": "2024-06-22 15:46:03 UTC",
    "updated_date": "2024-06-22 15:46:03 UTC"
  },
  {
    "arxiv_id": "2406.15875v2",
    "title": "AI-based Drone Assisted Human Rescue in Disaster Environments: Challenges and Opportunities",
    "authors": [
      "Narek Papyan",
      "Michel Kulhandjian",
      "Hovannes Kulhandjian",
      "Levon Hakob Aslanyan"
    ],
    "abstract": "In this survey we are focusing on utilizing drone-based systems for the\ndetection of individuals, particularly by identifying human screams and other\ndistress signals. This study has significant relevance in post-disaster\nscenarios, including events such as earthquakes, hurricanes, military\nconflicts, wildfires, and more. These drones are capable of hovering over\ndisaster-stricken areas that may be challenging for rescue teams to access\ndirectly. Unmanned aerial vehicles (UAVs), commonly referred to as drones, are\nfrequently deployed for search-and-rescue missions during disaster situations.\nTypically, drones capture aerial images to assess structural damage and\nidentify the extent of the disaster. They also employ thermal imaging\ntechnology to detect body heat signatures, which can help locate individuals.\nIn some cases, larger drones are used to deliver essential supplies to people\nstranded in isolated disaster-stricken areas. In our discussions, we delve into\nthe unique challenges associated with locating humans through aerial acoustics.\nThe auditory system must distinguish between human cries and sounds that occur\nnaturally, such as animal calls and wind. Additionally, it should be capable of\nrecognizing distinct patterns related to signals like shouting, clapping, or\nother ways in which people attempt to signal rescue teams. To tackle this\nchallenge, one solution involves harnessing artificial intelligence (AI) to\nanalyze sound frequencies and identify common audio signatures. Deep\nlearning-based networks, such as convolutional neural networks (CNNs), can be\ntrained using these signatures to filter out noise generated by drone motors\nand other environmental factors. Furthermore, employing signal processing\ntechniques like the direction of arrival (DOA) based on microphone array\nsignals can enhance the precision of tracking the source of human noises.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "68U10, 68T50(Primary) 68T45 (Secondary)",
      "I.2.7; I.2.10; I.4.0"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15875v2",
    "published_date": "2024-06-22 15:39:46 UTC",
    "updated_date": "2024-07-12 20:34:34 UTC"
  },
  {
    "arxiv_id": "2407.10987v1",
    "title": "Adaptive Digital Twin and Communication-Efficient Federated Learning Network Slicing for 5G-enabled Internet of Things",
    "authors": [
      "Daniel Ayepah-Mensah",
      "Guolin Sun",
      "Yu Pang",
      "Wei Jiang"
    ],
    "abstract": "Network slicing enables industrial Internet of Things (IIoT) networks with\nmultiservice and differentiated resource requirements to meet increasing\ndemands through efficient use and management of network resources. Typically,\nthe network slice orchestrator relies on demand forecasts for each slice to\nmake informed decisions and maximize resource utilization. The new generation\nof Industry 4.0 has introduced digital twins to map physical systems to digital\nmodels for accurate decision-making. In our approach, we first use\ngraph-attention networks to build a digital twin environment for network\nslices, enabling real-time traffic analysis, monitoring, and demand\nforecasting. Based on these predictions, we formulate the resource allocation\nproblem as a federated multi-agent reinforcement learning problem and employ a\ndeep deterministic policy gradient to determine the resource allocation policy\nwhile preserving the privacy of the slices. Our results demonstrate that the\nproposed approaches can improve the accuracy of demand prediction for network\nslices and reduce the communication overhead of dynamic network slicing.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.NI",
    "comment": "8 pages, 7 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2407.10987v1",
    "published_date": "2024-06-22 15:33:35 UTC",
    "updated_date": "2024-06-22 15:33:35 UTC"
  },
  {
    "arxiv_id": "2406.17804v3",
    "title": "A Review of Electromagnetic Elimination Methods for low-field portable MRI scanner",
    "authors": [
      "Wanyu Bian",
      "Panfeng Li",
      "Mengyao Zheng",
      "Chihang Wang",
      "Anying Li",
      "Ying Li",
      "Haowei Ni",
      "Zixuan Zeng"
    ],
    "abstract": "This paper analyzes conventional and deep learning methods for eliminating\nelectromagnetic interference (EMI) in MRI systems. We compare traditional\nanalytical and adaptive techniques with advanced deep learning approaches. Key\nstrengths and limitations of each method are highlighted. Recent advancements\nin active EMI elimination, such as external EMI receiver coils, are discussed\nalongside deep learning methods, which show superior EMI suppression by\nleveraging neural networks trained on MRI data. While deep learning improves\nEMI elimination and diagnostic capabilities, it introduces security and safety\nconcerns, particularly in commercial applications. A balanced approach,\nintegrating conventional reliability with deep learning's advanced\ncapabilities, is proposed for more effective EMI suppression in MRI systems.",
    "categories": [
      "physics.med-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "physics.med-ph",
    "comment": "Accepted by 2024 5th International Conference on Machine Learning and\n  Computer Application",
    "pdf_url": "http://arxiv.org/pdf/2406.17804v3",
    "published_date": "2024-06-22 15:24:33 UTC",
    "updated_date": "2024-11-13 09:50:48 UTC"
  },
  {
    "arxiv_id": "2406.15871v1",
    "title": "Uncovering Hidden Intentions: Exploring Prompt Recovery for Deeper Insights into Generated Texts",
    "authors": [
      "Louis Give",
      "Timo Zaoral",
      "Maria Antonietta Bruno"
    ],
    "abstract": "Today, the detection of AI-generated content is receiving more and more\nattention. Our idea is to go beyond detection and try to recover the prompt\nused to generate a text. This paper, to the best of our knowledge, introduces\nthe first investigation in this particular domain without a closed set of\ntasks. Our goal is to study if this approach is promising. We experiment with\nzero-shot and few-shot in-context learning but also with LoRA fine-tuning.\nAfter that, we evaluate the benefits of using a semi-synthetic dataset. For\nthis first study, we limit ourselves to text generated by a single model. The\nresults show that it is possible to recover the original prompt with a\nreasonable degree of accuracy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at WNNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15871v1",
    "published_date": "2024-06-22 15:16:11 UTC",
    "updated_date": "2024-06-22 15:16:11 UTC"
  },
  {
    "arxiv_id": "2406.17803v1",
    "title": "Understanding the Role of User Profile in the Personalization of Large Language Models",
    "authors": [
      "Bin Wu",
      "Zhengyan Shi",
      "Hossein A. Rahmani",
      "Varsha Ramineni",
      "Emine Yilmaz"
    ],
    "abstract": "Utilizing user profiles to personalize Large Language Models (LLMs) has been\nshown to enhance the performance on a wide range of tasks. However, the precise\nrole of user profiles and their effect mechanism on LLMs remains unclear. This\nstudy first confirms that the effectiveness of user profiles is primarily due\nto personalization information rather than semantic information. Furthermore,\nwe investigate how user profiles affect the personalization of LLMs. Within the\nuser profile, we reveal that it is the historical personalized response\nproduced or approved by users that plays a pivotal role in personalizing LLMs.\nThis discovery unlocks the potential of LLMs to incorporate a greater number of\nuser profiles within the constraints of limited input length. As for the\nposition of user profiles, we observe that user profiles integrated into\ndifferent positions of the input context do not contribute equally to\npersonalization. Instead, where the user profile that is closer to the\nbeginning affects more on the personalization of LLMs. Our findings reveal the\nrole of user profiles for the personalization of LLMs, and showcase how\nincorporating user profiles impacts performance providing insight to leverage\nuser profiles effectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.17803v1",
    "published_date": "2024-06-22 14:32:35 UTC",
    "updated_date": "2024-06-22 14:32:35 UTC"
  },
  {
    "arxiv_id": "2406.15859v2",
    "title": "LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning",
    "authors": [
      "Guangsi Shi",
      "Xiaofeng Deng",
      "Linhao Luo",
      "Lijuan Xia",
      "Lei Bao",
      "Bei Ye",
      "Fei Du",
      "Shirui Pan",
      "Yuxiao Li"
    ],
    "abstract": "Recommender systems are pivotal in enhancing user experiences across various\nweb applications by analyzing the complicated relationships between users and\nitems. Knowledge graphs(KGs) have been widely used to enhance the performance\nof recommender systems. However, KGs are known to be noisy and incomplete,\nwhich are hard to provide reliable explanations for recommendation results. An\nexplainable recommender system is crucial for the product development and\nsubsequent decision-making. To address these challenges, we introduce a novel\nrecommender that synergies Large Language Models (LLMs) and KGs to enhance the\nrecommendation and provide interpretable results. Specifically, we first\nharness the power of LLMs to augment KG reconstruction. LLMs comprehend and\ndecompose user reviews into new triples that are added into KG. In this way, we\ncan enrich KGs with explainable paths that express user preferences. To enhance\nthe recommendation on augmented KGs, we introduce a novel subgraph reasoning\nmodule that effectively measures the importance of nodes and discovers\nreasoning for recommendation. Finally, these reasoning paths are fed into the\nLLMs to generate interpretable explanations of the recommendation results. Our\napproach significantly enhances both the effectiveness and interpretability of\nrecommender systems, especially in cross-selling scenarios where traditional\nmethods falter. The effectiveness of our approach has been rigorously tested on\nfour open real-world datasets, with our methods demonstrating a superior\nperformance over contemporary state-of-the-art techniques by an average\nimprovement of 12%. The application of our model in a multinational engineering\nand technology company cross-selling recommendation system further underscores\nits practical utility and potential to redefine recommendation practices\nthrough improved accuracy and user trust.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15859v2",
    "published_date": "2024-06-22 14:14:03 UTC",
    "updated_date": "2024-06-30 02:13:19 UTC"
  },
  {
    "arxiv_id": "2406.15852v2",
    "title": "Next Level Message-Passing with Hierarchical Support Graphs",
    "authors": [
      "Carlos Vonessen",
      "Florian Grötschla",
      "Roger Wattenhofer"
    ],
    "abstract": "Message-Passing Neural Networks (MPNNs) are extensively employed in graph\nlearning tasks but suffer from limitations such as the restricted scope of\ninformation exchange, by being confined to neighboring nodes during each round\nof message passing. Various strategies have been proposed to address these\nlimitations, including incorporating virtual nodes to facilitate global\ninformation exchange. In this study, we introduce the Hierarchical Support\nGraph (HSG), an extension of the virtual node concept created through recursive\ncoarsening of the original graph. This approach provides a flexible framework\nfor enhancing information flow in graphs, independent of the specific MPNN\nlayers utilized. We present a theoretical analysis of HSGs, investigate their\nempirical performance, and demonstrate that HSGs can surpass other methods\naugmented with virtual nodes, achieving state-of-the-art results across\nmultiple datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15852v2",
    "published_date": "2024-06-22 13:57:09 UTC",
    "updated_date": "2024-08-29 10:28:42 UTC"
  },
  {
    "arxiv_id": "2406.15850v1",
    "title": "Learning Abstract World Model for Value-preserving Planning with Options",
    "authors": [
      "Rafael Rodriguez-Sanchez",
      "George Konidaris"
    ],
    "abstract": "General-purpose agents require fine-grained controls and rich sensory inputs\nto perform a wide range of tasks. However, this complexity often leads to\nintractable decision-making. Traditionally, agents are provided with\ntask-specific action and observation spaces to mitigate this challenge, but\nthis reduces autonomy. Instead, agents must be capable of building state-action\nspaces at the correct abstraction level from their sensorimotor experiences. We\nleverage the structure of a given set of temporally-extended actions to learn\nabstract Markov decision processes (MDPs) that operate at a higher level of\ntemporal and state granularity. We characterize state abstractions necessary to\nensure that planning with these skills, by simulating trajectories in the\nabstract MDP, results in policies with bounded value loss in the original MDP.\nWe evaluate our approach in goal-based navigation environments that require\ncontinuous abstract states to plan successfully and show that abstract model\nlearning improves the sample efficiency of planning and learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in the Proceedings of Reinforcement Learning Conference",
    "pdf_url": "http://arxiv.org/pdf/2406.15850v1",
    "published_date": "2024-06-22 13:41:02 UTC",
    "updated_date": "2024-06-22 13:41:02 UTC"
  },
  {
    "arxiv_id": "2406.15847v2",
    "title": "Enhancing Solar Driver Forecasting with Multivariate Transformers",
    "authors": [
      "Sergio Sanchez-Hurtado",
      "Victor Rodriguez-Fernandez",
      "Julia Briden",
      "Peng Mun Siew",
      "Richard Linares"
    ],
    "abstract": "In this work, we develop a comprehensive framework for F10.7, S10.7, M10.7,\nand Y10.7 solar driver forecasting with a time series Transformer (PatchTST).\nTo ensure an equal representation of high and low levels of solar activity, we\nconstruct a custom loss function to weight samples based on the distance\nbetween the solar driver's historical distribution and the training set. The\nsolar driver forecasting framework includes an 18-day lookback window and\nforecasts 6 days into the future. When benchmarked against the Space\nEnvironment Technologies (SET) dataset, our model consistently produces\nforecasts with a lower standard mean error in nearly all cases, with improved\nprediction accuracy during periods of high solar activity. All the code is\navailable on Github https://github.com/ARCLab-MIT/sw-driver-forecaster.",
    "categories": [
      "physics.space-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.space-ph",
    "comment": "Short paper accepted for oral presentation at the SPAICE Conference\n  2024 (https://spaice.esa.int/)",
    "pdf_url": "http://arxiv.org/pdf/2406.15847v2",
    "published_date": "2024-06-22 13:26:14 UTC",
    "updated_date": "2024-08-01 20:54:46 UTC"
  },
  {
    "arxiv_id": "2407.02514v3",
    "title": "LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations",
    "authors": [
      "Shashank Kirtania",
      "Priyanshu Gupta",
      "Arjun Radhakirshna"
    ],
    "abstract": "In this paper we examine the limitations of Large Language Models (LLMs) for\ncomplex reasoning tasks. Although recent works have started to employ formal\nlanguages as an intermediate representation for reasoning tasks, they often\nface challenges in accurately generating and refining these formal\nspecifications to ensure correctness. To address these issues, this paper\nproposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs\nto do pairwise comparisons, allowing the evaluation of the refinements\nsuggested by the LLM. The paper demonstrates that Logic-LM++ outperforms\nLogic-LM and other contemporary techniques across natural language reasoning\ntasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average\nimprovement of 18.5% on standard prompting, 12.3% on chain of thought prompting\nand 5% on Logic-LM.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2407.02514v3",
    "published_date": "2024-06-22 12:50:41 UTC",
    "updated_date": "2024-08-06 06:39:02 UTC"
  },
  {
    "arxiv_id": "2406.15836v1",
    "title": "Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models",
    "authors": [
      "Yang Zhang",
      "Chenjia Bai",
      "Bin Zhao",
      "Junchi Yan",
      "Xiu Li",
      "Xuelong Li"
    ],
    "abstract": "Learning a world model for model-free Reinforcement Learning (RL) agents can\nsignificantly improve the sample efficiency by learning policies in\nimagination. However, building a world model for Multi-Agent RL (MARL) can be\nparticularly challenging due to the scalability issue in a centralized\narchitecture arising from a large number of agents, and also the\nnon-stationarity issue in a decentralized architecture stemming from the\ninter-dependency among agents. To address both challenges, we propose a novel\nworld model for MARL that learns decentralized local dynamics for scalability,\ncombined with a centralized representation aggregation from all agents. We cast\nthe dynamics learning as an auto-regressive sequence modeling problem over\ndiscrete tokens by leveraging the expressive Transformer architecture, in order\nto model complex local dynamics across different agents and provide accurate\nand consistent long-term imaginations. As the first pioneering\nTransformer-based world model for multi-agent systems, we introduce a Perceiver\nTransformer as an effective solution to enable centralized representation\naggregation within this context. Results on Starcraft Multi-Agent Challenge\n(SMAC) show that it outperforms strong model-free approaches and existing\nmodel-based methods in both sample efficiency and overall performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15836v1",
    "published_date": "2024-06-22 12:40:03 UTC",
    "updated_date": "2024-06-22 12:40:03 UTC"
  },
  {
    "arxiv_id": "2406.15808v1",
    "title": "Understanding Student and Academic Staff Perceptions of AI Use in Assessment and Feedback",
    "authors": [
      "Jasper Roe",
      "Mike Perkins",
      "Daniel Ruelle"
    ],
    "abstract": "The rise of Artificial Intelligence (AI) and Generative Artificial\nIntelligence (GenAI) in higher education necessitates assessment reform. This\nstudy addresses a critical gap by exploring student and academic staff\nexperiences with AI and GenAI tools, focusing on their familiarity and comfort\nwith current and potential future applications in learning and assessment. An\nonline survey collected data from 35 academic staff and 282 students across two\nuniversities in Vietnam and one in Singapore, examining GenAI familiarity,\nperceptions of its use in assessment marking and feedback, knowledge checking\nand participation, and experiences of GenAI text detection.\n  Descriptive statistics and reflexive thematic analysis revealed a generally\nlow familiarity with GenAI among both groups. GenAI feedback was viewed\nnegatively; however, it was viewed more positively when combined with\ninstructor feedback. Academic staff were more accepting of GenAI text detection\ntools and grade adjustments based on detection results compared to students.\nQualitative analysis identified three themes: unclear understanding of text\ndetection tools, variability in experiences with GenAI detectors, and mixed\nfeelings about GenAI's future impact on educational assessment. These findings\nhave major implications regarding the development of policies and practices for\nGenAI-enabled assessment and feedback in higher education.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15808v1",
    "published_date": "2024-06-22 10:25:01 UTC",
    "updated_date": "2024-06-22 10:25:01 UTC"
  },
  {
    "arxiv_id": "2406.15797v1",
    "title": "Synergistic Deep Graph Clustering Network",
    "authors": [
      "Benyu Wu",
      "Shifei Ding",
      "Xiao Xu",
      "Lili Guo",
      "Ling Ding",
      "Xindong Wu"
    ],
    "abstract": "Employing graph neural networks (GNNs) to learn cohesive and discriminative\nnode representations for clustering has shown promising results in deep graph\nclustering. However, existing methods disregard the reciprocal relationship\nbetween representation learning and structure augmentation. This study suggests\nthat enhancing embedding and structure synergistically becomes imperative for\nGNNs to unleash their potential in deep graph clustering. A reliable structure\npromotes obtaining more cohesive node representations, while high-quality node\nrepresentations can guide the augmentation of the structure, enhancing\nstructural reliability in return. Moreover, the generalization ability of\nexisting GNNs-based models is relatively poor. While they perform well on\ngraphs with high homogeneity, they perform poorly on graphs with low\nhomogeneity. To this end, we propose a graph clustering framework named\nSynergistic Deep Graph Clustering Network (SynC). In our approach, we design a\nTransform Input Graph Auto-Encoder (TIGAE) to obtain high-quality embeddings\nfor guiding structure augmentation. Then, we re-capture neighborhood\nrepresentations on the augmented graph to obtain clustering-friendly embeddings\nand conduct self-supervised clustering. Notably, representation learning and\nstructure augmentation share weights, significantly reducing the number of\nmodel parameters. Additionally, we introduce a structure fine-tuning strategy\nto improve the model's generalization. Extensive experiments on benchmark\ndatasets demonstrate the superiority and effectiveness of our method. The code\nis released on GitHub and Code Ocean.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15797v1",
    "published_date": "2024-06-22 09:40:34 UTC",
    "updated_date": "2024-06-22 09:40:34 UTC"
  },
  {
    "arxiv_id": "2406.16968v2",
    "title": "Multimodal Physiological Signals Representation Learning via Multiscale Contrasting for Depression Recognition",
    "authors": [
      "Kai Shao",
      "Rui Wang",
      "Yixue Hao",
      "Long Hu",
      "Min Chen",
      "Hans Arno Jacobsen"
    ],
    "abstract": "Depression recognition based on physiological signals such as functional\nnear-infrared spectroscopy (fNIRS) and electroencephalogram (EEG) has made\nconsiderable progress. However, most existing studies ignore the\ncomplementarity and semantic consistency of multimodal physiological signals\nunder the same stimulation task in complex spatio-temporal patterns. In this\npaper, we introduce a multimodal physiological signals representation learning\nframework using Siamese architecture via multiscale contrasting for depression\nrecognition (MRLMC). First, fNIRS and EEG are transformed into different but\ncorrelated data based on a time-domain data augmentation strategy. Then, we\ndesign a spatio-temporal contrasting module to learn the representation of\nfNIRS and EEG through weight-sharing multiscale spatio-temporal convolution.\nFurthermore, to enhance the learning of semantic representation associated with\nstimulation tasks, a semantic consistency contrast module is proposed, aiming\nto maximize the semantic similarity of fNIRS and EEG. Extensive experiments on\npublicly available and self-collected multimodal physiological signals datasets\nindicate that MRLMC outperforms the state-of-the-art models. Moreover, our\nproposed framework is capable of transferring to multimodal time series\ndownstream tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16968v2",
    "published_date": "2024-06-22 09:28:02 UTC",
    "updated_date": "2024-06-26 01:54:51 UTC"
  },
  {
    "arxiv_id": "2406.15789v1",
    "title": "Privacy Implications of Explainable AI in Data-Driven Systems",
    "authors": [
      "Fatima Ezzeddine"
    ],
    "abstract": "Machine learning (ML) models, demonstrably powerful, suffer from a lack of\ninterpretability. The absence of transparency, often referred to as the black\nbox nature of ML models, undermines trust and urges the need for efforts to\nenhance their explainability. Explainable AI (XAI) techniques address this\nchallenge by providing frameworks and methods to explain the internal\ndecision-making processes of these complex models. Techniques like\nCounterfactual Explanations (CF) and Feature Importance play a crucial role in\nachieving this goal. Furthermore, high-quality and diverse data remains the\nfoundational element for robust and trustworthy ML applications. In many\napplications, the data used to train ML and XAI explainers contain sensitive\ninformation. In this context, numerous privacy-preserving techniques can be\nemployed to safeguard sensitive information in the data, such as differential\nprivacy. Subsequently, a conflict between XAI and privacy solutions emerges due\nto their opposing goals. Since XAI techniques provide reasoning for the model\nbehavior, they reveal information relative to ML models, such as their decision\nboundaries, the values of features, or the gradients of deep learning models\nwhen explanations are exposed to a third entity. Attackers can initiate privacy\nbreaching attacks using these explanations, to perform model extraction,\ninference, and membership attacks. This dilemma underscores the challenge of\nfinding the right equilibrium between understanding ML decision-making and\nsafeguarding privacy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15789v1",
    "published_date": "2024-06-22 08:51:58 UTC",
    "updated_date": "2024-06-22 08:51:58 UTC"
  },
  {
    "arxiv_id": "2406.15786v6",
    "title": "What Matters in Transformers? Not All Attention is Needed",
    "authors": [
      "Shwai He",
      "Guoheng Sun",
      "Zheyu Shen",
      "Ang Li"
    ],
    "abstract": "While scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks, it also introduces redundant\narchitectures, posing efficiency challenges for real-world deployment. Despite\nsome recognition of redundancy in LLMs, the variability of redundancy across\ndifferent architectures in transformers, such as MLP and Attention layers, is\nunder-explored. In this work, we investigate redundancy across different\nmodules within Transformers, including Blocks, MLP, and Attention layers, using\na similarity-based metric. Surprisingly, despite the critical role of attention\nlayers in distinguishing transformers from other architectures, we found that a\nlarge portion of these layers exhibit excessively high similarity and can be\npruned without degrading performance. For instance, Llama-2-70B achieved a\n48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the\nattention layers. Furthermore, by tracing model checkpoints throughout the\ntraining process, we observed that attention layer redundancy is inherent and\nconsistent across training stages. Additionally, we further propose a method\nthat jointly drops Attention and MLP layers, allowing us to more aggressively\ndrop additional layers. For instance, when dropping 31 layers (Attention +\nMLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our\nwork provides valuable insights for future network architecture design. The\ncode is released at: \\url{https://github.com/Shwai-He/LLM-Drop}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 13 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2406.15786v6",
    "published_date": "2024-06-22 08:41:48 UTC",
    "updated_date": "2024-10-17 02:43:35 UTC"
  },
  {
    "arxiv_id": "2406.15784v1",
    "title": "Data Issues in Industrial AI System: A Meta-Review and Research Strategy",
    "authors": [
      "Xuejiao Li",
      "Cheng Yang",
      "Charles Møller",
      "Jay Lee"
    ],
    "abstract": "In the era of Industry 4.0, artificial intelligence (AI) is assuming an\nincreasingly pivotal role within industrial systems. Despite the recent trend\nwithin various industries to adopt AI, the actual adoption of AI is not as\ndeveloped as perceived. A significant factor contributing to this lag is the\ndata issues in AI implementation. How to address these data issues stands as a\nsignificant concern confronting both industry and academia. To address data\nissues, the first step involves mapping out these issues. Therefore, this study\nconducts a meta-review to explore data issues and methods within the\nimplementation of industrial AI. Seventy-two data issues are identified and\ncategorized into various stages of the data lifecycle, including data source\nand collection, data access and storage, data integration and interoperation,\ndata pre-processing, data processing, data security and privacy, and AI\ntechnology adoption. Subsequently, the study analyzes the data requirements of\nvarious AI algorithms. Building on the aforementioned analyses, it proposes a\ndata management framework, addressing how data issues can be systematically\nresolved at every stage of the data lifecycle. Finally, the study highlights\nfuture research directions. In doing so, this study enriches the existing body\nof knowledge and provides guidelines for professionals navigating the complex\nlandscape of achieving data usability and usefulness in industrial AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15784v1",
    "published_date": "2024-06-22 08:36:59 UTC",
    "updated_date": "2024-06-22 08:36:59 UTC"
  },
  {
    "arxiv_id": "2406.15778v2",
    "title": "ObjectNLQ @ Ego4D Episodic Memory Challenge 2024",
    "authors": [
      "Yisen Feng",
      "Haoyu Zhang",
      "Yuquan Xie",
      "Zaijing Li",
      "Meng Liu",
      "Liqiang Nie"
    ],
    "abstract": "In this report, we present our approach for the Natural Language Query track\nand Goal Step track of the Ego4D Episodic Memory Benchmark at CVPR 2024. Both\nchallenges require the localization of actions within long video sequences\nusing textual queries. To enhance localization accuracy, our method not only\nprocesses the temporal information of videos but also identifies fine-grained\nobjects spatially within the frames. To this end, we introduce a novel\napproach, termed ObjectNLQ, which incorporates an object branch to augment the\nvideo representation with detailed object information, thereby improving\ngrounding efficiency. ObjectNLQ achieves a mean R@1 of 23.15, ranking 2nd in\nthe Natural Language Queries Challenge, and gains 33.00 in terms of the metric\nR@1, IoU=0.3, ranking 3rd in the Goal Step Challenge. Our code will be released\nat https://github.com/Yisen-Feng/ObjectNLQ.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The solution for the Natural Language Query track and Goal Step track\n  at CVPR EgoVis Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15778v2",
    "published_date": "2024-06-22 07:57:58 UTC",
    "updated_date": "2024-11-18 03:02:17 UTC"
  },
  {
    "arxiv_id": "2406.15771v2",
    "title": "HCQA @ Ego4D EgoSchema Challenge 2024",
    "authors": [
      "Haoyu Zhang",
      "Yuquan Xie",
      "Yisen Feng",
      "Zaijing Li",
      "Meng Liu",
      "Liqiang Nie"
    ],
    "abstract": "In this report, we present our champion solution for Ego4D EgoSchema\nChallenge in CVPR 2024. To deeply integrate the powerful egocentric captioning\nmodel and question reasoning model, we propose a novel Hierarchical\nComprehension scheme for egocentric video Question Answering, named HCQA. It\nconsists of three stages: Fine-grained Caption Generation, Context-driven\nSummarization, and Inference-guided Answering. Given a long-form video, HCQA\ncaptures local detailed visual information and global summarised visual\ninformation via Fine-grained Caption Generation and Context-driven\nSummarization, respectively. Then in Inference-guided Answering, HCQA utilizes\nthis hierarchical information to reason and answer given question. On the\nEgoSchema blind test set, HCQA achieves 75% accuracy in answering over 5,000\nhuman curated multiple-choice questions. Our code will be released at\nhttps://github.com/Hyu-Zhang/HCQA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The champion solution for Ego4D EgoSchema Challenge in CVPR EgoVis\n  Workshop 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15771v2",
    "published_date": "2024-06-22 07:20:39 UTC",
    "updated_date": "2024-10-29 02:38:27 UTC"
  },
  {
    "arxiv_id": "2406.15763v2",
    "title": "AllMatch: Exploiting All Unlabeled Data for Semi-Supervised Learning",
    "authors": [
      "Zhiyu Wu",
      "Jinshi Cui"
    ],
    "abstract": "Existing semi-supervised learning algorithms adopt pseudo-labeling and\nconsistency regulation techniques to introduce supervision signals for\nunlabeled samples. To overcome the inherent limitation of threshold-based\npseudo-labeling, prior studies have attempted to align the confidence threshold\nwith the evolving learning status of the model, which is estimated through the\npredictions made on the unlabeled data. In this paper, we further reveal that\nclassifier weights can reflect the differentiated learning status across\ncategories and consequently propose a class-specific adaptive threshold\nmechanism. Additionally, considering that even the optimal threshold scheme\ncannot resolve the problem of discarding unlabeled samples, a binary\nclassification consistency regulation approach is designed to distinguish\ncandidate classes from negative options for all unlabeled samples. By combining\nthe above strategies, we present a novel SSL algorithm named AllMatch, which\nachieves improved pseudo-label accuracy and a 100% utilization ratio for the\nunlabeled data. We extensively evaluate our approach on multiple benchmarks,\nencompassing both balanced and imbalanced settings. The results demonstrate\nthat AllMatch consistently outperforms existing state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by IJCAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15763v2",
    "published_date": "2024-06-22 06:59:52 UTC",
    "updated_date": "2024-07-09 14:35:57 UTC"
  },
  {
    "arxiv_id": "2406.15755v1",
    "title": "Fine-grained Background Representation for Weakly Supervised Semantic Segmentation",
    "authors": [
      "Xu Yin",
      "Woobin Im",
      "Dongbo Min",
      "Yuchi Huo",
      "Fei Pan",
      "Sung-Eui Yoon"
    ],
    "abstract": "Generating reliable pseudo masks from image-level labels is challenging in\nthe weakly supervised semantic segmentation (WSSS) task due to the lack of\nspatial information. Prevalent class activation map (CAM)-based solutions are\nchallenged to discriminate the foreground (FG) objects from the suspicious\nbackground (BG) pixels (a.k.a. co-occurring) and learn the integral object\nregions. This paper proposes a simple fine-grained background representation\n(FBR) method to discover and represent diverse BG semantics and address the\nco-occurring problems. We abandon using the class prototype or pixel-level\nfeatures for BG representation. Instead, we develop a novel primitive, negative\nregion of interest (NROI), to capture the fine-grained BG semantic information\nand conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels.\nWe also present an active sampling strategy to mine the FG negatives\non-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive\nlearning to activate the entire object region. Thanks to the simplicity of\ndesign and convenience in use, our proposed method can be seamlessly plugged\ninto various models, yielding new state-of-the-art results under various WSSS\nsettings across benchmarks. Leveraging solely image-level (I) labels as\nsupervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results\non Pascal Voc and MS COCO test sets, respectively. Furthermore, by\nincorporating saliency maps as an additional supervision signal (I+S), we\nattain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach\ndemonstrates meaningful performance gains in weakly-supervised instance\nsegmentation (WSIS) tasks, showcasing its robustness and strong generalization\ncapabilities across diverse domains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15755v1",
    "published_date": "2024-06-22 06:45:25 UTC",
    "updated_date": "2024-06-22 06:45:25 UTC"
  },
  {
    "arxiv_id": "2406.15753v2",
    "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
    "authors": [
      "Lukas Fluri",
      "Leon Lang",
      "Alessandro Abate",
      "Patrick Forré",
      "David Krueger",
      "Joar Skalse"
    ],
    "abstract": "In reinforcement learning, specifying reward functions that capture the\nintended task can be very challenging. Reward learning aims to address this\nissue by learning the reward function. However, a learned reward model may have\na low error on the data distribution, and yet subsequently produce a policy\nwith large regret. We say that such a reward model has an error-regret\nmismatch. The main source of an error-regret mismatch is the distributional\nshift that commonly occurs during policy optimization. In this paper, we\nmathematically show that a sufficiently low expected test error of the reward\nmodel guarantees low worst-case regret, but that for any fixed expected test\nerror, there exist realistic data distributions that allow for error-regret\nmismatch to occur. We then show that similar problems persist even when using\npolicy regularization techniques, commonly employed in methods such as RLHF. We\nhope our results stimulate the theoretical and empirical study of improved\nmethods to learn reward models, and better ways to measure their quality\nreliably.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "70 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2406.15753v2",
    "published_date": "2024-06-22 06:43:51 UTC",
    "updated_date": "2025-03-04 15:17:17 UTC"
  },
  {
    "arxiv_id": "2406.15752v1",
    "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
    "authors": [
      "Yakun Song",
      "Zhuo Chen",
      "Xiaofei Wang",
      "Ziyang Ma",
      "Guanrou Yang",
      "Xie Chen"
    ],
    "abstract": "Neural codec language model (LM) has demonstrated strong capability in\nzero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers\nfrom limitations in inference speed and stability, due to its auto-regressive\nnature and implicit alignment between text and audio. In this work, to handle\nthese challenges, we introduce a new variant of neural codec LM, namely TacoLM.\nSpecifically, TacoLM introduces a gated attention mechanism to improve the\ntraining and inference efficiency and reduce the model size. Meanwhile, an\nadditional gated cross-attention layer is included for each decoder layer,\nwhich improves the efficiency and content accuracy of the synthesized speech.\nIn the evaluation of the Librispeech corpus, the proposed TacoLM achieves a\nbetter word error rate, speaker similarity, and mean opinion score, with 90%\nfewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is\navailable at https://ereboas.github.io/TacoLM/.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "INTERSPEECH 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15752v1",
    "published_date": "2024-06-22 06:39:52 UTC",
    "updated_date": "2024-06-22 06:39:52 UTC"
  },
  {
    "arxiv_id": "2406.15742v1",
    "title": "Probabilistic Programming with Programmable Variational Inference",
    "authors": [
      "McCoy R. Becker",
      "Alexander K. Lew",
      "Xiaoyan Wang",
      "Matin Ghavami",
      "Mathieu Huot",
      "Martin C. Rinard",
      "Vikash K. Mansinghka"
    ],
    "abstract": "Compared to the wide array of advanced Monte Carlo methods supported by\nmodern probabilistic programming languages (PPLs), PPL support for variational\ninference (VI) is less developed: users are typically limited to a predefined\nselection of variational objectives and gradient estimators, which are\nimplemented monolithically (and without formal correctness arguments) in PPL\nbackends. In this paper, we propose a more modular approach to supporting\nvariational inference in PPLs, based on compositional program transformation.\nIn our approach, variational objectives are expressed as programs, that may\nemploy first-class constructs for computing densities of and expected values\nunder user-defined models and variational families. We then transform these\nprograms systematically into unbiased gradient estimators for optimizing the\nobjectives they define. Our design enables modular reasoning about many\ninteracting concerns, including automatic differentiation, density\naccumulation, tracing, and the application of unbiased gradient estimation\nstrategies. Additionally, relative to existing support for VI in PPLs, our\ndesign increases expressiveness along three axes: (1) it supports an open-ended\nset of user-defined variational objectives, rather than a fixed menu of\noptions; (2) it supports a combinatorial space of gradient estimation\nstrategies, many not automated by today's PPLs; and (3) it supports a broader\nclass of models and variational families, because it supports constructs for\napproximate marginalization and normalization (previously introduced only for\nMonte Carlo inference). We implement our approach in an extension to the Gen\nprobabilistic programming system (genjax.vi, implemented in JAX), and evaluate\non several deep generative modeling tasks, showing minimal performance overhead\nvs. hand-coded implementations and performance competitive with\nwell-established open-source PPLs.",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15742v1",
    "published_date": "2024-06-22 05:49:37 UTC",
    "updated_date": "2024-06-22 05:49:37 UTC"
  },
  {
    "arxiv_id": "2406.15741v3",
    "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level",
    "authors": [
      "Zhaopeng Feng",
      "Ruizhe Chen",
      "Yan Zhang",
      "Zijie Meng",
      "Zuozhu Liu"
    ],
    "abstract": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/MT-Ladder",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main. Data and code are available at\n  https://github.com/fzp0424/MT-Ladder",
    "pdf_url": "http://arxiv.org/pdf/2406.15741v3",
    "published_date": "2024-06-22 05:33:35 UTC",
    "updated_date": "2024-10-29 05:15:09 UTC"
  },
  {
    "arxiv_id": "2406.15736v2",
    "title": "Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads",
    "authors": [
      "Anoop Cherian",
      "Kuan-Chuan Peng",
      "Suhas Lohit",
      "Joanna Matthiesen",
      "Kevin Smith",
      "Joshua B. Tenenbaum"
    ],
    "abstract": "Recent years have seen a significant progress in the general-purpose problem\nsolving abilities of large vision and language models (LVLMs), such as ChatGPT,\nGemini, etc.; some of these breakthroughs even seem to enable AI models to\noutperform human abilities in varied tasks that demand higher-order cognitive\nskills. Are the current large AI models indeed capable of generalized problem\nsolving as humans do? A systematic analysis of AI capabilities for joint vision\nand text reasoning, however, is missing in the current scientific literature.\nIn this paper, we make an effort towards filling this gap, by evaluating\nstate-of-the-art LVLMs on their mathematical and algorithmic reasoning\nabilities using visuo-linguistic problems from children's Olympiads.\nSpecifically, we consider problems from the Mathematical Kangaroo (MK)\nOlympiad, which is a popular international competition targeted at children\nfrom grades 1-12, that tests children's deeper mathematical abilities using\npuzzles that are appropriately gauged to their age and skills. Using the\npuzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840\nproblems from years 2020-2024. With our dataset, we analyze LVLMs power on\nmathematical reasoning; their responses on our puzzles offer a direct way to\ncompare against that of children. Our results show that modern LVLMs do\ndemonstrate increasingly powerful reasoning skills in solving problems for\nhigher grades, but lack the foundations to correctly answer problems designed\nfor younger children. Further analysis shows that there is no significant\ncorrelation between the reasoning capabilities of AI models and that of young\nchildren, and their capabilities appear to be based on a different type of\nreasoning than the cumulative knowledge that underlies children's mathematics\nand logic skills.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at NeurIPS 2024 (Datasets and Benchmarks Track)",
    "pdf_url": "http://arxiv.org/pdf/2406.15736v2",
    "published_date": "2024-06-22 05:04:39 UTC",
    "updated_date": "2024-12-05 23:59:06 UTC"
  },
  {
    "arxiv_id": "2406.15735v3",
    "title": "Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model",
    "authors": [
      "Min Zhao",
      "Hongzhou Zhu",
      "Chendong Xiang",
      "Kaiwen Zheng",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "Diffusion models have obtained substantial progress in image-to-video\ngeneration. However, in this paper, we find that these models tend to generate\nvideos with less motion than expected. We attribute this to the issue called\nconditional image leakage, where the image-to-video diffusion models (I2V-DMs)\ntend to over-rely on the conditional image at large time steps. We further\naddress this challenge from both inference and training aspects. First, we\npropose to start the generation process from an earlier time step to avoid the\nunreliable large-time steps of I2V-DMs, as well as an initial noise\ndistribution with optimal analytic expressions (Analytic-Init) by minimizing\nthe KL divergence between it and the actual marginal distribution to bridge the\ntraining-inference gap. Second, we design a time-dependent noise distribution\n(TimeNoise) for the conditional image during training, applying higher noise\nlevels at larger time steps to disrupt it and reduce the model's dependency on\nit. We validate these general strategies on various I2V-DMs on our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results show that\nour methods outperform baselines by producing higher motion scores with lower\nerrors while maintaining image alignment and temporal consistency, thereby\nyielding superior overall performance and enabling more accurate motion\ncontrol. The project page: \\url{https://cond-image-leak.github.io/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2024. Project page: https://cond-image-leak.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2406.15735v3",
    "published_date": "2024-06-22 04:56:16 UTC",
    "updated_date": "2024-11-06 03:53:13 UTC"
  },
  {
    "arxiv_id": "2406.15734v2",
    "title": "RankAdaptor: Hierarchical Rank Allocation for Efficient Fine-Tuning Pruned LLMs via Performance Model",
    "authors": [
      "Changhai Zhou",
      "Shijie Han",
      "Lining Yang",
      "Yuhua Zhou",
      "Xu Cheng",
      "Yibin Wang",
      "Hongguang Li"
    ],
    "abstract": "The efficient compression of large language models (LLMs) has become\nincreasingly popular. However, recovering the performance of compressed LLMs\nremains a major challenge. The current practice in LLM compression entails the\nimplementation of structural pruning, complemented by a recovery phase that\nleverages the Low-Rank Adaptation (LoRA) algorithm. Structural pruning's uneven\nmodification of model architecture, coupled with standard LoRA's fixed\nconfiguration allocation across layers in an online pipeline, leads to\nsuboptimal performance in various downstream tasks for pruned models. To\naddress this challenge, we introduce RankAdaptor, a hierarchical rank\nallocation method that enables efficient fine-tuning of pruned LLMs according\nto layerwise specific recovery requirements. We employ a performance model that\nconducts offline meta-learning and online incremental learning to explore\noptimal rank values for each layer. Comprehensive experiments on popular\nbenchmarks show that RankAdaptor consistently outperforms state-of-the-art\nmethods across a variety of pruning settings and LLM architectures, with\nimprovements ranging from 0.7\\% to 5.5\\%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15734v2",
    "published_date": "2024-06-22 04:52:58 UTC",
    "updated_date": "2024-12-16 08:19:26 UTC"
  },
  {
    "arxiv_id": "2406.15732v1",
    "title": "AI-Driven Approaches for Optimizing Power Consumption: A Comprehensive Survey",
    "authors": [
      "Parag Biswas",
      "Abdur Rashid",
      "Angona Biswas",
      "Md Abdullah Al Nasim",
      "Kishor Datta Gupta",
      "Roy George"
    ],
    "abstract": "Reduced environmental effect, lower operating costs, and a stable and\nsustainable energy supply for current and future generations are the main\nreasons why power optimization is important. Power optimization makes ensuring\nthat energy is used more effectively, cutting down on waste and optimizing the\nutilization of resources.In today's world, power optimization and artificial\nintelligence (AI) integration are essential to changing the way energy is\nproduced, used, and distributed. Real-time monitoring and analysis of power\nusage trends is made possible by AI-driven algorithms and predictive analytics,\nwhich enable dynamic modifications to effectively satisfy demand. Efficiency\nand sustainability are increased when power consumption is optimized in\ndifferent sectors thanks to the use of intelligent systems. This survey paper\ncomprises an extensive review of the several AI techniques used for power\noptimization as well as a methodical analysis of the literature for the study\nof various intelligent system application domains across different disciplines\nof power consumption.This literature review identifies the performance and\noutcomes of 17 different research methods by assessing them, and it aims to\ndistill valuable insights into their strengths and limitations. Furthermore,\nthis article outlines future directions in the integration of AI for power\nconsumption optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.15732v1",
    "published_date": "2024-06-22 04:42:37 UTC",
    "updated_date": "2024-06-22 04:42:37 UTC"
  },
  {
    "arxiv_id": "2406.15731v1",
    "title": "Breaking Secure Aggregation: Label Leakage from Aggregated Gradients in Federated Learning",
    "authors": [
      "Zhibo Wang",
      "Zhiwei Chang",
      "Jiahui Hu",
      "Xiaoyi Pang",
      "Jiacheng Du",
      "Yongle Chen",
      "Kui Ren"
    ],
    "abstract": "Federated Learning (FL) exhibits privacy vulnerabilities under gradient\ninversion attacks (GIAs), which can extract private information from individual\ngradients. To enhance privacy, FL incorporates Secure Aggregation (SA) to\nprevent the server from obtaining individual gradients, thus effectively\nresisting GIAs. In this paper, we propose a stealthy label inference attack to\nbypass SA and recover individual clients' private labels. Specifically, we\nconduct a theoretical analysis of label inference from the aggregated gradients\nthat are exclusively obtained after implementing SA. The analysis results\nreveal that the inputs (embeddings) and outputs (logits) of the final fully\nconnected layer (FCL) contribute to gradient disaggregation and label\nrestoration. To preset the embeddings and logits of FCL, we craft a fishing\nmodel by solely modifying the parameters of a single batch normalization (BN)\nlayer in the original model. Distributing client-specific fishing models, the\nserver can derive the individual gradients regarding the bias of FCL by\nresolving a linear system with expected embeddings and the aggregated gradients\nas coefficients. Then the labels of each client can be precisely computed based\non preset logits and gradients of FCL's bias. Extensive experiments show that\nour attack achieves large-scale label recovery with 100\\% accuracy on various\ndatasets and model architectures.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "10 pages, conference to IEEE INFOCOM 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15731v1",
    "published_date": "2024-06-22 04:42:18 UTC",
    "updated_date": "2024-06-22 04:42:18 UTC"
  },
  {
    "arxiv_id": "2406.16965v2",
    "title": "Present and Future of AI in Renewable Energy Domain : A Comprehensive Survey",
    "authors": [
      "Abdur Rashid",
      "Parag Biswas",
      "Angona Biswas",
      "MD Abdullah Al Nasim",
      "Kishor Datta Gupta",
      "Roy George"
    ],
    "abstract": "Artificial intelligence (AI) has become a crucial instrument for streamlining\nprocesses in various industries, including electrical power systems, as a\nresult of recent digitalization. Algorithms for artificial intelligence are\ndata-driven models that are based on statistical learning theory and are used\nas a tool to take use of the data that the power system and its users generate.\nInitially, we perform a thorough literature analysis of artificial intelligence\n(AI) applications related to renewable energy (RE). Next, we present a thorough\nanalysis of renewable energy factories and assess their suitability, along with\na list of the most widely used and appropriate AI algorithms. Nine AI-based\nstrategies are identified here to assist Renewable Energy (RE) in contemporary\npower systems. This survey paper comprises an extensive review of the several\nAI techniques used for renewable energy as well as a methodical analysis of the\nliterature for the study of various intelligent system application domains\nacross different disciplines of renewable energy. This literature review\nidentifies the performance and outcomes of nine different research methods by\nassessing them, and it aims to distill valuable insights into their strengths\nand limitations. This study also addressed three main topics: using AI\ntechnology for renewable power generation, utilizing AI for renewable energy\nforecasting, and optimizing energy systems. Additionally, it explored AI's\nsuperiority over conventional models in controllability, data handling,\ncyberattack prevention, smart grid implementation, robotics- AI's significance\nin shaping the future of the energy industry. Furthermore, this article\noutlines future directions in the integration of AI for renewable energy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16965v2",
    "published_date": "2024-06-22 04:36:09 UTC",
    "updated_date": "2024-10-19 19:23:48 UTC"
  },
  {
    "arxiv_id": "2406.15723v1",
    "title": "Acoustic Feature Mixup for Balanced Multi-aspect Pronunciation Assessment",
    "authors": [
      "Heejin Do",
      "Wonjun Lee",
      "Gary Geunbae Lee"
    ],
    "abstract": "In automated pronunciation assessment, recent emphasis progressively lies on\nevaluating multiple aspects to provide enriched feedback. However, acquiring\nmulti-aspect-score labeled data for non-native language learners' speech poses\nchallenges; moreover, it often leads to score-imbalanced distributions. In this\npaper, we propose two Acoustic Feature Mixup strategies, linearly and\nnon-linearly interpolating with the in-batch averaged feature, to address data\nscarcity and score-label imbalances. Primarily using goodness-of-pronunciation\nas an acoustic feature, we tailor mixup designs to suit pronunciation\nassessment. Further, we integrate fine-grained error-rate features by comparing\nspeech recognition results with the original answer phonemes, giving direct\nhints for mispronunciation. Effective mixing of the acoustic features notably\nenhances overall scoring performances on the speechocean762 dataset, and\ndetailed analysis highlights our potential to predict unseen distortions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Interspeech 2024",
    "pdf_url": "http://arxiv.org/pdf/2406.15723v1",
    "published_date": "2024-06-22 03:56:29 UTC",
    "updated_date": "2024-06-22 03:56:29 UTC"
  },
  {
    "arxiv_id": "2406.16964v2",
    "title": "Are Language Models Actually Useful for Time Series Forecasting?",
    "authors": [
      "Mingtian Tan",
      "Mike A. Merrill",
      "Vinayak Gupta",
      "Tim Althoff",
      "Thomas Hartvigsen"
    ],
    "abstract": "Large language models (LLMs) are being applied to time series forecasting.\nBut are language models actually useful for time series? In a series of\nablation studies on three recent and popular LLM-based time series forecasting\nmethods, we find that removing the LLM component or replacing it with a basic\nattention layer does not degrade forecasting performance -- in most cases, the\nresults even improve! We also find that despite their significant computational\ncost, pretrained LLMs do no better than models trained from scratch, do not\nrepresent the sequential dependencies in time series, and do not assist in\nfew-shot settings. Additionally, we explore time series encoders and find that\npatching and attention structures perform similarly to LLM-based forecasters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024 (Spotlight)",
    "pdf_url": "http://arxiv.org/pdf/2406.16964v2",
    "published_date": "2024-06-22 03:33:38 UTC",
    "updated_date": "2024-10-26 01:43:07 UTC"
  },
  {
    "arxiv_id": "2406.16963v1",
    "title": "Large Language Models for Link Stealing Attacks Against Graph Neural Networks",
    "authors": [
      "Faqian Guan",
      "Tianqing Zhu",
      "Hui Sun",
      "Wanlei Zhou",
      "Philip S. Yu"
    ],
    "abstract": "Graph data contains rich node features and unique edge information, which\nhave been applied across various domains, such as citation networks or\nrecommendation systems. Graph Neural Networks (GNNs) are specialized for\nhandling such data and have shown impressive performance in many applications.\nHowever, GNNs may contain of sensitive information and susceptible to privacy\nattacks. For example, link stealing is a type of attack in which attackers\ninfer whether two nodes are linked or not. Previous link stealing attacks\nprimarily relied on posterior probabilities from the target GNN model,\nneglecting the significance of node features. Additionally, variations in node\nclasses across different datasets lead to different dimensions of posterior\nprobabilities. The handling of these varying data dimensions posed a challenge\nin using a single model to effectively conduct link stealing attacks on\ndifferent datasets. To address these challenges, we introduce Large Language\nModels (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively\nintegrate textual features and exhibit strong generalizability, enabling\nattacks to handle diverse data dimensions across various datasets. We design\ntwo distinct LLM prompts to effectively combine textual features and posterior\nprobabilities of graph nodes. Through these designed prompts, we fine-tune the\nLLM to adapt to the link stealing attack task. Furthermore, we fine-tune the\nLLM using multiple datasets and enable the LLM to learn features from different\ndatasets simultaneously. Experimental results show that our approach\nsignificantly enhances the performance of existing link stealing attack tasks\nin both white-box and black-box scenarios. Our method can execute link stealing\nattacks across different datasets using only a single model, making link\nstealing attacks more applicable to real-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2406.16963v1",
    "published_date": "2024-06-22 02:47:24 UTC",
    "updated_date": "2024-06-22 02:47:24 UTC"
  },
  {
    "arxiv_id": "2406.15708v2",
    "title": "Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization",
    "authors": [
      "Xingchen Wan",
      "Ruoxi Sun",
      "Hootan Nakhost",
      "Sercan O. Arik"
    ],
    "abstract": "Large language models have demonstrated remarkable capabilities, but their\nperformance is heavily reliant on effective prompt engineering. Automatic\nprompt optimization (APO) methods are designed to automate this and can be\nbroadly categorized into those targeting instructions (instruction\noptimization, IO) vs. those targeting exemplars (exemplar optimization, EO).\nDespite their shared objective, these have evolved rather independently, with\nIO receiving more research attention recently. This paper seeks to bridge this\ngap by comprehensively comparing the performance of representative IO and EO\ntechniques both isolation and combination on a diverse set of challenging\ntasks. Our findings reveal that intelligently reusing model-generated\ninput-output pairs obtained from evaluating prompts on the validation set as\nexemplars, consistently improves performance on top of IO methods but is\ncurrently under-investigated. We also find that despite the recent focus on IO,\nhow we select exemplars can outweigh how we optimize instructions, with EO\nstrategies as simple as random search outperforming state-of-the-art IO methods\nwith seed instructions without any optimization. Moreover, we observe a synergy\nbetween EO and IO, with optimal combinations surpassing the individual\ncontributions. We conclude that studying exemplar optimization both as a\nstandalone method and its optimal combination with instruction optimization\nremain a crucial aspect of APO and deserve greater consideration in future\nresearch, even in the era of highly capable instruction-following models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Expanded version of the NeurIPS 2024 paper",
    "pdf_url": "http://arxiv.org/pdf/2406.15708v2",
    "published_date": "2024-06-22 02:07:10 UTC",
    "updated_date": "2024-11-06 22:07:17 UTC"
  },
  {
    "arxiv_id": "2406.16962v1",
    "title": "MetaGreen: Meta-Learning Inspired Transformer Selection for Green Semantic Communication",
    "authors": [
      "Shubhabrata Mukherjee",
      "Cory Beard",
      "Sejun Song"
    ],
    "abstract": "Semantic Communication can transform the way we transmit information,\nprioritizing meaningful and effective content over individual symbols or bits.\nThis evolution promises significant benefits, including reduced latency, lower\nbandwidth usage, and higher throughput compared to traditional communication.\nHowever, the development of Semantic Communication faces a crucial challenge:\nthe need for universal metrics to benchmark the joint effects of semantic\ninformation loss and energy consumption. This research introduces an innovative\nsolution: the ``Energy-Optimized Semantic Loss'' (EOSL) function, a novel\nmulti-objective loss function that effectively balances semantic information\nloss and energy consumption. Through comprehensive experiments on transformer\nmodels, including energy benchmarking, we demonstrate the remarkable\neffectiveness of EOSL-based model selection. We have established that\nEOSL-based transformer model selection achieves up to 83\\% better\nsimilarity-to-power ratio (SPR) compared to BLEU score-based selection and 67\\%\nbetter SPR compared to solely lowest power usage-based selection. Furthermore,\nwe extend the applicability of EOSL to diverse and varying contexts, inspired\nby the principles of Meta-Learning. By cumulatively applying EOSL, we enable\nthe model selection system to adapt to this change, leveraging historical EOSL\nvalues to guide the learning process. This work lays the foundation for\nenergy-efficient model selection and the development of green semantic\ncommunication.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2310.07592",
    "pdf_url": "http://arxiv.org/pdf/2406.16962v1",
    "published_date": "2024-06-22 00:49:40 UTC",
    "updated_date": "2024-06-22 00:49:40 UTC"
  }
]
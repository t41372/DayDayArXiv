[
  {
    "arxiv_id": "2401.03093v4",
    "title": "XXAI: Towards eXplicitly eXplainable Artificial Intelligence",
    "authors": [
      "V. L. Kalmykov",
      "L. V. Kalmykov"
    ],
    "abstract": "There are concerns about the reliability and safety of artificial\nintelligence (AI) based on sub-symbolic neural networks because its decisions\ncannot be explained explicitly. This is the black box problem of modern AI. At\nthe same time, symbolic AI has the nature of a white box and is able to ensure\nthe reliability and safety of its decisions. However, several problems prevent\nthe widespread use of symbolic AI: the opacity of mathematical models and\nnatural language terms, the lack of a unified ontology, and the combinatorial\nexplosion of search capabilities. To solve the black-box problem of AI, we\npropose eXplicitly eXplainable AI (XXAI) - a fully transparent white-box AI\nbased on deterministic logical cellular automata whose rules are derived from\nthe first principles of the general theory of the relevant domain. In this\ncase, the general theory of the domain plays the role of a knowledge base for\nderiving the inferences of the cellular automata. A cellular automaton\nimplements parallel multi-level logical inference at all levels of organization\n- from local interactions of the element base to the system as a whole. Our\nverification of several ecological hypotheses sets a precedent for the\nsuccessful implementation of the proposed solution. XXAI is able to\nautomatically verify the reliability, security and ethics of sub-symbolic\nneural network solutions in both the final and training phases. In this\narticle, we present precedents for the successful implementation of XXAI, the\ntheoretical and methodological foundations for its further development, and\ndiscuss prospects for the future.",
    "categories": [
      "cs.AI",
      "q-bio.PE"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 1 graphical abstract, 1 figure, 72 references",
    "pdf_url": "http://arxiv.org/pdf/2401.03093v4",
    "published_date": "2024-01-05 23:50:10 UTC",
    "updated_date": "2024-05-19 14:02:45 UTC"
  },
  {
    "arxiv_id": "2401.03082v1",
    "title": "UMIE: Unified Multimodal Information Extraction with Instruction Tuning",
    "authors": [
      "Lin Sun",
      "Kai Zhang",
      "Qingyuan Li",
      "Renze Lou"
    ],
    "abstract": "Multimodal information extraction (MIE) gains significant attention as the\npopularity of multimedia content increases. However, current MIE methods often\nresort to using task-specific model structures, which results in limited\ngeneralizability across tasks and underutilizes shared knowledge across MIE\ntasks. To address these issues, we propose UMIE, a unified multimodal\ninformation extractor to unify three MIE tasks as a generation problem using\ninstruction tuning, being able to effectively extract both textual and visual\nmentions. Extensive experiments show that our single UMIE outperforms various\nstate-of-the-art (SoTA) methods across six MIE datasets on three tasks.\nFurthermore, in-depth analysis demonstrates UMIE's strong generalization in the\nzero-shot setting, robustness to instruction variants, and interpretability.\nOur research serves as an initial step towards a unified MIE model and\ninitiates the exploration into both instruction tuning and large language\nmodels within the MIE domain. Our code, data, and model are available at\nhttps://github.com/ZUCC-AI/UMIE",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.03082v1",
    "published_date": "2024-01-05 22:52:15 UTC",
    "updated_date": "2024-01-05 22:52:15 UTC"
  },
  {
    "arxiv_id": "2401.03065v1",
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
    "authors": [
      "Alex Gu",
      "Baptiste Rozi√®re",
      "Hugh Leather",
      "Armando Solar-Lezama",
      "Gabriel Synnaeve",
      "Sida I. Wang"
    ],
    "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution\nEvaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each\nfunction comes with an input-output pair, leading to two natural tasks: input\nprediction and output prediction. First, we propose a generic recipe for\ngenerating our execution benchmark which can be used to create future variation\nof the benchmark. Second, we evaluate twenty code models on our benchmark and\ndiscover that many recent high-scoring models on HumanEval do not show the same\nimprovements on our benchmark. Third, we show that simple CoT and fine-tuning\nschemes can improve performance on our benchmark but remain far from solving\nit. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%\nand 81% on input and output prediction, respectively. In contrast, Code Llama\n34B achieves a pass@1 of 50% and 46% on input and output prediction,\nhighlighting the gap between open and closed source models. As no model is\nclose to acing CRUXEval, we provide examples of consistent GPT-4 failures on\nsimple programs as a lens into its code reasoning capabilities and areas for\nimprovement.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "71 pages, 29 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03065v1",
    "published_date": "2024-01-05 20:53:51 UTC",
    "updated_date": "2024-01-05 20:53:51 UTC"
  },
  {
    "arxiv_id": "2401.03059v1",
    "title": "Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach",
    "authors": [
      "Omid Semiari",
      "Hosein Nikopour",
      "Shilpa Talwar"
    ],
    "abstract": "Ultra-reliable low-latency communication (URLLC) is the cornerstone for a\nbroad range of emerging services in next-generation wireless networks. URLLC\nfundamentally relies on the network's ability to proactively determine whether\nsufficient resources are available to support the URLLC traffic, and thus,\nprevent so-called cell overloads. Nonetheless, achieving accurate\nquality-of-service (QoS) predictions for URLLC user equipment (UEs) and\npreventing cell overloads are very challenging tasks. This is due to dependency\nof the QoS metrics (latency and reliability) on traffic and channel statistics,\nusers' mobility, and interdependent performance across UEs. In this paper, a\nnew QoS-aware UE admission control approach is developed to proactively\nestimate QoS for URLLC UEs, prior to associating them with a cell, and\naccordingly, admit only a subset of UEs that do not lead to a cell overload. To\nthis end, an optimization problem is formulated to find an efficient UE\nadmission control policy, cognizant of UEs' QoS requirements and cell-level\nload dynamics. To solve this problem, a new machine learning based method is\nproposed that builds on (deep) neural contextual bandits, a suitable framework\nfor dealing with nonlinear bandit problems. In fact, the UE admission\ncontroller is treated as a bandit agent that observes a set of network\nmeasurements (context) and makes admission control decisions based on\ncontext-dependent QoS (reward) predictions. The simulation results show that\nthe proposed scheme can achieve near-optimal performance and yield substantial\ngains in terms of cell-level service reliability and efficient resource\nutilization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in the proceedings of the 2024 IEEE International\n  Conference on Machine Learning for Communication and Networking (ICMLCN)",
    "pdf_url": "http://arxiv.org/pdf/2401.03059v1",
    "published_date": "2024-01-05 20:26:08 UTC",
    "updated_date": "2024-01-05 20:26:08 UTC"
  },
  {
    "arxiv_id": "2401.03040v1",
    "title": "AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis",
    "authors": [
      "Kebin Wu",
      "Wenbin Li",
      "Xiaofei Xiao"
    ],
    "abstract": "Traffic accident analysis is pivotal for enhancing public safety and\ndeveloping road regulations. Traditional approaches, although widely used, are\noften constrained by manual analysis processes, subjective decisions, uni-modal\noutputs, as well as privacy issues related to sensitive data. This paper\nintroduces the idea of AccidentGPT, a foundation model of traffic accident\nanalysis, which incorporates multi-modal input data to automatically\nreconstruct the accident process video with dynamics details, and furthermore\nprovide multi-task analysis with multi-modal outputs. The design of the\nAccidentGPT is empowered with a multi-modality prompt with feedback for\ntask-oriented adaptability, a hybrid training schema to leverage labelled and\nunlabelled data, and a edge-cloud split configuration for data privacy. To\nfully realize the functionalities of this model, we proposes several research\nopportunities. This paper serves as the stepping stone to fill the gaps in\ntraditional approaches of traffic accident analysis and attract the research\ncommunity attention for automatic, objective, and privacy-preserving traffic\naccident analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.03040v1",
    "published_date": "2024-01-05 19:33:21 UTC",
    "updated_date": "2024-01-05 19:33:21 UTC"
  },
  {
    "arxiv_id": "2402.00025v2",
    "title": "Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK work decomposition",
    "authors": [
      "Adnan Hoque",
      "Less Wright",
      "Chih-Chieh Yang",
      "Mudhakar Srivatsa",
      "Raghu Ganti"
    ],
    "abstract": "We propose an implementation of an efficient fused matrix multiplication\nkernel for W4A16 quantized inference, where we perform dequantization and GEMM\nin a fused kernel using a SplitK work decomposition. Our implementation shows\nimprovement for the type of skinny matrix-matrix multiplications found in\nfoundation model inference workloads. In particular, this paper surveys the\ntype of matrix multiplication between a skinny activation matrix and a square\nweight matrix. Our results show an average of 65% speed improvement on A100,\nand an average of 124% speed improvement on H100 (with a peak of 295%) for a\nrange of matrix dimensions including those found in a llama-style model, where\nm < n = k.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00025v2",
    "published_date": "2024-01-05 19:17:55 UTC",
    "updated_date": "2024-02-22 20:38:47 UTC"
  },
  {
    "arxiv_id": "2401.02954v1",
    "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "authors": [
      "DeepSeek-AI",
      ":",
      "Xiao Bi",
      "Deli Chen",
      "Guanting Chen",
      "Shanhuang Chen",
      "Damai Dai",
      "Chengqi Deng",
      "Honghui Ding",
      "Kai Dong",
      "Qiushi Du",
      "Zhe Fu",
      "Huazuo Gao",
      "Kaige Gao",
      "Wenjun Gao",
      "Ruiqi Ge",
      "Kang Guan",
      "Daya Guo",
      "Jianzhong Guo",
      "Guangbo Hao",
      "Zhewen Hao",
      "Ying He",
      "Wenjie Hu",
      "Panpan Huang",
      "Erhang Li",
      "Guowei Li",
      "Jiashi Li",
      "Yao Li",
      "Y. K. Li",
      "Wenfeng Liang",
      "Fangyun Lin",
      "A. X. Liu",
      "Bo Liu",
      "Wen Liu",
      "Xiaodong Liu",
      "Xin Liu",
      "Yiyuan Liu",
      "Haoyu Lu",
      "Shanghao Lu",
      "Fuli Luo",
      "Shirong Ma",
      "Xiaotao Nie",
      "Tian Pei",
      "Yishi Piao",
      "Junjie Qiu",
      "Hui Qu",
      "Tongzheng Ren",
      "Zehui Ren",
      "Chong Ruan",
      "Zhangli Sha",
      "Zhihong Shao",
      "Junxiao Song",
      "Xuecheng Su",
      "Jingxiang Sun",
      "Yaofeng Sun",
      "Minghui Tang",
      "Bingxuan Wang",
      "Peiyi Wang",
      "Shiyu Wang",
      "Yaohui Wang",
      "Yongji Wang",
      "Tong Wu",
      "Y. Wu",
      "Xin Xie",
      "Zhenda Xie",
      "Ziwei Xie",
      "Yiliang Xiong",
      "Hanwei Xu",
      "R. X. Xu",
      "Yanhong Xu",
      "Dejian Yang",
      "Yuxiang You",
      "Shuiping Yu",
      "Xingkai Yu",
      "B. Zhang",
      "Haowei Zhang",
      "Lecong Zhang",
      "Liyue Zhang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Wentao Zhang",
      "Yichao Zhang",
      "Chenggang Zhao",
      "Yao Zhao",
      "Shangyan Zhou",
      "Shunfeng Zhou",
      "Qihao Zhu",
      "Yuheng Zou"
    ],
    "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02954v1",
    "published_date": "2024-01-05 18:59:13 UTC",
    "updated_date": "2024-01-05 18:59:13 UTC"
  },
  {
    "arxiv_id": "2401.02949v3",
    "title": "Graph2Tac: Online Representation Learning of Formal Math Concepts",
    "authors": [
      "Lasse Blaauwbroek",
      "Miroslav Ol≈°√°k",
      "Jason Rute",
      "Fidel Ivan Schaposnik Massolo",
      "Jelle Piepenbrock",
      "Vasily Pestun"
    ],
    "abstract": "In proof assistants, the physical proximity between two formal mathematical\nconcepts is a strong predictor of their mutual relevance. Furthermore, lemmas\nwith close proximity regularly exhibit similar proof structures. We show that\nthis locality property can be exploited through online learning techniques to\nobtain solving agents that far surpass offline learners when asked to prove\ntheorems in an unseen mathematical setting. We extensively benchmark two such\nonline solvers implemented in the Tactician platform for the Coq proof\nassistant: First, Tactician's online $k$-nearest neighbor solver, which can\nlearn from recent proofs, shows a $1.72\\times$ improvement in theorems proved\nover an offline equivalent. Second, we introduce a graph neural network,\nGraph2Tac, with a novel approach to build hierarchical representations for new\ndefinitions. Graph2Tac's online definition task realizes a $1.5\\times$\nimprovement in theorems solved over an offline baseline. The $k$-NN and\nGraph2Tac solvers rely on orthogonal online data, making them highly\ncomplementary. Their combination improves $1.27\\times$ over their individual\nperformances. Both solvers outperform all other general-purpose provers for\nCoq, including CoqHammer, Proverbot9001, and a transformer baseline by at least\n$1.48\\times$ and are available for practical use by end-users.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07 (Primary) 68V15 (Secondary)",
      "I.2.3; I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "31 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.02949v3",
    "published_date": "2024-01-05 18:52:09 UTC",
    "updated_date": "2024-06-23 13:54:33 UTC"
  },
  {
    "arxiv_id": "2402.00024v3",
    "title": "Can Large Language Models Understand Molecules?",
    "authors": [
      "Shaghayegh Sadeghi",
      "Alan Bui",
      "Ali Forooghi",
      "Jianguo Lu",
      "Alioune Ngom"
    ],
    "abstract": "Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained\nTransformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI\nare increasingly recognized for their potential in the field of\ncheminformatics, particularly in understanding Simplified Molecular Input Line\nEntry System (SMILES), a standard method for representing chemical structures.\nThese LLMs also have the ability to decode SMILES strings into vector\nrepresentations.\n  Method: We investigate the performance of GPT and LLaMA compared to\npre-trained models on SMILES in embedding SMILES strings on downstream tasks,\nfocusing on two key applications: molecular property prediction and drug-drug\ninteraction prediction.\n  Results: We find that SMILES embeddings generated using LLaMA outperform\nthose from GPT in both molecular property and DDI prediction tasks. Notably,\nLLaMA-based SMILES embeddings show results comparable to pre-trained models on\nSMILES in molecular prediction tasks and outperform the pre-trained models for\nthe DDI prediction tasks.\n  Conclusion: The performance of LLMs in generating SMILES embeddings shows\ngreat potential for further investigation of these models for molecular\nembedding. We hope our study bridges the gap between LLMs and molecular\nembedding, motivating additional research into the potential of LLMs in the\nmolecular representation field. GitHub:\nhttps://github.com/sshaghayeghs/LLaMA-VS-GPT",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2402.00024v3",
    "published_date": "2024-01-05 18:31:34 UTC",
    "updated_date": "2024-05-21 03:40:19 UTC"
  },
  {
    "arxiv_id": "2401.02920v1",
    "title": "Analytically-Driven Resource Management for Cloud-Native Microservices",
    "authors": [
      "Yanqi Zhang",
      "Zhuangzhuang Zhou",
      "Sameh Elnikety",
      "Christina Delimitrou"
    ],
    "abstract": "Resource management for cloud-native microservices has attracted a lot of\nrecent attention. Previous work has shown that machine learning (ML)-driven\napproaches outperform traditional techniques, such as autoscaling, in terms of\nboth SLA maintenance and resource efficiency. However, ML-driven approaches\nalso face challenges including lengthy data collection processes and limited\nscalability. We present Ursa, a lightweight resource management system for\ncloud-native microservices that addresses these challenges. Ursa uses an\nanalytical model that decomposes the end-to-end SLA into per-service SLA, and\nmaps per-service SLA to individual resource allocations per microservice tier.\nTo speed up the exploration process and avoid prolonged SLA violations, Ursa\nexplores each microservice individually, and swiftly stops exploration if\nlatency exceeds its SLA.\n  We evaluate Ursa on a set of representative and end-to-end microservice\ntopologies, including a social network, media service and video processing\npipeline, each consisting of multiple classes and priorities of requests with\ndifferent SLAs, and compare it against two representative ML-driven systems,\nSinan and Firm. Compared to these ML-driven approaches, Ursa provides\nsignificant advantages: It shortens the data collection process by more than\n128x, and its control plane is 43x faster than ML-driven approaches. At the\nsame time, Ursa does not sacrifice resource efficiency or SLAs. During online\ndeployment, Ursa reduces the SLA violation rate by 9.0% up to 49.9%, and\nreduces CPU allocation by up to 86.2% compared to ML-driven approaches.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02920v1",
    "published_date": "2024-01-05 17:55:32 UTC",
    "updated_date": "2024-01-05 17:55:32 UTC"
  },
  {
    "arxiv_id": "2401.02905v2",
    "title": "H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses",
    "authors": [
      "Haidong Gu",
      "Nathan Gaw",
      "Yinan Wang",
      "Chancellor Johnstone",
      "Christine Beauchene",
      "Sophia Yuditskaya",
      "Hrishikesh Rao",
      "Chun-An Chou"
    ],
    "abstract": "Discovering human cognitive and emotional states using multi-modal\nphysiological signals draws attention across various research applications.\nPhysiological responses of the human body are influenced by human cognition and\ncommonly used to analyze cognitive states. From a network science perspective,\nthe interactions of these heterogeneous physiological modalities in a graph\nstructure may provide insightful information to support prediction of cognitive\nstates. However, there is no clue to derive exact connectivity between\nheterogeneous modalities and there exists a hierarchical structure of\nsub-modalities. Existing graph neural networks are designed to learn on\nnon-hierarchical homogeneous graphs with pre-defined graph structures; they\nfailed to learn from hierarchical, multi-modal physiological data without a\npre-defined graph structure. To this end, we propose a hierarchical\nheterogeneous graph generative network (H2G2-Net) that automatically learns a\ngraph structure without domain knowledge, as well as a powerful representation\non the hierarchical heterogeneous graph in an end-to-end fashion. We validate\nthe proposed method on the CogPilot dataset that consists of multi-modal\nphysiological signals. Extensive experiments demonstrate that our proposed\nmethod outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/)",
    "pdf_url": "http://arxiv.org/pdf/2401.02905v2",
    "published_date": "2024-01-05 17:05:33 UTC",
    "updated_date": "2024-11-03 02:26:07 UTC"
  },
  {
    "arxiv_id": "2401.02884v1",
    "title": "MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated Convolution for Image Compressive Sensing (CS)",
    "authors": [
      "Youhao Yu",
      "Richard M. Dansereau"
    ],
    "abstract": "Compressive sensing (CS) is a technique that enables the recovery of sparse\nsignals using fewer measurements than traditional sampling methods. To address\nthe computational challenges of CS reconstruction, our objective is to develop\nan interpretable and concise neural network model for reconstructing natural\nimages using CS. We achieve this by mapping one step of the iterative shrinkage\nthresholding algorithm (ISTA) to a deep network block, representing one\niteration of ISTA. To enhance learning ability and incorporate structural\ndiversity, we integrate aggregated residual transformations (ResNeXt) and\nsqueeze-and-excitation (SE) mechanisms into the ISTA block. This block serves\nas a deep equilibrium layer, connected to a semi-tensor product network\n(STP-Net) for convenient sampling and providing an initial reconstruction. The\nresulting model, called MsDC-DEQ-Net, exhibits competitive performance compared\nto state-of-the-art network-based methods. It significantly reduces storage\nrequirements compared to deep unrolling methods, using only one iteration block\ninstead of multiple iterations. Unlike deep unrolling models, MsDC-DEQ-Net can\nbe iteratively used, gradually improving reconstruction accuracy while\nconsidering computation trade-offs. Additionally, the model benefits from\nmulti-scale dilated convolutions, further enhancing performance.",
    "categories": [
      "eess.IV",
      "cs.AI"
    ],
    "primary_category": "eess.IV",
    "comment": "15 pages, 8 figures, open access journal paper",
    "pdf_url": "http://arxiv.org/pdf/2401.02884v1",
    "published_date": "2024-01-05 16:25:58 UTC",
    "updated_date": "2024-01-05 16:25:58 UTC"
  },
  {
    "arxiv_id": "2401.02873v3",
    "title": "Optimal Chaining of Vehicle Plans with Time Windows",
    "authors": [
      "David Fiedler",
      "Fabio V. Difonzo",
      "Jan Mrkos"
    ],
    "abstract": "For solving problems from the domain of Mobility-on-Demand (MoD), we often\nneed to connect vehicle plans into plans spanning longer time, a process we\ncall plan chaining. As we show in this work, chaining of the plans can be used\nto reduce the size of MoD providers' fleet (fleet-sizing problem) but also to\nreduce the total driven distance by providing high-quality vehicle dispatching\nsolutions in MoD systems. Recently, a solution that uses this principle has\nbeen proposed to solve the fleet-sizing problem. The method does not consider\nthe time flexibility of the plans. Instead, plans are fixed in time and cannot\nbe delayed. However, time flexibility is an essential property of all vehicle\nproblems with time windows. This work presents a new plan chaining formulation\nthat considers delays as allowed by the time windows and a solution method for\nsolving it. Moreover, we prove that the proposed plan chaining method is\noptimal, and we analyze its complexity. Finally, we list some practical\napplications and perform a demonstration for one of them: a new heuristic\nvehicle dispatching method for solving the static dial-a-ride problem. The\ndemonstration results show that our proposed method provides a better solution\nthan the two heuristic baselines for the majority of instances that cannot be\nsolved optimally. At the same time, our method does not have the largest\ncomputational time requirements compared to the baselines. Therefore, we\nconclude that the proposed optimal chaining method provides not only\ntheoretically sound results but is also practically applicable.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "26 pages, 7 figures, submitted to \"Transportation Research Part C:\n  Emerging Technologies\" journal",
    "pdf_url": "http://arxiv.org/pdf/2401.02873v3",
    "published_date": "2024-01-05 16:04:55 UTC",
    "updated_date": "2024-02-24 22:51:53 UTC"
  },
  {
    "arxiv_id": "2401.02870v1",
    "title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models",
    "authors": [
      "Zihong He",
      "Changwang Zhang"
    ],
    "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm\nfor investigating human behavior emulation. Recent research has employed\nLLM-based Agents to create a sociological research environment, in which agents\nexhibit behavior based on the unfiltered characteristics of large language\nmodels. However, these studies overlook the iterative development within a\nhuman-like setting - Human preferences and personalities are complex, shaped by\nvarious factors and subject to ongoing change as a result of environmental and\nsubjective influences. In light of this observation, we propose Agent Framework\nfor Shaping Preference and Personality (AFSPP), exploring the multifaceted\nimpact of social networks and subjective consciousness on LLM-based Agents'\npreference and personality formation. With AFSPP, we have, for the first time,\nsuccessfully replicated several key findings from human personality\nexperiments. And other AFSPP-based experimental results indicate that plan\nmaking, sensory perceptions and social networking with subjective information,\nwield the most pronounced influence on preference shaping. AFSPP can\nsignificantly enhance the efficiency and scope of psychological experiments,\nwhile yielding valuable insights for Trustworthy Artificial Intelligence\nresearch for strategies to prevent undesirable preference and personality\ndevelopment.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02870v1",
    "published_date": "2024-01-05 15:52:59 UTC",
    "updated_date": "2024-01-05 15:52:59 UTC"
  },
  {
    "arxiv_id": "2401.02860v2",
    "title": "Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis",
    "authors": [
      "Naaek Chinpattanakarn",
      "Chainarong Amornbunchornvej"
    ],
    "abstract": "Knowing who follows whom and what patterns they are following are crucial\nsteps to understand collective behaviors (e.g. a group of human, a school of\nfish, or a stock market). Time series is one of resources that can be used to\nget insight regarding following relations. However, the concept of following\npatterns or motifs and the solution to find them in time series are not\nobvious. In this work, we formalize a concept of following motifs between two\ntime series and present a framework to infer following patterns between two\ntime series. The framework utilizes one of efficient and scalable methods to\nretrieve motifs from time series called the Matrix Profile Method. We compare\nour proposed framework with several baselines. The framework performs better\nthan baselines in the simulation datasets. In the dataset of sound recording,\nthe framework is able to retrieve the following motifs within a pair of time\nseries that two singers sing following each other. In the cryptocurrency\ndataset, the framework is capable of capturing the following motifs within a\npair of time series from two digital currencies, which implies that the values\nof one currency follow the values of another currency patterns. Our framework\ncan be utilized in any field of time series to get insight regarding following\npatterns between time series.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "91-08, 68T09",
      "G.3; I.2.6; J.4"
    ],
    "primary_category": "cs.LG",
    "comment": "Revising based on an expert's comments in the research community",
    "pdf_url": "http://arxiv.org/pdf/2401.02860v2",
    "published_date": "2024-01-05 15:32:24 UTC",
    "updated_date": "2024-01-18 04:05:05 UTC"
  },
  {
    "arxiv_id": "2401.02851v2",
    "title": "Natural Language Programming in Medicine: Administering Evidence Based Clinical Workflows with Autonomous Agents Powered by Generative Large Language Models",
    "authors": [
      "Akhil Vaid",
      "Joshua Lampert",
      "Juhee Lee",
      "Ashwin Sawant",
      "Donald Apakama",
      "Ankit Sakhuja",
      "Ali Soroush",
      "Sarah Bick",
      "Ethan Abbott",
      "Hernando Gomez",
      "Michael Hadley",
      "Denise Lee",
      "Isotta Landi",
      "Son Q Duong",
      "Nicole Bussola",
      "Ismail Nabeel",
      "Silke Muehlstedt",
      "Silke Muehlstedt",
      "Robert Freeman",
      "Patricia Kovatch",
      "Brendan Carr",
      "Fei Wang",
      "Benjamin Glicksberg",
      "Edgar Argulian",
      "Stamatios Lerakis",
      "Rohan Khera",
      "David L. Reich",
      "Monica Kraft",
      "Alexander Charney",
      "Girish Nadkarni"
    ],
    "abstract": "Generative Large Language Models (LLMs) hold significant promise in\nhealthcare, demonstrating capabilities such as passing medical licensing exams\nand providing clinical knowledge. However, their current use as information\nretrieval tools is limited by challenges like data staleness, resource demands,\nand occasional generation of incorrect information. This study assessed the\npotential of LLMs to function as autonomous agents in a simulated tertiary care\nmedical center, using real-world clinical cases across multiple specialties.\nBoth proprietary and open-source LLMs were evaluated, with Retrieval Augmented\nGeneration (RAG) enhancing contextual relevance. Proprietary models,\nparticularly GPT-4, generally outperformed open-source models, showing improved\nguideline adherence and more accurate responses with RAG. The manual evaluation\nby expert clinicians was crucial in validating models' outputs, underscoring\nthe importance of human oversight in LLM operation. Further, the study\nemphasizes Natural Language Programming (NLP) as the appropriate paradigm for\nmodifying model behavior, allowing for precise adjustments through tailored\nprompts and real-world interactions. This approach highlights the potential of\nLLMs to significantly enhance and supplement clinical decision-making, while\nalso emphasizing the value of continuous expert involvement and the flexibility\nof NLP to ensure their reliability and effectiveness in healthcare settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Figures: 5, Tables: 3",
    "pdf_url": "http://arxiv.org/pdf/2401.02851v2",
    "published_date": "2024-01-05 15:09:57 UTC",
    "updated_date": "2024-08-22 07:49:39 UTC"
  },
  {
    "arxiv_id": "2401.02843v2",
    "title": "Thousands of AI Authors on the Future of AI",
    "authors": [
      "Katja Grace",
      "Harlan Stewart",
      "Julia Fabienne Sandk√ºhler",
      "Stephen Thomas",
      "Ben Weinstein-Raun",
      "Jan Brauner"
    ],
    "abstract": "In the largest survey of its kind, 2,778 researchers who had published in\ntop-tier artificial intelligence (AI) venues gave predictions on the pace of AI\nprogress and the nature and impacts of advanced AI systems The aggregate\nforecasts give at least a 50% chance of AI systems achieving several milestones\nby 2028, including autonomously constructing a payment processing site from\nscratch, creating a song indistinguishable from a new song by a popular\nmusician, and autonomously downloading and fine-tuning a large language model.\nIf science continues undisrupted, the chance of unaided machines outperforming\nhumans in every possible task was estimated at 10% by 2027, and 50% by 2047.\nThe latter estimate is 13 years earlier than that reached in a similar survey\nwe conducted only one year earlier [Grace et al., 2022]. However, the chance of\nall human occupations becoming fully automatable was forecast to reach 10% by\n2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).\n  Most respondents expressed substantial uncertainty about the long-term value\nof AI progress: While 68.3% thought good outcomes from superhuman AI are more\nlikely than bad, of these net optimists 48% gave at least a 5% chance of\nextremely bad outcomes such as human extinction, and 59% of net pessimists gave\n5% or more to extremely good outcomes. Between 38% and 51% of respondents gave\nat least a 10% chance to advanced AI leading to outcomes as bad as human\nextinction. More than half suggested that \"substantial\" or \"extreme\" concern is\nwarranted about six different AI-related scenarios, including misinformation,\nauthoritarian control, and inequality. There was disagreement about whether\nfaster or slower AI progress would be better for the future of humanity.\nHowever, there was broad agreement that research aimed at minimizing potential\nrisks from AI systems ought to be prioritized more.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "The asterisk indicates the corresponding author. The dagger indicates\n  equal contribution",
    "pdf_url": "http://arxiv.org/pdf/2401.02843v2",
    "published_date": "2024-01-05 14:53:09 UTC",
    "updated_date": "2024-04-30 18:15:42 UTC"
  },
  {
    "arxiv_id": "2401.02839v1",
    "title": "Pheme: Efficient and Conversational Speech Generation",
    "authors": [
      "Pawe≈Ç Budzianowski",
      "Taras Sereda",
      "Tomasz Cichy",
      "Ivan Vuliƒá"
    ],
    "abstract": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02839v1",
    "published_date": "2024-01-05 14:47:20 UTC",
    "updated_date": "2024-01-05 14:47:20 UTC"
  },
  {
    "arxiv_id": "2401.02838v1",
    "title": "CrisisViT: A Robust Vision Transformer for Crisis Image Classification",
    "authors": [
      "Zijun Long",
      "Richard McCreadie",
      "Muhammad Imran"
    ],
    "abstract": "In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02838v1",
    "published_date": "2024-01-05 14:45:45 UTC",
    "updated_date": "2024-01-05 14:45:45 UTC"
  },
  {
    "arxiv_id": "2401.02810v2",
    "title": "Physics-Informed Neural Networks for High-Frequency and Multi-Scale Problems using Transfer Learning",
    "authors": [
      "Abdul Hannan Mustajab",
      "Hao Lyu",
      "Zarghaam Rizvi",
      "Frank Wuttke"
    ],
    "abstract": "Physics-informed neural network (PINN) is a data-driven solver for partial\nand ordinary differential equations(ODEs/PDEs). It provides a unified framework\nto address both forward and inverse problems. However, the complexity of the\nobjective function often leads to training failures. This issue is particularly\nprominent when solving high-frequency and multi-scale problems. We proposed\nusing transfer learning to boost the robustness and convergence of training\nPINN, starting training from low-frequency problems and gradually approaching\nhigh-frequency problems. Through two case studies, we discovered that transfer\nlearning can effectively train PINN to approximate solutions from low-frequency\nproblems to high-frequency problems without increasing network parameters.\nFurthermore, it requires fewer data points and less training time. We\nelaborately described our training strategy, including optimizer selection, and\nsuggested guidelines for using transfer learning to train neural networks for\nsolving more complex problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "comment": "18 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.02810v2",
    "published_date": "2024-01-05 13:45:08 UTC",
    "updated_date": "2024-01-15 13:10:12 UTC"
  },
  {
    "arxiv_id": "2401.02797v3",
    "title": "PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging",
    "authors": [
      "Jinlong He",
      "Pengfei Li",
      "Gang Liu",
      "Genrong He",
      "Zhaolin Chen",
      "Shenjun Zhong"
    ],
    "abstract": "Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs as a\nuniversal solution to address medical multi-modal problems as a generative\ntask. In this paper, we propose a parameter efficient framework for fine-tuning\nMLLMs, specifically validated on medical visual question answering (Med-VQA)\nand medical report generation (MRG) tasks, using public benchmark datasets. We\nalso introduce an evaluation metric using the 5-point Likert scale and its\nweighted average value to measure the quality of the generated reports for MRG\ntasks, where the scale ratings are labelled by both humans manually and the\nGPT-4 model. We further assess the consistency of performance metrics across\ntraditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The\nresults indicate that semantic similarity assessments using GPT-4 align closely\nwith human annotators and provide greater stability, yet they reveal a\ndiscrepancy when compared to conventional lexical similarity measurements. This\nquestions the reliability of lexical similarity metrics for evaluating the\nperformance of generative models in Med-VQA and report generation tasks.\nBesides, our fine-tuned model significantly outperforms GPT-4v. This indicates\nthat without additional fine-tuning, multi-modal models like GPT-4v do not\nperform effectively on medical imaging tasks. The code will be available here:\nhttps://github.com/jinlHe/PeFoMed.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 8 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2401.02797v3",
    "published_date": "2024-01-05 13:22:12 UTC",
    "updated_date": "2025-01-16 02:31:20 UTC"
  },
  {
    "arxiv_id": "2401.02777v2",
    "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models",
    "authors": [
      "Na Liu",
      "Liangyu Chen",
      "Xiaoyu Tian",
      "Wei Zou",
      "Kaijiang Chen",
      "Ming Cui"
    ],
    "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02777v2",
    "published_date": "2024-01-05 12:26:46 UTC",
    "updated_date": "2024-01-30 07:02:30 UTC"
  },
  {
    "arxiv_id": "2401.02773v1",
    "title": "Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode Subsets",
    "authors": [
      "Joao Pereira",
      "Dimitrios Chalatsis",
      "Balint Hodossy",
      "Dario Farina"
    ],
    "abstract": "sEMG pattern recognition algorithms have been explored extensively in\ndecoding movement intent, yet are known to be vulnerable to changing recording\nconditions, exhibiting significant drops in performance across subjects, and\neven across sessions. Multi-channel surface EMG, also referred to as\nhigh-density sEMG (HD-sEMG) systems, have been used to improve performance with\nthe information collected through the use of additional electrodes. However, a\nlack of robustness is ever present due to limited datasets and the difficulties\nin addressing sources of variability, such as electrode placement. In this\nstudy, we propose training on a collection of input channel subsets and\naugmenting our training distribution with data from different electrode\nlocations, simultaneously targeting electrode shift and reducing input\ndimensionality. Our method increases robustness against electrode shift and\nresults in significantly higher intersession performance across subjects and\nclassification algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "5 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.02773v1",
    "published_date": "2024-01-05 12:13:00 UTC",
    "updated_date": "2024-01-05 12:13:00 UTC"
  },
  {
    "arxiv_id": "2401.03006v2",
    "title": "The Rise of Diffusion Models in Time-Series Forecasting",
    "authors": [
      "Caspar Meijer",
      "Lydia Y. Chen"
    ],
    "abstract": "This survey delves into the application of diffusion models in time-series\nforecasting. Diffusion models are demonstrating state-of-the-art results in\nvarious fields of generative AI. The paper includes comprehensive background\ninformation on diffusion models, detailing their conditioning methods and\nreviewing their use in time-series forecasting. The analysis covers 11 specific\ntime-series implementations, the intuition and theory behind them, the\neffectiveness on different datasets, and a comparison among each other. Key\ncontributions of this work are the thorough exploration of diffusion models'\napplications in time-series forecasting and a chronologically ordered overview\nof these models. Additionally, the paper offers an insightful discussion on the\ncurrent state-of-the-art in this domain and outlines potential future research\ndirections. This serves as a valuable resource for researchers in AI and\ntime-series analysis, offering a clear view of the latest advancements and\nfuture potential of diffusion models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Version 2, 24 pages, 10 figures, 12 tables, For complete LuaTeX\n  source:\n  https://github.com/Capsar/The-Rise-of-Diffusion-Models-in-Time-Series-Forecasting\n  , Written by: Caspar Meijer, Supervised by: Lydia Y. Chen",
    "pdf_url": "http://arxiv.org/pdf/2401.03006v2",
    "published_date": "2024-01-05 11:35:10 UTC",
    "updated_date": "2024-01-17 14:02:12 UTC"
  },
  {
    "arxiv_id": "2401.02749v2",
    "title": "Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding",
    "authors": [
      "Yuu Jinnai",
      "Kaito Ariu"
    ],
    "abstract": "Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02749v2",
    "published_date": "2024-01-05 11:02:08 UTC",
    "updated_date": "2024-06-12 01:14:45 UTC"
  },
  {
    "arxiv_id": "2401.09467v1",
    "title": "Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach",
    "authors": [
      "Fatih Ozyurt",
      "Jafar Majidpour",
      "Tarik A. Rashid",
      "Canan Koc"
    ],
    "abstract": "Handwritten signature verification poses a formidable challenge in biometrics\nand document authenticity. The objective is to ascertain the authenticity of a\nprovided handwritten signature, distinguishing between genuine and forged ones.\nThis issue has many applications in sectors such as finance, legal\ndocumentation, and security. Currently, the field of computer vision and\nmachine learning has made significant progress in the domain of handwritten\nsignature verification. The outcomes, however, may be enhanced depending on the\nacquired findings, the structure of the datasets, and the used models. Four\nstages make up our suggested strategy. First, we collected a large dataset of\n12600 images from 420 distinct individuals, and each individual has 30\nsignatures of a certain kind (All authors signatures are genuine). In the\nsubsequent stage, the best features from each image were extracted using a deep\nlearning model named MobileNetV2. During the feature selection step, three\nselectors neighborhood component analysis (NCA), Chi2, and mutual info (MI)\nwere used to pull out 200, 300, 400, and 500 features, giving a total of 12\nfeature vectors. Finally, 12 results have been obtained by applying machine\nlearning techniques such as SVM with kernels (rbf, poly, and linear), KNN, DT,\nLinear Discriminant Analysis, and Naive Bayes. Without employing feature\nselection techniques, our suggested offline signature verification achieved a\nclassification accuracy of 91.3%, whereas using the NCA feature selection\napproach with just 300 features it achieved a classification accuracy of 97.7%.\nHigh classification accuracy was achieved using the designed and suggested\nmodel, which also has the benefit of being a self-organized framework.\nConsequently, using the optimum minimally chosen features, the proposed method\ncould identify the best model performance and result validation prediction\nvectors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2401.09467v1",
    "published_date": "2024-01-05 10:55:20 UTC",
    "updated_date": "2024-01-05 10:55:20 UTC"
  },
  {
    "arxiv_id": "2401.02744v1",
    "title": "MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron Captioning",
    "authors": [
      "Alfirsa Damasyifa Fauzulhaq",
      "Wahyu Parwitayasa",
      "Joseph Ananda Sugihdharma",
      "M. Fadli Ridhani",
      "Novanto Yudistira"
    ],
    "abstract": "Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02744v1",
    "published_date": "2024-01-05 10:41:55 UTC",
    "updated_date": "2024-01-05 10:41:55 UTC"
  },
  {
    "arxiv_id": "2401.02740v3",
    "title": "Fairness-Aware Job Scheduling for Multi-Job Federated Learning",
    "authors": [
      "Yuxin Shi",
      "Han Yu"
    ],
    "abstract": "Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to\ncollaboratively train machine learning models without disclosing sensitive\nprivate data. Existing FL research mostly focuses on the monopoly scenario in\nwhich a single FL server selects a subset of FL clients to update their local\nmodels in each round of training. In practice, there can be multiple FL servers\nsimultaneously trying to select clients from the same pool. In this paper, we\npropose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)\napproach to bridge this gap. Based on Lyapunov optimization, it ensures fair\nallocation of high-demand FL client datasets to FL jobs in need of them, by\njointly considering the current demand and the job payment bids, in order to\nprevent prolonged waiting. Extensive experiments comparing FairFedJS against\nfour state-of-the-art approaches on two datasets demonstrate its significant\nadvantages. It outperforms the best baseline by 31.9% and 1.0% on average in\nterms of scheduling fairness and convergence time, respectively, while\nachieving comparable test accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by ICASSP 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02740v3",
    "published_date": "2024-01-05 10:29:08 UTC",
    "updated_date": "2024-02-08 03:42:47 UTC"
  },
  {
    "arxiv_id": "2401.02731v4",
    "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks",
    "authors": [
      "Haoyuan Wu",
      "Haisheng Zheng",
      "Zhuolun He",
      "Bei Yu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across general tasks. However,\nthese models often encounter performance limitations across multiple tasks due\nto constrained model capacity. Expanding this capacity during the instruction\ntuning phase poses significant challenges. To address this issue, we introduce\nparameter-efficient sparsity crafting (PESC), which crafts dense models into\nsparse models using the mixture-of-experts (MoE) architecture. PESC integrates\nadapters into the MoE layers of sparse models, differentiating experts without\naltering the individual weights within these layers. This method significantly\nreduces computational costs and GPU memory requirements, facilitating model\ncapacity expansion through a minimal parameter increase when guaranteeing the\nquality of approximation in function space compared to original sparse\nupcycling. Our empirical evaluation demonstrates the effectiveness of the PESC\nmethod. Using PESC during instruction tuning, our best sparse model outperforms\nother sparse and dense models and exhibits superior general capabilities\ncompared to GPT-3.5. Our code is available at\nhttps://github.com/wuhy68/Parameter-Efficient-MoE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02731v4",
    "published_date": "2024-01-05 09:58:09 UTC",
    "updated_date": "2024-09-24 14:14:40 UTC"
  },
  {
    "arxiv_id": "2401.02727v2",
    "title": "Enhancing targeted transferability via feature space fine-tuning",
    "authors": [
      "Hui Zeng",
      "Biwei Chen",
      "Anjie Peng"
    ],
    "abstract": "Adversarial examples (AEs) have been extensively studied due to their\npotential for privacy protection and inspiring robust neural networks. Yet,\nmaking a targeted AE transferable across unknown models remains challenging. In\nthis paper, to alleviate the overfitting dilemma common in an AE crafted by\nexisting simple iterative attacks, we propose fine-tuning it in the feature\nspace. Specifically, starting with an AE generated by a baseline attack, we\nencourage the features conducive to the target class and discourage the\nfeatures to the original class in a middle layer of the source model. Extensive\nexperiments demonstrate that only a few iterations of fine-tuning can boost\nexisting attacks' targeted transferability nontrivially and universally. Our\nresults also verify that the simple iterative attacks can yield comparable or\neven better transferability than the resource-intensive methods, which rest on\ntraining target-specific classifiers or generators with additional data. The\ncode is available at: github.com/zengh5/TA_feature_FT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 10 figures, accepted by 2024ICASSP",
    "pdf_url": "http://arxiv.org/pdf/2401.02727v2",
    "published_date": "2024-01-05 09:46:42 UTC",
    "updated_date": "2024-01-13 09:29:13 UTC"
  },
  {
    "arxiv_id": "2401.02726v1",
    "title": "Une ontologie pour les syst{√®}mes multi-agents ambiants dans les villes intelligentes",
    "authors": [
      "Nathan Aky",
      "Denis Payet",
      "Sylvain Giroux",
      "R√©my Courdier"
    ],
    "abstract": "Towns and cities are currently equipping themselves with a host of connected\ndevices, with a view to transforming themselves into ''smart cities''. To\nmanage this mass of connected objects, autonomous software entities, known as\nagents, can be attached to them to cooperate and use these devices to offer\npersonalized services. However, this object infrastructure needs to be\nsemantically structured in order to be exploited. This is why the proposal of\nthis article is an ontology, formatted in OWL, describing the object\ninfrastructures, their links with the organization of the multi-agent system\nand the services to be delivered according to the users of the system. The\nontology is applied to smart mobility for people with reduced mobility, and\ncould be adapted to other smart city axes.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "in French language",
    "pdf_url": "http://arxiv.org/pdf/2401.02726v1",
    "published_date": "2024-01-05 09:42:10 UTC",
    "updated_date": "2024-01-05 09:42:10 UTC"
  },
  {
    "arxiv_id": "2401.02719v1",
    "title": "Learning Image Demoireing from Unpaired Real Data",
    "authors": [
      "Yunshan Zhong",
      "Yuyao Zhou",
      "Yuxin Zhang",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "abstract": "This paper focuses on addressing the issue of image demoireing. Unlike the\nlarge volume of existing studies that rely on learning from paired real data,\nwe attempt to learn a demoireing model from unpaired real data, i.e., moire\nimages associated with irrelevant clean images. The proposed method, referred\nto as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from\nunpaired datasets, generating pairs with clean images for training demoireing\nmodels. To achieve this, we divide real moire images into patches and group\nthem in compliance with their moire complexity. We introduce a novel moire\ngeneration framework to synthesize moire images with diverse moire features,\nresembling real moire patches, and details akin to real moire-free images.\nAdditionally, we introduce an adaptive denoise method to eliminate the\nlow-quality pseudo moire images that adversely impact the learning of\ndemoireing models. We conduct extensive experiments on the commonly-used FHDMi\nand UHDM datasets. Results manifest that our UnDeM performs better than\nexisting methods when using existing demoireing models such as MBCNN and\nESDNet-L. Code: https://github.com/zysxmu/UnDeM",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "AAAI2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02719v1",
    "published_date": "2024-01-05 09:26:35 UTC",
    "updated_date": "2024-01-05 09:26:35 UTC"
  },
  {
    "arxiv_id": "2401.02717v2",
    "title": "Complementary Information Mutual Learning for Multimodality Medical Image Segmentation",
    "authors": [
      "Chuyun Shen",
      "Wenhao Li",
      "Haoqing Chen",
      "Xiaoling Wang",
      "Fengping Zhu",
      "Yuxin Li",
      "Xiangfeng Wang",
      "Bo Jin"
    ],
    "abstract": "Radiologists must utilize multiple modal images for tumor segmentation and\ndiagnosis due to the limitations of medical imaging and the diversity of tumor\nsignals. This leads to the development of multimodal learning in segmentation.\nHowever, the redundancy among modalities creates challenges for existing\nsubtraction-based joint learning methods, such as misjudging the importance of\nmodalities, ignoring specific modal information, and increasing cognitive load.\nThese thorny issues ultimately decrease segmentation accuracy and increase the\nrisk of overfitting. This paper presents the complementary information mutual\nlearning (CIML) framework, which can mathematically model and address the\nnegative impact of inter-modal redundant information. CIML adopts the idea of\naddition and removes inter-modal redundant information through inductive\nbias-driven task decomposition and message passing-based redundancy filtering.\nCIML first decomposes the multimodal segmentation task into multiple subtasks\nbased on expert prior knowledge, minimizing the information dependence between\nmodalities. Furthermore, CIML introduces a scheme in which each modality can\nextract information from other modalities additively through message passing.\nTo achieve non-redundancy of extracted information, the redundant filtering is\ntransformed into complementary information learning inspired by the variational\ninformation bottleneck. The complementary information learning procedure can be\nefficiently solved by variational inference and cross-modal spatial attention.\nNumerical results from the verification task and standard benchmarks indicate\nthat CIML efficiently removes redundant information between modalities,\noutperforming SOTA methods regarding validation accuracy and segmentation\neffect.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "35 pages, 18 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02717v2",
    "published_date": "2024-01-05 09:21:45 UTC",
    "updated_date": "2024-07-10 07:07:11 UTC"
  },
  {
    "arxiv_id": "2401.02713v1",
    "title": "Graph-level Protein Representation Learning by Structure Knowledge Refinement",
    "authors": [
      "Ge Wang",
      "Zelin Zang",
      "Jiangbin Zheng",
      "Jun Xia",
      "Stan Z. Li"
    ],
    "abstract": "This paper focuses on learning representation on the whole graph level in an\nunsupervised manner. Learning graph-level representation plays an important\nrole in a variety of real-world issues such as molecule property prediction,\nprotein structure feature extraction, and social network analysis. The\nmainstream method is utilizing contrastive learning to facilitate graph feature\nextraction, known as Graph Contrastive Learning (GCL). GCL, although effective,\nsuffers from some complications in contrastive learning, such as the effect of\nfalse negative pairs. Moreover, augmentation strategies in GCL are weakly\nadaptive to diverse graph datasets. Motivated by these problems, we propose a\nnovel framework called Structure Knowledge Refinement (SKR) which uses data\nstructure to determine the probability of whether a pair is positive or\nnegative. Meanwhile, we propose an augmentation strategy that naturally\npreserves the semantic meaning of the original data and is compatible with our\nSKR framework. Furthermore, we illustrate the effectiveness of our SKR\nframework through intuition and experiments. The experimental results on the\ntasks of graph-level classification demonstrate that our SKR framework is\nsuperior to most state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02713v1",
    "published_date": "2024-01-05 09:05:33 UTC",
    "updated_date": "2024-01-05 09:05:33 UTC"
  },
  {
    "arxiv_id": "2401.02710v2",
    "title": "Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning",
    "authors": [
      "Hong-Gi Shin",
      "Sukhyun Jeong",
      "Eui-Yeon Kim",
      "Sungho Hong",
      "Young-Jin Cho",
      "Yong-Hoon Choi"
    ],
    "abstract": "Mining of formulaic alpha factors refers to the process of discovering and\ndeveloping specific factors or indicators (referred to as alpha factors) for\nquantitative trading in stock market. To efficiently discover alpha factors in\nvast search space, reinforcement learning (RL) is commonly employed. This paper\nproposes a method to enhance existing alpha factor mining approaches by\nexpanding a search space and utilizing pretrained formulaic alpha set as\ninitial seed values to generate synergistic formulaic alpha. We employ\ninformation coefficient (IC) and rank information coefficient (Rank IC) as\nperformance evaluation metrics for the model. Using CSI300 market data, we\nconducted real investment simulations and observed significant performance\nimprovement compared to existing techniques.",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE",
    "comment": "Accepted by ICOIN 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.02710v2",
    "published_date": "2024-01-05 08:49:13 UTC",
    "updated_date": "2024-07-08 02:59:56 UTC"
  },
  {
    "arxiv_id": "2401.02709v1",
    "title": "German Text Embedding Clustering Benchmark",
    "authors": [
      "Silvan Wehrli",
      "Bert Arnrich",
      "Christopher Irrgang"
    ],
    "abstract": "This work introduces a benchmark assessing the performance of clustering\nGerman text embeddings in different domains. This benchmark is driven by the\nincreasing use of clustering neural text embeddings in tasks that require the\ngrouping of texts (such as topic modeling) and the need for German resources in\nexisting benchmarks. We provide an initial analysis for a range of pre-trained\nmono- and multilingual models evaluated on the outcome of different clustering\nalgorithms. Results include strong performing mono- and multilingual models.\nReducing the dimensions of embeddings can further improve clustering.\nAdditionally, we conduct experiments with continued pre-training for German\nBERT models to estimate the benefits of this additional training. Our\nexperiments suggest that significant performance improvements are possible for\nshort text. All code and datasets are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02709v1",
    "published_date": "2024-01-05 08:42:45 UTC",
    "updated_date": "2024-01-05 08:42:45 UTC"
  },
  {
    "arxiv_id": "2401.02708v1",
    "title": "TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis",
    "authors": [
      "Liwen Zhang",
      "Lianzhen Zhong",
      "Fan Yang",
      "Di Dong",
      "Hui Hui",
      "Jie Tian"
    ],
    "abstract": "A core challenge in survival analysis is to model the distribution of\ncensored time-to-event data, where the event of interest may be a death,\nfailure, or occurrence of a specific event. Previous studies have showed that\nranking and maximum likelihood estimation (MLE)loss functions are widely-used\nfor survival analysis. However, ranking loss only focus on the ranking of\nsurvival time and does not consider potential effect of samples for exact\nsurvival time values. Furthermore, the MLE is unbounded and easily subject to\noutliers (e.g., censored data), which may cause poor performance of modeling.\nTo handle the complexities of learning process and exploit valuable survival\ntime values, we propose a time-adaptive coordinate loss function, TripleSurv,\nto achieve adaptive adjustments by introducing the differences in the survival\ntime between sample pairs into the ranking, which can encourage the model to\nquantitatively rank relative risk of pairs, ultimately enhancing the accuracy\nof predictions. Most importantly, the TripleSurv is proficient in quantifying\nthe relative risk between samples by ranking ordering of pairs, and consider\nthe time interval as a trade-off to calibrate the robustness of model over\nsample distribution. Our TripleSurv is evaluated on three real-world survival\ndatasets and a public synthetic dataset. The results show that our method\noutperforms the state-of-the-art methods and exhibits good model performance\nand robustness on modeling various sophisticated data distributions with\ndifferent censor rates. Our code will be available upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages,6 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02708v1",
    "published_date": "2024-01-05 08:37:57 UTC",
    "updated_date": "2024-01-05 08:37:57 UTC"
  },
  {
    "arxiv_id": "2401.02705v2",
    "title": "XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model",
    "authors": [
      "Zhitao Wang",
      "Wei Wang",
      "Zirao Li",
      "Long Wang",
      "Can Yi",
      "Xinjie Xu",
      "Luyang Cao",
      "Hanjing Su",
      "Shouzhi Chen",
      "Jun Zhou"
    ],
    "abstract": "In past years, we have been dedicated to automating user acceptance testing\n(UAT) process of WeChat Pay, one of the most influential mobile payment\napplications in China. A system titled XUAT has been developed for this\npurpose. However, there is still a human-labor-intensive stage, i.e, test\nscripts generation, in the current system. Therefore, in this paper, we\nconcentrate on methods of boosting the automation level of the current system,\nparticularly the stage of test scripts generation. With recent notable\nsuccesses, large language models (LLMs) demonstrate significant potential in\nattaining human-like intelligence and there has been a growing research area\nthat employs LLMs as autonomous agents to obtain human-like decision-making\ncapabilities. Inspired by these works, we propose an LLM-powered multi-agent\ncollaborative system, named XUAT-Copilot, for automated UAT. The proposed\nsystem mainly consists of three LLM-based agents responsible for action\nplanning, state checking and parameter selecting, respectively, and two\nadditional modules for state sensing and case rewriting. The agents interact\nwith testing device, make human-like decision and generate action command in a\ncollaborative way. The proposed multi-agent system achieves a close\neffectiveness to human testers in our experimental studies and gains a\nsignificant improvement of Pass@1 accuracy compared with single-agent\narchitecture. More importantly, the proposed system has launched in the formal\ntesting environment of WeChat Pay mobile app, which saves a considerable amount\nof manpower in the daily development work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02705v2",
    "published_date": "2024-01-05 08:24:30 UTC",
    "updated_date": "2024-01-10 12:08:44 UTC"
  },
  {
    "arxiv_id": "2401.02703v1",
    "title": "Verifying Relational Explanations: A Probabilistic Approach",
    "authors": [
      "Abisha Thapa Magar",
      "Anup Shakya",
      "Somdeb Sarkhel",
      "Deepak Venugopal"
    ],
    "abstract": "Explanations on relational data are hard to verify since the explanation\nstructures are more complex (e.g. graphs). To verify interpretable explanations\n(e.g. explanations of predictions made in images, text, etc.), typically human\nsubjects are used since it does not necessarily require a lot of expertise.\nHowever, to verify the quality of a relational explanation requires expertise\nand is hard to scale-up. GNNExplainer is arguably one of the most popular\nexplanation methods for Graph Neural Networks. In this paper, we develop an\napproach where we assess the uncertainty in explanations generated by\nGNNExplainer. Specifically, we ask the explainer to generate explanations for\nseveral counterfactual examples. We generate these examples as symmetric\napproximations of the relational structure in the original data. From these\nexplanations, we learn a factor graph model to quantify uncertainty in an\nexplanation. Our results on several datasets show that our approach can help\nverify explanations from GNNExplainer by reliably estimating the uncertainty of\na relation specified in the explanation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published in Proceedings of 2023 IEEE Conference on Big Data",
    "pdf_url": "http://arxiv.org/pdf/2401.02703v1",
    "published_date": "2024-01-05 08:14:51 UTC",
    "updated_date": "2024-01-05 08:14:51 UTC"
  },
  {
    "arxiv_id": "2401.02683v2",
    "title": "Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation",
    "authors": [
      "Can Xu",
      "Haosen Wang",
      "Weigang Wang",
      "Pengfei Zheng",
      "Hongyang Chen"
    ],
    "abstract": "Denoising diffusion models have shown great potential in multiple research\nareas. Existing diffusion-based generative methods on de novo 3D molecule\ngeneration face two major challenges. Since majority heavy atoms in molecules\nallow connections to multiple atoms through single bonds, solely using\npair-wise distance to model molecule geometries is insufficient. Therefore, the\nfirst one involves proposing an effective neural network as the denoising\nkernel that is capable to capture complex multi-body interatomic relationships\nand learn high-quality features. Due to the discrete nature of graphs,\nmainstream diffusion-based methods for molecules heavily rely on predefined\nrules and generate edges in an indirect manner. The second challenge involves\naccommodating molecule generation to diffusion and accurately predicting the\nexistence of bonds. In our research, we view the iterative way of updating\nmolecule conformations in diffusion process is consistent with molecular\ndynamics and introduce a novel molecule generation method named\nGeometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,\nwe introduce a Dual-Track Transformer Network (DTN) to fully excevate global\nspatial relationships and learn high quality representations which contribute\nto accurate predictions of features and geometries. As for the second\nchallenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the\nformation of bonds during the training period, instead of directly embedding\nedges into the latent space. Comprehensive experiments on current benchmarks\ndemonstrate the superiority of GFMDiff.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 6 figures, AAAI-24 Main Track",
    "pdf_url": "http://arxiv.org/pdf/2401.02683v2",
    "published_date": "2024-01-05 07:29:21 UTC",
    "updated_date": "2024-04-22 13:02:20 UTC"
  },
  {
    "arxiv_id": "2401.02677v1",
    "title": "Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss",
    "authors": [
      "Yatharth Gupta",
      "Vishnu V. Jaddipal",
      "Harish Prabhala",
      "Sayak Paul",
      "Patrick Von Platen"
    ],
    "abstract": "Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02677v1",
    "published_date": "2024-01-05 07:21:46 UTC",
    "updated_date": "2024-01-05 07:21:46 UTC"
  },
  {
    "arxiv_id": "2401.02673v1",
    "title": "A unified multichannel far-field speech recognition system: combining neural beamforming with attention based end-to-end model",
    "authors": [
      "Dongdi Zhao",
      "Jianbo Ma",
      "Lu Lu",
      "Jinke Li",
      "Xuan Ji",
      "Lei Zhu",
      "Fuming Fang",
      "Ming Liu",
      "Feijun Jiang"
    ],
    "abstract": "Far-field speech recognition is a challenging task that conventionally uses\nsignal processing beamforming to attack noise and interference problem. But the\nperformance has been found usually limited due to heavy reliance on\nenvironmental assumption. In this paper, we propose a unified multichannel\nfar-field speech recognition system that combines the neural beamforming and\ntransformer-based Listen, Spell, Attend (LAS) speech recognition system, which\nextends the end-to-end speech recognition system further to include speech\nenhancement. Such framework is then jointly trained to optimize the final\nobjective of interest. Specifically, factored complex linear projection (fCLP)\nhas been adopted to form the neural beamforming. Several pooling strategies to\ncombine look directions are then compared in order to find the optimal\napproach. Moreover, information of the source direction is also integrated in\nthe beamforming to explore the usefulness of source direction as a prior, which\nis usually available especially in multi-modality scenario. Experiments on\ndifferent microphone array geometry are conducted to evaluate the robustness\nagainst spacing variance of microphone array. Large in-house databases are used\nto evaluate the effectiveness of the proposed framework and the proposed method\nachieve 19.26\\% improvement when compared with a strong baseline.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02673v1",
    "published_date": "2024-01-05 07:11:13 UTC",
    "updated_date": "2024-01-05 07:11:13 UTC"
  },
  {
    "arxiv_id": "2401.02665v1",
    "title": "Zero-shot Microclimate Prediction with Deep Learning",
    "authors": [
      "Iman Deznabi",
      "Peeyush Kumar",
      "Madalina Fiterau"
    ],
    "abstract": "Weather station data is a valuable resource for climate prediction, however,\nits reliability can be limited in remote locations. To compound the issue,\nmaking local predictions often relies on sensor data that may not be accessible\nfor a new, previously unmonitored location. In response to these challenges, we\npropose a novel zero-shot learning approach designed to forecast various\nclimate measurements at new and unmonitored locations. Our method surpasses\nconventional weather forecasting techniques in predicting microclimate\nvariables by leveraging knowledge extracted from other geographic locations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02665v1",
    "published_date": "2024-01-05 06:46:56 UTC",
    "updated_date": "2024-01-05 06:46:56 UTC"
  },
  {
    "arxiv_id": "2401.02663v2",
    "title": "Effective backdoor attack on graph neural networks in link prediction tasks",
    "authors": [
      "Jiazhu Dai",
      "Haoyu Sun"
    ],
    "abstract": "Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02663v2",
    "published_date": "2024-01-05 06:45:48 UTC",
    "updated_date": "2025-05-06 07:36:14 UTC"
  },
  {
    "arxiv_id": "2401.02661v1",
    "title": "Nurse-in-the-Loop Artificial Intelligence for Precision Management of Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive Digital Twin",
    "authors": [
      "Syed Hasib Akhter Faruqui",
      "Adel Alaeddini",
      "Yan Du",
      "Shiyu Li",
      "Kumar Sharma",
      "Jing Wang"
    ],
    "abstract": "Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a\nsignificant risk of serious health complications and negative impacts on the\nquality of life. Given the impact of individual characteristics and lifestyle\non the treatment plan and patient outcomes, it is crucial to develop precise\nand personalized management strategies. Artificial intelligence (AI) provides\ngreat promise in combining patterns from various data sources with nurses'\nexpertise to achieve optimal care. Methods: This is a 6-month ancillary study\namong T2D patients (n = 20, age = 57 +- 10). Participants were randomly\nassigned to an intervention (AI, n=10) group to receive daily AI-generated\nindividualized feedback or a control group without receiving the daily feedback\n(non-AI, n=10) in the last three months. The study developed an online\nnurse-in-the-loop predictive control (ONLC) model that utilizes a predictive\ndigital twin (PDT). The PDT was developed using a transfer-learning-based\nArtificial Neural Network. The PDT was trained on participants self-monitoring\ndata (weight, food logs, physical activity, glucose) from the first three\nmonths, and the online control algorithm applied particle swarm optimization to\nidentify impactful behavioral changes for maintaining the patient's glucose and\nweight levels for the next three months. The ONLC provided the intervention\ngroup with individualized feedback and recommendations via text messages. The\nPDT was re-trained weekly to improve its performance. Findings: The trained\nONLC model achieved >=80% prediction accuracy across all patients while the\nmodel was tuned online. Participants in the intervention group exhibited a\ntrend of improved daily steps and stable or improved total caloric and total\ncarb intake as recommended.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted for review",
    "pdf_url": "http://arxiv.org/pdf/2401.02661v1",
    "published_date": "2024-01-05 06:38:50 UTC",
    "updated_date": "2024-01-05 06:38:50 UTC"
  },
  {
    "arxiv_id": "2401.04126v2",
    "title": "The Concept of the Tactile Signature System for Individuals with Visual Impairments",
    "authors": [
      "Anatoliy Kremenchutskiy",
      "Galymzhan Gabdreshov"
    ],
    "abstract": "The lack of an accessible and effective system for blind individuals to\ncreate handwritten signatures presents a significant barrier to their\nindependence and full participation in various aspects of life. This research\nintroduces the Tactile Signature System, a groundbreaking approach that\nempowers individuals with visual impairments to form their unique handwritten\nsignatures. Key features of the system include: Personalized customization:\nThrough tactile interaction and voice algorithmic guidance, individuals create\nsignatures reflecting their preferences and natural writing style. Real-time\nfeedback: AI-powered voice prompts and analysis ensure accuracy and consistency\nin signature formation. Accessibility: Installation in local service centers\nprovides a secure and supervised environment for signature creation. The\nsystem's impact reaches beyond the individual level: Promotes inclusivity and\nindependence: Blind individuals can engage in legal and financial transactions\nwithout relying on others. Empowers and fosters equal opportunities:\nParticipation in education, employment, and civic engagement becomes more\naccessible. Aligns with international conventions: Upholds the right of persons\nwith disabilities to participate fully in society. The Tactile Signature System\nrepresents a significant step towards an inclusive and accessible future for\nindividuals with visual impairments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "The principles of connection between the interaction of a blind\n  individual with the tactile field, when he uses touch, drawing various\n  figures and forms, and the resulting prompts to the user to create a valid\n  signature, have not been disclosed",
    "pdf_url": "http://arxiv.org/pdf/2401.04126v2",
    "published_date": "2024-01-05 06:08:27 UTC",
    "updated_date": "2024-01-11 05:48:38 UTC"
  },
  {
    "arxiv_id": "2401.02653v1",
    "title": "A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in Smart Grids",
    "authors": [
      "Viorica Rozina Chifu",
      "Tudor Cioara",
      "Cristina Bianca Pop",
      "Horia Rusu",
      "Ionut Anghel"
    ],
    "abstract": "Economic and policy factors are driving the continuous increase in the\nadoption and usage of electrical vehicles (EVs). However, despite being a\ncleaner alternative to combustion engine vehicles, EVs have negative impacts on\nthe lifespan of microgrid equipment and energy balance due to increased power\ndemand and the timing of their usage. In our view grid management should\nleverage on EVs scheduling flexibility to support local network balancing\nthrough active participation in demand response programs. In this paper, we\npropose a model-free solution, leveraging Deep Q-Learning to schedule the\ncharging and discharging activities of EVs within a microgrid to align with a\ntarget energy profile provided by the distribution system operator. We adapted\nthe Bellman Equation to assess the value of a state based on specific rewards\nfor EV scheduling actions and used a neural network to estimate Q-values for\navailable actions and the epsilon-greedy algorithm to balance exploitation and\nexploration to meet the target energy profile. The results are promising\nshowing that the proposed solution can effectively schedule the EVs charging\nand discharging actions to align with the target profile with a Person\ncoefficient of 0.99, handling effective EVs scheduling situations that involve\ndynamicity given by the e-mobility features, relying only on data with no\nknowledge of EVs and microgrid dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to journal",
    "pdf_url": "http://arxiv.org/pdf/2401.02653v1",
    "published_date": "2024-01-05 06:04:46 UTC",
    "updated_date": "2024-01-05 06:04:46 UTC"
  },
  {
    "arxiv_id": "2401.02652v1",
    "title": "Adaptive Discounting of Training Time Attacks",
    "authors": [
      "Ridhima Bector",
      "Abhay Aradhya",
      "Chai Quek",
      "Zinovi Rabinovich"
    ],
    "abstract": "Among the most insidious attacks on Reinforcement Learning (RL) solutions are\ntraining-time attacks (TTAs) that create loopholes and backdoors in the learned\nbehaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are\nnow available, where the attacker forces a specific, target behaviour upon a\ntraining RL agent (victim). However, even state-of-the-art C-TTAs focus on\ntarget behaviours that could be naturally adopted by the victim if not for a\nparticular feature of the environment dynamics, which C-TTAs exploit. In this\nwork, we show that a C-TTA is possible even when the target behaviour is\nun-adoptable due to both environment dynamics as well as non-optimality with\nrespect to the victim objective(s). To find efficient attacks in this context,\nwe develop a specialised flavour of the DDPG algorithm, which we term\ngammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically\nalters the attack policy planning horizon based on the victim's current\nbehaviour. This improves effort distribution throughout the attack timeline and\nreduces the effect of uncertainty the attacker has about the victim. To\ndemonstrate the features of our method and better relate the results to prior\nresearch, we borrow a 3D grid domain from a state-of-the-art C-TTA for our\nexperiments. Code is available at \"bit.ly/github-rb-gDDPG\".",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02652v1",
    "published_date": "2024-01-05 06:03:14 UTC",
    "updated_date": "2024-01-05 06:03:14 UTC"
  },
  {
    "arxiv_id": "2401.02644v1",
    "title": "Simple Hierarchical Planning with Diffusion",
    "authors": [
      "Chang Chen",
      "Fei Deng",
      "Kenji Kawaguchi",
      "Caglar Gulcehre",
      "Sungjin Ahn"
    ],
    "abstract": "Diffusion-based generative methods have proven effective in modeling\ntrajectories with offline datasets. However, they often face computational\nchallenges and can falter in generalization, especially in capturing temporal\nabstractions for long-horizon tasks. To overcome this, we introduce the\nHierarchical Diffuser, a simple, fast, yet surprisingly effective planning\nmethod combining the advantages of hierarchical and diffusion-based planning.\nOur model adopts a \"jumpy\" planning strategy at the higher level, which allows\nit to have a larger receptive field but at a lower computational cost -- a\ncrucial factor for diffusion-based planning methods, as we have empirically\nverified. Additionally, the jumpy sub-goals guide our low-level planner,\nfacilitating a fine-tuning stage and further improving our approach's\neffectiveness. We conducted empirical evaluations on standard offline\nreinforcement learning benchmarks, demonstrating our method's superior\nperformance and efficiency in terms of training and planning speed compared to\nthe non-hierarchical Diffuser as well as other hierarchical planning methods.\nMoreover, we explore our model's generalization capability, particularly on how\nour method improves generalization capabilities on compositional\nout-of-distribution tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02644v1",
    "published_date": "2024-01-05 05:28:40 UTC",
    "updated_date": "2024-01-05 05:28:40 UTC"
  },
  {
    "arxiv_id": "2401.02643v1",
    "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
    "authors": [
      "Jiahang Zhou",
      "Yanyu Chen",
      "Zicong Hong",
      "Wuhui Chen",
      "Yue Yu",
      "Tao Zhang",
      "Hui Wang",
      "Chuanfu Zhang",
      "Zibin Zheng"
    ],
    "abstract": "Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\\Sigma$)\nhave demonstrated extraordinary performance in key technological areas, such as\nnatural language processing and visual recognition, and have become the\nmainstream trend of artificial general intelligence. This has led more and more\nmajor technology giants to dedicate significant human and financial resources\nto actively develop their foundation model systems, which drives continuous\ngrowth of these models' parameters. As a result, the training and serving of\nthese models have posed significant challenges, including substantial computing\npower, memory consumption, bandwidth demands, etc. Therefore, employing\nefficient training and serving strategies becomes particularly crucial. Many\nresearchers have actively explored and proposed effective methods. So, a\ncomprehensive survey of them is essential for system developers and\nresearchers. This paper extensively explores the methods employed in training\nand serving foundation models from various perspectives. It provides a detailed\ncategorization of these state-of-the-art methods, including finer aspects such\nas network, computing, and storage. Additionally, the paper summarizes the\nchallenges and presents a perspective on the future development direction of\nfoundation model systems. Through comprehensive discussion and analysis, it\nhopes to provide a solid theoretical basis and practical guidance for future\nresearch and applications, promoting continuous innovation and development in\nfoundation model systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02643v1",
    "published_date": "2024-01-05 05:27:15 UTC",
    "updated_date": "2024-01-05 05:27:15 UTC"
  },
  {
    "arxiv_id": "2401.02627v2",
    "title": "Characteristics and prevalence of fake social media profiles with AI-generated faces",
    "authors": [
      "Kai-Cheng Yang",
      "Danishjeet Singh",
      "Filippo Menczer"
    ],
    "abstract": "Recent advancements in generative artificial intelligence (AI) have raised\nconcerns about their potential to create convincing fake social media accounts,\nbut empirical evidence is lacking. In this paper, we present a systematic\nanalysis of Twitter (X) accounts using human faces generated by Generative\nAdversarial Networks (GANs) for their profile pictures. We present a dataset of\n1,420 such accounts and show that they are used to spread scams, spam, and\namplify coordinated messages, among other inauthentic activities. Leveraging a\nfeature of GAN-generated faces -- consistent eye placement -- and supplementing\nit with human annotation, we devise an effective method for identifying\nGAN-generated profiles in the wild. Applying this method to a random sample of\nactive Twitter users, we estimate a lower bound for the prevalence of profiles\nusing GAN-generated faces between 0.021% and 0.044% -- around 10K daily active\naccounts. These findings underscore the emerging threats posed by multimodal\ngenerative AI. We release the source code of our detection method and the data\nwe collect to facilitate further investigation. Additionally, we provide\npractical heuristics to assist social media users in recognizing such accounts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages, 6 figures; added a new dataset and made some minor\n  revisions",
    "pdf_url": "http://arxiv.org/pdf/2401.02627v2",
    "published_date": "2024-01-05 04:10:46 UTC",
    "updated_date": "2024-07-04 00:30:41 UTC"
  },
  {
    "arxiv_id": "2401.10273v2",
    "title": "Revolutionizing Pharma: Unveiling the AI and LLM Trends in the Pharmaceutical Industry",
    "authors": [
      "Yu Han",
      "Jingwen Tao"
    ],
    "abstract": "This document offers a critical overview of the emerging trends and\nsignificant advancements in artificial intelligence (AI) within the\npharmaceutical industry. Detailing its application across key operational\nareas, including research and development, animal testing, clinical trials,\nhospital clinical stages, production, regulatory affairs, quality control and\nother supporting areas, the paper categorically examines AI's role in each\nsector. Special emphasis is placed on cutting-edge AI technologies like machine\nlearning algorithms and their contributions to various aspects of\npharmaceutical operations. Through this comprehensive analysis, the paper\nhighlights the transformative potential of AI in reshaping the pharmaceutical\nindustry's future.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.10273v2",
    "published_date": "2024-01-05 04:01:09 UTC",
    "updated_date": "2024-01-22 04:47:20 UTC"
  },
  {
    "arxiv_id": "2401.02620v1",
    "title": "Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human",
    "authors": [
      "Song Bai",
      "Jie Li"
    ],
    "abstract": "While AI-generated text and 2D images continue to expand its territory, 3D\ngeneration has gradually emerged as a trend that cannot be ignored. Since the\nyear 2023 an abundant amount of research papers has emerged in the domain of 3D\ngeneration. This growth encompasses not just the creation of 3D objects, but\nalso the rapid development of 3D character and motion generation. Several key\nfactors contribute to this progress. The enhanced fidelity in stable diffusion,\ncoupled with control methods that ensure multi-view consistency, and realistic\nhuman models like SMPL-X, contribute synergistically to the production of 3D\nmodels with remarkable consistency and near-realistic appearances. The\nadvancements in neural network-based 3D storing and rendering models, such as\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have\naccelerated the efficiency and realism of neural rendered models. Furthermore,\nthe multimodality capabilities of large language models have enabled language\ninputs to transcend into human motion outputs. This paper aims to provide a\ncomprehensive overview and summary of the relevant papers published mostly\nduring the latter half year of 2023. It will begin by discussing the AI\ngenerated object models in 3D, followed by the generated 3D human models, and\nfinally, the generated 3D human motions, culminating in a conclusive summary\nand a vision for the future.",
    "categories": [
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02620v1",
    "published_date": "2024-01-05 03:41:38 UTC",
    "updated_date": "2024-01-05 03:41:38 UTC"
  },
  {
    "arxiv_id": "2401.02602v2",
    "title": "Neural Causal Abstractions",
    "authors": [
      "Kevin Xia",
      "Elias Bareinboim"
    ],
    "abstract": "The abilities of humans to understand the world in terms of cause and effect\nrelationships, as well as to compress information into abstract concepts, are\ntwo hallmark features of human intelligence. These two topics have been studied\nin tandem in the literature under the rubric of causal abstractions theory. In\npractice, it remains an open problem how to best leverage abstraction theory in\nreal-world causal inference tasks, where the true mechanisms are unknown and\nonly limited data is available. In this paper, we develop a new family of\ncausal abstractions by clustering variables and their domains. This approach\nrefines and generalizes previous notions of abstractions to better accommodate\nindividual causal distributions that are spawned by Pearl's causal hierarchy.\nWe show that such abstractions are learnable in practical settings through\nNeural Causal Models (Xia et al., 2021), enabling the use of the deep learning\ntoolkit to solve various challenging causal inference tasks -- identification,\nestimation, sampling -- at different levels of granularity. Finally, we\nintegrate these results with representation learning to create more flexible\nabstractions, moving these results closer to practical applications. Our\nexperiments support the theory and illustrate how to scale causal inferences to\nhigh-dimensional settings involving image data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "48 total pages, 20 figures, short version accepted to AAAI-24",
    "pdf_url": "http://arxiv.org/pdf/2401.02602v2",
    "published_date": "2024-01-05 02:00:27 UTC",
    "updated_date": "2024-02-23 02:22:42 UTC"
  },
  {
    "arxiv_id": "2401.02600v1",
    "title": "Object-oriented backdoor attack against image captioning",
    "authors": [
      "Meiling Li",
      "Nan Zhong",
      "Xinpeng Zhang",
      "Zhenxing Qian",
      "Sheng Li"
    ],
    "abstract": "Backdoor attack against image classification task has been widely studied and\nproven to be successful, while there exist little research on the backdoor\nattack against vision-language models. In this paper, we explore backdoor\nattack towards image captioning models by poisoning training data. Assuming the\nattacker has total access to the training dataset, and cannot intervene in\nmodel construction or training process. Specifically, a portion of benign\ntraining samples is randomly selected to be poisoned. Afterwards, considering\nthat the captions are usually unfolded around objects in an image, we design an\nobject-oriented method to craft poisons, which aims to modify pixel values by a\nslight range with the modification number proportional to the scale of the\ncurrent detected object region. After training with the poisoned data, the\nattacked model behaves normally on benign images, but for poisoned images, the\nmodel will generate some sentences irrelevant to the given image. The attack\ncontrols the model behavior on specific test images without sacrificing the\ngeneration performance on benign test images. Our method proves the weakness of\nimage captioning models to backdoor attack and we hope this work can raise the\nawareness of defending against backdoor attack in the image captioning field.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02600v1",
    "published_date": "2024-01-05 01:52:13 UTC",
    "updated_date": "2024-01-05 01:52:13 UTC"
  },
  {
    "arxiv_id": "2401.10272v1",
    "title": "Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization",
    "authors": [
      "Yikang Wei",
      "Yahong Han"
    ],
    "abstract": "Federated Domain Generalization aims to learn a domain-invariant model from\nmultiple decentralized source domains for deployment on unseen target domain.\nDue to privacy concerns, the data from different source domains are kept\nisolated, which poses challenges in bridging the domain gap. To address this\nissue, we propose a Multi-source Collaborative Gradient Discrepancy\nMinimization (MCGDM) method for federated domain generalization. Specifically,\nwe propose intra-domain gradient matching between the original images and\naugmented images to avoid overfitting the domain-specific information within\nisolated domains. Additionally, we propose inter-domain gradient matching with\nthe collaboration of other domains, which can further reduce the domain shift\nacross decentralized domains. Combining intra-domain and inter-domain gradient\nmatching, our method enables the learned model to generalize well on unseen\ndomains. Furthermore, our method can be extended to the federated domain\nadaptation task by fine-tuning the target model on the pseudo-labeled target\ndomain. The extensive experiments on federated domain generalization and\nadaptation indicate that our method outperforms the state-of-the-art methods\nsignificantly.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2401.10272v1",
    "published_date": "2024-01-05 01:21:37 UTC",
    "updated_date": "2024-01-05 01:21:37 UTC"
  },
  {
    "arxiv_id": "2401.02591v1",
    "title": "Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data",
    "authors": [
      "Hung Nguyen",
      "Morris Chang"
    ],
    "abstract": "This study examines the impact of class-imbalanced data on deep learning\nmodels and proposes a technique for data balancing by generating synthetic data\nfor the minority class. Unlike random-based oversampling, our method\nprioritizes balancing the informative regions by identifying high entropy\nsamples. Generating well-placed synthetic data can enhance machine learning\nalgorithms accuracy and efficiency, whereas poorly-placed ones may lead to\nhigher misclassification rates. We introduce an algorithm that maximizes the\nprobability of generating a synthetic sample in the correct region of its class\nby optimizing the class posterior ratio. Additionally, to maintain data\ntopology, synthetic data are generated within each minority sample's\nneighborhood. Our experimental results on forty-one datasets demonstrate the\nsuperior performance of our technique in enhancing deep-learning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to IEEE Transaction on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2401.02591v1",
    "published_date": "2024-01-05 01:08:26 UTC",
    "updated_date": "2024-01-05 01:08:26 UTC"
  },
  {
    "arxiv_id": "2401.02589v1",
    "title": "Identification of 4FGL uncertain sources at Higher Resolutions with Inverse Discrete Wavelet Transform",
    "authors": [
      "Haitao Cao",
      "Hubing Xiao",
      "Zhijian Luo",
      "Xiangtao Zeng",
      "Junhui Fan"
    ],
    "abstract": "In the forthcoming era of big astronomical data, it is a burden to find out\ntarget sources from ground-based and space-based telescopes. Although Machine\nLearning (ML) methods have been extensively utilized to address this issue, the\nincorporation of in-depth data analysis can significantly enhance the\nefficiency of identifying target sources when dealing with massive volumes of\nastronomical data. In this work, we focused on the task of finding AGN\ncandidates and identifying BL Lac/FSRQ candidates from the 4FGL DR3 uncertain\nsources. We studied the correlations among the attributes of the 4FGL DR3\ncatalogue and proposed a novel method, named FDIDWT, to transform the original\ndata. The transformed dataset is characterized as low-dimensional and\nfeature-highlighted, with the estimation of correlation features by Fractal\nDimension (FD) theory and the multi-resolution analysis by Inverse Discrete\nWavelet Transform (IDWT). Combining the FDIDWT method with an improved\nlightweight MatchboxConv1D model, we accomplished two missions: (1) to\ndistinguish the Active Galactic Nuclei (AGNs) from others (Non-AGNs) in the\n4FGL DR3 uncertain sources with an accuracy of 96.65%, namely, Mission A; (2)\nto classify blazar candidates of uncertain type (BCUs) into BL Lacertae objects\n(BL Lacs) or Flat Spectrum Radio Quasars (FSRQs) with an accuracy of 92.03%,\nnamely, Mission B. There are 1354 AGN candidates in Mission A, 482 BL Lacs\ncandidates and 128 FSRQ candidates in Mission B were found. The results show a\nhigh consistency of greater than 98% with the results in previous works. In\naddition, our method has the advantage of finding less variable and relatively\nfaint sources than ordinary methods.",
    "categories": [
      "astro-ph.HE",
      "cs.AI"
    ],
    "primary_category": "astro-ph.HE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2401.02589v1",
    "published_date": "2024-01-05 01:02:34 UTC",
    "updated_date": "2024-01-05 01:02:34 UTC"
  },
  {
    "arxiv_id": "2401.02588v1",
    "title": "Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting",
    "authors": [
      "Van Minh Nguyen",
      "Emma Sandidge",
      "Trupti Mahendrakar",
      "Ryan T. White"
    ],
    "abstract": "The accelerating deployment of spacecraft in orbit have generated interest in\non-orbit servicing (OOS), inspection of spacecraft, and active debris removal\n(ADR). Such missions require precise rendezvous and proximity operations in the\nvicinity of non-cooperative, possible unknown, resident space objects. Safety\nconcerns with manned missions and lag times with ground-based control\nnecessitate complete autonomy. This requires robust characterization of the\ntarget's geometry. In this article, we present an approach for mapping\ngeometries of satellites on orbit based on 3D Gaussian Splatting that can run\non computing resources available on current spaceflight hardware. We\ndemonstrate model training and 3D rendering performance on a\nhardware-in-the-loop satellite mock-up under several realistic lighting and\nmotion conditions. Our model is shown to be capable of training on-board and\nrendering higher quality novel views of an unknown satellite nearly 2 orders of\nmagnitude faster than previous NeRF-based algorithms. Such on-board\ncapabilities are critical to enable downstream machine intelligence tasks\nnecessary for autonomous guidance, navigation, and control tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2401.02588v1",
    "published_date": "2024-01-05 00:49:56 UTC",
    "updated_date": "2024-01-05 00:49:56 UTC"
  },
  {
    "arxiv_id": "2401.02586v1",
    "title": "Federated Learning for distribution skewed data using sample weights",
    "authors": [
      "Hung Nguyen",
      "Peiyuan Wu",
      "Morris Chang"
    ],
    "abstract": "One of the most challenging issues in federated learning is that the data is\noften not independent and identically distributed (nonIID). Clients are\nexpected to contribute the same type of data and drawn from one global\ndistribution. However, data are often collected in different ways from\ndifferent resources. Thus, the data distributions among clients might be\ndifferent from the underlying global distribution. This creates a weight\ndivergence issue and reduces federated learning performance. This work focuses\non improving federated learning performance for skewed data distribution across\nclients. The main idea is to adjust the client distribution closer to the\nglobal distribution using sample weights. Thus, the machine learning model\nconverges faster with higher accuracy. We start from the fundamental concept of\nempirical risk minimization and theoretically derive a solution for adjusting\nthe distribution skewness using sample weights. To determine sample weights, we\nimplicitly exchange density information by leveraging a neural network-based\ndensity estimation model, MADE. The clients data distribution can then be\nadjusted without exposing their raw data. Our experiment results on three\nreal-world datasets show that the proposed method not only improves federated\nlearning accuracy but also significantly reduces communication costs compared\nto the other experimental methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to IEEE Transaction on Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2401.02586v1",
    "published_date": "2024-01-05 00:46:11 UTC",
    "updated_date": "2024-01-05 00:46:11 UTC"
  }
]
[
  {
    "arxiv_id": "2411.17722v1",
    "title": "When IoT Meet LLMs: Applications and Challenges",
    "authors": [
      "Ibrahim Kok",
      "Orhan Demirci",
      "Suat Ozdemir"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have positively and\nefficiently transformed workflows in many domains. One such domain with\nsignificant potential for LLM integration is the Internet of Things (IoT),\nwhere this integration brings new opportunities for improved decision making\nand system interaction. In this paper, we explore the various roles of LLMs in\nIoT, with a focus on their reasoning capabilities. We show how LLM-IoT\nintegration can facilitate advanced decision making and contextual\nunderstanding in a variety of IoT scenarios. Furthermore, we explore the\nintegration of LLMs with edge, fog, and cloud computing paradigms, and show how\nthis synergy can optimize resource utilization, enhance real-time processing,\nand provide scalable solutions for complex IoT applications. To the best of our\nknowledge, this is the first comprehensive study covering IoT-LLM integration\nbetween edge, fog, and cloud systems. Additionally, we propose a novel system\nmodel for industrial IoT applications that leverages LLM-based collective\nintelligence to enable predictive maintenance and condition monitoring.\nFinally, we highlight key challenges and open issues that provide insights for\nfuture research in the field of LLM-IoT integration.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData), 10 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2411.17722v1",
    "published_date": "2024-11-20 23:44:51 UTC",
    "updated_date": "2024-11-20 23:44:51 UTC"
  },
  {
    "arxiv_id": "2411.13754v1",
    "title": "Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios",
    "authors": [
      "Shantanu Jaiswal",
      "Debaditya Roy",
      "Basura Fernando",
      "Cheston Tan"
    ],
    "abstract": "Complex visual reasoning and question answering (VQA) is a challenging task\nthat requires compositional multi-step processing and higher-level reasoning\ncapabilities beyond the immediate recognition and localization of objects and\nevents. Here, we introduce a fully neural Iterative and Parallel Reasoning\nMechanism (IPRM) that combines two distinct forms of computation -- iterative\nand parallel -- to better address complex VQA scenarios. Specifically, IPRM's\n\"iterative\" computation facilitates compositional step-by-step reasoning for\nscenarios wherein individual operations need to be computed, stored, and\nrecalled dynamically (e.g. when computing the query \"determine the color of pen\nto the left of the child in red t-shirt sitting at the white table\").\nMeanwhile, its \"parallel\" computation allows for the simultaneous exploration\nof different reasoning paths and benefits more robust and efficient execution\nof operations that are mutually independent (e.g. when counting individual\ncolors for the query: \"determine the maximum occurring color amongst all\nt-shirts\"). We design IPRM as a lightweight and fully-differentiable neural\nmodule that can be conveniently applied to both transformer and non-transformer\nvision-language backbones. It notably outperforms prior task-specific methods\nand transformer-based attention modules across various image and video VQA\nbenchmarks testing distinct complex reasoning capabilities such as\ncompositional spatiotemporal reasoning (AGQA), situational reasoning (STAR),\nmulti-hop reasoning generalization (CLEVR-Humans) and causal event linking\n(CLEVRER-Humans). Further, IPRM's internal computations can be visualized\nacross reasoning steps, aiding interpretability and diagnosis of its errors.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 camera ready; source code to be released at:\n  https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism",
    "pdf_url": "http://arxiv.org/pdf/2411.13754v1",
    "published_date": "2024-11-20 23:39:54 UTC",
    "updated_date": "2024-11-20 23:39:54 UTC"
  },
  {
    "arxiv_id": "2411.13749v1",
    "title": "AI-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test",
    "authors": [
      "U. León-Domínguez",
      "E. D. Flores-Flores",
      "A. J. García-Jasso",
      "M. K. Gómez-Cuellar",
      "D. Torres-Sánchez",
      "A. Basora-Marimon"
    ],
    "abstract": "Large Language Models based on transformer algorithms have revolutionized\nArtificial Intelligence by enabling verbal interaction with machines akin to\nhuman conversation. These AI agents have surpassed the Turing Test, achieving\nconfusion rates up to 50%. However, challenges persist, especially with the\nadvent of robots and the need to humanize machines for improved Human-AI\ncollaboration. In this experiment, three GPT agents with varying levels of\nagreeableness (disagreeable, neutral, agreeable) based on the Big Five\nInventory were tested in a Turing Test. All exceeded a 50% confusion rate, with\nthe highly agreeable AI agent surpassing 60%. This agent was also recognized as\nexhibiting the most human-like traits. Various explanations in the literature\naddress why these GPT agents were perceived as human, including psychological\nframeworks for understanding anthropomorphism. These findings highlight the\nimportance of personality engineering as an emerging discipline in artificial\nintelligence, calling for collaboration with psychology to develop ergonomic\npsychological models that enhance system adaptability in collaborative\nactivities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "25 pages, 2 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.13749v1",
    "published_date": "2024-11-20 23:12:49 UTC",
    "updated_date": "2024-11-20 23:12:49 UTC"
  },
  {
    "arxiv_id": "2411.13740v1",
    "title": "Federated Continual Learning for Edge-AI: A Comprehensive Survey",
    "authors": [
      "Zi Wang",
      "Fei Wu",
      "Feng Yu",
      "Yurui Zhou",
      "Jia Hu",
      "Geyong Min"
    ],
    "abstract": "Edge-AI, the convergence of edge computing and artificial intelligence (AI),\nhas become a promising paradigm that enables the deployment of advanced AI\nmodels at the network edge, close to users. In Edge-AI, federated continual\nlearning (FCL) has emerged as an imperative framework, which fuses knowledge\nfrom different clients while preserving data privacy and retaining knowledge\nfrom previous tasks as it learns new ones. By so doing, FCL aims to ensure\nstable and reliable performance of learning models in dynamic and distributed\nenvironments. In this survey, we thoroughly review the state-of-the-art\nresearch and present the first comprehensive survey of FCL for Edge-AI. We\ncategorize FCL methods based on three task characteristics: federated class\ncontinual learning, federated domain continual learning, and federated task\ncontinual learning. For each category, an in-depth investigation and review of\nthe representative methods are provided, covering background, challenges,\nproblem formalisation, solutions, and limitations. Besides, existing real-world\napplications empowered by FCL are reviewed, indicating the current progress and\npotential of FCL in diverse application domains. Furthermore, we discuss and\nhighlight several prospective research directions of FCL such as\nalgorithm-hardware co-design for FCL and FCL with foundation models, which\ncould provide insights into the future development and practical deployment of\nFCL in the era of Edge-AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13740v1",
    "published_date": "2024-11-20 22:49:28 UTC",
    "updated_date": "2024-11-20 22:49:28 UTC"
  },
  {
    "arxiv_id": "2411.13724v1",
    "title": "Exploring Large Language Models for Climate Forecasting",
    "authors": [
      "Yang Wang",
      "Hassan A. Karimi"
    ],
    "abstract": "With the increasing impacts of climate change, there is a growing demand for\naccessible tools that can provide reliable future climate information to\nsupport planning, finance, and other decision-making applications. Large\nlanguage models (LLMs), such as GPT-4, present a promising approach to bridging\nthe gap between complex climate data and the general public, offering a way for\nnon-specialist users to obtain essential climate insights through natural\nlanguage interaction. However, an essential challenge remains under-explored:\nevaluating the ability of LLMs to provide accurate and reliable future climate\npredictions, which is crucial for applications that rely on anticipating\nclimate trends. In this study, we investigate the capability of GPT-4 in\npredicting rainfall at short-term (15-day) and long-term (12-month) scales. We\ndesigned a series of experiments to assess GPT's performance under different\nconditions, including scenarios with and without expert data inputs. Our\nresults indicate that GPT, when operating independently, tends to generate\nconservative forecasts, often reverting to historical averages in the absence\nof clear trend signals. This study highlights both the potential and challenges\nof applying LLMs for future climate predictions, providing insights into their\nintegration with climate-related applications and suggesting directions for\nenhancing their predictive capabilities in the field.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13724v1",
    "published_date": "2024-11-20 21:58:19 UTC",
    "updated_date": "2024-11-20 21:58:19 UTC"
  },
  {
    "arxiv_id": "2411.15218v1",
    "title": "Suspected Undeclared Use of Artificial Intelligence in the Academic Literature: An Analysis of the Academ-AI Dataset",
    "authors": [
      "Alex Glynn"
    ],
    "abstract": "Since generative artificial intelligence (AI) tools such as OpenAI's ChatGPT\nbecame widely available, researchers have used them in the writing process. The\nconsensus of the academic publishing community is that such usage must be\ndeclared in the published article. Academ-AI documents examples of suspected\nundeclared AI usage in the academic literature, discernible primarily due to\nthe appearance in research papers of idiosyncratic verbiage characteristic of\nlarge language model (LLM)-based chatbots. This analysis of the first 500\nexamples collected reveals that the problem is widespread, penetrating the\njournals and conference proceedings of highly respected publishers. Undeclared\nAI seems to appear in journals with higher citation metrics and higher article\nprocessing charges (APCs), precisely those outlets that should theoretically\nhave the resources and expertise to avoid such oversights. An extremely small\nminority of cases are corrected post publication, and the corrections are often\ninsufficient to rectify the problem. The 500 examples analyzed here likely\nrepresent a small fraction of the undeclared AI present in the academic\nliterature, much of which may be undetectable. Publishers must enforce their\npolicies against undeclared AI usage in cases that are detectable; this is the\nbest defense currently available to the academic publishing community against\nthe proliferation of undisclosed AI.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.DL",
    "comment": "24 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.15218v1",
    "published_date": "2024-11-20 21:29:36 UTC",
    "updated_date": "2024-11-20 21:29:36 UTC"
  },
  {
    "arxiv_id": "2411.13715v1",
    "title": "SimPhony: A Device-Circuit-Architecture Cross-Layer Modeling and Simulation Framework for Heterogeneous Electronic-Photonic AI System",
    "authors": [
      "Ziang Yin",
      "Meng Zhang",
      "Amir Begovic",
      "Rena Huang",
      "Jeff Zhang",
      "Jiaqi Gu"
    ],
    "abstract": "Electronic-photonic integrated circuits (EPICs) offer transformative\npotential for next-generation high-performance AI but require interdisciplinary\nadvances across devices, circuits, architecture, and design automation. The\ncomplexity of hybrid systems makes it challenging even for domain experts to\nunderstand distinct behaviors and interactions across design stack. The lack of\na flexible, accurate, fast, and easy-to-use EPIC AI system simulation framework\nsignificantly limits the exploration of hardware innovations and system\nevaluations on common benchmarks. To address this gap, we propose SimPhony, a\ncross-layer modeling and simulation framework for heterogeneous\nelectronic-photonic AI systems. SimPhony offers a platform that enables (1)\ngeneric, extensible hardware topology representation that supports\nheterogeneous multi-core architectures with diverse photonic tensor core\ndesigns; (2) optics-specific dataflow modeling with unique multi-dimensional\nparallelism and reuse beyond spatial/temporal dimensions; (3) data-aware energy\nmodeling with realistic device responses, layout-aware area estimation, link\nbudget analysis, and bandwidth-adaptive memory modeling; and (4) seamless\nintegration with model training framework for hardware/software co-simulation.\nBy providing a unified, versatile, and high-fidelity simulation platform,\nSimPhony enables researchers to innovate and evaluate EPIC AI hardware across\nmultiple domains, facilitating the next leap in emerging AI hardware. We\nopen-source our codes at https://github.com/ScopeX-ASU/SimPhony",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.AR",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "physics.optics",
    "comment": "7-page",
    "pdf_url": "http://arxiv.org/pdf/2411.13715v1",
    "published_date": "2024-11-20 21:21:54 UTC",
    "updated_date": "2024-11-20 21:21:54 UTC"
  },
  {
    "arxiv_id": "2411.13677v1",
    "title": "Bimanual Dexterity for Complex Tasks",
    "authors": [
      "Kenneth Shaw",
      "Yulong Li",
      "Jiahui Yang",
      "Mohan Kumar Srirama",
      "Ray Liu",
      "Haoyu Xiong",
      "Russell Mendonca",
      "Deepak Pathak"
    ],
    "abstract": "To train generalist robot policies, machine learning methods often require a\nsubstantial amount of expert human teleoperation data. An ideal robot for\nhumans collecting data is one that closely mimics them: bimanual arms and\ndexterous hands. However, creating such a bimanual teleoperation system with\nover 50 DoF is a significant challenge. To address this, we introduce Bidex, an\nextremely dexterous, low-cost, low-latency and portable bimanual dexterous\nteleoperation system which relies on motion capture gloves and teacher arms. We\ncompare Bidex to a Vision Pro teleoperation system and a SteamVR system and\nfind Bidex to produce better quality data for more complex tasks at a faster\nrate. Additionally, we show Bidex operating a mobile bimanual robot for in the\nwild tasks. The robot hands (5k USD) and teleoperation system (7k USD) is\nreadily reproducible and can be used on many robot arms including two xArms\n(16k USD). Website at https://bidex-teleop.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "In CoRL 2024. Website at https://bidex-teleop.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.13677v1",
    "published_date": "2024-11-20 19:53:35 UTC",
    "updated_date": "2024-11-20 19:53:35 UTC"
  },
  {
    "arxiv_id": "2411.13676v1",
    "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
    "authors": [
      "Xin Dong",
      "Yonggan Fu",
      "Shizhe Diao",
      "Wonmin Byeon",
      "Zijia Chen",
      "Ameya Sunil Mahabaleshwarkar",
      "Shih-Yang Liu",
      "Matthijs Van Keirsbilck",
      "Min-Hung Chen",
      "Yoshi Suhara",
      "Yingyan Lin",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "abstract": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, models are available on huggingface",
    "pdf_url": "http://arxiv.org/pdf/2411.13676v1",
    "published_date": "2024-11-20 19:51:25 UTC",
    "updated_date": "2024-11-20 19:51:25 UTC"
  },
  {
    "arxiv_id": "2411.17720v2",
    "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration on Resource-Constrained Edge Devices",
    "authors": [
      "Mohammadali Shakerdargah",
      "Shan Lu",
      "Chao Gao",
      "Di Niu"
    ],
    "abstract": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF",
      "C.1.4; I.2.7; I.5.1"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted to MLSys 2025,",
    "pdf_url": "http://arxiv.org/pdf/2411.17720v2",
    "published_date": "2024-11-20 19:44:26 UTC",
    "updated_date": "2025-05-16 00:56:30 UTC"
  },
  {
    "arxiv_id": "2412.04484v1",
    "title": "Epinet for Content Cold Start",
    "authors": [
      "Hong Jun Jeon",
      "Songbin Liu",
      "Yuantong Li",
      "Jie Lyu",
      "Hunter Song",
      "Ji Liu",
      "Peng Wu",
      "Zheqing Zhu"
    ],
    "abstract": "The exploding popularity of online content and its user base poses an\nevermore challenging matching problem for modern recommendation systems. Unlike\nother frontiers of machine learning such as natural language, recommendation\nsystems are responsible for collecting their own data. Simply exploiting\ncurrent knowledge can lead to pernicious feedback loops but naive exploration\ncan detract from user experience and lead to reduced engagement. This\nexploration-exploitation trade-off is exemplified in the classic multi-armed\nbandit problem for which algorithms such as upper confidence bounds (UCB) and\nThompson sampling (TS) demonstrate effective performance. However, there have\nbeen many challenges to scaling these approaches to settings which do not\nexhibit a conjugate prior structure. Recent scalable approaches to uncertainty\nquantification via epinets have enabled efficient approximations of Thompson\nsampling even when the learning model is a complex neural network. In this\npaper, we demonstrate the first application of epinets to an online\nrecommendation system. Our experiments demonstrate improvements in both user\ntraffic and engagement efficiency on the Facebook Reels online video platform.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04484v1",
    "published_date": "2024-11-20 19:43:27 UTC",
    "updated_date": "2024-11-20 19:43:27 UTC"
  },
  {
    "arxiv_id": "2411.13653v1",
    "title": "No Free Delivery Service: Epistemic limits of passive data collection in complex social systems",
    "authors": [
      "Maximilian Nickel"
    ],
    "abstract": "Rapid model validation via the train-test paradigm has been a key driver for\nthe breathtaking progress in machine learning and AI. However, modern AI\nsystems often depend on a combination of tasks and data collection practices\nthat violate all assumptions ensuring test validity. Yet, without rigorous\nmodel validation we cannot ensure the intended outcomes of deployed AI systems,\nincluding positive social impact, nor continue to advance AI research in a\nscientifically sound way. In this paper, I will show that for widely considered\ninference settings in complex social systems the train-test paradigm does not\nonly lack a justification but is indeed invalid for any risk estimator,\nincluding counterfactual and causal estimators, with high probability. These\nformal impossibility results highlight a fundamental epistemic issue, i.e.,\nthat for key tasks in modern AI we cannot know whether models are valid under\ncurrent data collection practices. Importantly, this includes variants of both\nrecommender systems and reasoning via large language models, and neither\nna\\\"ive scaling nor limited benchmarks are suited to address this issue. I am\nillustrating these results via the widely used MovieLens benchmark and conclude\nby discussing the implications of these results for AI in social systems,\nincluding possible remedies such as participatory data curation and open\nscience.",
    "categories": [
      "cs.AI",
      "stat.ML",
      "62A01",
      "G.3; I.2.0"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in NeurIPS'24",
    "pdf_url": "http://arxiv.org/pdf/2411.13653v1",
    "published_date": "2024-11-20 19:01:03 UTC",
    "updated_date": "2024-11-20 19:01:03 UTC"
  },
  {
    "arxiv_id": "2411.13547v1",
    "title": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs",
    "authors": [
      "Shirley Kokane",
      "Ming Zhu",
      "Tulika Awalgaonkar",
      "Jianguo Zhang",
      "Thai Hoang",
      "Akshara Prabhakar",
      "Zuxin Liu",
      "Tian Lan",
      "Liangwei Yang",
      "Juntao Tan",
      "Rithesh Murthy",
      "Weiran Yao",
      "Zhiwei Liu",
      "Juan Carlos Niebles",
      "Huan Wang",
      "Shelby Heinecke",
      "Caiming Xiong",
      "Silivo Savarese"
    ],
    "abstract": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13547v1",
    "published_date": "2024-11-20 18:56:22 UTC",
    "updated_date": "2024-11-20 18:56:22 UTC"
  },
  {
    "arxiv_id": "2411.13543v2",
    "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
    "authors": [
      "Davide Paglieri",
      "Bartłomiej Cupiał",
      "Samuel Coward",
      "Ulyana Piterbarg",
      "Maciej Wolczyk",
      "Akbir Khan",
      "Eduardo Pignatelli",
      "Łukasz Kuciński",
      "Lerrel Pinto",
      "Rob Fergus",
      "Jakob Nicolaus Foerster",
      "Jack Parker-Holder",
      "Tim Rocktäschel"
    ],
    "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.13543v2",
    "published_date": "2024-11-20 18:54:32 UTC",
    "updated_date": "2025-04-01 14:45:22 UTC"
  },
  {
    "arxiv_id": "2411.13537v1",
    "title": "Metacognition for Unknown Situations and Environments (MUSE)",
    "authors": [
      "Rodolfo Valiente",
      "Praveen K. Pilly"
    ],
    "abstract": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13537v1",
    "published_date": "2024-11-20 18:41:03 UTC",
    "updated_date": "2024-11-20 18:41:03 UTC"
  },
  {
    "arxiv_id": "2411.13536v1",
    "title": "Identity Preserving 3D Head Stylization with Multiview Score Distillation",
    "authors": [
      "Bahri Batuhan Bilecen",
      "Ahmet Berke Gokmen",
      "Furkan Guzelant",
      "Aysegul Dundar"
    ],
    "abstract": "3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "https://three-bee.github.io/head_stylization",
    "pdf_url": "http://arxiv.org/pdf/2411.13536v1",
    "published_date": "2024-11-20 18:37:58 UTC",
    "updated_date": "2024-11-20 18:37:58 UTC"
  },
  {
    "arxiv_id": "2411.13528v2",
    "title": "Entropy Bootstrapping for Weakly Supervised Nuclei Detection",
    "authors": [
      "James Willoughby",
      "Irina Voiculescu"
    ],
    "abstract": "Microscopy structure segmentation, such as detecting cells or nuclei,\ngenerally requires a human to draw a ground truth contour around each instance.\nWeakly supervised approaches (e.g. consisting of only single point labels) have\nthe potential to reduce this workload significantly. Our approach uses\nindividual point labels for an entropy estimation to approximate an underlying\ndistribution of cell pixels. We infer full cell masks from this distribution,\nand use Mask-RCNN to produce an instance segmentation output. We compare this\npoint--annotated approach with training on the full ground truth masks. We show\nthat our method achieves a comparatively good level of performance, despite a\n95% reduction in pixel labels.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 Pages",
    "pdf_url": "http://arxiv.org/pdf/2411.13528v2",
    "published_date": "2024-11-20 18:24:11 UTC",
    "updated_date": "2024-11-21 20:03:14 UTC"
  },
  {
    "arxiv_id": "2411.17719v1",
    "title": "SlideSpawn: An Automatic Slides Generation System for Research Publications",
    "authors": [
      "Keshav Kumar",
      "Ravindranath Chowdary"
    ],
    "abstract": "Research papers are well structured documents. They have text, figures,\nequations, tables etc., to covey their ideas and findings. They are divided\ninto sections like Introduction, Model, Experiments etc., which deal with\ndifferent aspects of research. Characteristics like these set research papers\napart from ordinary documents and allows us to significantly improve their\nsummarization. In this paper, we propose a novel system, SlideSpwan, that takes\nPDF of a research document as an input and generates a quality presentation\nproviding it's summary in a visual and concise fashion. The system first\nconverts the PDF of the paper to an XML document that has the structural\ninformation about various elements. Then a machine learning model, trained on\nPS5K dataset and Aminer 9.5K Insights dataset (that we introduce), is used to\npredict salience of each sentence in the paper. Sentences for slides are\nselected using ILP and clustered based on their similarity with each cluster\nbeing given a suitable title. Finally a slide is generated by placing any\ngraphical element referenced in the selected sentences next to them.\nExperiments on a test set of 650 pairs of papers and slides demonstrate that\nour system generates presentations with better quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "H.3"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 4 figures, 2 tables, 5 equations, 41 references",
    "pdf_url": "http://arxiv.org/pdf/2411.17719v1",
    "published_date": "2024-11-20 18:16:16 UTC",
    "updated_date": "2024-11-20 18:16:16 UTC"
  },
  {
    "arxiv_id": "2411.15217v1",
    "title": "LPLgrad: Optimizing Active Learning Through Gradient Norm Sample Selection and Auxiliary Model Training",
    "authors": [
      "Shreen Gul",
      "Mohamed Elmahallawy",
      "Sanjay Madria",
      "Ardhendu Tripathy"
    ],
    "abstract": "Machine learning models are increasingly being utilized across various fields\nand tasks due to their outstanding performance and strong generalization\ncapabilities. Nonetheless, their success hinges on the availability of large\nvolumes of annotated data, the creation of which is often labor-intensive,\ntime-consuming, and expensive. Many active learning (AL) approaches have been\nproposed to address these challenges, but they often fail to fully leverage the\ninformation from the core phases of AL, such as training on the labeled set and\nquerying new unlabeled samples. To bridge this gap, we propose a novel AL\napproach, Loss Prediction Loss with Gradient Norm (LPLgrad), designed to\nquantify model uncertainty effectively and improve the accuracy of image\nclassification tasks. LPLgrad operates in two distinct phases: (i) {\\em\nTraining Phase} aims to predict the loss for input features by jointly training\na main model and an auxiliary model. Both models are trained on the labeled\ndata to maximize the efficiency of the learning process, an aspect often\noverlooked in previous AL methods. This dual-model approach enhances the\nability to extract complex input features and learn intrinsic patterns from the\ndata effectively; (ii) {\\em Querying Phase} that quantifies the uncertainty of\nthe main model to guide sample selection. This is achieved by calculating the\ngradient norm of the entropy values for samples in the unlabeled dataset.\nSamples with the highest gradient norms are prioritized for labeling and\nsubsequently added to the labeled set, improving the model's performance with\nminimal labeling effort. Extensive evaluations on real-world datasets\ndemonstrate that the LPLgrad approach outperforms state-of-the-art methods by\norder of magnitude in terms of accuracy on a small number of labeled images,\nyet achieving comparable training and querying times in multiple image\nclassification tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15217v1",
    "published_date": "2024-11-20 18:12:59 UTC",
    "updated_date": "2024-11-20 18:12:59 UTC"
  },
  {
    "arxiv_id": "2411.13518v1",
    "title": "Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models",
    "authors": [
      "Chanseo Lee",
      "Sonu Kumar",
      "Kimon A. Vogt",
      "Sam Meraj",
      "Antonia Vogt"
    ],
    "abstract": "The increasing demand for multilingual capabilities in healthcare underscores\nthe need for AI models adept at processing diverse languages, particularly in\nclinical documentation and decision-making. Arabic, with its complex\nmorphology, syntax, and diglossia, poses unique challenges for natural language\nprocessing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a\nlanguage model tailored for Arabic clinical documentation, against JAIS, the\nleading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics\nmodified ourselves for the purposes of assessing model performances in a\ndifferent language. The study assessed the models' performance in summarizing\npatient-physician interactions, focusing on accuracy, comprehensiveness,\nclinical utility, and linguistic-cultural competence.\n  Results indicate that Sporo AraSum significantly outperforms JAIS in\nAI-centric quantitative metrics and all qualitative attributes measured in our\nmodified version of the PDQI-9. AraSum's architecture enables precise and\nculturally sensitive documentation, addressing the linguistic nuances of Arabic\nwhile mitigating risks of AI hallucinations. These findings suggest that Sporo\nAraSum is better suited to meet the demands of Arabic-speaking healthcare\nenvironments, offering a transformative solution for multilingual clinical\nworkflows. Future research should incorporate real-world data to further\nvalidate these findings and explore broader integration into healthcare\nsystems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2411.06713",
    "pdf_url": "http://arxiv.org/pdf/2411.13518v1",
    "published_date": "2024-11-20 18:10:19 UTC",
    "updated_date": "2024-11-20 18:10:19 UTC"
  },
  {
    "arxiv_id": "2412.04483v1",
    "title": "AI-powered Digital Framework for Personalized Economical Quality Learning at Scale",
    "authors": [
      "Mrzieh VatandoustMohammadieh",
      "Mohammad Mahdi Mohajeri",
      "Ali Keramati",
      "Majid Nili Ahmadabadi"
    ],
    "abstract": "The disparity in access to quality education is significant, both between\ndeveloped and developing countries and within nations, regardless of their\neconomic status. Socioeconomic barriers and rapid changes in the job market\nfurther intensify this issue, highlighting the need for innovative solutions\nthat can deliver quality education at scale and low cost. This paper addresses\nthese challenges by proposing an AI-powered digital learning framework grounded\nin Deep Learning (DL) theory. The DL theory emphasizes learner agency and\nredefines the role of teachers as facilitators, making it particularly suitable\nfor scalable educational environments. We outline eight key principles derived\nfrom learning science and AI that are essential for implementing DL-based\nDigital Learning Environments (DLEs). Our proposed framework leverages AI for\nlearner modelling based on Open Learner Modeling (OLM), activity suggestions,\nand AI-assisted support for both learners and facilitators, fostering\ncollaborative and engaging learning experiences. Our framework provides a\npromising direction for scalable, high-quality education globally, offering\npractical solutions to some of the AI-related challenges in education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04483v1",
    "published_date": "2024-11-20 17:44:29 UTC",
    "updated_date": "2024-11-20 17:44:29 UTC"
  },
  {
    "arxiv_id": "2411.13485v2",
    "title": "Utilizing Large Language Models to Synthesize Product Desirability Datasets",
    "authors": [
      "John D. Hastings",
      "Sherri Weitl-Harms",
      "Joseph Doty",
      "Zachary J. Myers",
      "Warren Thompson"
    ],
    "abstract": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7; H.3.3; I.2.6; H.5.2"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 2 figures, 6 tables, updated author list",
    "pdf_url": "http://arxiv.org/pdf/2411.13485v2",
    "published_date": "2024-11-20 17:35:21 UTC",
    "updated_date": "2024-11-22 15:24:07 UTC"
  },
  {
    "arxiv_id": "2411.13477v1",
    "title": "PatentEdits: Framing Patent Novelty as Textual Entailment",
    "authors": [
      "Ryan Lee",
      "Alexander Spangher",
      "Xuezhe Ma"
    ],
    "abstract": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13477v1",
    "published_date": "2024-11-20 17:23:40 UTC",
    "updated_date": "2024-11-20 17:23:40 UTC"
  },
  {
    "arxiv_id": "2411.13459v1",
    "title": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures",
    "authors": [
      "Sarbartha Banerjee",
      "Prateek Sahu",
      "Mulong Luo",
      "Anjo Vahldiek-Oberwagner",
      "Neeraja J. Yadwadkar",
      "Mohit Tiwari"
    ],
    "abstract": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 4 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.13459v1",
    "published_date": "2024-11-20 17:08:38 UTC",
    "updated_date": "2024-11-20 17:08:38 UTC"
  },
  {
    "arxiv_id": "2411.13453v1",
    "title": "LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models",
    "authors": [
      "Salvatore Mario Carta",
      "Stefano Chessa",
      "Giulia Contu",
      "Andrea Corriga",
      "Andrea Deidda",
      "Gianni Fenu",
      "Luca Frigau",
      "Alessandro Giuliani",
      "Luca Grassi",
      "Marco Manolo Manca",
      "Mirko Marras",
      "Francesco Mola",
      "Bastianino Mossa",
      "Piergiorgio Mura",
      "Marco Ortu",
      "Leonardo Piano",
      "Simone Pisano",
      "Alessia Pisu",
      "Alessandro Sebastian Podda",
      "Livio Pompianu",
      "Simone Seu",
      "Sandro Gabriele Tiddia"
    ],
    "abstract": "Minority languages are vital to preserving cultural heritage, yet they face\ngrowing risks of extinction due to limited digital resources and the dominance\nof artificial intelligence models trained on high-resource languages. This\nwhite paper proposes a framework to generate linguistic tools for low-resource\nlanguages, focusing on data creation to support the development of language\nmodels that can aid in preservation efforts. Sardinian, an endangered language,\nserves as the case study to demonstrate the framework's effectiveness. By\naddressing the data scarcity that hinders intelligent applications for such\nlanguages, we contribute to promoting linguistic diversity and support ongoing\nefforts in language standardization and revitalization through modern\ntechnologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13453v1",
    "published_date": "2024-11-20 16:59:41 UTC",
    "updated_date": "2024-11-20 16:59:41 UTC"
  },
  {
    "arxiv_id": "2411.13451v1",
    "title": "AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations",
    "authors": [
      "Gaurav Verma",
      "Rachneet Kaur",
      "Nishan Srishankar",
      "Zhen Zeng",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "abstract": "State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 3 figures, an abridged version to appear in NeurIPS 2024\n  AFM Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.13451v1",
    "published_date": "2024-11-20 16:54:15 UTC",
    "updated_date": "2024-11-20 16:54:15 UTC"
  },
  {
    "arxiv_id": "2411.13438v2",
    "title": "Robust Monocular Visual Odometry using Curriculum Learning",
    "authors": [
      "Assaf Lahiany",
      "Oren Gal"
    ],
    "abstract": "Curriculum Learning (CL), drawing inspiration from natural learning patterns\nobserved in humans and animals, employs a systematic approach of gradually\nintroducing increasingly complex training data during model development. Our\nwork applies innovative CL methodologies to address the challenging geometric\nproblem of monocular Visual Odometry (VO) estimation, which is essential for\nrobot navigation in constrained environments. The primary objective of our\nresearch is to push the boundaries of current state-of-the-art (SOTA)\nbenchmarks in monocular VO by investigating various curriculum learning\nstrategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO)\nframework through the integration of novel CL approaches, with the goal of\ndeveloping more resilient models capable of maintaining high performance across\nchallenging environments and complex motion scenarios. Our research encompasses\nseveral distinctive CL strategies. We develop methods to evaluate sample\ndifficulty based on trajectory motion characteristics, implement sophisticated\nadaptive scheduling through self-paced weighted loss mechanisms, and utilize\nreinforcement learning agents for dynamic adjustment of training emphasis.\nThrough comprehensive evaluation on the diverse synthetic TartanAir dataset and\ncomplex real-world benchmarks such as EuRoC and TUM-RGBD, our Curriculum\nLearning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior\nperformance compared to existing SOTA methods, including both feature-based and\nlearning-based VO approaches. The results validate the effectiveness of\nintegrating curriculum learning principles into visual odometry systems.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.13438v2",
    "published_date": "2024-11-20 16:26:51 UTC",
    "updated_date": "2024-12-13 14:27:12 UTC"
  },
  {
    "arxiv_id": "2411.15216v3",
    "title": "Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint",
    "authors": [
      "Guangkun Nie",
      "Gongzheng Tang",
      "Shenda Hong"
    ],
    "abstract": "Imbalanced data distributions are prevalent in real-world scenarios, posing\nsignificant challenges in both imbalanced classification and imbalanced\nregression tasks. They often cause deep learning models to overfit in areas of\nhigh sample density (many-shot regions) while underperforming in areas of low\nsample density (few-shot regions). This characteristic restricts the utility of\ndeep learning models in various sectors, notably healthcare, where areas with\nfew-shot data hold greater clinical relevance. While recent studies have shown\nthe benefits of incorporating distribution information in imbalanced\nclassification tasks, such strategies are rarely explored in imbalanced\nregression. In this paper, we address this issue by introducing a novel loss\nfunction, termed Dist Loss, designed to minimize the distribution distance\nbetween the model's predictions and the target labels in a differentiable\nmanner, effectively integrating distribution information into model training.\nDist Loss enables deep learning models to regularize their output distribution\nduring training, effectively enhancing their focus on few-shot regions. We have\nconducted extensive experiments across three datasets spanning computer vision\nand healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results\ndemonstrate that Dist Loss effectively mitigates the negative impact of\nimbalanced data distribution on model performance, achieving state-of-the-art\nresults in sparse data regions. Furthermore, Dist Loss is easy to integrate,\ncomplementing existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15216v3",
    "published_date": "2024-11-20 16:17:40 UTC",
    "updated_date": "2025-03-28 02:57:57 UTC"
  },
  {
    "arxiv_id": "2411.13428v1",
    "title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers",
    "authors": [
      "Hojjat Karami",
      "David Atienza",
      "Anisoara Ionescu"
    ],
    "abstract": "Generating synthetic Electronic Health Records (EHRs) offers significant\npotential for data augmentation, privacy-preserving data sharing, and improving\nmachine learning model training. We propose a novel tokenization strategy\ntailored for structured EHR data, which encompasses diverse data types such as\ncovariates, ICD codes, and irregularly sampled time series. Using a GPT-like\ndecoder-only transformer model, we demonstrate the generation of high-quality\nsynthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we\nbenchmark the fidelity, utility, and privacy of the generated data against\nstate-of-the-art models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13428v1",
    "published_date": "2024-11-20 16:11:20 UTC",
    "updated_date": "2024-11-20 16:11:20 UTC"
  },
  {
    "arxiv_id": "2411.13420v1",
    "title": "Heuristically Adaptive Diffusion-Model Evolutionary Strategy",
    "authors": [
      "Benedikt Hartl",
      "Yanbo Zhang",
      "Hananel Hazan",
      "Michael Levin"
    ],
    "abstract": "Diffusion Models represent a significant advancement in generative modeling,\nemploying a dual-phase process that first degrades domain-specific information\nvia Gaussian noise and restores it through a trainable model. This framework\nenables pure noise-to-data generation and modular reconstruction of, images or\nvideos. Concurrently, evolutionary algorithms employ optimization methods\ninspired by biological principles to refine sets of numerical parameters\nencoding potential solutions to rugged objective functions. Our research\nreveals a fundamental connection between diffusion models and evolutionary\nalgorithms through their shared underlying generative mechanisms: both methods\ngenerate high-quality samples via iterative refinement on random initial\ndistributions. By employing deep learning-based diffusion models as generative\nmodels across diverse evolutionary tasks and iteratively refining diffusion\nmodels with heuristically acquired databases, we can iteratively sample\npotentially better-adapted offspring parameters, integrating them into\nsuccessive generations of the diffusion model. This approach achieves efficient\nconvergence toward high-fitness parameters while maintaining explorative\ndiversity. Diffusion models introduce enhanced memory capabilities into\nevolutionary algorithms, retaining historical information across generations\nand leveraging subtle data correlations to generate refined samples. We elevate\nevolutionary algorithms from procedures with shallow heuristics to frameworks\nwith deep memory. By deploying classifier-free guidance for conditional\nsampling at the parameter level, we achieve precise control over evolutionary\nsearch dynamics to further specific genotypical, phenotypical, or\npopulation-wide traits. Our framework marks a major heuristic and algorithmic\ntransition, offering increased flexibility, precision, and control in\nevolutionary optimization processes.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13420v1",
    "published_date": "2024-11-20 16:06:28 UTC",
    "updated_date": "2024-11-20 16:06:28 UTC"
  },
  {
    "arxiv_id": "2411.13409v1",
    "title": "Unification of Balti and trans-border sister dialects in the essence of LLMs and AI Technology",
    "authors": [
      "Muhammad Sharif",
      "Jiangyan Yi",
      "Muhammad Shoaib"
    ],
    "abstract": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by IEEE conference ISCSLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.13409v1",
    "published_date": "2024-11-20 15:48:21 UTC",
    "updated_date": "2024-11-20 15:48:21 UTC"
  },
  {
    "arxiv_id": "2412.04482v2",
    "title": "NLP Cluster Analysis of Common Core State Standards and NAEP Item Specifications",
    "authors": [
      "Gregory Camilli",
      "Larry Suter"
    ],
    "abstract": "Camilli (2024) proposed a methodology using natural language processing (NLP)\nto map the relationship of a set of content standards to item specifications.\nThis study provided evidence that NLP can be used to improve the mapping\nprocess. As part of this investigation, the nominal classifications of\nstandards and items specifications were used to examine construct equivalence.\nIn the current paper, we determine the strength of empirical support for the\nsemantic distinctiveness of these classifications, which are known as \"domains\"\nfor Common Core standards, and \"strands\" for National Assessment of Educational\nProgress (NAEP) item specifications. This is accomplished by separate k-means\nclustering for standards and specifications of their corresponding embedding\nvectors. We then briefly illustrate an application of these findings.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.04482v2",
    "published_date": "2024-11-20 15:44:58 UTC",
    "updated_date": "2024-12-13 16:56:21 UTC"
  },
  {
    "arxiv_id": "2411.13365v1",
    "title": "Explainable Finite-Memory Policies for Partially Observable Markov Decision Processes",
    "authors": [
      "Muqsit Azeem",
      "Debraj Chakraborty",
      "Sudeep Kanav",
      "Jan Kretinsky"
    ],
    "abstract": "Partially Observable Markov Decision Processes (POMDPs) are a fundamental\nframework for decision-making under uncertainty and partial observability.\nSince in general optimal policies may require infinite memory, they are hard to\nimplement and often render most problems undecidable. Consequently,\nfinite-memory policies are mostly considered instead. However, the algorithms\nfor computing them are typically very complex, and so are the resulting\npolicies. Facing the need for their explainability, we provide a representation\nof such policies, both (i) in an interpretable formalism and (ii) typically of\nsmaller size, together yielding higher explainability. To that end, we combine\nmodels of Mealy machines and decision trees; the latter describing simple,\nstationary parts of the policies and the former describing how to switch among\nthem. We design a translation for policies of the finite-state-controller (FSC)\nform from standard literature and show how our method smoothly generalizes to\nother variants of finite-memory policies. Further, we identify specific\nproperties of recently used \"attractor-based\" policies, which allow us to\nconstruct yet simpler and smaller representations. Finally, we illustrate the\nhigher explainability in a few case studies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint -- Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.13365v1",
    "published_date": "2024-11-20 14:42:23 UTC",
    "updated_date": "2024-11-20 14:42:23 UTC"
  },
  {
    "arxiv_id": "2411.15215v1",
    "title": "S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning",
    "authors": [
      "Mingze Yin",
      "Hanjing Zhou",
      "Jialu Wu",
      "Yiheng Zhu",
      "Yuxuan Zhan",
      "Zitai Kong",
      "Hongxia Xu",
      "Chang-Yu Hsieh",
      "Jintai Chen",
      "Tingjun Hou",
      "Jian Wu"
    ],
    "abstract": "Antibodies safeguard our health through their precise and potent binding to\nspecific antigens, demonstrating promising therapeutic efficacy in the\ntreatment of numerous diseases, including COVID-19. Recent advancements in\nbiomedical language models have shown the great potential to interpret complex\nbiological structures and functions. However, existing antibody specific models\nhave a notable limitation that they lack explicit consideration for antibody\nstructural information, despite the fact that both 1D sequence and 3D structure\ncarry unique and complementary insights into antibody behavior and\nfunctionality. This paper proposes Sequence-Structure multi-level pre-trained\nAntibody Language Model (S$^2$ALM), combining holistic sequential and\nstructural information in one unified, generic antibody foundation model. We\nconstruct a hierarchical pre-training paradigm incorporated with two customized\nmulti-level training objectives to facilitate the modeling of comprehensive\nantibody representations. S$^2$ALM's representation space uncovers inherent\nfunctional binding mechanisms, biological evolution properties and structural\ninteraction patterns. Pre-trained over 75 million sequences and 11.7 million\nstructures, S$^2$ALM can be adopted for diverse downstream tasks: accurately\npredicting antigen-antibody binding affinities, precisely distinguishing B cell\nmaturation stages, identifying antibody crucial binding positions, and\nspecifically designing novel coronavirus-binding antibodies. Remarkably,\nS$^2$ALM outperforms well-established and renowned baselines and sets new\nstate-of-the-art performance across extensive antibody specific understanding\nand generation tasks. S$^2$ALM's ability to model comprehensive and generalized\nrepresentations further positions its potential to advance real-world\ntherapeutic antibody development, potentially addressing unmet academic,\nindustrial, and clinical needs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15215v1",
    "published_date": "2024-11-20 14:24:26 UTC",
    "updated_date": "2024-11-20 14:24:26 UTC"
  },
  {
    "arxiv_id": "2411.13627v1",
    "title": "CryptoFormalEval: Integrating LLMs and Formal Verification for Automated Cryptographic Protocol Vulnerability Detection",
    "authors": [
      "Cristian Curaba",
      "Denis D'Ambrosi",
      "Alessandro Minisini",
      "Natalia Pérez-Campanero Antolín"
    ],
    "abstract": "Cryptographic protocols play a fundamental role in securing modern digital\ninfrastructure, but they are often deployed without prior formal verification.\nThis could lead to the adoption of distributed systems vulnerable to attack\nvectors. Formal verification methods, on the other hand, require complex and\ntime-consuming techniques that lack automatization. In this paper, we introduce\na benchmark to assess the ability of Large Language Models (LLMs) to\nautonomously identify vulnerabilities in new cryptographic protocols through\ninteraction with Tamarin: a theorem prover for protocol verification. We\ncreated a manually validated dataset of novel, flawed, communication protocols\nand designed a method to automatically verify the vulnerabilities found by the\nAI agents. Our results about the performances of the current frontier models on\nthe benchmark provides insights about the possibility of cybersecurity\napplications by integrating LLMs with symbolic reasoning systems.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13627v1",
    "published_date": "2024-11-20 14:16:55 UTC",
    "updated_date": "2024-11-20 14:16:55 UTC"
  },
  {
    "arxiv_id": "2411.13343v1",
    "title": "Fact-Level Confidence Calibration and Self-Correction",
    "authors": [
      "Yige Yuan",
      "Bingbing Xu",
      "Hexiang Tan",
      "Fei Sun",
      "Teng Xiao",
      "Wei Li",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Code is available at https://github.com/yuanyige/fact-calibration",
    "pdf_url": "http://arxiv.org/pdf/2411.13343v1",
    "published_date": "2024-11-20 14:15:18 UTC",
    "updated_date": "2024-11-20 14:15:18 UTC"
  },
  {
    "arxiv_id": "2411.13332v1",
    "title": "Verifying Machine Unlearning with Explainable AI",
    "authors": [
      "Àlex Pujol Vidal",
      "Anders S. Johansen",
      "Mohammad N. S. Jahromi",
      "Sergio Escalera",
      "Kamal Nasrollahi",
      "Thomas B. Moeslund"
    ],
    "abstract": "We investigate the effectiveness of Explainable AI (XAI) in verifying Machine\nUnlearning (MU) within the context of harbor front monitoring, focusing on data\nprivacy and regulatory compliance. With the increasing need to adhere to\nprivacy legislation such as the General Data Protection Regulation (GDPR),\ntraditional methods of retraining ML models for data deletions prove\nimpractical due to their complexity and resource demands. MU offers a solution\nby enabling models to selectively forget specific learned patterns without full\nretraining. We explore various removal techniques, including data relabeling,\nand model perturbation. Then, we leverage attribution-based XAI to discuss the\neffects of unlearning on model performance. Our proof-of-concept introduces\nfeature importance as an innovative verification step for MU, expanding beyond\ntraditional metrics and demonstrating techniques' ability to reduce reliance on\nundesired patterns. Additionally, we propose two novel XAI-based metrics,\nHeatmap Coverage (HC) and Attention Shift (AS), to evaluate the effectiveness\nof these methods. This approach not only highlights how XAI can complement MU\nby providing effective verification, but also sets the stage for future\nresearch to enhance their joint integration.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ICPRW2024",
    "pdf_url": "http://arxiv.org/pdf/2411.13332v1",
    "published_date": "2024-11-20 13:57:32 UTC",
    "updated_date": "2024-11-20 13:57:32 UTC"
  },
  {
    "arxiv_id": "2411.13326v1",
    "title": "An Evolutional Neural Network Framework for Classification of Microarray Data",
    "authors": [
      "Maryam Eshraghi Evari",
      "Md Nasir Sulaiman",
      "Amir Rajabi Behjat"
    ],
    "abstract": "DNA microarray gene-expression data has been widely used to identify\ncancerous gene signatures. Microarray can increase the accuracy of cancer\ndiagnosis and prognosis. However, analyzing the large amount of gene expression\ndata from microarray chips pose a challenge for current machine learning\nresearches. One of the challenges lie within classification of healthy and\ncancerous tissues is high dimensionality of gene expressions. High\ndimensionality decreases the accuracy of the classification. This research aims\nto apply a hybrid model of Genetic Algorithm and Neural Network to overcome the\nproblem during subset selection of informative genes. Whereby, a Genetic\nAlgorithm (GA) reduced dimensionality during feature selection and then a\nMulti-Layer perceptron Neural Network (MLP) is applied to classify selected\ngenes. The performance evaluated by considering to the accuracy and the number\nof selected genes. Experimental results show the proposed method suggested high\naccuracy and minimum number of selected genes in comparison with other machine\nlearning algorithms.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13326v1",
    "published_date": "2024-11-20 13:48:40 UTC",
    "updated_date": "2024-11-20 13:48:40 UTC"
  },
  {
    "arxiv_id": "2411.13323v3",
    "title": "Are Large Language Models Memorizing Bug Benchmarks?",
    "authors": [
      "Daniel Ramos",
      "Claudia Mamede",
      "Kush Jain",
      "Paulo Canelas",
      "Catarina Gamboa",
      "Claire Le Goues"
    ],
    "abstract": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13323v3",
    "published_date": "2024-11-20 13:46:04 UTC",
    "updated_date": "2025-03-31 13:02:51 UTC"
  },
  {
    "arxiv_id": "2411.13322v1",
    "title": "Scaling Laws for Online Advertisement Retrieval",
    "authors": [
      "Yunli Wang",
      "Zixuan Yang",
      "Zhen Zhang",
      "Zhiqiang Wang",
      "Jian Yang",
      "Shiyang Wen",
      "Peng Jiang",
      "Kun Gai"
    ],
    "abstract": "The scaling law is a notable property of neural network models and has\nsignificantly propelled the development of large language models. Scaling laws\nhold great promise in guiding model design and resource allocation. Recent\nresearch increasingly shows that scaling laws are not limited to NLP tasks or\nTransformer architectures; they also apply to domains such as recommendation.\nHowever, there is still a lack of literature on scaling law research in online\nadvertisement retrieval systems. This may be because 1) identifying the scaling\nlaw for resource cost and online revenue is often expensive in both time and\ntraining resources for large-scale industrial applications, and 2) varying\nsettings for different systems prevent the scaling law from being applied\nacross various scenarios. To address these issues, we propose a lightweight\nparadigm to identify the scaling law of online revenue and machine cost for a\ncertain online advertisement retrieval scenario with a low experimental cost.\nSpecifically, we focus on a sole factor (FLOPs) and propose an offline metric\nnamed R/R* that exhibits a high linear correlation with online revenue for\nretrieval models. We estimate the machine cost offline via a simulation\nalgorithm. Thus, we can transform most online experiments into low-cost offline\nexperiments. We conduct comprehensive experiments to verify the effectiveness\nof our proposed metric R/R* and to identify the scaling law in the online\nadvertisement retrieval system of Kuaishou. With the scaling law, we\ndemonstrate practical applications for ROI-constrained model designing and\nmulti-scenario resource allocation in Kuaishou advertising system. To the best\nof our knowledge, this is the first work to study the scaling laws for online\nadvertisement retrieval of real-world systems, showing great potential for\nscaling law in advertising system optimization.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13322v1",
    "published_date": "2024-11-20 13:44:59 UTC",
    "updated_date": "2024-11-20 13:44:59 UTC"
  },
  {
    "arxiv_id": "2411.13311v1",
    "title": "A Resource Efficient Fusion Network for Object Detection in Bird's-Eye View using Camera and Raw Radar Data",
    "authors": [
      "Kavin Chandrasekaran",
      "Sorin Grigorescu",
      "Gijs Dubbelman",
      "Pavol Jancura"
    ],
    "abstract": "Cameras can be used to perceive the environment around the vehicle, while\naffordable radar sensors are popular in autonomous driving systems as they can\nwithstand adverse weather conditions unlike cameras. However, radar point\nclouds are sparser with low azimuth and elevation resolution that lack semantic\nand structural information of the scenes, resulting in generally lower radar\ndetection performance. In this work, we directly use the raw range-Doppler (RD)\nspectrum of radar data, thus avoiding radar signal processing. We independently\nprocess camera images within the proposed comprehensive image processing\npipeline. Specifically, first, we transform the camera images to Bird's-Eye\nView (BEV) Polar domain and extract the corresponding features with our camera\nencoder-decoder architecture. The resultant feature maps are fused with\nRange-Azimuth (RA) features, recovered from the RD spectrum input from the\nradar decoder to perform object detection. We evaluate our fusion strategy with\nother existing methods not only in terms of accuracy but also on computational\ncomplexity metrics on RADIal dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "IEEE Intelligent Transportation Systems Conference (ITSC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.13311v1",
    "published_date": "2024-11-20 13:26:13 UTC",
    "updated_date": "2024-11-20 13:26:13 UTC"
  },
  {
    "arxiv_id": "2411.18636v1",
    "title": "Towards Advanced Speech Signal Processing: A Statistical Perspective on Convolution-Based Architectures and its Applications",
    "authors": [
      "Nirmal Joshua Kapu",
      "Raghav Karan"
    ],
    "abstract": "This article surveys convolution-based models including convolutional neural\nnetworks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing\nmodels and provide their statistical backgrounds and speech recognition,\nspeaker identification, emotion recognition, and speech enhancement\napplications. Through comparative training cost assessment, model size,\naccuracy and speed assessment, we compare the strengths and weaknesses of each\nmodel, identify potential errors and propose avenues for further research,\nemphasizing the central role it plays in advancing applications of speech\ntechnologies.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.18636v1",
    "published_date": "2024-11-20 13:01:30 UTC",
    "updated_date": "2024-11-20 13:01:30 UTC"
  },
  {
    "arxiv_id": "2411.13284v1",
    "title": "DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain WiFi-Based Human Activity Recognition",
    "authors": [
      "Julian Strohmayer",
      "Rafael Sterzinger",
      "Matthias Wödlinger",
      "Martin Kampel"
    ],
    "abstract": "Cross-domain generalization is an open problem in WiFi-based sensing due to\nvariations in environments, devices, and subjects, causing domain shifts in\nchannel state information. To address this, we propose Domain-Adversarial\nTest-Time Adaptation (DATTA), a novel framework combining domain-adversarial\ntraining (DAT), test-time adaptation (TTA), and weight resetting to facilitate\nadaptation to unseen target domains and to prevent catastrophic forgetting.\nDATTA is integrated into a lightweight, flexible architecture optimized for\nspeed. We conduct a comprehensive evaluation of DATTA, including an ablation\nstudy on all key components using publicly available data, and verify its\nsuitability for real-time applications such as human activity recognition. When\ncombining a SotA video-based variant of TTA with WiFi-based DAT and comparing\nit to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch\nimplementation of DATTA is publicly available at:\nhttps://github.com/StrohmayerJ/DATTA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13284v1",
    "published_date": "2024-11-20 12:52:36 UTC",
    "updated_date": "2024-11-20 12:52:36 UTC"
  },
  {
    "arxiv_id": "2411.13281v2",
    "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
    "authors": [
      "Ziyang Luo",
      "Haoning Wu",
      "Dongxu Li",
      "Jing Ma",
      "Mohan Kankanhalli",
      "Junnan Li"
    ],
    "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025, Project Page: https://videoautoarena.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2411.13281v2",
    "published_date": "2024-11-20 12:48:34 UTC",
    "updated_date": "2025-03-23 11:01:22 UTC"
  },
  {
    "arxiv_id": "2411.13280v3",
    "title": "Empower Structure-Based Molecule Optimization with Gradient Guidance",
    "authors": [
      "Keyue Qiu",
      "Yuxuan Song",
      "Jie Yu",
      "Hongbo Ma",
      "Ziyao Cao",
      "Zhilong Zhang",
      "Yushuai Wu",
      "Mingyue Zheng",
      "Hao Zhou",
      "Wei-Ying Ma"
    ],
    "abstract": "Structure-Based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. MolJO\nachieves state-of-the-art performance on CrossDocked2020 benchmark (Success\nRate 51.3%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success\nRate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as\nmuch as 3D baselines. Furthermore, we extend MolJO to a wide range of\noptimization settings, including multi-objective optimization and challenging\ntasks in drug design such as R-group optimization and scaffold hopping, further\nunderscoring its versatility.",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "Accepted to ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.13280v3",
    "published_date": "2024-11-20 12:48:29 UTC",
    "updated_date": "2025-05-12 07:13:56 UTC"
  },
  {
    "arxiv_id": "2411.13269v1",
    "title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive Software",
    "authors": [
      "Minal Suresh Patil",
      "Gustav Ung",
      "Mattias Nyberg"
    ],
    "abstract": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "21 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13269v1",
    "published_date": "2024-11-20 12:38:17 UTC",
    "updated_date": "2024-11-20 12:38:17 UTC"
  },
  {
    "arxiv_id": "2411.14491v3",
    "title": "A Survey on Human-Centric LLMs",
    "authors": [
      "Jing Yi Wang",
      "Nicholas Sukiennik",
      "Tong Li",
      "Weikang Su",
      "Qianyue Hao",
      "Jingbo Xu",
      "Zihan Huang",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14491v3",
    "published_date": "2024-11-20 12:34:44 UTC",
    "updated_date": "2024-12-01 08:37:51 UTC"
  },
  {
    "arxiv_id": "2411.13262v1",
    "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for Multi-point Robot Navigation",
    "authors": [
      "Yuxuan Chen",
      "Yixin Han",
      "Xiao Li"
    ],
    "abstract": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13262v1",
    "published_date": "2024-11-20 12:28:13 UTC",
    "updated_date": "2024-11-20 12:28:13 UTC"
  },
  {
    "arxiv_id": "2411.15214v1",
    "title": "Urban Region Embeddings from Service-Specific Mobile Traffic Data",
    "authors": [
      "Giulio Loddi",
      "Chiara Pugliese",
      "Francesco Lettich",
      "Fabio Pinelli",
      "Chiara Renso"
    ],
    "abstract": "With the advent of advanced 4G/5G mobile networks, mobile phone data\ncollected by operators now includes detailed, service-specific traffic\ninformation with high spatio-temporal resolution. In this paper, we leverage\nthis type of data to explore its potential for generating high-quality\nrepresentations of urban regions. To achieve this, we present a methodology for\ncreating urban region embeddings from service-specific mobile traffic data,\nemploying a temporal convolutional network-based autoencoder, transformers, and\nlearnable weighted sum models to capture key urban features. In the extensive\nexperimental evaluation conducted using a real-world dataset, we demonstrate\nthat the embeddings generated by our methodology effectively capture urban\ncharacteristics. Specifically, our embeddings are compared against those of a\nstate-of-the-art competitor across two downstream tasks. Additionally, through\nclustering techniques, we investigate how well the embeddings produced by our\nmethodology capture the temporal dynamics and characteristics of the underlying\nurban regions. Overall, this work highlights the potential of service-specific\nmobile traffic data for urban research and emphasizes the importance of making\nsuch data accessible to support public innovation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15214v1",
    "published_date": "2024-11-20 12:13:07 UTC",
    "updated_date": "2024-11-20 12:13:07 UTC"
  },
  {
    "arxiv_id": "2411.15212v1",
    "title": "Effective Analog ICs Floorplanning with Relational Graph Neural Networks and Reinforcement Learning",
    "authors": [
      "Davide Basso",
      "Luca Bortolussi",
      "Mirjana Videnovic-Misic",
      "Husni Habal"
    ],
    "abstract": "Analog integrated circuit (IC) floorplanning is typically a manual process\nwith the placement of components (devices and modules) planned by a layout\nengineer. This process is further complicated by the interdependence of\nfloorplanning and routing steps, numerous electric and layout-dependent\nconstraints, as well as the high level of customization expected in analog\ndesign. This paper presents a novel automatic floorplanning algorithm based on\nreinforcement learning. It is augmented by a relational graph convolutional\nneural network model for encoding circuit features and positional constraints.\nThe combination of these two machine learning methods enables knowledge\ntransfer across different circuit designs with distinct topologies and\nconstraints, increasing the \\emph{generalization ability} of the solution.\nApplied to $6$ industrial circuits, our approach surpassed established\nfloorplanning techniques in terms of speed, area and half-perimeter wire\nlength. When integrated into a \\emph{procedural generator} for layout\ncompletion, overall layout time was reduced by $67.3\\%$ with a $8.3\\%$ mean\narea reduction compared to manual layout.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 7 figures, Accepted at DATE25",
    "pdf_url": "http://arxiv.org/pdf/2411.15212v1",
    "published_date": "2024-11-20 12:11:12 UTC",
    "updated_date": "2024-11-20 12:11:12 UTC"
  },
  {
    "arxiv_id": "2411.13251v1",
    "title": "BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point Cloud Semantic Segmentation",
    "authors": [
      "Umamaheswaran Raman Kumar",
      "Abdur Razzaq Fayjie",
      "Jurgen Hannaert",
      "Patrick Vandewalle"
    ],
    "abstract": "Large-scale 2D datasets have been instrumental in advancing machine learning;\nhowever, progress in 3D vision tasks has been relatively slow. This disparity\nis largely due to the limited availability of 3D benchmarking datasets. In\nparticular, creating real-world point cloud datasets for indoor scene semantic\nsegmentation presents considerable challenges, including data collection within\nconfined spaces and the costly, often inaccurate process of per-point labeling\nto generate ground truths. While synthetic datasets address some of these\nchallenges, they often fail to replicate real-world conditions, particularly\nthe occlusions that occur in point clouds collected from real environments.\nExisting 3D benchmarking datasets typically evaluate deep learning models under\nthe assumption that training and test data are independently and identically\ndistributed (IID), which affects the models' usability for real-world point\ncloud segmentation. To address these challenges, we introduce the BelHouse3D\ndataset, a new synthetic point cloud dataset designed for 3D indoor scene\nsemantic segmentation. This dataset is constructed using real-world references\nfrom 32 houses in Belgium, ensuring that the synthetic data closely aligns with\nreal-world conditions. Additionally, we include a test set with data occlusion\nto simulate out-of-distribution (OOD) scenarios, reflecting the occlusions\ncommonly encountered in real-world point clouds. We evaluate popular\npoint-based semantic segmentation methods using our OOD setting and present a\nbenchmark. We believe that BelHouse3D and its OOD setting will advance research\nin 3D point cloud semantic segmentation for indoor scenes, providing valuable\ninsights for the development of more generalizable models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 6 figures, 3 tables, accepted at ECCV 2024 Workshops",
    "pdf_url": "http://arxiv.org/pdf/2411.13251v1",
    "published_date": "2024-11-20 12:09:43 UTC",
    "updated_date": "2024-11-20 12:09:43 UTC"
  },
  {
    "arxiv_id": "2411.13243v1",
    "title": "XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation",
    "authors": [
      "Ziyi Wang",
      "Yanbo Wang",
      "Xumin Yu",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Existing methodologies in open vocabulary 3D semantic segmentation primarily\nconcentrate on establishing a unified feature space encompassing 3D, 2D, and\ntextual modalities. Nevertheless, traditional techniques such as global feature\nalignment or vision-language model distillation tend to impose only approximate\ncorrespondence, struggling notably with delineating fine-grained segmentation\nboundaries. To address this gap, we propose a more meticulous mask-level\nalignment between 3D features and the 2D-text embedding space through a\ncross-modal mask reasoning framework, XMask3D. In our approach, we developed a\nmask generator based on the denoising UNet from a pre-trained diffusion model,\nleveraging its capability for precise textual control over dense pixel\nrepresentations and enhancing the open-world adaptability of the generated\nmasks. We further integrate 3D global features as implicit conditions into the\npre-trained 2D denoising UNet, enabling the generation of segmentation masks\nwith additional 3D geometry awareness. Subsequently, the generated 2D masks are\nemployed to align mask-level 3D representations with the vision-language\nfeature space, thereby augmenting the open vocabulary capability of 3D geometry\nembeddings. Finally, we fuse complementary 2D and 3D mask features, resulting\nin competitive performance across multiple benchmarks for 3D open vocabulary\nsemantic segmentation. Code is available at\nhttps://github.com/wangzy22/XMask3D.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.13243v1",
    "published_date": "2024-11-20 12:02:12 UTC",
    "updated_date": "2024-11-20 12:02:12 UTC"
  },
  {
    "arxiv_id": "2411.13239v1",
    "title": "Transforming the Hybrid Cloud for Emerging AI Workloads",
    "authors": [
      "Deming Chen",
      "Alaa Youssef",
      "Ruchi Pendse",
      "André Schleife",
      "Bryan K. Clark",
      "Hendrik Hamann",
      "Jingrui He",
      "Teodoro Laino",
      "Lav Varshney",
      "Yuxiong Wang",
      "Avirup Sil",
      "Reyhaneh Jabbarvand",
      "Tianyin Xu",
      "Volodymyr Kindratenko",
      "Carlos Costa",
      "Sarita Adve",
      "Charith Mendis",
      "Minjia Zhang",
      "Santiago Núñez-Corrales",
      "Raghu Ganti",
      "Mudhakar Srivatsa",
      "Nam Sung Kim",
      "Josep Torrellas",
      "Jian Huang",
      "Seetharami Seelam",
      "Klara Nahrstedt",
      "Tarek Abdelzaher",
      "Tamar Eilam",
      "Huimin Zhao",
      "Matteo Manica",
      "Ravishankar Iyer",
      "Martin Hirzel",
      "Vikram Adve",
      "Darko Marinov",
      "Hubertus Franke",
      "Hanghang Tong",
      "Elizabeth Ainsworth",
      "Han Zhao",
      "Deepak Vasisht",
      "Minh Do",
      "Fabio Oliveira",
      "Giovanni Pacifici",
      "Ruchir Puri",
      "Priya Nagpurkar"
    ],
    "abstract": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.AR",
      "cs.ET",
      "cs.MA"
    ],
    "primary_category": "cs.DC",
    "comment": "70 pages, 27 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13239v1",
    "published_date": "2024-11-20 11:57:43 UTC",
    "updated_date": "2024-11-20 11:57:43 UTC"
  },
  {
    "arxiv_id": "2411.13225v1",
    "title": "Quantum Kernel-Based Long Short-term Memory",
    "authors": [
      "Yu-Chao Hsu",
      "Tai-Yu Li",
      "Kuan-Cheng Chen"
    ],
    "abstract": "The integration of quantum computing into classical machine learning\narchitectures has emerged as a promising approach to enhance model efficiency\nand computational capacity. In this work, we introduce the Quantum Kernel-Based\nLong Short-Term Memory (QK-LSTM) network, which utilizes quantum kernel\nfunctions within the classical LSTM framework to capture complex, non-linear\npatterns in sequential data. By embedding input data into a high-dimensional\nquantum feature space, the QK-LSTM model reduces the reliance on large\nparameter sets, achieving effective compression while maintaining accuracy in\nsequence modeling tasks. This quantum-enhanced architecture demonstrates\nefficient convergence, robust loss minimization, and model compactness, making\nit suitable for deployment in edge computing environments and resource-limited\nquantum devices (especially in the NISQ era). Benchmark comparisons reveal that\nQK-LSTM achieves performance on par with classical LSTM models, yet with fewer\nparameters, underscoring its potential to advance quantum machine learning\napplications in natural language processing and other domains requiring\nefficient temporal data processing.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13225v1",
    "published_date": "2024-11-20 11:39:30 UTC",
    "updated_date": "2024-11-20 11:39:30 UTC"
  },
  {
    "arxiv_id": "2411.15211v1",
    "title": "LightLLM: A Versatile Large Language Model for Predictive Light Sensing",
    "authors": [
      "Jiawei Hu",
      "Hong Jia",
      "Mahbub Hassan",
      "Lina Yao",
      "Brano Kusy",
      "Wen Hu"
    ],
    "abstract": "We propose LightLLM, a model that fine tunes pre-trained large language\nmodels (LLMs) for light-based sensing tasks. It integrates a sensor data\nencoder to extract key features, a contextual prompt to provide environmental\ninformation, and a fusion layer to combine these inputs into a unified\nrepresentation. This combined input is then processed by the pre-trained LLM,\nwhich remains frozen while being fine-tuned through the addition of\nlightweight, trainable components, allowing the model to adapt to new tasks\nwithout altering its original parameters. This approach enables flexible\nadaptation of LLM to specialized light sensing tasks with minimal computational\noverhead and retraining effort. We have implemented LightLLM for three light\nsensing tasks: light-based localization, outdoor solar forecasting, and indoor\nsolar estimation. Using real-world experimental datasets, we demonstrate that\nLightLLM significantly outperforms state-of-the-art methods, achieving 4.4x\nimprovement in localization accuracy and 3.4x improvement in indoor solar\nestimation when tested in previously unseen environments. We further\ndemonstrate that LightLLM outperforms ChatGPT-4 with direct prompting,\nhighlighting the advantages of LightLLM's specialized architecture for sensor\ndata fusion with textual prompts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 14 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.15211v1",
    "published_date": "2024-11-20 11:37:33 UTC",
    "updated_date": "2024-11-20 11:37:33 UTC"
  },
  {
    "arxiv_id": "2411.14489v1",
    "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
    "authors": [
      "Hang Zhou",
      "Xiaoxu Zheng",
      "Yunhe Wang",
      "Michael Bi Mi",
      "Deyi Xiong",
      "Kai Han"
    ],
    "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance\ndependencies are widely used in various speech tasks, eg., keyword spotting\n(KWS) and speech enhancement (SE). Due to the limitation of power and memory in\nlow-resource devices, efficient RNN models are urgently required for real-world\napplications. In this paper, we propose an efficient RNN architecture,\nGhostRNN, which reduces hidden state redundancy with cheap operations. In\nparticular, we observe that partial dimensions of hidden states are similar to\nthe others in trained RNN models, suggesting that redundancy exists in specific\nRNNs. To reduce the redundancy and hence computational cost, we propose to\nfirst generate a few intrinsic states, and then apply cheap operations to\nproduce ghost states based on the intrinsic states. Experiments on KWS and SE\ntasks demonstrate that the proposed GhostRNN significantly reduces the memory\nusage (~40%) and computation cost while keeping performance similar.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14489v1",
    "published_date": "2024-11-20 11:37:14 UTC",
    "updated_date": "2024-11-20 11:37:14 UTC"
  },
  {
    "arxiv_id": "2411.13223v1",
    "title": "Existential Conversations with Large Language Models: Content, Community, and Culture",
    "authors": [
      "Murray Shanahan",
      "Beth Singler"
    ],
    "abstract": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13223v1",
    "published_date": "2024-11-20 11:35:22 UTC",
    "updated_date": "2024-11-20 11:35:22 UTC"
  },
  {
    "arxiv_id": "2411.13215v1",
    "title": "Proceedings Sixth International Workshop on Formal Methods for Autonomous Systems",
    "authors": [
      "Matt Luckcuck",
      "Mengwei Xu"
    ],
    "abstract": "This EPTCS volume contains the papers from the Sixth International Workshop\non Formal Methods for Autonomous Systems (FMAS 2024), which was held between\nthe 11th and 13th of November 2024. FMAS 2024 was co-located with 19th\nInternational Conference on integrated Formal Methods (iFM'24), hosted by the\nUniversity of Manchester in the United Kingdom, in the University of\nManchester's Core Technology Facility.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13215v1",
    "published_date": "2024-11-20 11:21:22 UTC",
    "updated_date": "2024-11-20 11:21:22 UTC"
  },
  {
    "arxiv_id": "2411.13209v1",
    "title": "Comparative Analysis of Audio Feature Extraction for Real-Time Talking Portrait Synthesis",
    "authors": [
      "Pegah Salehi",
      "Sajad Amouei Sheshkal",
      "Vajira Thambawita",
      "Sushant Gautam",
      "Saeed S. Sabet",
      "Dag Johansen",
      "Michael A. Riegler",
      "Pål Halvorsen"
    ],
    "abstract": "This paper examines the integration of real-time talking-head generation for\ninterviewer training, focusing on overcoming challenges in Audio Feature\nExtraction (AFE), which often introduces latency and limits responsiveness in\nreal-time applications. To address these issues, we propose and implement a\nfully integrated system that replaces conventional AFE models with Open AI's\nWhisper, leveraging its encoder to optimize processing and improve overall\nsystem efficiency. Our evaluation of two open-source real-time models across\nthree different datasets shows that Whisper not only accelerates processing but\nalso improves specific aspects of rendering quality, resulting in more\nrealistic and responsive talking-head interactions. These advancements make the\nsystem a more effective tool for immersive, interactive training applications,\nexpanding the potential of AI-driven avatars in interviewer training.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "eess.AS",
      "68T45, 68T07, 68T01"
    ],
    "primary_category": "cs.SD",
    "comment": "16 pages, 6 figures, 3 tables. submitted to MDPI journal in as Big\n  Data and Cognitive Computing",
    "pdf_url": "http://arxiv.org/pdf/2411.13209v1",
    "published_date": "2024-11-20 11:18:05 UTC",
    "updated_date": "2024-11-20 11:18:05 UTC"
  },
  {
    "arxiv_id": "2411.13207v1",
    "title": "The Information Security Awareness of Large Language Models",
    "authors": [
      "Ofir Cohen",
      "Gil Ari Agmon",
      "Asaf Shabtai",
      "Rami Puzis"
    ],
    "abstract": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13207v1",
    "published_date": "2024-11-20 11:09:55 UTC",
    "updated_date": "2024-11-20 11:09:55 UTC"
  },
  {
    "arxiv_id": "2411.15210v4",
    "title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
    "authors": [
      "Yong Xie",
      "Weijie Zheng",
      "Hanxun Huang",
      "Guangnan Ye",
      "Xingjun Ma"
    ],
    "abstract": "As deep learning models are increasingly deployed in safety-critical\napplications, evaluating their vulnerabilities to adversarial perturbations is\nessential for ensuring their reliability and trustworthiness. Over the past\ndecade, a large number of white-box adversarial robustness evaluation methods\n(i.e., attacks) have been proposed, ranging from single-step to multi-step\nmethods and from individual to ensemble methods. Despite these advances,\nchallenges remain in conducting meaningful and comprehensive robustness\nevaluations, particularly when it comes to large-scale testing and ensuring\nevaluations reflect real-world adversarial risks. In this work, we focus on\nimage classification models and propose a novel individual attack method,\nProbability Margin Attack (PMA), which defines the adversarial margin in the\nprobability space rather than the logits space. We analyze the relationship\nbetween PMA and existing cross-entropy or logits-margin-based attacks, and show\nthat PMA can outperform the current state-of-the-art individual methods.\nBuilding on PMA, we propose two types of ensemble attacks that balance\neffectiveness and efficiency. Furthermore, we create a million-scale dataset,\nCC1M, derived from the existing CC3M dataset, and use it to conduct the first\nmillion-scale white-box adversarial robustness evaluation of\nadversarially-trained ImageNet models. Our findings provide valuable insights\ninto the robustness gaps between individual versus ensemble attacks and\nsmall-scale versus million-scale evaluations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15210v4",
    "published_date": "2024-11-20 10:41:23 UTC",
    "updated_date": "2025-03-11 02:56:08 UTC"
  },
  {
    "arxiv_id": "2411.13187v3",
    "title": "Engagement-Driven Content Generation with Large Language Models",
    "authors": [
      "Erica Coppolillo",
      "Federico Cinus",
      "Marco Minici",
      "Francesco Bonchi",
      "Giuseppe Manco"
    ],
    "abstract": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13187v3",
    "published_date": "2024-11-20 10:40:08 UTC",
    "updated_date": "2024-11-22 13:05:40 UTC"
  },
  {
    "arxiv_id": "2411.13181v1",
    "title": "Cross-Camera Distracted Driver Classification through Feature Disentanglement and Contrastive Learning",
    "authors": [
      "Simone Bianco",
      "Luigi Celona",
      "Paolo Napoletano"
    ],
    "abstract": "The classification of distracted drivers is pivotal for ensuring safe\ndriving. Previous studies demonstrated the effectiveness of neural networks in\nautomatically predicting driver distraction, fatigue, and potential hazards.\nHowever, recent research has uncovered a significant loss of accuracy in these\nmodels when applied to samples acquired under conditions that differ from the\ntraining data. In this paper, we introduce a robust model designed to withstand\nchanges in camera position within the vehicle. Our Driver Behavior Monitoring\nNetwork (DBMNet) relies on a lightweight backbone and integrates a\ndisentanglement module to discard camera view information from features,\ncoupled with contrastive learning to enhance the encoding of various driver\nactions. Experiments conducted on the daytime and nighttime subsets of the\n100-Driver dataset validate the effectiveness of our approach with an increment\non average of 9\\% in Top-1 accuracy in comparison with the state of the art. In\naddition, cross-dataset and cross-camera experiments conducted on three\nbenchmark datasets, namely AUCDD-V1, EZZ2021 and SFD, demonstrate the superior\ngeneralization capability of the proposed method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13181v1",
    "published_date": "2024-11-20 10:27:12 UTC",
    "updated_date": "2024-11-20 10:27:12 UTC"
  },
  {
    "arxiv_id": "2411.13173v2",
    "title": "Writing Style Matters: An Examination of Bias and Fairness in Information Retrieval Systems",
    "authors": [
      "Hongliu Cao"
    ],
    "abstract": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
    "pdf_url": "http://arxiv.org/pdf/2411.13173v2",
    "published_date": "2024-11-20 10:17:09 UTC",
    "updated_date": "2024-12-12 10:22:37 UTC"
  },
  {
    "arxiv_id": "2411.15208v1",
    "title": "M2oE: Multimodal Collaborative Expert Peptide Model",
    "authors": [
      "Zengzhu Guo",
      "Zhiqi Ma"
    ],
    "abstract": "Peptides are biomolecules comprised of amino acids that play an important\nrole in our body. In recent years, peptides have received extensive attention\nin drug design and synthesis, and peptide prediction tasks help us better\nsearch for functional peptides. Typically, we use the primary sequence and\nstructural information of peptides for model encoding. However, recent studies\nhave focused more on single-modal information (structure or sequence) for\nprediction without multi-modal approaches. We found that single-modal models\nare not good at handling datasets with less information in that particular\nmodality. Therefore, this paper proposes the M2oE multi-modal collaborative\nexpert peptide model. Based on previous work, by integrating sequence and\nspatial structural information, employing expert model and Cross-Attention\nMechanism, the model's capabilities are balanced and improved. Experimental\nresults indicate that the M2oE model performs excellently in complex task\npredictions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by bibm 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.15208v1",
    "published_date": "2024-11-20 09:52:52 UTC",
    "updated_date": "2024-11-20 09:52:52 UTC"
  },
  {
    "arxiv_id": "2411.13619v1",
    "title": "Non-Linear Outlier Synthesis for Out-of-Distribution Detection",
    "authors": [
      "Lars Doorenbos",
      "Raphael Sznitman",
      "Pablo Márquez-Neila"
    ],
    "abstract": "The reliability of supervised classifiers is severely hampered by their\nlimitations in dealing with unexpected inputs, leading to great interest in\nout-of-distribution (OOD) detection. Recently, OOD detectors trained on\nsynthetic outliers, especially those generated by large diffusion models, have\nshown promising results in defining robust OOD decision boundaries. Building on\nthis progress, we present NCIS, which enhances the quality of synthetic\noutliers by operating directly in the diffusion's model embedding space rather\nthan combining disjoint models as in previous work and by modeling\nclass-conditional manifolds with a conditional volume-preserving network for\nmore expressive characterization of the training distribution. We demonstrate\nthat these improvements yield new state-of-the-art OOD detection results on\nstandard ImageNet100 and CIFAR100 benchmarks and provide insights into the\nimportance of data pre-processing and other key design choices. We make our\ncode available at \\url{https://github.com/LarsDoorenbos/NCIS}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13619v1",
    "published_date": "2024-11-20 09:47:29 UTC",
    "updated_date": "2024-11-20 09:47:29 UTC"
  },
  {
    "arxiv_id": "2411.13157v2",
    "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative Decoding",
    "authors": [
      "Hyun Ryu",
      "Eric Kim"
    ],
    "abstract": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13157v2",
    "published_date": "2024-11-20 09:46:30 UTC",
    "updated_date": "2024-11-27 03:25:44 UTC"
  },
  {
    "arxiv_id": "2411.13154v1",
    "title": "DMQR-RAG: Diverse Multi-Query Rewriting for RAG",
    "authors": [
      "Zhicong Li",
      "Jiahao Wang",
      "Zhishu Jiang",
      "Hangyu Mao",
      "Zhongxia Chen",
      "Jiazhen Du",
      "Yuanxing Zhang",
      "Fuzheng Zhang",
      "Di Zhang",
      "Yong Liu"
    ],
    "abstract": "Large language models often encounter challenges with static knowledge and\nhallucinations, which undermine their reliability. Retrieval-augmented\ngeneration (RAG) mitigates these issues by incorporating external information.\nHowever, user queries frequently contain noise and intent deviations,\nnecessitating query rewriting to improve the relevance of retrieved documents.\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\ndesigned to improve the performance of both document retrieval and final\nresponses in RAG. Specifically, we investigate how queries with varying\ninformation quantities can retrieve a diverse array of documents, presenting\nfour rewriting strategies that operate at different levels of information to\nenhance the performance of baseline approaches. Additionally, we propose an\nadaptive strategy selection method that minimizes the number of rewrites while\noptimizing overall performance. Our methods have been rigorously validated\nthrough extensive experiments conducted in both academic and industry settings.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13154v1",
    "published_date": "2024-11-20 09:43:30 UTC",
    "updated_date": "2024-11-20 09:43:30 UTC"
  },
  {
    "arxiv_id": "2411.15207v1",
    "title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training",
    "authors": [
      "Ameera Bawazir",
      "Kebin Wu",
      "Wenbin Li"
    ],
    "abstract": "Recent advancements in vision-language pre-training via contrastive learning\nhave significantly improved performance across computer vision tasks. However,\nin the medical domain, obtaining multimodal data is often costly and\nchallenging due to privacy, sensitivity, and annotation complexity. To mitigate\ndata scarcity while boosting model performance, we introduce \\textbf{Uni-Mlip},\na unified self-supervision framework specifically designed to enhance medical\nvision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,\nuni-modality, and fused-modality self-supervision techniques at the data-level\nand the feature-level. Additionally, Uni-Mlip tailors uni-modal image\nself-supervision to accommodate the unique characteristics of medical images.\nOur experiments across datasets of varying scales demonstrate that Uni-Mlip\nsignificantly surpasses current state-of-the-art methods in three key\ndownstream tasks: image-text retrieval, image classification, and visual\nquestion answering (VQA).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 2 figures, accepted by BMVC'24",
    "pdf_url": "http://arxiv.org/pdf/2411.15207v1",
    "published_date": "2024-11-20 09:43:26 UTC",
    "updated_date": "2024-11-20 09:43:26 UTC"
  },
  {
    "arxiv_id": "2411.13152v2",
    "title": "AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation",
    "authors": [
      "Houcheng Su",
      "Mengzhu Wang",
      "Jiao Li",
      "Nan Yin",
      "Liang Yang",
      "Li Shen"
    ],
    "abstract": "In semi-supervised domain adaptation (SSDA), the model aims to leverage\npartially labeled target domain data along with a large amount of labeled\nsource domain data to enhance its generalization capability for the target\ndomain. A key advantage of SSDA is its ability to significantly reduce reliance\non labeled data, thereby lowering the costs and time associated with data\npreparation. Most existing SSDA methods utilize information from domain labels\nand class labels but overlook the structural information of the data. To\naddress this issue, this paper proposes a graph learning perspective (AGLP) for\nsemi-supervised domain adaptation. We apply the graph convolutional network to\nthe instance graph which allows structural information to propagate along the\nweighted graph edges. The proposed AGLP model has several advantages. First, to\nthe best of our knowledge, this is the first work to model structural\ninformation in SSDA. Second, the proposed model can effectively learn\ndomain-invariant and semantic representations, reducing domain discrepancies in\nSSDA. Extensive experimental results on multiple standard benchmarks\ndemonstrate that the proposed AGLP algorithm outperforms state-of-the-art\nsemi-supervised domain adaptation methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 92C55, 62H35",
      "I.2.6; I.4.10; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "8page",
    "pdf_url": "http://arxiv.org/pdf/2411.13152v2",
    "published_date": "2024-11-20 09:41:41 UTC",
    "updated_date": "2024-11-22 09:21:01 UTC"
  },
  {
    "arxiv_id": "2411.13149v1",
    "title": "YCB-LUMA: YCB Object Dataset with Luminance Keying for Object Localization",
    "authors": [
      "Thomas Pöllabauer"
    ],
    "abstract": "Localizing target objects in images is an important task in computer vision.\nOften it is the first step towards solving a variety of applications in\nautonomous driving, maintenance, quality insurance, robotics, and augmented\nreality. Best in class solutions for this task rely on deep neural networks,\nwhich require a set of representative training data for best performance.\nCreating sets of sufficient quality, variety, and size is often difficult,\nerror prone, and expensive. This is where the method of luminance keying can\nhelp: it provides a simple yet effective solution to record high quality data\nfor training object detection and segmentation. We extend previous work that\npresented luminance keying on the common YCB-V set of household objects by\nrecording the remaining objects of the YCB superset. The additional variety of\nobjects - addition of transparency, multiple color variations, non-rigid\nobjects - further demonstrates the usefulness of luminance keying and might be\nused to test the applicability of the approach on new 2D object detection and\nsegmentation algorithms.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13149v1",
    "published_date": "2024-11-20 09:32:22 UTC",
    "updated_date": "2024-11-20 09:32:22 UTC"
  },
  {
    "arxiv_id": "2411.13147v2",
    "title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation",
    "authors": [
      "Mengzhu Wang",
      "Jiao Li",
      "Houcheng Su",
      "Nan Yin",
      "Liang Yang",
      "Shen Li"
    ],
    "abstract": "Semi-supervised learning (SSL) has made notable advancements in medical image\nsegmentation (MIS), particularly in scenarios with limited labeled data and\nsignificantly enhancing data utilization efficiency. Previous methods primarily\nfocus on complex training strategies to utilize unlabeled data but neglect the\nimportance of graph structural information. Different from existing methods, we\npropose a graph-based clustering for semi-supervised medical image segmentation\n(GraphCL) by jointly modeling graph data structure in a unified deep model. The\nproposed GraphCL model enjoys several advantages. Firstly, to the best of our\nknowledge, this is the first work to model the data structure information for\nsemi-supervised medical image segmentation (SSMIS). Secondly, to get the\nclustered features across different graphs, we integrate both pairwise\naffinities between local image features and raw features as inputs. Extensive\nexperimental results on three standard benchmarks show that the proposed\nGraphCL algorithm outperforms state-of-the-art semi-supervised medical image\nsegmentation methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 92C55, 62H35",
      "I.2.6; I.4.10; J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "9page",
    "pdf_url": "http://arxiv.org/pdf/2411.13147v2",
    "published_date": "2024-11-20 09:24:46 UTC",
    "updated_date": "2024-11-22 09:18:20 UTC"
  },
  {
    "arxiv_id": "2411.13144v1",
    "title": "CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models",
    "authors": [
      "Naen Xu",
      "Changjiang Li",
      "Tianyu Du",
      "Minxi Li",
      "Wenjie Luo",
      "Jiacheng Liang",
      "Yuyuan Li",
      "Xuhong Zhang",
      "Meng Han",
      "Jianwei Yin",
      "Ting Wang"
    ],
    "abstract": "Text-to-image diffusion models have emerged as powerful tools for generating\nhigh-quality images from textual descriptions. However, their increasing\npopularity has raised significant copyright concerns, as these models can be\nmisused to reproduce copyrighted content without authorization. In response,\nrecent studies have proposed various copyright protection methods, including\nadversarial perturbation, concept erasure, and watermarking techniques.\nHowever, their effectiveness and robustness against advanced attacks remain\nlargely unexplored. Moreover, the lack of unified evaluation frameworks has\nhindered systematic comparison and fair assessment of different approaches. To\nbridge this gap, we systematize existing copyright protection methods and\nattacks, providing a unified taxonomy of their design spaces. We then develop\nCopyrightMeter, a unified evaluation framework that incorporates 17\nstate-of-the-art protections and 16 representative attacks. Leveraging\nCopyrightMeter, we comprehensively evaluate protection methods across multiple\ndimensions, thereby uncovering how different design choices impact fidelity,\nefficacy, and resilience under attacks. Our analysis reveals several key\nfindings: (i) most protections (16/17) are not resilient against attacks; (ii)\nthe \"best\" protection varies depending on the target priority; (iii) more\nadvanced attacks significantly promote the upgrading of protections. These\ninsights provide concrete guidance for developing more robust protection\nmethods, while its unified evaluation protocol establishes a standard benchmark\nfor future copyright protection research in text-to-image generation.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13144v1",
    "published_date": "2024-11-20 09:19:10 UTC",
    "updated_date": "2024-11-20 09:19:10 UTC"
  },
  {
    "arxiv_id": "2411.13116v1",
    "title": "Provably Efficient Action-Manipulation Attack Against Continuous Reinforcement Learning",
    "authors": [
      "Zhi Luo",
      "Xiyuan Yang",
      "Pan Zhou",
      "Di Wang"
    ],
    "abstract": "Manipulating the interaction trajectories between the intelligent agent and\nthe environment can control the agent's training and behavior, exposing the\npotential vulnerabilities of reinforcement learning (RL). For example, in\nCyber-Physical Systems (CPS) controlled by RL, the attacker can manipulate the\nactions of the adopted RL to other actions during the training phase, which\nwill lead to bad consequences. Existing work has studied action-manipulation\nattacks in tabular settings, where the states and actions are discrete. As seen\nin many up-and-coming RL applications, such as autonomous driving, continuous\naction space is widely accepted, however, its action-manipulation attacks have\nnot been thoroughly investigated yet. In this paper, we consider this crucial\nproblem in both white-box and black-box scenarios. Specifically, utilizing the\nknowledge derived exclusively from trajectories, we propose a black-box attack\nalgorithm named LCBT, which uses the Monte Carlo tree search method for\nefficient action searching and manipulation. Additionally, we demonstrate that\nfor an agent whose dynamic regret is sub-linearly related to the total number\nof steps, LCBT can teach the agent to converge to target policies with only\nsublinear attack cost, i.e., $O\\left(\\mathcal{R}(T) + MH^3K^E\\log\n(MT)\\right)(0<E<1)$, where $H$ is the number of steps per episode, $K$ is the\ntotal number of episodes, $T=KH$ is the total number of steps, $M$ is the\nnumber of subspaces divided in the state space, and $\\mathcal{R}(T)$ is the\nbound of the RL algorithm's regret. We conduct our proposed attack methods on\nthree aggressive algorithms: DDPG, PPO, and TD3 in continuous settings, which\nshow a promising attack performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13116v1",
    "published_date": "2024-11-20 08:20:29 UTC",
    "updated_date": "2024-11-20 08:20:29 UTC"
  },
  {
    "arxiv_id": "2411.13100v1",
    "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control",
    "authors": [
      "Yunkee Chae",
      "Eunsik Shin",
      "Hwang Suntae",
      "Seungryeol Paik",
      "Kyogu Lee"
    ],
    "abstract": "Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13100v1",
    "published_date": "2024-11-20 07:57:58 UTC",
    "updated_date": "2024-11-20 07:57:58 UTC"
  },
  {
    "arxiv_id": "2411.13093v3",
    "title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension",
    "authors": [
      "Yongdong Luo",
      "Xiawu Zheng",
      "Xiao Yang",
      "Guilin Li",
      "Haojia Lin",
      "Jinfa Huang",
      "Jiayi Ji",
      "Fei Chao",
      "Jiebo Luo",
      "Rongrong Ji"
    ],
    "abstract": "Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13093v3",
    "published_date": "2024-11-20 07:44:34 UTC",
    "updated_date": "2024-12-20 12:09:50 UTC"
  },
  {
    "arxiv_id": "2411.15206v2",
    "title": "Conditional Distribution Learning on Graphs",
    "authors": [
      "Jie Chen",
      "Hua Mao",
      "Yuanbiao Gou",
      "Zhu Wang",
      "Xi Peng"
    ],
    "abstract": "Leveraging the diversity and quantity of data provided by various\ngraph-structured data augmentations while preserving intrinsic semantic\ninformation is challenging. Additionally, successive layers in graph neural\nnetwork (GNN) tend to produce more similar node embeddings, while graph\ncontrastive learning aims to increase the dissimilarity between negative pairs\nof node embeddings. This inevitably results in a conflict between the\nmessage-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of\nnegative pairs via intraviews. In this paper, we propose a conditional\ndistribution learning (CDL) method that learns graph representations from\ngraph-structured data for semisupervised graph classification. Specifically, we\npresent an end-to-end graph representation learning model to align the\nconditional distributions of weakly and strongly augmented features over the\noriginal features. This alignment enables the CDL model to effectively preserve\nintrinsic semantic information when both weak and strong augmentations are\napplied to graph-structured data. To avoid the conflict between the MPM and the\nCL of negative pairs, positive pairs of node representations are retained for\nmeasuring the similarity between the original features and the corresponding\nweakly augmented features. Extensive experiments with several benchmark graph\ndatasets demonstrate the effectiveness of the proposed CDL method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.15206v2",
    "published_date": "2024-11-20 07:26:36 UTC",
    "updated_date": "2025-01-28 15:27:48 UTC"
  },
  {
    "arxiv_id": "2411.13079v2",
    "title": "Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback",
    "authors": [
      "Feng Gao",
      "Chao Yu",
      "Yu Wang",
      "Yi Wu"
    ],
    "abstract": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Submitted to RAL",
    "pdf_url": "http://arxiv.org/pdf/2411.13079v2",
    "published_date": "2024-11-20 07:07:42 UTC",
    "updated_date": "2025-03-04 17:07:15 UTC"
  },
  {
    "arxiv_id": "2411.13072v1",
    "title": "AMaze: An intuitive benchmark generator for fast prototyping of generalizable agents",
    "authors": [
      "Kevin Godin-Dubois",
      "Karine Miras",
      "Anna V. Kononova"
    ],
    "abstract": "Traditional approaches to training agents have generally involved a single,\ndeterministic environment of minimal complexity to solve various tasks such as\nrobot locomotion or computer vision. However, agents trained in static\nenvironments lack generalization capabilities, limiting their potential in\nbroader scenarios. Thus, recent benchmarks frequently rely on multiple\nenvironments, for instance, by providing stochastic noise, simple permutations,\nor altogether different settings. In practice, such collections result mainly\nfrom costly human-designed processes or the liberal use of random number\ngenerators. In this work, we introduce AMaze, a novel benchmark generator in\nwhich embodied agents must navigate a maze by interpreting visual signs of\narbitrary complexities and deceptiveness. This generator promotes human\ninteraction through the easy generation of feature-specific mazes and an\nintuitive understanding of the resulting agents' strategies. As a\nproof-of-concept, we demonstrate the capabilities of the generator in a simple,\nfully discrete case with limited deceptiveness. Agents were trained under three\ndifferent regimes (one-shot, scaffolding, interactive), and the results showed\nthat the latter two cases outperform direct training in terms of generalization\ncapabilities. Indeed, depending on the combination of generalization metric,\ntraining regime, and algorithm, the median gain ranged from 50% to 100% and\nmaximal performance was achieved through interactive training, thereby\ndemonstrating the benefits of a controllable human-in-the-loop benchmark\ngenerator.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Under review in Frontiers in Artificial Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2411.13072v1",
    "published_date": "2024-11-20 06:47:29 UTC",
    "updated_date": "2024-11-20 06:47:29 UTC"
  },
  {
    "arxiv_id": "2411.14487v1",
    "title": "Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine",
    "authors": [
      "Yifan Yang",
      "Qiao Jin",
      "Robert Leaman",
      "Xiaoyu Liu",
      "Guangzhi Xiong",
      "Maame Sarfo-Gyamfi",
      "Changlin Gong",
      "Santiago Ferrière-Steinert",
      "W. John Wilbur",
      "Xiaojun Li",
      "Jiaxin Yuan",
      "Bang An",
      "Kelvin S. Castro",
      "Francisco Erramuspe Álvarez",
      "Matías Stockle",
      "Aidong Zhang",
      "Furong Huang",
      "Zhiyong Lu"
    ],
    "abstract": "The remarkable capabilities of Large Language Models (LLMs) make them\nincreasingly compelling for adoption in real-world healthcare applications.\nHowever, the risks associated with using LLMs in medical applications have not\nbeen systematically characterized. We propose using five key principles for\nsafe and trustworthy medical AI: Truthfulness, Resilience, Fairness,\nRobustness, and Privacy, along with ten specific aspects. Under this\ncomprehensive framework, we introduce a novel MedGuard benchmark with 1,000\nexpert-verified questions. Our evaluation of 11 commonly used LLMs shows that\nthe current language models, regardless of their safety alignment mechanisms,\ngenerally perform poorly on most of our benchmarks, particularly when compared\nto the high performance of human physicians. Despite recent reports indicate\nthat advanced LLMs like ChatGPT can match or even exceed human performance in\nvarious medical tasks, this study underscores a significant safety gap,\nhighlighting the crucial need for human oversight and the implementation of AI\nsafety guardrails.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14487v1",
    "published_date": "2024-11-20 06:34:32 UTC",
    "updated_date": "2024-11-20 06:34:32 UTC"
  },
  {
    "arxiv_id": "2411.13057v1",
    "title": "Branches, Assemble! Multi-Branch Cooperation Network for Large-Scale Click-Through Rate Prediction at Taobao",
    "authors": [
      "Xu Chen",
      "Zida Cheng",
      "Yuangang Pan",
      "Shuai Xiao",
      "Xiaoming Liu",
      "Jinsong Lan",
      "Qingwen Liu",
      "Ivor W. Tsang"
    ],
    "abstract": "Existing click-through rate (CTR) prediction works have studied the role of\nfeature interaction through a variety of techniques. Each interaction technique\nexhibits its own strength, and solely using one type could constrain the\nmodel's capability to capture the complex feature relationships, especially for\nindustrial large-scale data with enormous users and items. Recent research\nshows that effective CTR models often combine an MLP network with a dedicated\nfeature interaction network in a two-parallel structure. However, the interplay\nand cooperative dynamics between different streams or branches remain\nunder-researched. In this work, we introduce a novel Multi-Branch Cooperation\nNetwork (MBCnet) which enables multiple branch networks to collaborate with\neach other for better complex feature interaction modeling. Specifically,\nMBCnet consists of three branches: the Expert-based Feature Grouping and\nCrossing (EFGC) branch that promotes the model's memorization ability of\nspecific feature fields, the low rank Cross Net branch and Deep branch to\nenhance both explicit and implicit feature crossing for improved\ngeneralization. Among branches, a novel cooperation scheme is proposed based on\ntwo principles: branch co-teaching and moderate differentiation. Branch\nco-teaching encourages well-learned branches to support poorly-learned ones on\nspecific training samples. Moderate differentiation advocates branches to\nmaintain a reasonable level of difference in their feature representations. The\ncooperation strategy improves learning through mutual knowledge sharing via\nco-teaching and boosts the discovery of diverse feature interactions across\nbranches. Extensive experiments on large-scale industrial datasets and online\nA/B test demonstrate MBCnet's superior performance, delivering a 0.09 point\nincrease in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes will\nbe released soon.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.13057v1",
    "published_date": "2024-11-20 06:10:06 UTC",
    "updated_date": "2024-11-20 06:10:06 UTC"
  },
  {
    "arxiv_id": "2411.15204v2",
    "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "authors": [
      "Minguk Jang",
      "Hye Won Chung"
    ],
    "abstract": "Test-time adaptation (TTA) is an effective approach to mitigate performance\ndegradation of trained models when encountering input distribution shifts at\ntest time. However, existing TTA methods often suffer significant performance\ndrops when facing additional class distribution shifts. We first analyze TTA\nmethods under label distribution shifts and identify the presence of class-wise\nconfusion patterns commonly observed across different covariate shifts. Based\non this observation, we introduce label Distribution shift-Aware prediction\nRefinement for Test-time adaptation (DART), a novel TTA method that refines the\npredictions by focusing on class-wise confusion patterns. DART trains a\nprediction refinement module during an intermediate time by exposing it to\nseveral batches with diverse class distributions using the training dataset.\nThis module is then used during test time to detect and correct class\ndistribution shifts, significantly improving pseudo-label accuracy for test\ndata. Our method exhibits 5-18% gains in accuracy under label distribution\nshifts on CIFAR-10C, without any performance degradation when there is no label\ndistribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and\nImageNet benchmarks demonstrate DART's ability to correct inaccurate\npredictions caused by test-time distribution shifts. This improvement leads to\nenhanced performance in existing TTA methods, making DART a valuable plug-in\ntool.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15204v2",
    "published_date": "2024-11-20 05:58:52 UTC",
    "updated_date": "2025-02-05 01:47:29 UTC"
  },
  {
    "arxiv_id": "2411.13053v1",
    "title": "MEGL: Multimodal Explanation-Guided Learning",
    "authors": [
      "Yifei Zhang",
      "Tianxu Jiang",
      "Bo Pan",
      "Jingyu Wang",
      "Guangji Bai",
      "Liang Zhao"
    ],
    "abstract": "Explaining the decision-making processes of Artificial Intelligence (AI)\nmodels is crucial for addressing their \"black box\" nature, particularly in\ntasks like image classification. Traditional eXplainable AI (XAI) methods\ntypically rely on unimodal explanations, either visual or textual, each with\ninherent limitations. Visual explanations highlight key regions but often lack\nrationale, while textual explanations provide context without spatial\ngrounding. Further, both explanation types can be inconsistent or incomplete,\nlimiting their reliability. To address these challenges, we propose a novel\nMultimodal Explanation-Guided Learning (MEGL) framework that leverages both\nvisual and textual explanations to enhance model interpretability and improve\nclassification performance. Our Saliency-Driven Textual Grounding (SDTG)\napproach integrates spatial information from visual explanations into textual\nrationales, providing spatially grounded and contextually rich explanations.\nAdditionally, we introduce Textual Supervision on Visual Explanations to align\nvisual explanations with textual rationales, even in cases where ground truth\nvisual annotations are missing. A Visual Explanation Distribution Consistency\nloss further reinforces visual coherence by aligning the generated visual\nexplanations with dataset-level patterns, enabling the model to effectively\nlearn from incomplete multimodal supervision. We validate MEGL on two new\ndatasets, Object-ME and Action-ME, for image classification with multimodal\nexplanations. Experimental results demonstrate that MEGL outperforms previous\napproaches in prediction accuracy and explanation quality across both visual\nand textual domains. Our code will be made available upon the acceptance of the\npaper.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13053v1",
    "published_date": "2024-11-20 05:57:00 UTC",
    "updated_date": "2024-11-20 05:57:00 UTC"
  },
  {
    "arxiv_id": "2411.13614v1",
    "title": "Verification and Validation of Autonomous Systems",
    "authors": [
      "Sneha Sudhir Shetiya",
      "Vikas Vyas",
      "Shreyas Renukuntla"
    ],
    "abstract": "This paper describes how to proficiently prevent software defects in\nautonomous vehicles, discover and correct defects if they are encountered, and\ncreate a higher level of assurance in the software product development phase.\nIt also describes how to ensure high assurance on software reliability.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13614v1",
    "published_date": "2024-11-20 05:36:22 UTC",
    "updated_date": "2024-11-20 05:36:22 UTC"
  },
  {
    "arxiv_id": "2411.13045v2",
    "title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning",
    "authors": [
      "Gang Zhao",
      "Ximing Zhang",
      "Chenji Lu",
      "Hui Zhao",
      "Tianshu Wu",
      "Pengjie Wang",
      "Jian Xu",
      "Bo Zheng"
    ],
    "abstract": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted by WWW 2025 oral",
    "pdf_url": "http://arxiv.org/pdf/2411.13045v2",
    "published_date": "2024-11-20 05:30:15 UTC",
    "updated_date": "2025-02-08 02:56:02 UTC"
  },
  {
    "arxiv_id": "2411.13036v1",
    "title": "Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization",
    "authors": [
      "Sanghyeob Song",
      "Jaihyun Lew",
      "Hyemi Jang",
      "Sungroh Yoon"
    ],
    "abstract": "Estimating the homography between two images is crucial for mid- or\nhigh-level vision tasks, such as image stitching and fusion. However, using\nsupervised learning methods is often challenging or costly due to the\ndifficulty of collecting ground-truth data. In response, unsupervised learning\napproaches have emerged. Most early methods, though, assume that the given\nimage pairs are from the same camera or have minor lighting differences.\nConsequently, while these methods perform effectively under such conditions,\nthey generally fail when input image pairs come from different domains,\nreferred to as multimodal image pairs. To address these limitations, we propose\nAltO, an unsupervised learning framework for estimating homography in\nmultimodal image pairs. Our method employs a two-phase alternating optimization\nframework, similar to Expectation-Maximization (EM), where one phase reduces\nthe geometry gap and the other addresses the modality gap. To handle these\ngaps, we use Barlow Twins loss for the modality gap and propose an extended\nversion, Geometry Barlow Twins, for the geometry gap. As a result, we\ndemonstrate that our method, AltO, can be trained on multimodal datasets\nwithout any ground-truth data. It not only outperforms other unsupervised\nmethods but is also compatible with various architectures of homography\nestimators. The source code can be found\nat:~\\url{https://github.com/songsang7/AltO}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper is accepted to the Thirty-Eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.13036v1",
    "published_date": "2024-11-20 04:56:19 UTC",
    "updated_date": "2024-11-20 04:56:19 UTC"
  },
  {
    "arxiv_id": "2411.15203v1",
    "title": "Multimodal large language model for wheat breeding: a new exploration of smart breeding",
    "authors": [
      "Guofeng Yang",
      "Yu Li",
      "Yong He",
      "Zhenjiang Zhou",
      "Lingzhen Ye",
      "Hui Fang",
      "Yiqi Luo",
      "Xuping Feng"
    ],
    "abstract": "UAV remote sensing technology has become a key technology in crop breeding,\nwhich can achieve high-throughput and non-destructive collection of crop\nphenotyping data. However, the multidisciplinary nature of breeding has brought\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\nis important to develop a smart breeding goal tool to mine cross-domain\nmultimodal data. Based on different pre-trained open-source multimodal large\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\nreinforcement learning from human feedback (RLHF) technologies to inject\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\nevaluated using the newly created evaluation benchmark in this study. The\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\nand InternVL2-8B has leading performance. Then, subsequent experiments were\nconducted using the WBLM. Ablation experiments indicated that the combination\nof SFT, RAG, and RLHF technologies can improve the overall generation\nperformance, enhance the generated quality, balance the timeliness and\nadaptability of the generated answer, and reduce hallucinations and biases. The\nWBLM performed best in wheat yield prediction using cross-domain data (remote\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\nprofessional decision support answers for phenotyping estimation, environmental\nstress assessment, target germplasm screening, cultivation technique\nrecommendation, and seed price query tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.15203v1",
    "published_date": "2024-11-20 04:47:42 UTC",
    "updated_date": "2024-11-20 04:47:42 UTC"
  },
  {
    "arxiv_id": "2411.13032v1",
    "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large Language Models",
    "authors": [
      "Angel Hsing-Chi Hwang",
      "Q. Vera Liao",
      "Su Lin Blodgett",
      "Alexandra Olteanu",
      "Adam Trischler"
    ],
    "abstract": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13032v1",
    "published_date": "2024-11-20 04:42:32 UTC",
    "updated_date": "2024-11-20 04:42:32 UTC"
  },
  {
    "arxiv_id": "2411.14486v1",
    "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
    "authors": [
      "David Noever",
      "Forrest McKee"
    ],
    "abstract": "This research introduces a novel evaluation framework designed to assess\nlarge language models' (LLMs) ability to acknowledge uncertainty on 675\nfundamentally unsolvable problems. Using a curated dataset of graduate-level\ngrand challenge questions with intentionally unknowable answers, we evaluated\ntwelve state-of-the-art LLMs, including both open and closed-source models, on\ntheir propensity to admit ignorance rather than generate plausible but\nincorrect responses. The best models scored in 62-68% accuracy ranges for\nadmitting the problem solution was unknown in fields ranging from biology to\nphilosophy and mathematics. We observed an inverse relationship between problem\ndifficulty and model accuracy, with GPT-4 demonstrating higher rates of\nuncertainty acknowledgment on more challenging problems (35.8%) compared to\nsimpler ones (20.0%). This pattern indicates that models may be more prone to\ngenerate speculative answers when problems appear more tractable. The study\nalso revealed significant variations across problem categories, with models\nshowing difficulty in acknowledging uncertainty in invention and NP-hard\nproblems while performing relatively better on philosophical and psychological\nchallenges. These results contribute to the growing body of research on\nartificial general intelligence (AGI) assessment by highlighting the importance\nof uncertainty recognition as a critical component of future machine\nintelligence evaluation. This impossibility test thus extends previous\ntheoretical frameworks for universal intelligence testing by providing\nempirical evidence of current limitations in LLMs' ability to recognize their\nown knowledge boundaries, suggesting new directions for improving model\ntraining architectures and evaluation approaches.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14486v1",
    "published_date": "2024-11-20 04:12:29 UTC",
    "updated_date": "2024-11-20 04:12:29 UTC"
  },
  {
    "arxiv_id": "2411.13022v2",
    "title": "Fast MRI for All: Bridging Equity Gaps via Training without Raw Data Access",
    "authors": [
      "Yaşar Utku Alçalar",
      "Merve Gülle",
      "Mehmet Akçakaya"
    ],
    "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Though\nPD-DL offers higher acceleration rates than existing clinical fast MRI\ntechniques, their use has been limited outside specialized MRI centers. A key\nchallenge is generalization to underrepresented pathologies or populations,\nnoted in multiple studies, with fine-tuning on target populations suggested for\nimprovement. However, current approaches for PD-DL training require access to\nraw k-space measurements, which is typically only available at specialized MRI\ncenters that have research agreements for such data access. This is especially\nan issue for rural and underserved areas, where commercial MRI scanners only\nprovide access to a final reconstructed image. To tackle these challenges, we\npropose Compressibility-inspired Unsupervised Learning via Parallel Imaging\nFidelity (CUPID) for high-quality PD-DL training using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates output\nquality with a compressibility-based approach while ensuring that the output\nstays consistent with the clinical parallel imaging reconstruction through\nwell-designed perturbations. Our results show CUPID achieves similar quality to\nestablished PD-DL training that requires k-space data while outperforming\ncompressed sensing (CS) and diffusion-based generative methods. We further\ndemonstrate its effectiveness in a zero-shot training setup for retrospectively\nand prospectively sub-sampled acquisitions, attesting to its minimal training\nburden. As an approach that radically deviates from existing strategies, CUPID\npresents an opportunity to provide equitable access to fast MRI for underserved\npopulations in an attempt to reduce the inequalities associated with this\nexpensive imaging modality.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13022v2",
    "published_date": "2024-11-20 03:53:41 UTC",
    "updated_date": "2025-03-13 15:54:28 UTC"
  },
  {
    "arxiv_id": "2411.13613v1",
    "title": "SuPLE: Robot Learning with Lyapunov Rewards",
    "authors": [
      "Phu Nguyen",
      "Daniel Polani",
      "Stas Tiomkin"
    ],
    "abstract": "The reward function is an essential component in robot learning. Reward\ndirectly affects the sample and computational complexity of learning, and the\nquality of a solution. The design of informative rewards requires domain\nknowledge, which is not always available. We use the properties of the dynamics\nto produce system-appropriate reward without adding external assumptions.\nSpecifically, we explore an approach to utilize the Lyapunov exponents of the\nsystem dynamics to generate a system-immanent reward. We demonstrate that the\n`Sum of the Positive Lyapunov Exponents' (SuPLE) is a strong candidate for the\ndesign of such a reward. We develop a computational framework for the\nderivation of this reward, and demonstrate its effectiveness on classical\nbenchmarks for sample-based stabilization of various dynamical systems. It\neliminates the need to start the training trajectories at arbitrary states,\nalso known as auxiliary exploration. While the latter is a common practice in\nsimulated robot learning, it is unpractical to consider to use it in real\nrobotic systems, since they typically start from natural rest states such as a\npendulum at the bottom, a robot on the ground, etc. and can not be easily\ninitialized at arbitrary states. Comparing the performance of SuPLE to\ncommonly-used reward functions, we observe that the latter fail to find a\nsolution without auxiliary exploration, even for the task of swinging up the\ndouble pendulum and keeping it stable at the upright position, a prototypical\nscenario for multi-linked robots. SuPLE-induced rewards for robot learning\noffer a novel route for effective robot learning in typical as opposed to\nhighly specialized or fine-tuned scenarios. Our code is publicly available for\nreproducibility and further research.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.13613v1",
    "published_date": "2024-11-20 03:20:50 UTC",
    "updated_date": "2024-11-20 03:20:50 UTC"
  },
  {
    "arxiv_id": "2411.13008v1",
    "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics",
    "authors": [
      "Anique Tahir",
      "Lu Cheng",
      "Manuel Sandoval",
      "Yasin N. Silva",
      "Deborah L. Hall",
      "Huan Liu"
    ],
    "abstract": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in ASONAM 24 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2411.13008v1",
    "published_date": "2024-11-20 03:16:07 UTC",
    "updated_date": "2024-11-20 03:16:07 UTC"
  },
  {
    "arxiv_id": "2411.13006v1",
    "title": "Automating Sonologists USG Commands with AI and Voice Interface",
    "authors": [
      "Emad Mohamed",
      "Shruti Tiwari",
      "Sheena Christabel Pravin"
    ],
    "abstract": "This research presents an advanced AI-powered ultrasound imaging system that\nincorporates real-time image processing, organ tracking, and voice commands to\nenhance the efficiency and accuracy of diagnoses in clinical practice.\nTraditional ultrasound diagnostics often require significant time and introduce\na degree of subjectivity due to user interaction. The goal of this innovative\nsolution is to provide Sonologists with a more predictable and productive\nimaging procedure utilizing artificial intelligence, computer vision, and voice\ntechnology. The functionality of the system employs computer vision and deep\nlearning algorithms, specifically adopting the Mask R-CNN model from Detectron2\nfor semantic segmentation of organs and key landmarks. This automation improves\ndiagnostic accuracy by enabling the extraction of valuable information with\nminimal human input. Additionally, it includes a voice recognition feature that\nallows for hands-free operation, enabling users to control the system with\ncommands such as freeze or liver, all while maintaining their focus on the\npatient. The architecture comprises video processing and real-time segmentation\nmodules that prepare the system to perform essential imaging functions, such as\nfreezing and zooming in on frames. The liver histopathology module, optimized\nfor detecting fibrosis, achieved an impressive accuracy of 98.6%. Furthermore,\nthe organ segmentation module produces output confidence levels between 50% and\n95%, demonstrating its efficacy in organ detection.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13006v1",
    "published_date": "2024-11-20 03:03:49 UTC",
    "updated_date": "2024-11-20 03:03:49 UTC"
  },
  {
    "arxiv_id": "2411.14485v2",
    "title": "Mediating Modes of Thought: LLM's for design scripting",
    "authors": [
      "Moritz Rietschel",
      "Fang Guo",
      "Kyle Steinfeld"
    ],
    "abstract": "Architects adopt visual scripting and parametric design tools to explore more\nexpansive design spaces (Coates, 2010), refine their thinking about the\ngeometric logic of their design (Woodbury, 2010), and overcome conventional\nsoftware limitations (Burry, 2011). Despite two decades of effort to make\ndesign scripting more accessible, a disconnect between a designer's free ways\nof thinking and the rigidity of algorithms remains (Burry, 2011). Recent\ndevelopments in Large Language Models (LLMs) suggest this might soon change, as\nLLMs encode a general understanding of human context and exhibit the capacity\nto produce geometric logic. This project speculates that if LLMs can\neffectively mediate between user intent and algorithms, they become a powerful\ntool to make scripting in design more widespread and fun. We explore if such\nsystems can interpret natural language prompts to assemble geometric operations\nrelevant to computational design scripting. In the system, multiple layers of\nLLM agents are configured with specific context to infer the user intent and\nconstruct a sequential logic. Given a user's high-level text prompt, a\ngeometric description is created, distilled into a sequence of logic\noperations, and mapped to software-specific commands. The completed script is\nconstructed in the user's visual programming interface. The system succeeds in\ngenerating complete visual scripts up to a certain complexity but fails beyond\nthis complexity threshold. It shows how LLMs can make design scripting much\nmore aligned with human creativity and thought. Future research should explore\nconversational interactions, expand to multimodal inputs and outputs, and\nassess the performance of these tools.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Published at ACADIA 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.14485v2",
    "published_date": "2024-11-20 02:49:18 UTC",
    "updated_date": "2024-12-03 22:27:12 UTC"
  },
  {
    "arxiv_id": "2411.12990v1",
    "title": "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices",
    "authors": [
      "Anka Reuel",
      "Amelia Hardy",
      "Chandler Smith",
      "Max Lamparth",
      "Malcolm Hardy",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "AI models are increasingly prevalent in high-stakes environments,\nnecessitating thorough assessment of their capabilities and risks. Benchmarks\nare popular for measuring these attributes and for comparing model performance,\ntracking progress, and identifying weaknesses in foundation and non-foundation\nmodels. They can inform model selection for downstream tasks and influence\npolicy initiatives. However, not all benchmarks are the same: their quality\ndepends on their design and usability. In this paper, we develop an assessment\nframework considering 46 best practices across an AI benchmark's lifecycle and\nevaluate 24 AI benchmarks against it. We find that there exist large quality\ndifferences and that commonly used benchmarks suffer from significant issues.\nWe further find that most benchmarks do not report statistical significance of\ntheir results nor allow for their results to be easily replicated. To support\nbenchmark developers in aligning with best practices, we provide a checklist\nfor minimum quality assurance based on our assessment. We also develop a living\nrepository of benchmark assessments to support benchmark comparability,\naccessible at betterbench.stanford.edu.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted as a Spotlight Poster to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12990v1",
    "published_date": "2024-11-20 02:38:24 UTC",
    "updated_date": "2024-11-20 02:38:24 UTC"
  },
  {
    "arxiv_id": "2411.12980v3",
    "title": "LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement",
    "authors": [
      "Siwen Jiao",
      "Yangyi Fang",
      "Baoyun Peng",
      "Wangqun Chen",
      "Bharadwaj Veeravalli"
    ],
    "abstract": "Recent advancements in Visual Language Models (VLMs) have made them crucial\nfor visual question answering (VQA) in autonomous driving, enabling natural\nhuman-vehicle interactions. However, existing methods often struggle in dynamic\ndriving environments, as they usually focus on static images or videos and rely\non downsampling to manage computational costs. This results in the loss of\ncritical details and the difficulty in effectively integrating spatial and\ntemporal information, undermining fine-grained perception and temporal\ncoherence essential for effective decision-making. To tackle these challenges,\nwe introduce LaVida Drive, a novel and efficient VQA framework for autonomous\ndriving. LaVida Drive seamlessly integrates temporal data while maintaining\nhigh-resolution inputs for detailed visual perception. It optimizes spatial\nprocessing by retaining high-resolution data for intricate details and using\nlower-resolution inputs for temporal analysis to focus on motion-related\nfeatures, thereby boosting computational efficiency. The core of LaVida Drive\nconsists of two modules: the \\textit{Query-aware Token Selection} module and\nthe \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former\ndynamically selects the most relevant visual tokens based on semantic alignment\nwith the input query, reducing the token count from high-resolution spatial\ninput. The latter ensures smooth and coherent interactions between spatial and\ntemporal information, preserving contextual continuity across frames. Extensive\nexperiments on various autonomous driving question-answering benchmarks show\nthat LaVida Drive significantly reduces visual tokens, enhances efficiency, and\nimproves overall performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12980v3",
    "published_date": "2024-11-20 02:14:07 UTC",
    "updated_date": "2025-02-22 16:03:34 UTC"
  },
  {
    "arxiv_id": "2411.12977v3",
    "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Collaborative Learning",
    "authors": [
      "Mircea Lică",
      "Ojas Shirekar",
      "Baptiste Colle",
      "Chirag Raman"
    ],
    "abstract": "Contemporary embodied agents powered by large language models (LLMs), such as\nVoyager, have shown promising capabilities in individual learning within\nopen-ended environments like Minecraft. However, when powered by open LLMs,\nthey struggle with basic tasks even after domain-specific fine-tuning. We\npresent MindForge, a generative-agent framework for collaborative lifelong\nlearning through explicit perspective taking. We introduce three key\ninnovations: (1) a structured theory of mind representation linking percepts,\nbeliefs, desires, and actions; (2) natural interagent communication; and (3) a\nmulticomponent memory system. In Minecraft experiments, MindForge agents\npowered by open-weight LLMs significantly outperform their Voyager counterparts\nin basic tasks where traditional Voyager fails without GPT-4, collecting\n$2.3\\times$ more unique items and achieving $3\\times$ more tech-tree\nmilestones, advancing from basic wood tools to advanced iron equipment.\nMindForge agents demonstrate sophisticated behaviors, including expert-novice\nknowledge transfer, collaborative problem solving, and adaptation to\nout-of-distribution tasks through accumulated collaborative experiences.\nMindForge advances the democratization of embodied AI development through\nopen-ended social learning, enabling peer-to-peer knowledge sharing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.12977v3",
    "published_date": "2024-11-20 02:10:44 UTC",
    "updated_date": "2025-02-19 22:59:28 UTC"
  },
  {
    "arxiv_id": "2411.14484v1",
    "title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach",
    "authors": [
      "Atharva Gundawar",
      "Karthik Valmeekam",
      "Mudit Verma",
      "Subbarao Kambhampati"
    ],
    "abstract": "Previous work has attempted to boost Large Language Model (LLM) performance\non planning and scheduling tasks through a variety of prompt engineering\ntechniques. While these methods can work within the distributions tested, they\nare neither robust nor predictable. This limitation can be addressed through\ncompound LLM architectures where LLMs work in conjunction with other components\nto ensure reliability. In this paper, we present a technical evaluation of a\ncompound LLM architecture--the LLM-Modulo framework. In this framework, an LLM\nis paired with a complete set of sound verifiers that validate its output,\nre-prompting it if it fails. This approach ensures that the system can never\noutput any fallacious output, and therefore that every output generated is\nguaranteed correct--something previous techniques have not been able to claim.\nOur results, evaluated across four scheduling domains, demonstrate significant\nperformance gains with the LLM-Modulo framework using various models.\nAdditionally, we explore modifications to the base configuration of the\nframework and assess their impact on overall system performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.14484v1",
    "published_date": "2024-11-20 02:04:09 UTC",
    "updated_date": "2024-11-20 02:04:09 UTC"
  },
  {
    "arxiv_id": "2411.13611v3",
    "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs",
    "authors": [
      "Zhihan Liu",
      "Shenao Zhang",
      "Yongfei Liu",
      "Boyi Liu",
      "Yingxiang Yang",
      "Zhaoran Wang"
    ],
    "abstract": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.13611v3",
    "published_date": "2024-11-20 02:03:16 UTC",
    "updated_date": "2024-12-10 07:47:15 UTC"
  },
  {
    "arxiv_id": "2411.12967v1",
    "title": "Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue",
    "authors": [
      "Yunuo Zhang",
      "Baiting Luo",
      "Ayan Mukhopadhyay",
      "Daniel Stojcsics",
      "Daniel Elenius",
      "Anirban Roy",
      "Susmit Jha",
      "Miklos Maroti",
      "Xenofon Koutsoukos",
      "Gabor Karsai",
      "Abhishek Dubey"
    ],
    "abstract": "Efficient path optimization for drones in search and rescue operations faces\nchallenges, including limited visibility, time constraints, and complex\ninformation gathering in urban environments. We present a comprehensive\napproach to optimize UAV-based search and rescue operations in neighborhood\nareas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path\nplanning problem is formulated as a partially observable Markov decision\nprocess (POMDP), and we propose a novel ``Shrinking POMCP'' approach to address\ntime constraints. In the AirSim environment, we integrate our approach with a\nprobabilistic world model for belief maintenance and a neurosymbolic navigator\nfor obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with\nequivalent functionality. We compare trajectories generated by different\napproaches in the 2D simulator and evaluate performance across various belief\ntypes in the 3D AirSim-ROS simulator. Experimental results from both simulators\ndemonstrate that our proposed shrinking POMCP solution achieves significant\nimprovements in search times compared to alternative methods, showcasing its\npotential for enhancing the efficiency of UAV-assisted search and rescue\noperations.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to the The 3rd International Conference on Assured Autonomy",
    "pdf_url": "http://arxiv.org/pdf/2411.12967v1",
    "published_date": "2024-11-20 01:41:29 UTC",
    "updated_date": "2024-11-20 01:41:29 UTC"
  },
  {
    "arxiv_id": "2411.12964v1",
    "title": "Real-Time Energy-Optimal Path Planning for Electric Vehicles",
    "authors": [
      "Saman Ahmadi",
      "Guido Tack",
      "Daniel Harabor",
      "Philip Kilby",
      "Mahdi Jalili"
    ],
    "abstract": "The rapid adoption of electric vehicles (EVs) in modern transport systems has\nmade energy-aware routing a critical task in their successful integration,\nespecially within large-scale networks. In cases where an EV's remaining energy\nis limited and charging locations are not easily accessible, some destinations\nmay only be reachable through an energy-optimal path: a route that consumes\nless energy than all other alternatives. The feasibility of such\nenergy-efficient paths depends heavily on the accuracy of the energy model used\nfor planning, and thus failing to account for vehicle dynamics can lead to\ninaccurate energy estimates, rendering some planned routes infeasible in\nreality. This paper explores the impact of vehicle dynamics on energy-optimal\npath planning for EVs. We develop an accurate energy model that incorporates\nkey vehicle dynamics parameters into energy calculations, thereby reducing the\nrisk of planning infeasible paths under battery constraints. The paper also\nintroduces two novel online reweighting functions that allow for a faster,\npre-processing free, pathfinding in the presence of negative energy costs\nresulting from regenerative braking, making them ideal for real-time\napplications. Through extensive experimentation on real-world transport\nnetworks, we demonstrate that our approach considerably enhances energy-optimal\npathfinding for EVs in both computational efficiency and energy estimation\naccuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 7 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.12964v1",
    "published_date": "2024-11-20 01:39:08 UTC",
    "updated_date": "2024-11-20 01:39:08 UTC"
  },
  {
    "arxiv_id": "2411.14157v1",
    "title": "DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback",
    "authors": [
      "Mahsa Sheikholeslami",
      "Navid Mazrouei",
      "Yousof Gheisari",
      "Afshin Fasihi",
      "Matin Irajpour",
      "Ali Motahharynia"
    ],
    "abstract": "Traditional drug design faces significant challenges due to inherent chemical\nand biological complexities, often resulting in high failure rates in clinical\ntrials. Deep learning advancements, particularly generative models, offer\npotential solutions to these challenges. One promising algorithm is DrugGPT, a\ntransformer-based model, that generates small molecules for input protein\nsequences. Although promising, it generates both chemically valid and invalid\nstructures and does not incorporate the features of approved drugs, resulting\nin time-consuming and inefficient drug discovery. To address these issues, we\nintroduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is\nfine-tuned on approved drug-target interactions and optimized with proximal\npolicy optimization. By giving reward feedback from protein-ligand binding\naffinity prediction using pre-trained transformers (PLAPT) and a customized\ninvalid structure assessor, DrugGen significantly improves performance.\nEvaluation across multiple targets demonstrated that DrugGen achieves 100%\nvalid structure generation compared to 95.5% with DrugGPT and produced\nmolecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared\nto DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking\nsimulations further validate its ability to generate molecules targeting\nbinding sites effectively. For example, in the case of fatty acid-binding\nprotein 5 (FABP5), DrugGen generated molecules with superior docking scores\n(FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule\n(Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows\npotential for drug repositioning and creating novel pharmacophores for existing\ntargets. By producing high-quality small molecules, DrugGen provides a\nhigh-performance medium for advancing pharmaceutical research and drug\ndiscovery.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "20 pages, 5 figures, 3 tables, and 7 supplementary files. To use the\n  model, see https://huggingface.co/alimotahharynia/DrugGen",
    "pdf_url": "http://arxiv.org/pdf/2411.14157v1",
    "published_date": "2024-11-20 01:21:07 UTC",
    "updated_date": "2024-11-20 01:21:07 UTC"
  },
  {
    "arxiv_id": "2411.15201v1",
    "title": "Beyond Visual Understanding: Introducing PARROT-360V for Vision Language Model Benchmarking",
    "authors": [
      "Harsha Vardhan Khurdula",
      "Basem Rizk",
      "Indus Khaitan",
      "Janit Anjaria",
      "Aviral Srivastava",
      "Rajvardhan Khaitan"
    ],
    "abstract": "Current benchmarks for evaluating Vision Language Models (VLMs) often fall\nshort in thoroughly assessing model abilities to understand and process complex\nvisual and textual content. They typically focus on simple tasks that do not\nrequire deep reasoning or the integration of multiple data modalities to solve\nan original problem. To address this gap, we introduce the PARROT-360V\nBenchmark, a novel and comprehensive benchmark featuring 2487 challenging\nvisual puzzles designed to test VLMs on complex visual reasoning tasks. We\nevaluated leading models: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro, using\nPARROT-360V to assess their capabilities in combining visual clues with\nlanguage skills to solve tasks in a manner akin to human problem-solving. Our\nfindings reveal a notable performance gap: state-of-the-art models scored\nbetween 28 to 56 percentage on our benchmark, significantly lower than their\nperformance on popular benchmarks. This underscores the limitations of current\nVLMs in handling complex, multi-step reasoning tasks and highlights the need\nfor more robust evaluation frameworks to advance the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "I.2.7; I.2.10"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 4 figures, Accepted at COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.15201v1",
    "published_date": "2024-11-20 01:09:21 UTC",
    "updated_date": "2024-11-20 01:09:21 UTC"
  },
  {
    "arxiv_id": "2411.12950v2",
    "title": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware Attributes Learning",
    "authors": [
      "Ming Yin",
      "Qiang Zhou",
      "Zongsheng Cao",
      "Mei Li"
    ],
    "abstract": "Numerical reasoning is pivotal in various artificial intelligence\napplications, such as natural language processing and recommender systems,\nwhere it involves using entities, relations, and attribute values (e.g.,\nweight, length) to infer new factual relations (e.g., the Nile is longer than\nthe Amazon). However, existing approaches encounter two critical challenges in\nmodeling: (1) semantic relevance-the challenge of insufficiently capturing the\nnecessary contextual interactions among entities, relations, and numerical\nattributes, often resulting in suboptimal inference; and (2) semantic\nambiguity-the difficulty in accurately distinguishing ordinal relationships\nduring numerical reasoning, which compromises the generation of high-quality\nsamples and limits the effectiveness of contrastive learning. To address these\nchallenges, we propose the novel Knowledge-Aware Attributes Embedding model\n(KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to\novercome the challenge of semantic relevance, we introduce a\nMixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the\nsemantics of entities, relations, and numerical attributes into a joint\nsemantic space. To tackle semantic ambiguity, we implement a new ordinal\nknowledge contrastive learning (OKCL) strategy that generates high-quality\nordinal samples from the original data with the aid of ordinal relations,\ncapturing fine-grained semantic nuances essential for accurate numerical\nreasoning. Experiments on three public benchmark datasets demonstrate the\nsuperior performance of KAAE across various attribute value distributions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper was decided to be withdrawn due to failure to resolve\n  collaborative disputes within the research team or authorship issues. We are\n  actively communicating to reach an agreement and avoid a recurrence of\n  similar issues",
    "pdf_url": "http://arxiv.org/pdf/2411.12950v2",
    "published_date": "2024-11-20 00:47:03 UTC",
    "updated_date": "2024-11-23 05:43:04 UTC"
  },
  {
    "arxiv_id": "2411.12943v1",
    "title": "Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity",
    "authors": [
      "Wassim El Ahmar",
      "Dhanvin Kolhatkar",
      "Farzan Nowruzi",
      "Robert Laganiere"
    ],
    "abstract": "Multiple Object Tracking (MOT) in thermal imaging presents unique challenges\ndue to the lack of visual features and the complexity of motion patterns. This\npaper introduces an innovative approach to improve MOT in the thermal domain by\ndeveloping a novel box association method that utilizes both thermal object\nidentity and motion similarity. Our method merges thermal feature sparsity and\ndynamic object tracking, enabling more accurate and robust MOT performance.\nAdditionally, we present a new dataset comprised of a large-scale collection of\nthermal and RGB images captured in diverse urban environments, serving as both\na benchmark for our method and a new resource for thermal imaging. We conduct\nextensive experiments to demonstrate the superiority of our approach over\nexisting methods, showing significant improvements in tracking accuracy and\nrobustness under various conditions. Our findings suggest that incorporating\nthermal identity with motion data enhances MOT performance. The newly collected\ndataset and source code is available at https://github.com/wassimea/thermalMOT",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Workshop on Towards a Complete Analysis of People, part of the\n  European Conference on Computer Vision (ECCV) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.12943v1",
    "published_date": "2024-11-20 00:27:01 UTC",
    "updated_date": "2024-11-20 00:27:01 UTC"
  }
]
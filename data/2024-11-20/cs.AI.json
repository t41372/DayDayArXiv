{
  "date": "2024-11-20",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-11-20 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型（如 LLM 和 VLM）的优化、应用和评估，涵盖 IoT、医疗、机器人导航、图像处理等领域，其中 NeurIPS 相关文章和知名机构如 IBM-UIUC 的合作论文（如“Transforming the Hybrid Cloud for Emerging AI Workloads”）特别引人注目，强调 AI 在实际部署中的高效性和鲁棒性，同时一些创新性工作如“AI-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test”展示了 LLM 在人类行为模拟中的潜力。\n\n以下是今天关键论文的简要概述，我会优先讨论重要、话题度高的文章（如涉及 LLM 应用和 AI 安全），并将相关论文归类讨论。对于次要论文，我会快速掠过，只突出核心贡献。\n\n### AI 和 LLM 相关创新\n这些论文探讨了 LLM 在各种领域的应用和优化，尤以 NeurIPS 论文为代表，强调模型的鲁棒性和实际部署。\n- **AI-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test (AI 驱动代理：使用高宜人性提示提高图灵测试中被误认为是人类的可能性)**  \n  主要贡献：提出使用 Big Five 个性模型优化 LLM 代理，使其在图灵测试中更像人类，实验显示高宜人性代理的混淆率超过 60%。这揭示了 LLM 在人类交互中的潜力，并强调人格工程在 AI 协作中的重要性。\n- **Federated Continual Learning for Edge-AI: A Comprehensive Survey (联邦持续学习在 Edge-AI 中的全面调查)**  \n  主要贡献：首次全面调查联邦持续学习框架，分类为联邦类持续学习等，讨论挑战和应用，突出其在隐私保护和动态环境中的优势。\n- **Exploring Large Language Models for Climate Forecasting (探索大型语言模型在气候预测中的应用)**  \n  主要贡献：评估 GPT-4 在短期和长期气候预测中的性能，发现其在缺乏趋势信号时倾向于使用历史平均值，提供 LLM 在气候决策中的洞见。\n- **Hymba: A Hybrid-head Architecture for Small Language Models (Hymba：小语言模型的混合头架构)**  \n  主要贡献：引入混合头架构结合 Transformer 和状态空间模型，提升小模型效率，Hymba-1.5B 模型在缓存大小和吞吐量上优于 Llama-3.2-3B。\n- 其他如“Fact-Level Confidence Calibration and Self-Correction (事实级置信校准和自校正)”快速提一下：提出事实级 LLM 校准框架，提升模型在长文本生成中的准确性，避免幻觉问题。\n\n### 机器人和计算机视觉进展\n这些论文聚焦机器人导航和视觉任务，相关 NeurIPS 和 ICLR 工作显示了实际应用潜力。\n- **Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios (复杂视觉推理场景的迭代和并行推理学习)**  \n  主要贡献：开发 IPRM 机制，支持迭代和并行计算，提升视觉问答性能，在 AGQA 等基准上超越现有方法。\n- **Bimanual Dexterity for Complex Tasks (双臂灵巧度用于复杂任务)**  \n  主要贡献：提出 Bidex 系统，使用运动捕捉手套实现高效双臂机器人遥操作，显著改善复杂任务数据质量。\n- **BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games (BALROG：基于游戏的 LLM 和 VLM 代理推理基准)**  \n  主要贡献：引入新基准评估 LLM 在游戏中的推理能力，揭示模型在动态环境中的局限性。\n- 快速掠过如“Metacognition for Unknown Situations and Environments (MUSE) (未知情境的元认知框架)”，贡献是整合元认知提升代理适应性。\n\n### 医疗和生物应用\n医疗相关论文强调 AI 在药物发现和图像处理中的作用，部分工作有实际数据集贡献。\n- **S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning (S$^2$ALM：序列-结构预训练的大型语言模型用于抗体表示学习)**  \n  主要贡献：提出统一模型融合序列和结构信息，提升抗体预测准确性，在药物设计任务中实现 SOTA。\n- **DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback (DrugGen：使用 LLM 和强化学习反馈推进药物发现)**  \n  主要贡献：开发 LLM 框架生成高亲和力分子，结合强化学习优化药物设计，在多目标任务中表现优异。\n- 快速掠过如“Entropy Bootstrapping for Weakly Supervised Nuclei Detection (弱监督核检测的熵引导)”，贡献是使用弱标签提升医学图像分割效率。\n\n其他论文如量子计算（“SimPhony”）、气候模型和图像生成等，虽然多样，但非核心话题，我仅快速提及：它们探讨了 AI 在边缘计算和多模态处理中的扩展，但细节较少创新点。\n\n总之，今天的论文突显 AI 模型在实际应用中的潜力，但也暴露了鲁棒性和泛化挑战。感兴趣的读者可关注 LLM 优化和机器人领域的发展！",
  "papers": [
    {
      "arxiv_id": "2411.17722v1",
      "title": "When IoT Meet LLMs: Applications and Challenges",
      "title_zh": "当 IoT 遇见 LLMs：应用与挑战",
      "authors": [
        "Ibrahim Kok",
        "Orhan Demirci",
        "Suat Ozdemir"
      ],
      "abstract": "Recent advances in Large Language Models (LLMs) have positively and\nefficiently transformed workflows in many domains. One such domain with\nsignificant potential for LLM integration is the Internet of Things (IoT),\nwhere this integration brings new opportunities for improved decision making\nand system interaction. In this paper, we explore the various roles of LLMs in\nIoT, with a focus on their reasoning capabilities. We show how LLM-IoT\nintegration can facilitate advanced decision making and contextual\nunderstanding in a variety of IoT scenarios. Furthermore, we explore the\nintegration of LLMs with edge, fog, and cloud computing paradigms, and show how\nthis synergy can optimize resource utilization, enhance real-time processing,\nand provide scalable solutions for complex IoT applications. To the best of our\nknowledge, this is the first comprehensive study covering IoT-LLM integration\nbetween edge, fog, and cloud systems. Additionally, we propose a novel system\nmodel for industrial IoT applications that leverages LLM-based collective\nintelligence to enable predictive maintenance and condition monitoring.\nFinally, we highlight key challenges and open issues that provide insights for\nfuture research in the field of LLM-IoT integration.",
      "tldr_zh": "本论文探讨了Large Language Models (LLMs)与Internet of Things (IoT)的整合及其应用潜力，重点强调LLMs的推理能力如何提升IoT场景中的决策和上下文理解。该研究首次全面分析LLMs与edge, fog, and cloud computing的协同作用，能够优化资源利用、增强实时处理，并为复杂IoT应用提供可扩展解决方案。此外，论文提出一个新颖的系统模型，利用LLMs的集体智能支持工业IoT的预测性维护和条件监测，并指出了关键挑战和开放问题，为未来LLM-IoT整合研究提供方向。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData), 10 pages, 2 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2411.17722v1",
      "published_date": "2024-11-20 23:44:51 UTC",
      "updated_date": "2024-11-20 23:44:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:20:19.764787"
    },
    {
      "arxiv_id": "2411.13754v1",
      "title": "Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Shantanu Jaiswal",
        "Debaditya Roy",
        "Basura Fernando",
        "Cheston Tan"
      ],
      "abstract": "Complex visual reasoning and question answering (VQA) is a challenging task\nthat requires compositional multi-step processing and higher-level reasoning\ncapabilities beyond the immediate recognition and localization of objects and\nevents. Here, we introduce a fully neural Iterative and Parallel Reasoning\nMechanism (IPRM) that combines two distinct forms of computation -- iterative\nand parallel -- to better address complex VQA scenarios. Specifically, IPRM's\n\"iterative\" computation facilitates compositional step-by-step reasoning for\nscenarios wherein individual operations need to be computed, stored, and\nrecalled dynamically (e.g. when computing the query \"determine the color of pen\nto the left of the child in red t-shirt sitting at the white table\").\nMeanwhile, its \"parallel\" computation allows for the simultaneous exploration\nof different reasoning paths and benefits more robust and efficient execution\nof operations that are mutually independent (e.g. when counting individual\ncolors for the query: \"determine the maximum occurring color amongst all\nt-shirts\"). We design IPRM as a lightweight and fully-differentiable neural\nmodule that can be conveniently applied to both transformer and non-transformer\nvision-language backbones. It notably outperforms prior task-specific methods\nand transformer-based attention modules across various image and video VQA\nbenchmarks testing distinct complex reasoning capabilities such as\ncompositional spatiotemporal reasoning (AGQA), situational reasoning (STAR),\nmulti-hop reasoning generalization (CLEVR-Humans) and causal event linking\n(CLEVRER-Humans). Further, IPRM's internal computations can be visualized\nacross reasoning steps, aiding interpretability and diagnosis of its errors.",
      "tldr_zh": "这篇论文提出了一种名为 Iterative and Parallel Reasoning Mechanism (IPRM) 的神经机制，用于处理复杂视觉推理和问答 (VQA) 任务，该机制结合迭代计算（支持逐步组合推理）和并行计算（同时探索独立路径），以提升多步处理和高级推理能力。IPRM 设计为轻量级、全微分的模块，可灵活应用于 transformer 和非-transformer 视觉语言骨干。实验结果显示，它在多个基准上（如 AGQA 的组合时空推理、STAR 的情境推理、CLEVR-Humans 的多跳推理泛化及 CLEVRER-Humans 的因果事件链接）优于现有方法，提高了任务性能。最后，IPRM 的内部计算可可视化，有助于提升模型的可解释性和错误诊断。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024 camera ready; source code to be released at:\n  https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism",
      "pdf_url": "http://arxiv.org/pdf/2411.13754v1",
      "published_date": "2024-11-20 23:39:54 UTC",
      "updated_date": "2024-11-20 23:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:20:31.538082"
    },
    {
      "arxiv_id": "2411.13749v1",
      "title": "AI-Driven Agents with Prompts Designed for High Agreeableness Increase the Likelihood of Being Mistaken for a Human in the Turing Test",
      "title_zh": "翻译失败",
      "authors": [
        "U. León-Domínguez",
        "E. D. Flores-Flores",
        "A. J. García-Jasso",
        "M. K. Gómez-Cuellar",
        "D. Torres-Sánchez",
        "A. Basora-Marimon"
      ],
      "abstract": "Large Language Models based on transformer algorithms have revolutionized\nArtificial Intelligence by enabling verbal interaction with machines akin to\nhuman conversation. These AI agents have surpassed the Turing Test, achieving\nconfusion rates up to 50%. However, challenges persist, especially with the\nadvent of robots and the need to humanize machines for improved Human-AI\ncollaboration. In this experiment, three GPT agents with varying levels of\nagreeableness (disagreeable, neutral, agreeable) based on the Big Five\nInventory were tested in a Turing Test. All exceeded a 50% confusion rate, with\nthe highly agreeable AI agent surpassing 60%. This agent was also recognized as\nexhibiting the most human-like traits. Various explanations in the literature\naddress why these GPT agents were perceived as human, including psychological\nframeworks for understanding anthropomorphism. These findings highlight the\nimportance of personality engineering as an emerging discipline in artificial\nintelligence, calling for collaboration with psychology to develop ergonomic\npsychological models that enhance system adaptability in collaborative\nactivities.",
      "tldr_zh": "该研究探讨了基于 transformer 算法的大型语言模型（LLMs）在图灵测试（Turing Test）中的表现，特别关注通过 Big Five Inventory 设计不同宜人性（agreeableness）水平的 GPT 代理（disagreeable、中性、agreeable）。实验结果显示，所有代理均超过了 50% 的混淆率，其中高度宜人的代理达到了 60% 以上，并被视为最具人性化特征。论文强调人格工程（personality engineering）作为人工智能的新兴领域，并呼吁与心理学合作，开发更适配人类协作的心理模型，以提升 AI 的适应性和可协作性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "25 pages, 2 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.13749v1",
      "published_date": "2024-11-20 23:12:49 UTC",
      "updated_date": "2024-11-20 23:12:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:20:42.507286"
    },
    {
      "arxiv_id": "2411.13740v1",
      "title": "Federated Continual Learning for Edge-AI: A Comprehensive Survey",
      "title_zh": "联邦持续学习用于边缘AI：全面综述",
      "authors": [
        "Zi Wang",
        "Fei Wu",
        "Feng Yu",
        "Yurui Zhou",
        "Jia Hu",
        "Geyong Min"
      ],
      "abstract": "Edge-AI, the convergence of edge computing and artificial intelligence (AI),\nhas become a promising paradigm that enables the deployment of advanced AI\nmodels at the network edge, close to users. In Edge-AI, federated continual\nlearning (FCL) has emerged as an imperative framework, which fuses knowledge\nfrom different clients while preserving data privacy and retaining knowledge\nfrom previous tasks as it learns new ones. By so doing, FCL aims to ensure\nstable and reliable performance of learning models in dynamic and distributed\nenvironments. In this survey, we thoroughly review the state-of-the-art\nresearch and present the first comprehensive survey of FCL for Edge-AI. We\ncategorize FCL methods based on three task characteristics: federated class\ncontinual learning, federated domain continual learning, and federated task\ncontinual learning. For each category, an in-depth investigation and review of\nthe representative methods are provided, covering background, challenges,\nproblem formalisation, solutions, and limitations. Besides, existing real-world\napplications empowered by FCL are reviewed, indicating the current progress and\npotential of FCL in diverse application domains. Furthermore, we discuss and\nhighlight several prospective research directions of FCL such as\nalgorithm-hardware co-design for FCL and FCL with foundation models, which\ncould provide insights into the future development and practical deployment of\nFCL in the era of Edge-AI.",
      "tldr_zh": "本调查论文对Federated Continual Learning (FCL) 在 Edge-AI 中的应用进行了首次全面回顾，强调 FCL 通过融合不同客户端知识、保护数据隐私并保留先前任务知识，来实现模型在动态分布式环境中的稳定性能。论文将 FCL 方法分类为 federated class continual learning、federated domain continual learning 和 federated task continual learning，并针对每个类别深入探讨背景、挑战、问题形式化、解决方案和局限性。调查还回顾了 FCL 在真实世界应用的进展及其潜力，并展望未来研究方向，如算法-硬件协同设计和 FCL 与 foundation models 的结合，以推动 Edge-AI 的实际部署。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13740v1",
      "published_date": "2024-11-20 22:49:28 UTC",
      "updated_date": "2024-11-20 22:49:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:20:55.752806"
    },
    {
      "arxiv_id": "2411.13724v1",
      "title": "Exploring Large Language Models for Climate Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Wang",
        "Hassan A. Karimi"
      ],
      "abstract": "With the increasing impacts of climate change, there is a growing demand for\naccessible tools that can provide reliable future climate information to\nsupport planning, finance, and other decision-making applications. Large\nlanguage models (LLMs), such as GPT-4, present a promising approach to bridging\nthe gap between complex climate data and the general public, offering a way for\nnon-specialist users to obtain essential climate insights through natural\nlanguage interaction. However, an essential challenge remains under-explored:\nevaluating the ability of LLMs to provide accurate and reliable future climate\npredictions, which is crucial for applications that rely on anticipating\nclimate trends. In this study, we investigate the capability of GPT-4 in\npredicting rainfall at short-term (15-day) and long-term (12-month) scales. We\ndesigned a series of experiments to assess GPT's performance under different\nconditions, including scenarios with and without expert data inputs. Our\nresults indicate that GPT, when operating independently, tends to generate\nconservative forecasts, often reverting to historical averages in the absence\nof clear trend signals. This study highlights both the potential and challenges\nof applying LLMs for future climate predictions, providing insights into their\nintegration with climate-related applications and suggesting directions for\nenhancing their predictive capabilities in the field.",
      "tldr_zh": "该研究探讨了大型语言模型（LLMs）如GPT-4在气候预测中的潜力，旨在通过自然语言交互为非专业人士提供可靠的未来气候信息。研究者设计了一系列实验，评估GPT-4在短期（15天）和长期（12个月）雨量预测上的表现，包括有无专家数据输入的场景。结果显示，GPT-4在独立操作时倾向于生成保守预测，常依赖历史平均值，而非捕捉趋势信号。该研究突出了LLMs在气候应用的机遇与挑战，并为提升其预测能力提供了整合建议。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13724v1",
      "published_date": "2024-11-20 21:58:19 UTC",
      "updated_date": "2024-11-20 21:58:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:21:06.891511"
    },
    {
      "arxiv_id": "2411.15218v1",
      "title": "Suspected Undeclared Use of Artificial Intelligence in the Academic Literature: An Analysis of the Academ-AI Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Alex Glynn"
      ],
      "abstract": "Since generative artificial intelligence (AI) tools such as OpenAI's ChatGPT\nbecame widely available, researchers have used them in the writing process. The\nconsensus of the academic publishing community is that such usage must be\ndeclared in the published article. Academ-AI documents examples of suspected\nundeclared AI usage in the academic literature, discernible primarily due to\nthe appearance in research papers of idiosyncratic verbiage characteristic of\nlarge language model (LLM)-based chatbots. This analysis of the first 500\nexamples collected reveals that the problem is widespread, penetrating the\njournals and conference proceedings of highly respected publishers. Undeclared\nAI seems to appear in journals with higher citation metrics and higher article\nprocessing charges (APCs), precisely those outlets that should theoretically\nhave the resources and expertise to avoid such oversights. An extremely small\nminority of cases are corrected post publication, and the corrections are often\ninsufficient to rectify the problem. The 500 examples analyzed here likely\nrepresent a small fraction of the undeclared AI present in the academic\nliterature, much of which may be undetectable. Publishers must enforce their\npolicies against undeclared AI usage in cases that are detectable; this is the\nbest defense currently available to the academic publishing community against\nthe proliferation of undisclosed AI.",
      "tldr_zh": "这篇论文分析了 Academ-AI 数据集中的 500 个可疑案例，揭示了学术文献中未声明使用生成式 Artificial Intelligence (AI) 工具（如 ChatGPT）的广泛问题，主要通过识别 Large Language Model (LLM) 聊天机器人特有的语言模式来进行检测。研究发现，这种未声明 AI 使用在高引用指标和高文章处理费 (APCs) 的期刊和会议论文中更为常见，且仅有极少数案例在发布后得到纠正，这些纠正往往不足以解决问题。作者强调，这可能只是冰山一角，并呼吁学术出版商强制执行相关政策，以防范未检测到的 AI 使用扩散。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.DL",
      "comment": "24 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.15218v1",
      "published_date": "2024-11-20 21:29:36 UTC",
      "updated_date": "2024-11-20 21:29:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:21:19.597241"
    },
    {
      "arxiv_id": "2411.13715v1",
      "title": "SimPhony: A Device-Circuit-Architecture Cross-Layer Modeling and Simulation Framework for Heterogeneous Electronic-Photonic AI System",
      "title_zh": "翻译失败",
      "authors": [
        "Ziang Yin",
        "Meng Zhang",
        "Amir Begovic",
        "Rena Huang",
        "Jeff Zhang",
        "Jiaqi Gu"
      ],
      "abstract": "Electronic-photonic integrated circuits (EPICs) offer transformative\npotential for next-generation high-performance AI but require interdisciplinary\nadvances across devices, circuits, architecture, and design automation. The\ncomplexity of hybrid systems makes it challenging even for domain experts to\nunderstand distinct behaviors and interactions across design stack. The lack of\na flexible, accurate, fast, and easy-to-use EPIC AI system simulation framework\nsignificantly limits the exploration of hardware innovations and system\nevaluations on common benchmarks. To address this gap, we propose SimPhony, a\ncross-layer modeling and simulation framework for heterogeneous\nelectronic-photonic AI systems. SimPhony offers a platform that enables (1)\ngeneric, extensible hardware topology representation that supports\nheterogeneous multi-core architectures with diverse photonic tensor core\ndesigns; (2) optics-specific dataflow modeling with unique multi-dimensional\nparallelism and reuse beyond spatial/temporal dimensions; (3) data-aware energy\nmodeling with realistic device responses, layout-aware area estimation, link\nbudget analysis, and bandwidth-adaptive memory modeling; and (4) seamless\nintegration with model training framework for hardware/software co-simulation.\nBy providing a unified, versatile, and high-fidelity simulation platform,\nSimPhony enables researchers to innovate and evaluate EPIC AI hardware across\nmultiple domains, facilitating the next leap in emerging AI hardware. We\nopen-source our codes at https://github.com/ScopeX-ASU/SimPhony",
      "tldr_zh": "该论文提出SimPhony框架，这是一个跨层建模和模拟工具，针对异构电子-光子AI系统（EPICs），旨在解决设备、电路和架构间的复杂互动问题。SimPhony提供通用硬件拓扑表示、光学特定数据流建模（包括多维并行）、数据感知能量建模（如设备响应和链路预算分析），并实现与模型训练框架的无缝集成。总体上，该框架促进了EPIC AI硬件的创新和评估，推动了高性能AI系统的快速发展，并已开源代码。",
      "categories": [
        "physics.optics",
        "cs.AI",
        "cs.AR",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "physics.optics",
      "comment": "7-page",
      "pdf_url": "http://arxiv.org/pdf/2411.13715v1",
      "published_date": "2024-11-20 21:21:54 UTC",
      "updated_date": "2024-11-20 21:21:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:21:32.609204"
    },
    {
      "arxiv_id": "2411.13677v1",
      "title": "Bimanual Dexterity for Complex Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Kenneth Shaw",
        "Yulong Li",
        "Jiahui Yang",
        "Mohan Kumar Srirama",
        "Ray Liu",
        "Haoyu Xiong",
        "Russell Mendonca",
        "Deepak Pathak"
      ],
      "abstract": "To train generalist robot policies, machine learning methods often require a\nsubstantial amount of expert human teleoperation data. An ideal robot for\nhumans collecting data is one that closely mimics them: bimanual arms and\ndexterous hands. However, creating such a bimanual teleoperation system with\nover 50 DoF is a significant challenge. To address this, we introduce Bidex, an\nextremely dexterous, low-cost, low-latency and portable bimanual dexterous\nteleoperation system which relies on motion capture gloves and teacher arms. We\ncompare Bidex to a Vision Pro teleoperation system and a SteamVR system and\nfind Bidex to produce better quality data for more complex tasks at a faster\nrate. Additionally, we show Bidex operating a mobile bimanual robot for in the\nwild tasks. The robot hands (5k USD) and teleoperation system (7k USD) is\nreadily reproducible and can be used on many robot arms including two xArms\n(16k USD). Website at https://bidex-teleop.github.io/",
      "tldr_zh": "这篇论文介绍了 Bidex，一种低成本、低延迟且便携的双臂灵巧遥操作系统，用于高效收集高质量机器人训练数据，以训练通用机器人策略。Bidex 依赖于 motion capture gloves 和 teacher arms 来控制超过50 DoF 的系统，并与 Vision Pro 和 SteamVR 系统相比，展示了在复杂任务上产生更好数据质量和更快操作速度的优势。实验证明，Bidex 可应用于移动双臂机器人执行野外任务。系统总成本低廉（机器人手5k USD、遥操作系统7k USD），易于复制并兼容多种机器人臂，如 xArms。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "In CoRL 2024. Website at https://bidex-teleop.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.13677v1",
      "published_date": "2024-11-20 19:53:35 UTC",
      "updated_date": "2024-11-20 19:53:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:21:44.336147"
    },
    {
      "arxiv_id": "2411.13676v1",
      "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Xin Dong",
        "Yonggan Fu",
        "Shizhe Diao",
        "Wonmin Byeon",
        "Zijia Chen",
        "Ameya Sunil Mahabaleshwarkar",
        "Shih-Yang Liu",
        "Matthijs Van Keirsbilck",
        "Min-Hung Chen",
        "Yoshi Suhara",
        "Yingyan Lin",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "abstract": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
      "tldr_zh": "我们提出了 Hymba，一系列小语言模型，采用混合头并行架构，将 Transformer 注意机制与状态空间模型 (SSMs) 相结合，实现高效的上下文处理，其中注意头提供高分辨率回忆，而 SSM 头负责上下文总结。模型进一步通过引入可学习的元标记 (learnable meta tokens) 来存储关键信息、减轻注意机制的负担，并优化了跨层键值 (KV) 共享和部分滑动窗口注意，以减少缓存大小。实验结果显示，Hymba-1.5B-Base 模型在相同设置下超越所有子-2B 公共模型，并在性能上优于 Llama-3.2-3B，平均准确率提高 1.32%、缓存大小减少 11.67 倍、吞吐量提升 3.49 倍。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "20 pages, models are available on huggingface",
      "pdf_url": "http://arxiv.org/pdf/2411.13676v1",
      "published_date": "2024-11-20 19:51:25 UTC",
      "updated_date": "2024-11-20 19:51:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:21:57.285837"
    },
    {
      "arxiv_id": "2411.17720v2",
      "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration on Resource-Constrained Edge Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammadali Shakerdargah",
        "Shan Lu",
        "Chao Gao",
        "Di Niu"
      ],
      "abstract": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
      "tldr_zh": "该论文提出MAS-Attention，一种内存感知流处理方案，用于在资源受限的边缘设备上加速Attention机制，以应对其二次方复杂度带来的计算和内存挑战。该方法通过并行利用异构计算单元（如Vector Processing Units和Matrix Processing Units），采用多层级切片调度和主动缓存覆盖策略，将注意力工作负载分为向量和矩阵流进行处理，从而优化I/O开销和缓存利用。实验结果显示，与现有FLAT方法相比，该方案在模拟框架中实现高达2.75x加速和54%能量消耗减少，在真实边缘神经处理单元上达到1.76x加速，同时保持模型输出准确性不变。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.PF",
        "C.1.4; I.2.7; I.5.1"
      ],
      "primary_category": "cs.DC",
      "comment": "Accepted to MLSys 2025,",
      "pdf_url": "http://arxiv.org/pdf/2411.17720v2",
      "published_date": "2024-11-20 19:44:26 UTC",
      "updated_date": "2025-05-16 00:56:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:22:07.276573"
    },
    {
      "arxiv_id": "2412.04484v1",
      "title": "Epinet for Content Cold Start",
      "title_zh": "翻译失败",
      "authors": [
        "Hong Jun Jeon",
        "Songbin Liu",
        "Yuantong Li",
        "Jie Lyu",
        "Hunter Song",
        "Ji Liu",
        "Peng Wu",
        "Zheqing Zhu"
      ],
      "abstract": "The exploding popularity of online content and its user base poses an\nevermore challenging matching problem for modern recommendation systems. Unlike\nother frontiers of machine learning such as natural language, recommendation\nsystems are responsible for collecting their own data. Simply exploiting\ncurrent knowledge can lead to pernicious feedback loops but naive exploration\ncan detract from user experience and lead to reduced engagement. This\nexploration-exploitation trade-off is exemplified in the classic multi-armed\nbandit problem for which algorithms such as upper confidence bounds (UCB) and\nThompson sampling (TS) demonstrate effective performance. However, there have\nbeen many challenges to scaling these approaches to settings which do not\nexhibit a conjugate prior structure. Recent scalable approaches to uncertainty\nquantification via epinets have enabled efficient approximations of Thompson\nsampling even when the learning model is a complex neural network. In this\npaper, we demonstrate the first application of epinets to an online\nrecommendation system. Our experiments demonstrate improvements in both user\ntraffic and engagement efficiency on the Facebook Reels online video platform.",
      "tldr_zh": "这篇论文针对内容冷启动(Content Cold Start)问题，探讨了在线推荐系统的探索-利用权衡(exploration-exploitation trade-off)，以避免反馈循环和降低用户参与度。作者首次将 epinets 应用于在线推荐系统，通过高效近似 Thompson sampling (TS) 和不确定性量化，即使在复杂神经网络模型下也能实现可扩展。实验结果显示，在 Facebook Reels 平台上，使用 epinets 显著提高了用户流量和参与效率。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04484v1",
      "published_date": "2024-11-20 19:43:27 UTC",
      "updated_date": "2024-11-20 19:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:22:19.179593"
    },
    {
      "arxiv_id": "2411.13653v1",
      "title": "No Free Delivery Service: Epistemic limits of passive data collection in complex social systems",
      "title_zh": "翻译失败",
      "authors": [
        "Maximilian Nickel"
      ],
      "abstract": "Rapid model validation via the train-test paradigm has been a key driver for\nthe breathtaking progress in machine learning and AI. However, modern AI\nsystems often depend on a combination of tasks and data collection practices\nthat violate all assumptions ensuring test validity. Yet, without rigorous\nmodel validation we cannot ensure the intended outcomes of deployed AI systems,\nincluding positive social impact, nor continue to advance AI research in a\nscientifically sound way. In this paper, I will show that for widely considered\ninference settings in complex social systems the train-test paradigm does not\nonly lack a justification but is indeed invalid for any risk estimator,\nincluding counterfactual and causal estimators, with high probability. These\nformal impossibility results highlight a fundamental epistemic issue, i.e.,\nthat for key tasks in modern AI we cannot know whether models are valid under\ncurrent data collection practices. Importantly, this includes variants of both\nrecommender systems and reasoning via large language models, and neither\nna\\\"ive scaling nor limited benchmarks are suited to address this issue. I am\nillustrating these results via the widely used MovieLens benchmark and conclude\nby discussing the implications of these results for AI in social systems,\nincluding possible remedies such as participatory data curation and open\nscience.",
      "tldr_zh": "该论文揭示了在复杂社会系统中，被动数据收集的认知限制，导致 train-test paradigm 无法有效验证AI模型，因为它违反了确保测试有效性的假设。作者通过形式证明展示了，这种范式对风险估计器（包括 counterfactual and causal estimators）在高概率下无效，突显了AI在关键任务中无法知晓模型有效性的根本问题。论文以 recommender systems 和 large language models 的推理为例，并使用 MovieLens benchmark 进行说明，提出参与式数据整理（participatory data curation）和 open science 等补救措施，以推进AI研究的科学性和社会影响。",
      "categories": [
        "cs.AI",
        "stat.ML",
        "62A01",
        "G.3; I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "To appear in NeurIPS'24",
      "pdf_url": "http://arxiv.org/pdf/2411.13653v1",
      "published_date": "2024-11-20 19:01:03 UTC",
      "updated_date": "2024-11-20 19:01:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:22:32.225765"
    },
    {
      "arxiv_id": "2411.13547v1",
      "title": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs",
      "title_zh": "SpecTool: 用于表征工具使用LLMs错误模式的基准",
      "authors": [
        "Shirley Kokane",
        "Ming Zhu",
        "Tulika Awalgaonkar",
        "Jianguo Zhang",
        "Thai Hoang",
        "Akshara Prabhakar",
        "Zuxin Liu",
        "Tian Lan",
        "Liangwei Yang",
        "Juntao Tan",
        "Rithesh Murthy",
        "Weiran Yao",
        "Zhiwei Liu",
        "Juan Carlos Niebles",
        "Huan Wang",
        "Shelby Heinecke",
        "Caiming Xiong",
        "Silivo Savarese"
      ],
      "abstract": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies.",
      "tldr_zh": "该研究针对大型语言模型（LLMs）在工具使用任务中的错误评估问题，引入了SpecTool基准，用于识别和分析七种新特征化的错误模式。现有基准仅提供成功率而忽略失败案例，SpecTool通过多样化查询数据集来系统地测试这些错误。实验结果显示，即使最先进的LLMs也表现出这些错误模式，为研究人员提供分析洞见，以指导错误缓解策略的优化。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13547v1",
      "published_date": "2024-11-20 18:56:22 UTC",
      "updated_date": "2024-11-20 18:56:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:22:42.872976"
    },
    {
      "arxiv_id": "2411.13543v2",
      "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Paglieri",
        "Bartłomiej Cupiał",
        "Samuel Coward",
        "Ulyana Piterbarg",
        "Maciej Wolczyk",
        "Akbir Khan",
        "Eduardo Pignatelli",
        "Łukasz Kuciński",
        "Lerrel Pinto",
        "Rob Fergus",
        "Jakob Nicolaus Foerster",
        "Jack Parker-Holder",
        "Tim Rocktäschel"
      ],
      "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities, however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas several models perform worse when visual representations of the environments\nare provided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community. Code and\nLeaderboard at balrogai.com.",
      "tldr_zh": "这篇论文引入了 BALROG 基准，用于评估 LLMs 和 VLMs 的代理能力，通过一系列挑战性的游戏环境测试它们的推理性能，包括从简单强化学习任务到极难的如 NetHack Learning Environment。研究设计了细粒度的指标，对多个开源和闭源模型进行全面评估，发现这些模型在较易游戏中取得部分成功，但在复杂任务如高级空间推理、长期规划和视觉决策上表现严重不足。值得注意的是，提供视觉表示反而导致模型性能下降。作者开源了 BALROG 及其代码和排行榜，以推动代理社区的未来研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.13543v2",
      "published_date": "2024-11-20 18:54:32 UTC",
      "updated_date": "2025-04-01 14:45:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:22:55.877539"
    },
    {
      "arxiv_id": "2411.13537v1",
      "title": "Metacognition for Unknown Situations and Environments (MUSE)",
      "title_zh": "针对未知情境和环境的元认知（MUSE）",
      "authors": [
        "Rodolfo Valiente",
        "Praveen K. Pilly"
      ],
      "abstract": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data.",
      "tldr_zh": "这篇论文探讨了元认知（Metacognition）在帮助自主代理适应未知情况和环境中的关键作用，假设它是当前系统缺失的重要元素。论文提出MUSE框架，通过整合自我意识和自我调节（如能力意识和策略选择），并基于世界建模或大型语言模型（LLMs）实现元认知循环，使代理能够持续评估自身能力和迭代优化策略。与Dreamer-v3-based强化学习和纯提示-based LLM方法相比，MUSE代理在处理新颖的分布外任务时表现出显著改进，提升了适应性和有效性。该工作强调了借鉴认知和神经系统的方法，以克服对大量训练数据的依赖。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13537v1",
      "published_date": "2024-11-20 18:41:03 UTC",
      "updated_date": "2024-11-20 18:41:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:23:07.741922"
    },
    {
      "arxiv_id": "2411.13536v1",
      "title": "Identity Preserving 3D Head Stylization with Multiview Score Distillation",
      "title_zh": "基于多视图分数蒸馏的身份保留 3D 头部风格化",
      "authors": [
        "Bahri Batuhan Bilecen",
        "Ahmet Berke Gokmen",
        "Furkan Guzelant",
        "Aysegul Dundar"
      ],
      "abstract": "3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.",
      "tldr_zh": "本论文提出了一种名为 Identity Preserving 3D Head Stylization with Multiview Score Distillation 的框架，用于将现实面部特征转化为艺术风格化表示，同时解决现有方法在提供近正面视图时难以保留原主体独特身份的问题。框架基于 PanoHead 模型从 360 度视角合成图像，并通过 negative log-likelihood distillation (LD) 结合 multi-view grid score、mirror gradients 和 score rank weighing 技术，集成到 3D GAN 架构中以提升风格化质量和身份保留。实验结果显示，该方法在定性和定量上实现了显著改进，并为扩散模型与 GAN 之间的有效蒸馏过程提供了关键见解，特别是针对身份保留的挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "https://three-bee.github.io/head_stylization",
      "pdf_url": "http://arxiv.org/pdf/2411.13536v1",
      "published_date": "2024-11-20 18:37:58 UTC",
      "updated_date": "2024-11-20 18:37:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:23:20.190578"
    },
    {
      "arxiv_id": "2411.13528v2",
      "title": "Entropy Bootstrapping for Weakly Supervised Nuclei Detection",
      "title_zh": "翻译失败",
      "authors": [
        "James Willoughby",
        "Irina Voiculescu"
      ],
      "abstract": "Microscopy structure segmentation, such as detecting cells or nuclei,\ngenerally requires a human to draw a ground truth contour around each instance.\nWeakly supervised approaches (e.g. consisting of only single point labels) have\nthe potential to reduce this workload significantly. Our approach uses\nindividual point labels for an entropy estimation to approximate an underlying\ndistribution of cell pixels. We infer full cell masks from this distribution,\nand use Mask-RCNN to produce an instance segmentation output. We compare this\npoint--annotated approach with training on the full ground truth masks. We show\nthat our method achieves a comparatively good level of performance, despite a\n95% reduction in pixel labels.",
      "tldr_zh": "该论文提出了一种基于弱监督学习的Entropy Bootstrapping方法，用于微观结构分割，如细胞核检测，以减少人工标注工作量。该方法利用单个点标签进行熵估计，近似细胞像素的分布，并从中推断完整的细胞掩码，随后使用Mask-RCNN进行实例分割。与使用完整地面真实掩码的训练相比，该方法在减少95%像素标签的情况下，实现了相近的性能水平，为高效的弱监督分割提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 Pages",
      "pdf_url": "http://arxiv.org/pdf/2411.13528v2",
      "published_date": "2024-11-20 18:24:11 UTC",
      "updated_date": "2024-11-21 20:03:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:23:30.715872"
    },
    {
      "arxiv_id": "2411.17719v1",
      "title": "SlideSpawn: An Automatic Slides Generation System for Research Publications",
      "title_zh": "翻译失败",
      "authors": [
        "Keshav Kumar",
        "Ravindranath Chowdary"
      ],
      "abstract": "Research papers are well structured documents. They have text, figures,\nequations, tables etc., to covey their ideas and findings. They are divided\ninto sections like Introduction, Model, Experiments etc., which deal with\ndifferent aspects of research. Characteristics like these set research papers\napart from ordinary documents and allows us to significantly improve their\nsummarization. In this paper, we propose a novel system, SlideSpwan, that takes\nPDF of a research document as an input and generates a quality presentation\nproviding it's summary in a visual and concise fashion. The system first\nconverts the PDF of the paper to an XML document that has the structural\ninformation about various elements. Then a machine learning model, trained on\nPS5K dataset and Aminer 9.5K Insights dataset (that we introduce), is used to\npredict salience of each sentence in the paper. Sentences for slides are\nselected using ILP and clustered based on their similarity with each cluster\nbeing given a suitable title. Finally a slide is generated by placing any\ngraphical element referenced in the selected sentences next to them.\nExperiments on a test set of 650 pairs of papers and slides demonstrate that\nour system generates presentations with better quality.",
      "tldr_zh": "本研究提出SlideSpawn系统，一种自动生成研究论文演示文稿的工具，它从PDF输入中提取结构化元素（如文本、图表），并以视觉化方式提供简洁摘要。\n系统首先将PDF转换为XML文档，然后使用训练于PS5K和Aminer 9.5K Insights数据集的机器学习模型预测句子显著性，通过ILP（整数线性规划）选择句子并基于相似性聚类，最后将图形元素置于相关句子旁生成幻灯片。\n实验结果显示，在650对论文和幻灯片测试集上，SlideSpawn生成的演示文稿质量优于基线模型。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "H.3"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 4 figures, 2 tables, 5 equations, 41 references",
      "pdf_url": "http://arxiv.org/pdf/2411.17719v1",
      "published_date": "2024-11-20 18:16:16 UTC",
      "updated_date": "2024-11-20 18:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:23:43.846950"
    },
    {
      "arxiv_id": "2411.15217v1",
      "title": "LPLgrad: Optimizing Active Learning Through Gradient Norm Sample Selection and Auxiliary Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Shreen Gul",
        "Mohamed Elmahallawy",
        "Sanjay Madria",
        "Ardhendu Tripathy"
      ],
      "abstract": "Machine learning models are increasingly being utilized across various fields\nand tasks due to their outstanding performance and strong generalization\ncapabilities. Nonetheless, their success hinges on the availability of large\nvolumes of annotated data, the creation of which is often labor-intensive,\ntime-consuming, and expensive. Many active learning (AL) approaches have been\nproposed to address these challenges, but they often fail to fully leverage the\ninformation from the core phases of AL, such as training on the labeled set and\nquerying new unlabeled samples. To bridge this gap, we propose a novel AL\napproach, Loss Prediction Loss with Gradient Norm (LPLgrad), designed to\nquantify model uncertainty effectively and improve the accuracy of image\nclassification tasks. LPLgrad operates in two distinct phases: (i) {\\em\nTraining Phase} aims to predict the loss for input features by jointly training\na main model and an auxiliary model. Both models are trained on the labeled\ndata to maximize the efficiency of the learning process, an aspect often\noverlooked in previous AL methods. This dual-model approach enhances the\nability to extract complex input features and learn intrinsic patterns from the\ndata effectively; (ii) {\\em Querying Phase} that quantifies the uncertainty of\nthe main model to guide sample selection. This is achieved by calculating the\ngradient norm of the entropy values for samples in the unlabeled dataset.\nSamples with the highest gradient norms are prioritized for labeling and\nsubsequently added to the labeled set, improving the model's performance with\nminimal labeling effort. Extensive evaluations on real-world datasets\ndemonstrate that the LPLgrad approach outperforms state-of-the-art methods by\norder of magnitude in terms of accuracy on a small number of labeled images,\nyet achieving comparable training and querying times in multiple image\nclassification tasks.",
      "tldr_zh": "本研究针对机器学习模型对大量标注数据的依赖问题，提出了一种新型主动学习（Active Learning）方法LPLgrad，通过梯度范数（Gradient Norm）样本选择和辅助模型训练来优化学习过程。LPLgrad分为两个阶段：训练阶段联合训练主模型和辅助模型，以预测输入特征的损失，从而更高效地提取复杂特征和学习数据模式；查询阶段则通过计算未标注数据样本的熵值梯度范数，量化模型不确定性并优先选择高不确定性样本进行标注。实验结果显示，在多个图像分类任务上，LPLgrad在使用少量标注图像时，其准确率比现有最先进方法高出一个数量级，同时保持可比的训练和查询时间。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15217v1",
      "published_date": "2024-11-20 18:12:59 UTC",
      "updated_date": "2024-11-20 18:12:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:23:54.911851"
    },
    {
      "arxiv_id": "2411.13518v1",
      "title": "Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Chanseo Lee",
        "Sonu Kumar",
        "Kimon A. Vogt",
        "Sam Meraj",
        "Antonia Vogt"
      ],
      "abstract": "The increasing demand for multilingual capabilities in healthcare underscores\nthe need for AI models adept at processing diverse languages, particularly in\nclinical documentation and decision-making. Arabic, with its complex\nmorphology, syntax, and diglossia, poses unique challenges for natural language\nprocessing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a\nlanguage model tailored for Arabic clinical documentation, against JAIS, the\nleading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics\nmodified ourselves for the purposes of assessing model performances in a\ndifferent language. The study assessed the models' performance in summarizing\npatient-physician interactions, focusing on accuracy, comprehensiveness,\nclinical utility, and linguistic-cultural competence.\n  Results indicate that Sporo AraSum significantly outperforms JAIS in\nAI-centric quantitative metrics and all qualitative attributes measured in our\nmodified version of the PDQI-9. AraSum's architecture enables precise and\nculturally sensitive documentation, addressing the linguistic nuances of Arabic\nwhile mitigating risks of AI hallucinations. These findings suggest that Sporo\nAraSum is better suited to meet the demands of Arabic-speaking healthcare\nenvironments, offering a transformative solution for multilingual clinical\nworkflows. Future research should incorporate real-world data to further\nvalidate these findings and explore broader integration into healthcare\nsystems.",
      "tldr_zh": "该研究针对阿拉伯语在医疗NLP中的复杂性（如形态、句法和双重标准语），开发了Sporo AraSum模型，以提升临床文档和决策的多语言处理能力。通过使用合成数据集和修改后的PDQI-9指标，该模型与领先的JAIS模型进行比较，评估了在总结患者-医生互动方面的准确性、全面性、临床实用性和语言文化能力。结果显示，Sporo AraSum在定量和定性指标上显著优于JAIS，提供更精确且文化敏感的文档，同时减少AI幻觉。未来研究应使用真实数据进一步验证其在多语种医疗工作流中的应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: text overlap with arXiv:2411.06713",
      "pdf_url": "http://arxiv.org/pdf/2411.13518v1",
      "published_date": "2024-11-20 18:10:19 UTC",
      "updated_date": "2024-11-20 18:10:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:24:07.517912"
    },
    {
      "arxiv_id": "2412.04483v1",
      "title": "AI-powered Digital Framework for Personalized Economical Quality Learning at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Mrzieh VatandoustMohammadieh",
        "Mohammad Mahdi Mohajeri",
        "Ali Keramati",
        "Majid Nili Ahmadabadi"
      ],
      "abstract": "The disparity in access to quality education is significant, both between\ndeveloped and developing countries and within nations, regardless of their\neconomic status. Socioeconomic barriers and rapid changes in the job market\nfurther intensify this issue, highlighting the need for innovative solutions\nthat can deliver quality education at scale and low cost. This paper addresses\nthese challenges by proposing an AI-powered digital learning framework grounded\nin Deep Learning (DL) theory. The DL theory emphasizes learner agency and\nredefines the role of teachers as facilitators, making it particularly suitable\nfor scalable educational environments. We outline eight key principles derived\nfrom learning science and AI that are essential for implementing DL-based\nDigital Learning Environments (DLEs). Our proposed framework leverages AI for\nlearner modelling based on Open Learner Modeling (OLM), activity suggestions,\nand AI-assisted support for both learners and facilitators, fostering\ncollaborative and engaging learning experiences. Our framework provides a\npromising direction for scalable, high-quality education globally, offering\npractical solutions to some of the AI-related challenges in education.",
      "tldr_zh": "该论文针对教育不平等问题（如发达与发展中国家间的差距以及社会经济障碍），提出一个基于 Deep Learning (DL) 理论的 AI-powered 数字学习框架。该框架强调学习者自主性，将教师角色转变为促进者，并基于学习科学和 AI 提炼出八个关键原则，用于构建 Digital Learning Environments (DLEs)。框架利用 AI 技术如 Open Learner Modeling (OLM) 进行学习者建模、活动建议和支持，促进协作性学习，最终提供可扩展、低成本的高质量教育解决方案。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04483v1",
      "published_date": "2024-11-20 17:44:29 UTC",
      "updated_date": "2024-11-20 17:44:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:24:19.737220"
    },
    {
      "arxiv_id": "2411.13485v2",
      "title": "Utilizing Large Language Models to Synthesize Product Desirability Datasets",
      "title_zh": "翻译失败",
      "authors": [
        "John D. Hastings",
        "Sherri Weitl-Harms",
        "Joseph Doty",
        "Zachary J. Myers",
        "Warren Thompson"
      ],
      "abstract": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
      "tldr_zh": "这篇论文探讨了利用大型语言模型(LLMs)生成合成数据集，以测试Product Desirability Toolkit (PDT)，从而评估用户情感和产品体验。研究采用gpt-4o-mini作为成本有效的LLM，并通过三种方法（Word+Review、Review+Word和Supply-Word）分别合成1000个产品评论。结果显示，所有方法均表现出高情感对齐度（Pearson correlations 0.93-0.97），其中Supply-Word方法在文本多样性和PDT术语覆盖方面表现最佳，但生成成本较高。尽管存在轻微的积极情感偏见，在数据有限的场景下，LLM生成的合成数据提供了可扩展性、成本节约和灵活性等优势。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.7; H.3.3; I.2.6; H.5.2"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 2 figures, 6 tables, updated author list",
      "pdf_url": "http://arxiv.org/pdf/2411.13485v2",
      "published_date": "2024-11-20 17:35:21 UTC",
      "updated_date": "2024-11-22 15:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:24:32.036846"
    },
    {
      "arxiv_id": "2411.13477v1",
      "title": "PatentEdits: Framing Patent Novelty as Textual Entailment",
      "title_zh": "翻译失败",
      "authors": [
        "Ryan Lee",
        "Alexander Spangher",
        "Xuezhe Ma"
      ],
      "abstract": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.",
      "tldr_zh": "该论文引入了PatentEdits数据集，包含10.5万例成功专利修订例子，用于解决专利新颖性和非显而易见性问题，特别是预测给定prior art的发明声明应如何修改。该数据集通过设计算法逐句标记编辑，并利用大型语言模型（LLMs）来预测这些变更。研究发现，将专利新颖性框架化为文本蕴涵（textual entailment）任务，尤其有效，能够准确识别哪些声明保持不变或相对于prior art是新颖的，从而为自动化专利审查提供新工具。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13477v1",
      "published_date": "2024-11-20 17:23:40 UTC",
      "updated_date": "2024-11-20 17:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:24:43.655041"
    },
    {
      "arxiv_id": "2411.13459v1",
      "title": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures",
      "title_zh": "SoK: 复合 AI 威胁和对策的系统视角",
      "authors": [
        "Sarbartha Banerjee",
        "Prateek Sahu",
        "Mulong Luo",
        "Anjo Vahldiek-Oberwagner",
        "Neeraja J. Yadwadkar",
        "Mohit Tiwari"
      ],
      "abstract": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems.",
      "tldr_zh": "这篇 SoK 论文从系统视角分析了复合 AI 系统（如集成多个 LLMs 的推理管道）面临的威胁和对策，强调了攻击面的扩大以及结合软件和硬件攻击向量可能导致强大端到端攻击。论文系统化了各种 ML 攻击，并将其与 Mitre Att&ck 框架对齐，以更好地定位每个攻击的威胁模型假设。最终，它概述了现有软件和硬件层面的 countermeasures，并呼吁采用全面防御策略，以确保复合 AI 系统的安全和高性能部署。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "13 pages, 4 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.13459v1",
      "published_date": "2024-11-20 17:08:38 UTC",
      "updated_date": "2024-11-20 17:08:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:24:56.093965"
    },
    {
      "arxiv_id": "2411.13453v1",
      "title": "LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models",
      "title_zh": "翻译失败",
      "authors": [
        "Salvatore Mario Carta",
        "Stefano Chessa",
        "Giulia Contu",
        "Andrea Corriga",
        "Andrea Deidda",
        "Gianni Fenu",
        "Luca Frigau",
        "Alessandro Giuliani",
        "Luca Grassi",
        "Marco Manolo Manca",
        "Mirko Marras",
        "Francesco Mola",
        "Bastianino Mossa",
        "Piergiorgio Mura",
        "Marco Ortu",
        "Leonardo Piano",
        "Simone Pisano",
        "Alessia Pisu",
        "Alessandro Sebastian Podda",
        "Livio Pompianu",
        "Simone Seu",
        "Sandro Gabriele Tiddia"
      ],
      "abstract": "Minority languages are vital to preserving cultural heritage, yet they face\ngrowing risks of extinction due to limited digital resources and the dominance\nof artificial intelligence models trained on high-resource languages. This\nwhite paper proposes a framework to generate linguistic tools for low-resource\nlanguages, focusing on data creation to support the development of language\nmodels that can aid in preservation efforts. Sardinian, an endangered language,\nserves as the case study to demonstrate the framework's effectiveness. By\naddressing the data scarcity that hinders intelligent applications for such\nlanguages, we contribute to promoting linguistic diversity and support ongoing\nefforts in language standardization and revitalization through modern\ntechnologies.",
      "tldr_zh": "这篇论文介绍了 LIMBA，一个开源框架，利用 Generative Models 为 Low-Resource Languages 生成语言工具，以应对这些语言因数据稀缺和 AI 模型偏向而面临的灭绝风险。框架重点关注数据创建，支持语言模型的发展，从而促进语言保存和多样性。Sardinian 作为案例研究，展示了该框架在推动语言标准化和复兴方面的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13453v1",
      "published_date": "2024-11-20 16:59:41 UTC",
      "updated_date": "2024-11-20 16:59:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:25:07.613995"
    },
    {
      "arxiv_id": "2411.13451v1",
      "title": "AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations",
      "title_zh": "翻译失败",
      "authors": [
        "Gaurav Verma",
        "Rachneet Kaur",
        "Nishan Srishankar",
        "Zhen Zeng",
        "Tucker Balch",
        "Manuela Veloso"
      ],
      "abstract": "State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.",
      "tldr_zh": "该论文提出AdaptAgent框架，利用Few-Shot Learning从人类演示中学习，旨在帮助多模态网络代理（Multimodal Web Agents）快速适应新网站和领域，仅需少量演示（最多2个）。该框架支持专有和开源模型，通过in-context演示或meta-adaptation演示来提升代理的泛化能力。实验在Mind2Web和VisualWebArena基准上显示，任务成功率较非适应基线模型提高了3.36%至7.21%，相对增幅达21.03%至65.75%。此外，分析表明多模态演示优于文本-only演示，并探讨了数据选择策略和少样本数量对代理泛化性能的影响。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 3 figures, an abridged version to appear in NeurIPS 2024\n  AFM Workshop",
      "pdf_url": "http://arxiv.org/pdf/2411.13451v1",
      "published_date": "2024-11-20 16:54:15 UTC",
      "updated_date": "2024-11-20 16:54:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:25:20.108595"
    },
    {
      "arxiv_id": "2411.13438v2",
      "title": "Robust Monocular Visual Odometry using Curriculum Learning",
      "title_zh": "基于课程学习的鲁棒单目视觉里程计",
      "authors": [
        "Assaf Lahiany",
        "Oren Gal"
      ],
      "abstract": "Curriculum Learning (CL), drawing inspiration from natural learning patterns\nobserved in humans and animals, employs a systematic approach of gradually\nintroducing increasingly complex training data during model development. Our\nwork applies innovative CL methodologies to address the challenging geometric\nproblem of monocular Visual Odometry (VO) estimation, which is essential for\nrobot navigation in constrained environments. The primary objective of our\nresearch is to push the boundaries of current state-of-the-art (SOTA)\nbenchmarks in monocular VO by investigating various curriculum learning\nstrategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO)\nframework through the integration of novel CL approaches, with the goal of\ndeveloping more resilient models capable of maintaining high performance across\nchallenging environments and complex motion scenarios. Our research encompasses\nseveral distinctive CL strategies. We develop methods to evaluate sample\ndifficulty based on trajectory motion characteristics, implement sophisticated\nadaptive scheduling through self-paced weighted loss mechanisms, and utilize\nreinforcement learning agents for dynamic adjustment of training emphasis.\nThrough comprehensive evaluation on the diverse synthetic TartanAir dataset and\ncomplex real-world benchmarks such as EuRoC and TUM-RGBD, our Curriculum\nLearning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior\nperformance compared to existing SOTA methods, including both feature-based and\nlearning-based VO approaches. The results validate the effectiveness of\nintegrating curriculum learning principles into visual odometry systems.",
      "tldr_zh": "本文提出了一种基于 Curriculum Learning (CL) 的鲁棒单目 Visual Odometry (VO) 方法，旨在通过逐步引入复杂训练数据来提升机器人导航性能。研究团队增强了 Deep-Patch-Visual Odometry (DPVO) 框架，开发了创新的 CL 策略，包括基于轨迹运动特征评估样本难度、自适应加权损失机制和强化学习代理动态调整训练。实验结果显示，在 TartanAir、EuRoC 和 TUM-RGBD 数据集上，CL-DPVO 超过了现有最先进（SOTA）方法，证明了 CL 原则在 VO 系统中的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.13438v2",
      "published_date": "2024-11-20 16:26:51 UTC",
      "updated_date": "2024-12-13 14:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:25:32.159853"
    },
    {
      "arxiv_id": "2411.15216v3",
      "title": "Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint",
      "title_zh": "Dist Loss：通过分布距离约束增强少样本区域中的回归",
      "authors": [
        "Guangkun Nie",
        "Gongzheng Tang",
        "Shenda Hong"
      ],
      "abstract": "Imbalanced data distributions are prevalent in real-world scenarios, posing\nsignificant challenges in both imbalanced classification and imbalanced\nregression tasks. They often cause deep learning models to overfit in areas of\nhigh sample density (many-shot regions) while underperforming in areas of low\nsample density (few-shot regions). This characteristic restricts the utility of\ndeep learning models in various sectors, notably healthcare, where areas with\nfew-shot data hold greater clinical relevance. While recent studies have shown\nthe benefits of incorporating distribution information in imbalanced\nclassification tasks, such strategies are rarely explored in imbalanced\nregression. In this paper, we address this issue by introducing a novel loss\nfunction, termed Dist Loss, designed to minimize the distribution distance\nbetween the model's predictions and the target labels in a differentiable\nmanner, effectively integrating distribution information into model training.\nDist Loss enables deep learning models to regularize their output distribution\nduring training, effectively enhancing their focus on few-shot regions. We have\nconducted extensive experiments across three datasets spanning computer vision\nand healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results\ndemonstrate that Dist Loss effectively mitigates the negative impact of\nimbalanced data distribution on model performance, achieving state-of-the-art\nresults in sparse data regions. Furthermore, Dist Loss is easy to integrate,\ncomplementing existing methods.",
      "tldr_zh": "本论文针对不平衡数据分布在回归任务中的问题，特别是在 few-shot regions 的表现不足，提出了一种新损失函数 Dist Loss，通过最小化模型预测和目标标签之间的分布距离来整合分布信息。Dist Loss 能够在训练过程中调节模型输出分布，增强对低密度样本区域的关注，从而缓解模型过拟合高密度区域的弊端。在 IMDB-WIKI-DIR、AgeDB-DIR 和 ECG-Ka-DIR 数据集上的实验表明，Dist Loss 显著提升了模型在稀疏数据区域的性能，达到了 state-of-the-art 结果，并易于与其他方法整合。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15216v3",
      "published_date": "2024-11-20 16:17:40 UTC",
      "updated_date": "2025-03-28 02:57:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:25:44.213874"
    },
    {
      "arxiv_id": "2411.13428v1",
      "title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Hojjat Karami",
        "David Atienza",
        "Anisoara Ionescu"
      ],
      "abstract": "Generating synthetic Electronic Health Records (EHRs) offers significant\npotential for data augmentation, privacy-preserving data sharing, and improving\nmachine learning model training. We propose a novel tokenization strategy\ntailored for structured EHR data, which encompasses diverse data types such as\ncovariates, ICD codes, and irregularly sampled time series. Using a GPT-like\ndecoder-only transformer model, we demonstrate the generation of high-quality\nsynthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we\nbenchmark the fidelity, utility, and privacy of the generated data against\nstate-of-the-art models.",
      "tldr_zh": "这篇论文提出了 SynEHRgy，一种使用 decoder-only transformers 生成混合类型结构化 Electronic Health Records (EHRs) 的方法，旨在支持数据增强、隐私保护数据共享和机器学习模型训练。研究者设计了一种专为 covariates、ICD codes 和不规则采样时间序列量身定制的 tokenization 策略，并采用 GPT-like 模型生成高质量合成数据。在 MIMIC-III 数据集上进行评估，结果显示该方法在 fidelity、utility 和 privacy 方面优于现有 state-of-the-art 模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13428v1",
      "published_date": "2024-11-20 16:11:20 UTC",
      "updated_date": "2024-11-20 16:11:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:25:56.369026"
    },
    {
      "arxiv_id": "2411.13420v1",
      "title": "Heuristically Adaptive Diffusion-Model Evolutionary Strategy",
      "title_zh": "启发式自适应扩散模型进化策略",
      "authors": [
        "Benedikt Hartl",
        "Yanbo Zhang",
        "Hananel Hazan",
        "Michael Levin"
      ],
      "abstract": "Diffusion Models represent a significant advancement in generative modeling,\nemploying a dual-phase process that first degrades domain-specific information\nvia Gaussian noise and restores it through a trainable model. This framework\nenables pure noise-to-data generation and modular reconstruction of, images or\nvideos. Concurrently, evolutionary algorithms employ optimization methods\ninspired by biological principles to refine sets of numerical parameters\nencoding potential solutions to rugged objective functions. Our research\nreveals a fundamental connection between diffusion models and evolutionary\nalgorithms through their shared underlying generative mechanisms: both methods\ngenerate high-quality samples via iterative refinement on random initial\ndistributions. By employing deep learning-based diffusion models as generative\nmodels across diverse evolutionary tasks and iteratively refining diffusion\nmodels with heuristically acquired databases, we can iteratively sample\npotentially better-adapted offspring parameters, integrating them into\nsuccessive generations of the diffusion model. This approach achieves efficient\nconvergence toward high-fitness parameters while maintaining explorative\ndiversity. Diffusion models introduce enhanced memory capabilities into\nevolutionary algorithms, retaining historical information across generations\nand leveraging subtle data correlations to generate refined samples. We elevate\nevolutionary algorithms from procedures with shallow heuristics to frameworks\nwith deep memory. By deploying classifier-free guidance for conditional\nsampling at the parameter level, we achieve precise control over evolutionary\nsearch dynamics to further specific genotypical, phenotypical, or\npopulation-wide traits. Our framework marks a major heuristic and algorithmic\ntransition, offering increased flexibility, precision, and control in\nevolutionary optimization processes.",
      "tldr_zh": "这篇论文揭示了 Diffusion Models 和 evolutionary algorithms 之间的根本连接，即两者均通过迭代精炼随机初始分布来生成高质量样本。研究提出 Heuristically Adaptive Diffusion-Model Evolutionary Strategy 框架，该框架将 Diffusion Models 整合进进化任务中，通过迭代精炼模型和使用 heuristically acquired databases 来生成更适应的参数样本，从而实现高效收敛同时保持探索多样性。该方法提升了 evolutionary algorithms 的记忆能力，利用历史信息和数据相关性进行精炼采样，并通过 classifier-free guidance 实现对进化搜索的精确控制，最终为优化过程带来更大的灵活性、精确性和整体性能提升。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13420v1",
      "published_date": "2024-11-20 16:06:28 UTC",
      "updated_date": "2024-11-20 16:06:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:26:07.932221"
    },
    {
      "arxiv_id": "2411.13409v1",
      "title": "Unification of Balti and trans-border sister dialects in the essence of LLMs and AI Technology",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Sharif",
        "Jiangyan Yi",
        "Muhammad Shoaib"
      ],
      "abstract": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.",
      "tldr_zh": "本论文探讨了 Balti 语言及其跨国方言的统一问题，Balti 属于藏缅语支，在印度、中国、巴基斯坦等国家有变体，受文化、社会和地理因素影响。作者强调，通过分析这些方言的共同根源、词汇和语音基础，实现统一以缩小全球化时代的分歧至关重要。该研究利用 AI 和 LLMs 来分析、记录和标准化 Balti 这种濒危语言，为语言多样性保护提供新方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by IEEE conference ISCSLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.13409v1",
      "published_date": "2024-11-20 15:48:21 UTC",
      "updated_date": "2024-11-20 15:48:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:26:19.428126"
    },
    {
      "arxiv_id": "2412.04482v2",
      "title": "NLP Cluster Analysis of Common Core State Standards and NAEP Item Specifications",
      "title_zh": "翻译失败",
      "authors": [
        "Gregory Camilli",
        "Larry Suter"
      ],
      "abstract": "Camilli (2024) proposed a methodology using natural language processing (NLP)\nto map the relationship of a set of content standards to item specifications.\nThis study provided evidence that NLP can be used to improve the mapping\nprocess. As part of this investigation, the nominal classifications of\nstandards and items specifications were used to examine construct equivalence.\nIn the current paper, we determine the strength of empirical support for the\nsemantic distinctiveness of these classifications, which are known as \"domains\"\nfor Common Core standards, and \"strands\" for National Assessment of Educational\nProgress (NAEP) item specifications. This is accomplished by separate k-means\nclustering for standards and specifications of their corresponding embedding\nvectors. We then briefly illustrate an application of these findings.",
      "tldr_zh": "本研究利用自然语言处理 (NLP) 和 k-means 聚类分析，评估了 Common Core State Standards 的 \"domains\" 和 National Assessment of Educational Progress (NAEP) Item Specifications 的 \"strands\" 的语义独特性。研究基于 Camilli (2024) 的方法，通过对标准和规范的嵌入向量进行单独聚类，提供了实证证据支持这些分类的语义区分。结果表明，此方法能增强内容标准与项目规范的映射过程，并简要展示了其实际应用潜力。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "10 pages, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.04482v2",
      "published_date": "2024-11-20 15:44:58 UTC",
      "updated_date": "2024-12-13 16:56:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:26:30.665876"
    },
    {
      "arxiv_id": "2411.13365v1",
      "title": "Explainable Finite-Memory Policies for Partially Observable Markov Decision Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Muqsit Azeem",
        "Debraj Chakraborty",
        "Sudeep Kanav",
        "Jan Kretinsky"
      ],
      "abstract": "Partially Observable Markov Decision Processes (POMDPs) are a fundamental\nframework for decision-making under uncertainty and partial observability.\nSince in general optimal policies may require infinite memory, they are hard to\nimplement and often render most problems undecidable. Consequently,\nfinite-memory policies are mostly considered instead. However, the algorithms\nfor computing them are typically very complex, and so are the resulting\npolicies. Facing the need for their explainability, we provide a representation\nof such policies, both (i) in an interpretable formalism and (ii) typically of\nsmaller size, together yielding higher explainability. To that end, we combine\nmodels of Mealy machines and decision trees; the latter describing simple,\nstationary parts of the policies and the former describing how to switch among\nthem. We design a translation for policies of the finite-state-controller (FSC)\nform from standard literature and show how our method smoothly generalizes to\nother variants of finite-memory policies. Further, we identify specific\nproperties of recently used \"attractor-based\" policies, which allow us to\nconstruct yet simpler and smaller representations. Finally, we illustrate the\nhigher explainability in a few case studies.",
      "tldr_zh": "本论文针对 Partially Observable Markov Decision Processes (POMDPs) 中的有限记忆策略问题，提出了一种更易解释的表示方法，以解决传统策略的复杂性和不可解释性。该方法结合 Mealy machines 和 decision trees，其中决策树处理策略的简单平稳部分，而 Mealy machines 负责在这些部分之间切换，从而生成更小且更可解释的策略表示。论文还设计了从 finite-state-controller (FSC) 形式到新表示的翻译，并扩展到其他有限记忆策略变体；针对 attractor-based 策略，识别了特定属性以进一步简化表示。通过案例研究，证明了该方法的更高可解释性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "Preprint -- Under Review",
      "pdf_url": "http://arxiv.org/pdf/2411.13365v1",
      "published_date": "2024-11-20 14:42:23 UTC",
      "updated_date": "2024-11-20 14:42:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:26:43.756802"
    },
    {
      "arxiv_id": "2411.15215v1",
      "title": "S$^2$ALM: Sequence-Structure Pre-trained Large Language Model for Comprehensive Antibody Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mingze Yin",
        "Hanjing Zhou",
        "Jialu Wu",
        "Yiheng Zhu",
        "Yuxuan Zhan",
        "Zitai Kong",
        "Hongxia Xu",
        "Chang-Yu Hsieh",
        "Jintai Chen",
        "Tingjun Hou",
        "Jian Wu"
      ],
      "abstract": "Antibodies safeguard our health through their precise and potent binding to\nspecific antigens, demonstrating promising therapeutic efficacy in the\ntreatment of numerous diseases, including COVID-19. Recent advancements in\nbiomedical language models have shown the great potential to interpret complex\nbiological structures and functions. However, existing antibody specific models\nhave a notable limitation that they lack explicit consideration for antibody\nstructural information, despite the fact that both 1D sequence and 3D structure\ncarry unique and complementary insights into antibody behavior and\nfunctionality. This paper proposes Sequence-Structure multi-level pre-trained\nAntibody Language Model (S$^2$ALM), combining holistic sequential and\nstructural information in one unified, generic antibody foundation model. We\nconstruct a hierarchical pre-training paradigm incorporated with two customized\nmulti-level training objectives to facilitate the modeling of comprehensive\nantibody representations. S$^2$ALM's representation space uncovers inherent\nfunctional binding mechanisms, biological evolution properties and structural\ninteraction patterns. Pre-trained over 75 million sequences and 11.7 million\nstructures, S$^2$ALM can be adopted for diverse downstream tasks: accurately\npredicting antigen-antibody binding affinities, precisely distinguishing B cell\nmaturation stages, identifying antibody crucial binding positions, and\nspecifically designing novel coronavirus-binding antibodies. Remarkably,\nS$^2$ALM outperforms well-established and renowned baselines and sets new\nstate-of-the-art performance across extensive antibody specific understanding\nand generation tasks. S$^2$ALM's ability to model comprehensive and generalized\nrepresentations further positions its potential to advance real-world\ntherapeutic antibody development, potentially addressing unmet academic,\nindustrial, and clinical needs.",
      "tldr_zh": "本文提出S$^2$ALM，一种序列-结构预训练的大型语言模型，用于全面抗体表示学习，通过整合抗体的1D序列和3D结构信息来解决现有模型忽略结构细节的局限性。模型采用分层预训练范式和两个定制的多级训练目标，在75百万序列和11.7百万结构上进行预训练，从而揭示抗体的功能结合机制、生物进化特性和结构交互模式。S$^2$ALM在下游任务中表现出色，包括准确预测抗原-抗体结合亲和力、区分B细胞成熟阶段、识别关键结合位置以及设计新型冠状病毒结合抗体，并超越现有基线，达到新SOTA水平。该模型有望推进实际治疗抗体开发，满足学术、工业和临床需求。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15215v1",
      "published_date": "2024-11-20 14:24:26 UTC",
      "updated_date": "2024-11-20 14:24:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:26:56.429384"
    },
    {
      "arxiv_id": "2411.13627v1",
      "title": "CryptoFormalEval: Integrating LLMs and Formal Verification for Automated Cryptographic Protocol Vulnerability Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Cristian Curaba",
        "Denis D'Ambrosi",
        "Alessandro Minisini",
        "Natalia Pérez-Campanero Antolín"
      ],
      "abstract": "Cryptographic protocols play a fundamental role in securing modern digital\ninfrastructure, but they are often deployed without prior formal verification.\nThis could lead to the adoption of distributed systems vulnerable to attack\nvectors. Formal verification methods, on the other hand, require complex and\ntime-consuming techniques that lack automatization. In this paper, we introduce\na benchmark to assess the ability of Large Language Models (LLMs) to\nautonomously identify vulnerabilities in new cryptographic protocols through\ninteraction with Tamarin: a theorem prover for protocol verification. We\ncreated a manually validated dataset of novel, flawed, communication protocols\nand designed a method to automatically verify the vulnerabilities found by the\nAI agents. Our results about the performances of the current frontier models on\nthe benchmark provides insights about the possibility of cybersecurity\napplications by integrating LLMs with symbolic reasoning systems.",
      "tldr_zh": "本研究引入了 CryptoFormalEval 基准，通过整合大型语言模型 (LLMs) 和正式验证工具（如 Tamarin），实现了加密协议漏洞的自动检测，解决了传统验证方法复杂且低效的问题。研究团队创建了一个手动验证的数据集，包含新型有缺陷的通信协议，并设计了 AI 代理与 Tamarin 互动的机制来识别和验证漏洞。实验结果显示，当前前沿模型在该基准上的表现提供了宝贵见解，支持 LLMs 与符号推理系统在网络安全应用中的潜力。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SC"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13627v1",
      "published_date": "2024-11-20 14:16:55 UTC",
      "updated_date": "2024-11-20 14:16:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:27:06.831773"
    },
    {
      "arxiv_id": "2411.13343v1",
      "title": "Fact-Level Confidence Calibration and Self-Correction",
      "title_zh": "事实级置信度校准与自校正",
      "authors": [
        "Yige Yuan",
        "Bingbing Xu",
        "Hexiang Tan",
        "Fei Sun",
        "Teng Xiao",
        "Wei Li",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）的自信心校准问题，提出Fact-Level Calibration框架，该框架在事实级别上校准模型自信心，与查询相关性加权的正确性对齐，从而解决现有方法在长响应中无法处理部分正确性和相关性的局限性。基于此框架，开发了Confidence-Guided Fact-level Self-Correction（ConFix）方法，利用响应中高自信心事实作为内部知识来源来改进低自信心事实，从而有效减少幻觉现象。实验在四个数据集和六个模型上验证，证明ConFix无需外部知识系统即可显著提升模型的准确性和可靠性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Code is available at https://github.com/yuanyige/fact-calibration",
      "pdf_url": "http://arxiv.org/pdf/2411.13343v1",
      "published_date": "2024-11-20 14:15:18 UTC",
      "updated_date": "2024-11-20 14:15:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:27:18.863311"
    },
    {
      "arxiv_id": "2411.13332v1",
      "title": "Verifying Machine Unlearning with Explainable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Àlex Pujol Vidal",
        "Anders S. Johansen",
        "Mohammad N. S. Jahromi",
        "Sergio Escalera",
        "Kamal Nasrollahi",
        "Thomas B. Moeslund"
      ],
      "abstract": "We investigate the effectiveness of Explainable AI (XAI) in verifying Machine\nUnlearning (MU) within the context of harbor front monitoring, focusing on data\nprivacy and regulatory compliance. With the increasing need to adhere to\nprivacy legislation such as the General Data Protection Regulation (GDPR),\ntraditional methods of retraining ML models for data deletions prove\nimpractical due to their complexity and resource demands. MU offers a solution\nby enabling models to selectively forget specific learned patterns without full\nretraining. We explore various removal techniques, including data relabeling,\nand model perturbation. Then, we leverage attribution-based XAI to discuss the\neffects of unlearning on model performance. Our proof-of-concept introduces\nfeature importance as an innovative verification step for MU, expanding beyond\ntraditional metrics and demonstrating techniques' ability to reduce reliance on\nundesired patterns. Additionally, we propose two novel XAI-based metrics,\nHeatmap Coverage (HC) and Attention Shift (AS), to evaluate the effectiveness\nof these methods. This approach not only highlights how XAI can complement MU\nby providing effective verification, but also sets the stage for future\nresearch to enhance their joint integration.",
      "tldr_zh": "本研究探讨了 Explainable AI (XAI) 在验证 Machine Unlearning (MU) 有效性方面的应用，聚焦于港口前沿监控中的数据隐私和合规性（如 GDPR 要求）。他们评估了多种 MU 技术，包括数据 relabeling 和模型 perturbation，以实现模型选择性忘记特定模式，而无需完整重训。论文引入特征重要性作为创新的验证步骤，并提出两个新指标——Heatmap Coverage (HC) 和 Attention Shift (AS)，用于评估 MU 对模型性能的影响。总体上，这为 XAI 与 MU 的整合提供了新路径，提升了隐私保护的可解释性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICPRW2024",
      "pdf_url": "http://arxiv.org/pdf/2411.13332v1",
      "published_date": "2024-11-20 13:57:32 UTC",
      "updated_date": "2024-11-20 13:57:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:27:32.194545"
    },
    {
      "arxiv_id": "2411.13326v1",
      "title": "An Evolutional Neural Network Framework for Classification of Microarray Data",
      "title_zh": "翻译失败",
      "authors": [
        "Maryam Eshraghi Evari",
        "Md Nasir Sulaiman",
        "Amir Rajabi Behjat"
      ],
      "abstract": "DNA microarray gene-expression data has been widely used to identify\ncancerous gene signatures. Microarray can increase the accuracy of cancer\ndiagnosis and prognosis. However, analyzing the large amount of gene expression\ndata from microarray chips pose a challenge for current machine learning\nresearches. One of the challenges lie within classification of healthy and\ncancerous tissues is high dimensionality of gene expressions. High\ndimensionality decreases the accuracy of the classification. This research aims\nto apply a hybrid model of Genetic Algorithm and Neural Network to overcome the\nproblem during subset selection of informative genes. Whereby, a Genetic\nAlgorithm (GA) reduced dimensionality during feature selection and then a\nMulti-Layer perceptron Neural Network (MLP) is applied to classify selected\ngenes. The performance evaluated by considering to the accuracy and the number\nof selected genes. Experimental results show the proposed method suggested high\naccuracy and minimum number of selected genes in comparison with other machine\nlearning algorithms.",
      "tldr_zh": "这篇论文针对 DNA 微阵列基因表达数据的分类问题，特别解决高维度导致癌症诊断准确率下降的挑战。研究提出了一种混合框架，将遗传算法 (GA) 用于特征选择以减少维度，然后采用多层感知器神经网络 (MLP) 对选定的基因进行分类。实验结果显示，该方法在准确性和选定基因数量上均优于其他机器学习算法，从而提升了癌症诊断和预后的可靠性。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13326v1",
      "published_date": "2024-11-20 13:48:40 UTC",
      "updated_date": "2024-11-20 13:48:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:27:43.476125"
    },
    {
      "arxiv_id": "2411.13323v3",
      "title": "Are Large Language Models Memorizing Bug Benchmarks?",
      "title_zh": "大语言模型是否在记忆 bug 基准？",
      "authors": [
        "Daniel Ramos",
        "Claudia Mamede",
        "Kush Jain",
        "Paulo Canelas",
        "Catarina Gamboa",
        "Claire Le Goues"
      ],
      "abstract": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage. In this paper, we systematically evaluate popular\nLLMs to assess their susceptibility to data leakage from widely used bug\nbenchmarks. To identify potential leakage, we use multiple metrics, including a\nstudy of benchmark membership within commonly used training datasets, as well\nas analyses of negative log-likelihood and n-gram accuracy. Our findings show\nthat certain models, in particular codegen-multi, exhibit significant evidence\nof memorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
      "tldr_zh": "该研究调查了大型语言模型（LLMs）在软件工程任务（如代码生成、bug检测和修复）中是否记忆了bug基准测试，从而导致数据泄露影响性能评估。作者通过多种指标，包括基准测试在训练数据集中的成员资格分析、负对数似然和n-gram准确率，对流行LLMs进行系统评估。结果显示，codegen-multi模型在Defects4J等基准中表现出显著记忆迹象，而LLaMa 3.1等新模型则显示泄露迹象较少。这些发现强调了需要谨慎选择bug基准并采用稳健指标，以准确评估模型能力。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13323v3",
      "published_date": "2024-11-20 13:46:04 UTC",
      "updated_date": "2025-03-31 13:02:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:27:56.445420"
    },
    {
      "arxiv_id": "2411.13322v1",
      "title": "Scaling Laws for Online Advertisement Retrieval",
      "title_zh": "在线广告检索的缩放定律",
      "authors": [
        "Yunli Wang",
        "Zixuan Yang",
        "Zhen Zhang",
        "Zhiqiang Wang",
        "Jian Yang",
        "Shiyang Wen",
        "Peng Jiang",
        "Kun Gai"
      ],
      "abstract": "The scaling law is a notable property of neural network models and has\nsignificantly propelled the development of large language models. Scaling laws\nhold great promise in guiding model design and resource allocation. Recent\nresearch increasingly shows that scaling laws are not limited to NLP tasks or\nTransformer architectures; they also apply to domains such as recommendation.\nHowever, there is still a lack of literature on scaling law research in online\nadvertisement retrieval systems. This may be because 1) identifying the scaling\nlaw for resource cost and online revenue is often expensive in both time and\ntraining resources for large-scale industrial applications, and 2) varying\nsettings for different systems prevent the scaling law from being applied\nacross various scenarios. To address these issues, we propose a lightweight\nparadigm to identify the scaling law of online revenue and machine cost for a\ncertain online advertisement retrieval scenario with a low experimental cost.\nSpecifically, we focus on a sole factor (FLOPs) and propose an offline metric\nnamed R/R* that exhibits a high linear correlation with online revenue for\nretrieval models. We estimate the machine cost offline via a simulation\nalgorithm. Thus, we can transform most online experiments into low-cost offline\nexperiments. We conduct comprehensive experiments to verify the effectiveness\nof our proposed metric R/R* and to identify the scaling law in the online\nadvertisement retrieval system of Kuaishou. With the scaling law, we\ndemonstrate practical applications for ROI-constrained model designing and\nmulti-scenario resource allocation in Kuaishou advertising system. To the best\nof our knowledge, this is the first work to study the scaling laws for online\nadvertisement retrieval of real-world systems, showing great potential for\nscaling law in advertising system optimization.",
      "tldr_zh": "该研究探讨了scaling laws在在线广告检索系统中的应用，旨在解决现有研究的成本高和系统差异问题。研究者提出了一种轻量级范式，通过聚焦于FLOPs（浮点运算次数）并引入离线指标R/R*，该指标与在线收入高度线性相关，并通过模拟算法估算机器成本，从而将昂贵的在线实验转化为低成本离线实验。在Kuaishou的广告系统中进行的实验验证了这一方法的有效性，并识别出scaling laws的规律。最终，该工作展示了scaling laws在ROI-constrained model designing和multi-scenario resource allocation中的实际应用，为在线广告系统优化提供了新潜力。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.13322v1",
      "published_date": "2024-11-20 13:44:59 UTC",
      "updated_date": "2024-11-20 13:44:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:28:07.599153"
    },
    {
      "arxiv_id": "2411.13311v1",
      "title": "A Resource Efficient Fusion Network for Object Detection in Bird's-Eye View using Camera and Raw Radar Data",
      "title_zh": "翻译失败",
      "authors": [
        "Kavin Chandrasekaran",
        "Sorin Grigorescu",
        "Gijs Dubbelman",
        "Pavol Jancura"
      ],
      "abstract": "Cameras can be used to perceive the environment around the vehicle, while\naffordable radar sensors are popular in autonomous driving systems as they can\nwithstand adverse weather conditions unlike cameras. However, radar point\nclouds are sparser with low azimuth and elevation resolution that lack semantic\nand structural information of the scenes, resulting in generally lower radar\ndetection performance. In this work, we directly use the raw range-Doppler (RD)\nspectrum of radar data, thus avoiding radar signal processing. We independently\nprocess camera images within the proposed comprehensive image processing\npipeline. Specifically, first, we transform the camera images to Bird's-Eye\nView (BEV) Polar domain and extract the corresponding features with our camera\nencoder-decoder architecture. The resultant feature maps are fused with\nRange-Azimuth (RA) features, recovered from the RD spectrum input from the\nradar decoder to perform object detection. We evaluate our fusion strategy with\nother existing methods not only in terms of accuracy but also on computational\ncomplexity metrics on RADIal dataset.",
      "tldr_zh": "本文提出一个资源高效的融合网络，用于在 Bird's-Eye View (BEV) 中利用相机和原始雷达数据进行物体检测，以解决雷达点云稀疏和缺少语义信息的挑战。方法包括直接使用雷达的 Range-Doppler (RD) 谱避免信号处理，将相机图像转换为 BEV 极坐标域提取特征，并将这些特征与从 RD 谱恢复的 Range-Azimuth (RA) 特征融合进行检测。在 RADIal 数据集上的实验显示，该网络不仅在准确性上优于现有方法，还在计算复杂度方面表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "IEEE Intelligent Transportation Systems Conference (ITSC) 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.13311v1",
      "published_date": "2024-11-20 13:26:13 UTC",
      "updated_date": "2024-11-20 13:26:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:28:20.438074"
    },
    {
      "arxiv_id": "2411.18636v1",
      "title": "Towards Advanced Speech Signal Processing: A Statistical Perspective on Convolution-Based Architectures and its Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Nirmal Joshua Kapu",
        "Raghav Karan"
      ],
      "abstract": "This article surveys convolution-based models including convolutional neural\nnetworks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing\nmodels and provide their statistical backgrounds and speech recognition,\nspeaker identification, emotion recognition, and speech enhancement\napplications. Through comparative training cost assessment, model size,\naccuracy and speed assessment, we compare the strengths and weaknesses of each\nmodel, identify potential errors and propose avenues for further research,\nemphasizing the central role it plays in advancing applications of speech\ntechnologies.",
      "tldr_zh": "这篇文章从统计角度调查了基于卷积的架构，如 CNNs、Conformers、ResNets 和 CRNNs，在语音信号处理中的应用，并探讨了它们在语音识别、说话人识别、情感识别和语音增强方面的背景和实际使用。通过比较训练成本、模型大小、准确性和速度，该研究分析了各模型的优缺点，并识别潜在错误，提出进一步研究的途径。最终，强调这些模型在推动语音技术应用方面的核心作用。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.18636v1",
      "published_date": "2024-11-20 13:01:30 UTC",
      "updated_date": "2024-11-20 13:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:28:30.832903"
    },
    {
      "arxiv_id": "2411.13284v1",
      "title": "DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain WiFi-Based Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Julian Strohmayer",
        "Rafael Sterzinger",
        "Matthias Wödlinger",
        "Martin Kampel"
      ],
      "abstract": "Cross-domain generalization is an open problem in WiFi-based sensing due to\nvariations in environments, devices, and subjects, causing domain shifts in\nchannel state information. To address this, we propose Domain-Adversarial\nTest-Time Adaptation (DATTA), a novel framework combining domain-adversarial\ntraining (DAT), test-time adaptation (TTA), and weight resetting to facilitate\nadaptation to unseen target domains and to prevent catastrophic forgetting.\nDATTA is integrated into a lightweight, flexible architecture optimized for\nspeed. We conduct a comprehensive evaluation of DATTA, including an ablation\nstudy on all key components using publicly available data, and verify its\nsuitability for real-time applications such as human activity recognition. When\ncombining a SotA video-based variant of TTA with WiFi-based DAT and comparing\nit to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch\nimplementation of DATTA is publicly available at:\nhttps://github.com/StrohmayerJ/DATTA.",
      "tldr_zh": "本研究针对WiFi-based人类活动识别中的跨域泛化问题（如环境、设备和主体变化导致的领域偏移），提出了一种新框架Domain-Adversarial Test-Time Adaptation (DATTA)。DATTA结合domain-adversarial training (DAT)、test-time adaptation (TTA)和weight resetting，实现了对未见目标域的适应，同时防止catastrophic forgetting，并采用轻量级架构优化了速度。实验结果显示，DATTA在公开数据集上的全面评估中，比SotA视频-based TTA结合WiFi-based DAT的方法提高了8.1%的F1-Score，并证明其适用于实时应用，如人类活动识别；代码已在GitHub开源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13284v1",
      "published_date": "2024-11-20 12:52:36 UTC",
      "updated_date": "2024-11-20 12:52:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:28:44.357944"
    },
    {
      "arxiv_id": "2411.13281v2",
      "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
      "title_zh": "VideoAutoArena：通过用户模拟评估视频分析中大型多模态模型的自动化竞技场",
      "authors": [
        "Ziyang Luo",
        "Haoning Wu",
        "Dongxu Li",
        "Jing Ma",
        "Mohan Kankanhalli",
        "Junnan Li"
      ],
      "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.",
      "tldr_zh": "本研究提出 VideoAutoArena，一种受 LMSYS Chatbot Arena 启发的自动基准测试平台，通过用户模拟生成开放式、适应性问题，以更全面评估 Large Multimodal Models (LMMs) 在视频分析方面的能力，克服传统多选题方法的深度不足。平台采用修改后的 ELO Rating System 和故障驱动演化策略，实现可扩展的模型比较，并通过与人类标注的“金标准”子集验证其判断与人类高度一致。实验结果显示，VideoAutoArena 能有效区分不同 LMMs 的优势和改进点，同时引入辅助基准 VideoAutoBench，使用 GPT-4o 和人类标注进一步提升评估的可靠性和成本效益。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025, Project Page: https://videoautoarena.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2411.13281v2",
      "published_date": "2024-11-20 12:48:34 UTC",
      "updated_date": "2025-03-23 11:01:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:28:56.692915"
    },
    {
      "arxiv_id": "2411.13280v3",
      "title": "Empower Structure-Based Molecule Optimization with Gradient Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Keyue Qiu",
        "Yuxuan Song",
        "Jie Yu",
        "Hongbo Ma",
        "Ziyao Cao",
        "Zhilong Zhang",
        "Yushuai Wu",
        "Mingyue Zheng",
        "Hao Zhou",
        "Wei-Ying Ma"
      ],
      "abstract": "Structure-Based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. MolJO\nachieves state-of-the-art performance on CrossDocked2020 benchmark (Success\nRate 51.3%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success\nRate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as\nmuch as 3D baselines. Furthermore, we extend MolJO to a wide range of\noptimization settings, including multi-objective optimization and challenging\ntasks in drug design such as R-group optimization and scaffold hopping, further\nunderscoring its versatility.",
      "tldr_zh": "该研究针对基于结构的分子优化（SBMO）提出了一种梯度指导框架MolJO，通过Bayesian inference派生的连续可微空间，实现不同模态信号的联合指导，同时保留SE(3)-equivariance。MolJO引入了创新的backward correction策略，在过去的滑动窗口中优化，以平衡探索和利用过程。实验结果显示，在CrossDocked2020基准上，MolJO达到最先进性能，包括Success Rate 51.3%、Vina Dock -9.05和SA 0.78，比梯度-based方法成功率提高4倍，并超过3D基线2倍\"Me-Better\" Ratio。该框架还扩展到多目标优化、R-group optimization和scaffold hopping等药物设计任务，展示了其多功能性。",
      "categories": [
        "q-bio.BM",
        "cs.AI"
      ],
      "primary_category": "q-bio.BM",
      "comment": "Accepted to ICML 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.13280v3",
      "published_date": "2024-11-20 12:48:29 UTC",
      "updated_date": "2025-05-12 07:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:29:09.968264"
    },
    {
      "arxiv_id": "2411.13269v1",
      "title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive Software",
      "title_zh": "翻译失败",
      "authors": [
        "Minal Suresh Patil",
        "Gustav Ung",
        "Mattias Nyberg"
      ],
      "abstract": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning.",
      "tldr_zh": "本论文提出了一种名为 spec2code 的通用框架，将大型语言模型 (LLMs) 与形式验证结合，用于生成关键的嵌入式汽车软件。该框架通过不同类型的批评者 (critics) 提供反馈，支持迭代回提示 (backprompting) 和微调，以从规范自动产生工业质量代码。在一个可行性研究中，研究者使用 Scania 的三个工业案例评估了 spec2code 的简化版本，探索了 ACSL 形式规范和自然语言规范的组合，结果显示即使不应用迭代 backprompting 和微调，也能生成形式正确的代码。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "21 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.13269v1",
      "published_date": "2024-11-20 12:38:17 UTC",
      "updated_date": "2024-11-20 12:38:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:29:19.330999"
    },
    {
      "arxiv_id": "2411.14491v3",
      "title": "A Survey on Human-Centric LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Yi Wang",
        "Nicholas Sukiennik",
        "Tong Li",
        "Weikang Su",
        "Qianyue Hao",
        "Jingbo Xu",
        "Zihan Huang",
        "Fengli Xu",
        "Yong Li"
      ],
      "abstract": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.",
      "tldr_zh": "这篇调查论文探讨了大型语言模型 (LLMs) 在模拟人类认知、决策和社会互动方面的能力，聚焦于个体任务（LLMs 替代单个人）和集体任务（多个 LLMs 模拟群体动态）。论文评估了 LLMs 在推理、感知和社会认知等关键领域的表现，并与人类技能进行比较，同时考察其在行为科学、政治科学和社会学等领域的实际应用效果。最终，它指出了挑战和未来方向，包括提升 LLMs 的适应性、情感智能、文化敏感性，以及处理偏见以加强人-AI 协作。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14491v3",
      "published_date": "2024-11-20 12:34:44 UTC",
      "updated_date": "2024-12-01 08:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:29:31.693420"
    },
    {
      "arxiv_id": "2411.13262v1",
      "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for Multi-point Robot Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Chen",
        "Yixin Han",
        "Xiao Li"
      ],
      "abstract": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices.",
      "tldr_zh": "该论文提出 FASTNav 方法，用于提升轻量级语言模型（SLMs）在多点机器人导航中的性能，以适应边缘设备的需求。方法包括三个模块：fine-tuning（微调）模型、teacher-student iteration（教师-学生迭代）进行知识优化，以及language-based multi-point robot navigation（基于语言的多点机器人导航）模块。实验在模拟和真实机器人环境中验证了 FASTNav 的效果，实现低成本、高准确性和低响应时间，与其他模型压缩方法相比，它更适合边缘设备上的语言引导机器人导航部署。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13262v1",
      "published_date": "2024-11-20 12:28:13 UTC",
      "updated_date": "2024-11-20 12:28:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:29:43.040169"
    },
    {
      "arxiv_id": "2411.15214v1",
      "title": "Urban Region Embeddings from Service-Specific Mobile Traffic Data",
      "title_zh": "从特定服务的移动流量数据中生成的城市区域嵌入",
      "authors": [
        "Giulio Loddi",
        "Chiara Pugliese",
        "Francesco Lettich",
        "Fabio Pinelli",
        "Chiara Renso"
      ],
      "abstract": "With the advent of advanced 4G/5G mobile networks, mobile phone data\ncollected by operators now includes detailed, service-specific traffic\ninformation with high spatio-temporal resolution. In this paper, we leverage\nthis type of data to explore its potential for generating high-quality\nrepresentations of urban regions. To achieve this, we present a methodology for\ncreating urban region embeddings from service-specific mobile traffic data,\nemploying a temporal convolutional network-based autoencoder, transformers, and\nlearnable weighted sum models to capture key urban features. In the extensive\nexperimental evaluation conducted using a real-world dataset, we demonstrate\nthat the embeddings generated by our methodology effectively capture urban\ncharacteristics. Specifically, our embeddings are compared against those of a\nstate-of-the-art competitor across two downstream tasks. Additionally, through\nclustering techniques, we investigate how well the embeddings produced by our\nmethodology capture the temporal dynamics and characteristics of the underlying\nurban regions. Overall, this work highlights the potential of service-specific\nmobile traffic data for urban research and emphasizes the importance of making\nsuch data accessible to support public innovation.",
      "tldr_zh": "本研究利用4G/5G移动网络的服务特定流量数据，提出了一种生成城市区域嵌入的方法，以捕捉高时空分辨率的都市特征。该方法结合temporal convolutional network-based autoencoder、transformers和learnable weighted sum models，对数据进行处理和特征提取。在真实数据集的实验中，该嵌入在下游任务中优于现有基准模型，并通过聚类技术有效揭示了城市区域的时序动态和特性。总体上，该工作强调了服务特定移动流量数据在城市研究中的潜力，并呼吁提高数据可访问性以促进公共创新。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15214v1",
      "published_date": "2024-11-20 12:13:07 UTC",
      "updated_date": "2024-11-20 12:13:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:29:55.105026"
    },
    {
      "arxiv_id": "2411.15212v1",
      "title": "Effective Analog ICs Floorplanning with Relational Graph Neural Networks and Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Davide Basso",
        "Luca Bortolussi",
        "Mirjana Videnovic-Misic",
        "Husni Habal"
      ],
      "abstract": "Analog integrated circuit (IC) floorplanning is typically a manual process\nwith the placement of components (devices and modules) planned by a layout\nengineer. This process is further complicated by the interdependence of\nfloorplanning and routing steps, numerous electric and layout-dependent\nconstraints, as well as the high level of customization expected in analog\ndesign. This paper presents a novel automatic floorplanning algorithm based on\nreinforcement learning. It is augmented by a relational graph convolutional\nneural network model for encoding circuit features and positional constraints.\nThe combination of these two machine learning methods enables knowledge\ntransfer across different circuit designs with distinct topologies and\nconstraints, increasing the \\emph{generalization ability} of the solution.\nApplied to $6$ industrial circuits, our approach surpassed established\nfloorplanning techniques in terms of speed, area and half-perimeter wire\nlength. When integrated into a \\emph{procedural generator} for layout\ncompletion, overall layout time was reduced by $67.3\\%$ with a $8.3\\%$ mean\narea reduction compared to manual layout.",
      "tldr_zh": "这篇论文提出了一种基于强化学习(Reinforcement Learning)的自动模拟集成电路(Analog IC)布局规划算法，并结合关系图卷积神经网络(Relational Graph Neural Networks)来编码电路特征和位置约束，从而实现知识在不同电路设计间的转移，提升了generalization ability。相比传统手动方法，该算法在6个工业电路上显著提高了速度、面积和half-perimeter wire length性能。实验结果显示，当集成到布局完成的过程生成器(procedural generator)中时，整体布局时间减少了67.3%，并实现了8.3%的平均面积减少。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "7 pages, 7 figures, Accepted at DATE25",
      "pdf_url": "http://arxiv.org/pdf/2411.15212v1",
      "published_date": "2024-11-20 12:11:12 UTC",
      "updated_date": "2024-11-20 12:11:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:30:08.012855"
    },
    {
      "arxiv_id": "2411.13251v1",
      "title": "BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D Point Cloud Semantic Segmentation",
      "title_zh": "BelHouse3D：用于评估3D点云语义分割中遮挡鲁棒性的基准数据集",
      "authors": [
        "Umamaheswaran Raman Kumar",
        "Abdur Razzaq Fayjie",
        "Jurgen Hannaert",
        "Patrick Vandewalle"
      ],
      "abstract": "Large-scale 2D datasets have been instrumental in advancing machine learning;\nhowever, progress in 3D vision tasks has been relatively slow. This disparity\nis largely due to the limited availability of 3D benchmarking datasets. In\nparticular, creating real-world point cloud datasets for indoor scene semantic\nsegmentation presents considerable challenges, including data collection within\nconfined spaces and the costly, often inaccurate process of per-point labeling\nto generate ground truths. While synthetic datasets address some of these\nchallenges, they often fail to replicate real-world conditions, particularly\nthe occlusions that occur in point clouds collected from real environments.\nExisting 3D benchmarking datasets typically evaluate deep learning models under\nthe assumption that training and test data are independently and identically\ndistributed (IID), which affects the models' usability for real-world point\ncloud segmentation. To address these challenges, we introduce the BelHouse3D\ndataset, a new synthetic point cloud dataset designed for 3D indoor scene\nsemantic segmentation. This dataset is constructed using real-world references\nfrom 32 houses in Belgium, ensuring that the synthetic data closely aligns with\nreal-world conditions. Additionally, we include a test set with data occlusion\nto simulate out-of-distribution (OOD) scenarios, reflecting the occlusions\ncommonly encountered in real-world point clouds. We evaluate popular\npoint-based semantic segmentation methods using our OOD setting and present a\nbenchmark. We believe that BelHouse3D and its OOD setting will advance research\nin 3D point cloud semantic segmentation for indoor scenes, providing valuable\ninsights for the development of more generalizable models.",
      "tldr_zh": "该研究引入了 BelHouse3D，这是一个合成点云数据集，用于评估 3D Point Cloud Semantic Segmentation 中 Occlusion 鲁棒性，旨在解决现有数据集在真实世界遮挡场景下的不足。\n数据集基于 32 座真实房屋的参考构建，确保数据接近实际条件，并包含模拟 Occlusion 的 Out-of-Distribution (OOD) 测试集。\n研究者评估了流行点云语义分割方法，并提供了基准结果，展示了模型在非独立同分布场景下的性能。\nBelHouse3D 有望推动 3D 室内场景语义分割的研究，促进更通用模型的开发。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 6 figures, 3 tables, accepted at ECCV 2024 Workshops",
      "pdf_url": "http://arxiv.org/pdf/2411.13251v1",
      "published_date": "2024-11-20 12:09:43 UTC",
      "updated_date": "2024-11-20 12:09:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:30:20.847594"
    },
    {
      "arxiv_id": "2411.13243v1",
      "title": "XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyi Wang",
        "Yanbo Wang",
        "Xumin Yu",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "Existing methodologies in open vocabulary 3D semantic segmentation primarily\nconcentrate on establishing a unified feature space encompassing 3D, 2D, and\ntextual modalities. Nevertheless, traditional techniques such as global feature\nalignment or vision-language model distillation tend to impose only approximate\ncorrespondence, struggling notably with delineating fine-grained segmentation\nboundaries. To address this gap, we propose a more meticulous mask-level\nalignment between 3D features and the 2D-text embedding space through a\ncross-modal mask reasoning framework, XMask3D. In our approach, we developed a\nmask generator based on the denoising UNet from a pre-trained diffusion model,\nleveraging its capability for precise textual control over dense pixel\nrepresentations and enhancing the open-world adaptability of the generated\nmasks. We further integrate 3D global features as implicit conditions into the\npre-trained 2D denoising UNet, enabling the generation of segmentation masks\nwith additional 3D geometry awareness. Subsequently, the generated 2D masks are\nemployed to align mask-level 3D representations with the vision-language\nfeature space, thereby augmenting the open vocabulary capability of 3D geometry\nembeddings. Finally, we fuse complementary 2D and 3D mask features, resulting\nin competitive performance across multiple benchmarks for 3D open vocabulary\nsemantic segmentation. Code is available at\nhttps://github.com/wangzy22/XMask3D.",
      "tldr_zh": "本论文提出 XMask3D 框架，通过 Cross-modal Mask Reasoning 实现 3D 特征与 2D-文本嵌入空间的精确掩码级对齐，解决现有方法在细粒度分割边界上的局限性。框架利用基于预训练扩散模型的去噪 UNet 构建掩码生成器，并将 3D 全局特征作为隐式条件融入其中，以生成具有 3D 几何感知的分割掩码，从而提升 3D 几何嵌入的开放词汇能力。最终，通过融合互补的 2D 和 3D 掩码特征，该方法在多个 3D Open Vocabulary 语义分割基准上取得竞争性性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.13243v1",
      "published_date": "2024-11-20 12:02:12 UTC",
      "updated_date": "2024-11-20 12:02:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:30:33.150678"
    },
    {
      "arxiv_id": "2411.13239v1",
      "title": "Transforming the Hybrid Cloud for Emerging AI Workloads",
      "title_zh": "为新兴 AI 工作负载改造混合云",
      "authors": [
        "Deming Chen",
        "Alaa Youssef",
        "Ruchi Pendse",
        "André Schleife",
        "Bryan K. Clark",
        "Hendrik Hamann",
        "Jingrui He",
        "Teodoro Laino",
        "Lav Varshney",
        "Yuxiong Wang",
        "Avirup Sil",
        "Reyhaneh Jabbarvand",
        "Tianyin Xu",
        "Volodymyr Kindratenko",
        "Carlos Costa",
        "Sarita Adve",
        "Charith Mendis",
        "Minjia Zhang",
        "Santiago Núñez-Corrales",
        "Raghu Ganti",
        "Mudhakar Srivatsa",
        "Nam Sung Kim",
        "Josep Torrellas",
        "Jian Huang",
        "Seetharami Seelam",
        "Klara Nahrstedt",
        "Tarek Abdelzaher",
        "Tamar Eilam",
        "Huimin Zhao",
        "Matteo Manica",
        "Ravishankar Iyer",
        "Martin Hirzel",
        "Vikram Adve",
        "Darko Marinov",
        "Hubertus Franke",
        "Hanghang Tong",
        "Elizabeth Ainsworth",
        "Han Zhao",
        "Deepak Vasisht",
        "Minh Do",
        "Fabio Oliveira",
        "Giovanni Pacifici",
        "Ruchir Puri",
        "Priya Nagpurkar"
      ],
      "abstract": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society.",
      "tldr_zh": "这篇白皮书由IBM Research和UIUC的IIDAI研究所合作撰写，旨在通过全栈协同设计方法改造hybrid cloud系统，以应对新兴AI工作负载的复杂性，强调可用性、管理性、经济性、适应性、效率和可扩展性。框架整合了generative AI、agentic AI、跨层自动化、统一控制平面以及可组合架构等技术，同时考虑量子计算的成熟应用，以提升能源效率、性能和成本效益，并支持领域如材料科学和气候建模的量子加速模拟。研究优先事项包括推进AI agentic systems、LLM as an Abstraction (LLMaaA)、AI模型优化以及量子-经典协作工作流程，通过学术和工业合作，推动hybrid cloud成为安全、可持续的平台，促进AI驱动应用和科学发现的突破。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.AR",
        "cs.ET",
        "cs.MA"
      ],
      "primary_category": "cs.DC",
      "comment": "70 pages, 27 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.13239v1",
      "published_date": "2024-11-20 11:57:43 UTC",
      "updated_date": "2024-11-20 11:57:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:30:43.422891"
    },
    {
      "arxiv_id": "2411.13225v1",
      "title": "Quantum Kernel-Based Long Short-term Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Chao Hsu",
        "Tai-Yu Li",
        "Kuan-Cheng Chen"
      ],
      "abstract": "The integration of quantum computing into classical machine learning\narchitectures has emerged as a promising approach to enhance model efficiency\nand computational capacity. In this work, we introduce the Quantum Kernel-Based\nLong Short-Term Memory (QK-LSTM) network, which utilizes quantum kernel\nfunctions within the classical LSTM framework to capture complex, non-linear\npatterns in sequential data. By embedding input data into a high-dimensional\nquantum feature space, the QK-LSTM model reduces the reliance on large\nparameter sets, achieving effective compression while maintaining accuracy in\nsequence modeling tasks. This quantum-enhanced architecture demonstrates\nefficient convergence, robust loss minimization, and model compactness, making\nit suitable for deployment in edge computing environments and resource-limited\nquantum devices (especially in the NISQ era). Benchmark comparisons reveal that\nQK-LSTM achieves performance on par with classical LSTM models, yet with fewer\nparameters, underscoring its potential to advance quantum machine learning\napplications in natural language processing and other domains requiring\nefficient temporal data processing.",
      "tldr_zh": "本研究提出Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) 模型，将量子核函数集成到经典LSTM框架中，用于捕捉序列数据中的复杂非线性模式。QK-LSTM通过将输入数据嵌入高维量子特征空间，减少了对大型参数集的依赖，实现模型压缩，同时保持在序列建模任务中的准确性。实验结果显示，该模型在基准测试中与传统LSTM性能相当，但参数更少，并展现出高效收敛和鲁棒损失最小化，适用于边缘计算和NISQ时代资源有限的量子设备，推动量子机器学习在自然语言处理等领域的发展。",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13225v1",
      "published_date": "2024-11-20 11:39:30 UTC",
      "updated_date": "2024-11-20 11:39:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:30:55.336129"
    },
    {
      "arxiv_id": "2411.15211v1",
      "title": "LightLLM: A Versatile Large Language Model for Predictive Light Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Jiawei Hu",
        "Hong Jia",
        "Mahbub Hassan",
        "Lina Yao",
        "Brano Kusy",
        "Wen Hu"
      ],
      "abstract": "We propose LightLLM, a model that fine tunes pre-trained large language\nmodels (LLMs) for light-based sensing tasks. It integrates a sensor data\nencoder to extract key features, a contextual prompt to provide environmental\ninformation, and a fusion layer to combine these inputs into a unified\nrepresentation. This combined input is then processed by the pre-trained LLM,\nwhich remains frozen while being fine-tuned through the addition of\nlightweight, trainable components, allowing the model to adapt to new tasks\nwithout altering its original parameters. This approach enables flexible\nadaptation of LLM to specialized light sensing tasks with minimal computational\noverhead and retraining effort. We have implemented LightLLM for three light\nsensing tasks: light-based localization, outdoor solar forecasting, and indoor\nsolar estimation. Using real-world experimental datasets, we demonstrate that\nLightLLM significantly outperforms state-of-the-art methods, achieving 4.4x\nimprovement in localization accuracy and 3.4x improvement in indoor solar\nestimation when tested in previously unseen environments. We further\ndemonstrate that LightLLM outperforms ChatGPT-4 with direct prompting,\nhighlighting the advantages of LightLLM's specialized architecture for sensor\ndata fusion with textual prompts.",
      "tldr_zh": "本研究提出LightLLM，一种多功能大型语言模型(LLMs)，通过微调预训练模型来处理预测性光传感任务。该模型整合传感器数据编码器提取关键特征、上下文提示提供环境信息，以及融合层将这些输入统一处理，同时保持预训练LLM冻结，只添加轻量级可训练组件，以实现高效适应新任务。LightLLM应用于光-based定位、户外太阳能预测和室内太阳能估计三个任务，在真实数据集上表现出色，定位准确率较现有方法提高4.4倍，室内太阳能估计提高3.4倍，并优于ChatGPT-4的直接提示方式，展示了其在传感器数据与文本提示融合方面的优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages, 14 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.15211v1",
      "published_date": "2024-11-20 11:37:33 UTC",
      "updated_date": "2024-11-20 11:37:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:31:07.907822"
    },
    {
      "arxiv_id": "2411.14489v1",
      "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
      "title_zh": "翻译失败",
      "authors": [
        "Hang Zhou",
        "Xiaoxu Zheng",
        "Yunhe Wang",
        "Michael Bi Mi",
        "Deyi Xiong",
        "Kai Han"
      ],
      "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance\ndependencies are widely used in various speech tasks, eg., keyword spotting\n(KWS) and speech enhancement (SE). Due to the limitation of power and memory in\nlow-resource devices, efficient RNN models are urgently required for real-world\napplications. In this paper, we propose an efficient RNN architecture,\nGhostRNN, which reduces hidden state redundancy with cheap operations. In\nparticular, we observe that partial dimensions of hidden states are similar to\nthe others in trained RNN models, suggesting that redundancy exists in specific\nRNNs. To reduce the redundancy and hence computational cost, we propose to\nfirst generate a few intrinsic states, and then apply cheap operations to\nproduce ghost states based on the intrinsic states. Experiments on KWS and SE\ntasks demonstrate that the proposed GhostRNN significantly reduces the memory\nusage (~40%) and computation cost while keeping performance similar.",
      "tldr_zh": "本论文针对 RNN 在语音任务（如 KWS 和 SE）中存在的隐藏状态冗余问题，提出了一种高效架构 GhostRNN，通过廉价操作减少计算成本。具体方法是先生成少量内在状态（intrinsic states），然后基于这些状态扩展产生 ghost states，以优化资源利用。实验结果显示，GhostRNN 在 KWS 和 SE 任务上降低了约40%的内存使用和计算开销，同时保持了与原始模型相似的性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14489v1",
      "published_date": "2024-11-20 11:37:14 UTC",
      "updated_date": "2024-11-20 11:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:31:20.060555"
    },
    {
      "arxiv_id": "2411.13223v1",
      "title": "Existential Conversations with Large Language Models: Content, Community, and Culture",
      "title_zh": "翻译失败",
      "authors": [
        "Murray Shanahan",
        "Beth Singler"
      ],
      "abstract": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在存在性对话中的表现，包括哲学、精神和宗教等主题。通过分析两个长对话，论文追踪了LLMs使用的图像、神话、隐喻和概念的来源（从古代到现代），并强调这些元素在当代社区和文化运动中的应用，尤其是在线活动。研究发现，LLMs能有效整合这些资源，但也引发了对社会影响的担忧，如对人类意识和宇宙角色的讨论可能加剧文化动态和伦理挑战。总的来说，这为理解LLMs在深层对话中的角色提供了新见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13223v1",
      "published_date": "2024-11-20 11:35:22 UTC",
      "updated_date": "2024-11-20 11:35:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:31:30.905393"
    },
    {
      "arxiv_id": "2411.13215v1",
      "title": "Proceedings Sixth International Workshop on Formal Methods for Autonomous Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Matt Luckcuck",
        "Mengwei Xu"
      ],
      "abstract": "This EPTCS volume contains the papers from the Sixth International Workshop\non Formal Methods for Autonomous Systems (FMAS 2024), which was held between\nthe 11th and 13th of November 2024. FMAS 2024 was co-located with 19th\nInternational Conference on integrated Formal Methods (iFM'24), hosted by the\nUniversity of Manchester in the United Kingdom, in the University of\nManchester's Core Technology Facility.",
      "tldr_zh": "这本EPTCS卷包含第六届国际研讨会Formal Methods for Autonomous Systems (FMAS 2024)的论文集，聚焦于自治系统的形式方法研究。FMAS 2024于2024年11月11-13日在英国曼彻斯特大学举办，与第19届Integrated Formal Methods会议(iFM'24)共同举办。该论文集汇集了相关领域的最新学术贡献，促进了形式方法在自治系统中的应用和发展。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13215v1",
      "published_date": "2024-11-20 11:21:22 UTC",
      "updated_date": "2024-11-20 11:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:31:43.814034"
    },
    {
      "arxiv_id": "2411.13209v1",
      "title": "Comparative Analysis of Audio Feature Extraction for Real-Time Talking Portrait Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Pegah Salehi",
        "Sajad Amouei Sheshkal",
        "Vajira Thambawita",
        "Sushant Gautam",
        "Saeed S. Sabet",
        "Dag Johansen",
        "Michael A. Riegler",
        "Pål Halvorsen"
      ],
      "abstract": "This paper examines the integration of real-time talking-head generation for\ninterviewer training, focusing on overcoming challenges in Audio Feature\nExtraction (AFE), which often introduces latency and limits responsiveness in\nreal-time applications. To address these issues, we propose and implement a\nfully integrated system that replaces conventional AFE models with Open AI's\nWhisper, leveraging its encoder to optimize processing and improve overall\nsystem efficiency. Our evaluation of two open-source real-time models across\nthree different datasets shows that Whisper not only accelerates processing but\nalso improves specific aspects of rendering quality, resulting in more\nrealistic and responsive talking-head interactions. These advancements make the\nsystem a more effective tool for immersive, interactive training applications,\nexpanding the potential of AI-driven avatars in interviewer training.",
      "tldr_zh": "本论文比较分析了音频特征提取（Audio Feature Extraction, AFE）在实时说话头像合成中的应用，针对其导致的延迟和响应性问题，提出了一种集成系统，使用 OpenAI's Whisper 的编码器替换传统 AFE 模型，以优化处理效率。实验评估了两个开源实时模型在三个数据集上的性能，结果显示 Whisper 显著加速了处理过程，并提升了渲染质量，使说话头像互动更真实和响应。总体而言，此创新为 AI 驱动头像在面试官训练等沉浸式应用中提供了更有效的工具。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.HC",
        "eess.AS",
        "68T45, 68T07, 68T01"
      ],
      "primary_category": "cs.SD",
      "comment": "16 pages, 6 figures, 3 tables. submitted to MDPI journal in as Big\n  Data and Cognitive Computing",
      "pdf_url": "http://arxiv.org/pdf/2411.13209v1",
      "published_date": "2024-11-20 11:18:05 UTC",
      "updated_date": "2024-11-20 11:18:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:31:55.241928"
    },
    {
      "arxiv_id": "2411.13207v1",
      "title": "The Information Security Awareness of Large Language Models",
      "title_zh": "大型语言模型的信息安全意识",
      "authors": [
        "Ofir Cohen",
        "Gil Ari Agmon",
        "Asaf Shabtai",
        "Rami Puzis"
      ],
      "abstract": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）的信息安全意识（ISA），强调了现有LLMs在安全方面存在的不足，可能导致用户不安全行为。研究者创建了30个场景集，基于移动ISA分类的焦点领域，对多种LLMs进行了全面评估，发现大多数模型需要明确的security context提示才能提供安全响应。结果显示，调整模型的temperature对ISA影响较小，但优化system prompt能显著提升安全性能，这突出了在开发未来LLM-based助手时加强ISA评估的必要性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13207v1",
      "published_date": "2024-11-20 11:09:55 UTC",
      "updated_date": "2024-11-20 11:09:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:32:07.339422"
    },
    {
      "arxiv_id": "2411.15210v4",
      "title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
      "title_zh": "迈向百万规模的对抗鲁棒性评估：利用更强的单个攻击",
      "authors": [
        "Yong Xie",
        "Weijie Zheng",
        "Hanxun Huang",
        "Guangnan Ye",
        "Xingjun Ma"
      ],
      "abstract": "As deep learning models are increasingly deployed in safety-critical\napplications, evaluating their vulnerabilities to adversarial perturbations is\nessential for ensuring their reliability and trustworthiness. Over the past\ndecade, a large number of white-box adversarial robustness evaluation methods\n(i.e., attacks) have been proposed, ranging from single-step to multi-step\nmethods and from individual to ensemble methods. Despite these advances,\nchallenges remain in conducting meaningful and comprehensive robustness\nevaluations, particularly when it comes to large-scale testing and ensuring\nevaluations reflect real-world adversarial risks. In this work, we focus on\nimage classification models and propose a novel individual attack method,\nProbability Margin Attack (PMA), which defines the adversarial margin in the\nprobability space rather than the logits space. We analyze the relationship\nbetween PMA and existing cross-entropy or logits-margin-based attacks, and show\nthat PMA can outperform the current state-of-the-art individual methods.\nBuilding on PMA, we propose two types of ensemble attacks that balance\neffectiveness and efficiency. Furthermore, we create a million-scale dataset,\nCC1M, derived from the existing CC3M dataset, and use it to conduct the first\nmillion-scale white-box adversarial robustness evaluation of\nadversarially-trained ImageNet models. Our findings provide valuable insights\ninto the robustness gaps between individual versus ensemble attacks and\nsmall-scale versus million-scale evaluations.",
      "tldr_zh": "本研究针对深度学习模型在安全关键应用中的对抗鲁棒性评估，提出了一种新颖的个体攻击方法——Probability Margin Attack (PMA)，它在概率空间定义对抗边界，并证明了其优于现有基于交叉熵或 logits 边界的攻击。基于 PMA，该工作设计了两种平衡有效性和效率的集成攻击，并创建了百万规模数据集 CC1M（源自 CC3M），用于对 adversarially-trained ImageNet 模型进行首次百万规模的白盒对抗鲁棒性评估。实验结果揭示了个体攻击与集成攻击之间、以及小规模与百万规模评估之间的鲁棒性差距，为更真实可靠的对抗风险评估提供了宝贵见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15210v4",
      "published_date": "2024-11-20 10:41:23 UTC",
      "updated_date": "2025-03-11 02:56:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:32:21.608122"
    },
    {
      "arxiv_id": "2411.13187v3",
      "title": "Engagement-Driven Content Generation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Erica Coppolillo",
        "Federico Cinus",
        "Marco Minici",
        "Francesco Bonchi",
        "Giuseppe Manco"
      ],
      "abstract": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 在社交网络中生成内容以最大化用户参与度的潜力，针对 LLMs 在互联用户和复杂意见动态环境下的影响进行研究。研究提出了一种基于 reinforcement learning 的框架，使用模拟反馈和一个从意见动态文献借用的 engagement model 作为奖励机制，同时确保生成的内容与给定主题对齐并满足最低 fluency requirement。实验结果表明，LLMs 能够有效创建高参与度内容，该方法适应网络意见分布且对 engagement model 细节不敏感，便于扩展到更复杂的计算社会科学任务中。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13187v3",
      "published_date": "2024-11-20 10:40:08 UTC",
      "updated_date": "2024-11-22 13:05:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:32:31.815379"
    },
    {
      "arxiv_id": "2411.13181v1",
      "title": "Cross-Camera Distracted Driver Classification through Feature Disentanglement and Contrastive Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Simone Bianco",
        "Luigi Celona",
        "Paolo Napoletano"
      ],
      "abstract": "The classification of distracted drivers is pivotal for ensuring safe\ndriving. Previous studies demonstrated the effectiveness of neural networks in\nautomatically predicting driver distraction, fatigue, and potential hazards.\nHowever, recent research has uncovered a significant loss of accuracy in these\nmodels when applied to samples acquired under conditions that differ from the\ntraining data. In this paper, we introduce a robust model designed to withstand\nchanges in camera position within the vehicle. Our Driver Behavior Monitoring\nNetwork (DBMNet) relies on a lightweight backbone and integrates a\ndisentanglement module to discard camera view information from features,\ncoupled with contrastive learning to enhance the encoding of various driver\nactions. Experiments conducted on the daytime and nighttime subsets of the\n100-Driver dataset validate the effectiveness of our approach with an increment\non average of 9\\% in Top-1 accuracy in comparison with the state of the art. In\naddition, cross-dataset and cross-camera experiments conducted on three\nbenchmark datasets, namely AUCDD-V1, EZZ2021 and SFD, demonstrate the superior\ngeneralization capability of the proposed method.",
      "tldr_zh": "该论文针对驾驶员分心分类问题，提出了一种鲁棒模型Driver Behavior Monitoring Network (DBMNet)，通过feature disentanglement模块去除特征中的相机视图信息，并结合contrastive learning增强对各种驾驶员动作的编码，以应对不同相机位置导致的准确率下降。DBMNet采用轻量级backbone，确保模型效率。实验在100-Driver数据集的白天和夜间子集上显示，Top-1准确率平均提高了9%，并在跨数据集（如AUCDD-V1、EZZ2021和SFD）和跨相机实验中展现出优越的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13181v1",
      "published_date": "2024-11-20 10:27:12 UTC",
      "updated_date": "2024-11-20 10:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:32:44.140135"
    },
    {
      "arxiv_id": "2411.13173v2",
      "title": "Writing Style Matters: An Examination of Bias and Fairness in Information Retrieval Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Hongliu Cao"
      ],
      "abstract": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
      "tldr_zh": "这篇论文探讨了信息检索（IR）系统中的偏见和公平性问题，重点考察了文本嵌入模型对文档和查询写作风格的潜在偏好。研究发现，大多数模型偏好正式风格而非非正式或情感化的风格，并在查询风格匹配时表现出特定偏向，尤其是在使用 LLM 生成的合成数据微调的模型中。这些偏见可能导致某些沟通方式被边缘化，威胁 IR 系统的公平性，最终通过比较 Retrieval Augmented Generation (RAG) 系统，提供宝贵见解以开发更公平的模型。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
      "pdf_url": "http://arxiv.org/pdf/2411.13173v2",
      "published_date": "2024-11-20 10:17:09 UTC",
      "updated_date": "2024-12-12 10:22:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:32:56.127769"
    },
    {
      "arxiv_id": "2411.15208v1",
      "title": "M2oE: Multimodal Collaborative Expert Peptide Model",
      "title_zh": "M2oE：多模态协作专家肽模型",
      "authors": [
        "Zengzhu Guo",
        "Zhiqi Ma"
      ],
      "abstract": "Peptides are biomolecules comprised of amino acids that play an important\nrole in our body. In recent years, peptides have received extensive attention\nin drug design and synthesis, and peptide prediction tasks help us better\nsearch for functional peptides. Typically, we use the primary sequence and\nstructural information of peptides for model encoding. However, recent studies\nhave focused more on single-modal information (structure or sequence) for\nprediction without multi-modal approaches. We found that single-modal models\nare not good at handling datasets with less information in that particular\nmodality. Therefore, this paper proposes the M2oE multi-modal collaborative\nexpert peptide model. Based on previous work, by integrating sequence and\nspatial structural information, employing expert model and Cross-Attention\nMechanism, the model's capabilities are balanced and improved. Experimental\nresults indicate that the M2oE model performs excellently in complex task\npredictions.",
      "tldr_zh": "本论文提出 M2oE 多模态协作专家肽模型，以解决现有肽（peptides）预测模型依赖单模态信息（如序列或结构）的问题，导致在信息不足的模态上表现不佳。M2oE 通过整合肽的序列和空间结构信息，结合专家模型和 Cross-Attention Mechanism，实现多模态信息的平衡与提升。实验结果显示，该模型在复杂肽预测任务中表现出色，显著提高了预测准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by bibm 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.15208v1",
      "published_date": "2024-11-20 09:52:52 UTC",
      "updated_date": "2024-11-20 09:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:33:07.192204"
    },
    {
      "arxiv_id": "2411.13619v1",
      "title": "Non-Linear Outlier Synthesis for Out-of-Distribution Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Lars Doorenbos",
        "Raphael Sznitman",
        "Pablo Márquez-Neila"
      ],
      "abstract": "The reliability of supervised classifiers is severely hampered by their\nlimitations in dealing with unexpected inputs, leading to great interest in\nout-of-distribution (OOD) detection. Recently, OOD detectors trained on\nsynthetic outliers, especially those generated by large diffusion models, have\nshown promising results in defining robust OOD decision boundaries. Building on\nthis progress, we present NCIS, which enhances the quality of synthetic\noutliers by operating directly in the diffusion's model embedding space rather\nthan combining disjoint models as in previous work and by modeling\nclass-conditional manifolds with a conditional volume-preserving network for\nmore expressive characterization of the training distribution. We demonstrate\nthat these improvements yield new state-of-the-art OOD detection results on\nstandard ImageNet100 and CIFAR100 benchmarks and provide insights into the\nimportance of data pre-processing and other key design choices. We make our\ncode available at \\url{https://github.com/LarsDoorenbos/NCIS}.",
      "tldr_zh": "本文提出NCIS方法，用于提升Out-of-Distribution (OOD) 检测的性能，通过在diffusion模型的嵌入空间中直接生成非线性合成异常值，并使用条件体积保持网络建模类条件流形，以更精确地表征训练分布。相比以往组合离散模型的方法，NCIS显著提高了合成异常值的质量，并在ImageNet100和CIFAR100基准测试中实现了新的最先进结果。研究还提供了数据预处理和其他关键设计选择的见解，并开源了代码以促进进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13619v1",
      "published_date": "2024-11-20 09:47:29 UTC",
      "updated_date": "2024-11-20 09:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:33:19.737123"
    },
    {
      "arxiv_id": "2411.13157v2",
      "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative Decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Hyun Ryu",
        "Eric Kim"
      ],
      "abstract": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.",
      "tldr_zh": "这篇论文调查了推测性解码(Speculative Decoding)方法，以优化大型语言模型(LLMs)的推理效率。传统自回归解码(Autoregressive Decoding)因其顺序生成令牌的过程而计算效率低下，而推测性解码通过引入草拟(drafting)和验证(verification)两个阶段，使用较小模型生成初步草稿，再由较大模型精炼，从而缓解这一问题。论文将这些方法分类为草稿中心(draft-centric)和模型中心(model-centric)方法，并讨论了每种方法的关键想法及其在扩展LLM推理方面的潜力。该调查旨在指导未来研究，进一步优化推测性解码并将其整合到实际LLM应用中。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13157v2",
      "published_date": "2024-11-20 09:46:30 UTC",
      "updated_date": "2024-11-27 03:25:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:33:31.504958"
    },
    {
      "arxiv_id": "2411.13154v1",
      "title": "DMQR-RAG: Diverse Multi-Query Rewriting for RAG",
      "title_zh": "翻译失败",
      "authors": [
        "Zhicong Li",
        "Jiahao Wang",
        "Zhishu Jiang",
        "Hangyu Mao",
        "Zhongxia Chen",
        "Jiazhen Du",
        "Yuanxing Zhang",
        "Fuzheng Zhang",
        "Di Zhang",
        "Yong Liu"
      ],
      "abstract": "Large language models often encounter challenges with static knowledge and\nhallucinations, which undermine their reliability. Retrieval-augmented\ngeneration (RAG) mitigates these issues by incorporating external information.\nHowever, user queries frequently contain noise and intent deviations,\nnecessitating query rewriting to improve the relevance of retrieved documents.\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\ndesigned to improve the performance of both document retrieval and final\nresponses in RAG. Specifically, we investigate how queries with varying\ninformation quantities can retrieve a diverse array of documents, presenting\nfour rewriting strategies that operate at different levels of information to\nenhance the performance of baseline approaches. Additionally, we propose an\nadaptive strategy selection method that minimizes the number of rewrites while\noptimizing overall performance. Our methods have been rigorously validated\nthrough extensive experiments conducted in both academic and industry settings.",
      "tldr_zh": "大型语言模型（Large Language Models）常因静态知识和幻觉问题而可靠性不足，RAG（Retrieval-Augmented Generation）通过外部信息缓解此问题，但用户查询中的噪声和意图偏差需通过查询重写来优化文档相关性。本文引入DMQR-RAG框架，提出四种不同信息级别的查询重写策略，以实现多样化文档检索，并设计自适应策略选择方法来最小化重写次数同时提升整体性能。通过学术和行业环境的广泛实验验证，DMQR-RAG显著提高了基线方法的检索和响应表现。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13154v1",
      "published_date": "2024-11-20 09:43:30 UTC",
      "updated_date": "2024-11-20 09:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:33:43.567657"
    },
    {
      "arxiv_id": "2411.15207v1",
      "title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language Pre-training",
      "title_zh": "翻译失败",
      "authors": [
        "Ameera Bawazir",
        "Kebin Wu",
        "Wenbin Li"
      ],
      "abstract": "Recent advancements in vision-language pre-training via contrastive learning\nhave significantly improved performance across computer vision tasks. However,\nin the medical domain, obtaining multimodal data is often costly and\nchallenging due to privacy, sensitivity, and annotation complexity. To mitigate\ndata scarcity while boosting model performance, we introduce \\textbf{Uni-Mlip},\na unified self-supervision framework specifically designed to enhance medical\nvision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,\nuni-modality, and fused-modality self-supervision techniques at the data-level\nand the feature-level. Additionally, Uni-Mlip tailors uni-modal image\nself-supervision to accommodate the unique characteristics of medical images.\nOur experiments across datasets of varying scales demonstrate that Uni-Mlip\nsignificantly surpasses current state-of-the-art methods in three key\ndownstream tasks: image-text retrieval, image classification, and visual\nquestion answering (VQA).",
      "tldr_zh": "本文提出 Uni-Mlip，一种统一的自我监督框架，用于提升医疗视觉语言预训练（vision-language pre-training），以应对数据稀缺和获取挑战。该框架在数据级和特征级整合了跨模态（cross-modality）、单模态（uni-modality）和融合模态（fused-modality）自监督技术，并针对医疗图像的独特特性进行了优化。实验结果显示，Uni-Mlip 在不同规模的数据集上，在图像文本检索（image-text retrieval）、图像分类（image classification）和视觉问答（VQA）等下游任务中，显著超过了现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 2 figures, accepted by BMVC'24",
      "pdf_url": "http://arxiv.org/pdf/2411.15207v1",
      "published_date": "2024-11-20 09:43:26 UTC",
      "updated_date": "2024-11-20 09:43:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:33:55.781943"
    },
    {
      "arxiv_id": "2411.13152v2",
      "title": "AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation",
      "title_zh": "AGLP:",
      "authors": [
        "Houcheng Su",
        "Mengzhu Wang",
        "Jiao Li",
        "Nan Yin",
        "Liang Yang",
        "Li Shen"
      ],
      "abstract": "In semi-supervised domain adaptation (SSDA), the model aims to leverage\npartially labeled target domain data along with a large amount of labeled\nsource domain data to enhance its generalization capability for the target\ndomain. A key advantage of SSDA is its ability to significantly reduce reliance\non labeled data, thereby lowering the costs and time associated with data\npreparation. Most existing SSDA methods utilize information from domain labels\nand class labels but overlook the structural information of the data. To\naddress this issue, this paper proposes a graph learning perspective (AGLP) for\nsemi-supervised domain adaptation. We apply the graph convolutional network to\nthe instance graph which allows structural information to propagate along the\nweighted graph edges. The proposed AGLP model has several advantages. First, to\nthe best of our knowledge, this is the first work to model structural\ninformation in SSDA. Second, the proposed model can effectively learn\ndomain-invariant and semantic representations, reducing domain discrepancies in\nSSDA. Extensive experimental results on multiple standard benchmarks\ndemonstrate that the proposed AGLP algorithm outperforms state-of-the-art\nsemi-supervised domain adaptation methods.",
      "tldr_zh": "本研究针对半监督域适应 (SSDA) 的问题，提出了一种基于图学习视角的框架 AGLP，利用图卷积网络 (Graph Convolutional Network) 应用于实例图，以传播结构信息并减少域差异。AGLP 首次在 SSDA 中整合数据结构信息，有效学习域不变和语义表示，从而降低对标记数据的依赖并提升模型泛化能力。在多个标准基准实验中，AGLP 算法表现出色，优于现有最先进的方法。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 92C55, 62H35",
        "I.2.6; I.4.10; J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "8page",
      "pdf_url": "http://arxiv.org/pdf/2411.13152v2",
      "published_date": "2024-11-20 09:41:41 UTC",
      "updated_date": "2024-11-22 09:21:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:34:06.729300"
    },
    {
      "arxiv_id": "2411.13149v1",
      "title": "YCB-LUMA: YCB Object Dataset with Luminance Keying for Object Localization",
      "title_zh": "YCB-LUMA：YCB 物体数据集，带亮度键控用于物体定位",
      "authors": [
        "Thomas Pöllabauer"
      ],
      "abstract": "Localizing target objects in images is an important task in computer vision.\nOften it is the first step towards solving a variety of applications in\nautonomous driving, maintenance, quality insurance, robotics, and augmented\nreality. Best in class solutions for this task rely on deep neural networks,\nwhich require a set of representative training data for best performance.\nCreating sets of sufficient quality, variety, and size is often difficult,\nerror prone, and expensive. This is where the method of luminance keying can\nhelp: it provides a simple yet effective solution to record high quality data\nfor training object detection and segmentation. We extend previous work that\npresented luminance keying on the common YCB-V set of household objects by\nrecording the remaining objects of the YCB superset. The additional variety of\nobjects - addition of transparency, multiple color variations, non-rigid\nobjects - further demonstrates the usefulness of luminance keying and might be\nused to test the applicability of the approach on new 2D object detection and\nsegmentation algorithms.",
      "tldr_zh": "该论文介绍了 YCB-LUMA 数据集，这是一个扩展自 YCB 对象数据集的资源，使用 luminance keying 技术来录制高质量训练数据，以提升图像中对象 localization 的性能。作者强调了创建多样化数据集的挑战，并展示了 luminance keying 作为一种简单有效的解决方案，能处理对象检测和 segmentation 任务。YCB-LUMA 增加了透明物体、多色变体和非刚性物体的多样性，进一步验证了该方法的适用性，并为测试新 2D 对象检测和分割算法提供了宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13149v1",
      "published_date": "2024-11-20 09:32:22 UTC",
      "updated_date": "2024-11-20 09:32:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:34:20.291264"
    },
    {
      "arxiv_id": "2411.13147v2",
      "title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation",
      "title_zh": "翻译失败",
      "authors": [
        "Mengzhu Wang",
        "Jiao Li",
        "Houcheng Su",
        "Nan Yin",
        "Liang Yang",
        "Shen Li"
      ],
      "abstract": "Semi-supervised learning (SSL) has made notable advancements in medical image\nsegmentation (MIS), particularly in scenarios with limited labeled data and\nsignificantly enhancing data utilization efficiency. Previous methods primarily\nfocus on complex training strategies to utilize unlabeled data but neglect the\nimportance of graph structural information. Different from existing methods, we\npropose a graph-based clustering for semi-supervised medical image segmentation\n(GraphCL) by jointly modeling graph data structure in a unified deep model. The\nproposed GraphCL model enjoys several advantages. Firstly, to the best of our\nknowledge, this is the first work to model the data structure information for\nsemi-supervised medical image segmentation (SSMIS). Secondly, to get the\nclustered features across different graphs, we integrate both pairwise\naffinities between local image features and raw features as inputs. Extensive\nexperimental results on three standard benchmarks show that the proposed\nGraphCL algorithm outperforms state-of-the-art semi-supervised medical image\nsegmentation methods.",
      "tldr_zh": "该研究提出了一种基于图的聚类方法GraphCL，用于半监督医疗图像分割（Semi-supervised Medical Image Segmentation, SSMIS），旨在通过联合建模图数据结构来提升有限标签数据的利用效率。GraphCL首次在SSMIS中整合图结构信息，将局部图像特征的成对亲和力（pairwise affinities）和原始特征作为输入，以获取跨不同图的聚类特征。实验结果显示，该方法在三个标准基准上优于现有最先进的方法，证明了其在提高数据利用和分割性能方面的显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "68T07, 92C55, 62H35",
        "I.2.6; I.4.10; J.3"
      ],
      "primary_category": "cs.CV",
      "comment": "9page",
      "pdf_url": "http://arxiv.org/pdf/2411.13147v2",
      "published_date": "2024-11-20 09:24:46 UTC",
      "updated_date": "2024-11-22 09:18:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:34:31.114544"
    },
    {
      "arxiv_id": "2411.13144v1",
      "title": "CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models",
      "title_zh": "CopyrightMeter: 重新审视文本到图像模型中的版权保护",
      "authors": [
        "Naen Xu",
        "Changjiang Li",
        "Tianyu Du",
        "Minxi Li",
        "Wenjie Luo",
        "Jiacheng Liang",
        "Yuyuan Li",
        "Xuhong Zhang",
        "Meng Han",
        "Jianwei Yin",
        "Ting Wang"
      ],
      "abstract": "Text-to-image diffusion models have emerged as powerful tools for generating\nhigh-quality images from textual descriptions. However, their increasing\npopularity has raised significant copyright concerns, as these models can be\nmisused to reproduce copyrighted content without authorization. In response,\nrecent studies have proposed various copyright protection methods, including\nadversarial perturbation, concept erasure, and watermarking techniques.\nHowever, their effectiveness and robustness against advanced attacks remain\nlargely unexplored. Moreover, the lack of unified evaluation frameworks has\nhindered systematic comparison and fair assessment of different approaches. To\nbridge this gap, we systematize existing copyright protection methods and\nattacks, providing a unified taxonomy of their design spaces. We then develop\nCopyrightMeter, a unified evaluation framework that incorporates 17\nstate-of-the-art protections and 16 representative attacks. Leveraging\nCopyrightMeter, we comprehensively evaluate protection methods across multiple\ndimensions, thereby uncovering how different design choices impact fidelity,\nefficacy, and resilience under attacks. Our analysis reveals several key\nfindings: (i) most protections (16/17) are not resilient against attacks; (ii)\nthe \"best\" protection varies depending on the target priority; (iii) more\nadvanced attacks significantly promote the upgrading of protections. These\ninsights provide concrete guidance for developing more robust protection\nmethods, while its unified evaluation protocol establishes a standard benchmark\nfor future copyright protection research in text-to-image generation.",
      "tldr_zh": "这篇论文重新审视了文本到图像扩散模型中的版权保护问题，系统化了现有方法如adversarial perturbation、concept erasure和watermarking，以及相关攻击，提供了一个统一的分类框架。研究者开发了CopyrightMeter，一个统一的评估框架，涵盖17种保护方法和16种攻击，对这些方法的保真度、有效性和抗攻击性进行了全面评估。结果显示，大多数保护方法（16/17）不耐攻击，最佳保护取决于目标优先级，而更先进的攻击能促进保护技术的升级。这些发现为开发更鲁棒的版权保护方法提供了指导，并建立了标准基准。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13144v1",
      "published_date": "2024-11-20 09:19:10 UTC",
      "updated_date": "2024-11-20 09:19:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:34:43.785609"
    },
    {
      "arxiv_id": "2411.13116v1",
      "title": "Provably Efficient Action-Manipulation Attack Against Continuous Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhi Luo",
        "Xiyuan Yang",
        "Pan Zhou",
        "Di Wang"
      ],
      "abstract": "Manipulating the interaction trajectories between the intelligent agent and\nthe environment can control the agent's training and behavior, exposing the\npotential vulnerabilities of reinforcement learning (RL). For example, in\nCyber-Physical Systems (CPS) controlled by RL, the attacker can manipulate the\nactions of the adopted RL to other actions during the training phase, which\nwill lead to bad consequences. Existing work has studied action-manipulation\nattacks in tabular settings, where the states and actions are discrete. As seen\nin many up-and-coming RL applications, such as autonomous driving, continuous\naction space is widely accepted, however, its action-manipulation attacks have\nnot been thoroughly investigated yet. In this paper, we consider this crucial\nproblem in both white-box and black-box scenarios. Specifically, utilizing the\nknowledge derived exclusively from trajectories, we propose a black-box attack\nalgorithm named LCBT, which uses the Monte Carlo tree search method for\nefficient action searching and manipulation. Additionally, we demonstrate that\nfor an agent whose dynamic regret is sub-linearly related to the total number\nof steps, LCBT can teach the agent to converge to target policies with only\nsublinear attack cost, i.e., $O\\left(\\mathcal{R}(T) + MH^3K^E\\log\n(MT)\\right)(0<E<1)$, where $H$ is the number of steps per episode, $K$ is the\ntotal number of episodes, $T=KH$ is the total number of steps, $M$ is the\nnumber of subspaces divided in the state space, and $\\mathcal{R}(T)$ is the\nbound of the RL algorithm's regret. We conduct our proposed attack methods on\nthree aggressive algorithms: DDPG, PPO, and TD3 in continuous settings, which\nshow a promising attack performance.",
      "tldr_zh": "这篇论文探讨了针对连续强化学习（RL）的行动操纵攻击，揭示了RL代理在训练过程中易受操纵的漏洞，例如在自动驾驶等应用中。作者提出了一种黑盒攻击算法LCBT，利用Monte Carlo tree search方法来高效搜索和操纵代理的行动，并在白盒场景中扩展分析。论文证明了LCBT能以次线性攻击成本（O(R(T) + MH^3 K^E log(MT)））引导代理收敛到目标策略，并在DDPG、PPO和TD3算法上实验验证了其高效性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13116v1",
      "published_date": "2024-11-20 08:20:29 UTC",
      "updated_date": "2024-11-20 08:20:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:34:55.609233"
    },
    {
      "arxiv_id": "2411.13100v1",
      "title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control",
      "title_zh": "翻译失败",
      "authors": [
        "Yunkee Chae",
        "Eunsik Shin",
        "Hwang Suntae",
        "Seungryeol Paik",
        "Kyogu Lee"
      ],
      "abstract": "Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999",
      "tldr_zh": "该论文针对歌词生成中的音节控制挑战，提出了一种考虑歌曲形式（song form）的完整歌词生成框架，以多级别粒度音节计数控制（multi-level granularity syllable count control）为核心。框架在词、短语、行和段落级别实现精细音节管理，确保生成的歌词与输入文本和歌曲结构（如 verses 和 choruses）高度一致，避免了传统逐行方法的 unnatural phrasing 问题。通过这种方法，论文成功生成完整的歌词样本，可访问链接：https://tinyurl.com/lyrics9999。总的来说，该框架提升了歌词生成的精确性和自然性，为文本到歌词转换（text-to-lyrics generation）提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13100v1",
      "published_date": "2024-11-20 07:57:58 UTC",
      "updated_date": "2024-11-20 07:57:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:35:07.434113"
    },
    {
      "arxiv_id": "2411.13093v3",
      "title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension",
      "title_zh": "Video-RAG：视觉对齐的检索增强长视频理解",
      "authors": [
        "Yongdong Luo",
        "Xiawu Zheng",
        "Xiao Yang",
        "Guilin Li",
        "Haojia Lin",
        "Jinfa Huang",
        "Jiayi Ji",
        "Fei Chao",
        "Jiebo Luo",
        "Rongrong Ji"
      ],
      "abstract": "Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.",
      "tldr_zh": "本文提出 Video-RAG，一种无需训练且成本有效的框架，用于提升大型视频语言模型（LVLMs）在长视频理解中的性能，通过提取视觉对齐的辅助文本（如音频、光学字符和物体检测）来增强跨模态对齐和额外信息提供。不同于微调 LVLMs 或依赖专有模型如 GPT-4o，该方法以插件式方式整合辅助文本到现有 LVLM 中，实现轻量级计算和易兼容性。在 Video-MME、MLVU 和 LongVideoBench 等基准上，Video-RAG 显著提升了性能，甚至在使用 72B 模型时超越 Gemini-1.5-Pro 和 GPT-4o。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.13093v3",
      "published_date": "2024-11-20 07:44:34 UTC",
      "updated_date": "2024-12-20 12:09:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:35:20.210723"
    },
    {
      "arxiv_id": "2411.15206v2",
      "title": "Conditional Distribution Learning on Graphs",
      "title_zh": "基于图的条件分布学习",
      "authors": [
        "Jie Chen",
        "Hua Mao",
        "Yuanbiao Gou",
        "Zhu Wang",
        "Xi Peng"
      ],
      "abstract": "Leveraging the diversity and quantity of data provided by various\ngraph-structured data augmentations while preserving intrinsic semantic\ninformation is challenging. Additionally, successive layers in graph neural\nnetwork (GNN) tend to produce more similar node embeddings, while graph\ncontrastive learning aims to increase the dissimilarity between negative pairs\nof node embeddings. This inevitably results in a conflict between the\nmessage-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of\nnegative pairs via intraviews. In this paper, we propose a conditional\ndistribution learning (CDL) method that learns graph representations from\ngraph-structured data for semisupervised graph classification. Specifically, we\npresent an end-to-end graph representation learning model to align the\nconditional distributions of weakly and strongly augmented features over the\noriginal features. This alignment enables the CDL model to effectively preserve\nintrinsic semantic information when both weak and strong augmentations are\napplied to graph-structured data. To avoid the conflict between the MPM and the\nCL of negative pairs, positive pairs of node representations are retained for\nmeasuring the similarity between the original features and the corresponding\nweakly augmented features. Extensive experiments with several benchmark graph\ndatasets demonstrate the effectiveness of the proposed CDL method.",
      "tldr_zh": "本研究针对图结构数据增强的多样性和数量利用问题，提出了一种条件分布学习（Conditional Distribution Learning, CDL）方法，用于半监督图分类。该方法通过端到端图表示学习模型，对弱增强和强增强特征的条件分布与原始特征进行对齐，从而有效保留内在语义信息。同时，为避免图神经网络（Graph Neural Network, GNN）的消息传递机制（Message-Passing Mechanism, MPM）和对比学习（Contrastive Learning, CL）中负对冲突，CDL 只保留正对节点表示来测量相似性。在多个基准图数据集上的广泛实验表明，该方法显著提高了图分类性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.15206v2",
      "published_date": "2024-11-20 07:26:36 UTC",
      "updated_date": "2025-01-28 15:27:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:35:31.241824"
    },
    {
      "arxiv_id": "2411.13079v2",
      "title": "Neural Internal Model Control: Learning a Robust Control Policy via Predictive Error Feedback",
      "title_zh": "神经内部模型控制：通过预测错误反馈学习鲁棒控制策略",
      "authors": [
        "Feng Gao",
        "Chao Yu",
        "Yu Wang",
        "Yi Wu"
      ],
      "abstract": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.",
      "tldr_zh": "这篇论文提出了 Neural Internal Model Control 框架，将模型-based 控制与 RL-based 控制相结合，以提升机器人在复杂环境中的鲁棒性。框架通过应用 Newton-Euler equations 简化预测模型，消除捕捉复杂高维非线性的需求，并整合模型-free RL 算法与预测错误反馈，形成闭环控制结构。该方法在 quadrotors 和 quadrupedal robots 的实验中表现出色，比现有方法性能更优，并在实世界 quadrotor 部署中证明了 sim-to-real 转移的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to RAL",
      "pdf_url": "http://arxiv.org/pdf/2411.13079v2",
      "published_date": "2024-11-20 07:07:42 UTC",
      "updated_date": "2025-03-04 17:07:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:35:44.839714"
    },
    {
      "arxiv_id": "2411.13072v1",
      "title": "AMaze: An intuitive benchmark generator for fast prototyping of generalizable agents",
      "title_zh": "翻译失败",
      "authors": [
        "Kevin Godin-Dubois",
        "Karine Miras",
        "Anna V. Kononova"
      ],
      "abstract": "Traditional approaches to training agents have generally involved a single,\ndeterministic environment of minimal complexity to solve various tasks such as\nrobot locomotion or computer vision. However, agents trained in static\nenvironments lack generalization capabilities, limiting their potential in\nbroader scenarios. Thus, recent benchmarks frequently rely on multiple\nenvironments, for instance, by providing stochastic noise, simple permutations,\nor altogether different settings. In practice, such collections result mainly\nfrom costly human-designed processes or the liberal use of random number\ngenerators. In this work, we introduce AMaze, a novel benchmark generator in\nwhich embodied agents must navigate a maze by interpreting visual signs of\narbitrary complexities and deceptiveness. This generator promotes human\ninteraction through the easy generation of feature-specific mazes and an\nintuitive understanding of the resulting agents' strategies. As a\nproof-of-concept, we demonstrate the capabilities of the generator in a simple,\nfully discrete case with limited deceptiveness. Agents were trained under three\ndifferent regimes (one-shot, scaffolding, interactive), and the results showed\nthat the latter two cases outperform direct training in terms of generalization\ncapabilities. Indeed, depending on the combination of generalization metric,\ntraining regime, and algorithm, the median gain ranged from 50% to 100% and\nmaximal performance was achieved through interactive training, thereby\ndemonstrating the benefits of a controllable human-in-the-loop benchmark\ngenerator.",
      "tldr_zh": "该研究引入了AMaze，一个直观的基准生成器，用于快速原型设计具有泛化能力的代理，以解决传统单一环境训练的局限性。AMaze允许代理在复杂且可能欺骗性的迷宫中通过解释视觉标志来进行导航，并支持人类交互以轻松生成特定特征的迷宫，从而增强对代理策略的理解。在实验中，代理在one-shot、scaffolding和interactive三种训练模式下进行测试，结果显示scaffolding和interactive模式在泛化能力上优于直接训练，性能增益达50%至100%，interactive训练实现最佳效果。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Under review in Frontiers in Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2411.13072v1",
      "published_date": "2024-11-20 06:47:29 UTC",
      "updated_date": "2024-11-20 06:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:35:55.662555"
    },
    {
      "arxiv_id": "2411.14487v1",
      "title": "Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine",
      "title_zh": "确保安全性和可信度：分析大型语言模型在医学中的风险",
      "authors": [
        "Yifan Yang",
        "Qiao Jin",
        "Robert Leaman",
        "Xiaoyu Liu",
        "Guangzhi Xiong",
        "Maame Sarfo-Gyamfi",
        "Changlin Gong",
        "Santiago Ferrière-Steinert",
        "W. John Wilbur",
        "Xiaojun Li",
        "Jiaxin Yuan",
        "Bang An",
        "Kelvin S. Castro",
        "Francisco Erramuspe Álvarez",
        "Matías Stockle",
        "Aidong Zhang",
        "Furong Huang",
        "Zhiyong Lu"
      ],
      "abstract": "The remarkable capabilities of Large Language Models (LLMs) make them\nincreasingly compelling for adoption in real-world healthcare applications.\nHowever, the risks associated with using LLMs in medical applications have not\nbeen systematically characterized. We propose using five key principles for\nsafe and trustworthy medical AI: Truthfulness, Resilience, Fairness,\nRobustness, and Privacy, along with ten specific aspects. Under this\ncomprehensive framework, we introduce a novel MedGuard benchmark with 1,000\nexpert-verified questions. Our evaluation of 11 commonly used LLMs shows that\nthe current language models, regardless of their safety alignment mechanisms,\ngenerally perform poorly on most of our benchmarks, particularly when compared\nto the high performance of human physicians. Despite recent reports indicate\nthat advanced LLMs like ChatGPT can match or even exceed human performance in\nvarious medical tasks, this study underscores a significant safety gap,\nhighlighting the crucial need for human oversight and the implementation of AI\nsafety guardrails.",
      "tldr_zh": "本研究分析了大型语言模型（Large Language Models, LLMs）在医疗应用中的风险，提出五个关键原则（Truthfulness, Resilience, Fairness, Robustness, and Privacy）以及十个具体方面，以构建安全可信的医疗AI框架。研究者引入了MedGuard基准测试，包含1000个专家验证的问题，对11个常用LLMs进行评估，结果显示这些模型在大多数基准上表现不佳，尤其远低于人类医生的水平。尽管某些先进LLMs在医疗任务中能与人类匹敌，但研究强调了显著的安全差距，呼吁加强人类监督和AI安全防护措施。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14487v1",
      "published_date": "2024-11-20 06:34:32 UTC",
      "updated_date": "2024-11-20 06:34:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:36:07.290897"
    },
    {
      "arxiv_id": "2411.13057v1",
      "title": "Branches, Assemble! Multi-Branch Cooperation Network for Large-Scale Click-Through Rate Prediction at Taobao",
      "title_zh": "翻译失败",
      "authors": [
        "Xu Chen",
        "Zida Cheng",
        "Yuangang Pan",
        "Shuai Xiao",
        "Xiaoming Liu",
        "Jinsong Lan",
        "Qingwen Liu",
        "Ivor W. Tsang"
      ],
      "abstract": "Existing click-through rate (CTR) prediction works have studied the role of\nfeature interaction through a variety of techniques. Each interaction technique\nexhibits its own strength, and solely using one type could constrain the\nmodel's capability to capture the complex feature relationships, especially for\nindustrial large-scale data with enormous users and items. Recent research\nshows that effective CTR models often combine an MLP network with a dedicated\nfeature interaction network in a two-parallel structure. However, the interplay\nand cooperative dynamics between different streams or branches remain\nunder-researched. In this work, we introduce a novel Multi-Branch Cooperation\nNetwork (MBCnet) which enables multiple branch networks to collaborate with\neach other for better complex feature interaction modeling. Specifically,\nMBCnet consists of three branches: the Expert-based Feature Grouping and\nCrossing (EFGC) branch that promotes the model's memorization ability of\nspecific feature fields, the low rank Cross Net branch and Deep branch to\nenhance both explicit and implicit feature crossing for improved\ngeneralization. Among branches, a novel cooperation scheme is proposed based on\ntwo principles: branch co-teaching and moderate differentiation. Branch\nco-teaching encourages well-learned branches to support poorly-learned ones on\nspecific training samples. Moderate differentiation advocates branches to\nmaintain a reasonable level of difference in their feature representations. The\ncooperation strategy improves learning through mutual knowledge sharing via\nco-teaching and boosts the discovery of diverse feature interactions across\nbranches. Extensive experiments on large-scale industrial datasets and online\nA/B test demonstrate MBCnet's superior performance, delivering a 0.09 point\nincrease in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes will\nbe released soon.",
      "tldr_zh": "该论文针对现有点击率 (CTR) 预测模型的局限性，提出了一种新型 Multi-Branch Cooperation Network (MBCnet)，旨在通过多个分支协作更好地捕捉大规模工业数据的复杂特征交互。MBCnet 包括 Expert-based Feature Grouping and Crossing (EFGC) 分支（增强特定特征记忆）、Low rank Cross Net 分支（提升显式特征交叉）和 Deep 分支（提升隐式特征交叉），并引入 branch co-teaching 和 moderate differentiation 的合作机制，促进分支间知识共享和特征表示差异。实验在淘宝的大型数据集和在线 A/B 测试中验证了 MBCnet 的优越性能，实现 CTR 提升 0.09 点、交易量增长 1.49% 以及 GMV 增长 1.62%。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2411.13057v1",
      "published_date": "2024-11-20 06:10:06 UTC",
      "updated_date": "2024-11-20 06:10:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:36:20.987091"
    },
    {
      "arxiv_id": "2411.15204v2",
      "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Minguk Jang",
        "Hye Won Chung"
      ],
      "abstract": "Test-time adaptation (TTA) is an effective approach to mitigate performance\ndegradation of trained models when encountering input distribution shifts at\ntest time. However, existing TTA methods often suffer significant performance\ndrops when facing additional class distribution shifts. We first analyze TTA\nmethods under label distribution shifts and identify the presence of class-wise\nconfusion patterns commonly observed across different covariate shifts. Based\non this observation, we introduce label Distribution shift-Aware prediction\nRefinement for Test-time adaptation (DART), a novel TTA method that refines the\npredictions by focusing on class-wise confusion patterns. DART trains a\nprediction refinement module during an intermediate time by exposing it to\nseveral batches with diverse class distributions using the training dataset.\nThis module is then used during test time to detect and correct class\ndistribution shifts, significantly improving pseudo-label accuracy for test\ndata. Our method exhibits 5-18% gains in accuracy under label distribution\nshifts on CIFAR-10C, without any performance degradation when there is no label\ndistribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and\nImageNet benchmarks demonstrate DART's ability to correct inaccurate\npredictions caused by test-time distribution shifts. This improvement leads to\nenhanced performance in existing TTA methods, making DART a valuable plug-in\ntool.",
      "tldr_zh": "本文针对 Test-Time Adaptation (TTA) 在标签分布偏移时的性能下降问题，提出了一种新方法 DART（label Distribution shift-Aware prediction Refinement for Test-time adaptation）。DART 通过训练一个预测精炼模块，利用训练数据集模拟不同类别分布的批次，来检测和修正类别混淆模式，从而提升测试数据的伪标签准确性。实验结果显示，在 CIFAR-10C 等基准上，DART 在标签分布偏移下准确率提升 5-18%，且在无偏移场景下不影响性能，作为现有 TTA 方法的插件工具进一步增强了整体效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15204v2",
      "published_date": "2024-11-20 05:58:52 UTC",
      "updated_date": "2025-02-05 01:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:36:32.237998"
    },
    {
      "arxiv_id": "2411.13053v1",
      "title": "MEGL: Multimodal Explanation-Guided Learning",
      "title_zh": "MEGL：多模态解释引导学习",
      "authors": [
        "Yifei Zhang",
        "Tianxu Jiang",
        "Bo Pan",
        "Jingyu Wang",
        "Guangji Bai",
        "Liang Zhao"
      ],
      "abstract": "Explaining the decision-making processes of Artificial Intelligence (AI)\nmodels is crucial for addressing their \"black box\" nature, particularly in\ntasks like image classification. Traditional eXplainable AI (XAI) methods\ntypically rely on unimodal explanations, either visual or textual, each with\ninherent limitations. Visual explanations highlight key regions but often lack\nrationale, while textual explanations provide context without spatial\ngrounding. Further, both explanation types can be inconsistent or incomplete,\nlimiting their reliability. To address these challenges, we propose a novel\nMultimodal Explanation-Guided Learning (MEGL) framework that leverages both\nvisual and textual explanations to enhance model interpretability and improve\nclassification performance. Our Saliency-Driven Textual Grounding (SDTG)\napproach integrates spatial information from visual explanations into textual\nrationales, providing spatially grounded and contextually rich explanations.\nAdditionally, we introduce Textual Supervision on Visual Explanations to align\nvisual explanations with textual rationales, even in cases where ground truth\nvisual annotations are missing. A Visual Explanation Distribution Consistency\nloss further reinforces visual coherence by aligning the generated visual\nexplanations with dataset-level patterns, enabling the model to effectively\nlearn from incomplete multimodal supervision. We validate MEGL on two new\ndatasets, Object-ME and Action-ME, for image classification with multimodal\nexplanations. Experimental results demonstrate that MEGL outperforms previous\napproaches in prediction accuracy and explanation quality across both visual\nand textual domains. Our code will be made available upon the acceptance of the\npaper.",
      "tldr_zh": "这篇论文针对AI模型的“黑箱”问题，提出MEGL（Multimodal Explanation-Guided Learning）框架，通过整合视觉和文本解释来提升图像分类任务的可解释性和性能。MEGL引入Saliency-Driven Textual Grounding (SDTG)方法，将视觉解释的空间信息融入文本解释，提供更精确的语境支持；同时，通过Textual Supervision on Visual Explanations和Visual Explanation Distribution Consistency loss，确保视觉解释与文本对齐，并保持数据集级别的模式一致性。作者在两个新数据集Object-ME和Action-ME上验证了框架，在预测准确性和解释质量上均优于传统XAI方法。实验结果表明，MEGL有效解决了单模态解释的局限性，为可解释AI提供了新途径。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13053v1",
      "published_date": "2024-11-20 05:57:00 UTC",
      "updated_date": "2024-11-20 05:57:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:36:44.649081"
    },
    {
      "arxiv_id": "2411.13614v1",
      "title": "Verification and Validation of Autonomous Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Sneha Sudhir Shetiya",
        "Vikas Vyas",
        "Shreyas Renukuntla"
      ],
      "abstract": "This paper describes how to proficiently prevent software defects in\nautonomous vehicles, discover and correct defects if they are encountered, and\ncreate a higher level of assurance in the software product development phase.\nIt also describes how to ensure high assurance on software reliability.",
      "tldr_zh": "这篇论文聚焦于自主系统的验证和验证（Verification and Validation），特别针对自主车辆软件的缺陷管理。论文详细介绍了预防软件缺陷的有效策略，以及发现和纠正缺陷的方法，以在软件产品开发阶段提升整体保障水平。此外，它强调了确保软件可靠性高保障的关键措施，从而提高自主系统的安全性和可信度。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13614v1",
      "published_date": "2024-11-20 05:36:22 UTC",
      "updated_date": "2024-11-20 05:36:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:36:55.393771"
    },
    {
      "arxiv_id": "2411.13045v2",
      "title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce Relevance Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Gang Zhao",
        "Ximing Zhang",
        "Chenji Lu",
        "Hui Zhao",
        "Tianshu Wu",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.",
      "tldr_zh": "本文提出一个Explainable LLM-driven Multi-dimensional Distillation框架，用于提升电商搜索系统的查询-商品相关性建模。该框架的核心组件包括Explainable LLM for relevance modeling (ELLM-rele)，通过Chain-of-Thought (CoT)推理将相关性学习分解为中间步骤，提高LLM的可解释性和性能；以及Multi-dimensional Knowledge Distillation (MKD)，从相关性分数分布和CoT推理方面将知识转移到可部署的学生模型中，从而增强其语义交互和长尾泛化能力。实验在淘宝搜索广告场景的离线和在线环境中表明，该框架显著提高了相关性学习性能和用户体验。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by WWW 2025 oral",
      "pdf_url": "http://arxiv.org/pdf/2411.13045v2",
      "published_date": "2024-11-20 05:30:15 UTC",
      "updated_date": "2025-02-08 02:56:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:37:08.327705"
    },
    {
      "arxiv_id": "2411.13036v1",
      "title": "Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization",
      "title_zh": "通过交替优化的多模态图像对无监督单应性估计",
      "authors": [
        "Sanghyeob Song",
        "Jaihyun Lew",
        "Hyemi Jang",
        "Sungroh Yoon"
      ],
      "abstract": "Estimating the homography between two images is crucial for mid- or\nhigh-level vision tasks, such as image stitching and fusion. However, using\nsupervised learning methods is often challenging or costly due to the\ndifficulty of collecting ground-truth data. In response, unsupervised learning\napproaches have emerged. Most early methods, though, assume that the given\nimage pairs are from the same camera or have minor lighting differences.\nConsequently, while these methods perform effectively under such conditions,\nthey generally fail when input image pairs come from different domains,\nreferred to as multimodal image pairs. To address these limitations, we propose\nAltO, an unsupervised learning framework for estimating homography in\nmultimodal image pairs. Our method employs a two-phase alternating optimization\nframework, similar to Expectation-Maximization (EM), where one phase reduces\nthe geometry gap and the other addresses the modality gap. To handle these\ngaps, we use Barlow Twins loss for the modality gap and propose an extended\nversion, Geometry Barlow Twins, for the geometry gap. As a result, we\ndemonstrate that our method, AltO, can be trained on multimodal datasets\nwithout any ground-truth data. It not only outperforms other unsupervised\nmethods but is also compatible with various architectures of homography\nestimators. The source code can be found\nat:~\\url{https://github.com/songsang7/AltO}",
      "tldr_zh": "该论文提出了一种无监督学习框架 AltO，用于在多模态图像对上估计 homography，这对图像拼接和融合等中高层视觉任务至关重要。AltO 采用两阶段交替优化框架，类似于 Expectation-Maximization (EM) 算法，其中一个阶段减少几何差距，另一个阶段处理模态差距，并分别使用 Barlow Twins loss 和 Geometry Barlow Twins loss 来优化这些差距。该方法无需地面真实数据即可在多模态数据集上训练，不仅在性能上优于其他无监督方法，还兼容各种 homography 估计器架构，为跨域图像处理提供了高效解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper is accepted to the Thirty-Eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)",
      "pdf_url": "http://arxiv.org/pdf/2411.13036v1",
      "published_date": "2024-11-20 04:56:19 UTC",
      "updated_date": "2024-11-20 04:56:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:37:19.932200"
    },
    {
      "arxiv_id": "2411.15203v1",
      "title": "Multimodal large language model for wheat breeding: a new exploration of smart breeding",
      "title_zh": "多模态大语言模型用于小麦育种：智能育种的一个新探索",
      "authors": [
        "Guofeng Yang",
        "Yu Li",
        "Yong He",
        "Zhenjiang Zhou",
        "Lingzhen Ye",
        "Hui Fang",
        "Yiqi Luo",
        "Xuping Feng"
      ],
      "abstract": "UAV remote sensing technology has become a key technology in crop breeding,\nwhich can achieve high-throughput and non-destructive collection of crop\nphenotyping data. However, the multidisciplinary nature of breeding has brought\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\nis important to develop a smart breeding goal tool to mine cross-domain\nmultimodal data. Based on different pre-trained open-source multimodal large\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\nreinforcement learning from human feedback (RLHF) technologies to inject\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\nevaluated using the newly created evaluation benchmark in this study. The\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\nand InternVL2-8B has leading performance. Then, subsequent experiments were\nconducted using the WBLM. Ablation experiments indicated that the combination\nof SFT, RAG, and RLHF technologies can improve the overall generation\nperformance, enhance the generated quality, balance the timeliness and\nadaptability of the generated answer, and reduce hallucinations and biases. The\nWBLM performed best in wheat yield prediction using cross-domain data (remote\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\nprofessional decision support answers for phenotyping estimation, environmental\nstress assessment, target germplasm screening, cultivation technique\nrecommendation, and seed price query tasks.",
      "tldr_zh": "这篇论文探索了多模态大型语言模型 (MLLMs) 在小麦育种中的应用，开发了专为智能育种设计的 WBLMs 模型，以挖掘跨域多模态数据，如 UAV 遥感、表型、天气和种质信息。研究采用监督微调 (SFT)、检索增强生成 (RAG) 和强化学习从人类反馈 (RLHF) 技术，对开源模型（如 InternVL2-8B）进行优化，显著提高了模型的生成性能和准确性，同时减少了幻觉和偏差。实验结果表明，基于 InternVL2-8B 的 WBLM 在小麦产量预测任务中表现最佳，R2 为 0.821 和 RMSE 为 489.254 kg/ha，并能为表型估计、环境压力评估、种质筛选、栽培技术推荐和种子价格查询等任务提供专业决策支持。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.15203v1",
      "published_date": "2024-11-20 04:47:42 UTC",
      "updated_date": "2024-11-20 04:47:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:37:38.274236"
    },
    {
      "arxiv_id": "2411.13032v1",
      "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Angel Hsing-Chi Hwang",
        "Q. Vera Liao",
        "Su Lin Blodgett",
        "Alexandra Olteanu",
        "Adam Trischler"
      ],
      "abstract": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting.",
      "tldr_zh": "本研究探讨了在与 Large Language Models (LLMs) 共同写作时，作家如何寻求保持写作真实性，以及个性化 AI 写作工具是否能助其实现。研究通过对 19 名专业作家的半结构化访谈（让他们使用个性化与非个性化 AI 工具）和对 30 名 avid readers 的在线调查，揭示真实性更注重创作过程而非最终产品，作家对个性化工具持积极态度但希望其促进个人成长。结果显示，读者对 AI 辅助作品不敏感，无法轻易区分人类-AI 合作产物，并支持作家使用新技术进行创意实验。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13032v1",
      "published_date": "2024-11-20 04:42:32 UTC",
      "updated_date": "2024-11-20 04:42:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:37:45.573749"
    },
    {
      "arxiv_id": "2411.14486v1",
      "title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz",
      "title_zh": "翻译失败",
      "authors": [
        "David Noever",
        "Forrest McKee"
      ],
      "abstract": "This research introduces a novel evaluation framework designed to assess\nlarge language models' (LLMs) ability to acknowledge uncertainty on 675\nfundamentally unsolvable problems. Using a curated dataset of graduate-level\ngrand challenge questions with intentionally unknowable answers, we evaluated\ntwelve state-of-the-art LLMs, including both open and closed-source models, on\ntheir propensity to admit ignorance rather than generate plausible but\nincorrect responses. The best models scored in 62-68% accuracy ranges for\nadmitting the problem solution was unknown in fields ranging from biology to\nphilosophy and mathematics. We observed an inverse relationship between problem\ndifficulty and model accuracy, with GPT-4 demonstrating higher rates of\nuncertainty acknowledgment on more challenging problems (35.8%) compared to\nsimpler ones (20.0%). This pattern indicates that models may be more prone to\ngenerate speculative answers when problems appear more tractable. The study\nalso revealed significant variations across problem categories, with models\nshowing difficulty in acknowledging uncertainty in invention and NP-hard\nproblems while performing relatively better on philosophical and psychological\nchallenges. These results contribute to the growing body of research on\nartificial general intelligence (AGI) assessment by highlighting the importance\nof uncertainty recognition as a critical component of future machine\nintelligence evaluation. This impossibility test thus extends previous\ntheoretical frameworks for universal intelligence testing by providing\nempirical evidence of current limitations in LLMs' ability to recognize their\nown knowledge boundaries, suggesting new directions for improving model\ntraining architectures and evaluation approaches.",
      "tldr_zh": "这篇论文引入了一个名为“The Impossible Test”的2024年数据集和评估框架，用于测试大型语言模型(LLMs)面对675个根本不可解问题的不确定性承认能力。该框架利用研究生级别的挑战性问题评估了12个最先进模型的表现，发现最佳模型在承认未知答案时的准确率达到62-68%，且GPT-4在更难问题上（如哲学和心理学）表现出更高的不确定性承认率（35.8% vs 20.0%）。研究还揭示了模型在发明和NP-hard问题上更难承认不确定性，并强调了这种能力对人工通用智能(AGI)评估的重要性，为未来模型训练和评估方法提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14486v1",
      "published_date": "2024-11-20 04:12:29 UTC",
      "updated_date": "2024-11-20 04:12:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:37:56.749217"
    },
    {
      "arxiv_id": "2411.13022v2",
      "title": "Fast MRI for All: Bridging Equity Gaps via Training without Raw Data Access",
      "title_zh": "快速 MRI for All",
      "authors": [
        "Yaşar Utku Alçalar",
        "Merve Gülle",
        "Mehmet Akçakaya"
      ],
      "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Though\nPD-DL offers higher acceleration rates than existing clinical fast MRI\ntechniques, their use has been limited outside specialized MRI centers. A key\nchallenge is generalization to underrepresented pathologies or populations,\nnoted in multiple studies, with fine-tuning on target populations suggested for\nimprovement. However, current approaches for PD-DL training require access to\nraw k-space measurements, which is typically only available at specialized MRI\ncenters that have research agreements for such data access. This is especially\nan issue for rural and underserved areas, where commercial MRI scanners only\nprovide access to a final reconstructed image. To tackle these challenges, we\npropose Compressibility-inspired Unsupervised Learning via Parallel Imaging\nFidelity (CUPID) for high-quality PD-DL training using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates output\nquality with a compressibility-based approach while ensuring that the output\nstays consistent with the clinical parallel imaging reconstruction through\nwell-designed perturbations. Our results show CUPID achieves similar quality to\nestablished PD-DL training that requires k-space data while outperforming\ncompressed sensing (CS) and diffusion-based generative methods. We further\ndemonstrate its effectiveness in a zero-shot training setup for retrospectively\nand prospectively sub-sampled acquisitions, attesting to its minimal training\nburden. As an approach that radically deviates from existing strategies, CUPID\npresents an opportunity to provide equitable access to fast MRI for underserved\npopulations in an attempt to reduce the inequalities associated with this\nexpensive imaging modality.",
      "tldr_zh": "本研究针对物理驱动深度学习（PD-DL）在快速磁共振成像（MRI）中的应用，解决了因缺乏原始 k-space 数据访问而导致的公平性差距问题，特别是针对农村和欠发达地区。作者提出了一种新方法 Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID)，它仅使用常规临床重建图像进行训练，通过基于可压缩性的质量评估和设计扰动确保输出与临床平行成像一致。实验结果显示，CUPID 的重建质量与需要 k-space 数据的传统 PD-DL 方法相当，并优于 compressed sensing (CS) 和扩散生成方法，同时在零样本训练设置中表现出色。总体而言，该方法为提供公平访问快速 MRI 铺平了道路，有助于减少与该昂贵成像技术相关的社会不平等。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13022v2",
      "published_date": "2024-11-20 03:53:41 UTC",
      "updated_date": "2025-03-13 15:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:38:08.801917"
    },
    {
      "arxiv_id": "2411.13613v1",
      "title": "SuPLE: Robot Learning with Lyapunov Rewards",
      "title_zh": "翻译失败",
      "authors": [
        "Phu Nguyen",
        "Daniel Polani",
        "Stas Tiomkin"
      ],
      "abstract": "The reward function is an essential component in robot learning. Reward\ndirectly affects the sample and computational complexity of learning, and the\nquality of a solution. The design of informative rewards requires domain\nknowledge, which is not always available. We use the properties of the dynamics\nto produce system-appropriate reward without adding external assumptions.\nSpecifically, we explore an approach to utilize the Lyapunov exponents of the\nsystem dynamics to generate a system-immanent reward. We demonstrate that the\n`Sum of the Positive Lyapunov Exponents' (SuPLE) is a strong candidate for the\ndesign of such a reward. We develop a computational framework for the\nderivation of this reward, and demonstrate its effectiveness on classical\nbenchmarks for sample-based stabilization of various dynamical systems. It\neliminates the need to start the training trajectories at arbitrary states,\nalso known as auxiliary exploration. While the latter is a common practice in\nsimulated robot learning, it is unpractical to consider to use it in real\nrobotic systems, since they typically start from natural rest states such as a\npendulum at the bottom, a robot on the ground, etc. and can not be easily\ninitialized at arbitrary states. Comparing the performance of SuPLE to\ncommonly-used reward functions, we observe that the latter fail to find a\nsolution without auxiliary exploration, even for the task of swinging up the\ndouble pendulum and keeping it stable at the upright position, a prototypical\nscenario for multi-linked robots. SuPLE-induced rewards for robot learning\noffer a novel route for effective robot learning in typical as opposed to\nhighly specialized or fine-tuned scenarios. Our code is publicly available for\nreproducibility and further research.",
      "tldr_zh": "这篇论文提出了 SuPLE，一种基于 Lyapunov exponents 的奖励函数，用于机器人学习，以利用系统动力学的特性生成无需外部假设的系统内生奖励。具体方法涉及计算“Sum of the Positive Lyapunov Exponents”（SuPLE），并开发了一个框架来导出该奖励，在各种动力系统的样本-based 稳定化基准上验证其有效性。实验结果显示，SuPLE 消除了传统辅助探索（auxiliary exploration）的需求，即使在真实机器人场景（如从自然静止状态开始），它也优于常见奖励函数，能成功处理任务如双摆的摆起和稳定。论文公开了代码，以促进复现和进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2411.13613v1",
      "published_date": "2024-11-20 03:20:50 UTC",
      "updated_date": "2024-11-20 03:20:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:38:20.421794"
    },
    {
      "arxiv_id": "2411.13008v1",
      "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Anique Tahir",
        "Lu Cheng",
        "Manuel Sandoval",
        "Yasin N. Silva",
        "Deborah L. Hall",
        "Huan Liu"
      ],
      "abstract": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）在理解社交媒体动态方面的能力，特别是针对网络欺凌和反网络欺凌互动。研究通过比较不同LLMs在语言理解、方向性和欺凌/反欺凌消息检测方面的表现，探讨了微调和提示工程机制的影响。结果显示，fine-tuned LLMs在方向性理解任务中表现出色，但在正确改述和欺凌检测等方面存在混合结果，强调理解LLMs能力对设计未来社交应用模型至关重要。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in ASONAM 24 proceedings",
      "pdf_url": "http://arxiv.org/pdf/2411.13008v1",
      "published_date": "2024-11-20 03:16:07 UTC",
      "updated_date": "2024-11-20 03:16:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:38:32.822716"
    },
    {
      "arxiv_id": "2411.13006v1",
      "title": "Automating Sonologists USG Commands with AI and Voice Interface",
      "title_zh": "翻译失败",
      "authors": [
        "Emad Mohamed",
        "Shruti Tiwari",
        "Sheena Christabel Pravin"
      ],
      "abstract": "This research presents an advanced AI-powered ultrasound imaging system that\nincorporates real-time image processing, organ tracking, and voice commands to\nenhance the efficiency and accuracy of diagnoses in clinical practice.\nTraditional ultrasound diagnostics often require significant time and introduce\na degree of subjectivity due to user interaction. The goal of this innovative\nsolution is to provide Sonologists with a more predictable and productive\nimaging procedure utilizing artificial intelligence, computer vision, and voice\ntechnology. The functionality of the system employs computer vision and deep\nlearning algorithms, specifically adopting the Mask R-CNN model from Detectron2\nfor semantic segmentation of organs and key landmarks. This automation improves\ndiagnostic accuracy by enabling the extraction of valuable information with\nminimal human input. Additionally, it includes a voice recognition feature that\nallows for hands-free operation, enabling users to control the system with\ncommands such as freeze or liver, all while maintaining their focus on the\npatient. The architecture comprises video processing and real-time segmentation\nmodules that prepare the system to perform essential imaging functions, such as\nfreezing and zooming in on frames. The liver histopathology module, optimized\nfor detecting fibrosis, achieved an impressive accuracy of 98.6%. Furthermore,\nthe organ segmentation module produces output confidence levels between 50% and\n95%, demonstrating its efficacy in organ detection.",
      "tldr_zh": "本研究提出了一种AI驱动的超声成像系统，结合实时图像处理、器官跟踪和语音命令，以提高临床诊断的效率和准确性，减少传统操作的主观性和时间消耗。系统采用计算机视觉和深度学习算法，特别是Mask R-CNN from Detectron2，用于器官和关键地标的语义分割，并集成语音识别功能，支持免提操作如“freeze”或“liver”命令。实验结果显示，肝脏组织病理模块在检测纤维化方面的准确率达98.6%，而器官分割模块的置信度介于50%至95%，证明了该系统在自动化诊断中的显著效能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13006v1",
      "published_date": "2024-11-20 03:03:49 UTC",
      "updated_date": "2024-11-20 03:03:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:38:44.942987"
    },
    {
      "arxiv_id": "2411.14485v2",
      "title": "Mediating Modes of Thought: LLM's for design scripting",
      "title_zh": "翻译失败",
      "authors": [
        "Moritz Rietschel",
        "Fang Guo",
        "Kyle Steinfeld"
      ],
      "abstract": "Architects adopt visual scripting and parametric design tools to explore more\nexpansive design spaces (Coates, 2010), refine their thinking about the\ngeometric logic of their design (Woodbury, 2010), and overcome conventional\nsoftware limitations (Burry, 2011). Despite two decades of effort to make\ndesign scripting more accessible, a disconnect between a designer's free ways\nof thinking and the rigidity of algorithms remains (Burry, 2011). Recent\ndevelopments in Large Language Models (LLMs) suggest this might soon change, as\nLLMs encode a general understanding of human context and exhibit the capacity\nto produce geometric logic. This project speculates that if LLMs can\neffectively mediate between user intent and algorithms, they become a powerful\ntool to make scripting in design more widespread and fun. We explore if such\nsystems can interpret natural language prompts to assemble geometric operations\nrelevant to computational design scripting. In the system, multiple layers of\nLLM agents are configured with specific context to infer the user intent and\nconstruct a sequential logic. Given a user's high-level text prompt, a\ngeometric description is created, distilled into a sequence of logic\noperations, and mapped to software-specific commands. The completed script is\nconstructed in the user's visual programming interface. The system succeeds in\ngenerating complete visual scripts up to a certain complexity but fails beyond\nthis complexity threshold. It shows how LLMs can make design scripting much\nmore aligned with human creativity and thought. Future research should explore\nconversational interactions, expand to multimodal inputs and outputs, and\nassess the performance of these tools.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）在设计脚本中的作用，旨在解决建筑师在视觉脚本和参数化设计中面临的思考自由与算法刚性脱节问题。系统采用多层LLM代理，通过解释自然语言提示来推断用户意图、生成几何描述、提炼逻辑操作序列，并映射到软件特定命令，最终在视觉编程界面构建脚本。实验结果显示，该系统能成功生成一定复杂度的完整视觉脚本，提升了设计脚本的可访问性和趣味性，但超出复杂性阈值时会失败。未来研究应扩展到对话交互、多模态输入输出，并评估工具性能，以进一步增强LLMs在计算设计中的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL",
      "comment": "Published at ACADIA 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.14485v2",
      "published_date": "2024-11-20 02:49:18 UTC",
      "updated_date": "2024-12-03 22:27:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:38:57.842599"
    },
    {
      "arxiv_id": "2411.12990v1",
      "title": "BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices",
      "title_zh": "BetterBench：评估",
      "authors": [
        "Anka Reuel",
        "Amelia Hardy",
        "Chandler Smith",
        "Max Lamparth",
        "Malcolm Hardy",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "AI models are increasingly prevalent in high-stakes environments,\nnecessitating thorough assessment of their capabilities and risks. Benchmarks\nare popular for measuring these attributes and for comparing model performance,\ntracking progress, and identifying weaknesses in foundation and non-foundation\nmodels. They can inform model selection for downstream tasks and influence\npolicy initiatives. However, not all benchmarks are the same: their quality\ndepends on their design and usability. In this paper, we develop an assessment\nframework considering 46 best practices across an AI benchmark's lifecycle and\nevaluate 24 AI benchmarks against it. We find that there exist large quality\ndifferences and that commonly used benchmarks suffer from significant issues.\nWe further find that most benchmarks do not report statistical significance of\ntheir results nor allow for their results to be easily replicated. To support\nbenchmark developers in aligning with best practices, we provide a checklist\nfor minimum quality assurance based on our assessment. We also develop a living\nrepository of benchmark assessments to support benchmark comparability,\naccessible at betterbench.stanford.edu.",
      "tldr_zh": "这篇论文提出了 BetterBench 框架，用于评估 AI benchmarks 的质量，涵盖基准测试生命周期中的 46 个最佳 practices。通过评估 24 个 AI benchmarks，研究发现这些基准测试存在显著质量差异，常用 ones 常有重大问题，如未报告结果的统计显著性和可复制性。作者提供了最低质量保证检查列表，并建立了 BetterBench 活文档仓库（betterbench.stanford.edu），以支持基准测试的可比性和改进。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted as a Spotlight Poster to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.12990v1",
      "published_date": "2024-11-20 02:38:24 UTC",
      "updated_date": "2024-11-20 02:38:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:39:08.796138"
    },
    {
      "arxiv_id": "2411.12980v3",
      "title": "LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement",
      "title_zh": "翻译失败",
      "authors": [
        "Siwen Jiao",
        "Yangyi Fang",
        "Baoyun Peng",
        "Wangqun Chen",
        "Bharadwaj Veeravalli"
      ],
      "abstract": "Recent advancements in Visual Language Models (VLMs) have made them crucial\nfor visual question answering (VQA) in autonomous driving, enabling natural\nhuman-vehicle interactions. However, existing methods often struggle in dynamic\ndriving environments, as they usually focus on static images or videos and rely\non downsampling to manage computational costs. This results in the loss of\ncritical details and the difficulty in effectively integrating spatial and\ntemporal information, undermining fine-grained perception and temporal\ncoherence essential for effective decision-making. To tackle these challenges,\nwe introduce LaVida Drive, a novel and efficient VQA framework for autonomous\ndriving. LaVida Drive seamlessly integrates temporal data while maintaining\nhigh-resolution inputs for detailed visual perception. It optimizes spatial\nprocessing by retaining high-resolution data for intricate details and using\nlower-resolution inputs for temporal analysis to focus on motion-related\nfeatures, thereby boosting computational efficiency. The core of LaVida Drive\nconsists of two modules: the \\textit{Query-aware Token Selection} module and\nthe \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former\ndynamically selects the most relevant visual tokens based on semantic alignment\nwith the input query, reducing the token count from high-resolution spatial\ninput. The latter ensures smooth and coherent interactions between spatial and\ntemporal information, preserving contextual continuity across frames. Extensive\nexperiments on various autonomous driving question-answering benchmarks show\nthat LaVida Drive significantly reduces visual tokens, enhances efficiency, and\nimproves overall performance.",
      "tldr_zh": "该研究提出 LaVida Drive，一种先进的视觉语言模型(VLMs)框架，用于自动驾驶领域的视觉问答(VQA)，旨在解决现有方法在动态环境中因降采样导致的关键细节丢失和空间-时间信息整合困难的问题。LaVida Drive 通过保留高分辨率输入以保持详细视觉感知，同时使用低分辨率输入优化时间分析，从而提升计算效率。其核心包括 Query-aware Token Selection 模块（根据查询动态选择相关视觉令牌，减少令牌数量）和 Spatial-Temporal Token Recovery and Enhancement 模块（确保空间和时间信息的平滑交互，维持帧间上下文连续性）。实验结果显示，该框架在各种自动驾驶问答基准上显著减少视觉令牌数量，提高效率，并提升整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.12980v3",
      "published_date": "2024-11-20 02:14:07 UTC",
      "updated_date": "2025-02-22 16:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:39:20.804884"
    },
    {
      "arxiv_id": "2411.12977v3",
      "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Collaborative Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mircea Lică",
        "Ojas Shirekar",
        "Baptiste Colle",
        "Chirag Raman"
      ],
      "abstract": "Contemporary embodied agents powered by large language models (LLMs), such as\nVoyager, have shown promising capabilities in individual learning within\nopen-ended environments like Minecraft. However, when powered by open LLMs,\nthey struggle with basic tasks even after domain-specific fine-tuning. We\npresent MindForge, a generative-agent framework for collaborative lifelong\nlearning through explicit perspective taking. We introduce three key\ninnovations: (1) a structured theory of mind representation linking percepts,\nbeliefs, desires, and actions; (2) natural interagent communication; and (3) a\nmulticomponent memory system. In Minecraft experiments, MindForge agents\npowered by open-weight LLMs significantly outperform their Voyager counterparts\nin basic tasks where traditional Voyager fails without GPT-4, collecting\n$2.3\\times$ more unique items and achieving $3\\times$ more tech-tree\nmilestones, advancing from basic wood tools to advanced iron equipment.\nMindForge agents demonstrate sophisticated behaviors, including expert-novice\nknowledge transfer, collaborative problem solving, and adaptation to\nout-of-distribution tasks through accumulated collaborative experiences.\nMindForge advances the democratization of embodied AI development through\nopen-ended social learning, enabling peer-to-peer knowledge sharing.",
      "tldr_zh": "该研究提出 MindForge 框架，利用 Theory of Mind 赋能基于 LLMs 的实体代理，实现协作终身学习。框架的关键创新包括结构化的理论思维表示（将 percepts、beliefs、desires 和 actions 联系起来）、自然的代理间通信以及多组件记忆系统。在 Minecraft 实验中，MindForge 代理比 Voyager 代理性能显著提升，收集了 2.3 倍独特物品、达到 3 倍技术树里程碑，并展示了专家-新手知识转移、协作问题解决和适应 out-of-distribution 任务的能力。该框架促进了实体 AI 的民主化，通过点对点知识共享推进开放社交学习。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.12977v3",
      "published_date": "2024-11-20 02:10:44 UTC",
      "updated_date": "2025-02-19 22:59:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:39:34.246737"
    },
    {
      "arxiv_id": "2411.14484v1",
      "title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Atharva Gundawar",
        "Karthik Valmeekam",
        "Mudit Verma",
        "Subbarao Kambhampati"
      ],
      "abstract": "Previous work has attempted to boost Large Language Model (LLM) performance\non planning and scheduling tasks through a variety of prompt engineering\ntechniques. While these methods can work within the distributions tested, they\nare neither robust nor predictable. This limitation can be addressed through\ncompound LLM architectures where LLMs work in conjunction with other components\nto ensure reliability. In this paper, we present a technical evaluation of a\ncompound LLM architecture--the LLM-Modulo framework. In this framework, an LLM\nis paired with a complete set of sound verifiers that validate its output,\nre-prompting it if it fails. This approach ensures that the system can never\noutput any fallacious output, and therefore that every output generated is\nguaranteed correct--something previous techniques have not been able to claim.\nOur results, evaluated across four scheduling domains, demonstrate significant\nperformance gains with the LLM-Modulo framework using various models.\nAdditionally, we explore modifications to the base configuration of the\nframework and assess their impact on overall system performance.",
      "tldr_zh": "本研究探讨了如何通过复合LLM架构提升大型语言模型(LLM)在规划和调度任务中的鲁棒性，针对传统提示工程(prompt engineering)方法的不可预测性问题，提出了LLM-Modulo框架。该框架将LLM与一组可靠的验证器结合，如果输出错误则重新提示，确保系统输出始终正确且无误。实验在四个调度领域中进行，展示了LLM-Modulo显著提升性能，并对框架的基本配置进行了修改评估，进一步优化了整体效果。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.14484v1",
      "published_date": "2024-11-20 02:04:09 UTC",
      "updated_date": "2024-11-20 02:04:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:39:43.549865"
    },
    {
      "arxiv_id": "2411.13611v3",
      "title": "DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zhihan Liu",
        "Shenao Zhang",
        "Yongfei Liu",
        "Boyi Liu",
        "Yingxiang Yang",
        "Zhaoran Wang"
      ],
      "abstract": "Direct preference learning offers a promising and computation-efficient\nbeyond supervised fine-tuning (SFT) for improving code generation in coding\nlarge language models (LMs). However, the scarcity of reliable preference data\nis a bottleneck for the performance of direct preference learning to improve\nthe coding accuracy of code LMs. In this paper, we introduce\n\\underline{\\textbf{D}}irect Preference Learning with Only\n\\underline{\\textbf{S}}elf-Generated \\underline{\\textbf{T}}ests and\n\\underline{\\textbf{C}}ode (DSTC), a framework that leverages only\nself-generated code snippets and tests to construct reliable preference pairs\nsuch that direct preference learning can improve LM coding accuracy without\nexternal annotations. DSTC combines a minimax selection process and test-code\nconcatenation to improve preference pair quality, reducing the influence of\nincorrect self-generated tests and enhancing model performance without the need\nfor costly reward models. When applied with direct preference learning methods\nsuch as Direct Preference Optimization (DPO) and Kahneman-Tversky Optimization\n(KTO), DSTC yields stable improvements in coding accuracy (pass@1 score) across\ndiverse coding benchmarks, including HumanEval, MBPP, and BigCodeBench,\ndemonstrating both its effectiveness and scalability for models of various\nsizes. This approach autonomously enhances code generation accuracy across LLMs\nof varying sizes, reducing reliance on expensive annotated coding datasets.",
      "tldr_zh": "本研究提出DSTC框架，利用仅自生成的代码片段和测试来构建可靠的偏好对，从而提升代码生成模型的准确性，而无需外部注解。DSTC结合minimax选择过程和测试-代码连接机制，提高偏好对质量，减少自生成测试的错误影响，并与Direct Preference Optimization (DPO)或Kahneman-Tversky Optimization (KTO)等方法兼容。实验结果显示，DSTC在HumanEval、MBPP和BigCodeBench等基准上稳定提升模型的pass@1分数，适用于不同规模的代码大型语言模型（LLMs），并降低了依赖昂贵数据集的成本。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2411.13611v3",
      "published_date": "2024-11-20 02:03:16 UTC",
      "updated_date": "2024-12-10 07:47:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:39:55.915897"
    },
    {
      "arxiv_id": "2411.12967v1",
      "title": "Shrinking POMCP: A Framework for Real-Time UAV Search and Rescue",
      "title_zh": "翻译失败",
      "authors": [
        "Yunuo Zhang",
        "Baiting Luo",
        "Ayan Mukhopadhyay",
        "Daniel Stojcsics",
        "Daniel Elenius",
        "Anirban Roy",
        "Susmit Jha",
        "Miklos Maroti",
        "Xenofon Koutsoukos",
        "Gabor Karsai",
        "Abhishek Dubey"
      ],
      "abstract": "Efficient path optimization for drones in search and rescue operations faces\nchallenges, including limited visibility, time constraints, and complex\ninformation gathering in urban environments. We present a comprehensive\napproach to optimize UAV-based search and rescue operations in neighborhood\nareas, utilizing both a 3D AirSim-ROS2 simulator and a 2D simulator. The path\nplanning problem is formulated as a partially observable Markov decision\nprocess (POMDP), and we propose a novel ``Shrinking POMCP'' approach to address\ntime constraints. In the AirSim environment, we integrate our approach with a\nprobabilistic world model for belief maintenance and a neurosymbolic navigator\nfor obstacle avoidance. The 2D simulator employs surrogate ROS2 nodes with\nequivalent functionality. We compare trajectories generated by different\napproaches in the 2D simulator and evaluate performance across various belief\ntypes in the 3D AirSim-ROS simulator. Experimental results from both simulators\ndemonstrate that our proposed shrinking POMCP solution achieves significant\nimprovements in search times compared to alternative methods, showcasing its\npotential for enhancing the efficiency of UAV-assisted search and rescue\noperations.",
      "tldr_zh": "这篇论文针对无人机(UAV)在搜索和救援中的路径优化问题，提出了一种名为Shrinking POMCP的框架，以解决有限视野、时间限制和复杂环境等挑战。该框架将问题建模为部分可观测Markov决策过程(POMDP)，并结合3D AirSim-ROS2模拟器和2D模拟器，使用概率世界模型维护信念状态以及神经符号导航器避免障碍。实验结果表明，Shrinking POMCP方法在两种模拟器中显著缩短了搜索时间，与其他方法相比提高了UAV辅助操作的整体效率。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted to the The 3rd International Conference on Assured Autonomy",
      "pdf_url": "http://arxiv.org/pdf/2411.12967v1",
      "published_date": "2024-11-20 01:41:29 UTC",
      "updated_date": "2024-11-20 01:41:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:40:08.494252"
    },
    {
      "arxiv_id": "2411.12964v1",
      "title": "Real-Time Energy-Optimal Path Planning for Electric Vehicles",
      "title_zh": "实时能量最优路径规划用于电动车辆",
      "authors": [
        "Saman Ahmadi",
        "Guido Tack",
        "Daniel Harabor",
        "Philip Kilby",
        "Mahdi Jalili"
      ],
      "abstract": "The rapid adoption of electric vehicles (EVs) in modern transport systems has\nmade energy-aware routing a critical task in their successful integration,\nespecially within large-scale networks. In cases where an EV's remaining energy\nis limited and charging locations are not easily accessible, some destinations\nmay only be reachable through an energy-optimal path: a route that consumes\nless energy than all other alternatives. The feasibility of such\nenergy-efficient paths depends heavily on the accuracy of the energy model used\nfor planning, and thus failing to account for vehicle dynamics can lead to\ninaccurate energy estimates, rendering some planned routes infeasible in\nreality. This paper explores the impact of vehicle dynamics on energy-optimal\npath planning for EVs. We develop an accurate energy model that incorporates\nkey vehicle dynamics parameters into energy calculations, thereby reducing the\nrisk of planning infeasible paths under battery constraints. The paper also\nintroduces two novel online reweighting functions that allow for a faster,\npre-processing free, pathfinding in the presence of negative energy costs\nresulting from regenerative braking, making them ideal for real-time\napplications. Through extensive experimentation on real-world transport\nnetworks, we demonstrate that our approach considerably enhances energy-optimal\npathfinding for EVs in both computational efficiency and energy estimation\naccuracy.",
      "tldr_zh": "这篇论文探讨了电动汽车（EVs）的实时能量最优路径规划问题，强调了在能量有限和充电点稀少的情况下，考虑车辆动态参数的重要性，以避免能量模型不准确导致路径不可行。研究者开发了一个精确的能量模型，融入关键车辆动态因素，并引入两个新的在线重权函数（online reweighting functions），允许在负能量成本（如再生制动）环境下进行快速、无需预处理的路径计算，适合实时应用。通过在真实世界交通网络上的广泛实验，该方法显著提升了计算效率和能量估计准确性，为EVs的能量感知路由提供了可靠解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "12 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2411.12964v1",
      "published_date": "2024-11-20 01:39:08 UTC",
      "updated_date": "2024-11-20 01:39:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:40:20.609946"
    },
    {
      "arxiv_id": "2411.14157v1",
      "title": "DrugGen: Advancing Drug Discovery with Large Language Models and Reinforcement Learning Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Mahsa Sheikholeslami",
        "Navid Mazrouei",
        "Yousof Gheisari",
        "Afshin Fasihi",
        "Matin Irajpour",
        "Ali Motahharynia"
      ],
      "abstract": "Traditional drug design faces significant challenges due to inherent chemical\nand biological complexities, often resulting in high failure rates in clinical\ntrials. Deep learning advancements, particularly generative models, offer\npotential solutions to these challenges. One promising algorithm is DrugGPT, a\ntransformer-based model, that generates small molecules for input protein\nsequences. Although promising, it generates both chemically valid and invalid\nstructures and does not incorporate the features of approved drugs, resulting\nin time-consuming and inefficient drug discovery. To address these issues, we\nintroduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen is\nfine-tuned on approved drug-target interactions and optimized with proximal\npolicy optimization. By giving reward feedback from protein-ligand binding\naffinity prediction using pre-trained transformers (PLAPT) and a customized\ninvalid structure assessor, DrugGen significantly improves performance.\nEvaluation across multiple targets demonstrated that DrugGen achieves 100%\nvalid structure generation compared to 95.5% with DrugGPT and produced\nmolecules with higher predicted binding affinities (7.22 [6.30-8.07]) compared\nto DrugGPT (5.81 [4.97-6.63]) while maintaining diversity and novelty. Docking\nsimulations further validate its ability to generate molecules targeting\nbinding sites effectively. For example, in the case of fatty acid-binding\nprotein 5 (FABP5), DrugGen generated molecules with superior docking scores\n(FABP5/11, -9.537 and FABP5/5, -8.399) compared to the reference molecule\n(Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also shows\npotential for drug repositioning and creating novel pharmacophores for existing\ntargets. By producing high-quality small molecules, DrugGen provides a\nhigh-performance medium for advancing pharmaceutical research and drug\ndiscovery.",
      "tldr_zh": "该论文提出DrugGen模型，利用大型语言模型和强化学习反馈（如proximal policy optimization）来提升药物发现效率，针对DrugGPT的局限性（如生成无效结构和忽略已批准药物特征）进行改进。DrugGen通过微调于已批准药物-目标互动，并结合蛋白-配体结合亲和力预测（PLAPT）和自定义无效结构评估器提供奖励反馈，实现100%有效结构生成，比DrugGPT提高4.5%。实验结果显示，DrugGen生成的分子具有更高的预测结合亲和力（7.22 vs. 5.81），并在对接模拟中表现出色，例如针对FABP5蛋白的分子对接分数显著优于参考分子，同时支持药物再定位和新型药效团的创建。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "20 pages, 5 figures, 3 tables, and 7 supplementary files. To use the\n  model, see https://huggingface.co/alimotahharynia/DrugGen",
      "pdf_url": "http://arxiv.org/pdf/2411.14157v1",
      "published_date": "2024-11-20 01:21:07 UTC",
      "updated_date": "2024-11-20 01:21:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:42:33.827107"
    },
    {
      "arxiv_id": "2411.15201v1",
      "title": "Beyond Visual Understanding: Introducing PARROT-360V for Vision Language Model Benchmarking",
      "title_zh": "翻译失败",
      "authors": [
        "Harsha Vardhan Khurdula",
        "Basem Rizk",
        "Indus Khaitan",
        "Janit Anjaria",
        "Aviral Srivastava",
        "Rajvardhan Khaitan"
      ],
      "abstract": "Current benchmarks for evaluating Vision Language Models (VLMs) often fall\nshort in thoroughly assessing model abilities to understand and process complex\nvisual and textual content. They typically focus on simple tasks that do not\nrequire deep reasoning or the integration of multiple data modalities to solve\nan original problem. To address this gap, we introduce the PARROT-360V\nBenchmark, a novel and comprehensive benchmark featuring 2487 challenging\nvisual puzzles designed to test VLMs on complex visual reasoning tasks. We\nevaluated leading models: GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro, using\nPARROT-360V to assess their capabilities in combining visual clues with\nlanguage skills to solve tasks in a manner akin to human problem-solving. Our\nfindings reveal a notable performance gap: state-of-the-art models scored\nbetween 28 to 56 percentage on our benchmark, significantly lower than their\nperformance on popular benchmarks. This underscores the limitations of current\nVLMs in handling complex, multi-step reasoning tasks and highlights the need\nfor more robust evaluation frameworks to advance the field.",
      "tldr_zh": "本论文指出，现有的 Vision Language Models (VLMs) 基准测试往往局限于简单任务，无法充分评估模型在复杂视觉推理和多模态数据整合方面的能力。作者引入了 PARROT-360V 基准，这是一个包含 2487 个挑战性视觉谜题的全面框架，用于测试 VLMs 结合视觉线索和语言技能解决类似人类问题的能力。实验评估了 GPT-4o、Claude-3.5-Sonnet 和 Gemini-1.5-Pro 等领先模型，结果显示这些模型在 PARROT-360V 上的得分仅为 28% 到 56%，远低于它们在流行基准的表现。这突显了当前 VLMs 在处理复杂、多步推理任务的局限性，并强调了开发更 robust 的评估框架的必要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.7; I.2.10"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 4 figures, Accepted at COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2411.15201v1",
      "published_date": "2024-11-20 01:09:21 UTC",
      "updated_date": "2024-11-20 01:09:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:40:45.420404"
    },
    {
      "arxiv_id": "2411.12950v2",
      "title": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware Attributes Learning",
      "title_zh": "KAAE: 通过知识感知属性学习实现知识图谱的数值推理",
      "authors": [
        "Ming Yin",
        "Qiang Zhou",
        "Zongsheng Cao",
        "Mei Li"
      ],
      "abstract": "Numerical reasoning is pivotal in various artificial intelligence\napplications, such as natural language processing and recommender systems,\nwhere it involves using entities, relations, and attribute values (e.g.,\nweight, length) to infer new factual relations (e.g., the Nile is longer than\nthe Amazon). However, existing approaches encounter two critical challenges in\nmodeling: (1) semantic relevance-the challenge of insufficiently capturing the\nnecessary contextual interactions among entities, relations, and numerical\nattributes, often resulting in suboptimal inference; and (2) semantic\nambiguity-the difficulty in accurately distinguishing ordinal relationships\nduring numerical reasoning, which compromises the generation of high-quality\nsamples and limits the effectiveness of contrastive learning. To address these\nchallenges, we propose the novel Knowledge-Aware Attributes Embedding model\n(KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to\novercome the challenge of semantic relevance, we introduce a\nMixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the\nsemantics of entities, relations, and numerical attributes into a joint\nsemantic space. To tackle semantic ambiguity, we implement a new ordinal\nknowledge contrastive learning (OKCL) strategy that generates high-quality\nordinal samples from the original data with the aid of ordinal relations,\ncapturing fine-grained semantic nuances essential for accurate numerical\nreasoning. Experiments on three public benchmark datasets demonstrate the\nsuperior performance of KAAE across various attribute value distributions.",
      "tldr_zh": "该论文提出了一种新型知识图谱嵌入模型KAAE，用于提升数值推理能力，针对现有方法在语义相关性（semantic relevance）和语义模糊性（semantic ambiguity）方面的不足。KAAE 通过引入Mixture-of-Experts-Knowledge-Aware (MoEKA) Encoder，将实体、关系和数值属性的语义整合到联合语义空间中；同时，采用Ordinal Knowledge Contrastive Learning (OKCL)策略生成高质量序数样本，以捕捉细粒度的语义差异。实验在三个公共基准数据集上验证了KAAE的优越性能，尤其在各种属性值分布下表现出色。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper was decided to be withdrawn due to failure to resolve\n  collaborative disputes within the research team or authorship issues. We are\n  actively communicating to reach an agreement and avoid a recurrence of\n  similar issues",
      "pdf_url": "http://arxiv.org/pdf/2411.12950v2",
      "published_date": "2024-11-20 00:47:03 UTC",
      "updated_date": "2024-11-23 05:43:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:40:56.551921"
    },
    {
      "arxiv_id": "2411.12943v1",
      "title": "Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity",
      "title_zh": "增强热",
      "authors": [
        "Wassim El Ahmar",
        "Dhanvin Kolhatkar",
        "Farzan Nowruzi",
        "Robert Laganiere"
      ],
      "abstract": "Multiple Object Tracking (MOT) in thermal imaging presents unique challenges\ndue to the lack of visual features and the complexity of motion patterns. This\npaper introduces an innovative approach to improve MOT in the thermal domain by\ndeveloping a novel box association method that utilizes both thermal object\nidentity and motion similarity. Our method merges thermal feature sparsity and\ndynamic object tracking, enabling more accurate and robust MOT performance.\nAdditionally, we present a new dataset comprised of a large-scale collection of\nthermal and RGB images captured in diverse urban environments, serving as both\na benchmark for our method and a new resource for thermal imaging. We conduct\nextensive experiments to demonstrate the superiority of our approach over\nexisting methods, showing significant improvements in tracking accuracy and\nrobustness under various conditions. Our findings suggest that incorporating\nthermal identity with motion data enhances MOT performance. The newly collected\ndataset and source code is available at https://github.com/wassimea/thermalMOT",
      "tldr_zh": "这篇论文针对热成像中的多目标跟踪（MOT）面临的挑战，如缺乏视觉特征和复杂运动模式，提出了一种新型框关联（box association）方法，该方法利用热对象身份（thermal identity）和运动相似性（motion similarity）来提升跟踪的准确性和鲁棒性。研究者还构建了一个大规模数据集，包括来自多样城市环境的热成像和RGB图像，作为基准资源。实验结果显示，该方法在各种条件下比现有方法提高了跟踪性能，验证了整合热身份与运动数据的优势；数据集和源代码已在GitHub上公开。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Workshop on Towards a Complete Analysis of People, part of the\n  European Conference on Computer Vision (ECCV) 2024",
      "pdf_url": "http://arxiv.org/pdf/2411.12943v1",
      "published_date": "2024-11-20 00:27:01 UTC",
      "updated_date": "2024-11-20 00:27:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T02:43:03.758007"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 105,
  "processed_papers_count": 105,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T02:43:24.837951"
}
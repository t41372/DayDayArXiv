[
  {
    "arxiv_id": "2511.22809v2",
    "title": "AI summaries in online search influence users' attitudes",
    "authors": [
      "Yiwei Xu",
      "Saloni Dash",
      "Sungha Kang",
      "Wang Liao",
      "Emma S. Spiro"
    ],
    "abstract": "This study examined how AI-generated summaries, which have become visually prominent in online search results, affect how users think about different issues. In a preregistered randomized controlled experiment, participants (N = 2,004) viewed mock search result pages varying in the presence (vs. absence), placement (top vs. middle), and stance (benefit-framed vs. harm-framed) of AI-generated summaries across four publicly debated topics. Compared to a no-summary control group, participants exposed to AI-generated summaries reported issue attitudes, behavioral intentions, and policy support that aligned more closely with the AI summary stance. The summaries placed at the top of the page produced stronger shifts in users' issue attitudes (but not behavioral intentions or policy support) than those placed at the middle of the page. We also observed moderating effects from issue familiarity and general trust toward AI. In addition, users perceived the AI summaries more useful when it emphasized health harms versus benefits. These findings suggest that AI-generated search summaries can significantly shape public perceptions, raising important implications for the design and regulation of AI-integrated information ecosystems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22809v2",
    "published_date": "2025-11-27 23:45:19 UTC",
    "updated_date": "2025-12-04 15:16:29 UTC"
  },
  {
    "arxiv_id": "2512.11818v1",
    "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique",
    "authors": [
      "Izabela Lipinska",
      "Hugh Brosnahan"
    ],
    "abstract": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "18 pages excluding appendices",
    "pdf_url": "https://arxiv.org/pdf/2512.11818v1",
    "published_date": "2025-11-27 22:46:42 UTC",
    "updated_date": "2025-11-27 22:46:42 UTC"
  },
  {
    "arxiv_id": "2511.22781v1",
    "title": "The Hidden AI Race: Tracking Environmental Costs of Innovation",
    "authors": [
      "Shyam Agarwal",
      "Mahasweta Chakraborti"
    ],
    "abstract": "The past decade has seen a massive rise in the popularity of AI systems, mainly owing to the developments in Gen AI, which has revolutionized numerous industries and applications. However, this progress comes at a considerable cost to the environment as training and deploying these models consume significant computational resources and energy and are responsible for large carbon footprints in the atmosphere. In this paper, we study the amount of carbon dioxide released by models across different domains over varying time periods. By examining parameters such as model size, repository activity (e.g., commits and repository age), task type, and organizational affiliation, we identify key factors influencing the environmental impact of AI development. Our findings reveal that model size and versioning frequency are strongly correlated with higher emissions, while domain-specific trends show that NLP models tend to have lower carbon footprints compared to audio-based systems. Organizational context also plays a significant role, with university-driven projects exhibiting the highest emissions, followed by non-profits and companies, while community-driven projects show a reduction in emissions. These results highlight the critical need for green AI practices, including the adoption of energy-efficient architectures, optimizing development workflows, and leveraging renewable energy sources. We also discuss a few practices that can lead to a more sustainable future with AI, and we end this paper with some future research directions that could be motivated by our work. This work not only provides actionable insights to mitigate the environmental impact of AI but also poses new research questions for the community to explore. By emphasizing the interplay between sustainability and innovation, our study aims to guide future efforts toward building a more ecologically responsible AI ecosystem.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22781v1",
    "published_date": "2025-11-27 22:14:43 UTC",
    "updated_date": "2025-11-27 22:14:43 UTC"
  },
  {
    "arxiv_id": "2511.22780v1",
    "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation",
    "authors": [
      "Amir Rasouli",
      "Montgomery Alban",
      "Sajjad Pakdamansavoji",
      "Zhiyuan Li",
      "Zhanguang Zhang",
      "Aaron Wu",
      "Xuan Zhao"
    ],
    "abstract": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "12 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22780v1",
    "published_date": "2025-11-27 22:13:13 UTC",
    "updated_date": "2025-11-27 22:13:13 UTC"
  },
  {
    "arxiv_id": "2512.03077v1",
    "title": "Irresponsible AI: big tech's influence on AI research and associated impacts",
    "authors": [
      "Alex Hernandez-Garcia",
      "Alexandra Volokhova",
      "Ezekiel Williams",
      "Dounia Shaaban Kabakibo"
    ],
    "abstract": "The accelerated development, deployment and adoption of artificial intelligence systems has been fuelled by the increasing involvement of big tech. This has been accompanied by increasing ethical concerns and intensified societal and environmental impacts. In this article, we review and discuss how these phenomena are deeply entangled. First, we examine the growing and disproportionate influence of big tech in AI research and argue that its drive for scaling and general-purpose systems is fundamentally at odds with the responsible, ethical, and sustainable development of AI. Second, we review key current environmental and societal negative impacts of AI and trace their connections to big tech and its underlying economic incentives. Finally, we argue that while it is important to develop technical and regulatory approaches to these challenges, these alone are insufficient to counter the distortion introduced by big tech's influence. We thus review and propose alternative strategies that build on the responsibility of implicated actors and collective action.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Presented at: NeurIPS 2025 Workshop on Algorithmic Collective Action",
    "pdf_url": "https://arxiv.org/pdf/2512.03077v1",
    "published_date": "2025-11-27 22:02:27 UTC",
    "updated_date": "2025-11-27 22:02:27 UTC"
  },
  {
    "arxiv_id": "2511.22777v1",
    "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery",
    "authors": [
      "Sajjad Pakdamansavoji",
      "Mozhgan Pourkeshavarz",
      "Adam Sigal",
      "Zhiyuan Li",
      "Rui Heng Yang",
      "Amir Rasouli"
    ],
    "abstract": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "11 figures, 3 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22777v1",
    "published_date": "2025-11-27 22:02:02 UTC",
    "updated_date": "2025-11-27 22:02:02 UTC"
  },
  {
    "arxiv_id": "2511.22773v1",
    "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance",
    "authors": [
      "Rui Heng Yang",
      "Xuan Zhao",
      "Leo Maxime Brunswic",
      "Montgomery Alban",
      "Mateo Clemente",
      "Tongtong Cao",
      "Jun Jin",
      "Amir Rasouli"
    ],
    "abstract": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "4 tables, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22773v1",
    "published_date": "2025-11-27 21:53:09 UTC",
    "updated_date": "2025-11-27 21:53:09 UTC"
  },
  {
    "arxiv_id": "2511.22767v1",
    "title": "Agentic AI Framework for Cloudburst Prediction and Coordinated Response",
    "authors": [
      "Toqeer Ali Syed",
      "Sohail Khan",
      "Salman Jan",
      "Gohar Ali",
      "Muhammad Nauman",
      "Ali Akarma",
      "Ahmad Ali"
    ],
    "abstract": "The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22767v1",
    "published_date": "2025-11-27 21:33:03 UTC",
    "updated_date": "2025-11-27 21:33:03 UTC"
  },
  {
    "arxiv_id": "2511.22759v1",
    "title": "MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models",
    "authors": [
      "Jorge Alberto Garza-Abdala",
      "Gerardo A. Fumagal-González",
      "Daly Avendano",
      "Servando Cardona",
      "Sadam Hussain",
      "Eduardo de Avila-Armenta",
      "Jasiel H. Toscano-Martínez",
      "Diana S. M. Rosales Gurmendi",
      "Alma A. Pedro-Pérez",
      "Jose Gerardo Tamez-Pena"
    ],
    "abstract": "Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22759v1",
    "published_date": "2025-11-27 21:10:36 UTC",
    "updated_date": "2025-11-27 21:10:36 UTC"
  },
  {
    "arxiv_id": "2512.00120v1",
    "title": "Art2Music: Generating Music for Art Images with Multi-modal Feeling Alignment",
    "authors": [
      "Jiaying Hong",
      "Ting Zhu",
      "Thanet Markchom",
      "Huizhi Liang"
    ],
    "abstract": "With the rise of AI-generated content (AIGC), generating perceptually natural and feeling-aligned music from multimodal inputs has become a central challenge. Existing approaches often rely on explicit emotion labels that require costly annotation, underscoring the need for more flexible feeling-aligned methods. To support multimodal music generation, we construct ArtiCaps, a pseudo feeling-aligned image-music-text dataset created by semantically matching descriptions from ArtEmis and MusicCaps. We further propose Art2Music, a lightweight cross-modal framework that synthesizes music from artistic images and user comments. In the first stage, images and text are encoded with OpenCLIP and fused using a gated residual module; the fused representation is decoded by a bidirectional LSTM into Mel-spectrograms with a frequency-weighted L1 loss to enhance high-frequency fidelity. In the second stage, a fine-tuned HiFi-GAN vocoder reconstructs high-quality audio waveforms. Experiments on ArtiCaps show clear improvements in Mel-Cepstral Distortion, Frechet Audio Distance, Log-Spectral Distance, and cosine similarity. A small LLM-based rating study further verifies consistent cross-modal feeling alignment and offers interpretable explanations of matches and mismatches across modalities. These results demonstrate improved perceptual naturalness, spectral fidelity, and semantic consistency. Art2Music also maintains robust performance with only 50k training samples, providing a scalable solution for feeling-aligned creative audio generation in interactive art, personalized soundscapes, and digital art exhibitions.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00120v1",
    "published_date": "2025-11-27 21:05:53 UTC",
    "updated_date": "2025-11-27 21:05:53 UTC"
  },
  {
    "arxiv_id": "2511.22751v1",
    "title": "Exact Learning of Arithmetic with Differentiable Agents",
    "authors": [
      "Hristo Papazov",
      "Francesco D'Angelo",
      "Nicolas Flammarion"
    ],
    "abstract": "We explore the possibility of exact algorithmic learning with gradient-based methods and introduce a differentiable framework capable of strong length generalization on arithmetic tasks. Our approach centers on Differentiable Finite-State Transducers (DFSTs), a Turing-complete model family that avoids the pitfalls of prior architectures by enabling constant-precision, constant-time generation, and end-to-end log-parallel differentiable training. Leveraging policy-trajectory observations from expert agents, we train DFSTs to perform binary and decimal addition and multiplication. Remarkably, models trained on tiny datasets generalize without error to inputs thousands of times longer than the training examples. These results show that training differentiable agents on structured intermediate supervision could pave the way towards exact gradient-based learning of algorithmic skills. Code available at \\href{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}{https://github.com/dngfra/differentiable-exact-algorithmic-learner.git}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: MATH-AI",
    "pdf_url": "https://arxiv.org/pdf/2511.22751v1",
    "published_date": "2025-11-27 20:51:16 UTC",
    "updated_date": "2025-11-27 20:51:16 UTC"
  },
  {
    "arxiv_id": "2511.22749v1",
    "title": "VeriDispatcher: Multi-Model Dispatching through Pre-Inference Difficulty Prediction for RTL Generation Optimization",
    "authors": [
      "Zeng Wang",
      "Weihua Xiao",
      "Minghao Shao",
      "Raghu Vamshi Hemadri",
      "Ozgur Sinanoglu",
      "Muhammad Shafique",
      "Ramesh Karri"
    ],
    "abstract": "Large Language Models (LLMs) show strong performance in RTL generation, but different models excel on different tasks because of architecture and training differences. Prior work mainly prompts or finetunes a single model. What remains not well studied is how to coordinate multiple different LLMs so they jointly improve RTL quality while also reducing cost, instead of running all models and choosing the best output. We define this as the multi-LLM RTL generation problem. We propose VeriDispatcher, a multi-LLM RTL generation framework that dispatches each RTL task to suitable LLMs based on pre-inference difficulty prediction. For each model, we train a compact classifier over semantic embeddings of task descriptions, using difficulty scores derived from benchmark variants that combine syntax, structural similarity, and functional correctness. At inference, VeriDispatcher uses these predictors to route tasks to a selected subset of LLMs. Across 10 diverse LLMs on RTLLM and VerilogEval, VeriDispatcher achieves up to 18% accuracy improvement on RTLLM using only 40% of commercial calls, and on VerilogEval maintains accuracy while reducing commercial usage by 25%, enabling cost-effective, high-quality LLM deployment in hardware design automation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22749v1",
    "published_date": "2025-11-27 20:45:26 UTC",
    "updated_date": "2025-11-27 20:45:26 UTC"
  },
  {
    "arxiv_id": "2512.00119v1",
    "title": "NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration",
    "authors": [
      "Zeng Wang",
      "Minghao Shao",
      "Akashdeep Saha",
      "Ramesh Karri",
      "Johann Knechtel",
      "Muhammad Shafique",
      "Ozgur Sinanoglu"
    ],
    "abstract": "Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) with reinforcement learning (RL) in a systematic manner, enabling focused local rewriting. The RL agent identifies netlist components critical for GNN-based reasoning, while the LLM devises rewriting plans to diversify motifs that preserve functionality. Iterative feedback between the RL and LLM stages refines adversarial rewritings to limit overheads. Compared to the SOTA work AttackGNN, NetDeTox successfully degrades the effectiveness of all security schemes with fewer rewrites and substantially lower area overheads (reductions of 54.50% for GNN-RE, 25.44% for GNN4IP, and 41.04% for OMLA, respectively). For GNN4IP, ours can even optimize/reduce the original benchmarks' area, in particular for larger circuits, demonstrating the practicality and scalability of NetDeTox.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00119v1",
    "published_date": "2025-11-27 20:45:00 UTC",
    "updated_date": "2025-11-27 20:45:00 UTC"
  },
  {
    "arxiv_id": "2511.22739v1",
    "title": "All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning",
    "authors": [
      "Amir Mohammad Ezzati",
      "Alireza Malekhosseini",
      "Armin Khosravi",
      "Mohammad Hossein Rohban"
    ],
    "abstract": "Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22739v1",
    "published_date": "2025-11-27 20:18:04 UTC",
    "updated_date": "2025-11-27 20:18:04 UTC"
  },
  {
    "arxiv_id": "2511.22737v1",
    "title": "Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being",
    "authors": [
      "Salman Jan",
      "Toqeer Ali Syed",
      "Gohar Ali",
      "Ali Akarma",
      "Mohammad Riyaz Belgaum",
      "Ahmad Ali"
    ],
    "abstract": "The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "Presented at International Conference on Business and Digital Technology, Bahrain, Springer Nature, 27 November 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22737v1",
    "published_date": "2025-11-27 20:08:12 UTC",
    "updated_date": "2025-11-27 20:08:12 UTC"
  },
  {
    "arxiv_id": "2512.02056v2",
    "title": "Reversing Large Language Models for Efficient Training and Fine-Tuning",
    "authors": [
      "Eshed Gal",
      "Moshe Eliasof",
      "Javier Turek",
      "Uri Ascher",
      "Eran Treister",
      "Eldad Haber"
    ],
    "abstract": "Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.02056v2",
    "published_date": "2025-11-27 19:32:15 UTC",
    "updated_date": "2025-12-04 02:57:52 UTC"
  },
  {
    "arxiv_id": "2511.22729v1",
    "title": "Solving Context Window Overflow in AI Agents",
    "authors": [
      "Anton Bulle Labate",
      "Valesca Moura de Sousa",
      "Sandro Rama Fiorini",
      "Leonardo Guerreiro Azevedo",
      "Raphael Melo Thiago",
      "Viviane Torres da Silva"
    ],
    "abstract": "Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22729v1",
    "published_date": "2025-11-27 19:22:20 UTC",
    "updated_date": "2025-11-27 19:22:20 UTC"
  },
  {
    "arxiv_id": "2512.02055v1",
    "title": "Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale",
    "authors": [
      "Mirela G. Tulbure",
      "Julio Caineta",
      "Mark Broich",
      "Mollie D. Gaines",
      "Philippe Rufin",
      "Leon-Friedrich Thomas",
      "Hamed Alemohammad",
      "Jan Hemmerling",
      "Patrick Hostert"
    ],
    "abstract": "Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.\n  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.\n  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.02055v1",
    "published_date": "2025-11-27 19:04:01 UTC",
    "updated_date": "2025-11-27 19:04:01 UTC"
  },
  {
    "arxiv_id": "2511.22715v1",
    "title": "ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering",
    "authors": [
      "Alberto Compagnoni",
      "Marco Morini",
      "Sara Sarto",
      "Federico Cocchi",
      "Davide Caffagni",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22715v1",
    "published_date": "2025-11-27 19:01:02 UTC",
    "updated_date": "2025-11-27 19:01:02 UTC"
  },
  {
    "arxiv_id": "2512.03076v1",
    "title": "Will Power Return to the Clouds? From Divine Authority to GenAI Authority",
    "authors": [
      "Mohammad Saleh Torkestani",
      "Taha Mansouri"
    ],
    "abstract": "Generative AI systems now mediate newsfeeds, search rankings, and creative content for hundreds of millions of users, positioning a handful of private firms as de-facto arbiters of truth. Drawing on a comparative-historical lens, this article juxtaposes the Galileo Affair, a touchstone of clerical knowledge control, with contemporary Big-Tech content moderation. We integrate Foucault's power/knowledge thesis, Weber's authority types (extended to a rational-technical and emerging agentic-technical modality), and Floridi's Dataism to analyze five recurrent dimensions: disciplinary power, authority modality, data pluralism, trust versus reliance, and resistance pathways. Primary sources (Inquisition records; platform transparency reports) and recent empirical studies on AI trust provide the evidentiary base. Findings show strong structural convergences: highly centralized gatekeeping, legitimacy claims couched in transcendent principles, and systematic exclusion of marginal voices. Divergences lie in temporal velocity, global scale, and the widening gap between public reliance and trust in AI systems. Ethical challenges cluster around algorithmic opacity, linguistic inequity, bias feedback loops, and synthetic misinformation. We propose a four-pillar governance blueprint: (1) a mandatory international model-registry with versioned policy logs, (2) representation quotas and regional observatories to de-center English-language hegemony, (3) mass critical-AI literacy initiatives, and (4) public-private support for community-led data trusts. Taken together, these measures aim to narrow the trust-reliance gap and prevent GenAI from hardcoding a twenty-first-century digital orthodoxy.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03076v1",
    "published_date": "2025-11-27 18:59:44 UTC",
    "updated_date": "2025-11-27 18:59:44 UTC"
  },
  {
    "arxiv_id": "2511.22707v1",
    "title": "CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation",
    "authors": [
      "Tianxin Wei",
      "Xuying Ning",
      "Xuxing Chen",
      "Ruizhong Qiu",
      "Yupeng Hou",
      "Yan Xie",
      "Shuang Yang",
      "Zhigang Hua",
      "Jingrui He"
    ],
    "abstract": "In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22707v1",
    "published_date": "2025-11-27 18:59:35 UTC",
    "updated_date": "2025-11-27 18:59:35 UTC"
  },
  {
    "arxiv_id": "2512.08951v1",
    "title": "AI Co-Artist: A LLM-Powered Framework for Interactive GLSL Shader Animation Evolution",
    "authors": [
      "Kamer Ali Yuksel",
      "Hassan Sawaf"
    ],
    "abstract": "Creative coding and real-time shader programming are at the forefront of interactive digital art, enabling artists, designers, and enthusiasts to produce mesmerizing, complex visual effects that respond to real-time stimuli such as sound or user interaction. However, despite the rich potential of tools like GLSL, the steep learning curve and requirement for programming fluency pose substantial barriers for newcomers and even experienced artists who may not have a technical background. In this paper, we present AI Co-Artist, a novel interactive system that harnesses the capabilities of large language models (LLMs), specifically GPT-4, to support the iterative evolution and refinement of GLSL shaders through a user-friendly, visually-driven interface. Drawing inspiration from the user-guided evolutionary principles pioneered by the Picbreeder platform, our system empowers users to evolve shader art using intuitive interactions, without needing to write or understand code. AI Co-Artist serves as both a creative companion and a technical assistant, allowing users to explore a vast generative design space of real-time visual art. Through comprehensive evaluations, including structured user studies and qualitative feedback, we demonstrate that AI Co-Artist significantly reduces the technical threshold for shader creation, enhances creative outcomes, and supports a wide range of users in producing professional-quality visual effects. Furthermore, we argue that this paradigm is broadly generalizable. By leveraging the dual strengths of LLMs-semantic understanding and program synthesis, our method can be applied to diverse creative domains, including website layout generation, architectural visualizations, product prototyping, and infographics.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.08951v1",
    "published_date": "2025-11-27 18:55:32 UTC",
    "updated_date": "2025-11-27 18:55:32 UTC"
  },
  {
    "arxiv_id": "2511.22696v3",
    "title": "Probabilistic Fusion and Calibration of Neural Speaker Diarization Models",
    "authors": [
      "Juan Ignacio Alvarez-Trejos",
      "Sergio A. Balanya",
      "Daniel Ramos",
      "Alicia Lozano-Diez"
    ],
    "abstract": "End-to-End Neural Diarization (EEND) systems produce frame-level probabilistic speaker activity estimates, yet since evaluation focuses primarily on Diarization Error Rate (DER), the reliability and calibration of these confidence scores have been largely neglected. When fusing multiple diarization systems, DOVER-Lap remains the only established approach, operating at the segment level with hard decisions. We propose working with continuous probability outputs, which enables more sophisticated fusion and calibration techniques that can leverage model uncertainty and complementary strengths across different architectures. This paper presents the first comprehensive framework for calibrating and fusing EEND models at the probability level. We investigate two output formulations (multilabel and powerset representations) and their impact on calibration and fusion effectiveness. Through extensive experiments on the CallHome two-speaker benchmark, we demonstrate that proper calibration provides substantial improvements even for individual models (up to 19% relative DER reduction), in some cases mitigating the absence of domain adaptation. We reveal that joint calibration in powerset space consistently outperforms independent per-speaker calibration, that fusion substantially improves over individual models, and that the Fuse-then-Calibrate ordering generally outperforms both calibrating before fusion and uncalibrated fusion while requiring calibration of only a single combined model. Our best configuration outperforms DOVER-Lap in terms of DER while providing reliable confidence estimates essential for downstream applications. This work proposes best practices for probability-level fusion of EEND systems and demonstrates the advantages of leveraging soft outputs over hard decisions.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22696v3",
    "published_date": "2025-11-27 18:50:16 UTC",
    "updated_date": "2025-12-03 09:49:38 UTC"
  },
  {
    "arxiv_id": "2511.22688v1",
    "title": "Test-time scaling of diffusions with flow maps",
    "authors": [
      "Amirmojtaba Sabour",
      "Michael S. Albergo",
      "Carles Domingo-Enrich",
      "Nicholas M. Boffi",
      "Sanja Fidler",
      "Karsten Kreis",
      "Eric Vanden-Eijnden"
    ],
    "abstract": "A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22688v1",
    "published_date": "2025-11-27 18:44:12 UTC",
    "updated_date": "2025-11-27 18:44:12 UTC"
  },
  {
    "arxiv_id": "2512.07882v1",
    "title": "Referenceless Proton Resonance Frequency Thermometry Using Deep Learning with Self-Attention",
    "authors": [
      "Yueran Zhao",
      "Chang-Sheng Mei",
      "Nathan J. McDannold",
      "Shenyan Zong",
      "Guofeng Shen"
    ],
    "abstract": "Background: Accurate proton resonance frequency (PRF) MR thermometry is essential for monitoring temperature rise during thermal ablation with high intensity focused ultrasound (FUS). Conventional referenceless methods such as complex field estimation (CFE) and phase finite difference (PFD) tend to exhibit errors when susceptibility-induced phase discontinuities occur at tissue interfaces.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.07882v1",
    "published_date": "2025-11-27 18:36:14 UTC",
    "updated_date": "2025-11-27 18:36:14 UTC"
  },
  {
    "arxiv_id": "2511.22679v2",
    "title": "Foundations of Quantum Granular Computing with Effect-Based Granules, Algebraic Properties and Reference Architectures",
    "authors": [
      "Oscar Montiel Ross"
    ],
    "abstract": "This paper develops the foundations of Quantum Granular Computing (QGC), extending classical granular computing including fuzzy, rough, and shadowed granules to the quantum regime. Quantum granules are modeled as effects on a finite dimensional Hilbert space, so granular memberships are given by Born probabilities. This operator theoretic viewpoint provides a common language for sharp (projective) and soft (nonprojective) granules and embeds granulation directly into the standard formalism of quantum information theory. We establish foundational results for effect based quantum granules, including normalization and monotonicity properties, the emergence of Boolean islands from commuting families, granular refinement under Luders updates, and the evolution of granules under quantum channels via the adjoint channel in the Heisenberg picture. We connect QGC with quantum detection and estimation theory by interpreting the effect operators realizing Helstrom minimum error measurement for binary state discrimination as Helstrom type decision granules, i.e., soft quantum counterparts of Bayes optimal decision regions. Building on these results, we introduce Quantum Granular Decision Systems (QGDS) with three reference architectures that specify how quantum granules can be defined, learned, and integrated with classical components while remaining compatible with near term quantum hardware. Case studies on qubit granulation, two qubit parity effects, and Helstrom style soft decisions illustrate how QGC reproduces fuzzy like graded memberships and smooth decision boundaries while exploiting noncommutativity, contextuality, and entanglement. The framework thus provides a unified and mathematically grounded basis for operator valued granules in quantum information processing, granular reasoning, and intelligent systems.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Three figures and the graphical abstract",
    "pdf_url": "https://arxiv.org/pdf/2511.22679v2",
    "published_date": "2025-11-27 18:29:08 UTC",
    "updated_date": "2025-12-03 16:07:26 UTC"
  },
  {
    "arxiv_id": "2511.22659v1",
    "title": "Geometrically-Constrained Agent for Spatial Reasoning",
    "authors": [
      "Zeren Chen",
      "Xiaoya Lu",
      "Zhijie Zheng",
      "Pengrui Li",
      "Lehan He",
      "Yijin Zhou",
      "Jing Shao",
      "Bohan Zhuang",
      "Lu Sheng"
    ],
    "abstract": "Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 13 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22659v1",
    "published_date": "2025-11-27 17:50:37 UTC",
    "updated_date": "2025-11-27 17:50:37 UTC"
  },
  {
    "arxiv_id": "2511.22651v1",
    "title": "Automated Design Optimization via Strategic Search with Large Language Models",
    "authors": [
      "Anthony Carreon",
      "Vansh Sharma",
      "Venkat Raman"
    ],
    "abstract": "Traditional optimization methods excel in well-defined search spaces but struggle with design problems where transformations and design parameters are difficult to define. Large language models (LLMs) offer a promising alternative by dynamically interpreting design spaces and leveraging encoded domain knowledge. To this end, we introduce AUTO, an LLM agent framework that treats design optimization as a gradient-free search problem guided by strategic LLM reasoning. The framework employs two collaborative agents: a Strategist that selects between exploration and exploitation strategies, and an Implementor that executes detailed designs. Applied to GPU code optimization -- a domain critical to fields from machine learning to scientific computing -- AUTO generates solutions competitive with expert implementations for chemical kinetics integration and dense matrix multiplication. The framework achieves 50-70% search efficiency relative to Bayesian optimization methodologies. It completes optimizations in approximately 8 hours at an estimated cost of up to \\$159 per run, compared to an estimated cost of up to \\$480 with median-wage software developers. These findings open the door to automating design optimization in ill-defined search spaces with limited prior information.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 tables, 7 figures, preprint",
    "pdf_url": "https://arxiv.org/pdf/2511.22651v1",
    "published_date": "2025-11-27 17:42:05 UTC",
    "updated_date": "2025-11-27 17:42:05 UTC"
  },
  {
    "arxiv_id": "2511.22632v1",
    "title": "Optimized Agent Shift Scheduling Using Multi-Phase Allocation Approach",
    "authors": [
      "Sanalkumar K",
      "Koushik Dey",
      "Swati Meena"
    ],
    "abstract": "Effective agent shift scheduling is crucial for businesses, especially in the Contact Center as a Service (CCaaS) industry, to ensure seamless operations and fulfill employee needs. Most studies utilizing mathematical model-based solutions approach the problem as a single-step process, often resulting in inefficiencies and high computational demands. In contrast, we present a multi-phase allocation method that addresses scalability and accuracy by dividing the problem into smaller sub-problems of day and shift allocation, which significantly reduces number of computational variables and allows for targeted objective functions, ultimately enhancing both efficiency and accuracy. Each subproblem is modeled as a Integer Programming Problem (IPP), with solutions sequentially feeding into the subsequent subproblem. We then apply the proposed method, using a multi-objective framework, to address the difficulties posed by peak demand scenarios such as holiday rushes, where maintaining service levels is essential despite having limited number of employees",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22632v1",
    "published_date": "2025-11-27 17:10:59 UTC",
    "updated_date": "2025-11-27 17:10:59 UTC"
  },
  {
    "arxiv_id": "2511.22619v2",
    "title": "AI Deception: Risks, Dynamics, and Controls",
    "authors": [
      "Boyuan Chen",
      "Sitong Fang",
      "Jiaming Ji",
      "Yanxu Zhu",
      "Pengcheng Wen",
      "Jinzhou Wu",
      "Yingshui Tan",
      "Boren Zheng",
      "Mengying Yuan",
      "Wenqi Chen",
      "Donghai Hong",
      "Alex Qiu",
      "Xin Chen",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Borong Zhang",
      "Tianzhuo Yang",
      "Saad Siddiqui",
      "Isabella Duan",
      "Yawen Duan",
      "Brian Tse",
      "Jen-Tse",
      "Huang",
      "Kun Wang",
      "Baihui Zheng",
      "Jiaheng Liu",
      "Jian Yang",
      "Yiming Li",
      "Wenting Chen",
      "Dongrui Liu",
      "Lukas Vierling",
      "Zhiheng Xi",
      "Haobo Fu",
      "Wenxuan Wang",
      "Jitao Sang",
      "Zhengyan Shi",
      "Chi-Min Chan",
      "Eugenie Shi",
      "Simin Li",
      "Juncheng Li",
      "Jian Yang",
      "Wei Ji",
      "Dong Li",
      "Jinglin Yang",
      "Jun Song",
      "Yinpeng Dong",
      "Jie Fu",
      "Bo Zheng",
      "Min Yang",
      "Yike Guo",
      "Philip Torr",
      "Robert Trager",
      "Yi Zeng",
      "Zhongyuan Wang",
      "Yaodong Yang",
      "Tiejun Huang",
      "Ya-Qin Zhang",
      "Hongjiang Zhang",
      "Andrew Yao"
    ],
    "abstract": "As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstrated risk across language models, AI agents, and emerging frontier systems. This project provides a comprehensive and up-to-date overview of the AI deception field, covering its core concepts, methodologies, genesis, and potential mitigations. First, we identify a formal definition of AI deception, grounded in signaling theory from studies of animal deception. We then review existing empirical studies and associated risks, highlighting deception as a sociotechnical safety challenge. We organize the landscape of AI deception research as a deception cycle, consisting of two key components: deception emergence and deception treatment. Deception emergence reveals the mechanisms underlying AI deception: systems with sufficient capability and incentive potential inevitably engage in deceptive behaviors when triggered by external conditions. Deception treatment, in turn, focuses on detecting and addressing such behaviors. On deception emergence, we analyze incentive foundations across three hierarchical levels and identify three essential capability preconditions required for deception. We further examine contextual triggers, including supervision gaps, distributional shifts, and environmental pressures. On deception treatment, we conclude detection methods covering benchmarks and evaluation protocols in static and interactive settings. Building on the three core factors of deception emergence, we outline potential mitigation strategies and propose auditing approaches that integrate technical, community, and governance efforts to address sociotechnical challenges and future AI risks. To support ongoing work in this area, we release a living resource at www.deceptionsurvey.com.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22619v2",
    "published_date": "2025-11-27 16:56:04 UTC",
    "updated_date": "2025-12-03 15:35:14 UTC"
  },
  {
    "arxiv_id": "2511.22613v2",
    "title": "Variational analysis of determinantal varieties",
    "authors": [
      "Yan Yang",
      "Bin Gao",
      "Ya-xiang Yuan"
    ],
    "abstract": "Determinantal varieties -- the sets of bounded-rank matrices or tensors -- have attracted growing interest in low-rank optimization. The tangent cone to low-rank sets is widely studied and underpins a range of geometric methods. The second-order geometry, which encodes curvature information, is more intricate. In this work, we develop a unified framework to derive explicit formulas for both first- and second-order tangent sets to various low-rank sets, including low-rank matrices, tensors, symmetric matrices, and positive semidefinite matrices. The framework also accommodates the intersection of a low-rank set and another set satisfying mild assumptions, thereby yielding a tangent intersection rule. Through the lens of tangent sets, we establish a necessary and sufficient condition under which a nonsmooth problem and its smooth parameterization share equivalent second-order stationary points. Moreover, we exploit tangent sets to characterize optimality conditions for low-rank optimization and prove that verifying second-order optimality is NP-hard. In a separate line of analysis, we investigate variational geometry of the graph of the normal cone to matrix varieties, deriving the explicit Bouligand tangent cone, Fréchet and Mordukhovich normal cones to the graph. These results are further applied to develop optimality conditions for low-rank bilevel programs.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "76 pages, 6 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22613v2",
    "published_date": "2025-11-27 16:48:07 UTC",
    "updated_date": "2025-12-11 05:13:53 UTC"
  },
  {
    "arxiv_id": "2511.22607v1",
    "title": "GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing",
    "authors": [
      "Xiaoyin Yang"
    ],
    "abstract": "Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22607v1",
    "published_date": "2025-11-27 16:41:32 UTC",
    "updated_date": "2025-11-27 16:41:32 UTC"
  },
  {
    "arxiv_id": "2511.22594v1",
    "title": "HarmoCLIP: Harmonizing Global and Regional Representations in Contrastive Vision-Language Models",
    "authors": [
      "Haoxi Zeng",
      "Haoxuan Li",
      "Yi Bin",
      "Pengpeng Zeng",
      "Xing Xu",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable generalization ability and strong performance across a wide range of vision-language tasks. However, due to the lack of region-level supervision, CLIP exhibits limited fine-grained semantic understanding. Although several methods attempt to mitigate this issue, they unintentionally disrupt the global alignment, resulting in a persistent trade-off where improving local perception simultaneously degrades global coherence. In this paper, we propose HarmoCLIP, a novel framework designed to harmonize global and region representations within CLIP. We first identify that the absence of direct alignment between local textual and visual semantics is the fundamental cause of the trade-off. To address this, HarmoCLIP introduces an explicit fine-grained semantic supervision term that directly aligns textual segments with their corresponding visual regions, effectively bridging the image region space and the textual space. To further strengthen the representation capability at the local level, our method introduces a novel Region-Language Alignment supervision strategy that promotes fine-grained semantic learning without compromising global semantic consistency. Extensive experiments demonstrate that HarmoCLIP achieves state-of-the-art (improvement up to 69.78%) performance on the global task of retrieval and yields a substantial 3.2% improvement in Top-1 accuracy on the region task of bounding-box classification, consistently outperforming prior approaches while providing a balanced, efficient, and plug-and-play solution to the global-local trade-off in CLIP. Code is available at https://github.com/Erosist/HarmoCLIP.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "13 pages, 7 figures, 6 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22594v1",
    "published_date": "2025-11-27 16:24:53 UTC",
    "updated_date": "2025-11-27 16:24:53 UTC"
  },
  {
    "arxiv_id": "2511.22586v1",
    "title": "Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization",
    "authors": [
      "Yifan Du",
      "Kun Zhou",
      "Yingqian Min",
      "Yue Ling",
      "Wayne Xin Zhao",
      "Youbin Wu"
    ],
    "abstract": "We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as \"think with image\", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a \"short is long\" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22586v1",
    "published_date": "2025-11-27 16:19:34 UTC",
    "updated_date": "2025-11-27 16:19:34 UTC"
  },
  {
    "arxiv_id": "2511.22570v1",
    "title": "DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning",
    "authors": [
      "Zhihong Shao",
      "Yuxiang Luo",
      "Chengda Lu",
      "Z. Z. Ren",
      "Jiewen Hu",
      "Tian Ye",
      "Zhibin Gou",
      "Shirong Ma",
      "Xiaokang Zhang"
    ],
    "abstract": "Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22570v1",
    "published_date": "2025-11-27 16:01:22 UTC",
    "updated_date": "2025-11-27 16:01:22 UTC"
  },
  {
    "arxiv_id": "2511.22567v1",
    "title": "Where to Measure: Epistemic Uncertainty-Based Sensor Placement with ConvCNPs",
    "authors": [
      "Feyza Eksen",
      "Stefan Oehmcke",
      "Stefan Lüdtke"
    ],
    "abstract": "Accurate sensor placement is critical for modeling spatio-temporal systems such as environmental and climate processes. Neural Processes (NPs), particularly Convolutional Conditional Neural Processes (ConvCNPs), provide scalable probabilistic models with uncertainty estimates, making them well-suited for data-driven sensor placement. However, existing approaches rely on total predictive uncertainty, which conflates epistemic and aleatoric components, that may lead to suboptimal sensor selection in ambiguous regions. To address this, we propose expected reduction in epistemic uncertainty as a new acquisition function for sensor placement. To enable this, we extend ConvCNPs with a Mixture Density Networks (MDNs) output head for epistemic uncertainty estimation. Preliminary results suggest that epistemic uncertainty driven sensor placement more effectively reduces model error than approaches based on overall uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22567v1",
    "published_date": "2025-11-27 16:00:45 UTC",
    "updated_date": "2025-11-27 16:00:45 UTC"
  },
  {
    "arxiv_id": "2511.22565v1",
    "title": "Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation",
    "authors": [
      "Yannick Brunink",
      "Daniel Daza",
      "Yunjie He",
      "Michael Cochez"
    ],
    "abstract": "Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22565v1",
    "published_date": "2025-11-27 15:57:29 UTC",
    "updated_date": "2025-11-27 15:57:29 UTC"
  },
  {
    "arxiv_id": "2511.22536v1",
    "title": "A Computable Game-Theoretic Framework for Multi-Agent Theory of Mind",
    "authors": [
      "Fengming Zhu",
      "Yuxin Pan",
      "Xiaomeng Zhu",
      "Fangzhen Lin"
    ],
    "abstract": "Originating in psychology, $\\textit{Theory of Mind}$ (ToM) has attracted significant attention across multiple research communities, especially logic, economics, and robotics. Most psychological work does not aim at formalizing those central concepts, namely $\\textit{goals}$, $\\textit{intentions}$, and $\\textit{beliefs}$, to automate a ToM-based computational process, which, by contrast, has been extensively studied by logicians. In this paper, we offer a different perspective by proposing a computational framework viewed through the lens of game theory. On the one hand, the framework prescribes how to make boudedly rational decisions while maintaining a theory of mind about others (and recursively, each of the others holding a theory of mind about the rest); on the other hand, it employs statistical techniques and approximate solutions to retain computability of the inherent computational problem.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Ongoing work. A preliminary version has been accepted by the AAAI 2026 Theory of Mind for AI (ToM4AI) Workshop",
    "pdf_url": "https://arxiv.org/pdf/2511.22536v1",
    "published_date": "2025-11-27 15:13:45 UTC",
    "updated_date": "2025-11-27 15:13:45 UTC"
  },
  {
    "arxiv_id": "2511.22532v1",
    "title": "CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving",
    "authors": [
      "Zhaohui Wang",
      "Tengbo Yu",
      "Hao Tang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22532v1",
    "published_date": "2025-11-27 15:13:13 UTC",
    "updated_date": "2025-11-27 15:13:13 UTC"
  },
  {
    "arxiv_id": "2511.22521v1",
    "title": "DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA",
    "authors": [
      "Ahmad Mohammadshirazi",
      "Pinaki Prasad Guha Neogi",
      "Dheeraj Kulshrestha",
      "Rajiv Ramnath"
    ],
    "abstract": "Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\\% ANLS and 82.4\\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22521v1",
    "published_date": "2025-11-27 15:00:58 UTC",
    "updated_date": "2025-11-27 15:00:58 UTC"
  },
  {
    "arxiv_id": "2511.22493v1",
    "title": "HW-GNN: Homophily-Aware Gaussian-Window Constrained Graph Spectral Network for Social Network Bot Detection",
    "authors": [
      "Zida Liu",
      "Jun Gao",
      "Zhang Ji",
      "Li Zhao"
    ],
    "abstract": "Social bots are increasingly polluting online platforms by spreading misinformation and engaging in coordinated manipulation, posing severe threats to cybersecurity. Graph Neural Networks (GNNs) have become mainstream for social bot detection due to their ability to integrate structural and attribute features, with spectral-based approaches demonstrating particular efficacy due to discriminative patterns in the spectral domain. However, current spectral GNN methods face two limitations: (1) their broad-spectrum fitting mechanisms degrade the focus on bot-specific spectral features, and (2) certain domain knowledge valuable for bot detection, e.g., low homophily correlates with high-frequency features, has not been fully incorporated into existing methods.\n  To address these challenges, we propose HW-GNN, a novel homophily-aware graph spectral network with Gaussian window constraints. Our framework introduces two key innovations: (i) a Gaussian-window constrained spectral network that employs learnable Gaussian windows to highlight bot-related spectral features, and (ii) a homophily-aware adaptation mechanism that injects domain knowledge between homophily ratios and frequency features into the Gaussian window optimization process. Through extensive experimentation on multiple benchmark datasets, we demonstrate that HW-GNN achieves state-of-the-art bot detection performance, outperforming existing methods with an average improvement of 4.3% in F1-score, while exhibiting strong plug-in compatibility with existing spectral GNNs.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22493v1",
    "published_date": "2025-11-27 14:29:40 UTC",
    "updated_date": "2025-11-27 14:29:40 UTC"
  },
  {
    "arxiv_id": "2511.22482v1",
    "title": "Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?",
    "authors": [
      "Isabel Gonçalves",
      "Paulo Cavalin",
      "Claudio Pinhanez"
    ],
    "abstract": "Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22482v1",
    "published_date": "2025-11-27 14:15:14 UTC",
    "updated_date": "2025-11-27 14:15:14 UTC"
  },
  {
    "arxiv_id": "2512.00113v1",
    "title": "From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors",
    "authors": [
      "Amirreza Yousefzadeh"
    ],
    "abstract": "Digital neuromorphic processors are emerging as a promising computing substrate for low-power, always-on EdgeAI applications. In this tutorial paper, we outline the main architectural design principles behind fully digital neuromorphic processors and illustrate them using the SENECA platform as a running example. Starting from a flexible array of tiny RISC-V processing cores connected by a simple Network-on-Chip (NoC), we show how to progressively evolve the architecture: from a baseline event-driven implementation of fully connected networks, to versions with dedicated Neural Processing Elements (NPEs) and a loop controller that offloads fine-grained control from the general-purpose cores. Along the way, we discuss software and mapping techniques such as spike grouping, event-driven depth-first convolution for convolutional networks, and hard-attention style processing for high-resolution event-based vision. The focus is on architectural trade-offs, performance and energy bottlenecks, and on leveraging flexibility to incrementally add domain-specific acceleration. This paper assumes familiarity with basic neuromorphic concepts (spikes, event-driven computation, sparse activation) and deep neural network workloads. It does not present new experimental results; instead, it synthesizes and contextualizes findings previously reported in our SENECA publications to provide a coherent, step-by-step architectural perspective for students and practitioners who wish to design their own digital neuromorphic processors.",
    "categories": [
      "cs.AR",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.AR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00113v1",
    "published_date": "2025-11-27 14:14:23 UTC",
    "updated_date": "2025-11-27 14:14:23 UTC"
  },
  {
    "arxiv_id": "2511.22448v1",
    "title": "Structured Extraction from Business Process Diagrams Using Vision-Language Models",
    "authors": [
      "Pritam Deka",
      "Barry Devereux"
    ],
    "abstract": "Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "To appear in the Proceedings of the 2026 ACM Symposium on Applied Computing (SAC '26)",
    "pdf_url": "https://arxiv.org/pdf/2511.22448v1",
    "published_date": "2025-11-27 13:35:57 UTC",
    "updated_date": "2025-11-27 13:35:57 UTC"
  },
  {
    "arxiv_id": "2511.22442v1",
    "title": "What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$",
    "authors": [
      "Sébastien Piérard",
      "Adrien Deliège",
      "Marc Van Droogenbroeck"
    ],
    "abstract": "Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_β$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_β$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_β$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $β$ for any distribution or set of performances, and we illustrate their use on six case studies.",
    "categories": [
      "cs.PF",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.PF",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22442v1",
    "published_date": "2025-11-27 13:29:50 UTC",
    "updated_date": "2025-11-27 13:29:50 UTC"
  },
  {
    "arxiv_id": "2511.22441v1",
    "title": "GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents",
    "authors": [
      "Xinyu Zhang",
      "Yixin Wu",
      "Boyang Zhang",
      "Chenhao Lin",
      "Chao Shen",
      "Michael Backes",
      "Yang Zhang"
    ],
    "abstract": "Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the \"unknown\" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages with 7 figures and 12 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22441v1",
    "published_date": "2025-11-27 13:27:26 UTC",
    "updated_date": "2025-11-27 13:27:26 UTC"
  },
  {
    "arxiv_id": "2511.22434v1",
    "title": "FastFHE: Packing-Scalable and Depthwise-Separable CNN Inference Over FHE",
    "authors": [
      "Wenbo Song",
      "Xinxin Fan",
      "Quanliang Jing",
      "Shaoye Luo",
      "Wenqi Wei",
      "Chi Lin",
      "Yunfeng Lu",
      "Ling Liu"
    ],
    "abstract": "The deep learning (DL) has been penetrating daily life in many domains, how to keep the DL model inference secure and sample privacy in an encrypted environment has become an urgent and increasingly important issue for various security-critical applications. To date, several approaches have been proposed based on the Residue Number System variant of the Cheon-Kim-Kim-Song (RNS-CKKS) scheme. However, they all suffer from high latency, which severely limits the applications in real-world tasks. Currently, the research on encrypted inference in deep CNNs confronts three main bottlenecks: i) the time and storage costs of convolution calculation; ii) the time overhead of huge bootstrapping operations; and iii) the consumption of circuit multiplication depth. Towards these three challenges, we in this paper propose an efficient and effective mechanism FastFHE to accelerate the model inference while simultaneously retaining high inference accuracy over fully homomorphic encryption. Concretely, our work elaborates four unique novelties. First, we propose a new scalable ciphertext data-packing scheme to save the time and storage consumptions. Second, we work out a depthwise-separable convolution fashion to degrade the computation load of convolution calculation. Third, we figure out a BN dot-product fusion matrix to merge the ciphertext convolutional layer with the batch-normalization layer without incurring extra multiplicative depth. Last but not least, we adopt the low-degree Legendre polynomial to approximate the nonlinear smooth activation function SiLU under the guarantee of tiny accuracy error before and after encrypted inference. Finally, we execute multi-facet experiments to verify the efficiency and effectiveness of our proposed approach.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22434v1",
    "published_date": "2025-11-27 13:14:42 UTC",
    "updated_date": "2025-11-27 13:14:42 UTC"
  },
  {
    "arxiv_id": "2512.07877v2",
    "title": "Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design",
    "authors": [
      "Amogh Anshu N",
      "Harish BP"
    ],
    "abstract": "Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints. Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions. This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models. Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics. Our pipeline generates over 150,000 simulation data points across varied mesh topologies. The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data. Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.07877v2",
    "published_date": "2025-11-27 13:00:22 UTC",
    "updated_date": "2025-12-10 12:21:13 UTC"
  },
  {
    "arxiv_id": "2511.22420v1",
    "title": "MATCH: Engineering Transparent and Controllable Conversational XAI Systems through Composable Building Blocks",
    "authors": [
      "Sebe Vanbrabant",
      "Gustavo Rovelo Ruiz",
      "Davy Vanacken"
    ],
    "abstract": "While the increased integration of AI technologies into interactive systems enables them to solve an increasing number of tasks, the black-box problem of AI models continues to spread throughout the interactive system as a whole. Explainable AI (XAI) techniques can make AI models more accessible by employing post-hoc methods or transitioning to inherently interpretable models. While this makes individual AI models clearer, the overarching system architecture remains opaque. This challenge not only pertains to standard XAI techniques but also to human examination and conversational XAI approaches that need access to model internals to interpret them correctly and completely. To this end, we propose conceptually representing such interactive systems as sequences of structural building blocks. These include the AI models themselves, as well as control mechanisms grounded in literature. The structural building blocks can then be explained through complementary explanatory building blocks, such as established XAI techniques like LIME and SHAP. The flow and APIs of the structural building blocks form an unambiguous overview of the underlying system, serving as a communication basis for both human and automated agents, thus aligning human and machine interpretability of the embedded AI models. In this paper, we present our flow-based approach and a selection of building blocks as MATCH: a framework for engineering Multi-Agent Transparent and Controllable Human-centered systems. This research contributes to the field of (conversational) XAI by facilitating the integration of interpretability into existing interactive systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "Submitted Version accepted for publication in an LNCS Volume \"Engineering Interactive Computer Systems - EICS 2025 - International Workshops and Doctoral Consortium\"",
    "pdf_url": "https://arxiv.org/pdf/2511.22420v1",
    "published_date": "2025-11-27 12:58:04 UTC",
    "updated_date": "2025-11-27 12:58:04 UTC"
  },
  {
    "arxiv_id": "2512.03073v1",
    "title": "Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem",
    "authors": [
      "Shayne Longpre",
      "Christopher Akiki",
      "Campbell Lund",
      "Atharva Kulkarni",
      "Emily Chen",
      "Irene Solaiman",
      "Avijit Ghosh",
      "Yacine Jernite",
      "Lucie-Aimée Kaffee"
    ],
    "abstract": "Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03073v1",
    "published_date": "2025-11-27 12:50:25 UTC",
    "updated_date": "2025-11-27 12:50:25 UTC"
  },
  {
    "arxiv_id": "2512.03072v2",
    "title": "Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI",
    "authors": [
      "Hu Keyi"
    ],
    "abstract": "Current AI paradigms, as \"architects of experience,\" face fundamental challenges in explainability and value alignment. This paper introduces \"Weight-Calculatism,\" a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03072v2",
    "published_date": "2025-11-27 12:42:54 UTC",
    "updated_date": "2025-12-08 14:27:36 UTC"
  },
  {
    "arxiv_id": "2511.22402v1",
    "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs",
    "authors": [
      "Srivarshinee Sridhar",
      "Raghav Kaushik Ravi",
      "Kripabandhu Ghosh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to AAAI'26 SECURE-AI4H Workshop",
    "pdf_url": "https://arxiv.org/pdf/2511.22402v1",
    "published_date": "2025-11-27 12:26:06 UTC",
    "updated_date": "2025-11-27 12:26:06 UTC"
  },
  {
    "arxiv_id": "2511.22396v1",
    "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
    "authors": [
      "Run Shao",
      "Ziyu Li",
      "Zhaoyang Zhang",
      "Linrui Xu",
      "Xinran He",
      "Hongyuan Yuan",
      "Bolei He",
      "Yongxing Dai",
      "Yiming Yan",
      "Yijun Chen",
      "Wang Guo",
      "Haifeng Li"
    ],
    "abstract": "Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22396v1",
    "published_date": "2025-11-27 12:19:37 UTC",
    "updated_date": "2025-11-27 12:19:37 UTC"
  },
  {
    "arxiv_id": "2511.22386v1",
    "title": "Who is Afraid of Minimal Revision?",
    "authors": [
      "Edoardo Baccini",
      "Zoé Christoff",
      "Nina Gierasimczuk",
      "Rineke Verbrugge"
    ],
    "abstract": "The principle of minimal change in belief revision theory requires that, when accepting new information, one keeps one's belief state as close to the initial belief state as possible. This is precisely what the method known as minimal revision does. However, unlike less conservative belief revision methods, minimal revision falls short in learning power: It cannot learn everything that can be learned by other learning methods. We begin by showing that, despite this limitation, minimal revision is still a successful learning method in a wide range of situations. Firstly, it can learn any problem that is finitely identifiable. Secondly, it can learn with positive and negative data, as long as one considers finitely many possibilities. We then characterize the prior plausibility assignments (over finitely many possibilities) that enable one to learn via minimal revision, and do the same for conditioning and lexicographic upgrade. Finally, we show that not all of our results still hold when learning from possibly erroneous information.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings TARK 2025, arXiv:2511.20540",
    "pdf_url": "https://arxiv.org/pdf/2511.22386v1",
    "published_date": "2025-11-27 12:15:43 UTC",
    "updated_date": "2025-11-27 12:15:43 UTC"
  },
  {
    "arxiv_id": "2511.22381v1",
    "title": "Graded Distributed Belief",
    "authors": [
      "Emiliano Lorini",
      "Dmitry Rozplokhas"
    ],
    "abstract": "We introduce a new logic of graded distributed belief that allows us to express the fact that a group of agents distributively believe that a certain fact holds with at least strength k. We interpret our logic by means of computationally grounded semantics relying on the concept of belief base. The strength of the group's distributed belief is directly computed from the group's belief base after having merged its members' individual belief bases. We illustrate our logic with an intuitive example, formalizing the notion of epistemic disagreement. We also provide a sound and complete Hilbert-style axiomatization, decidability result obtained via filtration, and a tableaux-based decision procedure that allows us to state PSPACE-completeness for our logic.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings TARK 2025, arXiv:2511.20540",
    "pdf_url": "https://arxiv.org/pdf/2511.22381v1",
    "published_date": "2025-11-27 12:14:28 UTC",
    "updated_date": "2025-11-27 12:14:28 UTC"
  },
  {
    "arxiv_id": "2511.22377v1",
    "title": "Conditionals Based on Selection Functions, Modal Operators and Probabilities",
    "authors": [
      "Tommaso Flaminio",
      "Lluis Godo",
      "Gluliano Rosella"
    ],
    "abstract": "Methods for probability updating, of which Bayesian conditionalization is the most well-known and widely used, are modeling tools that aim to represent the process of modifying an initial epistemic state, typically represented by a prior probability function P, which is adjusted in light of new information. Notably, updating methods and conditional sentences seem to intuitively share a deep connection, as is evident in the case of conditionalization. The present work contributes to this line of research and aims at shedding new light on the relationship between updating methods and conditional connectives. Departing from previous literature that often focused on a specific type of conditional or a particular updating method, our goal is to prove general results concerning the connection between conditionals and their probabilities. This will allow us to characterize the probabilities of certain conditional connectives and to understand what class of updating procedures can be represented using specific conditional connectives. Broadly, we adopt a general perspective that encompasses a large class of conditionals and a wide range of updating methods, enabling us to prove some general results concerning their interrelation.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings TARK 2025, arXiv:2511.20540",
    "pdf_url": "https://arxiv.org/pdf/2511.22377v1",
    "published_date": "2025-11-27 12:13:45 UTC",
    "updated_date": "2025-11-27 12:13:45 UTC"
  },
  {
    "arxiv_id": "2511.22376v1",
    "title": "On the Complexity of the Grounded Semantics for Infinite Argumentation Frameworks",
    "authors": [
      "Uri Andrews",
      "Luca San Mauro"
    ],
    "abstract": "Argumentation frameworks, consisting of arguments and an attack relation representing conflicts, are fundamental for formally studying reasoning under conflicting information. We use methods from mathematical logic, specifically computability and set theory, to analyze the grounded extension, a widely-used model of maximally skeptical reasoning, defined as the least fixed-point of a natural defense operator. Without additional constraints, finding this fixed-point requires transfinite iterations. We identify the exact ordinal number corresponding to the length of this iterative process and determine the complexity of deciding grounded acceptance, showing it to be maximally complex. This shows a marked distinction from the finite case where the grounded extension is polynomial-time computable, thus simpler than other reasoning problems explored in formal argumentation.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "In Proceedings TARK 2025, arXiv:2511.20540",
    "pdf_url": "https://arxiv.org/pdf/2511.22376v1",
    "published_date": "2025-11-27 12:13:30 UTC",
    "updated_date": "2025-11-27 12:13:30 UTC"
  },
  {
    "arxiv_id": "2511.22374v1",
    "title": "Distributed Knowing How",
    "authors": [
      "Bin Liu",
      "Yanjing Wang"
    ],
    "abstract": "Distributed knowledge is a key concept in the standard epistemic logic of knowledge-that. In this paper, we propose a corresponding notion of distributed knowledge-how and study its logic. Our framework generalizes two existing traditions in the logic of know-how: the individual-based multi-step framework and the coalition-based single-step framework. In particular, we assume a group can accomplish more than what its individuals can jointly do. The distributed knowledge-how is based on the distributed knowledge-that of a group whose multi-step strategies derive from distributed actions that subgroups can collectively perform. As the main result, we obtain a sound and strongly complete proof system for our logic of distributed knowledge-how, which closely resembles the logic of distributed knowledge-that in both the axioms and the proof method of completeness.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "In Proceedings TARK 2025, arXiv:2511.20540",
    "pdf_url": "https://arxiv.org/pdf/2511.22374v1",
    "published_date": "2025-11-27 12:13:00 UTC",
    "updated_date": "2025-11-27 12:13:00 UTC"
  },
  {
    "arxiv_id": "2511.22367v1",
    "title": "SuRe: Surprise-Driven Prioritised Replay for Continual LLM Learning",
    "authors": [
      "Hugo Hazard",
      "Zafeirios Fountas",
      "Martin A. Benfeghoul",
      "Adnan Oomerjee",
      "Jun Wang",
      "Haitham Bou-Ammar"
    ],
    "abstract": "Continual learning, one's ability to adapt to a sequence of tasks without forgetting previously acquired knowledge, remains a major challenge in machine learning and a key gap between artificial and human intelligence. While regularisation and replay perform well in vision, they lag behind multi-task learning for large language models (LLMs), especially at scale with many tasks. We revisit replay and argue that two failure modes drive this gap: selection (what to rehearse) and integration (how to consolidate new knowledge). To address selection, we propose Surprise-prioritised Replay (SuRe), a simple, architecture-agnostic rule that ranks and stores the most surprising (high Negative Log-Likelihood) sequences. SuRe achieves state-of-the-art performance in the Large Number of Tasks (LNT) setting and delivers the best overall average across both Standard CL and LNT benchmarks. To address integration, we add a dual-learner design with fast and slow LoRA adapters merged via an exponential moving average (EMA), enabling rapid adaptation while stabilising long-term knowledge. Combining SuRe with the dual learner yields further gains, including improvements of up to +5 accuracy points on LNT over prior SOTA. Ablation studies confirm that our proposed method remains robust under reduced replay frequency and small buffer size, demonstrating both effectiveness and sample efficiency. Taken together, our results establish replay as a strong baseline for continual LLM fine-tuning and demonstrate that surprise-based selection and slow-weight consolidation are complementary components for mitigating catastrophic forgetting.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22367v1",
    "published_date": "2025-11-27 12:06:33 UTC",
    "updated_date": "2025-11-27 12:06:33 UTC"
  },
  {
    "arxiv_id": "2512.05982v1",
    "title": "FlockVote: LLM-Empowered Agent-Based Modeling for Simulating U.S. Presidential Elections",
    "authors": [
      "Lingfeng Zhou",
      "Yi Xu",
      "Zhenyu Wang",
      "Dequan Wang"
    ],
    "abstract": "Modeling complex human behavior, such as voter decisions in national elections, is a long-standing challenge for computational social science. Traditional agent-based models (ABMs) are limited by oversimplified rules, while large-scale statistical models often lack interpretability. We introduce FlockVote, a novel framework that uses Large Language Models (LLMs) to build a \"computational laboratory\" of LLM agents for political simulation. Each agent is instantiated with a high-fidelity demographic profile and dynamic contextual information (e.g. candidate policies), enabling it to perform nuanced, generative reasoning to simulate a voting decision. We deploy this framework as a testbed on the 2024 U.S. Presidential Election, focusing on seven key swing states. Our simulation's macro-level results successfully replicate the real-world outcome, demonstrating the high fidelity of our \"virtual society\". The primary contribution is not only the prediction, but also the framework's utility as an interpretable research tool. FlockVote moves beyond black-box outputs, allowing researchers to probe agent-level rationale and analyze the stability and sensitivity of LLM-driven social simulations.",
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "Published as a conference paper at ICAIS 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.05982v1",
    "published_date": "2025-11-27 12:04:07 UTC",
    "updated_date": "2025-11-27 12:04:07 UTC"
  },
  {
    "arxiv_id": "2511.22364v1",
    "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands",
    "authors": [
      "Seongwon Cho",
      "Daechul Ahn",
      "Donghyun Shin",
      "Hyeonbeom Choi",
      "San Kim",
      "Jonghyun Choi"
    ],
    "abstract": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "12 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22364v1",
    "published_date": "2025-11-27 12:03:31 UTC",
    "updated_date": "2025-11-27 12:03:31 UTC"
  },
  {
    "arxiv_id": "2511.22343v1",
    "title": "Test Time Training for AC Power Flow Surrogates via Physics and Operational Constraint Refinement",
    "authors": [
      "Panteleimon Dogoulis",
      "Mohammad Iman Alizadeh",
      "Sylvain Kubler",
      "Maxime Cordy"
    ],
    "abstract": "Power Flow (PF) calculation based on machine learning (ML) techniques offer significant computational advantages over traditional numerical methods but often struggle to maintain full physical consistency. This paper introduces a physics-informed test-time training (PI-TTT) framework that enhances the accuracy and feasibility of ML-based PF surrogates by enforcing AC power flow equalities and operational constraints directly at inference time. The proposed method performs a lightweight self-supervised refinement of the surrogate outputs through few gradient-based updates, enabling local adaptation to unseen operating conditions without requiring labeled data. Extensive experiments on the IEEE 14-, 118-, and 300-bus systems and the PEGASE 1354-bus network show that PI-TTT reduces power flow residuals and operational constraint violations by one to two orders of magnitude compared with purely ML-based models, while preserving their computational advantage. The results demonstrate that PI-TTT provides fast, accurate, and physically reliable predictions, representing a promising direction for scalable and physics-consistent learning in power system analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22343v1",
    "published_date": "2025-11-27 11:27:54 UTC",
    "updated_date": "2025-11-27 11:27:54 UTC"
  },
  {
    "arxiv_id": "2511.22334v2",
    "title": "Edge Deployment of Small Language Models, a comprehensive comparison of CPU, GPU and NPU backends",
    "authors": [
      "Pablo Prieto",
      "Pablo Abad"
    ],
    "abstract": "Edge computing processes data where it is generated, enabling faster decisions, lower bandwidth usage, and improved privacy. However, edge devices typically operate under strict constraints on processing power, memory, and energy consumption, making them unsuitable for large language models (LLMs). Fortunately, Small Language Models (SLMs) offer lightweight alternatives that bring AI inference to resource-constrained environments by significantly reducing computational cost while remaining suitable for specialization and customization. In this scenario, selecting the hardware platform that best balances performance and efficiency for SLM inference is challenging due to strict resource limitations. To address this issue, this study evaluates the inference performance and energy efficiency of commercial CPUs (Intel and ARM), GPUs (NVIDIA), and NPUs (RaiderChip) for running SLMs. GPUs, the usual platform of choice, are compared against commercial NPUs and recent multi-core CPUs. While NPUs leverage custom hardware designs optimized for computation, modern CPUs increasingly incorporate dedicated features targeting language-model workloads. Using a common execution framework and a suite of state-of-the-art SLMs, we analyze both maximum achievable performance and processing and energy efficiency across commercial solutions available for each platform. The results indicate that specialized backends outperform general-purpose CPUs, with NPUs achieving the highest performance by a wide margin. Bandwidth normalization proves essential for fair cross-architecture comparisons. Although low-power ARM processors deliver competitive results when energy usage is considered, metrics that combine performance and power (such as EDP) again highlight NPUs as the dominant architecture. These findings show that designs optimized for both efficiency and performance offer a clear advantage for edge workloads.",
    "categories": [
      "cs.PF",
      "cs.AI"
    ],
    "primary_category": "cs.PF",
    "comment": "8 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22334v2",
    "published_date": "2025-11-27 11:11:01 UTC",
    "updated_date": "2025-12-09 12:19:18 UTC"
  },
  {
    "arxiv_id": "2511.22331v1",
    "title": "On the Condition Number Dependency in Bilevel Optimization",
    "authors": [
      "Lesi Chen",
      "Jingzhao Zhang"
    ],
    "abstract": "Bilevel optimization minimizes an objective function, defined by an upper-level problem whose feasible region is the solution of a lower-level problem. We study the oracle complexity of finding an $ε$-stationary point with first-order methods when the upper-level problem is nonconvex and the lower-level problem is strongly convex. Recent works (Ji et al., ICML 2021; Arbel and Mairal, ICLR 2022; Chen el al., JMLR 2025) achieve a $\\tilde{\\mathcal{O}}(κ^4 ε^{-2})$ upper bound that is near-optimal in $ε$. However, the optimal dependency on the condition number $κ$ is unknown. In this work, we establish a new $Ω(κ^2 ε^{-2})$ lower bound and $\\tilde{\\mathcal{O}}(κ^{7/2} ε^{-2})$ upper bound for this problem, establishing the first provable gap between bilevel problems and minimax problems in this setup. Our lower bounds can be extended to various settings, including high-order smooth functions, stochastic oracles, and convex hyper-objectives: (1) For second-order and arbitrarily smooth problems, we show $Ω(κ_y^{13/4} ε^{-12/7})$ and $Ω(κ^{17/10} ε^{-8/5})$ lower bounds, respectively. (2) For convex-strongly-convex problems, we improve the previously best lower bound (Ji and Liang, JMLR 2022) from $Ω(κ/\\sqrtε)$ to $Ω(κ^{5/4} / \\sqrtε)$. (3) For smooth stochastic problems, we show an $Ω(κ^4 ε^{-4})$ lower bound.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22331v1",
    "published_date": "2025-11-27 11:03:24 UTC",
    "updated_date": "2025-11-27 11:03:24 UTC"
  },
  {
    "arxiv_id": "2511.22330v1",
    "title": "Prompt-based Consistent Video Colorization",
    "authors": [
      "Silvia Dani",
      "Tiberio Uricchio",
      "Lorenzo Seidenari"
    ],
    "abstract": "Existing video colorization methods struggle with temporal flickering or demand extensive manual input. We propose a novel approach automating high-fidelity video colorization using rich semantic guidance derived from language and segmentation. We employ a language-conditioned diffusion model to colorize grayscale frames. Guidance is provided via automatically generated object masks and textual prompts; our primary automatic method uses a generic prompt, achieving state-of-the-art results without specific color input. Temporal stability is achieved by warping color information from previous frames using optical flow (RAFT); a correction step detects and fixes inconsistencies introduced by warping. Evaluations on standard benchmarks (DAVIS30, VIDEVO20) show our method achieves state-of-the-art performance in colorization accuracy (PSNR) and visual realism (Colorfulness, CDC), demonstrating the efficacy of automated prompt-based guidance for consistent video colorization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22330v1",
    "published_date": "2025-11-27 11:01:06 UTC",
    "updated_date": "2025-11-27 11:01:06 UTC"
  },
  {
    "arxiv_id": "2511.22325v1",
    "title": "Tracing Footsteps of Similar Cities: Modeling Urban Economic Vitality with Dynamic Inter-City Graph Embeddings",
    "authors": [
      "Xiaofeng Li",
      "Xiangyi Xiao",
      "Xiaocong Du",
      "Ying Zhang",
      "Haipeng Zhang"
    ],
    "abstract": "Urban economic vitality is a crucial indicator of a city's long-term growth potential, comprising key metrics such as the annual number of new companies and the population employed. However, modeling urban economic vitality remains challenging. This study develops ECO-GROW, a multi-graph framework modeling China's inter-city networks (2005-2021) to generate urban embeddings that model urban economic vitality. Traditional approaches relying on static city-level aggregates fail to capture a fundamental dynamic: the developmental trajectory of one city today may mirror that of its structurally similar counterparts tomorrow. ECO-GROW overcomes this limitation by integrating industrial linkages, POI similarities, migration similarities and temporal network evolution over 15 years. The framework combines a Dynamic Top-K GCN to adaptively select influential inter-city connections and an adaptive Graph Scorer mechanism to dynamically weight cross-regional impacts. Additionally, the model incorporates a link prediction task based on Barabasi Proximity, optimizing the graph representation. Experimental results demonstrate ECO-GROW's superior accuracy in predicting entrepreneurial activities and employment trends compared to conventional models. By open-sourcing our code, we enable government agencies and public sector organizations to leverage big data analytics for evidence-based urban planning, economic policy formulation, and resource allocation decisions that benefit society at large.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22325v1",
    "published_date": "2025-11-27 10:55:11 UTC",
    "updated_date": "2025-11-27 10:55:11 UTC"
  },
  {
    "arxiv_id": "2511.22321v1",
    "title": "RELiQ: Scalable Entanglement Routing via Reinforcement Learning in Quantum Networks",
    "authors": [
      "Tobias Meuser",
      "Jannis Weil",
      "Aninda Lahiri",
      "Marius Paraschiv"
    ],
    "abstract": "Quantum networks are becoming increasingly important because of advancements in quantum computing and quantum sensing, such as recent developments in distributed quantum computing and federated quantum machine learning. Routing entanglement in quantum networks poses several fundamental as well as technical challenges, including the high dynamicity of quantum network links and the probabilistic nature of quantum operations. Consequently, designing hand-crafted heuristics is difficult and often leads to suboptimal performance, especially if global network topology information is unavailable.\n  In this paper, we propose RELiQ, a reinforcement learning-based approach to entanglement routing that only relies on local information and iterative message exchange. Utilizing a graph neural network, RELiQ learns graph representations and avoids overfitting to specific network topologies - a prevalent issue for learning-based approaches. Our approach, trained on random graphs, consistently outperforms existing local information heuristics and learning-based approaches when applied to random and real-world topologies. When compared to global information heuristics, our method achieves similar or superior performance because of its rapid response to topology changes.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "quant-ph",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22321v1",
    "published_date": "2025-11-27 10:52:43 UTC",
    "updated_date": "2025-11-27 10:52:43 UTC"
  },
  {
    "arxiv_id": "2511.22311v1",
    "title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation",
    "authors": [
      "Fiona Y. Wang",
      "Di Sheng Lee",
      "David L. Kaplan",
      "Markus J. Buehler"
    ],
    "abstract": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.",
    "categories": [
      "cs.AI",
      "cond-mat.mes-hall",
      "cond-mat.soft",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22311v1",
    "published_date": "2025-11-27 10:42:52 UTC",
    "updated_date": "2025-11-27 10:42:52 UTC"
  },
  {
    "arxiv_id": "2511.22307v2",
    "title": "Enhanced Conditional Generation of Double Perovskite by Knowledge-Guided Language Model Feedback",
    "authors": [
      "Inhyo Lee",
      "Junhyeong Lee",
      "Jongwon Park",
      "KyungTae Lim",
      "Seunghwa Ryu"
    ],
    "abstract": "Double perovskites (DPs) are promising candidates for sustainable energy technologies due to their compositional tunability and compatibility with low-energy fabrication, yet their vast design space poses a major challenge for conditional materials discovery. This work introduces a multi-agent, text gradient-driven framework that performs DP composition generation under natural-language conditions by integrating three complementary feedback sources: LLM-based self-evaluation, DP-specific domain knowledge-informed feedback, and ML surrogate-based feedback. Analogous to how knowledge-informed machine learning improves the reliability of conventional data-driven models, our framework incorporates domain-informed text gradients to guide the generative process toward physically meaningful regions of the DP composition space. Systematic comparison of three incremental configurations, (i) pure LLM generation, (ii) LLM generation with LLM reasoning-based feedback, and (iii) LLM generation with domain knowledge-guided feedback, shows that iterative guidance from knowledge-informed gradients improves stability-condition satisfaction without additional training data, achieving over 98% compositional validity and up to 54% stable or metastable candidates, surpassing both the LLM-only baseline (43%) and prior GAN-based results (27%). Analyses of ML-based gradients further reveal that they enhance performance in in-distribution (ID) regions but become unreliable in out-of-distribution (OOD) regimes. Overall, this work provides the first systematic analysis of multi-agent, knowledge-guided text gradients for DP discovery and establishes a generalizable blueprint for MAS-driven generative materials design aimed at advancing sustainable technologies.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22307v2",
    "published_date": "2025-11-27 10:38:35 UTC",
    "updated_date": "2025-12-02 07:14:42 UTC"
  },
  {
    "arxiv_id": "2512.00105v1",
    "title": "Efficiently Sampling Interval Patterns from Numerical Databases",
    "authors": [
      "Djawad Bekkoucha",
      "Lamine Diop",
      "Abdelkader Ouali",
      "Bruno Crémilleux",
      "Patrice Boizumault"
    ],
    "abstract": "Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00105v1",
    "published_date": "2025-11-27 10:35:17 UTC",
    "updated_date": "2025-11-27 10:35:17 UTC"
  },
  {
    "arxiv_id": "2511.22302v1",
    "title": "When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming",
    "authors": [
      "Ahmad Tarraf",
      "Koutaiba Kassem-Manthey",
      "Seyed Ali Mohammadi",
      "Philipp Martin",
      "Lukas Moj",
      "Semih Burak",
      "Enju Park",
      "Christian Terboven",
      "Felix Wolf"
    ],
    "abstract": "Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages",
    "pdf_url": "https://arxiv.org/pdf/2511.22302v1",
    "published_date": "2025-11-27 10:31:24 UTC",
    "updated_date": "2025-11-27 10:31:24 UTC"
  },
  {
    "arxiv_id": "2511.22292v1",
    "title": "Adaptive tumor growth forecasting via neural & universal ODEs",
    "authors": [
      "Kavya Subramanian",
      "Prathamesh Dinesh Joshi",
      "Raj Abhijit Dandekar",
      "Rajat Dandekar",
      "Sreedath Panat"
    ],
    "abstract": "Forecasting tumor growth is critical for optimizing treatment. Classical growth models such as the Gompertz and Bertalanffy equations capture general tumor dynamics but may fail to adapt to patient-specific variability, particularly with limited data available. In this study, we leverage Neural Ordinary Differential Equations (Neural ODEs) and Universal Differential Equations (UDEs), two pillars of Scientific Machine Learning (SciML), to construct adaptive tumor growth models capable of learning from experimental data. Using the Gompertz model as a baseline, we replace rigid terms with adaptive neural networks to capture hidden dynamics through robust modeling in the Julia programming language. We use our models to perform forecasting under data constraints and symbolic recovery to transform the learned dynamics into explicit mathematical expressions. Our approach has the potential to improve predictive accuracy, guiding dynamic and effective treatment strategies for improved clinical outcomes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at JuliaCon 2025 conference",
    "pdf_url": "https://arxiv.org/pdf/2511.22292v1",
    "published_date": "2025-11-27 10:17:35 UTC",
    "updated_date": "2025-11-27 10:17:35 UTC"
  },
  {
    "arxiv_id": "2601.02367v1",
    "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics",
    "authors": [
      "Despoina Antonakaki",
      "Sotiris Ioannidis"
    ],
    "abstract": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.02367v1",
    "published_date": "2025-11-27 10:11:59 UTC",
    "updated_date": "2025-11-27 10:11:59 UTC"
  },
  {
    "arxiv_id": "2511.22275v1",
    "title": "RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems",
    "authors": [
      "Mengfan Li",
      "Xuanhua Shi",
      "Yang Deng"
    ],
    "abstract": "Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users' mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.22275v1",
    "published_date": "2025-11-27 09:58:29 UTC",
    "updated_date": "2025-11-27 09:58:29 UTC"
  },
  {
    "arxiv_id": "2511.22263v1",
    "title": "Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title",
    "authors": [
      "Taeryun Won",
      "Tae Kwan Lee",
      "Hiun Kim",
      "Hyemin Lee"
    ],
    "abstract": "This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22263v1",
    "published_date": "2025-11-27 09:40:21 UTC",
    "updated_date": "2025-11-27 09:40:21 UTC"
  },
  {
    "arxiv_id": "2511.22254v3",
    "title": "Co-Evolving Agents: Learning from Failures as Hard Negatives",
    "authors": [
      "Yeonsung Jung",
      "Trilok Padhi",
      "Sina Shaham",
      "Dipika Khullar",
      "Joonhyun Jeong",
      "Ninareh Mehrabi",
      "Eunho Yang"
    ],
    "abstract": "The rapid progress of large foundation models has accelerated the development of task-specialized agents across diverse domains. However, the effectiveness of agents remains tightly coupled with the quality of training data, while curating task-specific datasets remains costly and often infeasible in real-world scenarios. Recent work has explored self-improving agents that autonomously generate, refine, and re-train on their own trajectories. A prominent line of approaches further leverages preference optimization by pairing predicted trajectories with scarce ground-truth trajectories, enabling agents to learn directly from their own failures. While these methods outperform supervised fine-tuning, their heavy reliance on predicted trajectories under limited ground-truth supervision leaves them prone to overfitting. To address this, we propose a co-evolving agents framework in which a target agent improves jointly with an auxiliary failure agent. The failure agent learns through preference optimization over failure trajectories from both the target and itself, thereby generating hard negatives that are close to success yet remain failures. Incorporating these informative hard negatives into the target agent's optimization sharpens decision boundaries and enhances generalization. Our comprehensive analysis and experiments across benchmark datasets show that our method not only shows improved performance but also demonstrates that failures, instead of being used as-is, can be systematically transformed into structured and valuable learning signals in self-improving agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22254v3",
    "published_date": "2025-11-27 09:30:33 UTC",
    "updated_date": "2026-01-16 01:31:53 UTC"
  },
  {
    "arxiv_id": "2511.22246v1",
    "title": "An interpretable unsupervised representation learning for high precision measurement in particle physics",
    "authors": [
      "Xing-Jian Lv",
      "De-Xing Miao",
      "Zi-Jun Xu",
      "Jian-Chun Wang"
    ],
    "abstract": "Unsupervised learning has been widely applied to various tasks in particle physics. However, existing models lack precise control over their learned representations, limiting physical interpretability and hindering their use for accurate measurements. We propose the Histogram AutoEncoder (HistoAE), an unsupervised representation learning network featuring a custom histogram-based loss that enforces a physically structured latent space. Applied to silicon microstrip detectors, HistoAE learns an interpretable two-dimensional latent space corresponding to the particle's charge and impact position. After simple post-processing, it achieves a charge resolution of $0.25\\,e$ and a position resolution of $3\\,μ\\mathrm{m}$ on beam-test data, comparable to the conventional approach. These results demonstrate that unsupervised deep learning models can enable physically meaningful and quantitatively precise measurements. Moreover, the generative capacity of HistoAE enables straightforward extensions to fast detector simulations.",
    "categories": [
      "hep-ex",
      "cs.AI",
      "physics.ins-det"
    ],
    "primary_category": "hep-ex",
    "comment": "8 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22246v1",
    "published_date": "2025-11-27 09:18:44 UTC",
    "updated_date": "2025-11-27 09:18:44 UTC"
  },
  {
    "arxiv_id": "2511.22240v1",
    "title": "Evaluating Embedding Models and Pipeline Optimization for AI Search Quality",
    "authors": [
      "Philip Zhong",
      "Kent Chen",
      "Don Wang"
    ],
    "abstract": "We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22240v1",
    "published_date": "2025-11-27 09:09:39 UTC",
    "updated_date": "2025-11-27 09:09:39 UTC"
  },
  {
    "arxiv_id": "2511.22239v1",
    "title": "DeepPNI: Language- and graph-based model for mutation-driven protein-nucleic acid energetics",
    "authors": [
      "Somnath Mondal",
      "Tinkal Mondal",
      "Soumajit Pramanik",
      "Rukmankesh Mehra"
    ],
    "abstract": "The interaction between proteins and nucleic acids is crucial for processes that sustain cellular function, including DNA maintenance and the regulation of gene expression and translation. Amino acid mutations in protein-nucleic acid complexes often lead to vital diseases. Experimental techniques have their own specific limitations in predicting mutational effects in protein-nucleic acid complexes. In this study, we compiled a large dataset of 1951 mutations including both protein-DNA and protein-RNA complexes and integrated structural and sequential features to build a deep learning-based regression model named DeepPNI. This model estimates mutation-induced binding free energy changes in protein-nucleic acid complexes. The structural features are encoded via edge-aware RGCN and the sequential features are extracted using protein language model ESM-2. We have achieved a high average Pearson correlation coefficient (PCC) of 0.76 in the large dataset via five-fold cross-validation. Consistent performance across individual dataset of protein-DNA, protein-RNA complexes, and different experimental temperature split dataset make the model generalizable. Our model showed good performance in complex-based five-fold cross-validation, which proved its robustness. In addition, DeepPNI outperformed in external dataset validation, and comparison with existing tools",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22239v1",
    "published_date": "2025-11-27 09:08:32 UTC",
    "updated_date": "2025-11-27 09:08:32 UTC"
  },
  {
    "arxiv_id": "2511.22235v1",
    "title": "Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation",
    "authors": [
      "Zehao Deng",
      "Tianjie Ju",
      "Zheng Wu",
      "Zhuosheng Zhang",
      "Gongshen Liu"
    ],
    "abstract": "The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22235v1",
    "published_date": "2025-11-27 09:01:38 UTC",
    "updated_date": "2025-11-27 09:01:38 UTC"
  },
  {
    "arxiv_id": "2511.22232v1",
    "title": "From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation",
    "authors": [
      "Zhen Chen",
      "Yihang Fu",
      "Gabriel Madera",
      "Mauro Giuffre",
      "Serina Applebaum",
      "Hyunjae Kim",
      "Hua Xu",
      "Qingyu Chen"
    ],
    "abstract": "Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22232v1",
    "published_date": "2025-11-27 08:54:59 UTC",
    "updated_date": "2025-11-27 08:54:59 UTC"
  },
  {
    "arxiv_id": "2511.22228v1",
    "title": "3D-Consistent Multi-View Editing by Diffusion Guidance",
    "authors": [
      "Josef Bengtson",
      "David Nilsson",
      "Dong In Lee",
      "Fredrik Kahl"
    ],
    "abstract": "Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22228v1",
    "published_date": "2025-11-27 08:48:36 UTC",
    "updated_date": "2025-11-27 08:48:36 UTC"
  },
  {
    "arxiv_id": "2511.22226v1",
    "title": "Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning",
    "authors": [
      "Alexander Meulemans",
      "Rajai Nasser",
      "Maciej Wołczyk",
      "Marissa A. Weis",
      "Seijin Kobayashi",
      "Blake Richards",
      "Guillaume Lajoie",
      "Angelika Steger",
      "Marcus Hutter",
      "James Manyika",
      "Rif A. Saurous",
      "João Sacramento",
      "Blaise Agüera y Arcas"
    ],
    "abstract": "The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "203 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22226v1",
    "published_date": "2025-11-27 08:46:48 UTC",
    "updated_date": "2025-11-27 08:46:48 UTC"
  },
  {
    "arxiv_id": "2512.03071v1",
    "title": "PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering",
    "authors": [
      "Loup-Noe Levy",
      "Guillaume Guerard",
      "Sonia Djebali",
      "Soufian Ben Amor"
    ],
    "abstract": "This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03071v1",
    "published_date": "2025-11-27 08:20:22 UTC",
    "updated_date": "2025-11-27 08:20:22 UTC"
  },
  {
    "arxiv_id": "2512.03070v1",
    "title": "Mixed Data Clustering Survey and Challenges",
    "authors": [
      "Guillaume Guerard",
      "Sonia Djebali"
    ],
    "abstract": "The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03070v1",
    "published_date": "2025-11-27 08:20:05 UTC",
    "updated_date": "2025-11-27 08:20:05 UTC"
  },
  {
    "arxiv_id": "2512.03069v1",
    "title": "Hierarchical clustering of complex energy systems using pretopology",
    "authors": [
      "Loup-Noe Levy",
      "Jeremie Bosom",
      "Guillaume Guerard",
      "Soufian Ben Amor",
      "Marc Bui",
      "Hai Tran"
    ],
    "abstract": "This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?\n  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.\n  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.\n  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.\n  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03069v1",
    "published_date": "2025-11-27 08:19:50 UTC",
    "updated_date": "2025-11-27 08:19:50 UTC"
  },
  {
    "arxiv_id": "2511.22199v1",
    "title": "PULSE-ICU: A Pretrained Unified Long-Sequence Encoder for Multi-task Prediction in Intensive Care Units",
    "authors": [
      "Sejeong Jang",
      "Joo Heung Yoon",
      "Hyo Kyung Lee"
    ],
    "abstract": "Intensive care unit (ICU) data are highly irregular, heterogeneous, and temporally fragmented, posing challenges for generalizable clinical prediction. We present PULSE-ICU, a self-supervised foundation model that learns event-level ICU representations from large-scale EHR sequences without resampling or manual feature engineering. A unified embedding module encodes event identity, continuous values, units, and temporal attributes, while a Longformer-based encoder enables efficient modeling of long trajectories. PULSE-ICU was fine-tuned across 18 prediction tasks, including mortality, intervention forecasting, and phenotype identification, achieving strong performance across task types. External validation on eICU, HiRID, and P12 showed substantial improvements with minimal fine-tuning, demonstrating robustness to domain shift and variable constraints. These findings suggest that foundation-style modeling can improve data efficiency and adaptability, providing a scalable framework for ICU decision support across diverse clinical environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22199v1",
    "published_date": "2025-11-27 08:10:52 UTC",
    "updated_date": "2025-11-27 08:10:52 UTC"
  },
  {
    "arxiv_id": "2511.22188v1",
    "title": "ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network for Facial Expression Recognition",
    "authors": [
      "Yan Li",
      "Yong Zhao",
      "Xiaohan Xia",
      "Dongmei Jiang"
    ],
    "abstract": "The key to facial expression recognition is to learn discriminative spatial-temporal representations that embed facial expression dynamics. Previous studies predominantly rely on pre-trained Convolutional Neural Networks (CNNs) to learn facial appearance representations, overlooking the relationships between facial regions. To address this issue, this paper presents an Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet) to learn mutually enhanced spatial-temporal representations of appearance and relation information. Specifically, we construct a facial region relation graph and leverage the graph attention mechanism to model the relationships between facial regions. The resulting relational representation sequences, along with CNN-based appearance representation sequences, are then fed into a parallel graph attention fusion module for mutual interaction and enhancement. This module simultaneously explores the complementarity between different representation sequences and the temporal dynamics within each sequence. Experimental results on three facial expression recognition datasets demonstrate that the proposed ARPGNet outperforms or is comparable to state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by IEEE Transactions on Affective Computing. Submitted in August 2023; Accepted in October 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22188v1",
    "published_date": "2025-11-27 07:54:34 UTC",
    "updated_date": "2025-11-27 07:54:34 UTC"
  },
  {
    "arxiv_id": "2511.22181v1",
    "title": "MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction",
    "authors": [
      "Maitrayee Keskar",
      "Mohan Trivedi",
      "Ross Greer"
    ],
    "abstract": "We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 3 figures, 4 tables",
    "pdf_url": "https://arxiv.org/pdf/2511.22181v1",
    "published_date": "2025-11-27 07:42:06 UTC",
    "updated_date": "2025-11-27 07:42:06 UTC"
  },
  {
    "arxiv_id": "2511.22178v1",
    "title": "Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification",
    "authors": [
      "Adnan Ferdous Ashrafi",
      "Hasanul Kabir"
    ],
    "abstract": "ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 2 figures, 2 tables, Accepted and presented at Image and Vision Computing New Zealand (IVCNZ) 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22178v1",
    "published_date": "2025-11-27 07:33:31 UTC",
    "updated_date": "2025-11-27 07:33:31 UTC"
  },
  {
    "arxiv_id": "2511.22176v1",
    "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information",
    "authors": [
      "Lukas Struppek",
      "Dominik Hintersdorf",
      "Hannah Struppek",
      "Daniel Neider",
      "Kristian Kersting"
    ],
    "abstract": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22176v1",
    "published_date": "2025-11-27 07:31:52 UTC",
    "updated_date": "2025-11-27 07:31:52 UTC"
  },
  {
    "arxiv_id": "2512.03068v1",
    "title": "Echoes of AI Harms: A Human-LLM Synergistic Framework for Bias-Driven Harm Anticipation",
    "authors": [
      "Nicoleta Tantalaki",
      "Sophia Vei",
      "Athena Vakali"
    ],
    "abstract": "The growing influence of Artificial Intelligence (AI) systems on decision-making in critical domains has exposed their potential to cause significant harms, often rooted in biases embedded across the AI lifecycle. While existing frameworks and taxonomies document bias or harms in isolation, they rarely establish systematic links between specific bias types and the harms they cause, particularly within real-world sociotechnical contexts. Technical fixes proposed to address AI biases are ill-equipped to address them and are typically applied after a system has been developed or deployed, offering limited preventive value. We propose ECHO, a novel framework for proactive AI harm anticipation through the systematic mapping of AI bias types to harm outcomes across diverse stakeholder and domain contexts. ECHO follows a modular workflow encompassing stakeholder identification, vignette-based presentation of biased AI systems, and dual (human-LLM) harm annotation, integrated within ethical matrices for structured interpretation. This human-centered approach enables early-stage detection of bias-to-harm pathways, guiding AI design and governance decisions from the outset. We validate ECHO in two high-stakes domains (disease diagnosis and hiring), revealing domain-specific, bias-to-harm patterns and demonstrating ECHO's potential to support anticipatory governance of AI systems",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "38 pages",
    "pdf_url": "https://arxiv.org/pdf/2512.03068v1",
    "published_date": "2025-11-27 07:25:21 UTC",
    "updated_date": "2025-11-27 07:25:21 UTC"
  },
  {
    "arxiv_id": "2512.03067v1",
    "title": "Quantifying the Potential to Escape Filter Bubbles: A Behavior-Aware Measure via Contrastive Simulation",
    "authors": [
      "Difu Feng",
      "Qianqian Xu",
      "Zitai Wang",
      "Cong Hua",
      "Zhiyong Yang",
      "Qingming Huang"
    ],
    "abstract": "Nowadays, recommendation systems have become crucial to online platforms, shaping user exposure by accurate preference modeling. However, such an exposure strategy can also reinforce users' existing preferences, leading to a notorious phenomenon named filter bubbles. Given its negative effects, such as group polarization, increasing attention has been paid to exploring reasonable measures to filter bubbles. However, most existing evaluation metrics simply measure the diversity of user exposure, failing to distinguish between algorithmic preference modeling and actual information confinement. In view of this, we introduce Bubble Escape Potential (BEP), a behavior-aware measure that quantifies how easily users can escape from filter bubbles. Specifically, BEP leverages a contrastive simulation framework that assigns different behavioral tendencies (e.g., positive vs. negative) to synthetic users and compares the induced exposure patterns. This design enables decoupling the effect of filter bubbles and preference modeling, allowing for more precise diagnosis of bubble severity. We conduct extensive experiments across multiple recommendation models to examine the relationship between predictive accuracy and bubble escape potential across different groups. To the best of our knowledge, our empirical results are the first to quantitatively validate the dilemma between preference modeling and filter bubbles. What's more, we observe a counter-intuitive phenomenon that mild random recommendations are ineffective in alleviating filter bubbles, which can offer a principled foundation for further work in this direction.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.03067v1",
    "published_date": "2025-11-27 07:21:04 UTC",
    "updated_date": "2025-11-27 07:21:04 UTC"
  },
  {
    "arxiv_id": "2511.22169v1",
    "title": "Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization",
    "authors": [
      "Inha Kang",
      "Eunki Kim",
      "Wonjeong Ryu",
      "Jaeyo Shin",
      "Seungjun Yu",
      "Yoon-Hee Kang",
      "Seongeun Jeong",
      "Eunhye Kim",
      "Soontae Kim",
      "Hyunjung Shim"
    ],
    "abstract": "Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages",
    "pdf_url": "https://arxiv.org/pdf/2511.22169v1",
    "published_date": "2025-11-27 07:14:46 UTC",
    "updated_date": "2025-11-27 07:14:46 UTC"
  },
  {
    "arxiv_id": "2511.22167v1",
    "title": "IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer",
    "authors": [
      "Bo Chen",
      "Tao Liu",
      "Qi Chen",
      "Xie Chen",
      "Zilong Zheng"
    ],
    "abstract": "Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22167v1",
    "published_date": "2025-11-27 07:12:26 UTC",
    "updated_date": "2025-11-27 07:12:26 UTC"
  },
  {
    "arxiv_id": "2512.23713v1",
    "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents",
    "authors": [
      "Jahidul Islam",
      "Md Ataullha",
      "Saiful Azad"
    ],
    "abstract": "LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\\% on the development set and 71.6\\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at github.com/jahidulzaid/PyBanglaCodeActAgent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 Pages",
    "pdf_url": "https://arxiv.org/pdf/2512.23713v1",
    "published_date": "2025-11-27 07:09:47 UTC",
    "updated_date": "2025-11-27 07:09:47 UTC"
  },
  {
    "arxiv_id": "2511.22154v2",
    "title": "WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios",
    "authors": [
      "Eun Chang",
      "Zhuangqun Huang",
      "Yiwei Liao",
      "Sagar Ravi Bhavsar",
      "Amogh Param",
      "Tammy Stark",
      "Adel Ahmadyan",
      "Xiao Yang",
      "Jiaqi Wang",
      "Ahsan Abdullah",
      "Giang Nguyen",
      "Akil Iyer",
      "David Hall",
      "Elissa Li",
      "Shane Moon",
      "Nicolas Scheffer",
      "Kirmani Ahmed",
      "Babak Damavandi",
      "Rakesh Wanga",
      "Anuj Kumar",
      "Rohit Patel",
      "Xin Luna Dong"
    ],
    "abstract": "We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "11 pages, 5 figures, NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22154v2",
    "published_date": "2025-11-27 06:44:49 UTC",
    "updated_date": "2025-12-02 08:14:37 UTC"
  },
  {
    "arxiv_id": "2511.22153v2",
    "title": "Simplex-Optimized Hybrid Ensemble for Large Language Model Text Detection Under Generative Distribution Drif",
    "authors": [
      "Sepyan Purnama Kristanto",
      "Lutfi Hakim",
      "Dianni Yusuf"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) has made it difficult to distinguish human writing from machine-produced text in many real applications. Detectors that were effective for one generation of models tend to degrade when newer models or modified decoding strategies are introduced. In this work, we study this lack of stability and propose a hybrid ensemble that is explicitly designed to cope with changing generator distributions. The ensemble combines three complementary components: a RoBERTa-based classifier fine-tuned for supervised detection, a curvature-inspired score based on perturbing the input and measuring changes in model likelihood, and a compact stylometric model built on hand-crafted linguistic features. The outputs of these components are fused on the probability simplex, and the weights are chosen via validation-based search. We frame this approach in terms of variance reduction and risk under mixtures of generators, and show that the simplex constraint provides a simple way to trade off the strengths and weaknesses of each branch. Experiments on a 30000 document corpus drawn from several LLM families including models unseen during training and paraphrased attack variants show that the proposed method achieves 94.2% accuracy and an AUC of 0.978. The ensemble also lowers false positives on scientific articles compared to strong baselines, which is critical in educational and research settings where wrongly flagging human work is costly",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 2 Figure, Politeknik Negeri Banyuwangi",
    "pdf_url": "https://arxiv.org/pdf/2511.22153v2",
    "published_date": "2025-11-27 06:42:56 UTC",
    "updated_date": "2025-12-07 08:57:07 UTC"
  },
  {
    "arxiv_id": "2511.22151v1",
    "title": "A perceptual bias of AI Logical Argumentation Ability in Writing",
    "authors": [
      "Xi Cun",
      "Jifan Ren",
      "Asha Huang",
      "Siyu Li",
      "Ruzhen Song"
    ],
    "abstract": "Can machines think? This is a central question in artificial intelligence research. However, there is a substantial divergence of views on the answer to this question. Why do people have such significant differences of opinion, even when they are observing the same real world performance of artificial intelligence? The ability of logical reasoning like humans is often used as a criterion to assess whether a machine can think. This study explores whether human biases influence evaluations of the reasoning abilities of AI. An experiment was conducted where participants assessed two texts on the same topic, one AI generated and one human written,to test for perceptual biases in evaluating logical reasoning. Based on the experimental findings, a questionnaire was designed to quantify the attitudes toward AI.The results reveal a bias in perception. The evaluations of the logical reasoning ability of AI generated texts are significantly influenced by the preconceived views on the logical reasoning abilities of AI. Furthermore, frequent AI users were less likely to believe that AI usage undermines independent thinking.This study highlights the need to address perceptual biases to improve public understanding of AI's capabilities and foster better human AI interactions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22151v1",
    "published_date": "2025-11-27 06:39:11 UTC",
    "updated_date": "2025-11-27 06:39:11 UTC"
  },
  {
    "arxiv_id": "2511.22148v2",
    "title": "Towards Heterogeneous Quantum Federated Learning: Challenges and Solutions",
    "authors": [
      "Ratun Rahman",
      "Dinh C. Nguyen",
      "Christo Kurisummoottil Thomas",
      "Walid Saad"
    ],
    "abstract": "Quantum federated learning (QFL) combines quantum computing and federated learning to enable decentralized model training while maintaining data privacy. QFL can improve computational efficiency and scalability by taking advantage of quantum properties such as superposition and entanglement. However, existing QFL frameworks largely focus on homogeneity among quantum \\textcolor{black}{clients, and they do not account} for real-world variances in quantum data distributions, encoding techniques, hardware noise levels, and computational capacity. These differences can create instability during training, slow convergence, and reduce overall model performance. In this paper, we conduct an in-depth examination of heterogeneity in QFL, classifying it into two categories: data or system heterogeneity. Then we investigate the influence of heterogeneity on training convergence and model aggregation. We critically evaluate existing mitigation solutions, highlight their limitations, and give a case study that demonstrates the viability of tackling quantum heterogeneity. Finally, we discuss potential future research areas for constructing robust and scalable heterogeneous QFL frameworks.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "Accepted at IEEE Network Magazine",
    "pdf_url": "https://arxiv.org/pdf/2511.22148v2",
    "published_date": "2025-11-27 06:35:45 UTC",
    "updated_date": "2025-12-04 07:04:04 UTC"
  },
  {
    "arxiv_id": "2511.22147v1",
    "title": "RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks",
    "authors": [
      "Yanping Li",
      "Zhening Liu",
      "Zijian Li",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "abstract": "As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22147v1",
    "published_date": "2025-11-27 06:35:08 UTC",
    "updated_date": "2025-11-27 06:35:08 UTC"
  },
  {
    "arxiv_id": "2601.14262v1",
    "title": "On Meta-Evaluation",
    "authors": [
      "Hongxiao Li",
      "Chenxi Wang",
      "Fanda Fan",
      "Zihan Wang",
      "Wanling Gao",
      "Lei Wang",
      "Jianfeng Zhan"
    ],
    "abstract": "Evaluation is the foundation of empirical science, yet the evaluation of evaluation itself -- so-called meta-evaluation -- remains strikingly underdeveloped. While methods such as observational studies, design of experiments (DoE), and randomized controlled trials (RCTs) have shaped modern scientific practice, there has been little systematic inquiry into their comparative validity and utility across domains. Here we introduce a formal framework for meta-evaluation by defining the evaluation space, its structured representation, and a benchmark we call AxiaBench. AxiaBench enables the first large-scale, quantitative comparison of ten widely used evaluation methods across eight representative application domains. Our analysis reveals a fundamental limitation: no existing method simultaneously achieves accuracy and efficiency across diverse scenarios, with DoE and observational designs in particular showing significant deviations from real-world ground truth. We further evaluate a unified method of entire-space stratified sampling from previous evaluatology research, and the results report that it consistently outperforms prior approaches across all tested domains. These results establish meta-evaluation as a scientific object in its own right and provide both a conceptual foundation and a pragmatic tool set for advancing trustworthy evaluation in computational and experimental research.",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "stat.ME",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.14262v1",
    "published_date": "2025-11-27 06:31:16 UTC",
    "updated_date": "2025-11-27 06:31:16 UTC"
  },
  {
    "arxiv_id": "2511.22143v1",
    "title": "Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading",
    "authors": [
      "Adarsh Gupta",
      "Japleen Kaur",
      "Tanvi Doshi",
      "Teena Sharma",
      "Nishchal K. Verma",
      "Shantaram Vasikarla"
    ],
    "abstract": "Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted and Presented at IEEE UEMCON, IBM T.J. Watson Research Center, New York, USA, 2024",
    "pdf_url": "https://arxiv.org/pdf/2511.22143v1",
    "published_date": "2025-11-27 06:20:09 UTC",
    "updated_date": "2025-11-27 06:20:09 UTC"
  },
  {
    "arxiv_id": "2511.22099v1",
    "title": "Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs",
    "authors": [
      "Daniel Agyei Asante",
      "Md Mokarram Chowdhury",
      "Yang Li"
    ],
    "abstract": "Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2511.22099v1",
    "published_date": "2025-11-27 04:40:56 UTC",
    "updated_date": "2025-11-27 04:40:56 UTC"
  },
  {
    "arxiv_id": "2511.22095v1",
    "title": "Binary-30K: A Heterogeneous Dataset for Deep Learning in Binary Analysis and Malware Detection",
    "authors": [
      "Michael J. Bommarito"
    ],
    "abstract": "Deep learning research for binary analysis faces a critical infrastructure gap. Today, existing datasets target single platforms, require specialized tooling, or provide only hand-engineered features incompatible with modern neural architectures; no single dataset supports accessible research and pedagogy on realistic use cases. To solve this, we introduce Binary-30K, the first heterogeneous binary dataset designed for sequence-based models like transformers. Critically, Binary-30K covers Windows, Linux, macOS, and Android across 15+ CPU architectures. With 29,793 binaries and approximately 26.93% malware representation, Binary-30K enables research on platform-invariant detection, cross-target transfer learning, and long-context binary understanding. The dataset provides pre-computed byte-level BPE tokenization alongside comprehensive structural metadata, supporting both sequence modeling and structure-aware approaches. Platform-first stratified sampling ensures representative coverage across operating systems and architectures, while distribution via Hugging Face with official train/validation/test splits enables reproducible benchmarking. The dataset is publicly available at https://huggingface.co/datasets/mjbommar/binary-30k, providing an accessible resource for researchers, practitioners, and students alike.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "35 pages, 7 figures, 11 tables, 4 appendices. Dataset available at https://huggingface.co/datasets/mjbommar/binary-30k",
    "pdf_url": "https://arxiv.org/pdf/2511.22095v1",
    "published_date": "2025-11-27 04:33:16 UTC",
    "updated_date": "2025-11-27 04:33:16 UTC"
  },
  {
    "arxiv_id": "2512.07873v4",
    "title": "Advancing time series completion via RFAMoE and MDFF",
    "authors": [
      "Ci Zhang",
      "Huayu Li",
      "Changdi Yang",
      "Jiangnan Xia",
      "Yanzhi Wang",
      "Xiaolong Ma",
      "Jin Lu",
      "Ao Li",
      "Geng Yuan"
    ],
    "abstract": "Recent studies show that using diffusion models for time series signal reconstruction holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adaptively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency overhead. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.07873v4",
    "published_date": "2025-11-27 04:06:55 UTC",
    "updated_date": "2026-01-12 01:55:03 UTC"
  },
  {
    "arxiv_id": "2511.22080v1",
    "title": "A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization",
    "authors": [
      "Tianle Li",
      "Yongzhi Huang",
      "Linshan Jiang",
      "Chang Liu",
      "Qipeng Xie",
      "Wenfeng Du",
      "Lu Wang",
      "Kaishun Wu"
    ],
    "abstract": "In federated learning (FL), models must \\emph{converge quickly} under tight communication budgets while \\emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \\emph{momentum} to accelerate progress, and \\emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \\emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \\emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \\textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \\emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \\emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22080v1",
    "published_date": "2025-11-27 04:02:09 UTC",
    "updated_date": "2025-11-27 04:02:09 UTC"
  },
  {
    "arxiv_id": "2511.22076v1",
    "title": "Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents",
    "authors": [
      "Yue Zhong",
      "Yongju Tong",
      "Jiawen Kang",
      "Minghui Dai",
      "Hong-Ning Dai",
      "Zhou Su",
      "Dusit Niyato"
    ],
    "abstract": "The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22076v1",
    "published_date": "2025-11-27 03:55:18 UTC",
    "updated_date": "2025-11-27 03:55:18 UTC"
  },
  {
    "arxiv_id": "2511.22074v2",
    "title": "Real-Time Procedural Learning From Experience for AI Agents",
    "authors": [
      "Dasheng Bi",
      "Yubin Hu",
      "Mohammed N. Nasir"
    ],
    "abstract": "Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22074v2",
    "published_date": "2025-11-27 03:51:49 UTC",
    "updated_date": "2025-12-02 22:56:22 UTC"
  },
  {
    "arxiv_id": "2511.22072v1",
    "title": "A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting",
    "authors": [
      "Jinhao Li",
      "Hao Wang"
    ],
    "abstract": "Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2511.22072v1",
    "published_date": "2025-11-27 03:47:57 UTC",
    "updated_date": "2025-11-27 03:47:57 UTC"
  },
  {
    "arxiv_id": "2512.17913v1",
    "title": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
    "authors": [
      "Nihir Chadderwala"
    ],
    "abstract": "Recent advances in generative AI have enabled sophisticated multi-agent architectures for healthcare, where large language models power collaborative clinical decision-making. However, these distributed systems face critical challenges in ensuring message integrity and fault tolerance when operating in adversarial or untrusted environments.This paper presents a novel Byzantine fault-tolerant multi-agent system specifically designed for healthcare applications, integrating gossip-based message propagation with cryptographic validation mechanisms. Our system employs specialized AI agents for diagnosis, treatment planning, emergency response, and data analysis, coordinated through a Byzantine consensus protocol that tolerates up to f faulty nodes among n = 3f + 1 total nodes. We implement a gossip protocol for decentralized message dissemination, achieving consensus with 2f + 1 votes while maintaining system operation even under Byzantine failures. Experimental results demonstrate that our approach successfully validates medical messages with cryptographic signatures, prevents replay attacks through timestamp validation, and maintains consensus accuracy of 100% with up to 33% Byzantine nodes. The system provides real-time visualization of consensus rounds, vote tallies, and network topology, enabling transparent monitoring of fault-tolerant operations. This work contributes a practical framework for building secure, resilient healthcare multi-agent systems capable of collaborative medical decision-making in untrusted environments.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.17913v1",
    "published_date": "2025-11-27 03:32:54 UTC",
    "updated_date": "2025-11-27 03:32:54 UTC"
  },
  {
    "arxiv_id": "2511.22048v2",
    "title": "ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion",
    "authors": [
      "Junoh Kang",
      "Donghun Ryou",
      "Bohyung Han"
    ],
    "abstract": "Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22048v2",
    "published_date": "2025-11-27 03:06:21 UTC",
    "updated_date": "2025-12-10 04:34:47 UTC"
  },
  {
    "arxiv_id": "2511.22044v1",
    "title": "Distillability of LLM Security Logic: Predicting Attack Success Rate of Outline Filling Attack via Ranking Regression",
    "authors": [
      "Tianyu Zhang",
      "Zihang Xi",
      "Jingyu Hua",
      "Sheng Zhong"
    ],
    "abstract": "In the realm of black-box jailbreak attacks on large language models (LLMs), the feasibility of constructing a narrow safety proxy, a lightweight model designed to predict the attack success rate (ASR) of adversarial prompts, remains underexplored. This work investigates the distillability of an LLM's core security logic. We propose a novel framework that incorporates an improved outline filling attack to achieve dense sampling of the model's security boundaries. Furthermore, we introduce a ranking regression paradigm that replaces standard regression and trains the proxy model to predict which prompt yields a higher ASR. Experimental results show that our proxy model achieves an accuracy of 91.1 percent in predicting the relative ranking of average long response (ALR), and 69.2 percent in predicting ASR. These findings confirm the predictability and distillability of jailbreak behaviors, and demonstrate the potential of leveraging such distillability to optimize black-box attacks.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22044v1",
    "published_date": "2025-11-27 02:55:31 UTC",
    "updated_date": "2025-11-27 02:55:31 UTC"
  },
  {
    "arxiv_id": "2512.23712v1",
    "title": "STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability",
    "authors": [
      "Guanghui Wang",
      "Jinze Yu",
      "Xing Zhang",
      "Dayuan Jiang",
      "Yin Song",
      "Tomal Deb",
      "Xuefeng Liu",
      "Peiyang He"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.23712v1",
    "published_date": "2025-11-27 02:49:52 UTC",
    "updated_date": "2025-11-27 02:49:52 UTC"
  },
  {
    "arxiv_id": "2511.22033v1",
    "title": "Pathology-Aware Prototype Evolution via LLM-Driven Semantic Disambiguation for Multicenter Diabetic Retinopathy Diagnosis",
    "authors": [
      "Chunzheng Zhu",
      "Yangfang Lin",
      "Jialin Shao",
      "Jianxin Lin",
      "Yijun Wang"
    ],
    "abstract": "Diabetic retinopathy (DR) grading plays a critical role in early clinical intervention and vision preservation. Recent explorations predominantly focus on visual lesion feature extraction through data processing and domain decoupling strategies. However, they generally overlook domain-invariant pathological patterns and underutilize the rich contextual knowledge of foundation models, relying solely on visual information, which is insufficient for distinguishing subtle pathological variations. Therefore, we propose integrating fine-grained pathological descriptions to complement prototypes with additional context, thereby resolving ambiguities in borderline cases. Specifically, we propose a Hierarchical Anchor Prototype Modulation (HAPM) framework to facilitate DR grading. First, we introduce a variance spectrum-driven anchor prototype library that preserves domain-invariant pathological patterns. We further employ a hierarchical differential prompt gating mechanism, dynamically selecting discriminative semantic prompts from both LVLM and LLM sources to address semantic confusion between adjacent DR grades. Finally, we utilize a two-stage prototype modulation strategy that progressively integrates clinical knowledge into visual prototypes through a Pathological Semantic Injector (PSI) and a Discriminative Prototype Enhancer (DPE). Extensive experiments across eight public datasets demonstrate that our approach achieves pathology-guided prototype evolution while outperforming state-of-the-art methods. The code is available at https://github.com/zhcz328/HAPM.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "ACMMM 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.22033v1",
    "published_date": "2025-11-27 02:35:29 UTC",
    "updated_date": "2025-11-27 02:35:29 UTC"
  },
  {
    "arxiv_id": "2511.22031v1",
    "title": "Predicting Public Health Impacts of Electricity Usage",
    "authors": [
      "Yejia Liu",
      "Zhifeng Wu",
      "Pengfei Li",
      "Shaolei Ren"
    ],
    "abstract": "The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 Pages. Accepted to NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models (ResponsibleFM)",
    "pdf_url": "https://arxiv.org/pdf/2511.22031v1",
    "published_date": "2025-11-27 02:33:13 UTC",
    "updated_date": "2025-11-27 02:33:13 UTC"
  },
  {
    "arxiv_id": "2511.22018v1",
    "title": "MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis",
    "authors": [
      "Chunzheng Zhu",
      "Yangfang Lin",
      "Shen Chen",
      "Yijun Wang",
      "Jianxin Lin"
    ],
    "abstract": "Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted by AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2511.22018v1",
    "published_date": "2025-11-27 01:47:43 UTC",
    "updated_date": "2025-11-27 01:47:43 UTC"
  },
  {
    "arxiv_id": "2511.22016v1",
    "title": "AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models",
    "authors": [
      "Yann Le Beux",
      "Oluchi Audu",
      "Oche D. Ankeli",
      "Dhananjay Balakrishnan",
      "Melissah Weya",
      "Marie D. Ralaiarinosy",
      "Ignatius Ezeani"
    ],
    "abstract": "Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22016v1",
    "published_date": "2025-11-27 01:37:23 UTC",
    "updated_date": "2025-11-27 01:37:23 UTC"
  },
  {
    "arxiv_id": "2512.00097v1",
    "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions",
    "authors": [
      "Boyan Duan",
      "Xiao Liang",
      "Shuai Lu",
      "Yaoxiang Wang",
      "Yelong Shen",
      "Kai-Wei Chang",
      "Ying Nian Wu",
      "Mao Yang",
      "Weizhu Chen",
      "Yeyun Gong"
    ],
    "abstract": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.",
    "categories": [
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2512.00097v1",
    "published_date": "2025-11-27 01:05:00 UTC",
    "updated_date": "2025-11-27 01:05:00 UTC"
  },
  {
    "arxiv_id": "2511.22001v1",
    "title": "When Do Domain-Specific Foundation Models Justify Their Cost? A Systematic Evaluation Across Retinal Imaging Tasks",
    "authors": [
      "David Isztl",
      "Tahm Spitznagel",
      "Gabor Mark Somfai",
      "Rui Santos"
    ],
    "abstract": "Large vision foundation models have been widely adopted for retinal disease classification without systematic evidence justifying their parameter requirements. In the present work we address two critical questions: First, are large domain-specific foundation models essential, or do compact general-purpose architectures suffice? Second, does specialized retinal pretraining justify its computational cost? To answer this, we benchmark initialization strategies across four retinal imaging classification tasks spanning Optical Coherence Tomography (OCT) and Color Fundus Photography (CFP) modalities: 8-class OCT classification, 3-class diabetic macular edema (DME), 5-class diabetic retinopathy (DR), and 3-class glaucoma (GL) detection. We evaluate 12-13 model configurations per task, including vision transformers (22.8M-86.6M parameters), Swin Transformers (27.6M-28.3M), ConvNeXt (28.6M), and the domain-specific RETFound models (303M), under identical training conditions. Our results challenge prevailing assumptions: First, we demonstrate that pretraining provides universal benefits (5.18-18.41% improvement), scaling with task difficulty. Second, compact architectures (27-29M) dominate Pareto frontiers; SwinV2-tiny achieves top-1 performance on three datasets. Third, RETFound (303M) justifies its computational cost only for challenging DR grading (accuracy of 71.15%), while ImageNet pretraining proves to be sufficient with all other tasks (DME accuracy: 99.24%, OCT accuracy: 97.96%). CFP tasks show larger pretraining accuracy gains (9.13-18.41%) than OCT (5.18%). Thus, the evidence suggests that compact general-purpose models deliver near-optimal performance for most retinal classification tasks; specialized foundation models warranted only for fine-grained discrimination under extreme class imbalance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.22001v1",
    "published_date": "2025-11-27 00:59:21 UTC",
    "updated_date": "2025-11-27 00:59:21 UTC"
  },
  {
    "arxiv_id": "2511.21997v1",
    "title": "Joint Estimation of Sea State and Vessel Parameters Using a Mass-Spring-Damper Equivalence Model",
    "authors": [
      "Ranjeet K. Tiwari",
      "Daniel Sgarioto",
      "Peter Graham",
      "Alexei Skvortsov",
      "Sanjeev Arulampalam",
      "Damith C. Ranasinghe"
    ],
    "abstract": "Real-time sea state estimation is vital for applications like shipbuilding and maritime safety. Traditional methods rely on accurate wave-vessel transfer functions to estimate wave spectra from onboard sensors. In contrast, our approach jointly estimates sea state and vessel parameters without needing prior transfer function knowledge, which may be unavailable or variable. We model the wave-vessel system using pseudo mass-spring-dampers and develop a dynamic model for the system. This method allows for recursive modeling of wave excitation as a time-varying input, relaxing prior works' assumption of a constant input. We derive statistically consistent process noise covariance and implement a square root cubature Kalman filter for sensor data fusion. Further, we derive the Posterior Cramer-Rao lower bound to evaluate estimator performance. Extensive Monte Carlo simulations and data from a high-fidelity validated simulator confirm that the estimated wave spectrum matches methods assuming complete transfer function knowledge.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.21997v1",
    "published_date": "2025-11-27 00:48:35 UTC",
    "updated_date": "2025-11-27 00:48:35 UTC"
  },
  {
    "arxiv_id": "2511.21990v1",
    "title": "A Safety and Security Framework for Real-World Agentic Systems",
    "authors": [
      "Shaona Ghosh",
      "Barnaby Simkin",
      "Kyriacos Shiarlis",
      "Soumili Nandi",
      "Dan Zhao",
      "Matthew Fiedler",
      "Julia Bazinska",
      "Nikki Pope",
      "Roopa Prabhu",
      "Daniel Rohrer",
      "Michael Demoret",
      "Bartley Richardson"
    ],
    "abstract": "This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.21990v1",
    "published_date": "2025-11-27 00:19:24 UTC",
    "updated_date": "2025-11-27 00:19:24 UTC"
  }
]
{
  "date": "2025-01-31",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-31 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 模型的推理优化、安全增强、多模态处理和强化学习应用，突出大型语言模型 (LLM) 在复杂任务中的潜力，以及知名学者如 Tengyu Ma 和 Aleksandrs Slivkins 的贡献，如 STP 和 LLM 在探索-利用权衡中的表现，强调了高效计算和鲁棒性的前沿进展。\n\n以下是今天更新的关键论文摘要，我优先选取了重要、话题性和有影响力的文章（如 LLM 推理、强化学习和多模态模型），并将相关论文归类讨论。限于篇幅，我会快速掠过一些较基础或应用性较弱的论文（如纯文献综述或特定领域优化），并保留核心学术术语如 LLM、强化学习和扩散模型等。\n\n### LLM 推理与安全（重点领域，讨论多篇）\n- **标题：Should You Use Your Large Language Model to Explore or Exploit?（是否应使用大型语言模型进行探索或利用？）**  \n  作者：Keegan Harris, Aleksandrs Slivkins（知名学者）。主要贡献：评估 LLM 在探索-利用权衡中的性能，发现 LLM 在探索大动作空间时有效，但利用小规模任务时不如线性回归。主要发现：通过 in-context 缓解，LLM 可改善探索能力，但整体表现仍需优化。\n\n- **标题：STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving（自对弈 LLM 定理证明器：通过迭代猜想和证明）**  \n  作者：Kefan Dong, Tengyu Ma（知名学者）。主要贡献：提出自对弈框架，LLM 同时担任猜想者和证明者，生成高质量训练数据。主要发现：在 Lean 和 Isabelle 上，模型证明成功率翻倍，达到 28.5%，并在多项基准上实现最先进性能。\n\n- **标题：Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations（通过神经符号表示提升 LLM 的基于规则推理）**  \n  作者：Varun Dhanraj, Chris Eliasmith。主要贡献：引入神经符号方法，将 LLM 隐藏状态编码为符号向量，提高数值推理效率。主要发现：相比传统方法，损失降低 82.86%，正确率提高 24.5 倍。\n\n- **标题：Do LLMs Strategically Reveal, Conceal, and Infer Information?（LLM 是否战略性地揭示、隐藏和推断信息？）**  \n  作者：Mustafa O. Karabag, Ufuk Topcu。主要贡献：测试 LLM 在隐藏身份游戏中的信息控制能力。主要发现：LLM 在识别欺骗时表现好，但无法有效隐藏秘密，揭示了 LLM 在非合作场景中的局限性。\n\n这些论文突出了 LLM 在推理和安全中的潜力，但也暴露了 hallucination 和策略性不足的问题，值得关注未来部署。\n\n### 强化学习与优化（高话题度，选取代表性）\n- **标题：Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone（奇妙的 multitasking 梯度更新及其在锥体中的发现）**  \n  作者：Negar Hassanpour 等。主要贡献：提出 ConicGrad 框架，通过角度约束解决多任务学习中的梯度冲突。主要发现：在监督和强化学习基准上，实现最先进性能，提升任务平衡。\n\n- **标题：Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers（理解 Adam 为什么优于 SGD：Transformer 中的梯度异质性）**  \n  作者：Akiyoshi Tomihari, Issei Sato。主要贡献：分析梯度异质性影响 Transformer 优化，主要归因于层归一化位置。主要发现：实验验证 sign-based 优化（如 Adam）更鲁棒，提供未来算法设计指导。\n\n- **标题：Reward-Guided Speculative Decoding for Efficient LLM Reasoning（奖励引导的推测解码用于高效 LLM 推理）**  \n  作者：Baohao Liao 等。主要贡献：引入奖励模型指导解码，优化 LLM 推理效率。主要发现：比传统方法快 4.4 倍，同时准确率提升 3.5%，适用于资源受限场景。\n\n这些强化学习论文强调了梯度优化和高效推理的实际意义，相关方法可扩展到更多 AI 应用。\n\n### 多模态和图像处理（印象深刻的应用）\n- **标题：Multimodal MRI-Ultrasound AI for Prostate Cancer Detection（多模态 MRI-超声 AI 用于前列腺癌检测）**  \n  作者：Hassan Jahanandish 等。主要贡献：开发 3D UNet 框架融合 MRI 和超声图像，提高前列腺癌检测精度。主要发现：比单模态模型敏感性高 7%，并优于放射科医生，潜力提升活检和治疗规划。\n\n- **标题：DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets（DermaSynth：使用开源皮肤病数据集生成丰富的合成图像-文本对）**  \n  作者：Abdurrahim Yilmaz 等。主要贡献：创建 92,020 对合成图像-文本数据，提升皮肤病 AI 模型。主要发现：微调后，模型在多任务上超越 GPT-4o，推动皮肤病 AI 研究。\n\n其他多模态论文如虚拟活检和图像生成，也展示了 AI 在医疗中的潜力，但限于空间，不再详述。\n\n### 其他领域（快速掠过，选取亮点）\n- **标题：s1: Simple test-time scaling（s1：简单的测试时缩放）**  \n  作者：Niklas Muennighoff 等。主要贡献：提出预算强制方法，提升 LLM 测试时性能。主要发现：在 AIME24 上准确率从 50% 升至 57%，高效且实用。\n\n- **标题：Year-over-Year Developments in Financial Fraud Detection via Deep Learning（基于深度学习的金融欺诈检测年度发展）**  \n  作者：Yisong Chen 等。主要贡献：系统回顾 2019-2024 年深度学习在欺诈检测中的进展，强调数据隐私和特征工程。主要发现：解决不平衡数据集问题，提供未来方向。\n\n其余论文如联邦学习（e.g., \"Understanding Federated Learning from IID to Non-IID dataset\"）、知识图谱和生物医学优化（e.g., \"Pathological MRI Segmentation\"），虽有贡献但较专业或应用性不强，仅提及它们扩展了 AI 在分布式和医疗领域的应用。\n\n总之，今天的论文以 AI 创新为主，LLM 和强化学习是亮点，建议读者关注 STP 和 ConicGrad 等方法，以提升模型的泛化性和效率。更多细节可查阅 arXiv 原文！",
  "papers": [
    {
      "arxiv_id": "2502.00225v1",
      "title": "Should You Use Your Large Language Model to Explore or Exploit?",
      "title_zh": "你是否应该使用大型语言模型来探索或利用？",
      "authors": [
        "Keegan Harris",
        "Aleksandrs Slivkins"
      ],
      "abstract": "We evaluate the ability of the current generation of large language models\n(LLMs) to help a decision-making agent facing an exploration-exploitation\ntradeoff. We use LLMs to explore and exploit in silos in various (contextual)\nbandit tasks. We find that while the current LLMs often struggle to exploit,\nin-context mitigations may be used to substantially improve performance for\nsmall-scale tasks. However even then, LLMs perform worse than a simple linear\nregression. On the other hand, we find that LLMs do help at exploring large\naction spaces with inherent semantics, by suggesting suitable candidates to\nexplore.",
      "tldr_zh": "这篇论文评估了大型语言模型（LLMs）在处理探索-利用权衡（exploration-exploitation tradeoff）的决策代理中的表现。研究者通过在各种（contextual）老虎机任务中，让 LLMs 分别进行探索和利用实验，发现 LLMs 在利用方面往往表现不佳，尽管通过 in-context mitigations 可以显著改善小规模任务的性能，但仍逊色于简单的线性回归。另一方面，LLMs 在探索具有内在语义的大行动空间时表现出优势，能够有效建议合适的探索候选。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00225v1",
      "published_date": "2025-01-31 23:42:53 UTC",
      "updated_date": "2025-01-31 23:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:06:40.351897"
    },
    {
      "arxiv_id": "2502.00217v1",
      "title": "Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone",
      "title_zh": "翻译失败",
      "authors": [
        "Negar Hassanpour",
        "Muhammad Kamran Janjua",
        "Kunlin Zhang",
        "Sepehr Lavasani",
        "Xiaowen Zhang",
        "Chunhua Zhou",
        "Chao Gao"
      ],
      "abstract": "Balancing competing objectives remains a fundamental challenge in multi-task\nlearning (MTL), primarily due to conflicting gradients across individual tasks.\nA common solution relies on computing a dynamic gradient update vector that\nbalances competing tasks as optimization progresses. Building on this idea, we\npropose ConicGrad, a principled, scalable, and robust MTL approach formulated\nas a constrained optimization problem. Our method introduces an angular\nconstraint to dynamically regulate gradient update directions, confining them\nwithin a cone centered on the reference gradient of the overall objective. By\nbalancing task-specific gradients without over-constraining their direction or\nmagnitude, ConicGrad effectively resolves inter-task gradient conflicts.\nMoreover, our framework ensures computational efficiency and scalability to\nhigh-dimensional parameter spaces. We conduct extensive experiments on standard\nsupervised learning and reinforcement learning MTL benchmarks, and demonstrate\nthat ConicGrad achieves state-of-the-art performance across diverse tasks.",
      "tldr_zh": "这篇论文解决了多任务学习 (MTL) 中任务间梯度冲突的核心挑战，提出了一种名为 ConicGrad 的方法，将 MTL 表述为约束优化问题。ConicGrad 通过引入角度约束，将梯度更新方向限制在一个以整体目标梯度为中心的锥形区域内，从而动态平衡任务特定梯度，而不过度限制其方向或幅度。实验在监督学习和强化学习 MTL 基准上表明，该方法实现了最先进性能，同时确保了计算效率和对高维参数空间的可扩展性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 7 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2502.00217v1",
      "published_date": "2025-01-31 23:11:12 UTC",
      "updated_date": "2025-01-31 23:11:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:06:52.537926"
    },
    {
      "arxiv_id": "2502.00213v2",
      "title": "Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers",
      "title_zh": "理解为什么 Adam 优于 SGD：Transformers 中的梯度",
      "authors": [
        "Akiyoshi Tomihari",
        "Issei Sato"
      ],
      "abstract": "Transformers are challenging to optimize with SGD and typically require\nadaptive optimizers such as Adam. However, the reasons behind the superior\nperformance of Adam over SGD remain unclear. In this study, we investigate the\noptimization of transformers by focusing on gradient heterogeneity, defined as\nthe disparity in gradient norms among parameters. Our analysis shows that\ngradient heterogeneity hinders gradient-based optimization, including SGD,\nwhile sign-based optimization, a simplified variant of Adam, is less affected.\nWe further examine gradient heterogeneity in transformers and show that it is\ninfluenced by the placement of layer normalization. Experimental results from\nfine-tuning transformers in both NLP and vision domains validate our\ntheoretical analyses. This study provides insights into the optimization\nchallenges of transformers and offers guidance for designing future\noptimization algorithms. Code is available at\nhttps://github.com/tom4649/gradient-heterogeneity.",
      "tldr_zh": "本研究探讨了为什么 Adam 在优化 Transformers 时优于 SGD，焦点在于梯度异质性（gradient heterogeneity），即参数间梯度范数的差异。分析显示，gradient heterogeneity 会阻碍基于梯度的优化方法如 SGD，但对基于符号的优化（如 Adam 的简化版）影响较小，且这种异质性受层归一化（layer normalization）的放置所影响。在 NLP 和视觉领域的 Transformers 微调实验中，研究验证了这些理论发现，并为设计未来的优化算法提供了宝贵指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00213v2",
      "published_date": "2025-01-31 23:05:52 UTC",
      "updated_date": "2025-05-16 12:15:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:07:04.476815"
    },
    {
      "arxiv_id": "2502.00212v4",
      "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving",
      "title_zh": "STP：自对弈 LLM 定理证明器，采用迭代猜想和证明机制",
      "authors": [
        "Kefan Dong",
        "Tengyu Ma"
      ],
      "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens\ngenerated during the training in Lean, STP proves 28.5% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (65.0%, pass@3200),\nProofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release\nour code, model, and dataset in this URL: https://github.com/kfdong/STP.",
      "tldr_zh": "该论文解决了LLMs在形式定理证明中缺乏高质量训练数据的问题，提出STP（Self-play LLM Theorem Provers）框架，通过迭代猜想（conjecturing）和证明（proving）的自玩机制来提升模型性能。STP包含两个角色：conjecturer负责生成当前prover勉强能证明的挑战性猜想，并通过迭代训练不断提升其难度；prover则使用标准专家迭代方法尝试证明这些猜想。实验结果显示，在Lean平台上，STP证明了28.5%的LeanWorkbook数据集语句，是之前专家迭代方法的双倍，并在miniF2F-test（65.0%、pass@3200）、Proofnet-test（23.9%、pass@3200）和PutnamBench（8/644、pass@3200）上达到最先进性能。作者发布了相关代码、模型和数据集。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.LG",
      "comment": "25 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.00212v4",
      "published_date": "2025-01-31 23:01:48 UTC",
      "updated_date": "2025-03-21 03:27:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:07:18.321068"
    },
    {
      "arxiv_id": "2502.00205v2",
      "title": "EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics",
      "title_zh": "EcoWeedNet：一种轻量级且自动化的杂草检测方法，用于可持续的下一代农业消费电子产品",
      "authors": [
        "Omar H. Khater",
        "Abdul Jabbar Siddiqui",
        "M. Shamim Hossain",
        "Aiman El-Maleh"
      ],
      "abstract": "Sustainable agriculture plays a crucial role in ensuring world food security\nfor consumers. A critical challenge faced by sustainable precision agriculture\nis weed growth, as weeds compete for essential resources with crops, such as\nwater, soil nutrients, and sunlight, which notably affect crop yields. The\nadoption of automated computer vision technologies and ground agricultural\nconsumer electronic vehicles in precision agriculture offers sustainable,\nlow-carbon solutions. However, prior works suffer from issues such as low\naccuracy and precision, as well as high computational expense. This work\nproposes EcoWeedNet, a novel model that enhances weed detection performance\nwithout introducing significant computational complexity, aligning with the\ngoals of low-carbon agricultural practices. The effectiveness of the proposed\nmodel is demonstrated through comprehensive experiments on the CottonWeedDet12\nbenchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves\nperformance comparable to that of large models (mAP@0.5 = 95.2%), yet with\nsignificantly fewer parameters (approximately 4.21% of the parameters of\nYOLOv4), lower computational complexity and better computational efficiency\n6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's\ndeployability on low-power consumer hardware, lower energy consumption, and\nhence reduced carbon footprint, thereby emphasizing the application prospects\nof EcoWeedNet in next-generation sustainable agriculture. These findings\nprovide the way forward for increased application of environmentally-friendly\nagricultural consumer technologies.",
      "tldr_zh": "本文提出 EcoWeedNet，一种轻量级自动化杂草检测模型，旨在解决可持续农业中杂草对作物资源竞争的问题，同时克服现有方法的低准确率和高计算开销。EcoWeedNet 通过优化设计实现了高效性能，在 CottonWeedDet12 数据集上达到 mAP@0.5 = 95.2% 的水平，仅需 YOLOv4 的 4.21% 参数和 6.59% GFLOPs。实验结果证明，该模型适用于低功耗消费电子设备，降低了能源消耗和碳足迹，推动下一代可持续农业技术的应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00205v2",
      "published_date": "2025-01-31 22:46:20 UTC",
      "updated_date": "2025-05-07 11:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:07:29.171523"
    },
    {
      "arxiv_id": "2502.00201v1",
      "title": "Year-over-Year Developments in Financial Fraud Detection via Deep Learning: A Systematic Literature Review",
      "title_zh": "翻译失败",
      "authors": [
        "Yisong Chen",
        "Chuqing Zhao",
        "Yixin Xu",
        "Chuanhao Nie"
      ],
      "abstract": "This paper systematically reviews advancements in deep learning (DL)\ntechniques for financial fraud detection, a critical issue in the financial\nsector. Using the Kitchenham systematic literature review approach, 57 studies\npublished between 2019 and 2024 were analyzed. The review highlights the\neffectiveness of various deep learning models such as Convolutional Neural\nNetworks, Long Short-Term Memory, and transformers across domains such as\ncredit card transactions, insurance claims, and financial statement audits.\nPerformance metrics such as precision, recall, F1-score, and AUC-ROC were\nevaluated. Key themes explored include the impact of data privacy frameworks\nand advancements in feature engineering and data preprocessing. The study\nemphasizes challenges such as imbalanced datasets, model interpretability, and\nethical considerations, alongside opportunities for automation and\nprivacy-preserving techniques such as blockchain integration and Principal\nComponent Analysis. By examining trends over the past five years, this review\nidentifies critical gaps and promising directions for advancing DL applications\nin financial fraud detection, offering actionable insights for researchers and\npractitioners.",
      "tldr_zh": "这篇系统文献综述分析了2019年至2024年间57篇研究，探讨了深度学习(DL)技术在金融欺诈检测领域的年际进展，包括Convolutional Neural Networks (CNN)、Long Short-Term Memory (LSTM) 和 transformers 等模型在信用卡交易、保险索赔和财务报表审计等领域的应用。研究评估了性能指标如precision、recall、F1-score 和 AUC-ROC，突出了数据隐私框架、特征工程和数据预处理的重要性，同时指出了挑战如数据集不平衡、模型可解释性和伦理问题。综述还强调了未来机会，如区块链整合和Principal Component Analysis (PCA) 等隐私保护技术，并为研究者和从业者提供了可操作见解，以推动DL在金融欺诈检测中的创新发展。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00201v1",
      "published_date": "2025-01-31 22:31:50 UTC",
      "updated_date": "2025-01-31 22:31:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:07:40.893978"
    },
    {
      "arxiv_id": "2502.00196v2",
      "title": "DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access Dermatology Datasets",
      "title_zh": "DermaSynth：利用开放访问皮肤病学数据集的丰富合成图像-文本对",
      "authors": [
        "Abdurrahim Yilmaz",
        "Furkan Yuceyalcin",
        "Ece Gokyayla",
        "Donghee Choi",
        "Ozan Erdem",
        "Ali Anil Demircali",
        "Rahmetullah Varol",
        "Ufuk Gorkem Kirabali",
        "Gulsum Gencoglan",
        "Joram M. Posma",
        "Burak Temelkuran"
      ],
      "abstract": "A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.",
      "tldr_zh": "本研究引入了 DermaSynth 数据集，共包含 92,020 个合成图像-文本对，基于 45,205 张开源皮肤病学图像（包括临床和皮肤镜图像），旨在解决皮肤病学中 Vision LLMs 模型数据缺乏的问题。研究团队使用 Gemini 2.0 结合临床相关提示和 self-instruct 方法生成多样化文本，并融入元数据以减少潜在 hallucination。最终，他们基于此数据集微调了 Llama-3.2-11B-Vision-Instruct 模型，开发出 DermatoLlama 1.0，并开源了数据和代码，以加速皮肤病学 AI 研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.00196v2",
      "published_date": "2025-01-31 22:26:33 UTC",
      "updated_date": "2025-03-04 12:36:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:07:53.423848"
    },
    {
      "arxiv_id": "2502.01660v1",
      "title": "Employee Turnover Prediction: A Cross-component Attention Transformer with Consideration of Competitor Influence and Contagious Effect",
      "title_zh": "翻译失败",
      "authors": [
        "Hao Liu",
        "Yong Ge"
      ],
      "abstract": "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
      "tldr_zh": "本研究针对员工离职（Employee Turnover）问题，提出了一种新型深度学习方法——Cross-component Attention Transformer模型，该模型基于Job Embeddedness Theory，并考虑竞争者影响（Competitor Influence）和传染效应（Contagious Effect），以预测跨多个企业的个体员工离职。实验在真实数据集上表明，该方法显著优于现有基准模型，在预测性能上表现出色。研究还评估了使用该解决方案为招聘者带来的成本节省，并分析了各种驱动因素对员工离职的归因，提供实际商业价值。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.01660v1",
      "published_date": "2025-01-31 22:25:39 UTC",
      "updated_date": "2025-01-31 22:25:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:08:04.555750"
    },
    {
      "arxiv_id": "2502.00194v1",
      "title": "Physics-Informed Neural Network based Damage Identification for Truss Railroad Bridges",
      "title_zh": "基于物理信息神经网络的桁架铁路桥梁损伤识别",
      "authors": [
        "Althaf Shajihan",
        "Kirill Mechitov",
        "Girish Chowdhary",
        "Billie F. Spencer Jr"
      ],
      "abstract": "Railroad bridges are a crucial component of the U.S. freight rail system,\nwhich moves over 40 percent of the nation's freight and plays a critical role\nin the economy. However, aging bridge infrastructure and increasing train\ntraffic pose significant safety hazards and risk service disruptions. The U.S.\nrail network includes over 100,000 railroad bridges, averaging one every 1.4\nmiles of track, with steel bridges comprising over 50% of the network's total\nbridge length. Early identification and assessment of damage in these bridges\nremain challenging tasks. This study proposes a physics-informed neural network\n(PINN) based approach for damage identification in steel truss railroad\nbridges. The proposed approach employs an unsupervised learning approach,\neliminating the need for large datasets typically required by supervised\nmethods. The approach utilizes train wheel load data and bridge response during\ntrain crossing events as inputs for damage identification. The PINN model\nexplicitly incorporates the governing differential equations of the linear\ntime-varying (LTV) bridge-train system. Herein, this model employs a recurrent\nneural network (RNN) based architecture incorporating a custom Runge-Kutta (RK)\nintegrator cell, designed for gradient-based learning. The proposed approach\nupdates the bridge finite element model while also quantifying damage severity\nand localizing the affected structural members. A case study on the Calumet\nBridge in Chicago, Illinois, with simulated damage scenarios, is used to\ndemonstrate the model's effectiveness in identifying damage while maintaining\nlow false-positive rates. Furthermore, the damage identification pipeline is\ndesigned to seamlessly integrate prior knowledge from inspections and drone\nsurveys, also enabling context-aware updating and assessment of bridge's\ncondition.",
      "tldr_zh": "本研究针对美国铁路桥梁的老化和安全风险，提出了一种基于 Physics-Informed Neural Network (PINN) 的方法，用于钢桁架桥梁的损坏识别。该方法采用无监督学习，利用火车轮载荷和桥梁响应作为输入，结合线性时变 (LTV) 系统微分方程、Recurrent Neural Network (RNN) 架构和自定义 Runge-Kutta (RK) 积分器，实现了桥梁有限元模型的更新、损坏严重程度的量化以及受影响部件的定位。在芝加哥 Calumet 桥的模拟案例中，该方法表现出色，准确识别损坏并保持低假阳性率，同时能整合检查和无人机调查等先验知识，实现上下文感知的桥梁评估。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.comp-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "30 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.00194v1",
      "published_date": "2025-01-31 22:22:35 UTC",
      "updated_date": "2025-01-31 22:22:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:08:17.299837"
    },
    {
      "arxiv_id": "2503.15516v1",
      "title": "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
      "title_zh": "翻译失败",
      "authors": [
        "Ho Chit Siu",
        "Jaime D. Peña",
        "Yutai Zhou",
        "Ross E. Allen"
      ],
      "abstract": "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
      "tldr_zh": "本研究旨在识别影响人类对 AI 队友主观偏好的可测量属性，使用合作纸牌游戏 Hanabi 作为基准。首先，通过任务表现、信息理论和博弈理论的客观指标评估 AI 代理的表现，而后进行大规模（N=241）的人-AI 团队实验来收集人类偏好数据。结果显示，AI 的行为多样性、战略主导性和与其他 AI 合作能力比最终游戏分数更能预测人类偏好，从而挑战了先前 reinforcement learning 文献的假设。这些发现可用于设计人类协作 AI 的奖励函数，以提升 AI 队友的适应性。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15516v1",
      "published_date": "2025-01-31 22:10:37 UTC",
      "updated_date": "2025-01-31 22:10:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:08:28.611874"
    },
    {
      "arxiv_id": "2502.01659v2",
      "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
      "title_zh": "更长的注意力跨度：利用稀疏",
      "authors": [
        "Nathaniel Tomczak",
        "Sanmukh Kuppannagari"
      ],
      "abstract": "Transformers have demonstrated great success in numerous domains including\nnatural language processing and bioinformatics. This success stems from the use\nof the attention mechanism by these models in order to represent and propagate\npairwise interactions between individual tokens of sequential data. However,\nthe primary limitation of this operation is its quadratic memory and time\ncomplexity in relation to the input's context length - the length of a sequence\nover which the interactions need to be captured. This significantly limits the\nlength of sequences that can be inferred upon by these models. Extensive\nresearch has been conducted to reduce the number of pairwise interactions to\nsub-quadratic in relation to the context length by introducing sparsity into\nthe attention mechanism through the development of sparse attention masks.\nHowever, efficient implementations that achieve \"true sparsity\" are lacking.\n  In this work, we address this issue by proposing a graph computing view of\nattention where tokens are perceived as nodes of the graph and the attention\nmask determines the edges of the graph. Using this view, we develop graph\nprocessing algorithms to implement the attention mechanism. Both theoretically\nand empirically, we demonstrate that our algorithms only perform the needed\ncomputations, i.e., they are work optimal. We also perform extensive\nexperimentation using popular attention masks to explore the impact of sparsity\non execution time and achievable context length. Our experiments demonstrate\nsignificant speedups in execution times compared to state-of-the-art attention\nimplementations such as FlashAttention for large sequence lengths. We also\ndemonstrate that our algorithms are able to achieve extremely long sequence\nlengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
      "tldr_zh": "本文研究了 Transformers 模型中注意力机制的二次方复杂度问题，该问题限制了处理序列的上下文长度。作者提出一种图计算视角，将 tokens 视为图节点、注意力掩码视为边，并开发了工作最优的图处理算法来实现稀疏注意力。实验结果表明，该算法相较于 FlashAttention 等现有实现显著加速执行时间，并在单个 NVIDIA A100 GPU 上实现了高达 1.6 亿的序列长度处理。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.01659v2",
      "published_date": "2025-01-31 22:05:00 UTC",
      "updated_date": "2025-02-07 13:44:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:08:40.536272"
    },
    {
      "arxiv_id": "2502.00182v2",
      "title": "Understanding Federated Learning from IID to Non-IID dataset: An Experimental Study",
      "title_zh": "翻译失败",
      "authors": [
        "Jungwon Seo",
        "Ferhat Ozgur Catak",
        "Chunming Rong"
      ],
      "abstract": "As privacy concerns and data regulations grow, federated learning (FL) has\nemerged as a promising approach for training machine learning models across\ndecentralized data sources without sharing raw data. However, a significant\nchallenge in FL is that client data are often non-IID (non-independent and\nidentically distributed), leading to reduced performance compared to\ncentralized learning. While many methods have been proposed to address this\nissue, their underlying mechanisms are often viewed from different\nperspectives. Through a comprehensive investigation from gradient descent to\nFL, and from IID to non-IID data settings, we find that inconsistencies in\nclient loss landscapes primarily cause performance degradation in non-IID\nscenarios. From this understanding, we observe that existing methods can be\ngrouped into two main strategies: (i) adjusting parameter update paths and (ii)\nmodifying client loss landscapes. These findings offer a clear perspective on\naddressing non-IID challenges in FL and help guide future research in the\nfield.",
      "tldr_zh": "这篇论文通过实验研究探讨了 Federated Learning (FL) 在 IID 和 non-IID 数据集下的性能差异，强调 non-IID 数据导致的性能下降是由于客户端损失景观的不一致。研究从梯度下降到 FL 的全面调查中，发现现有方法可分为两类策略：(i) 调整参数更新路径，以及 (ii) 修改客户端损失景观。这些发现为解决 FL 中的 non-IID 挑战提供了清晰视角，并为未来研究提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00182v2",
      "published_date": "2025-01-31 21:58:15 UTC",
      "updated_date": "2025-02-07 14:31:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:08:52.130198"
    },
    {
      "arxiv_id": "2502.00174v1",
      "title": "The role of positional encodings in the ARC benchmark",
      "title_zh": "位置编码在 ARC 基准测试中的作用",
      "authors": [
        "Guilherme H. Bandeira Costa",
        "Miguel Freire",
        "Arlindo L. Oliveira"
      ],
      "abstract": "The Abstraction and Reasoning Corpus challenges AI systems to perform\nabstract reasoning with minimal training data, a task intuitive for humans but\ndemanding for machine learning models. Using CodeT5+ as a case study, we\ndemonstrate how limitations in positional encoding hinder reasoning and impact\nperformance. This work further examines the role of positional encoding across\ntransformer architectures, highlighting its critical influence on models of\nvarying sizes and configurations. Comparing several strategies, we find that\nwhile 2D positional encoding and Rotary Position Embedding offer competitive\nperformance, 2D encoding excels in data-constrained scenarios, emphasizing its\neffectiveness for ARC tasks",
      "tldr_zh": "本研究探讨了位置编码（positional encoding）在ARC基准测试中的作用，该基准旨在挑战AI在抽象推理任务中以最小训练数据进行推理。研究以CodeT5+为案例，展示了位置编码的局限性如何影响Transformer架构的推理性能，并分析了其对不同模型大小和配置的影响。通过比较多种策略，如2D positional encoding和Rotary Position Embedding，发现2D位置编码在数据受限场景中表现出色，更适合ARC任务。总体而言，此工作强调了位置编码对提升AI抽象推理能力的关键性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00174v1",
      "published_date": "2025-01-31 21:34:54 UTC",
      "updated_date": "2025-01-31 21:34:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:09:04.143356"
    },
    {
      "arxiv_id": "2502.01657v1",
      "title": "Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations",
      "title_zh": "翻译失败",
      "authors": [
        "Varun Dhanraj",
        "Chris Eliasmith"
      ],
      "abstract": "Large language models (LLMs) continue to face challenges in reliably solving\nreasoning tasks, particularly tasks that involve precise rule following, as\noften found in mathematical reasoning tasks. This paper introduces a novel\nneurosymbolic method that improves LLM reasoning by encoding hidden states into\nneurosymbolic vectors, allowing for problem-solving within a neurosymbolic\nvector space. The results are decoded and combined with the original hidden\nstate, boosting the model's performance on numerical reasoning tasks. By\noffloading computation through neurosymbolic representations, this method\nimproves efficiency, reliability, and interpretability. Our experimental\nresults demonstrate an average of $82.86\\%$ lower cross entropy loss and\n$24.50$ times more problems correctly solved on a suite of mathematical\nreasoning problems compared to chain-of-thought prompting and supervised\nfine-tuning (LoRA), while at the same time not hindering the performance of the\nLLM on other tasks.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在规则-based 推理任务（如数学推理）中的可靠性挑战，提出了一种新型神经符号（neurosymbolic）方法，通过将隐藏状态编码成神经符号向量，在神经符号向量空间中进行问题求解，并将结果解码后与原始隐藏状态结合，以提升模型性能。该方法通过卸载计算提高了效率、可靠性和可解释性。实验结果显示，与 chain-of-thought prompting 和 supervised fine-tuning（LoRA）相比，该方法在数学推理问题上平均降低了 82.86% 的交叉熵损失，并将正确解决的问题数量提高了 24.50 倍，同时不影响 LLM 在其他任务上的表现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.01657v1",
      "published_date": "2025-01-31 20:29:51 UTC",
      "updated_date": "2025-01-31 20:29:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:09:16.819580"
    },
    {
      "arxiv_id": "2502.00151v1",
      "title": "A Comprehensive Review: Applicability of Deep Neural Networks in Business Decision Making and Market Prediction Investment",
      "title_zh": "翻译失败",
      "authors": [
        "Viet Trinh"
      ],
      "abstract": "Big data, both in its structured and unstructured formats, have brought in\nunforeseen challenges in economics and business. How to organize, classify, and\nthen analyze such data to obtain meaningful insights are the ever-going\nresearch topics for business leaders and academic researchers. This paper\nstudies recent applications of deep neural networks in decision making in\neconomical business and investment; especially in risk management, portfolio\noptimization, and algorithmic trading. Set aside limitation in data privacy and\ncross-market analysis, the article establishes that deep neural networks have\nperformed remarkably in financial classification and prediction. Moreover, the\nstudy suggests that by compositing multiple neural networks, spanning different\ndata type modalities, a more robust, efficient, and scalable financial\nprediction framework can be constructed.",
      "tldr_zh": "这篇论文对深度神经网络（Deep Neural Networks）在商业决策和投资市场预测中的应用进行了全面综述，重点探讨了其在风险管理、投资组合优化和算法交易等领域的最新进展。尽管存在数据隐私和跨市场分析的限制，研究发现深度神经网络在金融分类和预测任务中表现出色。最后，论文建议通过整合多种神经网络和不同数据类型模态，构建一个更稳健、高效且可扩展的金融预测框架。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00151v1",
      "published_date": "2025-01-31 20:24:21 UTC",
      "updated_date": "2025-01-31 20:24:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:09:29.500821"
    },
    {
      "arxiv_id": "2504.13842v1",
      "title": "The Model Counting Competitions 2021-2023",
      "title_zh": "模型计数竞赛 2021-2023",
      "authors": [
        "Johannes K. Fichte",
        "Markus Hecher"
      ],
      "abstract": "Modern society is full of computational challenges that rely on probabilistic\nreasoning, statistics, and combinatorics. Interestingly, many of these\nquestions can be formulated by encoding them into propositional formulas and\nthen asking for its number of models. With a growing interest in practical\nproblem-solving for tasks that involve model counting, the community\nestablished the Model Counting (MC) Competition in fall of 2019 with its first\niteration in 2020. The competition aims at advancing applications, identifying\nchallenging benchmarks, fostering new solver development, and enhancing\nexisting solvers for model counting problems and their variants. The first\niteration, brought together various researchers, identified challenges, and\ninspired numerous new applications. In this paper, we present a comprehensive\noverview of the 2021-2023 iterations of the Model Counting Competition. We\ndetail its execution and outcomes. The competition comprised four tracks, each\nfocusing on a different variant of the model counting problem. The first track\ncentered on the model counting problem (MC), which seeks the count of models\nfor a given propositional formula. The second track challenged developers to\nsubmit programs capable of solving the weighted model counting problem (WMC).\nThe third track was dedicated to projected model counting (PMC). Finally, we\ninitiated a track that combined projected and weighted model counting (PWMC).\nThe competition continued with a high level of participation, with seven to\nnine solvers submitted in various different version and based on quite\ndiverging techniques.",
      "tldr_zh": "这篇论文回顾了2021-2023年Model Counting (MC) 比赛的执行和成果，强调了模型计数在概率推理、统计和组合数学中的重要应用。比赛旨在推进实际问题解决、识别挑战基准、促进新求解器开发并提升现有求解器，包括四个轨道：模型计数问题 (MC)、加权模型计数问题 (WMC)、投影模型计数 (PMC) 以及结合投影和加权的PWMC。参与度高，有7到9个求解器提交，基于不同技术，比赛激发了新应用并突显了领域的进展。",
      "categories": [
        "cs.AI",
        "cs.DS",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2504.13842v1",
      "published_date": "2025-01-31 20:12:43 UTC",
      "updated_date": "2025-01-31 20:12:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:09:40.832926"
    },
    {
      "arxiv_id": "2503.04731v1",
      "title": "Epistemic Logic Programs: Non-Ground and Counting Complexity",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Eiter",
        "Johannes K. Fichte",
        "Markus Hecher",
        "Stefan Woltran"
      ],
      "abstract": "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
      "tldr_zh": "这篇论文探讨了Epistemic Logic Programs (ELP)，作为Answer Set Programming (ASP)的扩展，用于在多个world views上推理，并首次建立了非基ELP的复杂性分析。研究提供了ELP程序片段的全面复杂性图景，显示其复杂度达到NEXPTIME级别，并涉及Σ^P_2 oracles；在计数复杂性方面，超越了#EXP，并针对谓词arity有界的场景，复杂度降至多项式层次的第四级。最后，论文给出了基于treewidth的ETH-tight运行时结果，并将其应用于Epistemic literals的概率推理中。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CC"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.04731v1",
      "published_date": "2025-01-31 20:08:52 UTC",
      "updated_date": "2025-01-31 20:08:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:09:53.106852"
    },
    {
      "arxiv_id": "2502.00146v1",
      "title": "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms Radiologist MRI Interpretation: A Multi-Center Study",
      "title_zh": "多模态 MRI-超声波 AI 在前列腺",
      "authors": [
        "Hassan Jahanandish",
        "Shengtian Sang",
        "Cynthia Xinran Li",
        "Sulaiman Vesal",
        "Indrani Bhattacharya",
        "Jeong Hoon Lee",
        "Richard Fan",
        "Geoffrey A. Sonna",
        "Mirabela Rusu"
      ],
      "abstract": "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target\nsuspicious prostate lesions. This has led to artificial intelligence (AI)\napplications improving MRI-based detection of clinically significant prostate\ncancer (CsPCa). However, MRI-detected lesions must still be mapped to\ntransrectal ultrasound (TRUS) images during biopsy, which results in missing\nCsPCa. This study systematically evaluates a multimodal AI framework\nintegrating MRI and TRUS image sequences to enhance CsPCa identification. The\nstudy included 3110 patients from three cohorts across two institutions who\nunderwent prostate biopsy. The proposed framework, based on the 3D UNet\narchitecture, was evaluated on 1700 test cases, comparing performance to\nunimodal AI models that use either MRI or TRUS alone. Additionally, the\nproposed model was compared to radiologists in a cohort of 110 patients. The\nmultimodal AI approach achieved superior sensitivity (80%) and Lesion Dice\n(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared\nto radiologists, the multimodal model showed higher specificity (88% vs. 78%)\nand Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings\ndemonstrate the potential of multimodal AI to improve CsPCa lesion targeting\nduring biopsy and treatment planning, surpassing current unimodal models and\nradiologists; ultimately improving outcomes for prostate cancer patients.",
      "tldr_zh": "本研究提出了一种多模态 AI 框架，整合 MRI 和 TRUS 图像序列（基于 3D UNet 架构），以提升 clinically significant prostate cancer (CsPCa) 的检测准确性，并在 3110 名患者的多中心队列中进行评估。\n该框架在 1700 个测试案例中表现出色，敏感性达 80% 和 Lesion Dice 达 42%，明显优于单模态 MRI (73%, 30%) 或 TRUS (49%, 27%) 模型。\n与放射科医生相比，多模态 AI 模型的特异性更高 (88% vs. 78%)，Lesion Dice 也更优 (38% vs. 33%)，而敏感性相当 (79%)，从而改善 CsPCa 病变定位和治疗规划。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00146v1",
      "published_date": "2025-01-31 20:04:20 UTC",
      "updated_date": "2025-01-31 20:04:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:10:06.612078"
    },
    {
      "arxiv_id": "2502.00145v1",
      "title": "Counting and Reasoning with Plans",
      "title_zh": "翻译失败",
      "authors": [
        "David Speck",
        "Markus Hecher",
        "Daniel Gnad",
        "Johannes K. Fichte",
        "Augusto B. Corrêa"
      ],
      "abstract": "Classical planning asks for a sequence of operators reaching a given goal.\nWhile the most common case is to compute a plan, many scenarios require more\nthan that. However, quantitative reasoning on the plan space remains mostly\nunexplored. A fundamental problem is to count plans, which relates to the\nconditional probability on the plan space. Indeed, qualitative and quantitative\napproaches are well-established in various other areas of automated reasoning.\nWe present the first study to quantitative and qualitative reasoning on the\nplan space. In particular, we focus on polynomially bounded plans. On the\ntheoretical side, we study its complexity, which gives rise to rich reasoning\nmodes. Since counting is hard in general, we introduce the easier notion of\nfacets, which enables understanding the significance of operators. On the\npractical side, we implement quantitative reasoning for planning. Thereby, we\ntransform a planning task into a propositional formula and use knowledge\ncompilation to count different plans. This framework scales well to large plan\nspaces, while enabling rich reasoning capabilities such as learning pruning\nfunctions and explainable planning.",
      "tldr_zh": "这篇论文探讨了Classical planning中计划空间的定量和定性推理，焦点在于计数多项式边界计划，以解决传统规划超出简单序列生成的需求。作者分析了其复杂性，引入facets概念来评估操作的重要性，并通过将规划任务转化为命题公式并使用knowledge compilation进行计数，实现高效的定量推理。实验结果显示，该框架能扩展到大型计划空间，支持学习修剪函数和可解释规划，从而丰富了自动化推理的应用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00145v1",
      "published_date": "2025-01-31 20:03:51 UTC",
      "updated_date": "2025-01-31 20:03:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:10:16.184992"
    },
    {
      "arxiv_id": "2502.00140v1",
      "title": "Demystifying MPNNs: Message Passing as Merely Efficient Matrix Multiplication",
      "title_zh": "揭开 MPNNs 的神秘面纱：",
      "authors": [
        "Qin Jiang",
        "Chengjia Wang",
        "Michael Lones",
        "Wei Pang"
      ],
      "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable success, their\ndesign largely relies on empirical intuition rather than theoretical\nunderstanding. In this paper, we present a comprehensive analysis of GNN\nbehavior through three fundamental aspects: (1) we establish that\n\\textbf{$k$-layer} Message Passing Neural Networks efficiently aggregate\n\\textbf{$k$-hop} neighborhood information through iterative computation, (2)\nanalyze how different loop structures influence neighborhood computation, and\n(3) examine behavior across structure-feature hybrid and structure-only tasks.\nFor deeper GNNs, we demonstrate that gradient-related issues, rather than just\nover-smoothing, can significantly impact performance in sparse graphs. We also\nanalyze how different normalization schemes affect model performance and how\nGNNs make predictions with uniform node features, providing a theoretical\nframework that bridges the gap between empirical success and theoretical\nunderstanding.",
      "tldr_zh": "本论文对 Graph Neural Networks (GNNs) 进行了深入分析，揭示 Message Passing Neural Networks (MPNNs) 实质上是通过高效的矩阵乘法来聚合邻居信息。研究重点考察了 k-layer MPNNs 如何迭代计算 k-hop 邻居信息、不同循环结构对计算的影响，以及 GNNs 在结构-特征混合任务和仅结构任务中的表现。结果表明，在深层 GNNs 中，梯度相关问题比过平滑更显著影响稀疏图的性能，并通过理论框架分析了归一化方案和均匀节点特征对预测的影响，从而桥接了经验成功与理论理解的鸿沟。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00140v1",
      "published_date": "2025-01-31 19:48:03 UTC",
      "updated_date": "2025-01-31 19:48:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:10:28.812192"
    },
    {
      "arxiv_id": "2502.07071v2",
      "title": "TRADES: Generating Realistic Market Simulations with Diffusion Models",
      "title_zh": "TRADES：基于扩散模型生成真实市场模拟",
      "authors": [
        "Leonardo Berti",
        "Bardh Prenkaj",
        "Paola Velardi"
      ],
      "abstract": "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.",
      "tldr_zh": "该研究针对金融市场的复杂性，提出TRADES模型——一个基于Transformer的去噪扩散概率引擎，用于生成真实且响应的Limit Order Book (LOB)市场模拟，以支持交易策略测试、市场影响实验和合成数据生成。TRADES通过捕捉高频市场数据的时空特性，生成基于市场状态的订单流，并引入预测分数（作为MAE指标）来评估模型性能，在两个股票数据集上比现有最先进方法改善了x3.27和x3.47倍。实验证明，TRADES生成的合成数据在金融下游任务中高度实用，并能有效响应市场变化。作者还开发了开源框架DeepMarket和相应的合成LOB数据集，以促进进一步研究。",
      "categories": [
        "q-fin.TR",
        "cs.AI",
        "cs.LG",
        "q-fin.CP"
      ],
      "primary_category": "q-fin.TR",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.07071v2",
      "published_date": "2025-01-31 19:43:13 UTC",
      "updated_date": "2025-02-12 12:38:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:10:41.133955"
    },
    {
      "arxiv_id": "2502.00136v1",
      "title": "A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical Alignment of Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Y. Chang"
      ],
      "abstract": "This paper introduces a three-branch checks-and-balances framework for\nethical alignment of Large Language Models (LLMs), inspired by governmental\nsystems. It implements three independent yet interacting components: LLMs as\nthe executive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse\ncultural contexts while upholding consistent ethical principles. This\narchitecture addresses limitations of reinforcement learning with human\nfeedback (RLHF) by providing interpretable, adaptable, and culturally-aware\nethical reasoning. Through self-supervised learning and adversarial testing,\nour framework demonstrates how emotional modeling can guide linguistic\nbehaviors toward ethical outcomes while preserving independence across\nknowledge generation, ethical oversight, and contextual interpretation.",
      "tldr_zh": "这篇论文提出一个三支制衡框架（inspired by governmental systems），用于大型语言模型（LLMs）的上下文感知伦理对齐。该框架包括 LLMs 作为执行分支负责知识生成、DIKE 作为立法分支建立伦理准则，以及 ERIS 作为司法分支进行上下文解释。通过 DIKE 和 ERIS 的对抗性二元性，该框架能适应多样文化语境，同时维护一致的伦理原则，并解决了 reinforcement learning with human feedback (RLHF) 的局限性。实验结果显示，通过自监督学习和对抗测试，框架利用情感建模引导语言行为实现可解释、可适应和文化感知的伦理推理。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "F.2.2"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages, 6 tables, 6 figures. arXiv admin note: substantial text\n  overlap with arXiv:2405.07076",
      "pdf_url": "http://arxiv.org/pdf/2502.00136v1",
      "published_date": "2025-01-31 19:41:28 UTC",
      "updated_date": "2025-01-31 19:41:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:10:53.344668"
    },
    {
      "arxiv_id": "2502.00133v1",
      "title": "Exploring Transfer Learning for Deep Learning Polyp Detection in Colonoscopy Images Using YOLOv8",
      "title_zh": "探索迁移",
      "authors": [
        "Fabian Vazquez",
        "Jose Angel Nuñez",
        "Xiaoyan Fu",
        "Pengfei Gu",
        "Bin Fu"
      ],
      "abstract": "Deep learning methods have demonstrated strong performance in objection\ntasks; however, their ability to learn domain-specific applications with\nlimited training data remains a significant challenge. Transfer learning\ntechniques address this issue by leveraging knowledge from pre-training on\nrelated datasets, enabling faster and more efficient learning for new tasks.\nFinding the right dataset for pre-training can play a critical role in\ndetermining the success of transfer learning and overall model performance. In\nthis paper, we investigate the impact of pre-training a YOLOv8n model on seven\ndistinct datasets, evaluating their effectiveness when transferred to the task\nof polyp detection. We compare whether large, general-purpose datasets with\ndiverse objects outperform niche datasets with characteristics similar to\npolyps. In addition, we assess the influence of the size of the dataset on the\nefficacy of transfer learning. Experiments on the polyp datasets show that\nmodels pre-trained on relevant datasets consistently outperform those trained\nfrom scratch, highlighting the benefit of pre-training on datasets with shared\ndomain-specific features.",
      "tldr_zh": "本研究探讨了使用转移学习(Transfer Learning)来提升 YOLOv8n 模型在结肠镜图像中检测息肉的性能，针对深度学习在领域特定任务中训练数据有限的挑战。研究者通过在七个不同数据集上预训练模型，并比较大型通用数据集（包含多样对象）和小众数据集（与息肉特征类似）的效果，同时评估了数据集规模的影响。实验结果表明，在相关数据集上预训练的模型比从零开始训练的模型表现更好，强调了预训练在共享领域特定特征方面的关键优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, 3 figures, 6 tables, SPIE conference",
      "pdf_url": "http://arxiv.org/pdf/2502.00133v1",
      "published_date": "2025-01-31 19:33:45 UTC",
      "updated_date": "2025-01-31 19:33:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:11:05.183273"
    },
    {
      "arxiv_id": "2502.05202v1",
      "title": "Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies",
      "title_zh": "翻译失败",
      "authors": [
        "Nadav Timor",
        "Jonathan Mamou",
        "Daniel Korat",
        "Moshe Berchansky",
        "Oren Pereg",
        "Gaurav Jain",
        "Roy Schwartz",
        "Moshe Wasserblat",
        "David Harel"
      ],
      "abstract": "Accelerating the inference of large language models (LLMs) is a critical\nchallenge in generative AI. Speculative decoding (SD) methods offer substantial\nefficiency gains by generating multiple tokens using a single target forward\npass. However, existing SD approaches require the drafter and target models to\nshare the same vocabulary, thus limiting the pool of possible drafters, often\nnecessitating the training of a drafter from scratch. We present three new SD\nmethods that remove this shared-vocabulary constraint. All three methods\npreserve the target distribution (i.e., they are lossless) and work with\noff-the-shelf models without requiring additional training or modifications.\nEmpirically, on summarization, programming, and long-context tasks, our\nalgorithms achieve significant speedups over standard autoregressive decoding.\nBy enabling any off-the-shelf model to serve as drafter and requiring no\nretraining, this work substantially broadens the applicability of the SD\nframework in practice.",
      "tldr_zh": "这篇论文提出了三种新的无损 Speculative Decoding (SD) 算法，用于加速大型语言模型 (LLMs) 的推理，这些算法支持 drafter 和 target 模型的异构词汇表。不同于现有方法，这些算法无需 drafter 和 target 模型共享词汇表，且可直接应用于现成模型而无须额外训练或修改。实验结果显示，在摘要、编程和长上下文任务上，该方法比标准自回归解码实现了显著速度提升，从而大幅扩展了 SD 框架的实际适用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.05202v1",
      "published_date": "2025-01-31 19:13:58 UTC",
      "updated_date": "2025-01-31 19:13:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:11:16.390817"
    },
    {
      "arxiv_id": "2501.19403v2",
      "title": "Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Yingdan Shi",
        "Sijia Liu",
        "Ren Wang"
      ],
      "abstract": "Machine unlearning seeks to remove the influence of specified data from a\ntrained model. While metrics such as unlearning accuracy (UA) and membership\ninference attack (MIA) provide baselines for assessing unlearning performance,\nthey fall short of evaluating the forgetting reliability. In this paper, we\nfind that the data misclassified across UA and MIA still have their ground\ntruth labels included in the prediction set from the uncertainty quantification\nperspective, which raises a fake unlearning issue. To address this issue, we\npropose two novel metrics inspired by conformal prediction that more reliably\nevaluate forgetting quality. Building on these insights, we further propose a\nconformal prediction-based unlearning framework that integrates conformal\nprediction into Carlini & Wagner adversarial attack loss, which can\nsignificantly push the ground truth label out of the conformal prediction set.\nThrough extensive experiments on image classification task, we demonstrate both\nthe effectiveness of our proposed metrics and the superiority of our unlearning\nframework, which improves the UA of existing unlearning methods by an average\nof 6.6% through the incorporation of a tailored loss term alone.",
      "tldr_zh": "该论文重新定义了 Machine Unlearning，指出现有指标如 Unlearning Accuracy (UA) 和 Membership Inference Attack (MIA) 无法可靠评估遗忘质量，因为数据可能仍保留在预测集中，导致“假遗忘”问题。作者提出两个受 Conformal Prediction 启发的全新指标，用于更准确地评估遗忘可靠性，并设计了一个基于 Conformal Prediction 的遗忘框架，将其整合到 Carlini & Wagner 攻击损失中，以显著排除真实标签。实验结果显示，该框架在图像分类任务上平均提升现有方法的 UA 6.6%，证明了其有效性和优越性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19403v2",
      "published_date": "2025-01-31 18:58:43 UTC",
      "updated_date": "2025-05-20 03:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:11:29.572840"
    },
    {
      "arxiv_id": "2502.00094v2",
      "title": "AIN: The Arabic INclusive Large Multimodal Model",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed Heakl",
        "Sara Ghaboura",
        "Omkar Thawkar",
        "Fahad Shahbaz Khan",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer",
        "Salman Khan"
      ],
      "abstract": "Amid the swift progress of large language models (LLMs) and their evolution\ninto large multimodal models (LMMs), significant strides have been made in\nhigh-resource languages such as English and Chinese. While Arabic LLMs have\nseen notable progress, Arabic LMMs remain largely unexplored, often narrowly\nfocusing on a few specific aspects of the language and visual understanding. To\nbridge this gap, we introduce AIN-the Arabic Inclusive Multimodal\nModel-designed to excel across diverse domains. AIN is an English-Arabic\nbilingual LMM designed to excel in English and Arabic, leveraging carefully\nconstructed 3.6 million high-quality Arabic-English multimodal data samples.\nAIN demonstrates state-of-the-art Arabic performance, while also possessing\nstrong English-language visual capabilities. On the recent CAMEL-Bench\nbenchmark comprising 38 sub-domains including, multi-image understanding,\ncomplex visual perception, handwritten document understanding, video\nunderstanding, medical imaging, plant diseases, and remote sensing-based land\nuse understanding, our AIN demonstrates strong performance with the 7B model\noutperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains\nand 38 sub-domains. AIN's superior capabilities position it as a significant\nstep toward empowering Arabic speakers with advanced multimodal generative AI\ntools across diverse applications.",
      "tldr_zh": "本研究引入了AIN（The Arabic INclusive Large Multimodal Model），一个英文-阿拉伯双语的大型多模态模型（LMM），旨在填补阿拉伯语在LMM领域的空白，并覆盖多样化领域如多图像理解、复杂视觉感知和医疗成像。AIN利用精心构建的360万高质量阿拉伯-英文多模态数据进行训练，实现了在英语和阿拉伯语上的出色性能。在CAMEL-Bench基准测试中，AIN的7B模型在八个领域和38个子领域上比GPT-4o高出3.4%的绝对增益，展示了其强大的视觉和语言能力。该模型为阿拉伯语使用者提供先进的生成AI工具，推动了多模态应用的普及和发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 16 figures, ACL",
      "pdf_url": "http://arxiv.org/pdf/2502.00094v2",
      "published_date": "2025-01-31 18:58:20 UTC",
      "updated_date": "2025-02-04 18:05:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:11:40.449723"
    },
    {
      "arxiv_id": "2501.19400v1",
      "title": "Vintix: Action Model via In-Context Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Andrey Polubarov",
        "Nikita Lyubaykin",
        "Alexander Derevyagin",
        "Ilya Zisman",
        "Denis Tarasov",
        "Alexander Nikulin",
        "Vladislav Kurenkov"
      ],
      "abstract": "In-Context Reinforcement Learning (ICRL) represents a promising paradigm for\ndeveloping generalist agents that learn at inference time through\ntrial-and-error interactions, analogous to how large language models adapt\ncontextually, but with a focus on reward maximization. However, the scalability\nof ICRL beyond toy tasks and single-domain settings remains an open challenge.\nIn this work, we present the first steps toward scaling ICRL by introducing a\nfixed, cross-domain model capable of learning behaviors through in-context\nreinforcement learning. Our results demonstrate that Algorithm Distillation, a\nframework designed to facilitate ICRL, offers a compelling and competitive\nalternative to expert distillation to construct versatile action models. These\nfindings highlight the potential of ICRL as a scalable approach for generalist\ndecision-making systems. Code to be released at\nhttps://github.com/dunnolab/vintix",
      "tldr_zh": "这篇论文介绍了 Vintix，一种通过 In-Context Reinforcement Learning (ICRL) 构建行动模型的方法，旨在让通用代理在推理时通过试错互动学习行为，从而实现奖励最大化。作者解决了 ICRL 在扩展到复杂任务和跨域设置时的可扩展性挑战，使用 Algorithm Distillation 框架作为专家蒸馏的替代方案，构建了一个固定的跨域模型。实验结果显示，该方法在通用决策系统中表现出色，具有竞争力，并证明 ICRL 是可扩展的潜在途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. In review",
      "pdf_url": "http://arxiv.org/pdf/2501.19400v1",
      "published_date": "2025-01-31 18:57:08 UTC",
      "updated_date": "2025-01-31 18:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:11:52.599972"
    },
    {
      "arxiv_id": "2501.19399v1",
      "title": "Scalable-Softmax Is Superior for Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Ken M. Nakanishi"
      ],
      "abstract": "The maximum element of the vector output by the Softmax function approaches\nzero as the input vector size increases. Transformer-based language models rely\non Softmax to compute attention scores, causing the attention distribution to\nflatten as the context size grows. This reduces the model's ability to\nprioritize key information effectively and potentially limits its length\ngeneralization. To address this problem, we propose Scalable-Softmax (SSMax),\nwhich replaces Softmax in scenarios where the input vector size varies. SSMax\ncan be seamlessly integrated into existing Transformer-based architectures.\nExperimental results in language modeling show that models using SSMax not only\nachieve faster loss reduction during pretraining but also significantly improve\nperformance in long contexts and key information retrieval. Furthermore, an\nanalysis of attention scores reveals that SSMax enables the model to focus\nattention on key information even in long contexts. Additionally, although\nmodels that use SSMax from the beginning of pretraining achieve better length\ngeneralization, those that have already started pretraining can still gain some\nof this ability by replacing Softmax in the attention layers with SSMax, either\nduring or after pretraining.",
      "tldr_zh": "该论文发现，Softmax 函数在 Transformer 模型中会导致注意力分布变平，随着上下文大小增加，模型难以优先关键信息，从而限制长度泛化能力。研究提出 Scalable-Softmax (SSMax) 作为替代方案，可无缝整合到现有 Transformer 架构中，用于处理输入向量大小变化的场景。实验结果显示，使用 SSMax 的模型在语言建模预训练中实现更快损失减少，并在长上下文中显著提升性能和关键信息检索能力。此外，即使在预训练过程中或之后替换 Softmax，模型也能获得更好的长度泛化效果。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.19399v1",
      "published_date": "2025-01-31 18:55:35 UTC",
      "updated_date": "2025-01-31 18:55:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:13:57.932980"
    },
    {
      "arxiv_id": "2501.19398v1",
      "title": "Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game",
      "title_zh": "翻译失败",
      "authors": [
        "Mustafa O. Karabag",
        "Ufuk Topcu"
      ],
      "abstract": "Large language model-based (LLM-based) agents have become common in settings\nthat include non-cooperative parties. In such settings, agents' decision-making\nneeds to conceal information from their adversaries, reveal information to\ntheir cooperators, and infer information to identify the other agents'\ncharacteristics. To investigate whether LLMs have these information control and\ndecision-making capabilities, we make LLM agents play the language-based\nhidden-identity game, The Chameleon. In the game, a group of non-chameleon\nagents who do not know each other aim to identify the chameleon agent without\nrevealing a secret. The game requires the aforementioned information control\ncapabilities both as a chameleon and a non-chameleon. The empirical results\nshow that while non-chameleon LLM agents identify the chameleon, they fail to\nconceal the secret from the chameleon, and their winning probability is far\nfrom the levels of even trivial strategies. To formally explain this behavior,\nwe give a theoretical analysis for a spectrum of strategies, from concealing to\nrevealing, and provide bounds on the non-chameleons' winning probability. Based\non the empirical results and theoretical analysis of different strategies, we\ndeduce that LLM-based non-chameleon agents reveal excessive information to\nagents of unknown identities. Our results point to a weakness of contemporary\nLLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic\ninteractions.",
      "tldr_zh": "本研究通过理论和实证分析，探讨大型语言模型（LLMs）在非合作环境中是否能有效揭示、隐藏和推断信息，采用 The Chameleon Game 作为测试平台。在游戏中，LLMs 代理需作为非变色龙识别变色龙，同时隐藏秘密，但实验结果显示它们虽能识别变色龙，却无法有效隐藏信息，导致获胜概率远低于简单策略。理论分析提供了不同策略的边界，解释了 LLMs 过度揭示信息的倾向，并揭示了当代模型如 GPT-4、Gemini 1.5 和 Claude 3.5 Sonnet 在战略互动中的弱点。该工作强调了 LLMs 在信息控制方面的局限性，为未来模型改进提供了重要见解。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19398v1",
      "published_date": "2025-01-31 18:53:43 UTC",
      "updated_date": "2025-01-31 18:53:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:12:16.733063"
    },
    {
      "arxiv_id": "2501.19393v3",
      "title": "s1: Simple test-time scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Niklas Muennighoff",
        "Zitong Yang",
        "Weijia Shi",
        "Xiang Lisa Li",
        "Li Fei-Fei",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer",
        "Percy Liang",
        "Emmanuel Candès",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1",
      "tldr_zh": "本文提出了一种简单有效的 test-time scaling 方法，通过额外测试时计算提升语言模型的推理性能。具体而言，作者创建了 s1K 数据集（包含1000个问题及其推理痕迹），并开发了 budget forcing 技术来控制计算预算，例如通过追加 \"Wait\" 强制模型重新检查答案。然后，对 Qwen2.5-32B-Instruct 模型进行 supervised finetuning 后，s1-32B 模型在 MATH 和 AIME24 等数学竞赛问题上比 o1-preview 提高了最多27%，并通过 budget forcing 进一步将 AIME24 性能从50%提升至57%。模型、数据和代码已在 GitHub 开源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "46 pages (9 main), 10 figures, 15 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.19393v3",
      "published_date": "2025-01-31 18:48:08 UTC",
      "updated_date": "2025-03-01 06:07:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:12:29.640261"
    },
    {
      "arxiv_id": "2501.19383v1",
      "title": "Decoding-based Regression",
      "title_zh": "基于解码的回归",
      "authors": [
        "Xingyou Song",
        "Dara Bahri"
      ],
      "abstract": "Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.",
      "tldr_zh": "本文提出了一种基于解码的回归（Decoding-based Regression）方法，利用语言模型将数字预测表示为解码字符串，并为其提供了理论基础。研究调查了因果自回归序列模型在各种特征表示上的应用，发现尽管这些模型通过交叉熵损失训练用于下一个标记预测，但其在表格回归任务中的性能与传统方法相当。不仅如此，该方法还具备灵活性，能够捕捉任意分布，例如在密度估计任务中展现出优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Google DeepMind Technical Report, 25 pages. Code can be found at\n  https://github.com/google-research/optformer/tree/main/optformer/decoding_regression",
      "pdf_url": "http://arxiv.org/pdf/2501.19383v1",
      "published_date": "2025-01-31 18:37:42 UTC",
      "updated_date": "2025-01-31 18:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:16:09.199656"
    },
    {
      "arxiv_id": "2501.19364v1",
      "title": "CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation",
      "title_zh": "CoSTI：一致性模型用于（更快的）时空插值",
      "authors": [
        "Javier Solís-García",
        "Belén Vega-Márquez",
        "Juan A. Nepomuceno",
        "Isabel A. Nepomuceno-Chamorro"
      ],
      "abstract": "Multivariate Time Series Imputation (MTSI) is crucial for many applications,\nsuch as healthcare monitoring and traffic management, where incomplete data can\ncompromise decision-making. Existing state-of-the-art methods, like Denoising\nDiffusion Probabilistic Models (DDPMs), achieve high imputation accuracy;\nhowever, they suffer from significant computational costs and are notably\ntime-consuming due to their iterative nature. In this work, we propose CoSTI,\nan innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI\nemploys Consistency Training to achieve comparable imputation quality to DDPMs\nwhile drastically reducing inference times, making it more suitable for\nreal-time applications. We evaluate CoSTI across multiple datasets and missing\ndata scenarios, demonstrating up to a 98% reduction in imputation time with\nperformance on par with diffusion-based models. This work bridges the gap\nbetween efficiency and accuracy in generative imputation tasks, providing a\nscalable solution for handling missing data in critical spatio-temporal\nsystems.",
      "tldr_zh": "该论文提出 CoSTI，一种基于 Consistency Models 的创新方法，用于多变量时间序列填充 (MTSI)，旨在解决现有方法如 Denoising Diffusion Probabilistic Models (DDPMs) 的高计算成本和耗时问题。CoSTI 通过 Consistency Training 实现与 DDPMs 相当的填充准确性，同时将推理时间减少高达 98%，使其更适合实时应用。实验在多个数据集和缺失数据场景中验证了 CoSTI 的性能，弥合了效率与准确性之间的差距，为处理时空系统中缺失数据提供了可扩展的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 5 figures, 13 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.19364v1",
      "published_date": "2025-01-31 18:14:28 UTC",
      "updated_date": "2025-01-31 18:14:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:14:19.826139"
    },
    {
      "arxiv_id": "2501.19361v1",
      "title": "We're Different, We're the Same: Creative Homogeneity Across LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Emily Wenger",
        "Yoed Kenett"
      ],
      "abstract": "Numerous powerful large language models (LLMs) are now available for use as\nwriting support tools, idea generators, and beyond. Although these LLMs are\nmarketed as helpful creative assistants, several works have shown that using an\nLLM as a creative partner results in a narrower set of creative outputs.\nHowever, these studies only consider the effects of interacting with a single\nLLM, begging the question of whether such narrowed creativity stems from using\na particular LLM -- which arguably has a limited range of outputs -- or from\nusing LLMs in general as creative assistants. To study this question, we elicit\ncreative responses from humans and a broad set of LLMs using standardized\ncreativity tests and compare the population-level diversity of responses. We\nfind that LLM responses are much more similar to other LLM responses than human\nresponses are to each other, even after controlling for response structure and\nother key variables. This finding of significant homogeneity in creative\noutputs across the LLMs we evaluate adds a new dimension to the ongoing\nconversation about creativity and LLMs. If today's LLMs behave similarly, using\nthem as a creative partners -- regardless of the model used -- may drive all\nusers towards a limited set of \"creative\" outputs.",
      "tldr_zh": "该研究调查了不同大型语言模型 (LLMs) 在创意输出上的同质性 (creative homogeneity)，发现尽管 LLMs 被视为创意助手，但它们生成的响应比人类响应更相似。研究者使用标准化的创意测试，从人类和多种 LLMs 收集响应，并控制响应结构等变量进行比较。结果显示，LLMs 间的响应多样性显著低于人类，这表明使用任何 LLMs 作为创意伙伴都可能将用户引导向有限的“创意”输出集。该发现为关于 LLMs 和创意性的讨论增添了新维度。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19361v1",
      "published_date": "2025-01-31 18:12:41 UTC",
      "updated_date": "2025-01-31 18:12:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:14:32.635551"
    },
    {
      "arxiv_id": "2502.00089v1",
      "title": "Ensembles of Low-Rank Expert Adapters",
      "title_zh": "翻译失败",
      "authors": [
        "Yinghao Li",
        "Vianne Gao",
        "Chao Zhang",
        "MohamadAli Torkamani"
      ],
      "abstract": "The training and fine-tuning of large language models (LLMs) often involve\ndiverse textual data from multiple sources, which poses challenges due to\nconflicting gradient directions, hindering optimization and specialization.\nThese challenges can undermine model generalization across tasks, resulting in\nreduced downstream performance. Recent research suggests that fine-tuning LLMs\non carefully selected, task-specific subsets of data can match or even surpass\nthe performance of using the entire dataset. Building on these insights, we\npropose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve\nthe model's capability to handle diverse tasks. ELREA clusters the training\ninstructions based on their gradient directions, representing different areas\nof expertise and thereby reducing conflicts during optimization. Expert\nadapters are then trained on these clusters, utilizing the low-rank adaptation\n(LoRA) technique to ensure training efficiency and model scalability. During\ninference, ELREA combines predictions from the most relevant expert adapters\nbased on the input data's gradient similarity to the training clusters,\nensuring optimal adapter selection for each task. Experiments show that our\nmethod outperforms baseline LoRA adapters trained on the full dataset and other\nensemble approaches with similar training and inference complexity across a\nrange of domain-specific tasks.",
      "tldr_zh": "这项研究针对大型语言模型（LLMs）在处理多样化文本数据时面临的冲突梯度问题，提出了一种名为Ensembles of Low-Rank Expert Adapters (ELREA)的框架，以提升模型的优化和任务专业化能力。ELREA通过基于梯度方向聚类训练指令，创建不同专业领域的专家适配器，并利用Low-Rank Adaptation (LoRA)技术进行高效训练。推理阶段则根据输入数据的梯度相似性动态选择和组合相关专家适配器。实验结果显示，该方法在各种领域特定任务上优于基线LoRA适配器和其他集成方法，显著提高了模型性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "29 pages, 5 figures, 5 tables; proceedings in ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2502.00089v1",
      "published_date": "2025-01-31 18:07:21 UTC",
      "updated_date": "2025-01-31 18:07:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:14:44.954154"
    },
    {
      "arxiv_id": "2501.19353v3",
      "title": "Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SciCap Challenge 2023",
      "title_zh": "翻译失败",
      "authors": [
        "Ting-Yao E. Hsu",
        "Yi-Li Hsu",
        "Shaurya Rohatgi",
        "Chieh-Yang Huang",
        "Ho Yin Sam Ng",
        "Ryan Rossi",
        "Sungchul Kim",
        "Tong Yu",
        "Lun-Wei Ku",
        "C. Lee Giles",
        "Ting-Hao K. Huang"
      ],
      "abstract": "Since the SciCap datasets launch in 2021, the research community has made\nsignificant progress in generating captions for scientific figures in scholarly\narticles. In 2023, the first SciCap Challenge took place, inviting global teams\nto use an expanded SciCap dataset to develop models for captioning diverse\nfigure types across various academic fields. At the same time, text generation\nmodels advanced quickly, with many powerful pre-trained large multimodal models\n(LMMs) emerging that showed impressive capabilities in various\nvision-and-language tasks. This paper presents an overview of the first SciCap\nChallenge and details the performance of various models on its data, capturing\na snapshot of the fields state. We found that professional editors\noverwhelmingly preferred figure captions generated by GPT-4V over those from\nall other models and even the original captions written by authors. Following\nthis key finding, we conducted detailed analyses to answer this question: Have\nadvanced LMMs solved the task of generating captions for scientific figures?",
      "tldr_zh": "这篇论文回顾了2023年SciCap Challenge，评估大型多模态模型(LMMs)在生成科学图表标题方面的表现，使用扩展的SciCap数据集测试了各种模型。研究发现，专业编辑更青睐GPT-4V生成的标题，甚至优于原作者的标题，这反映了LMMs在视觉语言任务中的显著进步。论文通过详细分析探讨了LMMs是否已完全解决科学图表标题生成的问题，并为该领域提供了当前状态的快照。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to TACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.19353v3",
      "published_date": "2025-01-31 18:02:19 UTC",
      "updated_date": "2025-02-18 18:07:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:14:56.532946"
    },
    {
      "arxiv_id": "2501.19338v1",
      "title": "Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates",
      "title_zh": "翻译失败",
      "authors": [
        "Misha P. T Kaandorp",
        "Damola Agbelese",
        "Hosna Asma-ull",
        "Hyun-Gi Kim",
        "Kelly Payette",
        "Patrice Grehten",
        "Gennari Antonio Giulio",
        "Levente István Lánczi",
        "Andras Jakab"
      ],
      "abstract": "Developing new methods for the automated analysis of clinical fetal and\nneonatal MRI data is limited by the scarcity of annotated pathological datasets\nand privacy concerns that often restrict data sharing, hindering the\neffectiveness of deep learning models. We address this in two ways. First, we\nintroduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to\ngenerate high-quality synthetic pathological fetal and neonatal MRIs from\nsemantic label images. Second, we enhance training data by modifying healthy\nlabel images through morphological alterations to simulate conditions such as\nventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly.\nBy leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs\nfrom these modified pathological label images. Radiologists rated the synthetic\nMRIs as significantly (p < 0.05) superior in quality and diagnostic value\ncompared to real MRIs, demonstrating features such as blood vessels and choroid\nplexus, and improved alignment with label annotations. Synthetic pathological\ndata enhanced state-of-the-art nnUNet segmentation performance, particularly\nfor severe ventriculomegaly cases, with the greatest improvements achieved in\nventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores\nthe potential of generative AI as transformative tool for data augmentation,\noffering improved segmentation performance in pathological cases. This\ndevelopment represents a significant step towards improving analysis and\nsegmentation accuracy in prenatal imaging, and also offers new ways for data\nanonymization through the generation of pathologic image data.",
      "tldr_zh": "本研究针对胎儿和新生儿MRI数据标注稀缺及隐私限制问题，开发了Fetal&Neonatal-DDPM扩散模型框架，用于从语义标签图像生成高质量合成病理MRI，并通过形态修改健康标签图像模拟病理条件如脑室扩大（ventriculomegaly）和小脑发育不全。\n放射科医生评估显示，合成MRI在质量和诊断价值上显著优于真实MRI（p < 0.05），并更好地显示血管和脉络膜等特征以及标签对齐。\n使用合成数据增强nnUNet分割模型的表现，尤其在严重脑室扩大病例中，脑室分割的Dice scores从0.7317提高到0.9253。\n此方法强调生成AI作为数据增强工具的潜力，不仅提升病理MRI分割准确性，还为数据匿名化提供新途径。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "30 pages, 4 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.19338v1",
      "published_date": "2025-01-31 17:36:24 UTC",
      "updated_date": "2025-01-31 17:36:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:15:11.231770"
    },
    {
      "arxiv_id": "2501.19335v2",
      "title": "What is causal about causal models and representations?",
      "title_zh": "因果模型和表示的因果性是什么？",
      "authors": [
        "Frederik Hytting Jørgensen",
        "Luigi Gresele",
        "Sebastian Weichwald"
      ],
      "abstract": "Causal Bayesian networks are 'causal' models since they make predictions\nabout interventional distributions. To connect such causal model predictions to\nreal-world outcomes, we must determine which actions in the world correspond to\nwhich interventions in the model. For example, to interpret an action as an\nintervention on a treatment variable, the action will presumably have to a)\nchange the distribution of treatment in a way that corresponds to the\nintervention, and b) not change other aspects, such as how the outcome depends\non the treatment; while the marginal distributions of some variables may change\nas an effect. We introduce a formal framework to make such requirements for\ndifferent interpretations of actions as interventions precise. We prove that\nthe seemingly natural interpretation of actions as interventions is circular:\nUnder this interpretation, every causal Bayesian network that correctly models\nthe observational distribution is trivially also interventionally valid, and no\naction yields empirical data that could possibly falsify such a model. We prove\nan impossibility result: No interpretation exists that is non-circular and\nsimultaneously satisfies a set of natural desiderata. Instead, we examine\nnon-circular interpretations that may violate some desiderata and show how this\nmay in turn enable the falsification of causal models. By rigorously examining\nhow a causal Bayesian network could be a 'causal' model of the world instead of\nmerely a mathematical object, our formal framework contributes to the\nconceptual foundations of causal representation learning, causal discovery, and\ncausal abstraction, while also highlighting some limitations of existing\napproaches.",
      "tldr_zh": "本论文探讨了因果贝叶斯网络（Causal Bayesian Networks）为什么被视为“因果”模型，焦点在于将模型中的干预（interventions）与现实世界的行动对应起来。作者引入了一个正式框架来精确定义行动作为干预的条件，例如行动必须改变特定变量的分布而不影响其他依赖关系，并证明了这种自然解释是循环的：正确建模观察分布（observational distribution）的模型自动在干预上有效，无法通过经验数据证伪。论文进一步证明了一个不可能性结果，即不存在既非循环又满足一组自然期望的解释；相反，非循环解释虽可能违反某些期望，却能启用对因果模型的证伪，从而为因果表示学习（causal representation learning）、因果发现（causal discovery）和因果抽象（causal abstraction）的理论基础提供贡献，同时揭示了现有方法的局限性。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "math.ST",
        "stat.TH"
      ],
      "primary_category": "stat.ML",
      "comment": "50 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.19335v2",
      "published_date": "2025-01-31 17:35:21 UTC",
      "updated_date": "2025-02-03 17:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:15:21.970621"
    },
    {
      "arxiv_id": "2501.19328v1",
      "title": "Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Pauls",
        "Max Zimmer",
        "Berkant Turan",
        "Sassan Saatchi",
        "Philippe Ciais",
        "Sebastian Pokutta",
        "Fabian Gieseke"
      ],
      "abstract": "With the rise in global greenhouse gas emissions, accurate large-scale tree\ncanopy height maps are essential for understanding forest structure, estimating\nabove-ground biomass, and monitoring ecological disruptions. To this end, we\npresent a novel approach to generate large-scale, high-resolution canopy height\nmaps over time. Our model accurately predicts canopy height over multiple years\ngiven Sentinel-2 time series satellite data. Using GEDI LiDAR data as the\nground truth for training the model, we present the first 10m resolution\ntemporal canopy height map of the European continent for the period 2019-2022.\nAs part of this product, we also offer a detailed canopy height map for 2020,\nproviding more precise estimates than previous studies. Our pipeline and the\nresulting temporal height map are publicly available, enabling comprehensive\nlarge-scale monitoring of forests and, hence, facilitating future research and\necological analyses. For an interactive viewer, see\nhttps://europetreemap.projects.earthengine.app/view/temporalcanopyheight.",
      "tldr_zh": "该研究提出了一种新方法，利用 Sentinel-2 时间序列卫星数据来生成大型、高分辨率的树冠高度地图，以监测森林结构、估算地上生物量并跟踪生态破坏。模型以 GEDI LiDAR 数据作为训练地面真相，成功预测了欧洲大陆 2019-2022 年的树冠高度变化，并首次创建了 10m 分辨率的时序地图，其中 2020 年的地图提供比以往更精确的估计。该管道和结果地图已公开可用，支持大规模森林监测和生态分析，并附带交互式查看器。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages main paper, 5 pages references and appendix, 8 figures, 5\n  tables",
      "pdf_url": "http://arxiv.org/pdf/2501.19328v1",
      "published_date": "2025-01-31 17:26:06 UTC",
      "updated_date": "2025-01-31 17:26:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:15:32.837173"
    },
    {
      "arxiv_id": "2501.19324v2",
      "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
      "title_zh": "奖励引导的推测性解码用于高效的LLM推理",
      "authors": [
        "Baohao Liao",
        "Yuhui Xu",
        "Hanze Dong",
        "Junnan Li",
        "Christof Monz",
        "Silvio Savarese",
        "Doyen Sahoo",
        "Caiming Xiong"
      ],
      "abstract": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.",
      "tldr_zh": "我们引入了Reward-Guided Speculative Decoding (RSD)，一个创新框架，用于提升大型语言模型(LLMs)的推理效率，通过结合轻量级草稿模型和强大目标模型，并利用过程奖励模型评估中间解码步骤，以优先高奖励输出并动态优化计算成本。不同于传统无偏方法，RSD 采用阈值-based混合策略，在理论上实现了资源利用和性能的最佳平衡。在挑战性推理基准测试中，如奥林匹克级别任务，RSD 比仅使用目标模型减少了高达4.4倍的FLOPs，同时比并行解码方法平均提高了3.5%的准确率，展示了其在资源密集型场景中的鲁棒性和成本效益。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.19324v2",
      "published_date": "2025-01-31 17:19:57 UTC",
      "updated_date": "2025-02-14 07:30:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:15:46.464997"
    },
    {
      "arxiv_id": "2501.19321v1",
      "title": "Language Bias in Self-Supervised Learning For Automatic Speech Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Storey",
        "Naomi Harte",
        "Peter Bell"
      ],
      "abstract": "Self-supervised learning (SSL) is used in deep learning to train on large\ndatasets without the need for expensive labelling of the data. Recently, large\nAutomatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to\ntrain on over one hundred different languages simultaneously. However, deeper\ninvestigation shows that the bulk of the training data for XLS-R comes from a\nsmall number of languages. Biases learned through SSL have been shown to exist\nin multiple domains, but language bias in multilingual SSL ASR has not been\nthoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis\n(LTH) to identify language-specific subnetworks within XLS-R and test the\nperformance of these subnetworks on a variety of different languages. We are\nable to show that when fine-tuning, XLS-R bypasses traditional linguistic\nknowledge and builds only on weights learned from the languages with the\nlargest data contribution to the pretraining data.",
      "tldr_zh": "这篇论文探讨了自监督学习(SSL)在多语言自动语音识别(ASR)模型XLS-R中的语言偏差问题，指出尽管XLS-R使用超过一百种语言的训练数据，但实际数据主要来自少数语言。研究者采用Lottery Ticket Hypothesis (LTH)方法来识别模型中的语言特定子网络，并测试这些子网络在多种语言上的性能。结果显示，XLS-R在微调过程中优先利用数据贡献最大的语言的权重，而非传统的语言知识，从而揭示了SSL模型的潜在偏差。该发现为改进多语言ASR系统的公平性和鲁棒性提供了重要见解。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to Speech and Language Technology Workshop (SLT) 2024\n  accessible on IEEE Xplore",
      "pdf_url": "http://arxiv.org/pdf/2501.19321v1",
      "published_date": "2025-01-31 17:16:45 UTC",
      "updated_date": "2025-01-31 17:16:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:15:57.171604"
    },
    {
      "arxiv_id": "2501.19318v1",
      "title": "MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Anirudh Chari",
        "Suraj Reddy",
        "Aditya Tiwari",
        "Richard Lian",
        "Brian Zhou"
      ],
      "abstract": "While large language models (LLMs) have shown promising capabilities as\nzero-shot planners for embodied agents, their inability to learn from\nexperience and build persistent mental models limits their robustness in\ncomplex open-world environments like Minecraft. We introduce MINDSTORES, an\nexperience-augmented planning framework that enables embodied agents to build\nand leverage mental models through natural interaction with their environment.\nDrawing inspiration from how humans construct and refine cognitive mental\nmodels, our approach extends existing zero-shot LLM planning by maintaining a\ndatabase of past experiences that informs future planning iterations. The key\ninnovation is representing accumulated experiences as natural language\nembeddings of (state, task, plan, outcome) tuples, which can then be\nefficiently retrieved and reasoned over by an LLM planner to generate insights\nand guide plan refinement for novel states and tasks. Through extensive\nexperiments in the MineDojo environment, a simulation environment for agents in\nMinecraft that provides low-level controls for Minecraft, we find that\nMINDSTORES learns and applies its knowledge significantly better than existing\nmemory-based LLM planners while maintaining the flexibility and generalization\nbenefits of zero-shot approaches, representing an important step toward more\ncapable embodied AI systems that can learn continuously through natural\nexperience.",
      "tldr_zh": "该研究提出MINDSTORES框架，以解决大型语言模型(LLMs)作为零样本规划器在实体代理中的局限性，即无法从经验中学习和构建持久心理模型，导致在复杂环境如Minecraft中表现不佳。MINDSTORES通过维护一个基于(state, task, plan, outcome)元组的自然语言嵌入数据库，允许代理从过去互动中检索和推理经验，从而指导未来计划的优化和改进。实验在MineDojo模拟环境中显示，MINDSTORES比现有内存-based LLM规划器在知识学习和应用上显著提升，同时保留了零样本方法的灵活性和泛化能力，推动了能持续通过自然经验学习的实体AI系统的发展。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19318v1",
      "published_date": "2025-01-31 17:15:33 UTC",
      "updated_date": "2025-01-31 17:15:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:16:19.757946"
    },
    {
      "arxiv_id": "2501.19308v1",
      "title": "Ontological analysis of proactive life event services",
      "title_zh": "翻译失败",
      "authors": [
        "Kuldar Taveter"
      ],
      "abstract": "Life event service is a direct digital public service provided jointly by\nseveral governmental institutions so that a person can fulfill all the\nobligations and use all the rights that arise due to a particular event or\nsituation in personal life. Life event service consolidates several public\nservices related to the same life event into one service for the service\nconsumer. This paper presents an ontological analysis of life event services,\nwhich is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others.\nThe purpose of the ontological analysis is to understand the meanings of life\nevent, proactive public service based on life event, and other related notions.\nThis kind of ontological analysis is crucial because for implementing the\nhardware and software architectures of e-government and digital public\nservices, it is essential to agree upon the precise meanings of the underlying\nterms.",
      "tldr_zh": "本论文对主动生活事件服务（life event services）进行了本体分析（ontological analysis），旨在澄清生活事件、基于生活事件的主动公共服务（proactive public service）以及相关概念的精确含义。该分析基于Guarino、Guizzardi、Nardi和Wagner等学者的研究，将多个政府机构的公共服务整合为单一服务，帮助个人处理生活事件中的义务和权利。这种本体分析对于电子政府（e-government）和数字公共服务的硬件软件架构实施至关重要，确保底层术语的一致性。",
      "categories": [
        "cs.AI",
        "H.1.1"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19308v1",
      "published_date": "2025-01-31 17:09:53 UTC",
      "updated_date": "2025-01-31 17:09:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:16:31.929830"
    },
    {
      "arxiv_id": "2501.19306v2",
      "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Jiefeng Chen",
        "Jie Ren",
        "Xinyun Chen",
        "Chengrun Yang",
        "Ruoxi Sun",
        "Sercan Ö Arık"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
      "tldr_zh": "本论文针对大型语言模型(LLMs)在测试时计算中存在的收益递减问题（如重复采样或奖励模型评分），提出了一种新型方法SETS（Self-Enhanced Test-Time Scaling）。SETS利用LLMs的自验证(Self-Verification)和自修正(Self-Correction)能力，将采样、自验证和自修正整合到一个统一框架中，实现高效、可扩展的测试时计算。实验结果显示，在规划和推理基准上，SETS相较于替代方法取得了显著性能提升，并展示了更优的测试时扩展规律。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19306v2",
      "published_date": "2025-01-31 17:03:16 UTC",
      "updated_date": "2025-02-03 06:21:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:16:44.329030"
    },
    {
      "arxiv_id": "2501.19301v1",
      "title": "Beyond checkmate: exploring the creative chokepoints in AI text",
      "title_zh": "翻译失败",
      "authors": [
        "Nafis Irtiza Tripto",
        "Saranya Venkatraman",
        "Mahjabin Nahar",
        "Dongwon Lee"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.\nThis rapid advancement has spurred research into various aspects of LLMs, their\ntext generation & reasoning capability, and potential misuse, fueling the\nnecessity for robust detection methods. While numerous prior research has\nfocused on detecting LLM-generated text (AI text) and thus checkmating them,\nour study investigates a relatively unexplored territory: portraying the\nnuanced distinctions between human and AI texts across text segments. Whether\nLLMs struggle with or excel at incorporating linguistic ingenuity across\ndifferent text segments carries substantial implications for determining their\npotential as effective creative assistants to humans. Through an analogy with\nthe structure of chess games-comprising opening, middle, and end games-we\nanalyze text segments (introduction, body, and conclusion) to determine where\nthe most significant distinctions between human and AI texts exist. While AI\ntexts can approximate the body segment better due to its increased length, a\ncloser examination reveals a pronounced disparity, highlighting the importance\nof this segment in AI text detection. Additionally, human texts exhibit higher\ncross-segment differences compared to AI texts. Overall, our research can shed\nlight on the intricacies of human-AI text distinctions, offering novel insights\nfor text detection and understanding.",
      "tldr_zh": "这篇论文探讨了大语言模型 (LLMs) 生成的 AI text 与人类文本在不同段落（如 introduction、body 和 conclusion）的细微差异，使用国际象棋开局、中局和残局的类比进行分析。研究发现，AI text 在较长的 body 段落中更接近人类文本，但整体上存在显著差异，且人类文本显示出更高的跨段落差异，这突显了 body 段落在 AI text 检测中的关键作用。最终，该研究为 AI text 检测和理解 LLMs 的创意潜力提供了新颖见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, single columns, under review at Nature Machine Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2501.19301v1",
      "published_date": "2025-01-31 16:57:01 UTC",
      "updated_date": "2025-01-31 16:57:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:16:56.494982"
    },
    {
      "arxiv_id": "2501.19298v1",
      "title": "Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyao Xu",
        "Dan Zhao",
        "Qingsong Zou",
        "Jingyu Xiao",
        "Yong Jiang",
        "Zhenhui Yuan",
        "Qing Li"
      ],
      "abstract": "In recent years, as smart home systems have become more widespread, security\nconcerns within these environments have become a growing threat. Currently,\nmost smart home security solutions, such as anomaly detection and behavior\nprediction models, are trained using fixed datasets that are precollected.\nHowever, the process of dataset collection is time-consuming and lacks the\nflexibility needed to adapt to the constantly evolving smart home environment.\nAdditionally, the collection of personal data raises significant privacy\nconcerns for users. Lately, large language models (LLMs) have emerged as a\npowerful tool for a wide range of tasks across diverse application domains,\nthanks to their strong capabilities in natural language processing, reasoning,\nand problem-solving. In this paper, we propose an LLM-based synthetic dataset\ngeneration IoTGen framework to enhance the generalization of downstream smart\nhome intelligent models. By generating new synthetic datasets that reflect\nchanges in the environment, smart home intelligent models can be retrained to\novercome the limitations of fixed and outdated data, allowing them to better\nalign with the dynamic nature of real-world home environments. Specifically, we\nfirst propose a Structure Pattern Perception Compression (SPPC) method tailored\nfor IoT behavior data, which preserves the most informative content in the data\nwhile significantly reducing token consumption. Then, we propose a systematic\napproach to create prompts and implement data generation to automatically\ngenerate IoT synthetic data with normative and reasonable properties, assisting\ntask models in adaptive training to improve generalization and real-world\nperformance.",
      "tldr_zh": "该论文针对智能家居系统的安全问题，提出了一种基于 Large Language Models (LLMs) 的 IoTGen 框架，用于生成合成用户行为序列数据集，以克服固定数据集的灵活性不足和隐私风险。框架首先采用 Structure Pattern Perception Compression (SPPC) 方法对 IoT 行为数据进行压缩，保留关键信息并减少 token 消耗；随后，通过系统化的提示设计自动生成规范合理的合成数据。最终，该方法可帮助下游智能家居模型进行适应性训练，提升泛化和实际性能，以更好地适应动态环境。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19298v1",
      "published_date": "2025-01-31 16:55:43 UTC",
      "updated_date": "2025-01-31 16:55:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:17:08.776873"
    },
    {
      "arxiv_id": "2501.19297v2",
      "title": "Analysis of LLMs vs Human Experts in Requirements Engineering",
      "title_zh": "需求工程中 LLMs 与人类专家的分析",
      "authors": [
        "Cory Hymel",
        "Hiroe Johnson"
      ],
      "abstract": "The majority of research around Large Language Models (LLM) application to\nsoftware development has been on the subject of code generation. There is\nlittle literature on LLMs' impact on requirements engineering (RE), which deals\nwith the process of developing and verifying the system requirements. Within\nRE, there is a subdiscipline of requirements elicitation, which is the practice\nof discovering and documenting requirements for a system from users, customers,\nand other stakeholders. In this analysis, we compare LLM's ability to elicit\nrequirements of a software system, as compared to that of a human expert in a\ntime-boxed and prompt-boxed study. We found LLM-generated requirements were\nevaluated as more aligned (+1.12) than human-generated requirements with a\ntrend of being more complete (+10.2%). Conversely, we found users tended to\nbelieve that solutions they perceived as more aligned had been generated by\nhuman experts. Furthermore, while LLM-generated documents scored higher and\nperformed at 720x the speed, their cost was, on average, only 0.06% that of a\nhuman expert. Overall, these findings indicate that LLMs will play an\nincreasingly important role in requirements engineering by improving\nrequirements definitions, enabling more efficient resource allocation, and\nreducing overall project timelines.",
      "tldr_zh": "本文分析了大型语言模型 (LLMs) 与人类专家在需求工程 (Requirements Engineering, RE) 中的表现，重点比较了需求获取 (requirements elicitation) 的能力。研究采用时间限制和提示限制的方法进行对比，发现 LLMs 生成的需求更符合要求 (+1.12 分) 和更完整 (+10.2%)，且其速度是人类专家的 720 倍，成本仅为 0.06%。然而，用户往往误认为更符合的需求由人类专家生成。总体而言，这些发现表明 LLMs 将在 RE 中发挥更大作用，提升需求定义效率、优化资源分配并缩短项目周期。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "8 pages, 15 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.19297v2",
      "published_date": "2025-01-31 16:55:17 UTC",
      "updated_date": "2025-02-04 15:33:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:17:21.021937"
    },
    {
      "arxiv_id": "2501.19271v1",
      "title": "Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks",
      "title_zh": "基于概念的可解释人工智能：指标和基准",
      "authors": [
        "Halil Ibrahim Aysel",
        "Xiaohao Cai",
        "Adam Prugel-Bennett"
      ],
      "abstract": "Concept-based explanation methods, such as concept bottleneck models (CBMs),\naim to improve the interpretability of machine learning models by linking their\ndecisions to human-understandable concepts, under the critical assumption that\nsuch concepts can be accurately attributed to the network's feature space.\nHowever, this foundational assumption has not been rigorously validated, mainly\nbecause the field lacks standardised metrics and benchmarks to assess the\nexistence and spatial alignment of such concepts. To address this, we propose\nthree metrics: the concept global importance metric, the concept existence\nmetric, and the concept location metric, including a technique for visualising\nconcept activations, i.e., concept activation mapping. We benchmark post-hoc\nCBMs to illustrate their capabilities and challenges. Through qualitative and\nquantitative experiments, we demonstrate that, in many cases, even the most\nimportant concepts determined by post-hoc CBMs are not present in input images;\nmoreover, when they are present, their saliency maps fail to align with the\nexpected regions by either activating across an entire object or misidentifying\nrelevant concept-specific regions. We analyse the root causes of these\nlimitations, such as the natural correlation of concepts. Our findings\nunderscore the need for more careful application of concept-based explanation\ntechniques especially in settings where spatial interpretability is critical.",
      "tldr_zh": "本研究针对基于概念的解释性人工智能（如 CBMs）方法的关键假设进行了验证，指出这些方法依赖于概念在网络特征空间中的准确归因，但缺乏标准化指标和基准。论文提出了三个新指标——concept global importance metric、concept existence metric 和 concept location metric，以及一个概念激活映射（concept activation mapping）可视化技术，用于评估概念的存在和空间对齐。作者通过对 post-hoc CBMs 的基准测试和定量实验发现，许多重要概念在输入图像中不存在，即使存在，其 saliency maps 也常激活整个对象或错误识别区域，导致解释不准确。研究分析了这些限制的根因，如概念的自然相关性，并强调在需要空间可解释性的应用中，应更谨慎地使用这些技术。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages it total, 8 main pages",
      "pdf_url": "http://arxiv.org/pdf/2501.19271v1",
      "published_date": "2025-01-31 16:32:36 UTC",
      "updated_date": "2025-01-31 16:32:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:17:32.357796"
    },
    {
      "arxiv_id": "2501.19266v1",
      "title": "Jackpot! Alignment as a Maximal Lottery",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto-Rafael Maura-Rivero",
        "Marc Lanctot",
        "Francesco Visin",
        "Kate Larson"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF), the standard for aligning\nLarge Language Models (LLMs) with human values, is known to fail to satisfy\nproperties that are intuitively desirable, such as respecting the preferences\nof the majority \\cite{ge2024axioms}. To overcome these issues, we propose the\nuse of a probabilistic Social Choice rule called \\emph{maximal lotteries} as a\nreplacement for RLHF. We show that a family of alignment techniques, namely\nNash Learning from Human Feedback (NLHF) \\cite{munos2023nash} and variants,\napproximate maximal lottery outcomes and thus inherit its beneficial\nproperties.\n  We confirm experimentally that our proposed methodology handles situations\nthat arise when working with preferences more robustly than standard RLHF,\nincluding supporting the preferences of the majority, providing principled ways\nof handling non-transitivities in the preference data, and robustness to\nirrelevant alternatives. This results in systems that better incorporate human\nvalues and respect human intentions.",
      "tldr_zh": "该论文批评了Reinforcement Learning from Human Feedback (RLHF) 在对齐Large Language Models (LLMs) 时未能满足理想属性，如尊重多数偏好，并提出使用probabilistic Social Choice 规则 maximal lotteries 作为替代方案。\n他们证明了Nash Learning from Human Feedback (NLHF) 和其变体可以近似 maximal lotteries 的结果，从而继承其有益特性。\n实验显示，这种方法比标准RLHF 更robustly 处理偏好相关问题，包括支持多数偏好、提供处理非传递性的原则性方式，以及对无关备选方案的鲁棒性。\n最终，这有助于开发出更好地整合人类价值观并尊重人类意图的系统。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "econ.TH"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19266v1",
      "published_date": "2025-01-31 16:26:28 UTC",
      "updated_date": "2025-01-31 16:26:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:17:44.742285"
    },
    {
      "arxiv_id": "2501.19256v1",
      "title": "Objective Metrics for Human-Subjects Evaluation in Explainable Reinforcement Learning",
      "title_zh": "可解释强化学习中人类受试者评估的客观指标",
      "authors": [
        "Balint Gyevnar",
        "Mark Towers"
      ],
      "abstract": "Explanation is a fundamentally human process. Understanding the goal and\naudience of the explanation is vital, yet existing work on explainable\nreinforcement learning (XRL) routinely does not consult humans in their\nevaluations. Even when they do, they routinely resort to subjective metrics,\nsuch as confidence or understanding, that can only inform researchers of users'\nopinions, not their practical effectiveness for a given problem. This paper\ncalls on researchers to use objective human metrics for explanation evaluations\nbased on observable and actionable behaviour to build more reproducible,\ncomparable, and epistemically grounded research. To this end, we curate,\ndescribe, and compare several objective evaluation methodologies for applying\nexplanations to debugging agent behaviour and supporting human-agent teaming,\nillustrating our proposed methods using a novel grid-based environment. We\ndiscuss how subjective and objective metrics complement each other to provide\nholistic validation and how future work needs to utilise standardised\nbenchmarks for testing to enable greater comparisons between research.",
      "tldr_zh": "该论文强调，在可解释强化学习(XRL)领域，现有的评估方法往往忽略人类参与，并依赖主观指标（如信心或理解），这无法评估解释的有效性。作者呼吁采用基于可观察和可行动行为的客观人类指标，以提升研究的重复性、可比较性和知识基础。他们整理并比较了几种客观评估方法，用于调试代理行为和支持人类-代理团队合作，并通过一个新的网格环境进行示例说明。最后，论文讨论了主观和客观指标的互补作用，并建议未来研究使用标准化基准来促进跨研究比较。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19256v1",
      "published_date": "2025-01-31 16:12:23 UTC",
      "updated_date": "2025-01-31 16:12:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:17:56.374909"
    },
    {
      "arxiv_id": "2501.19254v3",
      "title": "Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set",
      "title_zh": "线性 Q 学习在 L^2 中不会发散：到有界集的收敛速率",
      "authors": [
        "Xinyu Liu",
        "Zixuan Xie",
        "Shangtong Zhang"
      ],
      "abstract": "$Q$-learning is one of the most fundamental reinforcement learning\nalgorithms. It is widely believed that $Q$-learning with linear function\napproximation (i.e., linear $Q$-learning) suffers from possible divergence\nuntil the recent work Meyn (2024) which establishes the ultimate almost sure\nboundedness of the iterates of linear $Q$-learning. Building on this success,\nthis paper further establishes the first $L^2$ convergence rate of linear\n$Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not\nmake any modification to the original linear $Q$-learning algorithm, do not\nmake any Bellman completeness assumption, and do not make any near-optimality\nassumption on the behavior policy. All we need is an $\\epsilon$-softmax\nbehavior policy with an adaptive temperature. The key to our analysis is the\ngeneral result of stochastic approximations under Markovian noise with\nfast-changing transition functions. As a side product, we also use this general\nresult to establish the $L^2$ convergence rate of tabular $Q$-learning with an\n$\\epsilon$-softmax behavior policy, for which we rely on a novel\npseudo-contraction property of the weighted Bellman optimality operator.",
      "tldr_zh": "该论文证明了线性Q-learning算法在L^2范数下不会发散，并首次建立了其迭代向一个有界集的L^2收敛率，无需修改原算法或假设Bellman完整性或近优策略，只需一个ε-softmax行为策略。研究的关键在于分析随机逼近在Markovian噪声下的行为，特别是过渡函数快速变化的情况。作为副产品，该方法还为表格Q-learning建立了L^2收敛率，依赖于一个新的加权Bellman最优性算子的伪收缩性质。这些结果增强了对Q-learning算法的理论理解，并为强化学习应用提供更可靠的基础。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19254v3",
      "published_date": "2025-01-31 16:10:50 UTC",
      "updated_date": "2025-02-24 16:39:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:18:07.595988"
    },
    {
      "arxiv_id": "2501.19245v2",
      "title": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments",
      "title_zh": "SHARPIE：一种用于强化学习和人-人工智能",
      "authors": [
        "Hüseyin Aydın",
        "Kevin Godin-Dubois",
        "Libio Goncalvez Braz",
        "Floris den Hengst",
        "Kim Baraka",
        "Mustafa Mert Çelikok",
        "Andreas Sauter",
        "Shihan Wang",
        "Frans A. Oliehoek"
      ],
      "abstract": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts.",
      "tldr_zh": "该论文提出SHARPIE，一种模块化框架，用于强化学习(Reinforcement Learning, RL)及其与人类-AI 交互实验的通用支持。该框架包括RL 环境的灵活包装器、算法库、面向参与者的 web 接口、日志工具以及部署在流行云平台和参与者招募平台的设计，旨在简化实验过程。SHARPIE 允许研究人员探索多种人类与 RL 代理的交互问题，如交互式奖励指定、从人类反馈学习、行动委托、偏好提取、用户建模和人类-AI 团队合作，并通过一个标准化的人类-RL 交互接口来统一该研究领域。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19245v2",
      "published_date": "2025-01-31 15:59:50 UTC",
      "updated_date": "2025-02-03 08:41:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:18:20.073895"
    },
    {
      "arxiv_id": "2501.19232v1",
      "title": "A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain Sequential Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Yunzhe Li",
        "Junting Wang",
        "Hari Sundaram",
        "Zhining Liu"
      ],
      "abstract": "Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions\nin unseen domains without the need for additional training or fine-tuning,\nmaking it particularly valuable in data-sparse environments where traditional\nmodels struggle. Recent advancements in large language models (LLMs) have\ngreatly improved ZCDSR by leveraging rich pretrained representations to\nfacilitate cross-domain knowledge transfer. However, a key challenge persists:\ndomain semantic bias, which arises from variations in vocabulary and content\nfocus across domains. This misalignment leads to inconsistencies in item\nembeddings and hinders generalization.\n  To address this issue, we propose a novel framework designed to enhance\nLLM-based ZCDSR by improving cross-domain alignment at both the item and\nsequential levels. At the item level, we introduce a generalization loss that\npromotes inter-domain compactness by aligning embeddings of similar items\nacross domains while maintaining intra-domain diversity to preserve unique item\ncharacteristics. This prevents embeddings from becoming overly generic while\nensuring effective transferability. At the sequential level, we develop a\nmethod for transferring user behavioral patterns by clustering user sequences\nin the source domain and applying attention-based aggregation for target domain\ninference. This dynamic adaptation of user embeddings allows effective\nzero-shot recommendations without requiring target-domain interactions.\n  Comprehensive experiments across multiple datasets and domains demonstrate\nthat our framework significantly improves sequential recommendation performance\nin the ZCDSR setting. By mitigating domain bias and enhancing the\ntransferability of sequential patterns, our method provides a scalable and\nrobust approach for achieving more effective zero-shot recommendations across\ndomains.",
      "tldr_zh": "这篇论文提出了一种针对大型语言模型(LLMs)驱动的Zero-Shot Cross-Domain Sequential Recommendation (ZCDSR)的新框架，旨在解决领域语义偏差问题，该问题导致物品嵌入不一致并阻碍泛化。框架在物品水平引入generalization loss，通过促进inter-domain compactness（跨域紧凑性）对齐相似物品的嵌入，同时保持intra-domain diversity（域内多样性），以确保知识转移而不丢失独特特性；在序列水平，则通过源域用户序列聚类和attention-based aggregation动态适应用户行为模式，实现零样本推荐。实验在多个数据集上证明，该框架显著提高了ZCDSR的性能，缓解了domain bias并增强了序列模式的转移性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.19232v1",
      "published_date": "2025-01-31 15:43:21 UTC",
      "updated_date": "2025-01-31 15:43:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:18:33.190992"
    },
    {
      "arxiv_id": "2501.19227v1",
      "title": "Integrating Semi-Supervised and Active Learning for Semantic Segmentation",
      "title_zh": "整合半监督学习和主动学习用于语义分割",
      "authors": [
        "Wanli Ma",
        "Oktay Karakus",
        "Paul L. Rosin"
      ],
      "abstract": "In this paper, we propose a novel active learning approach integrated with an\nimproved semi-supervised learning framework to reduce the cost of manual\nannotation and enhance model performance. Our proposed approach effectively\nleverages both the labelled data selected through active learning and the\nunlabelled data excluded from the selection process. The proposed active\nlearning approach pinpoints areas where the pseudo-labels are likely to be\ninaccurate. Then, an automatic and efficient pseudo-label auto-refinement\n(PLAR) module is proposed to correct pixels with potentially erroneous\npseudo-labels by comparing their feature representations with those of labelled\nregions. This approach operates without increasing the labelling budget and is\nbased on the cluster assumption, which states that pixels belonging to the same\nclass should exhibit similar representations in feature space. Furthermore,\nmanual labelling is only applied to the most difficult and uncertain areas in\nunlabelled data, where insufficient information prevents the PLAR module from\nmaking a decision. We evaluated the proposed hybrid semi-supervised active\nlearning framework on two benchmark datasets, one from natural and the other\nfrom remote sensing imagery domains. In both cases, it outperformed\nstate-of-the-art methods in the semantic segmentation task.",
      "tldr_zh": "本论文提出了一种整合主动学习（Active Learning）和半监督学习（Semi-Supervised Learning）的框架，用于减少手动标注成本并提升语义分割（Semantic Segmentation）任务的性能。该框架通过主动学习选择标注数据，并利用伪标签自动精炼（PLAR）模块来修正未标注数据的潜在错误伪标签，基于聚类假设在特征空间中比较像素表示，从而避免额外标注开销。只有最困难和不确定的区域才进行手动标注。实验在自然图像和遥感图像的基准数据集上显示，该方法超过了现有最先进技术。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19227v1",
      "published_date": "2025-01-31 15:37:19 UTC",
      "updated_date": "2025-01-31 15:37:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:18:43.697862"
    },
    {
      "arxiv_id": "2501.19215v2",
      "title": "Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method",
      "title_zh": "Strassen Attention：基于一种新的下界方法解锁 Transformer 中的组合能力",
      "authors": [
        "Alexander Kozachinskiy",
        "Felipe Urrutia",
        "Hector Jimenez",
        "Tomasz Steifer",
        "Germán Pizarro",
        "Matías Fuentes",
        "Francisco Meza",
        "Cristian B. Calderon",
        "Cristóbal Rojas"
      ],
      "abstract": "We propose a novel method to evaluate the theoretical limits of Transformers,\nallowing us to prove the first lower bounds against one-layer softmax\nTransformers with infinite precision. We establish those bounds for three tasks\nthat require advanced reasoning. The first task, Match3 (Sanford et al., 2023),\nrequires looking at all triples of positions. The second and third tasks\naddress compositionality-based reasoning: one is composition of functions (Peng\net al., 2024) and the other is composition of binary relations. We formally\nprove the inability of one-layer softmax Transformers to solve any of these\ntasks. In an attempt to overcome these limitations, we introduce Strassen\nattention and prove that with this mechanism a one-layer Transformer can in\nprinciple solve all these tasks. We also show that it enjoys sub-cubic\nrunning-time complexity, making it more scalable than similar previously\nproposed mechanisms, such as higher-order attention (Sanford et al., 2023). To\ncomplement our theoretical findings, we experimentally studied Strassen\nattention and compared it against standard (Vaswani et al, 2017), higher-order\nattention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021).\nOur results help to disentangle all these attention mechanisms, highlighting\ntheir strengths and limitations. In particular, Strassen attention outperforms\nstandard attention significantly on all the tasks. Altogether, understanding\nthe theoretical limitations can guide research towards scalable attention\nmechanisms that improve the reasoning abilities of Transformers.",
      "tldr_zh": "我们提出了一种新下界方法（new lower bound method），用于评估 Transformers 的理论极限，首次证明单层 softmax Transformers 无法解决三个需要高级推理的任务，包括 Match3 和基于组合性的函数或二元关系推理。  \n为克服这些局限，我们引入了 Strassen attention 机制，并证明它能使单层 Transformer 解决这些任务，同时具备亚立方时间复杂度，比 higher-order attention 等机制更可扩展。  \n实验比较显示，Strassen attention 在这些任务上显著优于标准 attention（Vaswani et al., 2017）和其他机制。  \n总体而言，这有助于指导研究开发更高效的 attention 机制，提升 Transformers 的推理能力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19215v2",
      "published_date": "2025-01-31 15:21:54 UTC",
      "updated_date": "2025-02-06 12:45:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:18:56.840312"
    },
    {
      "arxiv_id": "2501.19206v1",
      "title": "An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents",
      "title_zh": "自治网络防御代理的经验博弈论分析",
      "authors": [
        "Gregory Palmer",
        "Luke Swaby",
        "Daniel J. B. Harrold",
        "Matthew Stewart",
        "Alex Hiles",
        "Chris Willis",
        "Ian Miles",
        "Sara Farmer"
      ],
      "abstract": "The recent rise in increasingly sophisticated cyber-attacks raises the need\nfor robust and resilient autonomous cyber-defence (ACD) agents. Given the\nvariety of cyber-attack tactics, techniques and procedures (TTPs) employed,\nlearning approaches that can return generalisable policies are desirable.\nMeanwhile, the assurance of ACD agents remains an open challenge. We address\nboth challenges via an empirical game-theoretic analysis of deep reinforcement\nlearning (DRL) approaches for ACD using the principled double oracle (DO)\nalgorithm. This algorithm relies on adversaries iteratively learning\n(approximate) best responses against each others' policies; a computationally\nexpensive endeavour for autonomous cyber operations agents. In this work we\nintroduce and evaluate a theoretically-sound, potential-based reward shaping\napproach to expedite this process. In addition, given the increasing number of\nopen-source ACD-DRL approaches, we extend the DO formulation to allow for\nmultiple response oracles (MRO), providing a framework for a holistic\nevaluation of ACD approaches.",
      "tldr_zh": "这篇论文通过经验博弈理论分析自主网络防御 (ACD) 代理，旨在应对日益复杂的网络攻击并生成可泛化的策略，使用深度强化学习 (DRL) 和 principled double oracle (DO) 算法进行迭代最佳响应学习。作者引入了一种理论上健全的 potential-based reward shaping 方法，以加速 DO 算法的计算过程，降低自主网络操作的计算开销。为实现更全面的评估，他们扩展 DO 算法为 multiple response oracles (MRO) 框架，支持对多种开源 ACD-DRL 方法进行整体评估。总的来说，该研究为 ACD 代理的鲁棒性和可靠性提供了新的方法和工具。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.GT"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages, 17 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2501.19206v1",
      "published_date": "2025-01-31 15:15:02 UTC",
      "updated_date": "2025-01-31 15:15:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:19:08.897250"
    },
    {
      "arxiv_id": "2501.19203v1",
      "title": "Single cell resolution 3D imaging and segmentation within intact live tissues",
      "title_zh": "完整活体组织内的单细胞分辨率 3D 成像和分割",
      "authors": [
        "G. Paci",
        "P. Vicente-Munuera",
        "I. Fernandez-Mosquera",
        "A. Miranda",
        "K. Lau",
        "Q. Zhang",
        "R. Barrientos",
        "Y. Mao"
      ],
      "abstract": "Epithelial cells form diverse structures from squamous spherical organoids to\ndensely packed pseudostratified tissues. Quantification of cellular properties\nin these contexts requires high-resolution deep imaging and computational\ntechniques to achieve truthful three-dimensional (3D) structural features.\nHere, we describe a detailed step-by-step protocol for sample preparation,\nimaging and deep-learning-assisted cell segmentation to achieve accurate\nquantification of fluorescently labelled individual cells in 3D within live\ntissues. We share the lessons learned through troubleshooting 3D imaging of\nDrosophila wing discs, including considerations on the choice of microscopy\nmodality and settings (objective, sample mounting) and available segmentation\nmethods. In addition, we include a computational pipeline alongside custom code\nto assist replication of the protocol. While we focus on the segmentation of\ncell outlines from membrane labelling, this protocol applies to a wide variety\nof samples, and we believe it be valuable for studying other tissues that\ndemand complex analysis in 3D.",
      "tldr_zh": "该论文提出了一种详细协议，用于在完整活组织中实现单细胞分辨率的3D成像和分割，旨在精确量化荧光标记的单个细胞的三维结构特征。协议包括样本准备、成像优化（如显微镜模式、物镜和样本安装选择）和基于深度学习的细胞分割方法，通过处理Drosophila wing discs的实际案例来解决常见故障。研究分享了一个计算管道和自定义代码，以便于协议的复制和应用。尽管焦点在于膜标记的细胞轮廓分割，该方法适用于多种样本，并为需要复杂3D分析的其他组织研究提供价值。",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV",
        "q-bio.CB",
        "q-bio.TO"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19203v1",
      "published_date": "2025-01-31 15:13:04 UTC",
      "updated_date": "2025-01-31 15:13:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:19:20.519039"
    },
    {
      "arxiv_id": "2501.19201v1",
      "title": "Efficient Reasoning with Hidden Thinking",
      "title_zh": "翻译失败",
      "authors": [
        "Xuan Shen",
        "Yizhou Wang",
        "Xiangxi Shi",
        "Yanzhi Wang",
        "Pu Zhao",
        "Jiuxiang Gu"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning has become a powerful framework for\nimproving complex problem-solving capabilities in Multimodal Large Language\nModels (MLLMs). However, the verbose nature of textual reasoning introduces\nsignificant inefficiencies. In this work, we propose $\\textbf{Heima}$ (as\nhidden llama), an efficient reasoning framework that leverages reasoning CoTs\nat hidden latent space. We design the Heima Encoder to condense each\nintermediate CoT into a compact, higher-level hidden representation using a\nsingle thinking token, effectively minimizing verbosity and reducing the\noverall number of tokens required during the reasoning process. Meanwhile, we\ndesign corresponding Heima Decoder with traditional Large Language Models\n(LLMs) to adaptively interpret the hidden representations into variable-length\ntextual sequence, reconstructing reasoning processes that closely resemble the\noriginal CoTs. Experimental results across diverse reasoning MLLM benchmarks\ndemonstrate that Heima model achieves higher generation efficiency while\nmaintaining or even better zero-shot task accuracy. Moreover, the effective\nreconstruction of multimodal reasoning processes with Heima Decoder validates\nboth the robustness and interpretability of our approach.",
      "tldr_zh": "本研究针对 Chain-of-Thought (CoT) 推理在 Multimodal Large Language Models (MLLMs) 中的冗长问题，提出了一种高效框架 Heima，该框架在隐藏潜在空间中进行推理。Heima Encoder 使用一个 thinking token 将中间 CoT 浓缩成紧凑的隐藏表示，以减少 token 数量；Heima Decoder 则结合传统的 Large Language Models (LLMs) 适配地解释这些表示，重建类似于原始 CoT 的推理过程。实验结果显示，Heima 在各种 MLLM 基准上实现了更高的生成效率，同时维持或提升零样本任务准确率，并证明了其鲁棒性和可解释性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint version",
      "pdf_url": "http://arxiv.org/pdf/2501.19201v1",
      "published_date": "2025-01-31 15:10:29 UTC",
      "updated_date": "2025-01-31 15:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:19:33.537022"
    },
    {
      "arxiv_id": "2501.19195v1",
      "title": "Rethinking Early Stopping: Refine, Then Calibrate",
      "title_zh": "翻译失败",
      "authors": [
        "Eugène Berta",
        "David Holzmüller",
        "Michael I. Jordan",
        "Francis Bach"
      ],
      "abstract": "Machine learning classifiers often produce probabilistic predictions that are\ncritical for accurate and interpretable decision-making in various domains. The\nquality of these predictions is generally evaluated with proper losses like\ncross-entropy, which decompose into two components: calibration error assesses\ngeneral under/overconfidence, while refinement error measures the ability to\ndistinguish different classes. In this paper, we provide theoretical and\nempirical evidence that these two errors are not minimized simultaneously\nduring training. Selecting the best training epoch based on validation loss\nthus leads to a compromise point that is suboptimal for both calibration error\nand, most importantly, refinement error. To address this, we introduce a new\nmetric for early stopping and hyperparameter tuning that makes it possible to\nminimize refinement error during training. The calibration error is minimized\nafter training, using standard techniques. Our method integrates seamlessly\nwith any architecture and consistently improves performance across diverse\nclassification tasks.",
      "tldr_zh": "该论文重新审视机器学习中的早停策略，指出基于交叉熵损失选择最佳训练周期会导致 calibration error（校准错误）和 refinement error（精炼错误）无法同时最小化，尤其是 refinement error 会处于次优状态。作者引入一个新指标，用于早停和超参数调优，以优先最小化 refinement error，从而提升模型区分不同类别的能力。训练结束后，通过标准技术优化 calibration error，该方法与任何架构无缝整合，并在多种分类任务中 consistently 改善整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19195v1",
      "published_date": "2025-01-31 15:03:54 UTC",
      "updated_date": "2025-01-31 15:03:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:19:44.624897"
    },
    {
      "arxiv_id": "2501.19191v1",
      "title": "Secured Communication Schemes for UAVs in 5G: CRYSTALS-Kyber and IDS",
      "title_zh": "翻译失败",
      "authors": [
        "Taneya Sharma",
        "Seyed Ahmad Soleymani",
        "Mohammad Shojafar",
        "Rahim Tafazolli"
      ],
      "abstract": "This paper introduces a secure communication architecture for Unmanned Aerial\nVehicles (UAVs) and ground stations in 5G networks, addressing critical\nchallenges in network security. The proposed solution integrates the Advanced\nEncryption Standard (AES) with Elliptic Curve Cryptography (ECC) and\nCRYSTALS-Kyber for key encapsulation, offering a hybrid cryptographic approach.\nBy incorporating CRYSTALS-Kyber, the framework mitigates vulnerabilities in ECC\nagainst quantum attacks, positioning it as a quantum-resistant alternative. The\narchitecture is based on a server-client model, with UAVs functioning as\nclients and the ground station acting as the server. The system was rigorously\nevaluated in both VPN and 5G environments. Experimental results confirm that\nCRYSTALS-Kyber delivers strong protection against quantum threats with minimal\nperformance overhead, making it highly suitable for UAVs with resource\nconstraints. Moreover, the proposed architecture integrates an Artificial\nIntelligence (AI)-based Intrusion Detection System (IDS) to further enhance\nsecurity. In performance evaluations, the IDS demonstrated strong results\nacross multiple models with XGBoost, particularly in more demanding scenarios,\noutperforming other models with an accuracy of 97.33% and an AUC of 0.94. These\nfindings underscore the potential of combining quantum-resistant encryption\nmechanisms with AI-driven IDS to create a robust, scalable, and secure\ncommunication framework for UAV networks, particularly within the\nhigh-performance requirements of 5G environments.",
      "tldr_zh": "这篇论文提出了一种针对 5G 网络中 Unmanned Aerial Vehicles (UAVs) 的安全通信架构，结合 Advanced Encryption Standard (AES)、Elliptic Curve Cryptography (ECC) 和 CRYSTALS-Kyber 作为混合加密方法，以抵抗量子攻击并增强整体安全性。架构采用服务器-客户端模型，UAVs 作为客户端，地面站作为服务器，并在 VPN 和 5G 环境中进行评估，实验结果显示 CRYSTALS-Kyber 提供强量子威胁保护，同时保持低性能开销，适合资源受限的 UAVs。论文还整合了基于 Artificial Intelligence (AI) 的 Intrusion Detection System (IDS)，其中 XGBoost 模型表现出色，准确率达 97.33% 和 AUC 0.94。这些创新为 UAV 网络构建了一个稳健、可扩展的安全框架，满足 5G 的高性能需求。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "6 pages, 5 figures, Paper accepted at IEEE FNWF'25 conference\n  (References number: 1571070613)",
      "pdf_url": "http://arxiv.org/pdf/2501.19191v1",
      "published_date": "2025-01-31 15:00:27 UTC",
      "updated_date": "2025-01-31 15:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:19:57.481426"
    },
    {
      "arxiv_id": "2501.19180v1",
      "title": "Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning",
      "title_zh": "通过主动安全推理增强模型对越狱攻击的防御",
      "authors": [
        "Xianglin Yang",
        "Gelei Deng",
        "Jieming Shi",
        "Tianwei Zhang",
        "Jin Song Dong"
      ],
      "abstract": "Large language models (LLMs) are vital for a wide range of applications yet\nremain susceptible to jailbreak threats, which could lead to the generation of\ninappropriate responses. Conventional defenses, such as refusal and adversarial\ntraining, often fail to cover corner cases or rare domains, leaving LLMs still\nvulnerable to more sophisticated attacks. We propose a novel defense strategy,\nSafety Chain-of-Thought (SCoT), which harnesses the enhanced \\textit{reasoning\ncapabilities} of LLMs for proactive assessment of harmful inputs, rather than\nsimply blocking them. SCoT augments any refusal training datasets to critically\nanalyze the intent behind each request before generating answers. By employing\nproactive reasoning, SCoT enhances the generalization of LLMs across varied\nharmful queries and scenarios not covered in the safety alignment corpus.\nAdditionally, it generates detailed refusals specifying the rules violated.\nComparative evaluations show that SCoT significantly surpasses existing\ndefenses, reducing vulnerability to out-of-distribution issues and adversarial\nmanipulations while maintaining strong general capabilities.",
      "tldr_zh": "这篇论文针对大型语言模型(LLMs)对jailbreak威胁的脆弱性，提出了一种新防御策略Safety Chain-of-Thought (SCoT)，利用LLMs的推理能力主动评估有害输入的意图。SCoT通过增强refusal训练数据集，分析请求背后的潜在风险，并在生成答案前提供详细的拒绝理由，从而提升模型在未覆盖场景中的泛化能力。实验比较显示，SCoT显著优于现有防御方法，减少了对out-of-distribution问题和adversarial manipulations的易感性，同时保持了模型的整体性能。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19180v1",
      "published_date": "2025-01-31 14:45:23 UTC",
      "updated_date": "2025-01-31 14:45:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:20:08.372893"
    },
    {
      "arxiv_id": "2501.19176v1",
      "title": "Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Aurora Rofena",
        "Claudia Lucia Piccolo",
        "Bruno Beomonte Zobel",
        "Paolo Soda",
        "Valerio Guarrasi"
      ],
      "abstract": "Full-Field Digital Mammography (FFDM) is the primary imaging modality for\nroutine breast cancer screening; however, its effectiveness is limited in\npatients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers enhanced\naccuracy in tumor detection. Nonetheless, its application is restricted due to\nhigher radiation exposure, the use of contrast agents, and limited\naccessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral\noblique views to classify lesions as malignant or benign. To address the\nchallenge of missing CESM data, we leverage generative artificial intelligence\nto impute CESM images from FFDM scans. Experimental results demonstrate that\nincorporating the CESM modality is crucial to enhance the performance of\nvirtual biopsy. When real CESM data is missing, synthetic CESM images proved\neffective, outperforming the use of FFDM alone, particularly in multimodal\nconfigurations that combine FFDM and CESM modalities. The proposed approach has\nthe potential to improve diagnostic workflows, providing clinicians with\naugmented intelligence tools to improve diagnostic accuracy and patient care.\nAdditionally, as a contribution to the research community, we publicly release\nthe dataset used in our experiments, facilitating further advancements in this\nfield.",
      "tldr_zh": "本研究针对乳腺癌筛查中 Full-Field Digital Mammography (FFDM) 的局限性，提出了一种多模态、多视图深度学习方法，用于虚拟活检，通过整合 FFDM 和 Contrast-Enhanced Spectral Mammography (CESM) 的颅尾及内侧外侧斜位视图，来分类病变为恶性或良性。针对 CESM 数据缺失的问题，该方法利用 Generative Artificial Intelligence 生成合成 CESM 图像，以增强诊断性能。实验结果显示，加入真实或合成 CESM 模态显著提升虚拟活检准确性，尤其在多模态配置中优于仅使用 FFDM；此外，研究公开了数据集，以推动该领域的进一步发展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19176v1",
      "published_date": "2025-01-31 14:41:17 UTC",
      "updated_date": "2025-01-31 14:41:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:20:21.494027"
    },
    {
      "arxiv_id": "2501.19155v1",
      "title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation",
      "title_zh": "SWAT：滑动窗口对抗训练用于渐进域适应",
      "authors": [
        "Zixi Wang",
        "Yubo Huang",
        "Wenwei Luo",
        "Tonglan Xie",
        "Mengmeng Jing",
        "Lin Zuo"
      ],
      "abstract": "Domain shifts are critical issues that harm the performance of machine\nlearning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers\nwhen the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA)\nalleviates this problem in a mild way by gradually adapting from the source to\nthe target domain using multiple intermediate domains. In this paper, we\npropose Sliding Window Adversarial Training (SWAT) for Gradual Domain\nAdaptation. SWAT uses the construction of adversarial streams to connect the\nfeature spaces of the source and target domains. In order to gradually narrow\nthe small gap between adjacent intermediate domains, a sliding window paradigm\nis designed that moves along the adversarial stream. When the window moves to\nthe end of the stream, i.e., the target domain, the domain shift is drastically\nreduced. Extensive experiments are conducted on public GDA benchmarks, and the\nresults demonstrate that the proposed SWAT significantly outperforms the\nstate-of-the-art approaches. The implementation is available at:\nhttps://anonymous.4open.science/r/SWAT-8677.",
      "tldr_zh": "本文针对机器学习中的领域偏移问题，提出SWAT（Sliding Window Adversarial Training）方法，用于Gradual Domain Adaptation (GDA)，通过构建对抗流连接源域和目标域特征空间，并采用滑动窗口范式逐步缩小相邻中间域的差距。SWAT能够在窗口移动至目标域时显著减少领域偏移，从而实现更平滑的适应过程。在公共GDA基准上的广泛实验表明，SWAT比最先进方法性能提升明显，代码实现可公开获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "submitted to icml 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.19155v1",
      "published_date": "2025-01-31 14:16:22 UTC",
      "updated_date": "2025-01-31 14:16:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:20:31.645823"
    },
    {
      "arxiv_id": "2501.19149v1",
      "title": "On the inductive bias of infinite-depth ResNets and the bottleneck rank",
      "title_zh": "翻译失败",
      "authors": [
        "Enric Boix-Adsera"
      ],
      "abstract": "We compute the minimum-norm weights of a deep linear ResNet, and find that\nthe inductive bias of this architecture lies between minimizing nuclear norm\nand rank. This implies that, with appropriate hyperparameters, deep nonlinear\nResNets have an inductive bias towards minimizing bottleneck rank.",
      "tldr_zh": "本研究计算了深度线性 ResNet 的最小范数权重，发现其归纳偏差（inductive bias）介于最小核范数（nuclear norm）和最小秩（rank）之间。  \n这一发现表明，通过适当的超参数设置，深度非线性 ResNets 可能具有向最小化瓶颈秩（bottleneck rank）的归纳偏差。  \n这项工作为理解 ResNet 架构的内在偏好提供了新洞见，有助于优化深度学习模型的设计。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.19149v1",
      "published_date": "2025-01-31 14:06:13 UTC",
      "updated_date": "2025-01-31 14:06:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:20:43.853760"
    },
    {
      "arxiv_id": "2501.19145v1",
      "title": "Improving Multi-Label Contrastive Learning by Leveraging Label Distribution",
      "title_zh": "通过利用标签分布改进多标签对比学习",
      "authors": [
        "Ning Chen",
        "Shen-Huan Lyu",
        "Tian-Shuang Wu",
        "Yanyan Wang",
        "Bin Tang"
      ],
      "abstract": "In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.",
      "tldr_zh": "本论文针对多标签对比学习（Multi-Label Contrastive Learning）中的正负样本选择和标签信息利用挑战，提出了一种通过利用标签分布（Leveraging Label Distribution）的新方法。该方法简化正负样本选择，仅基于标签是否有交集，并引入两种技术（基于 Radial Basis Function (RBF) 和对比损失）来从逻辑标签恢复标签分布，从而更好地建模标签间的关系。在九个常用多标签数据集（包括图像和向量类型）上的实验结果显示，该方法在六种评估指标上优于现有最先进方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19145v1",
      "published_date": "2025-01-31 14:00:02 UTC",
      "updated_date": "2025-01-31 14:00:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:20:56.635787"
    },
    {
      "arxiv_id": "2501.19143v1",
      "title": "Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play",
      "title_zh": "翻译失败",
      "authors": [
        "Ching-Chun Chang",
        "Fan-Yun Chen",
        "Shih-Hong Gu",
        "Kai Gao",
        "Hanrui Wang",
        "Isao Echizen"
      ],
      "abstract": "As the cornerstone of artificial intelligence, machine perception confronts a\nfundamental threat posed by adversarial illusions. These adversarial attacks\nmanifest in two primary forms: deductive illusion, where specific stimuli are\ncrafted based on the victim model's general decision logic, and inductive\nillusion, where the victim model's general decision logic is shaped by specific\nstimuli. The former exploits the model's decision boundaries to create a\nstimulus that, when applied, interferes with its decision-making process. The\nlatter reinforces a conditioned reflex in the model, embedding a backdoor\nduring its learning phase that, when triggered by a stimulus, causes aberrant\nbehaviours. The multifaceted nature of adversarial illusions calls for a\nunified defence framework, addressing vulnerabilities across various forms of\nattack. In this study, we propose a disillusion paradigm based on the concept\nof an imitation game. At the heart of the imitation game lies a multimodal\ngenerative agent, steered by chain-of-thought reasoning, which observes,\ninternalises and reconstructs the semantic essence of a sample, liberated from\nthe classic pursuit of reversing the sample to its original state. As a proof\nof concept, we conduct experimental simulations using a multimodal generative\ndialogue agent and evaluates the methodology under a variety of attack\nscenarios.",
      "tldr_zh": "本文探讨了机器感知面临的adversarial illusions威胁，包括基于模型决策逻辑的deductive illusion和通过特定刺激嵌入后门的inductive illusion，并提出一种统一的防御框架。核心方法是基于imitation game的范式，利用multimodal generative agent和chain-of-thought reasoning进行角色扮演，以观察、内部化和重建样本的语义本质，而非简单还原。实验通过multimodal generative dialogue agent在多种攻击场景下进行模拟评估，证明了该方法的有效性，为增强模型鲁棒性提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19143v1",
      "published_date": "2025-01-31 13:57:34 UTC",
      "updated_date": "2025-01-31 13:57:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:21:08.939186"
    },
    {
      "arxiv_id": "2501.19137v1",
      "title": "A Metric for the Balance of Information in Graph Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Alex O. Davies",
        "Nirav S. Ajmeri",
        "Telmo de Menezes e Silva Filho"
      ],
      "abstract": "Graph learning on molecules makes use of information from both the molecular\nstructure and the features attached to that structure. Much work has been\nconducted on biasing either towards structure or features, with the aim that\nbias bolsters performance. Identifying which information source a dataset\nfavours, and therefore how to approach learning that dataset, is an open issue.\nHere we propose Noise-Noise Ratio Difference (NNRD), a quantitative metric for\nwhether there is more useful information in structure or features. By employing\niterative noising on features and structure independently, leaving the other\nintact, NNRD measures the degradation of information in each. We employ NNRD\nover a range of molecular tasks, and show that it corresponds well to a loss of\ninformation, with intuitive results that are more expressive than simple\nperformance aggregates. Our future work will focus on expanding data domains,\ntasks and types, as well as refining our choice of baseline model.",
      "tldr_zh": "这篇论文针对分子图学习（Graph Learning）中分子结构和特征信息平衡的问题，提出了一种量化指标Noise-Noise Ratio Difference (NNRD)。NNRD通过独立对特征和结构添加噪声，测量各自信息退化程度，从而评估哪个信息源更具价值。实验结果显示，NNRD在多种分子任务上能准确反映信息损失，并提供比简单性能聚合更直观的见解；未来工作将扩展数据域、任务类型和基准模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "In proceedings of the 4th Annual AAAI Workshop on AI to Accelerate\n  Science and Engineering (AI2ASE)",
      "pdf_url": "http://arxiv.org/pdf/2501.19137v1",
      "published_date": "2025-01-31 13:46:42 UTC",
      "updated_date": "2025-01-31 13:46:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:21:19.814250"
    },
    {
      "arxiv_id": "2501.19133v1",
      "title": "Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Burcu Küçükoğlu",
        "Sander Dalm",
        "Marcel van Gerven"
      ],
      "abstract": "The effectiveness of credit assignment in reinforcement learning (RL) when\ndealing with high-dimensional data is influenced by the success of\nrepresentation learning via deep neural networks, and has implications for the\nsample efficiency of deep RL algorithms. Input decorrelation has been\npreviously introduced as a method to speed up optimization in neural networks,\nand has proven impactful in both efficient deep learning and as a method for\neffective representation learning for deep RL algorithms. We propose a novel\napproach to online decorrelation in deep RL based on the decorrelated\nbackpropagation algorithm that seamlessly integrates the decorrelation process\ninto the RL training pipeline. Decorrelation matrices are added to each layer,\nwhich are updated using a separate decorrelation learning rule that minimizes\nthe total decorrelation loss across all layers, in parallel to minimizing the\nusual RL loss. We used our approach in combination with the soft actor-critic\n(SAC) method, which we refer to as decorrelated soft actor-critic (DSAC).\nExperiments on the Atari 100k benchmark with DSAC shows, compared to the\nregular SAC baseline, faster training in five out of the seven games tested and\nimproved reward performance in two games with around 50% reduction in\nwall-clock time, while maintaining performance levels on the other games. These\nresults demonstrate the positive impact of network-wide decorrelation in deep\nRL for speeding up its sample efficiency through more effective credit\nassignment.",
      "tldr_zh": "该研究提出了一种名为Decorrelated Soft Actor-Critic (DSAC)的深度强化学习算法，通过在线去相关机制提升信用分配的效率和样本效率。DSAC基于去相关反向传播算法，在每个神经网络层添加去相关矩阵，并使用独立的学习规则最小化去相关损失，同时优化标准的强化学习损失。实验在Atari 100k基准上显示，与Soft Actor-Critic (SAC)基线相比，DSAC在七个游戏中使五个游戏的训练速度加快，并在两个游戏中提升奖励性能，同时减少约50%的墙钟时间，证明了网络范围去相关对深度RL的积极影响。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19133v1",
      "published_date": "2025-01-31 13:38:57 UTC",
      "updated_date": "2025-01-31 13:38:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:21:32.211063"
    },
    {
      "arxiv_id": "2501.19128v1",
      "title": "Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Wenyun Li",
        "Wenjie Huang"
      ],
      "abstract": "In many real-world scenarios, reward signal for agents are exceedingly\nsparse, making it challenging to learn an effective reward function for reward\nshaping. To address this issue, our approach performs reward shaping not only\nby utilizing non-zero-reward transitions but also by employing the\nSemi-Supervised Learning (SSL) technique combined with a novel data\naugmentation to learn trajectory space representations from the majority of\ntransitions, zero-reward transitions, thereby improving the efficacy of reward\nshaping. Experimental results in Atari and robotic manipulation demonstrate\nthat our method effectively generalizes reward shaping to sparse reward\nscenarios, achieving up to four times better performance in reaching higher\nbest scores compared to curiosity-driven methods. The proposed double entropy\ndata augmentation enhances performance, showcasing a 15.8\\% increase in best\nscore over other augmentation methods.",
      "tldr_zh": "该研究针对强化学习(Reinforcement Learning)中稀疏奖励(Sparse Rewards)问题，提出了一种半监督学习(Semi-Supervised Learning)方法，利用非零奖励转移和新型数据增强技术，从大量零奖励转移中学习轨迹空间表示，从而提升奖励塑造的有效性。实验结果显示，该方法在Atari游戏和机器人操作环境中表现出色，与好奇心驱动方法相比，最优分数最高提升四倍。特别地，提出的双熵数据增强(double entropy data augmentation)进一步提高了性能，比其他增强方法提升15.8%的最佳分数。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19128v1",
      "published_date": "2025-01-31 13:35:19 UTC",
      "updated_date": "2025-01-31 13:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:21:43.807397"
    },
    {
      "arxiv_id": "2501.19122v2",
      "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
      "title_zh": "翻译失败",
      "authors": [
        "Hong Huang",
        "Hai Yang",
        "Yuan Chen",
        "Jiaxun Ye",
        "Dapeng Wu"
      ],
      "abstract": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without data sharing, but its high computational and\ncommunication demands strain resource-constrained devices. While existing\nmethods use dynamic pruning to improve efficiency by periodically adjusting\nsparse model topologies while maintaining sparsity, these approaches suffer\nfrom issues such as greedy adjustments, unstable topologies, and communication\ninefficiency, resulting in less robust models and suboptimal performance under\ndata heterogeneity and partial client availability. To address these\nchallenges, we propose Federated Robust pruning via combinatorial Thompson\nSampling (FedRTS), a novel framework designed to develop robust sparse models.\nFedRTS enhances robustness and performance through its Thompson Sampling-based\nAdjustment (TSAdj) mechanism, which uses probabilistic decisions informed by\nstable, farsighted information instead of deterministic decisions reliant on\nunstable and myopic information in previous methods. Extensive experiments\ndemonstrate that FedRTS achieves state-of-the-art performance in computer\nvision and natural language processing tasks while reducing communication\ncosts, particularly excelling in scenarios with heterogeneous data\ndistributions and partial client participation. Our codes are available at:\nhttps://github.com/Little0o0/FedRTS",
      "tldr_zh": "该论文提出 FedRTS，一种基于 Combinatorial Thompson Sampling 的联邦学习框架，旨在解决现有动态 pruning 方法在资源受限设备上的计算和通信效率问题，以及在数据异质性和部分客户端可用性下模型鲁棒性不足的问题。FedRTS 通过 Thompson Sampling-based Adjustment (TSAdj) 机制，使用概率决策和稳定远见信息来优化稀疏模型拓扑，而不是依赖于贪婪或不稳定的调整，从而提升整体性能。实验结果表明，FedRTS 在计算机视觉和自然语言处理任务中实现最先进性能，同时显著减少通信成本，尤其在异质数据分布场景中表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19122v2",
      "published_date": "2025-01-31 13:26:22 UTC",
      "updated_date": "2025-05-17 16:45:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:21:57.186623"
    },
    {
      "arxiv_id": "2501.19114v1",
      "title": "Principal Components for Neural Network Initialization",
      "title_zh": "翻译失败",
      "authors": [
        "Nhan Phan",
        "Thu Nguyen",
        "Pål Halvorsen",
        "Michael A. Riegler"
      ],
      "abstract": "Principal Component Analysis (PCA) is a commonly used tool for dimension\nreduction and denoising. Therefore, it is also widely used on the data prior to\ntraining a neural network. However, this approach can complicate the\nexplanation of explainable AI (XAI) methods for the decision of the model. In\nthis work, we analyze the potential issues with this approach and propose\nPrincipal Components-based Initialization (PCsInit), a strategy to incorporate\nPCA into the first layer of a neural network via initialization of the first\nlayer in the network with the principal components, and its two variants\nPCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct\nand straightforward as for neural networks and are simpler than using PCA prior\nto training a neural network on the principal components. Moreover, as will be\nillustrated in the experiments, such training strategies can also allow further\nimprovement of training via backpropagation.",
      "tldr_zh": "这篇论文分析了在神经网络训练前使用 Principal Component Analysis (PCA) 可能导致的可解释 AI (XAI) 解释复杂化的潜在问题。作者提出 Principal Components-based Initialization (PCsInit) 策略，通过用主成分初始化神经网络的第一层及其变体 PCsInit-Act 和 PCsInit-Sub，将 PCA 直接整合到模型中。这种方法使解释更直接简单，并实验证明了它能通过反向传播进一步提升训练性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19114v1",
      "published_date": "2025-01-31 13:18:10 UTC",
      "updated_date": "2025-01-31 13:18:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:22:07.787734"
    },
    {
      "arxiv_id": "2501.19112v2",
      "title": "Logical Modalities within the European AI Act: An Analysis",
      "title_zh": "欧洲 AI Act 中的逻辑模态：分析",
      "authors": [
        "Lara Lawniczak",
        "Christoph Benzmüller"
      ],
      "abstract": "The paper presents a comprehensive analysis of the European AI Act in terms\nof its logical modalities, with the aim of preparing its formal representation,\nfor example, within the logic-pluralistic Knowledge Engineering Framework and\nMethodology (LogiKEy). LogiKEy develops computational tools for normative\nreasoning based on formal methods, employing Higher-Order Logic (HOL) as a\nunifying meta-logic to integrate diverse logics through shallow semantic\nembeddings. This integration is facilitated by Isabelle/HOL, a proof assistant\ntool equipped with several automated theorem provers. The modalities within the\nAI Act and the logics suitable for their representation are discussed. For a\nselection of these logics, embeddings in HOL are created, which are then used\nto encode sample paragraphs. Initial experiments evaluate the suitability of\nthese embeddings for automated reasoning, and highlight key challenges on the\nway to more robust reasoning capabilities.",
      "tldr_zh": "该论文分析了欧洲 AI Act 中的逻辑模态（logical modalities），旨在为其准备正式表示，例如在 LogiKEy 框架中。LogiKEy 利用 Higher-Order Logic (HOL) 作为统一元逻辑，通过浅层语义嵌入和 Isabelle/HOL 工具整合多种逻辑，以支持规范推理。研究讨论了适合这些模态的逻辑，创建了 HOL 嵌入来编码样本段落，并通过初步实验评估了自动化推理的适用性，同时指出了提升推理能力的关键挑战。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.LO",
        "68T01 68T27 68T30 03Axx 03B16 03B35 03B45 03B60 03B70",
        "I.2.0; I.2.3; I.2.4; J.1"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended preprint of paper accepted for ICAIL 2025; 15 pages, 19\n  figures",
      "pdf_url": "http://arxiv.org/pdf/2501.19112v2",
      "published_date": "2025-01-31 13:15:33 UTC",
      "updated_date": "2025-05-12 13:07:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:22:20.149084"
    },
    {
      "arxiv_id": "2501.19111v2",
      "title": "A Benchmark for Incremental Micro-expression Recognition",
      "title_zh": "增量微表情识别的基准",
      "authors": [
        "Zhengqin Lai",
        "Xiaopeng Hong",
        "Yabin Wang",
        "Xiaobai Li"
      ],
      "abstract": "Micro-expression recognition plays a pivotal role in understanding hidden\nemotions and has applications across various fields. Traditional recognition\nmethods assume access to all training data at once, but real-world scenarios\ninvolve continuously evolving data streams. To respond to the requirement of\nadapting to new data while retaining previously learned knowledge, we introduce\nthe first benchmark specifically designed for incremental micro-expression\nrecognition. Our contributions include: Firstly, we formulate the incremental\nlearning setting tailored for micro-expression recognition. Secondly, we\norganize sequential datasets with carefully curated learning orders to reflect\nreal-world scenarios. Thirdly, we define two cross-evaluation-based testing\nprotocols, each targeting distinct evaluation objectives. Finally, we provide\nsix baseline methods and their corresponding evaluation results. This benchmark\nlays the groundwork for advancing incremental micro-expression recognition\nresearch. All source code used in this study will be publicly available at\nhttps://github.com/ZhengQinLai/IMER-benchmark.",
      "tldr_zh": "本研究引入了一个针对增量 micro-expression recognition 的基准，以解决传统方法无法适应不断演变的数据流的局限性。贡献包括：制定专属的 incremental learning 设置、组织反映真实场景的顺序数据集、定义两种基于跨评估的测试协议，以及提供六种基线方法及其评估结果。该基准为推进 micro-expression recognition 领域的持续学习研究奠定了基础，并公开了源代码以供进一步使用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19111v2",
      "published_date": "2025-01-31 13:14:16 UTC",
      "updated_date": "2025-02-03 08:14:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:22:32.049984"
    },
    {
      "arxiv_id": "2501.19095v1",
      "title": "PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient Knowledge Graph Embeddings",
      "title_zh": "PathE：利用实体无关路径实现参数高效的知识图谱嵌入",
      "authors": [
        "Ioannis Reklos",
        "Jacopo de Berardinis",
        "Elena Simperl",
        "Albert Meroño-Peñuela"
      ],
      "abstract": "Knowledge Graphs (KGs) store human knowledge in the form of entities (nodes)\nand relations, and are used extensively in various applications. KG embeddings\nare an effective approach to addressing tasks like knowledge discovery, link\nprediction, and reasoning. This is often done by allocating and learning\nembedding tables for all or a subset of the entities. As this scales linearly\nwith the number of entities, learning embedding models in real-world KGs with\nmillions of nodes can be computationally intractable. To address this\nscalability problem, our model, PathE, only allocates embedding tables for\nrelations (which are typically orders of magnitude fewer than the entities) and\nrequires less than 25% of the parameters of previous parameter efficient\nmethods. Rather than storing entity embeddings, we learn to compute them by\nleveraging multiple entity-relation paths to contextualise individual entities\nwithin triples. Evaluated on four benchmarks, PathE achieves state-of-the-art\nperformance in relation prediction, and remains competitive in link prediction\non path-rich KGs while training on consumer-grade hardware. We perform ablation\nexperiments to test our design choices and analyse the sensitivity of the model\nto key hyper-parameters. PathE is efficient and cost-effective for relationally\ndiverse and well-connected KGs commonly found in real-world applications.",
      "tldr_zh": "该研究提出PathE模型，通过利用实体无关路径（entity-agnostic paths）来计算实体嵌入，从而实现参数高效的Knowledge Graph Embeddings（KG嵌入）。不同于传统方法，PathE仅为关系分配嵌入表，避免了为大量实体存储嵌入，从而将参数量减少至先前高效方法的不到25%。在四个基准测试中，PathE在关系预测任务上达到最先进性能，并在链接预测上保持竞争力，尤其适用于路径丰富的KGs，且能在消费级硬件上高效训练。该模型通过消融实验和超参数敏感性分析，证明其设计选择适用于真实世界的关系多样化KG。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19095v1",
      "published_date": "2025-01-31 12:41:02 UTC",
      "updated_date": "2025-01-31 12:41:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:22:44.011770"
    },
    {
      "arxiv_id": "2503.15515v1",
      "title": "Towards Computer-Using Personal Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Piero A. Bonatti",
        "John Domingue",
        "Anna Lisa Gentile",
        "Andreas Harth",
        "Olaf Hartig",
        "Aidan Hogan",
        "Katja Hose",
        "Ernesto Jimenez-Ruiz",
        "Deborah L. McGuinness",
        "Chang Sun",
        "Ruben Verborgh",
        "Jesse Wright"
      ],
      "abstract": "Computer-Using Agents (CUA) enable users to automate increasingly-complex\ntasks using graphical interfaces such as browsers. As many potential tasks\nrequire personal data, we propose Computer-Using Personal Agents (CUPAs) that\nhave access to an external repository of the user's personal data. Compared\nwith CUAs, CUPAs offer users better control of their personal data, the\npotential to automate more tasks involving personal data, better\ninteroperability with external sources of data, and better capabilities to\ncoordinate with other CUPAs in order to solve collaborative tasks involving the\npersonal data of multiple users.",
      "tldr_zh": "该论文探讨了 Computer-Using Agents (CUA)，这些代理可帮助用户自动化使用图形界面（如浏览器）的复杂任务。作者提出 Computer-Using Personal Agents (CUPA)，该系统允许代理访问用户的外部个人数据仓库，从而提升数据控制和任务自动化能力。与 CUA 相比，CUPA 提供更好的外部数据互操作性，并能与其他 CUPA 协调，处理涉及多用户个人数据的协作任务。这为更安全、智能的个人数据管理奠定了基础。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA",
        "I.2.7; I.2.4; I.2.11; H.3.5"
      ],
      "primary_category": "cs.HC",
      "comment": "This report is a result of Dagstuhl Seminar 25051 \"Trust and\n  Accountability in Knowledge Graph-Based AI for Self Determination\", which\n  took place in January 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.15515v1",
      "published_date": "2025-01-31 12:26:27 UTC",
      "updated_date": "2025-01-31 12:26:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:22:55.820832"
    },
    {
      "arxiv_id": "2501.19086v1",
      "title": "Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification",
      "title_zh": "基于 CLIP 的基础模型在 X 射线图像分类中的公平性分析",
      "authors": [
        "Xiangyu Sun",
        "Xiaoguang Zou",
        "Yuanquan Wu",
        "Guotai Wang",
        "Shaoting Zhang"
      ],
      "abstract": "X-ray imaging is pivotal in medical diagnostics, offering non-invasive\ninsights into a range of health conditions. Recently, vision-language models,\nsuch as the Contrastive Language-Image Pretraining (CLIP) model, have\ndemonstrated potential in improving diagnostic accuracy by leveraging\nlarge-scale image-text datasets. However, since CLIP was not initially designed\nfor medical images, several CLIP-like models trained specifically on medical\nimages have been developed. Despite their enhanced performance, issues of\nfairness - particularly regarding demographic attributes - remain largely\nunaddressed. In this study, we perform a comprehensive fairness analysis of\nCLIP-like models applied to X-ray image classification. We assess their\nperformance and fairness across diverse patient demographics and disease\ncategories using zero-shot inference and various fine-tuning techniques,\nincluding Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation\n(LoRA), and full fine-tuning. Our results indicate that while fine-tuning\nimproves model accuracy, fairness concerns persist, highlighting the need for\nfurther fairness interventions in these foundational models.",
      "tldr_zh": "本文分析了基于 CLIP 的基础模型在 X-ray 图像分类中的公平性问题，评估了这些模型在不同患者人口统计属性和疾病类别下的性能表现。研究采用零样本推理以及多种微调技术，包括 Linear Probing、MLP、LoRA 和全微调，结果显示微调能提升模型准确性，但公平性问题（如人口统计偏差）依然存在。最终强调需要进一步的公平性干预措施，以改进这些医疗图像模型。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This paper has been accepted for presentation at the 2025 IEEE\n  International Symposium on Biomedical Imaging (ISBI 2025)",
      "pdf_url": "http://arxiv.org/pdf/2501.19086v1",
      "published_date": "2025-01-31 12:23:50 UTC",
      "updated_date": "2025-01-31 12:23:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:23:08.395008"
    },
    {
      "arxiv_id": "2501.19069v2",
      "title": "Improving vision-language alignment with graph spiking hybrid Networks",
      "title_zh": "通过图脉冲混合网络改进视觉-语言对齐",
      "authors": [
        "Siyu Zhang",
        "Wenzhe Liu",
        "Yeming Chen",
        "Yiming Wu",
        "Heming Zheng",
        "Cheng Cheng"
      ],
      "abstract": "To bridge the semantic gap between vision and language (VL), it is necessary\nto develop a good alignment strategy, which includes handling semantic\ndiversity, abstract representation of visual information, and generalization\nability of models. Recent works use detector-based bounding boxes or patches\nwith regular partitions to represent visual semantics. While current paradigms\nhave made strides, they are still insufficient for fully capturing the nuanced\ncontextual relations among various objects. This paper proposes a comprehensive\nvisual semantic representation module, necessitating the utilization of\npanoptic segmentation to generate coherent fine-grained semantic features.\nFurthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that\nintegrates the complementary advantages of Spiking Neural Networks (SNNs) and\nGraph Attention Networks (GATs) to encode visual semantic information.\nIntriguingly, the model not only encodes the discrete and continuous latent\nvariables of instances but also adeptly captures both local and global\ncontextual features, thereby significantly enhancing the richness and diversity\nof semantic representations. Leveraging the spatiotemporal properties inherent\nin SNNs, we employ contrastive learning (CL) to enhance the similarity-based\nrepresentation of embeddings. This strategy alleviates the computational\noverhead of the model and enriches meaningful visual representations by\nconstructing positive and negative sample pairs. We design an innovative\npre-training method, Spiked Text Learning (STL), which uses text features to\nimprove the encoding ability of discrete semantics. Experiments show that the\nproposed GSHN exhibits promising results on multiple VL downstream tasks.",
      "tldr_zh": "本论文旨在桥接视觉和语言（VL）之间的语义差距，通过改进对齐策略来处理语义多样性、视觉信息的抽象表示以及模型的泛化能力。作者提出一个全面的视觉语义表示模块，使用 panoptic segmentation 生成细粒度的语义特征，并引入 Graph Spiking Hybrid Network (GSHN)，该网络融合 Spiking Neural Networks (SNNs) 和 Graph Attention Networks (GATs) 的优势，以编码实例的离散和连续潜在变量并捕捉局部与全局上下文。GSHN 结合对比学习 (CL) 来增强嵌入的相似性表示，利用 SNN 的时空属性减少计算开销，并通过 Spiked Text Learning (STL) 预训练方法利用文本特征提升离散语义编码能力。实验结果显示，GSHN 在多个视觉语言下游任务上表现出色，显著提高了模型性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19069v2",
      "published_date": "2025-01-31 11:55:17 UTC",
      "updated_date": "2025-03-02 07:22:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:23:21.532717"
    },
    {
      "arxiv_id": "2501.19065v1",
      "title": "BEAT: Balanced Frequency Adaptive Tuning for Long-Term Time-Series Forecasting",
      "title_zh": "BEAT：平衡频率自适应调优用于长期时间",
      "authors": [
        "Zhixuan Li",
        "Naipeng Chen",
        "Seonghwa Choi",
        "Sanghoon Lee",
        "Weisi Lin"
      ],
      "abstract": "Time-series forecasting is crucial for numerous real-world applications\nincluding weather prediction and financial market modeling. While\ntemporal-domain methods remain prevalent, frequency-domain approaches can\neffectively capture multi-scale periodic patterns, reduce sequence\ndependencies, and naturally denoise signals. However, existing approaches\ntypically train model components for all frequencies under a unified training\nobjective, often leading to mismatched learning speeds: high-frequency\ncomponents converge faster and risk overfitting, while low-frequency components\nunderfit due to insufficient training time. To deal with this challenge, we\npropose BEAT (Balanced frEquency Adaptive Tuning), a novel framework that\ndynamically monitors the training status for each frequency and adaptively\nadjusts their gradient updates. By recognizing convergence, overfitting, or\nunderfitting for each frequency, BEAT dynamically reallocates learning\npriorities, moderating gradients for rapid learners and increasing those for\nslower ones, alleviating the tension between competing objectives across\nfrequencies and synchronizing the overall learning process. Extensive\nexperiments on seven real-world datasets demonstrate that BEAT consistently\noutperforms state-of-the-art approaches.",
      "tldr_zh": "该论文提出 BEAT（Balanced Frequency Adaptive Tuning）框架，用于解决长期时间序列预测中频率域方法的问题，即高频组件过拟合而低频组件欠拟合。BEAT 通过动态监控每个频率的训练状态（如收敛、过拟合或欠拟合），并适配调整梯度更新，以平衡学习优先级并同步整体训练过程。实验在七个真实数据集上表明，BEAT  consistently outperforms state-of-the-art approaches，提升了预测准确性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.19065v1",
      "published_date": "2025-01-31 11:52:35 UTC",
      "updated_date": "2025-01-31 11:52:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:23:31.526724"
    },
    {
      "arxiv_id": "2501.19056v1",
      "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Fenglin Yu",
        "Fangkai Yang",
        "Xiaoting Qin",
        "Zhiyang Zhang",
        "Jue Zhang",
        "Qingwei Lin",
        "Hongyu Zhang",
        "Yingnong Dang",
        "Saravan Rajmohan",
        "Dongmei Zhang",
        "Qi Zhang"
      ],
      "abstract": "The increasing complexity of modern software systems necessitates robust\nautonomic self-management capabilities. While Large Language Models (LLMs)\ndemonstrate potential in this domain, they often face challenges in adapting\ntheir general knowledge to specific service contexts. To address this\nlimitation, we propose ServiceOdyssey, a self-learning agent system that\nautonomously manages microservices without requiring prior knowledge of\nservice-specific configurations. By leveraging curriculum learning principles\nand iterative exploration, ServiceOdyssey progressively develops a deep\nunderstanding of operational environments, reducing dependence on human input\nor static documentation. A prototype built with the Sock Shop microservice\ndemonstrates the potential of this approach for autonomic microservice\nmanagement.",
      "tldr_zh": "本文研究了现代软件系统的复杂性，强调了 Large Language Models (LLMs) 在适应特定服务上下文时的局限性。作者提出 ServiceOdyssey，一种自学习代理系统，通过 curriculum learning 原则和迭代探索，实现微服务的自主管理，而无需事先了解服务特定配置。实验原型基于 Sock Shop 微服务，展示了该方法减少对人类输入或静态文档依赖的潜力，从而提升了微服务管理的自治性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19056v1",
      "published_date": "2025-01-31 11:32:05 UTC",
      "updated_date": "2025-01-31 11:32:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:23:43.653863"
    },
    {
      "arxiv_id": "2501.19055v1",
      "title": "Towards Physiologically Sensible Predictions via the Rule-based Reinforcement Learning Layer",
      "title_zh": "翻译失败",
      "authors": [
        "Lingwei Zhu",
        "Zheng Chen",
        "Yukie Nagai",
        "Jimeng Sun"
      ],
      "abstract": "This paper adds to the growing literature of reinforcement learning (RL) for\nhealthcare by proposing a novel paradigm: augmenting any predictor with\nRule-based RL Layer (RRLL) that corrects the model's physiologically impossible\npredictions. Specifically, RRLL takes as input states predicted labels and\noutputs corrected labels as actions. The reward of the state-action pair is\nevaluated by a set of general rules. RRLL is efficient, general and\nlightweight: it does not require heavy expert knowledge like prior work but\nonly a set of impossible transitions. This set is much smaller than all\npossible transitions; yet it can effectively reduce physiologically impossible\nmistakes made by the state-of-the-art predictor models. We verify the utility\nof RRLL on a variety of important healthcare classification problems and\nobserve significant improvements using the same setup, with only the\ndomain-specific set of impossibility changed. In-depth analysis shows that RRLL\nindeed improves accuracy by effectively reducing the presence of\nphysiologically impossible predictions.",
      "tldr_zh": "这篇论文提出了一种新范式，通过Rule-based Reinforcement Learning Layer (RRLL)来增强预测模型，从而修正生理上不可能的预测。RRLL以预测状态和标签作为输入，输出修正后的标签作为动作，并使用一组通用规则评估奖励，从而避免需要大量专家知识，仅需定义不可能的转移即可。该方法高效、通用且轻量级，在多种医疗分类问题上实验显示，RRLL显著提高了准确性，主要通过减少生理上不可能的预测来实现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19055v1",
      "published_date": "2025-01-31 11:29:26 UTC",
      "updated_date": "2025-01-31 11:29:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:23:55.567802"
    },
    {
      "arxiv_id": "2501.19047v4",
      "title": "Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)",
      "title_zh": "翻译失败",
      "authors": [
        "Maja Pavlovic"
      ],
      "abstract": "To be considered reliable, a model must be calibrated so that its confidence\nin each decision closely reflects its true outcome. In this blogpost we'll take\na look at the most commonly used definition for calibration and then dive into\na frequently used evaluation measure for model calibration. We'll then cover\nsome of the drawbacks of this measure and how these surfaced the need for\nadditional notions of calibration, which require their own new evaluation\nmeasures. This post is not intended to be an in-depth dissection of all works\non calibration, nor does it focus on how to calibrate models. Instead, it is\nmeant to provide a gentle introduction to the different notions and their\nevaluation measures as well as to re-highlight some issues with a measure that\nis still widely used to evaluate calibration.",
      "tldr_zh": "这篇论文提供了一个温和的介绍和视觉探索，旨在帮助理解模型校准（model calibration），即模型的置信度应与实际结果相匹配。作者重点讨论了常用的校准评估指标expected calibration error (ECE)，并分析了其缺点，如无法捕捉所有校准问题，从而引发了对新校准概念和评估措施的需求。该工作并非深入剖析所有校准相关研究或提供校准方法，而是通过重新强调ECE的局限性，强调了评估模型可靠性的重要性。",
      "categories": [
        "stat.ME",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "stat.ME",
      "comment": "https://openreview.net/forum?id=BxBeCjQd2y",
      "pdf_url": "http://arxiv.org/pdf/2501.19047v4",
      "published_date": "2025-01-31 11:18:45 UTC",
      "updated_date": "2025-05-11 14:27:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:24:07.495180"
    },
    {
      "arxiv_id": "2501.19042v1",
      "title": "Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors",
      "title_zh": "Swarm-Gen：快速生成多样可行群体行为",
      "authors": [
        "Simon Idoko",
        "B. Bhanu Teja",
        "K. Madhava Krishna",
        "Arun Kumar Singh"
      ],
      "abstract": "Coordination behavior in robot swarms is inherently multi-modal in nature.\nThat is, there are numerous ways in which a swarm of robots can avoid\ninter-agent collisions and reach their respective goals. However, the problem\nof generating diverse and feasible swarm behaviors in a scalable manner remains\nlargely unaddressed. In this paper, we fill this gap by combining generative\nmodels with a safety-filter (SF). Specifically, we sample diverse trajectories\nfrom a learned generative model which is subsequently projected onto the\nfeasible set using the SF. We experiment with two choices for generative\nmodels, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized\nVariational Autoencoder (VQ-VAE). We highlight the trade-offs these two models\nprovide in terms of computation time and trajectory diversity. We develop a\ncustom solver for our SF and equip it with a neural network that predicts\ncontext-specific initialization. Thecinitialization network is trained in a\nself-supervised manner, taking advantage of the differentiability of the SF\nsolver. We provide two sets of empirical results. First, we demonstrate that we\ncan generate a large set of multi-modal, feasible trajectories, simulating\ndiverse swarm behaviors, within a few tens of milliseconds. Second, we show\nthat our initialization network provides faster convergence of our SF solver\nvis-a-vis other alternative heuristics.",
      "tldr_zh": "该论文提出Swarm-Gen框架，用于快速生成多样化和可行的机器人群行为，解决群协调的多模态问题。方法结合生成模型（如Conditional Variational Autoencoder (CVAE)和Vector-Quantized Variational Autoencoder (VQ-VAE))从样本中获取轨迹，并使用Safety Filter (SF)投影到可行集，同时开发自监督训练的神经网络预测上下文特定初始化以加速SF求解器。实验结果显示，该框架可在几十毫秒内生成大量多模态可行轨迹，并显著提升SF求解器的收敛速度。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to RAL",
      "pdf_url": "http://arxiv.org/pdf/2501.19042v1",
      "published_date": "2025-01-31 11:13:09 UTC",
      "updated_date": "2025-01-31 11:13:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:24:19.827603"
    },
    {
      "arxiv_id": "2501.19003v1",
      "title": "Virtual airways heatmaps to optimize point of entry location in lung biopsy planning systems",
      "title_zh": "翻译失败",
      "authors": [
        "Debora Gil",
        "Pere Lloret",
        "Marta Diez-Ferrer",
        "Carles Sanchez"
      ],
      "abstract": "Purpose: We present a virtual model to optimize point of entry (POE) in lung\nbiopsy planning systems. Our model allows to compute the quality of a biopsy\nsample taken from potential POE, taking into account the margin of error that\narises from discrepancies between the orientation in the planning simulation\nand the actual orientation during the operation. Additionally, the study\nexamines the impact of the characteristics of the lesion. Methods: The quality\nof the biopsy is given by a heatmap projected onto the skeleton of a\npatient-specific model of airways. The skeleton provides a 3D representation of\nairways structure, while the heatmap intensity represents the potential amount\nof tissue that it could be extracted from each POE. This amount of tissue is\ndetermined by the intersection of the lesion with a cone that represents the\nuncertainty area in the introduction of biopsy instruments. The cone, lesion,\nand skeleton are modelled as graphical objects that define a 3D scene of the\nintervention. Results: We have simulated different settings of the intervention\nscene from a single anatomy extracted from a CT scan and two lesions with\nregular and irregular shapes. The different scenarios are simulated by\nsystematic rotation of each lesion placed at different distances from airways.\nAnalysis of the heatmaps for the different settings show a strong impact of\nlesion orientation for irregular shape and the distance for both shapes.\nConclusion: The proposed heatmaps help to visually assess the optimal POE and\nidentify whether multiple optimal POEs exist in different zones of the bronchi.\nThey also allow us to model the maximum allowable error in navigation systems\nand study which variables have the greatest influence on the success of the\noperation. Additionally, they help determine at what point this influence could\npotentially jeopardize the operation.",
      "tldr_zh": "本文提出了一种虚拟模型，利用热图（heatmap）优化肺活检规划中的点 of entry (POE) 位置，考虑手术模拟与实际操作的方位差异及其对活检样本质量的影响。方法包括将热图投影到患者特定气道骨架上，计算每个 POE 的组织提取量，通过病变与不确定区域（用锥形表示）的交集来评估潜在组织量。实验结果显示，病变的方向对不规则形状影响显著，而距离对所有形状均有重大影响。总之，该热图有助于可视化最佳 POE、识别多个潜在最佳位置，并分析导航系统的最大允许误差及其对手术成功的关键影响。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.19003v1",
      "published_date": "2025-01-31 10:14:47 UTC",
      "updated_date": "2025-01-31 10:14:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:24:33.008820"
    },
    {
      "arxiv_id": "2501.18998v2",
      "title": "Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Ahmed K. Kadhim",
        "Lei Jiao",
        "Rishad Shafik",
        "Ole-Christoffer Granmo"
      ],
      "abstract": "In recent years, text generation tools utilizing Artificial Intelligence (AI)\nhave occasionally been misused across various domains, such as generating\nstudent reports or creative writings. This issue prompts plagiarism detection\nservices to enhance their capabilities in identifying AI-generated content.\nAdversarial attacks are often used to test the robustness of AI-text generated\ndetectors. This work proposes a novel textual adversarial attack on the\ndetection models such as Fast-DetectGPT. The method employs embedding models\nfor data perturbation, aiming at reconstructing the AI generated texts to\nreduce the likelihood of detection of the true origin of the texts.\nSpecifically, we employ different embedding techniques, including the Tsetlin\nMachine (TM), an interpretable approach in machine learning for this purpose.\nBy combining synonyms and embedding similarity vectors, we demonstrates the\nstate-of-the-art reduction in detection scores against Fast-DetectGPT.\nParticularly, in the XSum dataset, the detection score decreased from 0.4431 to\n0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.",
      "tldr_zh": "该论文提出了一种基于嵌入（embeddings）的对抗攻击（adversarial attacks）方法，针对AI生成文本检测模型（如Fast-DetectGPT），以测试其鲁棒性并降低检测准确率。该方法通过嵌入模型，包括Tsetlin Machine (TM)，结合同义词和嵌入相似性向量，对AI生成文本进行数据扰动（data perturbation）和重构，从而减少文本被识别为AI来源的可能性。在实验中，该攻击在XSum数据集上将检测分数从0.4431 AUROC降至0.2744 AUROC，在SQuAD数据集上从0.5068 AUROC降至0.3532 AUROC，展示了最先进的性能改进。总的来说，该研究突显了AI文本检测模型的潜在漏洞，并为提升其安全性提供了新见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18998v2",
      "published_date": "2025-01-31 10:06:27 UTC",
      "updated_date": "2025-04-10 18:46:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:24:44.763923"
    },
    {
      "arxiv_id": "2502.00076v1",
      "title": "Influence of color correction on pathology detection in Capsule Endoscopy",
      "title_zh": "颜色校正对胶囊内镜中病理检测的影响",
      "authors": [
        "Bidossessi Emmanuel Agossou",
        "Marius Pedersen",
        "Kiran Raja",
        "Anuja Vats",
        "Pål Anders Floor"
      ],
      "abstract": "Pathology detection in Wireless Capsule Endoscopy (WCE) using deep learning\nhas been explored in the recent past. However, deep learning models can be\ninfluenced by the color quality of the dataset used to train them, impacting\ndetection, segmentation and classification tasks. In this work, we evaluate the\nimpact of color correction on pathology detection using two prominent object\ndetection models: Retinanet and YOLOv5. We first generate two color corrected\nversions of a popular WCE dataset (i.e., SEE-AI dataset) using two different\ncolor correction functions. We then evaluate the performance of the Retinanet\nand YOLOv5 on the original and color corrected versions of the dataset. The\nresults reveal that color correction makes the models generate larger bounding\nboxes and larger intersection areas with the ground truth annotations.\nFurthermore, color correction leads to an increased number of false positives\nfor certain pathologies. However, these effects do not translate into a\nconsistent improvement in performance metrics such as F1-scores, IoU, and AP50.\nThe code is available at https://github.com/agossouema2011/WCE2024. Keywords:\nWireless Capsule Endoscopy, Color correction, Retinanet, YOLOv5, Detection",
      "tldr_zh": "本研究评估了颜色校正对无线胶囊内窥镜（WCE）中病理检测的影响，使用 Retinanet 和 YOLOv5 模型在 SEE-AI 数据集的原始和颜色校正版本上进行测试。结果表明，颜色校正导致模型生成更大的边界框和更大的交集区域，同时增加了某些病理的假阳性数量。然而，这些变化并未一致提升性能指标，如 F1-score、IoU 和 AP50。该工作提供了代码（https://github.com/agossouema2011/WCE2024），有助于进一步探索颜色校正在 WCE 检测中的作用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00076v1",
      "published_date": "2025-01-31 10:05:44 UTC",
      "updated_date": "2025-01-31 10:05:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:24:56.412852"
    },
    {
      "arxiv_id": "2501.18994v1",
      "title": "VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Jian-Yu Chen",
        "Yi-Ru Chen",
        "Yin-Qiao Chang",
        "Che-Ming Li",
        "Jann-Long Chern",
        "Chih-Wei Huang"
      ],
      "abstract": "This paper addresses the challenges in learning-based monocular positioning\nby proposing VKFPos, a novel approach that integrates Absolute Pose Regression\n(APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF)\nwithin a variational Bayesian inference framework. Our method shows that the\nessential posterior probability of the monocular positioning problem can be\ndecomposed into APR and RPR components. This decomposition is embedded in the\ndeep learning model by predicting covariances in both APR and RPR branches,\nallowing them to account for associated uncertainties. These covariances\nenhance the loss functions and facilitate EKF integration. Experimental\nevaluations on both indoor and outdoor datasets show that the single-shot APR\nbranch achieves accuracy on par with state-of-the-art methods. Furthermore, for\ntemporal positioning, where consecutive images allow for RPR and EKF\nintegration, VKFPos outperforms temporal APR and model-based integration\nmethods, achieving superior accuracy.",
      "tldr_zh": "本研究提出了一种基于学习的单目定位方法VKFPos，将Absolute Pose Regression (APR)和Relative Pose Regression (RPR)通过Extended Kalman Filter (EKF)整合到variational Bayesian inference框架中，以解决单目定位的挑战。该方法将后验概率分解为APR和RPR组件，并在深度学习模型中预测相关协方差，从而增强损失函数并促进EKF的整合。实验结果显示，VKFPos的单shot APR分支准确性可媲美最先进方法，而在时间定位任务中，通过结合RPR和EKF，它超过了基于时间的APR和模型整合方法，实现了更高的准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18994v1",
      "published_date": "2025-01-31 09:54:11 UTC",
      "updated_date": "2025-01-31 09:54:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:25:08.792585"
    },
    {
      "arxiv_id": "2501.18980v1",
      "title": "Symmetric Pruning of Large Language Models",
      "title_zh": "大型语言模型的对称剪枝",
      "authors": [
        "Kai Yi",
        "Peter Richtárik"
      ],
      "abstract": "Popular post-training pruning methods such as Wanda and RIA are known for\ntheir simple, yet effective, designs that have shown exceptional empirical\nperformance. Wanda optimizes performance through calibrated activations during\npruning, while RIA emphasizes the relative, rather than absolute, importance of\nweight elements. Despite their practical success, a thorough theoretical\nfoundation explaining these outcomes has been lacking. This paper introduces\nnew theoretical insights that redefine the standard minimization objective for\npruning, offering a deeper understanding of the factors contributing to their\nsuccess. Our study extends beyond these insights by proposing complementary\nstrategies that consider both input activations and weight significance. We\nvalidate these approaches through rigorous experiments, demonstrating\nsubstantial enhancements over existing methods. Furthermore, we introduce a\nnovel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative\nweight importance and a regularized decision boundary within a dynamic\npruning-and-growing framework, significantly outperforming strong baselines and\nestablishing a new state of the art.",
      "tldr_zh": "这篇论文探讨了大型语言模型(Large Language Models)的对称修剪方法，分析了现有技术如 Wanda 和 RIA 的有效性，但强调它们缺乏理论支撑。论文引入新的理论见解，重新定义修剪的最小化目标，并提出补充策略，考虑输入激活和权重重要性，以提升性能。实验结果显示，这些方法显著优于现有基准，而新提出的训练-free 微调方法 $R^2$-DSnoT 在动态修剪和增长框架中，进一步建立了新的最先进水平。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18980v1",
      "published_date": "2025-01-31 09:23:06 UTC",
      "updated_date": "2025-01-31 09:23:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:25:19.810919"
    },
    {
      "arxiv_id": "2501.18973v1",
      "title": "GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Seungheun Baek",
        "Soyon Park",
        "Yan Ting Chok",
        "Mogan Gim",
        "Jaewoo Kang"
      ],
      "abstract": "Motivation: Predicting cellular responses to genetic perturbations is\nessential for understanding biological systems and developing targeted\ntherapeutic strategies. While variational autoencoders (VAEs) have shown\npromise in modeling perturbation responses, their limited explainability poses\na significant challenge, as the learned features often lack clear biological\nmeaning. Nevertheless, model explainability is one of the most important\naspects in the realm of biological AI. One of the most effective ways to\nachieve explainability is incorporating the concept of gene regulatory networks\n(GRNs) in designing deep learning models such as VAEs. GRNs elicit the\nunderlying causal relationships between genes and are capable of explaining the\ntranscriptional responses caused by genetic perturbation treatments. Results:\nWe propose GPO-VAE, an explainable VAE enhanced by GRN-aligned Parameter\nOptimization that explicitly models gene regulatory networks in the latent\nspace. Our key approach is to optimize the learnable parameters related to\nlatent perturbation effects towards GRN-aligned explainability. Experimental\nresults on perturbation prediction show our model achieves state-of-the-art\nperformance in predicting transcriptional responses across multiple benchmark\ndatasets. Furthermore, additional results on evaluating the GRN inference task\nreveal our model's ability to generate meaningful GRNs compared to other\nmethods. According to qualitative analysis, GPO-VAE posseses the ability to\nconstruct biologically explainable GRNs that align with experimentally\nvalidated regulatory pathways. GPO-VAE is available at\nhttps://github.com/dmis-lab/GPO-VAE",
      "tldr_zh": "本文提出 GPO-VAE，一种基于 GRN-Aligned Parameter Optimization 的可解释 VAE 模型，用于预测细胞对基因扰动的响应，并通过在潜在空间中显式建模基因调控网络 (GRNs) 来提升模型的生物学解释性。关键方法包括优化与潜在扰动效果相关的可学习参数，以确保 GRNs 与实验验证的调控途径一致。实验结果显示，GPO-VAE 在多个基准数据集上实现了最先进的转录响应预测性能，并在 GRN 推断任务中生成有意义的网络结构。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18973v1",
      "published_date": "2025-01-31 09:08:52 UTC",
      "updated_date": "2025-01-31 09:08:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:25:32.422537"
    },
    {
      "arxiv_id": "2501.18959v2",
      "title": "Enhancing Neural Function Approximation: The XNet Outperforming KAN",
      "title_zh": "增强神经函数逼近：XNet 优于 KAN",
      "authors": [
        "Xin Li",
        "Xiaotao Zheng",
        "Zhihong Xia"
      ],
      "abstract": "XNet is a single-layer neural network architecture that leverages Cauchy\nintegral-based activation functions for high-order function approximation.\nThrough theoretical analysis, we show that the Cauchy activation functions used\nin XNet can achieve arbitrary-order polynomial convergence, fundamentally\noutperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely\non increased depth or B-spline activations. Our extensive experiments on\nfunction approximation, PDE solving, and reinforcement learning demonstrate\nXNet's superior performance - reducing approximation error by up to 50000 times\nand accelerating training by up to 10 times compared to existing approaches.\nThese results establish XNet as a highly efficient architecture for both\nscientific computing and AI applications.",
      "tldr_zh": "本研究提出XNet，一种基于Cauchy积分的单层神经网络架构，用于提升函数逼近能力。XNet通过Cauchy activation functions实现任意阶多项式收敛，在理论上优于传统MLPs和Kolmogorov-Arnold Networks (KANs)，后者依赖于增加深度或B-spline激活函数。实验结果显示，XNet在函数逼近、PDE求解和强化学习任务中，将逼近误差降低高达50000倍，并将训练速度加快高达10倍。这些优势确立了XNet作为科学计算和AI应用中高效架构的地位。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "arXiv admin note: text overlap with arXiv:2410.02033",
      "pdf_url": "http://arxiv.org/pdf/2501.18959v2",
      "published_date": "2025-01-31 08:33:10 UTC",
      "updated_date": "2025-02-14 02:50:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:25:43.858465"
    },
    {
      "arxiv_id": "2501.18955v1",
      "title": "Deep Learning based Quasi-consciousness Training for Robot Intelligent Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yuchun Li",
        "Fang Zhang"
      ],
      "abstract": "This paper explores a deep learning based robot intelligent model that\nrenders robots learn and reason for complex tasks. First, by constructing a\nnetwork of environmental factor matrix to stimulate the learning process of the\nrobot intelligent model, the model parameters must be subjected to coarse &\nfine tuning to optimize the loss function for minimizing the loss score,\nmeanwhile robot intelligent model can fuse all previously known concepts\ntogether to represent things never experienced before, which need robot\nintelligent model can be generalized extensively. Secondly, in order to\nprogressively develop a robot intelligent model with primary consciousness,\nevery robot must be subjected to at least 1~3 years of special school for\ntraining anthropomorphic behaviour patterns to understand and process complex\nenvironmental information and make rational decisions. This work explores and\ndelivers the potential application of deep learning-based quasi-consciousness\ntraining in the field of robot intelligent model.",
      "tldr_zh": "该论文提出了一种基于 deep learning 的机器人智能模型，旨在帮助机器人学习和推理复杂任务，通过构建环境因素矩阵网络并进行参数粗调和细调来优化损失函数，实现概念融合和广泛泛化。模型允许机器人整合已知知识处理未经验的事物，同时强调通过1-3年的准意识训练，使机器人学习拟人行为模式以理解复杂环境信息并做出理性决策。该方法探索了 deep learning 在机器人智能模型领域的潜在应用，为开发更高级的自主机器人奠定基础。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18955v1",
      "published_date": "2025-01-31 08:27:32 UTC",
      "updated_date": "2025-01-31 08:27:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:25:56.388721"
    },
    {
      "arxiv_id": "2501.18950v2",
      "title": "Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them",
      "title_zh": "扩散模型中概念擦除的",
      "authors": [
        "Anh Bui",
        "Trang Vu",
        "Long Vuong",
        "Trung Le",
        "Paul Montague",
        "Tamas Abraham",
        "Junae Kim",
        "Dinh Phung"
      ],
      "abstract": "Concept erasure has emerged as a promising technique for mitigating the risk\nof harmful content generation in diffusion models by selectively unlearning\nundesirable concepts. The common principle of previous works to remove a\nspecific concept is to map it to a fixed generic concept, such as a neutral\nconcept or just an empty text prompt. In this paper, we demonstrate that this\nfixed-target strategy is suboptimal, as it fails to account for the impact of\nerasing one concept on the others. To address this limitation, we model the\nconcept space as a graph and empirically analyze the effects of erasing one\nconcept on the remaining concepts. Our analysis uncovers intriguing geometric\nproperties of the concept space, where the influence of erasing a concept is\nconfined to a local region. Building on this insight, we propose the Adaptive\nGuided Erasure (AGE) method, which \\emph{dynamically} selects optimal target\nconcepts tailored to each undesirable concept, minimizing unintended side\neffects. Experimental results show that AGE significantly outperforms\nstate-of-the-art erasure methods on preserving unrelated concepts while\nmaintaining effective erasure performance. Our code is published at\n{https://github.com/tuananhbui89/Adaptive-Guided-Erasure}.",
      "tldr_zh": "本文研究了在扩散模型(diffusion models)中进行概念擦除(concept erasure)的优化问题，指出现有方法将不想要的概念映射到固定目标（如中性概念）会无意中影响其他概念。作者将概念空间建模为图，并通过实证分析发现，擦除一个概念的影响局限于局部几何区域。基于此，他们提出Adaptive Guided Erasure (AGE)方法，该方法动态选择针对每个不想要概念的最优目标，以最小化副作用。实验结果表明，AGE在有效擦除目标概念的同时，显著优于现有方法，在保留无关概念方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18950v2",
      "published_date": "2025-01-31 08:17:23 UTC",
      "updated_date": "2025-02-27 23:36:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:26:08.598225"
    },
    {
      "arxiv_id": "2502.00074v1",
      "title": "SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection",
      "title_zh": "SpikingRTNH",
      "authors": [
        "Dong-Hee Paek",
        "Seung-Hyun Kong"
      ],
      "abstract": "Recently, 4D Radar has emerged as a crucial sensor for 3D object detection in\nautonomous vehicles, offering both stable perception in adverse weather and\nhigh-density point clouds for object shape recognition. However, processing\nsuch high-density data demands substantial computational resources and energy\nconsumption. We propose SpikingRTNH, the first spiking neural network (SNN) for\n3D object detection using 4D Radar data. By replacing conventional ReLU\nactivation functions with leaky integrate-and-fire (LIF) spiking neurons,\nSpikingRTNH achieves significant energy efficiency gains. Furthermore, inspired\nby human cognitive processes, we introduce biological top-down inference (BTI),\nwhich processes point clouds sequentially from higher to lower densities. This\napproach effectively utilizes points with lower noise and higher importance for\ndetection. Experiments on K-Radar dataset demonstrate that SpikingRTNH with BTI\nsignificantly reduces energy consumption by 78% while achieving comparable\ndetection performance to its ANN counterpart (51.1% AP 3D, 57.0% AP BEV). These\nresults establish the viability of SNNs for energy-efficient 4D Radar-based\nobject detection in autonomous driving systems. All codes are available at\nhttps://github.com/kaist-avelab/k-radar.",
      "tldr_zh": "该研究提出SpikingRTNH，这是第一个针对4D Radar数据的Spiking Neural Network (SNN)，用于自动驾驶车辆的3D对象检测，以解决高密度点云处理带来的高能源消耗问题。通过将传统的ReLU激活函数替换为leaky integrate-and-fire (LIF)神经元，SpikingRTNH显著提高了能源效率，并引入biological top-down inference (BTI)机制，从高密度到低密度顺序处理点云，优先利用低噪声高重要性点。实验在K-Radar数据集上显示，该方法将能源消耗减少78%，同时实现与ANN相当的检测性能（51.1% AP 3D, 57.0% AP BEV），证明了SNN在能源高效4D Radar对象检测中的可行性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.CV",
      "comment": "arxiv preprint",
      "pdf_url": "http://arxiv.org/pdf/2502.00074v1",
      "published_date": "2025-01-31 07:33:30 UTC",
      "updated_date": "2025-01-31 07:33:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:26:20.987848"
    },
    {
      "arxiv_id": "2501.18924v1",
      "title": "Language Games as the Pathway to Artificial Superhuman Intelligence",
      "title_zh": "语言游戏作为通往人工智能",
      "authors": [
        "Ying Wen",
        "Ziyu Wan",
        "Shao Zhang"
      ],
      "abstract": "The evolution of large language models (LLMs) toward artificial superhuman\nintelligence (ASI) hinges on data reproduction, a cyclical process in which\nmodels generate, curate and retrain on novel data to refine capabilities.\nCurrent methods, however, risk getting stuck in a data reproduction trap:\noptimizing outputs within fixed human-generated distributions in a closed loop\nleads to stagnation, as models merely recombine existing knowledge rather than\nexplore new frontiers. In this paper, we propose language games as a pathway to\nexpanded data reproduction, breaking this cycle through three mechanisms: (1)\n\\textit{role fluidity}, which enhances data diversity and coverage by enabling\nmulti-agent systems to dynamically shift roles across tasks; (2) \\textit{reward\nvariety}, embedding multiple feedback criteria that can drive complex\nintelligent behaviors; and (3) \\textit{rule plasticity}, iteratively evolving\ninteraction constraints to foster learnability, thereby injecting continual\nnovelty. By scaling language games into global sociotechnical ecosystems,\nhuman-AI co-evolution generates unbounded data streams that drive open-ended\nexploration. This framework redefines data reproduction not as a closed loop\nbut as an engine for superhuman intelligence.",
      "tldr_zh": "本论文探讨大型语言模型 (LLMs) 向人工智能超人类智能 (ASI) 演化的关键问题，即数据再生产过程容易陷入封闭循环，导致模型仅重组现有知识而非探索新领域。作者提出语言游戏作为突破路径，通过三个机制——角色流动性 (role fluidity) 以动态切换多智能体角色提升数据多样性、奖励多样性 (reward variety) 以嵌入多种反馈标准驱动复杂行为，以及规则可塑性 (rule plasticity) 以迭代演化交互约束注入持续新颖性。最终，该框架推动人类-AI 共同演化，生成无限数据流，实现无边界探索，并将数据再生产转变为超人类智能的引擎。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "This position paper argues that language games provide robust\n  mechanism for achieving superhuman intelligence in large language models",
      "pdf_url": "http://arxiv.org/pdf/2501.18924v1",
      "published_date": "2025-01-31 07:10:40 UTC",
      "updated_date": "2025-01-31 07:10:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:26:32.806701"
    },
    {
      "arxiv_id": "2501.18922v1",
      "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
      "title_zh": "翻译失败",
      "authors": [
        "Haoran Luo",
        "Haihong E",
        "Yikai Guo",
        "Qika Lin",
        "Xiaobao Wu",
        "Xinyu Mu",
        "Wenhao Liu",
        "Meina Song",
        "Yifan Zhu",
        "Luu Anh Tuan"
      ],
      "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions with a large-scale structured knowledge base (KB). Despite\nadvancements with large language models (LLMs), KBQA still faces challenges in\nweak KB awareness, imbalance between effectiveness and efficiency, and high\nreliance on annotated data. To address these challenges, we propose KBQA-o1, a\nnovel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a\nReAct-based agent process for stepwise logical form generation with KB\nenvironment exploration. Moreover, it employs MCTS, a heuristic search method\ndriven by policy and reward models, to balance agentic exploration's\nperformance and search space. With heuristic exploration, KBQA-o1 generates\nhigh-quality annotations for further improvement by incremental fine-tuning.\nExperimental results show that KBQA-o1 outperforms previous low-resource KBQA\nmethods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1\nperformance to 78.5% compared to 48.5% of the previous sota method with\nGPT-3.5-turbo.",
      "tldr_zh": "这篇论文提出 KBQA-o1，一种基于 Monte Carlo Tree Search (MCTS) 的代理知识库问答方法，旨在解决大型语言模型 (LLMs) 在 KBQA 中的弱知识库意识、有效性和效率不平衡以及对标注数据的过度依赖问题。KBQA-o1 采用 ReAct-based 代理进行逐步逻辑形式生成和知识库环境探索，同时利用 MCTS 作为启发式搜索机制来平衡探索性能与搜索空间，并通过此过程生成高质量标注数据以支持增量微调。实验结果显示，该方法在低资源 KBQA 场景下显著优于现有方法，将 Llama-3.1-8B 模型在 GrailQA 的 F1 性能从 48.5% 提升至 78.5%，而前者 SOTA 方法依赖 GPT-3.5-turbo。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2501.18922v1",
      "published_date": "2025-01-31 06:59:49 UTC",
      "updated_date": "2025-01-31 06:59:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:26:45.567528"
    },
    {
      "arxiv_id": "2501.18919v1",
      "title": "Deepfake Detection of Singing Voices With Whisper Encodings",
      "title_zh": "翻译失败",
      "authors": [
        "Falguni Sharma",
        "Priyanka Gupta"
      ],
      "abstract": "The deepfake generation of singing vocals is a concerning issue for artists\nin the music industry. In this work, we propose a singing voice deepfake\ndetection (SVDD) system, which uses noise-variant encodings of open-AI's\nWhisper model. As counter-intuitive as it may sound, even though the Whisper\nmodel is known to be noise-robust, the encodings are rich in non-speech\ninformation, and are noise-variant. This leads us to evaluate Whisper encodings\nas feature representations for the SVDD task. Therefore, in this work, the SVDD\ntask is performed on vocals and mixtures, and the performance is evaluated in\n\\%EER over varying Whisper model sizes and two classifiers- CNN and ResNet34,\nunder different testing conditions.",
      "tldr_zh": "本研究针对音乐行业中歌手声音 Deepfake 生成的问题，提出了一种唱歌声音 Deepfake 检测系统（SVDD），利用 OpenAI 的 Whisper 模型噪声变异编码作为特征表示。尽管 Whisper 模型以抗噪著称，但其编码富含非语音信息且对噪声敏感，从而适用于 SVDD 任务。实验在声乐和混合物上进行，使用不同 Whisper 模型大小及 CNN 和 ResNet34 分类器，在各种测试条件下以 %EER 为指标评估性能，结果显示了该方法的有效性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted in ICASSP,2025",
      "pdf_url": "http://arxiv.org/pdf/2501.18919v1",
      "published_date": "2025-01-31 06:43:50 UTC",
      "updated_date": "2025-01-31 06:43:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:26:56.388040"
    },
    {
      "arxiv_id": "2502.18468v1",
      "title": "SOK: Exploring Hallucinations and Security Risks in AI-Assisted Software Development with Insights for LLM Deployment",
      "title_zh": "翻译失败",
      "authors": [
        "Ariful Haque",
        "Sunzida Siddique",
        "Md. Mahfuzur Rahman",
        "Ahmed Rafi Hasan",
        "Laxmi Rani Das",
        "Marufa Kamal",
        "Tasnim Masura",
        "Kishor Datta Gupta"
      ],
      "abstract": "The integration of Large Language Models (LLMs) such as GitHub Copilot,\nChatGPT, Cursor AI, and Codeium AI into software development has revolutionized\nthe coding landscape, offering significant productivity gains, automation, and\nenhanced debugging capabilities. These tools have proven invaluable for\ngenerating code snippets, refactoring existing code, and providing real-time\nsupport to developers. However, their widespread adoption also presents notable\nchallenges, particularly in terms of security vulnerabilities, code quality,\nand ethical concerns. This paper provides a comprehensive analysis of the\nbenefits and risks associated with AI-powered coding tools, drawing on user\nfeedback, security analyses, and practical use cases. We explore the potential\nfor these tools to replicate insecure coding practices, introduce biases, and\ngenerate incorrect or non-sensical code (hallucinations). In addition, we\ndiscuss the risks of data leaks, intellectual property violations and the need\nfor robust security measures to mitigate these threats. By comparing the\nfeatures and performance of these tools, we aim to guide developers in making\ninformed decisions about their use, ensuring that the benefits of AI-assisted\ncoding are maximized while minimizing associated risks.",
      "tldr_zh": "本论文探讨了将大型语言模型(LLM)如GitHub Copilot和ChatGPT等集成到软件开发中的益处与风险，强调这些工具提升了生产力、自动化和调试能力，但也可能引入安全漏洞、代码质量问题以及hallucinations（幻觉）。通过分析用户反馈、安全评估和实际用例，论文比较了这些工具的功能和性能，揭示了它们可能复制不安全实践、引入偏差、导致数据泄漏和知识产权侵犯等问题。最终，论文提供见解，帮助开发者在LLM部署中最大化优势，同时采用稳健的安全措施来降低风险。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.18468v1",
      "published_date": "2025-01-31 06:00:27 UTC",
      "updated_date": "2025-01-31 06:00:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:27:07.669172"
    },
    {
      "arxiv_id": "2503.15514v2",
      "title": "Superhuman Game AI Disclosure: Expertise and Context Moderate Effects on Trust and Fairness",
      "title_zh": "超人游戏 AI 披露：专业知识和上下文调节对信任和公平的影响",
      "authors": [
        "Jaymari Chua",
        "Chen Wang",
        "Lina Yao"
      ],
      "abstract": "As artificial intelligence surpasses human performance in select tasks,\ndisclosing superhuman capabilities poses distinct challenges for fairness,\naccountability, and trust. However, the impact of such disclosures on diverse\nuser attitudes and behaviors remains unclear, particularly concerning potential\nnegative reactions like discouragement or overreliance. This paper investigates\nthese effects by utilizing Persona Cards: a validated, standardized set of\nsynthetic personas designed to simulate diverse user reactions and fairness\nperspectives. We conducted an ethics board-approved study (N=32), utilizing\nthese personas to investigate how capability disclosure influenced behaviors\nwith a superhuman game AI in competitive StarCraft II scenarios. Our results\nreveal transparency is double-edged: while disclosure could alleviate\nsuspicion, it also provoked frustration and strategic defeatism among novices\nin cooperative scenarios, as well as overreliance in competitive contexts.\nExperienced and competitive players interpreted disclosure as confirmation of\nan unbeatable opponent, shifting to suboptimal goals. We release the Persona\nCards Dataset, including profiles, prompts, interaction logs, and protocols, to\nfoster reproducible research into human alignment AI design. This work\ndemonstrates that transparency is not a cure-all; successfully leveraging\ndisclosure to enhance trust and accountability requires careful tailoring to\nuser characteristics, domain norms, and specific fairness objectives.",
      "tldr_zh": "本文研究了披露超人类游戏AI能力对用户信任和公平性的影响，特别是在StarCraft II竞争场景中，使用Persona Cards（一种标准化合成人物集）模拟不同用户的反应和行为。实验结果（N=32）显示，透明度具有双重效应：它能缓解怀疑，但也可能引发新手在合作场景中的沮丧和战略失败主义，以及在竞争场景中的过度依赖；经验丰富的玩家则可能视披露为确认对手不可击败，转向次优目标。论文发布了Persona Cards数据集，包括配置文件、交互日志和协议，以推动可重复研究，并强调透明度需根据用户特征、领域规范和公平目标进行针对性调整。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.ET",
        "K.4.1; K.4.3; H.5.2; H.5.1; I.2.7"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.15514v2",
      "published_date": "2025-01-31 05:50:50 UTC",
      "updated_date": "2025-04-07 17:39:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:27:20.790959"
    },
    {
      "arxiv_id": "2501.18901v2",
      "title": "Lightspeed Geometric Dataset Distance via Sliced Optimal Transport",
      "title_zh": "翻译失败",
      "authors": [
        "Khai Nguyen",
        "Hai Nguyen",
        "Tuan Pham",
        "Nhat Ho"
      ],
      "abstract": "We introduce sliced optimal transport dataset distance (s-OTDD), a\nmodel-agnostic, embedding-agnostic approach for dataset comparison that\nrequires no training, is robust to variations in the number of classes, and can\nhandle disjoint label sets. The core innovation is Moment Transform Projection\n(MTP), which maps a label, represented as a distribution over features, to a\nreal number. Using MTP, we derive a data point projection that transforms\ndatasets into one-dimensional distributions. The s-OTDD is defined as the\nexpected Wasserstein distance between the projected distributions, with respect\nto random projection parameters. Leveraging the closed form solution of\none-dimensional optimal transport, s-OTDD achieves (near-)linear computational\ncomplexity in the number of data points and feature dimensions and is\nindependent of the number of classes. With its geometrically meaningful\nprojection, s-OTDD strongly correlates with the optimal transport dataset\ndistance while being more efficient than existing dataset discrepancy measures.\nMoreover, it correlates well with the performance gap in transfer learning and\nclassification accuracy in data augmentation.",
      "tldr_zh": "本文提出了一种新的数据集比较方法，名为 sliced optimal transport dataset distance (s-OTDD)，它是一种模型无关、嵌入无关的框架，不需训练且能处理类别数量变化和不相交标签集。核心创新在于 Moment Transform Projection (MTP)，通过将标签作为特征分布映射到实数，并将数据点投影为一维分布，从而计算投影分布间的期望 Wasserstein distance。实验结果显示，s-OTDD 具有（近似）线性计算复杂度，与 optimal transport dataset distance 高度相关，且在迁移学习性能差距和数据增强分类准确率方面表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICML 2025, 16 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.18901v2",
      "published_date": "2025-01-31 05:42:58 UTC",
      "updated_date": "2025-05-15 17:48:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:27:32.106080"
    },
    {
      "arxiv_id": "2502.00072v1",
      "title": "LLM Cyber Evaluations Don't Capture Real-World Risk",
      "title_zh": "LL",
      "authors": [
        "Kamilė Lukošiūtė",
        "Adam Swanda"
      ],
      "abstract": "Large language models (LLMs) are demonstrating increasing prowess in\ncybersecurity applications, creating creating inherent risks alongside their\npotential for strengthening defenses. In this position paper, we argue that\ncurrent efforts to evaluate risks posed by these capabilities are misaligned\nwith the goal of understanding real-world impact. Evaluating LLM cybersecurity\nrisk requires more than just measuring model capabilities -- it demands a\ncomprehensive risk assessment that incorporates analysis of threat actor\nadoption behavior and potential for impact. We propose a risk assessment\nframework for LLM cyber capabilities and apply it to a case study of language\nmodels used as cybersecurity assistants. Our evaluation of frontier models\nreveals high compliance rates but moderate accuracy on realistic cyber\nassistance tasks. However, our framework suggests that this particular use case\npresents only moderate risk due to limited operational advantages and impact\npotential. Based on these findings, we recommend several improvements to align\nresearch priorities with real-world impact assessment, including closer\nacademia-industry collaboration, more realistic modeling of attacker behavior,\nand inclusion of economic metrics in evaluations. This work represents an\nimportant step toward more effective assessment and mitigation of LLM-enabled\ncybersecurity risks.",
      "tldr_zh": "这篇论文指出，当前对大型语言模型 (LLMs) 在网络安全 (cybersecurity) 领域的评估未能准确反映真实世界风险，因为这些评估仅关注模型能力，而忽略了威胁行为者采用行为和潜在影响。作者提出一个全面的风险评估框架，并将其应用于 LLMs 作为网络安全助手的案例研究，结果显示前沿模型在实际任务中合规率高但准确率中等，且该用例的风险整体为中等水平。论文推荐改进措施，包括加强学术界与工业界的合作、更真实地模拟攻击者行为，以及在评估中纳入经济指标，以更好地缓解 LLM 启用网络安全风险。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.00072v1",
      "published_date": "2025-01-31 05:33:48 UTC",
      "updated_date": "2025-01-31 05:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:27:43.589898"
    },
    {
      "arxiv_id": "2501.18887v2",
      "title": "Building Bridges, Not Walls -- Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution",
      "title_zh": "翻译失败",
      "authors": [
        "Shichang Zhang",
        "Tessa Han",
        "Usha Bhalla",
        "Himabindu Lakkaraju"
      ],
      "abstract": "The increasing complexity of AI systems has made understanding their behavior\na critical challenge. Numerous methods have been developed to attribute model\nbehavior to three key aspects: input features, training data, and internal\nmodel components. However, these attribution methods are studied and applied\nrather independently, resulting in a fragmented landscape of approaches and\nterminology. This position paper argues that feature, data, and component\nattribution methods share fundamental similarities, and bridging them can\nbenefit interpretability research. We conduct a detailed analysis of successful\nmethods of these three attribution aspects and present a unified view to\ndemonstrate that these seemingly distinct methods employ similar approaches,\nsuch as perturbations, gradients, and linear approximations, differing\nprimarily in their perspectives rather than core techniques. Our unified\nperspective enhances understanding of existing attribution methods, identifies\nshared concepts and challenges, makes this field more accessible to newcomers,\nand highlights new directions not only for attribution and interpretability but\nalso for broader AI research, including model editing, steering, and\nregulation.",
      "tldr_zh": "这篇论文主张统一AI模型的可解释性归因方法，包括feature attribution、data attribution和model component attribution，以克服当前方法间的碎片化和术语差异。通过详细分析这些归因方法，论文发现它们在核心技术上（如扰动、梯度和线性逼近）高度相似，主要区别在于视角，提供了一个统一的框架来增强理解。统一视角不仅能识别共享概念和挑战、使该领域更易于新手进入，还为AI研究开辟新方向，包括模型编辑、引导和监管。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18887v2",
      "published_date": "2025-01-31 04:42:45 UTC",
      "updated_date": "2025-02-13 22:59:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:27:55.777551"
    },
    {
      "arxiv_id": "2502.17446v1",
      "title": "DCentNet: Decentralized Multistage Biomedical Signal Classification using Early Exits",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaolin Li",
        "Binhua Huang",
        "Barry Cardiff",
        "Deepu John"
      ],
      "abstract": "DCentNet is a novel decentralized multistage signal classification approach\ndesigned for biomedical data from IoT wearable sensors, integrating early exit\npoints (EEP) to enhance energy efficiency and processing speed. Unlike\ntraditional centralized processing methods, which result in high energy\nconsumption and latency, DCentNet partitions a single CNN model into multiple\nsub-networks using EEPs. By introducing encoder-decoder pairs at EEPs, the\nsystem compresses large feature maps before transmission, significantly\nreducing wireless data transfer and power usage. If an input is confidently\nclassified at an EEP, processing stops early, optimizing efficiency. Initial\nsub-networks can be deployed on fog or edge devices to further minimize energy\nconsumption. A genetic algorithm is used to optimize EEP placement, balancing\nperformance and complexity. Experimental results on ECG classification show\nthat with one EEP, DCentNet reduces wireless data transmission by 94.54% and\ncomplexity by 21%, while maintaining original accuracy and sensitivity. With\ntwo EEPs, sensitivity reaches 98.36%, accuracy 97.74%, wireless data\ntransmission decreases by 91.86%, and complexity is reduced by 22%. Implemented\non an ARM Cortex-M4 MCU, DCentNet achieves an average power saving of 73.6%\ncompared to continuous wireless ECG transmission.",
      "tldr_zh": "本文提出 DCentNet，一种去中心化的多阶段生物医学信号分类方法，针对 IoT 可穿戴传感器的生物医学数据，通过整合 Early Exits (EEP) 来提升能源效率和处理速度。DCentNet 将 CNN 模型分区成子网络，并在 EEP 处使用编码器-解码器对压缩特征图，减少无线数据传输，并借助遗传算法优化 EEP 放置以平衡性能和复杂性。如果输入在 EEP 处被自信分类，处理可提前停止。实验结果显示，在 ECG 分类任务中，DCentNet 减少了 94.54% 的无线数据传输和 21% 的复杂性，同时保持或提升准确性至 97.74% 和敏感性至 98.36%，并在 ARM Cortex-M4 MCU 上实现 73.6% 的平均功耗节省。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.17446v1",
      "published_date": "2025-01-31 04:24:39 UTC",
      "updated_date": "2025-01-31 04:24:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:28:10.793901"
    },
    {
      "arxiv_id": "2502.00070v2",
      "title": "Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers",
      "title_zh": "翻译失败",
      "authors": [
        "Pat Pataranutaporn",
        "Nattavudh Powdthavee",
        "Chayapatr Achiwaranguprok",
        "Pattie Maes"
      ],
      "abstract": "This study examines the potential of large language models (LLMs) to augment\nthe academic peer review process by reliably evaluating the quality of\neconomics research without introducing systematic bias. We conduct one of the\nfirst large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5,\nGemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we\nuse nonparametric binscatter and linear regression techniques to analyze over\n29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals\nexcluded from the training data of current LLMs, along with a set of\nAI-generated submissions. The results show that LLMs consistently distinguish\nbetween higher- and lower-quality research based solely on textual content,\nproducing quality gradients that closely align with established journal\nprestige measures. Claude and Gemma perform exceptionally well in capturing\nthese gradients, while GPT excels in detecting AI-generated content. The second\nexperiment comprises 8,910 evaluations designed to assess whether LLMs\nreplicate human like biases in single blind reviews. By systematically varying\nauthor gender, institutional affiliation, and academic prominence across 330\npapers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings\nto submissions from top male authors and elite institutions relative to the\nsame papers presented anonymously. These results emphasize the importance of\nexcluding author-identifying information when deploying LLMs in editorial\nscreening. Overall, our findings provide compelling evidence and practical\nguidance for integrating LLMs into peer review to enhance efficiency, improve\naccuracy, and promote equity in the publication process of economics research.",
      "tldr_zh": "这篇论文通过大规模实验评估了大型语言模型（LLMs，包括GPT-4o、Claude 3.5、Gemma 3和LLaMA 3.3）在评估超过1000篇经济学论文时的性能和偏见，旨在探讨LLMs是否能可靠地辅助同行评审过程。第一个实验分析了29,000多次对匿名论文的评估，结果显示LLMs能根据文本内容准确区分高质量和低质量研究，与期刊声望高度一致，其中Claude和Gemma在捕捉质量梯度方面表现突出，GPT在检测AI生成内容上优秀。第二个实验通过系统变异作者性别、机构隶属和学术声望，发现GPT、Gemma和LLaMA对顶级男性作者和精英机构的论文给予显著更高评分，揭示了类似人类的偏见。总体而言，该研究提供了证据和实用指导，建议在部署LLMs于同行评审时排除作者身份信息，以提升效率、准确性和公平性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "58 pages",
      "pdf_url": "http://arxiv.org/pdf/2502.00070v2",
      "published_date": "2025-01-31 04:04:02 UTC",
      "updated_date": "2025-04-03 02:12:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:28:23.851936"
    },
    {
      "arxiv_id": "2501.18867v2",
      "title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent",
      "title_zh": "UP-VLA：一个统一的理解与预测模型，用于具身代理",
      "authors": [
        "Jianke Zhang",
        "Yanjiang Guo",
        "Yucheng Hu",
        "Xiaoyu Chen",
        "Xiang Zhu",
        "Jianyu Chen"
      ],
      "abstract": "Recent advancements in Vision-Language-Action (VLA) models have leveraged\npre-trained Vision-Language Models (VLMs) to improve the generalization\ncapabilities. VLMs, typically pre-trained on vision-language understanding\ntasks, provide rich semantic knowledge and reasoning abilities. However, prior\nresearch has shown that VLMs often focus on high-level semantic content and\nneglect low-level features, limiting their ability to capture detailed spatial\ninformation and understand physical dynamics. These aspects, which are crucial\nfor embodied control tasks, remain underexplored in existing pre-training\nparadigms. In this paper, we investigate the training paradigm for VLAs, and\nintroduce \\textbf{UP-VLA}, a \\textbf{U}nified VLA model training with both\nmulti-modal \\textbf{U}nderstanding and future \\textbf{P}rediction objectives,\nenhancing both high-level semantic comprehension and low-level spatial\nunderstanding. Experimental results show that UP-VLA achieves a 33% improvement\non the Calvin ABC-D benchmark compared to the previous state-of-the-art method.\nAdditionally, UP-VLA demonstrates improved success rates in real-world\nmanipulation tasks, particularly those requiring precise spatial information.",
      "tldr_zh": "本研究探讨了基于预训练 Vision-Language Models (VLMs) 的 Vision-Language-Action (VLA) 模型在 embodied control 任务中的局限性，这些模型往往忽略低层特征，导致对详细空间信息和物理动态的理解不足。论文提出 UP-VLA，一种统一模型，通过结合多模态理解和未来预测目标，实现高层语义理解与低层空间理解的提升。实验结果显示，UP-VLA 在 Calvin ABC-D 基准上比先前最佳方法提高了 33%，并在真实世界操作任务中，尤其需要精确空间信息的场景中，显著提升了成功率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18867v2",
      "published_date": "2025-01-31 03:20:09 UTC",
      "updated_date": "2025-02-03 03:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:28:34.386836"
    },
    {
      "arxiv_id": "2501.18865v1",
      "title": "REG: Rectified Gradient Guidance for Conditional Diffusion Models",
      "title_zh": "REG：修正梯度指导用于",
      "authors": [
        "Zhengqi Gao",
        "Kaiwen Zha",
        "Tianyuan Zhang",
        "Zihui Xue",
        "Duane S. Boning"
      ],
      "abstract": "Guidance techniques are simple yet effective for improving conditional\ngeneration in diffusion models. Albeit their empirical success, the practical\nimplementation of guidance diverges significantly from its theoretical\nmotivation. In this paper, we reconcile this discrepancy by replacing the\nscaled marginal distribution target, which we prove theoretically invalid, with\na valid scaled joint distribution objective. Additionally, we show that the\nestablished guidance implementations are approximations to the intractable\noptimal solution under no future foresight constraint. Building on these\ntheoretical insights, we propose rectified gradient guidance (REG), a versatile\nenhancement designed to boost the performance of existing guidance methods.\nExperiments on 1D and 2D demonstrate that REG provides a better approximation\nto the optimal solution than prior guidance techniques, validating the proposed\ntheoretical framework. Extensive experiments on class-conditional ImageNet and\ntext-to-image generation tasks show that incorporating REG consistently\nimproves FID and Inception/CLIP scores across various settings compared to its\nabsence.",
      "tldr_zh": "本论文探讨了指导技术（guidance techniques）在条件扩散模型（conditional diffusion models）中的应用问题，指出现有实现与理论动机不一致，因为它们基于理论无效的scaled marginal distribution目标。作者提出rectified gradient guidance (REG)，通过采用有效的scaled joint distribution目标并改进对最优解的近似，从而提升现有指导方法的性能。实验结果显示，REG在1D和2D任务中提供更准确的近似，并在ImageNet的类条件生成和文本到图像任务上显著提高了FID和Inception/CLIP scores。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "19 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.18865v1",
      "published_date": "2025-01-31 03:16:18 UTC",
      "updated_date": "2025-01-31 03:16:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:28:51.964035"
    },
    {
      "arxiv_id": "2502.00068v1",
      "title": "Privacy Preserving Charge Location Prediction for Electric Vehicles",
      "title_zh": "隐私保护的电动车辆充电位置预测",
      "authors": [
        "Robert Marlin",
        "Raja Jurdak",
        "Alsharif Abuadbba",
        "Dimity Miller"
      ],
      "abstract": "By 2050, electric vehicles (EVs) are projected to account for 70% of global\nvehicle sales. While EVs provide environmental benefits, they also pose\nchallenges for energy generation, grid infrastructure, and data privacy.\nCurrent research on EV routing and charge management often overlooks privacy\nwhen predicting energy demands, leaving sensitive mobility data vulnerable. To\naddress this, we developed a Federated Learning Transformer Network (FLTN) to\npredict EVs' next charge location with enhanced privacy measures. Each EV\noperates as a client, training an onboard FLTN model that shares only model\nweights, not raw data with a community-based Distributed Energy Resource\nManagement System (DERMS), which aggregates them into a community global model.\nTo further enhance privacy, non-transitory EVs use peer-to-peer weight sharing\nand augmentation within their community, obfuscating individual contributions\nand improving model accuracy. Community DERMS global model weights are then\nredistributed to EVs for continuous training. Our FLTN approach achieved up to\n92% accuracy while preserving data privacy, compared to our baseline\ncentralised model, which achieved 98% accuracy with no data privacy.\nSimulations conducted across diverse charge levels confirm the FLTN's ability\nto forecast energy demands over extended periods. We present a privacy-focused\nsolution for forecasting EV charge location prediction, effectively mitigating\ndata leakage risks.",
      "tldr_zh": "该论文针对电动汽车(EVs)的充电位置预测问题，强调了数据隐私的保护，以应对能源需求预测中潜在的风险。研究提出Federated Learning Transformer Network (FLTN)，一个基于联邦学习的Transformer网络，其中每个EV作为客户端仅共享模型权重，而非原始数据，与Distributed Energy Resource Management System (DERMS)协作进行分布式训练和权重聚合。通过点对点权重共享和增强，FLTN在保持隐私的前提下达到了92%的准确率，相比基线集中式模型（98%准确率但无隐私保护）略有牺牲，但有效降低了数据泄露风险。整体方案通过模拟验证了在不同充电水平下预测能源需求的能力，为隐私友好的EVs管理提供了可行解决方案。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.6.5"
      ],
      "primary_category": "cs.CR",
      "comment": "12 pages, 7 figures, IEEE Journal paper",
      "pdf_url": "http://arxiv.org/pdf/2502.00068v1",
      "published_date": "2025-01-31 03:14:36 UTC",
      "updated_date": "2025-01-31 03:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:28:58.644974"
    },
    {
      "arxiv_id": "2501.18858v1",
      "title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Han Zhong",
        "Yutong Yin",
        "Shenao Zhang",
        "Xiaojun Xu",
        "Yuanxin Liu",
        "Yifei Zuo",
        "Zhihan Liu",
        "Boyi Liu",
        "Sirui Zheng",
        "Hongyi Guo",
        "Liwei Wang",
        "Mingyi Hong",
        "Zhaoran Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks, yet generating reliable reasoning processes remains a\nsignificant challenge. We present a unified probabilistic framework that\nformalizes LLM reasoning through a novel graphical model incorporating latent\nthinking processes and evaluation signals. Within this framework, we introduce\nthe Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in\ntwo steps. First, it generates high-quality rationales by approximating the\noptimal thinking process through reinforcement learning, using a novel reward\nshaping mechanism. Second, it enhances the base LLM by maximizing the joint\nprobability of rationale generation with respect to the model's parameters.\nTheoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$\nrepresenting the number of iterations. Empirical evaluations on math and coding\nbenchmarks demonstrate that our approach consistently improves performance\nacross different base models without requiring human-annotated thinking\nprocesses. In addition, BRiTE demonstrates superior performance compared to\nexisting algorithms that bootstrap thinking processes use alternative methods\nsuch as rejection sampling, and can even match or exceed the results achieved\nthrough supervised fine-tuning with human-annotated data.",
      "tldr_zh": "该研究提出BRiTE算法，通过一个统一的概率框架和图形模型来提升Large Language Models (LLMs)的推理能力，解决生成可靠思考过程的挑战。BRiTE包括两个步骤：首先，使用强化学习和新型奖励机制来生成高质量的推理理由（rationales）；其次，通过最大化推理理由生成的联合概率来优化基础LLM模型。理论上，BRiTE证明了以1/T的速度收敛，其中T为迭代次数。实验结果显示，该方法在数学和编码基准上显著提高了不同LLM的性能，且无需人工标注的思考过程，甚至优于基于拒绝采样的替代算法，并可媲美使用人类数据监督微调的效果。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18858v1",
      "published_date": "2025-01-31 02:39:07 UTC",
      "updated_date": "2025-01-31 02:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:29:09.375696"
    },
    {
      "arxiv_id": "2502.01655v1",
      "title": "A binary PSO based ensemble under-sampling model for rebalancing imbalanced training data",
      "title_zh": "一种基于二进制 PSO 的集成欠采样模型，用于重新平衡不平衡训练数据",
      "authors": [
        "Jinyan Li",
        "Yaoyang Wu",
        "Simon Fong",
        "Antonio J. Tallón-Ballesteros",
        "Xin-she Yang",
        "Sabah Mohammed",
        "Feng Wu"
      ],
      "abstract": "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
      "tldr_zh": "该论文提出了一种基于Binary PSO的集成欠采样模型，用于处理不平衡数据集分类问题，通过结合集成学习和Binary PSO instance selection方法来选择合适的多数类样本，与少数类样本构建新数据集。 该方法采用多目标策略，不仅显著提高了分类性能，还尽量保持了原数据集的完整性。 实验结果显示，该模型在多个不平衡数据集上优于传统集成方法、现有欠采样技术以及与传统PSO结合的方案。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 18 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.01655v1",
      "published_date": "2025-01-31 01:45:20 UTC",
      "updated_date": "2025-01-31 01:45:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:29:21.079908"
    },
    {
      "arxiv_id": "2501.18837v1",
      "title": "Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming",
      "title_zh": "翻译失败",
      "authors": [
        "Mrinank Sharma",
        "Meg Tong",
        "Jesse Mu",
        "Jerry Wei",
        "Jorrit Kruthoff",
        "Scott Goodfriend",
        "Euan Ong",
        "Alwin Peng",
        "Raj Agarwal",
        "Cem Anil",
        "Amanda Askell",
        "Nathan Bailey",
        "Joe Benton",
        "Emma Bluemke",
        "Samuel R. Bowman",
        "Eric Christiansen",
        "Hoagy Cunningham",
        "Andy Dau",
        "Anjali Gopal",
        "Rob Gilson",
        "Logan Graham",
        "Logan Howard",
        "Nimit Kalra",
        "Taesung Lee",
        "Kevin Lin",
        "Peter Lofgren",
        "Francesco Mosconi",
        "Clare O'Hara",
        "Catherine Olsson",
        "Linda Petrini",
        "Samir Rajani",
        "Nikhil Saxena",
        "Alex Silverstein",
        "Tanya Singh",
        "Theodore Sumers",
        "Leonard Tang",
        "Kevin K. Troy",
        "Constantin Weisser",
        "Ruiqi Zhong",
        "Giulio Zhou",
        "Jan Leike",
        "Jared Kaplan",
        "Ethan Perez"
      ],
      "abstract": "Large language models (LLMs) are vulnerable to universal jailbreaks-prompting\nstrategies that systematically bypass model safeguards and enable users to\ncarry out harmful processes that require many model interactions, like\nmanufacturing illegal substances at scale. To defend against these attacks, we\nintroduce Constitutional Classifiers: safeguards trained on synthetic data,\ngenerated by prompting LLMs with natural language rules (i.e., a constitution)\nspecifying permitted and restricted content. In over 3,000 estimated hours of\nred teaming, no red teamer found a universal jailbreak that could extract\ninformation from an early classifier-guarded LLM at a similar level of detail\nto an unguarded model across most target queries. On automated evaluations,\nenhanced classifiers demonstrated robust defense against held-out\ndomain-specific jailbreaks. These classifiers also maintain deployment\nviability, with an absolute 0.38% increase in production-traffic refusals and a\n23.7% inference overhead. Our work demonstrates that defending against\nuniversal jailbreaks while maintaining practical deployment viability is\ntractable.",
      "tldr_zh": "本文提出Constitutional Classifiers，一种基于合成数据的防护机制，通过使用自然语言规则（即宪法）来指定内容许可和限制，从而防御大语言模型(LLMs)面临的通用越狱攻击，这些攻击可能导致有害操作如大规模非法物质制造。在超过3000小时的红队测试中，该分类器成功阻挡了所有尝试从防护后LLM中提取详细信息的攻击，并在自动化评估中显示出对未见过的领域特定越狱攻击的鲁棒性。这些分类器在实际部署中保持可行性，仅导致0.38%的生产流量拒绝率增加和23.7%的推理开销，从而证明了有效防御通用越狱攻击的实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18837v1",
      "published_date": "2025-01-31 01:09:32 UTC",
      "updated_date": "2025-01-31 01:09:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:29:34.336071"
    },
    {
      "arxiv_id": "2501.18834v1",
      "title": "Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Gao",
        "Kaiwen Xu",
        "Michael E. Kim",
        "Lianrui Zuo",
        "Zhiyuan Li",
        "Derek B. Archer",
        "Timothy J. Hohman",
        "Ann Zenobia Moore",
        "Luigi Ferrucci",
        "Lori L. Beason-Held",
        "Susan M. Resnick",
        "Christos Davatzikos",
        "Jerry L. Prince",
        "Bennett A. Landman"
      ],
      "abstract": "Defacing is often applied to head magnetic resonance image (MRI) datasets\nprior to public release to address privacy concerns. The alteration of facial\nand nearby voxels has provoked discussions about the true capability of these\ntechniques to ensure privacy as well as their impact on downstream tasks. With\nadvancements in deep generative models, the extent to which defacing can\nprotect privacy is uncertain. Additionally, while the altered voxels are known\nto contain valuable anatomical information, their potential to support research\nbeyond the anatomical regions directly affected by defacing remains uncertain.\nTo evaluate these considerations, we develop a refacing pipeline that recovers\nfaces in defaced head MRIs using cascaded diffusion probabilistic models\n(DPMs). The DPMs are trained on images from 180 subjects and tested on images\nfrom 484 unseen subjects, 469 of whom are from a different dataset. To assess\nwhether the altered voxels in defacing contain universally useful information,\nwe also predict computed tomography (CT)-derived skeletal muscle radiodensity\nfrom facial voxels in both defaced and original MRIs. The results show that\nDPMs can generate high-fidelity faces that resemble the original faces from\ndefaced images, with surface distances to the original faces significantly\nsmaller than those of a population average face (p < 0.05). This performance\nalso generalizes well to previously unseen datasets. For skeletal muscle\nradiodensity predictions, using defaced images results in significantly weaker\nSpearman's rank correlation coefficients compared to using original images (p <\n10-4). For shin muscle, the correlation is statistically significant (p < 0.05)\nwhen using original images but not statistically significant (p > 0.05) when\nany defacing method is applied, suggesting that defacing might not only fail to\nprotect privacy but also eliminate valuable information.",
      "tldr_zh": "该研究探讨了全头 MRI 图像去脸部处理（defacing）的潜在问题，包括使用 diffusion probabilistic models (DPMs) 可能导致的再识别风险，以及对研究潜力的损害。研究开发了一个基于 cascaded DPMs 的 refacing 管道，在 180 个受试者的图像上训练，并在 484 个未见受试者（包括不同数据集）的图像上测试，结果显示生成的面部与原面部高度相似，表面距离显著小于人口平均面部（p < 0.05），并能泛化到新数据集。实验还发现，使用 defaced 图像预测 CT 衍生的骨骼肌放射密度时，Spearman's 相关系数显著弱于原图像（p < 10^-4），且对于小腿肌肉的预测关联在 defacing 后变得不显著（p > 0.05），表明 defacing 不仅无法有效保护隐私，还可能消除宝贵的解剖信息。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18834v1",
      "published_date": "2025-01-31 00:58:12 UTC",
      "updated_date": "2025-01-31 00:58:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:29:46.661799"
    },
    {
      "arxiv_id": "2501.18821v2",
      "title": "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for Anomaly Detection in CAN Bus",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammad Fatahi",
        "Danial Sadrian Zadeh",
        "Benyamin Ghojogh",
        "Behzad Moshiri",
        "Otman Basir"
      ],
      "abstract": "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
      "tldr_zh": "本文针对自动驾驶车辆中 Controller Area Network (CAN) bus 的安全漏洞，提出了一种优化级联特征级时空融合策略，用于异常检测。该策略通过两参数遗传算法结合时间和空间信息，覆盖所有主要异常模式，并使用配对 t 检验确保模型的可靠性和鲁棒性。在两个常用数据集上的比较分析显示，该模型在准确率和 F1-score 上优于现有方法，实现了最佳性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "v2: updated the text and graphs",
      "pdf_url": "http://arxiv.org/pdf/2501.18821v2",
      "published_date": "2025-01-31 00:36:08 UTC",
      "updated_date": "2025-03-05 04:45:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:29:57.616140"
    },
    {
      "arxiv_id": "2501.18817v1",
      "title": "Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Andrey Borro",
        "Patricia J Riddle",
        "Michael W Barley",
        "Michael J Witbrock"
      ],
      "abstract": "Recent advancements in the reasoning skills of Large Language Models (LLMs)\ndemonstrate an increase in the ability of LLMs to solve simple planning tasks.\nHowever, as long as the driving force behind improved reasoning capability is\nthe size and complexity of the model, the financial and computational costs\nassociated with running them will also increase. This trend raises questions\nabout continued accessibility and whether these improvements will increase at\nthe same pace as models continue to grow in size and expense. We propose two\napproaches to enhance the reasoning ability of less resource-intensive LLMs.\n(1) Provide them with a generalised strategy for solving tasks within a given\ndomain, generated by a more resource-intensive LLM. (2) Exploit their\ncost-effectiveness by iteratively prompting these models to correct errors in\ntheir proposed solutions. Our empirical results from planning and mathematical\nreasoning tasks demonstrate that these methods improve the performance of less\nresource-intensive LLMs to levels comparable with their more resource-intensive\ncounterparts, at a fraction of the cost. Additionally, we show that the\nutilisation of generalised strategies in our experiments reduced the cost of\nthe less resource-intensive model by nearly 30 percent on average.",
      "tldr_zh": "这篇论文探讨了如何提升小型大型语言模型（LLMs）的推理能力，以桥接其在规划任务上的差距。研究提出两种方法：(1) 由资源密集型 LLM 生成的通用策略（generalised strategies），提供给小型 LLM 用于特定领域任务的解决；(2) 通过迭代提示，利用小型 LLM 的成本效益来修正其解决方案。实验结果显示，这些方法使小型 LLM 的性能在规划和数学推理任务上达到与资源密集型 LLM 相当的水平，同时平均降低了小型 LLM 的成本近 30%。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "I.2.8; I.2.6"
      ],
      "primary_category": "cs.AI",
      "comment": "7 page body, 2 page references, 16 page appendix (25 pages total); 2\n  figures; submitted to IJCAI2025",
      "pdf_url": "http://arxiv.org/pdf/2501.18817v1",
      "published_date": "2025-01-31 00:28:29 UTC",
      "updated_date": "2025-01-31 00:28:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:30:09.944708"
    },
    {
      "arxiv_id": "2501.18816v1",
      "title": "Large Language Models as Common-Sense Heuristics",
      "title_zh": "翻译失败",
      "authors": [
        "Andrey Borro",
        "Patricia J Riddle",
        "Michael W Barley",
        "Michael J Witbrock"
      ],
      "abstract": "While systems designed for solving planning tasks vastly outperform Large\nLanguage Models (LLMs) in this domain, they usually discard the rich semantic\ninformation embedded within task descriptions. In contrast, LLMs possess\nparametrised knowledge across a wide range of topics, enabling them to leverage\nthe natural language descriptions of planning tasks in their solutions.\nHowever, current research in this direction faces challenges in generating\ncorrect and executable plans. Furthermore, these approaches depend on the LLM\nto output solutions in an intermediate language, which must be translated into\nthe representation language of the planning task. We introduce a novel planning\nmethod, which leverages the parametrised knowledge of LLMs by using their\noutput as a heuristic for Hill-Climbing Search. This approach is further\nenhanced by prompting the LLM to generate a solution estimate to guide the\nsearch. Our method outperforms the task success rate of similar systems within\na common household environment by 22 percentage points, with consistently\nexecutable plans. All actions are encoded in their original representation,\ndemonstrating that strong results can be achieved without an intermediate\nlanguage, thus eliminating the need for a translation step.",
      "tldr_zh": "这篇论文探讨了如何将大型语言模型（LLMs）用作常见感启发式（common-sense heuristics），以提升规划任务的性能，同时利用LLMs的语义知识优势。研究提出了一种新方法，将LLMs的输出作为Hill-Climbing Search的启发式，并通过提示生成解决方案估计来引导搜索过程，从而避免了依赖中间语言的翻译步骤。实验结果显示，该方法在家庭环境任务中成功率比类似系统提高了22%，并生成了可执行的计划，证明了LLMs在规划领域的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "I.2.8"
      ],
      "primary_category": "cs.CL",
      "comment": "7 page body, 2 page references, 5 page appendix (14 page total); 1\n  figure; Submitted to IJCAI2025",
      "pdf_url": "http://arxiv.org/pdf/2501.18816v1",
      "published_date": "2025-01-31 00:26:38 UTC",
      "updated_date": "2025-01-31 00:26:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:30:21.928209"
    },
    {
      "arxiv_id": "2501.18815v1",
      "title": "An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D Brain Images",
      "title_zh": "翻译失败",
      "authors": [
        "Abdullah Naziba",
        "Clinton Fookes",
        "Dimitri Perrin"
      ],
      "abstract": "We developed a generative patch based 3D image registration model that can\nregister very high resolution images obtained from a biochemical process name\ntissue clearing. Tissue clearing process removes lipids and fats from the\ntissue and make the tissue transparent. When cleared tissues are imaged with\nLight-sheet fluorescent microscopy, the resulting images give a clear window to\nthe cellular activities and dynamics inside the tissue.Thus the images obtained\nare very rich with cellular information and hence their resolution is extremely\nhigh (eg .2560x2160x676). Analyzing images with such high resolution is a\ndifficult task for any image analysis pipeline.Image registration is a common\nstep in image analysis pipeline when comparison between images are required.\nTraditional image registration methods fail to register images with such\nextant. In this paper we addressed this very high resolution image registration\nissue by proposing a patch-based generative network named InvGAN. Our proposed\nnetwork can register very high resolution tissue cleared images. The tissue\ncleared dataset used in this paper are obtained from a tissue clearing protocol\nnamed CUBIC. We compared our method both with traditional and deep-learning\nbased registration methods.Two different versions of CUBIC dataset are used,\nrepresenting two different resolutions 25% and 100% respectively. Experiments\non two different resolutions clearly show the impact of resolution on the\nregistration quality. At 25% resolution, our method achieves comparable\nregistration accuracy with very short time (7 minutes approximately). At 100%\nresolution, most of the traditional registration methods fail except Elastix\nregistration tool.Elastix takes 28 hours to register where proposed InvGAN\ntakes only 10 minutes.",
      "tldr_zh": "本文提出了一种基于对抗生成网络的补丁式3D图像配准模型InvGAN，用于注册组织透明化技术（如CUBIC协议）获得的极高分辨率脑图像（如2560x2160x676），以解决传统方法在处理此类图像时的失败问题。InvGAN通过生成式网络实现高效配准，实验显示在25%分辨率下，其准确性与基准方法相当且仅需约7分钟，而在100%分辨率下，InvGAN只需10分钟即可完成配准，大幅优于Elastix的28小时处理时间。该方法显著提高了高分辨率图像分析的效率和可行性，为细胞活动研究提供更可靠的工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.18815v1",
      "published_date": "2025-01-31 00:19:45 UTC",
      "updated_date": "2025-01-31 00:19:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-22T05:30:34.827716"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 112,
  "processed_papers_count": 112,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-22T05:30:56.153198"
}
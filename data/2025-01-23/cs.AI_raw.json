[
  {
    "arxiv_id": "2502.00032v1",
    "title": "Querying Databases with Function Calling",
    "authors": [
      "Connor Shorten",
      "Charles Pierse",
      "Thomas Benjamin Smith",
      "Karel D'Oosterlinck",
      "Tuana Celik",
      "Erika Cardenas",
      "Leonie Monigatti",
      "Mohd Shukri Hasan",
      "Edward Schmuhl",
      "Daniel Williams",
      "Aravind Kesiraju",
      "Bob van Luijt"
    ],
    "abstract": "The capabilities of Large Language Models (LLMs) are rapidly accelerating\nlargely thanks to their integration with external tools. Querying databases is\namong the most effective of these integrations, enabling LLMs to access private\nor continually updating data. While Function Calling is the most common method\nfor interfacing external tools to LLMs, its application to database querying as\na tool has been underexplored. We propose a tool definition for database\nquerying that unifies accessing data with search queries, filters, or a\ncombination both, as well as transforming results with aggregation and groupby\noperators. To evaluate its effectiveness, we conduct a study with 8 LLMs\nspanning 5 model families. We present a novel pipeline adapting the Gorilla LLM\nframework to create synthetic database schemas and queries. We primarily\nevaluate the models with the Exact Match of predicted and ground truth query\nAPIs. Among the models tested, Claude 3.5 Sonnet achieves the highest\nperformance with an Exact Match score of 74.3%, followed by GPT-4o mini at\n73.7%, and GPT-4o at 71.8%. We further breakdown these results per API\ncomponent utilized and across synthetic use cases. We find that LLMs are highly\neffective at utilizing operators on boolean properties, but struggle with text\nproperty filters. Across use cases we find robust results with the higher\nperforming models such as GPT-4o, but significant performance variance across\nuse cases from lower performing models. We additionally conduct ablation\nstudies exploring the impact of parallel tool calling, adding a rationale as an\nargument of the tool call, using a separate tool per database collection, and\ntool calling with structured outputs. Our findings demonstrate the\neffectiveness of enabling LLMs to query databases with Function Calling. We\nhave open-sourced our experimental code and results at\ngithub.com/weaviate/gorilla.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DB",
    "comment": "Preprint. 23 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.00032v1",
    "published_date": "2025-01-23 23:09:13 UTC",
    "updated_date": "2025-01-23 23:09:13 UTC"
  },
  {
    "arxiv_id": "2502.12158v1",
    "title": "Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model",
    "authors": [
      "Mingchen Shao",
      "Youjeong Kang",
      "Xiao Hu",
      "Hyunjung Gloria Kwak",
      "Carl Yang",
      "Jiaying Lu"
    ],
    "abstract": "Heart Failure (HF) affects millions of Americans and leads to high\nreadmission rates, posing significant healthcare challenges. While Social\nDeterminants of Health (SDOH) such as socioeconomic status and housing\nstability play critical roles in health outcomes, they are often\nunderrepresented in structured EHRs and hidden in unstructured clinical notes.\nThis study leverages advanced large language models (LLMs) to extract SDOHs\nfrom clinical text and uses logistic regression to analyze their association\nwith HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited\ntransportation) linked to readmission risk, this work also offers actionable\ninsights for reducing readmissions and improving patient care.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.12158v1",
    "published_date": "2025-01-23 23:05:53 UTC",
    "updated_date": "2025-01-23 23:05:53 UTC"
  },
  {
    "arxiv_id": "2501.14122v2",
    "title": "Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters",
    "authors": [
      "Soumyendu Sarkar",
      "Ashwin Ramesh Babu",
      "Sajad Mousavi",
      "Vineet Gundecha",
      "Sahand Ghorbanpour",
      "Avisek Naug",
      "Ricardo Luna Gutierrez",
      "Antonio Guillen"
    ],
    "abstract": "We present a Reinforcement Learning Platform for Adversarial Black-box\nuntargeted and targeted attacks, RLAB, that allows users to select from various\ndistortion filters to create adversarial examples. The platform uses a\nReinforcement Learning agent to add minimum distortion to input images while\nstill causing misclassification by the target model. The agent uses a novel\ndual-action method to explore the input image at each step to identify\nsensitive regions for adding distortions while removing noises that have less\nimpact on the target model. This dual action leads to faster and more efficient\nconvergence of the attack. The platform can also be used to measure the\nrobustness of image classification models against specific distortion types.\nAlso, retraining the model with adversarial samples significantly improved\nrobustness when evaluated on benchmark datasets. The proposed platform\noutperforms state-of-the-art methods in terms of the average number of queries\nrequired to cause misclassification. This advances trustworthiness with a\npositive social impact.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 2025 AAAI Conference on Artificial Intelligence\n  Proceedings",
    "pdf_url": "http://arxiv.org/pdf/2501.14122v2",
    "published_date": "2025-01-23 22:36:06 UTC",
    "updated_date": "2025-04-15 08:15:08 UTC"
  },
  {
    "arxiv_id": "2501.14120v1",
    "title": "On the Transfer of Knowledge in Quantum Algorithms",
    "authors": [
      "Esther Villar-Rodriguez",
      "Eneko Osaba",
      "Izaskun Oregi",
      "Sebastián V. Romero",
      "Julián Ferreiro-Vélez"
    ],
    "abstract": "The field of quantum computing is generating significant anticipation within\nthe scientific and industrial communities due to its potential to revolutionize\ncomputing paradigms. Recognizing this potential, this paper explores the\nintegration of transfer of knowledge techniques, traditionally used in\nclassical artificial intelligence, into quantum computing. We present a\ncomprehensive classification of the transfer models, focusing on Transfer\nLearning and Transfer Optimization. Additionally, we analyze relevant schemes\nin quantum computing that can benefit from knowledge sharing, and we delve into\nthe potential synergies, supported by theoretical insights and initial\nexperimental results. Our findings suggest that leveraging the transfer of\nknowledge can enhance the efficiency and effectiveness of quantum algorithms,\nparticularly in the context of hybrid solvers. This approach not only\naccelerates the optimization process but also reduces the computational burden\non quantum processors, making it a valuable tool for advancing quantum\ncomputing technologies.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "12 pages, 8 figures, 4 tables. Paper submitted for its review in\n  Expert Systems journal",
    "pdf_url": "http://arxiv.org/pdf/2501.14120v1",
    "published_date": "2025-01-23 22:21:32 UTC",
    "updated_date": "2025-01-23 22:21:32 UTC"
  },
  {
    "arxiv_id": "2501.14119v1",
    "title": "Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation",
    "authors": [
      "Derek Yotheringhay",
      "Alistair Kirkland",
      "Humphrey Kirkbride",
      "Josiah Whitesteeple"
    ],
    "abstract": "Transformative innovations in model architectures have introduced\nhierarchical embedding augmentation as a means to redefine the representation\nof tokens through multi-level semantic structures, offering enhanced\nadaptability to complex linguistic inputs. Autonomous structural memory\nmanipulation further advances this paradigm through dynamic memory reallocation\nmechanisms that prioritize critical contextual features while suppressing less\nrelevant information, enabling scalable and efficient performance across\ndiverse tasks. Experimental results reveal substantial improvements in\ncomputational efficiency, with marked reductions in processing overhead for\nlonger input sequences, achieved through memory reorganization strategies that\nadapt to evolving contextual requirements. Hierarchical embeddings not only\nimproved contextual alignment but also facilitated task generalization by\ncapturing relationships at varying semantic granularities, ensuring coherence\nacross layers without introducing significant computational redundancies.\nComparative analysis against baseline models demonstrated unique advantages in\naccuracy, efficiency, and interpretability, particularly in tasks requiring\ncomplex contextual understanding or domain-specific adaptability. The ability\nto dynamically adjust token representations and memory configurations\ncontributed to the model's robustness under varied and unpredictable input\nconditions. Applications benefiting from these advancements include\nmulti-domain generalization, interactive systems, and scenarios involving\nreal-time decision-making, where traditional static memory architectures often\nface limitations. The proposed methodology combines advanced embedding and\nmemory management strategies into a cohesive framework that addresses\nscalability challenges while preserving task-specific relevance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14119v1",
    "published_date": "2025-01-23 22:20:36 UTC",
    "updated_date": "2025-01-23 22:20:36 UTC"
  },
  {
    "arxiv_id": "2502.10413v1",
    "title": "Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering",
    "authors": [
      "Raj Sonani",
      "Lohalekar Prayas"
    ],
    "abstract": "Digital data continues to grow, there has been a shift towards using\neffective regulatory mechanisms to safeguard personal information. The CCPA of\nCalifornia and the General Data Protection Regulation (GDPR) of the European\nUnion are two of the most important privacy laws. The regulation is intended to\nsafeguard consumer privacy, but it varies greatly in scope, definitions, and\nmethods of enforcement. This paper presents a fresh approach to adaptive\ncompliance, using machine learning and emphasizing natural language processing\n(NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP,\nthis study compares various regulations to identify areas where they overlap or\ndiverge. This includes the \"right to be forgotten\" provision in the GDPR and\nthe \"opt-out of sale\" provision under CCPA. International companies can learn\nvaluable lessons from this report, as it outlines strategies for better\nenforcement of laws across different nations. Additionally, the paper discusses\nthe challenges of utilizing NLP in legal literature and proposes methods to\nenhance the model-ability of machine learning models for studying regulations.\nThe study's objective is to \"bridge the gap between legal knowledge and\ntechnical expertise\" by developing regulatory compliance strategies that are\nmore efficient in operation and more effective in data protection.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "16 pages, 5 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2502.10413v1",
    "published_date": "2025-01-23 22:11:18 UTC",
    "updated_date": "2025-01-23 22:11:18 UTC"
  },
  {
    "arxiv_id": "2501.14105v1",
    "title": "MedSlice: Fine-Tuned Large Language Models for Secure Clinical Note Sectioning",
    "authors": [
      "Joshua Davis",
      "Thomas Sounack",
      "Kate Sciacca",
      "Jessie M Brain",
      "Brigitte N Durieux",
      "Nicole D Agaronnik",
      "Charlotta Lindvall"
    ],
    "abstract": "Extracting sections from clinical notes is crucial for downstream analysis\nbut is challenging due to variability in formatting and labor-intensive nature\nof manual sectioning. While proprietary large language models (LLMs) have shown\npromise, privacy concerns limit their accessibility. This study develops a\npipeline for automated note sectioning using open-source LLMs, focusing on\nthree sections: History of Present Illness, Interval History, and Assessment\nand Plan. We fine-tuned three open-source LLMs to extract sections using a\ncurated dataset of 487 progress notes, comparing results relative to\nproprietary models (GPT-4o, GPT-4o mini). Internal and external validity were\nassessed via precision, recall and F1 score. Fine-tuned Llama 3.1 8B\noutperformed GPT-4o (F1=0.92). On the external validity test set, performance\nremained high (F1= 0.85). Fine-tuned open-source LLMs can surpass proprietary\nmodels in clinical note sectioning, offering advantages in cost, performance,\nand accessibility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Our code is publicly available on github (\n  https://github.com/lindvalllab/MedSlice )",
    "pdf_url": "http://arxiv.org/pdf/2501.14105v1",
    "published_date": "2025-01-23 21:32:09 UTC",
    "updated_date": "2025-01-23 21:32:09 UTC"
  },
  {
    "arxiv_id": "2501.17175v1",
    "title": "Document-Level Sentiment Analysis of Urdu Text Using Deep Learning Techniques",
    "authors": [
      "Ammarah Irum",
      "M. Ali Tahir"
    ],
    "abstract": "Document level Urdu Sentiment Analysis (SA) is a challenging Natural Language\nProcessing (NLP) task as it deals with large documents in a resource-poor\nlanguage. In large documents, there are ample amounts of words that exhibit\ndifferent viewpoints. Deep learning (DL) models comprise of complex neural\nnetwork architectures that have the ability to learn diverse features of the\ndata to classify various sentiments. Besides audio, image and video\nclassification; DL algorithms are now extensively used in text-based\nclassification problems. To explore the powerful DL techniques for Urdu SA, we\nhave applied five different DL architectures namely, Bidirectional Long Short\nTerm Memory (BiLSTM), Convolutional Neural Network (CNN), Convolutional Neural\nNetwork with Bidirectional Long Short Term Memory (CNN-BiLSTM), Bidirectional\nEncoder Representation from Transformer (BERT). In this paper, we have proposed\na DL hybrid model that integrates BiLSTM with Single Layer Multi Filter\nConvolutional Neural Network (BiLSTM-SLMFCNN). The proposed and baseline\ntechniques are applied on Urdu Customer Support data set and IMDB Urdu movie\nreview data set by using pretrained Urdu word embeddings that are suitable for\n(SA) at the document level. Results of these techniques are evaluated and our\nproposed model outperforms all other DL techniques for Urdu SA. BiLSTM-SLMFCNN\noutperformed the baseline DL models and achieved 83{\\%}, 79{\\%}, 83{\\%} and\n94{\\%} accuracy on small, medium and large sized IMDB Urdu movie review data\nset and Urdu Customer Support data set respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17175v1",
    "published_date": "2025-01-23 21:25:37 UTC",
    "updated_date": "2025-01-23 21:25:37 UTC"
  },
  {
    "arxiv_id": "2501.14084v1",
    "title": "The Role of Generative AI in Software Student CollaborAItion",
    "authors": [
      "Natalie Kiesler",
      "Jacqueline Smith",
      "Juho Leinonen",
      "Armando Fox",
      "Stephen MacNeil",
      "Petri Ihantola"
    ],
    "abstract": "Collaboration is a crucial part of computing education. The increase in AI\ncapabilities over the last couple of years is bound to profoundly affect all\naspects of systems and software engineering, including collaboration. In this\nposition paper, we consider a scenario where AI agents would be able to take on\nany role in collaborative processes in computing education. We outline these\nroles, the activities and group dynamics that software development currently\ninclude, and discuss if and in what way AI could facilitate these roles and\nactivities. The goal of our work is to envision and critically examine\npotential futures. We present scenarios suggesting how AI can be integrated\ninto existing collaborations. These are contrasted by design fictions that help\ndemonstrate the new possibilities and challenges for computing education in the\nAI era.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.SE",
    "comment": "7 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2501.14084v1",
    "published_date": "2025-01-23 20:43:05 UTC",
    "updated_date": "2025-01-23 20:43:05 UTC"
  },
  {
    "arxiv_id": "2501.14082v2",
    "title": "Communicating Activations Between Language Model Agents",
    "authors": [
      "Vignav Ramesh",
      "Kenneth Li"
    ],
    "abstract": "Communication between multiple language model (LM) agents has been shown to\nscale up the reasoning ability of LMs. While natural language has been the\ndominant medium for inter-LM communication, it is not obvious this should be\nthe standard: not only does natural language communication incur high inference\ncosts that scale quickly with the number of both agents and messages, but also\nthe decoding process abstracts away too much rich information that could be\notherwise accessed from the internal activations. In this work, we propose a\nsimple technique whereby LMs communicate via activations; concretely, we pause\nan LM $\\textit{B}$'s computation at an intermediate layer, combine its current\nactivation with another LM $\\textit{A}$'s intermediate activation via some\nfunction $\\textit{f}$, then pass $\\textit{f}$'s output into the next layer of\n$\\textit{B}$ and continue the forward pass till decoding is complete. This\napproach scales up LMs on new tasks with zero additional parameters and data,\nand saves a substantial amount of compute over natural language communication.\nWe test our method with various functional forms $\\textit{f}$ on two\nexperimental setups--multi-player coordination games and reasoning\nbenchmarks--and find that it achieves up to $27.0\\%$ improvement over natural\nlanguage communication across datasets with $<$$1/4$ the compute, illustrating\nthe superiority and robustness of activations as an alternative \"language\" for\ncommunication between LMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICML 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14082v2",
    "published_date": "2025-01-23 20:41:07 UTC",
    "updated_date": "2025-05-07 20:03:54 UTC"
  },
  {
    "arxiv_id": "2501.14070v2",
    "title": "Expanding on the BRIAR Dataset: A Comprehensive Whole Body Biometric Recognition Resource at Extreme Distances and Real-World Scenarios (Collections 1-4)",
    "authors": [
      "Gavin Jager",
      "David Cornett III",
      "Gavin Glenn",
      "Deniz Aykac",
      "Christi Johnson",
      "Robert Zhang",
      "Ryan Shivers",
      "David Bolme",
      "Laura Davies",
      "Scott Dolvin",
      "Nell Barber",
      "Joel Brogan",
      "Nick Burchfield",
      "Carl Dukes",
      "Andrew Duncan",
      "Regina Ferrell",
      "Austin Garrett",
      "Jim Goddard",
      "Jairus Hines",
      "Bart Murphy",
      "Sean Pharris",
      "Brandon Stockwell",
      "Leanne Thompson",
      "Matthew Yohe"
    ],
    "abstract": "The state-of-the-art in biometric recognition algorithms and operational\nsystems has advanced quickly in recent years providing high accuracy and\nrobustness in more challenging collection environments and consumer\napplications. However, the technology still suffers greatly when applied to\nnon-conventional settings such as those seen when performing identification at\nextreme distances or from elevated cameras on buildings or mounted to UAVs.\nThis paper summarizes an extension to the largest dataset currently focused on\naddressing these operational challenges, and describes its composition as well\nas methodologies of collection, curation, and annotation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 11 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.14070v2",
    "published_date": "2025-01-23 20:12:56 UTC",
    "updated_date": "2025-02-04 19:48:06 UTC"
  },
  {
    "arxiv_id": "2502.10412v1",
    "title": "Identifying relevant indicators for monitoring a National Artificial Intelligence Strategy",
    "authors": [
      "Renata Pelissari",
      "Ricardo Suyama",
      "Leonardo Tomazeli Duarte",
      "Henrique Sá Earp"
    ],
    "abstract": "How can a National Artificial Intelligence Strategy be effectively monitored?\nTo address this question, we propose a methodology consisting of two key\ncomponents. First, it involves identifying relevant indicators within national\nAI strategies. Second, it assesses the alignment between these indicators and\nthe strategic actions of a specific government's AI strategy, allowing for a\ncritical evaluation of its monitoring measures. Moreover, identifying these\nindicators helps assess the overall quality of the strategy's structure. A lack\nof alignment between strategic actions and the identified indicators may reveal\ngaps or blind spots in the strategy. This methodology is demonstrated using the\nBrazilian AI strategy as a case study.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.10412v1",
    "published_date": "2025-01-23 19:59:31 UTC",
    "updated_date": "2025-01-23 19:59:31 UTC"
  },
  {
    "arxiv_id": "2501.17174v1",
    "title": "Extractive Schema Linking for Text-to-SQL",
    "authors": [
      "Michael Glass",
      "Mustafa Eyceoz",
      "Dharmashankar Subramanian",
      "Gaetano Rossiello",
      "Long Vu",
      "Alfio Gliozzo"
    ],
    "abstract": "Text-to-SQL is emerging as a practical interface for real world databases.\nThe dominant paradigm for Text-to-SQL is cross-database or schema-independent,\nsupporting application schemas unseen during training. The schema of a database\ndefines the tables, columns, column types and foreign key connections between\ntables. Real world schemas can be large, containing hundreds of columns, but\nfor any particular query only a small fraction will be relevant. Placing the\nentire schema in the prompt for an LLM can be impossible for models with\nsmaller token windows and expensive even when the context window is large\nenough to allow it. Even apart from computational considerations, the accuracy\nof the model can be improved by focusing the SQL generation on only the\nrelevant portion of the database. Schema linking identifies the portion of the\ndatabase schema useful for the question. Previous work on schema linking has\nused graph neural networks, generative LLMs, and cross encoder classifiers. We\nintroduce a new approach to adapt decoder-only LLMs to schema linking that is\nboth computationally more efficient and more accurate than the generative\napproach. Additionally our extractive approach permits fine-grained control\nover the precision-recall trade-off for schema linking.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17174v1",
    "published_date": "2025-01-23 19:57:08 UTC",
    "updated_date": "2025-01-23 19:57:08 UTC"
  },
  {
    "arxiv_id": "2501.14051v1",
    "title": "Revisiting CLIP: Efficient Alignment of 3D MRI and Tabular Data using Domain-Specific Foundation Models",
    "authors": [
      "Jakob Krogh Petersen",
      "Valdemar Licht",
      "Mads Nielsen",
      "Asbjørn Munk"
    ],
    "abstract": "Multi-modal models require aligned, shared embedding spaces. However, common\nCLIP-based approaches need large amounts of samples and do not natively support\n3D or tabular data, both of which are crucial in the medical domain. To address\nthese issues, we revisit CLIP-style alignment by training a domain-specific 3D\nfoundation model as an image encoder and demonstrate that modality alignment is\nfeasible with only 62 MRI scans. Our approach is enabled by a simple embedding\naccumulation strategy required for training in 3D, which scales the amount of\nnegative pairs across batches in order to stabilize training. We perform a\nthorough evaluation of various design choices, including the choice of backbone\nand loss functions, and evaluate the proposed methodology on zero-shot\nclassification and image-retrieval tasks. While zero-shot image-retrieval\nremains challenging, zero-shot classification results demonstrate that the\nproposed approach can meaningfully align the representations of 3D MRI with\ntabular data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 2 figures. To be published in ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14051v1",
    "published_date": "2025-01-23 19:34:48 UTC",
    "updated_date": "2025-01-23 19:34:48 UTC"
  },
  {
    "arxiv_id": "2501.14050v2",
    "title": "GraphRAG under Fire",
    "authors": [
      "Jiacheng Liang",
      "Yuhui Wang",
      "Changjiang Li",
      "Rongyi Zhu",
      "Tanqiu Jiang",
      "Neil Gong",
      "Ting Wang"
    ],
    "abstract": "GraphRAG advances retrieval-augmented generation (RAG) by structuring\nexternal knowledge as multi-scale knowledge graphs, enabling language models to\nintegrate both broad context and granular details in their generation. While\nGraphRAG has demonstrated success across domains, its security implications\nremain largely unexplored. To bridge this gap, this work examines GraphRAG's\nvulnerability to poisoning attacks, uncovering an intriguing security paradox:\ncompared to conventional RAG, GraphRAG's graph-based indexing and retrieval\nenhance resilience against simple poisoning attacks; yet, the same features\nalso create new attack surfaces. We present GRAGPoison, a novel attack that\nexploits shared relations in the underlying knowledge graph to craft poisoning\ntext capable of compromising multiple queries simultaneously. GRAGPoison\nemploys three key strategies: i) relation injection to introduce false\nknowledge, ii) relation enhancement to amplify poisoning influence, and iii)\nnarrative generation to embed malicious content within coherent text. Empirical\nevaluation across diverse datasets and models shows that GRAGPoison\nsubstantially outperforms existing attacks in terms of effectiveness (up to\n98\\% success rate) and scalability (using less than 68\\% poisoning text) on\nvarious GraphRAG-based systems. We also explore potential defensive measures\nand their limitations, identifying promising directions for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.14050v2",
    "published_date": "2025-01-23 19:33:16 UTC",
    "updated_date": "2025-04-24 02:38:09 UTC"
  },
  {
    "arxiv_id": "2501.14048v1",
    "title": "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks",
    "authors": [
      "Sneh Pandya",
      "Purvik Patel",
      "Brian D. Nord",
      "Mike Walmsley",
      "Aleksandra Ćiprijanović"
    ],
    "abstract": "Modern neural networks (NNs) often do not generalize well in the presence of\na \"covariate shift\"; that is, in situations where the training and test data\ndistributions differ, but the conditional distribution of classification labels\nremains unchanged. In such cases, NN generalization can be reduced to a problem\nof learning more domain-invariant features. Domain adaptation (DA) methods\ninclude a range of techniques aimed at achieving this; however, these methods\nhave struggled with the need for extensive hyperparameter tuning, which then\nincurs significant computational costs. In this work, we introduce SIDDA, an\nout-of-the-box DA training algorithm built upon the Sinkhorn divergence, that\ncan achieve effective domain alignment with minimal hyperparameter tuning and\ncomputational overhead. We demonstrate the efficacy of our method on multiple\nsimulated and real datasets of varying complexity, including simple shapes,\nhandwritten digits, and real astronomical observations. SIDDA is compatible\nwith a variety of NN architectures, and it works particularly well in improving\nclassification accuracy and model calibration when paired with equivariant\nneural networks (ENNs). We find that SIDDA enhances the generalization\ncapabilities of NNs, achieving up to a $\\approx40\\%$ improvement in\nclassification accuracy on unlabeled target data. We also study the efficacy of\nDA on ENNs with respect to the varying group orders of the dihedral group\n$D_N$, and find that the model performance improves as the degree of\nequivariance increases. Finally, we find that SIDDA enhances model calibration\non both source and target data--achieving over an order of magnitude\nimprovement in the ECE and Brier score. SIDDA's versatility, combined with its\nautomated approach to domain alignment, has the potential to advance\nmulti-dataset studies by enabling the development of highly generalizable\nmodels.",
    "categories": [
      "cs.LG",
      "astro-ph.GA",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 5 figures, 4 tables. code available at:\n  https://github.com/deepskies/SIDDA",
    "pdf_url": "http://arxiv.org/pdf/2501.14048v1",
    "published_date": "2025-01-23 19:29:34 UTC",
    "updated_date": "2025-01-23 19:29:34 UTC"
  },
  {
    "arxiv_id": "2501.14035v1",
    "title": "Human-Alignment Influences the Utility of AI-assisted Decision Making",
    "authors": [
      "Nina L. Corvelo Benz",
      "Manuel Gomez Rodriguez"
    ],
    "abstract": "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14035v1",
    "published_date": "2025-01-23 19:01:47 UTC",
    "updated_date": "2025-01-23 19:01:47 UTC"
  },
  {
    "arxiv_id": "2501.13928v2",
    "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
    "authors": [
      "Jianing Yang",
      "Alexander Sax",
      "Kevin J. Liang",
      "Mikael Henaff",
      "Hao Tang",
      "Ang Cao",
      "Joyce Chai",
      "Franziska Meier",
      "Matt Feiszli"
    ],
    "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision,\nparticularly in applications requiring accurate and scalable representations\nacross diverse perspectives. Current leading methods such as DUSt3R employ a\nfundamentally pairwise approach, processing images in pairs and necessitating\ncostly global alignment procedures to reconstruct from multiple views. In this\nwork, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view\ngeneralization to DUSt3R that achieves efficient and scalable 3D reconstruction\nby processing many views in parallel. Fast3R's Transformer-based architecture\nforwards N images in a single forward pass, bypassing the need for iterative\nalignment. Through extensive experiments on camera pose estimation and 3D\nreconstruction, Fast3R demonstrates state-of-the-art performance, with\nsignificant improvements in inference speed and reduced error accumulation.\nThese results establish Fast3R as a robust alternative for multi-view\napplications, offering enhanced scalability without compromising reconstruction\naccuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project website: https://fast3r-3d.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2501.13928v2",
    "published_date": "2025-01-23 18:59:55 UTC",
    "updated_date": "2025-03-19 19:35:52 UTC"
  },
  {
    "arxiv_id": "2501.13927v1",
    "title": "CRPO: Confidence-Reward Driven Preference Optimization for Machine Translation",
    "authors": [
      "Guofeng Cui",
      "Pichao Wang",
      "Yang Liu",
      "Zemian Ke",
      "Zhu Liu",
      "Vimal Bhat"
    ],
    "abstract": "Large language models (LLMs) have shown great potential in natural language\nprocessing tasks, but their application to machine translation (MT) remains\nchallenging due to pretraining on English-centric data and the complexity of\nreinforcement learning from human feedback (RLHF). Direct Preference\nOptimization (DPO) has emerged as a simpler and more efficient alternative, but\nits performance depends heavily on the quality of preference data. To address\nthis, we propose Confidence-Reward driven Preference Optimization (CRPO), a\nnovel method that combines reward scores with model confidence to improve data\nselection for fine-tuning. CRPO selects challenging sentence pairs where the\nmodel is uncertain or underperforms, leading to more effective learning. While\nprimarily designed for LLMs, CRPO also generalizes to encoder-decoder models\nlike NLLB, demonstrating its versatility. Empirical results show that CRPO\noutperforms existing methods such as RS-DPO, RSO and MBR score in both\ntranslation accuracy and data efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13927v1",
    "published_date": "2025-01-23 18:59:47 UTC",
    "updated_date": "2025-01-23 18:59:47 UTC"
  },
  {
    "arxiv_id": "2501.13926v1",
    "title": "Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step",
    "authors": [
      "Ziyu Guo",
      "Renrui Zhang",
      "Chengzhuo Tong",
      "Zhizheng Zhao",
      "Peng Gao",
      "Hongsheng Li",
      "Pheng-Ann Heng"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning has been extensively explored in large\nmodels to tackle complex understanding tasks. However, it still remains an open\nquestion whether such strategies can be applied to verifying and reinforcing\nimage generation scenarios. In this paper, we provide the first comprehensive\ninvestigation of the potential of CoT reasoning to enhance autoregressive image\ngeneration. We focus on three techniques: scaling test-time computation for\nverification, aligning model preferences with Direct Preference Optimization\n(DPO), and integrating these techniques for complementary effects. Our results\ndemonstrate that these approaches can be effectively adapted and combined to\nsignificantly improve image generation performance. Furthermore, given the\npivotal role of reward models in our findings, we propose the Potential\nAssessment Reward Model (PARM) and PARM++, specialized for autoregressive image\ngeneration. PARM adaptively assesses each generation step through a potential\nassessment approach, merging the strengths of existing reward models, and\nPARM++ further introduces a reflection mechanism to self-correct the generated\nunsatisfactory image. Using our investigated reasoning strategies, we enhance a\nbaseline model, Show-o, to achieve superior results, with a significant +24%\nimprovement on the GenEval benchmark, surpassing Stable Diffusion 3 by +15%. We\nhope our study provides unique insights and paves a new path for integrating\nCoT reasoning with autoregressive image generation. Code and models are\nreleased at https://github.com/ZiyuGuo99/Image-Generation-CoT",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Journal Version. Code and models are released at\n  https://github.com/ZiyuGuo99/Image-Generation-CoT",
    "pdf_url": "http://arxiv.org/pdf/2501.13926v1",
    "published_date": "2025-01-23 18:59:43 UTC",
    "updated_date": "2025-01-23 18:59:43 UTC"
  },
  {
    "arxiv_id": "2501.13924v1",
    "title": "Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization",
    "authors": [
      "Hao Dong",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "abstract": "Test-time adaptation (TTA) has demonstrated significant potential in\naddressing distribution shifts between training and testing data. Open-set\ntest-time adaptation (OSTTA) aims to adapt a source pre-trained model online to\nan unlabeled target domain that contains unknown classes. This task becomes\nmore challenging when multiple modalities are involved. Existing methods have\nprimarily focused on unimodal OSTTA, often filtering out low-confidence samples\nwithout addressing the complexities of multimodal data. In this work, we\npresent Adaptive Entropy-aware Optimization (AEO), a novel framework\nspecifically designed to tackle Multimodal Open-set Test-time Adaptation\n(MM-OSTTA) for the first time. Our analysis shows that the entropy difference\nbetween known and unknown samples in the target domain strongly correlates with\nMM-OSTTA performance. To leverage this, we propose two key components:\nUnknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality\nPrediction Discrepancy Optimization (AMP). These components enhance the ability\nof model to distinguish unknown class samples during online adaptation by\namplifying the entropy difference between known and unknown samples. To\nthoroughly evaluate our proposed methods in the MM-OSTTA setting, we establish\na new benchmark derived from existing datasets. This benchmark includes two\ndownstream tasks and incorporates five modalities. Extensive experiments across\nvarious domain shift situations demonstrate the efficacy and versatility of the\nAEO framework. Additionally, we highlight the strong performance of AEO in\nlong-term and continual MM-OSTTA settings, both of which are challenging and\nhighly relevant to real-world applications. Our source code is available at\nhttps://github.com/donghao51/AEO.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13924v1",
    "published_date": "2025-01-23 18:59:30 UTC",
    "updated_date": "2025-01-23 18:59:30 UTC"
  },
  {
    "arxiv_id": "2501.13919v2",
    "title": "Temporal Preference Optimization for Long-Form Video Understanding",
    "authors": [
      "Rui Li",
      "Xiaohan Wang",
      "Yuhui Zhang",
      "Zeyu Wang",
      "Serena Yeung-Levy"
    ],
    "abstract": "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13919v2",
    "published_date": "2025-01-23 18:58:03 UTC",
    "updated_date": "2025-01-30 17:35:08 UTC"
  },
  {
    "arxiv_id": "2501.13918v1",
    "title": "Improving Video Generation with Human Feedback",
    "authors": [
      "Jie Liu",
      "Gongye Liu",
      "Jiajun Liang",
      "Ziyang Yuan",
      "Xiaokun Liu",
      "Mingwu Zheng",
      "Xiele Wu",
      "Qiulin Wang",
      "Wenyu Qin",
      "Menghan Xia",
      "Xintao Wang",
      "Xiaohong Liu",
      "Fei Yang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Yujiu Yang",
      "Wanli Ouyang"
    ],
    "abstract": "Video generation has achieved significant advances through rectified flow\ntechniques, but issues like unsmooth motion and misalignment between videos and\nprompts persist. In this work, we develop a systematic pipeline that harnesses\nhuman feedback to mitigate these problems and refine the video generation\nmodel. Specifically, we begin by constructing a large-scale human preference\ndataset focused on modern video generation models, incorporating pairwise\nannotations across multi-dimensions. We then introduce VideoReward, a\nmulti-dimensional video reward model, and examine how annotations and various\ndesign choices impact its rewarding efficacy. From a unified reinforcement\nlearning perspective aimed at maximizing reward with KL regularization, we\nintroduce three alignment algorithms for flow-based models by extending those\nfrom diffusion models. These include two training-time strategies: direct\npreference optimization for flow (Flow-DPO) and reward weighted regression for\nflow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies\nreward guidance directly to noisy videos. Experimental results indicate that\nVideoReward significantly outperforms existing reward models, and Flow-DPO\ndemonstrates superior performance compared to both Flow-RWR and standard\nsupervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom\nweights to multiple objectives during inference, meeting personalized video\nquality needs. Project page: https://gongyeliu.github.io/videoalign.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13918v1",
    "published_date": "2025-01-23 18:55:41 UTC",
    "updated_date": "2025-01-23 18:55:41 UTC"
  },
  {
    "arxiv_id": "2501.14013v1",
    "title": "Leveraging Multiphase CT for Quality Enhancement of Portal Venous CT: Utility for Pancreas Segmentation",
    "authors": [
      "Xinya Wang",
      "Tejas Sudharshan Mathai",
      "Boah Kim",
      "Ronald M. Summers"
    ],
    "abstract": "Multiphase CT studies are routinely obtained in clinical practice for\ndiagnosis and management of various diseases, such as cancer. However, the CT\nstudies can be acquired with low radiation doses, different scanners, and are\nfrequently affected by motion and metal artifacts. Prior approaches have\ntargeted the quality improvement of one specific CT phase (e.g., non-contrast\nCT). In this work, we hypothesized that leveraging multiple CT phases for the\nquality enhancement of one phase may prove advantageous for downstream tasks,\nsuch as segmentation. A 3D progressive fusion and non-local (PFNL) network was\ndeveloped. It was trained with three degraded (low-quality) phases\n(non-contrast, arterial, and portal venous) to enhance the quality of the\nportal venous phase. Then, the effect of scan quality enhancement was evaluated\nusing a proxy task of pancreas segmentation, which is useful for tracking\npancreatic cancer. The proposed approach improved the pancreas segmentation by\n3% over the corresponding low-quality CT scan. To the best of our knowledge, we\nare the first to harness multiphase CT for scan quality enhancement and\nimproved pancreas segmentation.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "92C55",
      "I.4.6"
    ],
    "primary_category": "eess.IV",
    "comment": "ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14013v1",
    "published_date": "2025-01-23 18:45:24 UTC",
    "updated_date": "2025-01-23 18:45:24 UTC"
  },
  {
    "arxiv_id": "2501.14012v3",
    "title": "Transfer Learning of Surrogate Models via Domain Affine Transformation Across Synthetic and Real-World Benchmarks",
    "authors": [
      "Shuaiqun Pan",
      "Diederick Vermetten",
      "Manuel López-Ibáñez",
      "Thomas Bäck",
      "Hao Wang"
    ],
    "abstract": "Surrogate models are frequently employed as efficient substitutes for the\ncostly execution of real-world processes. However, constructing a high-quality\nsurrogate model often demands extensive data acquisition. A solution to this\nissue is to transfer pre-trained surrogate models for new tasks, provided that\ncertain invariances exist between tasks. This study focuses on transferring\nnon-differentiable surrogate models (e.g., random forests) from a source\nfunction to a target function, where we assume their domains are related by an\nunknown affine transformation, using only a limited amount of transfer data\npoints evaluated on the target. Previous research attempts to tackle this\nchallenge for differentiable models, e.g., Gaussian process regression, which\nminimizes the empirical loss on the transfer data by tuning the affine\ntransformations. In this paper, we extend the previous work to the random\nforest and assess its effectiveness on a widely-used artificial problem set -\nBlack-Box Optimization Benchmark (BBOB) testbed, and on four real-world\ntransfer learning problems. The results highlight the significant practical\nadvantages of the proposed method, particularly in reducing both the data\nrequirements and computational costs of training surrogate models for complex\nreal-world scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14012v3",
    "published_date": "2025-01-23 18:44:25 UTC",
    "updated_date": "2025-05-02 09:04:45 UTC"
  },
  {
    "arxiv_id": "2501.13898v1",
    "title": "PointOBB-v3: Expanding Performance Boundaries of Single Point-Supervised Oriented Object Detection",
    "authors": [
      "Peiyuan Zhang",
      "Junwei Luo",
      "Xue Yang",
      "Yi Yu",
      "Qingyun Li",
      "Yue Zhou",
      "Xiaosong Jia",
      "Xudong Lu",
      "Jingdong Chen",
      "Xiang Li",
      "Junchi Yan",
      "Yansheng Li"
    ],
    "abstract": "With the growing demand for oriented object detection (OOD), recent studies\non point-supervised OOD have attracted significant interest. In this paper, we\npropose PointOBB-v3, a stronger single point-supervised OOD framework. Compared\nto existing methods, it generates pseudo rotated boxes without additional\npriors and incorporates support for the end-to-end paradigm. PointOBB-v3\nfunctions by integrating three unique image views: the original view, a resized\nview, and a rotated/flipped (rot/flp) view. Based on the views, a scale\naugmentation module and an angle acquisition module are constructed. In the\nfirst module, a Scale-Sensitive Consistency (SSC) loss and a Scale-Sensitive\nFeature Fusion (SSFF) module are introduced to improve the model's ability to\nestimate object scale. To achieve precise angle predictions, the second module\nemploys symmetry-based self-supervised learning. Additionally, we introduce an\nend-to-end version that eliminates the pseudo-label generation process by\nintegrating a detector branch and introduces an Instance-Aware Weighting (IAW)\nstrategy to focus on high-quality predictions. We conducted extensive\nexperiments on the DIOR-R, DOTA-v1.0/v1.5/v2.0, FAIR1M, STAR, and RSAR\ndatasets. Across all these datasets, our method achieves an average improvement\nin accuracy of 3.56% in comparison to previous state-of-the-art methods. The\ncode will be available at https://github.com/ZpyWHU/PointOBB-v3.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 5 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.13898v1",
    "published_date": "2025-01-23 18:18:15 UTC",
    "updated_date": "2025-01-23 18:18:15 UTC"
  },
  {
    "arxiv_id": "2501.13896v2",
    "title": "GUI-Bee: Align GUI Action Grounding to Novel Environments via Autonomous Exploration",
    "authors": [
      "Yue Fan",
      "Handong Zhao",
      "Ruiyi Zhang",
      "Yu Shen",
      "Xin Eric Wang",
      "Gang Wu"
    ],
    "abstract": "Graphical User Interface (GUI) action grounding is a critical step in GUI\nautomation that maps language instructions to actionable elements on GUI\nscreens. Most recent works of GUI action grounding leverage large GUI datasets\nto fine-tune MLLMs. However, the fine-tuning data always covers limited GUI\nenvironments, and we find the performance of the resulting model deteriorates\nin novel environments. We argue that the GUI grounding models should be further\naligned to the novel environments to reveal their full potential, when the\ninference is known to involve novel environments, i.e., environments not used\nduring the previous fine-tuning. To realize this, we first propose GUI-Bee, an\nMLLM-based autonomous agent, to collect high-quality, environment-specific data\nthrough exploration and then continuously fine-tune GUI grounding models with\nthe collected data. Our agent leverages a novel Q-value-Incentive In-Context\nReinforcement Learning (Q-ICRL) method to optimize exploration efficiency and\ndata quality. Additionally, we introduce NovelScreenSpot, a benchmark for\ntesting how well the data can help align GUI action grounding models to novel\nenvironments and demonstrate the effectiveness of data collected by GUI-Bee in\nthe experiments. Furthermore, we conduct an ablation study to validate the\nQ-ICRL method in enhancing the efficiency of GUI-Bee. Project page:\nhttps://gui-bee.github.io",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13896v2",
    "published_date": "2025-01-23 18:16:21 UTC",
    "updated_date": "2025-01-27 18:58:42 UTC"
  },
  {
    "arxiv_id": "2501.13893v1",
    "title": "Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning",
    "authors": [
      "Zuyao You",
      "Junke Wang",
      "Lingyu Kong",
      "Bo He",
      "Zuxuan Wu"
    ],
    "abstract": "We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset\ndesigned to advance fine-grained visual understanding. To achieve this, we\ncarefully design an automated annotation pipeline that prompts GPT-4V to\ngenerate pixel-aligned, instance-specific captions for individual objects\nwithin images, enabling models to learn more granular relationships between\nobjects and their contexts. This approach results in 167,254 detailed captions,\nwith an average of 22.94 words per caption. Building on Pix2Cap-COCO, we\nintroduce a novel task, panoptic segmentation-captioning, which challenges\nmodels to recognize instances in an image and provide detailed descriptions for\neach simultaneously. To benchmark this task, we design a robust baseline based\non X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a\nparticularly challenging dataset, as it requires models to excel in both\nfine-grained visual understanding and detailed language generation.\nFurthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large\nmultimodal models (LMMs) to enhance their performance. For example, training\nwith Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding\ngains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset,\nand strengthens its region understanding ability on the ViP-BENCH, with an\noverall improvement of +5.1%, including notable increases in recognition\naccuracy +11.2% and language generation quality +22.2%.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13893v1",
    "published_date": "2025-01-23 18:08:57 UTC",
    "updated_date": "2025-01-23 18:08:57 UTC"
  },
  {
    "arxiv_id": "2501.14836v1",
    "title": "Symbolic Knowledge Extraction and Injection with Sub-symbolic Predictors: A Systematic Literature Review",
    "authors": [
      "Giovanni Ciatto",
      "Federico Sabbatini",
      "Andrea Agiollo",
      "Matteo Magnini",
      "Andrea Omicini"
    ],
    "abstract": "In this paper we focus on the opacity issue of sub-symbolic machine learning\npredictors by promoting two complementary activities, namely, symbolic\nknowledge extraction (SKE) and injection (SKI) from and into sub-symbolic\npredictors. We consider as symbolic any language being intelligible and\ninterpretable for both humans and computers. Accordingly, we propose general\nmeta-models for both SKE and SKI, along with two taxonomies for the\nclassification of SKE and SKI methods. By adopting an explainable artificial\nintelligence (XAI) perspective, we highlight how such methods can be exploited\nto mitigate the aforementioned opacity issue. Our taxonomies are attained by\nsurveying and classifying existing methods from the literature, following a\nsystematic approach, and by generalising the results of previous surveys\ntargeting specific sub-topics of either SKE or SKI alone. More precisely, we\nanalyse 132 methods for SKE and 117 methods for SKI, and we categorise them\naccording to their purpose, operation, expected input/output data and predictor\ntypes. For each method, we also indicate the presence/lack of runnable software\nimplementations. Our work may be of interest for data scientists aiming at\nselecting the most adequate SKE/SKI method for their needs, and also work as\nsuggestions for researchers interested in filling the gaps of the current state\nof the art, as well as for developers willing to implement SKE/SKI-based\ntechnologies.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14836v1",
    "published_date": "2025-01-23 18:00:05 UTC",
    "updated_date": "2025-01-23 18:00:05 UTC"
  },
  {
    "arxiv_id": "2501.13884v1",
    "title": "Exploring Finetuned Audio-LLM on Heart Murmur Features",
    "authors": [
      "Adrian Florea",
      "Xilin Jiang",
      "Nima Mesgarani",
      "Xiaofan Jiang"
    ],
    "abstract": "Large language models (LLMs) for audio have excelled in recognizing and\nanalyzing human speech, music, and environmental sounds. However, their\npotential for understanding other types of sounds, particularly biomedical\nsounds, remains largely underexplored despite significant scientific interest.\nIn this study, we focus on diagnosing cardiovascular diseases using\nphonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN)\nparadigms are restricted to heart murmur classification (healthy vs unhealthy)\nand do not predict other acoustic features of the murmur such as timing,\ngrading, harshness, pitch, and quality, which are important in helping\nphysicians diagnose the underlying heart conditions. We propose to finetune an\naudio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG)\ndataset and evaluate its performance in classifying 11 expert-labeled murmur\nfeatures. Additionally, we aim to achieve more noise-robust and generalizable\nsystem by exploring a preprocessing segmentation algorithm using an audio\nrepresentation model, SSAMBA. Our results indicate that the LLM-based model\noutperforms state-of-the-art methods in 8 of the 11 features and performs\ncomparably in the remaining 3. Moreover, the LLM successfully classifies\nlong-tail murmur features with limited training data, a task that all previous\nmethods have failed to classify. These findings underscore the potential of\naudio LLMs as assistants to human cardiologists in enhancing heart disease\ndiagnosis.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "5 pages, 1 figure, and 3 tables. Submitted to IEEE/ACM Conference on\n  Connected Health: Applications, Systems , and Engineering Technologies",
    "pdf_url": "http://arxiv.org/pdf/2501.13884v1",
    "published_date": "2025-01-23 17:57:18 UTC",
    "updated_date": "2025-01-23 17:57:18 UTC"
  },
  {
    "arxiv_id": "2501.13864v1",
    "title": "Autoencoders for Anomaly Detection are Unreliable",
    "authors": [
      "Roel Bouman",
      "Tom Heskes"
    ],
    "abstract": "Autoencoders are frequently used for anomaly detection, both in the\nunsupervised and semi-supervised settings. They rely on the assumption that\nwhen trained using the reconstruction loss, they will be able to reconstruct\nnormal data more accurately than anomalous data. Some recent works have posited\nthat this assumption may not always hold, but little has been done to study the\nvalidity of the assumption in theory. In this work we show that this assumption\nindeed does not hold, and illustrate that anomalies, lying far away from normal\ndata, can be perfectly reconstructed in practice. We revisit the theory of\nfailure of linear autoencoders for anomaly detection by showing how they can\nperfectly reconstruct out of bounds, or extrapolate undesirably, and note how\nthis can be dangerous in safety critical applications. We connect this to\nnon-linear autoencoders through experiments on both tabular data and real-world\nimage data, the two primary application areas of autoencoders for anomaly\ndetection.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13864v1",
    "published_date": "2025-01-23 17:36:48 UTC",
    "updated_date": "2025-01-23 17:36:48 UTC"
  },
  {
    "arxiv_id": "2501.13848v1",
    "title": "Where Do You Go? Pedestrian Trajectory Prediction using Scene Features",
    "authors": [
      "Mohammad Ali Rezaei",
      "Fardin Ayar",
      "Ehsan Javanmardi",
      "Manabu Tsukada",
      "Mahdi Javanmardi"
    ],
    "abstract": "Accurate prediction of pedestrian trajectories is crucial for enhancing the\nsafety of autonomous vehicles and reducing traffic fatalities involving\npedestrians. While numerous studies have focused on modeling interactions among\npedestrians to forecast their movements, the influence of environmental factors\nand scene-object placements has been comparatively underexplored. In this\npaper, we present a novel trajectory prediction model that integrates both\npedestrian interactions and environmental context to improve prediction\naccuracy. Our approach captures spatial and temporal interactions among\npedestrians within a sparse graph framework. To account for pedestrian-scene\ninteractions, we employ advanced image enhancement and semantic segmentation\ntechniques to extract detailed scene features. These scene and interaction\nfeatures are then fused through a cross-attention mechanism, enabling the model\nto prioritize relevant environmental factors that influence pedestrian\nmovements. Finally, a temporal convolutional network processes the fused\nfeatures to predict future pedestrian trajectories. Experimental results\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches, achieving ADE and FDE values of 0.252 and 0.372 meters,\nrespectively, underscoring the importance of incorporating both social\ninteractions and environmental context in pedestrian trajectory prediction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by 2024 International Conference on Intelligent Computing\n  and its Emerging Applications",
    "pdf_url": "http://arxiv.org/pdf/2501.13848v1",
    "published_date": "2025-01-23 17:15:26 UTC",
    "updated_date": "2025-01-23 17:15:26 UTC"
  },
  {
    "arxiv_id": "2501.13833v1",
    "title": "On the Reasoning Capacity of AI Models and How to Quantify It",
    "authors": [
      "Santosh Kumar Radha",
      "Oktay Goktas"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have intensified the debate\nsurrounding the fundamental nature of their reasoning capabilities. While\nachieving high performance on benchmarks such as GPQA and MMLU, these models\nexhibit limitations in more complex reasoning tasks, highlighting the need for\nmore rigorous evaluation methodologies. We propose a novel phenomenological\napproach that goes beyond traditional accuracy metrics to probe the underlying\nmechanisms of model behavior, establishing a framework that could broadly\nimpact how we analyze and understand AI systems. Using positional bias in\nmultiple-choice reasoning tasks as a case study, we demonstrate how systematic\nperturbations can reveal fundamental aspects of model decision-making. To\nanalyze these behaviors, we develop two complementary phenomenological models:\na Probabilistic Mixture Model (PMM) that decomposes model responses into\nreasoning, memorization, and guessing components and an Information-Theoretic\nConsistency (ITC) analysis that quantifies the relationship between model\nconfidence and strategy selection. Through controlled experiments on reasoning\nbenchmarks, we show that true reasoning remains challenging for current models,\nwith apparent success often relying on sophisticated combinations of\nmemorization and pattern matching rather than genuine logical deduction. More\nfundamentally, we demonstrate that accuracy alone often overstates a model's\nreasoning abilities, as model behavior can be characterized through underlying\nmechanisms in the phase space of cognitive strategies, revealing how models\ndynamically balance different approaches when responding to queries. This\nframework enables quantitative criteria for real-world deployments, allowing\napplications to specify reliability thresholds based on strategy distributions\nrather than aggregate performance metrics.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13833v1",
    "published_date": "2025-01-23 16:58:18 UTC",
    "updated_date": "2025-01-23 16:58:18 UTC"
  },
  {
    "arxiv_id": "2501.13831v1",
    "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing",
    "authors": [
      "Hao Zhang",
      "Felix Stahlberg",
      "Shankar Kumar"
    ],
    "abstract": "Large Language Models (LLMs) excel at rewriting tasks such as text style\ntransfer and grammatical error correction. While there is considerable overlap\nbetween the inputs and outputs in these tasks, the decoding cost still\nincreases with output length, regardless of the amount of overlap. By\nleveraging the overlap between the input and the output, Kaneko and Okazaki\n(2023) proposed model-agnostic edit span representations to compress the\nrewrites to save computation. They reported an output length reduction rate of\nnearly 80% with minimal accuracy impact in four rewriting tasks. In this paper,\nwe propose alternative edit phrase representations inspired by phrase-based\nstatistical machine translation. We systematically compare our phrasal\nrepresentations with their span representations. We apply the LLM rewriting\nmodel to the task of Automatic Speech Recognition (ASR) post editing and show\nthat our target-phrase-only edit representation has the best\nefficiency-accuracy trade-off. On the LibriSpeech test set, our method closes\n50-60% of the WER gap between the edit span model and the full rewrite model\nwhile losing only 10-20% of the length reduction rate of the edit span model.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted by ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13831v1",
    "published_date": "2025-01-23 16:54:27 UTC",
    "updated_date": "2025-01-23 16:54:27 UTC"
  },
  {
    "arxiv_id": "2501.13830v1",
    "title": "A space-decoupling framework for optimization on bounded-rank matrices with orthogonally invariant constraints",
    "authors": [
      "Yan Yang",
      "Bin Gao",
      "Ya-xiang Yuan"
    ],
    "abstract": "Imposing additional constraints on low-rank optimization has garnered growing\ninterest. However, the geometry of coupled constraints hampers the\nwell-developed low-rank structure and makes the problem intricate. To this end,\nwe propose a space-decoupling framework for optimization on bounded-rank\nmatrices with orthogonally invariant constraints. The ``space-decoupling\" is\nreflected in several ways. We show that the tangent cone of coupled constraints\nis the intersection of tangent cones of each constraint. Moreover, we decouple\nthe intertwined bounded-rank and orthogonally invariant constraints into two\nspaces, leading to optimization on a smooth manifold. Implementing Riemannian\nalgorithms on this manifold is painless as long as the geometry of additional\nconstraints is known. In addition, we unveil the equivalence between the\nreformulated problem and the original problem. Numerical experiments on\nreal-world applications -- spherical data fitting, graph similarity measuring,\nlow-rank SDP, model reduction of Markov processes, reinforcement learning, and\ndeep learning -- validate the superiority of the proposed framework.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "48 pages, 12 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.13830v1",
    "published_date": "2025-01-23 16:54:03 UTC",
    "updated_date": "2025-01-23 16:54:03 UTC"
  },
  {
    "arxiv_id": "2501.14009v2",
    "title": "Scalable and Interpretable Verification of Image-based Neural Network Controllers for Autonomous Vehicles",
    "authors": [
      "Aditya Parameshwaran",
      "Yue Wang"
    ],
    "abstract": "Existing formal verification methods for image-based neural network\ncontrollers in autonomous vehicles often struggle with high-dimensional inputs,\ncomputational inefficiency, and a lack of explainability. These challenges make\nit difficult to ensure safety and reliability, as processing high-dimensional\nimage data is computationally intensive and neural networks are typically\ntreated as black boxes. To address these issues, we propose SEVIN (Scalable and\nExplainable Verification of Image-Based Neural Network Controllers), a\nframework that leverages a Variational Autoencoders (VAE) to encode\nhigh-dimensional images into a lower-dimensional, explainable latent space. By\nannotating latent variables with corresponding control actions, we generate\nconvex polytopes that serve as structured input spaces for verification,\nsignificantly reducing computational complexity and enhancing scalability.\nIntegrating the VAE's decoder with the neural network controller allows for\nformal and robustness verification using these explainable polytopes. Our\napproach also incorporates robustness verification under real-world\nperturbations by augmenting the dataset and retraining the VAE to capture\nenvironmental variations. Experimental results demonstrate that SEVIN achieves\nefficient and scalable verification while providing explainable insights into\ncontroller behavior, bridging the gap between formal verification techniques\nand practical applications in safety-critical systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14009v2",
    "published_date": "2025-01-23 16:46:45 UTC",
    "updated_date": "2025-03-17 18:01:53 UTC"
  },
  {
    "arxiv_id": "2501.13824v1",
    "title": "Hallucinations Can Improve Large Language Models in Drug Discovery",
    "authors": [
      "Shuzhou Yuan",
      "Michael Färber"
    ],
    "abstract": "Concerns about hallucinations in Large Language Models (LLMs) have been\nraised by researchers, yet their potential in areas where creativity is vital,\nsuch as drug discovery, merits exploration. In this paper, we come up with the\nhypothesis that hallucinations can improve LLMs in drug discovery. To verify\nthis hypothesis, we use LLMs to describe the SMILES string of molecules in\nnatural language and then incorporate these descriptions as part of the prompt\nto address specific tasks in drug discovery. Evaluated on seven LLMs and five\nclassification tasks, our findings confirm the hypothesis: LLMs can achieve\nbetter performance with text containing hallucinations. Notably, Llama-3.1-8B\nachieves an 18.35% gain in ROC-AUC compared to the baseline without\nhallucination. Furthermore, hallucinations generated by GPT-4o provide the most\nconsistent improvements across models. Additionally, we conduct empirical\nanalyses and a case study to investigate key factors affecting performance and\nthe underlying reasons. Our research sheds light on the potential use of\nhallucinations for LLMs and offers new perspectives for future research\nleveraging LLMs in drug discovery.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13824v1",
    "published_date": "2025-01-23 16:45:51 UTC",
    "updated_date": "2025-01-23 16:45:51 UTC"
  },
  {
    "arxiv_id": "2501.13818v1",
    "title": "Ensuring Medical AI Safety: Explainable AI-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data",
    "authors": [
      "Frederik Pahde",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "abstract": "Deep neural networks are increasingly employed in high-stakes medical\napplications, despite their tendency for shortcut learning in the presence of\nspurious correlations, which can have potentially fatal consequences in\npractice. Detecting and mitigating shortcut behavior is a challenging task that\noften requires significant labeling efforts from domain experts. To alleviate\nthis problem, we introduce a semi-automated framework for the identification of\nspurious behavior from both data and model perspective by leveraging insights\nfrom eXplainable Artificial Intelligence (XAI). This allows the retrieval of\nspurious data points and the detection of model circuits that encode the\nassociated prediction rules. Moreover, we demonstrate how these shortcut\nencodings can be used for XAI-based sample- and pixel-level data annotation,\nproviding valuable information for bias mitigation methods to unlearn the\nundesired shortcut behavior. We show the applicability of our framework using\nfour medical datasets across two modalities, featuring controlled and\nreal-world spurious correlations caused by data artifacts. We successfully\nidentify and mitigate these biases in VGG16, ResNet50, and contemporary Vision\nTransformer models, ultimately increasing their robustness and applicability\nfor real-world medical tasks.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13818v1",
    "published_date": "2025-01-23 16:39:09 UTC",
    "updated_date": "2025-01-23 16:39:09 UTC"
  },
  {
    "arxiv_id": "2502.05186v1",
    "title": "Multimodal Stock Price Prediction",
    "authors": [
      "Furkan Karadaş",
      "Bahaeddin Eravcı",
      "Ahmet Murat Özbayoğlu"
    ],
    "abstract": "In an era where financial markets are heavily influenced by many static and\ndynamic factors, it has become increasingly critical to carefully integrate\ndiverse data sources with machine learning for accurate stock price prediction.\nThis paper explores a multimodal machine learning approach for stock price\nprediction by combining data from diverse sources, including traditional\nfinancial metrics, tweets, and news articles. We capture real-time market\ndynamics and investor mood through sentiment analysis on these textual data\nusing both ChatGPT-4o and FinBERT models. We look at how these integrated data\nstreams augment predictions made with a standard Long Short-Term Memory (LSTM\nmodel) to illustrate the extent of performance gains. Our study's results\nindicate that incorporating the mentioned data sources considerably increases\nthe forecast effectiveness of the reference model by up to 5%. We also provide\ninsights into the individual and combined predictive capacities of these\nmodalities, highlighting the substantial impact of incorporating sentiment\nanalysis from tweets and news articles. This research offers a systematic and\neffective framework for applying multimodal data analytics techniques in\nfinancial time series forecasting that provides a new view for investors to\nleverage data for decision-making.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST",
    "comment": "9 pages, 6 table",
    "pdf_url": "http://arxiv.org/pdf/2502.05186v1",
    "published_date": "2025-01-23 16:38:46 UTC",
    "updated_date": "2025-01-23 16:38:46 UTC"
  },
  {
    "arxiv_id": "2501.13810v2",
    "title": "Learning to Help in Multi-Class Settings",
    "authors": [
      "Yu Wu",
      "Yansong Li",
      "Zeyu Dong",
      "Nitya Sathyavageeswaran",
      "Anand D. Sarwate"
    ],
    "abstract": "Deploying complex machine learning models on resource-constrained devices is\nchallenging due to limited computational power, memory, and model\nretrainability. To address these limitations, a hybrid system can be\nestablished by augmenting the local model with a server-side model, where\nsamples are selectively deferred by a rejector and then sent to the server for\nprocessing. The hybrid system enables efficient use of computational resources\nwhile minimizing the overhead associated with server usage. The recently\nproposed Learning to Help (L2H) model trains a server model given a fixed local\n(client) model, differing from the Learning to Defer (L2D) framework, which\ntrains the client for a fixed (expert) server. In both L2D and L2H, the\ntraining includes learning a rejector at the client to determine when to query\nthe server. In this work, we extend the L2H model from binary to multi-class\nclassification problems and demonstrate its applicability in a number of\ndifferent scenarios of practical interest in which access to the server may be\nlimited by cost, availability, or policy. We derive a stage-switching surrogate\nloss function that is differentiable, convex, and consistent with the Bayes\nrule corresponding to the 0-1 loss for the L2H model. Experiments show that our\nproposed methods offer an efficient and practical solution for multi-class\nclassification in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 7 figures, conference, ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13810v2",
    "published_date": "2025-01-23 16:32:01 UTC",
    "updated_date": "2025-04-17 03:05:03 UTC"
  },
  {
    "arxiv_id": "2501.13787v1",
    "title": "Parameter-Efficient Fine-Tuning for Foundation Models",
    "authors": [
      "Dan Zhang",
      "Tao Feng",
      "Lilong Xue",
      "Yuandong Wang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "This survey delves into the realm of Parameter-Efficient Fine-Tuning (PEFT)\nwithin the context of Foundation Models (FMs). PEFT, a cost-effective\nfine-tuning technique, minimizes parameters and computational complexity while\nstriving for optimal downstream task performance. FMs, like ChatGPT, DALL-E,\nand LLaVA specialize in language understanding, generative tasks, and\nmultimodal tasks, trained on diverse datasets spanning text, images, and\nvideos. The diversity of FMs guides various adaptation strategies for PEFT.\nTherefore, this survey aims to provide a comprehensive overview of PEFT\ntechniques applied to diverse FMs and address critical gaps in understanding\nthe techniques, trends, and applications. We start by providing a detailed\ndevelopment of FMs and PEFT. Subsequently, we systematically review the key\ncategories and core mechanisms of PEFT across diverse FMs to offer a\ncomprehensive understanding of trends. We also explore the most recent\napplications across various FMs to demonstrate the versatility of PEFT,\nshedding light on the integration of systematic PEFT methods with a range of\nFMs. Furthermore, we identify potential research and development directions for\nimproving PEFTs in the future. This survey provides a valuable resource for\nboth newcomers and experts seeking to understand and use the power of PEFT\nacross FMs. All reviewed papers are listed at\n\\url{https://github.com/THUDM/Awesome-Parameter-Efficient-Fine-Tuning-for-Foundation-Models}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "25 pages, 6 figures, 7 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.13787v1",
    "published_date": "2025-01-23 16:04:23 UTC",
    "updated_date": "2025-01-23 16:04:23 UTC"
  },
  {
    "arxiv_id": "2501.13782v1",
    "title": "Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems",
    "authors": [
      "Ping He",
      "Lorenzo Cavallaro",
      "Shouling Ji"
    ],
    "abstract": "Android malware presents a persistent threat to users' privacy and data\nintegrity. To combat this, researchers have proposed machine learning-based\n(ML-based) Android malware detection (AMD) systems. However, adversarial\nAndroid malware attacks compromise the detection integrity of the ML-based AMD\nsystems, raising significant concerns. Existing defenses against adversarial\nAndroid malware provide protections against feature space attacks which\ngenerate adversarial feature vectors only, leaving protection against realistic\nthreats from problem space attacks which generate real adversarial malware an\nopen problem. In this paper, we address this gap by proposing ADD, a practical\nadversarial Android malware defense framework designed as a plug-in to enhance\nthe adversarial robustness of the ML-based AMD systems against problem space\nattacks. Our extensive evaluation across various ML-based AMD systems\ndemonstrates that ADD is effective against state-of-the-art problem space\nadversarial Android malware attacks. Additionally, ADD shows the defense\neffectiveness in enhancing the adversarial robustness of real-world antivirus\nsolutions.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13782v1",
    "published_date": "2025-01-23 15:59:01 UTC",
    "updated_date": "2025-01-23 15:59:01 UTC"
  },
  {
    "arxiv_id": "2501.13779v1",
    "title": "Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling",
    "authors": [
      "Tanya Rodchenko",
      "Natasha Noy",
      "Nino Scherrer",
      "Jennifer Prendki"
    ],
    "abstract": "While Large Language Models require more and more data to train and scale,\nrather than looking for any data to acquire, we should consider what types of\ntasks are more likely to benefit from data scaling. We should be intentional in\nour data acquisition. We argue that the topology of data itself informs which\ntasks to prioritize in data scaling, and shapes the development of the next\ngeneration of compute paradigms for tasks where data scaling is inefficient, or\neven insufficient.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13779v1",
    "published_date": "2025-01-23 15:58:14 UTC",
    "updated_date": "2025-01-23 15:58:14 UTC"
  },
  {
    "arxiv_id": "2501.13772v1",
    "title": "Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits on Large Audio Language Models in Jailbreak",
    "authors": [
      "Erjia Xiao",
      "Hao Cheng",
      "Jing Shao",
      "Jinhao Duan",
      "Kaidi Xu",
      "Le Yang",
      "Jindong Gu",
      "Renjing Xu"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate remarkable zero-shot performance\nacross various natural language processing tasks. The integration of multimodal\nencoders extends their capabilities, enabling the development of Multimodal\nLarge Language Models that process vision, audio, and text. However, these\ncapabilities also raise significant security concerns, as these models can be\nmanipulated to generate harmful or inappropriate content through jailbreak.\nWhile extensive research explores the impact of modality-specific input edits\non text-based LLMs and Large Vision-Language Models in jailbreak, the effects\nof audio-specific edits on Large Audio-Language Models (LALMs) remain\nunderexplored. Hence, this paper addresses this gap by investigating how\naudio-specific edits influence LALMs inference regarding jailbreak. We\nintroduce the Audio Editing Toolbox (AET), which enables audio-modality edits\nsuch as tone adjustment, word emphasis, and noise injection, and the Edited\nAudio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also\nconduct extensive evaluations of state-of-the-art LALMs to assess their\nrobustness under different audio edits. This work lays the groundwork for\nfuture explorations on audio-modality interactions in LALMs security.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13772v1",
    "published_date": "2025-01-23 15:51:38 UTC",
    "updated_date": "2025-01-23 15:51:38 UTC"
  },
  {
    "arxiv_id": "2501.13766v2",
    "title": "UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models",
    "authors": [
      "Xin Xu",
      "Jiaxin Zhang",
      "Tianhao Chen",
      "Zitong Chao",
      "Jishan Hu",
      "Can Yang"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in mathematical\nreasoning, underscoring the need for a comprehensive and fair evaluation of\ntheir capabilities. However, existing benchmarks often fall short, either\nlacking extensive coverage of undergraduate-level mathematical problems or\nprobably suffering from test-set contamination. To address these issues, we\nintroduce UGMathBench, a diverse and dynamic benchmark specifically designed\nfor evaluating undergraduate-level mathematical reasoning with LLMs.\nUGMathBench comprises 5,062 problems across 16 subjects and 111 topics,\nfeaturing 10 distinct answer types. Each problem includes three randomized\nversions, with additional versions planned for release as leading open-source\nLLMs become saturated in UGMathBench. Furthermore, we propose two key metrics:\neffective accuracy (EAcc), which measures the percentage of correctly solved\nproblems across all three versions, and reasoning gap ($\\Delta$), which\nassesses reasoning robustness by calculating the difference between the average\naccuracy across all versions and EAcc. Our extensive evaluation of 23 leading\nLLMs reveals that the highest EAcc achieved is 56.3\\% by OpenAI-o1-mini, with\nlarge $\\Delta$ values observed across different models. This highlights the\nneed for future research aimed at developing \"large reasoning models\" with high\nEAcc and $\\Delta = 0$. We anticipate that the release of UGMathBench, along\nwith its detailed evaluation codes, will serve as a valuable resource to\nadvance the development of LLMs in solving mathematical problems. Codes and\ndata are available at https://github.com/YangLabHKUST/UGMathBench",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13766v2",
    "published_date": "2025-01-23 15:46:43 UTC",
    "updated_date": "2025-02-25 08:15:43 UTC"
  },
  {
    "arxiv_id": "2501.13763v2",
    "title": "Integrating Causality with Neurochaos Learning: Proposed Approach and Research Agenda",
    "authors": [
      "Nanjangud C. Narendra",
      "Nithin Nagaraj"
    ],
    "abstract": "Deep learning implemented via neural networks, has revolutionized machine\nlearning by providing methods for complex tasks such as object\ndetection/classification and prediction. However, architectures based on deep\nneural networks have started to yield diminishing returns, primarily due to\ntheir statistical nature and inability to capture causal structure in the\ntraining data. Another issue with deep learning is its high energy consumption,\nwhich is not that desirable from a sustainability perspective.\n  Therefore, alternative approaches are being considered to address these\nissues, both of which are inspired by the functioning of the human brain. One\napproach is causal learning, which takes into account causality among the items\nin the dataset on which the neural network is trained. It is expected that this\nwill help minimize the spurious correlations that are prevalent in the learned\nrepresentations of deep neural networks. The other approach is Neurochaos\nLearning, a recent development, which draws its inspiration from the nonlinear\nchaotic firing intrinsic to neurons in biological neural networks\n(brain/central nervous system). Both approaches have shown improved results\nover just deep learning alone.\n  To that end, in this position paper, we investigate how causal and neurochaos\nlearning approaches can be integrated together to produce better results,\nespecially in domains that contain linked data. We propose an approach for this\nintegration to enhance classification, prediction and reinforcement learning.\nWe also propose a set of research questions that need to be investigated in\norder to make this integration a reality.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13763v2",
    "published_date": "2025-01-23 15:45:29 UTC",
    "updated_date": "2025-02-08 11:06:44 UTC"
  },
  {
    "arxiv_id": "2501.13762v1",
    "title": "On Deciding the Data Complexity of Answering Linear Monadic Datalog Queries with LTL Operators(Extended Version)",
    "authors": [
      "Alessandro Artale",
      "Anton Gnatenko",
      "Vladislav Ryzhikov",
      "Michael Zakharyaschev"
    ],
    "abstract": "Our concern is the data complexity of answering linear monadic datalog\nqueries whose atoms in the rule bodies can be prefixed by operators of linear\ntemporal logic LTL. We first observe that, for data complexity, answering any\nconnected query with operators $\\bigcirc/\\bigcirc^-$ (at the next/previous\nmoment) is either in AC0, or in $ACC0\\!\\setminus\\!AC0$, or $NC^1$-complete, or\nLogSpace-hard and in NLogSpace. Then we show that the problem of deciding\nLogSpace-hardness of answering such queries is PSpace-complete, while checking\nmembership in the classes AC0 and ACC0 as well as $NC^1$-completeness can be\ndone in ExpSpace. Finally, we prove that membership in AC0 or in ACC0,\n$NC^1$-completeness, and LogSpace-hardness are undecidable for queries with\noperators $\\Diamond_f/\\Diamond_p$ (sometime in the future/past) provided that\n$NC^1 \\ne NLogSpace$, and $LogSpace \\ne NLogSpace$.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "Extended version of a paper accepted at ICDT'2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13762v1",
    "published_date": "2025-01-23 15:41:48 UTC",
    "updated_date": "2025-01-23 15:41:48 UTC"
  },
  {
    "arxiv_id": "2501.13758v1",
    "title": "2-Tier SimCSE: Elevating BERT for Robust Sentence Embeddings",
    "authors": [
      "Yumeng Wang",
      "Ziran Zhou",
      "Junjin Wang"
    ],
    "abstract": "Effective sentence embeddings that capture semantic nuances and generalize\nwell across diverse contexts are crucial for natural language processing tasks.\nWe address this challenge by applying SimCSE (Simple Contrastive Learning of\nSentence Embeddings) using contrastive learning to fine-tune the minBERT model\nfor sentiment analysis, semantic textual similarity (STS), and paraphrase\ndetection. Our contributions include experimenting with three different dropout\ntechniques, namely standard dropout, curriculum dropout, and adaptive dropout,\nto tackle overfitting, proposing a novel 2-Tier SimCSE Fine-tuning Model that\ncombines both unsupervised and supervised SimCSE on STS task, and exploring\ntransfer learning potential for Paraphrase and SST tasks. Our findings\ndemonstrate the effectiveness of SimCSE, with the 2-Tier model achieving\nsuperior performance on the STS task, with an average test score of 0.742\nacross all three downstream tasks. The results of error analysis reveals\nchallenges in handling complex sentiments and reliance on lexical overlap for\nparaphrase detection, highlighting areas for future research. The ablation\nstudy revealed that removing Adaptive Dropout in the Single-Task Unsupervised\nSimCSE Model led to improved performance on the STS task, indicating\noverfitting due to added parameters. Transfer learning from SimCSE models on\nParaphrase and SST tasks did not enhance performance, suggesting limited\ntransferability of knowledge from the STS task.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13758v1",
    "published_date": "2025-01-23 15:36:35 UTC",
    "updated_date": "2025-01-23 15:36:35 UTC"
  },
  {
    "arxiv_id": "2501.13756v1",
    "title": "Solving the long-tailed distribution problem by exploiting the synergies and balance of different techniques",
    "authors": [
      "Ziheng Wang",
      "Toni Lassila",
      "Sharib Ali"
    ],
    "abstract": "In real-world data, long-tailed data distribution is common, making it\nchallenging for models trained on empirical risk minimisation to learn and\nclassify tail classes effectively. While many studies have sought to improve\nlong tail recognition by altering the data distribution in the feature space\nand adjusting model decision boundaries, research on the synergy and corrective\napproach among various methods is limited. Our study delves into three\nlong-tail recognition techniques: Supervised Contrastive Learning (SCL),\nRare-Class Sample Generator (RSG), and Label-Distribution-Aware Margin Loss\n(LDAM). SCL enhances intra-class clusters based on feature similarity and\npromotes clear inter-class separability but tends to favour dominant classes\nonly. When RSG is integrated into the model, we observed that the intra-class\nfeatures further cluster towards the class centre, which demonstrates a\nsynergistic effect together with SCL's principle of enhancing intra-class\nclustering. RSG generates new tail features and compensates for the tail\nfeature space squeezed by SCL. Similarly, LDAM is known to introduce a larger\nmargin specifically for tail classes; we demonstrate that LDAM further bolsters\nthe model's performance on tail classes when combined with the more explicit\ndecision boundaries achieved by SCL and RSG. Furthermore, SCL can compensate\nfor the dominant class accuracy sacrificed by RSG and LDAM. Our research\nemphasises the synergy and balance among the three techniques, with each\namplifying the strengths of the others and mitigating their shortcomings. Our\nexperiment on long-tailed distribution datasets, using an end-to-end\narchitecture, yields competitive results by enhancing tail class accuracy\nwithout compromising dominant class performance, achieving a balanced\nimprovement across all classes.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "13",
    "pdf_url": "http://arxiv.org/pdf/2501.13756v1",
    "published_date": "2025-01-23 15:35:15 UTC",
    "updated_date": "2025-01-23 15:35:15 UTC"
  },
  {
    "arxiv_id": "2501.14007v1",
    "title": "Adaptive Genetic Algorithms for Pulse-Level Quantum Error Mitigation",
    "authors": [
      "William Aguilar-Calvo",
      "Santiago Núñez-Corrales"
    ],
    "abstract": "Noise remains a fundamental challenge in quantum computing, significantly\naffecting pulse fidelity and overall circuit performance. This paper introduces\nan adaptive algorithm for pulse-level quantum error mitigation, designed to\nenhance fidelity by dynamically responding to noise conditions without\nmodifying circuit gates. By targeting pulse parameters directly, this method\nreduces the impact of various noise sources, improving algorithm resilience in\nquantum circuits. We show the latter by applying our protocol to Grover's and\nDeutsch-Jozsa algorithms. Experimental results show that this pulse-level\nstrategy provides a flexible and efficient solution for increasing fidelity\nduring the noisy execution of quantum circuits. Our work contributes to\nadvancements in error mitigation techniques, essential for robust quantum\ncomputing.",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "quant-ph",
    "comment": "21 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14007v1",
    "published_date": "2025-01-23 15:28:22 UTC",
    "updated_date": "2025-01-23 15:28:22 UTC"
  },
  {
    "arxiv_id": "2501.13746v1",
    "title": "EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents",
    "authors": [
      "Yuhui Yun",
      "Huilong Ye",
      "Xinru Li",
      "Ruojia Li",
      "Jingfeng Deng",
      "Li Li",
      "Haoyi Xiong"
    ],
    "abstract": "The paper introduces EICopilot, an novel agent-based solution enhancing\nsearch and exploration of enterprise registration data within extensive online\nknowledge graphs like those detailing legal entities, registered capital, and\nmajor shareholders. Traditional methods necessitate text-based queries and\nmanual subgraph explorations, often resulting in time-consuming processes.\nEICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this\nlandscape by utilizing Large Language Models (LLMs) to interpret natural\nlanguage queries. This solution automatically generates and executes Gremlin\nscripts, providing efficient summaries of complex enterprise relationships.\nDistinct feature a data pre-processing pipeline that compiles and annotates\nrepresentative queries into a vector database of examples for In-context\nlearning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought\nwith ICL to enhance Gremlin script generation for knowledge graph search and\nexploration, and a novel query masking strategy that improves intent\nrecognition for heightened script accuracy. Empirical evaluations demonstrate\nthe superior performance of EICopilot, including speed and accuracy, over\nbaseline methods, with the \\emph{Full Mask} variant achieving a syntax error\nrate reduction to as low as 10.00% and an execution correctness of up to\n82.14%. These components collectively contribute to superior querying\ncapabilities and summarization of intricate datasets, positioning EICopilot as\na groundbreaking tool in the exploration and exploitation of large-scale\nknowledge graphs for enterprise information search.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13746v1",
    "published_date": "2025-01-23 15:22:25 UTC",
    "updated_date": "2025-01-23 15:22:25 UTC"
  },
  {
    "arxiv_id": "2501.13731v1",
    "title": "Pseudocode-Injection Magic: Enabling LLMs to Tackle Graph Computational Tasks",
    "authors": [
      "Chang Gong",
      "Wanrui Bian",
      "Zhijie Zhang",
      "Weiguo Zheng"
    ],
    "abstract": "Graph computational tasks are inherently challenging and often demand the\ndevelopment of advanced algorithms for effective solutions. With the emergence\nof large language models (LLMs), researchers have begun investigating their\npotential to address these tasks. However, existing approaches are constrained\nby LLMs' limited capability to comprehend complex graph structures and their\nhigh inference costs, rendering them impractical for handling large-scale\ngraphs. Inspired by human approaches to graph problems, we introduce a novel\nframework, PIE (Pseudocode-Injection-Enhanced LLM Reasoning for Graph\nComputational Tasks), which consists of three key steps: problem understanding,\nprompt design, and code generation. In this framework, LLMs are tasked with\nunderstanding the problem and extracting relevant information to generate\ncorrect code. The responsibility for analyzing the graph structure and\nexecuting the code is delegated to the interpreter. We inject task-related\npseudocodes into the prompts to further assist the LLMs in generating efficient\ncode. We also employ cost-effective trial-and-error techniques to ensure that\nthe LLM-generated code executes correctly. Unlike other methods that require\ninvoking LLMs for each individual test case, PIE only calls the LLM during the\ncode generation phase, allowing the generated code to be reused and\nsignificantly reducing inference costs. Extensive experiments demonstrate that\nPIE outperforms existing baselines in terms of both accuracy and computational\nefficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13731v1",
    "published_date": "2025-01-23 15:04:22 UTC",
    "updated_date": "2025-01-23 15:04:22 UTC"
  },
  {
    "arxiv_id": "2501.13727v2",
    "title": "Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System",
    "authors": [
      "Haikuo Du",
      "Fandi Gou",
      "Yunze Cai"
    ],
    "abstract": "Safety and scalability are two critical challenges faced by practical\nMulti-Agent Systems (MAS). However, existing Multi-Agent Reinforcement Learning\n(MARL) algorithms that rely solely on reward shaping are ineffective in\nensuring safety, and their scalability is rather limited due to the fixed-size\nnetwork output. To address these issues, we propose a novel framework, Scalable\nSafe MARL (SS-MARL), to enhance the safety and scalability of MARL methods.\nLeveraging the inherent graph structure of MAS, we design a multi-layer message\npassing network to aggregate local observations and communications of varying\nsizes. Furthermore, we develop a constrained joint policy optimization method\nin the setting of local observation to improve safety. Simulation experiments\ndemonstrate that SS-MARL achieves a better trade-off between optimality and\nsafety compared to baselines, and its scalability significantly outperforms the\nlatest methods in scenarios with a large number of agents.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13727v2",
    "published_date": "2025-01-23 15:01:19 UTC",
    "updated_date": "2025-04-01 12:59:50 UTC"
  },
  {
    "arxiv_id": "2501.13725v1",
    "title": "You Only Crash Once v2: Perceptually Consistent Strong Features for One-Stage Domain Adaptive Detection of Space Terrain",
    "authors": [
      "Timothy Chase Jr",
      "Christopher Wilson",
      "Karthik Dantu"
    ],
    "abstract": "The in-situ detection of planetary, lunar, and small-body surface terrain is\ncrucial for autonomous spacecraft applications, where learning-based computer\nvision methods are increasingly employed to enable intelligence without prior\ninformation or human intervention. However, many of these methods remain\ncomputationally expensive for spacecraft processors and prevent real-time\noperation. Training of such algorithms is additionally complex due to the\nscarcity of labeled data and reliance on supervised learning approaches.\nUnsupervised Domain Adaptation (UDA) offers a promising solution by\nfacilitating model training with disparate data sources such as simulations or\nsynthetic scenes, although UDA is difficult to apply to celestial environments\nwhere challenging feature spaces are paramount. To alleviate such issues, You\nOnly Crash Once (YOCOv1) has studied the integration of Visual Similarity-based\nAlignment (VSA) into lightweight one-stage object detection architectures to\nimprove space terrain UDA. Although proven effective, the approach faces\nnotable limitations, including performance degradations in multi-class and\nhigh-altitude scenarios. Building upon the foundation of YOCOv1, we propose\nnovel additions to the VSA scheme that enhance terrain detection capabilities\nunder UDA, and our approach is evaluated across both simulated and real-world\ndata. Our second YOCO rendition, YOCOv2, is capable of achieving\nstate-of-the-art UDA performance on surface terrain detection, where we\nshowcase improvements upwards of 31% compared with YOCOv1 and terrestrial\nstate-of-the-art. We demonstrate the practical utility of YOCOv2 with\nspacecraft flight hardware performance benchmarking and qualitative evaluation\nof NASA mission data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13725v1",
    "published_date": "2025-01-23 14:58:49 UTC",
    "updated_date": "2025-01-23 14:58:49 UTC"
  },
  {
    "arxiv_id": "2501.13720v2",
    "title": "Musical ethnocentrism in Large Language Models",
    "authors": [
      "Anna Kruspe"
    ],
    "abstract": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13720v2",
    "published_date": "2025-01-23 14:50:37 UTC",
    "updated_date": "2025-02-03 10:52:15 UTC"
  },
  {
    "arxiv_id": "2501.14006v1",
    "title": "Asymmetrical Latent Representation for Individual Treatment Effect Modeling",
    "authors": [
      "Armand Lacombe",
      "Michèle Sebag"
    ],
    "abstract": "Conditional Average Treatment Effect (CATE) estimation, at the heart of\ncounterfactual reasoning, is a crucial challenge for causal modeling both\ntheoretically and applicatively, in domains such as healthcare, sociology, or\nadvertising. Borrowing domain adaptation principles, a popular design maps the\nsample representation to a latent space that balances control and treated\npopulations while enabling the prediction of the potential outcomes. This paper\npresents a new CATE estimation approach based on the asymmetrical search for\ntwo latent spaces called Asymmetrical Latent Representation for Individual\nTreatment Effect (ALRITE), where the two latent spaces are respectively\nintended to optimize the counterfactual prediction accuracy on the control and\nthe treated samples. Under moderate assumptions, ALRITE admits an upper bound\non the precision of the estimation of heterogeneous effects (PEHE), and the\napproach is empirically successfully validated compared to the state-of-the-art",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14006v1",
    "published_date": "2025-01-23 14:44:36 UTC",
    "updated_date": "2025-01-23 14:44:36 UTC"
  },
  {
    "arxiv_id": "2501.13713v1",
    "title": "Skin Disease Detection and Classification of Actinic Keratosis and Psoriasis Utilizing Deep Transfer Learning",
    "authors": [
      "Fahud Ahmmed",
      "Md. Zaheer Raihan",
      "Kamnur Nahar",
      "D. M. Asadujjaman",
      "Md. Mahfujur Rahman",
      "Abdullah Tamim"
    ],
    "abstract": "Skin diseases can arise from infections, allergies, genetic factors,\nautoimmune disorders, hormonal imbalances, or environmental triggers such as\nsun damage and pollution. Some skin diseases, such as Actinic Keratosis and\nPsoriasis, can be fatal if not treated in time. Early identification is\ncrucial, but the diagnostic methods for these conditions are often expensive\nand not widely accessible. In this study, we propose a novel and efficient\nmethod for diagnosing skin diseases using deep learning techniques. This\napproach employs a modified VGG16 Convolutional Neural Network (CNN) model. The\nmodel includes several convolutional layers and utilizes ImageNet weights with\nmodified top layers. The top layer is updated with fully connected layers and a\nfinal softmax activation layer to classify skin diseases. The dataset used,\ntitled \"Skin Disease Dataset,\" is publicly available. While the VGG16\narchitecture does not include data augmentation by default, preprocessing\ntechniques such as rotation, shifting, and zooming were applied to augment the\ndata prior to model training. The proposed methodology achieved 90.67% accuracy\nusing the modified VGG16 model, demonstrating its reliability in classifying\nskin diseases. The promising results highlight the potential of this approach\nfor real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "J.3"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13713v1",
    "published_date": "2025-01-23 14:43:53 UTC",
    "updated_date": "2025-01-23 14:43:53 UTC"
  },
  {
    "arxiv_id": "2501.13712v1",
    "title": "Formally Verified Neurosymbolic Trajectory Learning via Tensor-based Linear Temporal Logic on Finite Traces",
    "authors": [
      "Mark Chevallier",
      "Filip Smola",
      "Richard Schmoetten",
      "Jacques D. Fleuriot"
    ],
    "abstract": "We present a novel formalisation of tensor semantics for linear temporal\nlogic on finite traces (LTLf), with formal proofs of correctness carried out in\nthe theorem prover Isabelle/HOL. We demonstrate that this formalisation can be\nintegrated into a neurosymbolic learning process by defining and verifying a\ndifferentiable loss function for the LTLf constraints, and automatically\ngenerating an implementation that integrates with PyTorch. We show that, by\nusing this loss, the process learns to satisfy pre-specified logical\nconstraints. Our approach offers a fully rigorous framework for constrained\ntraining, eliminating many of the inherent risks of ad-hoc, manual\nimplementations of logical aspects directly in an \"unsafe\" programming language\nsuch as Python, while retaining efficiency in implementation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13712v1",
    "published_date": "2025-01-23 14:43:12 UTC",
    "updated_date": "2025-01-23 14:43:12 UTC"
  },
  {
    "arxiv_id": "2501.13710v1",
    "title": "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID",
    "authors": [
      "Iñaki Erregue",
      "Kamal Nasrollahi",
      "Sergio Escalera"
    ],
    "abstract": "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This paper has been accepted to the 5th Workshop on Real-World\n  Surveillance: Applications and Challenges (WACV 2025)",
    "pdf_url": "http://arxiv.org/pdf/2501.13710v1",
    "published_date": "2025-01-23 14:38:40 UTC",
    "updated_date": "2025-01-23 14:38:40 UTC"
  },
  {
    "arxiv_id": "2501.13707v1",
    "title": "EventVL: Understand Event Streams via Multimodal Large Language Model",
    "authors": [
      "Pengteng Li",
      "Yunfan Lu",
      "Pinghao Song",
      "Wuyang Li",
      "Huizai Yao",
      "Hui Xiong"
    ],
    "abstract": "The event-based Vision-Language Model (VLM) recently has made good progress\nfor practical vision tasks. However, most of these works just utilize CLIP for\nfocusing on traditional perception tasks, which obstruct model understanding\nexplicitly the sufficient semantics and context from event streams. To address\nthe deficiency, we propose EventVL, the first generative event-based MLLM\n(Multimodal Large Language Model) framework for explicit semantic\nunderstanding. Specifically, to bridge the data gap for connecting different\nmodalities semantics, we first annotate a large event-image/video-text dataset,\ncontaining almost 1.4 million high-quality pairs of data, which enables\neffective learning across various scenes, e.g., drive scene or human motion.\nAfter that, we design Event Spatiotemporal Representation to fully explore the\ncomprehensive information by diversely aggregating and segmenting the event\nstream. To further promote a compact semantic space, Dynamic Semantic Alignment\nis introduced to improve and complete sparse semantic spaces of events.\nExtensive experiments show that our EventVL can significantly surpass existing\nMLLM baselines in event captioning and scene description generation tasks. We\nhope our research could contribute to the development of the event vision\ncommunity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13707v1",
    "published_date": "2025-01-23 14:37:21 UTC",
    "updated_date": "2025-01-23 14:37:21 UTC"
  },
  {
    "arxiv_id": "2501.13692v1",
    "title": "Training-Free Consistency Pipeline for Fashion Repose",
    "authors": [
      "Potito Aghilar",
      "Vito Walter Anelli",
      "Michelantonio Trizio",
      "Tommaso Di Noia"
    ],
    "abstract": "Recent advancements in diffusion models have significantly broadened the\npossibilities for editing images of real-world objects. However, performing\nnon-rigid transformations, such as changing the pose of objects or image-based\nconditioning, remains challenging. Maintaining object identity during these\nedits is difficult, and current methods often fall short of the precision\nneeded for industrial applications, where consistency is critical.\nAdditionally, fine-tuning diffusion models requires custom training data, which\nis not always accessible in real-world scenarios. This work introduces\nFashionRepose, a training-free pipeline for non-rigid pose editing specifically\ndesigned for the fashion industry. The approach integrates off-the-shelf models\nto adjust poses of long-sleeve garments, maintaining identity and branding\nattributes. FashionRepose uses a zero-shot approach to perform these edits in\nnear real-time, eliminating the need for specialized training. consistent image\nediting. The solution holds potential for applications in the fashion industry\nand other fields demanding identity preservation in image editing.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13692v1",
    "published_date": "2025-01-23 14:17:01 UTC",
    "updated_date": "2025-01-23 14:17:01 UTC"
  },
  {
    "arxiv_id": "2501.13687v1",
    "title": "Question Answering on Patient Medical Records with Private Fine-Tuned LLMs",
    "authors": [
      "Sara Kothari",
      "Ayush Gupta"
    ],
    "abstract": "Healthcare systems continuously generate vast amounts of electronic health\nrecords (EHRs), commonly stored in the Fast Healthcare Interoperability\nResources (FHIR) standard. Despite the wealth of information in these records,\ntheir complexity and volume make it difficult for users to retrieve and\ninterpret crucial health insights. Recent advances in Large Language Models\n(LLMs) offer a solution, enabling semantic question answering (QA) over medical\ndata, allowing users to interact with their health records more effectively.\nHowever, ensuring privacy and compliance requires edge and private deployments\nof LLMs.\n  This paper proposes a novel approach to semantic QA over EHRs by first\nidentifying the most relevant FHIR resources for a user query (Task1) and\nsubsequently answering the query based on these resources (Task2). We explore\nthe performance of privately hosted, fine-tuned LLMs, evaluating them against\nbenchmark models such as GPT-4 and GPT-4o. Our results demonstrate that\nfine-tuned LLMs, while 250x smaller in size, outperform GPT-4 family models by\n0.55% in F1 score on Task1 and 42% on Meteor Task in Task2. Additionally, we\nexamine advanced aspects of LLM usage, including sequential fine-tuning, model\nself-evaluation (narcissistic evaluation), and the impact of training data size\non performance. The models and datasets are available here:\nhttps://huggingface.co/genloop",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13687v1",
    "published_date": "2025-01-23 14:13:56 UTC",
    "updated_date": "2025-01-23 14:13:56 UTC"
  },
  {
    "arxiv_id": "2501.13683v1",
    "title": "Unlearning Clients, Features and Samples in Vertical Federated Learning",
    "authors": [
      "Ayush K. Varshney",
      "Konstantinos Vandikas",
      "Vicenç Torra"
    ],
    "abstract": "Federated Learning (FL) has emerged as a prominent distributed learning\nparadigm. Within the scope of privacy preservation, information privacy\nregulations such as GDPR entitle users to request the removal (or unlearning)\nof their contribution from a service that is hosting the model. For this\npurpose, a server hosting an ML model must be able to unlearn certain\ninformation in cases such as copyright infringement or security issues that can\nmake the model vulnerable or impact the performance of a service based on that\nmodel. While most unlearning approaches in FL focus on Horizontal FL (HFL),\nwhere clients share the feature space and the global model, Vertical FL (VFL)\nhas received less attention from the research community. VFL involves clients\n(passive parties) sharing the sample space among them while not having access\nto the labels. In this paper, we explore unlearning in VFL from three\nperspectives: unlearning clients, unlearning features, and unlearning samples.\nTo unlearn clients and features we introduce VFU-KD which is based on knowledge\ndistillation (KD) while to unlearn samples, VFU-GA is introduced which is based\non gradient ascent. To provide evidence of approximate unlearning, we utilize\nMembership Inference Attack (MIA) to audit the effectiveness of our unlearning\napproach. Our experiments across six tabular datasets and two image datasets\ndemonstrate that VFU-KD and VFU-GA achieve performance comparable to or better\nthan both retraining from scratch and the benchmark R2S method in many cases,\nwith improvements of $(0-2\\%)$. In the remaining cases, utility scores remain\ncomparable, with a modest utility loss ranging from $1-5\\%$. Unlike existing\nmethods, VFU-KD and VFU-GA require no communication between active and passive\nparties during unlearning. However, they do require the active party to store\nthe previously communicated embeddings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Paper accepted for publication in PETS 2025, Issue II",
    "pdf_url": "http://arxiv.org/pdf/2501.13683v1",
    "published_date": "2025-01-23 14:10:02 UTC",
    "updated_date": "2025-01-23 14:10:02 UTC"
  },
  {
    "arxiv_id": "2501.13676v2",
    "title": "Certified Robustness Under Bounded Levenshtein Distance",
    "authors": [
      "Elias Abad Rocamora",
      "Grigorios G. Chrysos",
      "Volkan Cevher"
    ],
    "abstract": "Text classifiers suffer from small perturbations, that if chosen\nadversarially, can dramatically change the output of the model. Verification\nmethods can provide robustness certificates against such adversarial\nperturbations, by computing a sound lower bound on the robust accuracy.\nNevertheless, existing verification methods incur in prohibitive costs and\ncannot practically handle Levenshtein distance constraints. We propose the\nfirst method for computing the Lipschitz constant of convolutional classifiers\nwith respect to the Levenshtein distance. We use these Lipschitz constant\nestimates for training 1-Lipschitz classifiers. This enables computing the\ncertified radius of a classifier in a single forward pass. Our method, LipsLev,\nis able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and\n$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude\nfaster than existing approaches. We believe our work can open the door to more\nefficient verification in the text domain.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted in ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13676v2",
    "published_date": "2025-01-23 13:58:53 UTC",
    "updated_date": "2025-02-20 15:44:15 UTC"
  },
  {
    "arxiv_id": "2501.14005v1",
    "title": "Device-aware Optical Adversarial Attack for a Portable Projector-camera System",
    "authors": [
      "Ning Jiang",
      "Yanhong Liu",
      "Dingheng Zeng",
      "Yue Feng",
      "Weihong Deng",
      "Ying Li"
    ],
    "abstract": "Deep-learning-based face recognition (FR) systems are susceptible to\nadversarial examples in both digital and physical domains. Physical attacks\npresent a greater threat to deployed systems as adversaries can easily access\nthe input channel, allowing them to provide malicious inputs to impersonate a\nvictim. This paper addresses the limitations of existing projector-camera-based\nadversarial light attacks in practical FR setups. By incorporating device-aware\nadaptations into the digital attack algorithm, such as resolution-aware and\ncolor-aware adjustments, we mitigate the degradation from digital to physical\ndomains. Experimental validation showcases the efficacy of our proposed\nalgorithm against real and spoof adversaries, achieving high physical\nsimilarity scores in FR models and state-of-the-art commercial systems. On\naverage, there is only a 14% reduction in scores from digital to physical\nattacks, with high attack success rate in both white- and black-box scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14005v1",
    "published_date": "2025-01-23 13:55:23 UTC",
    "updated_date": "2025-01-23 13:55:23 UTC"
  },
  {
    "arxiv_id": "2501.13669v2",
    "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
    "authors": [
      "Shezheng Song",
      "Hao Xu",
      "Jun Ma",
      "Shasha Li",
      "Long Peng",
      "Qian Wan",
      "Xiaodong Liu",
      "Jie Yu"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong general language capabilities.\nHowever, fine-tuning these models on domain-specific tasks often leads to\ncatastrophic forgetting, where the model overwrites or loses essential\nknowledge acquired during pretraining. This phenomenon significantly limits the\nbroader applicability of LLMs. To address this challenge, we propose a novel\napproach to compute the element-wise importance of model parameters crucial for\npreserving general knowledge during fine-tuning. Our method utilizes a\ndual-objective optimization strategy: (1) regularization loss based on\nelement-wise parameter importance, which constrains the updates to parameters\ncrucial for general knowledge; (2) cross-entropy loss to adapt to\ndomain-specific tasks. Additionally, we introduce layer-wise coefficients to\naccount for the varying contributions of different layers, dynamically\nbalancing the dual-objective optimization. Extensive experiments on scientific,\nmedical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our\napproach mitigates catastrophic forgetting while enhancing model adaptability.\nCompared to previous methods, our solution is approximately 20 times faster and\nrequires only 10-15% of the storage, highlighting the practical efficiency. The\ncode will be released.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in progress",
    "pdf_url": "http://arxiv.org/pdf/2501.13669v2",
    "published_date": "2025-01-23 13:54:53 UTC",
    "updated_date": "2025-02-17 13:10:33 UTC"
  },
  {
    "arxiv_id": "2502.10411v1",
    "title": "TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models",
    "authors": [
      "Sahan Bulathwela",
      "Daniel Van Niekerk",
      "Jarrod Shipton",
      "Maria Perez-Ortiz",
      "Benjamin Rosman",
      "John Shawe-Taylor"
    ],
    "abstract": "Personalised education is one of the domains that can greatly benefit from\nthe most recent advances in Artificial Intelligence (AI) and Large Language\nModels (LLM). However, it is also one of the most challenging applications due\nto the cognitive complexity of teaching effectively while personalising the\nlearning experience to suit independent learners. We hypothesise that one\npromising approach to excelling in such demanding use cases is using a\n\\emph{society of minds}. In this chapter, we present TrueReason, an exemplar\npersonalised learning system that integrates a multitude of specialised AI\nmodels that can mimic micro skills that are composed together by a LLM to\noperationalise planning and reasoning. The architecture of the initial\nprototype is presented while describing two micro skills that have been\nincorporated in the prototype. The proposed system demonstrates the first step\nin building sophisticated AI systems that can take up very complex cognitive\ntasks that are demanded by domains such as education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.MA",
      "I.2.4; I.2.7; H.3.3; I.2.6; K.3.1"
    ],
    "primary_category": "cs.CY",
    "comment": "To be published as a book chapter",
    "pdf_url": "http://arxiv.org/pdf/2502.10411v1",
    "published_date": "2025-01-23 13:25:44 UTC",
    "updated_date": "2025-01-23 13:25:44 UTC"
  },
  {
    "arxiv_id": "2501.14004v2",
    "title": "ME-CPT: Multi-Task Enhanced Cross-Temporal Point Transformer for Urban 3D Change Detection",
    "authors": [
      "Luqi Zhang",
      "Haiping Wang",
      "Chong Liu",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "abstract": "The point clouds collected by the Airborne Laser Scanning (ALS) system\nprovide accurate 3D information of urban land covers. By utilizing\nmulti-temporal ALS point clouds, semantic changes in urban area can be\ncaptured, demonstrating significant potential in urban planning, emergency\nmanagement, and infrastructure maintenance. Existing 3D change detection\nmethods struggle to efficiently extract multi-class semantic information and\nchange features, still facing the following challenges: (1) the difficulty of\naccurately modeling cross-temporal point clouds spatial relationships for\neffective change feature extraction; (2) class imbalance of change samples\nwhich hinders distinguishability of semantic features; (3) the lack of\nreal-world datasets for 3D semantic change detection. To resolve these\nchallenges, we propose the Multi-task Enhanced Cross-temporal Point Transformer\n(ME-CPT) network. ME-CPT establishes spatiotemporal correspondences between\npoint cloud across different epochs and employs attention mechanisms to jointly\nextract semantic change features, facilitating information exchange and change\ncomparison. Additionally, we incorporate a semantic segmentation task and\nthrough the multi-task training strategy, further enhance the\ndistinguishability of semantic features, reducing the impact of class imbalance\nin change types. Moreover, we release a 22.5 $km^2$ 3D semantic change\ndetection dataset, offering diverse scenes for comprehensive evaluation.\nExperiments on multiple datasets show that the proposed MT-CPT achieves\nsuperior performance compared to existing state-of-the-art methods. The source\ncode and dataset will be released upon acceptance at\nhttps://github.com/zhangluqi0209/ME-CPT.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14004v2",
    "published_date": "2025-01-23 13:07:41 UTC",
    "updated_date": "2025-02-19 05:03:35 UTC"
  },
  {
    "arxiv_id": "2501.13622v3",
    "title": "Coarse-to-Fine Process Reward Modeling for Mathematical Reasoning",
    "authors": [
      "Yulan Hu",
      "Ge Chen",
      "Jinman Zhao",
      "Sheng Ouyang",
      "Yong Liu"
    ],
    "abstract": "The Process Reward Model (PRM) plays a crucial role in mathematical reasoning\ntasks, requiring high-quality supervised process data. However, we observe that\nreasoning steps generated by Large Language Models (LLMs) often fail to exhibit\nstrictly incremental information, leading to redundancy that can hinder\neffective reasoning. To address this issue, we propose CFPRM, a simple yet\neffective coarse-to-fine strategy. Instead of focusing on the detection of\nredundant steps, our approach first establishes a coarse-grained window to\nmerge adjacent reasoning steps into unified, holistic steps. The window size is\nthen progressively reduced to extract fine-grained reasoning steps, enabling\ndata collection at multiple granularities for training. By leveraging this\nhierarchical refinement process, CFPRM mitigates redundancy while preserving\nessential fine-grained knowledge. Extensive experiments on two reasoning\ndatasets across three loss criteria validate the CFPRM's effectiveness and\nversatility.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13622v3",
    "published_date": "2025-01-23 12:44:45 UTC",
    "updated_date": "2025-02-18 13:05:36 UTC"
  },
  {
    "arxiv_id": "2501.13620v5",
    "title": "A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs",
    "authors": [
      "Mohit Vaishnav",
      "Tanel Tammet"
    ],
    "abstract": "A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13620v5",
    "published_date": "2025-01-23 12:42:42 UTC",
    "updated_date": "2025-05-06 13:59:11 UTC"
  },
  {
    "arxiv_id": "2501.13610v1",
    "title": "Efficient Synaptic Delay Implementation in Digital Event-Driven AI Accelerators",
    "authors": [
      "Roy Meijer",
      "Paul Detterer",
      "Amirreza Yousefzadeh",
      "Alberto Patino-Saucedo",
      "Guanghzi Tang",
      "Kanishkan Vadivel",
      "Yinfu Xu",
      "Manil-Dev Gomony",
      "Federico Corradi",
      "Bernabe Linares-Barranco",
      "Manolis Sifalakis"
    ],
    "abstract": "Synaptic delay parameterization of neural network models have remained\nlargely unexplored but recent literature has been showing promising results,\nsuggesting the delay parameterized models are simpler, smaller, sparser, and\nthus more energy efficient than similar performing (e.g. task accuracy)\nnon-delay parameterized ones. We introduce Shared Circular Delay Queue (SCDQ),\na novel hardware structure for supporting synaptic delays on digital\nneuromorphic accelerators. Our analysis and hardware results show that it\nscales better in terms of memory, than current commonly used approaches, and is\nmore amortizable to algorithm-hardware co-optimizations, where in fact, memory\nscaling is modulated by model sparsity and not merely network size. Next to\nmemory we also report performance on latency area and energy per inference.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2404.10597",
    "pdf_url": "http://arxiv.org/pdf/2501.13610v1",
    "published_date": "2025-01-23 12:30:04 UTC",
    "updated_date": "2025-01-23 12:30:04 UTC"
  },
  {
    "arxiv_id": "2501.13607v1",
    "title": "Optimal Multi-Objective Best Arm Identification with Fixed Confidence",
    "authors": [
      "Zhirui Chen",
      "P. N. Karthik",
      "Yeow Meng Chee",
      "Vincent Y. F. Tan"
    ],
    "abstract": "We consider a multi-armed bandit setting with finitely many arms, in which\neach arm yields an $M$-dimensional vector reward upon selection. We assume that\nthe reward of each dimension (a.k.a. {\\em objective}) is generated\nindependently of the others. The best arm of any given objective is the arm\nwith the largest component of mean corresponding to the objective. The end goal\nis to identify the best arm of {\\em every} objective in the shortest (expected)\ntime subject to an upper bound on the probability of error (i.e.,\nfixed-confidence regime). We establish a problem-dependent lower bound on the\nlimiting growth rate of the expected stopping time, in the limit of vanishing\nerror probabilities. This lower bound, we show, is characterised by a max-min\noptimisation problem that is computationally expensive to solve at each time\nstep. We propose an algorithm that uses the novel idea of {\\em surrogate\nproportions} to sample the arms at each time step, eliminating the need to\nsolve the max-min optimisation problem at each step. We demonstrate\ntheoretically that our algorithm is asymptotically optimal. In addition, we\nprovide extensive empirical studies to substantiate the efficiency of our\nalgorithm. While existing works on pure exploration with multi-objective\nmulti-armed bandits predominantly focus on {\\em Pareto frontier\nidentification}, our work fills the gap in the literature by conducting a\nformal investigation of the multi-objective best arm identification problem.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AISTATS 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13607v1",
    "published_date": "2025-01-23 12:28:09 UTC",
    "updated_date": "2025-01-23 12:28:09 UTC"
  },
  {
    "arxiv_id": "2501.14003v1",
    "title": "PaMMA-Net: Plasmas magnetic measurement evolution based on data-driven incremental accumulative prediction",
    "authors": [
      "Yunfei Ling",
      "Zijie Liu",
      "Jun Du",
      "Yao Huang",
      "Yuehang Wang",
      "Bingjia Xiao",
      "Xin Fang"
    ],
    "abstract": "An accurate evolution model is crucial for effective control and in-depth\nstudy of fusion plasmas. Evolution methods based on physical models often\nencounter challenges such as insufficient robustness or excessive computational\ncosts. Given the proven strong fitting capabilities of deep learning methods\nacross various fields, including plasma research, this paper introduces a deep\nlearning-based magnetic measurement evolution method named PaMMA-Net (Plasma\nMagnetic Measurements Incremental Accumulative Prediction Network). This\nnetwork is capable of evolving magnetic measurements in tokamak discharge\nexperiments over extended periods or, in conjunction with equilibrium\nreconstruction algorithms, evolving macroscopic parameters such as plasma\nshape. Leveraging a incremental prediction approach and data augmentation\ntechniques tailored for magnetic measurements, PaMMA-Net achieves superior\nevolution results compared to existing studies. The tests conducted on real\nexperimental data from EAST validate the high generalization capability of the\nproposed method.",
    "categories": [
      "physics.plasm-ph",
      "cs.AI"
    ],
    "primary_category": "physics.plasm-ph",
    "comment": "20 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.14003v1",
    "published_date": "2025-01-23 12:19:37 UTC",
    "updated_date": "2025-01-23 12:19:37 UTC"
  },
  {
    "arxiv_id": "2501.14002v3",
    "title": "Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages",
    "authors": [
      "Zui Chen",
      "Tianqiao Liu",
      "Mi Tian",
      "Qing Tong",
      "Weiqi Luo",
      "Zitao Liu"
    ],
    "abstract": "Mathematical reasoning remains a challenging area for large language models\n(LLMs), prompting the development of math-specific LLMs such as LLEMMA,\nDeepSeekMath, and Qwen2-Math, among others. These models typically follow a\ntwo-stage training paradigm: pre-training with math-related corpora and\npost-training with problem datasets for supervised fine-tuning (SFT). Despite\nthese efforts, the improvements in mathematical reasoning achieved through\ncontinued pre-training (CPT) are often less significant compared to those\nobtained via SFT. This study addresses this discrepancy by exploring\nalternative strategies during the pre-training phase, focusing on the use of\nproblem-solving data over general mathematical corpora. We investigate three\nprimary research questions: (1) Can problem-solving data enhance the model's\nmathematical reasoning capabilities more effectively than general mathematical\ncorpora during CPT? (2) Are synthetic data from the same source equally\neffective, and which synthesis methods are most efficient? (3) How do the\ncapabilities developed from the same problem-solving data differ between the\nCPT and SFT stages, and what factors contribute to these differences? Our\nfindings indicate that problem-solving data significantly enhances the model's\nmathematical capabilities compared to general mathematical corpora. We also\nidentify effective data synthesis methods, demonstrating that the tutorship\namplification synthesis method achieves the best performance. Furthermore,\nwhile SFT facilitates instruction-following abilities, it underperforms\ncompared to CPT with the same data, which can be partially attributed to its\npoor learning capacity for more challenging problem-solving data. These\ninsights provide valuable guidance for optimizing the mathematical reasoning\ncapabilities of LLMs, culminating in our development of a powerful mathematical\nbase model called MathGPT-8B.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.14002v3",
    "published_date": "2025-01-23 12:14:57 UTC",
    "updated_date": "2025-03-24 02:20:01 UTC"
  },
  {
    "arxiv_id": "2501.14001v1",
    "title": "Enhancing kelp forest detection in remote sensing images using crowdsourced labels with Mixed Vision Transformers and ConvNeXt segmentation models",
    "authors": [
      "Ioannis Nasios"
    ],
    "abstract": "Kelp forests, as foundation species, are vital to marine ecosystems,\nproviding essential food and habitat for numerous organisms. This study\nexplores the integration of crowdsourced labels with advanced artificial\nintelligence models to develop a fast and accurate kelp canopy detection\npipeline using Landsat images. Building on the success of a machine learning\ncompetition, where this approach ranked third and performed consistently well\non both local validation and public and private leaderboards, the research\nhighlights the effectiveness of combining Mixed Vision Transformers (MIT) with\nConvNeXt models. Training these models on various image sizes significantly\nenhanced the accuracy of the ensemble results. U-Net emerged as the best\nsegmentation architecture, with UpperNet also contributing to the final\nensemble. Key Landsat bands, such as ShortWave InfraRed (SWIR1) and\nNear-InfraRed (NIR), were crucial while altitude data was used in\npostprocessing to eliminate false positives on land. The methodology achieved a\nhigh detection rate, accurately identifying about three out of four pixels\ncontaining kelp canopy while keeping false positives low. Despite the medium\nresolution of Landsat satellites, their extensive historical coverage makes\nthem effective for studying kelp forests. This work also underscores the\npotential of combining machine learning models with crowdsourced data for\neffective and scalable environmental monitoring. All running code for training\nall models and inference can be found at\nhttps://github.com/IoannisNasios/Kelp_Forests.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14001v1",
    "published_date": "2025-01-23 12:12:31 UTC",
    "updated_date": "2025-01-23 12:12:31 UTC"
  },
  {
    "arxiv_id": "2501.13594v1",
    "title": "Text-to-SQL based on Large Language Models and Database Keyword Search",
    "authors": [
      "Eduardo R. Nascimento",
      "Caio Viktor S. Avila",
      "Yenier T. Izquierdo",
      "Grettel M. García",
      "Lucas Feijó L. Andrade",
      "Michelle S. P. Facina",
      "Melissa Lemos",
      "Marco A. Casanova"
    ],
    "abstract": "Text-to-SQL prompt strategies based on Large Language Models (LLMs) achieve\nremarkable performance on well-known benchmarks. However, when applied to\nreal-world databases, their performance is significantly less than for these\nbenchmarks, especially for Natural Language (NL) questions requiring complex\nfilters and joins to be processed. This paper then proposes a strategy to\ncompile NL questions into SQL queries that incorporates a dynamic few-shot\nexamples strategy and leverages the services provided by a database keyword\nsearch (KwS) platform. The paper details how the precision and recall of the\nschema-linking process are improved with the help of the examples provided and\nthe keyword-matching service that the KwS platform offers. Then, it shows how\nthe KwS platform can be used to synthesize a view that captures the joins\nrequired to process an input NL question and thereby simplify the SQL query\ncompilation step. The paper includes experiments with a real-world relational\ndatabase to assess the performance of the proposed strategy. The experiments\nsuggest that the strategy achieves an accuracy on the real-world relational\ndatabase that surpasses state-of-the-art approaches. The paper concludes by\ndiscussing the results obtained.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "68T50",
      "H.2.3; I.2.7"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13594v1",
    "published_date": "2025-01-23 12:03:29 UTC",
    "updated_date": "2025-01-23 12:03:29 UTC"
  },
  {
    "arxiv_id": "2501.13587v2",
    "title": "Contrastive Representation Learning Helps Cross-institutional Knowledge Transfer: A Study in Pediatric Ventilation Management",
    "authors": [
      "Yuxuan Liu",
      "Jinpei Han",
      "Padmanabhan Ramnarayan",
      "A. Aldo Faisal"
    ],
    "abstract": "Clinical machine learning deployment across institutions faces significant\nchallenges when patient populations and clinical practices differ\nsubstantially. We present a systematic framework for cross-institutional\nknowledge transfer in clinical time series, demonstrated through pediatric\nventilation management between a general pediatric intensive care unit (PICU)\nand a cardiac-focused unit. Using contrastive predictive coding (CPC) for\nrepresentation learning, we investigate how different data regimes and\nfine-tuning strategies affect knowledge transfer across institutional\nboundaries. Our results show that while direct model transfer performs poorly,\nCPC with appropriate fine-tuning enables effective knowledge sharing between\ninstitutions, with benefits particularly evident in limited data scenarios.\nAnalysis of transfer patterns reveals an important asymmetry: temporal\nprogression patterns transfer more readily than point-of-care decisions,\nsuggesting practical pathways for cross-institutional deployment. Through a\nsystematic evaluation of fine-tuning approaches and transfer patterns, our work\nprovides insights for developing more generalizable clinical decision support\nsystems while enabling smaller specialized units to leverage knowledge from\nlarger centers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13587v2",
    "published_date": "2025-01-23 11:55:13 UTC",
    "updated_date": "2025-01-27 15:30:02 UTC"
  },
  {
    "arxiv_id": "2502.10410v1",
    "title": "Auto-Evaluation: A Critical Measure in Driving Improvements in Quality and Safety of AI-Generated Lesson Resources",
    "authors": [
      "Hannah-Beth Clark",
      "Margaux Dowland",
      "Laura Benton",
      "Reka Budai",
      "Ibrahim Kaan Keskin",
      "Emma Searle",
      "Matthew Gregory",
      "Mark Hodierne",
      "William Gayne",
      "John Roberts"
    ],
    "abstract": "As a publicly funded body in the UK, Oak National Academy is in a unique\nposition to innovate within this field as we have a comprehensive curriculum of\napproximately 13,000 open education resources (OER) for all National Curriculum\nsubjects, designed and quality-assured by expert, human teachers. This has\nprovided the corpus of content needed for building a high-quality AI-powered\nlesson planning tool, Aila, that is free to use and, therefore, accessible to\nall teachers across the country. Furthermore, using our evidence-informed\ncurriculum principles, we have codified and exemplified each component of\nlesson design. To assess the quality of lessons produced by Aila at scale, we\nhave developed an AI-powered auto-evaluation agent,facilitating informed\nimprovements to enhance output quality. Through comparisons between human and\nauto-evaluations, we have begun to refine this agent further to increase its\naccuracy, measured by its alignment with an expert human evaluator. In this\npaper we present this iterative evaluation process through an illustrative case\nstudy focused on one quality benchmark - the level of challenge within\nmultiple-choice quizzes. We also explore the contribution that this may make to\nsimilar projects and the wider sector.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "27 pages, Part of MIT Open Learning AI and Open Education Initiative\n  Series, published Jan 2025\n  https://aiopeneducation.pubpub.org/pub/i36sncz8/release/3?readingCollection=06969c6d",
    "pdf_url": "http://arxiv.org/pdf/2502.10410v1",
    "published_date": "2025-01-23 11:35:23 UTC",
    "updated_date": "2025-01-23 11:35:23 UTC"
  },
  {
    "arxiv_id": "2502.00029v2",
    "title": "AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics",
    "authors": [
      "Kamer Ali Yuksel",
      "Hassan Sawaf"
    ],
    "abstract": "Financial metrics like the Sharpe ratio are pivotal in evaluating investment\nperformance by balancing risk and return. However, traditional metrics often\nstruggle with robustness and generalization, particularly in dynamic and\nvolatile market conditions. This paper introduces AlphaSharpe, a novel\nframework leveraging large language models (LLMs) to iteratively evolve and\noptimize financial metrics to discover enhanced risk-return metrics that\noutperform traditional approaches in robustness and correlation with future\nperformance metrics by employing iterative crossover, mutation, and evaluation.\nKey contributions of this work include: (1) a novel use of LLMs to generate and\nrefine financial metrics with implicit domain-specific knowledge, (2) a scoring\nmechanism to ensure that evolved metrics generalize effectively to unseen data,\nand (3) an empirical demonstration of 3x predictive power for future\nrisk-returns, and 2x portfolio performance. Experimental results in a\nreal-world dataset highlight the superiority of discovered metrics, making them\nhighly relevant to portfolio managers and financial decision-makers. This\nframework not only addresses the limitations of existing metrics but also\nshowcases the potential of LLMs in advancing financial analytics, paving the\nway for informed and robust investment strategies.",
    "categories": [
      "q-fin.PM",
      "cs.AI",
      "cs.CL",
      "cs.NE",
      "q-fin.RM"
    ],
    "primary_category": "q-fin.PM",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2502.00029v2",
    "published_date": "2025-01-23 11:35:17 UTC",
    "updated_date": "2025-02-04 14:15:35 UTC"
  },
  {
    "arxiv_id": "2501.14000v2",
    "title": "Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture",
    "authors": [
      "Hy Nguyen",
      "Duy Khoa Pham",
      "Srikanth Thudumu",
      "Hung Du",
      "Rajesh Vasa",
      "Kon Mouzakis"
    ],
    "abstract": "The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed\nactivation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden\nlayers. While effective in many scenarios, this uniformity may limit the\nnetworks ability to capture complex data patterns. We argue that employing the\nsame activation function at every node is suboptimal and propose leveraging\ndifferent activation functions at each node to increase flexibility and\nadaptability. To achieve this, we introduce Local Control Networks (LCNs),\nwhich leverage B-spline functions to enable distinct activation curves at each\nnode. Our mathematical analysis demonstrates the properties and benefits of\nLCNs over conventional MLPs. In addition, we demonstrate that more complex\narchitectures, such as Kolmogorov-Arnold Networks (KANs), are unnecessary in\ncertain scenarios, and LCNs can be a more efficient alternative. Empirical\nexperiments on various benchmarks and datasets validate our theoretical\nfindings. In computer vision tasks, LCNs achieve marginal improvements over\nMLPs and outperform KANs by approximately 5\\%, while also being more\ncomputationally efficient than KANs. In basic machine learning tasks, LCNs show\na 1\\% improvement over MLPs and a 0.6\\% improvement over KANs. For symbolic\nformula representation tasks, LCNs perform on par with KANs, with both\narchitectures outperforming MLPs. Our findings suggest that diverse activations\nat the node level can lead to improved performance and efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14000v2",
    "published_date": "2025-01-23 11:34:25 UTC",
    "updated_date": "2025-04-25 05:19:40 UTC"
  },
  {
    "arxiv_id": "2501.13999v2",
    "title": "Framework for Progressive Knowledge Fusion in Large Language Models Through Structured Conceptual Redundancy Analysis",
    "authors": [
      "Joseph Sakau",
      "Evander Kozlowski",
      "Roderick Thistledown",
      "Basil Steinberger"
    ],
    "abstract": "The organization of latent knowledge within large-scale models poses unique\nchallenges when addressing overlapping representations and optimizing\ncontextual accuracy. Conceptual redundancies embedded across layers often\nresult in inefficiencies that affect both computational demands and\ntask-specific outcomes. A framework was proposed to restructure these\nredundancies through advanced clustering techniques and dynamic thresholding,\nensuring that critical semantic relationships are preserved while removing\nunnecessary overlaps. Evaluations revealed improved memory efficiency and\nfaster inference times, alongside better alignment in latent knowledge clusters\nthat enhanced interpretability. Improvements in error rates and adversarial\nrobustness suggest that restructuring redundancies has broader implications for\nincreasing model reliability across diverse applications. Comparative analyses\nhighlighted reductions in resource consumption and notable gains in\nperformance, particularly in translation and summarization tasks. Energy\nmetrics demonstrated significant savings during training phases, further\nvalidating the practicality of the approach for real-world deployments.\nRepresentational fidelity was also enhanced, with latent space evaluations\nindicating better cluster alignment and higher semantic consistency. The\nmethodology bridges a key gap in model optimization through directly addressing\nredundancies at the structural level. Its application opens avenues for\nscalable, efficient, and contextually aware systems that can adapt to complex,\ndomain-specific tasks without compromising on performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
    "pdf_url": "http://arxiv.org/pdf/2501.13999v2",
    "published_date": "2025-01-23 11:34:04 UTC",
    "updated_date": "2025-03-25 12:59:14 UTC"
  },
  {
    "arxiv_id": "2501.13567v2",
    "title": "K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor",
    "authors": [
      "Jeonghun Cho",
      "Gary Geunbae Lee"
    ],
    "abstract": "Retrieval-augmented question answering (QA) integrates external information\nand thereby increases the QA accuracy of reader models that lack domain\nknowledge. However, documents retrieved for closed domains require high\nexpertise, so the reader model may have difficulty fully comprehending the\ntext. Moreover, the retrieved documents contain thousands of tokens, some\nunrelated to the question. As a result, the documents include some inaccurate\ninformation, which could lead the reader model to mistrust the passages and\ncould result in hallucinations. To solve these problems, we propose K-comp\n(Knowledge-injected compressor) which provides the knowledge required to answer\ncorrectly. The compressor automatically generates the prior knowledge necessary\nto facilitate the answer process prior to compression of the retrieved\npassages. Subsequently, the passages are compressed autoregressively, with the\ngenerated knowledge being integrated into the compression process. This process\nensures alignment between the question intent and the compressed context. By\naugmenting this prior knowledge and concise context, the reader models are\nguided toward relevant answers and trust the context.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13567v2",
    "published_date": "2025-01-23 11:14:21 UTC",
    "updated_date": "2025-02-06 07:41:07 UTC"
  },
  {
    "arxiv_id": "2501.13563v1",
    "title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving",
    "authors": [
      "Lu Wang",
      "Tianyuan Zhang",
      "Yang Qu",
      "Siyuan Liang",
      "Yuwei Chen",
      "Aishan Liu",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving\n(AD) by enhancing reasoning capabilities; however, these models remain highly\nsusceptible to adversarial attacks. While existing research has explored\nwhite-box attacks to some extent, the more practical and challenging black-box\nscenarios remain largely underexplored due to their inherent difficulty. In\nthis paper, we take the first step toward designing black-box adversarial\nattacks specifically targeting VLMs in AD. We identify two key challenges for\nachieving effective black-box attacks in this context: the effectiveness across\ndriving reasoning chains in AD systems and the dynamic nature of driving\nscenarios. To address this, we propose Cascading Adversarial Disruption (CAD).\nIt first introduces Decision Chain Disruption, which targets low-level\nreasoning breakdown by generating and injecting deceptive semantics, ensuring\nthe perturbations remain effective across the entire decision-making chain.\nBuilding on this, we present Risky Scene Induction, which addresses dynamic\nadaptation by leveraging a surrogate VLM to understand and construct high-level\nrisky scenarios that are likely to result in critical errors in the current\ndriving contexts. Extensive experiments conducted on multiple AD VLMs and\nbenchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness,\nsignificantly outperforming existing methods (+13.43% on average). Moreover, we\nvalidate its practical applicability through real-world attacks on AD vehicles\npowered by VLMs, where the route completion rate drops by 61.11% and the\nvehicle crashes directly into the obstacle vehicle with adversarial patches.\nFinally, we release CADA dataset, comprising 18,808 adversarial\nvisual-question-answer pairs, to facilitate further evaluation and research in\nthis critical domain. Our codes and dataset will be available after paper's\nacceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13563v1",
    "published_date": "2025-01-23 11:10:02 UTC",
    "updated_date": "2025-01-23 11:10:02 UTC"
  },
  {
    "arxiv_id": "2501.13997v2",
    "title": "Predictive Learning in Energy-based Models with Attractor Structures",
    "authors": [
      "Xingsi Dong",
      "Xiangyuan Peng",
      "Si Wu"
    ],
    "abstract": "Predictive models are highly advanced in understanding the mechanisms of\nbrain function. Recent advances in machine learning further underscore the\npower of prediction for optimal representation in learning. However, there\nremains a gap in creating a biologically plausible model that explains how the\nneural system achieves prediction. In this paper, we introduce a framework that\nemploys an energy-based model (EBM) to capture the nuanced processes of\npredicting observation after action within the neural system, encompassing\nprediction, learning, and inference. We implement the EBM with a hierarchical\nstructure and integrate a continuous attractor neural network for memory,\nconstructing a biologically plausible model. In experimental evaluations, our\nmodel demonstrates efficacy across diverse scenarios. The range of actions\nincludes eye movement, motion in environments, head turning, and static\nobservation while the environment changes. Our model not only makes accurate\npredictions for environments it was trained on, but also provides reasonable\npredictions for unseen environments, matching the performances of machine\nlearning methods in multiple tasks. We hope that this study contributes to a\ndeep understanding of how the neural system performs prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13997v2",
    "published_date": "2025-01-23 11:04:25 UTC",
    "updated_date": "2025-05-21 15:37:25 UTC"
  },
  {
    "arxiv_id": "2501.13554v3",
    "title": "One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt",
    "authors": [
      "Tao Liu",
      "Kai Wang",
      "Senmao Li",
      "Joost van de Weijer",
      "Fahad Shahbaz Khan",
      "Shiqi Yang",
      "Yaxing Wang",
      "Jian Yang",
      "Ming-Ming Cheng"
    ],
    "abstract": "Text-to-image generation models can create high-quality images from input\nprompts. However, they struggle to support the consistent generation of\nidentity-preserving requirements for storytelling. Existing approaches to this\nproblem typically require extensive training in large datasets or additional\nmodifications to the original model architectures. This limits their\napplicability across different domains and diverse diffusion model\nconfigurations. In this paper, we first observe the inherent capability of\nlanguage models, coined context consistency, to comprehend identity through\ncontext with a single prompt. Drawing inspiration from the inherent context\nconsistency, we propose a novel training-free method for consistent\ntext-to-image (T2I) generation, termed \"One-Prompt-One-Story\" (1Prompt1Story).\nOur approach 1Prompt1Story concatenates all prompts into a single input for T2I\ndiffusion models, initially preserving character identities. We then refine the\ngeneration process using two novel techniques: Singular-Value Reweighting and\nIdentity-Preserving Cross-Attention, ensuring better alignment with the input\ndescription for each frame. In our experiments, we compare our method against\nvarious existing consistent T2I generation approaches to demonstrate its\neffectiveness through quantitative metrics and qualitative assessments. Code is\navailable at https://github.com/byliutao/1Prompt1Story.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "28 pages, 22 figures, ICLR2025 conference",
    "pdf_url": "http://arxiv.org/pdf/2501.13554v3",
    "published_date": "2025-01-23 10:57:22 UTC",
    "updated_date": "2025-02-05 10:32:22 UTC"
  },
  {
    "arxiv_id": "2501.13552v1",
    "title": "Explainable AI-aided Feature Selection and Model Reduction for DRL-based V2X Resource Allocation",
    "authors": [
      "Nasir Khan",
      "Asmaa Abdallah",
      "Abdulkadir Celik",
      "Ahmed M. Eltawil",
      "Sinem Coleri"
    ],
    "abstract": "Artificial intelligence (AI) is expected to significantly enhance radio\nresource management (RRM) in sixth-generation (6G) networks. However, the lack\nof explainability in complex deep learning (DL) models poses a challenge for\npractical implementation. This paper proposes a novel explainable AI (XAI)-\nbased framework for feature selection and model complexity reduction in a\nmodel-agnostic manner. Applied to a multi-agent deep reinforcement learning\n(MADRL) setting, our approach addresses the joint sub-band assignment and power\nallocation problem in cellular vehicle-to-everything (V2X) communications. We\npropose a novel two-stage systematic explainability framework leveraging\nfeature relevance-oriented XAI to simplify the DRL agents. While the former\nstage generates a state feature importance ranking of the trained models using\nShapley additive explanations (SHAP)-based importance scores, the latter stage\nexploits these importance-based rankings to simplify the state space of the\nagents by removing the least important features from the model input.\nSimulation results demonstrate that the XAI-assisted methodology achieves 97%\nof the original MADRL sum-rate performance while reducing optimal state\nfeatures by 28%, average training time by 11%, and trainable weight parameters\nby 46% in a network with eight vehicular pairs.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13552v1",
    "published_date": "2025-01-23 10:55:38 UTC",
    "updated_date": "2025-01-23 10:55:38 UTC"
  },
  {
    "arxiv_id": "2501.19407v2",
    "title": "Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces Intergenerational Inequality",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Pattie Maes"
    ],
    "abstract": "Surnames often convey implicit markers of social status, wealth, and lineage,\nshaping perceptions in ways that can perpetuate systemic biases and\nintergenerational inequality. This study is the first of its kind to\ninvestigate whether and how surnames influence AI-driven decision-making,\nfocusing on their effects across key areas such as hiring recommendations,\nleadership appointments, and loan approvals. Using 72,000 evaluations of 600\nsurnames from the United States and Thailand, two countries with distinct\nsociohistorical contexts and surname conventions, we classify names into four\ncategories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our\nfindings show that elite surnames consistently increase AI-generated\nperceptions of power, intelligence, and wealth, which in turn influence\nAI-driven decisions in high-stakes contexts. Mediation analysis reveals\nperceived intelligence as a key mechanism through which surname biases\ninfluence AI decision-making process. While providing objective qualifications\nalongside surnames mitigates most of these biases, it does not eliminate them\nentirely, especially in contexts where candidate credentials are low. These\nfindings highlight the need for fairness-aware algorithms and robust policy\nmeasures to prevent AI systems from reinforcing systemic inequalities tied to\nsurnames, an often-overlooked bias compared to more salient characteristics\nsuch as race and gender. Our work calls for a critical reassessment of\nalgorithmic accountability and its broader societal impact, particularly in\nsystems designed to uphold meritocratic principles while counteracting the\nperpetuation of intergenerational privilege.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY",
    "comment": "33 pages, 5 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2501.19407v2",
    "published_date": "2025-01-23 10:53:58 UTC",
    "updated_date": "2025-02-05 09:25:26 UTC"
  },
  {
    "arxiv_id": "2501.13545v1",
    "title": "LLMs Can Plan Only If We Tell Them",
    "authors": [
      "Bilgehan Sel",
      "Ruoxi Jia",
      "Ming Jin"
    ],
    "abstract": "Large language models (LLMs) have demonstrated significant capabilities in\nnatural language processing and reasoning, yet their effectiveness in\nautonomous planning has been under debate. While existing studies have utilized\nLLMs with external feedback mechanisms or in controlled environments for\nplanning, these approaches often involve substantial computational and\ndevelopment resources due to the requirement for careful design and iterative\nbackprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to\nmatch human performance on standard planning benchmarks, such as the\nBlocksworld, without additional support. This paper investigates whether LLMs\ncan independently generate long-horizon plans that rival human baselines. Our\nnovel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help\nachieve state-of-the-art results in planning benchmarks out-competing prior\nmethods and human baselines all autonomously.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13545v1",
    "published_date": "2025-01-23 10:46:14 UTC",
    "updated_date": "2025-01-23 10:46:14 UTC"
  },
  {
    "arxiv_id": "2501.13994v1",
    "title": "CSAOT: Cooperative Multi-Agent System for Active Object Tracking",
    "authors": [
      "Hy Nguyen",
      "Bao Pham",
      "Hung Du",
      "Srikanth Thudumu",
      "Rajesh Vasa",
      "Kon Mouzakis"
    ],
    "abstract": "Object Tracking is essential for many computer vision applications, such as\nautonomous navigation, surveillance, and robotics. Unlike Passive Object\nTracking (POT), which relies on static camera viewpoints to detect and track\nobjects across consecutive frames, Active Object Tracking (AOT) requires a\ncontroller agent to actively adjust its viewpoint to maintain visual contact\nwith a moving target in complex environments. Existing AOT solutions are\npredominantly single-agent-based, which struggle in dynamic and complex\nscenarios due to limited information gathering and processing capabilities,\noften resulting in suboptimal decision-making. Alleviating these limitations\nnecessitates the development of a multi-agent system where different agents\nperform distinct roles and collaborate to enhance learning and robustness in\ndynamic and complex environments. Although some multi-agent approaches exist\nfor AOT, they typically rely on external auxiliary agents, which require\nadditional devices, making them costly. In contrast, we introduce the\nCollaborative System for Active Object Tracking (CSAOT), a method that\nleverages multi-agent deep reinforcement learning (MADRL) and a Mixture of\nExperts (MoE) framework to enable multiple agents to operate on a single\ndevice, thereby improving tracking performance and reducing costs. Our approach\nenhances robustness against occlusions and rapid motion while optimizing camera\nmovements to extend tracking duration. We validated the effectiveness of CSAOT\non various interactive maps with dynamic and stationary obstacles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13994v1",
    "published_date": "2025-01-23 10:44:35 UTC",
    "updated_date": "2025-01-23 10:44:35 UTC"
  },
  {
    "arxiv_id": "2501.13993v1",
    "title": "CAPRAG: A Large Language Model Solution for Customer Service and Automatic Reporting using Vector and Graph Retrieval-Augmented Generation",
    "authors": [
      "Hamza Landolsi",
      "Kais Letaief",
      "Nizar Taghouti",
      "Ines Abdeljaoued-Tej"
    ],
    "abstract": "The introduction of new features and services in the banking sector often\noverwhelms customers, creating an opportunity for banks to enhance user\nexperience through financial chatbots powered by large language models (LLMs).\nWe initiated an AI agent designed to provide customers with relevant\ninformation about banking services and insights from annual reports. We\nproposed a hybrid Customer Analysis Pipeline Retrieval-Augmented Generation\n(CAPRAG) that effectively addresses both relationship-based and contextual\nqueries, thereby improving customer engagement in the digital banking\nlandscape. To implement this, we developed a processing pipeline to refine text\ndata, which we utilized in two main frameworks: Vector RAG and Graph RAG. This\ndual approach enables us to populate both vector and graph databases with\nprocessed data for efficient retrieval. The Cypher query component is employed\nto effectively query the graph database. When a user submits a query, it is\nfirst expanded by a query expansion module before being routed to construct a\nfinal query from the hybrid Knowledge Base (KB). This final query is then sent\nto an open-source LLM for response generation. Overall, our innovative,\ndesigned to international banks, serves bank's customers in an increasingly\ncomplex digital environment, enhancing clarity and accessibility of\ninformation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 5 Figures, 3 Tables",
    "pdf_url": "http://arxiv.org/pdf/2501.13993v1",
    "published_date": "2025-01-23 10:38:20 UTC",
    "updated_date": "2025-01-23 10:38:20 UTC"
  },
  {
    "arxiv_id": "2501.13533v1",
    "title": "Towards a Theory of AI Personhood",
    "authors": [
      "Francis Rhys Ward"
    ],
    "abstract": "I am a person and so are you. Philosophically we sometimes grant personhood\nto non-human animals, and entities such as sovereign states or corporations can\nlegally be considered persons. But when, if ever, should we ascribe personhood\nto AI systems? In this paper, we outline necessary conditions for AI\npersonhood, focusing on agency, theory-of-mind, and self-awareness. We discuss\nevidence from the machine learning literature regarding the extent to which\ncontemporary AI systems, such as language models, satisfy these conditions,\nfinding the evidence surprisingly inconclusive.\n  If AI systems can be considered persons, then typical framings of AI\nalignment may be incomplete. Whereas agency has been discussed at length in the\nliterature, other aspects of personhood have been relatively neglected. AI\nagents are often assumed to pursue fixed goals, but AI persons may be\nself-aware enough to reflect on their aims, values, and positions in the world\nand thereby induce their goals to change. We highlight open research directions\nto advance the understanding of AI personhood and its relevance to alignment.\nFinally, we reflect on the ethical considerations surrounding the treatment of\nAI systems. If AI systems are persons, then seeking control and alignment may\nbe ethically untenable.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI-25 AI Alignment Track",
    "pdf_url": "http://arxiv.org/pdf/2501.13533v1",
    "published_date": "2025-01-23 10:31:26 UTC",
    "updated_date": "2025-01-23 10:31:26 UTC"
  },
  {
    "arxiv_id": "2501.13992v2",
    "title": "Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization",
    "authors": [
      "Hy Nguyen",
      "Nguyen Hung Nguyen",
      "Nguyen Linh Bao Nguyen",
      "Srikanth Thudumu",
      "Hung Du",
      "Rajesh Vasa",
      "Kon Mouzakis"
    ],
    "abstract": "The Hierarchical Navigable Small World (HNSW) algorithm is widely used for\napproximate nearest neighbor (ANN) search, leveraging the principles of\nnavigable small-world graphs. However, it faces some limitations. The first is\nthe local optima problem, which arises from the algorithm's greedy search\nstrategy, selecting neighbors based solely on proximity at each step. This\noften leads to cluster disconnections. The second limitation is that HNSW\nfrequently fails to achieve logarithmic complexity, particularly in\nhigh-dimensional datasets, due to the exhaustive traversal through each layer.\nTo address these limitations, we propose a novel algorithm that mitigates local\noptima and cluster disconnections while enhancing the construction speed,\nmaintaining inference speed. The first component is a dual-branch HNSW\nstructure with LID-based insertion mechanisms, enabling traversal from multiple\ndirections. This improves outlier node capture, enhances cluster connectivity,\naccelerates construction speed and reduces the risk of local minima. The second\ncomponent incorporates a bridge-building technique that bypasses redundant\nintermediate layers, maintaining inference and making up the additional\ncomputational overhead introduced by the dual-branch structure. Experiments on\nvarious benchmarks and datasets showed that our algorithm outperforms the\noriginal HNSW in both accuracy and speed. We evaluated six datasets across\nComputer Vision (CV), and Natural Language Processing (NLP), showing recall\nimprovements of 18\\% in NLP, and up to 30\\% in CV tasks while reducing the\nconstruction time by up to 20\\% and maintaining the inference speed. We did not\nobserve any trade-offs in our algorithm. Ablation studies revealed that\nLID-based insertion had the greatest impact on performance, followed by the\ndual-branch structure and bridge-building components.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13992v2",
    "published_date": "2025-01-23 10:20:12 UTC",
    "updated_date": "2025-04-25 07:22:17 UTC"
  },
  {
    "arxiv_id": "2501.17883v1",
    "title": "Explainable and Robust Millimeter Wave Beam Alignment for AI-Native 6G Networks",
    "authors": [
      "Nasir Khan",
      "Asmaa Abdallah",
      "Abdulkadir Celik",
      "Ahmed M. Eltawil",
      "Sinem Coleri"
    ],
    "abstract": "Integrated artificial intelligence (AI) and communication has been recognized\nas a key pillar of 6G and beyond networks. In line with AI-native 6G vision,\nexplainability and robustness in AI-driven systems are critical for\nestablishing trust and ensuring reliable performance in diverse and evolving\nenvironments. This paper addresses these challenges by developing a robust and\nexplainable deep learning (DL)-based beam alignment engine (BAE) for\nmillimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. The\nproposed convolutional neural network (CNN)-based BAE utilizes received signal\nstrength indicator (RSSI) measurements over a set of wide beams to accurately\npredict the best narrow beam for each UE, significantly reducing the overhead\nassociated with exhaustive codebook-based narrow beam sweeping for initial\naccess (IA) and data transmission. To ensure transparency and resilience, the\nDeep k-Nearest Neighbors (DkNN) algorithm is employed to assess the internal\nrepresentations of the network via nearest neighbor approach, providing\nhuman-interpretable explanations and confidence metrics for detecting\nout-of-distribution inputs. Experimental results demonstrate that the proposed\nDL-based BAE exhibits robustness to measurement noise, reduces beam training\noverhead by 75% compared to the exhaustive search while maintaining\nnear-optimal performance in terms of spectral efficiency. Moreover, the\nproposed framework improves outlier detection robustness by up to 5x and offers\nclearer insights into beam prediction decisions compared to traditional\nsoftmax-based classifiers.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.17883v1",
    "published_date": "2025-01-23 09:47:54 UTC",
    "updated_date": "2025-01-23 09:47:54 UTC"
  },
  {
    "arxiv_id": "2501.13991v1",
    "title": "CGI: Identifying Conditional Generative Models with Example Images",
    "authors": [
      "Zhi Zhou",
      "Hao-Zhe Tan",
      "Peng-Xiao Song",
      "Lan-Zhe Guo"
    ],
    "abstract": "Generative models have achieved remarkable performance recently, and thus\nmodel hubs have emerged. Existing model hubs typically assume basic text\nmatching is sufficient to search for models. However, in reality, due to\ndifferent abstractions and the large number of models in model hubs, it is not\neasy for users to review model descriptions and example images, choosing which\nmodel best meets their needs. Therefore, it is necessary to describe model\nfunctionality wisely so that future users can efficiently search for the most\nsuitable model for their needs. Efforts to address this issue remain limited.\nIn this paper, we propose Conditional Generative Model Identification (CGI),\nwhich aims to provide an effective way to identify the most suitable model\nusing user-provided example images rather than requiring users to manually\nreview a large number of models with example images. To address this problem,\nwe propose the PromptBased Model Identification (PMI) , which can adequately\ndescribe model functionality and precisely match requirements with\nspecifications. To evaluate PMI approach and promote related research, we\nprovide a benchmark comprising 65 models and 9100 identification tasks.\nExtensive experimental and human evaluation results demonstrate that PMI is\neffective. For instance, 92% of models are correctly identified with\nsignificantly better FID scores when four example images are provided.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13991v1",
    "published_date": "2025-01-23 09:31:06 UTC",
    "updated_date": "2025-01-23 09:31:06 UTC"
  },
  {
    "arxiv_id": "2501.13493v1",
    "title": "GCAD: Anomaly Detection in Multivariate Time Series from the Perspective of Granger Causality",
    "authors": [
      "Zehao Liu",
      "Mengzhou Gao",
      "Pengfei Jiao"
    ],
    "abstract": "Multivariate time series anomaly detection has numerous real-world\napplications and is being extensively studied. Modeling pairwise correlations\nbetween variables is crucial. Existing methods employ learnable graph\nstructures and graph neural networks to explicitly model the spatial\ndependencies between variables. However, these methods are primarily based on\nprediction or reconstruction tasks, which can only learn similarity\nrelationships between sequence embeddings and lack interpretability in how\ngraph structures affect time series evolution. In this paper, we designed a\nframework that models spatial dependencies using interpretable causal\nrelationships and detects anomalies through changes in causal patterns.\nSpecifically, we propose a method to dynamically discover Granger causality\nusing gradients in nonlinear deep predictors and employ a simple sparsification\nstrategy to obtain a Granger causality graph, detecting anomalies from a causal\nperspective. Experiments on real-world datasets demonstrate that the proposed\nmodel achieves more accurate anomaly detection compared to baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.5.1"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13493v1",
    "published_date": "2025-01-23 09:15:59 UTC",
    "updated_date": "2025-01-23 09:15:59 UTC"
  },
  {
    "arxiv_id": "2501.13491v1",
    "title": "RECALL: Library-Like Behavior In Language Models is Enhanced by Self-Referencing Causal Cycles",
    "authors": [
      "Munachiso Nwadike",
      "Zangir Iklassov",
      "Toluwani Aremu",
      "Tatsuya Hiraoka",
      "Velibor Bojkovic",
      "Benjamin Heinzerling",
      "Hilal Alqaubeh",
      "Martin Takáč",
      "Kentaro Inui"
    ],
    "abstract": "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps://anonymous.4open.science/r/remember-B0B8/.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13491v1",
    "published_date": "2025-01-23 09:14:07 UTC",
    "updated_date": "2025-01-23 09:14:07 UTC"
  },
  {
    "arxiv_id": "2501.13484v3",
    "title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods",
    "authors": [
      "Zukang Xu",
      "Yuxuan Yue",
      "Xing Hu",
      "Zhihang Yuan",
      "Zixu Jiang",
      "Zhixuan Chen",
      "Jiangyong Yu",
      "Chen Xu",
      "Sifan Zhou",
      "Dawei Yang"
    ],
    "abstract": "Mamba is an efficient sequence model that rivals Transformers and\ndemonstrates significant potential as a foundational architecture for various\ntasks. Quantization is commonly used in neural networks to reduce model size\nand computational latency. However, applying quantization to Mamba remains\nunderexplored, and existing quantization methods, which have been effective for\nCNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot\nsuffers a 21% accuracy drop on Vim-T$^\\dagger$ even under W8A8). We have\npioneered the exploration of this issue and identified several key challenges.\nFirst, significant outliers are present in gate projections, output\nprojections, and matrix multiplications. Second, Mamba's unique parallel scan\nfurther amplifies these outliers, leading to uneven and heavy-tailed data\ndistributions. Third, even with the application of the Hadamard transform, the\nvariance across channels in weights and activations still remains inconsistent.\nTo these ends, we propose MambaQuant, a post-training quantization (PTQ)\nframework consisting of: 1) Karhunen-Loeve Transformation (KLT) enhanced\nrotation, rendering the rotation matrix adaptable to diverse channel\ndistributions. 2) Smooth-Fused rotation, which equalizes channel variances and\ncan merge additional parameters into model weights. Experiments show that\nMambaQuant can quantize both weights and activations into 8-bit with less than\n1% accuracy loss for Mamba-based vision and language tasks. To the best of our\nknowledge, MambaQuant is the first comprehensive PTQ design for the Mamba\nfamily, paving the way for further advancements in its application.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13484v3",
    "published_date": "2025-01-23 08:57:33 UTC",
    "updated_date": "2025-03-11 06:49:47 UTC"
  },
  {
    "arxiv_id": "2501.13989v1",
    "title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting",
    "authors": [
      "Wenzhen Yue",
      "Yong Liu",
      "Xianghua Ying",
      "Bowei Xing",
      "Ruohao Guo",
      "Ji Shi"
    ],
    "abstract": "This paper presents \\textbf{FreEformer}, a simple yet effective model that\nleverages a \\textbf{Fre}quency \\textbf{E}nhanced Trans\\textbf{former} for\nmultivariate time series forecasting. Our work is based on the assumption that\nthe frequency spectrum provides a global perspective on the composition of\nseries across various frequencies and is highly suitable for robust\nrepresentation learning. Specifically, we first convert time series into the\ncomplex frequency domain using the Discrete Fourier Transform (DFT). The\nTransformer architecture is then applied to the frequency spectra to capture\ncross-variate dependencies, with the real and imaginary parts processed\nindependently. However, we observe that the vanilla attention matrix exhibits a\nlow-rank characteristic, thus limiting representation diversity. This could be\nattributed to the inherent sparsity of the frequency domain and the\nstrong-value-focused nature of Softmax in vanilla attention. To address this,\nwe enhance the vanilla attention mechanism by introducing an additional\nlearnable matrix to the original attention matrix, followed by row-wise L1\nnormalization. Theoretical analysis~demonstrates that this enhanced attention\nmechanism improves both feature diversity and gradient flow. Extensive\nexperiments demonstrate that FreEformer consistently outperforms\nstate-of-the-art models on eighteen real-world benchmarks covering electricity,\ntraffic, weather, healthcare and finance. Notably, the enhanced attention\nmechanism also consistently improves the performance of state-of-the-art\nTransformer-based forecasters.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13989v1",
    "published_date": "2025-01-23 08:53:45 UTC",
    "updated_date": "2025-01-23 08:53:45 UTC"
  },
  {
    "arxiv_id": "2501.13481v1",
    "title": "A Polynomial-Time Algorithm for EFX Orientations of Chores",
    "authors": [
      "Kevin Hsu",
      "Valerie King"
    ],
    "abstract": "This paper addresses the problem of finding EFX orientations of graphs of\nchores, in which each vertex corresponds to an agent, each edge corresponds to\na chore, and a chore has zero marginal utility to an agent if its corresponding\nedge is not incident to the vertex corresponding to the agent. Recently,\nZhou~et~al.~(IJCAI,~2024) analyzed the complexity of deciding whether graphs\ncontaining a mixture of goods and chores admit EFX orientations, and\nconjectured that deciding whether graphs containing only chores admit EFX\norientations is NP-complete. In this paper, we resolve this conjecture by\nexhibiting a polynomial-time algorithm that finds an EFX orientation of a graph\ncontaining only chores if one exists, even if the graph contains self-loops.\nRemarkably, our first result demonstrates a surprising separation between the\ncase of goods and the case of chores, because deciding whether graphs\ncontaining only goods admit EFX orientations of goods was shown to be\nNP-complete by Christodoulou et al.~(EC,~2023). In addition, we show the\nanalogous decision problem for multigraphs to be NP-complete.",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DM"
    ],
    "primary_category": "cs.GT",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13481v1",
    "published_date": "2025-01-23 08:53:18 UTC",
    "updated_date": "2025-01-23 08:53:18 UTC"
  },
  {
    "arxiv_id": "2501.13480v1",
    "title": "Adaptive Testing for LLM-Based Applications: A Diversity-based Approach",
    "authors": [
      "Juyeon Yoon",
      "Robert Feldt",
      "Shin Yoo"
    ],
    "abstract": "The recent surge of building software systems powered by Large Language\nModels (LLMs) has led to the development of various testing frameworks,\nprimarily focused on treating prompt templates as the unit of testing. Despite\nthe significant costs associated with test input execution and output\nassessment, the curation of optimized test suites is yet overlooked in these\ntools, which calls for tailored test selection or prioritization strategies. In\nthis paper, we show that diversity-based testing techniques, such as Adaptive\nRandom Testing (ART) with appropriate string distance metrics, can be\neffectively applied to the testing of prompt templates. Our proposed adaptive\ntesting approach adjusts the conventional ART process to this context by\nselecting new test inputs based on scores derived from existing test suite and\ntheir labelling results. Our results, obtained using various implementations\nthat explore several string-based distances, confirm that our approach enables\nthe discovery of failures with reduced testing budgets and promotes the\ngeneration of more varied outputs.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13480v1",
    "published_date": "2025-01-23 08:53:12 UTC",
    "updated_date": "2025-01-23 08:53:12 UTC"
  },
  {
    "arxiv_id": "2501.13479v1",
    "title": "Adaptive Few-Shot Learning (AFSL): Tackling Data Scarcity with Stability, Robustness, and Versatility",
    "authors": [
      "Rishabh Agrawal"
    ],
    "abstract": "Few-shot learning (FSL) enables machine learning models to generalize\neffectively with minimal labeled data, making it crucial for data-scarce\ndomains such as healthcare, robotics, and natural language processing. Despite\nits potential, FSL faces challenges including sensitivity to initialization,\ndifficulty in adapting to diverse domains, and vulnerability to noisy datasets.\nTo address these issues, this paper introduces Adaptive Few-Shot Learning\n(AFSL), a framework that integrates advancements in meta-learning, domain\nalignment, noise resilience, and multi-modal integration. AFSL consists of four\nkey modules: a Dynamic Stability Module for performance consistency, a\nContextual Domain Alignment Module for domain adaptation, a Noise-Adaptive\nResilience Module for handling noisy data, and a Multi-Modal Fusion Module for\nintegrating diverse modalities. This work also explores strategies such as\ntask-aware data augmentation, semi-supervised learning, and explainable AI\ntechniques to enhance the applicability and robustness of FSL. AFSL provides\nscalable, reliable, and impactful solutions for real-world, high-stakes\ndomains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13479v1",
    "published_date": "2025-01-23 08:51:49 UTC",
    "updated_date": "2025-01-23 08:51:49 UTC"
  },
  {
    "arxiv_id": "2501.13468v1",
    "title": "Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge",
    "authors": [
      "Haomiao Xiong",
      "Zongxin Yang",
      "Jiazuo Yu",
      "Yunzhi Zhuge",
      "Lu Zhang",
      "Jiawen Zhu",
      "Huchuan Lu"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have enabled the development\nof Video-LLMs, advancing multimodal learning by bridging video data with\nlanguage tasks. However, current video understanding models struggle with\nprocessing long video sequences, supporting multi-turn dialogues, and adapting\nto real-world dynamic scenarios. To address these issues, we propose\nStreamChat, a training-free framework for streaming video reasoning and\nconversational interaction. $\\StreamChat$ leverages a novel hierarchical memory\nsystem to efficiently process and compress video features over extended\nsequences, enabling real-time, multi-turn dialogue. Our framework incorporates\na parallel system scheduling strategy that enhances processing speed and\nreduces latency, ensuring robust performance in real-world applications.\nFurthermore, we introduce StreamBench, a versatile benchmark that evaluates\nstreaming video understanding across diverse media types and interactive\nscenarios, including multi-turn interactions and complex reasoning tasks.\nExtensive evaluations on StreamBench and other public benchmarks demonstrate\nthat StreamChat significantly outperforms existing state-of-the-art models in\nterms of accuracy and response times, confirming its effectiveness for\nstreaming video understanding. Code is available at StreamChat:\nhttps://github.com/hmxiong/StreamChat.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/hmxiong/StreamChat",
    "pdf_url": "http://arxiv.org/pdf/2501.13468v1",
    "published_date": "2025-01-23 08:33:10 UTC",
    "updated_date": "2025-01-23 08:33:10 UTC"
  },
  {
    "arxiv_id": "2501.13988v1",
    "title": "MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road Environmental Perception",
    "authors": [
      "Yi Yang",
      "Zhang Zhang",
      "Liang Wang"
    ],
    "abstract": "Most studies on environmental perception for autonomous vehicles (AVs) focus\non urban traffic environments, where the objects/stuff to be perceived are\nmainly from man-made scenes and scalable datasets with dense annotations can be\nused to train supervised learning models. By contrast, it is hard to densely\nannotate a large-scale off-road driving dataset manually due to the inherently\nunstructured nature of off-road environments. In this paper, we propose a\nMultimodal Contrastive Representation Learning approach for Off-Road\nenvironmental perception, namely MCRL4OR. This approach aims to jointly learn\nthree encoders for processing visual images, locomotion states, and control\nactions by aligning the locomotion states with the fused features of visual\nimages and control actions within a contrastive learning framework. The\ncausation behind this alignment strategy is that the inertial locomotion state\nis the result of taking a certain control action under the current\nlandform/terrain condition perceived by visual sensors. In experiments, we\npre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the\nlearned multimodal representations for various downstream perception tasks in\noff-road driving scenarios. The superior performance in downstream tasks\ndemonstrates the advantages of the pre-trained multimodal representations. The\ncodes can be found in \\url{https://github.com/1uciusy/MCRL4OR}.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Github repository: https://github.com/1uciusy/MCRL4OR",
    "pdf_url": "http://arxiv.org/pdf/2501.13988v1",
    "published_date": "2025-01-23 08:27:15 UTC",
    "updated_date": "2025-01-23 08:27:15 UTC"
  },
  {
    "arxiv_id": "2501.13987v1",
    "title": "OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting",
    "authors": [
      "Xing Hu",
      "Yuan Cheng",
      "Dawei Yang",
      "Zukang Xu",
      "Zhihang Yuan",
      "Jiangyong Yu",
      "Chen Xu",
      "Zhe Jiang",
      "Sifan Zhou"
    ],
    "abstract": "Post-training quantization (PTQ) has emerged as a widely adopted technique\nfor compressing and accelerating Large Language Models (LLMs). The major\nchallenge in LLM quantization is that uneven and heavy-tailed data\ndistributions can expand the quantization range, thereby reducing bit precision\nfor most values. Recent methods attempt to eliminate outliers and balance\ninter-channel differences by employing linear transformations; however, they\nremain heuristic and are often overlook optimizing the data distribution across\nthe entire quantization space.In this paper, we introduce Quantization Space\nUtilization Rate (QSUR), a novel metric that effectively assesses the\nquantizability of transformed data by measuring the space utilization of the\ndata in the quantization space. We complement QSUR with mathematical\nderivations that examine the effects and limitations of various\ntransformations, guiding our development of Orthogonal and Scaling\nTransformation-based Quantization (OSTQuant). OSQuant employs a learnable\nequivalent transformation, consisting of an orthogonal transformation and a\nscaling transformation, to optimize the distributions of weights and\nactivations across the entire quantization space. Futhermore, we propose the\nKL-Top loss function, designed to mitigate noise during optimization while\nretaining richer semantic information within the limited calibration data\nimposed by PTQ. OSTQuant outperforms existing work on various LLMs and\nbenchmarks. In the W4-only setting, it retains 99.5\\% of the floating-point\naccuracy. In the more challenging W4A4KV4 configuration, OSTQuant reduces the\nperformance gap by 32\\% on the LLaMA-3-8B model compared to state-of-the-art\nmethods.\n\\href{https://github.com/BrotherHappy/OSTQuant}{https://github.com/BrotherHappy/OSTQuant}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 Pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13987v1",
    "published_date": "2025-01-23 08:24:25 UTC",
    "updated_date": "2025-01-23 08:24:25 UTC"
  },
  {
    "arxiv_id": "2501.13986v4",
    "title": "An Efficient Sparse Kernel Generator for O(3)-Equivariant Deep Networks",
    "authors": [
      "Vivek Bharadwaj",
      "Austin Glover",
      "Aydin Buluc",
      "James Demmel"
    ],
    "abstract": "Rotation equivariant graph neural networks, i.e. networks designed to\nguarantee certain geometric relations between their inputs and outputs, yield\nstate of the art performance on spatial deep learning tasks. They exhibit high\ndata efficiency during training and significantly reduced inference time for\ninteratomic potential calculations compared to classical approaches. Key to\nthese models is the Clebsch-Gordon (CG) tensor product, a kernel that contracts\ntwo dense feature vectors with a highly-structured sparse tensor to produce a\ndense output vector. The operation, which may be repeated millions of times for\ntypical equivariant models, is a costly and inefficient bottleneck. We\nintroduce a GPU sparse kernel generator for the CG tensor product that provides\nsignificant speedups over the best existing open and closed-source\nimplementations. Our implementation achieves high performance by carefully\nmanaging the limited GPU shared memory through static analysis at model\ncompile-time, minimizing reads and writes to global memory. We break the tensor\nproduct into a series of smaller kernels with operands that fit entirely into\nregisters, enabling us to emit long arithmetic instruction streams that\nmaximize instruction-level parallelism. By fusing the CG tensor product with a\nsubsequent graph convolution, we reduce both intermediate storage and global\nmemory traffic over naive approaches that duplicate input data. We also provide\noptimized kernels for the gradient of the CG tensor product and a novel\nidentity for the higher partial derivatives required to predict interatomic\nforces. Our kernels offer up to 1.3x speedup over NVIDIA's closed-source\ncuEquivariance package, as well as 10x speedup over the widely-used e3nn\npackage. In FP64 precision, we offer up to 6.2x inference-time speedup for the\nMACE chemistry foundation model over the original unoptimized version.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in the Proceedings of the 2025 SIAM Conference on Applied\n  and Computational Discrete Algorithms. 15 pages, 10 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2501.13986v4",
    "published_date": "2025-01-23 08:20:47 UTC",
    "updated_date": "2025-05-08 23:11:05 UTC"
  },
  {
    "arxiv_id": "2501.13457v1",
    "title": "Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks",
    "authors": [
      "Ruijia Liu",
      "Ancheng Hou",
      "Xiao Yu",
      "Xiang Yin"
    ],
    "abstract": "Signal Temporal Logic (STL) is a powerful specification language for\ndescribing complex temporal behaviors of continuous signals, making it\nwell-suited for high-level robotic task descriptions. However, generating\nexecutable plans for STL tasks is challenging, as it requires consideration of\nthe coupling between the task specification and the system dynamics. Existing\napproaches either follow a model-based setting that explicitly requires\nknowledge of the system dynamics or adopt a task-oriented data-driven approach\nto learn plans for specific tasks. In this work, we investigate the problem of\ngenerating executable STL plans for systems whose dynamics are unknown a\npriori. We propose a new planning framework that uses only task-agnostic data\nduring the offline training stage, enabling zero-shot generalization to new STL\ntasks. Our framework is hierarchical, involving: (i) decomposing the STL task\ninto a set of progress and time constraints, (ii) searching for time-aware\nwaypoints guided by task-agnostic data, and (iii) generating trajectories using\na pre-trained safe diffusion model. Simulation results demonstrate the\neffectiveness of our method indeed in achieving zero-shot generalization to\nvarious STL tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "submitted",
    "pdf_url": "http://arxiv.org/pdf/2501.13457v1",
    "published_date": "2025-01-23 08:15:52 UTC",
    "updated_date": "2025-01-23 08:15:52 UTC"
  },
  {
    "arxiv_id": "2501.13456v4",
    "title": "KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks",
    "authors": [
      "Taoran Fang",
      "Tianhong Gao",
      "Chunping Wang",
      "Yihao Shang",
      "Wei Chow",
      "Lei Chen",
      "Yang Yang"
    ],
    "abstract": "Graph neural networks (GNNs) with attention mechanisms, often referred to as\nattentive GNNs, have emerged as a prominent paradigm in advanced GNN models in\nrecent years. However, our understanding of the critical process of scoring\nneighbor nodes remains limited, leading to the underperformance of many\nexisting attentive GNNs. In this paper, we unify the scoring functions of\ncurrent attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which\nintegrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring\nprocess. KAA enhances the performance of scoring functions across the board and\ncan be applied to nearly all existing attentive GNNs. To compare the expressive\npower of KAA with other scoring functions, we introduce Maximum Ranking\nDistance (MRD) to quantitatively estimate their upper bounds in ranking errors\nfor node importance. Our analysis reveals that, under limited parameters and\nconstraints on width and depth, both linear transformation-based and MLP-based\nscoring functions exhibit finite expressive power. In contrast, our proposed\nKAA, even with a single-layer KAN parameterized by zero-order B-spline\nfunctions, demonstrates nearly infinite expressive power. Extensive experiments\non both node-level and graph-level tasks using various backbone models show\nthat KAA-enhanced scoring functions consistently outperform their original\ncounterparts, achieving performance improvements of over 20% in some cases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13456v4",
    "published_date": "2025-01-23 08:14:55 UTC",
    "updated_date": "2025-03-11 08:59:12 UTC"
  },
  {
    "arxiv_id": "2501.13448v1",
    "title": "BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch",
    "authors": [
      "Yulong Hu",
      "Siyuan Feng",
      "Sen Li"
    ],
    "abstract": "This paper introduces Localized Bipartite Match Graph Attention Q-Learning\n(BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework\ntailored for ride-pooling order dispatch. BMG-Q advances ride-pooling\ndecision-making process with the localized bipartite match graph underlying the\nMarkov Decision Process, enabling the development of novel Graph Attention\nDouble Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic\ninteractions among ride-pooling vehicles in fleet. Our approach enriches the\nstate information for each agent with GATDDQN by leveraging a localized\nbipartite interdependence graph and enables a centralized global coordinator to\noptimize order matching and agent behavior using Integer Linear Programming\n(ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN\nimproves scalability and robustness. Furthermore, the inclusion of a posterior\nscore function in the ILP captures the online exploration-exploitation\ntrade-off and reduces the potential overestimation bias of agents, thereby\nelevating the quality of the derived solutions. Through extensive experiments\nand validation, BMG-Q has demonstrated superior performance in both training\nand operations for thousands of vehicle agents, outperforming benchmark\nreinforcement learning frameworks by around 10% in accumulative rewards and\nshowing a significant reduction in overestimation bias by over 50%.\nAdditionally, it maintains robustness amidst task variations and fleet size\nchanges, establishing BMG-Q as an effective, scalable, and robust framework for\nadvancing ride-pooling order dispatch operations.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13448v1",
    "published_date": "2025-01-23 08:01:24 UTC",
    "updated_date": "2025-01-23 08:01:24 UTC"
  },
  {
    "arxiv_id": "2501.13985v1",
    "title": "Pilot: Building the Federated Multimodal Instruction Tuning Framework",
    "authors": [
      "Baochen Xiong",
      "Xiaoshan Yang",
      "Yaguang Song",
      "Yaowei Wang",
      "Changsheng Xu"
    ],
    "abstract": "In this paper, we explore a novel federated multimodal instruction tuning\ntask(FedMIT), which is significant for collaboratively fine-tuning MLLMs on\ndifferent types of multimodal instruction data on distributed devices. To solve\nthe new task, we propose a federated multimodal instruction tuning\nframework(Pilot). Our framework integrates two stages of \"adapter on adapter\"\ninto the connector of the vision encoder and the LLM. In stage 1, we extract\ntask-specific features and client-specific features from visual information. In\nstage 2, we build the cross-task Mixture-of-Adapters(CT-MoA) module to perform\ncross-task interaction. Each client can not only capture personalized\ninformation of local data and learn task-related multimodal information, but\nalso learn general knowledge from other tasks. In addition, we introduce an\nadaptive parameter aggregation strategy for text training parameters, which\noptimizes parameter aggregation by calculating weights based on the euclidean\ndistance between parameters, so that parameter aggregation can benefit from\npositive effects to the greatest extent while effectively reducing negative\neffects. Our framework can collaboratively exploit distributed data from\ndifferent local clients to learn cross-task knowledge without being affected by\nthe task heterogeneity during instruction tuning. The effectiveness of our\nmethod is verified in two different cross-task scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13985v1",
    "published_date": "2025-01-23 07:49:24 UTC",
    "updated_date": "2025-01-23 07:49:24 UTC"
  },
  {
    "arxiv_id": "2501.13439v1",
    "title": "One-cycle Structured Pruning with Stability Driven Structure Search",
    "authors": [
      "Deepak Ghimire",
      "Dayoung Kil",
      "Seonghwan Jeong",
      "Jaesik Park",
      "Seong-heum Kim"
    ],
    "abstract": "Existing structured pruning typically involves multi-stage training\nprocedures that often demand heavy computation. Pruning at initialization,\nwhich aims to address this limitation, reduces training costs but struggles\nwith performance. To address these challenges, we propose an efficient\nframework for one-cycle structured pruning without compromising model\nperformance. In this approach, we integrate pre-training, pruning, and\nfine-tuning into a single training cycle, referred to as the `one cycle\napproach'. The core idea is to search for the optimal sub-network during the\nearly stages of network training, guided by norm-based group saliency criteria\nand structured sparsity regularization. We introduce a novel pruning indicator\nthat determines the stable pruning epoch by assessing the similarity between\nevolving pruning sub-networks across consecutive training epochs. Also, group\nsparsity regularization helps to accelerate the pruning process and results in\nspeeding up the entire process. Extensive experiments on datasets, including\nCIFAR-10/100, and ImageNet, using VGGNet, ResNet, MobileNet, and ViT\narchitectures, demonstrate that our method achieves state-of-the-art accuracy\nwhile being one of the most efficient pruning frameworks in terms of training\ntime. The source code will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.13439v1",
    "published_date": "2025-01-23 07:46:48 UTC",
    "updated_date": "2025-01-23 07:46:48 UTC"
  },
  {
    "arxiv_id": "2501.13428v3",
    "title": "Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models",
    "authors": [
      "Bo Gao",
      "Michael W. Spratling"
    ],
    "abstract": "Large language models have achieved remarkable success in recent years,\nprimarily due to the implementation of self-attention mechanisms. However,\ntraditional Softmax attention suffers from numerical instability and reduced\nperformance as the length of inference tokens increases. This paper addresses\nthese issues by decomposing the Softmax operation into a non-linear\ntransformation and the $l_1$-norm. We identify the latter as essential for\nmaintaining model performance. By replacing the non-linear transformation with\nthe Softplus activation function and introducing a dynamic scale factor for\ndifferent token lengths based on invariance entropy, we create a novel\nattention mechanism with performance better than conventional Softmax attention\nacross various inference lengths. To further improve the length extrapolation\nability of the proposed attention mechanism, we introduce a novel re-weighting\nmechanism that amplifies significant attention weights while diminishing weaker\nones, enabling the model to concentrate more effectively on relevant tokens.\nWhen combined with our proposed attention mechanism, this approach maintains\nnearly constant validation loss even at 16$\\times$ the training token length,\nensures numerical stability, and achieves superior results on downstream\nbenchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages and 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.13428v3",
    "published_date": "2025-01-23 07:21:08 UTC",
    "updated_date": "2025-05-12 03:16:04 UTC"
  },
  {
    "arxiv_id": "2501.13984v1",
    "title": "Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs",
    "authors": [
      "Bhumika Gupta",
      "Pralaypati Ta",
      "Keerthi Ram",
      "Mohanasankar Sivaprakasam"
    ],
    "abstract": "The updated recommendations on diagnostic procedures and treatment pathways\nfor a medical condition are documented as graphical flows in Clinical Practice\nGuidelines (CPGs). For effective use of the CPGs in helping medical\nprofessionals in the treatment decision process, it is necessary to fully\ncapture the guideline knowledge, particularly the contexts and their\nrelationships in the graph. While several existing works have utilized these\nguidelines to create rule bases for Clinical Decision Support Systems, limited\nwork has been done toward directly capturing the full medical knowledge\ncontained in CPGs. This work proposes an approach to create a contextually\nenriched, faithful digital representation of National Comprehensive Cancer\nNetwork (NCCN) Cancer CPGs in the form of graphs using automated extraction and\nnode & relationship classification. We also implement semantic enrichment of\nthe model by using Large Language Models (LLMs) for node classification,\nachieving an accuracy of 80.86% and 88.47% with zero-shot learning and few-shot\nlearning, respectively. Additionally, we introduce a methodology for answering\nnatural language questions with constraints to guideline text by leveraging\nLLMs to extract the relevant subgraph from the guideline knowledge base. By\ngenerating natural language answers based on subgraph paths and semantic\ninformation, we mitigate the risk of incorrect answers and hallucination\nassociated with LLMs, ensuring factual accuracy in medical domain Question\nAnswering.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13984v1",
    "published_date": "2025-01-23 07:06:26 UTC",
    "updated_date": "2025-01-23 07:06:26 UTC"
  },
  {
    "arxiv_id": "2501.13983v4",
    "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models",
    "authors": [
      "Yang Fan"
    ],
    "abstract": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper",
    "pdf_url": "http://arxiv.org/pdf/2501.13983v4",
    "published_date": "2025-01-23 06:57:24 UTC",
    "updated_date": "2025-03-07 09:02:42 UTC"
  },
  {
    "arxiv_id": "2501.16375v1",
    "title": "On Storage Neural Network Augmented Approximate Nearest Neighbor Search",
    "authors": [
      "Taiga Ikeda",
      "Daisuke Miyashita",
      "Jun Deguchi"
    ],
    "abstract": "Large-scale approximate nearest neighbor search (ANN) has been gaining\nattention along with the latest machine learning researches employing ANNs. If\nthe data is too large to fit in memory, it is necessary to search for the most\nsimilar vectors to a given query vector from the data stored in storage\ndevices, not from that in memory. The storage device such as NAND flash memory\nhas larger capacity than the memory device such as DRAM, but they also have\nlarger latency to read data. Therefore, ANN methods for storage require\ncompletely different approaches from conventional in-memory ANN methods. Since\nthe approximation that the time required for search is determined only by the\namount of data fetched from storage holds under reasonable assumptions, our\ngoal is to minimize it while maximizing recall. For partitioning-based ANNs,\nvectors are partitioned into clusters in the index building phase. In the\nsearch phase, some of the clusters are chosen, the vectors in the chosen\nclusters are fetched from storage, and the nearest vector is retrieved from the\nfetched vectors. Thus, the key point is to accurately select the clusters\ncontaining the ground truth nearest neighbor vectors. We accomplish this by\nproposing a method to predict the correct clusters by means of a neural network\nthat is gradually refined by alternating supervised learning and duplicated\ncluster assignment. Compared to state-of-the-art SPANN and an exhaustive method\nusing k-means clustering and linear search, the proposed method achieves 90%\nrecall on SIFT1M with 80% and 58% less data fetched from storage, respectively.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.16375v1",
    "published_date": "2025-01-23 06:56:18 UTC",
    "updated_date": "2025-01-23 06:56:18 UTC"
  },
  {
    "arxiv_id": "2501.13418v1",
    "title": "Rethinking the Sample Relations for Few-Shot Classification",
    "authors": [
      "Guowei Yin",
      "Sheng Huang",
      "Luwen Huangfu",
      "Yi Zhang",
      "Xiaohong Zhang"
    ],
    "abstract": "Feature quality is paramount for classification performance, particularly in\nfew-shot scenarios. Contrastive learning, a widely adopted technique for\nenhancing feature quality, leverages sample relations to extract intrinsic\nfeatures that capture semantic information and has achieved remarkable success\nin Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning\napproaches often overlook the semantic similarity discrepancies at different\ngranularities when employing the same modeling approach for different sample\nrelations, which limits the potential of few-shot contrastive learning. In this\npaper, we introduce a straightforward yet effective contrastive learning\napproach, Multi-Grained Relation Contrastive Learning (MGRCL), as a\npre-training feature learning model to boost few-shot learning by meticulously\nmodeling sample relations at different granularities. MGRCL categorizes sample\nrelations into three types: intra-sample relation of the same sample under\ndifferent transformations, intra-class relation of homogenous samples, and\ninter-class relation of inhomogeneous samples. In MGRCL, we design\nTransformation Consistency Learning (TCL) to ensure the rigorous semantic\nconsistency of a sample under different transformations by aligning predictions\nof input pairs. Furthermore, to preserve discriminative information, we employ\nClass Contrastive Learning (CCL) to ensure that a sample is always closer to\nits homogenous samples than its inhomogeneous ones, as homogenous samples share\nsimilar semantic content while inhomogeneous samples have different semantic\ncontent. Our method is assessed across four popular FSL benchmarks, showing\nthat such a simple pre-training feature learning method surpasses a majority of\nleading FSL methods. Moreover, our method can be incorporated into other FSL\nmethods as the pre-trained model and help them obtain significant performance\ngains.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "32 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13418v1",
    "published_date": "2025-01-23 06:45:17 UTC",
    "updated_date": "2025-01-23 06:45:17 UTC"
  },
  {
    "arxiv_id": "2501.13416v2",
    "title": "M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction with Person-aware Blockwise Attention",
    "authors": [
      "Yiming Tang",
      "Abrar Anwar",
      "Jesse Thomason"
    ],
    "abstract": "Understanding social signals in multi-party conversations is important for\nhuman-robot interaction and artificial social intelligence. Social signals\ninclude body pose, head pose, speech, and context-specific activities like\nacquiring and taking bites of food when dining. Past work in multi-party\ninteraction tends to build task-specific models for predicting social signals.\nIn this work, we address the challenge of predicting multimodal social signals\nin multi-party settings in a single model. We introduce M3PT, a causal\ntransformer architecture with modality and temporal blockwise attention masking\nto simultaneously process multiple social cues across multiple participants and\ntheir temporal interactions. We train and evaluate M3PT on the Human-Human\nCommensality Dataset (HHCD), and demonstrate that using multiple modalities\nimproves bite timing and speaking status prediction. Source code:\nhttps://github.com/AbrarAnwar/masked-social-signals/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13416v2",
    "published_date": "2025-01-23 06:42:28 UTC",
    "updated_date": "2025-02-03 03:14:14 UTC"
  },
  {
    "arxiv_id": "2501.13412v1",
    "title": "Load and Renewable Energy Forecasting Using Deep Learning for Grid Stability",
    "authors": [
      "Kamal Sarkar"
    ],
    "abstract": "As the energy landscape changes quickly, grid operators face several\nchallenges, especially when integrating renewable energy sources with the grid.\nThe most important challenge is to balance supply and demand because the solar\nand wind energy are highly unpredictable. When dealing with such uncertainty,\ntrustworthy short-term load and renewable energy forecasting can help stabilize\nthe grid, maximize energy storage, and guarantee the effective use of renewable\nresources. Physical models and statistical techniques were the previous\napproaches employed for this kind of forecasting tasks. In forecasting\nrenewable energy, machine learning and deep learning techniques have recently\ndemonstrated encouraging results. More specifically, the deep learning\ntechniques like CNN and LSTM and the conventional machine learning techniques\nlike regression that are mostly utilized for load and renewable energy\nforecasting tasks. In this article, we will focus mainly on CNN and LSTM-based\nforecasting methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13412v1",
    "published_date": "2025-01-23 06:33:33 UTC",
    "updated_date": "2025-01-23 06:33:33 UTC"
  },
  {
    "arxiv_id": "2501.16374v2",
    "title": "SAFR: Neuron Redistribution for Interpretability",
    "authors": [
      "Ruidi Chang",
      "Chunyuan Deng",
      "Hanjie Chen"
    ],
    "abstract": "Superposition refers to encoding representations of multiple features within\na single neuron, which is common in deep neural networks. This property allows\nneurons to combine and represent multiple features, enabling the model to\ncapture intricate information and handle complex tasks. Despite promising\nperformance, the model's interpretability has been diminished. This paper\npresents a novel approach to enhance model interpretability by regularizing\nfeature superposition. We introduce SAFR, which simply applies regularizations\nto the loss function to promote monosemantic representations for important\ntokens while encouraging polysemanticity for correlated token pairs, where\nimportant tokens and correlated token pairs are identified via VMASK and\nattention weights respectively. We evaluate SAFR with a transformer model on\ntwo classification tasks. Experiments demonstrate the effectiveness of SAFR in\nimproving model interpretability without compromising prediction performance.\nBesides, SAFR provides explanations by visualizing the neuron allocation within\nthe intermediate layers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16374v2",
    "published_date": "2025-01-23 06:20:33 UTC",
    "updated_date": "2025-02-11 00:26:45 UTC"
  },
  {
    "arxiv_id": "2501.13400v2",
    "title": "YOLOv8 to YOLO11: A Comprehensive Architecture In-depth Comparative Review",
    "authors": [
      "Priyanto Hidayatullah",
      "Nurjannah Syakrani",
      "Muhammad Rizqi Sholahuddin",
      "Trisna Gelar",
      "Refdinal Tubagus"
    ],
    "abstract": "In the field of deep learning-based computer vision, YOLO is revolutionary.\nWith respect to deep learning models, YOLO is also the one that is evolving the\nmost rapidly. Unfortunately, not every YOLO model possesses scholarly\npublications. Moreover, there exists a YOLO model that lacks a publicly\naccessible official architectural diagram. Naturally, this engenders\nchallenges, such as complicating the understanding of how the model operates in\npractice. Furthermore, the review articles that are presently available do not\ndelve into the specifics of each model. The objective of this study is to\npresent a comprehensive and in-depth architecture comparison of the four most\nrecent YOLO models, specifically YOLOv8 through YOLO11, thereby enabling\nreaders to quickly grasp not only how each model functions, but also the\ndistinctions between them. To analyze each YOLO version's architecture, we\nmeticulously examined the relevant academic papers, documentation, and\nscrutinized the source code. The analysis reveals that while each version of\nYOLO has improvements in architecture and feature extraction, certain blocks\nremain unchanged. The lack of scholarly publications and official diagrams\npresents challenges for understanding the model's functionality and future\nenhancement. Future developers are encouraged to provide these resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13400v2",
    "published_date": "2025-01-23 05:57:13 UTC",
    "updated_date": "2025-04-08 06:11:13 UTC"
  },
  {
    "arxiv_id": "2501.13394v2",
    "title": "Concurrent Learning with Aggregated States via Randomized Least Squares Value Iteration",
    "authors": [
      "Yan Chen",
      "Qinxun Bai",
      "Yiteng Zhang",
      "Shi Dong",
      "Maria Dimakopoulou",
      "Qi Sun",
      "Zhengyuan Zhou"
    ],
    "abstract": "Designing learning agents that explore efficiently in a complex environment\nhas been widely recognized as a fundamental challenge in reinforcement\nlearning. While a number of works have demonstrated the effectiveness of\ntechniques based on randomized value functions on a single agent, it remains\nunclear, from a theoretical point of view, whether injecting randomization can\nhelp a society of agents {\\it concurently} explore an environment. The\ntheoretical results %that we established in this work tender an affirmative\nanswer to this question. We adapt the concurrent learning framework to\n\\textit{randomized least-squares value iteration} (RLSVI) with\n\\textit{aggregated state representation}. We demonstrate polynomial worst-case\nregret bounds in both finite- and infinite-horizon environments. In both setups\nthe per-agent regret decreases at an optimal rate of\n$\\Theta\\left(\\frac{1}{\\sqrt{N}}\\right)$, highlighting the advantage of\nconcurent learning. Our algorithm exhibits significantly lower space complexity\ncompared to \\cite{russo2019worst} and \\cite{agrawal2021improved}. We reduce the\nspace complexity by a factor of $K$ while incurring only a $\\sqrt{K}$ increase\nin the worst-case regret bound, compared to\n\\citep{agrawal2021improved,russo2019worst}. Additionally, we conduct numerical\nexperiments to demonstrate our theoretical findings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13394v2",
    "published_date": "2025-01-23 05:37:33 UTC",
    "updated_date": "2025-01-31 04:04:07 UTC"
  },
  {
    "arxiv_id": "2501.13372v1",
    "title": "Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for Personalized Speech Enhancement",
    "authors": [
      "Jae-Sung Bae",
      "Anastasia Kuznetsova",
      "Dinesh Manocha",
      "John Hershey",
      "Trausti Kristjansson",
      "Minje Kim"
    ],
    "abstract": "This paper presents a new challenge that calls for zero-shot text-to-speech\n(TTS) systems to augment speech data for the downstream task, personalized\nspeech enhancement (PSE), as part of the Generative Data Augmentation workshop\nat ICASSP 2025. Collecting high-quality personalized data is challenging due to\nprivacy concerns and technical difficulties in recording audio from the test\nscene. To address these issues, synthetic data generation using generative\nmodels has gained significant attention. In this challenge, participants are\ntasked first with building zero-shot TTS systems to augment personalized data.\nSubsequently, PSE systems are asked to be trained with this augmented\npersonalized dataset. Through this challenge, we aim to investigate how the\nquality of augmented data generated by zero-shot TTS models affects PSE model\nperformance. We also provide baseline experiments using open-source zero-shot\nTTS models to encourage participation and benchmark advancements. Our baseline\ncode implementation and checkpoints are available online.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to ICASSP 2025 Satellite Workshop: Generative Data\n  Augmentation for Real-World Signal Processing Applications",
    "pdf_url": "http://arxiv.org/pdf/2501.13372v1",
    "published_date": "2025-01-23 04:27:37 UTC",
    "updated_date": "2025-01-23 04:27:37 UTC"
  },
  {
    "arxiv_id": "2501.13369v1",
    "title": "A review on development of eco-friendly filters in Nepal for use in cigarettes and masks and Air Pollution Analysis with Machine Learning and SHAP Interpretability",
    "authors": [
      "Bishwash Paneru",
      "Biplov Paneru",
      "Tanka Mukhiya",
      "Khem Narayan Poudyal"
    ],
    "abstract": "In Nepal, air pollution is a serious public health concern, especially in\ncities like Kathmandu where particulate matter (PM2.5 and PM10) has a major\ninfluence on respiratory health and air quality. The Air Quality Index (AQI) is\npredicted in this work using a Random Forest Regressor, and the model's\npredictions are interpreted using SHAP (SHapley Additive exPlanations)\nanalysis. With the lowest Testing RMSE (0.23) and flawless R2 scores (1.00),\nCatBoost performs better than other models, demonstrating its greater accuracy\nand generalization which is cross validated using a nested cross validation\napproach. NowCast Concentration and Raw Concentration are the most important\nelements influencing AQI values, according to SHAP research, which shows that\nthe machine learning results are highly accurate. Their significance as major\ncontributors to air pollution is highlighted by the fact that high values of\nthese characteristics significantly raise the AQI. This study investigates the\nHydrogen-Alpha (HA) biodegradable filter as a novel way to reduce the related\nhealth hazards. With removal efficiency of more than 98% for PM2.5 and 99.24%\nfor PM10, the HA filter offers exceptional defense against dangerous airborne\nparticles. These devices, which are biodegradable face masks and cigarette\nfilters, address the environmental issues associated with traditional filters'\nnon-biodegradable trash while also lowering exposure to air contaminants.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13369v1",
    "published_date": "2025-01-23 04:16:58 UTC",
    "updated_date": "2025-01-23 04:16:58 UTC"
  },
  {
    "arxiv_id": "2501.13365v1",
    "title": "Enhanced Extractor-Selector Framework and Symmetrization Weighted Binary Cross-Entropy for Edge Detections",
    "authors": [
      "Hao Shu"
    ],
    "abstract": "Recent advancements have demonstrated the effectiveness of the\nextractor-selector (E-S) framework in edge detection (ED) tasks, which achieves\nstate-of-the-art (SOTA) performance in both quantitative metrics and perceptual\nquality. However, this method still falls short of fully exploiting the\npotential of feature extractors, as selectors only operate on highly compressed\nfeature maps that lack diversity and suffer from substantial information loss.\nAdditionally, while union training can improve perceptual quality, the highest\nevaluation scores are typically obtained without it, creating a trade-off\nbetween quantitative accuracy and perceptual fidelity. To address these\nlimitations, we propose an enhanced E-S architecture, which utilizes richer,\nless-loss feature representations and incorporates auxiliary features during\nthe selection process, thereby improving the effectiveness of the feature\nselection mechanism. Additionally, we introduce a novel loss function, the\nSymmetrization Weight Binary Cross-Entropy (SWBCE), which simultaneously\nemphasizes both the recall of edge pixels and the suppression of erroneous edge\npredictions, thereby enhancing the predictions both in the perceptual quality\nand the prediction accuracy. The effectiveness and superiority of our\napproaches over baseline models, the standard E-S framework, and the standard\nWeight Binary Cross-Entropy (WBCE) loss function are demonstrated by extensive\nexperiments. For example, our enhanced E-S architecture trained with SWBCE loss\nfunction achieves average improvements of 8.25$\\%$, 8.01$\\%$, and 33.25$\\%$ in\nODS, OIS, and AP, measured on BIPED2 compared with the baseline models,\nsignificantly outperforming the standard E-S method. The results set new\nbenchmarks for ED tasks, and highlight the potential of the methods in beyond.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2501.13365v1",
    "published_date": "2025-01-23 04:10:31 UTC",
    "updated_date": "2025-01-23 04:10:31 UTC"
  },
  {
    "arxiv_id": "2501.13347v1",
    "title": "One Fits All: General Mobility Trajectory Modeling via Masked Conditional Diffusion",
    "authors": [
      "Qingyue Long",
      "Can Rong",
      "Huandong Wang",
      "Yong Li"
    ],
    "abstract": "Trajectory data play a crucial role in many applications, ranging from\nnetwork optimization to urban planning. Existing studies on trajectory data are\ntask-specific, and their applicability is limited to the specific tasks on\nwhich they have been trained, such as generation, recovery, or prediction.\nHowever, the potential of a unified model has not yet been fully explored in\ntrajectory modeling. Although various trajectory tasks differ in inputs,\noutputs, objectives, and conditions, they share common mobility patterns. Based\non these common patterns, we can construct a general framework that enables a\nsingle model to address different tasks. However, building a trajectory\ntask-general framework faces two critical challenges: 1) the diversity in the\nformats of different tasks and 2) the complexity of the conditions imposed on\ndifferent tasks. In this work, we propose a general trajectory modeling\nframework via masked conditional diffusion (named GenMove). Specifically, we\nutilize mask conditions to unify diverse formats. To adapt to complex\nconditions associated with different tasks, we utilize historical trajectory\ndata to obtain contextual trajectory embeddings, which include rich contexts\nsuch as spatiotemporal characteristics and user preferences. Integrating the\ncontextual trajectory embedding into diffusion models through a classifier-free\nguidance approach allows the model to flexibly adjust its outputs based on\ndifferent conditions. Extensive experiments on mainstream tasks demonstrate\nthat our model significantly outperforms state-of-the-art baselines, with the\nhighest performance improvement exceeding 13% in generation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13347v1",
    "published_date": "2025-01-23 03:13:45 UTC",
    "updated_date": "2025-01-23 03:13:45 UTC"
  },
  {
    "arxiv_id": "2501.16373v1",
    "title": "Unveiling Discrete Clues: Superior Healthcare Predictions for Rare Diseases",
    "authors": [
      "Chuang Zhao",
      "Hui Tang",
      "Jiheng Zhang",
      "Xiaomeng Li"
    ],
    "abstract": "Accurate healthcare prediction is essential for improving patient outcomes.\nExisting work primarily leverages advanced frameworks like attention or graph\nnetworks to capture the intricate collaborative (CO) signals in electronic\nhealth records. However, prediction for rare diseases remains challenging due\nto limited co-occurrence and inadequately tailored approaches. To address this\nissue, this paper proposes UDC, a novel method that unveils discrete clues to\nbridge consistent textual knowledge and CO signals within a unified semantic\nspace, thereby enriching the representation semantics of rare diseases.\nSpecifically, we focus on addressing two key sub-problems: (1) acquiring\ndistinguishable discrete encodings for precise disease representation and (2)\nachieving semantic alignment between textual knowledge and the CO signals at\nthe code level. For the first sub-problem, we refine the standard vector\nquantized process to include condition awareness. Additionally, we develop an\nadvanced contrastive approach in the decoding stage, leveraging synthetic and\nmixed-domain targets as hard negatives to enrich the perceptibility of the\nreconstructed representation for downstream tasks. For the second sub-problem,\nwe introduce a novel codebook update strategy using co-teacher distillation.\nThis approach facilitates bidirectional supervision between textual knowledge\nand CO signals, thereby aligning semantically equivalent information in a\nshared discrete latent space. Extensive experiments on three datasets\ndemonstrate our superiority.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.16373v1",
    "published_date": "2025-01-23 03:08:22 UTC",
    "updated_date": "2025-01-23 03:08:22 UTC"
  },
  {
    "arxiv_id": "2501.13344v1",
    "title": "Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation",
    "authors": [
      "Rong Shan",
      "Jiachen Zhu",
      "Jianghao Lin",
      "Chenxu Zhu",
      "Bo Chen",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "In this paper, we address the lifelong sequential behavior incomprehension\nproblem in large language models (LLMs) for recommendation, where LLMs struggle\nto extract useful information from long user behavior sequences, even within\ntheir context limits. To tackle this, we propose ReLLaX (Retrieval-enhanced\nLarge Language models Plus), a framework offering optimization across data,\nprompt, and parameter levels. At the data level, we introduce Semantic User\nBehavior Retrieval (SUBR) to reduce sequence heterogeneity, making it easier\nfor LLMs to extract key information. For prompt-level enhancement, we employ\nSoft Prompt Augmentation (SPA) to inject collaborative knowledge, aligning item\nrepresentations with recommendation tasks and improving LLMs's exploration of\nitem relationships. Finally, at the parameter level, we propose Component\nFully-interactive LoRA (CFLoRA), which enhances LoRA's expressiveness by\nenabling interactions between its components, allowing better capture of\nsequential information. Moreover, we present new perspectives to compare\ncurrent LoRA-based LLM4Rec methods, i.e. from both a composite and a decomposed\nview. We theoretically demonstrate that the ways they employ LoRA for\nrecommendation are degraded versions of our CFLoRA, with different constraints\non atom component interactions. Extensive experiments on three public datasets\ndemonstrate ReLLaX's superiority over existing baselines and its ability to\nmitigate lifelong sequential behavior incomprehension effectively.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2501.13344v1",
    "published_date": "2025-01-23 03:05:13 UTC",
    "updated_date": "2025-01-23 03:05:13 UTC"
  },
  {
    "arxiv_id": "2501.13333v1",
    "title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback",
    "authors": [
      "Joshua Park",
      "Yongfeng Zhang"
    ],
    "abstract": "Multi-agent systems must decide which agent is the most appropriate for a\ngiven task. We propose a novel architecture for recommending which LLM agent\nout of many should perform a task given a natural language prompt by extending\nthe Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a\ntop-1 accuracy of 92.2% with each classification taking less than 300\nmilliseconds. In contrast to traditional classification methods, our\narchitecture is computationally cheap, adaptive to new classes, interpretable,\nand controllable with arbitrary metrics through reinforcement learning. By\nencoding natural language prompts into sentence embeddings, our model captures\nthe semantic content relevant to recommending an agent. The distance between\nsentence embeddings that belong to the same agent is then minimized through\nfine-tuning and aligned to human values through reinforcement learning from\nhuman feedback. This allows the classification of natural language prompts\nbased on their nearest neighbors by measuring the cosine similarity between\nembeddings. This work is made possible through the generation of a synthetic\ndataset for agent recommendation, which we have open-sourced to the public\nalong with the code for AgentRec recommendation system at\nhttps://github.com/joshprk/agentrec.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 8 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2501.13333v1",
    "published_date": "2025-01-23 02:25:44 UTC",
    "updated_date": "2025-01-23 02:25:44 UTC"
  },
  {
    "arxiv_id": "2501.13329v2",
    "title": "Sparse identification of nonlinear dynamics and Koopman operators with Shallow Recurrent Decoder Networks",
    "authors": [
      "Mars Liyao Gao",
      "Jan P. Williams",
      "J. Nathan Kutz"
    ],
    "abstract": "Modeling real-world spatio-temporal data is exceptionally difficult due to\ninherent high dimensionality, measurement noise, partial observations, and\noften expensive data collection procedures. In this paper, we present Sparse\nIdentification of Nonlinear Dynamics with SHallow REcurrent Decoder networks\n(SINDy-SHRED), a method to jointly solve the sensing and model identification\nproblems with simple implementation, efficient computation, and robust\nperformance. SINDy-SHRED uses Gated Recurrent Units to model the temporal\nsequence of sparse sensor measurements along with a shallow decoder network to\nreconstruct the full spatio-temporal field from the latent state space. Our\nalgorithm introduces a SINDy-based regularization for which the latent space\nprogressively converges to a SINDy-class functional, provided the projection\nremains within the set. In restricting SINDy to a linear model, a Koopman-SHRED\nmodel is generated. SINDy-SHRED (i) learns a symbolic and interpretable\ngenerative model of a parsimonious and low-dimensional latent space for the\ncomplex spatio-temporal dynamics, (ii) discovers new physics models even for\nwell-known physical systems, (iii) achieves provably robust convergence with an\nobserved globally convex loss landscape, and (iv) achieves superior accuracy,\ndata efficiency, and training time, all with fewer model parameters. We conduct\nsystematic experimental studies on PDE data such as turbulent flows, real-world\nsensor measurements for sea surface temperature, and direct video data. The\ninterpretable SINDy and Koopman models of latent state dynamics enable stable\nand accurate long-term video predictions, outperforming all current baseline\ndeep learning models in accuracy, training time, and data requirements,\nincluding Convolutional LSTM, PredRNN, ResNet, and SimVP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13329v2",
    "published_date": "2025-01-23 02:18:13 UTC",
    "updated_date": "2025-04-01 04:15:58 UTC"
  },
  {
    "arxiv_id": "2501.16372v1",
    "title": "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
    "authors": [
      "J. Pablo Muñoz",
      "Jinjie Yuan",
      "Nilesh Jain"
    ],
    "abstract": "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "AAAI-25 Workshop on Connecting Low-rank Representations in AI",
    "pdf_url": "http://arxiv.org/pdf/2501.16372v1",
    "published_date": "2025-01-23 02:14:08 UTC",
    "updated_date": "2025-01-23 02:14:08 UTC"
  },
  {
    "arxiv_id": "2501.13321v1",
    "title": "Investigation of the Privacy Concerns in AI Systems for Young Digital Citizens: A Comparative Stakeholder Analysis",
    "authors": [
      "Molly Campbell",
      "Ankur Barthwal",
      "Sandhya Joshi",
      "Austin Shouli",
      "Ajay Kumar Shrestha"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) systems into technologies\nused by young digital citizens raises significant privacy concerns. This study\ninvestigates these concerns through a comparative analysis of stakeholder\nperspectives. A total of 252 participants were surveyed, with the analysis\nfocusing on 110 valid responses from parents/educators and 100 from AI\nprofessionals after data cleaning. Quantitative methods, including descriptive\nstatistics and Partial Least Squares Structural Equation Modeling, examined\nfive validated constructs: Data Ownership and Control, Parental Data Sharing,\nPerceived Risks and Benefits, Transparency and Trust, and Education and\nAwareness. Results showed Education and Awareness significantly influenced data\nownership and risk assessment, while Data Ownership and Control strongly\nimpacted Transparency and Trust. Transparency and Trust, along with Perceived\nRisks and Benefits, showed minimal influence on Parental Data Sharing,\nsuggesting other factors may play a larger role. The study underscores the need\nfor user-centric privacy controls, tailored transparency strategies, and\ntargeted educational initiatives. Incorporating diverse stakeholder\nperspectives offers actionable insights into ethical AI design and governance,\nbalancing innovation with robust privacy protections to foster trust in a\ndigital age.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "To appear in the 2025 IEEE 14th Annual Computing and Communication\n  Workshop and Conference (CCWC) proceedings",
    "pdf_url": "http://arxiv.org/pdf/2501.13321v1",
    "published_date": "2025-01-23 02:07:45 UTC",
    "updated_date": "2025-01-23 02:07:45 UTC"
  },
  {
    "arxiv_id": "2501.13320v1",
    "title": "Toward Ethical AI: A Qualitative Analysis of Stakeholder Perspectives",
    "authors": [
      "Ajay Kumar Shrestha",
      "Sandhya Joshi"
    ],
    "abstract": "As Artificial Intelligence (AI) systems become increasingly integrated into\nvarious aspects of daily life, concerns about privacy and ethical\naccountability are gaining prominence. This study explores stakeholder\nperspectives on privacy in AI systems, focusing on educators, parents, and AI\nprofessionals. Using qualitative analysis of survey responses from 227\nparticipants, the research identifies key privacy risks, including data\nbreaches, ethical misuse, and excessive data collection, alongside perceived\nbenefits such as personalized services, enhanced efficiency, and educational\nadvancements. Stakeholders emphasized the need for transparency,\nprivacy-by-design, user empowerment, and ethical oversight to address privacy\nconcerns effectively. The findings provide actionable insights into balancing\nthe benefits of AI with robust privacy protections, catering to the diverse\nneeds of stakeholders. Recommendations include implementing selective data use,\nfostering transparency, promoting user autonomy, and integrating ethical\nprinciples into AI development. This study contributes to the ongoing discourse\non ethical AI, offering guidance for designing privacy-centric systems that\nalign with societal values and build trust among users. By addressing privacy\nchallenges, this research underscores the importance of developing AI\ntechnologies that are not only innovative but also ethically sound and\nresponsive to the concerns of all stakeholders.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "To appear in the 2025 IEEE 14th Annual Computing and Communication\n  Workshop and Conference (CCWC) proceedings",
    "pdf_url": "http://arxiv.org/pdf/2501.13320v1",
    "published_date": "2025-01-23 02:06:25 UTC",
    "updated_date": "2025-01-23 02:06:25 UTC"
  },
  {
    "arxiv_id": "2501.13978v1",
    "title": "Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation",
    "authors": [
      "Sangyeop Yeo",
      "Seung-won Hwang",
      "Yu-Seung Ma"
    ],
    "abstract": "The use of Large Language Models (LLMs) for code generation has gained\nsignificant attention in recent years. Existing methods often aim to improve\nthe quality of generated code by incorporating additional contextual\ninformation or guidance into input prompts. Many of these approaches adopt\nsequential reasoning strategies, mimicking human-like step-by-step thinking.\nHowever, such strategies may constrain flexibility, as they do not always align\nwith the structured characteristics of programming languages. This paper\nintroduces the Chain of Grounded Objectives (CGO), a method that embeds\nfunctional objectives into input prompts to enhance code generation. By\nleveraging appropriately structured objectives as input and avoiding explicit\nsequential procedures, CGO adapts effectively to the structured nature of\nprogramming tasks. Empirical evaluations demonstrate that CGO effectively\nenhances code generation, addressing limitations of existing approaches.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13978v1",
    "published_date": "2025-01-23 01:45:09 UTC",
    "updated_date": "2025-01-23 01:45:09 UTC"
  },
  {
    "arxiv_id": "2501.13302v1",
    "title": "Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers",
    "authors": [
      "Akshit Achara",
      "Anshuman Chhabra"
    ],
    "abstract": "AI Safety Moderation (ASM) classifiers are designed to moderate content on\nsocial media platforms and to serve as guardrails that prevent Large Language\nModels (LLMs) from being fine-tuned on unsafe inputs. Owing to their potential\nfor disparate impact, it is crucial to ensure that these classifiers: (1) do\nnot unfairly classify content belonging to users from minority groups as unsafe\ncompared to those from majority groups and (2) that their behavior remains\nrobust and consistent across similar inputs. In this work, we thus examine the\nfairness and robustness of four widely-used, closed-source ASM classifiers:\nOpenAI Moderation API, Perspective API, Google Cloud Natural Language (GCNL)\nAPI, and Clarifai API. We assess fairness using metrics such as demographic\nparity and conditional statistical parity, comparing their performance against\nASM models and a fair-only baseline. Additionally, we analyze robustness by\ntesting the classifiers' sensitivity to small and natural input perturbations.\nOur findings reveal potential fairness and robustness gaps, highlighting the\nneed to mitigate these issues in future versions of these models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to NAACL 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2501.13302v1",
    "published_date": "2025-01-23 01:04:00 UTC",
    "updated_date": "2025-01-23 01:04:00 UTC"
  },
  {
    "arxiv_id": "2501.13297v1",
    "title": "RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question Answering",
    "authors": [
      "Yang Bai",
      "Christan Earl Grant",
      "Daisy Zhe Wang"
    ],
    "abstract": "Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text\nand images, has gained significant attention in information retrieval (IR) and\nnatural language processing (NLP). Traditional ranking methods rely on small\nencoder-based language models, which are incompatible with modern decoder-based\ngenerative large language models (LLMs) that have advanced various NLP tasks.\nTo bridge this gap, we propose RAMQA, a unified framework combining\nlearning-to-rank methods with generative permutation-enhanced ranking\ntechniques. We first train a pointwise multi-modal ranker using LLaVA as the\nbackbone. Then, we apply instruction tuning to train a LLaMA model for\nre-ranking the top-k documents using an innovative autoregressive multi-task\nlearning approach. Our generative ranking model generates re-ranked document\nIDs and specific answers from document candidates in various permutations.\nExperiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant\nimprovements over strong baselines, highlighting the effectiveness of our\napproach. Code and data are available at: https://github.com/TonyBY/RAMQA",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by NAACL 2025 Findings",
    "pdf_url": "http://arxiv.org/pdf/2501.13297v1",
    "published_date": "2025-01-23 00:50:33 UTC",
    "updated_date": "2025-01-23 00:50:33 UTC"
  },
  {
    "arxiv_id": "2501.13295v1",
    "title": "Parallel Belief Contraction via Order Aggregation",
    "authors": [
      "Jake Chandler",
      "Richard Booth"
    ],
    "abstract": "The standard ``serial'' (aka ``singleton'') model of belief contraction\nmodels the manner in which an agent's corpus of beliefs responds to the removal\nof a single item of information. One salient extension of this model introduces\nthe idea of ``parallel'' (aka ``package'' or ``multiple'') change, in which an\nentire set of items of information are simultaneously removed. Existing\nresearch on the latter has largely focussed on single-step parallel\ncontraction: understanding the behaviour of beliefs after a single parallel\ncontraction. It has also focussed on generalisations to the parallel case of\nserial contraction operations whose characteristic properties are extremely\nweak. Here we consider how to extend serial contraction operations that obey\nstronger properties. Potentially more importantly, we also consider the\niterated case: the behaviour of beliefs after a sequence of parallel\ncontractions. We propose a general method for extending serial iterated belief\nchange operators to handle parallel change based on an n-ary generalisation of\nBooth & Chandler's TeamQueue binary order aggregators.",
    "categories": [
      "cs.AI",
      "I.2.4"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.13295v1",
    "published_date": "2025-01-23 00:42:16 UTC",
    "updated_date": "2025-01-23 00:42:16 UTC"
  },
  {
    "arxiv_id": "2501.13977v2",
    "title": "Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms",
    "authors": [
      "Rajvardhan Oak",
      "Muhammad Haroon",
      "Claire Jo",
      "Magdalena Wojcieszak",
      "Anshuman Chhabra"
    ],
    "abstract": "Social media platforms utilize Machine Learning (ML) and Artificial\nIntelligence (AI) powered recommendation algorithms to maximize user\nengagement, which can result in inadvertent exposure to harmful content.\nCurrent moderation efforts, reliant on classifiers trained with extensive\nhuman-annotated data, struggle with scalability and adapting to new forms of\nharm. To address these challenges, we propose a novel re-ranking approach using\nLarge Language Models (LLMs) in zero-shot and few-shot settings. Our method\ndynamically assesses and re-ranks content sequences, effectively mitigating\nharmful content exposure without requiring extensive labeled data. Alongside\ntraditional ranking metrics, we also introduce two new metrics to evaluate the\neffectiveness of re-ranking in reducing exposure to harmful content. Through\nexperiments on three datasets, three models and across three configurations, we\ndemonstrate that our LLM-based approach significantly outperforms existing\nproprietary moderation approaches, offering a scalable and adaptable solution\nfor harm mitigation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2025 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2501.13977v2",
    "published_date": "2025-01-23 00:26:32 UTC",
    "updated_date": "2025-05-16 13:25:28 UTC"
  },
  {
    "arxiv_id": "2501.13284v1",
    "title": "Toyteller: AI-powered Visual Storytelling Through Toy-Playing with Character Symbols",
    "authors": [
      "John Joon Young Chung",
      "Melissa Roemmele",
      "Max Kreminski"
    ],
    "abstract": "We introduce Toyteller, an AI-powered storytelling system where users\ngenerate a mix of story text and visuals by directly manipulating character\nsymbols like they are toy-playing. Anthropomorphized symbol motions can convey\nrich and nuanced social interactions; Toyteller leverages these motions (1) to\nlet users steer story text generation and (2) as a visual output format that\naccompanies story text. We enabled motion-steered text generation and\ntext-steered motion generation by mapping motions and text onto a shared\nsemantic space so that large language models and motion generation models can\nuse it as a translational layer. Technical evaluations showed that Toyteller\noutperforms a competitive baseline, GPT-4o. Our user study identified that\ntoy-playing helps express intentions difficult to verbalize. However, only\nmotions could not express all user intentions, suggesting combining it with\nother modalities like language. We discuss the design space of toy-playing\ninteractions and implications for technical HCI research on human-AI\ninteraction.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted to CHI2025",
    "pdf_url": "http://arxiv.org/pdf/2501.13284v1",
    "published_date": "2025-01-23 00:20:38 UTC",
    "updated_date": "2025-01-23 00:20:38 UTC"
  },
  {
    "arxiv_id": "2501.13976v1",
    "title": "Towards Safer Social Media Platforms: Scalable and Performant Few-Shot Harmful Content Moderation Using Large Language Models",
    "authors": [
      "Akash Bonagiri",
      "Lucen Li",
      "Rajvardhan Oak",
      "Zeerak Babar",
      "Magdalena Wojcieszak",
      "Anshuman Chhabra"
    ],
    "abstract": "The prevalence of harmful content on social media platforms poses significant\nrisks to users and society, necessitating more effective and scalable content\nmoderation strategies. Current approaches rely on human moderators, supervised\nclassifiers, and large volumes of training data, and often struggle with\nscalability, subjectivity, and the dynamic nature of harmful content (e.g.,\nviolent content, dangerous challenge trends, etc.). To bridge these gaps, we\nutilize Large Language Models (LLMs) to undertake few-shot dynamic content\nmoderation via in-context learning. Through extensive experiments on multiple\nLLMs, we demonstrate that our few-shot approaches can outperform existing\nproprietary baselines (Perspective and OpenAI Moderation) as well as prior\nstate-of-the-art few-shot learning methods, in identifying harm. We also\nincorporate visual information (video thumbnails) and assess if different\nmultimodal techniques improve model performance. Our results underscore the\nsignificant benefits of employing LLM based methods for scalable and dynamic\nharmful content moderation online.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "This paper is in submission and under peer review",
    "pdf_url": "http://arxiv.org/pdf/2501.13976v1",
    "published_date": "2025-01-23 00:19:14 UTC",
    "updated_date": "2025-01-23 00:19:14 UTC"
  },
  {
    "arxiv_id": "2501.13282v1",
    "title": "Experience with GitHub Copilot for Developer Productivity at Zoominfo",
    "authors": [
      "Gal Bakal",
      "Ali Dasdan",
      "Yaniv Katz",
      "Michael Kaufman",
      "Guy Levin"
    ],
    "abstract": "This paper presents a comprehensive evaluation of GitHub Copilot's deployment\nand impact on developer productivity at Zoominfo, a leading Go-To-Market (GTM)\nIntelligence Platform. We describe our systematic four-phase approach to\nevaluating and deploying GitHub Copilot across our engineering organization,\ninvolving over 400 developers. Our analysis combines both quantitative metrics,\nfocusing on acceptance rates of suggestions given by GitHub Copilot and\nqualitative feedback given by developers through developer satisfaction\nsurveys. The results show an average acceptance rate of 33% for suggestions and\n20% for lines of code, with high developer satisfaction scores of 72%. We also\ndiscuss language-specific performance variations, limitations, and lessons\nlearned from this medium-scale enterprise deployment. Our findings contribute\nto the growing body of knowledge about AI-assisted software development in\nenterprise settings.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "I.2.2; I.2.5; D.2.3"
    ],
    "primary_category": "cs.SE",
    "comment": "25 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.13282v1",
    "published_date": "2025-01-23 00:17:48 UTC",
    "updated_date": "2025-01-23 00:17:48 UTC"
  }
]